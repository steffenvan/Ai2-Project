,frame_name,count,ex1,ex2,ex3,ex4,ex5,Anders,Meriem,Terne,Steffen,Paul
0,Cardinal_numbers,2992,"< newSection > 1 Introduction In Information Extraction domain , named entities -LRB- NEs -RRB- are one of the most important textual units as they express an important part of the meaning of a document . | Text : one | Annotation Set : [{'name': 'Number', 'spans': [{'start': 16, 'end': 17, 'text': 'one'}]}, {'name': 'Entity', 'spans': [{'start': 17, 'end': 23, 'text': 'of the most important textual units'}]}]","We address two kinds of issues : first , we want to detect and correctly annotate corpus-specific NEs3 that the NER system could have missed ; second , we want to correct some wrong annotations provided by the existing NER system due to ambiguity . | Text : two | Annotation Set : [{'name': 'Number', 'spans': [{'start': 2, 'end': 3, 'text': 'two'}]}, {'name': 'Entity', 'spans': [{'start': 3, 'end': 4, 'text': 'kinds'}]}]","In section 3 , we give some examples of such corrections . | Text : 3 | Annotation Set : [{'name': 'Number', 'spans': [{'start': 2, 'end': 3, 'text': '3'}]}, {'name': 'Entity', 'spans': [{'start': 4, 'end': 5, 'text': 'we'}]}]","We present , in section 2 , the global architecture of our system and from ยง2.1 to ยง2.6 , we give details about each of its steps . | Text : 2 | Annotation Set : [{'name': 'Number', 'spans': [{'start': 5, 'end': 6, 'text': '2'}]}]","In section 3 , we present the evaluation of our approach when it is combined with other classic NER systems . | Text : 3 | Annotation Set : [{'name': 'Number', 'spans': [{'start': 2, 'end': 3, 'text': '3'}]}, {'name': 'Entity', 'spans': [{'start': 4, 'end': 5, 'text': 'we'}]}]",0,0,0,0,0
1,Quantity,2400,"We show that native speakers do accept quite some variation in word order , but there are also clearly factors that make certain realisation alternatives more natural . | Text : some | Annotation Set : [{'name': 'Mass', 'spans': [{'start': 9, 'end': 10, 'text': 'variation'}]}, {'name': 'Denoted_quantity', 'spans': [{'start': 8, 'end': 9, 'text': 'some'}]}]","The main aims of this experiment were : -LRB- i -RRB- to establish how much variation in German word order is acceptable for human judges , -LRB- ii -RRB- to find an automatic evaluation metric that mirrors the findings of the human evaluation , -LRB- iii -RRB- to provide detailed feedback for the designers of the surface realisation ranking model and -LRB- iv -RRB- to establish what effect preceding context has on the choice of realisation . | Text : much | Annotation Set : [{'name': 'Denoted_quantity', 'spans': [{'start': 14, 'end': 15, 'text': 'much'}]}]","F-structures are attribute-value matrices representing grammatical functions and morphosyntactic features ; roughly speaking , they are predicate-argument structures . | Text : roughly | Annotation Set : [{'name': 'Denoted_quantity', 'spans': [{'start': 11, 'end': 12, 'text': 'roughly'}]}]","The output of the realisation ranker is evaluated in terms of exact match and BLEU score , both measured against the actually observed corpus sentences . | Text : both | Annotation Set : [{'name': 'Quantity', 'spans': [{'start': 17, 'end': 18, 'text': 'both'}]}]","The output of the realisation ranker is evaluated in terms of exact match and BLEU score , both measured against the actually observed corpus sentences . | Text : measured | Annotation Set : [{'name': 'Quantity', 'spans': [{'start': 18, 'end': 19, 'text': 'measured'}]}]",0,0,0,0,0
2,Using,2022,"< newSection > Abstract This paper presents an application of finite state transducers weighted with feature structure descriptions , following Amtrup -LRB- 2003 -RRB- , to the morphology of the Semitic language Tigrinya . | Text : application | Annotation Set : [{'name': 'Instrument', 'spans': [{'start': 9, 'end': 18, 'text': 'of finite state transducers weighted with feature structure descriptions'}]}]","The relationship between these levels has concerned many phonologists and morphologists over the years , and traditional descriptions , since the pioneering work of Chomsky and Halle -LRB- 1968 -RRB- , have characterized it in terms of a series of ordered content-sensitive rewrite rules , which apply in the generation , but not the analysis , direction . | Text : apply | Annotation Set : [{'name': 'Agent', 'spans': [{'start': 45, 'end': 46, 'text': 'which'}]}, {'name': 'Place', 'spans': [{'start': 47, 'end': 50, 'text': 'in the generation'}]}]","Within computational morphology , a very significant advance came with the demonstration that phonological rules could be implemented as finite state transducers -LRB- Johnson , 1972 ; Kaplan and Kay , 1994 -RRB- -LRB- FSTs -RRB- and that the rule ordering could be dispensed with using FSTs that relate the surface and lexical levels directly -LRB- Koskenniemi , 1983 -RRB- . | Text : using | Annotation Set : [{'name': 'Instrument', 'spans': [{'start': 46, 'end': 47, 'text': 'FSTs'}]}]","Given the extra compileand-replace operation , this resulting system maps directly between abstract lexical expressions and surface strings . | Text : operation | Annotation Set : []","In addition to Arabic , this approach has been applied to a portion of the verb morphology system of the Ethio-Semitic language Amharic -LRB- Amsalu and Demeke , 2006 -RRB- , which is characterized by all of the same sorts of complexity as Tigrinya . | Text : applied | Annotation Set : [{'name': 'Instrument', 'spans': [{'start': 5, 'end': 7, 'text': 'this approach'}]}, {'name': 'Purpose', 'spans': [{'start': 10, 'end': 44, 'text': 'to a portion of the verb morphology system of the Ethio-Semitic language Amharic -LRB- Amsalu and Demeke , 2006 -RRB- , which is characterized by all of the same sorts of complexity as Tigrinya'}]}]",0,0,0,0,0
3,Simple_name,1587,< newSection > 1 Introduction Word Sense Disambiguation -LRB- WSD -RRB- is a key enabling-technology that automatically chooses the intended sense of a word in context . | Text : word | Annotation Set : [],Traditional knowledge-based WSD systems assign a sense to an ambiguous word by comparing each of its senses with those of the surrounding context . | Text : word | Annotation Set : [],One of the major drawbacks of these approaches stems from the fact that senses are compared in a pairwise fashion and thus the number of computations can grow exponentially with the number of words . | Text : words | Annotation Set : [],"Although alternatives like simulated annealing -LRB- Cowie et al. , 1992 -RRB- and conceptual density -LRB- Agirre and Rigau , 1996 -RRB- were tried , most of past knowledge based WSD was done in a suboptimal word-by-word process , i.e. , disambiguating words one at a time . | Text : words | Annotation Set : [{'name': 'Entity', 'spans': [{'start': 43, 'end': 47, 'text': 'one at a time'}]}]","Graphbased WSD methods are particularly suited for disambiguating word sequences , and they manage to exploit the interrelations among the senses in the given context . | Text : word | Annotation Set : []",0,0,0,0,0
4,Likelihood,1461,"It is explained how metadata are the key to the added value of techniques such as text and link mining , and an outline is given of what measures could be taken to increase the chances for a bright future for the old ties between NLP and the humanities . | Text : chances | Annotation Set : [{'name': 'Hypothetical_event', 'spans': [{'start': 36, 'end': 49, 'text': 'for a bright future for the old ties between NLP and the humanities'}]}]","The liaison was never constrained to linguistics ; also philosophical , philological and literary studies have had their impact on NLP , and there have always been dedicated conferences and journals for the humanities and the NLP community of which the journal Computers and the Humanities -LRB- 1966-2004 -RRB- is probably known best . | Text : probably | Annotation Set : [{'name': 'Hypothetical_event', 'spans': [{'start': 44, 'end': 46, 'text': 'the Humanities'}]}]","Nowadays semantic layers can be analysed at much more complex levels of granularity . | Text : can | Annotation Set : [{'name': 'Hypothetical_event', 'spans': [{'start': 2, 'end': 3, 'text': 'layers'}]}]","Even though the humanities have been able to conduct NLP-empowered research that would have been impossible without the the early tools and resources already for many decades , the more recent introduction of statistical methods in langauge is affecting research practises in the humanities at yet another scale . | Text : impossible | Annotation Set : [{'name': 'Hypothetical_event', 'spans': [{'start': 11, 'end': 12, 'text': 'that'}]}]","All kinds of initiatives for converting analogue resources into data sets that can be stored in digital repositories have been initiated . | Text : can | Annotation Set : [{'name': 'Hypothetical_event', 'spans': [{'start': 9, 'end': 11, 'text': 'data sets'}]}]",0,0,0,0,0
5,Causation,1318,"< newSection > 1 Introduction The output of an automatic speech recognition -LRB- ASR -RRB- system is often not what is required for subsequent processing , in part because speakers themselves often make mistakes -LRB- e.g. stuttering , selfcorrecting , or using filler words -RRB- . | Text : make | Annotation Set : [{'name': 'Cause', 'spans': [{'start': 28, 'end': 30, 'text': 'because speakers'}]}, {'name': 'Effect', 'spans': [{'start': 33, 'end': 44, 'text': 'mistakes -LRB- e.g. stuttering , selfcorrecting , or using filler words'}]}]","Such a system could also be applied not only to spontaneous English speech , but to correct common mistakes made by non-native speakers -LRB- Lee and Seneff , 2006 -RRB- , and possibly extended to non-English speaker errors . | Text : made | Annotation Set : [{'name': 'Effect', 'spans': [{'start': 17, 'end': 19, 'text': 'common mistakes'}]}]","The presence of disfluencies were found to hurt SMT in two ways : making utterances longer without adding semantic content -LRB- and sometimes adding false content -RRB- and exacerbating the data mismatch between the spontaneous input and the clean text training data . | Text : making | Annotation Set : [{'name': 'Effect', 'spans': [{'start': 15, 'end': 42, 'text': 'longer without adding semantic content -LRB- and sometimes adding false content -RRB- and exacerbating the data mismatch between the spontaneous input and the clean text training data'}]}]","Common simple disfluencies in sentence-like utterances -LRB- SUs -RRB- include filler words -LRB- i.e. `` um '' , `` ah '' , and discourse markers like `` you know '' -RRB- , as well as speaker edits consisting of a reparandum , an interruption point -LRB- IP -RRB- , an optional interregnum -LRB- like `` I mean '' -RRB- , and a repair region -LRB- Shriberg , 1994 -RRB- , as seen in Figure 1. and other examples , reparandum regions are in brackets -LRB- โ -LSB- โ , โ -RSB- โ -RRB- , interregna are in braces -LRB- 'I ' ' 'f ' , โ -RCB- โ -RRB- , and interruption points are marked by โ+โ . | Text : mean | Annotation Set : [{'name': 'Cause', 'spans': [{'start': 55, 'end': 56, 'text': 'I'}]}, {'name': 'Effect', 'spans': [{'start': 61, 'end': 64, 'text': 'a repair region'}]}]","-LRB- 2004 -RRB- , but otherwise appears to produce comparable results to those reported . | Text : results | Annotation Set : [{'name': 'Cause', 'spans': [{'start': 9, 'end': 10, 'text': 'comparable'}]}]",0,0,0,0,0
6,Increment,1283,"The literacy skills of the readers , their motivations , background knowledge , and other internal characteristics play an important role in determining whether a text is readable for a particular group of people . | Text : other | Annotation Set : [{'name': 'Class', 'spans': [{'start': 15, 'end': 16, 'text': 'internal'}]}]","Our research addresses two literacy impairments that distinguish people with ID from other low-literacy adults : limitations in -LRB- 1 -RRB- working memory and -LRB- 2 -RRB- discourse representation . | Text : other | Annotation Set : [{'name': 'Class', 'spans': [{'start': 13, 'end': 14, 'text': 'low-literacy'}]}]","Other formulas rely on lexical information ; e.g. , the New Dale-Chall readability formula consults a static , manually-built list of `` easy '' words to determine whether a text contains unfamiliar words -LRB- Chall and Dale , 1995 -RRB- . | Text : Other | Annotation Set : [{'name': 'Class', 'spans': [{'start': 1, 'end': 2, 'text': 'formulas'}]}]","Researchers in computational linguistics have investigated the use of statistical language models -LRB- unigram in particular -RRB- to capture the range of vocabulary from one grade level to another -LRB- Si and Callan , 2001 ; Collins-Thompson and Callan , 2004 -RRB- . | Text : another | Annotation Set : [{'name': 'Added_set', 'spans': [{'start': 28, 'end': 35, 'text': 'another -LRB- Si and Callan , 2001'}]}]","The Yngve -LRB- 1960 -RRB- measure , for instance , focuses on the depth of embedding of nodes in the parse tree ; others use the ratio of terminal to nonterminal nodes in the parse tree of a sentence -LRB- Miller and Chomsky , 1963 ; Frazier , 1985 -RRB- . | Text : others | Annotation Set : []",0,0,0,0,0
7,Means,1237,"< newSection > Abstract This paper describes a method using morphological rules and heuristics , for the automatic extraction of large-coverage lexicons of stems and root word-forms from a raw text corpus . | Text : method | Annotation Set : [{'name': 'Means', 'spans': [{'start': 9, 'end': 14, 'text': 'using morphological rules and heuristics'}]}]","In some previous work the process of lexicon extraction involves incremental or post-construction manual validation of the entire lexicon -LRB- Clยดement et al. , 2004 ; Sagot , 2005 ; Forsberg et al. , 2006 ; Sagot et al. , 2006 ; Sagot , 2007 -RRB- . | Text : process | Annotation Set : [{'name': 'Means', 'spans': [{'start': 4, 'end': 9, 'text': 'the process of lexicon extraction'}]}]","Our method attempts to improve on and extend the previous work by increasing the precision and recall of the system to such a point that manual validation might even be rendered unnecessary . | Text : method | Annotation Set : [{'name': 'Means', 'spans': [{'start': 1, 'end': 2, 'text': 'method'}]}]","Yet another difference , to our knowledge , is that in our method we cast the problem of lexicon extraction as two subproblems : that of stemming and following it , that of root word-form selection . | Text : method | Annotation Set : [{'name': 'Means', 'spans': [{'start': 13, 'end': 14, 'text': 'we'}]}]","In the rest of the paper , we provide a brief overview of the morphological features of the Hindi language , followed by a description of our method including the specification of rules , the corpora and the heuristics for stemming and root word-form selection . | Text : method | Annotation Set : [{'name': 'Means', 'spans': [{'start': 27, 'end': 28, 'text': 'method'}]}]",0,0,0,0,0
8,Statement,1207,"Likewise , it is not clear how to learn from consistently misannotated data ; studies often only note the presence of errors or eliminate them from evaluation -LRB- e.g. , Hogan , 2007 -RRB- , and a previous attempt at correction was limited to POS annotation -LRB- Dickinson , 2006 -RRB- . | Text : note | Annotation Set : [{'name': 'Message', 'spans': [{'start': 18, 'end': 22, 'text': 'the presence of errors'}]}]","The nucleus with its repeated surrounding context is referred to as a variation n-gram . | Text : referred | Annotation Set : [{'name': 'Message', 'spans': [{'start': 3, 'end': 7, 'text': 'its repeated surrounding context'}]}]","While the original proposal expanded the context as far as possible given the repeated n-gram , using only the immediately surrounding words as context is sufficient for detecting errors with high precision -LRB- Boyd et al. , 2008 -RRB- . | Text : proposal | Annotation Set : []","Namely , ambiguity class information -LRB- e.g. , IN/RB/RP -RRB- is added to each corpus position for training , creating complex ambiguity tags , such as < IN/RB/RP , IN > . | Text : added | Annotation Set : [{'name': 'Message', 'spans': [{'start': 12, 'end': 18, 'text': 'to each corpus position for training'}]}]","< newSection > 3 Modeling the data For our data set , we use the written portion -LRB- sec-tions P and G -RRB- of the Swedish Talbanken05 treebank -LRB- Nivre et al. , 2006 -RRB- , a reconstruction of the Talbanken76 corpus -LRB- Einarsson , 1976 -RRB- The written data of Talbanken05 consists of 11 , 431 sentences with 197 , 123 tokens , annotated using 69 types of dependency relations . | Text : written | Annotation Set : [{'name': 'Message', 'spans': [{'start': 16, 'end': 17, 'text': 'portion'}]}]",0,0,0,0,0
9,Intentionally_act,1085,"Given such a rule collection , the next step to focus on is how to successfully use it in NLP applications . | Text : step | Annotation Set : [{'name': 'Act', 'spans': [{'start': 7, 'end': 8, 'text': 'next'}]}, {'name': 'Purpose', 'spans': [{'start': 9, 'end': 12, 'text': 'to focus on'}]}]","The algorithm does not extract directional inference rules , it can only identify candidate paraphrases ; many of the rules are however unidirectional . | Text : does | Annotation Set : [{'name': 'Agent', 'spans': [{'start': 0, 'end': 2, 'text': 'The algorithm'}]}]","In -LRB- Clark et al. , 2007 -RRB- 's approach , semantic parsing to clause representation is performed and true entailment is decided only if every clause in the semantic representation of T semantically matches some clause in H . | Text : performed | Annotation Set : [{'name': 'Act', 'spans': [{'start': 19, 'end': 39, 'text': 'true entailment is decided only if every clause in the semantic representation of T semantically matches some clause in H'}]}]","In their system a paraphrase substitution step is added on top of a system based on a tree alignment algorithm . | Text : step | Annotation Set : [{'name': 'Act', 'spans': [{'start': 3, 'end': 7, 'text': 'a paraphrase substitution step'}]}]","The basic paraphrase substitution method follows three steps . | Text : steps | Annotation Set : [{'name': 'Act', 'spans': [{'start': 7, 'end': 8, 'text': 'steps'}]}]",0,0,0,0,0
10,Purpose,1052,"Our goal is to create an automatic metric to predict the readability of local news articles for adults with ID . | Text : goal | Annotation Set : [{'name': 'Goal', 'spans': [{'start': 3, 'end': 20, 'text': 'to create an automatic metric to predict the readability of local news articles for adults with ID'}]}, {'name': 'Agent', 'spans': [{'start': 0, 'end': 1, 'text': 'Our'}]}]","Because of the low levels of written literacy among our target users , we intend to focus on comprehension of texts displayed on a computer screen and read aloud by text-to-speech software ; although some users may depend on the text-tospeech software , we use the term readability . | Text : target | Annotation Set : []","Because of the low levels of written literacy among our target users , we intend to focus on comprehension of texts displayed on a computer screen and read aloud by text-to-speech software ; although some users may depend on the text-tospeech software , we use the term readability . | Text : intend | Annotation Set : [{'name': 'Goal', 'spans': [{'start': 15, 'end': 32, 'text': 'to focus on comprehension of texts displayed on a computer screen and read aloud by text-to-speech software'}]}, {'name': 'Agent', 'spans': [{'start': 13, 'end': 14, 'text': 'we'}]}]","Researchers in computational linguistics have investigated the use of statistical language models -LRB- unigram in particular -RRB- to capture the range of vocabulary from one grade level to another -LRB- Si and Callan , 2001 ; Collins-Thompson and Callan , 2004 -RRB- . | Text : use | Annotation Set : []","The use of syntactic features was also investigated -LRB- Schwarm and Ostendorf , 2005 ; Heilman et al. , 2007 ; Petersen and Ostendorf , 2009 -RRB- in the assessment of text readability for English as a Second Language readers . | Text : use | Annotation Set : []",0,0,0,0,0
11,Aggregate,1048,"Thus , every passage received four sets of corrections made collaboratively with the system and four sets of corrections made based solely on the participantsโ internal language models . | Text : sets | Annotation Set : [{'name': 'Aggregate', 'spans': [{'start': 6, 'end': 7, 'text': 'sets'}]}, {'name': 'Individuals', 'spans': [{'start': 7, 'end': 20, 'text': 'of corrections made collaboratively with the system and four sets of corrections made'}]}]","Thus , every passage received four sets of corrections made collaboratively with the system and four sets of corrections made based solely on the participantsโ internal language models . | Text : sets | Annotation Set : [{'name': 'Aggregate', 'spans': [{'start': 16, 'end': 17, 'text': 'sets'}]}, {'name': 'Individuals', 'spans': [{'start': 17, 'end': 20, 'text': 'of corrections made'}]}]","The outcomes between different groups of users are compared , and the significance of the difference is determined using the two-sample t-test assuming unequal variances . | Text : groups | Annotation Set : [{'name': 'Aggregate', 'spans': [{'start': 4, 'end': 5, 'text': 'groups'}]}, {'name': 'AggregateProperty', 'spans': [{'start': 3, 'end': 4, 'text': 'different'}]}]","Participantsโ Background For this study , we strove to maintain a relatively heterogeneous population ; participants were selected to be varied in their exposures to NLP , experiences with foreign languages , as well as their age and gender . | Text : population | Annotation Set : [{'name': 'Aggregate', 'spans': [{'start': 13, 'end': 14, 'text': 'population'}]}]","Each judge is then shown a set of candidate translations -LRB- the original MT output , an alternative translation by a bilingual speaker , and corrected translations by the participants -RRB- in a randomized order . | Text : set | Annotation Set : [{'name': 'Aggregate', 'spans': [{'start': 6, 'end': 7, 'text': 'set'}]}]",0,0,0,0,0
12,Gizmo,1034,"This is also the main reason why most summarization systems applied to news articles do not outperform a simple baseline that just uses the first 100 words of an article -LRB- Svore et al. , 2007 ; Nenkova , 2005 -RRB- . | Text : systems | Annotation Set : [{'name': 'Use', 'spans': [{'start': 8, 'end': 9, 'text': 'summarization'}]}]","The weights of the features are estimated using machine learning techniques , trained on an annotated corpus . | Text : machine | Annotation Set : []",The system we have implemented is named AURUM : AUtomatic Retrieval of Unique information with Machine learning . | Text : Machine | Annotation Set : [],"The KEA keyphrase extraction system -LRB- Witten et al. , 1999 -RRB- mainly relies on purely statistical features such as term frequencies , using the tf.idf measure from Information Retrieval , 1 as well as on a term 's position in the text . | Text : system | Annotation Set : []","In addition to tf.idf scores , Hulth -LRB- 2004 -RRB- uses part-of-speech tags and NP chunks and complements this with machine learning ; the latter has been used to good results in similar cases -LRB- Turney , 2000 ; Neto et al. , 2002 -RRB- . | Text : machine | Annotation Set : []",0,0,0,0,0
13,Similarity,971,"This approach assumes and results in some type of uniformity regarding the nature of the induced senses : clear-cut -LRB- e.g. homonymic -RRB- and finer sense distinctions are all handled in the same way . | Text : distinctions | Annotation Set : [{'name': 'Entities', 'spans': [{'start': 25, 'end': 26, 'text': 'sense'}]}]","For instance , a SL word w having three equivalents -LRB- a , b and c -RRB- is considered to have three distinct senses -LRB- described as `w-a ' , `w-b ' et `w-c -RRB- ' . | Text : distinct | Annotation Set : []","The assumption of biunivocal -LRB- one-to-one -RRB- correspondences between senses and equivalents disregards the fact that semantically similar equivalents may be used to translate the same sense of a SL word in context . | Text : similar | Annotation Set : [{'name': 'Entity_1', 'spans': [{'start': 18, 'end': 19, 'text': 'equivalents'}]}]","different from the one effectively carried by an instance of an ambiguous word , but not totally wrong , this is directly considered as a false choice . | Text : different | Annotation Set : [{'name': 'Entity_2', 'spans': [{'start': 1, 'end': 18, 'text': 'from the one effectively carried by an instance of an ambiguous word , but not totally wrong'}]}]","A differing weighting of WSD errors would be preferable in these cases , if sense distance information was available -LRB- Resnik and Yarowsky , 2000 -RRB- . | Text : differing | Annotation Set : []",0,0,0,0,0
14,Temporal_collocation,950,"Indeed , early work has demonstrated that syntactic features , and branching properties in particular , are helpful features for automatically distinguishing human translations from machine translations -LRB- Corston-Oliver et al. , 2001 -RRB- . | Text : early | Annotation Set : []",Branching preference mismatch manifest themselves in the English output when translating from languages whose branching properties are radically different from English . | Text : when | Annotation Set : [],"Generally , longer sentences are syntactically more complex but when sentences are approximately the same length the larger parse tree depth can be indicative of increased complexity that can slow processing and lead to lower perceived fluency of the sentence . | Text : when | Annotation Set : []","The length in number of words of each phrase type was counted , then divided by the sentence length . | Text : then | Annotation Set : [{'name': 'Trajector_event', 'spans': [{'start': 0, 'end': 10, 'text': 'The length in number of words of each phrase type'}]}]","One can conclude then that a model of fluency/readability that will allow systems to produce fluent text is key for developing a successful machine translation system . | Text : then | Annotation Set : [{'name': 'Trajector_event', 'spans': [{'start': 4, 'end': 26, 'text': 'that a model of fluency/readability that will allow systems to produce fluent text is key for developing a successful machine translation system'}]}]",0,0,0,0,0
15,Being_obligated,857,"< newSection > 1 Introduction Numerous NLP tasks utilize lexical databases that incorporate concepts -LRB- or word categories -RRB- : sets of terms that share a significant aspect of their meanings -LRB- e.g. , terms denoting types of food , tool names , etc -RRB- . | Text : tasks | Annotation Set : []","In both cases a concept as a set of words should be translated as a whole from one language to another . | Text : should | Annotation Set : [{'name': 'Responsible_party', 'spans': [{'start': 3, 'end': 10, 'text': 'a concept as a set of words'}]}, {'name': 'Duty', 'spans': [{'start': 12, 'end': 13, 'text': 'translated'}]}]","Unlike in the majority of recent studies where the acquisition framework is designed with specific languages in mind , in our task the algorithm should be able to deal well with a wide variety of target languages without any significant manual adaptations . | Text : task | Annotation Set : [{'name': 'Responsible_party', 'spans': [{'start': 20, 'end': 21, 'text': 'our'}]}]","Unlike in the majority of recent studies where the acquisition framework is designed with specific languages in mind , in our task the algorithm should be able to deal well with a wide variety of target languages without any significant manual adaptations . | Text : should | Annotation Set : [{'name': 'Responsible_party', 'spans': [{'start': 22, 'end': 24, 'text': 'the algorithm'}]}, {'name': 'Duty', 'spans': [{'start': 27, 'end': 42, 'text': 'to deal well with a wide variety of target languages without any significant manual adaptations'}]}]","Many MT tasks require automated creation or improvement of dictionaries -LRB- Koehn and Knight , 2001 -RRB- . | Text : tasks | Annotation Set : [{'name': 'Duty', 'spans': [{'start': 2, 'end': 3, 'text': 'tasks'}]}]",0,0,0,0,0
16,Sentencing,844,The idea that features such as average word length and average sentence length could allow an author to be identified dates to Mendenhall -LRB- 1887 -RRB- . | Text : sentence | Annotation Set : [],"LIWC2001 analyzes text and produces 88 output variables , among them word count and average words per sentence . | Text : sentence | Annotation Set : []","Given an input piece of text -LRB- typically one sentence , or a small set of contiguous sentences -RRB- , we want to disambiguate all open-class words in the input taken the rest as context . | Text : sentence | Annotation Set : []","Given an input piece of text -LRB- typically one sentence , or a small set of contiguous sentences -RRB- , we want to disambiguate all open-class words in the input taken the rest as context . | Text : sentences | Annotation Set : []","In our experiments we build a context of at least 20 content words for each sentence to be disambiguated , taking the sentences immediately before and after it in the case that the original sentence was too short . | Text : sentence | Annotation Set : []",0,0,0,0,0
17,Instance,828,"Using the full SMT sentences , we get an added advantage of being able to detect one of the major errors of this technique , also identified by -LRB- Munteanu and Marcu , 2005 -RRB- , i.e , the cases where the initial sentences are identical but the retrieved sentence has a tail of extra words at sentence end . | Text : cases | Annotation Set : [{'name': 'Type', 'spans': [{'start': 40, 'end': 59, 'text': 'where the initial sentences are identical but the retrieved sentence has a tail of extra words at sentence end'}]}, {'name': 'Instance', 'spans': [{'start': 39, 'end': 40, 'text': 'cases'}]}]","In the next section we first describe the baseline SMT system trained on human-provided translations only . | Text : only | Annotation Set : [{'name': 'Type', 'spans': [{'start': 13, 'end': 15, 'text': 'human-provided translations'}]}]","The translation model was trained on the news-commentary corpus -LRB- 1.56M words -RRB- 1 and a bilingual dictionary of about 500k entries.2 This system uses only a limited amount of human-translated parallel texts , in comparison to the bitexts that are available in NIST evaluations . | Text : only | Annotation Set : []","Note that only one reference translation is available . | Text : only | Annotation Set : [{'name': 'Type', 'spans': [{'start': 3, 'end': 4, 'text': 'one'}]}]","In our experiments we considered only the most recent texts -LRB- 2002-2006 , 5.5M sentences ; about 217M French words -RRB- . | Text : only | Annotation Set : []",0,0,0,0,0
18,Relational_quantity,803,"On a different vein of research , current POS tagging technology deals with much larger quantities of training data than treebanks can provide , and lexicon-based unsupervised approaches to POS tagging are practically unlimited in the amount of training data they can use . | Text : much | Annotation Set : [{'name': 'Denoted_quantity', 'spans': [{'start': 13, 'end': 14, 'text': 'much'}]}]","External lexical information enhances unlexicalized parsing performance by as much as 6.67 F-points , an error reduction of 20 % over a Treebank-only parser . | Text : much | Annotation Set : [{'name': 'Denoted_quantity', 'spans': [{'start': 9, 'end': 10, 'text': 'much'}]}]","It is based on a lexicon of roughly 25 , 000 word lemmas and their inflection patterns . | Text : roughly | Annotation Set : [{'name': 'Denoted_quantity', 'spans': [{'start': 7, 'end': 8, 'text': 'roughly'}]}]","Roughly 1 , 500 unique tokens from the Hebrew Treebank can not be assigned any analysis by the KC Lexicon , and Adler et al. -LRB- 2008a -RRB- report that roughly 4.5 % of the tokens in a 42M tokens corpus of news text are unknown to the Lexicon . | Text : Roughly | Annotation Set : [{'name': 'Denoted_quantity', 'spans': [{'start': 0, 'end': 1, 'text': 'Roughly'}]}]","Roughly 1 , 500 unique tokens from the Hebrew Treebank can not be assigned any analysis by the KC Lexicon , and Adler et al. -LRB- 2008a -RRB- report that roughly 4.5 % of the tokens in a 42M tokens corpus of news text are unknown to the Lexicon . | Text : roughly | Annotation Set : [{'name': 'Reference_quantity', 'spans': [{'start': 31, 'end': 32, 'text': '4.5'}]}, {'name': 'Denoted_quantity', 'spans': [{'start': 30, 'end': 31, 'text': 'roughly'}]}]",0,0,0,0,0
19,Text,801,"Instead , we propose a novel , layered approach -LRB- Sec. 2.1 -RRB- , in which syntactic -LRB- TB -RRB- tags are viewed as contextual refinements of the lexicon -LRB- KC -RRB- tags , and conversely , KC tags are viewed as lexical clustering of the syntactic ones . | Text : novel | Annotation Set : [{'name': 'Text', 'spans': [{'start': 5, 'end': 6, 'text': 'novel'}]}]","Hebrew parsing is further complicated by the fact that common prepositions , conjunctions and articles are prefixed to the following word and pronominal elements often appear as suffixes . | Text : articles | Annotation Set : [{'name': 'Text', 'spans': [{'start': 14, 'end': 15, 'text': 'articles'}]}]","Roughly 1 , 500 unique tokens from the Hebrew Treebank can not be assigned any analysis by the KC Lexicon , and Adler et al. -LRB- 2008a -RRB- report that roughly 4.5 % of the tokens in a 42M tokens corpus of news text are unknown to the Lexicon . | Text : report | Annotation Set : [{'name': 'Text', 'spans': [{'start': 28, 'end': 29, 'text': 'report'}]}]","6A `` tag '' in this context means the complete morphological information available for a morpheme in the Treebank : its part of speech , inflectional features and possessive suffixes , but not prefixes or nominative and accusative suffixes , which are taken to be separate morphemes . | Text : speech | Annotation Set : [{'name': 'Text', 'spans': [{'start': 23, 'end': 24, 'text': 'speech'}]}]","In practice , we propose an integrated representation of the tree in which the bottommost layer represents the yield of the tree , the surface forms are tagged with dictionary-based KC POS tags , and syntactic TB POS tags are in turn mapped onto the KC ones -LRB- see Figure 1 -RRB- . | Text : see | Annotation Set : [{'name': 'Source_of_information', 'spans': [{'start': 49, 'end': 51, 'text': 'Figure 1'}]}]",0,0,0,0,0
20,Leadership,783,"Our syntactic trees are annotated with a syntactic head for each constituent . | Text : head | Annotation Set : [{'name': 'Leader', 'spans': [{'start': 8, 'end': 9, 'text': 'head'}]}, {'name': 'Governed', 'spans': [{'start': 7, 'end': 8, 'text': 'syntactic'}]}]","head -LRB- i -RRB- returns the index of the head of i . | Text : head | Annotation Set : [{'name': 'Leader', 'spans': [{'start': 0, 'end': 1, 'text': 'head'}]}]","head -LRB- i -RRB- returns the index of the head of i . | Text : head | Annotation Set : [{'name': 'Leader', 'spans': [{'start': 9, 'end': 10, 'text': 'head'}]}, {'name': 'Governed', 'spans': [{'start': 10, 'end': 12, 'text': 'of i'}]}]","The head of a POS tag is its own position . | Text : head | Annotation Set : [{'name': 'Leader', 'spans': [{'start': 1, 'end': 2, 'text': 'head'}]}, {'name': 'Governed', 'spans': [{'start': 2, 'end': 6, 'text': 'of a POS tag'}]}]","Feature CrdBin counts binary events involving the heads of coordinated phrases . | Text : heads | Annotation Set : [{'name': 'Leader', 'spans': [{'start': 7, 'end': 8, 'text': 'heads'}]}, {'name': 'Governed', 'spans': [{'start': 8, 'end': 11, 'text': 'of coordinated phrases'}]}]",0,0,0,0,0
21,Categorization,779,"Following this , we derive a dependency-based structure representation from texts , which aims to provide a proper base for the inference rule application . | Text : representation | Annotation Set : [{'name': 'Speaker', 'spans': [{'start': 3, 'end': 4, 'text': 'we'}]}]","One way to deal with textual inference is through rule representation , for example X wrote Y Pz๏ฟฝ X is author of Y . | Text : representation | Annotation Set : []","In particular , we use a structure representation derived from the dependency parse trees of T and H , which aims to capture the essential information they convey . | Text : representation | Annotation Set : [{'name': 'Speaker', 'spans': [{'start': 3, 'end': 4, 'text': 'we'}]}]","Section 4 discusses the application of the rules for the RTE data , describing the structure representation we use to identify the appropriate context for the rule application . | Text : describing | Annotation Set : [{'name': 'Item', 'spans': [{'start': 14, 'end': 17, 'text': 'the structure representation'}]}]","Section 4 discusses the application of the rules for the RTE data , describing the structure representation we use to identify the appropriate context for the rule application . | Text : representation | Annotation Set : []",0,0,0,0,0
22,Communicate_categorization,761,"In each iteration of self-training , the system labels the training corpus and its decisions are treated as input for the next training phase . | Text : treated | Annotation Set : [{'name': 'Item', 'spans': [{'start': 9, 'end': 15, 'text': 'the training corpus and its decisions'}]}, {'name': 'Category', 'spans': [{'start': 17, 'end': 24, 'text': 'as input for the next training phase'}]}]","There are also several papers which treat coference as an unsupervised clustering problem -LRB- Cardie and Wagstaff , 1999 ; Angheluta et al. , 2004 -RRB- . | Text : treat | Annotation Set : [{'name': 'Speaker', 'spans': [{'start': 4, 'end': 5, 'text': 'papers'}]}, {'name': 'Item', 'spans': [{'start': 7, 'end': 8, 'text': 'coference'}]}, {'name': 'Category', 'spans': [{'start': 8, 'end': 19, 'text': 'as an unsupervised clustering problem -LRB- Cardie and Wagstaff , 1999'}]}]","The system described here handles all of these cases . | Text : described | Annotation Set : [{'name': 'Manner', 'spans': [{'start': 3, 'end': 4, 'text': 'here'}]}, {'name': 'Speaker', 'spans': [{'start': 0, 'end': 2, 'text': 'The system'}]}, {'name': 'Item', 'spans': [{'start': 5, 'end': 9, 'text': 'all of these cases'}]}]","< newSection > 6 Definition of Correctness We evaluate all programs according to Mitkov 's `` resolution etiquette '' scoring metric -LRB- also used in Cherry and Bergsma -LRB- 2005 -RRB- -RRB- , which is defined as follows : if N is the number of non-anaphoric pronouns correctly identified , A the number of anaphoric pronouns correctly linked to their antecedent , and P the total number of pronouns , then a pronoun-anaphora program 's percentage correct Most papers dealing with pronoun coreference use this simple ratio , or the variant that ignores non-anaphoric pronouns . | Text : Definition | Annotation Set : [{'name': 'Item', 'spans': [{'start': 5, 'end': 7, 'text': 'of Correctness'}]}]","< newSection > 6 Definition of Correctness We evaluate all programs according to Mitkov 's `` resolution etiquette '' scoring metric -LRB- also used in Cherry and Bergsma -LRB- 2005 -RRB- -RRB- , which is defined as follows : if N is the number of non-anaphoric pronouns correctly identified , A the number of anaphoric pronouns correctly linked to their antecedent , and P the total number of pronouns , then a pronoun-anaphora program 's percentage correct Most papers dealing with pronoun coreference use this simple ratio , or the variant that ignores non-anaphoric pronouns . | Text : defined | Annotation Set : [{'name': 'Item', 'spans': [{'start': 33, 'end': 34, 'text': 'which'}]}, {'name': 'Category', 'spans': [{'start': 36, 'end': 38, 'text': 'as follows'}]}]",0,0,0,0,0
23,Education_teaching,755,"Automatically learned reordering models , which can be conditioned on lexical items from both the source and the target , provide some limited reordering capability when added to SMT systems . | Text : learned | Annotation Set : [{'name': 'Manner', 'spans': [{'start': 0, 'end': 1, 'text': 'Automatically'}]}]","Obviously , the same reordering has to be applied to both training data and test data . | Text : training | Annotation Set : []",We show that using the reordering rules results in gains in the translation scores and study the effect of the training data size on those gains . | Text : study | Annotation Set : [],We show that using the reordering rules results in gains in the translation scores and study the effect of the training data size on those gains . | Text : training | Annotation Set : [],"Morphological segmentation has been shown to benefit Arabic-to-English -LRB- Habash and Sadat , 2006 -RRB- and English-to-Arabic -LRB- Badr et al. , 2008 -RRB- translation , although the gains tend to decrease with increasing training data size . | Text : training | Annotation Set : []",0,0,0,0,0
24,Capability,754,"This can be very useful for cross-lingual information retrieval and the preparation of multi-lingual lexical resources . | Text : can | Annotation Set : [{'name': 'Event', 'spans': [{'start': 4, 'end': 5, 'text': 'useful'}]}, {'name': 'Entity', 'spans': [{'start': 0, 'end': 1, 'text': 'This'}]}]","Thus given a few Hebrew words as a description for some category , it can be useful to obtain a similar -LRB- and probably more extended -RRB- set of English words representing the same category . | Text : can | Annotation Set : [{'name': 'Event', 'spans': [{'start': 17, 'end': 35, 'text': 'to obtain a similar -LRB- and probably more extended -RRB- set of English words representing the same category'}]}, {'name': 'Entity', 'spans': [{'start': 13, 'end': 14, 'text': 'it'}]}]","Unlike in the majority of recent studies where the acquisition framework is designed with specific languages in mind , in our task the algorithm should be able to deal well with a wide variety of target languages without any significant manual adaptations . | Text : able | Annotation Set : [{'name': 'Event', 'spans': [{'start': 27, 'end': 42, 'text': 'to deal well with a wide variety of target languages without any significant manual adaptations'}]}, {'name': 'Entity', 'spans': [{'start': 22, 'end': 24, 'text': 'the algorithm'}]}]","While some of the proposed frameworks could potentially be language-independent , little research has been done to confirm it yet . | Text : could | Annotation Set : [{'name': 'Event', 'spans': [{'start': 7, 'end': 13, 'text': 'potentially be language-independent , little research'}]}, {'name': 'Entity', 'spans': [{'start': 1, 'end': 6, 'text': 'some of the proposed frameworks'}]}]","Most studies specify seed patterns in advance , and it is not clear whether translated patterns can work well on different languages . | Text : can | Annotation Set : [{'name': 'Event', 'spans': [{'start': 17, 'end': 18, 'text': 'work'}]}, {'name': 'Entity', 'spans': [{'start': 14, 'end': 16, 'text': 'translated patterns'}]}]",0,0,0,0,0
25,Locative_relation,738,"The key feature of such systems is that , even though the FSTs making up the cascade must be composed in a particular order , the result of composition is a single FST relating surface and lexical levels directly , as in two-level morphology . | Text : up | Annotation Set : [{'name': 'Figure', 'spans': [{'start': 11, 'end': 13, 'text': 'the FSTs'}]}]","This presents problems in the case of non-concatenative morphology : discontinuous morphemes -LRB- circumfix-ation -RRB- ; infixation , which breaks up a morpheme by inserting another within it ; reduplication , by which part or all of some morpheme is copied ; and the template morphology -LRB- also called stempattern morphology , intercalation , and interdigitation -RRB- that characterizes Semitic languages , and which is the focus of much of this paper . | Text : up | Annotation Set : []","The stem of a Semitic verb consists of a root , essentially a sequence of consonants , and a pattern , a sort of template which inserts other segments between the root consonants and possibly copies certain of them -LRB- see Tigrinya examples in the next section -RRB- . | Text : next | Annotation Set : [{'name': 'Figure', 'spans': [{'start': 35, 'end': 43, 'text': 'copies certain of them -LRB- see Tigrinya examples'}]}, {'name': 'Ground', 'spans': [{'start': 46, 'end': 47, 'text': 'section'}]}]","The next section gives an overview of Tigrinya verb morphology . | Text : next | Annotation Set : [{'name': 'Figure', 'spans': [{'start': 2, 'end': 3, 'text': 'section'}]}]","In the next section I consider an augmentation to finite state morphology offering clear advantages for this language . | Text : next | Annotation Set : [{'name': 'Figure', 'spans': [{'start': 6, 'end': 18, 'text': 'an augmentation to finite state morphology offering clear advantages for this language'}]}, {'name': 'Ground', 'spans': [{'start': 3, 'end': 4, 'text': 'section'}]}]",0,0,0,0,0
26,Frequency,732,"-LRB- 1991 -RRB- , who represented the two main senses of a SL word by its two most frequent translations in the target language -LRB- TL -RRB- . | Text : frequent | Annotation Set : []","Nevertheless , the theoretical soundness of these senses is not really addressed . | Text : Nevertheless | Annotation Set : []","A correct translation may be attained even without WSD , as in the case of parallel ambiguities where the SL and TL words are similarly ambiguous -LRB- Resnik and Yarowsky , 2000 -RRB- .2 However , this conception of senses is not theoretically sound , as translation equivalents do not always constitute valid sense indicators . | Text : always | Annotation Set : [{'name': 'Event', 'spans': [{'start': 46, 'end': 48, 'text': 'translation equivalents'}]}]","This is often neglected in an attempt to render the sense inventory as close as possible to the training corpus of the SMT system . | Text : often | Annotation Set : [{'name': 'Event', 'spans': [{'start': 4, 'end': 24, 'text': 'in an attempt to render the sense inventory as close as possible to the training corpus of the SMT system'}]}]","In the case of a one-equivalent cluster , this information corresponds to the set of the equivalent 's ' Most often near-synonyms but they may be linked by other relations -LRB- hyperonymy , hyponymy , etc. -RRB- . | Text : often | Annotation Set : [{'name': 'Event', 'spans': [{'start': 23, 'end': 27, 'text': 'they may be linked'}]}]",0,0,0,0,0
27,Evidence,731,"In section 4 , we illustrate the difficulties of error correction and show how simple combinations of local features perform poorly . | Text : illustrate | Annotation Set : [{'name': 'Proposition', 'spans': [{'start': 6, 'end': 11, 'text': 'the difficulties of error correction'}]}, {'name': 'Support', 'spans': [{'start': 4, 'end': 5, 'text': 'we'}]}]","From a subset of 210 nuclei -LRB- 917 tokens -RRB- , hand-evaluation reveals error detection precision to be 93 % -LRB- 195/210 -RRB- , with 274 -LRB- of the 917 -RRB- corpus positions in need of correction -LRB- Boyd et al. , 2008 -RRB- . | Text : reveals | Annotation Set : [{'name': 'Proposition', 'spans': [{'start': 13, 'end': 16, 'text': 'error detection precision'}]}, {'name': 'Support', 'spans': [{'start': 11, 'end': 12, 'text': 'hand-evaluation'}]}]","-LRB- 2008 -RRB- , we add L or R to the label to indicate which word is the head , the left -LRB- L -RRB- or the right -LRB- R -RRB- . | Text : indicate | Annotation Set : [{'name': 'Proposition', 'spans': [{'start': 14, 'end': 19, 'text': 'which word is the head'}]}]","Pulling apart the features commonly employed in dependency parsing can help indicate the contributions each has on the classification . | Text : indicate | Annotation Set : [{'name': 'Proposition', 'spans': [{'start': 12, 'end': 19, 'text': 'the contributions each has on the classification'}]}, {'name': 'Support', 'spans': [{'start': 2, 'end': 9, 'text': 'the features commonly employed in dependency parsing'}]}]","The variation ยจar vยจag -LRB- โis wayโ -RRB- , for example , appears twice with the same local context shown in -LRB- 1 -RRB- , 4 once incorrectly labeled as OO-L -LRB- other object -LSB- head on the left -RSB- -RRB- and once correctly as SPL -LRB- subjective predicative complement -RRB- . | Text : shown | Annotation Set : [{'name': 'Proposition', 'spans': [{'start': 20, 'end': 21, 'text': 'in'}]}, {'name': 'Support', 'spans': [{'start': 15, 'end': 19, 'text': 'the same local context'}]}]",0,0,0,0,0
28,Assessing,711,"Specifically , he shows that : linear context-free rewriting systems -LRB- LCFRS -RRB- with fan-out k -LRB- Vijay-Shanker et al. , 1987 ; Satta , 1992 -RRB- induce the set of dependency structures with gap degree at most k โ 1 ; coupled context-free grammars in which the maximal rank of a nonterminal is k -LRB- Hotz and Pitsch , 1996 -RRB- induce the set of well-nested dependency structures with gap degree at most k โ 1 ; and LTAGs -LRB- Joshi and Schabes , 1997 -RRB- induce the set of well-nested dependency structures with gap degree at most 1 . | Text : rank | Annotation Set : [{'name': 'Phenomenon', 'spans': [{'start': 50, 'end': 53, 'text': 'of a nonterminal'}]}]","Again , this complexity result is in line with what could be expected from previous research in constituency parsing : Kuhlmann -LRB- 2007 -RRB- shows that the set of well-nested dependency structures with gap degree at most k is closely related to coupled context-free grammars in which the maximal rank of a nonterminal is k + 1 ; and the constituency parser defined by Hotz and Pitsch -LRB- 1996 -RRB- for these grammars also adds an n2 factor for each unit increment of k . | Text : rank | Annotation Set : [{'name': 'Phenomenon', 'spans': [{'start': 50, 'end': 53, 'text': 'of a nonterminal'}]}]","Therefore , our O -LRB- n3k+4 -RRB- parser can analyse all the gap degree k structures in these treebanks . | Text : analyse | Annotation Set : [{'name': 'Phenomenon', 'spans': [{'start': 10, 'end': 19, 'text': 'all the gap degree k structures in these treebanks'}]}]","Secondly , an evaluation is performed on a large set of data , showing the benefits and the limits of such approach . | Text : evaluation | Annotation Set : []","In this paper , we describe the prototype we built to evaluate the feasibility of such approach . | Text : evaluate | Annotation Set : [{'name': 'Assessor', 'spans': [{'start': 8, 'end': 9, 'text': 'we'}]}, {'name': 'Phenomenon', 'spans': [{'start': 12, 'end': 17, 'text': 'the feasibility of such approach'}]}]",0,0,0,0,0
29,Vehicle,674,"ta -LRB- f -RRB- -RRB- is a tree of category A if ti is a tree of category Bi and there is a production : A โ f -LSB- B1 ... | Text : Bi | Annotation Set : [{'name': 'Vehicle', 'spans': [{'start': 18, 'end': 19, 'text': 'Bi'}]}]","wk as a prefix in constituent l for any sequence of arguments ti : Bi . | Text : Bi | Annotation Set : [{'name': 'Vehicle', 'spans': [{'start': 14, 'end': 15, 'text': 'Bi'}]}]","Doing so limits the features that are available to our models , requiring features to be structurally local . | Text : models | Annotation Set : [{'name': 'Possessor', 'spans': [{'start': 9, 'end': 10, 'text': 'our'}]}, {'name': 'Vehicle', 'spans': [{'start': 10, 'end': 11, 'text': 'models'}]}]","Meanwhile , some learning algorithms , like maximum likelihood for conditional log-linear models -LRB- Laf-ferty et al. , 2001 -RRB- , unsupervised models -LRB- Pereira and Schabes , 1992 -RRB- , and models with hidden variables -LRB- Koo and Collins , 2005 ; Wang et al. , 2007 ; Blunsom et al. , 2008 -RRB- , require summing over the scores of many structures to calculate marginals . | Text : models | Annotation Set : [{'name': 'Vehicle', 'spans': [{'start': 12, 'end': 13, 'text': 'models'}]}]","We first review the semiring-weighted logic programming view of dynamic programming algorithms -LRB- Shieber et al. , 1995 -RRB- and identify an intuitive property of a program called proof locality that follows from feature locality in the underlying probability model -LRB- ยง2 -RRB- . | Text : model | Annotation Set : [{'name': 'Vehicle', 'spans': [{'start': 39, 'end': 40, 'text': 'model'}]}]",0,0,0,0,0
30,Type,664,"Here , the challenge is to identify the most relevant facts in a document , but not necessarily in a coherent full-text form as is done in summarization . | Text : form | Annotation Set : [{'name': 'Subtype', 'spans': [{'start': 22, 'end': 23, 'text': 'form'}]}, {'name': 'Type_Property', 'spans': [{'start': 21, 'end': 22, 'text': 'full-text'}]}]","AURUM has two types of features : sentence features , such as the position of the sentence or the existence of a negation word , receive the same value for the entire sentence . | Text : types | Annotation Set : [{'name': 'Subtype', 'spans': [{'start': 3, 'end': 4, 'text': 'types'}]}, {'name': 'Category', 'spans': [{'start': 4, 'end': 6, 'text': 'of features'}]}]","Verb classes . | Text : classes | Annotation Set : [{'name': 'Subtype', 'spans': [{'start': 1, 'end': 2, 'text': 'classes'}]}]","After exploring the training data we manually compiled two classes of verbs , each containing 15-20 inflected and uninflected lexemes , talkVerbs and actionVerbs . | Text : classes | Annotation Set : [{'name': 'Subtype', 'spans': [{'start': 9, 'end': 10, 'text': 'classes'}]}, {'name': 'Category', 'spans': [{'start': 10, 'end': 12, 'text': 'of verbs'}]}]","Our approach emphasizes a wide variety of features , including many linguistic features . | Text : variety | Annotation Set : [{'name': 'Subtype', 'spans': [{'start': 5, 'end': 6, 'text': 'variety'}]}, {'name': 'Category', 'spans': [{'start': 6, 'end': 8, 'text': 'of features'}]}, {'name': 'Type_Property', 'spans': [{'start': 4, 'end': 5, 'text': 'wide'}]}]",0,0,0,0,0
31,Scrutiny,662,"< newSection > Abstract Sentence fluency is an important component of overall text readability but few studies in natural language processing have sought to understand the factors that define it . | Text : studies | Annotation Set : [{'name': 'Ground', 'spans': [{'start': 17, 'end': 21, 'text': 'in natural language processing'}]}]","We report the results of an initial study into the predictive power of surface syntactic statistics for the task ; we use fluency assessments done for the purpose of evaluating machine translation . | Text : study | Annotation Set : [{'name': 'Ground', 'spans': [{'start': 8, 'end': 16, 'text': 'into the predictive power of surface syntactic statistics'}]}]","This finding suggests that developing a dedicated , task-independent corpus of fluency judgments will be beneficial for further investigations of the problem . | Text : investigations | Annotation Set : [{'name': 'Ground', 'spans': [{'start': 19, 'end': 22, 'text': 'of the problem'}]}]","The exploration of branching properties of human and machine translations was motivated by the observations during failure analysis that MT system output tends to favor right-branching structures over noun compounding . | Text : analysis | Annotation Set : [{'name': 'Cognizer', 'spans': [{'start': 12, 'end': 15, 'text': 'by the observations'}]}]",In our work we continue the investigation of sentence level fluency based on features that capture surface statistics of the syntactic structure in a sentence . | Text : investigation | Annotation Set : [],0,0,0,0,0
32,Inclusion,654,"One problem with Personalized PageRank is that if one of the target words has two senses which are related by semantic relations , those senses reinforce each other , and could thus dampen the effect of the other senses in the context . | Text : has | Annotation Set : [{'name': 'Part', 'spans': [{'start': 14, 'end': 16, 'text': 'two senses'}]}, {'name': 'Total', 'spans': [{'start': 8, 'end': 13, 'text': 'one of the target words'}]}]","The main idea of this approach is to avoid biasing the initial score of concepts associated to target word Wi , and let the surrounding words decide which concept associated to Wi has more relevance . | Text : has | Annotation Set : [{'name': 'Part', 'spans': [{'start': 33, 'end': 35, 'text': 'more relevance'}]}, {'name': 'Total', 'spans': [{'start': 28, 'end': 32, 'text': 'concept associated to Wi'}]}]","-LRB- Tsatsaronis et al. , 2007 -RRB- is another example of a two-stage process , the first one consisting on finding a relevant subgraph by performing a BFS dataset , including MFS and the best supervised system in the competition . | Text : including | Annotation Set : [{'name': 'Part', 'spans': [{'start': 31, 'end': 40, 'text': 'MFS and the best supervised system in the competition'}]}, {'name': 'Total', 'spans': [{'start': 21, 'end': 24, 'text': 'a relevant subgraph'}]}]","The dataset contains examples of the 150 most frequent nouns in the CESS-ECE corpus , manually annotated with Spanish WordNet synsets . | Text : contains | Annotation Set : [{'name': 'Part', 'spans': [{'start': 3, 'end': 10, 'text': 'examples of the 150 most frequent nouns'}]}, {'name': 'Total', 'spans': [{'start': 0, 'end': 2, 'text': 'The dataset'}]}]","It is split into a train and test part , and has an `` all words '' shape i.e. input consists on sentences , each one having at least one occurrence of a target noun . | Text : has | Annotation Set : [{'name': 'Part', 'spans': [{'start': 12, 'end': 35, 'text': ""an `` all words '' shape i.e. input consists on sentences , each one having at least one occurrence of a target noun""}]}, {'name': 'Total', 'spans': [{'start': 3, 'end': 9, 'text': 'into a train and test part'}]}]",0,0,0,0,0
33,Part_whole,604,"The remainder of this paper is organized as follows : The next section describes the relevant work done to date in keyfact extraction and automatic summarization . | Text : section | Annotation Set : [{'name': 'Part', 'spans': [{'start': 12, 'end': 13, 'text': 'section'}]}, {'name': 'Part_Prop', 'spans': [{'start': 11, 'end': 12, 'text': 'next'}]}]","Section 3 lays out our features and explains how they were learned and estimated . | Text : Section | Annotation Set : [{'name': 'Part', 'spans': [{'start': 0, 'end': 1, 'text': 'Section'}]}]","Section 4 presents the experimental setup and our results , and Section 5 concludes with a short discussion . | Text : Section | Annotation Set : [{'name': 'Part', 'spans': [{'start': 0, 'end': 1, 'text': 'Section'}]}]","Section 4 presents the experimental setup and our results , and Section 5 concludes with a short discussion . | Text : Section | Annotation Set : [{'name': 'Part', 'spans': [{'start': 11, 'end': 13, 'text': 'Section 5'}]}]","< newSection > 3 Approach In this section we describe which features were used and how the data was annotated to facilitate feature extraction and estimation . | Text : section | Annotation Set : [{'name': 'Part', 'spans': [{'start': 7, 'end': 8, 'text': 'section'}]}]",0,0,0,0,0
34,Relative_time,573,"Previous work shows that current state-of-theart WSD systems are not able to obtain better results on the adaptation scenario compared to the target scenario -LRB- Escudero et al. , 2000 ; Agirre and Martinez , 2004 ; Chan and Ng , 2007 -RRB- . | Text : Previous | Annotation Set : [{'name': 'Focal_occasion', 'spans': [{'start': 1, 'end': 2, 'text': 'work'}]}]","The paper is structured as follows . | Text : follows | Annotation Set : [{'name': 'Focal_occasion', 'spans': [{'start': 0, 'end': 2, 'text': 'The paper'}]}, {'name': 'Degree', 'spans': [{'start': 4, 'end': 5, 'text': 'as'}]}]","In the supervised setting , a recent paper by Daumยดe III -LRB- 2007 -RRB- shows that a simple feature augmentation method for SVM is able to effectively use both labeled target and source data to provide the best domainadaptation results in a number of NLP tasks . | Text : recent | Annotation Set : [{'name': 'Focal_occasion', 'spans': [{'start': 7, 'end': 8, 'text': 'paper'}]}]","Compared to the DSO corpus used in prior work -LRB- cf. Section 2 -RRB- this corpus has been explicitly created for domain adaptation studies . | Text : prior | Annotation Set : [{'name': 'Focal_occasion', 'spans': [{'start': 8, 'end': 9, 'text': 'work'}]}]","We can map any feature vector i๏ฟฝ -LRB- which represents either a train or test example -RRB- into the p-dimensional space as follows : tp = ๏ฟฝiT VpE๏ฟฝ1 p . Those mapped vectors have p dimensions , and each of the dimensions is what we call a SVD feature . | Text : follows | Annotation Set : []",0,0,0,0,0
35,Translating,571,"Furthermore , recent work in machine translation -LRB- Vickrey et al. , 2005 -RRB- and information retrieval -LRB- Vยดeronis , 2004 -RRB- indicates that induced senses can lead to improved performance in areas where methods based on a fixed sense inventory have previously failed -LRB- Carpuat and Wu , 2005 ; Voorhees , 1993 -RRB- . | Text : translation | Annotation Set : []","While EM has worked quite well for a few tasks , notably machine translations -LRB- starting with the IBM models 1-5 -LRB- Brown et al. , 1993 -RRB- , it has not had success in most others , such as part-of-speech tagging -LRB- Meri-aldo , 1991 -RRB- , named-entity recognition -LRB- Collins and Singer , 1999 -RRB- and context-free-grammar induction -LRB- numerous attempts , too many to mention -RRB- . | Text : translations | Annotation Set : [{'name': 'Cognizer', 'spans': [{'start': 12, 'end': 20, 'text': 'machine translations -LRB- starting with the IBM models'}]}]",A class of features that thus far has not been considered is based on a projection of the syntactic structure of a translation of the text to be parsed . | Text : translation | Annotation Set : [],"< newSection > 1 Introduction Parallel text or bitext is an important knowledge source for solving many problems such as machine translation , cross-language information retrieval , and the projection of linguistic resources from one language to another . | Text : translation | Annotation Set : []","First , one limiting factor for syntactic approaches to statistical machine translation is parse quality -LRB- Quirk and Corston-Oliver , 2006 -RRB- . | Text : translation | Annotation Set : []",0,0,0,0,0
36,Sole_instance,547,"It is possible for a meaning to be expressed in only one side of the sentence pair . | Text : only | Annotation Set : [{'name': 'Type', 'spans': [{'start': 12, 'end': 17, 'text': 'side of the sentence pair'}]}]","In that case , we will have a `` one-sided '' concept consisting of only one word . | Text : only | Annotation Set : []","The joint distribution of e , f and a is then : This model only take into consideration oneto-one alignments . | Text : only | Annotation Set : []","Considering only one-to-one alignments can be seen as a limitation compared to others models that can often produce at least one-to-many alignments , but on the good side , this allow the monolink model to be nicely symmetric . | Text : only | Annotation Set : []","Additionally , as already argued in -LRB- Melamed , 2000 -RRB- , there are ways to determine the boundaries of some multi-words phrases -LRB- Melamed , 2002 -RRB- , allowing to treat several words as a single token . | Text : single | Annotation Set : [{'name': 'Type', 'spans': [{'start': 37, 'end': 38, 'text': 'token'}]}]",0,0,0,0,0
37,Usefulness,520,"Argument values may be variables -LRB- e.g. , e8 , x4 : variables are the only possibility for values of ARG0 -RRB- , constants -LRB- strings such as `` London '' -RRB- , or holes -LRB- e.g. h5 -RRB- , which indicate scopal relationships . | Text : values | Annotation Set : [{'name': 'Entity', 'spans': [{'start': 0, 'end': 1, 'text': 'Argument'}]}]","Argument values may be variables -LRB- e.g. , e8 , x4 : variables are the only possibility for values of ARG0 -RRB- , constants -LRB- strings such as `` London '' -RRB- , or holes -LRB- e.g. h5 -RRB- , which indicate scopal relationships . | Text : values | Annotation Set : []","However , grammar engineers respond to consumers : if more detailed role labelling had a clear utility and required an analysis at the syntax level , we would want to do it in the grammar . | Text : utility | Annotation Set : [{'name': 'Entity', 'spans': [{'start': 15, 'end': 16, 'text': 'clear'}]}]","For our purposes , a consistent semantic interpretation involves entailment of one or more useful real world propositions -LRB- allowing for exceptions to the entailment for unusual individual sentences -RRB- . | Text : useful | Annotation Set : [{'name': 'Entity', 'spans': [{'start': 17, 'end': 18, 'text': 'propositions'}]}]","The labels add no additional information and could trivially be added automatically to an RMRS if this were useful for human readers . | Text : useful | Annotation Set : [{'name': 'Purpose', 'spans': [{'start': 19, 'end': 22, 'text': 'for human readers'}]}, {'name': 'Entity', 'spans': [{'start': 16, 'end': 17, 'text': 'this'}]}]",0,0,0,0,0
38,Dimension,488,"Short delays are usually preferred for several reasons . | Text : Short | Annotation Set : [{'name': 'Object', 'spans': [{'start': 1, 'end': 2, 'text': 'delays'}]}, {'name': 'Dimension', 'spans': [{'start': 0, 'end': 1, 'text': 'Short'}]}]","For example , the audience is irritated when the delay is too large and is soon asking whether there is a problem with the interpretation . | Text : large | Annotation Set : [{'name': 'Dimension', 'spans': [{'start': 12, 'end': 13, 'text': 'large'}]}]","< newSection > 3 Automatic Simultaneous Translation Given the explanations above on human interpretation , one has to weigh two factors when considering the use of simultaneous translation systems : translation quality and cost . | Text : weigh | Annotation Set : [{'name': 'Object', 'spans': [{'start': 19, 'end': 21, 'text': 'two factors'}]}, {'name': 'Dimension', 'spans': [{'start': 15, 'end': 16, 'text': 'one'}]}]","One such advantage is its considerable shortterm memory : storing long sequences of words is not a problem for a computer system . | Text : long | Annotation Set : [{'name': 'Object', 'spans': [{'start': 11, 'end': 12, 'text': 'sequences'}]}, {'name': 'Dimension', 'spans': [{'start': 10, 'end': 11, 'text': 'long'}]}]","To that aim , we reused a large part of the evaluation protocol from the TC-STAR project -LRB- Hamon et al. , 2007 -RRB- . | Text : large | Annotation Set : [{'name': 'Object', 'spans': [{'start': 8, 'end': 9, 'text': 'part'}]}, {'name': 'Dimension', 'spans': [{'start': 7, 'end': 8, 'text': 'large'}]}]",0,0,0,0,0
39,Contingency,477,"Wikipedia3 has been also recently used to overcome some problems of automatic learning methods : excessively fineโgrained definition of meanings , lack of annotated data and strong domain dependence of existing annotated corpora . | Text : dependence | Annotation Set : []","Thus , depending on the type of relations considered to be counted and the threshold established , different sets of BLC can be easily obtained for each WN version . | Text : depending | Annotation Set : [{'name': 'Determinant', 'spans': [{'start': 3, 'end': 29, 'text': 'on the type of relations considered to be counted and the threshold established , different sets of BLC can be easily obtained for each WN version'}]}]","When classifying an example , we obtain the value of the output function for each SVM classifier corresponding to each semantic class for the word example . | Text : function | Annotation Set : [{'name': 'Determinant', 'spans': [{'start': 13, 'end': 26, 'text': 'for each SVM classifier corresponding to each semantic class for the word example'}]}]","As we have seen , supervised WSD systems are very dependent of the corpora used to train and test the system . | Text : dependent | Annotation Set : [{'name': 'Degree', 'spans': [{'start': 9, 'end': 10, 'text': 'very'}]}]","These factors adversely affect the performance of Arabic-to-English SMT , especially in the English-to-Arabic direction . | Text : factors | Annotation Set : [{'name': 'Determinant', 'spans': [{'start': 1, 'end': 2, 'text': 'factors'}]}]",0,0,0,0,0
40,Giving,473,"Instead , the meaning and intention of a given sentence have to be reexpressed in a natural and fluent way in another language . | Text : given | Annotation Set : [{'name': 'Theme', 'spans': [{'start': 9, 'end': 10, 'text': 'sentence'}]}]","According to a study by Kopczynski -LRB- 1994 -RRB- , fluency and style were third on a list of priorities -LRB- after content and terminology -RRB- of elements rated by speakers and attendees as contributing to quality . | Text : contributing | Annotation Set : [{'name': 'Recipient', 'spans': [{'start': 35, 'end': 37, 'text': 'to quality'}]}]","< newSection > 3 Automatic Simultaneous Translation Given the explanations above on human interpretation , one has to weigh two factors when considering the use of simultaneous translation systems : translation quality and cost . | Text : Given | Annotation Set : [{'name': 'Theme', 'spans': [{'start': 8, 'end': 14, 'text': 'the explanations above on human interpretation'}]}]","< newSection > 4 Evaluation Tasks The evaluation is carried out on the simultaneously translated speech of a single speaker 's talks and lectures in the field of speech processing , given in English , and translated into Spanish . | Text : given | Annotation Set : [{'name': 'Recipient', 'spans': [{'start': 32, 'end': 34, 'text': 'in English'}]}, {'name': 'Theme', 'spans': [{'start': 36, 'end': 37, 'text': 'translated'}]}]","At a second stage , the answers are analysed : for each answer a Spanish validator gives a score according to a binary scale -LRB- the information is either correct or incorrect -RRB- . | Text : gives | Annotation Set : [{'name': 'Donor', 'spans': [{'start': 11, 'end': 13, 'text': 'each answer'}]}, {'name': 'Theme', 'spans': [{'start': 17, 'end': 19, 'text': 'a score'}]}]",0,0,0,0,0
41,Ordinal_numbers,452,"The first of these is argument indexing and its relationship to semantic role labelling , the second is semantic dependency structure . | Text : first | Annotation Set : [{'name': 'Item', 'spans': [{'start': 0, 'end': 4, 'text': 'The first of these'}]}]","The first , argument indexing -LRB- ยง3 -RRB- , is a relatively clear case in which the constraints imposed by grammar engineering have a significant effect on choice between plausible alternatives . | Text : first | Annotation Set : []","For instance , in Fig 1 , the third line contains the EPs corresponding to the -LRB- single node -RRB- backbone tree and the first two lines show the EPs comprising the tree for the quantified NP -LRB- one node for the quantifier and one for the N ' which it connects to via the RSTR and its qeq -RRB- . | Text : third | Annotation Set : [{'name': 'Type', 'spans': [{'start': 9, 'end': 10, 'text': 'line'}]}]","For instance , in Fig 1 , the third line contains the EPs corresponding to the -LRB- single node -RRB- backbone tree and the first two lines show the EPs comprising the tree for the quantified NP -LRB- one node for the quantifier and one for the N ' which it connects to via the RSTR and its qeq -RRB- . | Text : first | Annotation Set : []","For instance , we should distinguish between the โdepartโ and โbequeathโ senses of leave because the first takes an ARG1 and an ARG2 -LRB- op-tional -RRB- and the second ARG1 , ARG2 -LRB- optional -RRB- , ARG3 . | Text : first | Annotation Set : []",0,0,0,0,0
42,Reasoning,437,"Our experiments show that our discourselevel , cognitively-motivated features improve automatic readability assessment . | Text : show | Annotation Set : [{'name': 'Content', 'spans': [{'start': 3, 'end': 13, 'text': 'that our discourselevel , cognitively-motivated features improve automatic readability assessment'}]}]","While such features are essential , we argue that audiencespecific features that model the cognitive characteristics of a user group can improve the accuracy of a readability assessment tool . | Text : argue | Annotation Set : [{'name': 'Content', 'spans': [{'start': 8, 'end': 29, 'text': 'that audiencespecific features that model the cognitive characteristics of a user group can improve the accuracy of a readability assessment tool'}]}, {'name': 'Arguer', 'spans': [{'start': 6, 'end': 7, 'text': 'we'}]}]","The only two features which did not show a significant difference -LRB- p > 0.01 -RRB- between the complex and simple versions of the articles were : average lexical chain length -LRB- aLCL -RRB- and number of lexical chains with span greater than half the document length -LRB- nLC2 -RRB- . | Text : show | Annotation Set : [{'name': 'Content', 'spans': [{'start': 8, 'end': 31, 'text': 'a significant difference -LRB- p > 0.01 -RRB- between the complex and simple versions of the articles were : average lexical chain length'}]}, {'name': 'Arguer', 'spans': [{'start': 4, 'end': 5, 'text': 'which'}]}]","This study has also demonstrated the value of collecting readability judgments from target users when designing a readability assessment tool . | Text : demonstrated | Annotation Set : [{'name': 'Content', 'spans': [{'start': 5, 'end': 20, 'text': 'the value of collecting readability judgments from target users when designing a readability assessment tool'}]}, {'name': 'Arguer', 'spans': [{'start': 1, 'end': 2, 'text': 'study'}]}]","We have demonstrated the usefulness of these novel features in modeling the grade level of elementary school texts and in correlating to readability judgments from adults with ID . | Text : demonstrated | Annotation Set : [{'name': 'Content', 'spans': [{'start': 9, 'end': 28, 'text': 'in modeling the grade level of elementary school texts and in correlating to readability judgments from adults with ID'}]}, {'name': 'Support', 'spans': [{'start': 3, 'end': 9, 'text': 'the usefulness of these novel features'}]}, {'name': 'Arguer', 'spans': [{'start': 0, 'end': 1, 'text': 'We'}]}]",0,0,0,0,0
43,Information,431,"In particular , we use a structure representation derived from the dependency parse trees of T and H , which aims to capture the essential information they convey . | Text : information | Annotation Set : [{'name': 'Information', 'spans': [{'start': 25, 'end': 26, 'text': 'information'}]}]","Following this , another non-trivial task is to determine the way this knowledge interacts with the rest of information conveyed in an entailment pair . | Text : information | Annotation Set : [{'name': 'Information', 'spans': [{'start': 18, 'end': 19, 'text': 'information'}]}]","In all these cases , the additional information conveyed by the text or the hypothesis which can not be captured by our current approach , affects the entailment . | Text : information | Annotation Set : [{'name': 'Information', 'spans': [{'start': 7, 'end': 8, 'text': 'information'}]}]","Consider cases such as threaten to boycott and boycott or similar constructions with other embedding verbs such as manage , forget , attempt . | Text : forget | Annotation Set : []","Each item contains information about the sentence 's structure , and a successful parsing process produces at least one final item providing a full dependency analysis for the sentence or guaranteeing its existence . | Text : information | Annotation Set : [{'name': 'Information', 'spans': [{'start': 3, 'end': 4, 'text': 'information'}]}, {'name': 'Topic', 'spans': [{'start': 4, 'end': 9, 'text': ""about the sentence 's structure""}]}]",0,0,0,0,0
44,Existence,386,"1. There are M feature functions h1 , ... | Text : There are | Annotation Set : [{'name': 'Entity', 'spans': [{'start': 3, 'end': 4, 'text': 'M'}]}]","If there is an obvious disagreement , it is probably caused by wrong attachment or other syntactic mistakes in parsing . | Text : there is | Annotation Set : [{'name': 'Entity', 'spans': [{'start': 3, 'end': 6, 'text': 'an obvious disagreement'}]}]","We found that there is a significant difference with the baseline -LRB- t -LRB- 3717 -RRB- = 6.42 , p < .01 -RRB- . | Text : there is | Annotation Set : [{'name': 'Entity', 'spans': [{'start': 5, 'end': 23, 'text': 'a significant difference with the baseline -LRB- t -LRB- 3717 -RRB- = 6.42 , p < .01 -RRB-'}]}]","However , the applicability of these systems is limited to those words for which labeled data exists , and their accuracy is strongly correlated with the amount of labeled data available . | Text : exists | Annotation Set : [{'name': 'Entity', 'spans': [{'start': 15, 'end': 16, 'text': 'data'}]}]","The idea here is to leverage the existing English FrameNet and rely on word or constituent alignments to automatically create an annotated corpus in a new language . | Text : existing | Annotation Set : [{'name': 'Entity', 'spans': [{'start': 11, 'end': 27, 'text': 'rely on word or constituent alignments to automatically create an annotated corpus in a new language'}]}]",0,0,0,0,0
45,Time_vector,369,"A key assumption underlying previous work is that the context surrounding an ambiguous word is indicative of its meaning . | Text : previous | Annotation Set : [{'name': 'Event', 'spans': [{'start': 5, 'end': 6, 'text': 'work'}]}]","Although the bulk of previous work has been devoted to the disambiguation problem1 , there are good reasons to believe that sense induction may be able to overcome some of the issues associated with WSD . | Text : previous | Annotation Set : []","Furthermore , recent work in machine translation -LRB- Vickrey et al. , 2005 -RRB- and information retrieval -LRB- Vยดeronis , 2004 -RRB- indicates that induced senses can lead to improved performance in areas where methods based on a fixed sense inventory have previously failed -LRB- Carpuat and Wu , 2005 ; Voorhees , 1993 -RRB- . | Text : previously | Annotation Set : [{'name': 'Event', 'spans': [{'start': 34, 'end': 41, 'text': 'methods based on a fixed sense inventory'}]}, {'name': 'Direction', 'spans': [{'start': 42, 'end': 43, 'text': 'previously'}]}]",This is in marked contrast with previous LDA-based models which mostly take only word-based information into account . | Text : previous | Annotation Set : [],"Unlike the work mentioned above , we do not rely on a pre-existing list of senses , and do not assume a correspondence between our automatically derived sense-clusters and those of any given inventory.2 A key element in these previous attempts at adapting LDA for WSD is the tendency to remain at a high level , document-like , setting . | Text : previous | Annotation Set : []",0,0,0,0,0
46,Cause_to_make_progress,366,"Future work will focus on fine-tuning the verb FST , developing an FST for nouns , and applying this same approach to other Semitic languages . | Text : developing | Annotation Set : [{'name': 'Project', 'spans': [{'start': 11, 'end': 15, 'text': 'an FST for nouns'}]}]","The first step is used to improve the generalization power of the model . | Text : improve | Annotation Set : [{'name': 'Agent', 'spans': [{'start': 0, 'end': 3, 'text': 'The first step'}]}, {'name': 'Project', 'spans': [{'start': 7, 'end': 13, 'text': 'the generalization power of the model'}]}]","This means that re-ranking the m-best hypotheses in a convenient way could improve the SLU performance . | Text : improve | Annotation Set : [{'name': 'Agent', 'spans': [{'start': 3, 'end': 11, 'text': 're-ranking the m-best hypotheses in a convenient way'}]}, {'name': 'Project', 'spans': [{'start': 13, 'end': 16, 'text': 'the SLU performance'}]}]","We note that our re-rankers relevantly improve our baselines , i.e. the FST and SVM concept classifiers on both corpora . | Text : improve | Annotation Set : [{'name': 'Agent', 'spans': [{'start': 3, 'end': 5, 'text': 'our re-rankers'}]}, {'name': 'Project', 'spans': [{'start': 7, 'end': 9, 'text': 'our baselines'}]}]","For example , SVM reranker using STK , MT and RR-A improves FST concept classifier of 23.2-15.6 = 7.6 points . | Text : improves | Annotation Set : [{'name': 'Agent', 'spans': [{'start': 5, 'end': 11, 'text': 'using STK , MT and RR-A'}]}, {'name': 'Project', 'spans': [{'start': 12, 'end': 20, 'text': 'FST concept classifier of 23.2-15.6 = 7.6 points'}]}]",0,0,0,0,0
47,People,364,"< newSection > Abstract We present a Hebrew to English transliteration method in the context of a machine translation system . | Text : English | Annotation Set : [{'name': 'Person', 'spans': [{'start': 9, 'end': 10, 'text': 'English'}]}]","This paper addresses transliteration from Hebrew to English as part of a machine translation system . | Text : English | Annotation Set : [{'name': 'Person', 'spans': [{'start': 7, 'end': 8, 'text': 'English'}]}]","Transliteration of terms from Hebrew into English is a hard task , for the most part because of the differences in the phonological and orthographic systems of the two languages . | Text : English | Annotation Set : [{'name': 'Person', 'spans': [{'start': 6, 'end': 7, 'text': 'English'}]}]","On the one hand , many NEs should be translated rather than transliterated , for example : 1 m $ rd hm $ p @ im misrad hamishpatim ministry-of the-sentences โMinistry of Justiceโ 1To facilitate readability , examples are presented with interlinear gloss , including an ASCII representation of Hebrew orthography followed by a broad phonemic transcription , a word-for-word gloss in English where relevant , and the corresponding free text in English . | Text : English | Annotation Set : [{'name': 'Person', 'spans': [{'start': 62, 'end': 63, 'text': 'English'}]}]","On the one hand , many NEs should be translated rather than transliterated , for example : 1 m $ rd hm $ p @ im misrad hamishpatim ministry-of the-sentences โMinistry of Justiceโ 1To facilitate readability , examples are presented with interlinear gloss , including an ASCII representation of Hebrew orthography followed by a broad phonemic transcription , a word-for-word gloss in English where relevant , and the corresponding free text in English . | Text : English | Annotation Set : [{'name': 'Person', 'spans': [{'start': 72, 'end': 73, 'text': 'English'}]}]",0,0,0,0,0
48,Encoding,358,Each candidate annotation si is described by a word sequence where each word is followed by its concept annotation . | Text : word | Annotation Set : [],< newSection > 4 Classification Frequencies of words in word categories were determined using Linguistic Inquiry and Word Count -LRB- LIWC -RRB- . | Text : Word | Annotation Set : [],"This set also contains the abbreviations and emotional expressions `` lol '' , `` ur '' , `` tru '' , `` wat '' , and `` haha '' . | Text : expressions | Annotation Set : []",Some of the expressions could be characteristic of particular individuals . | Text : expressions | Annotation Set : [],"All 756 samples were divided into 126 `` documents , '' each consisting of all six samples of a person 's expression in a single genre , regardless of topic . | Text : expression | Annotation Set : [{'name': 'Speaker', 'spans': [{'start': 18, 'end': 21, 'text': ""a person 's""}]}]",0,0,0,0,0
49,Importance,357,"However , there are a significant number of more complex cases of coordination that defy this generalization and that make the parsing task of detecting the right scope of individual conjuncts and correctly delineating the correct scope of the coordinate structure as a whole difficult . | Text : significant | Annotation Set : [{'name': 'Factor', 'spans': [{'start': 6, 'end': 7, 'text': 'number'}]}]","Complex coordinations can take up a considerable part of the input string and accordingly of the overall sentence structure . | Text : considerable | Annotation Set : [{'name': 'Factor', 'spans': [{'start': 7, 'end': 8, 'text': 'part'}]}]","2. The introduction of partial brackets that delimit the scope of the coordination improve overall results on the full test set by 4.7 percent points , a rather significant improvement when we consider that only approximately one third of the test sentences were modified . | Text : significant | Annotation Set : [{'name': 'Interested_party', 'spans': [{'start': 27, 'end': 28, 'text': 'rather'}]}, {'name': 'Factor', 'spans': [{'start': 29, 'end': 30, 'text': 'improvement'}]}]","More generally , experiment 4 suggests that for the notoriously difficult problem of parsing coordination structures , a hybrid approach that combines parse selection of n best analyses with pre-bracketed scope in the input results in a considerable reduction in error rate compared to each of these methods used in isolation . | Text : considerable | Annotation Set : [{'name': 'Factor', 'spans': [{'start': 38, 'end': 39, 'text': 'reduction'}]}]","< newSection > 8 Related Work Parsing of coordinate structures for English has received considerable attention in computational linguistics . | Text : considerable | Annotation Set : [{'name': 'Factor', 'spans': [{'start': 15, 'end': 16, 'text': 'attention'}]}]",0,0,0,0,0
50,Coming_to_believe,353,"< newSection > Abstract We present parsing algorithms for various mildly non-projective dependency formalisms . | Text : Abstract | Annotation Set : [{'name': 'Content', 'spans': [{'start': 4, 'end': 14, 'text': 'We present parsing algorithms for various mildly non-projective dependency formalisms'}]}]","The parsing schema approach considers parsing as deduction , generating intermediate results called items . | Text : deduction | Annotation Set : []","An initial set of items is obtained from the input sentence , and the parsing process involves deduction steps which produce new items from existing ones . | Text : deduction | Annotation Set : []","To define a parser by means of a schema , we must define an item set and provide a set of deduction steps that operate on it . | Text : deduction | Annotation Set : []","These concepts can be used to prove the correctness of a parser : for each input string , a parsing schema 's deduction steps allow us to infer a set of items , called valid items for that string . | Text : deduction | Annotation Set : [{'name': 'Cognizer', 'spans': [{'start': 18, 'end': 22, 'text': ""a parsing schema 's""}]}]",0,0,0,0,0
51,Arriving,340,"Our method has several benefits : We apply our method to the most recent version of Cornetto -LRB- Vossen et al. , 2007 -RRB- , an extension of the Dutch WordNet , and we experiment with various parameters of the algorithm , in order to arrive at a good setting for porting the method to other languages . | Text : arrive | Annotation Set : [{'name': 'Goal', 'spans': [{'start': 46, 'end': 57, 'text': 'at a good setting for porting the method to other languages'}]}]","The authors start with an English subjectivity lexicon with 6 , 856 entries , OpinionFinder -LRB- Wiebe and Riloff , 2005 -RRB- , and automatically translate it into Romanian using two bilingual dictionaries , obtaining a Romanian lexicon with 4 , 983 entries . | Text : entries | Annotation Set : [{'name': 'Theme', 'spans': [{'start': 3, 'end': 10, 'text': 'with an English subjectivity lexicon with 6'}]}]","The authors start with an English subjectivity lexicon with 6 , 856 entries , OpinionFinder -LRB- Wiebe and Riloff , 2005 -RRB- , and automatically translate it into Romanian using two bilingual dictionaries , obtaining a Romanian lexicon with 4 , 983 entries . | Text : entries | Annotation Set : []","A manual evaluation of a sample of 123 entries of this lexicon showed that 50 % of the entries do indicate subjectivity . | Text : entries | Annotation Set : [{'name': 'Theme', 'spans': [{'start': 9, 'end': 12, 'text': 'of this lexicon'}]}]",A manual evaluation of a sample of 123 entries of this lexicon showed that 50 % of the entries do indicate subjectivity . | Text : entries | Annotation Set : [],0,0,0,0,0
52,Choosing,337,"We chose the system of Otterbacher et al . | Text : chose | Annotation Set : [{'name': 'Cognizer', 'spans': [{'start': 0, 'end': 1, 'text': 'We'}]}, {'name': 'Chosen', 'spans': [{'start': 2, 'end': 8, 'text': 'the system of Otterbacher et al'}]}]","< newSection > 6 Evaluation We randomly selected 23 company stock names , and constructed a document collection for each containing all the news provided in the Yahoo ! | Text : selected | Annotation Set : [{'name': 'Cognizer', 'spans': [{'start': 5, 'end': 6, 'text': 'We'}]}, {'name': 'Chosen', 'spans': [{'start': 8, 'end': 21, 'text': '23 company stock names , and constructed a document collection for each containing'}]}]","Fi nance news feed for that company in a period of two days -LRB- the time period was chosen randomly -RRB- . | Text : chosen | Annotation Set : [{'name': 'Chosen', 'spans': [{'start': 19, 'end': 20, 'text': 'randomly'}]}]","When selecting the company names , we took care of not picking those which have only a few news articles for that period of time . | Text : selecting | Annotation Set : [{'name': 'Chosen', 'spans': [{'start': 2, 'end': 5, 'text': 'the company names'}]}]","When selecting the company names , we took care of not picking those which have only a few news articles for that period of time . | Text : picking | Annotation Set : [{'name': 'Cognizer', 'spans': [{'start': 6, 'end': 7, 'text': 'we'}]}, {'name': 'Chosen', 'spans': [{'start': 12, 'end': 25, 'text': 'those which have only a few news articles for that period of time'}]}]",0,0,0,0,0
53,Desirability,334,"We present a head to head evaluation and find that our performance is significantly better than the competition . | Text : better | Annotation Set : [{'name': 'Comparison_set', 'spans': [{'start': 15, 'end': 18, 'text': 'than the competition'}]}, {'name': 'Evaluee', 'spans': [{'start': 10, 'end': 12, 'text': 'our performance'}]}, {'name': 'Degree', 'spans': [{'start': 13, 'end': 14, 'text': 'significantly'}]}]","Rather we limit ourselves to particular papers and systems that have had the greatest impact on , and similarity to , ours . | Text : greatest | Annotation Set : [{'name': 'Evaluee', 'spans': [{'start': 14, 'end': 15, 'text': 'impact'}]}]","Their EM requires careful initialization โ sufficiently careful that the EM version only performs 0.4 % better than the initialized program alone . | Text : better | Annotation Set : [{'name': 'Comparison_set', 'spans': [{'start': 17, 'end': 22, 'text': 'than the initialized program alone'}]}, {'name': 'Evaluee', 'spans': [{'start': 14, 'end': 16, 'text': '0.4 %'}]}]","The system uses the conventional ranking approach , applying a maximum-entropy classifier to pairs of pronoun and potential antecedent and selecting the best antecedent . | Text : best | Annotation Set : [{'name': 'Evaluee', 'spans': [{'start': 23, 'end': 24, 'text': 'antecedent'}]}]","By the time we start training the gender assignment probabilities the model has learned to prefer nearer antecedents as well as ones with other desirable properties . | Text : desirable | Annotation Set : [{'name': 'Evaluee', 'spans': [{'start': 25, 'end': 26, 'text': 'properties'}]}]",0,0,0,0,0
54,Intentionally_create,323,"We tested our approach on the MEDIA corpus -LRB- human-machine dialogs -RRB- and on a new corpus -LRB- human-machine and humanhuman dialogs -RRB- produced in the European LUNA project . | Text : produced | Annotation Set : [{'name': 'Created_entity', 'spans': [{'start': 18, 'end': 22, 'text': 'human-machine and humanhuman dialogs'}]}, {'name': 'Time', 'spans': [{'start': 24, 'end': 29, 'text': 'in the European LUNA project'}]}]","To evaluate the above kernels between two T1 and T2 , we need to define a set F = -LCB- f1 , f2 , . | Text : set | Annotation Set : [{'name': 'Created_entity', 'spans': [{'start': 18, 'end': 19, 'text': '='}]}]","More specifically , in our approach , we use tree fragments to establish the order of correctness between two alternative annotations . | Text : establish | Annotation Set : [{'name': 'Creator', 'spans': [{'start': 7, 'end': 8, 'text': 'we'}]}, {'name': 'Created_entity', 'spans': [{'start': 13, 'end': 17, 'text': 'the order of correctness'}]}]","Given the small size of LUNA corpus , we did not carried out parameterization on a development set but we used default or a priori parameters . | Text : set | Annotation Set : [{'name': 'Created_entity', 'spans': [{'start': 12, 'end': 17, 'text': 'out parameterization on a development'}]}]","Regarding the generation of the training instances ๏ฟฝsi , sj/ , we set m to 10 and we choose one of the 10-best hypotheses as the second element of the pair , sj , thus generating 10 different pairs . | Text : set | Annotation Set : [{'name': 'Creator', 'spans': [{'start': 11, 'end': 12, 'text': 'we'}]}, {'name': 'Created_entity', 'spans': [{'start': 13, 'end': 16, 'text': 'm to 10'}]}]",0,0,0,0,0
55,Buildings,316,"The first of these is argument indexing and its relationship to semantic role labelling , the second is semantic dependency structure . | Text : structure | Annotation Set : [{'name': 'Building', 'spans': [{'start': 20, 'end': 21, 'text': 'structure'}]}]","I have chosen to talk about this both because of its relationship with the currently popular task of semantic role labelling and because the DELPH-IN approach is now fairly stable after a quite considerable degree of experimentation . | Text : stable | Annotation Set : [{'name': 'Building', 'spans': [{'start': 29, 'end': 30, 'text': 'stable'}]}]","An ARG3 may occur without an instantiated ARG2 when a syntactically optional argument is missing -LRB- e.g. , Kim gave to the library -RRB- , but this is explicit in the linearised form -LRB- e.g. , give v -LRB- e , x , u , y -RRB- -RRB- . | Text : library | Annotation Set : [{'name': 'Building', 'spans': [{'start': 22, 'end': 23, 'text': 'library'}]}]","However this does nothing for semantic generalisation , blocks the use of argument labels in syntactic generalisations and leads to an extreme proliferation of lexical types when using typed feature structure formalisms -LRB- one type would be required per lexeme -RRB- . | Text : structure | Annotation Set : [{'name': 'Building', 'spans': [{'start': 30, 'end': 31, 'text': 'structure'}]}]","< newSection > 4 Dependency MRS The second main topic I want to address is a form of semantic dependency structure -LRB- DMRS : see wiki.delph-in.net for the evolving details -RRB- . | Text : structure | Annotation Set : [{'name': 'Building', 'spans': [{'start': 20, 'end': 21, 'text': 'structure'}]}]",0,0,0,0,0
56,Discussion,315,"However , while researchers have shown that it is sometimes possible to annotate corpora that capture features of interpretation , to provide empirical support for theories , as in -LRB- Eugenio et al. , 2000 -RRB- , or to build classifiers that assist in dialogue reasoning , as in -LRB- Jordan and Walker , 2005 -RRB- , it is rarely feasible to fully annotate the interpretations themselves . | Text : dialogue | Annotation Set : []","As an alternative to annotation , we argue here that dialogue systems can and should prepare their own training data by inference from underspecified models , which provide sets of candidate meanings , and from skilled engagement with their interlocutors , who know which meanings are right . | Text : dialogue | Annotation Set : []","As an alternative to annotation , we argue here that dialogue systems can and should prepare their own training data by inference from underspecified models , which provide sets of candidate meanings , and from skilled engagement with their interlocutors , who know which meanings are right . | Text : interlocutors | Annotation Set : [{'name': 'Interlocutor_1', 'spans': [{'start': 38, 'end': 39, 'text': 'their'}]}]","As subsequent utterances are interpreted in those contexts , ambiguities may ramify , cascade , or disappear , giving new insight into the pattern of activity that the interlocutor is engaged in . | Text : interlocutor | Annotation Set : []",The interlocutor 's answer may indicate not only what they mean now but also what they must have meant earlier when they used the original ambiguous utterance . | Text : interlocutor | Annotation Set : [],0,0,0,0,0
57,Operational_testing,314,"We train 2 PCFG grammars , one on each tagged version of the Treebank , and test them on the subset of the development set in which every token is completely covered by the KC Analyzer -LRB- 351 sentences -RRB- .7 The input to the parser is the yields and disambiguated pre-terminals of the trees to be parsed . | Text : test | Annotation Set : [{'name': 'Tester', 'spans': [{'start': 0, 'end': 1, 'text': 'We'}]}, {'name': 'Product', 'spans': [{'start': 17, 'end': 18, 'text': 'them'}]}]","To test this hypothesis , we use it to estimate p -LRB- tKC โ* w -RRB- in some of our models . | Text : test | Annotation Set : [{'name': 'Product', 'spans': [{'start': 2, 'end': 4, 'text': 'this hypothesis'}]}]","We perform all our experiments on Version 2 of the Hebrew Treebank , and follow the train/test/dev split introduced in -LRB- Tsarfaty and Simaโan , 2007 -RRB- : section 1 is used for development , sections 2-12 for training , and section 13 is the test set , which we do not use in this work . | Text : test | Annotation Set : []","All the reported results are on the development set.10 After removal of empty sentences , we have 5241 sentences for training , and 483 for testing . | Text : testing | Annotation Set : []","In order to compare the performance of the model on the various tagset representations -LRB- TB tags , KC tags , Layered -RRB- , we remove from the test set 51 sentences in which at least one token is marked as not having any correct segmentation in the KC Analyzer . | Text : test | Annotation Set : []",0,0,0,0,0
58,Presence,312,"An example of unfolding tuple extraction , contrasted with the SAMT chunk-based rules construction , is presented in Figure 1 . | Text : presented | Annotation Set : [{'name': 'Location', 'spans': [{'start': 17, 'end': 20, 'text': 'in Figure 1'}]}]","Stage-by-stage RAM and time requirements are presented in Table 4 , while translation quality evaluation results can be found in Table 3 . | Text : presented | Annotation Set : [{'name': 'Location', 'spans': [{'start': 7, 'end': 10, 'text': 'in Table 4'}]}, {'name': 'Entity', 'spans': [{'start': 0, 'end': 5, 'text': 'Stage-by-stage RAM and time requirements'}]}]","Table 5 presents the comparative statistics of errors generated by the SAMT and the N-gram-based SMT systems . | Text : presents | Annotation Set : [{'name': 'Entity', 'spans': [{'start': 0, 'end': 2, 'text': 'Table 5'}]}]","We present two methods for parsing the voice query into different fields with particular emphasis on exploiting the ASR output beyond the 1-best hypothesis . | Text : present | Annotation Set : [{'name': 'Entity', 'spans': [{'start': 0, 'end': 1, 'text': 'We'}]}]","We present two different query parsing models in Section 4 and Section 5 and discuss experimental results in Section 6 . | Text : present | Annotation Set : [{'name': 'Entity', 'spans': [{'start': 0, 'end': 1, 'text': 'We'}]}]",0,0,0,0,0
59,Identicality,309,"For example , the word according caused other words in the same sentence to appear in the highlights nearly 25 % of the time . | Text : same | Annotation Set : [{'name': 'Type', 'spans': [{'start': 12, 'end': 13, 'text': 'sentence'}]}]","Of course , simply using the identities of words neglects the issue of lexical paraphrasing , e.g. , involving synonyms , which we address to some extent by using WordNet and other features described in this Section . | Text : identities | Annotation Set : [{'name': 'Type', 'spans': [{'start': 7, 'end': 9, 'text': 'of words'}]}]","AURUM has two types of features : sentence features , such as the position of the sentence or the existence of a negation word , receive the same value for the entire sentence . | Text : same | Annotation Set : [{'name': 'Type', 'spans': [{'start': 28, 'end': 29, 'text': 'value'}]}]","For the thresh experiment , the baseline always selected the same number of sentences as AURUM-thresh , but from the beginning of the article . | Text : same | Annotation Set : [{'name': 'Type', 'spans': [{'start': 11, 'end': 12, 'text': 'number'}]}]","An important aspect of web search is to be able to narrow down search results by distinguishing among people with the same name leading to multiple efforts focusing on web person name disambiguation in the literature -LRB- Mann and Yarowsky , 2003 ; Artiles et al. , 2007 , Cucerzan , 2007 -RRB- . | Text : same | Annotation Set : [{'name': 'Type', 'spans': [{'start': 22, 'end': 23, 'text': 'name'}]}]",0,0,0,0,0
60,Getting,299,"The relation between NLP and the humanities is worth reviewing , as a closer look into the way in which techniques such as text and link mining can demonstrate that the potential for mutual impact has gained in strength and diversity , and that important lessons can be learned for other application areas than the humanities . | Text : gained | Annotation Set : [{'name': 'Theme', 'spans': [{'start': 30, 'end': 35, 'text': 'the potential for mutual impact'}]}]","The data used in research in the domain of the humanities comes from a variety of sources : archives , musea -LRB- or in general cultural heritage collections -RRB- , libraries , etc . | Text : sources | Annotation Set : [{'name': 'Source', 'spans': [{'start': 16, 'end': 17, 'text': 'sources'}]}]","But whatever the genesis is of annotations capturing the semantics of an entire document , they are a very valuable source for the training of automatic classifiers . | Text : source | Annotation Set : [{'name': 'Source', 'spans': [{'start': 15, 'end': 16, 'text': 'they'}]}, {'name': 'Descriptor', 'spans': [{'start': 18, 'end': 20, 'text': 'very valuable'}]}]","Various kinds of multimedia collections can be a primary source of information for humanities researchers , in particular if there is a substantial amount of spoken word content , e.g. , broadcast news archives , and even more prominently : oral history collections . | Text : source | Annotation Set : [{'name': 'Source', 'spans': [{'start': 9, 'end': 10, 'text': 'source'}]}, {'name': 'Theme', 'spans': [{'start': 10, 'end': 12, 'text': 'of information'}]}]","Research has shown that the quality of the automatically generated speech transcriptions , and as a consequence also the index quality , can increase if the language models applied have been optimised to both the available metadata -LRB- in particular on the named entities in the annotations -RRB- and the collateral sources available -LRB- Huijbregts et al. , 2007 -RRB- . | Text : sources | Annotation Set : [{'name': 'Source', 'spans': [{'start': 51, 'end': 52, 'text': 'sources'}]}, {'name': 'Theme', 'spans': [{'start': 50, 'end': 51, 'text': 'collateral'}]}]",0,0,0,0,0
61,Evaluative_comparison,299,"In fact , -LRB- Villarejo et al. , 2005 -RRB- studied the performance of classโbased WSD comparing only SuperSenses and SUMO by 10โfold crossโvalidation on SemCor , but they did not provide results for SensEval2 nor SensEval3 . | Text : comparing | Annotation Set : []","Those results showing a statistically significant13 positive difference when compared with the baseline are in marked bold . | Text : compared | Annotation Set : [{'name': 'Standard_item', 'spans': [{'start': 10, 'end': 13, 'text': 'with the baseline'}]}, {'name': 'Profiled_attribute', 'spans': [{'start': 2, 'end': 8, 'text': 'showing a statistically significant13 positive difference'}]}]",In order to compare their contribution we also performed a `` basicFeat '' test without including semantic features . | Text : compare | Annotation Set : [],"Finally , we also compare the 14Each portion contains also the same files than the previous portion . | Text : compare | Annotation Set : []","Although this result is achieved for the all words SensEval3 task , including adjectives , we can compare both results since in SE2 and SE3 adjectives obtain very high performance figures . | Text : compare | Annotation Set : []",0,0,0,0,0
62,Labeling,296,"We rank candidates according to this probability and the row labeled `` trans '' in Table 4 shows that this model helps in subsantially improving the recall of `` Occupation '' and `` Religion '' , yielding a 7 % and 3 % average improvement in F-measure respectively , on top of the position model described in Section 6 . | Text : labeled | Annotation Set : [{'name': 'Label', 'spans': [{'start': 19, 'end': 21, 'text': 'this model'}]}, {'name': 'Entity', 'spans': [{'start': 2, 'end': 10, 'text': 'candidates according to this probability and the row'}]}]","The rows with label `` combined+corr '' in Table 4 and Table 3 shows substantial performaance gains using inter-attribute correlations , such as the 7 % absolute average gain for Birthplace over the Section 8 combined models , and a 3 % absolute gain for Nationality and Religion . | Text : label | Annotation Set : []","State transitions are performed as stack shift operations followed by a push of a preterminal semantic category label . | Text : label | Annotation Set : [{'name': 'Entity', 'spans': [{'start': 16, 'end': 17, 'text': 'category'}]}]","The best SLU hypothesis is given by where bestpathn -LRB- in this case n is 1 for the 1-best hypothesis -RRB- performs a Viterbi search on the FST quences in common between two sentences , in the space of n-grams -LRB- for any n -RRB- . and outputs the n-best hypotheses and projectC performs a projection of the FST on the output labels , in this case the concepts . | Text : labels | Annotation Set : [{'name': 'Entity', 'spans': [{'start': 61, 'end': 62, 'text': 'output'}]}]","Words that were not tagged as positive or negative were manually labeled . | Text : labeled | Annotation Set : [{'name': 'Entity', 'spans': [{'start': 0, 'end': 1, 'text': 'Words'}]}]",0,0,0,0,0
63,Entity,294,"Evaluation conditions were case-insensitive and sensitive to tokenization . | Text : conditions | Annotation Set : [{'name': 'State', 'spans': [{'start': 1, 'end': 2, 'text': 'conditions'}]}, {'name': 'Entity', 'spans': [{'start': 0, 'end': 1, 'text': 'Evaluation'}]}]","Because the graph is analyzed as a whole , these techniques have the remarkable property of being able to find globally optimal solutions , given the relations between entities . | Text : entities | Annotation Set : [{'name': 'Entity', 'spans': [{'start': 28, 'end': 29, 'text': 'entities'}]}]","We have put particular effort into COREF 's skills with three kinds of ambiguity : word-sense ambiguities , where COREF finds multiple resolutions for the domain concept evoked by the use of a lexical item , as in the interaction -LRB- 1 -RRB- of Figure 1 ; referential ambiguities , where COREF takes a noun phrase to be compatible with multiple objects from the display ; and speech act ambiguities , where alternative interpretations communicate or implicate different kinds of contributions to the ongoing task . | Text : item | Annotation Set : [{'name': 'Entity', 'spans': [{'start': 34, 'end': 35, 'text': 'item'}]}]","Because we seek to estimate P -LRB- I = it , j|o , 5t = sk -RRB- , which conditions the probability assigned to I = it , j on the correctness of state sk , we consider only those interpretations arising in states that are retrospectively identified as correct . | Text : conditions | Annotation Set : [{'name': 'Entity', 'spans': [{'start': 18, 'end': 19, 'text': 'which'}]}]","Closer in spirit is AI research on learning vocabulary items by connecting user vocabulary to the agent 's perceptual representations at the time of utterance -LRB- Oates et al. , 2000 ; Roy and Pentland , 2002 ; Cohen et al. , 2002 ; Yu and Ballard , 2004 ; Steels and Belpaeme , 2005 -RRB- . | Text : items | Annotation Set : [{'name': 'Entity', 'spans': [{'start': 9, 'end': 10, 'text': 'items'}]}]",0,0,0,0,0
64,Questioning,293,< newSection > 4 Classification Frequencies of words in word categories were determined using Linguistic Inquiry and Word Count -LRB- LIWC -RRB- . | Text : Inquiry | Annotation Set : [],"We introduce a novel query -LRB- i.e. , company name -RRB- expansion method and a simple unsupervized algorithm for sentence ranking . | Text : query | Annotation Set : []","โข The name of the company is not a query , e.g. , as it is specified in the context of the DUC competitions1 , and requires an exten sion . | Text : query | Annotation Set : [{'name': 'Speaker', 'spans': [{'start': 1, 'end': 6, 'text': 'The name of the company'}]}]","Initially , a query consists exclusively of the `` symbol '' , i.e. , the abbreviation of the name of a company as it is listed on the stock market . | Text : query | Annotation Set : []","Thus , the similarity to the query alone is not the decisive parameter in com puting sentence relevance . | Text : query | Annotation Set : []",0,0,0,0,0
65,Performing_arts,282,"In order to facilitate the comparison between our results and the baseline , we use thef-measure -LRB- f-score -RRB- that combines precision and recall in a unique measure : We evaluate here the performance of our WSD method on the 150 ambiguous nouns of our sample . | Text : performance | Annotation Set : [{'name': 'Performance', 'spans': [{'start': 33, 'end': 34, 'text': 'performance'}]}]","To our opinion , to evaluate the performance of a complete speech-to-speech translation system , we need to compare the source speech used as input to the translated output speech in the target language . | Text : performance | Annotation Set : [{'name': 'Performance', 'spans': [{'start': 7, 'end': 8, 'text': 'performance'}]}]","Performance is good for the interpreter : 98 % of the information correctly translated by the automatic system is also correctly interpreted by the human . | Text : Performance | Annotation Set : [{'name': 'Performance', 'spans': [{'start': 0, 'end': 1, 'text': 'Performance'}]}]","Although we can not compare the performance of the restricted automatic system to that of the restricted interpreter -LRB- since data sets of questions are different -RRB- , it seems that of the interpreter is better . | Text : performance | Annotation Set : [{'name': 'Performance', 'spans': [{'start': 6, 'end': 7, 'text': 'performance'}]}]","These overspecifications have two objectives : optimizing the analysis performance -LRB- reducing the noise of homographic character strings that look like constructed neologisms but that are only misspellings - see below in the evaluation section -RRB- , and refining the analysis , i.e. selecting the appropriate LFR and , consequently , the appropriate translation . | Text : performance | Annotation Set : [{'name': 'Performance', 'spans': [{'start': 9, 'end': 10, 'text': 'performance'}]}]",0,0,0,0,0
66,Removing,281,"The paper takes the position that for the humanities a variant of Mercer 's saying is even more true . | Text : takes | Annotation Set : [{'name': 'Agent', 'spans': [{'start': 0, 'end': 2, 'text': 'The paper'}]}, {'name': 'Theme', 'spans': [{'start': 3, 'end': 19, 'text': ""the position that for the humanities a variant of Mercer 's saying is even more true""}]}]","The P -LRB- j -RRB- function is monotonic because the algorithm only adds new productions and never removes . | Text : removes | Annotation Set : [{'name': 'Theme', 'spans': [{'start': 13, 'end': 15, 'text': 'new productions'}]}]","If the parsing is successful we need a way to extract the syntax trees . | Text : extract | Annotation Set : [{'name': 'Theme', 'spans': [{'start': 11, 'end': 14, 'text': 'the syntax trees'}]}]","We combine DAL scores with syntactic constituents and then extract ngrams of constituents from all sentences . | Text : extract | Annotation Set : [{'name': 'Source', 'spans': [{'start': 13, 'end': 16, 'text': 'from all sentences'}]}, {'name': 'Theme', 'spans': [{'start': 2, 'end': 7, 'text': 'DAL scores with syntactic constituents'}]}]","Here we describe our scoring scheme and the features we extract from sentences for classification tasks . | Text : extract | Annotation Set : [{'name': 'Source', 'spans': [{'start': 11, 'end': 13, 'text': 'from sentences'}]}, {'name': 'Theme', 'spans': [{'start': 9, 'end': 10, 'text': 'we'}]}]",0,0,0,0,0
67,Creating,275,"For instance , if & is translated by $ , ' and -LRB- , three sets of TUs are formed -LRB- where & is translated by $ -LRB- ' & D $ ' TUs -RRB- , by ' -LRB- ' & D'' TUs -RRB- , etc. -RRB- . | Text : formed | Annotation Set : [{'name': 'Created_entity', 'spans': [{'start': 14, 'end': 18, 'text': 'three sets of TUs'}]}]","The created sense clusters , grouping semantically similar equivalents , can be compared to WordNet synsets . | Text : created | Annotation Set : []",The algorithm exploits a view of PMCFG as an infinite contextfree grammar where new context-free categories and productions are generated during parsing . | Text : productions | Annotation Set : [],"The algorithm exploits a view of PMCFG as an infinite contextfree grammar where new context-free categories and productions are generated during parsing . | Text : generated | Annotation Set : [{'name': 'Created_entity', 'spans': [{'start': 13, 'end': 18, 'text': 'new context-free categories and productions'}]}, {'name': 'Time', 'spans': [{'start': 20, 'end': 22, 'text': 'during parsing'}]}]","For the production to be well formed the conditions We use the same definition of PMCFG as is used by Seki and Kato -LRB- 2008 -RRB- and Seki et al . | Text : formed | Annotation Set : [{'name': 'Created_entity', 'spans': [{'start': 7, 'end': 30, 'text': 'the conditions We use the same definition of PMCFG as is used by Seki and Kato -LRB- 2008 -RRB- and Seki et al'}]}]",0,0,0,0,0
68,Measure_duration,263,"On the left pane is a large workspace for the user to explore the source text one sentence at a time . | Text : time | Annotation Set : [{'name': 'Unit', 'spans': [{'start': 20, 'end': 21, 'text': 'time'}]}]","< newSection > 5 Results The results of human evaluations for the user experiment are summarized in Table 3 , and the corresponding timing statistics -LRB- average minutes spent editing a sentence -RRB- is shown in Table 4 . | Text : minutes | Annotation Set : [{'name': 'Unit', 'spans': [{'start': 27, 'end': 28, 'text': 'minutes'}]}, {'name': 'Count', 'spans': [{'start': 26, 'end': 27, 'text': 'average'}]}]","The higher quality of corrections does require the participants to put in more time . | Text : time | Annotation Set : [{'name': 'Unit', 'spans': [{'start': 13, 'end': 14, 'text': 'time'}]}]","Overall , the participants took 2.5 times as long when they have the interface than when they do not . | Text : times | Annotation Set : [{'name': 'Unit', 'spans': [{'start': 6, 'end': 7, 'text': 'times'}]}, {'name': 'Count', 'spans': [{'start': 5, 'end': 6, 'text': '2.5'}]}]","Relatedly , the difference in the amount of time spent is the smallest for this document ; participants using The Chinese Room took about 1.5 times longer . | Text : time | Annotation Set : [{'name': 'Unit', 'spans': [{'start': 8, 'end': 9, 'text': 'time'}]}]",0,0,0,0,0
69,Awareness,262,โข The summary has to provide novel informa tion related to the company and should avoid general facts about it which the user is sup posed to know . | Text : know | Annotation Set : [],"Unfortunately , exist ing query expansion techniques which utilize such knowledge sources as WordNet or Wikipedia are not useful for symbol expansion . | Text : knowledge | Annotation Set : []","A straightforward way to define the novelty weight of a word would be to draw a line between the `` known '' words , i.e. , words appearing in the busi ness summary , and the rest . | Text : known | Annotation Set : []","All an notators had average to good understanding of the financial domain . | Text : understanding | Annotation Set : [{'name': 'Content', 'spans': [{'start': 8, 'end': 12, 'text': 'of the financial domain'}]}, {'name': 'Degree', 'spans': [{'start': 6, 'end': 7, 'text': 'good'}]}]","We also mention some work on using textual news data for stock indices prediction which we are aware of . | Text : aware | Annotation Set : [{'name': 'Content', 'spans': [{'start': 14, 'end': 15, 'text': 'which'}]}, {'name': 'Cognizer', 'spans': [{'start': 15, 'end': 16, 'text': 'we'}]}]",0,0,0,0,0
70,System,260,"It manages additional NLP tools for the source language and translation resources so that the user can explore this extra information to gain enough understanding of the source text to correct MT errors . | Text : manages | Annotation Set : [{'name': 'System', 'spans': [{'start': 2, 'end': 5, 'text': 'additional NLP tools'}]}, {'name': 'Operator', 'spans': [{'start': 0, 'end': 1, 'text': 'It'}]}]","Fourth , based on previous studies reporting that automatic translations may improve when given decomposed source inputs -LRB- Mellebeek et al. , 2005 -RRB- , we allow the users to select a substring from the source text for the MT system to translate . | Text : system | Annotation Set : [{'name': 'Complex', 'spans': [{'start': 40, 'end': 41, 'text': 'system'}]}, {'name': 'Function', 'spans': [{'start': 39, 'end': 40, 'text': 'MT'}]}]","The list is kept short ; its purpose is less for reranking but more to give the users a sense of the kinds of hypotheses that the MT system is considering . | Text : system | Annotation Set : [{'name': 'Complex', 'spans': [{'start': 28, 'end': 29, 'text': 'system'}]}, {'name': 'Function', 'spans': [{'start': 27, 'end': 28, 'text': 'MT'}]}]","Prior to the start of the study , the participants received a 20 minute long presentational tutorial about the basic functionalities supported by our system , but they did not have an opportunity to explore the system on their own . | Text : system | Annotation Set : [{'name': 'Complex', 'spans': [{'start': 36, 'end': 37, 'text': 'system'}]}]","The oracle user 's average score is 0.70 ; in contrast , an oracle of users who did not use the system is 0.54 -LRB- cf. the MT 's overall of 0.35 and the bilingual translator 's overall of 0.83 -RRB- . | Text : system | Annotation Set : [{'name': 'Complex', 'spans': [{'start': 21, 'end': 22, 'text': 'system'}]}]",0,0,0,0,0
71,Topic,256,"The rest of the paper is organized as follows : Section 2 introduces the inference rule collection we use , based on the Discovery of Inference Rules from Text -LRB- henceforth DIRT -RRB- algorithm and discusses previous work on applying it to the RTE task . | Text : discusses | Annotation Set : [{'name': 'Text', 'spans': [{'start': 36, 'end': 38, 'text': 'previous work'}]}, {'name': 'Topic', 'spans': [{'start': 38, 'end': 45, 'text': 'on applying it to the RTE task'}]}]","Section 4 discusses the application of the rules for the RTE data , describing the structure representation we use to identify the appropriate context for the rule application . | Text : discusses | Annotation Set : [{'name': 'Text', 'spans': [{'start': 0, 'end': 2, 'text': 'Section 4'}]}, {'name': 'Topic', 'spans': [{'start': 3, 'end': 12, 'text': 'the application of the rules for the RTE data'}]}]","This aspect is very important and we plan to address it in our future work . | Text : address | Annotation Set : [{'name': 'Occasion', 'spans': [{'start': 10, 'end': 11, 'text': 'it'}]}]","In order to address the issue of missing rules , we investigate the effects of combining DIRT with an exact hand-coded lexical resource in order to create new rules . | Text : address | Annotation Set : []","The first issue is concerned with correctly identifying the pairs in which the knowledge encoded in these rules is needed . | Text : concerned | Annotation Set : [{'name': 'Text', 'spans': [{'start': 0, 'end': 3, 'text': 'The first issue'}]}]",0,0,0,0,0
72,Building,253,"The construction of a prototype is firstly presented , highlighting the methodological issues of such approach . | Text : construction | Annotation Set : [{'name': 'Created_entity', 'spans': [{'start': 2, 'end': 16, 'text': 'of a prototype is firstly presented , highlighting the methodological issues of such approach'}]}]","< newSection > 1 Introduction Formalising morphological information to deal with morphologically constructed unknown words in machine translation seems attractive , but raises many questions about the resources and the prerequisites -LRB- both theoretical and practical -RRB- that would make such symbolic treatment efficient and feasible . | Text : constructed | Annotation Set : [{'name': 'Created_entity', 'spans': [{'start': 13, 'end': 14, 'text': 'unknown'}]}]","In this paper , we describe the prototype we built to evaluate the feasibility of such approach . | Text : built | Annotation Set : [{'name': 'Created_entity', 'spans': [{'start': 6, 'end': 8, 'text': 'the prototype'}]}, {'name': 'Time', 'spans': [{'start': 0, 'end': 3, 'text': 'In this paper'}]}, {'name': 'Agent', 'spans': [{'start': 8, 'end': 9, 'text': 'we'}]}]","We focus on the knowledge required to build such system and on its evaluation . | Text : build | Annotation Set : [{'name': 'Created_entity', 'spans': [{'start': 8, 'end': 10, 'text': 'such system'}]}, {'name': 'Agent', 'spans': [{'start': 0, 'end': 1, 'text': 'We'}]}]","We then explain why implementing morphology in the context of machine translation -LRB- MT -RRB- is a real challenge and what kind of aspects need to be taken into account -LRB- section 4 -RRB- , and we show that translating constructed neologisms is not only a mechanical decomposition but requires more fine-grained analysis . | Text : constructed | Annotation Set : [{'name': 'Created_entity', 'spans': [{'start': 41, 'end': 42, 'text': 'neologisms'}]}]",0,0,0,0,0
73,Concessive,251,"However , if the scores are similar but there is a difference in the number of unique sentences extracted , this means a system has gone beyond the first 4 sentences and extracted others from deeper down inside the text . | Text : However | Annotation Set : [{'name': 'Main_statement', 'spans': [{'start': 2, 'end': 19, 'text': 'if the scores are similar but there is a difference in the number of unique sentences extracted'}]}]","However , we will give some theoretical and empirical evidences that Loopy Belief Propagation can give us a good approximation procedure . | Text : However | Annotation Set : []",We differ however in the training and decoding procedure we propose . | Text : however | Annotation Set : [],"It has been shown , however , that the BP algorithm do converge in many practical cases , and that the results it produces are often surprisingly good approximations -LRB- see , for example , -LRB- Murphy et al. , 1999 -RRB- or -LRB- Weiss and Freeman , 2001 -RRB- -RRB- . | Text : however | Annotation Set : []",The training and decoding procedures we propose are however different . | Text : however | Annotation Set : [],0,0,0,0,0
74,Change_position_on_a_scale,248,"When varying the rare words threshold from 2 to 10 , performance drops considerably . | Text : drops | Annotation Set : []","Morphological segmentation has been shown to benefit Arabic-to-English -LRB- Habash and Sadat , 2006 -RRB- and English-to-Arabic -LRB- Badr et al. , 2008 -RRB- translation , although the gains tend to decrease with increasing training data size . | Text : decrease | Annotation Set : [{'name': 'Item', 'spans': [{'start': 32, 'end': 37, 'text': 'with increasing training data size'}]}]","Morphological segmentation has been shown to benefit Arabic-to-English -LRB- Habash and Sadat , 2006 -RRB- and English-to-Arabic -LRB- Badr et al. , 2008 -RRB- translation , although the gains tend to decrease with increasing training data size . | Text : increasing | Annotation Set : []","Similarly , for Arabic-to-English , Lee -LRB- 2004 -RRB- , and Habash and Sadat -LRB- 2006 -RRB- show that various segmentation schemes lead to improvements that decrease with increasing parallel corpus size . | Text : decrease | Annotation Set : []","Similarly , for Arabic-to-English , Lee -LRB- 2004 -RRB- , and Habash and Sadat -LRB- 2006 -RRB- show that various segmentation schemes lead to improvements that decrease with increasing parallel corpus size . | Text : increasing | Annotation Set : []",0,0,0,0,0
75,Trust,247,"Using fewer senses with the BNC-trained system can result in a drop in accuracy of almost 2 % . | Text : accuracy | Annotation Set : [{'name': 'Information', 'spans': [{'start': 13, 'end': 14, 'text': 'accuracy'}]}]","The second column denotes the accuracies obtained when testing on SPORTS , and the third column the accuracies for FINANCES . | Text : accuracies | Annotation Set : []","The second column denotes the accuracies obtained when testing on SPORTS , and the third column the accuracies for FINANCES . | Text : accuracies | Annotation Set : []","The low accuracy obtained with MFS , e.g. 39.0 of precision in SPORTS , shows the difficulty of this task . | Text : accuracy | Annotation Set : []","While such features are essential , we argue that audiencespecific features that model the cognitive characteristics of a user group can improve the accuracy of a readability assessment tool . | Text : accuracy | Annotation Set : [{'name': 'Information', 'spans': [{'start': 12, 'end': 20, 'text': 'model the cognitive characteristics of a user group'}]}]",0,0,0,0,0
76,Political_locales,247,"An example of generative model is the Hidden Vector State model -LRB- HVS -RRB- -LRB- He and Young , 2005 -RRB- . | Text : State | Annotation Set : [{'name': 'Locale', 'spans': [{'start': 9, 'end': 10, 'text': 'State'}]}]","This approach extends the discrete Markov model encoding the context of each state as a vector . | Text : state | Annotation Set : [{'name': 'Locale', 'spans': [{'start': 12, 'end': 13, 'text': 'state'}]}]","State transitions are performed as stack shift operations followed by a push of a preterminal semantic category label . | Text : State | Annotation Set : [{'name': 'Locale', 'spans': [{'start': 0, 'end': 1, 'text': 'State'}]}]","Another simpler but effective generative model is the one based on Finite State Transducers . | Text : State | Annotation Set : [{'name': 'Locale', 'spans': [{'start': 12, 'end': 13, 'text': 'State'}]}]","It performs SLU as a translation process from words to concepts using Finite State Transducers -LRB- FST -RRB- . | Text : State | Annotation Set : [{'name': 'Locale', 'spans': [{'start': 13, 'end': 14, 'text': 'State'}]}]",0,0,0,0,0
77,Possession,245,"The inflections for the paradigms that the words laDkA -LRB- meaning boy -RRB- and laDkI -LRB- mean-ing girl -RRB- belong to are shown in Table 2 . | Text : belong | Annotation Set : [{'name': 'Possession', 'spans': [{'start': 17, 'end': 18, 'text': 'girl'}]}, {'name': 'Owner', 'spans': [{'start': 6, 'end': 9, 'text': 'the words laDkA'}]}]","The morphological rules input into the system are used to recognize word-forms that together belong to a paradigm . | Text : belong | Annotation Set : [{'name': 'Possession', 'spans': [{'start': 11, 'end': 12, 'text': 'word-forms'}]}, {'name': 'Owner', 'spans': [{'start': 15, 'end': 18, 'text': 'to a paradigm'}]}]","For highly agglutinative languages , such as Tamil and Malayalam , which can have thousands of suffixes , it would be necessary to use a Finite State Machine representation of the rules . | Text : have | Annotation Set : [{'name': 'Possession', 'spans': [{'start': 14, 'end': 17, 'text': 'thousands of suffixes'}]}, {'name': 'Owner', 'spans': [{'start': 11, 'end': 12, 'text': 'which'}]}]","It will be seen from Table 6 that about 20 % of the words have a BSE of only 1 . | Text : have | Annotation Set : [{'name': 'Possession', 'spans': [{'start': 15, 'end': 20, 'text': 'a BSE of only 1'}]}, {'name': 'Owner', 'spans': [{'start': 10, 'end': 14, 'text': '% of the words'}]}]","Table 10 lists the number of correct stems , incorrect stems , and finally a count of those incorrect stems that the HSE+Sup heuristic would have gotten right if the POS category had been available . | Text : have gotten | Annotation Set : [{'name': 'Possession', 'spans': [{'start': 27, 'end': 28, 'text': 'right'}]}, {'name': 'Owner', 'spans': [{'start': 21, 'end': 24, 'text': 'the HSE+Sup heuristic'}]}]",0,0,0,0,0
78,Degree,245,"Dependency information is very informative when present , but extremely sparse . | Text : very | Annotation Set : [{'name': 'Gradable_attribute', 'spans': [{'start': 4, 'end': 5, 'text': 'informative'}]}]","Additionally , the way in which these approaches encode prior knowledge is very different and their relative performance changes based on the task . | Text : very | Annotation Set : [{'name': 'Gradable_attribute', 'spans': [{'start': 13, 'end': 14, 'text': 'different'}]}]","Both approaches , used separately , have shown a good performance -LRB- Raymond and Riccardi , 2007 -RRB- , but they have very different characteristics and the way they encode prior knowledge is very different , thus designing models able to take into account characteristics of both approaches are particularly promising . | Text : very | Annotation Set : [{'name': 'Gradable_attribute', 'spans': [{'start': 23, 'end': 24, 'text': 'different'}]}]","Both approaches , used separately , have shown a good performance -LRB- Raymond and Riccardi , 2007 -RRB- , but they have very different characteristics and the way they encode prior knowledge is very different , thus designing models able to take into account characteristics of both approaches are particularly promising . | Text : very | Annotation Set : [{'name': 'Gradable_attribute', 'spans': [{'start': 34, 'end': 35, 'text': 'different'}]}]","Next , although the training data is small , the rerankers based on kernels appear to be very effective . | Text : very | Annotation Set : [{'name': 'Gradable_attribute', 'spans': [{'start': 18, 'end': 19, 'text': 'effective'}]}]",0,0,0,0,0
79,Natural_features,244,"< newSection > 1 Introduction In Information Extraction domain , named entities -LRB- NEs -RRB- are one of the most important textual units as they express an important part of the meaning of a document . | Text : Extraction | Annotation Set : [{'name': 'Locale', 'spans': [{'start': 7, 'end': 8, 'text': 'Extraction'}]}, {'name': 'Name', 'spans': [{'start': 6, 'end': 7, 'text': 'Information'}]}]","A maximal clique is a clique that is not a subset of any other clique . | Text : subset | Annotation Set : [{'name': 'Locale', 'spans': [{'start': 10, 'end': 11, 'text': 'subset'}]}]","The local maximum is the synset in the hypernymy chain having more relations than its immediate hyponym and immediate hypernym . | Text : hypernym | Annotation Set : [{'name': 'Locale', 'spans': [{'start': 19, 'end': 20, 'text': 'hypernym'}]}]","< newSection > 4.2 Results Tables 4 and 5 present the F1 measures -LRB- harmonic mean of recall and precision -RRB- for nouns and verbs respectively when training our systems on SemCor and testing on SE2 and SE3 . | Text : precision | Annotation Set : [{'name': 'Locale', 'spans': [{'start': 19, 'end': 20, 'text': 'precision'}]}]","In particular , those corresponding to nouns -LRB- ranging from 70 % to 80 % -RRB- . | Text : ranging | Annotation Set : [{'name': 'Locale', 'spans': [{'start': 8, 'end': 9, 'text': 'ranging'}]}]",0,0,0,0,0
80,Cognitive_connection,242,"Although the bulk of previous work has been devoted to the disambiguation problem1 , there are good reasons to believe that sense induction may be able to overcome some of the issues associated with WSD . | Text : associated | Annotation Set : [{'name': 'Concept_1', 'spans': [{'start': 31, 'end': 32, 'text': 'issues'}]}, {'name': 'Concept_2', 'spans': [{'start': 33, 'end': 35, 'text': 'with WSD'}]}]","A related problem concerns the granularity of the sense distinctions which is fixed , and may not be entirely suitable for different applications . | Text : related | Annotation Set : [{'name': 'Concept_1', 'spans': [{'start': 2, 'end': 3, 'text': 'problem'}]}]","We first present an overview of related work -LRB- Section 2 -RRB- and then describe our Bayesian model in more detail -LRB- Sections 3 and 4 -RRB- . | Text : related | Annotation Set : [{'name': 'Concept_1', 'spans': [{'start': 7, 'end': 8, 'text': 'work'}]}]","< newSection > 2 Related Work Sense induction is typically treated as a clustering problem , where instances of a target word are partitioned into classes by considering their co-occurring contexts . | Text : Related | Annotation Set : [{'name': 'Concept_1', 'spans': [{'start': 7, 'end': 8, 'text': 'induction'}]}]","Methods developed for supervised WSD often use a variety of information sources based not only on words but also on lemmas , parts of speech , collocations and syntactic relationships -LRB- Lee and Ng , 2002 -RRB- . | Text : relationships | Annotation Set : [{'name': 'Concept_2', 'spans': [{'start': 28, 'end': 29, 'text': 'syntactic'}]}]",0,0,0,0,0
81,Weapon,239,"The use of a maximum entropy approach simplifies the introduction of several additional models explaining the translation process : e* = arg max Pr -LRB- e|f -RRB- Aihi -LRB- e , f -RRB- -RRB- -RCB- -LRB- 3 -RRB- ๏ฟฝ= arg max -LCB- exp -LRB- e i The feature functions hi are the system models and the Ai weights are typically optimized to maximize a scoring function on a development set -LRB- Och and Ney , 2002 -RRB- . | Text : weights | Annotation Set : [{'name': 'Weapon', 'spans': [{'start': 57, 'end': 58, 'text': 'weights'}]}]","The Lemur IR toolkit -LRB- Ogilvie and Callan , 2001 -RRB- was used for sentence extraction . | Text : extraction | Annotation Set : [{'name': 'Weapon', 'spans': [{'start': 15, 'end': 16, 'text': 'extraction'}]}, {'name': 'Type', 'spans': [{'start': 14, 'end': 15, 'text': 'sentence'}]}]","There also exist domain specific comparable corpora -LRB- which are probably potentially parallel -RRB- , like the documentations that are done in the national/regional language as well as English , or the translations of many English research papers in French or some other language used for academic proposes . | Text : papers | Annotation Set : [{'name': 'Weapon', 'spans': [{'start': 37, 'end': 38, 'text': 'papers'}]}]","This feature vector is used to query the appropriate classifier model to obtain a vector of labels with weights . | Text : weights | Annotation Set : [{'name': 'Weapon', 'spans': [{'start': 18, 'end': 19, 'text': 'weights'}]}]","The evaluation of our approach on the recognizing textual entailment data shows promising results on precision and the error analysis suggests possible improvements . | Text : precision | Annotation Set : [{'name': 'Weapon', 'spans': [{'start': 15, 'end': 16, 'text': 'precision'}]}]",0,0,0,0,0
82,Position_on_a_scale,234,"They achieve a very high 80 % correct , but this is given hand-annotated number , gender and syntactic binding features to filter candidate antecedents and also ignores non-anaphoric pronouns . | Text : high | Annotation Set : [{'name': 'Variable', 'spans': [{'start': 5, 'end': 6, 'text': '80'}]}, {'name': 'Value', 'spans': [{'start': 4, 'end': 5, 'text': 'high'}]}, {'name': 'Degree', 'spans': [{'start': 3, 'end': 4, 'text': 'very'}]}]","We smooth them using Kneser-Ney smoothing , but even then their dynamic range -LRB- a factor of 106 -RRB- greatly exceeds those of the other parameters . | Text : exceeds | Annotation Set : [{'name': 'Item', 'spans': [{'start': 21, 'end': 26, 'text': 'those of the other parameters'}]}]","Furthermore , typically by this point there will be , say , twenty NPs to share the probability mass , so each one will only get an increase of 0.05 . | Text : increase | Annotation Set : [{'name': 'Item', 'spans': [{'start': 28, 'end': 30, 'text': 'of 0.05'}]}]","Now the closer ones have higher probability so forth and so on . | Text : higher | Annotation Set : [{'name': 'Item', 'spans': [{'start': 6, 'end': 7, 'text': 'probability'}]}, {'name': 'Value', 'spans': [{'start': 5, 'end': 6, 'text': 'higher'}]}]","This is correct , although the exact number probably is too low . | Text : low | Annotation Set : [{'name': 'Degree', 'spans': [{'start': 10, 'end': 11, 'text': 'too'}]}]",0,0,0,0,0
83,Placing,232,"RMRS is a โflatโ representation , consisting of a bag of elementary predications -LRB- EP -RRB- , a set of argument relations , and a set of constraints on the possible linkages of the EPs when the RMRS is resolved to scoped form . | Text : bag | Annotation Set : [{'name': 'Theme', 'spans': [{'start': 10, 'end': 13, 'text': 'of elementary predications'}]}]","Thus we can derive a graph between EPs , such that each link is labelled with an argument position and points to a unique EP . | Text : position | Annotation Set : [{'name': 'Theme', 'spans': [{'start': 17, 'end': 18, 'text': 'argument'}]}]","For example , to handle written English nouns , we could create a cascade of FSTs covering the rules that insert an e in words like bushes and parties and relate lexical y to surface i in words like buggies and parties and an FST that represents the possible sequences of morphemes in English nouns , including all of the noun stems in the English lexicon . | Text : insert | Annotation Set : [{'name': 'Goal', 'spans': [{'start': 23, 'end': 29, 'text': 'in words like bushes and parties'}]}, {'name': 'Agent', 'spans': [{'start': 17, 'end': 19, 'text': 'the rules'}]}, {'name': 'Theme', 'spans': [{'start': 21, 'end': 23, 'text': 'an e'}]}]","This presents problems in the case of non-concatenative morphology : discontinuous morphemes -LRB- circumfix-ation -RRB- ; infixation , which breaks up a morpheme by inserting another within it ; reduplication , by which part or all of some morpheme is copied ; and the template morphology -LRB- also called stempattern morphology , intercalation , and interdigitation -RRB- that characterizes Semitic languages , and which is the focus of much of this paper . | Text : inserting | Annotation Set : [{'name': 'Goal', 'spans': [{'start': 26, 'end': 28, 'text': 'within it'}]}, {'name': 'Theme', 'spans': [{'start': 21, 'end': 23, 'text': 'a morpheme'}]}]","The stem of a Semitic verb consists of a root , essentially a sequence of consonants , and a pattern , a sort of template which inserts other segments between the root consonants and possibly copies certain of them -LRB- see Tigrinya examples in the next section -RRB- . | Text : inserts | Annotation Set : [{'name': 'Agent', 'spans': [{'start': 25, 'end': 26, 'text': 'which'}]}, {'name': 'Theme', 'spans': [{'start': 27, 'end': 29, 'text': 'other segments'}]}]",0,0,0,0,0
84,Linguistic_meaning,232,We write wi โ* wj to denote that there exists a -LRB- possi-bly empty -RRB- path from wi to wj . | Text : denote | Annotation Set : [],"The projection of a node wi , denoted bwic , is the set of reflexivetransitive dependents of wi , that is : bwic = -LCB- wj โ V |wj โ* wi -RCB- . | Text : denoted | Annotation Set : []","Note that this set is the same as in WG1 , as these are the items that we denoted -LSB- 1 , n , h , o , o -RSB- in the previous parser . | Text : denoted | Annotation Set : []","Two main classes of errors common in such tasks : firstly , cases where the two sentences share many common words but actually convey different meaning , and secondly , cases where the two sentences are -LRB- exactly -RRB- parallel except at sentence ends where one sentence has more information than the other . | Text : meaning | Annotation Set : []","This means that 643 positions do not need to be corrected , setting a baseline of 70.1 % -LRB- 643/917 -RRB- for error correction.2 Following Dickinson -LRB- 2006 -RRB- , we train our models on the entire corpus , explicitly including NIL relations -LRB- see section 3.2 -RRB- ; we train on the original annotation , but not the corrections . | Text : means | Annotation Set : [{'name': 'Form', 'spans': [{'start': 0, 'end': 1, 'text': 'This'}]}]",0,0,0,0,0
85,Becoming_aware,229,"< newSection > Abstract We present a classifier to predict contextual polarity of subjective phrases in a sentence . | Text : Abstract | Annotation Set : [{'name': 'Cognizer', 'spans': [{'start': 0, 'end': 2, 'text': '< newSection'}]}, {'name': 'Phenomenon', 'spans': [{'start': 4, 'end': 18, 'text': 'We present a classifier to predict contextual polarity of subjective phrases in a sentence'}]}]","The dictionary has previously been used for detecting deceptive speech -LRB- Hirschberg et al. , 2005 -RRB- and recognizing emotion in speech -LRB- Athanaselis et al. , 2006 -RRB- . | Text : recognizing | Annotation Set : [{'name': 'Phenomenon', 'spans': [{'start': 19, 'end': 20, 'text': 'emotion'}]}]","This list is sequentially traversed until a match is found in DAL or the list ends , in which case no scores are assigned . | Text : found | Annotation Set : [{'name': 'Phenomenon', 'spans': [{'start': 6, 'end': 8, 'text': 'a match'}]}]","When a negation -LRB- e.g. , not , no , never , can not , did n't -RRB- is encountered , the state changes to the INVERT state . | Text : encountered | Annotation Set : [{'name': 'Phenomenon', 'spans': [{'start': 21, 'end': 28, 'text': 'the state changes to the INVERT state'}]}]","While in the INVERT state , if โbutโ is encountered , it switches back to the RETAIN state . | Text : encountered | Annotation Set : [{'name': 'Phenomenon', 'spans': [{'start': 7, 'end': 8, 'text': 'โbutโ'}]}]",0,0,0,0,0
86,Source_of_getting,228,"Unfortunately , exist ing query expansion techniques which utilize such knowledge sources as WordNet or Wikipedia are not useful for symbol expansion . | Text : sources | Annotation Set : [{'name': 'Source', 'spans': [{'start': 11, 'end': 12, 'text': 'sources'}]}, {'name': 'Theme', 'spans': [{'start': 10, 'end': 11, 'text': 'knowledge'}]}]","These 101 sentences are 10-20 words long and all of them are chosen from Xinhua sources 4 . | Text : sources | Annotation Set : [{'name': 'Source', 'spans': [{'start': 15, 'end': 17, 'text': 'sources 4'}]}]","COREF treats interpretation broadly as a problem of abductive intention recognition -LRB- Hobbs et al. , 1993 -RRB- .2 We give a brief sketch here to highlight the content of COREF 's representations , the sources of information that COREF uses to construct them , and the demands they place on disambiguation . | Text : sources | Annotation Set : [{'name': 'Source', 'spans': [{'start': 35, 'end': 36, 'text': 'sources'}]}, {'name': 'Theme', 'spans': [{'start': 36, 'end': 44, 'text': 'of information that COREF uses to construct them'}]}]","In general , dialogue coherence is an important source of evidence for all aspects of language , for both human language learning -LRB- Saxton et al. , 2005 -RRB- as well as machine models . | Text : source | Annotation Set : [{'name': 'Source', 'spans': [{'start': 8, 'end': 9, 'text': 'source'}]}, {'name': 'Theme', 'spans': [{'start': 9, 'end': 11, 'text': 'of evidence'}]}, {'name': 'Descriptor', 'spans': [{'start': 7, 'end': 8, 'text': 'important'}]}]","We present these results as a proof-of-concept that contribution tracking provides a source of information that an agent can use to improve its statistical interpretation process . | Text : source | Annotation Set : [{'name': 'Source', 'spans': [{'start': 12, 'end': 13, 'text': 'source'}]}, {'name': 'Theme', 'spans': [{'start': 13, 'end': 15, 'text': 'of information'}]}]",0,0,0,0,0
87,Connectors,228,"Argument values may be variables -LRB- e.g. , e8 , x4 : variables are the only possibility for values of ARG0 -RRB- , constants -LRB- strings such as `` London '' -RRB- , or holes -LRB- e.g. h5 -RRB- , which indicate scopal relationships . | Text : strings | Annotation Set : [{'name': 'Connector', 'spans': [{'start': 25, 'end': 26, 'text': 'strings'}]}]","In particular qeq constraints -LRB- the only type considered here -RRB- indicate that , in the scoped forms , a label must either plug a hole directly or be connected to it via a chain of quantifiers . | Text : chain | Annotation Set : []","Directed link labels are of the form ARG/H , ARG/EQ or ARG/NEQ , where ARG corresponds to an RMRS argument label . | Text : link | Annotation Set : [{'name': 'Connector', 'spans': [{'start': 1, 'end': 2, 'text': 'link'}]}]","By taking the EP with the hole as the origin , we can construct an EP-to-EP graph , using the argument name as a label for the link : of course , such links are asymmetric and thus the graph is directed . | Text : link | Annotation Set : [{'name': 'Connector', 'spans': [{'start': 27, 'end': 28, 'text': 'link'}]}]","Thus we can derive a graph between EPs , such that each link is labelled with an argument position and points to a unique EP . | Text : link | Annotation Set : [{'name': 'Connector', 'spans': [{'start': 12, 'end': 13, 'text': 'link'}]}]",0,0,0,0,0
88,Documents,225,"-LRB- 2005 -RRB- as a a starting point for our approach and also as a competitive baseline because it has been successfully tested in a simi lar settingโit has been applied to multi-document query-focused summarization of news documents . | Text : documents | Annotation Set : [{'name': 'Document', 'spans': [{'start': 37, 'end': 38, 'text': 'documents'}]}]","Given a graph G = -LRB- 5 , E -RRB- , where 5 is the set of all sentences from all input documents , and E is the set of edges representing normalized sentence similarities , Otterbacher et al . | Text : documents | Annotation Set : [{'name': 'Document', 'spans': [{'start': 22, 'end': 23, 'text': 'documents'}]}]","Similarity between sentences is de fined as the cosine of their vector representations : where tfw , s is the frequency of w in sentence s , |S |is the total number of sentences in the docu ments from which sentences are to be extracted , and sfw is the number of sentences which contain the word w -LRB- all words in the documents as well wโq where tfw , x stands for the number of times w ap pears in x , be it a sentence -LRB- s -RRB- or the query -LRB- q -RRB- . | Text : documents | Annotation Set : [{'name': 'Document', 'spans': [{'start': 63, 'end': 64, 'text': 'documents'}]}]","In defining the relevance to the query , in Equa tion -LRB- 6 -RRB- , words which do not appear in too many sentences in the document collection weigh more . | Text : document | Annotation Set : []","To penalize such words , when computing the relevance to the query , we multiply the relevance score of a given word w with the in verted document frequency of w on the corpus of business summaries Q โ idfw , Q : We also replace tfw , s with the indicator function s -LRB- w -RRB- since it has been reported to be more ad equate for sentences , in particular for sentence alignment -LRB- Nelken & Shieber , 2006 -RRB- : Thus , the modified formula we use to compute sentence ranks is as follows : We call these two ranking algorithms that use the formula in -LRB- 2 -RRB- OTTERBACHER and QUERY WEIGHTS , the difference being the way the rel evance to the query is computed : -LRB- 6 -RRB- or -LRB- 9 -RRB- . | Text : document | Annotation Set : []",0,0,0,0,0
89,Event,221,"For consistency , only the people names whose articles occur in Wikipedia where selected as part of seed and test sets . | Text : occur | Annotation Set : [{'name': 'Time', 'spans': [{'start': 3, 'end': 7, 'text': 'only the people names'}]}, {'name': 'Event', 'spans': [{'start': 8, 'end': 9, 'text': 'articles'}]}, {'name': 'Place', 'spans': [{'start': 10, 'end': 21, 'text': 'in Wikipedia where selected as part of seed and test sets'}]}]","Deathdate also tends to occur near the beginning of the article , but almost always some point after the birthdate . | Text : occur | Annotation Set : [{'name': 'Event', 'spans': [{'start': 0, 'end': 1, 'text': 'Deathdate'}]}, {'name': 'Place', 'spans': [{'start': 5, 'end': 11, 'text': 'near the beginning of the article'}]}]","This motivates a second , sequence-based position model based on the rank of the attribute values among other values in the domain of the attribute , as follows : where P -LRB- rankv|A -RRB- is the fraction of biographies having attribute a with the correct value occuring at rank rankv , where rank is measured according to the relative order in which the values belonging to the attribute domain occur from the beginning of the article . | Text : occuring | Annotation Set : [{'name': 'Event', 'spans': [{'start': 43, 'end': 46, 'text': 'the correct value'}]}]","This motivates a second , sequence-based position model based on the rank of the attribute values among other values in the domain of the attribute , as follows : where P -LRB- rankv|A -RRB- is the fraction of biographies having attribute a with the correct value occuring at rank rankv , where rank is measured according to the relative order in which the values belonging to the attribute domain occur from the beginning of the article . | Text : occur | Annotation Set : [{'name': 'Event', 'spans': [{'start': 62, 'end': 69, 'text': 'the values belonging to the attribute domain'}]}, {'name': 'Place', 'spans': [{'start': 70, 'end': 76, 'text': 'from the beginning of the article'}]}]","For example , the distribution of words occuring in a biography of a politician would be different from that of a scientist . | Text : occuring | Annotation Set : [{'name': 'Event', 'spans': [{'start': 8, 'end': 14, 'text': 'in a biography of a politician'}]}]",0,0,0,0,0
90,Sensation,220,< newSection > 1 Introduction An automatic speech recognition -LRB- ASR -RRB- system consists of acoustic models of speech sounds and of a statistical language model -LRB- LM -RRB- . | Text : sounds | Annotation Set : [],The query selection scheme proposed in this paper is economical in the sense that it strives to download as much relevant text from the web as possible using as few queries as possible avoiding overlap between the set of pages found by different queries . | Text : sense | Annotation Set : [],"The results are consistent in the sense that the web mixture models outperform the in-domain models , and augmentation helps more with larger models . | Text : sense | Annotation Set : []","This is to be expected as they are rules which hold in specific contexts , but difficult to be captured by a sense distinction of the lexical items involved . | Text : sense | Annotation Set : []","Of course at this moment due to the lack of sense disambiguation , our method introduces lots of rules that are not correct . | Text : sense | Annotation Set : []",0,0,0,0,0
91,Reliance,219,"The whole prototype relies on one hand on lexical resources -LRB- two monolingual and one bilingual -RRB- and on a set of bilingual Lexeme Formation Rules -LRB- LFR -RRB- . | Text : relies | Annotation Set : [{'name': 'Means', 'spans': [{'start': 4, 'end': 7, 'text': 'on one hand'}]}, {'name': 'Protagonist', 'spans': [{'start': 0, 'end': 3, 'text': 'The whole prototype'}]}]","Consequently , the whole system relies on the quality of both the lexical resources and the LFR . | Text : relies | Annotation Set : [{'name': 'Degree', 'spans': [{'start': 0, 'end': 1, 'text': 'Consequently'}]}]","Our MT prototype relies on lexical resources : it aims at dealing with unknown words that are not in a Reference lexicon and these unknown words are analyzed with lexical material that is in this lexicon . | Text : relies | Annotation Set : [{'name': 'Protagonist', 'spans': [{'start': 0, 'end': 3, 'text': 'Our MT prototype'}]}]","Together with the word glosses and other resources , the discourse level clues helped to guide users to make better lexical choices than when they made corrections without the full system , relying on sentence coherence alone . | Text : relying | Annotation Set : []",One is domain dependency . | Text : dependency | Annotation Set : [],0,0,0,0,0
92,Working_on,217,This may indicate that the set of stop words may be very genre specific โ a hypothesis we will test in future work . | Text : work | Annotation Set : [],We will test this in future work . | Text : work | Annotation Set : [],The integration of the cluster-based WSD method into a real SMT system and the evaluation of its impact on translation quality constitute the main perspectives of the work presented in this article and the object of future work . | Text : work | Annotation Set : [],The integration of the cluster-based WSD method into a real SMT system and the evaluation of its impact on translation quality constitute the main perspectives of the work presented in this article and the object of future work . | Text : work | Annotation Set : [],"Although the bulk of previous work has been devoted to the disambiguation problem1 , there are good reasons to believe that sense induction may be able to overcome some of the issues associated with WSD . | Text : work | Annotation Set : []",0,0,0,0,0
93,Predicament,214,"To better understand these problems and identify areas class labels , where - denotes a non-error , FL denotes a filler , E generally denotes reparanda , and RC and NC indicate rough copy and non-copy speaker errors , respectively . | Text : problems | Annotation Set : [{'name': 'Situation', 'spans': [{'start': 4, 'end': 5, 'text': 'problems'}]}]","Spreading neighborhood expansion reduces the search errors and improves BLEU score significantly but search errors remain a problem . | Text : problem | Annotation Set : [{'name': 'Situation', 'spans': [{'start': 13, 'end': 15, 'text': 'search errors'}]}]","< newSection > 1 Introduction In dialogue , the basic problem of interpretation is to identify the contribution a speaker is making to the conversation . | Text : problem | Annotation Set : [{'name': 'Situation', 'spans': [{'start': 10, 'end': 11, 'text': 'problem'}]}]","In principle , the problem of pragmatic interpretation is qualitatively no different from the many problems that have been tackled successfully by data-driven models in NLP . | Text : problem | Annotation Set : [{'name': 'Situation', 'spans': [{'start': 4, 'end': 5, 'text': 'problem'}]}]","In principle , the problem of pragmatic interpretation is qualitatively no different from the many problems that have been tackled successfully by data-driven models in NLP . | Text : problems | Annotation Set : [{'name': 'Cause', 'spans': [{'start': 16, 'end': 26, 'text': 'that have been tackled successfully by data-driven models in NLP'}]}, {'name': 'Situation', 'spans': [{'start': 15, 'end': 16, 'text': 'problems'}]}]",0,0,0,0,0
94,Grant_permission,212,"But this simplicity will allow us to try and experiment different ideas for making a better use of the sentence structures in the alignment process . | Text : allow | Annotation Set : [{'name': 'Grantee', 'spans': [{'start': 5, 'end': 6, 'text': 'us'}]}, {'name': 'Grantor', 'spans': [{'start': 2, 'end': 3, 'text': 'simplicity'}]}]","The three most common types of graphical models -LRB- Factor Graphs , Bayesian Network and Markov Network -RRB- share the same purpose : intuitively , they allow to represent the dependencies among random variables ; mathematically , they represent a factorization of the joint probability of these variables . | Text : allow | Annotation Set : [{'name': 'Grantor', 'spans': [{'start': 25, 'end': 26, 'text': 'they'}]}, {'name': 'Action', 'spans': [{'start': 27, 'end': 34, 'text': 'to represent the dependencies among random variables'}]}]","As a short example , let us consider a problem classically used to introduce Bayesian Network . | Text : let | Annotation Set : [{'name': 'Grantee', 'spans': [{'start': 6, 'end': 7, 'text': 'us'}]}, {'name': 'Grantor', 'spans': [{'start': 0, 'end': 4, 'text': 'As a short example'}]}, {'name': 'Action', 'spans': [{'start': 7, 'end': 16, 'text': 'consider a problem classically used to introduce Bayesian Network'}]}]","Considering only one-to-one alignments can be seen as a limitation compared to others models that can often produce at least one-to-many alignments , but on the good side , this allow the monolink model to be nicely symmetric . | Text : allow | Annotation Set : [{'name': 'Grantor', 'spans': [{'start': 29, 'end': 30, 'text': 'this'}]}, {'name': 'Action', 'spans': [{'start': 31, 'end': 34, 'text': 'the monolink model'}]}]","Additionally , as already argued in -LRB- Melamed , 2000 -RRB- , there are ways to determine the boundaries of some multi-words phrases -LRB- Melamed , 2002 -RRB- , allowing to treat several words as a single token . | Text : allowing | Annotation Set : [{'name': 'Grantor', 'spans': [{'start': 14, 'end': 15, 'text': 'ways'}]}, {'name': 'Action', 'spans': [{'start': 30, 'end': 38, 'text': 'to treat several words as a single token'}]}]",0,0,0,0,0
95,Supply,211,"However , this results in a very skewed class distribution because the next speaker is the intended addressee 41 % of the time , and 38 % of instances are plural - the 3Addressee annotations are not provided for some dialogue act types - see -LRB- Jovanovic et al. , 2006b -RRB- . | Text : provided | Annotation Set : [{'name': 'Supplier', 'spans': [{'start': 32, 'end': 35, 'text': 'the 3Addressee annotations'}]}, {'name': 'Theme', 'spans': [{'start': 38, 'end': 43, 'text': 'for some dialogue act types'}]}]","< newSection > 4 Visual Information We derived per-utterance visual features from the Focus Of Attention -LRB- FOA -RRB- annotations provided by the AMI corpus . | Text : provided | Annotation Set : [{'name': 'Supplier', 'spans': [{'start': 21, 'end': 25, 'text': 'by the AMI corpus'}]}, {'name': 'Theme', 'spans': [{'start': 19, 'end': 20, 'text': 'annotations'}]}]","Dialogue act features provide useful information as well . | Text : provide | Annotation Set : [{'name': 'Supplier', 'spans': [{'start': 0, 'end': 3, 'text': 'Dialogue act features'}]}, {'name': 'Theme', 'spans': [{'start': 4, 'end': 6, 'text': 'useful information'}]}]","Advantages of the SSR data include As reconstructions are sometimes nondeterministic -LRB- illustrated in EX6 in Section 1.1 -RRB- , the SSR provides two manual reconstructions for each utterance in the data . | Text : provides | Annotation Set : [{'name': 'Supplier', 'spans': [{'start': 20, 'end': 22, 'text': 'the SSR'}]}, {'name': 'Theme', 'spans': [{'start': 23, 'end': 26, 'text': 'two manual reconstructions'}]}]","However , to take advantage of the double reconstruction annotations provided in SSR -LRB- and more importantly , in recognition of the occasional ambiguities of reconstruction -RRB- we modified these calculations slightly as shown below . | Text : provided | Annotation Set : [{'name': 'Theme', 'spans': [{'start': 6, 'end': 10, 'text': 'the double reconstruction annotations'}]}]",0,0,0,0,0
96,People_by_origin,211,"German parse quality is considered to be worse than English parse quality , and the annotation style is different , e.g. , NP structure in German is flatter . | Text : English | Annotation Set : [{'name': 'Person', 'spans': [{'start': 9, 'end': 10, 'text': 'English'}]}]","German parse quality is considered to be worse than English parse quality , and the annotation style is different , e.g. , NP structure in German is flatter . | Text : German | Annotation Set : [{'name': 'Person', 'spans': [{'start': 25, 'end': 26, 'text': 'German'}]}]","We conduct our research in the framework of N-best parse reranking , but apply it to bitext and add only features based on syntactic projection from German to English . | Text : English | Annotation Set : [{'name': 'Person', 'spans': [{'start': 28, 'end': 29, 'text': 'English'}]}]","The system takes as input -LRB- i -RRB- English sentences with a list of automatically generated syntactic parses , -LRB- ii -RRB- a translation of the English sentences into German , -LRB- iii -RRB- an automatically generated parse of the German translation , and -LRB- iv -RRB- an automatically generated word alignment . | Text : English | Annotation Set : [{'name': 'Person', 'spans': [{'start': 8, 'end': 9, 'text': 'English'}]}]","The system takes as input -LRB- i -RRB- English sentences with a list of automatically generated syntactic parses , -LRB- ii -RRB- a translation of the English sentences into German , -LRB- iii -RRB- an automatically generated parse of the German translation , and -LRB- iv -RRB- an automatically generated word alignment . | Text : German | Annotation Set : [{'name': 'Person', 'spans': [{'start': 29, 'end': 30, 'text': 'German'}]}]",0,0,0,0,0
97,Participation,208,It involves a very complex distortion model -LRB- here and in subsequent usages `` dis-tortion '' will be a generic term for the reordering of the words occurring in the translation process -RRB- with many parameters that make it very complex to train . | Text : involves | Annotation Set : [],"The paper presents four experiments that aim at improving parsing performance of coordinate structures : the first experiment involves reranking of n-best parses produced by a PCFG parser , the second experiment enriches the input to a PCFG parser by offering gold pre-bracketings for any coordinate structures that occur in the sentence . | Text : involves | Annotation Set : []","More specifically , it is the observation that coordination involves two or more constituents of the same categories . | Text : involves | Annotation Set : []","For example , due to constituent fronting to clause-initial position in German verb-second main clauses , cases of nonconstituent conjunction can involve any two NPs -LRB- including the subject -RRB- of a ditransitive verb to the exclusion of the third NP complement that appears in clause-initial position . | Text : involve | Annotation Set : [{'name': 'Participant_1', 'spans': [{'start': 3, 'end': 20, 'text': 'due to constituent fronting to clause-initial position in German verb-second main clauses , cases of nonconstituent conjunction'}]}, {'name': 'Event', 'spans': [{'start': 22, 'end': 47, 'text': 'any two NPs -LRB- including the subject -RRB- of a ditransitive verb to the exclusion of the third NP complement that appears in clause-initial position'}]}]","Such cases of subject gap coordination are frequently found in text corpora -LRB- cf. -LRB- 4 -RRB- below -RRB- and involve conjunction of a full verb-second clause with a VP whose subject is identical to the subject in the first conjunct . | Text : involve | Annotation Set : [{'name': 'Event', 'spans': [{'start': 21, 'end': 41, 'text': 'conjunction of a full verb-second clause with a VP whose subject is identical to the subject in the first conjunct'}]}]",0,0,0,0,0
98,Roadways,207,"The second term represents , loosely speaking , the probability of a surfer randomly jumping to any node , e.g. without following any paths on the graph . | Text : paths | Annotation Set : [{'name': 'Roadway', 'spans': [{'start': 23, 'end': 24, 'text': 'paths'}]}]","Such a subgraph is called a `` disambigua-tion subgraph '' GD , and it is built in the following way . | Text : way | Annotation Set : [{'name': 'Roadway', 'spans': [{'start': 19, 'end': 20, 'text': 'way'}]}]","Each run of the BFS calculates the minimum distance paths between vi and the rest of concepts of GKB . In particular , we are interested in the minimum distance paths between vi and the concepts associated to the rest of the words in the context , vj E Uj= , 4i Conceptsj . | Text : paths | Annotation Set : [{'name': 'Roadway', 'spans': [{'start': 9, 'end': 10, 'text': 'paths'}]}]","Each run of the BFS calculates the minimum distance paths between vi and the rest of concepts of GKB . In particular , we are interested in the minimum distance paths between vi and the concepts associated to the rest of the words in the context , vj E Uj= , 4i Conceptsj . | Text : paths | Annotation Set : [{'name': 'Roadway', 'spans': [{'start': 30, 'end': 31, 'text': 'paths'}]}]","Let mdpvi be the set of these shortest paths . | Text : paths | Annotation Set : [{'name': 'Roadway', 'spans': [{'start': 8, 'end': 9, 'text': 'paths'}]}]",0,0,0,0,0
99,Expectation,193,"In order to determine the features used for predicting which sentences are the sources for story highlights , we gathered statistics from 1 , 200 CNN newswire articles . | Text : predicting | Annotation Set : [{'name': 'Phenomenon', 'spans': [{'start': 9, 'end': 17, 'text': 'which sentences are the sources for story highlights'}]}]","Over the entire set , such phrases become significant . | Text : entire | Annotation Set : [{'name': 'Entity', 'spans': [{'start': 3, 'end': 4, 'text': 'set'}]}]","AURUM has two types of features : sentence features , such as the position of the sentence or the existence of a negation word , receive the same value for the entire sentence . | Text : entire | Annotation Set : [{'name': 'Entity', 'spans': [{'start': 32, 'end': 33, 'text': 'sentence'}]}]","As to be expected , AURUM-fixed achieves higher precision gains , while AURUM-thresh achieves higher recall gains . | Text : expected | Annotation Set : []","The belief bi -LRB- x -RRB- is expected to converge to the marginal probability -LRB- or an approximation of it -RRB- of Vi taking the value x . An interesting point to note is that each message can be `` scaled '' -LRB- that is , multiplied by a constant -RRB- by any factor at any point without changing the result of the algorithm . | Text : expected | Annotation Set : [{'name': 'Phenomenon', 'spans': [{'start': 0, 'end': 3, 'text': 'The belief bi'}]}]",0,0,0,0,0
