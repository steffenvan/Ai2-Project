{
    "output": [
        {
            "sentence": "<newSection> Abstract In this paper we propose a new graph-based method that uses the knowledge in a LKB -LRB- based on WordNet -RRB- in order to perform unsupervised Word Sense Disambiguation . ",
            "negation": [
                {
                    "cue": "un",
                    "scope": "perform supervised Word Sense Disambiguation "
                }
            ]
        },
        {
            "sentence": "Our algorithm uses the full graph of the LKB efficiently , performing better than previous approaches in English all-words datasets . ",
            "negation": []
        },
        {
            "sentence": "We also show that the algorithm can be easily ported to other languages with good results , with the only requirement of having a wordnet . ",
            "negation": []
        },
        {
            "sentence": "In addition , we make an analysis of the performance of the algorithm , showing that it is efficient and that it could be tuned to be faster . ",
            "negation": []
        },
        {
            "sentence": "<newSection> 1 Introduction Word Sense Disambiguation -LRB- WSD -RRB- is a key enabling-technology that automatically chooses the intended sense of a word in context . ",
            "negation": []
        },
        {
            "sentence": "Supervised WSD systems are the best performing in public evaluations -LRB- Palmer et al. , 2001 ; Snyder and Palmer , 2004 ; Pradhan et al. , 2007 -RRB- but they need large amounts of hand-tagged data , which is typically very expensive to build . ",
            "negation": []
        },
        {
            "sentence": "Given the relatively small amount of training data available , current state-of-the-art systems only beat the simple most frequent sense -LRB- MFS -RRB- baseline1 by a small margin . ",
            "negation": []
        },
        {
            "sentence": "As an alternative to supervised systems , knowledge-based WSD systems exploit the information present in a lexical knowledge base -LRB- LKB -RRB- to perform WSD , without using any further corpus evidence . ",
            "negation": [
                {
                    "cue": "without",
                    "scope": "using any further corpus evidence "
                }
            ]
        },
        {
            "sentence": "Traditional knowledge-based WSD systems assign a sense to an ambiguous word by comparing each of its senses with those of the surrounding context . ",
            "negation": []
        },
        {
            "sentence": "Typically , some semantic similarity metric is used for calculating the relatedness among senses -LRB- Lesk , 1986 ; McCarthy et al. , 2004 -RRB- . ",
            "negation": []
        },
        {
            "sentence": "One of the major drawbacks of these approaches stems from the fact that senses are compared in a pairwise fashion and thus the number of computations can grow exponentially with the number of words . ",
            "negation": []
        },
        {
            "sentence": "Although alternatives like simulated annealing -LRB- Cowie et al. , 1992 -RRB- and conceptual density -LRB- Agirre and Rigau , 1996 -RRB- were tried , most of past knowledge based WSD was done in a suboptimal word-by-word process , i.e. , disambiguating words one at a time . ",
            "negation": []
        },
        {
            "sentence": "Recently , graph-based methods for knowledge-based WSD have gained much attention in the NLP community -LRB- Sinha and Mihalcea , 2007 ; Nav-igli and Lapata , 2007 ; Mihalcea , 2005 ; Agirre and Soroa , 2008 -RRB- . ",
            "negation": []
        },
        {
            "sentence": "These methods use well-known graph-based techniques to find and exploit the structural properties of the graph underlying a particular LKB . ",
            "negation": []
        },
        {
            "sentence": "Because the graph is analyzed as a whole , these techniques have the remarkable property of being able to find globally optimal solutions , given the relations between entities . ",
            "negation": []
        },
        {
            "sentence": "Graph-based WSD methods are particularly suited for disambiguating word sequences , and they manage to exploit the interrelations among the senses in the given context . ",
            "negation": []
        },
        {
            "sentence": "In this sense , they provide a principled solution to the exponential explosion problem , with excellent performance . ",
            "negation": []
        },
        {
            "sentence": "Graph-based WSD is performed over a graph composed by senses -LRB- nodes -RRB- and relations between pairs of senses -LRB- edges -RRB- . ",
            "negation": []
        },
        {
            "sentence": "The relations may be of several types -LRB- lexico-semantic , coocurrence relations , etc. -RRB- and may have some weight attached to them . ",
            "negation": []
        },
        {
            "sentence": "The disambiguation is typically performed by applying a ranking algorithm over the graph , and then assigning the concepts with highest rank to the corresponding words . ",
            "negation": []
        },
        {
            "sentence": "Given the computational cost of using large graphs like WordNet , many researchers use smaller subgraphs built online for each target context . ",
            "negation": []
        },
        {
            "sentence": "In this paper we present a novel graph-based WSD algorithm which uses the full graph of WordNet efficiently , performing significantly better that previously published approaches in English all-words datasets . ",
            "negation": []
        },
        {
            "sentence": "We also show that the algorithm can be easily ported to other languages with good results , with the only requirement of having a wordnet . ",
            "negation": []
        },
        {
            "sentence": "The algorithm is publicly available2 and can be applied easily to sense inventories and knowledge bases different from WordNet . ",
            "negation": []
        },
        {
            "sentence": "Our analysis shows that our algorithm is efficient compared to previously proposed alternatives , and that a good choice of WordNet versions and relations is fundamental for good performance . ",
            "negation": []
        },
        {
            "sentence": "The paper is structured as follows . ",
            "negation": []
        },
        {
            "sentence": "We first describe the PageRank and Personalized PageRank algorithms . ",
            "negation": []
        },
        {
            "sentence": "Section 3 introduces the graph based methods used for WSD . ",
            "negation": []
        },
        {
            "sentence": "Section 4 shows the experimental setting and the main results , and Section 5 compares our methods with related experiments on graph-based WSD systems . ",
            "negation": []
        },
        {
            "sentence": "Section 6 shows the results of the method when applied to a Spanish dataset . ",
            "negation": []
        },
        {
            "sentence": "Section 7 analyzes the performance of the algorithm . ",
            "negation": []
        },
        {
            "sentence": "Finally , we draw some conclusions in Section 8 . ",
            "negation": []
        },
        {
            "sentence": "<newSection> 2 PageRank and Personalized PageRank The celebrated PageRank algorithm -LRB- Brin and Page , 1998 -RRB- is a method for ranking the vertices in a graph according to their relative structural importance . ",
            "negation": []
        },
        {
            "sentence": "The main idea of PageRank is that whenever a link from vi to vj exists in a graph , a vote from node i to node j is produced , and hence the rank of node j increases . ",
            "negation": []
        },
        {
            "sentence": "Besides , the strength of the vote from i to j also depends on the rank of node i : the more important node i is , the more strength its votes will have . ",
            "negation": []
        },
        {
            "sentence": "Alternatively , PageRank can also be viewed as the result of a random walk process , where the final rank of node i represents the probability of a random walk over the graph ending on node i , at a sufficiently large time . ",
            "negation": []
        },
        {
            "sentence": "Let G be a graph with N vertices vi , ... , vN and di be the outdegree of node i ; let M be a In the equation , v is a N \u00d7 1 vector whose elements are 1N and c is the so called damping factor , a scalar value between 0 and 1 . ",
            "negation": []
        },
        {
            "sentence": "The first term of the sum on the equation models the voting scheme described in the beginning of the section . ",
            "negation": []
        },
        {
            "sentence": "The second term represents , loosely speaking , the probability of a surfer randomly jumping to any node , e.g. without following any paths on the graph . ",
            "negation": [
                {
                    "cue": "without",
                    "scope": "following any paths on the graph "
                }
            ]
        },
        {
            "sentence": "The damping factor , usually set in the -LSB- 0.85 . ",
            "negation": []
        },
        {
            "sentence": ".0.95 -RSB- range , models the way in which these two terms are combined at each step . ",
            "negation": []
        },
        {
            "sentence": "The second term on Eq . ",
            "negation": []
        },
        {
            "sentence": "-LRB- 1 -RRB- can also be seen as a smoothing factor that makes any graph fulfill the property of being aperiodic and irreducible , and thus guarantees that PageRank calculation converges to a unique stationary distribution . ",
            "negation": [
                {
                    "cue": "ir",
                    "scope": "reducible "
                }
            ]
        },
        {
            "sentence": "In the traditional PageRank formulation the vector v is a stochastic normalized vector whose element values are all 1N , thus assigning equal probabilities to all nodes in the graph in case of random jumps . ",
            "negation": []
        },
        {
            "sentence": "However , as pointed out by -LRB- Haveliwala , 2002 -RRB- , the vector v can be non-uniform and assign stronger probabilities to certain kinds of nodes , effectively biasing the resulting PageRank vector to prefer these nodes . ",
            "negation": []
        },
        {
            "sentence": "For example , if we concentrate all the probability mass on a unique node i , all random jumps on the walk will return to i and thus its rank will be high ; moreover , the high rank of i will make all the nodes in its vicinity also receive a high rank . ",
            "negation": []
        },
        {
            "sentence": "Thus , the importance of node i given by the initial distribution of v spreads along the graph on successive iterations of the algorithm . ",
            "negation": []
        },
        {
            "sentence": "In this paper , we will use traditional PageRank to refer to the case when a uniform v vector is used in Eq . ",
            "negation": []
        },
        {
            "sentence": "-LRB- 1 -RRB- ; and whenever a modified v is used , we will call it Personalized PageRank . ",
            "negation": []
        },
        {
            "sentence": "The next section shows how we define a modified v. PageRank is actually calculated by applying an iterative algorithm which computes Eq . ",
            "negation": []
        },
        {
            "sentence": "-LRB- 1 -RRB- successively until convergence below a given threshold is achieved , or , more typically , until a fixed number of iterations are executed . ",
            "negation": []
        },
        {
            "sentence": "Regarding PageRank implementation details , we chose a damping value of 0.85 and finish the calculation after 30 iterations . ",
            "negation": []
        },
        {
            "sentence": "We did not try other damping factors . ",
            "negation": [
                {
                    "cue": "not",
                    "scope": "We did try other damping factors "
                }
            ]
        },
        {
            "sentence": "Some preliminary experiments with higher iteration counts showed that although sometimes the node ranks varied , the relative order among particular word synsets remained stable after the initial iterations -LRB- cf. Section 7 for further details -RRB- . ",
            "negation": []
        },
        {
            "sentence": "Note that , in order to discard the effect of dangling nodes -LRB- i.e. nodes without outlinks -RRB- we slightly modified Eq . ",
            "negation": [
                {
                    "cue": "without",
                    "scope": "outlinks "
                }
            ]
        },
        {
            "sentence": "-LRB- 1 -RRB- . ",
            "negation": []
        },
        {
            "sentence": "For the sake of brevity we omit the details , which the interested reader can check in -LRB- Langville and Meyer , 2003 -RRB- . ",
            "negation": []
        },
        {
            "sentence": "<newSection> 3 Using PageRank for WSD In this section we present the application of PageRank to WSD . ",
            "negation": []
        },
        {
            "sentence": "If we were to apply the traditional PageRank over the whole WordNet we would get a context-independent ranking of word senses , which is not what we want . ",
            "negation": [
                {
                    "cue": "not",
                    "scope": "which is what we want "
                }
            ]
        },
        {
            "sentence": "Given an input piece of text -LRB- typically one sentence , or a small set of contiguous sentences -RRB- , we want to disambiguate all open-class words in the input taken the rest as context . ",
            "negation": []
        },
        {
            "sentence": "In this framework , we need to rank the senses of the target words according to the other words in the context . ",
            "negation": []
        },
        {
            "sentence": "Theare two main alternatives to achieve this : The first method has been explored in the literature -LRB- cf. Section 5 -RRB- , and we also presented a variant in -LRB- Agirre and Soroa , 2008 -RRB- but the second method is novel in WSD . ",
            "negation": []
        },
        {
            "sentence": "In both cases , the algorithms return a list of ranked senses for each target word in the context . ",
            "negation": []
        },
        {
            "sentence": "We will see each of them in turn , but first we will present some notation and a preliminary step . ",
            "negation": []
        },
        {
            "sentence": "A LKB is formed by a set of concepts and relations among them , and a dictionary , i.e. , a list of words -LRB- typically , word lemmas -RRB- each of them linked to at least one concept of the LKB . ",
            "negation": []
        },
        {
            "sentence": "Given any such LKB , we build an undirected graph G = -LRB- V , E -RRB- where nodes represent LKB concepts -LRB- vi -RRB- , and each relation between concepts vi and vj is represented by an undirected edge ei , j . ",
            "negation": [
                {
                    "cue": "un",
                    "scope": "an directed graph G = -LRB- V "
                },
                {
                    "cue": "un",
                    "scope": "an directed edge ei "
                }
            ]
        },
        {
            "sentence": "In our experiments we have tried our algorithms using three different LKBs : Given an input text , we extract the list Wi i = 1 ... m of content words -LRB- i.e. nouns , verbs , adjectives and adverbs -RRB- which have an entry in the dictionary , and thus can be related to LKB concepts . ",
            "negation": []
        },
        {
            "sentence": "Let Conceptsi = -LCB- v1 , ... , vi . -RCB- ",
            "negation": []
        },
        {
            "sentence": "be the im associated concepts of word Wi in the LKB graph . ",
            "negation": []
        },
        {
            "sentence": "Note that monosemous words will be related to just one concept , whereas polysemous words may be attached to several . ",
            "negation": []
        },
        {
            "sentence": "As a result of the disambiguation process , every concept in Conceptsi , i = 1 , ... , m receives a score . ",
            "negation": []
        },
        {
            "sentence": "Then , for each target word to be disambiguated , we just choose its associated concept in G with maximal score . ",
            "negation": []
        },
        {
            "sentence": "In our experiments we build a context of at least 20 content words for each sentence to be disam-biguated , taking the sentences immediately before and after it in the case that the original sentence was too short . ",
            "negation": []
        },
        {
            "sentence": "We follow the algorithm presented in -LRB- Agirre and Soroa , 2008 -RRB- , which we explain here for completeness . ",
            "negation": []
        },
        {
            "sentence": "The main idea of the subgraph method is to extract the subgraph of GKB whose vertices and relations are particularly relevant for a given input context . ",
            "negation": []
        },
        {
            "sentence": "Such a subgraph is called a `` disambigua-tion subgraph '' GD , and it is built in the following way . ",
            "negation": [
                {
                    "cue": "dis",
                    "scope": "ambigua-tion "
                }
            ]
        },
        {
            "sentence": "For each word Wi in the input context and each concept vi E Conceptsi , a standard breath-first search -LRB- BFS -RRB- over GKB is performed , starting at node vi . ",
            "negation": []
        },
        {
            "sentence": "Each run of the BFS calculates the minimum distance paths between vi and the rest of concepts of GKB . ",
            "negation": []
        },
        {
            "sentence": "In particular , we are interested in the minimum distance paths between vi and the concepts associated to the rest of the words in the context , vj E Uj = ,4 i Conceptsj . ",
            "negation": []
        },
        {
            "sentence": "Let mdpvi be the set of these shortest paths . ",
            "negation": []
        },
        {
            "sentence": "This BFS computation is repeated for every concept of every word in the input context , storing mdpvi accordingly . ",
            "negation": []
        },
        {
            "sentence": "At the end , we obtain a set of minimum length paths each of them having a different concept as a source . ",
            "negation": []
        },
        {
            "sentence": "The disambiguation graph GD is then just the union of the vertices and edges of the shortest paths , GD = Umi = 1 -LCB- mdpv , / vj E Conceptsi -RCB- . ",
            "negation": [
                {
                    "cue": "un",
                    "scope": "the ion "
                }
            ]
        },
        {
            "sentence": "The disambiguation graph GD is thus a sub-graph of the original GKB graph obtained by computing the shortest paths between the concepts of the words co-occurring in the context . ",
            "negation": []
        },
        {
            "sentence": "Thus , we hypothesize that it captures the most relevant concepts and relations in the knowledge base for the particular input context . ",
            "negation": []
        },
        {
            "sentence": "Once the GD graph is built , we compute the traditional PageRank algorithm over it . ",
            "negation": []
        },
        {
            "sentence": "The intuition behind this step is that the vertices representing the correct concepts will be more relevant in GD than the rest of the possible concepts of the context words , which should have less relations on average and be more isolated . ",
            "negation": []
        },
        {
            "sentence": "As usual , the disambiguation step is performed by assigning to each word Wi the associated concept in Conceptsi which has maximum rank . ",
            "negation": []
        },
        {
            "sentence": "In case of ties we assign all the concepts with maximum rank . ",
            "negation": []
        },
        {
            "sentence": "Note that the standard evaluation script provided in the Senseval competitions treats multiple senses as if one was chosen at random , i.e. for evaluation purposes our method is equivalent to breaking ties at random . ",
            "negation": []
        },
        {
            "sentence": "As mentioned before , personalized PageRank allows us to use the full LKB . ",
            "negation": []
        },
        {
            "sentence": "We first insert the context words into the graph G as nodes , and link them with directed edges to their respective concepts . ",
            "negation": []
        },
        {
            "sentence": "Then , we compute the personalized PageRank of the graph G by concentrating the initial probability mass uniformly over the newly introduced word nodes . ",
            "negation": [
                {
                    "cue": "un",
                    "scope": "mass iformly over the newly introduced word nodes "
                }
            ]
        },
        {
            "sentence": "As the words are linked to the concepts by directed edges , they act as source nodes injecting mass into the concepts they are associated with , which thus become relevant nodes , and spread their mass over the LKB graph . ",
            "negation": []
        },
        {
            "sentence": "Therefore , the resulting personalized PageRank vector can be seen as a measure of the structural relevance of LKB concepts in the presence of the input context . ",
            "negation": []
        },
        {
            "sentence": "One problem with Personalized PageRank is that if one of the target words has two senses which are related by semantic relations , those senses reinforce each other , and could thus dampen the effect of the other senses in the context . ",
            "negation": []
        },
        {
            "sentence": "With this observation in mind we devised a variant -LRB- dubbed Ppr w2w -RRB- , where we build the graph for each target word in the context : for each target word Wi , we concentrate the initial probability mass in the senses of the words surrounding Wi , but not in the senses of the target word itself , so that context words increase its relative importance in the graph . ",
            "negation": [
                {
                    "cue": "not",
                    "scope": "in the senses of the target word itself "
                }
            ]
        },
        {
            "sentence": "The main idea of this approach is to avoid biasing the initial score of concepts associated to target word Wi , and let the surrounding words decide which concept associated to Wi has more relevance . ",
            "negation": []
        },
        {
            "sentence": "Contrary to the other two approaches , Ppr w2w does not disambiguate all target words of the context in a single run , which makes it less efficient -LRB- cf. Section 7 -RRB- . ",
            "negation": [
                {
                    "cue": "not",
                    "scope": "w2w does disambiguate all target words of the context in a single run "
                }
            ]
        },
        {
            "sentence": "<newSection> 4 Evaluation framework and results In this paper we will use two datasets for comparing graph-based WSD methods , namely , the Senseval-2 -LRB- S2AW -RRB- and Senseval-3 -LRB- S3AW -RRB- all words datasets -LRB- Snyder and Palmer , 2004 ; Palmer et al. , 2001 -RRB- , which are both labeled with WordNet 1.7 tags . ",
            "negation": []
        },
        {
            "sentence": "We did not use the Semeval dataset , for the sake of comparing our results to related work , none of which used Semeval data . ",
            "negation": [
                {
                    "cue": "not",
                    "scope": "We did use the Semeval dataset , for the sake of comparing our results to related work "
                },
                {
                    "cue": "none",
                    "scope": "of which used Semeval data "
                }
            ]
        },
        {
            "sentence": "Table 1 shows the results as recall of the graph-based WSD system over these datasets on the different LKBs . ",
            "negation": []
        },
        {
            "sentence": "We detail overall results , as well as results per PoS , and the confidence interval for the overall results . ",
            "negation": []
        },
        {
            "sentence": "The interval was computed using bootstrap resam-pling with 95 % confidence . ",
            "negation": []
        },
        {
            "sentence": "The table shows that Ppr w2w is consistently the best method in both datasets and for all LKBs . ",
            "negation": []
        },
        {
            "sentence": "Ppr and Spr obtain comparable results , which is remarkable , given the simplicity of the Ppr algo-baseline and the best results of supervised systems at competition time -LRB- SMUaw , GAMBL -RRB- . ",
            "negation": []
        },
        {
            "sentence": "rithm , compared to the more elaborate algorithm to construct the graph . ",
            "negation": []
        },
        {
            "sentence": "The differences between methods are not statistically significant , which is a common problem on this relatively small datasets -LRB- Snyder and Palmer , 2004 ; Palmer et al. , 2001 -RRB- . ",
            "negation": [
                {
                    "cue": "not",
                    "scope": "The differences between methods are statistically significant which is a common problem on this relatively small datasets -LRB- Snyder "
                }
            ]
        },
        {
            "sentence": "Regarding LKBs , the best results are obtained using WordNet 1.7 and eXtended WordNet . ",
            "negation": []
        },
        {
            "sentence": "Here the differences are in many cases significant . ",
            "negation": []
        },
        {
            "sentence": "These results are surprising , as we would expect that the manually disambiguated gloss relations from WordNet 3.0 would lead to better results , compared to the automatically disam-biguated gloss relations from the eXtended WordNet -LRB- linked to version 1.7 -RRB- . ",
            "negation": []
        },
        {
            "sentence": "The lower performance of WNet30 + gloss can be due to the fact that the Senseval all words data set is tagged using WordNet 1.7 synsets . ",
            "negation": []
        },
        {
            "sentence": "When using a different LKB for WSD , a mapping to WordNet 1.7 is required . ",
            "negation": []
        },
        {
            "sentence": "Although the mapping is cited as having a correctness on the high 90s -LRB- Daude et al. , 2000 -RRB- , it could have introduced sufficient noise to counteract the benefits of the hand-disambiguated glosses . ",
            "negation": []
        },
        {
            "sentence": "Table 1 also shows the most frequent sense -LRB- MFS -RRB- , as well as the best supervised systems -LRB- Snyder and Palmer , 2004 ; Palmer et al. , 2001 -RRB- that participated in each competition -LRB- SMUaw and GAMBL , respectively -RRB- . ",
            "negation": []
        },
        {
            "sentence": "The MFS is a baseline for supervised systems , but it is considered a difficult competitor for unsupervised systems , which rarely come close to it . ",
            "negation": [
                {
                    "cue": "un",
                    "scope": "for supervised systems "
                }
            ]
        },
        {
            "sentence": "In this case the MFS baseline was computed using previously availabel training data like SemCor . ",
            "negation": []
        },
        {
            "sentence": "Our best results are close to the MFS in both Senseval-2 and Senseval-3 datasets . ",
            "negation": []
        },
        {
            "sentence": "The results for the supervised system are given for reference , and we can see that the gap is relatively small , specially for Senseval-3 . ",
            "negation": []
        },
        {
            "sentence": "<newSection> 5 Comparison to Related work In this section we will briefly describe some graph-based methods for knowledge-based WSD . ",
            "negation": []
        },
        {
            "sentence": "The methods here presented cope with the problem of sequence-labeling , i.e. , they disambiguate all the words coocurring in a sequence -LRB- typically , all content words of a sentence -RRB- . ",
            "negation": []
        },
        {
            "sentence": "All the methods rely on the information represented on some LKB , which typically is some version of WordNet , sometimes enriched with proprietary relations . ",
            "negation": []
        },
        {
            "sentence": "The results on our datasets , when available , are shown in Table 2 . ",
            "negation": []
        },
        {
            "sentence": "The table also shows the performance of supervised systems . ",
            "negation": []
        },
        {
            "sentence": "The TexRank algorithm -LRB- Mihalcea , 2005 -RRB- for WSD creates a complete weighted graph -LRB- e.g. a graph where every pair of distinct vertices is connected by a weighted edge -RRB- formed by the synsets of the words in the input context . ",
            "negation": []
        },
        {
            "sentence": "The weight of the links joining two synsets is calculated by executing Lesk 's algorithm -LRB- Lesk , 1986 -RRB- between them , i.e. , by calculating the overlap between the words in the glosses of the correspongind senses . ",
            "negation": []
        },
        {
            "sentence": "Once the complete graph is built , the PageRank algorithm is executed over it and words are assigned to the most relevant synset . ",
            "negation": []
        },
        {
            "sentence": "In this sense , PageRank is used an alternative to simulated annealing to find the optimal pairwise combinations . ",
            "negation": []
        },
        {
            "sentence": "The method was evaluated on the Senseval-3 dataset , as shown in row Mih05 on Table 2 . ",
            "negation": []
        },
        {
            "sentence": "-LRB- Sinha and Mihalcea , 2007 -RRB- extends their previous work by using a collection of semantic similarity measures when assigning a weight to the links across synsets . ",
            "negation": []
        },
        {
            "sentence": "They also compare different graph-based centrality algorithms to rank the vertices of the complete graph . ",
            "negation": []
        },
        {
            "sentence": "They use different similarity metrics for different POS types and a voting scheme among the centrality algorithm ranks . ",
            "negation": []
        },
        {
            "sentence": "Here , the Senseval-3 corpus was used as a development data set , and we can thus see those results as the upper-bound of their method . ",
            "negation": []
        },
        {
            "sentence": "We can see in Table 2 that the methods presented in this paper clearly outperform both Mih05 and Sin07 . ",
            "negation": []
        },
        {
            "sentence": "This result suggests that analyzing the LKB structure as a whole is preferable than computing pairwise similarity measures over synsets . ",
            "negation": []
        },
        {
            "sentence": "The results of various in-house made experiments replicating -LRB- Mihalcea , 2005 -RRB- also confirm this observation . ",
            "negation": []
        },
        {
            "sentence": "Note also that our methods are simpler than the combination strategy used in -LRB- Sinha and Mihalcea , 2007 -RRB- , and that we did not perform any parameter tuning as they did . ",
            "negation": [
                {
                    "cue": "not",
                    "scope": "we did perform any parameter tuning as they did "
                }
            ]
        },
        {
            "sentence": "In -LRB- Navigli and Velardi , 2005 -RRB- the authors develop a knowledge-based WSD method based on lexical chains called structural semantic intercon-nections -LRB- SSI -RRB- . ",
            "negation": []
        },
        {
            "sentence": "Although the system was first designed to find the meaning of the words in WordNet glosses , the authors also apply the method for labeling text sequences . ",
            "negation": []
        },
        {
            "sentence": "Given a text sequence , SSI first identifies monosemous words and assigns the corresponding synset to them . ",
            "negation": []
        },
        {
            "sentence": "Then , it iteratively disambiguates the rest of terms by selecting the senses that get the strongest interconnec-tion with the synsets selected so far . ",
            "negation": []
        },
        {
            "sentence": "The inter-connection is calculated by searching for paths on the LKB , constrained by some hand-made rules of possible semantic patterns . ",
            "negation": []
        },
        {
            "sentence": "The method was evaluated on the Senseval-3 dataset , as shown in row Nav05 on Table 2 . ",
            "negation": []
        },
        {
            "sentence": "Note that the method labels an instance with the most frequent sense of the word if the algorithm produces no output for that instance , which makes comparison to our system unfair , specially given the fact that the MFS performs better than SSI . ",
            "negation": [
                {
                    "cue": "no",
                    "scope": "the algorithm produces output for that instance , which makes comparison to our system unfair "
                },
                {
                    "cue": "un",
                    "scope": "fair "
                }
            ]
        },
        {
            "sentence": "In fact it is not possible to separate the effect of SSI from that of the MFS . ",
            "negation": [
                {
                    "cue": "not",
                    "scope": "it is possible to separate the effect of SSI from that of the MFS "
                }
            ]
        },
        {
            "sentence": "For this reason we place this method close to the MFS baseline in Table 2 . ",
            "negation": []
        },
        {
            "sentence": "In -LRB- Navigli and Lapata , 2007 -RRB- , the authors perform a two-stage process for WSD . ",
            "negation": []
        },
        {
            "sentence": "Given an input context , the method first explores the whole LKB in order to find a subgraph which is particularly relevant for the words of the context . ",
            "negation": []
        },
        {
            "sentence": "Then , they study different graph-based centrality algorithms for deciding the relevance of the nodes on the sub-graph . ",
            "negation": []
        },
        {
            "sentence": "As a result , every word of the context is attached to the highest ranking concept among its possible senses . ",
            "negation": []
        },
        {
            "sentence": "The Spr method is very similar to -LRB- Navigli and Lapata , 2007 -RRB- , the main difference lying on the initial method for extracting the context subgraph . ",
            "negation": []
        },
        {
            "sentence": "Whereas -LRB- Navigli and Lapata , 2007 -RRB- apply a depth-first search algorithm over the LKB graph -- and restrict the depth of the subtree to a value of 3 -- , Spr relies on shortest paths between word synsets . ",
            "negation": []
        },
        {
            "sentence": "Navigli and Lapata do n't report overall results and therefore , we ca n't directly compare our results with theirs . ",
            "negation": [
                {
                    "cue": "n't",
                    "scope": "Navigli Lapata do report overall results "
                },
                {
                    "cue": "n't",
                    "scope": "we ca directly compare our results with theirs "
                }
            ]
        },
        {
            "sentence": "However , we can see that on a PoS-basis evaluation our results are consistently better for nouns and verbs -LRB- especially the Ppr w2w method -RRB- and rather similar for adjectives . ",
            "negation": []
        },
        {
            "sentence": "-LRB- Tsatsaronis et al. , 2007 -RRB- is another example of a two-stage process , the first one consisting on finding a relevant subgraph by performing a BFS dataset , including MFS and the best supervised system in the competition . ",
            "negation": []
        },
        {
            "sentence": "search over the LKB . ",
            "negation": []
        },
        {
            "sentence": "The authors apply a spreading activation algorithm over the subgraph for node ranking . ",
            "negation": []
        },
        {
            "sentence": "Edges of the subgraph are weighted according to its type , following a tf.idf like approach . ",
            "negation": []
        },
        {
            "sentence": "The results show that our methods clearly outperform Tsatsa07 . ",
            "negation": []
        },
        {
            "sentence": "The fact that the Spr method works better suggests that the traditional PageRank algorithm is a superior method for ranking the subgraph nodes . ",
            "negation": []
        },
        {
            "sentence": "As stated before , all methods presented here use some LKB for performing WSD . ",
            "negation": []
        },
        {
            "sentence": "-LRB- Mihalcea , 2005 -RRB- and -LRB- Sinha and Mihalcea , 2007 -RRB- use WordNet relations as a knowledge source , but neither of them specify which particular version did they use . ",
            "negation": []
        },
        {
            "sentence": "-LRB- Tsatsaronis et al. , 2007 -RRB- uses WordNet 1.7 enriched with eXtended WordNet relations , just as we do . ",
            "negation": []
        },
        {
            "sentence": "Both -LRB- Navigli and Velardi , 2005 ; Nav-igli and Lapata , 2007 -RRB- use WordNet 2.0 as the underlying LKB , albeit enriched with several new relations , which are manually created . ",
            "negation": []
        },
        {
            "sentence": "Unfortunately , those manual relations are not publicly available , so we ca n't directly compare their results with the rest of the methods . ",
            "negation": [
                {
                    "cue": "un",
                    "scope": "Unfortately "
                },
                {
                    "cue": "not",
                    "scope": "those manual relations are publicly available , so we ca n't directly compare their results with the rest of the methods "
                },
                {
                    "cue": "n't",
                    "scope": "we ca directly compare their results with the rest of the methods "
                }
            ]
        },
        {
            "sentence": "In -LRB- Agirre and Soroa , 2008 -RRB- we experiment with different LKBs formed by combining relations of different MCR versions along with relations extracted from Sem-Cor , which we call supervised and unsupervised relations , respectively . ",
            "negation": [
                {
                    "cue": "un",
                    "scope": "supervised relations "
                }
            ]
        },
        {
            "sentence": "The unsupervised relations that yielded bests results are also used in this paper -LRB- c.f Section 3.1 -RRB- . ",
            "negation": [
                {
                    "cue": "un",
                    "scope": "The supervised relations "
                }
            ]
        },
        {
            "sentence": "<newSection> 6 Experiments on Spanish Our WSD algorithm can be applied over non-english texts , provided that a LKB for this particular language exists . ",
            "negation": []
        },
        {
            "sentence": "We have tested the graph-algorithms proposed in this paper on a Spanish dataset , using the Spanish WordNet as knowledge source -LRB- Atserias et al. , 2004a -RRB- . ",
            "negation": []
        },
        {
            "sentence": "We used the Semeval-2007 Task 09 dataset as evaluation gold standard -LRB- M`arquez et al. , 2007 -RRB- . ",
            "negation": []
        },
        {
            "sentence": "The dataset contains examples of the 150 most frequent nouns in the CESS-ECE corpus , manually annotated with Spanish WordNet synsets . ",
            "negation": []
        },
        {
            "sentence": "It is split into a train and test part , and has an `` all words '' shape i.e. input consists on sentences , each one having at least one occurrence of a target noun . ",
            "negation": []
        },
        {
            "sentence": "We ran the experiment over the test part -LRB- 792 instances -RRB- , and used the train part for calculating the MFS baseline . ",
            "negation": []
        },
        {
            "sentence": "We used the Spanish WordNet as LKB , enriched with eXtended WordNet relations . ",
            "negation": []
        },
        {
            "sentence": "It contains 105 , 501 nodes and 623,316 relations . ",
            "negation": []
        },
        {
            "sentence": "The results in Table 3 are consistent with those for English , with our algorithm approaching MFS performance . ",
            "negation": []
        },
        {
            "sentence": "Note that for this dataset the supervised algorithm could barely improve over the MFS , suggesting that for this particular dataset MFS is particularly strong . ",
            "negation": []
        },
        {
            "sentence": "<newSection> 7 Performance analysis Table 4 shows the time spent by the different algorithms when applied to the Senseval-2 all words dataset , using the WNet17 + Xwn as LKB . ",
            "negation": []
        },
        {
            "sentence": "The dataset consists on 2473 word instances appearing on 476 different sentences . ",
            "negation": []
        },
        {
            "sentence": "The experiments were done on a computer with four 2.66 Ghz processors and 16 Gb memory . ",
            "negation": []
        },
        {
            "sentence": "The table shows that the time elapsed by the algorithms varies between 30 minutes for the Ppr method -LRB- which thus dis-ambiguates circa 82 instances per minute -RRB- to almost 3 hours spent by the Ppr w2w method -LRB- circa 15 instances per minute -RRB- . ",
            "negation": []
        },
        {
            "sentence": "The Spr method lies in between , requiring 2 hours for completing the task , but its overall performance is well below the PageRank based Ppr w2w method . ",
            "negation": []
        },
        {
            "sentence": "Note that the algorithm is coded in C++ for greater efficiency , and uses the Boost Graph Library . ",
            "negation": []
        },
        {
            "sentence": "Regarding PageRank calculation , we have tried different numbers of iterations , and analyze the rate of convergence of the algorithm . ",
            "negation": []
        },
        {
            "sentence": "Figure 1 depicts the performance of the Ppr w2w method for different iterations of the algorithm . ",
            "negation": []
        },
        {
            "sentence": "As before , the algorithm is applied over the MCR17 + Xwn LKB , and evaluated on the Senseval-2 all words dataset . ",
            "negation": []
        },
        {
            "sentence": "The algorithm converges very quickly : one sole iteration suffices for achieving a relatively high performance , and 20 iterations are enough for achieving convergence . ",
            "negation": []
        },
        {
            "sentence": "The figure shows that , depending on the LKB complexity , the user can tune the algorithm and lower the number of iterations , thus considerably reducing the time required for disambiguation . ",
            "negation": []
        },
        {
            "sentence": "<newSection> 8 Conclusions In this paper we propose a new graph-based method that uses the knowledge in a LKB -LRB- based on WordNet -RRB- in order to perform unsupervised Word Sense Disambuation . ",
            "negation": [
                {
                    "cue": "un",
                    "scope": "perform supervised Word Sense Disambuation "
                }
            ]
        },
        {
            "sentence": "Our algorithm uses the full graph of the LKB efficiently , performing better than previous approaches in English all-words datasets . ",
            "negation": []
        },
        {
            "sentence": "We also show that the algorithm can be easily ported to other languages with good results , with the only requirement of having a wordnet . ",
            "negation": []
        },
        {
            "sentence": "Both for Spanish and English the algorithm attains performances close to the MFS . ",
            "negation": []
        },
        {
            "sentence": "The algorithm is publicly available5 and can be applied easily to sense inventories and knowledge bases different from WordNet . ",
            "negation": []
        },
        {
            "sentence": "Our analysis shows that our algorithm is efficient compared to previously proposed alternatives , and that a good choice of WordNet versions and relations is fundamental for good performance . ",
            "negation": []
        },
        {
            "sentence": "<newSection> Acknowledgments This work has been partially funded by the EU Commission -LRB- project KYOTO ICT-2007-211423 -RRB- and Spanish Research Department -LRB- project KNOW TIN2006-15049-C03-01 -RRB- . ",
            "negation": []
        },
        {
            "sentence": "<newSection> References ",
            "negation": []
        }
    ]
}