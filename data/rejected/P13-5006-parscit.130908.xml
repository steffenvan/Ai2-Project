<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.228188">
<title confidence="0.990203">
Variational Inference for Structured NLP Models
</title>
<author confidence="0.997019">
David Burkett and Dan Klein
</author>
<affiliation confidence="0.9983565">
Computer Science Division
University of California, Berkeley
</affiliation>
<email confidence="0.991045">
{dburkett,klein}@cs.berkeley.edu
</email>
<sectionHeader confidence="0.962537" genericHeader="abstract">
Description
</sectionHeader>
<bodyText confidence="0.991696106382979">
Historically, key breakthroughs in structured NLP
models, such as chain CRFs or PCFGs, have re-
lied on imposing careful constraints on the local-
ity of features in order to permit efficient dynamic
programming for computing expectations or find-
ing the highest-scoring structures. However, as
modern structured models become more complex
and seek to incorporate longer-range features, it is
more and more often the case that performing ex-
act inference is impossible (or at least impractical)
and it is necessary to resort to some sort of approx-
imation technique, such as beam search, pruning,
or sampling. In the NLP community, one increas-
ingly popular approach is the use of variational
methods for computing approximate distributions.
The goal of the tutorial is to provide an intro-
duction to variational methods for approximate in-
ference, particularly mean field approximation and
belief propagation. The intuition behind the math-
ematical derivation of variational methods is fairly
simple: instead of trying to directly compute the
distribution of interest, first consider some effi-
ciently computable approximation of the original
inference problem, then find the solution of the ap-
proximate inference problem that minimizes the
distance to the true distribution. Though the full
derivations can be somewhat tedious, the resulting
procedures are quite straightforward, and typically
consist of an iterative process of individually up-
dating specific components of the model, condi-
tioned on the rest. Although we will provide some
theoretical background, the main goal of the tu-
torial is to provide a concrete procedural guide to
using these approximate inference techniques, il-
lustrated with detailed walkthroughs of examples
from recent NLP literature.
Once both variational inference procedures
have been described in detail, we’ll provide a sum-
mary comparison of the two, along with some in-
tuition about which approach is appropriate when.
We’ll also provide a guide to further exploration of
the topic, briefly discussing other variational tech-
niques, such as expectation propagation and con-
vex relaxations, but concentrating mainly on pro-
viding pointers to additional resources for those
who wish to learn more.
Outline
</bodyText>
<listItem confidence="0.983388384615384">
1. Structured Models and Factor Graphs
• Factor graph notation
• Example structured NLP models
• Inference
2. Mean Field
• Warmup (iterated conditional modes)
• Mean field procedure
• Derivation of mean field update
• Example
3. Structured Mean Field
• Structured approximation
• Computing structured updates
• Example: Joint parsing and alignment
4. Belief Propagation
• Intro
• Messages and beliefs
• Loopy BP
5. Structured Belief Propagation
• Warmup (efficient products for mes-
sages)
• Example: Word alignment
• Example: Dependency parsing
6. Wrap-Up
• Mean field vs BP
• Other approximation techniques
9
</listItem>
<note confidence="0.775861">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 9–10,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<subsectionHeader confidence="0.937575">
Presenter Bios
</subsectionHeader>
<bodyText confidence="0.999534416666667">
David Burkett is a postdoctoral researcher in the
Computer Science Division at the University of
California, Berkeley. The main focus of his re-
search is on modeling syntactic agreement in bilin-
gual corpora. His interests are diverse, though, and
he has worked on parsing, phrase alignment, lan-
guage evolution, coreference resolution, and even
video game AI. He has worked as an instructional
assistant for multiple AI courses at Berkeley and
won multiple Outstanding Graduate Student In-
structor awards.
Dan Klein is an Associate Professor of Com-
puter Science at the University of California,
Berkeley. His research includes many areas of
statistical natural language processing, includ-
ing grammar induction, parsing, machine trans-
lation, information extraction, document summa-
rization, historical linguistics, and speech recog-
nition. His academic awards include a Sloan Fel-
lowship, a Microsoft Faculty Fellowship, an NSF
CAREER Award, the ACM Grace Murray Hop-
per Award, Best Paper Awards at ACL, EMNLP
and NAACL, and the UC Berkeley Distinguished
Teaching Award.
</bodyText>
<page confidence="0.996667">
10
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.005627">
<title confidence="0.993693">Variational Inference for Structured NLP Models</title>
<author confidence="0.98704">Burkett</author>
<affiliation confidence="0.9999285">Computer Science University of California,</affiliation>
<title confidence="0.3891825">Description Historically, key breakthroughs in structured NLP</title>
<abstract confidence="0.935592287671232">models, such as chain CRFs or PCFGs, have relied on imposing careful constraints on the locality of features in order to permit efficient dynamic programming for computing expectations or finding the highest-scoring structures. However, as modern structured models become more complex and seek to incorporate longer-range features, it is more and more often the case that performing exact inference is impossible (or at least impractical) and it is necessary to resort to some sort of approximation technique, such as beam search, pruning, or sampling. In the NLP community, one increasingly popular approach is the use of variational methods for computing approximate distributions. The goal of the tutorial is to provide an introduction to variational methods for approximate inference, particularly mean field approximation and belief propagation. The intuition behind the mathematical derivation of variational methods is fairly simple: instead of trying to directly compute the distribution of interest, first consider some efficiently computable approximation of the original inference problem, then find the solution of the approximate inference problem that minimizes the distance to the true distribution. Though the full derivations can be somewhat tedious, the resulting procedures are quite straightforward, and typically consist of an iterative process of individually updating specific components of the model, conditioned on the rest. Although we will provide some theoretical background, the main goal of the tutorial is to provide a concrete procedural guide to using these approximate inference techniques, illustrated with detailed walkthroughs of examples from recent NLP literature. Once both variational inference procedures have been described in detail, we’ll provide a summary comparison of the two, along with some intuition about which approach is appropriate when. We’ll also provide a guide to further exploration of the topic, briefly discussing other variational techsuch as expectation propagation and convex relaxations, but concentrating mainly on providing pointers to additional resources for those who wish to learn more. Outline 1. Structured Models and Factor Graphs • Factor graph notation • Example structured NLP models • Inference 2. Mean Field • Warmup (iterated conditional modes) • Mean field procedure • Derivation of mean field update • Example 3. Structured Mean Field • Structured approximation • Computing structured updates • Example: Joint parsing and alignment 4. Belief Propagation • Intro • Messages and beliefs • Loopy BP 5. Structured Belief Propagation • Warmup (efficient products for messages) • Example: Word alignment • Example: Dependency parsing 6. Wrap-Up • Mean field vs BP • Other approximation techniques 9 of the 51st Annual Meeting of the Association for Computational pages 9–10,</abstract>
<title confidence="0.654948">Bulgaria, August 4-9 2013. Association for Computational Linguistics Presenter Bios</title>
<author confidence="0.976917">David Burkett is a postdoctoral researcher in the</author>
<affiliation confidence="0.987234">Computer Science Division at the University of</affiliation>
<abstract confidence="0.95927">California, Berkeley. The main focus of his research is on modeling syntactic agreement in bilingual corpora. His interests are diverse, though, and he has worked on parsing, phrase alignment, language evolution, coreference resolution, and even video game AI. He has worked as an instructional assistant for multiple AI courses at Berkeley and won multiple Outstanding Graduate Student Instructor awards.</abstract>
<author confidence="0.532235">Dan Klein is an Associate Professor of Com-</author>
<affiliation confidence="0.834929">puter Science at the University of California,</affiliation>
<address confidence="0.540484">Berkeley. His research includes many areas of</address>
<abstract confidence="0.9097989">statistical natural language processing, including grammar induction, parsing, machine translation, information extraction, document summarization, historical linguistics, and speech recognition. His academic awards include a Sloan Fellowship, a Microsoft Faculty Fellowship, an NSF CAREER Award, the ACM Grace Murray Hopper Award, Best Paper Awards at ACL, EMNLP and NAACL, and the UC Berkeley Distinguished Teaching Award.</abstract>
<intro confidence="0.779381">10</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>