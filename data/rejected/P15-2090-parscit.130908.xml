<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002289">
<title confidence="0.946856">
UNRAVEL—A Decipherment Toolkit
</title>
<author confidence="0.90671">
Malte Nuhn and Julian Schamper and Hermann Ney
</author>
<affiliation confidence="0.8916085">
Human Language Technology and Pattern Recognition
Computer Science Department, RWTH Aachen University, Aachen, Germany
</affiliation>
<email confidence="0.974226">
&lt;surname&gt;@cs.rwth-aachen.de
</email>
<sectionHeader confidence="0.993381" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998880666666667">
In this paper we present the UNRAVEL
toolkit: It implements many of the recently
published works on decipherment, includ-
ing decipherment for deterministic ciphers
like e.g. the ZODIAC-408 cipher and Part
two of the BEALE ciphers, as well as deci-
pherment of probabilistic ciphers and un-
supervised training for machine transla-
tion. It also includes data and example
configuration files so that the previously
published experiments are easy to repro-
duce.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999945823529412">
The idea of applying decipherment techniques to
the problem of machine translation has driven re-
search on decipherment in the recent time. Even
though the theoretical knowledge has been pub-
lished in the form of papers there has not been
any release of software until now. This made it
very difficult to follow upon the recent research
and to contribute new ideas. With this publica-
tion we want to share our implementation of two
important decipherment algorithms: Beam search
for deterministic substitution ciphers and beamed
EM training for probabilistic ciphers. It is clear
that the field of decipherment is still under heavy
research and that the true value of this release does
not lie in the current implementations themselves,
but rather in the opportunity for other researchers
to contribute their ideas to the field.
</bodyText>
<sectionHeader confidence="0.997015" genericHeader="introduction">
2 Overview
</sectionHeader>
<bodyText confidence="0.999892195121951">
Enciphering a plaintext into a ciphertext can be
done using a myriad of encipherment methods.
Each of these methods needs its own customized
tools and tweaks in order to be deciphered auto-
matically. The goal of UNRAVEL is not to provide
a solver for every single encipherment method, but
rather to provide reusable tools that can be applied
to unsupervised learning for machine translation.
UNRAVEL contains two tools: DET-UNRAVEL
for decipherment of deterministic ciphers, and
EM-UNRAVEL for EM decipherment for proba-
bilistic substitution ciphers and simple machine
translation tasks. A comparison of both tools is
given in Table 1.
The code base is implemented in C++11 and
uses many publicly available libraries: The
GOOGLE-GLOG logging library is used for all log-
ging purposes, the GOOGLE-GFLAGS library is
used for providing command line flags, and the
GOOGLETEST library is used for unit testing and
consistency checks throughout the code base.
Classes for compressed I/O, access to
OpenFST (Allauzen et al., 2007), access to
KENLM(Heafield, 2011), representing mappings,
n-gram counts, vocabularies, lexicons, etc. are
shared across the code base.
For building we use the GNU build system. UN-
RAVEL can be compiled using GCC, ICC, and
CLANG on various Linux distributions and on
MacOS X. Scripts to download and compile nec-
essary libraries are also included: This makes it
easy to install UNRAVEL and its dependencies in
different computing environments.
Also, configuration- and data files (if possible
from a license point of view) for various experi-
ments (see Section 4.2 and Section 5.2) are dis-
tributed. Amongst others this includes setups for
the ZODIAC-408 and Part two of the BEALE ci-
phers (deterministic ciphers), as well as the OPUS
corpus and the VERBMOBIL corpus (probabilistic
cipher/machine translation).
</bodyText>
<sectionHeader confidence="0.999963" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.99908">
We list the most important publications that lead to
the implementation of UNRAVEL: Regarding DET-
</bodyText>
<page confidence="0.932011">
549
</page>
<bodyText confidence="0.957987225806452">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 549–553,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
UNRAVEL, the following literature is relevant:
Hart (1994) presents a tree search algorithm for
simple substitution ciphers with known word seg-
mentations. The idea of performing a tree search
and looking for mappings fulfilling consistency
constraints was later adopted to n-gram based de-
cipherment in an A* search approach presented
by Corlett and Penn (2010). DET-UNRAVEL im-
plements the beam search approach presented by
Nuhn et al. (2013) together with the refinements
presented in (Nuhn et al., 2014). The Bayesian
approach presented by Ravi and Knight (2011a) to
break the ZODIAC-408 cipher is not implemented,
but configuration and data to solve the ZODIAC-
408 cipher with DET-UNRAVEL is included. Also
it is worth noting that Hauer et al. (2014) provided
further work towards homophonic decipherment
that is not included in UNRAVEL.
The EM training for the decipherment of prob-
abilistic substitution ciphers, as first described by
Lee (2002) is implemented in EM-UNRAVEL to-
gether with various improvements and extensions:
The beam- and preselection search approxima-
tions presented by Nuhn and Ney (2014), the con-
text vector based candidate induction presented
by Nuhn et al. (2012), as well as training of the
simplified machine translation model presented by
Ravi and Knight (2011b).
</bodyText>
<sectionHeader confidence="0.997008" genericHeader="method">
4 Deterministic Ciphers: DET-UNRAVEL
</sectionHeader>
<bodyText confidence="0.999359684210526">
Given an input sequence fN1 with tokens fn from
a vocabulary Vf and a language model of a tar-
get language p(eN1 ) with the target tokens from
a target vocabulary Ve, the task is to find a
mapping function 0 : Vf —* Ve so that the
language model probability of the decipherment
p(0(f1)0(f2) ... 0(fN)) is maximized.
DET-UNRAVEL solves this optimization prob-
lem using the beam search approach presented by
Nuhn et al. (2013): The main idea is to structure
all partial 0s into a search tree: If a cipher con-
tains |Vf |unique symbols, then the search tree is
of height |Vf|. At each level a decision about the
n-th symbol is made. The leaves of the tree form
full hypotheses. Instead of traversing the whole
search tree, beam search traverses the tree top to
bottom and only keeps the most promising candi-
dates at each level. Table 2 shows the important
parameters of the algorithm.
</bodyText>
<subsectionHeader confidence="0.9976">
4.1 Implementation Details
</subsectionHeader>
<bodyText confidence="0.999988576923077">
During search, our implementation keeps track of
all partial hypotheses in two arrays H3 and Ht. We
use two different data structures for the hypothe-
ses in H3 and the hypotheses in Ht: H3 contains
the full information of the current partial mapping
0. The candidates in the array Ht are generated
by augmenting hypotheses from the array H3 by
just one additional mapping decision f —* e and
thus we use a different data structure for these hy-
potheses: They contain the current mapping deci-
sion f —* e and a pointer to the parent node in
H3. This saves memory in comparison to storing
the complete mapping at every point in time and
is faster than storing the mapping as a tree, which
would have to be traversed for every score estima-
tion.
The fact that only one additional decision is
made during the expansion process is also used
when calculating the scores for the new hypothe-
sis: Only the additional terms of the final score for
the current partial hypothesis 0 are added to the
predecessor score (i.e. the scheme is scorenew =
scoreold + S, where scoreold is independent of the
current decision f —* e).
The now scored hypotheses in Ht (our imple-
mentation also includes the improved rest cost es-
</bodyText>
<subsectionHeader confidence="0.578378">
Aspect Deterministic Ciphers: DET-UNRAVEL Probabilistic Ciphers: EM-UNRAVEL
</subsectionHeader>
<bodyText confidence="0.968851857142857">
Search Space Mappings 0 Substitution tables {p(f|e)}
Training Beam search over all 0. The order in EM-training: In the E-step use beam
which the decisions for 0(f) for each f search to obtain the most probable deci-
are made is based on the extension order. pherments eI1 for a given ciphertext se-
quence fJ1 . Update {p(f|e)} in M-step.
Decoding Apply 0 to cipher text. Viterbi decoding using final {p(f|e)}.
Experiments ZODIAC-408, pt. two of BEALE ciphers OPUS, VERBMOBIL
</bodyText>
<tableCaption confidence="0.998805">
Table 1: Comparison of DET-UNRAVEL and EM-UNRAVEL.
</tableCaption>
<page confidence="0.991016">
550
</page>
<bodyText confidence="0.999974464285714">
timation as described in (Nuhn et al., 2014)) are
pruned using different pruning strategies: Thresh-
old pruning—given the best hypothesis, add a
threshold score and prune the hypotheses with
scores lower than best hypothesis plus this thresh-
old score—and histogram pruning—which only
keeps the best Bhi,to hypothesis at every level of
the search tree. Further, the surviving hypotheses
are checked whether they fulfill certain constraints
C(o) like e.g. enforcing 1-to-1 mappings during
search.
Those hypotheses in Ht that survived the prun-
ing step and the constraints check are converted to
full hypotheses so that they can be stored in H,.
Then, the search continues with the next cardinal-
ity.
The order in which decisions about the symbols
f E Vf are made during search (called extension
order) can be computed using different strategies:
We implement a simple frequency sorting heuris-
tic, as well as a more advanced strategy that uses
beam search to find an improved enumeration of
f E Vf, as presented in (Nuhn et al., 2014).
Our implementation expands the partial hy-
potheses in H, in parallel: The implementation
has been tested with up to 128 threads (on a 128
core machine) with parallelization overhead of
less than 20%.
</bodyText>
<subsectionHeader confidence="0.766684">
4.2 Experiments
</subsectionHeader>
<bodyText confidence="0.9999804375">
The configurations for decoding the ZODIAC-408
cipher as well as Part two of the BEALE ciphers are
almost identical: For both setups we use an 8-gram
character language model trained on a subset of
the English Gigaword corpus (Parker et al., 2011).
We obtain n-gram counts (order 2 to 8) from the
input ciphers and pass these to DET-UNRAVEL. In
both cases we use the improved heuristic together
with the improved extension order as presented in
(Nuhn et al., 2014).
For the ZODIAC-408, using a beam size Bhi,t =
26 yields 52 out of 54 correct mappings. For the
Part two of the BEALE ciphers a much larger beam
size of Bhi,t = 10M yields 157 correct mappings
out of 185, resulting in an error rate on the string
of 762 symbols is 5.4 %.
</bodyText>
<sectionHeader confidence="0.993371" genericHeader="method">
5 Probabilistic Ciphers: EM-UNRAVEL
</sectionHeader>
<bodyText confidence="0.999858125">
For probabilistic ciphers, the goal is to find a prob-
abilistic substitution table {p(f|e)} with normal-
ization constraint be E f p(f|e) = 1. Learning
this table is done iteratively using the EM algo-
rithm (Dempster et al., 1977).
Each iteration consists of two steps: Hypoth-
esis generation (E-Step) and retraining the table
{p(f|e)} using the posterior probability pj(e|fJ1 )
that any translation eI1 of fJ1 has the word e aligned
to the source word fj (M-Step).
From a higher level view, EM-UNRAVEL can be
seen as a specialized word based MT decoder that
can efficiently generate and organize all possible
translations in the E-step, and efficiently retrain
the model {p(f|e)} on all these hypotheses in the
M-step.
</bodyText>
<subsectionHeader confidence="0.979379">
5.1 Implementation Details
</subsectionHeader>
<bodyText confidence="0.9999885625">
In contrast to DET-UNRAVEL, EM-UNRAVEL pro-
cesses the input corpus sentence by sentence. For
each sentence, we build hypotheses eI1 from left to
right, one word at a time:
First, the empty hypothesis is added to a set
of currently active partial hypotheses. Then, for
each partial hypothesis, a new source word is cho-
sen such that local reordering constraints are ful-
filled. For this, a coverage vector (which encodes
the words that have already been translated) has
to be updated for each hypothesis. Once the cur-
rent source word to be translated next has been
chosen, hypotheses for all possible translations of
this source word are generated and scored. Af-
ter having processed the entire set of partial hy-
potheses, the set of newly generated hypotheses is
</bodyText>
<subsectionHeader confidence="0.840579">
Name Description
</subsectionHeader>
<bodyText confidence="0.764396285714286">
Pruning
Bhist Histogram pruning. Only the best Bhist
hypotheses are kept.
Bthres Threshold pruning. Hypotheses with
scores S worse than Sbest+Bthres, where
Sbest is the score of the best hyptohesis,
are pruned.
</bodyText>
<figure confidence="0.620074">
Constraints
C(0) Substitution constraint. Hypotheses not
fulfilling the constraint C(0) are discarded
from search.
Extension Order
Vext Extension order. Enumeration of the vo-
cabulary Vf in which the search tree over
all 0 is visited.
Bext
hist Histogram Pruning for extension order
search.
W ext Weight for n−gram language model
n
</figure>
<tableCaption confidence="0.69952">
lookahead score.
Table 2: Important parameters of DET-UNRAVEL.
</tableCaption>
<page confidence="0.997967">
551
</page>
<bodyText confidence="0.99998048">
pruned: Here, the partial hypotheses are organized
and pruned with respect to their cardinality. For
each cardinality, we keep the Bhi3to best scoring
hypotheses.
Similarly to DET-UNRAVEL, the previously de-
scribed expansion and pruning step is imple-
mented using two arrays H3 and Ht. However,
in EM-UNRAVEL the partial hypotheses in H3 and
Ht use the same data structures since—in contrast
to DET-UNRAVEL—recombination of hypotheses
is possible.
In the case of large vocabularies it is not feasi-
ble to keep track of all possible substitutions for a
given source word. This step can also be approx-
imated using the preselection technique by Nuhn
and Ney (2014): Instead of adding hypotheses for
all possible target words, only a small subset of
possible successor hypotheses is generated: These
are based on the current source word that is to be
translated, as well as the current language model
state.
Once the search is completed we compute pos-
teriors on the resulting word graph and accumu-
late those across all sentences in the corpus. Hav-
ing finished one pass over the corpus, the accumu-
</bodyText>
<subsectionHeader confidence="0.686928">
Name Description
</subsectionHeader>
<table confidence="0.956026551724138">
Pruning
Bhist Histogram pruning. Only the best Bhist
hypotheses are kept.
Preselection Search
BBlex
cand Lexical candidates. Try only the best
lex
cand substitutions e for each word f
based on p(f|e)
BLM
cand LM candidates. Try only the best BLM
hist
successor words e with respect to the pre-
vious hypothesis’ LM state.
Translation Model
Wjump Jump width. Maximum jump size allowed
in local reordering.
Cjump Jump cost. Cost for non-monotonic tran-
sitions.
Cins Insertion cost. Cost for insertions of
words.
Mins Maximum number of insertions per sen-
tence.
Cdel Deletion cost. Cost for deletions of words.
Mdel Maximum number of of deletions per sen-
tence.
Other
Alex Lexical smoothing parameter.
Nctx Number of candidate translations allowed
</table>
<tableCaption confidence="0.98106">
in lexicon generation in context vector
step.
Table 3: Important parameters of EM-UNRAVEL.
</tableCaption>
<bodyText confidence="0.998552166666667">
lated posteriors are used to re-estimate We|f)}
and the next iteration of the EM algorithm begins.
Also, with every new parameter table We|f)},
the Viterbi decoding of the source corpus is com-
puted.
While full EM training is feasible and gives
good results for the OPUS corpus, Nuhn et al.
(2012) suggest to include a context vector step in
between EM iterations for large vocabulary tasks.
Using the Viterbi decoding of the source se-
quence from the last E-step and the corpus used
to train the LM, we create normalized context vec-
tors for each word e and f. The idea is that vec-
tors for words e and f that are translations of each
other are similar. For each word f E Vf, a set of
candidates e E Ve can be computed. These candi-
dates are used to initialize a new lexicon, which is
further refined using standard EM iterations after-
wards.
Both, EM training and the context vector step
are implemented in a parallel fashion (running in
a single process). Parallelization is done on a sen-
tence level: We successfully used our implemen-
tation with up to 128 cores.
</bodyText>
<subsectionHeader confidence="0.988042">
5.2 Experiments
</subsectionHeader>
<bodyText confidence="0.999982785714286">
We briefly mention experiments on two corpora:
The OPUS corpus and the VERBMOBIL corpus.
The OPUS corpus is a subtitle corpus of roughly
100k running words. Here the vocabulary size
of the source language (Spanish) is 562 and the
target language (English) contains 411 unique
words. Using a 3-gram language model UNRAVEL
achieves 19.5 % BLEU on this task.
The VERBMOBIL corpus contains roughly 600k
running words. The target language vocabulary
size is 3,723 (English) and the source language
vocabulary size is 5,964 (German). Using a 3-
gram language model and the context vector ap-
proach, UNRAVEL achieves 15.5 % BLEU.
</bodyText>
<sectionHeader confidence="0.991733" genericHeader="evaluation">
6 Download and License
</sectionHeader>
<bodyText confidence="0.999250333333333">
UNRAVEL can be downloaded at
www.hltpr.rwth-aachen.de/unravel.
UNRAVEL is distributed under a custom open
source license. This includes free usage for
noncommercial purposes as long as any changes
made to the original software are published
under the terms of the same license. The exact
formulation is available at the download page for
UNRAVEL.
</bodyText>
<page confidence="0.990931">
552
</page>
<bodyText confidence="0.9999774">
We have chosen to keep this paper independent
of actual implementation details such as method-
and parameter names. Please consult the README
files and comments in UNRAVEL’s source code for
implementation details.
</bodyText>
<sectionHeader confidence="0.996737" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999841428571429">
UNRAVEL is a flexible and efficient decipherment
toolkit that is freely available to the scientific com-
munity. It implements algorithms for solving de-
terministic and probabilistic substitution ciphers.
We hope that this release sparks more interest-
ing research on decipherment and its applications
to machine translation.
</bodyText>
<sectionHeader confidence="0.998912" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999631705128205">
[Allauzen et al.2007] Cyril Allauzen, Michael Riley,
Johan Schalkwyk, Wojciech Skut, and Mehryar
Mohri. 2007. Openfst: A general and efficient
weighted finite-state transducer library. In Jan
Holub and Jan Zd´arek, editors, CIAA, volume 4783
of Lecture Notes in Computer Science, pages 11–23.
Springer.
[Corlett and Penn2010] Eric Corlett and Gerald Penn.
2010. An exact A* method for deciphering letter-
substitution ciphers. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 1040–1047, Uppsala, Swe-
den, July. The Association for Computer Linguis-
tics.
[Dempster et al.1977] Arthur P. Dempster, Nan M.
Laird, and Donald B. Rubin. 1977. Maximum like-
lihood from incomplete data via the EM algorithm.
Journal of the Royal Statistical Society, B, 39.
[Hart1994] George W Hart. 1994. To decode
short cryptograms. Communications of the ACM,
37(9):102–108.
[Hauer et al.2014] Bradley Hauer, Ryan Hayward, and
Grzegorz Kondrak. 2014. Solving substitution ci-
phers with combined language models. In Pro-
ceedings of COLING 2014, the 25th International
Conference on Computational Linguistics: Techni-
cal Papers, pages 2314–2325. Dublin City Univer-
sity and Association for Computational Linguistics.
[Heafield2011] Kenneth Heafield. 2011. KenLM:
Faster and Smaller Language Model Queries. In
Proceedings of the Sixth Workshop on Statistical
Machine Translation, pages 187–197, Edinburgh,
Scotland, July. Association for Computational Lin-
guistics.
[Lee2002] Dar-Shyang Lee. 2002. Substitution deci-
phering based on hmms with applications to com-
pressed document processing. Pattern Analysis
and Machine Intelligence, IEEE Transactions on,
24(12):1661–1666.
[Nuhn and Ney2014] Malte Nuhn and Hermann Ney.
2014. Em decipherment for large vocabularies. In
Annual Meeting of the Assoc. for Computational
Linguistics, pages 759–764, Baltimore, MD, USA,
June.
[Nuhn et al.2012] Malte Nuhn, Arne Mauser, and Her-
mann Ney. 2012. Deciphering foreign language by
combining language models and context vectors. In
Proceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
156–164, Jeju, Republic of Korea, July. Association
for Computational Linguistics.
[Nuhn et al.2013] Malte Nuhn, Julian Schamper, and
Hermann Ney. 2013. Beam search for solving sub-
stitution ciphers. In Annual Meeting of the Assoc.
for Computational Linguistics, pages 1569–1576,
Sofia, Bulgaria, August.
[Nuhn et al.2014] Malte Nuhn, Julian Schamper, and
Hermann Ney. 2014. Improved decipherment of
homophonic ciphers. In Conference on Empirical
Methods in Natural Language Processing, Doha,
Qatar, October.
[Parker et al.2011] Robert Parker, David Graff, Junbo
Kong, Ke Chen, and Kazuaki Maeda. 2011. English
gigaword fifth edition. Linguistic Data Consortium,
Philadelphia.
[Ravi and Knight2011a] Sujith Ravi and Kevin Knight.
2011a. Bayesian inference for Zodiac and other ho-
mophonic ciphers. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 239–247, Portland, Ore-
gon, June. Association for Computational Linguis-
tics.
[Ravi and Knight2011b] Sujith Ravi and Kevin Knight.
2011b. Deciphering foreign language. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies, pages 12–21, Portland, Oregon, USA,
June. Association for Computational Linguistics.
</reference>
<page confidence="0.999028">
553
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.448518">
<title confidence="0.862202">Decipherment Toolkit Nuhn Schamper</title>
<author confidence="0.634738">Human Language Technology</author>
<author confidence="0.634738">Pattern</author>
<affiliation confidence="0.98962">Computer Science Department, RWTH Aachen University, Aachen,</affiliation>
<email confidence="0.99568"><surname>@cs.rwth-aachen.de</email>
<abstract confidence="0.989633384615384">this paper we present the toolkit: It implements many of the recently published works on decipherment, including decipherment for deterministic ciphers e.g. the and Part of the as well as decipherment of probabilistic ciphers and unsupervised training for machine translation. It also includes data and example configuration files so that the previously published experiments are easy to reproduce.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Cyril Allauzen</author>
<author>Michael Riley</author>
<author>Johan Schalkwyk</author>
</authors>
<title>Wojciech Skut, and Mehryar Mohri.</title>
<date>2007</date>
<booktitle>of Lecture Notes in Computer Science,</booktitle>
<volume>4783</volume>
<pages>11--23</pages>
<editor>In Jan Holub and Jan Zd´arek, editors, CIAA,</editor>
<publisher>Springer.</publisher>
<marker>[Allauzen et al.2007]</marker>
<rawString>Cyril Allauzen, Michael Riley, Johan Schalkwyk, Wojciech Skut, and Mehryar Mohri. 2007. Openfst: A general and efficient weighted finite-state transducer library. In Jan Holub and Jan Zd´arek, editors, CIAA, volume 4783 of Lecture Notes in Computer Science, pages 11–23. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Corlett</author>
<author>Gerald Penn</author>
</authors>
<title>An exact A* method for deciphering lettersubstitution ciphers.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>1040--1047</pages>
<location>Uppsala, Sweden,</location>
<marker>[Corlett and Penn2010]</marker>
<rawString>Eric Corlett and Gerald Penn. 2010. An exact A* method for deciphering lettersubstitution ciphers. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1040–1047, Uppsala, Sweden, July. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur P Dempster</author>
<author>Nan M Laird</author>
<author>Donald B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society, B,</journal>
<volume>39</volume>
<marker>[Dempster et al.1977]</marker>
<rawString>Arthur P. Dempster, Nan M. Laird, and Donald B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, B, 39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George W Hart</author>
</authors>
<title>To decode short cryptograms.</title>
<date>1994</date>
<journal>Communications of the ACM,</journal>
<volume>37</volume>
<issue>9</issue>
<marker>[Hart1994]</marker>
<rawString>George W Hart. 1994. To decode short cryptograms. Communications of the ACM, 37(9):102–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bradley Hauer</author>
<author>Ryan Hayward</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>Solving substitution ciphers with combined language models.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,</booktitle>
<pages>2314--2325</pages>
<institution>Dublin City University and Association for Computational Linguistics.</institution>
<marker>[Hauer et al.2014]</marker>
<rawString>Bradley Hauer, Ryan Hayward, and Grzegorz Kondrak. 2014. Solving substitution ciphers with combined language models. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 2314–2325. Dublin City University and Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
</authors>
<title>KenLM: Faster and Smaller Language Model Queries.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>187--197</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland,</location>
<marker>[Heafield2011]</marker>
<rawString>Kenneth Heafield. 2011. KenLM: Faster and Smaller Language Model Queries. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 187–197, Edinburgh, Scotland, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dar-Shyang Lee</author>
</authors>
<title>Substitution deciphering based on hmms with applications to compressed document processing.</title>
<date>2002</date>
<journal>Pattern Analysis and Machine Intelligence, IEEE Transactions on,</journal>
<volume>24</volume>
<issue>12</issue>
<marker>[Lee2002]</marker>
<rawString>Dar-Shyang Lee. 2002. Substitution deciphering based on hmms with applications to compressed document processing. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 24(12):1661–1666.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Malte Nuhn</author>
<author>Hermann Ney</author>
</authors>
<title>Em decipherment for large vocabularies.</title>
<date>2014</date>
<booktitle>In Annual Meeting of the Assoc. for Computational Linguistics,</booktitle>
<pages>759--764</pages>
<location>Baltimore, MD, USA,</location>
<marker>[Nuhn and Ney2014]</marker>
<rawString>Malte Nuhn and Hermann Ney. 2014. Em decipherment for large vocabularies. In Annual Meeting of the Assoc. for Computational Linguistics, pages 759–764, Baltimore, MD, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Malte Nuhn</author>
<author>Arne Mauser</author>
<author>Hermann Ney</author>
</authors>
<title>Deciphering foreign language by combining language models and context vectors.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>156--164</pages>
<institution>Korea, July. Association for Computational Linguistics.</institution>
<location>Jeju, Republic of</location>
<marker>[Nuhn et al.2012]</marker>
<rawString>Malte Nuhn, Arne Mauser, and Hermann Ney. 2012. Deciphering foreign language by combining language models and context vectors. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL), pages 156–164, Jeju, Republic of Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Malte Nuhn</author>
<author>Julian Schamper</author>
<author>Hermann Ney</author>
</authors>
<title>Beam search for solving substitution ciphers.</title>
<date>2013</date>
<booktitle>In Annual Meeting of the Assoc. for Computational Linguistics,</booktitle>
<pages>1569--1576</pages>
<location>Sofia, Bulgaria,</location>
<marker>[Nuhn et al.2013]</marker>
<rawString>Malte Nuhn, Julian Schamper, and Hermann Ney. 2013. Beam search for solving substitution ciphers. In Annual Meeting of the Assoc. for Computational Linguistics, pages 1569–1576, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Malte Nuhn</author>
<author>Julian Schamper</author>
<author>Hermann Ney</author>
</authors>
<title>Improved decipherment of homophonic ciphers.</title>
<date>2014</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Doha, Qatar,</location>
<marker>[Nuhn et al.2014]</marker>
<rawString>Malte Nuhn, Julian Schamper, and Hermann Ney. 2014. Improved decipherment of homophonic ciphers. In Conference on Empirical Methods in Natural Language Processing, Doha, Qatar, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Parker</author>
<author>David Graff</author>
<author>Junbo Kong</author>
<author>Ke Chen</author>
<author>Kazuaki Maeda</author>
</authors>
<title>English gigaword fifth edition. Linguistic Data Consortium,</title>
<date>2011</date>
<location>Philadelphia.</location>
<marker>[Parker et al.2011]</marker>
<rawString>Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. 2011. English gigaword fifth edition. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujith Ravi</author>
<author>Kevin Knight</author>
</authors>
<title>Bayesian inference for Zodiac and other homophonic ciphers.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>239--247</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon,</location>
<marker>[Ravi and Knight2011a]</marker>
<rawString>Sujith Ravi and Kevin Knight. 2011a. Bayesian inference for Zodiac and other homophonic ciphers. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL), pages 239–247, Portland, Oregon, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujith Ravi</author>
<author>Kevin Knight</author>
</authors>
<title>Deciphering foreign language.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>12--21</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<marker>[Ravi and Knight2011b]</marker>
<rawString>Sujith Ravi and Kevin Knight. 2011b. Deciphering foreign language. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 12–21, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>