<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.013444">
<title confidence="0.968589">
Webis: An Ensemble for Twitter Sentiment Detection
</title>
<author confidence="0.785029">
Matthias Hagen Martin Potthast Michel Büchner Benno Stein
</author>
<affiliation confidence="0.300809">
Bauhaus-Universität Weimar
</affiliation>
<email confidence="0.205068">
&lt;first name&gt;.&lt;last name&gt;@uni-weimar.de
</email>
<sectionHeader confidence="0.968312" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999961733333333">
We reproduce four Twitter sentiment classi-
fication approaches that participated in pre-
vious SemEval editions with diverse feature
sets. The reproduced approaches are com-
bined in an ensemble, averaging the individ-
ual classifiers’ confidence scores for the three
classes (positive, neutral, negative) and decid-
ing sentiment polarity based on these aver-
ages. The experimental evaluation on Sem-
Eval data shows our re-implementations to
slightly outperform their respective originals.
Moreover, not too surprisingly, the ensem-
ble of the reproduced approaches serves as a
strong baseline in the current edition where it
is top-ranked on the 2015 test set.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999968641025641">
We reproduce four state-of-the-art approaches to
classifying the sentiment expressed in a given tweet,
and combine the four approaches to an ensem-
ble based on the individual classifiers’ confidence
scores. In particular, we focus on subtask B of Sem-
Eval 2015’s task 10 “Sentiment Analysis in Twitter,”
where the goal is to classify the whole tweet as either
positive, neutral, or negative. Since the notebook
descriptions accompanying submissions to shared
tasks are understandably very terse, it is often a chal-
lenge to reproduce the results reported. Therefore,
we attempt to reproduce the state-of-the-art Twitter
sentiment detection algorithms that have been sub-
mitted to the aforementioned task in its previous two
editions. Furthermore, we combine the reproduced
classifiers in an ensemble. Since the individual ap-
proaches employ diverse feature sets, the goal of the
ensemble is to combine their individual strengths.
The paper at hand is a slight extension of the ap-
proach from our ECIR 2015 reproducibility track
paper (Hagen et al., 2015) such that also text pas-
sages are reused. In our ECIR paper, we showed that
three selected approaches participating in the Sem-
Eval 2013 Twitter sentiment task 2 could be repro-
duced from the papers accompanying the individual
approaches. Adding the best participant of the re-
spective SemEval 2014 task 9 is shown to form a
very strong baseline that was not outperformed by
the SemEval 2015 participants on the 2015 test data
and that also places in the top-10 in the progress test.
In Section 2 we briefly describe some related
work while in Section 3 we provide more details on
the four individual approaches as well as our ensem-
ble scheme. Some concluding remarks and an out-
look on future work close the paper in Section 4.
An experimental evaluation of our approach and an
in-depth comparison to the other participants is not
included in this paper since it can be found in the
task overview (Rosenthal et al., 2015).
</bodyText>
<sectionHeader confidence="0.999833" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9967715">
Sentiment detection is a classic problem of text clas-
sification. Unlike other text classification tasks, the
goal is not to identify topics, entities, or authors of a
text but to rate the expressed sentiment as positive,
negative, or neutral. Most approaches used for sen-
timent detection usually involve methods from ma-
chine learning, computational linguistics, and statis-
tics. Typically, several approaches from these fields
are combined for sentiment detection (Pang et al.,
2002; Turney, 2002; Feldman, 2013).
Since Twitter is one of the richest sources of opin-
ion, a lot of different approaches to sentiment de-
</bodyText>
<page confidence="0.958063">
582
</page>
<note confidence="0.9532615">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 582–589,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999957369230769">
tection in tweets have been proposed. Different
approaches use different feature sets ranging from
standard word polarity expressions or unigram fea-
tures also applied in general sentiment detection (Go
et al., 2009; Kouloumpis et al., 2011), to the usage
of emoticons and uppercases (Barbosa and Feng,
2010), word lengthening (Brody and Diakopoulos,
2011), phonetic features (Ermakov and Ermakova,
2013), multi-lingual machine translation (Balahur
and Turchi, 2013), or word embeddings (Tang et al.,
2014). The task usually is to detect the sentiment
expressed in a tweet as a whole (also focus of this
paper). But it can also be used to identify the senti-
ment in a tweet with respect to a given target concept
expressed in a query (Jiang et al., 2011). The differ-
ence is that a generally negative tweet might not say
anything about the target concept and must thus be
considered neutral with respect to the target concept.
Both tasks, namely sentiment detection in a tweet,
and sentiment detection with respect to a specific tar-
get concept, are part of the SemEval sentiment anal-
ysis tasks since 2013 (Nakov et al., 2013; Rosenthal
et al., 2014). SemEval fosters research on sentiment
detection for short texts in particular, and gathers the
best-performing approaches in a friendly competi-
tion. The problem we are dealing with is formulated
as subtask B: given a tweet, decide whether its mes-
sage is positive, negative, or neutral.
State-of-the-art approaches have been submitted
to the SemEval tasks. However, up to now, no one
had trained a meta-classifier based on the submitted
approaches to determine what can be achieved when
combining them, whereas each participating team
only trains their individual classifier using respec-
tive individual feature sets. Our idea is to combine
four of the best-performing approaches from the last
years with different feature sets, and to form an en-
semble classifier that leverages the individual classi-
fiers’ strengths forming a strong baseline.
Ensemble learning is a classic approach of com-
bining several classifiers to a more powerful en-
semble (Opitz and Maclin, 1999; Polikar, 2006;
Rokach, 2010). The classic approaches of Bag-
ging (Breiman, 1996) and Boosting (Schapire, 1990;
Freund and Schapire, 1996) try to either combine the
outputs of different classifiers trained on different
random instances of the training set or on training
the classifiers on instances that were misclassified
by the other classifiers. Both rather work on the final
predictions of the classifiers just as for instance av-
eraging or majority voting on the predictions (Asker
and Maclin, 1997) would do. In our case, we employ
the confidence scores of the participating classifiers.
Several papers describe different ways of working
with the classifiers’ confidence scores, such as learn-
ing a dynamic confidence weighting scheme (Fung
et al., 2006), or deriving a set cover with averaging
confidences (Rokach et al., 2014). Instead, we sim-
ply average the three confidence scores of the three
classifiers for each individual class. This straight-
forward approach performs superior to its individ-
ual parts and performs competitive in the SemEval
competitions. Thus, its sentiment detection results
can be directly used in any of the above use cases
for Twitter sentiment detection.
</bodyText>
<sectionHeader confidence="0.98932" genericHeader="method">
3 Individual Approaches and Ensemble
</sectionHeader>
<bodyText confidence="0.999983888888889">
For our ECIR 2015 reproducibility paper (Hagen
et al., 2015), we originally selected three state-of-
the-art approaches for Twitter sentiment detection
among the 38 participants of SemEval 2013. To
identify worthy candidates—and to satisfy the claim
“state of the art”—we picked the top-ranked ap-
proach by team NRC-Canada (Mohammad et al.,
2013). However, instead of simply picking the ap-
proaches on ranks two and three to complete our set,
we first analyzed the notebooks of the top-ranked
teams in order to identify approaches that are signif-
icantly dissimilar from NRC-Canada. We decided
to handpick approaches this way so they comple-
ment each other in an ensemble. As a second can-
didate, we picked team GU-MLT-LT (Günther and
Furrer, 2013) since it uses some other features and
a different sentiment lexicon. As a third candidate,
we picked team KLUE (Proisl et al., 2013), which
was ranked fifth. We discarded the third-ranked
approach as it is using a large set of not publicly
available rules probably hindering reproducibility,
whereas the fourth-ranked system seemed too sim-
ilar to NRC and GU-MLT-LT to add something new
to the planned ensemble. Finally, for participation
in SemEval 2015, we also included TeamX (Miura
et al., 2014) as the 2014 top-performing approach
resulting in an ensemble of four.
</bodyText>
<page confidence="0.997969">
583
</page>
<bodyText confidence="0.9999501">
Note that due to the selection process, reproduc-
ing the four approaches does not deteriorate into
reimplementing the feature set of one approach and
reusing it for the other two. Moreover, combin-
ing the four approaches into an ensemble classifier
actually makes sense, since, due to the feature set
diversity, they tap sufficiently different information
sources. In what follows, we first briefly recap the
features used by the individual classifiers and then
explain our ensemble strategy.
</bodyText>
<subsectionHeader confidence="0.998962">
3.1 NRC-Canada
</subsectionHeader>
<bodyText confidence="0.998993695652174">
Team NRC-Canada (Mohammad et al., 2013) used
a classifier with a wide range of features. A tweet
is first preprocessed by replacing URLs and user
names by some placeholder. The tweets are then to-
kenized and POS-tagged. An SVM with linear ker-
nel is trained using the following feature set.
N-grams The occurrence of word 1- to 4-grams
as well as occurrences of pairs of non-consecutive
words where the intermediate words are replaced by
a placeholder. No term-weighting like tf ·idf is used.
Similarly for occurrence of character 3- to 5-grams.
ALLCAPS Number of all-capitalized words.
Parts of speech Occurrence of part-of-speech tags.
Polarity dictionaries In total, five polarity dictio-
naries are used. Three of these were manually cre-
ated: the NRC Emotion Lexicon (Mohammad and
Turney, 2010; Mohammad and Turney, 2013) with
14,000 words, the MPQA Lexicon (Wilson et al.,
2005) with 8,000 words, and the Bing Liu Lexi-
con (Hu and Liu, 2004) with 6,800 words. Two
other dictionaries were created automatically. For
the first one, the idea is that several hash tags can
express sentiment (e.g., #good). Team NRC crawled
775,000 tweets from April to December 2012 that
contain at least one of 32 positive or 38 negative
hash tags that were manually created (e.g., #good
and #bad). For word 1-grams and word 2-grams in
the tweets, PMI-scores were calculated for each of
the 70 hash tags to yield a score for the n-grams
(i.e., the ones with higher positive hash tag PMI are
positive, the others negative). The resulting dictio-
nary contains 54,129 unigrams, 316,531 bigrams,
and 308,808 pairs of non-consecutive words. The
second automatically created dictionary is not based
on PMI for hash tags but for emoticons. It was cre-
ated in a similar way as the hash tag dictionary and
contains 62,468 unigrams, 677,698 bigrams, and
480,010 pairs of non-consecutive words.
For each entry of the five dictionaries, the dictio-
nary score is either positive, negative, or zero. For
a tweet and each individual dictionary, several fea-
tures are computed: the number of dictionary entries
with a positive score and the number of entries with
a negative score, the sum of the positive scores and
the sum of the negative scores of the tweet’s dictio-
nary entries, the maximum positive score and mini-
mum negative score of the tweet’s dictionary entries,
and the last positive score and negative score.
Punctuation marks The number of non-single
punctuation marks (e.g., !! or ?!) is used as a fea-
ture and whether the last one is an exclamation or a
question mark.
Emoticons The emoticons contained in a tweet,
their polarity, and whether the last token of a tweet
is an emoticon are employed features.
Word lengthening The number of words that are
lengthened by repeating a letter more than twice
(e.g., cooooolll) is a feature.
Clustering Via unsupervised Brown cluster-
ing (Brown et al., 1992) a set of 56,345,753 tweets
by Owoputi (Owoputi et al., 2013) clustered into
1,000 clusters. The IDs of the clusters in which the
terms of a tweet occur are also used as features.
Negation The number of negated segments is a
feature. A negated segment starts with a nega-
tion (e.g., shouldn’t) and ends with a punctuation
mark (Pang et al., 2002). Every token in a negated
segment (words, emoticons) gets a suffix NEG at-
tached (e.g., perfect_NEG).
</bodyText>
<subsectionHeader confidence="0.998754">
3.2 GU-MLT-LT
</subsectionHeader>
<bodyText confidence="0.999416333333333">
Team GU-MLT-LT (Günther and Furrer, 2013) was
ranked second in SemEval 2013. They train a
stochastic gradient decent classifier on a much
smaller feature set compared to NRC. The follow-
ing feature set is computed for tokenized versions
of the original raw tweet, a lowercased normalized
version of the tweet, and a version of the lower-
cased tweet where consecutive identical letters are
collapsed (e.g., helllo gets hello).
</bodyText>
<page confidence="0.99055">
584
</page>
<bodyText confidence="0.999452533333334">
Normalized unigrams The occurrence of the nor-
malized word unigrams is one feature set. No term
weighting like for instance tf ·idf is used.
Stems Porter stemming (Porter, 1980) is used to
identify the occurrence of the stems of the collapsed
word unigrams as another feature set. Again, no
term weighting is applied.
Clustering Similar to NRC, the cluster IDs of the
raw, normalized, and collapsed tokens are features.
Polarity dictionary The SentiWordNet assess-
ments (Baccianella et al., 2010) of the individual
collapsed tokens and the sum of all tokens’ scores
in a tweet are further features.
Negation Normalized tokens and stems are added
as negated features similar to NRC.
</bodyText>
<subsectionHeader confidence="0.902588">
3.3 KLUE
</subsectionHeader>
<bodyText confidence="0.999779820512821">
Team KLUE (Proisl et al., 2013) was ranked fifth in
the SemEval 2013 ranking. Similarly to NRC, team
KLUE first replaces URLs and user names by some
placeholder and tokenizes the lowercased tweets. A
maximum entropy-based classifier is trained on the
following features.
N-grams Word unigrams and bigrams are used as
features but in contrast to NRC and GU-MLT-LT not
just by occurrence but frequency-weighted. Due to
the short tweet length this however often boils down
to a simple occurrence feature. To be part of the
feature set, an n-gram has to be contained in at least
five tweets. This excludes some rather obscure and
rare terms or misspellings.
Length The number of tokens in a tweet (i.e., its
length) is used as a feature. Interestingly, NRC and
GU-MLT-LT do not explicitly use this feature.
Polarity dictionary The employed dictionary is
the AFINN-111 lexicon (Nielsen, 2011) containing
2,447 words with assessments from −5 (very nega-
tive) to +5 (very positive). Team KLUE added an-
other 343 words. Employed features are the number
of positive tokens in a tweet, the number of negative
tokens, the number of tokens with a dictionary score,
and the arithmetic mean of the scores in a tweet.
Emoticons and abbreviations A list of
212 emoticons and 95 colloquial abbreviations
from Wikipedia was manually scored as positive,
negative, or neutral. For a tweet, again the number
of positive and negative tokens from this list, the
total number of scored tokens, and the arithmetic
mean are used as features.
Negation Negation is not treated for the whole
segment as NRC and GU-MLT-LT do but only on
the next three tokens except the case that the punc-
tuation comes earlier. Only negated word unigrams
are used as an additional feature set. The polarity
scores from the above dictionary are multiplied by
−1 for terms up to 4 tokens after the negation.
</bodyText>
<subsectionHeader confidence="0.738107">
3.4 TeamX
</subsectionHeader>
<bodyText confidence="0.999576692307692">
TeamX (Miura et al., 2014) was ranked first in the
SemEval 2014 ranking. The approach was inspired
by NRC Canada’s 2013 method but uses fewer fea-
tures and more polarity dictionaries—some differ-
ences are outlined below. Although it is very close
to NRC Canada, some differences exist that jus-
tify TeamX’s selection for our ensemble—besides
its good performance in SemEval 2014.
Parts of speech Two different POS taggers are
used: the Stanford POS tagger’s tags are used
for the polarity dictionaries based on formal lan-
guage and for word sense disambiguation while
the CMU ARK POS tagger is used for the polar-
ity dictionaries containing more informal expres-
sions, n-grams and the cluster features. Since the
CMU ARK tagger was explicitly developed for han-
dling tweets, it is better suited for the informal lan-
guage often used in tweets while the Stanford tagger
better addresses the needs of the formal dictionaries.
N-grams Word uni- up to 4-grams (consecutive
words but also with gaps) and consecutive character
3- up to 5-grams are used as features similar to NRC.
Polarity dictionaries TeamX uses all the dictio-
naries of NRC, GU-ML-LT, and KLUE except for
the NRC emoticon dictionary. Additionally, also
SentiWordNet is used.
</bodyText>
<subsectionHeader confidence="0.929171">
3.5 Remarks on Reimplementing
</subsectionHeader>
<bodyText confidence="0.999895142857143">
As was to be expected, it turned out to be impossible
to re-implement all features precisely as the origi-
nal authors did. Either not all data were publicly
available, or the features themselves were not suffi-
ciently explained in the notebooks. We deliberated
to contact the original authors to give them a chance
to supply missing data as well as to elaborate on
</bodyText>
<page confidence="0.993415">
585
</page>
<bodyText confidence="0.999946872340426">
missing information. However, we ultimately opted
against doing so for the following reason: our goal
was to reproduce their results, not to repeat them.
The difference between reproducibility and repeata-
bility is subtle, yet important. If an approach can be
re-implemented with incomplete information and if
it then achieves a performance within the ballpark of
the original, it can be considered much more robust
than an approach that must be precisely the same
as the original to achieve its expected performance.
The former hints reproducibility, the latter only re-
peatability. This is why we have partly re-invented
the approaches on our own, wherever information or
data were missing. In doing so, we sometimes found
ourselves in a situation where departing from the
original approach would yield better performance.
In such cases, we decided to maximize performance
rather than sticking to the original, since in an eval-
uation setting, it is unfair to not maximize perfor-
mance wherever one can.
In particular, the emoticons and abbreviations
added by the KLUE team were not available, such
that we only choose the AFINN-111 polarity dic-
tionary and re-implemented an emoticon detec-
tion and manual polarity scoring ourselves. We
also chose not to use the frequency information
in the KLUE system but only Boolean occurrence
like NRC and GU-MLT-LT, since pilot studies on
the SemEval 2013 training and development sets
showed that to perform much better. For all three
approaches, we unified tweet normalization regard-
ing lowercasing and completely removing URLs and
user names instead of adding a placeholder. As for
the classifier itself, we did not use the learning al-
gorithms used originally but L2-regularized logis-
tic regression from the LIBLINEAR SVM library
for all three approaches. In our pilot experiments
on the SemEval 2013 training and development set
this showed a very good trade-off between training
time and accuracy. We set the cost parameter to 0.5
for NRC, to 0.15 for GU-MLT-LT, and to 0.05 for
TeamX and KLUE.
Note that most of our design decisions do not
hurt the individual performances but instead im-
prove the accuracy for GU-MLT-LT and KLUE on
the SemEval 2013 test set. Table 1 shows the per-
formance of the original SemEval 2013 and 2014
</bodyText>
<tableCaption confidence="0.984233">
Table 1: F1-scores of the original and reimplemented
classifiers on the SemEval 2013 and 2014 test data and
performance of the final system on the 2015 test data.
</tableCaption>
<table confidence="0.946645333333333">
Classifier Original SemEval 2013 Reimplemented
NRC 69.02 69.44
GU-MLT-LT 65.27 67.27
KLUE 63.06 67.05
Original SemEval 2014 Reimplemented
TeamX 72.12 70.09
</table>
<subsectionHeader confidence="0.530885">
SemEval 2015 result
</subsectionHeader>
<bodyText confidence="0.984012296296296">
Ensemble 64.84 (rank 1 among 40 systems)
rankings and that of our re-implementations based
on the averaged F1-score for the positive and neg-
ative class only (as is done at SemEval). While
the reimplemented NRC performance is slightly bet-
ter, GU-MLT-LT and KLUE are substantially im-
proved. That TeamX lost performance is proba-
bly due to a fact that we only recognized after the
competition: The word sense feature was uninten-
tionally not switched on in the re-implementation
of TeamX. Since for this “handicapped” version
of TeamX (again, we just noticed the reason for
the handicap after the SemEval 2015 deadline) the
weighting scheme of the classification probabilities
proposed for the original approach (Miura et al.,
2014) did decrease the performance, we also did not
use these weights. If we would have noticed our mis-
take before, the performance of the TeamX classifier
would probably have been better.
Altogether, we conclude that reproducing the
SemEval approaches was generally possible but in-
volved some subtleties that sometimes lead to dif-
ficult design decisions. Our resolution is to maxi-
mize performance rather than to dogmatically stick
to the original approach; even though this includes
the error in the TeamX re-implementation that went
through unnoticed until after the deadline.
</bodyText>
<subsectionHeader confidence="0.974985">
3.6 Ensemble Combination
</subsectionHeader>
<bodyText confidence="0.999995571428571">
In our pilot studies on the SemEval 2013 training
and development sets, we tested several ways of
combining the classifiers to an ensemble method.
One of the main observations was that each in-
dividual approach classifies some tweets correctly
that others fail for. This is not too surprising
given the different feature sets but also supports
</bodyText>
<page confidence="0.996656">
586
</page>
<bodyText confidence="0.99998598245614">
the idea of using an ensemble to combine the in-
dividual strengths. Although we briefly tried dif-
ferent ways of bagging and boosting the three clas-
sifiers, it soon turned out that some simpler com-
bination performs better. A problem, for instance,
was that some misclassified tweets are very dif-
ficult (e.g., the positive Cant wait for the UCLA
midnight madness tomorrow night). Since often
at least two classifiers fail on a hard tweet, this rules
out some basic combination schemes, such as the
majority vote which turned out to perform worse on
the SemEval 2013 development set than NRC alone.
The solution that we finally came up with is moti-
vated by observing how the classifiers trained on the
SemEval 2013 training set behave for tweets in the
development set. Typically, not the four final deci-
sions but the respective confidences or probabilities
of the individual classifiers give a good hint on un-
certainties. If two are not really sure about the final
classification, sometimes the remaining ones favor
another class with high confidence. Thus, instead
of looking at the classifications, we decided to use
the confidence scores or probabilities to build the
ensemble. This approach is also motivated by old
and also more recent research on ensemble learn-
ing (Asker and Maclin, 1997; Fung et al., 2006;
Rokach et al., 2014). But instead of learning a
weighting scheme for the different individual clas-
sifiers, we decided to simply compute the average
probability of the four classifiers for each of the
three classes (positive, negative, neutral).
Our ensemble thus works as follows. The
four individual re-implementations of the TeamX,
the NRC, the GU-MLT-LT, and the KLUE classi-
fier are individually trained on the SemEval 2013
training and development set as if being applied
individually—without boosting or bagging. As
for the classification of a tweet, the ensemble ig-
nores the individual classifiers’ classification deci-
sions but requests the classifiers’ probabilities (or
confidences) for each class. The ensemble deci-
sion then chooses the class with the highest average
probability—again, no sophisticated techniques like
dynamic confidence weighting (Fung et al., 2006)
or set covering schemes (Rokach et al., 2014) are in-
volved. Thus, our final ensemble method is a rather
straightforward system based on averaging confi-
dences instead of voting schemes on the actual clas-
sifications of the individual classifiers. It can be eas-
ily implemented on top of the four classifiers and
thus incurs no additional overhead. It also proves
a very strong baseline in the SemEval 2015 evalu-
ation. This is not really surprising since typically
ensembles of good and diverse approaches should
achieve better performances. Our code for the four
reproduced approaches as well as that of the ensem-
ble is publicly available.1
</bodyText>
<sectionHeader confidence="0.971413" genericHeader="conclusions">
4 Conclusion and Outlook
</sectionHeader>
<bodyText confidence="0.999976580645161">
We have reproduced four state-of-the-art approaches
to sentiment detection for Twitter tweets. Our find-
ings include that not all aspects of the approaches
could be reproduced precisely, but that missing
data, missing information, as well as opportuni-
ties to improve the approaches’ performances lead
us to re-invent them and to depart to some ex-
tent from the original descriptions. Most of our
changes have improved the performances of the
original approaches (except the erroneously and un-
intentionally switched off word sense feature of
TeamX). Moreover, we have demonstrated that the
approaches can be reproduced even with incomplete
information about them, which is a much stronger
property than being merely repeatable.
In addition, we investigated a combination of con-
fidence scores of the four approaches within an en-
semble that altogether yields a top-performing Twit-
ter sentiment detection system forming a very strong
baseline. The ensemble computation is as efficient
as its components, and its effectiveness can be seen
from the top rank on the SemEval 2015 test set and
the top-10 ranking in the progress test involving the
previous years’ test data.
Promising directions for future research are an ex-
tensive error analysis and the identification of further
classifiers potentially strengthening the ensemble.
Following our philosophy of selecting approaches
that are significantly different from each other, it will
be interesting to observe how much new approaches
can improve the existing ensemble.
</bodyText>
<footnote confidence="0.998957">
1http://www.uni-weimar.de/medien/webis/
publications/by-year/#stein_2015d
</footnote>
<page confidence="0.994118">
587
</page>
<sectionHeader confidence="0.935951" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.987901203883495">
Lars Asker and Richard Maclin. 1997. Ensembles as
a sequence of classifiers. In Proceedings of the Fif-
teenth International Joint Conference on Artificial In-
telligence, IJCAI 97, Nagoya, Japan, August 23-29,
1997, 2 Volumes, pages 860–865.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining. In
Proceedings of the International Conference on Lan-
guage Resources and Evaluation, LREC 2010, 17-23
May 2010, Valletta, Malta.
Alexandra Balahur and Marco Turchi. 2013. Improving
sentiment analysis in twitter using multilingual ma-
chine translated data. In Proceedings of the Interna-
tional Conference Recent Advances in Natural Lan-
guage Processing, RANLP 2013, pages 49–55.
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on twitter from biased and noisy data.
In COLING 2010, 23rd International Conference on
Computational Linguistics, Posters Volume, 23-27Au-
gust 2010, Beijing, China, pages 36–44.
Leo Breiman. 1996. Bagging predictors. Machine
Learning, 24(2):123–140.
Samuel Brody and Nicholas Diakopoulos. 2011.
Cooooooooooooooollllllllllllll! !!!!!!!!!!!!! using word
lengthening to detect sentiment in microblogs. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP 2011,
27-31 July 2011, John McIntyre Conference Centre,
Edinburgh, UK, A meeting of SIGDAT, a Special In-
terest Group of the ACL, pages 562–570.
Peter F. Brown, Vincent J. Della Pietra, Peter V. de Souza,
Jennifer C. Lai, and Robert L. Mercer. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467–479.
Sergei Ermakov and Liana Ermakova. 2013. Sentiment
classification based on phonetic characteristics. In Ad-
vances in Information Retrieval - 35th European Con-
ference on IR Research, ECIR 2013, Moscow, Russia,
March 24-27, 2013. Proceedings, pages 706–709.
Ronen Feldman. 2013. Techniques and applications
for sentiment analysis. Communications of the ACM,
56(4):82–89.
Yoav Freund and Robert E. Schapire. 1996. Experiments
with a new boosting algorithm. In Machine Learn-
ing, Proceedings of the Thirteenth International Con-
ference (ICML ’96), Bari, Italy, July 3-6, 1996, pages
148–156.
Gabriel Pui Cheong Fung, Jeffrey Xu Yu, Haixun Wang,
David W. Cheung, and Huan Liu. 2006. A bal-
anced ensemble approach to weighting classifiers for
text classification. In Proceedings of the 6th IEEE In-
ternational Conference on Data Mining (ICDM 2006),
18-22 December 2006, Hong Kong, China, pages 869–
873.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
Technical Report CS224N Project Report, Stanford
University.
Tobias Günther and Lenz Furrer. 2013. GU-MLT-LT:
Sentiment analysis of short messages using linguis-
tic features and stochastic gradient descent. In Sec-
ond Joint Conference on Lexical and Computational
Semantics (*SEM), Volume 2: Proceedings of the Sev-
enth International Workshop on Semantic Evaluation
(SemEval 2013), pages 328–332.
Matthias Hagen, Martin Potthast, Michel Büchner, and
Benno Stein. 2015. Twitter sentiment detection
via ensemble classification using averaged confidence
scores. In Proceedings of ECIR 2015 (to appear).
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the Tenth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, Seattle, Washington,
USA, August 22-25, 2004, pages 168–177.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and Tiejun
Zhao. 2011. Target-dependent twitter sentiment clas-
sification. In The 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, Proceedings of the Conference, 19-24
June, 2011, Portland, Oregon, USA, pages 151–160.
Efthymios Kouloumpis, Theresa Wilson, and Johanna D.
Moore. 2011. Twitter sentiment analysis: The good
the bad and the OMG! In Proceedings of the Fifth In-
ternational Conference on Weblogs and Social Media,
Barcelona, Catalonia, Spain, July 17-21, 2011.
Yasuhide Miura, Shigeyuki Sakaki, Keigo Hattori, and
Tomoko Ohkuma. 2014. Teamx: A sentiment ana-
lyzer with enhanced lexicon mapping and weighting
scheme for unbalanced data. In Proceedings of the 8th
International Workshop on Semantic Evaluation (Sem-
Eval 2014), pages 628–632, Dublin, Ireland, August.
Association for Computational Linguistics and Dublin
City University.
Saif M. Mohammad and Peter D. Turney. 2010. Emo-
tions evoked by common words and phrases: Using
mechanical turk to create an emotion lexicon. In Pro-
ceedings of the NAACL HLT 2010 Workshop on Com-
putational Approaches to Analysis and Generation of
Emotion in Text, CAAGET 2010, pages 26–34.
Saif M. Mohammad and Peter D. Turney. 2013. Crowd-
sourcing a word-emotion association lexicon. Compu-
tational Intelligence, 29(3):436–465.
</reference>
<page confidence="0.981551">
588
</page>
<reference confidence="0.999892561797752">
Saif M. Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the state-of-the-
art in sentiment analysis of tweets. In Second Joint
Conference on Lexical and Computational Semantics
(*SEM), Volume 2: Proceedings of the Seventh Inter-
national Workshop on Semantic Evaluation (SemEval
2013), pages 321–327.
Preslav Nakov, Zornitsa Kozareva, Alan Ritter, Sara
Rosenthal, Veselin Stoyanov, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis in
twitter. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop on Se-
mantic Evaluation (SemEval 2013), pages 312–320.
Finn Årup Nielsen. 2011. A new ANEW: evaluation of
a word list for sentiment analysis in microblogs. In
Proceedings of the ESWC2011 Workshop on ’Making
Sense of Microposts’: Big things come in small pack-
ages, Heraklion, Crete, Greece, May 30, 2011, pages
93–98.
David W. Opitz and Richard Maclin. 1999. Popular en-
semble methods: An empirical study. Journal of Arti-
ficial Intelligence Research, 11:169–198.
Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin
Gimpel, Nathan Schneider, and Noah A. Smith. 2013.
Improved part-of-speech tagging for online conversa-
tional text with word clusters. In Human Language
Technologies: Conference of the North American
Chapter of the Association of Computational Linguis-
tics, Proceedings, June 9-14, 2013, Westin Peachtree
Plaza Hotel, Atlanta, Georgia, USA, pages 380–390.
Bo Pang, Lillian Lee, and Shiuvakumar Vaithyanathan.
2002. Thumbs up?: Sentiment classification using ma-
chine learning techniques. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP 2002, pages 79–86.
Robi Polikar. 2006. Ensemble based systems in deci-
sion making. IEEE Circuits and Systems Magazine,
6(3):21–45.
Martin Porter. 1980. An algorithm for suffix stripping.
Program: electronic library and information systems,
14(3):130–137.
Thomas Proisl, Paul Greiner, Stefan Evert, and Besim
Kabashi. 2013. Klue: Simple and robust methods
for polarity classification. In Second Joint Conference
on Lexical and Computational Semantics (*SEM), Vol-
ume 2: Proceedings of the Seventh International Work-
shop on Semantic Evaluation (SemEval 2013), pages
395–401.
Lior Rokach, Alon Schclar, and Ehud Itach. 2014. En-
semble methods for multi-label classification. Expert
Systems with Applications, 41(16):7507–7523.
Lior Rokach. 2010. Ensemble-based classifiers. Artifi-
cial Intelligence Review, 33(1-2):1–39.
Sara Rosenthal, Alan Ritter, Preslav Nakov, and Veselin
Stoyanov. 2014. Semeval-2014 task 9: Sentiment
analysis in twitter. In Proceedings of the 8th Inter-
national Workshop on Semantic Evaluation (SemEval
2014), pages 73–80.
Sara Rosenthal, Preslav Nakov, Svetlana Kiritchenko,
Saif M Mohammad, Alan Ritter, and Veselin Stoy-
anov. 2015. Semeval-2015 task 10: Sentiment analy-
sis in twitter. In Proceedings of the 9th International
Workshop on Semantic Evaluation, SemEval 2015,
SemEval 2015, Denver, Colorado, June. Association
for Computational Linguistics.
Robert E. Schapire. 1990. The strength of weak learn-
ability. Machine Learning, 5:197–227.
Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting Liu,
and Bing Qin. 2014. Learning sentiment-specific
word embedding for twitter sentiment classification.
In Proceedings of the 52nd Annual Meeting of the As-
sociation for Computational Linguistics, ACL 2014,
June 22-27, 2014, Baltimore, MD, USA, Volume 1:
Long Papers, pages 1555–1565.
Peter D. Turney. 2002. Thumbs up or thumbs down?
Semantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics, ACL 2002,J uly 6-12, 2002, Philadelphia, PA,
USA., pages 417–424.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In HLT/EMNLP 2005, Human
Language Technology Conference and Conference on
Empirical Methods in Natural Language Processing,
Proceedings of the Conference, 6-8 October 2005,
Vancouver, British Columbia, Canada, pages 347–
354.
</reference>
<page confidence="0.998838">
589
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.626047">
<title confidence="0.999978">Webis: An Ensemble for Twitter Sentiment Detection</title>
<author confidence="0.8949935">Matthias Hagen Martin Potthast Michel Büchner Benno Bauhaus-Universität Weimar</author>
<email confidence="0.826485"><firstname>.<lastname>@uni-weimar.de</email>
<abstract confidence="0.9957156875">We reproduce four Twitter sentiment classification approaches that participated in previous SemEval editions with diverse feature sets. The reproduced approaches are combined in an ensemble, averaging the individual classifiers’ confidence scores for the three classes (positive, neutral, negative) and deciding sentiment polarity based on these averages. The experimental evaluation on Sem- Eval data shows our re-implementations to slightly outperform their respective originals. Moreover, not too surprisingly, the ensemble of the reproduced approaches serves as a strong baseline in the current edition where it is top-ranked on the 2015 test set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Lars Asker</author>
<author>Richard Maclin</author>
</authors>
<title>Ensembles as a sequence of classifiers.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence, IJCAI 97,</booktitle>
<volume>2</volume>
<pages>860--865</pages>
<location>Nagoya, Japan,</location>
<contexts>
<context position="6251" citStr="Asker and Maclin, 1997" startWordPosition="981" endWordPosition="984">ble learning is a classic approach of combining several classifiers to a more powerful ensemble (Opitz and Maclin, 1999; Polikar, 2006; Rokach, 2010). The classic approaches of Bagging (Breiman, 1996) and Boosting (Schapire, 1990; Freund and Schapire, 1996) try to either combine the outputs of different classifiers trained on different random instances of the training set or on training the classifiers on instances that were misclassified by the other classifiers. Both rather work on the final predictions of the classifiers just as for instance averaging or majority voting on the predictions (Asker and Maclin, 1997) would do. In our case, we employ the confidence scores of the participating classifiers. Several papers describe different ways of working with the classifiers’ confidence scores, such as learning a dynamic confidence weighting scheme (Fung et al., 2006), or deriving a set cover with averaging confidences (Rokach et al., 2014). Instead, we simply average the three confidence scores of the three classifiers for each individual class. This straightforward approach performs superior to its individual parts and performs competitive in the SemEval competitions. Thus, its sentiment detection result</context>
<context position="22311" citStr="Asker and Maclin, 1997" startWordPosition="3607" endWordPosition="3610">e classifiers trained on the SemEval 2013 training set behave for tweets in the development set. Typically, not the four final decisions but the respective confidences or probabilities of the individual classifiers give a good hint on uncertainties. If two are not really sure about the final classification, sometimes the remaining ones favor another class with high confidence. Thus, instead of looking at the classifications, we decided to use the confidence scores or probabilities to build the ensemble. This approach is also motivated by old and also more recent research on ensemble learning (Asker and Maclin, 1997; Fung et al., 2006; Rokach et al., 2014). But instead of learning a weighting scheme for the different individual classifiers, we decided to simply compute the average probability of the four classifiers for each of the three classes (positive, negative, neutral). Our ensemble thus works as follows. The four individual re-implementations of the TeamX, the NRC, the GU-MLT-LT, and the KLUE classifier are individually trained on the SemEval 2013 training and development set as if being applied individually—without boosting or bagging. As for the classification of a tweet, the ensemble ignores th</context>
</contexts>
<marker>Asker, Maclin, 1997</marker>
<rawString>Lars Asker and Richard Maclin. 1997. Ensembles as a sequence of classifiers. In Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence, IJCAI 97, Nagoya, Japan, August 23-29, 1997, 2 Volumes, pages 860–865.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefano Baccianella</author>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>SentiWordNet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>In Proceedings of the International Conference on Language Resources and Evaluation, LREC</booktitle>
<pages>17--23</pages>
<location>Valletta,</location>
<contexts>
<context position="13050" citStr="Baccianella et al., 2010" startWordPosition="2089" endWordPosition="2092"> the tweet, and a version of the lowercased tweet where consecutive identical letters are collapsed (e.g., helllo gets hello). 584 Normalized unigrams The occurrence of the normalized word unigrams is one feature set. No term weighting like for instance tf ·idf is used. Stems Porter stemming (Porter, 1980) is used to identify the occurrence of the stems of the collapsed word unigrams as another feature set. Again, no term weighting is applied. Clustering Similar to NRC, the cluster IDs of the raw, normalized, and collapsed tokens are features. Polarity dictionary The SentiWordNet assessments (Baccianella et al., 2010) of the individual collapsed tokens and the sum of all tokens’ scores in a tweet are further features. Negation Normalized tokens and stems are added as negated features similar to NRC. 3.3 KLUE Team KLUE (Proisl et al., 2013) was ranked fifth in the SemEval 2013 ranking. Similarly to NRC, team KLUE first replaces URLs and user names by some placeholder and tokenizes the lowercased tweets. A maximum entropy-based classifier is trained on the following features. N-grams Word unigrams and bigrams are used as features but in contrast to NRC and GU-MLT-LT not just by occurrence but frequency-weigh</context>
</contexts>
<marker>Baccianella, Esuli, Sebastiani, 2010</marker>
<rawString>Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani. 2010. SentiWordNet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining. In Proceedings of the International Conference on Language Resources and Evaluation, LREC 2010, 17-23 May 2010, Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Balahur</author>
<author>Marco Turchi</author>
</authors>
<title>Improving sentiment analysis in twitter using multilingual machine translated data.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Conference Recent Advances in Natural Language Processing, RANLP</booktitle>
<pages>49--55</pages>
<contexts>
<context position="4098" citStr="Balahur and Turchi, 2013" startWordPosition="630" endWordPosition="633">International Workshop on Semantic Evaluation (SemEval 2015), pages 582–589, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics tection in tweets have been proposed. Different approaches use different feature sets ranging from standard word polarity expressions or unigram features also applied in general sentiment detection (Go et al., 2009; Kouloumpis et al., 2011), to the usage of emoticons and uppercases (Barbosa and Feng, 2010), word lengthening (Brody and Diakopoulos, 2011), phonetic features (Ermakov and Ermakova, 2013), multi-lingual machine translation (Balahur and Turchi, 2013), or word embeddings (Tang et al., 2014). The task usually is to detect the sentiment expressed in a tweet as a whole (also focus of this paper). But it can also be used to identify the sentiment in a tweet with respect to a given target concept expressed in a query (Jiang et al., 2011). The difference is that a generally negative tweet might not say anything about the target concept and must thus be considered neutral with respect to the target concept. Both tasks, namely sentiment detection in a tweet, and sentiment detection with respect to a specific target concept, are part of the SemEval</context>
</contexts>
<marker>Balahur, Turchi, 2013</marker>
<rawString>Alexandra Balahur and Marco Turchi. 2013. Improving sentiment analysis in twitter using multilingual machine translated data. In Proceedings of the International Conference Recent Advances in Natural Language Processing, RANLP 2013, pages 49–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luciano Barbosa</author>
<author>Junlan Feng</author>
</authors>
<title>Robust sentiment detection on twitter from biased and noisy data.</title>
<date>2010</date>
<booktitle>In COLING 2010, 23rd International Conference on Computational Linguistics, Posters Volume,</booktitle>
<pages>36--44</pages>
<location>Beijing, China,</location>
<contexts>
<context position="3940" citStr="Barbosa and Feng, 2010" startWordPosition="611" endWordPosition="614">rney, 2002; Feldman, 2013). Since Twitter is one of the richest sources of opinion, a lot of different approaches to sentiment de582 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 582–589, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics tection in tweets have been proposed. Different approaches use different feature sets ranging from standard word polarity expressions or unigram features also applied in general sentiment detection (Go et al., 2009; Kouloumpis et al., 2011), to the usage of emoticons and uppercases (Barbosa and Feng, 2010), word lengthening (Brody and Diakopoulos, 2011), phonetic features (Ermakov and Ermakova, 2013), multi-lingual machine translation (Balahur and Turchi, 2013), or word embeddings (Tang et al., 2014). The task usually is to detect the sentiment expressed in a tweet as a whole (also focus of this paper). But it can also be used to identify the sentiment in a tweet with respect to a given target concept expressed in a query (Jiang et al., 2011). The difference is that a generally negative tweet might not say anything about the target concept and must thus be considered neutral with respect to the</context>
</contexts>
<marker>Barbosa, Feng, 2010</marker>
<rawString>Luciano Barbosa and Junlan Feng. 2010. Robust sentiment detection on twitter from biased and noisy data. In COLING 2010, 23rd International Conference on Computational Linguistics, Posters Volume, 23-27August 2010, Beijing, China, pages 36–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leo Breiman</author>
</authors>
<title>Bagging predictors.</title>
<date>1996</date>
<booktitle>Machine Learning,</booktitle>
<volume>24</volume>
<issue>2</issue>
<contexts>
<context position="5828" citStr="Breiman, 1996" startWordPosition="917" endWordPosition="918">ed approaches to determine what can be achieved when combining them, whereas each participating team only trains their individual classifier using respective individual feature sets. Our idea is to combine four of the best-performing approaches from the last years with different feature sets, and to form an ensemble classifier that leverages the individual classifiers’ strengths forming a strong baseline. Ensemble learning is a classic approach of combining several classifiers to a more powerful ensemble (Opitz and Maclin, 1999; Polikar, 2006; Rokach, 2010). The classic approaches of Bagging (Breiman, 1996) and Boosting (Schapire, 1990; Freund and Schapire, 1996) try to either combine the outputs of different classifiers trained on different random instances of the training set or on training the classifiers on instances that were misclassified by the other classifiers. Both rather work on the final predictions of the classifiers just as for instance averaging or majority voting on the predictions (Asker and Maclin, 1997) would do. In our case, we employ the confidence scores of the participating classifiers. Several papers describe different ways of working with the classifiers’ confidence scor</context>
</contexts>
<marker>Breiman, 1996</marker>
<rawString>Leo Breiman. 1996. Bagging predictors. Machine Learning, 24(2):123–140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Brody</author>
<author>Nicholas Diakopoulos</author>
</authors>
<title>Cooooooooooooooollllllllllllll! !!!!!!!!!!!!! using word lengthening to detect sentiment in microblogs.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, EMNLP 2011,</booktitle>
<pages>27--31</pages>
<institution>John McIntyre Conference Centre,</institution>
<location>Edinburgh, UK,</location>
<contexts>
<context position="3988" citStr="Brody and Diakopoulos, 2011" startWordPosition="617" endWordPosition="620">s one of the richest sources of opinion, a lot of different approaches to sentiment de582 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 582–589, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics tection in tweets have been proposed. Different approaches use different feature sets ranging from standard word polarity expressions or unigram features also applied in general sentiment detection (Go et al., 2009; Kouloumpis et al., 2011), to the usage of emoticons and uppercases (Barbosa and Feng, 2010), word lengthening (Brody and Diakopoulos, 2011), phonetic features (Ermakov and Ermakova, 2013), multi-lingual machine translation (Balahur and Turchi, 2013), or word embeddings (Tang et al., 2014). The task usually is to detect the sentiment expressed in a tweet as a whole (also focus of this paper). But it can also be used to identify the sentiment in a tweet with respect to a given target concept expressed in a query (Jiang et al., 2011). The difference is that a generally negative tweet might not say anything about the target concept and must thus be considered neutral with respect to the target concept. Both tasks, namely sentiment de</context>
</contexts>
<marker>Brody, Diakopoulos, 2011</marker>
<rawString>Samuel Brody and Nicholas Diakopoulos. 2011. Cooooooooooooooollllllllllllll! !!!!!!!!!!!!! using word lengthening to detect sentiment in microblogs. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, EMNLP 2011, 27-31 July 2011, John McIntyre Conference Centre, Edinburgh, UK, A meeting of SIGDAT, a Special Interest Group of the ACL, pages 562–570.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Peter V de Souza</author>
<author>Jennifer C Lai</author>
<author>Robert L Mercer</author>
</authors>
<title>Classbased n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<marker>Brown, Pietra, de Souza, Lai, Mercer, 1992</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Peter V. de Souza, Jennifer C. Lai, and Robert L. Mercer. 1992. Classbased n-gram models of natural language. Computational Linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergei Ermakov</author>
<author>Liana Ermakova</author>
</authors>
<title>Sentiment classification based on phonetic characteristics.</title>
<date>2013</date>
<booktitle>In Advances in Information Retrieval - 35th European Conference on IR Research, ECIR 2013,</booktitle>
<pages>706--709</pages>
<location>Moscow, Russia,</location>
<contexts>
<context position="4036" citStr="Ermakov and Ermakova, 2013" startWordPosition="623" endWordPosition="626"> different approaches to sentiment de582 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 582–589, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics tection in tweets have been proposed. Different approaches use different feature sets ranging from standard word polarity expressions or unigram features also applied in general sentiment detection (Go et al., 2009; Kouloumpis et al., 2011), to the usage of emoticons and uppercases (Barbosa and Feng, 2010), word lengthening (Brody and Diakopoulos, 2011), phonetic features (Ermakov and Ermakova, 2013), multi-lingual machine translation (Balahur and Turchi, 2013), or word embeddings (Tang et al., 2014). The task usually is to detect the sentiment expressed in a tweet as a whole (also focus of this paper). But it can also be used to identify the sentiment in a tweet with respect to a given target concept expressed in a query (Jiang et al., 2011). The difference is that a generally negative tweet might not say anything about the target concept and must thus be considered neutral with respect to the target concept. Both tasks, namely sentiment detection in a tweet, and sentiment detection with</context>
</contexts>
<marker>Ermakov, Ermakova, 2013</marker>
<rawString>Sergei Ermakov and Liana Ermakova. 2013. Sentiment classification based on phonetic characteristics. In Advances in Information Retrieval - 35th European Conference on IR Research, ECIR 2013, Moscow, Russia, March 24-27, 2013. Proceedings, pages 706–709.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronen Feldman</author>
</authors>
<title>Techniques and applications for sentiment analysis.</title>
<date>2013</date>
<journal>Communications of the ACM,</journal>
<volume>56</volume>
<issue>4</issue>
<contexts>
<context position="3343" citStr="Feldman, 2013" startWordPosition="523" endWordPosition="524">uded in this paper since it can be found in the task overview (Rosenthal et al., 2015). 2 Related Work Sentiment detection is a classic problem of text classification. Unlike other text classification tasks, the goal is not to identify topics, entities, or authors of a text but to rate the expressed sentiment as positive, negative, or neutral. Most approaches used for sentiment detection usually involve methods from machine learning, computational linguistics, and statistics. Typically, several approaches from these fields are combined for sentiment detection (Pang et al., 2002; Turney, 2002; Feldman, 2013). Since Twitter is one of the richest sources of opinion, a lot of different approaches to sentiment de582 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 582–589, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics tection in tweets have been proposed. Different approaches use different feature sets ranging from standard word polarity expressions or unigram features also applied in general sentiment detection (Go et al., 2009; Kouloumpis et al., 2011), to the usage of emoticons and uppercases (Barbosa and Feng, 2010), w</context>
</contexts>
<marker>Feldman, 2013</marker>
<rawString>Ronen Feldman. 2013. Techniques and applications for sentiment analysis. Communications of the ACM, 56(4):82–89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert E Schapire</author>
</authors>
<title>Experiments with a new boosting algorithm.</title>
<date>1996</date>
<booktitle>In Machine Learning, Proceedings of the Thirteenth International Conference (ICML ’96),</booktitle>
<pages>148--156</pages>
<location>Bari, Italy,</location>
<contexts>
<context position="5885" citStr="Freund and Schapire, 1996" startWordPosition="923" endWordPosition="926">ed when combining them, whereas each participating team only trains their individual classifier using respective individual feature sets. Our idea is to combine four of the best-performing approaches from the last years with different feature sets, and to form an ensemble classifier that leverages the individual classifiers’ strengths forming a strong baseline. Ensemble learning is a classic approach of combining several classifiers to a more powerful ensemble (Opitz and Maclin, 1999; Polikar, 2006; Rokach, 2010). The classic approaches of Bagging (Breiman, 1996) and Boosting (Schapire, 1990; Freund and Schapire, 1996) try to either combine the outputs of different classifiers trained on different random instances of the training set or on training the classifiers on instances that were misclassified by the other classifiers. Both rather work on the final predictions of the classifiers just as for instance averaging or majority voting on the predictions (Asker and Maclin, 1997) would do. In our case, we employ the confidence scores of the participating classifiers. Several papers describe different ways of working with the classifiers’ confidence scores, such as learning a dynamic confidence weighting schem</context>
</contexts>
<marker>Freund, Schapire, 1996</marker>
<rawString>Yoav Freund and Robert E. Schapire. 1996. Experiments with a new boosting algorithm. In Machine Learning, Proceedings of the Thirteenth International Conference (ICML ’96), Bari, Italy, July 3-6, 1996, pages 148–156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Pui Cheong Fung</author>
<author>Jeffrey Xu Yu</author>
<author>Haixun Wang</author>
<author>David W Cheung</author>
<author>Huan Liu</author>
</authors>
<title>A balanced ensemble approach to weighting classifiers for text classification.</title>
<date>2006</date>
<booktitle>In Proceedings of the 6th IEEE International Conference on Data Mining (ICDM</booktitle>
<pages>18--22</pages>
<location>Hong Kong, China,</location>
<contexts>
<context position="6506" citStr="Fung et al., 2006" startWordPosition="1020" endWordPosition="1023">y to either combine the outputs of different classifiers trained on different random instances of the training set or on training the classifiers on instances that were misclassified by the other classifiers. Both rather work on the final predictions of the classifiers just as for instance averaging or majority voting on the predictions (Asker and Maclin, 1997) would do. In our case, we employ the confidence scores of the participating classifiers. Several papers describe different ways of working with the classifiers’ confidence scores, such as learning a dynamic confidence weighting scheme (Fung et al., 2006), or deriving a set cover with averaging confidences (Rokach et al., 2014). Instead, we simply average the three confidence scores of the three classifiers for each individual class. This straightforward approach performs superior to its individual parts and performs competitive in the SemEval competitions. Thus, its sentiment detection results can be directly used in any of the above use cases for Twitter sentiment detection. 3 Individual Approaches and Ensemble For our ECIR 2015 reproducibility paper (Hagen et al., 2015), we originally selected three state-ofthe-art approaches for Twitter se</context>
<context position="22330" citStr="Fung et al., 2006" startWordPosition="3611" endWordPosition="3614"> the SemEval 2013 training set behave for tweets in the development set. Typically, not the four final decisions but the respective confidences or probabilities of the individual classifiers give a good hint on uncertainties. If two are not really sure about the final classification, sometimes the remaining ones favor another class with high confidence. Thus, instead of looking at the classifications, we decided to use the confidence scores or probabilities to build the ensemble. This approach is also motivated by old and also more recent research on ensemble learning (Asker and Maclin, 1997; Fung et al., 2006; Rokach et al., 2014). But instead of learning a weighting scheme for the different individual classifiers, we decided to simply compute the average probability of the four classifiers for each of the three classes (positive, negative, neutral). Our ensemble thus works as follows. The four individual re-implementations of the TeamX, the NRC, the GU-MLT-LT, and the KLUE classifier are individually trained on the SemEval 2013 training and development set as if being applied individually—without boosting or bagging. As for the classification of a tweet, the ensemble ignores the individual classi</context>
</contexts>
<marker>Fung, Yu, Wang, Cheung, Liu, 2006</marker>
<rawString>Gabriel Pui Cheong Fung, Jeffrey Xu Yu, Haixun Wang, David W. Cheung, and Huan Liu. 2006. A balanced ensemble approach to weighting classifiers for text classification. In Proceedings of the 6th IEEE International Conference on Data Mining (ICDM 2006), 18-22 December 2006, Hong Kong, China, pages 869– 873.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alec Go</author>
<author>Richa Bhayani</author>
<author>Lei Huang</author>
</authors>
<title>Twitter sentiment classification using distant supervision.</title>
<date>2009</date>
<tech>Technical Report CS224N Project Report,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="3847" citStr="Go et al., 2009" startWordPosition="596" endWordPosition="599">roaches from these fields are combined for sentiment detection (Pang et al., 2002; Turney, 2002; Feldman, 2013). Since Twitter is one of the richest sources of opinion, a lot of different approaches to sentiment de582 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 582–589, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics tection in tweets have been proposed. Different approaches use different feature sets ranging from standard word polarity expressions or unigram features also applied in general sentiment detection (Go et al., 2009; Kouloumpis et al., 2011), to the usage of emoticons and uppercases (Barbosa and Feng, 2010), word lengthening (Brody and Diakopoulos, 2011), phonetic features (Ermakov and Ermakova, 2013), multi-lingual machine translation (Balahur and Turchi, 2013), or word embeddings (Tang et al., 2014). The task usually is to detect the sentiment expressed in a tweet as a whole (also focus of this paper). But it can also be used to identify the sentiment in a tweet with respect to a given target concept expressed in a query (Jiang et al., 2011). The difference is that a generally negative tweet might not </context>
</contexts>
<marker>Go, Bhayani, Huang, 2009</marker>
<rawString>Alec Go, Richa Bhayani, and Lei Huang. 2009. Twitter sentiment classification using distant supervision. Technical Report CS224N Project Report, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tobias Günther</author>
<author>Lenz Furrer</author>
</authors>
<title>GU-MLT-LT: Sentiment analysis of short messages using linguistic features and stochastic gradient descent.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>328--332</pages>
<contexts>
<context position="7715" citStr="Günther and Furrer, 2013" startWordPosition="1211" endWordPosition="1214">s for Twitter sentiment detection among the 38 participants of SemEval 2013. To identify worthy candidates—and to satisfy the claim “state of the art”—we picked the top-ranked approach by team NRC-Canada (Mohammad et al., 2013). However, instead of simply picking the approaches on ranks two and three to complete our set, we first analyzed the notebooks of the top-ranked teams in order to identify approaches that are significantly dissimilar from NRC-Canada. We decided to handpick approaches this way so they complement each other in an ensemble. As a second candidate, we picked team GU-MLT-LT (Günther and Furrer, 2013) since it uses some other features and a different sentiment lexicon. As a third candidate, we picked team KLUE (Proisl et al., 2013), which was ranked fifth. We discarded the third-ranked approach as it is using a large set of not publicly available rules probably hindering reproducibility, whereas the fourth-ranked system seemed too similar to NRC and GU-MLT-LT to add something new to the planned ensemble. Finally, for participation in SemEval 2015, we also included TeamX (Miura et al., 2014) as the 2014 top-performing approach resulting in an ensemble of four. 583 Note that due to the selec</context>
<context position="12169" citStr="Günther and Furrer, 2013" startWordPosition="1947" endWordPosition="1950"> a letter more than twice (e.g., cooooolll) is a feature. Clustering Via unsupervised Brown clustering (Brown et al., 1992) a set of 56,345,753 tweets by Owoputi (Owoputi et al., 2013) clustered into 1,000 clusters. The IDs of the clusters in which the terms of a tweet occur are also used as features. Negation The number of negated segments is a feature. A negated segment starts with a negation (e.g., shouldn’t) and ends with a punctuation mark (Pang et al., 2002). Every token in a negated segment (words, emoticons) gets a suffix NEG attached (e.g., perfect_NEG). 3.2 GU-MLT-LT Team GU-MLT-LT (Günther and Furrer, 2013) was ranked second in SemEval 2013. They train a stochastic gradient decent classifier on a much smaller feature set compared to NRC. The following feature set is computed for tokenized versions of the original raw tweet, a lowercased normalized version of the tweet, and a version of the lowercased tweet where consecutive identical letters are collapsed (e.g., helllo gets hello). 584 Normalized unigrams The occurrence of the normalized word unigrams is one feature set. No term weighting like for instance tf ·idf is used. Stems Porter stemming (Porter, 1980) is used to identify the occurrence o</context>
</contexts>
<marker>Günther, Furrer, 2013</marker>
<rawString>Tobias Günther and Lenz Furrer. 2013. GU-MLT-LT: Sentiment analysis of short messages using linguistic features and stochastic gradient descent. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 328–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Hagen</author>
<author>Martin Potthast</author>
<author>Michel Büchner</author>
<author>Benno Stein</author>
</authors>
<title>Twitter sentiment detection via ensemble classification using averaged confidence scores.</title>
<date>2015</date>
<booktitle>In Proceedings of ECIR</booktitle>
<note>(to appear).</note>
<contexts>
<context position="1897" citStr="Hagen et al., 2015" startWordPosition="279" endWordPosition="282">nying submissions to shared tasks are understandably very terse, it is often a challenge to reproduce the results reported. Therefore, we attempt to reproduce the state-of-the-art Twitter sentiment detection algorithms that have been submitted to the aforementioned task in its previous two editions. Furthermore, we combine the reproduced classifiers in an ensemble. Since the individual approaches employ diverse feature sets, the goal of the ensemble is to combine their individual strengths. The paper at hand is a slight extension of the approach from our ECIR 2015 reproducibility track paper (Hagen et al., 2015) such that also text passages are reused. In our ECIR paper, we showed that three selected approaches participating in the SemEval 2013 Twitter sentiment task 2 could be reproduced from the papers accompanying the individual approaches. Adding the best participant of the respective SemEval 2014 task 9 is shown to form a very strong baseline that was not outperformed by the SemEval 2015 participants on the 2015 test data and that also places in the top-10 in the progress test. In Section 2 we briefly describe some related work while in Section 3 we provide more details on the four individual ap</context>
<context position="7034" citStr="Hagen et al., 2015" startWordPosition="1102" endWordPosition="1105">nfidence scores, such as learning a dynamic confidence weighting scheme (Fung et al., 2006), or deriving a set cover with averaging confidences (Rokach et al., 2014). Instead, we simply average the three confidence scores of the three classifiers for each individual class. This straightforward approach performs superior to its individual parts and performs competitive in the SemEval competitions. Thus, its sentiment detection results can be directly used in any of the above use cases for Twitter sentiment detection. 3 Individual Approaches and Ensemble For our ECIR 2015 reproducibility paper (Hagen et al., 2015), we originally selected three state-ofthe-art approaches for Twitter sentiment detection among the 38 participants of SemEval 2013. To identify worthy candidates—and to satisfy the claim “state of the art”—we picked the top-ranked approach by team NRC-Canada (Mohammad et al., 2013). However, instead of simply picking the approaches on ranks two and three to complete our set, we first analyzed the notebooks of the top-ranked teams in order to identify approaches that are significantly dissimilar from NRC-Canada. We decided to handpick approaches this way so they complement each other in an ens</context>
</contexts>
<marker>Hagen, Potthast, Büchner, Stein, 2015</marker>
<rawString>Matthias Hagen, Martin Potthast, Michel Büchner, and Benno Stein. 2015. Twitter sentiment detection via ensemble classification using averaged confidence scores. In Proceedings of ECIR 2015 (to appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>168--177</pages>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="9736" citStr="Hu and Liu, 2004" startWordPosition="1537" endWordPosition="1540">o 4-grams as well as occurrences of pairs of non-consecutive words where the intermediate words are replaced by a placeholder. No term-weighting like tf ·idf is used. Similarly for occurrence of character 3- to 5-grams. ALLCAPS Number of all-capitalized words. Parts of speech Occurrence of part-of-speech tags. Polarity dictionaries In total, five polarity dictionaries are used. Three of these were manually created: the NRC Emotion Lexicon (Mohammad and Turney, 2010; Mohammad and Turney, 2013) with 14,000 words, the MPQA Lexicon (Wilson et al., 2005) with 8,000 words, and the Bing Liu Lexicon (Hu and Liu, 2004) with 6,800 words. Two other dictionaries were created automatically. For the first one, the idea is that several hash tags can express sentiment (e.g., #good). Team NRC crawled 775,000 tweets from April to December 2012 that contain at least one of 32 positive or 38 negative hash tags that were manually created (e.g., #good and #bad). For word 1-grams and word 2-grams in the tweets, PMI-scores were calculated for each of the 70 hash tags to yield a score for the n-grams (i.e., the ones with higher positive hash tag PMI are positive, the others negative). The resulting dictionary contains 54,1</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Seattle, Washington, USA, August 22-25, 2004, pages 168–177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Long Jiang</author>
<author>Mo Yu</author>
<author>Ming Zhou</author>
<author>Xiaohua Liu</author>
<author>Tiejun Zhao</author>
</authors>
<title>Target-dependent twitter sentiment classification.</title>
<date>2011</date>
<booktitle>In The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference,</booktitle>
<pages>151--160</pages>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="4385" citStr="Jiang et al., 2011" startWordPosition="686" endWordPosition="689"> or unigram features also applied in general sentiment detection (Go et al., 2009; Kouloumpis et al., 2011), to the usage of emoticons and uppercases (Barbosa and Feng, 2010), word lengthening (Brody and Diakopoulos, 2011), phonetic features (Ermakov and Ermakova, 2013), multi-lingual machine translation (Balahur and Turchi, 2013), or word embeddings (Tang et al., 2014). The task usually is to detect the sentiment expressed in a tweet as a whole (also focus of this paper). But it can also be used to identify the sentiment in a tweet with respect to a given target concept expressed in a query (Jiang et al., 2011). The difference is that a generally negative tweet might not say anything about the target concept and must thus be considered neutral with respect to the target concept. Both tasks, namely sentiment detection in a tweet, and sentiment detection with respect to a specific target concept, are part of the SemEval sentiment analysis tasks since 2013 (Nakov et al., 2013; Rosenthal et al., 2014). SemEval fosters research on sentiment detection for short texts in particular, and gathers the best-performing approaches in a friendly competition. The problem we are dealing with is formulated as subtas</context>
</contexts>
<marker>Jiang, Yu, Zhou, Liu, Zhao, 2011</marker>
<rawString>Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and Tiejun Zhao. 2011. Target-dependent twitter sentiment classification. In The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference, 19-24 June, 2011, Portland, Oregon, USA, pages 151–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Efthymios Kouloumpis</author>
<author>Theresa Wilson</author>
<author>Johanna D Moore</author>
</authors>
<title>Twitter sentiment analysis: The good the bad and the OMG!</title>
<date>2011</date>
<booktitle>In Proceedings of the Fifth International Conference on Weblogs and Social</booktitle>
<location>Media, Barcelona, Catalonia, Spain,</location>
<contexts>
<context position="3873" citStr="Kouloumpis et al., 2011" startWordPosition="600" endWordPosition="603">e fields are combined for sentiment detection (Pang et al., 2002; Turney, 2002; Feldman, 2013). Since Twitter is one of the richest sources of opinion, a lot of different approaches to sentiment de582 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 582–589, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics tection in tweets have been proposed. Different approaches use different feature sets ranging from standard word polarity expressions or unigram features also applied in general sentiment detection (Go et al., 2009; Kouloumpis et al., 2011), to the usage of emoticons and uppercases (Barbosa and Feng, 2010), word lengthening (Brody and Diakopoulos, 2011), phonetic features (Ermakov and Ermakova, 2013), multi-lingual machine translation (Balahur and Turchi, 2013), or word embeddings (Tang et al., 2014). The task usually is to detect the sentiment expressed in a tweet as a whole (also focus of this paper). But it can also be used to identify the sentiment in a tweet with respect to a given target concept expressed in a query (Jiang et al., 2011). The difference is that a generally negative tweet might not say anything about the tar</context>
</contexts>
<marker>Kouloumpis, Wilson, Moore, 2011</marker>
<rawString>Efthymios Kouloumpis, Theresa Wilson, and Johanna D. Moore. 2011. Twitter sentiment analysis: The good the bad and the OMG! In Proceedings of the Fifth International Conference on Weblogs and Social Media, Barcelona, Catalonia, Spain, July 17-21, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yasuhide Miura</author>
<author>Shigeyuki Sakaki</author>
<author>Keigo Hattori</author>
<author>Tomoko Ohkuma</author>
</authors>
<title>Teamx: A sentiment analyzer with enhanced lexicon mapping and weighting scheme for unbalanced data.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>628--632</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics and Dublin City University.</institution>
<location>Dublin, Ireland,</location>
<contexts>
<context position="8214" citStr="Miura et al., 2014" startWordPosition="1292" endWordPosition="1295">so they complement each other in an ensemble. As a second candidate, we picked team GU-MLT-LT (Günther and Furrer, 2013) since it uses some other features and a different sentiment lexicon. As a third candidate, we picked team KLUE (Proisl et al., 2013), which was ranked fifth. We discarded the third-ranked approach as it is using a large set of not publicly available rules probably hindering reproducibility, whereas the fourth-ranked system seemed too similar to NRC and GU-MLT-LT to add something new to the planned ensemble. Finally, for participation in SemEval 2015, we also included TeamX (Miura et al., 2014) as the 2014 top-performing approach resulting in an ensemble of four. 583 Note that due to the selection process, reproducing the four approaches does not deteriorate into reimplementing the feature set of one approach and reusing it for the other two. Moreover, combining the four approaches into an ensemble classifier actually makes sense, since, due to the feature set diversity, they tap sufficiently different information sources. In what follows, we first briefly recap the features used by the individual classifiers and then explain our ensemble strategy. 3.1 NRC-Canada Team NRC-Canada (Mo</context>
<context position="15138" citStr="Miura et al., 2014" startWordPosition="2441" endWordPosition="2444">reviations from Wikipedia was manually scored as positive, negative, or neutral. For a tweet, again the number of positive and negative tokens from this list, the total number of scored tokens, and the arithmetic mean are used as features. Negation Negation is not treated for the whole segment as NRC and GU-MLT-LT do but only on the next three tokens except the case that the punctuation comes earlier. Only negated word unigrams are used as an additional feature set. The polarity scores from the above dictionary are multiplied by −1 for terms up to 4 tokens after the negation. 3.4 TeamX TeamX (Miura et al., 2014) was ranked first in the SemEval 2014 ranking. The approach was inspired by NRC Canada’s 2013 method but uses fewer features and more polarity dictionaries—some differences are outlined below. Although it is very close to NRC Canada, some differences exist that justify TeamX’s selection for our ensemble—besides its good performance in SemEval 2014. Parts of speech Two different POS taggers are used: the Stanford POS tagger’s tags are used for the polarity dictionaries based on formal language and for word sense disambiguation while the CMU ARK POS tagger is used for the polarity dictionaries c</context>
<context position="20074" citStr="Miura et al., 2014" startWordPosition="3243" endWordPosition="3246">1-score for the positive and negative class only (as is done at SemEval). While the reimplemented NRC performance is slightly better, GU-MLT-LT and KLUE are substantially improved. That TeamX lost performance is probably due to a fact that we only recognized after the competition: The word sense feature was unintentionally not switched on in the re-implementation of TeamX. Since for this “handicapped” version of TeamX (again, we just noticed the reason for the handicap after the SemEval 2015 deadline) the weighting scheme of the classification probabilities proposed for the original approach (Miura et al., 2014) did decrease the performance, we also did not use these weights. If we would have noticed our mistake before, the performance of the TeamX classifier would probably have been better. Altogether, we conclude that reproducing the SemEval approaches was generally possible but involved some subtleties that sometimes lead to difficult design decisions. Our resolution is to maximize performance rather than to dogmatically stick to the original approach; even though this includes the error in the TeamX re-implementation that went through unnoticed until after the deadline. 3.6 Ensemble Combination I</context>
</contexts>
<marker>Miura, Sakaki, Hattori, Ohkuma, 2014</marker>
<rawString>Yasuhide Miura, Shigeyuki Sakaki, Keigo Hattori, and Tomoko Ohkuma. 2014. Teamx: A sentiment analyzer with enhanced lexicon mapping and weighting scheme for unbalanced data. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 628–632, Dublin, Ireland, August. Association for Computational Linguistics and Dublin City University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif M Mohammad</author>
<author>Peter D Turney</author>
</authors>
<title>Emotions evoked by common words and phrases: Using mechanical turk to create an emotion lexicon.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, CAAGET</booktitle>
<pages>26--34</pages>
<contexts>
<context position="9588" citStr="Mohammad and Turney, 2010" startWordPosition="1510" endWordPosition="1513">der. The tweets are then tokenized and POS-tagged. An SVM with linear kernel is trained using the following feature set. N-grams The occurrence of word 1- to 4-grams as well as occurrences of pairs of non-consecutive words where the intermediate words are replaced by a placeholder. No term-weighting like tf ·idf is used. Similarly for occurrence of character 3- to 5-grams. ALLCAPS Number of all-capitalized words. Parts of speech Occurrence of part-of-speech tags. Polarity dictionaries In total, five polarity dictionaries are used. Three of these were manually created: the NRC Emotion Lexicon (Mohammad and Turney, 2010; Mohammad and Turney, 2013) with 14,000 words, the MPQA Lexicon (Wilson et al., 2005) with 8,000 words, and the Bing Liu Lexicon (Hu and Liu, 2004) with 6,800 words. Two other dictionaries were created automatically. For the first one, the idea is that several hash tags can express sentiment (e.g., #good). Team NRC crawled 775,000 tweets from April to December 2012 that contain at least one of 32 positive or 38 negative hash tags that were manually created (e.g., #good and #bad). For word 1-grams and word 2-grams in the tweets, PMI-scores were calculated for each of the 70 hash tags to yield </context>
</contexts>
<marker>Mohammad, Turney, 2010</marker>
<rawString>Saif M. Mohammad and Peter D. Turney. 2010. Emotions evoked by common words and phrases: Using mechanical turk to create an emotion lexicon. In Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, CAAGET 2010, pages 26–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif M Mohammad</author>
<author>Peter D Turney</author>
</authors>
<title>Crowdsourcing a word-emotion association lexicon.</title>
<date>2013</date>
<journal>Computational Intelligence,</journal>
<volume>29</volume>
<issue>3</issue>
<contexts>
<context position="9616" citStr="Mohammad and Turney, 2013" startWordPosition="1514" endWordPosition="1517">kenized and POS-tagged. An SVM with linear kernel is trained using the following feature set. N-grams The occurrence of word 1- to 4-grams as well as occurrences of pairs of non-consecutive words where the intermediate words are replaced by a placeholder. No term-weighting like tf ·idf is used. Similarly for occurrence of character 3- to 5-grams. ALLCAPS Number of all-capitalized words. Parts of speech Occurrence of part-of-speech tags. Polarity dictionaries In total, five polarity dictionaries are used. Three of these were manually created: the NRC Emotion Lexicon (Mohammad and Turney, 2010; Mohammad and Turney, 2013) with 14,000 words, the MPQA Lexicon (Wilson et al., 2005) with 8,000 words, and the Bing Liu Lexicon (Hu and Liu, 2004) with 6,800 words. Two other dictionaries were created automatically. For the first one, the idea is that several hash tags can express sentiment (e.g., #good). Team NRC crawled 775,000 tweets from April to December 2012 that contain at least one of 32 positive or 38 negative hash tags that were manually created (e.g., #good and #bad). For word 1-grams and word 2-grams in the tweets, PMI-scores were calculated for each of the 70 hash tags to yield a score for the n-grams (i.e</context>
</contexts>
<marker>Mohammad, Turney, 2013</marker>
<rawString>Saif M. Mohammad and Peter D. Turney. 2013. Crowdsourcing a word-emotion association lexicon. Computational Intelligence, 29(3):436–465.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif M Mohammad</author>
<author>Svetlana Kiritchenko</author>
<author>Xiaodan Zhu</author>
</authors>
<title>NRC-Canada: Building the state-of-theart in sentiment analysis of tweets.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>321--327</pages>
<contexts>
<context position="7317" citStr="Mohammad et al., 2013" startWordPosition="1144" endWordPosition="1147">traightforward approach performs superior to its individual parts and performs competitive in the SemEval competitions. Thus, its sentiment detection results can be directly used in any of the above use cases for Twitter sentiment detection. 3 Individual Approaches and Ensemble For our ECIR 2015 reproducibility paper (Hagen et al., 2015), we originally selected three state-ofthe-art approaches for Twitter sentiment detection among the 38 participants of SemEval 2013. To identify worthy candidates—and to satisfy the claim “state of the art”—we picked the top-ranked approach by team NRC-Canada (Mohammad et al., 2013). However, instead of simply picking the approaches on ranks two and three to complete our set, we first analyzed the notebooks of the top-ranked teams in order to identify approaches that are significantly dissimilar from NRC-Canada. We decided to handpick approaches this way so they complement each other in an ensemble. As a second candidate, we picked team GU-MLT-LT (Günther and Furrer, 2013) since it uses some other features and a different sentiment lexicon. As a third candidate, we picked team KLUE (Proisl et al., 2013), which was ranked fifth. We discarded the third-ranked approach as i</context>
<context position="8834" citStr="Mohammad et al., 2013" startWordPosition="1388" endWordPosition="1391">4) as the 2014 top-performing approach resulting in an ensemble of four. 583 Note that due to the selection process, reproducing the four approaches does not deteriorate into reimplementing the feature set of one approach and reusing it for the other two. Moreover, combining the four approaches into an ensemble classifier actually makes sense, since, due to the feature set diversity, they tap sufficiently different information sources. In what follows, we first briefly recap the features used by the individual classifiers and then explain our ensemble strategy. 3.1 NRC-Canada Team NRC-Canada (Mohammad et al., 2013) used a classifier with a wide range of features. A tweet is first preprocessed by replacing URLs and user names by some placeholder. The tweets are then tokenized and POS-tagged. An SVM with linear kernel is trained using the following feature set. N-grams The occurrence of word 1- to 4-grams as well as occurrences of pairs of non-consecutive words where the intermediate words are replaced by a placeholder. No term-weighting like tf ·idf is used. Similarly for occurrence of character 3- to 5-grams. ALLCAPS Number of all-capitalized words. Parts of speech Occurrence of part-of-speech tags. Pol</context>
</contexts>
<marker>Mohammad, Kiritchenko, Zhu, 2013</marker>
<rawString>Saif M. Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu. 2013. NRC-Canada: Building the state-of-theart in sentiment analysis of tweets. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 321–327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Zornitsa Kozareva</author>
<author>Alan Ritter</author>
<author>Sara Rosenthal</author>
<author>Veselin Stoyanov</author>
<author>Theresa Wilson</author>
</authors>
<title>Semeval-2013 task 2: Sentiment analysis in twitter.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>312--320</pages>
<contexts>
<context position="4754" citStr="Nakov et al., 2013" startWordPosition="749" endWordPosition="752">4). The task usually is to detect the sentiment expressed in a tweet as a whole (also focus of this paper). But it can also be used to identify the sentiment in a tweet with respect to a given target concept expressed in a query (Jiang et al., 2011). The difference is that a generally negative tweet might not say anything about the target concept and must thus be considered neutral with respect to the target concept. Both tasks, namely sentiment detection in a tweet, and sentiment detection with respect to a specific target concept, are part of the SemEval sentiment analysis tasks since 2013 (Nakov et al., 2013; Rosenthal et al., 2014). SemEval fosters research on sentiment detection for short texts in particular, and gathers the best-performing approaches in a friendly competition. The problem we are dealing with is formulated as subtask B: given a tweet, decide whether its message is positive, negative, or neutral. State-of-the-art approaches have been submitted to the SemEval tasks. However, up to now, no one had trained a meta-classifier based on the submitted approaches to determine what can be achieved when combining them, whereas each participating team only trains their individual classifier</context>
</contexts>
<marker>Nakov, Kozareva, Ritter, Rosenthal, Stoyanov, Wilson, 2013</marker>
<rawString>Preslav Nakov, Zornitsa Kozareva, Alan Ritter, Sara Rosenthal, Veselin Stoyanov, and Theresa Wilson. 2013. Semeval-2013 task 2: Sentiment analysis in twitter. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 312–320.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Finn Årup Nielsen</author>
</authors>
<title>A new ANEW: evaluation of a word list for sentiment analysis in microblogs.</title>
<date>2011</date>
<booktitle>In Proceedings of the ESWC2011 Workshop on ’Making Sense of Microposts’: Big things come in small packages, Heraklion,</booktitle>
<pages>93--98</pages>
<location>Crete, Greece,</location>
<contexts>
<context position="14132" citStr="Nielsen, 2011" startWordPosition="2270" endWordPosition="2271">grams Word unigrams and bigrams are used as features but in contrast to NRC and GU-MLT-LT not just by occurrence but frequency-weighted. Due to the short tweet length this however often boils down to a simple occurrence feature. To be part of the feature set, an n-gram has to be contained in at least five tweets. This excludes some rather obscure and rare terms or misspellings. Length The number of tokens in a tweet (i.e., its length) is used as a feature. Interestingly, NRC and GU-MLT-LT do not explicitly use this feature. Polarity dictionary The employed dictionary is the AFINN-111 lexicon (Nielsen, 2011) containing 2,447 words with assessments from −5 (very negative) to +5 (very positive). Team KLUE added another 343 words. Employed features are the number of positive tokens in a tweet, the number of negative tokens, the number of tokens with a dictionary score, and the arithmetic mean of the scores in a tweet. Emoticons and abbreviations A list of 212 emoticons and 95 colloquial abbreviations from Wikipedia was manually scored as positive, negative, or neutral. For a tweet, again the number of positive and negative tokens from this list, the total number of scored tokens, and the arithmetic </context>
</contexts>
<marker>Nielsen, 2011</marker>
<rawString>Finn Årup Nielsen. 2011. A new ANEW: evaluation of a word list for sentiment analysis in microblogs. In Proceedings of the ESWC2011 Workshop on ’Making Sense of Microposts’: Big things come in small packages, Heraklion, Crete, Greece, May 30, 2011, pages 93–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David W Opitz</author>
<author>Richard Maclin</author>
</authors>
<title>Popular ensemble methods: An empirical study.</title>
<date>1999</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>11--169</pages>
<contexts>
<context position="5747" citStr="Opitz and Maclin, 1999" startWordPosition="903" endWordPosition="906">Eval tasks. However, up to now, no one had trained a meta-classifier based on the submitted approaches to determine what can be achieved when combining them, whereas each participating team only trains their individual classifier using respective individual feature sets. Our idea is to combine four of the best-performing approaches from the last years with different feature sets, and to form an ensemble classifier that leverages the individual classifiers’ strengths forming a strong baseline. Ensemble learning is a classic approach of combining several classifiers to a more powerful ensemble (Opitz and Maclin, 1999; Polikar, 2006; Rokach, 2010). The classic approaches of Bagging (Breiman, 1996) and Boosting (Schapire, 1990; Freund and Schapire, 1996) try to either combine the outputs of different classifiers trained on different random instances of the training set or on training the classifiers on instances that were misclassified by the other classifiers. Both rather work on the final predictions of the classifiers just as for instance averaging or majority voting on the predictions (Asker and Maclin, 1997) would do. In our case, we employ the confidence scores of the participating classifiers. Severa</context>
</contexts>
<marker>Opitz, Maclin, 1999</marker>
<rawString>David W. Opitz and Richard Maclin. 1999. Popular ensemble methods: An empirical study. Journal of Artificial Intelligence Research, 11:169–198.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Olutobi Owoputi</author>
<author>Brendan O’Connor</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Improved part-of-speech tagging for online conversational text with word clusters.</title>
<date>2013</date>
<booktitle>In Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings,</booktitle>
<pages>380--390</pages>
<location>Atlanta, Georgia, USA,</location>
<marker>Owoputi, O’Connor, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A. Smith. 2013. Improved part-of-speech tagging for online conversational text with word clusters. In Human Language Technologies: Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings, June 9-14, 2013, Westin Peachtree Plaza Hotel, Atlanta, Georgia, USA, pages 380–390.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shiuvakumar Vaithyanathan</author>
</authors>
<title>Thumbs up?: Sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP</booktitle>
<pages>79--86</pages>
<contexts>
<context position="3313" citStr="Pang et al., 2002" startWordPosition="517" endWordPosition="520">he other participants is not included in this paper since it can be found in the task overview (Rosenthal et al., 2015). 2 Related Work Sentiment detection is a classic problem of text classification. Unlike other text classification tasks, the goal is not to identify topics, entities, or authors of a text but to rate the expressed sentiment as positive, negative, or neutral. Most approaches used for sentiment detection usually involve methods from machine learning, computational linguistics, and statistics. Typically, several approaches from these fields are combined for sentiment detection (Pang et al., 2002; Turney, 2002; Feldman, 2013). Since Twitter is one of the richest sources of opinion, a lot of different approaches to sentiment de582 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 582–589, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics tection in tweets have been proposed. Different approaches use different feature sets ranging from standard word polarity expressions or unigram features also applied in general sentiment detection (Go et al., 2009; Kouloumpis et al., 2011), to the usage of emoticons and uppercas</context>
<context position="12012" citStr="Pang et al., 2002" startWordPosition="1923" endWordPosition="1926">ity, and whether the last token of a tweet is an emoticon are employed features. Word lengthening The number of words that are lengthened by repeating a letter more than twice (e.g., cooooolll) is a feature. Clustering Via unsupervised Brown clustering (Brown et al., 1992) a set of 56,345,753 tweets by Owoputi (Owoputi et al., 2013) clustered into 1,000 clusters. The IDs of the clusters in which the terms of a tweet occur are also used as features. Negation The number of negated segments is a feature. A negated segment starts with a negation (e.g., shouldn’t) and ends with a punctuation mark (Pang et al., 2002). Every token in a negated segment (words, emoticons) gets a suffix NEG attached (e.g., perfect_NEG). 3.2 GU-MLT-LT Team GU-MLT-LT (Günther and Furrer, 2013) was ranked second in SemEval 2013. They train a stochastic gradient decent classifier on a much smaller feature set compared to NRC. The following feature set is computed for tokenized versions of the original raw tweet, a lowercased normalized version of the tweet, and a version of the lowercased tweet where consecutive identical letters are collapsed (e.g., helllo gets hello). 584 Normalized unigrams The occurrence of the normalized wor</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shiuvakumar Vaithyanathan. 2002. Thumbs up?: Sentiment classification using machine learning techniques. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP 2002, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robi Polikar</author>
</authors>
<title>Ensemble based systems in decision making.</title>
<date>2006</date>
<booktitle>IEEE Circuits and Systems Magazine,</booktitle>
<pages>6--3</pages>
<contexts>
<context position="5762" citStr="Polikar, 2006" startWordPosition="907" endWordPosition="908">to now, no one had trained a meta-classifier based on the submitted approaches to determine what can be achieved when combining them, whereas each participating team only trains their individual classifier using respective individual feature sets. Our idea is to combine four of the best-performing approaches from the last years with different feature sets, and to form an ensemble classifier that leverages the individual classifiers’ strengths forming a strong baseline. Ensemble learning is a classic approach of combining several classifiers to a more powerful ensemble (Opitz and Maclin, 1999; Polikar, 2006; Rokach, 2010). The classic approaches of Bagging (Breiman, 1996) and Boosting (Schapire, 1990; Freund and Schapire, 1996) try to either combine the outputs of different classifiers trained on different random instances of the training set or on training the classifiers on instances that were misclassified by the other classifiers. Both rather work on the final predictions of the classifiers just as for instance averaging or majority voting on the predictions (Asker and Maclin, 1997) would do. In our case, we employ the confidence scores of the participating classifiers. Several papers descri</context>
</contexts>
<marker>Polikar, 2006</marker>
<rawString>Robi Polikar. 2006. Ensemble based systems in decision making. IEEE Circuits and Systems Magazine, 6(3):21–45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Porter</author>
</authors>
<title>An algorithm for suffix stripping. Program: electronic library and information systems,</title>
<date>1980</date>
<pages>14--3</pages>
<contexts>
<context position="12732" citStr="Porter, 1980" startWordPosition="2041" endWordPosition="2042">U-MLT-LT Team GU-MLT-LT (Günther and Furrer, 2013) was ranked second in SemEval 2013. They train a stochastic gradient decent classifier on a much smaller feature set compared to NRC. The following feature set is computed for tokenized versions of the original raw tweet, a lowercased normalized version of the tweet, and a version of the lowercased tweet where consecutive identical letters are collapsed (e.g., helllo gets hello). 584 Normalized unigrams The occurrence of the normalized word unigrams is one feature set. No term weighting like for instance tf ·idf is used. Stems Porter stemming (Porter, 1980) is used to identify the occurrence of the stems of the collapsed word unigrams as another feature set. Again, no term weighting is applied. Clustering Similar to NRC, the cluster IDs of the raw, normalized, and collapsed tokens are features. Polarity dictionary The SentiWordNet assessments (Baccianella et al., 2010) of the individual collapsed tokens and the sum of all tokens’ scores in a tweet are further features. Negation Normalized tokens and stems are added as negated features similar to NRC. 3.3 KLUE Team KLUE (Proisl et al., 2013) was ranked fifth in the SemEval 2013 ranking. Similarly</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>Martin Porter. 1980. An algorithm for suffix stripping. Program: electronic library and information systems, 14(3):130–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Proisl</author>
<author>Paul Greiner</author>
<author>Stefan Evert</author>
<author>Besim Kabashi</author>
</authors>
<title>Klue: Simple and robust methods for polarity classification.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>395--401</pages>
<contexts>
<context position="7848" citStr="Proisl et al., 2013" startWordPosition="1234" endWordPosition="1237"> of the art”—we picked the top-ranked approach by team NRC-Canada (Mohammad et al., 2013). However, instead of simply picking the approaches on ranks two and three to complete our set, we first analyzed the notebooks of the top-ranked teams in order to identify approaches that are significantly dissimilar from NRC-Canada. We decided to handpick approaches this way so they complement each other in an ensemble. As a second candidate, we picked team GU-MLT-LT (Günther and Furrer, 2013) since it uses some other features and a different sentiment lexicon. As a third candidate, we picked team KLUE (Proisl et al., 2013), which was ranked fifth. We discarded the third-ranked approach as it is using a large set of not publicly available rules probably hindering reproducibility, whereas the fourth-ranked system seemed too similar to NRC and GU-MLT-LT to add something new to the planned ensemble. Finally, for participation in SemEval 2015, we also included TeamX (Miura et al., 2014) as the 2014 top-performing approach resulting in an ensemble of four. 583 Note that due to the selection process, reproducing the four approaches does not deteriorate into reimplementing the feature set of one approach and reusing it</context>
<context position="13276" citStr="Proisl et al., 2013" startWordPosition="2128" endWordPosition="2131">ting like for instance tf ·idf is used. Stems Porter stemming (Porter, 1980) is used to identify the occurrence of the stems of the collapsed word unigrams as another feature set. Again, no term weighting is applied. Clustering Similar to NRC, the cluster IDs of the raw, normalized, and collapsed tokens are features. Polarity dictionary The SentiWordNet assessments (Baccianella et al., 2010) of the individual collapsed tokens and the sum of all tokens’ scores in a tweet are further features. Negation Normalized tokens and stems are added as negated features similar to NRC. 3.3 KLUE Team KLUE (Proisl et al., 2013) was ranked fifth in the SemEval 2013 ranking. Similarly to NRC, team KLUE first replaces URLs and user names by some placeholder and tokenizes the lowercased tweets. A maximum entropy-based classifier is trained on the following features. N-grams Word unigrams and bigrams are used as features but in contrast to NRC and GU-MLT-LT not just by occurrence but frequency-weighted. Due to the short tweet length this however often boils down to a simple occurrence feature. To be part of the feature set, an n-gram has to be contained in at least five tweets. This excludes some rather obscure and rare </context>
</contexts>
<marker>Proisl, Greiner, Evert, Kabashi, 2013</marker>
<rawString>Thomas Proisl, Paul Greiner, Stefan Evert, and Besim Kabashi. 2013. Klue: Simple and robust methods for polarity classification. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 395–401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lior Rokach</author>
<author>Alon Schclar</author>
<author>Ehud Itach</author>
</authors>
<title>Ensemble methods for multi-label classification. Expert Systems with Applications,</title>
<date>2014</date>
<pages>41--16</pages>
<contexts>
<context position="6580" citStr="Rokach et al., 2014" startWordPosition="1032" endWordPosition="1035">erent random instances of the training set or on training the classifiers on instances that were misclassified by the other classifiers. Both rather work on the final predictions of the classifiers just as for instance averaging or majority voting on the predictions (Asker and Maclin, 1997) would do. In our case, we employ the confidence scores of the participating classifiers. Several papers describe different ways of working with the classifiers’ confidence scores, such as learning a dynamic confidence weighting scheme (Fung et al., 2006), or deriving a set cover with averaging confidences (Rokach et al., 2014). Instead, we simply average the three confidence scores of the three classifiers for each individual class. This straightforward approach performs superior to its individual parts and performs competitive in the SemEval competitions. Thus, its sentiment detection results can be directly used in any of the above use cases for Twitter sentiment detection. 3 Individual Approaches and Ensemble For our ECIR 2015 reproducibility paper (Hagen et al., 2015), we originally selected three state-ofthe-art approaches for Twitter sentiment detection among the 38 participants of SemEval 2013. To identify w</context>
<context position="22352" citStr="Rokach et al., 2014" startWordPosition="3615" endWordPosition="3618">raining set behave for tweets in the development set. Typically, not the four final decisions but the respective confidences or probabilities of the individual classifiers give a good hint on uncertainties. If two are not really sure about the final classification, sometimes the remaining ones favor another class with high confidence. Thus, instead of looking at the classifications, we decided to use the confidence scores or probabilities to build the ensemble. This approach is also motivated by old and also more recent research on ensemble learning (Asker and Maclin, 1997; Fung et al., 2006; Rokach et al., 2014). But instead of learning a weighting scheme for the different individual classifiers, we decided to simply compute the average probability of the four classifiers for each of the three classes (positive, negative, neutral). Our ensemble thus works as follows. The four individual re-implementations of the TeamX, the NRC, the GU-MLT-LT, and the KLUE classifier are individually trained on the SemEval 2013 training and development set as if being applied individually—without boosting or bagging. As for the classification of a tweet, the ensemble ignores the individual classifiers’ classification </context>
</contexts>
<marker>Rokach, Schclar, Itach, 2014</marker>
<rawString>Lior Rokach, Alon Schclar, and Ehud Itach. 2014. Ensemble methods for multi-label classification. Expert Systems with Applications, 41(16):7507–7523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lior Rokach</author>
</authors>
<title>Ensemble-based classifiers.</title>
<date>2010</date>
<journal>Artificial Intelligence Review,</journal>
<pages>33--1</pages>
<contexts>
<context position="5777" citStr="Rokach, 2010" startWordPosition="909" endWordPosition="910">had trained a meta-classifier based on the submitted approaches to determine what can be achieved when combining them, whereas each participating team only trains their individual classifier using respective individual feature sets. Our idea is to combine four of the best-performing approaches from the last years with different feature sets, and to form an ensemble classifier that leverages the individual classifiers’ strengths forming a strong baseline. Ensemble learning is a classic approach of combining several classifiers to a more powerful ensemble (Opitz and Maclin, 1999; Polikar, 2006; Rokach, 2010). The classic approaches of Bagging (Breiman, 1996) and Boosting (Schapire, 1990; Freund and Schapire, 1996) try to either combine the outputs of different classifiers trained on different random instances of the training set or on training the classifiers on instances that were misclassified by the other classifiers. Both rather work on the final predictions of the classifiers just as for instance averaging or majority voting on the predictions (Asker and Maclin, 1997) would do. In our case, we employ the confidence scores of the participating classifiers. Several papers describe different wa</context>
</contexts>
<marker>Rokach, 2010</marker>
<rawString>Lior Rokach. 2010. Ensemble-based classifiers. Artificial Intelligence Review, 33(1-2):1–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Rosenthal</author>
<author>Alan Ritter</author>
<author>Preslav Nakov</author>
<author>Veselin Stoyanov</author>
</authors>
<title>Semeval-2014 task 9: Sentiment analysis in twitter.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>73--80</pages>
<contexts>
<context position="4779" citStr="Rosenthal et al., 2014" startWordPosition="753" endWordPosition="756"> is to detect the sentiment expressed in a tweet as a whole (also focus of this paper). But it can also be used to identify the sentiment in a tweet with respect to a given target concept expressed in a query (Jiang et al., 2011). The difference is that a generally negative tweet might not say anything about the target concept and must thus be considered neutral with respect to the target concept. Both tasks, namely sentiment detection in a tweet, and sentiment detection with respect to a specific target concept, are part of the SemEval sentiment analysis tasks since 2013 (Nakov et al., 2013; Rosenthal et al., 2014). SemEval fosters research on sentiment detection for short texts in particular, and gathers the best-performing approaches in a friendly competition. The problem we are dealing with is formulated as subtask B: given a tweet, decide whether its message is positive, negative, or neutral. State-of-the-art approaches have been submitted to the SemEval tasks. However, up to now, no one had trained a meta-classifier based on the submitted approaches to determine what can be achieved when combining them, whereas each participating team only trains their individual classifier using respective individ</context>
</contexts>
<marker>Rosenthal, Ritter, Nakov, Stoyanov, 2014</marker>
<rawString>Sara Rosenthal, Alan Ritter, Preslav Nakov, and Veselin Stoyanov. 2014. Semeval-2014 task 9: Sentiment analysis in twitter. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 73–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Rosenthal</author>
<author>Preslav Nakov</author>
<author>Svetlana Kiritchenko</author>
<author>Saif M Mohammad</author>
<author>Alan Ritter</author>
<author>Veselin Stoyanov</author>
</authors>
<title>Semeval-2015 task 10: Sentiment analysis in twitter.</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval 2015, SemEval 2015,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Denver, Colorado,</location>
<contexts>
<context position="2815" citStr="Rosenthal et al., 2015" startWordPosition="441" endWordPosition="444"> task 9 is shown to form a very strong baseline that was not outperformed by the SemEval 2015 participants on the 2015 test data and that also places in the top-10 in the progress test. In Section 2 we briefly describe some related work while in Section 3 we provide more details on the four individual approaches as well as our ensemble scheme. Some concluding remarks and an outlook on future work close the paper in Section 4. An experimental evaluation of our approach and an in-depth comparison to the other participants is not included in this paper since it can be found in the task overview (Rosenthal et al., 2015). 2 Related Work Sentiment detection is a classic problem of text classification. Unlike other text classification tasks, the goal is not to identify topics, entities, or authors of a text but to rate the expressed sentiment as positive, negative, or neutral. Most approaches used for sentiment detection usually involve methods from machine learning, computational linguistics, and statistics. Typically, several approaches from these fields are combined for sentiment detection (Pang et al., 2002; Turney, 2002; Feldman, 2013). Since Twitter is one of the richest sources of opinion, a lot of diffe</context>
</contexts>
<marker>Rosenthal, Nakov, Kiritchenko, Mohammad, Ritter, Stoyanov, 2015</marker>
<rawString>Sara Rosenthal, Preslav Nakov, Svetlana Kiritchenko, Saif M Mohammad, Alan Ritter, and Veselin Stoyanov. 2015. Semeval-2015 task 10: Sentiment analysis in twitter. In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval 2015, SemEval 2015, Denver, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert E Schapire</author>
</authors>
<title>The strength of weak learnability.</title>
<date>1990</date>
<booktitle>Machine Learning,</booktitle>
<pages>5--197</pages>
<contexts>
<context position="5857" citStr="Schapire, 1990" startWordPosition="921" endWordPosition="922">at can be achieved when combining them, whereas each participating team only trains their individual classifier using respective individual feature sets. Our idea is to combine four of the best-performing approaches from the last years with different feature sets, and to form an ensemble classifier that leverages the individual classifiers’ strengths forming a strong baseline. Ensemble learning is a classic approach of combining several classifiers to a more powerful ensemble (Opitz and Maclin, 1999; Polikar, 2006; Rokach, 2010). The classic approaches of Bagging (Breiman, 1996) and Boosting (Schapire, 1990; Freund and Schapire, 1996) try to either combine the outputs of different classifiers trained on different random instances of the training set or on training the classifiers on instances that were misclassified by the other classifiers. Both rather work on the final predictions of the classifiers just as for instance averaging or majority voting on the predictions (Asker and Maclin, 1997) would do. In our case, we employ the confidence scores of the participating classifiers. Several papers describe different ways of working with the classifiers’ confidence scores, such as learning a dynami</context>
</contexts>
<marker>Schapire, 1990</marker>
<rawString>Robert E. Schapire. 1990. The strength of weak learnability. Machine Learning, 5:197–227.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Duyu Tang</author>
<author>Furu Wei</author>
<author>Nan Yang</author>
<author>Ming Zhou</author>
<author>Ting Liu</author>
<author>Bing Qin</author>
</authors>
<title>Learning sentiment-specific word embedding for twitter sentiment classification.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL</booktitle>
<volume>Volume</volume>
<pages>1555--1565</pages>
<location>Baltimore, MD, USA,</location>
<contexts>
<context position="4138" citStr="Tang et al., 2014" startWordPosition="637" endWordPosition="640">SemEval 2015), pages 582–589, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics tection in tweets have been proposed. Different approaches use different feature sets ranging from standard word polarity expressions or unigram features also applied in general sentiment detection (Go et al., 2009; Kouloumpis et al., 2011), to the usage of emoticons and uppercases (Barbosa and Feng, 2010), word lengthening (Brody and Diakopoulos, 2011), phonetic features (Ermakov and Ermakova, 2013), multi-lingual machine translation (Balahur and Turchi, 2013), or word embeddings (Tang et al., 2014). The task usually is to detect the sentiment expressed in a tweet as a whole (also focus of this paper). But it can also be used to identify the sentiment in a tweet with respect to a given target concept expressed in a query (Jiang et al., 2011). The difference is that a generally negative tweet might not say anything about the target concept and must thus be considered neutral with respect to the target concept. Both tasks, namely sentiment detection in a tweet, and sentiment detection with respect to a specific target concept, are part of the SemEval sentiment analysis tasks since 2013 (Na</context>
</contexts>
<marker>Tang, Wei, Yang, Zhou, Liu, Qin, 2014</marker>
<rawString>Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting Liu, and Bing Qin. 2014. Learning sentiment-specific word embedding for twitter sentiment classification. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014, June 22-27, 2014, Baltimore, MD, USA, Volume 1: Long Papers, pages 1555–1565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, ACL 2002,J uly 6-12,</booktitle>
<pages>417--424</pages>
<location>Philadelphia, PA, USA.,</location>
<contexts>
<context position="3327" citStr="Turney, 2002" startWordPosition="521" endWordPosition="522">ts is not included in this paper since it can be found in the task overview (Rosenthal et al., 2015). 2 Related Work Sentiment detection is a classic problem of text classification. Unlike other text classification tasks, the goal is not to identify topics, entities, or authors of a text but to rate the expressed sentiment as positive, negative, or neutral. Most approaches used for sentiment detection usually involve methods from machine learning, computational linguistics, and statistics. Typically, several approaches from these fields are combined for sentiment detection (Pang et al., 2002; Turney, 2002; Feldman, 2013). Since Twitter is one of the richest sources of opinion, a lot of different approaches to sentiment de582 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 582–589, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics tection in tweets have been proposed. Different approaches use different feature sets ranging from standard word polarity expressions or unigram features also applied in general sentiment detection (Go et al., 2009; Kouloumpis et al., 2011), to the usage of emoticons and uppercases (Barbosa an</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter D. Turney. 2002. Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, ACL 2002,J uly 6-12, 2002, Philadelphia, PA, USA., pages 417–424.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In HLT/EMNLP 2005, Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference,</booktitle>
<pages>6--8</pages>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="9674" citStr="Wilson et al., 2005" startWordPosition="1524" endWordPosition="1527">ng the following feature set. N-grams The occurrence of word 1- to 4-grams as well as occurrences of pairs of non-consecutive words where the intermediate words are replaced by a placeholder. No term-weighting like tf ·idf is used. Similarly for occurrence of character 3- to 5-grams. ALLCAPS Number of all-capitalized words. Parts of speech Occurrence of part-of-speech tags. Polarity dictionaries In total, five polarity dictionaries are used. Three of these were manually created: the NRC Emotion Lexicon (Mohammad and Turney, 2010; Mohammad and Turney, 2013) with 14,000 words, the MPQA Lexicon (Wilson et al., 2005) with 8,000 words, and the Bing Liu Lexicon (Hu and Liu, 2004) with 6,800 words. Two other dictionaries were created automatically. For the first one, the idea is that several hash tags can express sentiment (e.g., #good). Team NRC crawled 775,000 tweets from April to December 2012 that contain at least one of 32 positive or 38 negative hash tags that were manually created (e.g., #good and #bad). For word 1-grams and word 2-grams in the tweets, PMI-scores were calculated for each of the 70 hash tags to yield a score for the n-grams (i.e., the ones with higher positive hash tag PMI are positive</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In HLT/EMNLP 2005, Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference, 6-8 October 2005, Vancouver, British Columbia, Canada, pages 347– 354.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>