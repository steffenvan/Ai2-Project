<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.464258">
<title confidence="0.985751">
Invited Talk:
Counting Rankings
</title>
<author confidence="0.99867">
Jason Riggle
</author>
<affiliation confidence="0.998574">
University of Chicago
</affiliation>
<email confidence="0.999198">
jriggle@uchicago.edu
</email>
<sectionHeader confidence="0.993908" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999705076923077">
In this talk, I present a recursive algorithm to calculate the number of rankings that are consistent with a
set of data (optimal candidates) in the framework of Optimality Theory (OT; Prince and Smolensky 1993).1
Computing this quantity, which I call r-volume, makes possible a simple and effective Bayesian heuristic in
learning – all else equal, choose candidates that are preferred by the highest number of rankings consistent
with previous observations. This heuristic yields an r-volume learning algorithm (RVL) that is guaranteed
to make fewer than k lg k errors while learning rankings of k constraints. This log-linear error bound is
an improvement over the quadratic bound of Recursive Constraint Demotion (RCD; Tesar and Smolensky
1996) and it is within a logarithmic factor of the best possible mistake bound for any OT learning algorithm.
Computing r-volume: The violations in an OT tableau can be given as a [n x k] array of integers in
which the Þrst row t1 corresponds to the winner. Following Prince (2002), the ranking information can be
extracted by comparing t1 with each ‘losing’ row t2, ..., to to create an Elementary Ranking Condition as
follows: erc(t1, ti) = (α1, ..., αk) where αj = L if t1j &lt; tij, αj = W if t1j &gt; tij, and αj = e otherwise.
The meaning of α is that at least one constraint associated with W dominates all those associated with L.
</bodyText>
<equation confidence="0.882415666666667">
input C1 C2 C3
candidate t1 * **
candidate t2 ** *
candidate t3 **
candidate t4 *** *
winner
erc(t1, t2) = (W, L, e) i.e. t1 beats t2 if C1 outranks C2
erc(t1, t3) = (L, L, W) i.e. t1 beats t3 if C3 outranks C1 and C2
erc(t1, t4) = (L, W, W) i.e. t1 beats t4 if C2 or C3 outranks C1
</equation>
<bodyText confidence="0.998872642857143">
For a set E of length-k ERCs, E−wi denotes r-vol(Ek)= � ⎧ 0 if xi = L for any x E E
a set E&apos; derived from E by removing ERCs 1≤i≤k ⎨ (k − 1)! if xi = W for all x E E
with W in column i and removing column i. ⎩ r (E − wi) otherwise
Mistake bounds: To make predictions, RVL selects in each tableau the candidate that yields the highest
r-volume when the ERCs that allow it to win are combined with E (the ERCs for past winners). To establish
a mistake bound, assume that the RVL chooses candidate e when, in fact, candidate o was optimal according
to the target ranking RT. Assuming e =� o, the rankings that make o optimal must be half or fewer of the
rankings consistent with E or else RVL would have chosen o. Because all rankings that make candidates
other than o optimal will be eliminated once the ERCs for o are added to E, each error reduces the number
of rankings consistent with all observed data by at least half and thus there can be no more than lg k! errors.
Applications: The r-volume seems to encode ‘restrictiveness’ in a way similar to Tesar and Prince’s
(1999) r-measure. As a factor in learning, it predicts typological frequency (cf. Bane and Riggle 2008) and
priors other than the ‘ßat’ distribution over rankings can easily be included to test models of ranking bias.
More generally, this research suggests the concept of g-volume for any parameterized model of grammar.
</bodyText>
<footnote confidence="0.900035">
1Full bibliography available on the Rutgers Optimality Archive (roa.rutgers.edu) with the paper Counting Rankings.
</footnote>
<page confidence="0.97257">
28
</page>
<reference confidence="0.824239">
Proceedings of the Tenth Meeting of the ACL Special Interest Group on Computational Morphology and Phonology, page 28,
Columbus, Ohio, USA June 2008. c�2008 Association for Computational Linguistics
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.315050">
<title confidence="0.961823">Invited Talk: Counting Rankings</title>
<author confidence="0.999993">Jason Riggle</author>
<affiliation confidence="0.999962">University of Chicago</affiliation>
<email confidence="0.999786">jriggle@uchicago.edu</email>
<abstract confidence="0.989392605263158">this talk, I present a recursive algorithm to calculate the rankings that are consistent with a of data (optimal candidates) in the framework of Optimality Theory (OT; Prince and Smolensky this quantity, which I call makes possible a simple and effective Bayesian heuristic in – else equal, choose candidates that are preferred by the highest number of rankings consistent previous This heuristic yields an learning algorithm (RVL) that is guaranteed make fewer than while learning rankings of This log-linear error bound is an improvement over the quadratic bound of Recursive Constraint Demotion (RCD; Tesar and Smolensky 1996) and it is within a logarithmic factor of the best possible mistake bound for any OT learning algorithm. violations in an OT tableau can be given as a array of integers in the row to the winner. Following Prince (2002), the ranking information can be by comparing each ‘losing’ row ..., to create an Elementary Ranking Condition as = ..., and meaning of that at least one constraint associated with all those associated with input * ** ** * ** *** * winner = = = a set set derived from removing ERCs column removing column ⎧ ⎨ ⎩ bounds: make predictions, RVL selects in each tableau the candidate that yields the highest when the ERCs that allow it to win are combined with ERCs for past winners). To establish mistake bound, assume that the RVL chooses candidate in fact, candidate optimal according the target ranking Assuming the rankings that make must be half or fewer of the consistent with else RVL would have chosen Because all rankings that make candidates than will be eliminated once the ERCs for added to each error reduces the number rankings consistent with all observed data by at least half and thus there can be no more than seems to encode ‘restrictiveness’ in a way similar to Tesar and Prince’s As a factor in learning, it predicts typological frequency (cf. Bane and Riggle 2008) and other than the distribution over rankings can easily be included to test models of ranking bias. generally, this research suggests the concept of for any parameterized model of grammar. bibliography available on the Rutgers Optimality Archive with the paper Counting Rankings.</abstract>
<note confidence="0.807375">28 of the Tenth Meeting of the ACL Special Interest Group on Computational Morphology and page 28, Ohio, USA June 2008. Association for Computational Linguistics</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<booktitle>Proceedings of the Tenth Meeting of the ACL Special Interest Group on Computational Morphology and Phonology,</booktitle>
<pages>28</pages>
<marker></marker>
<rawString>Proceedings of the Tenth Meeting of the ACL Special Interest Group on Computational Morphology and Phonology, page 28,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Columbus</author>
</authors>
<date>2008</date>
<booktitle>c�2008 Association for Computational Linguistics</booktitle>
<location>Ohio, USA</location>
<marker>Columbus, 2008</marker>
<rawString>Columbus, Ohio, USA June 2008. c�2008 Association for Computational Linguistics</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>