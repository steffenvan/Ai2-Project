<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.161918">
<title confidence="0.987683">
Keynote Address:
More Words and Bigger Pictures
</title>
<author confidence="0.997298">
David Forsyth
</author>
<affiliation confidence="0.854838">
University of Illinois
Urbana-Champaign
</affiliation>
<email confidence="0.996994">
daf@illinois.edu
</email>
<sectionHeader confidence="0.99554" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999455458333333">
Object recognition is a little like translation: a pic-
ture (text in a source language) goes in, and a de-
scription (text in a target language) comes out. I
will use this analogy, which has proven fertile, to
describe recent progress in object recognition.
We have very good methods to spot some objects
in images, but extending these methods to produce
descriptions of images remains very difficult. The
description might come in the form of a set of words,
indicating objects, and boxes or regions spanned by
the object. This representation is difficult to work
with, because some objects seem to be much more
important than others, and because objects interact.
An alternative is a sentence or a paragraph describ-
ing the picture, and recent work indicates how one
might generate rich structures like this. Further-
more, recent work suggests that it is easier and more
effective to generate descriptions of images in terms
of chunks of meaning (”person on a horse”) rather
than just objects (”person”; ”horse”).
Finally, if the picture contains objects that are un-
familiar, then we need to generate useful descrip-
tions that will make it possible to interact with them,
even though we don’t know what they are.
</bodyText>
<subsectionHeader confidence="0.673139">
About the Speaker
</subsectionHeader>
<bodyText confidence="0.999467066666667">
David Forsyth is currently a full professor at U. Illi-
nois at Urbana-Champaign, where he moved from
U.C Berkeley, where he was also full professor. He
has published over 130 papers on computer vision,
computer graphics and machine learning. He has
served as program chair and as general chair for var-
ious international conferences on computer vision.
He received an IEEE technical achievement award
for 2005 for his research and became an IEEE fellow
in 2009. His textbook, ”Computer Vision: A Mod-
ern Approach” (joint with J. Ponce and published by
Prentice Hall) is widely adopted as a course text. A
second edition appeared in 2011. He was named ed-
itor in chief of IEEE TPAMI for a term starting in
Jan 2013.
</bodyText>
<page confidence="0.944799">
254
</page>
<note confidence="0.4280865">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, page 254, Atlanta, Georgia, June 13-14, 2013. c�2013 Association for Computational Linguistics
</note>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.122552">
<title confidence="0.994014">Keynote Address: More Words and Bigger Pictures</title>
<author confidence="0.984417">David</author>
<affiliation confidence="0.998934">University of</affiliation>
<email confidence="0.999163">daf@illinois.edu</email>
<abstract confidence="0.945591175">Object recognition is a little like translation: a picture (text in a source language) goes in, and a description (text in a target language) comes out. I will use this analogy, which has proven fertile, to describe recent progress in object recognition. We have very good methods to spot some objects in images, but extending these methods to produce descriptions of images remains very difficult. The description might come in the form of a set of words, indicating objects, and boxes or regions spanned by the object. This representation is difficult to work with, because some objects seem to be much more important than others, and because objects interact. An alternative is a sentence or a paragraph describing the picture, and recent work indicates how one might generate rich structures like this. Furthermore, recent work suggests that it is easier and more effective to generate descriptions of images in terms of chunks of meaning (”person on a horse”) rather than just objects (”person”; ”horse”). Finally, if the picture contains objects that are unfamiliar, then we need to generate useful descriptions that will make it possible to interact with them, even though we don’t know what they are. About the Speaker David Forsyth is currently a full professor at U. Illinois at Urbana-Champaign, where he moved from U.C Berkeley, where he was also full professor. He has published over 130 papers on computer vision, computer graphics and machine learning. He has served as program chair and as general chair for various international conferences on computer vision. He received an IEEE technical achievement award for 2005 for his research and became an IEEE fellow in 2009. His textbook, ”Computer Vision: A Modern Approach” (joint with J. Ponce and published by Prentice Hall) is widely adopted as a course text. A second edition appeared in 2011. He was named editor in chief of IEEE TPAMI for a term starting in</abstract>
<note confidence="0.8219095">Jan 2013. 254 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference the Shared page 254, Atlanta, Georgia, June 13-14, 2013. Association for Computational Linguistics</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>