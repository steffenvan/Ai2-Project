<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004716">
<title confidence="0.997725">
A Simple Approach to Building Ensembles of Naive Bayesian
Classifiers for Word Sense Disambiguation
</title>
<author confidence="0.992209">
Ted Pedersen
</author>
<affiliation confidence="0.9995055">
Department of Computer Science
University of Minnesota Duluth
</affiliation>
<address confidence="0.519615">
Duluth, MN 55812 USA
</address>
<email confidence="0.941772">
tpederseacLumn.edu
</email>
<sectionHeader confidence="0.984038" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999902111111111">
This paper presents a corpus-based approach to
word sense disambiguation that builds an ensemble
of Naive Bayesian classifiers, each of which is based
on lexical features that represent co—occurring words
in varying sized windows of context. Despite the
simplicity of this approach, empirical results disam-
biguating the widely studied nouns line and interest
show that such an ensemble achieves accuracy rival-
ing the best previously published results.
</bodyText>
<sectionHeader confidence="0.993714" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999915746031746">
Word sense disambiguation is often cast as a prob-
lem in supervised learning, where a disambiguator is
induced from a corpus of manually sense—tagged text
using methods from statistics or machine learning.
These approaches typically represent the context in
which each sense—tagged instance of a word occurs
with a set of linguistically motivated features. A
learning algorithm induces a representative model
from these features which is employed as a classifier
to perform disambiguation.
This paper presents a corpus—based approach that
results in high accuracy by combining a number of
very simple classifiers into an ensemble that per-
forms disambiguation via a majority vote. This is
motivated by the observation that enhancing the fea-
ture set or learning algorithm used in a corpus—based
approach does not usually improve disambiguation
accuracy beyond what can be attained with shallow
lexical features and a simple supervised learning al-
gorithm.
For example, a Naive Bayesian classifier (Duda
and Hart, 1973) is based on a blanket assumption
about the interactions among features in a sense-
tagged corpus and does not learn a representative
model. Despite making such an assumption, this
proves to be among the most accurate techniques
in comparative studies of corpus—based word sense
disambiguation methodologies (e.g., (Leacock et al.,
1993), (Mooney, 1996), (Ng and Lee, 1996), (Peder-
sen and Bruce, 1997)). These studies represent the
context in which an ambiguous word occurs with a
wide variety of features. However, when the con-
tribution of each type of feature to overall accuracy
is analyzed (eg. (Ng and Lee, 1996)), shallow lexi-
cal features such as co—occurrences and collocations
prove to be stronger contributors to accuracy than
do deeper, linguistically motivated features such as
part—of—speech and verb—object relationships.
It has also been shown that the combined accuracy
of an ensemble of multiple classifiers is often signifi-
cantly greater than that of any of the individual clas-
sifiers that make up the ensemble (e.g., (Dietterich,
1997)). In natural language processing, ensemble
techniques have been successfully applied to part—
of—speech tagging (e.g., (Brill and Wu, 1998)) and
parsing (e.g., (Henderson and Brill, 1999)). When
combined with a history of disambiguation success
using shallow lexical features and Naive Bayesian
classifiers, these findings suggest that word sense dis-
ambiguation might best be improved by combining
the output of a number of such classifiers into an
ensemble.
This paper begins with an introduction to the
Naive Bayesian classifier. The features used to rep-
resent the context in which ambiguous words occur
are presented, followed by the method for selecting
the classifiers to include in the ensemble. Then, the
line and interesi data is described. Experimental re-
sults disambiguating these words with an ensemble
of Naive Bayesian classifiers are shown to rival pre-
viously published results. This paper closes with a
discussion of the choices made in formulating this
methodology and plans for future work.
</bodyText>
<sectionHeader confidence="0.941002" genericHeader="method">
2 Naive Bayesian Classifiers
</sectionHeader>
<bodyText confidence="0.937077">
A Naive Bayesian classifier assumes that all the fea-
ture variables representing a problem are condition-
ally independent given the value of a classification
variable. In word sense disambiguation, the context
in which an ambiguous word occurs is represented by
the feature variables (F1, F2, , Fn) and the sense
of the ambiguous word is represented by the classi-
fication variable (S). In this paper, all feature vari-
ables Fi are binary and represent whether or not a
particular word occurs within some number of words
to the left or right of an ambiguous word, i.e., a win-
</bodyText>
<page confidence="0.999077">
63
</page>
<bodyText confidence="0.98707925">
dow of context. For a Naive Bayesian classifier, the
joint probability of observing a certain combination
of contextual features with a particular sense is ex-
pressed as:
</bodyText>
<equation confidence="0.76157">
P(Fi, F2 , Fn, S) = p(S) p(FS)
</equation>
<bodyText confidence="0.997412611111111">
The parameters of this model are p(S) and
FilS)• The sufficient statistics, i.e., the summaries
of the data needed for parameter estimation, are the
frequency counts of the events described by the in-
terdependent variables (Fi, S). In this paper, these
counts are the number of sentences in the sense-
tagged text where the word represented by Fi oc-
curs within some specified window of context of the
ambiguous word when it is used in sense S.
Any parameter that has a value of zero indicates
that the associated word never occurs with the spec-
ified sense value. These zero values are smoothed
by assigning them a very small default probability.
Once all the parameters have been estimated, the
model has been trained and can be used as a clas-
sifier to perform disambiguation by determining the
most probable sense for an ambiguous word, given
the context in which it occurs.
</bodyText>
<subsectionHeader confidence="0.999771">
2.1 Representation of Context
</subsectionHeader>
<bodyText confidence="0.998642">
The contextual features used in this paper are bi-
nary and indicate if a given word occurs within some
number of words to the left or right of the ambigu-
ous word. No additional positional information is
contained in these features; they simply indicate if
the word occurs within some number of surrounding
words.
Punctuation and capitalization are removed from
the windows of context. All other lexical items are
included in their original form; no stemming is per-
formed and non-content words remain.
This representation of context is a variation on
the bag-of-words feature set, where a single window
of context includes words that occur to both the left
and right of the ambiguous word. An early use of
this representation is described in (Gale et al., 1992),
where word sense disambiguation is performed with
a Naive Bayesian classifier. The work in this pa-
per differs in that there are two windows of context,
one representing words that occur to the left of the
ambiguous word and another for those to the right.
</bodyText>
<subsectionHeader confidence="0.999767">
2.2 Ensembles of Naive Bayesian Classifiers
</subsectionHeader>
<bodyText confidence="0.99854872972973">
The left and right windows of context have nine dif-
ferent sizes; 0, 1, 2, 3, 4, 5, 10, 25, and 50 words.
The first step in the ensemble approach is to train a
separate Naive Bayesian classifier for each of the 81
possible combination of left and right window sizes.
Naive_Bayes (1,r) represents a classifier where the
model parameters have been estimated based on fre-
quency counts of shallow lexical features from two
windows of context; one including 1 words to the left
of the ambiguous word and the other including r
words to the right. Note that Naive_Bayes (0,0) in-
cludes no words to the left or right; this classifier acts
as a majority classifier that assigns every instance of
an ambiguous word to the most frequent sense in
the training data. Once the individual classifiers are
trained they are evaluated using previously held-out
test data.
The crucial step in building an ensemble is select-
ing the classifiers to include as members. The ap-
proach here is to group the 81 Naive Bayesian clas-
sifiers into general categories representing the sizes
of the windows of context. There are three such
ranges; narrow corresponds to windows 0, 1 and 2
words wide, medium to windows 3, 4, and 5 words
wide, and wide to windows 10, 25, and 50 words
wide. There are nine possible range categories since
there are separate left and right windows. For exam-
ple, Naive_Bayes(1,3) belongs to the range category
(narrow, medium) since it is based on a one word
window to the left and a three word window to the
right. The most accurate classifier in each of the
nine range categories is selected for inclusion in the
ensemble. Each of the nine member classifiers votes
for the most probable sense given the particular con-
text represented by that classifier; the ensemble dis-
ambiguates by assigning the sense that receives a
majority of the votes.
</bodyText>
<sectionHeader confidence="0.997277" genericHeader="method">
3 Experimental Data
</sectionHeader>
<bodyText confidence="0.9999834">
The line data was created by (Leacock et al., 1993)
by tagging every occurrence of line in the ACL/DCI
Wall Street Journal corpus and the American Print-
ing House for the Blind corpus with one of six pos-
sible WordNet senses. These senses and their fre-
quency distribution are shown in Table 1. This data
has since been used in studies by (Mooney, 1996),
(Towell and Voorhees, 1998), and (Leacock et al.,
1998). In that work, as well as in this paper, a subset
of the corpus is utilized such that each sense is uni-
formly distributed; this reduces the accuracy of the
majority classifier to 17%. The uniform distribution
is created by randomly sampling 349 sense-tagged
examples from each sense, resulting in a training cor-
pus of 2094 sense-tagged sentences.
The interest data was created by (Bruce and
Wiebe, 1994) by tagging all occurrences of interest
in the ACL/DCI Wall Street Journal corpus with
senses from the Longman Dictionary of Contempo-
rary English. This data set was subsequently used
for word sense disambiguation experiments by (Ng
and Lee, 1996), (Pedersen et al., 1997), and (Peder-
sen and Bruce, 1997). The previous studies and this
paper use the entire 2,368 sense-tagged sentence cor-
pus in their experiments. The senses and their fre-
</bodyText>
<page confidence="0.995929">
64
</page>
<bodyText confidence="0.968641642857143">
sense count
product 2218
written or spoken text 405
telephone connection 429
formation of people or things; queue 349
an artificial division; boundary 376
a thin, flexible object; cord 371
total 4148
Table 1: Distribution of senses for line - the exper-
iments in this paper and previous work use a uni-
formly distributed subset of this corpus, where each
sense occurs 349 times.
sense count
money paid for the use of money 1252
a share in a company or business 500
readiness to give attention 361
advantage, advancement or favor 178
activity that one gives attention to 66
causing attention to be given to 11
total 2368
Table 2: Distribution of senses for interest - the ex-
periments in this paper and previous work use the
entire corpus, where each sense occurs the number
of times shown above.
quency distribution are shown in Table 2. Unlike
line, the sense distribution is skewed; the majority
sense occurs in 53% of the sentences, while the small-
est minority sense occurs in less than 1%.
</bodyText>
<sectionHeader confidence="0.997707" genericHeader="method">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.999949785714286">
Eighty-one Naive Bayesian classifiers were trained
and tested with the line and interest data. Five-
fold cross validation was employed; all of the sense-
tagged examples for a word were randomly shuffled
and divided into five equal folds. Four folds were
used to train the Naive Bayesian classifier while the
remaining fold was randomly divided into two equal
sized test sets. The first, devtest, was used to eval-
uate the individual classifiers for inclusion in the en-
semble. The second, test, was used to evaluate the
accuracy of the ensemble. Thus the training data
for each word consists of 80% of the available sense-
tagged text, while each of the test sets contains 10%.
This process is repeated five times so that each
fold serves as the source of the test data once. The
average accuracy of the individual Naive Bayesian
classifiers across the five folds is reported in Tables
3 and 4. The standard deviations were between .01
and .025 and are not shown given their relative con-
sistency.
Each classifier is based upon a distinct representa-
tion of context since each employs a different com-
bination of right and left window sizes. The size
and range of the left window of context is indicated
along the horizontal margin in Tables 3 and 4 while
the right window size and range is shown along the
vertical margin. Thus, the boxes that subdivide each
table correspond to a particular range category. The
classifier that achieves the highest accuracy in each
range category is included as a member of the ensem-
ble. In case of a tie, the classifier with the smallest
total window of context is included in the ensemble.
The most accurate single classifier for line is
Naive_Bayes (4,25), which attains accuracy of 84%
The accuracy of the ensemble created from the most
accurate classifier in each of the range categories is
88%. The single most accurate classifier for interest
is Naive_Bayes(4,1), which attains accuracy of 86%
while the ensemble approach reaches 89%. The in-
crease in accuracy achieved by both ensembles over
the best individual classifier is statistically signifi-
cant, as judged by McNemar&apos;s test with p = .01.
</bodyText>
<subsectionHeader confidence="0.993585">
4.1 Comparison to Previous Results
</subsectionHeader>
<bodyText confidence="0.9999946">
These experiments use the same sense-tagged cor-
pora for interest and line as previous studies. Sum-
maries of previous results in Tables 5 and 6 show
that the accuracy of the Naive Bayesian ensemble
is comparable to that of any other approach. How-
ever, due to variations in experimental methodolo-
gies, it can not be concluded that the differences
among the most accurate methods are statistically
significant. For example, in this work five-fold cross
validation is employed to assess accuracy while (Ng
and Lee, 1996) train and test using 100 randomly
sampled sets of data. Similar differences in train-
ing and testing methodology exist among the other
studies. Still, the results in this paper are encourag-
ing due to the simplicity of the approach.
</bodyText>
<subsubsectionHeader confidence="0.53031">
4.1.1 Interest
</subsubsectionHeader>
<bodyText confidence="0.999606352941177">
The interest data was first studied by (Bruce and
Wiebe, 1994). They employ a representation of
context that includes the part-of-speech of the two
words surrounding interest, a morphological feature
indicating whether or not interest is singular or plu-
ral, and the three most statistically significant co-
occurring words in the sentence with interest, as de-
termined by a test of independence. These features
are abbreviated as p-o-s, morph, and co-occur in
Table 5. A decomposable probabilistic model is in-
duced from the sense-tagged corpora using a back-
ward sequential search where candidate models are
evaluated with the log-likelihood ratio test. The se-
lected model was used as a probabilistic classifier on
a held-out set of test data and achieved accuracy of
78%.
The interest data was included in a study by (Ng
</bodyText>
<page confidence="0.993492">
65
</page>
<figure confidence="0.730984789473684">
.63 .73 .80 .82 .83 .83 .83 .83 .83
.63 .74 .80 .82 .84 .83 .83 .83 .83
.62 .75 .81 .82 .83 .83 .83 .83 .84
.61 .75 .80 .81 .82 .82 .82 .82 .83
.60 .73 .80 .82 .82 .82 .82 .82 .82
.58 .73 .79 .82 .83 .83 .82 .81 .82
.53 .71 .79 .81 .82 .82 .81 .81 .81
.42 .68 .78 .79 .80 .79 .80 .81 .81
.14 .58 .73 .77 .79 .79 .79 .79 .80
50
wide 25
10
5
medium 4
3
2
narrow
1 2 3 4 10 25 50
narrow medium wide
</figure>
<tableCaption confidence="0.958502666666667">
Table 3: Accuracy of Naive Bayesian classifiers for line evaluated with the devtest data. The italicized
accuracies are associated with the classifiers included in the ensemble, which attained accuracy of 88% when
evaluated with the test data.
</tableCaption>
<figure confidence="0.992184894736842">
.74 .80 .82 .83 .83 .83 .82 .80 .81
.73 .80 .82 .83 .83 .83 .81 .80 .80
.75 .82 .84 .84 .84 .84 .82 .81 .81
.73 .83 .85 .86 .85 .85 .83 .81 .81
.72 .83 .85 .85 .84 .84 .83 .81 .80
.70 .84 .86 .86 .86 .85 .83 .81 .80
.66 .83 .85 .86 .86 .84 .83 .80 .80
.63 .82 .85 .85 .86 .85 .82 .81 .80
.53 .72 .77 .78 .79 .77 .77 .76 .75
50
wide 25
10
5
medium 4
3
2
narrow
4 10 25
narrow medium wide
</figure>
<tableCaption confidence="0.76985">
Table 4: Accuracy of Naive Bayesian classifiers for interest evaluated with the devtest data. The italicized
</tableCaption>
<bodyText confidence="0.945773166666667">
accuracies are associated with the classifiers included in the ensemble, which attained accuracy of 89% when
evaluated with the test data.
and Lee, 1996), who represent the context of an
ambiguous word with the part-of-speech of three
words to the left and right of interest, a morpho-
logical feature indicating if interest is singular or
plural, an unordered set of frequently occurring
keywords that surround interest, local collocations
that include interest, and verb-object syntactic re-
lationships. These features are abbreviated p-o-s,
morph, co-occur, collocates, and verb-obj in Table
5. A nearest-neighbor classifier was employed and
achieved an average accuracy of 87% over repeated
trials using randomly drawn training and test sets.
(Pedersen et al., 1997) and (Pedersen and Bruce,
1997) present studies that utilize the original Bruce
and Wiebe feature set and include the interest data.
The first compares a range of probabilistic model
selection methodologies and finds that none outper-
form the Naive Bayesian classifier, which attains ac-
curacy of 74%. The second compares a range of ma-
chine learning algorithms and finds that a decision
tree learner (78%) and a Naive Bayesian classifier
(74%) are most accurate.
</bodyText>
<subsubsectionHeader confidence="0.568066">
4.1.2 Line
</subsubsectionHeader>
<bodyText confidence="0.999914428571429">
The line data was first studied by (Leacock et al.,
1993). They evaluate the disambiguation accuracy
of a Naive Bayesian classifier, a content vector, and
a neural network. The context of an ambiguous
word is represented by a bag-of-words where the
window of context is two sentences wide. This fea-
ture set is abbreviated as 2 sentence b-o-w in Table
6. When the Naive Bayesian classifier is evaluated
words are not stemmed and capitalization remains.
However, with the content vector and the neural net-
work words are stemmed and words from a stop-list
are removed. They report no significant differences
in accuracy among the three approaches; the Naive
Bayesian classifier achieved 71% accuracy, the con-
tent vector 72%, and the neural network 76%.
The line data was studied again by (Mooney,
1996), where seven different machine learning
methodologies are compared. All learning algo-
rithms represent the context of an ambiguous word
using the bag-of-words with a two sentence window
of context. In these experiments words from a stop-
</bodyText>
<page confidence="0.986747">
66
</page>
<table confidence="0.994805285714286">
accuracy method feature set
Naive Bayesian Ensemble 89% ensemble of 9 varying left Si right b-o-w
Ng St Lee, 1996 87% nearest neighbor p-o-s, morph, co-occur
collocates, verb-obj
Bruce Si Wiebe, 1994 78% model selection p-o-s, morph, co—occur
Pedersen Si Bruce, 1997 78% decision tree p-o-s, morph, co—occur
74% naive bayes
</table>
<tableCaption confidence="0.999158">
Table 5: Comparison to previous results for interest
</tableCaption>
<table confidence="0.995264666666667">
accuracy method feature set
Naive Bayesian Ensemble 88% ensemble of 9 varying left St right b-o-w
Towel! St Voorhess, 1998 87% neural net local St topical b-o-w, p-o-s
Leacock, Chodorow, St Miller, 1998 84% naive bayes local St topical b-o-w, p-o-s
Leacock, Towel!, St Voorhees, 1993 76% neural net 2 sentence b-o-w
72% content vector
71% naive bayes
Mooney, 1996 72% naive bayes 2 sentence b-o-w
71% perceptron
</table>
<tableCaption confidence="0.999733">
Table 6: Comparison to previous results for line
</tableCaption>
<bodyText confidence="0.9994305">
list are removed, capitalization is ignored, and words
are stemmed. The two most accurate methods in
this study proved to be a Naive Bayesian classifier
(72%) and a perceptron (71%).
The line data was recently revisited by both (Tow-
ell and Voorhees, 1998) and (Leacock et al., 1998).
The former take an ensemble approach where the
output from two neural networks is combined; one
network is based on a representation of local con-
text while the other represents topical context. The
latter utilize a Naive Bayesian classifier. In both
cases context is represented by a set of topical and
local features. The topical features correspond to
the open—class words that occur in a two sentence
window of context. The local features occur within a
window of context three words to the left and right
of the ambiguous word and include co—occurrence
features as well as the part—of—speech of words in
this window. These features are represented as lo-
cal &amp; topical b-o-w and p-o-s in Table 6. (Towel!
and Voorhees, 1998) report accuracy of 87% while
(Leacock et al., 1998) report accuracy of 84%.
</bodyText>
<sectionHeader confidence="0.997138" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.9998375">
The word sense disambiguation ensembles in this pa-
per have the following characteristics:
</bodyText>
<listItem confidence="0.980039363636364">
• The members of the ensemble are Naive
Bayesian classifiers,
• the context in which an ambiguous word oc-
curs is represented by co—occurrence features
extracted from varying sized windows of sur-
rounding words,
• member classifiers are selected for the ensembles
based on their performance relative to others
with comparable window sizes, and
• a majority vote of the member classifiers deter-
mines the outcome of the ensemble.
</listItem>
<bodyText confidence="0.82808">
Each point is discussed below.
</bodyText>
<subsectionHeader confidence="0.992587">
5.1 Naive Bayesian classifiers
</subsectionHeader>
<bodyText confidence="0.99994555">
The Naive Bayesian classifier has emerged as a con-
sistently strong performer in a wide range of com-
parative studies of machine learning methodologies.
A recent survey of such results, as well as pos-
sible explanations for its success, is presented in
(Domingos and Pazzani, 1997). A similar finding
has emerged in word sense disambiguation, where
a number of comparative studies have all reported
that no method achieves significantly greater accu-
racy than the Naive Bayesian classifier (e.g., (Lea-
cock et al., 1993), (Mooney, 1996), (Ng and Lee,
1996), (Pedersen and Bruce, 1997)).
In many ensemble approaches the member classi-
fiers are learned with different algorithms that are
trained with the same data. For example, an en-
semble could consist of a decision tree, a neural net-
work, and a nearest neighbor classifier, all of which
are learned from exactly the same set of training
data. This paper takes a different approach, where
the learning algorithm is the same for all classifiers
</bodyText>
<page confidence="0.998775">
67
</page>
<bodyText confidence="0.999920777777778">
but the training data is different. This is motivated
by the belief that there is more to be gained by vary-
ing the representation of context than there is from
using many different learning algorithms on the same
data. This is especially true in this domain since the
Naive Bayesian classifier has a history of success and
since there is no generally agreed upon set of features
that have been shown to be optimal for word sense
disambiguation.
</bodyText>
<subsectionHeader confidence="0.996772">
5.2 Co—occurrence features
</subsectionHeader>
<bodyText confidence="0.9999424">
Shallow lexical features such as co—occurrences and
collocations are recognized as potent sources of dis-
ambiguation information. While many other con-
textual features are often employed, it isn&apos;t clear
that they offer substantial advantages. For exam-
ple, (Ng and Lee, 1996) report that local collocations
alone achieve 80% accuracy disambiguating interest,
while their full set of features result in 87%. Prelim-
inary experiments for this paper used feature sets
that included collocates, co—occurrences, part—of—
speech and grammatical information for surrounding
words. However, it was clear that no combination of
features resulted in disambiguation accuracy signifi-
cantly higher than that achieved with co—occurrence
features.
</bodyText>
<subsectionHeader confidence="0.998119">
5.3 Selecting ensemble members
</subsectionHeader>
<bodyText confidence="0.999994848484848">
The most accurate classifier from each of nine pos-
sible category ranges is selected as a member of
the ensemble. This is based on preliminary experi-
ments that showed that member classifiers with sim-
ilar sized windows of context often result in little or
no overall improvement in disambiguation accuracy.
This was expected since slight differences in window
sizes lead to roughly equivalent representations of
context and classifiers that have little opportunity
for collective improvement. For example, an ensem-
ble was created for interest using the nine classifiers
in the range category (medium, medium). The ac-
curacy of this ensemble was 84%, slightly less than
the most accurate individual classifiers in that range
which achieved accuracy of 86%.
Early experiments also revealed that an ensemble
based on a majority vote of all 81 classifiers per-
formed rather poorly. The accuracy for interest was
approximately 81% and line was disambiguated with
slightly less than 80% accuracy. The lesson taken
from these results was that an ensemble should con-
sist of classifiers that represent as differently sized
windows of context as possible; this reduces the im-
pact of redundant errors made by classifiers that
represent very similarly sized windows of context.
The ultimate success of an ensemble depends on the
ability to select classifiers that make complementary
errors. This is discussed in the context of combin-
ing part—of—speech taggers in (Brill and Wu, 1998).
They provide a measure for assessing the comple-
mentarity of errors between two taggers that could
be adapted for use with larger ensembles such as the
one discussed here, which has nine members.
</bodyText>
<subsectionHeader confidence="0.861187">
5.4 Disambiguation by majority vote
</subsectionHeader>
<bodyText confidence="0.9999505">
In this paper ensemble disambiguation is based on a
simple majority vote of the nine member classifiers.
An alternative strategy is to weight each vote by
the estimated joint probability found by the Naive
Bayesian classifier. However, a preliminary study
found that the accuracy of a Naive Bayesian ensem-
ble using a weighted vote was poor. For interest,
it resulted in accuracy of 83% while for line it was
82%. The simple majority vote resulted in accuracy
of 89% for interest and 88% for line.
</bodyText>
<sectionHeader confidence="0.999531" genericHeader="method">
6 Future Work
</sectionHeader>
<bodyText confidence="0.999949111111111">
A number of issues have arisen in the course of this
work that merit further investigation.
The simplicity of the contextual representation
can lead to large numbers of parameters in the Naive
Bayesian model when using wide windows of con-
text. Some combination of stop-lists and stemming
could reduce the numbers of parameters and thus
improve the overall quality of the parameter esti-
mates made from the training data.
In addition to simple co—occurrence features, the
use of collocation features seems promising. These
are distinct from co—occurrences in that they are
words that occur in close proximity to the ambiguous
word and do so to a degree that is judged statisti-
cally significant.
One limitation of the majority vote in this paper
is that there is no mechanism for dealing with out-
comes where no sense gets a majority of the votes.
This did not arise in this study but will certainly
occur as Naive Bayesian ensembles are applied to
larger sets of data.
Finally, further experimentation with the size of
the windows of context seems warranted. The cur-
rent formulation is based on a combination of intu-
ition and empirical study. An algorithm to deter-
mine optimal windows sizes is currently under de-
velopment.
</bodyText>
<sectionHeader confidence="0.999077" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999933181818182">
This paper shows that word sense disambiguation
accuracy can be improved by combining a number
of simple classifiers into an ensemble. A methodol-
ogy for formulating an ensemble of Naive Bayesian
classifiers is presented, where each member classifier
is based on co—occurrence features extracted from
a different sized window of context. This approach
was evaluated using the widely studied nouns line
and interest, which are disambiguated with accuracy
of 88% and 89%, which rivals the best previously
published results.
</bodyText>
<page confidence="0.999156">
68
</page>
<sectionHeader confidence="0.997727" genericHeader="acknowledgments">
8 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999830875">
This work extends ideas that began in collabora-
tion with Rebecca Bruce and Janyce Wiebe. Clau-
dia Leacock and Raymond Mooney provided valu-
able assistance with the line data. I am indebted to
an anonymous reviewer who pointed out the impor-
tance of separate test and devtest data sets.
A preliminary version of this paper appears in
(Pedersen, 2000).
</bodyText>
<sectionHeader confidence="0.999221" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999916758064516">
E. Brill and J. Wu. 1998. Classifier combination for
improved lexical disambiguation. In Proceedings
of the 36th Annual Meeting of the Association for
Computational Linguistics, Montreal.
R. Bruce and J. Wiebe. 1994. Word-sense disam-
biguation using decomposable models. In Proceed-
ings of the 32nd Annual Meeting of the Associ-
ation for Computational Linguistics, pages 139-
146.
T. Dietterich. 1997. Machine—learning research:
Four current directions. Al magazine, 18(4):97-
136.
P. Domingos and M. Pazzani. 1997. On the optimal-
ity of the simple Bayesian classifier under zero—one
loss. Machine Learning, 29:103-130.
R. Duda and P. Hart. 1973. Pattern Classification
and Scene Analysis. Wiley, New York, NY.
W. Gale, K. Church, and D. Yarowsky. 1992. A
method for disambiguating word senses in a large
corpus. Computers and the Humanities, 26:415-
439.
J. Henderson and E. Brill. 1999. Exploiting diver-
sity in natural language processing: Combining
parsers. In Proceedings of the Fourth Conference
on Empirical Methods in Natural Language Pro-
cessing, College Park, MD, June.
C. Leacock, G. Towell, and E. Voorhees. 1993.
Corpus-based statistical sense resolution. In Pro-
ceedings of the ARPA Workshop on Human Lan-
guage Technology, pages 260-265, March.
C. Leacock, M. Chodorow, and G. Miller. 1998. Us-
ing corpus statistics and WordNet relations for
sense identification. Computational Linguistics,
24(4147-165, March.
R. Mooney. 1996. Comparative experiments on dis-
ambiguating word senses: An illustration of the
role of bias in machine learning. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, pages 82-91, May.
H.T. Ng and H.B. Lee. 1996. Integrating multiple
knowledge sources to disambiguate word sense:
An exemplar-based approach. In Proceedings of
the 34th Annual Meeting of the Society for Com-
putational Linguistics, pages 40-47.
T. Pedersen and R. Bruce. 1997. A new supervised
learning algorithm for word sense disambiguation.
In Proceedings of the Fourteenth National Con-
ference on Artificial Intelligence, pages 604-609,
Providence, RI, July.
T. Pedersen, R. Bruce, and J. Wiebe. 1997. Sequen-
tial model selection for word sense disambigua-
tion. In Proceedings of the Fifth Conference on
Applied Natural Language Processing, pages 388-
395, Washington, DC, April.
T. Pedersen. 2000. An ensemble approach to
corpus—based word sense disambiguation. In Pro-
ceedings of the Conference on Intelligent Text
Processing and Computational Linguistics, pages
205-218, Mexico City, February.
G. Towell and E. Voorhees. 1998. Disambiguating
highly ambiguous words. Computational Linguis-
tics, 24(1):125-146, March.
</reference>
<page confidence="0.999318">
69
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.940034">
<title confidence="0.99971">A Simple Approach to Building Ensembles of Naive Bayesian Classifiers for Word Sense Disambiguation</title>
<author confidence="0.999972">Ted Pedersen</author>
<affiliation confidence="0.999576">Department of Computer Science University of Minnesota Duluth</affiliation>
<address confidence="0.94906">Duluth, MN 55812 USA</address>
<email confidence="0.999804">tpederseacLumn.edu</email>
<abstract confidence="0.9992079">This paper presents a corpus-based approach to word sense disambiguation that builds an ensemble of Naive Bayesian classifiers, each of which is based on lexical features that represent co—occurring words in varying sized windows of context. Despite the simplicity of this approach, empirical results disamthe widely studied nouns show that such an ensemble achieves accuracy rivaling the best previously published results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Brill</author>
<author>J Wu</author>
</authors>
<title>Classifier combination for improved lexical disambiguation.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Montreal.</location>
<contexts>
<context position="2896" citStr="Brill and Wu, 1998" startWordPosition="433" endWordPosition="436">ccuracy is analyzed (eg. (Ng and Lee, 1996)), shallow lexical features such as co—occurrences and collocations prove to be stronger contributors to accuracy than do deeper, linguistically motivated features such as part—of—speech and verb—object relationships. It has also been shown that the combined accuracy of an ensemble of multiple classifiers is often significantly greater than that of any of the individual classifiers that make up the ensemble (e.g., (Dietterich, 1997)). In natural language processing, ensemble techniques have been successfully applied to part— of—speech tagging (e.g., (Brill and Wu, 1998)) and parsing (e.g., (Henderson and Brill, 1999)). When combined with a history of disambiguation success using shallow lexical features and Naive Bayesian classifiers, these findings suggest that word sense disambiguation might best be improved by combining the output of a number of such classifiers into an ensemble. This paper begins with an introduction to the Naive Bayesian classifier. The features used to represent the context in which ambiguous words occur are presented, followed by the method for selecting the classifiers to include in the ensemble. Then, the line and interesi data is d</context>
<context position="24024" citStr="Brill and Wu, 1998" startWordPosition="3985" endWordPosition="3988"> performed rather poorly. The accuracy for interest was approximately 81% and line was disambiguated with slightly less than 80% accuracy. The lesson taken from these results was that an ensemble should consist of classifiers that represent as differently sized windows of context as possible; this reduces the impact of redundant errors made by classifiers that represent very similarly sized windows of context. The ultimate success of an ensemble depends on the ability to select classifiers that make complementary errors. This is discussed in the context of combining part—of—speech taggers in (Brill and Wu, 1998). They provide a measure for assessing the complementarity of errors between two taggers that could be adapted for use with larger ensembles such as the one discussed here, which has nine members. 5.4 Disambiguation by majority vote In this paper ensemble disambiguation is based on a simple majority vote of the nine member classifiers. An alternative strategy is to weight each vote by the estimated joint probability found by the Naive Bayesian classifier. However, a preliminary study found that the accuracy of a Naive Bayesian ensemble using a weighted vote was poor. For interest, it resulted </context>
</contexts>
<marker>Brill, Wu, 1998</marker>
<rawString>E. Brill and J. Wu. 1998. Classifier combination for improved lexical disambiguation. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics, Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bruce</author>
<author>J Wiebe</author>
</authors>
<title>Word-sense disambiguation using decomposable models.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>139--146</pages>
<contexts>
<context position="9203" citStr="Bruce and Wiebe, 1994" startWordPosition="1505" endWordPosition="1508">ne of six possible WordNet senses. These senses and their frequency distribution are shown in Table 1. This data has since been used in studies by (Mooney, 1996), (Towell and Voorhees, 1998), and (Leacock et al., 1998). In that work, as well as in this paper, a subset of the corpus is utilized such that each sense is uniformly distributed; this reduces the accuracy of the majority classifier to 17%. The uniform distribution is created by randomly sampling 349 sense-tagged examples from each sense, resulting in a training corpus of 2094 sense-tagged sentences. The interest data was created by (Bruce and Wiebe, 1994) by tagging all occurrences of interest in the ACL/DCI Wall Street Journal corpus with senses from the Longman Dictionary of Contemporary English. This data set was subsequently used for word sense disambiguation experiments by (Ng and Lee, 1996), (Pedersen et al., 1997), and (Pedersen and Bruce, 1997). The previous studies and this paper use the entire 2,368 sense-tagged sentence corpus in their experiments. The senses and their fre64 sense count product 2218 written or spoken text 405 telephone connection 429 formation of people or things; queue 349 an artificial division; boundary 376 a thi</context>
<context position="13645" citStr="Bruce and Wiebe, 1994" startWordPosition="2256" endWordPosition="2259">parable to that of any other approach. However, due to variations in experimental methodologies, it can not be concluded that the differences among the most accurate methods are statistically significant. For example, in this work five-fold cross validation is employed to assess accuracy while (Ng and Lee, 1996) train and test using 100 randomly sampled sets of data. Similar differences in training and testing methodology exist among the other studies. Still, the results in this paper are encouraging due to the simplicity of the approach. 4.1.1 Interest The interest data was first studied by (Bruce and Wiebe, 1994). They employ a representation of context that includes the part-of-speech of the two words surrounding interest, a morphological feature indicating whether or not interest is singular or plural, and the three most statistically significant cooccurring words in the sentence with interest, as determined by a test of independence. These features are abbreviated as p-o-s, morph, and co-occur in Table 5. A decomposable probabilistic model is induced from the sense-tagged corpora using a backward sequential search where candidate models are evaluated with the log-likelihood ratio test. The selected</context>
</contexts>
<marker>Bruce, Wiebe, 1994</marker>
<rawString>R. Bruce and J. Wiebe. 1994. Word-sense disambiguation using decomposable models. In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, pages 139-146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Dietterich</author>
</authors>
<title>Machine—learning research: Four current directions. Al magazine,</title>
<date>1997</date>
<pages>18--4</pages>
<contexts>
<context position="2756" citStr="Dietterich, 1997" startWordPosition="416" endWordPosition="417">ext in which an ambiguous word occurs with a wide variety of features. However, when the contribution of each type of feature to overall accuracy is analyzed (eg. (Ng and Lee, 1996)), shallow lexical features such as co—occurrences and collocations prove to be stronger contributors to accuracy than do deeper, linguistically motivated features such as part—of—speech and verb—object relationships. It has also been shown that the combined accuracy of an ensemble of multiple classifiers is often significantly greater than that of any of the individual classifiers that make up the ensemble (e.g., (Dietterich, 1997)). In natural language processing, ensemble techniques have been successfully applied to part— of—speech tagging (e.g., (Brill and Wu, 1998)) and parsing (e.g., (Henderson and Brill, 1999)). When combined with a history of disambiguation success using shallow lexical features and Naive Bayesian classifiers, these findings suggest that word sense disambiguation might best be improved by combining the output of a number of such classifiers into an ensemble. This paper begins with an introduction to the Naive Bayesian classifier. The features used to represent the context in which ambiguous words</context>
</contexts>
<marker>Dietterich, 1997</marker>
<rawString>T. Dietterich. 1997. Machine—learning research: Four current directions. Al magazine, 18(4):97-136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Domingos</author>
<author>M Pazzani</author>
</authors>
<title>On the optimality of the simple Bayesian classifier under zero—one loss.</title>
<date>1997</date>
<booktitle>Machine Learning,</booktitle>
<pages>29--103</pages>
<contexts>
<context position="20611" citStr="Domingos and Pazzani, 1997" startWordPosition="3448" endWordPosition="3451">ce features extracted from varying sized windows of surrounding words, • member classifiers are selected for the ensembles based on their performance relative to others with comparable window sizes, and • a majority vote of the member classifiers determines the outcome of the ensemble. Each point is discussed below. 5.1 Naive Bayesian classifiers The Naive Bayesian classifier has emerged as a consistently strong performer in a wide range of comparative studies of machine learning methodologies. A recent survey of such results, as well as possible explanations for its success, is presented in (Domingos and Pazzani, 1997). A similar finding has emerged in word sense disambiguation, where a number of comparative studies have all reported that no method achieves significantly greater accuracy than the Naive Bayesian classifier (e.g., (Leacock et al., 1993), (Mooney, 1996), (Ng and Lee, 1996), (Pedersen and Bruce, 1997)). In many ensemble approaches the member classifiers are learned with different algorithms that are trained with the same data. For example, an ensemble could consist of a decision tree, a neural network, and a nearest neighbor classifier, all of which are learned from exactly the same set of trai</context>
</contexts>
<marker>Domingos, Pazzani, 1997</marker>
<rawString>P. Domingos and M. Pazzani. 1997. On the optimality of the simple Bayesian classifier under zero—one loss. Machine Learning, 29:103-130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Duda</author>
<author>P Hart</author>
</authors>
<title>Pattern Classification and Scene Analysis.</title>
<date>1973</date>
<publisher>Wiley,</publisher>
<location>New York, NY.</location>
<contexts>
<context position="1705" citStr="Duda and Hart, 1973" startWordPosition="250" endWordPosition="253">ntative model from these features which is employed as a classifier to perform disambiguation. This paper presents a corpus—based approach that results in high accuracy by combining a number of very simple classifiers into an ensemble that performs disambiguation via a majority vote. This is motivated by the observation that enhancing the feature set or learning algorithm used in a corpus—based approach does not usually improve disambiguation accuracy beyond what can be attained with shallow lexical features and a simple supervised learning algorithm. For example, a Naive Bayesian classifier (Duda and Hart, 1973) is based on a blanket assumption about the interactions among features in a sensetagged corpus and does not learn a representative model. Despite making such an assumption, this proves to be among the most accurate techniques in comparative studies of corpus—based word sense disambiguation methodologies (e.g., (Leacock et al., 1993), (Mooney, 1996), (Ng and Lee, 1996), (Pedersen and Bruce, 1997)). These studies represent the context in which an ambiguous word occurs with a wide variety of features. However, when the contribution of each type of feature to overall accuracy is analyzed (eg. (Ng</context>
</contexts>
<marker>Duda, Hart, 1973</marker>
<rawString>R. Duda and P. Hart. 1973. Pattern Classification and Scene Analysis. Wiley, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Gale</author>
<author>K Church</author>
<author>D Yarowsky</author>
</authors>
<title>A method for disambiguating word senses in a large corpus. Computers and the Humanities,</title>
<date>1992</date>
<pages>26--415</pages>
<contexts>
<context position="6231" citStr="Gale et al., 1992" startWordPosition="989" endWordPosition="992"> the ambiguous word. No additional positional information is contained in these features; they simply indicate if the word occurs within some number of surrounding words. Punctuation and capitalization are removed from the windows of context. All other lexical items are included in their original form; no stemming is performed and non-content words remain. This representation of context is a variation on the bag-of-words feature set, where a single window of context includes words that occur to both the left and right of the ambiguous word. An early use of this representation is described in (Gale et al., 1992), where word sense disambiguation is performed with a Naive Bayesian classifier. The work in this paper differs in that there are two windows of context, one representing words that occur to the left of the ambiguous word and another for those to the right. 2.2 Ensembles of Naive Bayesian Classifiers The left and right windows of context have nine different sizes; 0, 1, 2, 3, 4, 5, 10, 25, and 50 words. The first step in the ensemble approach is to train a separate Naive Bayesian classifier for each of the 81 possible combination of left and right window sizes. Naive_Bayes (1,r) represents a c</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>W. Gale, K. Church, and D. Yarowsky. 1992. A method for disambiguating word senses in a large corpus. Computers and the Humanities, 26:415-439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Henderson</author>
<author>E Brill</author>
</authors>
<title>Exploiting diversity in natural language processing: Combining parsers.</title>
<date>1999</date>
<booktitle>In Proceedings of the Fourth Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>College Park, MD,</location>
<contexts>
<context position="2944" citStr="Henderson and Brill, 1999" startWordPosition="440" endWordPosition="443">6)), shallow lexical features such as co—occurrences and collocations prove to be stronger contributors to accuracy than do deeper, linguistically motivated features such as part—of—speech and verb—object relationships. It has also been shown that the combined accuracy of an ensemble of multiple classifiers is often significantly greater than that of any of the individual classifiers that make up the ensemble (e.g., (Dietterich, 1997)). In natural language processing, ensemble techniques have been successfully applied to part— of—speech tagging (e.g., (Brill and Wu, 1998)) and parsing (e.g., (Henderson and Brill, 1999)). When combined with a history of disambiguation success using shallow lexical features and Naive Bayesian classifiers, these findings suggest that word sense disambiguation might best be improved by combining the output of a number of such classifiers into an ensemble. This paper begins with an introduction to the Naive Bayesian classifier. The features used to represent the context in which ambiguous words occur are presented, followed by the method for selecting the classifiers to include in the ensemble. Then, the line and interesi data is described. Experimental results disambiguating th</context>
</contexts>
<marker>Henderson, Brill, 1999</marker>
<rawString>J. Henderson and E. Brill. 1999. Exploiting diversity in natural language processing: Combining parsers. In Proceedings of the Fourth Conference on Empirical Methods in Natural Language Processing, College Park, MD, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Leacock</author>
<author>G Towell</author>
<author>E Voorhees</author>
</authors>
<title>Corpus-based statistical sense resolution.</title>
<date>1993</date>
<booktitle>In Proceedings of the ARPA Workshop on Human Language Technology,</booktitle>
<pages>260--265</pages>
<contexts>
<context position="2040" citStr="Leacock et al., 1993" startWordPosition="301" endWordPosition="304">ng the feature set or learning algorithm used in a corpus—based approach does not usually improve disambiguation accuracy beyond what can be attained with shallow lexical features and a simple supervised learning algorithm. For example, a Naive Bayesian classifier (Duda and Hart, 1973) is based on a blanket assumption about the interactions among features in a sensetagged corpus and does not learn a representative model. Despite making such an assumption, this proves to be among the most accurate techniques in comparative studies of corpus—based word sense disambiguation methodologies (e.g., (Leacock et al., 1993), (Mooney, 1996), (Ng and Lee, 1996), (Pedersen and Bruce, 1997)). These studies represent the context in which an ambiguous word occurs with a wide variety of features. However, when the contribution of each type of feature to overall accuracy is analyzed (eg. (Ng and Lee, 1996)), shallow lexical features such as co—occurrences and collocations prove to be stronger contributors to accuracy than do deeper, linguistically motivated features such as part—of—speech and verb—object relationships. It has also been shown that the combined accuracy of an ensemble of multiple classifiers is often sign</context>
<context position="8443" citStr="Leacock et al., 1993" startWordPosition="1374" endWordPosition="1377">ories since there are separate left and right windows. For example, Naive_Bayes(1,3) belongs to the range category (narrow, medium) since it is based on a one word window to the left and a three word window to the right. The most accurate classifier in each of the nine range categories is selected for inclusion in the ensemble. Each of the nine member classifiers votes for the most probable sense given the particular context represented by that classifier; the ensemble disambiguates by assigning the sense that receives a majority of the votes. 3 Experimental Data The line data was created by (Leacock et al., 1993) by tagging every occurrence of line in the ACL/DCI Wall Street Journal corpus and the American Printing House for the Blind corpus with one of six possible WordNet senses. These senses and their frequency distribution are shown in Table 1. This data has since been used in studies by (Mooney, 1996), (Towell and Voorhees, 1998), and (Leacock et al., 1998). In that work, as well as in this paper, a subset of the corpus is utilized such that each sense is uniformly distributed; this reduces the accuracy of the majority classifier to 17%. The uniform distribution is created by randomly sampling 34</context>
<context position="16836" citStr="Leacock et al., 1993" startWordPosition="2832" endWordPosition="2835">7% over repeated trials using randomly drawn training and test sets. (Pedersen et al., 1997) and (Pedersen and Bruce, 1997) present studies that utilize the original Bruce and Wiebe feature set and include the interest data. The first compares a range of probabilistic model selection methodologies and finds that none outperform the Naive Bayesian classifier, which attains accuracy of 74%. The second compares a range of machine learning algorithms and finds that a decision tree learner (78%) and a Naive Bayesian classifier (74%) are most accurate. 4.1.2 Line The line data was first studied by (Leacock et al., 1993). They evaluate the disambiguation accuracy of a Naive Bayesian classifier, a content vector, and a neural network. The context of an ambiguous word is represented by a bag-of-words where the window of context is two sentences wide. This feature set is abbreviated as 2 sentence b-o-w in Table 6. When the Naive Bayesian classifier is evaluated words are not stemmed and capitalization remains. However, with the content vector and the neural network words are stemmed and words from a stop-list are removed. They report no significant differences in accuracy among the three approaches; the Naive Ba</context>
<context position="20848" citStr="Leacock et al., 1993" startWordPosition="3484" endWordPosition="3488">ers determines the outcome of the ensemble. Each point is discussed below. 5.1 Naive Bayesian classifiers The Naive Bayesian classifier has emerged as a consistently strong performer in a wide range of comparative studies of machine learning methodologies. A recent survey of such results, as well as possible explanations for its success, is presented in (Domingos and Pazzani, 1997). A similar finding has emerged in word sense disambiguation, where a number of comparative studies have all reported that no method achieves significantly greater accuracy than the Naive Bayesian classifier (e.g., (Leacock et al., 1993), (Mooney, 1996), (Ng and Lee, 1996), (Pedersen and Bruce, 1997)). In many ensemble approaches the member classifiers are learned with different algorithms that are trained with the same data. For example, an ensemble could consist of a decision tree, a neural network, and a nearest neighbor classifier, all of which are learned from exactly the same set of training data. This paper takes a different approach, where the learning algorithm is the same for all classifiers 67 but the training data is different. This is motivated by the belief that there is more to be gained by varying the represen</context>
</contexts>
<marker>Leacock, Towell, Voorhees, 1993</marker>
<rawString>C. Leacock, G. Towell, and E. Voorhees. 1993. Corpus-based statistical sense resolution. In Proceedings of the ARPA Workshop on Human Language Technology, pages 260-265, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Leacock</author>
<author>M Chodorow</author>
<author>G Miller</author>
</authors>
<title>Using corpus statistics and WordNet relations for sense identification.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<pages>24--4147</pages>
<contexts>
<context position="8799" citStr="Leacock et al., 1998" startWordPosition="1437" endWordPosition="1440">assifiers votes for the most probable sense given the particular context represented by that classifier; the ensemble disambiguates by assigning the sense that receives a majority of the votes. 3 Experimental Data The line data was created by (Leacock et al., 1993) by tagging every occurrence of line in the ACL/DCI Wall Street Journal corpus and the American Printing House for the Blind corpus with one of six possible WordNet senses. These senses and their frequency distribution are shown in Table 1. This data has since been used in studies by (Mooney, 1996), (Towell and Voorhees, 1998), and (Leacock et al., 1998). In that work, as well as in this paper, a subset of the corpus is utilized such that each sense is uniformly distributed; this reduces the accuracy of the majority classifier to 17%. The uniform distribution is created by randomly sampling 349 sense-tagged examples from each sense, resulting in a training corpus of 2094 sense-tagged sentences. The interest data was created by (Bruce and Wiebe, 1994) by tagging all occurrences of interest in the ACL/DCI Wall Street Journal corpus with senses from the Longman Dictionary of Contemporary English. This data set was subsequently used for word sens</context>
<context position="18937" citStr="Leacock et al., 1998" startWordPosition="3173" endWordPosition="3176"> 87% neural net local St topical b-o-w, p-o-s Leacock, Chodorow, St Miller, 1998 84% naive bayes local St topical b-o-w, p-o-s Leacock, Towel!, St Voorhees, 1993 76% neural net 2 sentence b-o-w 72% content vector 71% naive bayes Mooney, 1996 72% naive bayes 2 sentence b-o-w 71% perceptron Table 6: Comparison to previous results for line list are removed, capitalization is ignored, and words are stemmed. The two most accurate methods in this study proved to be a Naive Bayesian classifier (72%) and a perceptron (71%). The line data was recently revisited by both (Towell and Voorhees, 1998) and (Leacock et al., 1998). The former take an ensemble approach where the output from two neural networks is combined; one network is based on a representation of local context while the other represents topical context. The latter utilize a Naive Bayesian classifier. In both cases context is represented by a set of topical and local features. The topical features correspond to the open—class words that occur in a two sentence window of context. The local features occur within a window of context three words to the left and right of the ambiguous word and include co—occurrence features as well as the part—of—speech of</context>
</contexts>
<marker>Leacock, Chodorow, Miller, 1998</marker>
<rawString>C. Leacock, M. Chodorow, and G. Miller. 1998. Using corpus statistics and WordNet relations for sense identification. Computational Linguistics, 24(4147-165, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mooney</author>
</authors>
<title>Comparative experiments on disambiguating word senses: An illustration of the role of bias in machine learning.</title>
<date>1996</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>82--91</pages>
<contexts>
<context position="2056" citStr="Mooney, 1996" startWordPosition="305" endWordPosition="306">arning algorithm used in a corpus—based approach does not usually improve disambiguation accuracy beyond what can be attained with shallow lexical features and a simple supervised learning algorithm. For example, a Naive Bayesian classifier (Duda and Hart, 1973) is based on a blanket assumption about the interactions among features in a sensetagged corpus and does not learn a representative model. Despite making such an assumption, this proves to be among the most accurate techniques in comparative studies of corpus—based word sense disambiguation methodologies (e.g., (Leacock et al., 1993), (Mooney, 1996), (Ng and Lee, 1996), (Pedersen and Bruce, 1997)). These studies represent the context in which an ambiguous word occurs with a wide variety of features. However, when the contribution of each type of feature to overall accuracy is analyzed (eg. (Ng and Lee, 1996)), shallow lexical features such as co—occurrences and collocations prove to be stronger contributors to accuracy than do deeper, linguistically motivated features such as part—of—speech and verb—object relationships. It has also been shown that the combined accuracy of an ensemble of multiple classifiers is often significantly greate</context>
<context position="8742" citStr="Mooney, 1996" startWordPosition="1430" endWordPosition="1431">usion in the ensemble. Each of the nine member classifiers votes for the most probable sense given the particular context represented by that classifier; the ensemble disambiguates by assigning the sense that receives a majority of the votes. 3 Experimental Data The line data was created by (Leacock et al., 1993) by tagging every occurrence of line in the ACL/DCI Wall Street Journal corpus and the American Printing House for the Blind corpus with one of six possible WordNet senses. These senses and their frequency distribution are shown in Table 1. This data has since been used in studies by (Mooney, 1996), (Towell and Voorhees, 1998), and (Leacock et al., 1998). In that work, as well as in this paper, a subset of the corpus is utilized such that each sense is uniformly distributed; this reduces the accuracy of the majority classifier to 17%. The uniform distribution is created by randomly sampling 349 sense-tagged examples from each sense, resulting in a training corpus of 2094 sense-tagged sentences. The interest data was created by (Bruce and Wiebe, 1994) by tagging all occurrences of interest in the ACL/DCI Wall Street Journal corpus with senses from the Longman Dictionary of Contemporary E</context>
<context position="17578" citStr="Mooney, 1996" startWordPosition="2955" endWordPosition="2956">f an ambiguous word is represented by a bag-of-words where the window of context is two sentences wide. This feature set is abbreviated as 2 sentence b-o-w in Table 6. When the Naive Bayesian classifier is evaluated words are not stemmed and capitalization remains. However, with the content vector and the neural network words are stemmed and words from a stop-list are removed. They report no significant differences in accuracy among the three approaches; the Naive Bayesian classifier achieved 71% accuracy, the content vector 72%, and the neural network 76%. The line data was studied again by (Mooney, 1996), where seven different machine learning methodologies are compared. All learning algorithms represent the context of an ambiguous word using the bag-of-words with a two sentence window of context. In these experiments words from a stop66 accuracy method feature set Naive Bayesian Ensemble 89% ensemble of 9 varying left Si right b-o-w Ng St Lee, 1996 87% nearest neighbor p-o-s, morph, co-occur collocates, verb-obj Bruce Si Wiebe, 1994 78% model selection p-o-s, morph, co—occur Pedersen Si Bruce, 1997 78% decision tree p-o-s, morph, co—occur 74% naive bayes Table 5: Comparison to previous resul</context>
<context position="20864" citStr="Mooney, 1996" startWordPosition="3489" endWordPosition="3490">me of the ensemble. Each point is discussed below. 5.1 Naive Bayesian classifiers The Naive Bayesian classifier has emerged as a consistently strong performer in a wide range of comparative studies of machine learning methodologies. A recent survey of such results, as well as possible explanations for its success, is presented in (Domingos and Pazzani, 1997). A similar finding has emerged in word sense disambiguation, where a number of comparative studies have all reported that no method achieves significantly greater accuracy than the Naive Bayesian classifier (e.g., (Leacock et al., 1993), (Mooney, 1996), (Ng and Lee, 1996), (Pedersen and Bruce, 1997)). In many ensemble approaches the member classifiers are learned with different algorithms that are trained with the same data. For example, an ensemble could consist of a decision tree, a neural network, and a nearest neighbor classifier, all of which are learned from exactly the same set of training data. This paper takes a different approach, where the learning algorithm is the same for all classifiers 67 but the training data is different. This is motivated by the belief that there is more to be gained by varying the representation of contex</context>
</contexts>
<marker>Mooney, 1996</marker>
<rawString>R. Mooney. 1996. Comparative experiments on disambiguating word senses: An illustration of the role of bias in machine learning. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 82-91, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Ng</author>
<author>H B Lee</author>
</authors>
<title>Integrating multiple knowledge sources to disambiguate word sense: An exemplar-based approach.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the Society for Computational Linguistics,</booktitle>
<pages>40--47</pages>
<contexts>
<context position="2076" citStr="Ng and Lee, 1996" startWordPosition="307" endWordPosition="310"> used in a corpus—based approach does not usually improve disambiguation accuracy beyond what can be attained with shallow lexical features and a simple supervised learning algorithm. For example, a Naive Bayesian classifier (Duda and Hart, 1973) is based on a blanket assumption about the interactions among features in a sensetagged corpus and does not learn a representative model. Despite making such an assumption, this proves to be among the most accurate techniques in comparative studies of corpus—based word sense disambiguation methodologies (e.g., (Leacock et al., 1993), (Mooney, 1996), (Ng and Lee, 1996), (Pedersen and Bruce, 1997)). These studies represent the context in which an ambiguous word occurs with a wide variety of features. However, when the contribution of each type of feature to overall accuracy is analyzed (eg. (Ng and Lee, 1996)), shallow lexical features such as co—occurrences and collocations prove to be stronger contributors to accuracy than do deeper, linguistically motivated features such as part—of—speech and verb—object relationships. It has also been shown that the combined accuracy of an ensemble of multiple classifiers is often significantly greater than that of any o</context>
<context position="9449" citStr="Ng and Lee, 1996" startWordPosition="1544" endWordPosition="1547">s paper, a subset of the corpus is utilized such that each sense is uniformly distributed; this reduces the accuracy of the majority classifier to 17%. The uniform distribution is created by randomly sampling 349 sense-tagged examples from each sense, resulting in a training corpus of 2094 sense-tagged sentences. The interest data was created by (Bruce and Wiebe, 1994) by tagging all occurrences of interest in the ACL/DCI Wall Street Journal corpus with senses from the Longman Dictionary of Contemporary English. This data set was subsequently used for word sense disambiguation experiments by (Ng and Lee, 1996), (Pedersen et al., 1997), and (Pedersen and Bruce, 1997). The previous studies and this paper use the entire 2,368 sense-tagged sentence corpus in their experiments. The senses and their fre64 sense count product 2218 written or spoken text 405 telephone connection 429 formation of people or things; queue 349 an artificial division; boundary 376 a thin, flexible object; cord 371 total 4148 Table 1: Distribution of senses for line - the experiments in this paper and previous work use a uniformly distributed subset of this corpus, where each sense occurs 349 times. sense count money paid for th</context>
<context position="13336" citStr="Ng and Lee, 1996" startWordPosition="2204" endWordPosition="2207">tistically significant, as judged by McNemar&apos;s test with p = .01. 4.1 Comparison to Previous Results These experiments use the same sense-tagged corpora for interest and line as previous studies. Summaries of previous results in Tables 5 and 6 show that the accuracy of the Naive Bayesian ensemble is comparable to that of any other approach. However, due to variations in experimental methodologies, it can not be concluded that the differences among the most accurate methods are statistically significant. For example, in this work five-fold cross validation is employed to assess accuracy while (Ng and Lee, 1996) train and test using 100 randomly sampled sets of data. Similar differences in training and testing methodology exist among the other studies. Still, the results in this paper are encouraging due to the simplicity of the approach. 4.1.1 Interest The interest data was first studied by (Bruce and Wiebe, 1994). They employ a representation of context that includes the part-of-speech of the two words surrounding interest, a morphological feature indicating whether or not interest is singular or plural, and the three most statistically significant cooccurring words in the sentence with interest, a</context>
<context position="20884" citStr="Ng and Lee, 1996" startWordPosition="3491" endWordPosition="3494">le. Each point is discussed below. 5.1 Naive Bayesian classifiers The Naive Bayesian classifier has emerged as a consistently strong performer in a wide range of comparative studies of machine learning methodologies. A recent survey of such results, as well as possible explanations for its success, is presented in (Domingos and Pazzani, 1997). A similar finding has emerged in word sense disambiguation, where a number of comparative studies have all reported that no method achieves significantly greater accuracy than the Naive Bayesian classifier (e.g., (Leacock et al., 1993), (Mooney, 1996), (Ng and Lee, 1996), (Pedersen and Bruce, 1997)). In many ensemble approaches the member classifiers are learned with different algorithms that are trained with the same data. For example, an ensemble could consist of a decision tree, a neural network, and a nearest neighbor classifier, all of which are learned from exactly the same set of training data. This paper takes a different approach, where the learning algorithm is the same for all classifiers 67 but the training data is different. This is motivated by the belief that there is more to be gained by varying the representation of context than there is from</context>
</contexts>
<marker>Ng, Lee, 1996</marker>
<rawString>H.T. Ng and H.B. Lee. 1996. Integrating multiple knowledge sources to disambiguate word sense: An exemplar-based approach. In Proceedings of the 34th Annual Meeting of the Society for Computational Linguistics, pages 40-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
<author>R Bruce</author>
</authors>
<title>A new supervised learning algorithm for word sense disambiguation.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fourteenth National Conference on Artificial Intelligence,</booktitle>
<pages>604--609</pages>
<location>Providence, RI,</location>
<contexts>
<context position="2104" citStr="Pedersen and Bruce, 1997" startWordPosition="311" endWordPosition="315">sed approach does not usually improve disambiguation accuracy beyond what can be attained with shallow lexical features and a simple supervised learning algorithm. For example, a Naive Bayesian classifier (Duda and Hart, 1973) is based on a blanket assumption about the interactions among features in a sensetagged corpus and does not learn a representative model. Despite making such an assumption, this proves to be among the most accurate techniques in comparative studies of corpus—based word sense disambiguation methodologies (e.g., (Leacock et al., 1993), (Mooney, 1996), (Ng and Lee, 1996), (Pedersen and Bruce, 1997)). These studies represent the context in which an ambiguous word occurs with a wide variety of features. However, when the contribution of each type of feature to overall accuracy is analyzed (eg. (Ng and Lee, 1996)), shallow lexical features such as co—occurrences and collocations prove to be stronger contributors to accuracy than do deeper, linguistically motivated features such as part—of—speech and verb—object relationships. It has also been shown that the combined accuracy of an ensemble of multiple classifiers is often significantly greater than that of any of the individual classifiers</context>
<context position="9506" citStr="Pedersen and Bruce, 1997" startWordPosition="1553" endWordPosition="1557">that each sense is uniformly distributed; this reduces the accuracy of the majority classifier to 17%. The uniform distribution is created by randomly sampling 349 sense-tagged examples from each sense, resulting in a training corpus of 2094 sense-tagged sentences. The interest data was created by (Bruce and Wiebe, 1994) by tagging all occurrences of interest in the ACL/DCI Wall Street Journal corpus with senses from the Longman Dictionary of Contemporary English. This data set was subsequently used for word sense disambiguation experiments by (Ng and Lee, 1996), (Pedersen et al., 1997), and (Pedersen and Bruce, 1997). The previous studies and this paper use the entire 2,368 sense-tagged sentence corpus in their experiments. The senses and their fre64 sense count product 2218 written or spoken text 405 telephone connection 429 formation of people or things; queue 349 an artificial division; boundary 376 a thin, flexible object; cord 371 total 4148 Table 1: Distribution of senses for line - the experiments in this paper and previous work use a uniformly distributed subset of this corpus, where each sense occurs 349 times. sense count money paid for the use of money 1252 a share in a company or business 500 </context>
<context position="16338" citStr="Pedersen and Bruce, 1997" startWordPosition="2750" endWordPosition="2753">context of an ambiguous word with the part-of-speech of three words to the left and right of interest, a morphological feature indicating if interest is singular or plural, an unordered set of frequently occurring keywords that surround interest, local collocations that include interest, and verb-object syntactic relationships. These features are abbreviated p-o-s, morph, co-occur, collocates, and verb-obj in Table 5. A nearest-neighbor classifier was employed and achieved an average accuracy of 87% over repeated trials using randomly drawn training and test sets. (Pedersen et al., 1997) and (Pedersen and Bruce, 1997) present studies that utilize the original Bruce and Wiebe feature set and include the interest data. The first compares a range of probabilistic model selection methodologies and finds that none outperform the Naive Bayesian classifier, which attains accuracy of 74%. The second compares a range of machine learning algorithms and finds that a decision tree learner (78%) and a Naive Bayesian classifier (74%) are most accurate. 4.1.2 Line The line data was first studied by (Leacock et al., 1993). They evaluate the disambiguation accuracy of a Naive Bayesian classifier, a content vector, and a ne</context>
<context position="20912" citStr="Pedersen and Bruce, 1997" startWordPosition="3495" endWordPosition="3498">scussed below. 5.1 Naive Bayesian classifiers The Naive Bayesian classifier has emerged as a consistently strong performer in a wide range of comparative studies of machine learning methodologies. A recent survey of such results, as well as possible explanations for its success, is presented in (Domingos and Pazzani, 1997). A similar finding has emerged in word sense disambiguation, where a number of comparative studies have all reported that no method achieves significantly greater accuracy than the Naive Bayesian classifier (e.g., (Leacock et al., 1993), (Mooney, 1996), (Ng and Lee, 1996), (Pedersen and Bruce, 1997)). In many ensemble approaches the member classifiers are learned with different algorithms that are trained with the same data. For example, an ensemble could consist of a decision tree, a neural network, and a nearest neighbor classifier, all of which are learned from exactly the same set of training data. This paper takes a different approach, where the learning algorithm is the same for all classifiers 67 but the training data is different. This is motivated by the belief that there is more to be gained by varying the representation of context than there is from using many different learni</context>
</contexts>
<marker>Pedersen, Bruce, 1997</marker>
<rawString>T. Pedersen and R. Bruce. 1997. A new supervised learning algorithm for word sense disambiguation. In Proceedings of the Fourteenth National Conference on Artificial Intelligence, pages 604-609, Providence, RI, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
<author>R Bruce</author>
<author>J Wiebe</author>
</authors>
<title>Sequential model selection for word sense disambiguation.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth Conference on Applied Natural Language Processing,</booktitle>
<pages>388--395</pages>
<location>Washington, DC,</location>
<contexts>
<context position="9474" citStr="Pedersen et al., 1997" startWordPosition="1548" endWordPosition="1551"> the corpus is utilized such that each sense is uniformly distributed; this reduces the accuracy of the majority classifier to 17%. The uniform distribution is created by randomly sampling 349 sense-tagged examples from each sense, resulting in a training corpus of 2094 sense-tagged sentences. The interest data was created by (Bruce and Wiebe, 1994) by tagging all occurrences of interest in the ACL/DCI Wall Street Journal corpus with senses from the Longman Dictionary of Contemporary English. This data set was subsequently used for word sense disambiguation experiments by (Ng and Lee, 1996), (Pedersen et al., 1997), and (Pedersen and Bruce, 1997). The previous studies and this paper use the entire 2,368 sense-tagged sentence corpus in their experiments. The senses and their fre64 sense count product 2218 written or spoken text 405 telephone connection 429 formation of people or things; queue 349 an artificial division; boundary 376 a thin, flexible object; cord 371 total 4148 Table 1: Distribution of senses for line - the experiments in this paper and previous work use a uniformly distributed subset of this corpus, where each sense occurs 349 times. sense count money paid for the use of money 1252 a sha</context>
<context position="16307" citStr="Pedersen et al., 1997" startWordPosition="2745" endWordPosition="2748">e, 1996), who represent the context of an ambiguous word with the part-of-speech of three words to the left and right of interest, a morphological feature indicating if interest is singular or plural, an unordered set of frequently occurring keywords that surround interest, local collocations that include interest, and verb-object syntactic relationships. These features are abbreviated p-o-s, morph, co-occur, collocates, and verb-obj in Table 5. A nearest-neighbor classifier was employed and achieved an average accuracy of 87% over repeated trials using randomly drawn training and test sets. (Pedersen et al., 1997) and (Pedersen and Bruce, 1997) present studies that utilize the original Bruce and Wiebe feature set and include the interest data. The first compares a range of probabilistic model selection methodologies and finds that none outperform the Naive Bayesian classifier, which attains accuracy of 74%. The second compares a range of machine learning algorithms and finds that a decision tree learner (78%) and a Naive Bayesian classifier (74%) are most accurate. 4.1.2 Line The line data was first studied by (Leacock et al., 1993). They evaluate the disambiguation accuracy of a Naive Bayesian classif</context>
</contexts>
<marker>Pedersen, Bruce, Wiebe, 1997</marker>
<rawString>T. Pedersen, R. Bruce, and J. Wiebe. 1997. Sequential model selection for word sense disambiguation. In Proceedings of the Fifth Conference on Applied Natural Language Processing, pages 388-395, Washington, DC, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
</authors>
<title>An ensemble approach to corpus—based word sense disambiguation.</title>
<date>2000</date>
<booktitle>In Proceedings of the Conference on Intelligent Text Processing and Computational Linguistics,</booktitle>
<pages>205--218</pages>
<location>Mexico City,</location>
<marker>Pedersen, 2000</marker>
<rawString>T. Pedersen. 2000. An ensemble approach to corpus—based word sense disambiguation. In Proceedings of the Conference on Intelligent Text Processing and Computational Linguistics, pages 205-218, Mexico City, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Towell</author>
<author>E Voorhees</author>
</authors>
<title>Disambiguating highly ambiguous words.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<pages>24--1</pages>
<contexts>
<context position="8771" citStr="Towell and Voorhees, 1998" startWordPosition="1432" endWordPosition="1435">emble. Each of the nine member classifiers votes for the most probable sense given the particular context represented by that classifier; the ensemble disambiguates by assigning the sense that receives a majority of the votes. 3 Experimental Data The line data was created by (Leacock et al., 1993) by tagging every occurrence of line in the ACL/DCI Wall Street Journal corpus and the American Printing House for the Blind corpus with one of six possible WordNet senses. These senses and their frequency distribution are shown in Table 1. This data has since been used in studies by (Mooney, 1996), (Towell and Voorhees, 1998), and (Leacock et al., 1998). In that work, as well as in this paper, a subset of the corpus is utilized such that each sense is uniformly distributed; this reduces the accuracy of the majority classifier to 17%. The uniform distribution is created by randomly sampling 349 sense-tagged examples from each sense, resulting in a training corpus of 2094 sense-tagged sentences. The interest data was created by (Bruce and Wiebe, 1994) by tagging all occurrences of interest in the ACL/DCI Wall Street Journal corpus with senses from the Longman Dictionary of Contemporary English. This data set was sub</context>
<context position="18910" citStr="Towell and Voorhees, 1998" startWordPosition="3167" endWordPosition="3171">t b-o-w Towel! St Voorhess, 1998 87% neural net local St topical b-o-w, p-o-s Leacock, Chodorow, St Miller, 1998 84% naive bayes local St topical b-o-w, p-o-s Leacock, Towel!, St Voorhees, 1993 76% neural net 2 sentence b-o-w 72% content vector 71% naive bayes Mooney, 1996 72% naive bayes 2 sentence b-o-w 71% perceptron Table 6: Comparison to previous results for line list are removed, capitalization is ignored, and words are stemmed. The two most accurate methods in this study proved to be a Naive Bayesian classifier (72%) and a perceptron (71%). The line data was recently revisited by both (Towell and Voorhees, 1998) and (Leacock et al., 1998). The former take an ensemble approach where the output from two neural networks is combined; one network is based on a representation of local context while the other represents topical context. The latter utilize a Naive Bayesian classifier. In both cases context is represented by a set of topical and local features. The topical features correspond to the open—class words that occur in a two sentence window of context. The local features occur within a window of context three words to the left and right of the ambiguous word and include co—occurrence features as we</context>
</contexts>
<marker>Towell, Voorhees, 1998</marker>
<rawString>G. Towell and E. Voorhees. 1998. Disambiguating highly ambiguous words. Computational Linguistics, 24(1):125-146, March.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>