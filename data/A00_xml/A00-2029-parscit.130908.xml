<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000445">
<title confidence="0.9949415">
Predicting Automatic Speech Recognition Performance Using
Prosodic Cues
</title>
<note confidence="0.457033666666667">
Diane J. Litman and Julia B. Hirschberg Marc Swerts
AT&amp;T Labs — Research IPO, Center for User-System Interaction
Florham Park, NJ 07932-0971 USA Eindhoven, The Netherlands
</note>
<email confidence="0.933947">
{diane,julia} @research. att. corn swerts@ipo .tue.n1
</email>
<sectionHeader confidence="0.996449" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999773333333333">
In spoken dialogue systems, it is important for a
system to know how likely a speech recognition hy-
pothesis is to be correct, so it can reprompt for
fresh input, or, in cases where many errors have
occurred, change its interaction strategy or switch
the caller to a human attendant. We have discov-
ered prosodic features which more accurately predict
when a recognition hypothesis contains a word error
than the acoustic confidence score thresholds tradi-
tionally used in automatic speech recognition. We
present analytic results indicating that there are sig-
nificant prosodic differences between correctly and
incorrectly recognized turns in the TOOT train in-
formation corpus. We then present machine learn-
ing results showing how the use of prosodic features
to automatically predict correct versus incorrectly
recognized turns improves over the use of acoustic
confidence scores alone.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999977746268657">
One of the central tasks of the dialogue manager
in most current spoken dialogue systems (SDSs) is
error handling. The automatic speech recognition
(ASR) component of such systems is prone to error,
especially when the system has to operate in noisy
conditions or when the domain of the system is large.
Given that it is impossible to fully prevent ASR er-
rors, it is important for a system to know how likely
a speech recognition hypothesis is to be correct, so
it can take appropriate action, since users have con-
siderable difficulty correcting incorrect information
that is presented by the system as true (Krahmer
et al., 1999). Such action may include verifying the
user&apos;s input, reprompting for fresh input, or, in cases
where many errors have occurred, changing the in-
teraction strategy or switching the caller to a human
attendant (Smith, 1998; Litman et al., 1999; Langk-
ilde et al., 1999). Traditionally, the decision to re-
ject a recognition hypothesis is based on acoustic
confidence score thresholds, which provide a relia-
bility measure on the hypothesis and are set in the
application (Zeljkovic, 1996). However, this process
often fails, as there is no simple one-to-one mapping
between low confidence scores and incorrect recog-
nitions, and the setting of a rejection threshold is
a matter of trial and error (Bouwman et al., 1999).
Also, some incorrect recognitions do not necessarily
lead to misunderstandings at a conceptual level (e.g.
&amp;quot;a.m.&amp;quot; recognized as &amp;quot;in the morning&amp;quot;).
The current paper looks at prosody as one possible
predictor of ASR performance. ASR performance
is known to vary based upon speaking style (Wein-
traub et al., 1996), speaker gender and age, na-
tive versus non-native speaker status, and, in gen-
eral, the deviation of new speech from the training
data. Some of this variation is linked to prosody, as
prosodic differences have been found to character-
ize differences in speaking style (Blaauw, 1992) and
idiosyncratic differences (Kraayeveld, 1997). Sev-
eral other studies (Wade et al., 1992; Oviatt et al.,
1996; Swerts and Ostendorf, 1997; Levow, 1998; Bell
and Gustafson, 1999) report that hyperarticulated
speech, characterized by careful enunciation, slowed
speaking rate, and increase in pitch and loudness,
often occurs when users in human-machine interac-
tions try to correct system errors. Still others have
shown that such speech also decreases recognition
performance (Soltau and Waibel, 1998). Prosodic
features have also been shown to be effective in
ranking recognition hypotheses, as a post-processing
filter to score ASR hypotheses (Hirschberg, 1991;
Veilleux, 1994; Hirose, 1997).
In this paper we present results of empirical stud-
ies testing the hypothesis that prosodic features pro-
vide an important clue to ASR performance. We
first present results comparing prosodic analyses of
correctly and incorrectly recognized speaker turns
in TOOT, an experimental SDS for obtaining train
information over the phone. We then describe ma-
chine learning experiments based on these results
that explore the predictive power of prosodic fea-
tures alone and in combination with other automat-
ically available information, including ASR confi-
dence scores and recognized string. Our results in-
dicate that there are significant prosodic differences
between correctly and incorrectly recognized utter-
ances. These differences can in fact be used to pre-
</bodyText>
<page confidence="0.997367">
218
</page>
<bodyText confidence="0.989195">
dict whether an utterance has been misrecognized,
with a high degree of accuracy.
</bodyText>
<sectionHeader confidence="0.856825" genericHeader="method">
2 The TOOT Corpus
</sectionHeader>
<bodyText confidence="0.966269423076923">
Our corpus consists of a set of dialogues between
humans and TOOT, an SDS for accessing train
schedules from the web via telephone, which was
collected to study both variations in SDS strat-
egy and user-adapted interaction (Litman and Pan,
1999). TOOT is implemented on a platform com-
bining ASR, text-to-speech, a phone interface, a
finite-state dialogue manager, and application func-
tions (Kamm et al., 1997). The speech recognizer is
a speaker-independent hidden Markov model system
with context-dependent phone models for telephone
speech and constrained grammars for each dialogue
state. Confidence scores for recognition were avail-
able only at the turn, not the word, level (Zeljkovic,
1996). An example TOOT dialogue is shown in Fig-
ure 1.
Subjects performed four tasks with one of sev-
eral versions of TOOT, that differed in terms of locus
of initiative (system, user, or mixed), confirmation
strategy (explicit, implicit, or none), and whether
these conditions could be changed by the user during
the task. Subjects were 39 students, 20 native speak-
ers of standard American English and 19 non-native
speakers; 16 subjects were female and 23 male. Dia-
logues were recorded and system and user behavior
logged automatically. The concept accuracy (CA) of
each turn was manually labeled by one of the exper-
imenters. If the ASR output correctly captured all
the task-related information in the turn (e.g. time,
departure and arrival cities), the turn was given a
CA score of 1 (a semantically correct recognition).
Otherwise, the CA score reflected the percentage of
correctly recognized task information in the turn.
The dialogues were also transcribed by hand and
these transcriptions automatically compared to the
ASR recognized string to produce a word error rate
(WER) for each turn. Note that a concept can be
correctly recognized even though all words are not,
so the CA metric does not penalize for errors that
are unimportant to overall utterance interpretation.
For the study described below, we examined 1994
user turns from 152 dialogues in this corpus. The
speech recognizer was able to generate a recognized
string and an associated acoustic confidence score
per turn for 1975 of these turns.&apos; 1410 of these 1975
turns had a CA score of 1 (for an overall conceptual
accuracy score of 71%) and 961 had a WER of 0 (for
an overall transcription accuracy score of 49%, with
a mean WER per turn of 47%).
&apos;For the remaining turns, ASR output &amp;quot;no speech&amp;quot; (and
TOOT played a timeout message) or &amp;quot;garbage&amp;quot; (Tour played
a rejection message).
</bodyText>
<sectionHeader confidence="0.7543965" genericHeader="method">
3 Distinguishing Correct from
Incorrect Recognitions
</sectionHeader>
<bodyText confidence="0.982807444444444">
We first looked for distinguishing prosodic charac-
teristics of misrecognitions, defining misrecognitions
in two ways: a) as turns with WER&gt;0; and b) as
turns with CA&lt;1. As noted in Section 1, previous
studies have speculated that hyperarticulated speech
(slower and louder speech which contains wider pitch
excursions) may be associated with recognition fail-
ure. So, we examined the following features for each
user turn:2
</bodyText>
<listItem confidence="0.887872181818182">
• maximum and mean fundamental frequency
values (F0 Max, FO Mean)
• maximum and mean energy values (RMS Max,
RMS Mean)
• total duration
• length of pause preceding the turn (Prior Pause)
• speaking rate (Tempo)
• amount of silence within the turn (% Silence)
FO and RMS values, representing measures of pitch
excursion and loudness, were calculated from the
output of Entropic Research Laboratory&apos;s pitch
</listItem>
<bodyText confidence="0.996460407407407">
tracker, get _f0, with no post-correction. Timing vari-
ation was represented by four features. Duration
within and length of pause between turns was com-
puted from the temporal labels associated with each
turn&apos;s beginning and end. Speaking rate was ap-
proximated in terms of syllables in the recognized
string per second, while % Silence was defined as the
percentage of zero frames in the turn, i.e., roughly
the percentage of time within the turn that the
speaker was silent. These features were chosen based
upon previous findings (see Section 1) and observa-
tions from our data.
To ensure that our results were speaker indepen-
dent, we calculated mean values for each speaker&apos;s
recognized turns and their misrecognized turns for
every feature. Then, for each feature, we created
vectors of speaker means for recognized and misrec-
ognized turns and performed paired t-tests on the
vectors. For example, for the feature &amp;quot;FO max&amp;quot;,
we calculated mean maxima for misrecognized turns
and for correctly recognized turns for each of our
thirty-nine speakers. We then performed a paired
t-test on these thirty-nine pairs of means to de-
rive speaker-independent results for differences in FO
maxima between correct and incorrect recognitions.
Tables 1 and 2 show results of these compar-
isons when we calculate misrecognition in terms of
</bodyText>
<footnote confidence="0.850803">
2While the features were automatically computed, turn
beginnings and endings were hand segmented in dialogue-level
speech files, as the turn-level files created by TOOT were not
available.
</footnote>
<page confidence="0.997945">
219
</page>
<bodyText confidence="0.959852333333333">
Toot: Hi, this is AT&amp;T Amtrak schedule system. This is TOOT. How may I help you?
User: I want the trains from New York City to Washington DC on Monday at 9:30 in the evening.
Toot: Do you want me to find the trains from New York City to Washington DC on Monday
approximately at 9:30 in the evening now?
User: Yes.
Toot: I am going to get the train schedule for you ...
</bodyText>
<figureCaption confidence="0.992952">
Figure 1: Example Dialogue Excerpt with TOOT.
</figureCaption>
<tableCaption confidence="0.982459333333333">
Table 1: Comparison of Misrecognized (WER&gt;0)
vs. Recognized Turns by Prosodic Feature Across
Speakers.
</tableCaption>
<table confidence="0.999732909090909">
Feature T-stat Mean Misrec&apos;d P
- Rec&apos;d
*FO Max 7.83 30.31 Hz 0
*FO Mean 3.66 4.12 Hz 0
*RMS Max 5.70 235.93 0
RMS Mean -.57 -8.50 .57
*Duration 10.30 2.20 sec 0
*Prior Pause 5.55 .35 sec 0
Tempo -.05 .15 sps .13
*% Silence -5.15 -.06% 0
*significant at a 95% confidence level (p&lt; .05)
</table>
<tableCaption confidence="0.983194666666667">
Table 2: Comparison of Misrecognized (CA&lt;l)
vs. Recognized Turns by Prosodic Feature Across
Speakers.
</tableCaption>
<table confidence="0.991972545454545">
Feature T-stat Mean Misrec&apos;d P
- Rec&apos;d
*FO Max 5.60 29.64 Hz 0
FO Mean 1.70 2.10 Hz .10
*RMS Max 2.86 173.87 .007
RMS Mean -1.85 -27.75 .07
*Duration 9.80 2.15 sec 0
*Prior Pause 4.05 .38 sec 0
*Tempo -4.21 -.58 sps 0
% Silence -1.42 -.02% .16
*significant at a 95% confidence level (p&lt; .05)
</table>
<bodyText confidence="0.994940661290323">
WER&gt;0 and CA&lt;1, respectively. These results in-
dicate that misrecognized turns do differ from cor-
rectly recognized ones in terms of prosodic features,
although the features on which they differ vary
slightly, depending upon the way &amp;quot;misrecognition&amp;quot;
is defined. Whether defined by WER or CA, mis-
recognized turns exhibit significantly higher FO and
RMS maxima, longer durations, and longer preced-
ing pauses than correctly recognized speaker turns.
For a traditional WER definition of misrecognition,
misrecognitions are slightly higher in mean FO and
contain a lower percentage of internal silence. For a
CA definition, on the other hand, tempo is a signif-
icant factor, with misrecognitions spoken at a faster
rate than correct recognitions - contrary to our hy-
pothesis about the role of hyperarticulation in recog-
nition error.
While the comparisons in Tables 1 and 2 were
made on the means of raw values for all prosodic fea-
tures, little difference is found when values are nor-
malized by value of first or preceding turn, or by con-
verting to z scores.3 From this similarity between the
performance of raw and normalized values, it would
seem to be relative differences in speakers&apos; prosodic
values, not deviation from some &apos;acceptable&apos; range,
that distinguishes recognition failures from success-
ful recognitions. A given speaker&apos;s turns that are
3 The only differences occur for CA defined misrecognition,
where normalizing by first utterance results in significant dif-
ferences in mean RMS, and normalizing by preceding turn
results in no significant differences in tempo.
higher in pitch or loudness, or that are longer, or
that follow longer pauses, are less likely to be recog-
nized correctly than that same speaker&apos;s turns that
are lower in pitch or loudness, shorter, and follow
shorter pauses - however correct recognition is de-
fined.
It is interesting to note that the features we found
to be significant indicators of failed recognitions (FO
excursion, loudness, long prior pause, and longer du-
ration) are all features previously associated with
hyperarticulated speech. Since prior research has
suggested that speakers may respond to failed recog-
nition attempts by hyperarticulating, which itself
may lead to more recognition failures, had we in fact
simply identified a means of characterizing and iden-
tifying hyperarticulated speech prosodically?
Since we had independently labeled all speaker
turns for evidence of hyperarticulation (two of the
authors labeled each turn as &amp;quot;not hyperarticulated&amp;quot;,
&amp;quot;some hyperarticulation in the turn&amp;quot;, and &amp;quot;hyperar-
ticulated&amp;quot;, following Wade et al. (1992)), we were
able to test this possibility. We excluded any turn
either labeler had labeled as partially or fully hy-
perarticulated, and again performed paired t-tests
on mean values of misrecognized versus recognized
turns for each speaker. Results show that for both
WER-defined and CA-defined misrecognitions, not
only are the same features significant differentiators
when hyperarticulated turns are excluded from the
analysis, but in addition, tempo also is significant
for WER-defined misrecognition. So, our findings
</bodyText>
<page confidence="0.974703">
220
</page>
<bodyText confidence="0.997965666666667">
for the prosodic characteristics of recognized and of
misrecognized turns hold even when perceptibly hy-
perarticulated turns are excluded from the corpus.
</bodyText>
<sectionHeader confidence="0.7804205" genericHeader="method">
4 Predicting Misrecognitions Using
Machine Learning
</sectionHeader>
<bodyText confidence="0.999851307692308">
Given the prosodic differences between misrecog-
nized and correctly recognized utterances in our
corpus, is it possible to predict accurately when a
particular utterance will be misrecognized or not?
This section describes experiments using the ma-
chine learning program RIPPER (Cohen, 1996) to au-
tomatically induce prediction models, using prosodic
as well as additional features. Like many learning
programs, RIPPER takes as input the classes to be
learned, a set of feature names and possible values,
and training data specifying the class and feature
values for each training example. RIPPER outputs
a classification model for predicting the class of fu-
ture examples. The model is learned using greedy
search guided by an information gain metric, and is
expressed as an ordered set of if-then rules.
Our predicted classes correspond to correct recog-
nition (T) or not (F). As in Section 3, we examine
both WER-defined and CA-defined notions of cor-
rect recognition, and represent each user turn as a
set of features. The features used in our learning
experiments include the raw prosodic features in Ta-
bles 1 and 2 (which we will refer to as the feature set
&amp;quot;Prosody&amp;quot;), the hyperarticulation score discussed in
Section 3, and the following additional potential pre-
dictors of misrecognition (described in Section 2):
</bodyText>
<listItem confidence="0.999830888888889">
• ASR grammar
• ASR confidence
• ASR string
• system adaptability
• dialogue strategy
• task number
• subject
• gender
• native speaker
</listItem>
<bodyText confidence="0.988351609375">
The first three features are derived from the ASR
process (the context-dependent grammar used to
recognize the turn, the turn-level acoustic confidence
score output by the recognizer, and the recognized
string). We included these features as a baseline
against which to test new methods of predicting
misrecognitions, although, currently, we know of no
ASR system that includes recognized string in its
rejection calculations.&apos; TOOT itself used only the
4Note that, while the entire recognized string is provided
to the learning algorithm, RIPPER rules test for the presence
of particular words in the string.
first two features to calculate rejections and ask the
user to repeat the utterance, whenever the confi-
dence score fell below a pre-defined grammar-specific
threshold. The other features represent the exper-
imental conditions under which the data was col-
lected (whether users could adapt TOOT&apos;S dialogue
strategies, TooT&apos;s initial initiative and confirmation
strategies, experimental task, speaker&apos;s name and
characteristics). We included these features to de-
termine the extent to which particulars of task, sub-
ject, or interaction influenced ASR success rates or
our ability to predict them; previous work showed
that these factors impact TOOT&apos;S performance (Lit-
man and Pan, 1999; Hirschberg et al., 1999). Except
for the task, subject, gender, native language, and
hyperarticulation scores, all of our features are au-
tomatically available.
Table 3 shows the relative performance of a num-
ber of the feature sets we examined; results here
are for misrecognition defined in terms of WER.5 A
baseline classifier for misrecognition, predicting that
ASR is always wrong (the majority class of F), has
an error of 48.66%. The best performing feature
set includes only the raw prosodic and ASR features
and reduces this error to an impressive 6.53% +1-
.63%. Note that this performance is not improved
by adding manually labeled features or experimen-
tal conditions: the feature set corresponding to ALL
features yielded the statistically equivalent 6.68%
+1- 0.63%.
With respect to the performance of prosodic fea-
tures, Table 3 shows that using them in conjunction
with ASR features (error of 6.53%) significantly out-
performs prosodic features alone (error of 12.76%),
which, in turn, significantly outperforms any single
prosodic feature; duration, with an error of 17.42%,
is the best such feature. Although not shown in
the table, the unnormalized prosodic features sig-
nificantly outperform the normalized versions by 7-
13%. Recall that prosodic features normalized by
first task utterance, by previous utterance, or by
z scores showed little performance difference in the
analyses performed in Section 3. This difference may
indicate that there are indeed limits on the ranges
in features such as FO and RMS maxima, duration
and preceding pause within which recognition per-
formance is optimal. It seems reasonable that ex-
treme deviation from characteristics of the acoustic
training material should in fact impact ASR perfor-
mance, and our experiments may have uncovered, if
not the critical variants, at least important acoustic
correlates of them. However, it is difficult to com-
</bodyText>
<footnote confidence="0.6291722">
&apos;The errors and standard errors (SE) result from 25-fold
cross-validation on the 1975 turns where ASR yielded a string
and confidence. When two errors plus or minus twice the stan-
dard error do not overlap, they are statistically significantly
different.
</footnote>
<page confidence="0.998985">
221
</page>
<tableCaption confidence="0.999477">
Table 3: Estimated Error for Predicting Misrecognized Turns (WER&gt;0).
</tableCaption>
<table confidence="0.999665545454545">
Features Used Error SE
Prosody, ASR Confidence, ASR String, ASR Grammar 6.53% .63
ALL 6.68% .63
Prosody, ASR String 7.34% .75
ASR Confidence, ASR String, ASR Grammar 9.01% .70
Prosody, ASR Confidence, ASR Grammar 10.63% .88
Prosody, ASR Confidence 10.99% .87
Prosody 12.76% .79
ASR String 15.24% 1.11
Duration 17.42% .88
ASR Confidence, ASR Grammar 17.77% .72
ASR Confidence 22.23% 1.16
ASR Grammar 26.28% .84
Tempo 32.76% 1.03
Hyperarticulation 35.24% 1.46
% Silence 36.46% .79
Prior Pause 36.61% .97
FO Max 38.73% .82
RMS Max 42.23% .96
FO Mean 46.33% 1.10
RMS Mean 48.35% 1.15
Majority Baseline 48.66%
</table>
<bodyText confidence="0.999876589285714">
pare our machine learning results with the statisti-
cal analyses, since a) the statistical analyses looked
at only a single prosodic variable at a time, and b)
data points for that analysis were means calculated
per speaker, while the learning algorithm operated
on all utterances, allowing for unequal contributions
by speaker.
We now address the issue of what prosodic fea-
tures are contributing to misrecognition identifica-
tion, relative to the more traditional ASR tech-
niques. Do our prosodic features simply correlate
with information already in use by ASR systems
(e.g., confidence score, grammar), or at least avail-
able to them (e.g., recognized string)? First, the
error using ASR confidence score alone (22.23%)
is significantly worse than the error when prosodic
features are combined with ASR confidence scores
(10.99%) — and is also significantly worse than
the use of prosodic features alone (12.76%). Simi-
larly, the error using ASR confidence scores and the
ASR grammar (17.77%) is significantly worse than
prosodic features alone (12.76%). Thus, prosodic
features, either alone or in conjunction with tradi-
tional ASR features, significantly outperform these
traditional features alone for predicting WER-based
misrecognitions.
Another interesting finding from our experiments
is the predictive power of information available to
current ASR systems but not made use of in calcu-
lating rejection likelihoods, the identity of the recog-
nized string. This feature is in fact the best perform-
ing single feature in predicting our data (15.24%).
And, at a 95% confidence level, the error using
ASR confidence scores, the recognized string, and
grammar (9.01%) matches the performance of our
best performing feature set (6.53%). It seems that,
at least in our task and for our ASR system, the
appearance of particular words in the recognized
strings is an extremely useful cue to recognition ac-
curacy. So, even by making use of information cur-
rently available from the traditional ASR process,
ASR systems could improve their performance on
identifying rejections by a considerable margin. A
caveat here is that this feature, like grammar state,
is unlikely to generalize from task to task or recog-
nizer to recognizer, but these findings suggest that
both should be considered as a means of improving
rejection performance in stable systems.
The classification model learned from the best per-
forming feature set in Table 3 is shown in Figure 2.6
The first rule RIPPER finds with this feature set is
that if the user turn is less than .9 seconds and the
recognized string contains the word &amp;quot;yes&amp;quot; (and possi-
bly other words as well), with an acoustic confidence
score &gt; -2.6, then predict that the turn will be cor-
rectly recognized.&apos; Note that all of the prosodic fea-
</bodyText>
<footnote confidence="0.9948932">
6Rules are presented in order of importance in classifying
data. When multiple rules are applicable, RIPPER uses the
first rule.
7The confidence scores observed in our data ranged from
a high of -0.087662 to a low of -9.884418.
</footnote>
<page confidence="0.987696">
222
</page>
<construct confidence="0.973079166666667">
if (duration &lt; 0.897073) A (confidence &gt; -2.62744 ) A (string contains &apos;yes&apos;) then T
if (duration &lt; 1.03872 ) A (confidence &gt; -2.69775) A (string contains &apos;no&apos;) then T
if (duration &lt;0.982051) A (confidence &gt; -1.99705) A (tempo &gt; 3.1147) then T
if (duration &lt; 0.813633) A (duration &gt; 0.642652) A (confidence &gt; -3.33945) A (F0 Mean &gt; 176.794) then T
if (duration &lt; 1.30312) A (confidence &gt; -3.37301) A (% silences &gt; 0.647059) then T
if (duration &lt; 0.610734) A (confidence &gt; -3.37301) A (% silences &gt; 0.521739) then T
if (duration &lt; 1.09537) A (string contains &apos;Baltimore&apos;) then T
if (duration &lt; 0.982051) A (string contains &apos;no&apos;) then T
if (duration &lt; 1.1803) A (confidence &gt; -2.93085) A (grammar = date) then T
if (duration &lt; 1.09537) A (confidence &gt; -2.30717) A (% silences &gt; 0.356436) A (FO Max &gt; 249.225) then T
if (duration &lt; 0.868743) A (confidence &gt; -4.14926 ) A (% silences &gt; 0.51923) A (F0 Max &gt; 205.296) then T
if (duration &lt; 1.18036) A (string contains &apos;Philadelphia&apos;) then T
</construct>
<figure confidence="0.540972">
else F
</figure>
<figureCaption confidence="0.982915">
Figure 2: Ruleset for Predicting Correctly Recognized Turns (WER = 0) from Prosodic and ASR Features.
</figureCaption>
<bodyText confidence="0.99990225">
tures except for RMS mean, max, and prior pause
appear in at least one rule, and that the features
shown to be significant in our statistical analyses
(Section 3) are not the same features as in the rules.
But, as noted above, our data points in these two
experiments differ. It is useful to note though, that
while this ruleset contains all three ASR features,
none of the experimental parameters was found to
be a useful predictor, suggesting that our results are
not specific to the particular conditions of and par-
ticipants in the corpus collection, although they are
specific to the lexicon and grammars.
Results of our learning experiments with mis-
recognition defined in terms of CA rather than WER
show the overall role of the features which predict
WER-defined misrecognition to be less successful
in predicting CA-defined error. Table 4 shows the
relative performance of the same feature sets dis-
cussed above, with misrecognition now defined in
terms of CA&lt;1. As with the WER, experiments, the
best performing feature set makes use of prosodic
and ASR-derived features. However, the predictive
power of prosodic over ASR features decreases when
misrecognition is defined in terms of CA - which is
particularly interesting since ASR confidence scores
are intended to predict WER rather than CA; the er-
ror rate using ASR confidence scores alone (13.52%)
is now significantly lower than the error obtained
using prosody (18.18%). However, prosodic features
still improve the predictive power of ASR confidence
scores, to 11.34%, although this difference is not sig-
nificant at a 95% confidence level. And the error
rate of the three ASR features combined (11.70%) is
reduced to the lowest error rate in our table when
prosodic features are added (10.43%); this error rate
is (just) significantly different from the use of ASR
confidence scores alone. Thus, for CA-defined mis-
recognitions, our experiments have uncovered only
minor improvements over traditional ASR rejection
calculation procedures.
</bodyText>
<sectionHeader confidence="0.998899" genericHeader="discussions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999993146341463">
A statistical comparison of recognized versus mis-
recognized utterances indicates that FO excursion,
loudness, longer prior pause, and longer duration
are significant prosodic characteristics of both WER
and CA-defined failed recognition attempts. Results
from a set of machine learning experiments show
that prosodic differences can in fact be used to im-
prove the prediction of misrecognitions with a high
degree of accuracy (12.76% error) for WER-based
misrecognitions - and an even higher degree (6.53%
error) when combined with information currently
available from ASR systems. The use of ASR confi-
dence scores alone had a predicted WER of 22.23%,
so the improvement over traditional methods is quite
considerable. For CA-defined misrecognitions, the
improvement provided by prosodic features is con-
siderably less. One of our future research directions
will be to understand this difference.
Another future direction will be to address the is-
sue of just why prosodic features provide such use-
ful indicators of recognition failure. Do the features
themselves make recognition difficult, or are they
instead indirect correlates of other phenomena not
captured in our study? While the negative influence
of speaking rate variation on ASR has been reported
before (e.g. (Ostendorf et al., 1996), it is tradition-
ally assumed that ASR is impervious to differences
in FO and RMS; yet, it is known that FO and RMS
variations co-vary to some extent with spectral char-
acteristics (e.g. (Swerts and Veldhuis, 1997; Fant et
al., 1995)), so that it is not unlikely that utterances
with extreme values for these may differ critically
from the training data. Other prosodic features may
be more indirect indicators of errors. Longer ut-
terances may simply provide more chance for error
than shorter ones, while speakers who pause longer
before utterances and take more time making them
may also produce more disfluencies than others.
We are currently replicating our experiment on a
new domain with a new speech recognizer. We are
examining the W99 corpus, which was collected in a
</bodyText>
<page confidence="0.999396">
223
</page>
<tableCaption confidence="0.999177">
Table 4: Estimated Error for Predicting Misrecognized Turns (CA&lt;1).
</tableCaption>
<table confidence="0.999640727272727">
Features Used Error SE
Prosody, ASR Confidence, ASR String, ASR Grammar 10.43% .63
ALL 10.68% .71
Prosody, ASR Confidence, ASR Grammar 11.24% .68
Prosody, ASR Confidence 11.34% .64
ASR Confidence, ASR String, ASR Grammar 11.70% .68
ASR Confidence 13.52% .82
ASR Confidence, ASR Grammar 13.52% .84
ASR String 13.62% .83
Prosody, ASR String 15.04% .84
Prosody 18.18% .85
Duration 18.38% .90
ASR Grammar 22.73% .96
Tempo 24.61% 1.28
Hyperarticulation 25.27% 1.05
FO Mean 28.61% 1.19
FO Max 28.76% .90
RMS Mean 28.86% 1.17
% Silence 28.91% 1.23
RMS Max 29.01% 1.16
Prior Pause 29.22% 1.26
Majority Baseline 28.61%
</table>
<bodyText confidence="0.9996105">
spoken dialogue system that supported registration,
checking paper status, and information access for the
IEEE Automatic Speech Recognition and Under-
standing Workshop (ASRU99) (Rahim et al., 1999).
This system employed the AT&amp;T WATSON speech
recognition technology (Sharp et al., 1997). Prelim-
inary results indicate that our TOOT results do in
fact hold up across recognizers. We also are extend-
ing our TOOT corpus analysis to include prosodic
analyses of turns in which users become aware of
misrecognitions and correct them. In addition, we
are exploring whether prosodic differences can help
explain the &amp;quot;goat&amp;quot; phenomenon - the fact that
some voices are recognized much more poorly than
others (Doddington et al., 1998; Hirschberg et al.,
1999). Our ultimate goal is to provide prosodically-
based mechanisms for identifying and reacting to
ASR failures in SDS systems.
</bodyText>
<sectionHeader confidence="0.992632" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999925833333333">
We would like to thank Jennifer Chu-Carroll, Candy
Kamm, participants in the AT&amp;T &amp;quot;SLUG&amp;quot; seminar
series, and participants in the 1999 JHU Summer
Language Engineering Workshop, for providing us
with useful comments on this research and on earlier
versions of this paper.
</bodyText>
<sectionHeader confidence="0.986181" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.94396140625">
Linda Bell and Joakim Gustafson. 1999. Repe-
tition and its phonetic realizations: Investigat-
ing a Swedish database of spontaneous computer-
directed speech. In Proceedings of ICPhS-99, San
Francisco. International Congress of Phonetic Sci-
ences.
E. Blaauw. 1992. Phonetic differences between read
and spontaneous speech. In Proceedings of IC-
SLP92, volume 1, pages 751-758, Banff.
A. G. Bouwman, J. Sturm, and L. Boves. 1999.
Incorporating confidence measures in the dutch
train timetable information system developed in
the ARISE project. In Proc. International Con-
ference on Acoustics, Speech and Signal Process-
ing, volume 1, pages 493-496, Phoenix.
William Cohen. 1996. Learning trees and rules with
set-valued features. In 14th Conference of the
American Association of Artificial Intelligence,
AAAI.
George Doddington, Walter Liggett, Alvin Martin,
Mark Przybocki, and Douglas Reynolds. 1998.
Sheep, goats, lambs and wolves: A statistical anal-
ysis of speaker performance in the NIST 1998
speaker recognition evaluation. In Proceedings of
ICSLP-98.
G. Fant, J. Liljencrants, I. Karlsson, and
M. Bavegard. 1995. Time and frequency do-
main aspects of voice source modelling. BR
Speechmaps 6975, ESPRIT. Deliverable 27 WP
1.3.
Keikichi Hirose. 1997. Disambiguating recogni-
tion results by prosodic features. In Computing
</bodyText>
<page confidence="0.997009">
224
</page>
<reference confidence="0.99820811">
Prosody: Computational Models for Processing
Spontaneous Speech, pages 327-342. Springer.
Julia Hirschberg, Diane Litman, and Marc Swerts.
1999. Prosodic cues to recognition errors. In Pro-
ceedings of the Automatic Speech Recognition and
Understanding Workshop (ASRU&apos;99).
Julia Hirschberg. 1991. Using text analysis to pre-
dict intonational boundaries. In Proceedings of the
Second European Conference on Speech Commu-
nication and Technology, Genova. ESCA.
C. Kamm, S. Narayanan, D. Dutton, and R. Rite-
flour. 1997. Evaluating spoken dialog systems
for telecommunication services. In 5th European
Conference on Speech Technology and Communi-
cation, EUROSPEECH 97.
Hans Kraayeveld. 1997. Idiosyncrasy in prosody.
Speaker and speaker group identification in Dutch
using melodic and temporal information. Ph.D.
thesis, Nijmegen University.
E. Krahmer, M. Swerts, M. Theune, and
M. Weegels. 1999. Error spotting in human-
machine interactions. In Proceedings of
EUROSPEECH-99.
Irene Langkilde, Marilyn Walker, Jerry Wright,
Al Gorin, and Diane Litman. 1999. Automatic
prediction of problematic human-computer dia-
logues in &apos;how may i help you?&apos;. In Proceedings
of the Automatic Speech Recognition and Under-
standing Workshop (ASRU&apos;99).
Gina-Anne Levow. 1998. Characterizing and recog-
nizing spoken corrections in human-computer dia-
logue. In Proceedings of the 36th Annual Meeting
of the Association of Computational Linguistics,
COLING/ACL 98, pages 736-742.
Diane J. Litman and Shimei Pan. 1999. Empirically
evaluating an adaptable spoken dialogue system.
In Proceedings of the 7th International Conference
on User Modeling (UM).
Diane J. Litman, Marilyn A. Walker, and Michael J.
Kearns. 1999. Automatic detection of poor
speech recognition at the dialogue level. In Pro-
ceedings of the 37th Annual Meeting of the As-
sociation of Computational Linguistics , ACL99,
pages 309-316.
M. Ostendorf, B. Byrne, M. Bacchiani, M. Finke,
A. Gunawardana, K. Ross, S. Roweis, E. Shriberg,
D. Talkin, A. Waibel, B. Wheatley, and T. Zep-
penfeld. 1996. Modeling systematic variations
in pronunciation via a language-dependent hid-
den speaking mode. Report on 1996 CLSP/JHU
Workshop on Innovative Techniques for Large Vo-
cabulary Continuous Speech Recognition.
S. L. Oviatt, G. Levow, M. MacEarchern, and
K. Kuhn. 1996. Modeling hyperarticulate speech
during human-computer error resolution. In Pro-
ceedings of ICSLP-96, pages 801-804, Philadel-
phia.
M. Rahim, R. Pieracini, W. Eckert, E. Levin, G. Di
Fabbrizio, G. Riccardi, C. Lin, and C. Kamm.
1999. W99 - a spoken dialogue system for the
asru&apos;99 workshop. In Proc. ASRU&apos;99.
R.D. Sharp, E. Bocchieri, C. Castillo,
S. Parthasarathy, C. Rath, M. Riley, and
J Rowland. 1997. The watson speech recognition
engine. In Proc. ICASSP97, pages 4065-4068.
Ronnie W. Smith. 1998. An evaluation of strate-
gies for selectively verifying utterance meanings
in spoken natural language dialog. International
Journal of Human-Computer Studies, 48:627-647.
Hagen Soltau and Alex Waibel. 1998. On the in-
fluence of hyperarticulated speech on recognition
performance. In Proceedings of ICSLP-98, Syd-
ney. International Conference on Spoken Lan-
guage Processing.
M. Swerts and M. Ostendorf. 1997. Prosodic
and lexical indications of discourse structure in
human-machine interactions. Speech Communica-
tion, 22:25-41.
Marc Swerts and Raymond Veldhuis. 1997. Interac-
tions between intonation and glottal-pulse char-
acteristics. In A. Botinis, G. Kouroupetroglou,
and G. Carayiannis, editors, Intonation: Theory,
Models and Applications, pages 297-300, Athens.
ESCA.
Nanette Veilleux. 1994. Computational Models of
the Prosody/Syntax Mapping for Spoken Language
Systems. Ph.D. thesis, Boston University.
E. Wade, E. E. Shriberg, and P. J. Price. 1992. User
behaviors affecting speech recognition. In Pro-
ceedings of ICSLP-92, volume 2, pages 995-998,
Banff.
M. Weintraub, K. Taussig, K. Hunicke-Smith, and
A. Snodgrass. 1996. Effect of speaking style on
LVCSR performance. In Proceedings of ICSLP-
96, Philadelphia. International Conference on
Spoken Language Processing.
Ilija Zeljkovic, 1996. Decoding optimal state se-
quences with smooth state likelihoods. In Interna-
tional Conference on Acoustics, Speech, and Sig-
nal Processing, ICASSP 96, pages 129-132.
</reference>
<page confidence="0.998804">
225
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.332530">
<title confidence="0.9996775">Predicting Automatic Speech Recognition Performance Using Prosodic Cues</title>
<author confidence="0.999601">J Litman B Hirschberg Marc Swerts</author>
<affiliation confidence="0.99757">AT&amp;T Labs — Research IPO, Center for User-System Interaction</affiliation>
<address confidence="0.981434">Florham Park, NJ 07932-0971 USA Eindhoven, The Netherlands</address>
<email confidence="0.33645">diane@research.att.cornswerts@ipo.tue.n1</email>
<email confidence="0.33645">julia@research.att.cornswerts@ipo.tue.n1</email>
<abstract confidence="0.999630684210526">In spoken dialogue systems, it is important for a system to know how likely a speech recognition hypothesis is to be correct, so it can reprompt for fresh input, or, in cases where many errors have occurred, change its interaction strategy or switch the caller to a human attendant. We have discovered prosodic features which more accurately predict when a recognition hypothesis contains a word error than the acoustic confidence score thresholds traditionally used in automatic speech recognition. We present analytic results indicating that there are significant prosodic differences between correctly and recognized turns in the information corpus. We then present machine learning results showing how the use of prosodic features to automatically predict correct versus incorrectly recognized turns improves over the use of acoustic confidence scores alone.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Prosody</author>
</authors>
<title>Computational Models for Processing Spontaneous Speech,</title>
<pages>327--342</pages>
<publisher>Springer.</publisher>
<marker>Prosody, </marker>
<rawString>Prosody: Computational Models for Processing Spontaneous Speech, pages 327-342. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hirschberg</author>
<author>Diane Litman</author>
<author>Marc Swerts</author>
</authors>
<title>Prosodic cues to recognition errors.</title>
<date>1999</date>
<booktitle>In Proceedings of the Automatic Speech Recognition and Understanding Workshop (ASRU&apos;99).</booktitle>
<contexts>
<context position="16933" citStr="Hirschberg et al., 1999" startWordPosition="2684" endWordPosition="2687">ce, whenever the confidence score fell below a pre-defined grammar-specific threshold. The other features represent the experimental conditions under which the data was collected (whether users could adapt TOOT&apos;S dialogue strategies, TooT&apos;s initial initiative and confirmation strategies, experimental task, speaker&apos;s name and characteristics). We included these features to determine the extent to which particulars of task, subject, or interaction influenced ASR success rates or our ability to predict them; previous work showed that these factors impact TOOT&apos;S performance (Litman and Pan, 1999; Hirschberg et al., 1999). Except for the task, subject, gender, native language, and hyperarticulation scores, all of our features are automatically available. Table 3 shows the relative performance of a number of the feature sets we examined; results here are for misrecognition defined in terms of WER.5 A baseline classifier for misrecognition, predicting that ASR is always wrong (the majority class of F), has an error of 48.66%. The best performing feature set includes only the raw prosodic and ASR features and reduces this error to an impressive 6.53% +1- .63%. Note that this performance is not improved by adding </context>
<context position="29346" citStr="Hirschberg et al., 1999" startWordPosition="4673" endWordPosition="4676">eech Recognition and Understanding Workshop (ASRU99) (Rahim et al., 1999). This system employed the AT&amp;T WATSON speech recognition technology (Sharp et al., 1997). Preliminary results indicate that our TOOT results do in fact hold up across recognizers. We also are extending our TOOT corpus analysis to include prosodic analyses of turns in which users become aware of misrecognitions and correct them. In addition, we are exploring whether prosodic differences can help explain the &amp;quot;goat&amp;quot; phenomenon - the fact that some voices are recognized much more poorly than others (Doddington et al., 1998; Hirschberg et al., 1999). Our ultimate goal is to provide prosodicallybased mechanisms for identifying and reacting to ASR failures in SDS systems. Acknowledgements We would like to thank Jennifer Chu-Carroll, Candy Kamm, participants in the AT&amp;T &amp;quot;SLUG&amp;quot; seminar series, and participants in the 1999 JHU Summer Language Engineering Workshop, for providing us with useful comments on this research and on earlier versions of this paper. References Linda Bell and Joakim Gustafson. 1999. Repetition and its phonetic realizations: Investigating a Swedish database of spontaneous computerdirected speech. In Proceedings of ICPhS-</context>
</contexts>
<marker>Hirschberg, Litman, Swerts, 1999</marker>
<rawString>Julia Hirschberg, Diane Litman, and Marc Swerts. 1999. Prosodic cues to recognition errors. In Proceedings of the Automatic Speech Recognition and Understanding Workshop (ASRU&apos;99).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hirschberg</author>
</authors>
<title>Using text analysis to predict intonational boundaries.</title>
<date>1991</date>
<booktitle>In Proceedings of the Second European Conference on Speech Communication and Technology,</booktitle>
<location>Genova. ESCA.</location>
<contexts>
<context position="3809" citStr="Hirschberg, 1991" startWordPosition="590" endWordPosition="591">). Several other studies (Wade et al., 1992; Oviatt et al., 1996; Swerts and Ostendorf, 1997; Levow, 1998; Bell and Gustafson, 1999) report that hyperarticulated speech, characterized by careful enunciation, slowed speaking rate, and increase in pitch and loudness, often occurs when users in human-machine interactions try to correct system errors. Still others have shown that such speech also decreases recognition performance (Soltau and Waibel, 1998). Prosodic features have also been shown to be effective in ranking recognition hypotheses, as a post-processing filter to score ASR hypotheses (Hirschberg, 1991; Veilleux, 1994; Hirose, 1997). In this paper we present results of empirical studies testing the hypothesis that prosodic features provide an important clue to ASR performance. We first present results comparing prosodic analyses of correctly and incorrectly recognized speaker turns in TOOT, an experimental SDS for obtaining train information over the phone. We then describe machine learning experiments based on these results that explore the predictive power of prosodic features alone and in combination with other automatically available information, including ASR confidence scores and reco</context>
</contexts>
<marker>Hirschberg, 1991</marker>
<rawString>Julia Hirschberg. 1991. Using text analysis to predict intonational boundaries. In Proceedings of the Second European Conference on Speech Communication and Technology, Genova. ESCA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Kamm</author>
<author>S Narayanan</author>
<author>D Dutton</author>
<author>R Riteflour</author>
</authors>
<title>Evaluating spoken dialog systems for telecommunication services.</title>
<date>1997</date>
<booktitle>In 5th European Conference on Speech Technology and Communication, EUROSPEECH 97.</booktitle>
<contexts>
<context position="5106" citStr="Kamm et al., 1997" startWordPosition="790" endWordPosition="793">nces between correctly and incorrectly recognized utterances. These differences can in fact be used to pre218 dict whether an utterance has been misrecognized, with a high degree of accuracy. 2 The TOOT Corpus Our corpus consists of a set of dialogues between humans and TOOT, an SDS for accessing train schedules from the web via telephone, which was collected to study both variations in SDS strategy and user-adapted interaction (Litman and Pan, 1999). TOOT is implemented on a platform combining ASR, text-to-speech, a phone interface, a finite-state dialogue manager, and application functions (Kamm et al., 1997). The speech recognizer is a speaker-independent hidden Markov model system with context-dependent phone models for telephone speech and constrained grammars for each dialogue state. Confidence scores for recognition were available only at the turn, not the word, level (Zeljkovic, 1996). An example TOOT dialogue is shown in Figure 1. Subjects performed four tasks with one of several versions of TOOT, that differed in terms of locus of initiative (system, user, or mixed), confirmation strategy (explicit, implicit, or none), and whether these conditions could be changed by the user during the ta</context>
</contexts>
<marker>Kamm, Narayanan, Dutton, Riteflour, 1997</marker>
<rawString>C. Kamm, S. Narayanan, D. Dutton, and R. Riteflour. 1997. Evaluating spoken dialog systems for telecommunication services. In 5th European Conference on Speech Technology and Communication, EUROSPEECH 97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Kraayeveld</author>
</authors>
<title>Idiosyncrasy in prosody. Speaker and speaker group identification in Dutch using melodic and temporal information.</title>
<date>1997</date>
<tech>Ph.D. thesis,</tech>
<institution>Nijmegen University.</institution>
<contexts>
<context position="3194" citStr="Kraayeveld, 1997" startWordPosition="500" endWordPosition="501">ognitions do not necessarily lead to misunderstandings at a conceptual level (e.g. &amp;quot;a.m.&amp;quot; recognized as &amp;quot;in the morning&amp;quot;). The current paper looks at prosody as one possible predictor of ASR performance. ASR performance is known to vary based upon speaking style (Weintraub et al., 1996), speaker gender and age, native versus non-native speaker status, and, in general, the deviation of new speech from the training data. Some of this variation is linked to prosody, as prosodic differences have been found to characterize differences in speaking style (Blaauw, 1992) and idiosyncratic differences (Kraayeveld, 1997). Several other studies (Wade et al., 1992; Oviatt et al., 1996; Swerts and Ostendorf, 1997; Levow, 1998; Bell and Gustafson, 1999) report that hyperarticulated speech, characterized by careful enunciation, slowed speaking rate, and increase in pitch and loudness, often occurs when users in human-machine interactions try to correct system errors. Still others have shown that such speech also decreases recognition performance (Soltau and Waibel, 1998). Prosodic features have also been shown to be effective in ranking recognition hypotheses, as a post-processing filter to score ASR hypotheses (H</context>
</contexts>
<marker>Kraayeveld, 1997</marker>
<rawString>Hans Kraayeveld. 1997. Idiosyncrasy in prosody. Speaker and speaker group identification in Dutch using melodic and temporal information. Ph.D. thesis, Nijmegen University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Krahmer</author>
<author>M Swerts</author>
<author>M Theune</author>
<author>M Weegels</author>
</authors>
<title>Error spotting in humanmachine interactions.</title>
<date>1999</date>
<booktitle>In Proceedings of EUROSPEECH-99.</booktitle>
<contexts>
<context position="1838" citStr="Krahmer et al., 1999" startWordPosition="283" endWordPosition="286">e central tasks of the dialogue manager in most current spoken dialogue systems (SDSs) is error handling. The automatic speech recognition (ASR) component of such systems is prone to error, especially when the system has to operate in noisy conditions or when the domain of the system is large. Given that it is impossible to fully prevent ASR errors, it is important for a system to know how likely a speech recognition hypothesis is to be correct, so it can take appropriate action, since users have considerable difficulty correcting incorrect information that is presented by the system as true (Krahmer et al., 1999). Such action may include verifying the user&apos;s input, reprompting for fresh input, or, in cases where many errors have occurred, changing the interaction strategy or switching the caller to a human attendant (Smith, 1998; Litman et al., 1999; Langkilde et al., 1999). Traditionally, the decision to reject a recognition hypothesis is based on acoustic confidence score thresholds, which provide a reliability measure on the hypothesis and are set in the application (Zeljkovic, 1996). However, this process often fails, as there is no simple one-to-one mapping between low confidence scores and incor</context>
</contexts>
<marker>Krahmer, Swerts, Theune, Weegels, 1999</marker>
<rawString>E. Krahmer, M. Swerts, M. Theune, and M. Weegels. 1999. Error spotting in humanmachine interactions. In Proceedings of EUROSPEECH-99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Langkilde</author>
<author>Marilyn Walker</author>
<author>Jerry Wright</author>
<author>Al Gorin</author>
<author>Diane Litman</author>
</authors>
<title>Automatic prediction of problematic human-computer dialogues in &apos;how may i help you?&apos;.</title>
<date>1999</date>
<booktitle>In Proceedings of the Automatic Speech Recognition and Understanding Workshop (ASRU&apos;99).</booktitle>
<contexts>
<context position="2104" citStr="Langkilde et al., 1999" startWordPosition="326" endWordPosition="330">domain of the system is large. Given that it is impossible to fully prevent ASR errors, it is important for a system to know how likely a speech recognition hypothesis is to be correct, so it can take appropriate action, since users have considerable difficulty correcting incorrect information that is presented by the system as true (Krahmer et al., 1999). Such action may include verifying the user&apos;s input, reprompting for fresh input, or, in cases where many errors have occurred, changing the interaction strategy or switching the caller to a human attendant (Smith, 1998; Litman et al., 1999; Langkilde et al., 1999). Traditionally, the decision to reject a recognition hypothesis is based on acoustic confidence score thresholds, which provide a reliability measure on the hypothesis and are set in the application (Zeljkovic, 1996). However, this process often fails, as there is no simple one-to-one mapping between low confidence scores and incorrect recognitions, and the setting of a rejection threshold is a matter of trial and error (Bouwman et al., 1999). Also, some incorrect recognitions do not necessarily lead to misunderstandings at a conceptual level (e.g. &amp;quot;a.m.&amp;quot; recognized as &amp;quot;in the morning&amp;quot;). The </context>
</contexts>
<marker>Langkilde, Walker, Wright, Gorin, Litman, 1999</marker>
<rawString>Irene Langkilde, Marilyn Walker, Jerry Wright, Al Gorin, and Diane Litman. 1999. Automatic prediction of problematic human-computer dialogues in &apos;how may i help you?&apos;. In Proceedings of the Automatic Speech Recognition and Understanding Workshop (ASRU&apos;99).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gina-Anne Levow</author>
</authors>
<title>Characterizing and recognizing spoken corrections in human-computer dialogue.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association of Computational Linguistics, COLING/ACL 98,</booktitle>
<pages>736--742</pages>
<contexts>
<context position="3298" citStr="Levow, 1998" startWordPosition="518" endWordPosition="519">e morning&amp;quot;). The current paper looks at prosody as one possible predictor of ASR performance. ASR performance is known to vary based upon speaking style (Weintraub et al., 1996), speaker gender and age, native versus non-native speaker status, and, in general, the deviation of new speech from the training data. Some of this variation is linked to prosody, as prosodic differences have been found to characterize differences in speaking style (Blaauw, 1992) and idiosyncratic differences (Kraayeveld, 1997). Several other studies (Wade et al., 1992; Oviatt et al., 1996; Swerts and Ostendorf, 1997; Levow, 1998; Bell and Gustafson, 1999) report that hyperarticulated speech, characterized by careful enunciation, slowed speaking rate, and increase in pitch and loudness, often occurs when users in human-machine interactions try to correct system errors. Still others have shown that such speech also decreases recognition performance (Soltau and Waibel, 1998). Prosodic features have also been shown to be effective in ranking recognition hypotheses, as a post-processing filter to score ASR hypotheses (Hirschberg, 1991; Veilleux, 1994; Hirose, 1997). In this paper we present results of empirical studies te</context>
</contexts>
<marker>Levow, 1998</marker>
<rawString>Gina-Anne Levow. 1998. Characterizing and recognizing spoken corrections in human-computer dialogue. In Proceedings of the 36th Annual Meeting of the Association of Computational Linguistics, COLING/ACL 98, pages 736-742.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane J Litman</author>
<author>Shimei Pan</author>
</authors>
<title>Empirically evaluating an adaptable spoken dialogue system.</title>
<date>1999</date>
<booktitle>In Proceedings of the 7th International Conference on User Modeling (UM).</booktitle>
<contexts>
<context position="4942" citStr="Litman and Pan, 1999" startWordPosition="765" endWordPosition="768">with other automatically available information, including ASR confidence scores and recognized string. Our results indicate that there are significant prosodic differences between correctly and incorrectly recognized utterances. These differences can in fact be used to pre218 dict whether an utterance has been misrecognized, with a high degree of accuracy. 2 The TOOT Corpus Our corpus consists of a set of dialogues between humans and TOOT, an SDS for accessing train schedules from the web via telephone, which was collected to study both variations in SDS strategy and user-adapted interaction (Litman and Pan, 1999). TOOT is implemented on a platform combining ASR, text-to-speech, a phone interface, a finite-state dialogue manager, and application functions (Kamm et al., 1997). The speech recognizer is a speaker-independent hidden Markov model system with context-dependent phone models for telephone speech and constrained grammars for each dialogue state. Confidence scores for recognition were available only at the turn, not the word, level (Zeljkovic, 1996). An example TOOT dialogue is shown in Figure 1. Subjects performed four tasks with one of several versions of TOOT, that differed in terms of locus </context>
<context position="16907" citStr="Litman and Pan, 1999" startWordPosition="2679" endWordPosition="2683"> to repeat the utterance, whenever the confidence score fell below a pre-defined grammar-specific threshold. The other features represent the experimental conditions under which the data was collected (whether users could adapt TOOT&apos;S dialogue strategies, TooT&apos;s initial initiative and confirmation strategies, experimental task, speaker&apos;s name and characteristics). We included these features to determine the extent to which particulars of task, subject, or interaction influenced ASR success rates or our ability to predict them; previous work showed that these factors impact TOOT&apos;S performance (Litman and Pan, 1999; Hirschberg et al., 1999). Except for the task, subject, gender, native language, and hyperarticulation scores, all of our features are automatically available. Table 3 shows the relative performance of a number of the feature sets we examined; results here are for misrecognition defined in terms of WER.5 A baseline classifier for misrecognition, predicting that ASR is always wrong (the majority class of F), has an error of 48.66%. The best performing feature set includes only the raw prosodic and ASR features and reduces this error to an impressive 6.53% +1- .63%. Note that this performance </context>
</contexts>
<marker>Litman, Pan, 1999</marker>
<rawString>Diane J. Litman and Shimei Pan. 1999. Empirically evaluating an adaptable spoken dialogue system. In Proceedings of the 7th International Conference on User Modeling (UM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane J Litman</author>
<author>Marilyn A Walker</author>
<author>Michael J Kearns</author>
</authors>
<title>Automatic detection of poor speech recognition at the dialogue level.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association of Computational Linguistics , ACL99,</booktitle>
<pages>309--316</pages>
<contexts>
<context position="2079" citStr="Litman et al., 1999" startWordPosition="322" endWordPosition="325">nditions or when the domain of the system is large. Given that it is impossible to fully prevent ASR errors, it is important for a system to know how likely a speech recognition hypothesis is to be correct, so it can take appropriate action, since users have considerable difficulty correcting incorrect information that is presented by the system as true (Krahmer et al., 1999). Such action may include verifying the user&apos;s input, reprompting for fresh input, or, in cases where many errors have occurred, changing the interaction strategy or switching the caller to a human attendant (Smith, 1998; Litman et al., 1999; Langkilde et al., 1999). Traditionally, the decision to reject a recognition hypothesis is based on acoustic confidence score thresholds, which provide a reliability measure on the hypothesis and are set in the application (Zeljkovic, 1996). However, this process often fails, as there is no simple one-to-one mapping between low confidence scores and incorrect recognitions, and the setting of a rejection threshold is a matter of trial and error (Bouwman et al., 1999). Also, some incorrect recognitions do not necessarily lead to misunderstandings at a conceptual level (e.g. &amp;quot;a.m.&amp;quot; recognized a</context>
</contexts>
<marker>Litman, Walker, Kearns, 1999</marker>
<rawString>Diane J. Litman, Marilyn A. Walker, and Michael J. Kearns. 1999. Automatic detection of poor speech recognition at the dialogue level. In Proceedings of the 37th Annual Meeting of the Association of Computational Linguistics , ACL99, pages 309-316.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ostendorf</author>
<author>B Byrne</author>
<author>M Bacchiani</author>
<author>M Finke</author>
<author>A Gunawardana</author>
<author>K Ross</author>
<author>S Roweis</author>
<author>E Shriberg</author>
<author>D Talkin</author>
<author>A Waibel</author>
<author>B Wheatley</author>
<author>T Zeppenfeld</author>
</authors>
<title>Modeling systematic variations in pronunciation via a language-dependent hidden speaking mode. Report on</title>
<date>1996</date>
<contexts>
<context position="27141" citStr="Ostendorf et al., 1996" startWordPosition="4320" endWordPosition="4323">over traditional methods is quite considerable. For CA-defined misrecognitions, the improvement provided by prosodic features is considerably less. One of our future research directions will be to understand this difference. Another future direction will be to address the issue of just why prosodic features provide such useful indicators of recognition failure. Do the features themselves make recognition difficult, or are they instead indirect correlates of other phenomena not captured in our study? While the negative influence of speaking rate variation on ASR has been reported before (e.g. (Ostendorf et al., 1996), it is traditionally assumed that ASR is impervious to differences in FO and RMS; yet, it is known that FO and RMS variations co-vary to some extent with spectral characteristics (e.g. (Swerts and Veldhuis, 1997; Fant et al., 1995)), so that it is not unlikely that utterances with extreme values for these may differ critically from the training data. Other prosodic features may be more indirect indicators of errors. Longer utterances may simply provide more chance for error than shorter ones, while speakers who pause longer before utterances and take more time making them may also produce mor</context>
</contexts>
<marker>Ostendorf, Byrne, Bacchiani, Finke, Gunawardana, Ross, Roweis, Shriberg, Talkin, Waibel, Wheatley, Zeppenfeld, 1996</marker>
<rawString>M. Ostendorf, B. Byrne, M. Bacchiani, M. Finke, A. Gunawardana, K. Ross, S. Roweis, E. Shriberg, D. Talkin, A. Waibel, B. Wheatley, and T. Zeppenfeld. 1996. Modeling systematic variations in pronunciation via a language-dependent hidden speaking mode. Report on 1996 CLSP/JHU Workshop on Innovative Techniques for Large Vocabulary Continuous Speech Recognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S L Oviatt</author>
<author>G Levow</author>
<author>M MacEarchern</author>
<author>K Kuhn</author>
</authors>
<title>Modeling hyperarticulate speech during human-computer error resolution.</title>
<date>1996</date>
<booktitle>In Proceedings of ICSLP-96,</booktitle>
<pages>801--804</pages>
<location>Philadelphia.</location>
<contexts>
<context position="3257" citStr="Oviatt et al., 1996" startWordPosition="510" endWordPosition="513">onceptual level (e.g. &amp;quot;a.m.&amp;quot; recognized as &amp;quot;in the morning&amp;quot;). The current paper looks at prosody as one possible predictor of ASR performance. ASR performance is known to vary based upon speaking style (Weintraub et al., 1996), speaker gender and age, native versus non-native speaker status, and, in general, the deviation of new speech from the training data. Some of this variation is linked to prosody, as prosodic differences have been found to characterize differences in speaking style (Blaauw, 1992) and idiosyncratic differences (Kraayeveld, 1997). Several other studies (Wade et al., 1992; Oviatt et al., 1996; Swerts and Ostendorf, 1997; Levow, 1998; Bell and Gustafson, 1999) report that hyperarticulated speech, characterized by careful enunciation, slowed speaking rate, and increase in pitch and loudness, often occurs when users in human-machine interactions try to correct system errors. Still others have shown that such speech also decreases recognition performance (Soltau and Waibel, 1998). Prosodic features have also been shown to be effective in ranking recognition hypotheses, as a post-processing filter to score ASR hypotheses (Hirschberg, 1991; Veilleux, 1994; Hirose, 1997). In this paper w</context>
</contexts>
<marker>Oviatt, Levow, MacEarchern, Kuhn, 1996</marker>
<rawString>S. L. Oviatt, G. Levow, M. MacEarchern, and K. Kuhn. 1996. Modeling hyperarticulate speech during human-computer error resolution. In Proceedings of ICSLP-96, pages 801-804, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rahim</author>
<author>R Pieracini</author>
<author>W Eckert</author>
<author>E Levin</author>
<author>G Di Fabbrizio</author>
<author>G Riccardi</author>
<author>C Lin</author>
<author>C Kamm</author>
</authors>
<title>W99 - a spoken dialogue system for the asru&apos;99 workshop. In</title>
<date>1999</date>
<booktitle>Proc. ASRU&apos;99.</booktitle>
<marker>Rahim, Pieracini, Eckert, Levin, Di Fabbrizio, Riccardi, Lin, Kamm, 1999</marker>
<rawString>M. Rahim, R. Pieracini, W. Eckert, E. Levin, G. Di Fabbrizio, G. Riccardi, C. Lin, and C. Kamm. 1999. W99 - a spoken dialogue system for the asru&apos;99 workshop. In Proc. ASRU&apos;99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R D Sharp</author>
<author>E Bocchieri</author>
<author>C Castillo</author>
<author>S Parthasarathy</author>
<author>C Rath</author>
<author>M Riley</author>
<author>J Rowland</author>
</authors>
<title>The watson speech recognition engine.</title>
<date>1997</date>
<booktitle>In Proc. ICASSP97,</booktitle>
<pages>4065--4068</pages>
<contexts>
<context position="28884" citStr="Sharp et al., 1997" startWordPosition="4598" endWordPosition="4601">R Grammar 13.52% .84 ASR String 13.62% .83 Prosody, ASR String 15.04% .84 Prosody 18.18% .85 Duration 18.38% .90 ASR Grammar 22.73% .96 Tempo 24.61% 1.28 Hyperarticulation 25.27% 1.05 FO Mean 28.61% 1.19 FO Max 28.76% .90 RMS Mean 28.86% 1.17 % Silence 28.91% 1.23 RMS Max 29.01% 1.16 Prior Pause 29.22% 1.26 Majority Baseline 28.61% spoken dialogue system that supported registration, checking paper status, and information access for the IEEE Automatic Speech Recognition and Understanding Workshop (ASRU99) (Rahim et al., 1999). This system employed the AT&amp;T WATSON speech recognition technology (Sharp et al., 1997). Preliminary results indicate that our TOOT results do in fact hold up across recognizers. We also are extending our TOOT corpus analysis to include prosodic analyses of turns in which users become aware of misrecognitions and correct them. In addition, we are exploring whether prosodic differences can help explain the &amp;quot;goat&amp;quot; phenomenon - the fact that some voices are recognized much more poorly than others (Doddington et al., 1998; Hirschberg et al., 1999). Our ultimate goal is to provide prosodicallybased mechanisms for identifying and reacting to ASR failures in SDS systems. Acknowledgemen</context>
</contexts>
<marker>Sharp, Bocchieri, Castillo, Parthasarathy, Rath, Riley, Rowland, 1997</marker>
<rawString>R.D. Sharp, E. Bocchieri, C. Castillo, S. Parthasarathy, C. Rath, M. Riley, and J Rowland. 1997. The watson speech recognition engine. In Proc. ICASSP97, pages 4065-4068.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronnie W Smith</author>
</authors>
<title>An evaluation of strategies for selectively verifying utterance meanings in spoken natural language dialog.</title>
<date>1998</date>
<journal>International Journal of Human-Computer Studies,</journal>
<pages>48--627</pages>
<contexts>
<context position="2058" citStr="Smith, 1998" startWordPosition="320" endWordPosition="321">e in noisy conditions or when the domain of the system is large. Given that it is impossible to fully prevent ASR errors, it is important for a system to know how likely a speech recognition hypothesis is to be correct, so it can take appropriate action, since users have considerable difficulty correcting incorrect information that is presented by the system as true (Krahmer et al., 1999). Such action may include verifying the user&apos;s input, reprompting for fresh input, or, in cases where many errors have occurred, changing the interaction strategy or switching the caller to a human attendant (Smith, 1998; Litman et al., 1999; Langkilde et al., 1999). Traditionally, the decision to reject a recognition hypothesis is based on acoustic confidence score thresholds, which provide a reliability measure on the hypothesis and are set in the application (Zeljkovic, 1996). However, this process often fails, as there is no simple one-to-one mapping between low confidence scores and incorrect recognitions, and the setting of a rejection threshold is a matter of trial and error (Bouwman et al., 1999). Also, some incorrect recognitions do not necessarily lead to misunderstandings at a conceptual level (e.g</context>
</contexts>
<marker>Smith, 1998</marker>
<rawString>Ronnie W. Smith. 1998. An evaluation of strategies for selectively verifying utterance meanings in spoken natural language dialog. International Journal of Human-Computer Studies, 48:627-647.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hagen Soltau</author>
<author>Alex Waibel</author>
</authors>
<title>On the influence of hyperarticulated speech on recognition performance.</title>
<date>1998</date>
<booktitle>In Proceedings of ICSLP-98, Sydney. International Conference on Spoken Language Processing.</booktitle>
<contexts>
<context position="3648" citStr="Soltau and Waibel, 1998" startWordPosition="565" endWordPosition="568">s linked to prosody, as prosodic differences have been found to characterize differences in speaking style (Blaauw, 1992) and idiosyncratic differences (Kraayeveld, 1997). Several other studies (Wade et al., 1992; Oviatt et al., 1996; Swerts and Ostendorf, 1997; Levow, 1998; Bell and Gustafson, 1999) report that hyperarticulated speech, characterized by careful enunciation, slowed speaking rate, and increase in pitch and loudness, often occurs when users in human-machine interactions try to correct system errors. Still others have shown that such speech also decreases recognition performance (Soltau and Waibel, 1998). Prosodic features have also been shown to be effective in ranking recognition hypotheses, as a post-processing filter to score ASR hypotheses (Hirschberg, 1991; Veilleux, 1994; Hirose, 1997). In this paper we present results of empirical studies testing the hypothesis that prosodic features provide an important clue to ASR performance. We first present results comparing prosodic analyses of correctly and incorrectly recognized speaker turns in TOOT, an experimental SDS for obtaining train information over the phone. We then describe machine learning experiments based on these results that ex</context>
</contexts>
<marker>Soltau, Waibel, 1998</marker>
<rawString>Hagen Soltau and Alex Waibel. 1998. On the influence of hyperarticulated speech on recognition performance. In Proceedings of ICSLP-98, Sydney. International Conference on Spoken Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Swerts</author>
<author>M Ostendorf</author>
</authors>
<title>Prosodic and lexical indications of discourse structure in human-machine interactions.</title>
<date>1997</date>
<journal>Speech Communication,</journal>
<pages>22--25</pages>
<contexts>
<context position="3285" citStr="Swerts and Ostendorf, 1997" startWordPosition="514" endWordPosition="517"> &amp;quot;a.m.&amp;quot; recognized as &amp;quot;in the morning&amp;quot;). The current paper looks at prosody as one possible predictor of ASR performance. ASR performance is known to vary based upon speaking style (Weintraub et al., 1996), speaker gender and age, native versus non-native speaker status, and, in general, the deviation of new speech from the training data. Some of this variation is linked to prosody, as prosodic differences have been found to characterize differences in speaking style (Blaauw, 1992) and idiosyncratic differences (Kraayeveld, 1997). Several other studies (Wade et al., 1992; Oviatt et al., 1996; Swerts and Ostendorf, 1997; Levow, 1998; Bell and Gustafson, 1999) report that hyperarticulated speech, characterized by careful enunciation, slowed speaking rate, and increase in pitch and loudness, often occurs when users in human-machine interactions try to correct system errors. Still others have shown that such speech also decreases recognition performance (Soltau and Waibel, 1998). Prosodic features have also been shown to be effective in ranking recognition hypotheses, as a post-processing filter to score ASR hypotheses (Hirschberg, 1991; Veilleux, 1994; Hirose, 1997). In this paper we present results of empiric</context>
</contexts>
<marker>Swerts, Ostendorf, 1997</marker>
<rawString>M. Swerts and M. Ostendorf. 1997. Prosodic and lexical indications of discourse structure in human-machine interactions. Speech Communication, 22:25-41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Swerts</author>
<author>Raymond Veldhuis</author>
</authors>
<title>Interactions between intonation and glottal-pulse characteristics.</title>
<date>1997</date>
<booktitle>Intonation: Theory, Models and Applications,</booktitle>
<pages>297--300</pages>
<editor>In A. Botinis, G. Kouroupetroglou, and G. Carayiannis, editors,</editor>
<location>Athens. ESCA.</location>
<contexts>
<context position="27353" citStr="Swerts and Veldhuis, 1997" startWordPosition="4357" endWordPosition="4360">is difference. Another future direction will be to address the issue of just why prosodic features provide such useful indicators of recognition failure. Do the features themselves make recognition difficult, or are they instead indirect correlates of other phenomena not captured in our study? While the negative influence of speaking rate variation on ASR has been reported before (e.g. (Ostendorf et al., 1996), it is traditionally assumed that ASR is impervious to differences in FO and RMS; yet, it is known that FO and RMS variations co-vary to some extent with spectral characteristics (e.g. (Swerts and Veldhuis, 1997; Fant et al., 1995)), so that it is not unlikely that utterances with extreme values for these may differ critically from the training data. Other prosodic features may be more indirect indicators of errors. Longer utterances may simply provide more chance for error than shorter ones, while speakers who pause longer before utterances and take more time making them may also produce more disfluencies than others. We are currently replicating our experiment on a new domain with a new speech recognizer. We are examining the W99 corpus, which was collected in a 223 Table 4: Estimated Error for Pre</context>
</contexts>
<marker>Swerts, Veldhuis, 1997</marker>
<rawString>Marc Swerts and Raymond Veldhuis. 1997. Interactions between intonation and glottal-pulse characteristics. In A. Botinis, G. Kouroupetroglou, and G. Carayiannis, editors, Intonation: Theory, Models and Applications, pages 297-300, Athens. ESCA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nanette Veilleux</author>
</authors>
<title>Computational Models of the Prosody/Syntax Mapping for Spoken Language Systems.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>Boston University.</institution>
<contexts>
<context position="3825" citStr="Veilleux, 1994" startWordPosition="592" endWordPosition="593">tudies (Wade et al., 1992; Oviatt et al., 1996; Swerts and Ostendorf, 1997; Levow, 1998; Bell and Gustafson, 1999) report that hyperarticulated speech, characterized by careful enunciation, slowed speaking rate, and increase in pitch and loudness, often occurs when users in human-machine interactions try to correct system errors. Still others have shown that such speech also decreases recognition performance (Soltau and Waibel, 1998). Prosodic features have also been shown to be effective in ranking recognition hypotheses, as a post-processing filter to score ASR hypotheses (Hirschberg, 1991; Veilleux, 1994; Hirose, 1997). In this paper we present results of empirical studies testing the hypothesis that prosodic features provide an important clue to ASR performance. We first present results comparing prosodic analyses of correctly and incorrectly recognized speaker turns in TOOT, an experimental SDS for obtaining train information over the phone. We then describe machine learning experiments based on these results that explore the predictive power of prosodic features alone and in combination with other automatically available information, including ASR confidence scores and recognized string. O</context>
</contexts>
<marker>Veilleux, 1994</marker>
<rawString>Nanette Veilleux. 1994. Computational Models of the Prosody/Syntax Mapping for Spoken Language Systems. Ph.D. thesis, Boston University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Wade</author>
<author>E E Shriberg</author>
<author>P J Price</author>
</authors>
<title>User behaviors affecting speech recognition.</title>
<date>1992</date>
<booktitle>In Proceedings of ICSLP-92,</booktitle>
<volume>2</volume>
<pages>995--998</pages>
<location>Banff.</location>
<contexts>
<context position="3236" citStr="Wade et al., 1992" startWordPosition="506" endWordPosition="509">derstandings at a conceptual level (e.g. &amp;quot;a.m.&amp;quot; recognized as &amp;quot;in the morning&amp;quot;). The current paper looks at prosody as one possible predictor of ASR performance. ASR performance is known to vary based upon speaking style (Weintraub et al., 1996), speaker gender and age, native versus non-native speaker status, and, in general, the deviation of new speech from the training data. Some of this variation is linked to prosody, as prosodic differences have been found to characterize differences in speaking style (Blaauw, 1992) and idiosyncratic differences (Kraayeveld, 1997). Several other studies (Wade et al., 1992; Oviatt et al., 1996; Swerts and Ostendorf, 1997; Levow, 1998; Bell and Gustafson, 1999) report that hyperarticulated speech, characterized by careful enunciation, slowed speaking rate, and increase in pitch and loudness, often occurs when users in human-machine interactions try to correct system errors. Still others have shown that such speech also decreases recognition performance (Soltau and Waibel, 1998). Prosodic features have also been shown to be effective in ranking recognition hypotheses, as a post-processing filter to score ASR hypotheses (Hirschberg, 1991; Veilleux, 1994; Hirose, 1</context>
<context position="13435" citStr="Wade et al. (1992)" startWordPosition="2147" endWordPosition="2150">pause, and longer duration) are all features previously associated with hyperarticulated speech. Since prior research has suggested that speakers may respond to failed recognition attempts by hyperarticulating, which itself may lead to more recognition failures, had we in fact simply identified a means of characterizing and identifying hyperarticulated speech prosodically? Since we had independently labeled all speaker turns for evidence of hyperarticulation (two of the authors labeled each turn as &amp;quot;not hyperarticulated&amp;quot;, &amp;quot;some hyperarticulation in the turn&amp;quot;, and &amp;quot;hyperarticulated&amp;quot;, following Wade et al. (1992)), we were able to test this possibility. We excluded any turn either labeler had labeled as partially or fully hyperarticulated, and again performed paired t-tests on mean values of misrecognized versus recognized turns for each speaker. Results show that for both WER-defined and CA-defined misrecognitions, not only are the same features significant differentiators when hyperarticulated turns are excluded from the analysis, but in addition, tempo also is significant for WER-defined misrecognition. So, our findings 220 for the prosodic characteristics of recognized and of misrecognized turns h</context>
</contexts>
<marker>Wade, Shriberg, Price, 1992</marker>
<rawString>E. Wade, E. E. Shriberg, and P. J. Price. 1992. User behaviors affecting speech recognition. In Proceedings of ICSLP-92, volume 2, pages 995-998, Banff.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Weintraub</author>
<author>K Taussig</author>
<author>K Hunicke-Smith</author>
<author>A Snodgrass</author>
</authors>
<title>Effect of speaking style on LVCSR performance.</title>
<date>1996</date>
<booktitle>In Proceedings of ICSLP96, Philadelphia. International Conference on Spoken Language Processing.</booktitle>
<contexts>
<context position="2864" citStr="Weintraub et al., 1996" startWordPosition="446" endWordPosition="450">ility measure on the hypothesis and are set in the application (Zeljkovic, 1996). However, this process often fails, as there is no simple one-to-one mapping between low confidence scores and incorrect recognitions, and the setting of a rejection threshold is a matter of trial and error (Bouwman et al., 1999). Also, some incorrect recognitions do not necessarily lead to misunderstandings at a conceptual level (e.g. &amp;quot;a.m.&amp;quot; recognized as &amp;quot;in the morning&amp;quot;). The current paper looks at prosody as one possible predictor of ASR performance. ASR performance is known to vary based upon speaking style (Weintraub et al., 1996), speaker gender and age, native versus non-native speaker status, and, in general, the deviation of new speech from the training data. Some of this variation is linked to prosody, as prosodic differences have been found to characterize differences in speaking style (Blaauw, 1992) and idiosyncratic differences (Kraayeveld, 1997). Several other studies (Wade et al., 1992; Oviatt et al., 1996; Swerts and Ostendorf, 1997; Levow, 1998; Bell and Gustafson, 1999) report that hyperarticulated speech, characterized by careful enunciation, slowed speaking rate, and increase in pitch and loudness, often</context>
</contexts>
<marker>Weintraub, Taussig, Hunicke-Smith, Snodgrass, 1996</marker>
<rawString>M. Weintraub, K. Taussig, K. Hunicke-Smith, and A. Snodgrass. 1996. Effect of speaking style on LVCSR performance. In Proceedings of ICSLP96, Philadelphia. International Conference on Spoken Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilija Zeljkovic</author>
</authors>
<title>Decoding optimal state sequences with smooth state likelihoods.</title>
<date>1996</date>
<booktitle>In International Conference on Acoustics, Speech, and Signal Processing, ICASSP 96,</booktitle>
<pages>129--132</pages>
<contexts>
<context position="2321" citStr="Zeljkovic, 1996" startWordPosition="362" endWordPosition="363">ce users have considerable difficulty correcting incorrect information that is presented by the system as true (Krahmer et al., 1999). Such action may include verifying the user&apos;s input, reprompting for fresh input, or, in cases where many errors have occurred, changing the interaction strategy or switching the caller to a human attendant (Smith, 1998; Litman et al., 1999; Langkilde et al., 1999). Traditionally, the decision to reject a recognition hypothesis is based on acoustic confidence score thresholds, which provide a reliability measure on the hypothesis and are set in the application (Zeljkovic, 1996). However, this process often fails, as there is no simple one-to-one mapping between low confidence scores and incorrect recognitions, and the setting of a rejection threshold is a matter of trial and error (Bouwman et al., 1999). Also, some incorrect recognitions do not necessarily lead to misunderstandings at a conceptual level (e.g. &amp;quot;a.m.&amp;quot; recognized as &amp;quot;in the morning&amp;quot;). The current paper looks at prosody as one possible predictor of ASR performance. ASR performance is known to vary based upon speaking style (Weintraub et al., 1996), speaker gender and age, native versus non-native speake</context>
<context position="5393" citStr="Zeljkovic, 1996" startWordPosition="833" endWordPosition="834">ccessing train schedules from the web via telephone, which was collected to study both variations in SDS strategy and user-adapted interaction (Litman and Pan, 1999). TOOT is implemented on a platform combining ASR, text-to-speech, a phone interface, a finite-state dialogue manager, and application functions (Kamm et al., 1997). The speech recognizer is a speaker-independent hidden Markov model system with context-dependent phone models for telephone speech and constrained grammars for each dialogue state. Confidence scores for recognition were available only at the turn, not the word, level (Zeljkovic, 1996). An example TOOT dialogue is shown in Figure 1. Subjects performed four tasks with one of several versions of TOOT, that differed in terms of locus of initiative (system, user, or mixed), confirmation strategy (explicit, implicit, or none), and whether these conditions could be changed by the user during the task. Subjects were 39 students, 20 native speakers of standard American English and 19 non-native speakers; 16 subjects were female and 23 male. Dialogues were recorded and system and user behavior logged automatically. The concept accuracy (CA) of each turn was manually labeled by one o</context>
</contexts>
<marker>Zeljkovic, 1996</marker>
<rawString>Ilija Zeljkovic, 1996. Decoding optimal state sequences with smooth state likelihoods. In International Conference on Acoustics, Speech, and Signal Processing, ICASSP 96, pages 129-132.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>