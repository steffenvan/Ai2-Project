<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.970254">
A Hybrid Approach for Named Entity and Sub-Type Tagging
</title>
<author confidence="0.836816">
Rohini Srihari Cheng Niu and Wei Li
</author>
<affiliation confidence="0.671699">
Cymfony Net, Inc. Cymfony Net, Inc.
</affiliation>
<address confidence="0.6684055">
5500 Main Street 5500 Main Street
Williamsville, NY 14260 Williamsville, NY 14260
</address>
<email confidence="0.941433">
rohini@cymfony.com chengniu@cymfony.com
wei@cymfony.com
</email>
<sectionHeader confidence="0.98553" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999779111111111">
This paper presents a hybrid approach for
named entity (NE) tagging which combines
Maximum Entropy Model (MaxEnt), Hidden
Markov Model (HMM) and handcrafted
grammatical rules. Each has innate strengths
and weaknesses; the combination results in a
very high precision tagger. MaxEnt includes
external gazetteers in the system. Sub-category
generation is also discussed.
</bodyText>
<sectionHeader confidence="0.812751" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.999964418181818">
Named entity (NE) tagging is a task in which
location names, person names, organization
names, monetary amounts, time and percentage
expressions are recognized and classified in
unformatted text documents. This task provides
important semantic information, and is a critical
first step in any information extraction system.
Intense research has been focused on
improving NE tagging accuracy using several
different techniques. These include rule-based
systems [Krupka 1998], Hidden Markov Models
(HMM) [Bikel et al. 1997] and Maximum
Entropy Models (MaxEnt) [Borthwick 1998]. A
system based on manual rules may provide the
best performance; however these require
painstaking intense skilled labor. Furthermore,
shifting domains involves significant effort and
may result in performance degradation. The
strength of HMM models lie in their capacity for
modeling local contextual information. HMMs
have been widely used in continuous speech
recognition, part-of-speech tagging, OCR, etc.,
and are generally regarded as the most successful
statistical modelling paradigm in these domains.
MaxEnt is a powerful tool to be used in situations
where several ambiguous information sources
need to be combined. Since statistical techniques
such as HMM are only as good as the data they
are trained on, they are required to use back-off
models to compensate for unreliable statistics. In
contrast to empirical back-off models used in
HMMs, MaxEnt provides a systematic method
by which a statistical model consistent with all
obtained knowledge can be trained. [Borthwick
et al. 1998] discuss a technique for combining the
output of several NE taggers in a black box
fashion by using MaxEnt. They demonstrate the
superior performance of this system; however,
the system is computationally inefficient since
many taggers need to be run.
In this paper we prop,ose a hybrid method for
NE tagging which combines all the modelling
techniques mentioned above. NE tagging is a
complex task and high-performance systems are
required in order to be practically usable.
Furthermore, the task demonstrates
characteristics that can be exploited by all three
techniques. For example, time and monetary
expressions are fairly predictable and hence
processed most efficiently with handcrafted
grammar rules. Name, location and organization
entities are highly variable and thus lend
themselves to statistical training algorithms such
as HMMs. Finally, many conflicting pieces of
information regarding the class of a tag are
</bodyText>
<footnote confidence="0.6045505">
* This work was supported in part by the SBIR grant F30602-98-C-0043 from Air Force Research Laboratory
(AFRL)/IFED.
</footnote>
<page confidence="0.996203">
247
</page>
<bodyText confidence="0.9416786">
frequently present. This includes information
from less than perfect gazetteers. For this, a
MaxEnt approach works well in utilizing diverse
sources of information in determining the final
tag. The structure of our system is shown in
</bodyText>
<figureCaption confidence="0.62289">
Figure 1.
</figureCaption>
<figure confidence="0.922203444444445">
PeTINVLMNAc
Corner (Maoist)
(mein ANCII
/Anal Rlehlxile
(=trailed IMA
1 M.CStarckrdW
SibcatgptadmiNtwErt)
1
Fnt knits ON Mategries)
</figure>
<subsectionHeader confidence="0.965764">
Struzsed PE Twar
</subsectionHeader>
<bodyText confidence="0.999978657894737">
The first module is a rule-based tagger
containing pattern match rules, or templates, for
time, date, percentage, and monetary
expressions. These tags include the standard
MUC tags [Chinchor 1998], as well as several
other sub-categories defined by our organization.
More details concerning the sub-categories are
presented later. The pattern matcher is based on
Finite State Transducer (FST) technology
[Roches &amp; Schabes 1997] that has been
implemented in-house. The subsequent modules
are focused on location, person and organization
names. The second module assigns tentative
person and location tags based on external person
and location gazetteers. Rather than relying on
simple lookup of the gazetteer which is very error
prone, this module employs MaxEnt to build a
statistical model that incorporates gazetteers with
common contextual information. The core
module of the system is a bigram-based HMM
[Bikel et al.1997]. Rules designed to correct
errors in NE segmentation are incorporated into a
constrained HMM network. These rules serve as
constraints on the HMM model and enable it to
utilize information beyond bigrams and remove
obvious errors due to the limitation of the training
corpus. HMM generates the standard MUC tags,
person, location and organization. Based on
MaxEnt, the last module derives sub-categories
such as city, airport, government, etc. from the
basic tags.
Section 1 describes the FST rule module.
Section 2 discusses combining gazetteer
information using MaxEnt. The constrained
HMM is described in Section 3. Section 4
discusses sub-type generation by MaxEnt. The
experimental results and conclusion are
presented finally.
</bodyText>
<sectionHeader confidence="0.871135" genericHeader="method">
1 FST-based Pattern Matching Rules for
</sectionHeader>
<subsectionHeader confidence="0.941124">
Textract NE
</subsectionHeader>
<bodyText confidence="0.999244482758621">
The most attractive feature of the FST (Finite
State Transducer) formalism lies in its superior
time and space efficiency [Mohri 1997] [Roche
&amp; Schabes 1997]. Applying a deterministic FST
depends linearly only on the input size of the text.
Our experiments also show that an FST rule
system is extraordinarily robust. In addition, it
has been verified by many research programs
[Krupka &amp; Hausman 1998] [Hobbs 1993]
[Silberztein 1998] [Srihari 1998] [Li &amp; Srihari
2000], that FST is also a convenient tool for
capturing linguistic phenomena, especially for
idioms and semi-productive expressions like time
NEs and numerical NEs.
The rules which we have currently
implemented include a grammar for temporal
expressions (time, date, duration, frequency, age,
etc.), a grammar for numerical expressions
(money, percentage, length, weight, etc.), and a
grammar for other non-MUC NEs (e.g. contact
information like address, email).
The following sample pattern rules give an
idea of what our NE grammars look like. These
rules capture typical US addresses, like: 5500
Main St., Williamsville, NY14221; 12345 Xyz
Avenue, Apt. 678, Los Angeles, CA98765-432I.
The following notation is used: @ for macro; 1
for logical OR; + for one or more; (...) for
optionality.
</bodyText>
<equation confidence="0.985188518518519">
0_9 = 0 11 1 2 1 3 1 4 1 5 1 6 1 7 1 8 1 9
number = @0_9+
uppercase = AIBICID1EIFIGIHII1J1
KILIMINIO1P1Q1RISIT
UIVIWIXIY1Z
248
lowercase= alblcIdielfIglhliliikIll
mInI
xlylz
letter = @uppercase I @lowercase
word = @letter+
delimiter = (&amp;quot;,&amp;quot;) &amp;quot; &amp;quot;+
zip = @0_9 @0_9 @0_9 @0_9 @0_9
(&amp;quot;-&amp;quot; @0_9 @0_9 @0_9 @0_9)
street= aStISTIRdIRDIDrIDRI
Ave I AVE ] (&amp;quot;.&amp;quot;)] I Street I
Road I Drive I Avenue
city = @word (@ word)
state = @uppercase (&amp;quot;.&amp;quot;) @uppercase (&amp;quot;.&amp;quot;)
us = USA I U.S.A I US I U.S. I
(The) United States (of America)
street_addr = @number @word @street
apt_addr = [APT (&amp;quot;.&amp;quot;) I Apt (&amp;quot;.&amp;quot;)
Apartment] @number
local_addr = @street_addr
(@delimiter @apt_addr)
address = @local_addr
</equation>
<bodyText confidence="0.973115529411765">
@delimiter @city
@delimiter @state @zip
(@ delimiter @us)
Our work is similar to the research on FST
local grammars at LADL/University Paris VII
[Silberztein 1998]1, but that research was not
turned into a functional rule based NE system.
The rules in our NE grammars cover
expressions with very predictable patterns. They
were designed to address the weaknesses of our
statistical NE tagger. For example, the following
missings (underlined) and mistagging originally
made by our statistical NE tagger have all been
correctly identified by our temporal NE
grammar.
began &lt;TIMEX TYPE=&amp;quot;DATE&amp;quot;&gt;Dec. 15,
the&lt;JTIMEX&gt; space agency
on Jan. 28, &lt;TIMEX
TYPE=&amp;quot;DATE&amp;quot;&gt;1986&lt;/TIMEX&gt;,
in September &lt;TIMEX
TYPE=&amp;quot;DATE&amp;quot;&gt;1994&lt;/TIMEX&gt;on &lt;TIMEX
1 They have made public their research results at their
website (http://www.ladl. jussieu fr/index.html),
including a grammar for certain temporal expressions
and a grammar for stock exchange sub-language.
TYPE=&amp;quot;TIME&amp;quot;&gt;Saturday at&lt;/TIMEX&gt; 2:42
a.m. ES&lt;ENAMEX
TYPE=&amp;quot;PERSON&amp;quot;&gt;T.&lt;/ENAMEX&gt;
He left the United States in &lt;TIMEX
TYPE=&amp;quot;DATE&amp;quot;&gt;1984 and&lt;/TIMEX&gt; moved
in early &lt;TIMEX TYPE=&amp;quot;DATE&amp;quot;&gt;1962
and&lt;JTIMEX&gt;
in &lt;TIMEX TYPE=&amp;quot;DATE&amp;quot;&gt;1987 the
Bonn&lt;/TIMEX&gt; government ruled
</bodyText>
<sectionHeader confidence="0.9659515" genericHeader="method">
2 Incorporating Gazetteers with the
Maximum Entropy Model
</sectionHeader>
<bodyText confidence="0.999707878787879">
We use two gazetteers in our system, one for
person and one for location. The person gazetteer
consists of 3,000 male names, 5,000 female
names and 14,000 family names. The location
gazetteer consists of 250,000 location names with
their categories such as CITY, PROVINCE,
COUNTRY, AIRPORT, etc. The containing and
being-contained relationship among locations is
also provided.
The following is a sample line in the location
gazetteer, which denotes &amp;quot;Aberdeen&amp;quot; as a city in
&amp;quot;California&amp;quot;, and &amp;quot;California&amp;quot; as a province of
&amp;quot;United States&amp;quot;.
Aberdeen (CITY) California (PROVINCE)
United States (COUNTRY)
Although gazetteers obviously contain useful
name entity information, a straightforward word
match approach may even degrade the system
performance since the information from
gazetteers is too ambiguous. There are a lot of
common words that exist in the gazetteers, such
as &amp;quot;I&amp;quot;, &amp;quot;A&amp;quot;, &amp;quot;Friday&amp;quot;, &amp;quot;June&amp;quot;, &amp;quot;Friendship&amp;quot;, etc.
Also, there is large overlap between person
names and location names, such as &amp;quot;Clinton&amp;quot;,
&amp;quot;Jordan&amp;quot;, etc.
Here we propose a machine learning
approach to incorporate the gazetteer information
with other common contextual information based
on MaxEnt. Using MaxEnt, the system may
learn under what situation the occurrence in
gazetteers is a reliable evidence for a name entity.
We first define &amp;quot;LFEATURE&amp;quot; based on
occurrence in the location gazetteer as follows:
</bodyText>
<page confidence="0.981744">
249
</page>
<equation confidence="0.255902">
COUNTRY (country name)
USSTATE (US state name)
</equation>
<bodyText confidence="0.790966516129032">
MULTITOKEN (a location name consisting
of multiple tokens)
BIGCITY (a location name occurring
in OXFD dictionary)
COEXIST (where COEXIST(A,B) is
true iff A and B are in the same US state, or in
the same foreign country)
OTHER
There is precedence from the first
LFEATURE to the last one. Each token in the
input document is assigned a unique
&amp;quot;LFEATURE&amp;quot;. We also define &amp;quot;NFEATURE&amp;quot;
based on occurrence in the name gazetteer as
follows:
FAMILY (family name)
MALE (male name)
FEMALE (female name)
FAMILYANDMALE (family and male
name)
FAMILYANDFEMALE (family and female
name)
OTHER
With these two extra features, every token in
the document is regarded as a three-component
vector (word, LFEATURE, NFEATURE). We
can build a statistical model to evaluate the
conditional probability based on these contextual
and gazetteer features. Here &amp;quot;tag&amp;quot; represents one
of the three possible tags (Person, Location,
Other), and history represents any possible
contextual history. Generally, we have:
</bodyText>
<equation confidence="0.97100875">
p(tag,history)
p(tag !history) = (1)
p(tag ,history)
tag
</equation>
<bodyText confidence="0.901033">
A maximum entropy solution for probability has
the form [Rosenfeld 1994] [Ratnaparkhi 1998]
</bodyText>
<equation confidence="0.94997">
JJ af,(history Jag)
p(tag,history)= (2)
Z (history)
Z(history) = HE a if,(history,tag )
tag. i
</equation>
<bodyText confidence="0.953346607142857">
where f ,(history , tag) are binary-valued feature
functions that are dependent on whether the
feature is applicable to the current contextual
history. Here is an example of our feature
function:
if current token is a country name, and tag is location
otherwise
(4)
In (2) and (3) oci are weights associated to feature
functions.
The weight evaluation scheme is as follows:
We first compute the average value of each
feature function according to a specific training
corpus. The obtained average observations are
set as constraints, and the Improved Iterative
Scaling (IIS) algorithm [Pieta et al. 1995] is
employed to evaluate the weights. The resulting
probability distribution (2) possesses the
maximum entropy among all the probability
distributions consistent with the constraints
imposed by feature function average values.
In the training stage, our gazetteer module
contains two sub-modules: feature function
induction and weight evaluation [Pietra et al.
1995]. The structure is shown in Figure 2.
We predefine twenty-four feature function
templates. The following are some examples and
others have similar structures:
</bodyText>
<table confidence="0.8650164">
Rule Searching Space
Rule Selection Module
Select next rule reduce the entropy most
Evaluate weight for each selected rule
Iterative Scaling (11S)
</table>
<figure confidence="0.54073132">
Fig.2, Structure of MasEnt learning Process
j(histoty,tag)—
°di
{1 if LFEATURE = _,and tag = _
(3)
f (history,tag)= 0 else
250
f (history,tag)
I
11
f (history,tag) —
0
f (history, tag) —
- 0
11
f (history, tag) —
- 0
if NFEATURE = _, and tag = _
el se
if current word = _, and tag = _
else
if previous word = _,and tag = _
else
if following word = _, and tag = _
else
</figure>
<bodyText confidence="0.987530209876543">
current module generates results with the highest
possible precision. For each tagged token we will
compute the entropy of the answer. If the entropy
is higher than a pre-set threshold, the system will
not be certain enough about the answer, and the
word will be untagged. The missed location or
person names may be recognized by the
following HMM module.
where the symbol &amp;quot;_&amp;quot; denotes any possible
values which may be inserted into that field.
Different fields will be filled different values.
Then, using a training corpus containing
230,000 tokens, we set up a feature function
candidate space based on the feature function
templates. The &amp;quot;Feature Function Induction
Module&amp;quot; can select next feature function that
reduces the Kullback-Leibler divergence the
most [Pietra et al. 1995]. To make the weight
evaluation computation tractable at the feature
function induction stage, when trying a new
feature function, all previous computed weights
are held constant, and we only fit one new
constraint that is imposed by the candidate
feature function. Once the next feature function
is selected, we recalculate the weights by IIS to
satisfy all the constraints, and thus obtain the next
tentative probability. The feature function
induction module will stop when the
Log-likelihood gain is less than a pre-set
threshold.
The gazetteer module recognizes the person
and location names in the document despite the
fact that some of them may be embedded in an
organization name. For example, &amp;quot;New York
Fire Department&amp;quot; may be tagged as
&lt;LOCATION&gt; New York &lt;/NE&gt; Fire
Department. In the input stream for HMM, each
token being tagged as location is accordingly
transformed into one of the built-in tokens
&amp;quot;CITY&amp;quot; ,&amp;quot;PROVINCE&amp;quot;, &amp;quot;COUNTRY&amp;quot;. The
HMM may group &amp;quot;CITY Fire Department&amp;quot; into
an organization name. A similar technique is
applied for person names.
Since the tagged tokens from the gazetteer
module are regarded by later modules as either
person or location names, we require that the
3 Improving NE Segmentation through
constrained HMM
Our original HMM is similar to the Nymble
[Bikel et al. 1997] system that is based on bigram
statistics. To correct some of the leading errors,
we incorporate manual segmentation rules with
HMM. These syntactic rules may provide
information beyond bigram and balance the
limitation of the training corpus.
Our manual rules focus on improving the NE
segmentation. For example, in the token
sequence &amp;quot;College of William and Mary&amp;quot;, we
have rules based on global sequence checking to
determine if the words &amp;quot;and&amp;quot; or &amp;quot;of&apos; are common
words or parts of organization name.
The output of the rules are some constraints
on the HMM transition network, such as &amp;quot;same
tags for tokens A, B&amp;quot;, or &amp;quot;common word for
token A&amp;quot;. The Viterbi algorithm will select the
optimized path that is consistent with such
constraints.
The manual rules are divided into three
categories: (i) preposition disambiguation, (ii)
spurious capitalized word disambiguation, and
(iii) spurious NE sequence disambiguation.
The rules of preposition disambiguation are
responsible for determination of boundaries
involving prepositions (&amp;quot;of&apos;, &amp;quot;and&amp;quot;, &amp;quot;s&amp;quot;, etc.).
For example, for the sequence &amp;quot;A of B&amp;quot;, we have
the following rule: A and B have same tags if the
lowercase of A and B both occur in OXFD
dictionary. A &amp;quot;global word sequence checking&amp;quot;
[Mikheev, 1999] is also employed. For the
sequence &amp;quot;Sprint and MCI&amp;quot;, we search the
document globally. If the word &amp;quot;Sprint&amp;quot; or
</bodyText>
<page confidence="0.993542">
251
</page>
<bodyText confidence="0.999717777777778">
&amp;quot;MCI&amp;quot; occurs individually somewhere else, we
mark &amp;quot;and&amp;quot; as a common word.
The rules of spurious capitalized word
disambiguation are designed to recognize the
first word in the sentence. If the first word is
unknown in the training corpus, but occurs in
OXFD as a common word in lowercase, HHM&apos;s
unknown word model may be not accurate
enough. The rules in the following paragraph are
designed to treat such a situation.
If the second word of the same sentence is in
lowercase, the first word is tagged as a common
word since it never occurs as an isolated NE
token in the training corpus unless it has been
recognized as a NE elsewhere in the document.
If the second word is capitalized, we will check
globally if the same sequence occurs somewhere
else. If so, the HMM is constrained to assign the
same tag to the two tokens. Otherwise, the
capitalized token is tagged as a common word.
The rules of spurious NE sequence
disambiguation are responsible for finding
spurious NE output from HMM, adding
constraints, and re-computing NE by HMM. For
example, in a sequence &amp;quot;Person Organization&amp;quot;,
we will require the same output tag for these two
tokens and run HMM again.
</bodyText>
<sectionHeader confidence="0.98989" genericHeader="conclusions">
4 NE Sub-Type Tagging using Maximum
</sectionHeader>
<subsectionHeader confidence="0.993686">
Entropy Model
</subsectionHeader>
<bodyText confidence="0.99975475">
The output document from constrained HMM
contains MUC-standard NE tags such as person,
location and organization. However, for a real
information extraction system, the
MUC-standard NE tag may not be enough and
further detailed NE information might be
necessary. We have predefined the following
sub-types for person, location and organization:
</bodyText>
<figure confidence="0.967797761904762">
Person: Military Person
Religious Person
Man
Woman
Location: City
Province
Country
Continent
Lake
River
Mountain
Road
Region
District
Airport
Organization: Company
Government
Army
School
Association
Mass Medium
</figure>
<bodyText confidence="0.995105608695652">
If a NE is not covered by any of the above
sub-categories, it should remain a MUC-standard
tag. Obviously, the sub-categorization requires
much more information beyond bigram than
MUC-standard tagging. For example, it is hard
to recognize CNN as a Mass Media company by
bigram if the token &amp;quot;CNN&amp;quot; never occurs in the
training corpus. External gazetteer information is
critical for some sub-category recognition, and
trigger word models may also play an important
role.
With such considerations, we use the
Maximum entropy model for sub-categorization,
since MaxEnt is powerful enough to incorporate
into the system gazetteer or other information
sources which might become available at some
later time.
Similar to the gazetteer module in Section 2,
the sub-categorization module in the training
stage contains two sub-modules, (i) feature
function induction and (ii) weight evaluation.
We have the following seven feature function
templates:
</bodyText>
<equation confidence="0.904805055555556">
1 if MUC_tag --= _,and tag = _
f (history,tag) =
0 else
f (history, tag) = {1 if MUCMUC_tag = _, LFEATURE .-- _,and tag = _
0 else
{1 if contain_wordL), MUC_tag(history) = _, and tag = _
f (history,tag)=
o
_e
_ { i if Previous_Word = MUC_tag = _, and tag = _
— lse
f (history, tag)
0
else
_11 if following_Word = _,MUC_
f (history,tag) = 1tag = _,and tag = _
f (history, tag) —
0
</equation>
<construct confidence="0.6849">
if MUC_tag = _, contaieni_semale_name, and tag = _
</construct>
<page confidence="0.859012">
1.0
</page>
<subsubsectionHeader confidence="0.16648">
else
</subsubsectionHeader>
<page confidence="0.940197">
252
</page>
<equation confidence="0.85854">
{1 if MUC_tag = contain_female_name, and tag = _
f (history,tag) =
0 else
</equation>
<bodyText confidence="0.996569695652174">
We have trained 1,000 feature functions by
the feature function induction module according
to the above templates.
Because much more external gazetteer
information is necessary for the
sub-categorization and there is an overlap
between male and female name gazetteers, the
result from the current MaxEnt module is not
sufficiently accurate. Therefore, a conservative
strategy has been applied. If the entropy of the
output answer is higher than a threshold, we will
back-off to the MUC-standard tags. Unlike
MUC NE categories, local contextual
information is not sufficient for
sub-categorization. In the future more external
gazetteers focusing on recognition of
government, company, army, etc. will be
incorporated into our system. And we are
considering using trigger words [Rosenfeld,
1994] to recognize some sub-categories. For
example, &amp;quot;psalms&amp;quot; may be a trigger word for
&amp;quot;religious person&amp;quot;, and &amp;quot;Navy&amp;quot; may be a trigger
word for &amp;quot;military person&amp;quot;.
</bodyText>
<subsectionHeader confidence="0.86039">
Experiment and Conclusion
</subsectionHeader>
<bodyText confidence="0.9997868">
We have tested our system on MUC-7 dry run
data; this data consists of 22,000 words and
represents articles from The New York Times.
Since a key was provided with the data, it is
possible to properly evaluate the performance of
our NE tagger. The scoring program computes
both the precision and recall, and combines these
two measures into f-measure as the weighted
harmonic mean [Chinchor, 1998]. The formulas
are as follows:
</bodyText>
<table confidence="0.634739166666667">
number of correct responses
Precision —
number responses
number of correct responses
Recall =
number correct in key
F = (fi2 +1)Precision *Recall
(102Recall) + Precision
The score of our system is as follows:
Recall Precision
Organization 95 95
Person 96 93
Location 96 94
Date 92 91
Time 92 91
Money 100 86
Percentage 100 75
F-measure =93.39
</table>
<tableCaption confidence="0.409569">
If the gazetteer module is removed from our
system, and the constrained HMM is restored to
the standard HMM, the f-measures for person,
location, and organization are as follows:
</tableCaption>
<table confidence="0.997136">
Recall Precision
Organization 94 92
Person 95 91
Location 95 92
</table>
<bodyText confidence="0.974713785714286">
Obviously, our gazetteer model and
constrained HMM have greatly increased the
system accuracy on the recognition of persons,
locations, and organizations. Currently, there are
some errors in our gazetteers. Some common
words such as &amp;quot;Changes&amp;quot;, &amp;quot;USER&amp;quot;,
&amp;quot;Administrator&amp;quot;, etc. are mistakenly included in
the person name gazetteer. Also, too many
person names are included into the location
gazetteer. By cleaning up the gazetteers, we can
continue improving the precision on person name
and locations.
We also ran our NE tagger on the formal test
files of MUC-7. The following are the results:
</bodyText>
<table confidence="0.914233">
Recall Precision
Person 92 95
Organization 85 86
Location 90 92
Date 95 85
253
Time 79 72
Money 95 82
Percentage 97 80
Overall F-measure 89
</table>
<bodyText confidence="0.995808157894737">
There is some performance degradation in
the formal test. This decrease is because that the
formal test is focused on satellite and rocket
domains in which our system has not been
trained. There are some person/location names
used as spacecraft or robot names (ex. Mir, Alvin,
Columbia...), and there are many high-tech
company names which do not occur in our HMM
training corpus. Since the finding of organization
names totally relies on the HMM model, it suffers
most from domain shift (10% degradation). This
difference implies that gazetteer information may
be useful in overcoming the domain dependency.
This paper has demonstrated improved
performance in an NE tagger by combining
symbolic and statistical approaches. MaxEnt has
been demonstrated to be a viable technique for
integrating diverse sources of information and
has been used in NE sub-categorization.
</bodyText>
<sectionHeader confidence="0.993659" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997240729166667">
G. R Krupka and K. Hausman, &amp;quot;IsoQuest Inc:
Description of the NetOwl Text Extraction System
as used for MUC-7&amp;quot; in Proceedings of Seventh
Machine Understanding Conference (MUC-7)
(1998)
D. M. Bikel, &amp;quot;Nymble: a high-performance learning
name-finder&amp;quot; in Proceedings of the Fifth
Conference on Applied Natural Language
Processing, 1997, pp. 194-201, Morgan Kaufmann
Publishers.
A. Borthwick, et al., Description of the MENE named
Entity System, In Proceedings of the Seventh
Machine Understanding Conference (MUC-7)
(1998)
R. Rosenfeld, Adaptive Statistical language Modeling,
PHD thesis, Carnegie Mellon University, (1994)
A. Ratnaparkhi, Maximum Entropy Models for
Natural Language Ambiguity resolution, PHD
thesis, Univ. of Pennsylvania, (1998)
S. D. Pietra, Vincent Della Pietra, and John Lafferty,
Inducing Features of Random Fields, Tech Report,
Carnegie Mellon University, (1995)
A. Mikheev, A Knowledge-free Method for
Capitalized Word Disambiguation, in Proceedings
of the 37th Annual Meeting of the Association for
Computational Linguistics, (1999), pp. 159-166
J. R. Hobbs, 1993. FASTUS: A System for Extracting
Information from Text, Proceedings of the DARPA
workshop on Human Language Technology&amp;quot;,
Princeton, NJ, pp. 133-137.
Emmanuel Roche &amp; Yves Schabes, 1997. Finite-State
Language Processing, The MIT Press, Cambridge,
MA.
Li, W &amp; Srihari, R. 2000. Flexible Information
Extraction Learning Algorithm, Final Technical
Report, Air Force Research Laboratory, Rome
Research Site, New York
M. Silberztein, 1998. Tutorial Notes: Finite State
Processing with INTEX, COLING-ACI298,
Montreal (also available at
http://w w w.ladl.j ussieufr)
M. Mohri,. 1997. Finite-State Transducers in
Language and Speech Processing, Computational
Linguistics, Vol. 23, No. 2, pp. 269-311.
R. Srihari, 1998. A Domain Independent Event
Extraction Toolkit, AFRL-IF-RS-TR-1998-152
Final Technical Report, Air Force Research
Laboratory, Rome Research Site, New York
</reference>
<page confidence="0.998651">
254
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.917224">
<title confidence="0.99994">A Hybrid Approach for Named Entity and Sub-Type Tagging</title>
<author confidence="0.999072">Rohini Srihari Cheng Niu</author>
<author confidence="0.999072">Wei Li</author>
<affiliation confidence="0.942906">Cymfony Net, Inc. Cymfony Net, Inc.</affiliation>
<address confidence="0.999328">5500 Main Street 5500 Main Street Williamsville, NY 14260 Williamsville, NY 14260</address>
<email confidence="0.999044">wei@cymfony.com</email>
<abstract confidence="0.9971383">This paper presents a hybrid approach for named entity (NE) tagging which combines Maximum Entropy Model (MaxEnt), Hidden Markov Model (HMM) and handcrafted grammatical rules. Each has innate strengths and weaknesses; the combination results in a very high precision tagger. MaxEnt includes external gazetteers in the system. Sub-category generation is also discussed.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G R Krupka</author>
<author>K Hausman</author>
</authors>
<title>IsoQuest Inc: Description of the NetOwl Text Extraction System as used for MUC-7&amp;quot; in</title>
<date>1998</date>
<booktitle>Proceedings of Seventh Machine Understanding Conference (MUC-7)</booktitle>
<contexts>
<context position="5780" citStr="Krupka &amp; Hausman 1998" startWordPosition="854" endWordPosition="857"> MaxEnt. The constrained HMM is described in Section 3. Section 4 discusses sub-type generation by MaxEnt. The experimental results and conclusion are presented finally. 1 FST-based Pattern Matching Rules for Textract NE The most attractive feature of the FST (Finite State Transducer) formalism lies in its superior time and space efficiency [Mohri 1997] [Roche &amp; Schabes 1997]. Applying a deterministic FST depends linearly only on the input size of the text. Our experiments also show that an FST rule system is extraordinarily robust. In addition, it has been verified by many research programs [Krupka &amp; Hausman 1998] [Hobbs 1993] [Silberztein 1998] [Srihari 1998] [Li &amp; Srihari 2000], that FST is also a convenient tool for capturing linguistic phenomena, especially for idioms and semi-productive expressions like time NEs and numerical NEs. The rules which we have currently implemented include a grammar for temporal expressions (time, date, duration, frequency, age, etc.), a grammar for numerical expressions (money, percentage, length, weight, etc.), and a grammar for other non-MUC NEs (e.g. contact information like address, email). The following sample pattern rules give an idea of what our NE grammars lo</context>
</contexts>
<marker>Krupka, Hausman, 1998</marker>
<rawString>G. R Krupka and K. Hausman, &amp;quot;IsoQuest Inc: Description of the NetOwl Text Extraction System as used for MUC-7&amp;quot; in Proceedings of Seventh Machine Understanding Conference (MUC-7) (1998)</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Bikel</author>
</authors>
<title>Nymble: a high-performance learning name-finder&amp;quot;</title>
<date>1997</date>
<booktitle>in Proceedings of the Fifth Conference on Applied Natural Language Processing,</booktitle>
<pages>194--201</pages>
<publisher>Morgan Kaufmann Publishers.</publisher>
<marker>Bikel, 1997</marker>
<rawString>D. M. Bikel, &amp;quot;Nymble: a high-performance learning name-finder&amp;quot; in Proceedings of the Fifth Conference on Applied Natural Language Processing, 1997, pp. 194-201, Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Borthwick</author>
</authors>
<title>Description of the MENE named Entity System,</title>
<date>1998</date>
<booktitle>In Proceedings of the Seventh Machine Understanding Conference (MUC-7)</booktitle>
<contexts>
<context position="1230" citStr="Borthwick 1998" startWordPosition="170" endWordPosition="171">neration is also discussed. Introduction Named entity (NE) tagging is a task in which location names, person names, organization names, monetary amounts, time and percentage expressions are recognized and classified in unformatted text documents. This task provides important semantic information, and is a critical first step in any information extraction system. Intense research has been focused on improving NE tagging accuracy using several different techniques. These include rule-based systems [Krupka 1998], Hidden Markov Models (HMM) [Bikel et al. 1997] and Maximum Entropy Models (MaxEnt) [Borthwick 1998]. A system based on manual rules may provide the best performance; however these require painstaking intense skilled labor. Furthermore, shifting domains involves significant effort and may result in performance degradation. The strength of HMM models lie in their capacity for modeling local contextual information. HMMs have been widely used in continuous speech recognition, part-of-speech tagging, OCR, etc., and are generally regarded as the most successful statistical modelling paradigm in these domains. MaxEnt is a powerful tool to be used in situations where several ambiguous information </context>
</contexts>
<marker>Borthwick, 1998</marker>
<rawString>A. Borthwick, et al., Description of the MENE named Entity System, In Proceedings of the Seventh Machine Understanding Conference (MUC-7) (1998)</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rosenfeld</author>
</authors>
<title>Adaptive Statistical language Modeling, PHD thesis,</title>
<date>1994</date>
<institution>Carnegie Mellon University,</institution>
<contexts>
<context position="11091" citStr="Rosenfeld 1994" startWordPosition="1661" endWordPosition="1662">LYANDMALE (family and male name) FAMILYANDFEMALE (family and female name) OTHER With these two extra features, every token in the document is regarded as a three-component vector (word, LFEATURE, NFEATURE). We can build a statistical model to evaluate the conditional probability based on these contextual and gazetteer features. Here &amp;quot;tag&amp;quot; represents one of the three possible tags (Person, Location, Other), and history represents any possible contextual history. Generally, we have: p(tag,history) p(tag !history) = (1) p(tag ,history) tag A maximum entropy solution for probability has the form [Rosenfeld 1994] [Ratnaparkhi 1998] JJ af,(history Jag) p(tag,history)= (2) Z (history) Z(history) = HE a if,(history,tag ) tag. i where f ,(history , tag) are binary-valued feature functions that are dependent on whether the feature is applicable to the current contextual history. Here is an example of our feature function: if current token is a country name, and tag is location otherwise (4) In (2) and (3) oci are weights associated to feature functions. The weight evaluation scheme is as follows: We first compute the average value of each feature function according to a specific training corpus. The obtai</context>
<context position="20340" citStr="Rosenfeld, 1994" startWordPosition="3162" endWordPosition="3163">or the sub-categorization and there is an overlap between male and female name gazetteers, the result from the current MaxEnt module is not sufficiently accurate. Therefore, a conservative strategy has been applied. If the entropy of the output answer is higher than a threshold, we will back-off to the MUC-standard tags. Unlike MUC NE categories, local contextual information is not sufficient for sub-categorization. In the future more external gazetteers focusing on recognition of government, company, army, etc. will be incorporated into our system. And we are considering using trigger words [Rosenfeld, 1994] to recognize some sub-categories. For example, &amp;quot;psalms&amp;quot; may be a trigger word for &amp;quot;religious person&amp;quot;, and &amp;quot;Navy&amp;quot; may be a trigger word for &amp;quot;military person&amp;quot;. Experiment and Conclusion We have tested our system on MUC-7 dry run data; this data consists of 22,000 words and represents articles from The New York Times. Since a key was provided with the data, it is possible to properly evaluate the performance of our NE tagger. The scoring program computes both the precision and recall, and combines these two measures into f-measure as the weighted harmonic mean [Chinchor, 1998]. The formulas are</context>
</contexts>
<marker>Rosenfeld, 1994</marker>
<rawString>R. Rosenfeld, Adaptive Statistical language Modeling, PHD thesis, Carnegie Mellon University, (1994)</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>Maximum Entropy Models for Natural Language Ambiguity resolution, PHD thesis,</title>
<date>1998</date>
<institution>Univ. of Pennsylvania,</institution>
<contexts>
<context position="11110" citStr="Ratnaparkhi 1998" startWordPosition="1663" endWordPosition="1664"> and male name) FAMILYANDFEMALE (family and female name) OTHER With these two extra features, every token in the document is regarded as a three-component vector (word, LFEATURE, NFEATURE). We can build a statistical model to evaluate the conditional probability based on these contextual and gazetteer features. Here &amp;quot;tag&amp;quot; represents one of the three possible tags (Person, Location, Other), and history represents any possible contextual history. Generally, we have: p(tag,history) p(tag !history) = (1) p(tag ,history) tag A maximum entropy solution for probability has the form [Rosenfeld 1994] [Ratnaparkhi 1998] JJ af,(history Jag) p(tag,history)= (2) Z (history) Z(history) = HE a if,(history,tag ) tag. i where f ,(history , tag) are binary-valued feature functions that are dependent on whether the feature is applicable to the current contextual history. Here is an example of our feature function: if current token is a country name, and tag is location otherwise (4) In (2) and (3) oci are weights associated to feature functions. The weight evaluation scheme is as follows: We first compute the average value of each feature function according to a specific training corpus. The obtained average observa</context>
</contexts>
<marker>Ratnaparkhi, 1998</marker>
<rawString>A. Ratnaparkhi, Maximum Entropy Models for Natural Language Ambiguity resolution, PHD thesis, Univ. of Pennsylvania, (1998)</rawString>
</citation>
<citation valid="true">
<authors>
<author>S D Pietra</author>
<author>Vincent Della Pietra</author>
<author>John Lafferty</author>
</authors>
<title>Inducing Features of Random Fields,</title>
<date>1995</date>
<tech>Tech Report,</tech>
<institution>Carnegie Mellon University,</institution>
<contexts>
<context position="12175" citStr="Pietra et al. 1995" startWordPosition="1823" endWordPosition="1826">n scheme is as follows: We first compute the average value of each feature function according to a specific training corpus. The obtained average observations are set as constraints, and the Improved Iterative Scaling (IIS) algorithm [Pieta et al. 1995] is employed to evaluate the weights. The resulting probability distribution (2) possesses the maximum entropy among all the probability distributions consistent with the constraints imposed by feature function average values. In the training stage, our gazetteer module contains two sub-modules: feature function induction and weight evaluation [Pietra et al. 1995]. The structure is shown in Figure 2. We predefine twenty-four feature function templates. The following are some examples and others have similar structures: Rule Searching Space Rule Selection Module Select next rule reduce the entropy most Evaluate weight for each selected rule Iterative Scaling (11S) Fig.2, Structure of MasEnt learning Process j(histoty,tag)— °di {1 if LFEATURE = _,and tag = _ (3) f (history,tag)= 0 else 250 f (history,tag) I 11 f (history,tag) — 0 f (history, tag) — - 0 11 f (history, tag) — - 0 if NFEATURE = _, and tag = _ el se if current word = _, and tag = _ else if </context>
<context position="13633" citStr="Pietra et al. 1995" startWordPosition="2075" endWordPosition="2078">a pre-set threshold, the system will not be certain enough about the answer, and the word will be untagged. The missed location or person names may be recognized by the following HMM module. where the symbol &amp;quot;_&amp;quot; denotes any possible values which may be inserted into that field. Different fields will be filled different values. Then, using a training corpus containing 230,000 tokens, we set up a feature function candidate space based on the feature function templates. The &amp;quot;Feature Function Induction Module&amp;quot; can select next feature function that reduces the Kullback-Leibler divergence the most [Pietra et al. 1995]. To make the weight evaluation computation tractable at the feature function induction stage, when trying a new feature function, all previous computed weights are held constant, and we only fit one new constraint that is imposed by the candidate feature function. Once the next feature function is selected, we recalculate the weights by IIS to satisfy all the constraints, and thus obtain the next tentative probability. The feature function induction module will stop when the Log-likelihood gain is less than a pre-set threshold. The gazetteer module recognizes the person and location names in</context>
</contexts>
<marker>Pietra, Pietra, Lafferty, 1995</marker>
<rawString>S. D. Pietra, Vincent Della Pietra, and John Lafferty, Inducing Features of Random Fields, Tech Report, Carnegie Mellon University, (1995)</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mikheev</author>
</authors>
<title>A Knowledge-free Method for Capitalized Word Disambiguation,</title>
<date>1999</date>
<booktitle>in Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>159--166</pages>
<contexts>
<context position="16199" citStr="Mikheev, 1999" startWordPosition="2486" endWordPosition="2487"> A&amp;quot;. The Viterbi algorithm will select the optimized path that is consistent with such constraints. The manual rules are divided into three categories: (i) preposition disambiguation, (ii) spurious capitalized word disambiguation, and (iii) spurious NE sequence disambiguation. The rules of preposition disambiguation are responsible for determination of boundaries involving prepositions (&amp;quot;of&apos;, &amp;quot;and&amp;quot;, &amp;quot;s&amp;quot;, etc.). For example, for the sequence &amp;quot;A of B&amp;quot;, we have the following rule: A and B have same tags if the lowercase of A and B both occur in OXFD dictionary. A &amp;quot;global word sequence checking&amp;quot; [Mikheev, 1999] is also employed. For the sequence &amp;quot;Sprint and MCI&amp;quot;, we search the document globally. If the word &amp;quot;Sprint&amp;quot; or 251 &amp;quot;MCI&amp;quot; occurs individually somewhere else, we mark &amp;quot;and&amp;quot; as a common word. The rules of spurious capitalized word disambiguation are designed to recognize the first word in the sentence. If the first word is unknown in the training corpus, but occurs in OXFD as a common word in lowercase, HHM&apos;s unknown word model may be not accurate enough. The rules in the following paragraph are designed to treat such a situation. If the second word of the same sentence is in lowercase, the firs</context>
</contexts>
<marker>Mikheev, 1999</marker>
<rawString>A. Mikheev, A Knowledge-free Method for Capitalized Word Disambiguation, in Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, (1999), pp. 159-166</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Hobbs</author>
</authors>
<title>FASTUS: A System for Extracting Information from Text,</title>
<date>1993</date>
<booktitle>Proceedings of the DARPA workshop on Human Language Technology&amp;quot;,</booktitle>
<pages>133--137</pages>
<location>Princeton, NJ,</location>
<contexts>
<context position="5793" citStr="Hobbs 1993" startWordPosition="858" endWordPosition="859"> HMM is described in Section 3. Section 4 discusses sub-type generation by MaxEnt. The experimental results and conclusion are presented finally. 1 FST-based Pattern Matching Rules for Textract NE The most attractive feature of the FST (Finite State Transducer) formalism lies in its superior time and space efficiency [Mohri 1997] [Roche &amp; Schabes 1997]. Applying a deterministic FST depends linearly only on the input size of the text. Our experiments also show that an FST rule system is extraordinarily robust. In addition, it has been verified by many research programs [Krupka &amp; Hausman 1998] [Hobbs 1993] [Silberztein 1998] [Srihari 1998] [Li &amp; Srihari 2000], that FST is also a convenient tool for capturing linguistic phenomena, especially for idioms and semi-productive expressions like time NEs and numerical NEs. The rules which we have currently implemented include a grammar for temporal expressions (time, date, duration, frequency, age, etc.), a grammar for numerical expressions (money, percentage, length, weight, etc.), and a grammar for other non-MUC NEs (e.g. contact information like address, email). The following sample pattern rules give an idea of what our NE grammars look like. Thes</context>
</contexts>
<marker>Hobbs, 1993</marker>
<rawString>J. R. Hobbs, 1993. FASTUS: A System for Extracting Information from Text, Proceedings of the DARPA workshop on Human Language Technology&amp;quot;, Princeton, NJ, pp. 133-137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanuel Roche</author>
<author>Yves Schabes</author>
</authors>
<title>Finite-State Language Processing,</title>
<date>1997</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="5536" citStr="Roche &amp; Schabes 1997" startWordPosition="814" endWordPosition="817"> location and organization. Based on MaxEnt, the last module derives sub-categories such as city, airport, government, etc. from the basic tags. Section 1 describes the FST rule module. Section 2 discusses combining gazetteer information using MaxEnt. The constrained HMM is described in Section 3. Section 4 discusses sub-type generation by MaxEnt. The experimental results and conclusion are presented finally. 1 FST-based Pattern Matching Rules for Textract NE The most attractive feature of the FST (Finite State Transducer) formalism lies in its superior time and space efficiency [Mohri 1997] [Roche &amp; Schabes 1997]. Applying a deterministic FST depends linearly only on the input size of the text. Our experiments also show that an FST rule system is extraordinarily robust. In addition, it has been verified by many research programs [Krupka &amp; Hausman 1998] [Hobbs 1993] [Silberztein 1998] [Srihari 1998] [Li &amp; Srihari 2000], that FST is also a convenient tool for capturing linguistic phenomena, especially for idioms and semi-productive expressions like time NEs and numerical NEs. The rules which we have currently implemented include a grammar for temporal expressions (time, date, duration, frequency, age, </context>
</contexts>
<marker>Roche, Schabes, 1997</marker>
<rawString>Emmanuel Roche &amp; Yves Schabes, 1997. Finite-State Language Processing, The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Li</author>
<author>R Srihari</author>
</authors>
<title>Flexible Information Extraction Learning Algorithm,</title>
<date>2000</date>
<tech>Final Technical Report,</tech>
<institution>Air Force Research Laboratory, Rome Research Site,</institution>
<location>New York</location>
<contexts>
<context position="5847" citStr="Li &amp; Srihari 2000" startWordPosition="864" endWordPosition="867">sses sub-type generation by MaxEnt. The experimental results and conclusion are presented finally. 1 FST-based Pattern Matching Rules for Textract NE The most attractive feature of the FST (Finite State Transducer) formalism lies in its superior time and space efficiency [Mohri 1997] [Roche &amp; Schabes 1997]. Applying a deterministic FST depends linearly only on the input size of the text. Our experiments also show that an FST rule system is extraordinarily robust. In addition, it has been verified by many research programs [Krupka &amp; Hausman 1998] [Hobbs 1993] [Silberztein 1998] [Srihari 1998] [Li &amp; Srihari 2000], that FST is also a convenient tool for capturing linguistic phenomena, especially for idioms and semi-productive expressions like time NEs and numerical NEs. The rules which we have currently implemented include a grammar for temporal expressions (time, date, duration, frequency, age, etc.), a grammar for numerical expressions (money, percentage, length, weight, etc.), and a grammar for other non-MUC NEs (e.g. contact information like address, email). The following sample pattern rules give an idea of what our NE grammars look like. These rules capture typical US addresses, like: 5500 Main </context>
</contexts>
<marker>Li, Srihari, 2000</marker>
<rawString>Li, W &amp; Srihari, R. 2000. Flexible Information Extraction Learning Algorithm, Final Technical Report, Air Force Research Laboratory, Rome Research Site, New York</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Silberztein</author>
</authors>
<title>Tutorial Notes: Finite State Processing with INTEX, COLING-ACI298, Montreal (also available at http://w w w.ladl.j ussieufr)</title>
<date>1998</date>
<journal>M. Mohri,.</journal>
<booktitle>Finite-State Transducers in Language and Speech Processing, Computational Linguistics,</booktitle>
<volume>23</volume>
<pages>269--311</pages>
<contexts>
<context position="5812" citStr="Silberztein 1998" startWordPosition="860" endWordPosition="861">ibed in Section 3. Section 4 discusses sub-type generation by MaxEnt. The experimental results and conclusion are presented finally. 1 FST-based Pattern Matching Rules for Textract NE The most attractive feature of the FST (Finite State Transducer) formalism lies in its superior time and space efficiency [Mohri 1997] [Roche &amp; Schabes 1997]. Applying a deterministic FST depends linearly only on the input size of the text. Our experiments also show that an FST rule system is extraordinarily robust. In addition, it has been verified by many research programs [Krupka &amp; Hausman 1998] [Hobbs 1993] [Silberztein 1998] [Srihari 1998] [Li &amp; Srihari 2000], that FST is also a convenient tool for capturing linguistic phenomena, especially for idioms and semi-productive expressions like time NEs and numerical NEs. The rules which we have currently implemented include a grammar for temporal expressions (time, date, duration, frequency, age, etc.), a grammar for numerical expressions (money, percentage, length, weight, etc.), and a grammar for other non-MUC NEs (e.g. contact information like address, email). The following sample pattern rules give an idea of what our NE grammars look like. These rules capture typ</context>
<context position="7469" citStr="Silberztein 1998" startWordPosition="1132" endWordPosition="1133">tter+ delimiter = (&amp;quot;,&amp;quot;) &amp;quot; &amp;quot;+ zip = @0_9 @0_9 @0_9 @0_9 @0_9 (&amp;quot;-&amp;quot; @0_9 @0_9 @0_9 @0_9) street= aStISTIRdIRDIDrIDRI Ave I AVE ] (&amp;quot;.&amp;quot;)] I Street I Road I Drive I Avenue city = @word (@ word) state = @uppercase (&amp;quot;.&amp;quot;) @uppercase (&amp;quot;.&amp;quot;) us = USA I U.S.A I US I U.S. I (The) United States (of America) street_addr = @number @word @street apt_addr = [APT (&amp;quot;.&amp;quot;) I Apt (&amp;quot;.&amp;quot;) Apartment] @number local_addr = @street_addr (@delimiter @apt_addr) address = @local_addr @delimiter @city @delimiter @state @zip (@ delimiter @us) Our work is similar to the research on FST local grammars at LADL/University Paris VII [Silberztein 1998]1, but that research was not turned into a functional rule based NE system. The rules in our NE grammars cover expressions with very predictable patterns. They were designed to address the weaknesses of our statistical NE tagger. For example, the following missings (underlined) and mistagging originally made by our statistical NE tagger have all been correctly identified by our temporal NE grammar. began &lt;TIMEX TYPE=&amp;quot;DATE&amp;quot;&gt;Dec. 15, the&lt;JTIMEX&gt; space agency on Jan. 28, &lt;TIMEX TYPE=&amp;quot;DATE&amp;quot;&gt;1986&lt;/TIMEX&gt;, in September &lt;TIMEX TYPE=&amp;quot;DATE&amp;quot;&gt;1994&lt;/TIMEX&gt;on &lt;TIMEX 1 They have made public their research </context>
</contexts>
<marker>Silberztein, 1998</marker>
<rawString>M. Silberztein, 1998. Tutorial Notes: Finite State Processing with INTEX, COLING-ACI298, Montreal (also available at http://w w w.ladl.j ussieufr) M. Mohri,. 1997. Finite-State Transducers in Language and Speech Processing, Computational Linguistics, Vol. 23, No. 2, pp. 269-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Srihari</author>
</authors>
<title>A Domain Independent Event Extraction</title>
<date>1998</date>
<tech>Toolkit, AFRL-IF-RS-TR-1998-152 Final Technical Report,</tech>
<institution>Air Force Research Laboratory, Rome Research Site,</institution>
<location>New York</location>
<contexts>
<context position="5827" citStr="Srihari 1998" startWordPosition="862" endWordPosition="863">Section 4 discusses sub-type generation by MaxEnt. The experimental results and conclusion are presented finally. 1 FST-based Pattern Matching Rules for Textract NE The most attractive feature of the FST (Finite State Transducer) formalism lies in its superior time and space efficiency [Mohri 1997] [Roche &amp; Schabes 1997]. Applying a deterministic FST depends linearly only on the input size of the text. Our experiments also show that an FST rule system is extraordinarily robust. In addition, it has been verified by many research programs [Krupka &amp; Hausman 1998] [Hobbs 1993] [Silberztein 1998] [Srihari 1998] [Li &amp; Srihari 2000], that FST is also a convenient tool for capturing linguistic phenomena, especially for idioms and semi-productive expressions like time NEs and numerical NEs. The rules which we have currently implemented include a grammar for temporal expressions (time, date, duration, frequency, age, etc.), a grammar for numerical expressions (money, percentage, length, weight, etc.), and a grammar for other non-MUC NEs (e.g. contact information like address, email). The following sample pattern rules give an idea of what our NE grammars look like. These rules capture typical US address</context>
</contexts>
<marker>Srihari, 1998</marker>
<rawString>R. Srihari, 1998. A Domain Independent Event Extraction Toolkit, AFRL-IF-RS-TR-1998-152 Final Technical Report, Air Force Research Laboratory, Rome Research Site, New York</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>