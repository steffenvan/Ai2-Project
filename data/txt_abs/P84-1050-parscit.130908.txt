<newSection> I.
INTRODUCTION A.
Specialized vs generic software tools for MT Developing the software for a specific task or class of tasks requires that one knows the structure of the tasks involved.
In the case of Machine Translation (MT) this structure is not a priori known.
Yet it has been envisaged in the planning of the Eurotra project that the software development takes place before a general MT theory is present.
This approach has both advantages and disadvantages.
It is an advantage that the presence of a software framework will provide a formal language for expressing the MT theory, either explicitly or implicitly.
On the other hand this places a heavy responsibility on the shoulders of the software designers, since they will have to provide a language without knowing what this language will have to express.
We are grateful to the Commission of the European Communities for continuing support for the Eurotra Machine Translation project and for permission to publish this paper; and also to our colleagues in Eurotra for many interesting and stimulating discussions.
* * order not significant There are several ways open to the software designer.
One would be to create a framework that is sufficiently general to accomodate any theory.
This is not very attractive, not only because this could trivially be achieved by selecting any existing programming language, but also because this would not be of any help for the people doing the linguistic work.
Another, equally unattractive alternative would be to produce a very specific and specialized formalism and offer this to the linguistic community.
Unfortunately there is no way to decide in a sensible way in which directions this formalism should be specialized, and hence it would be a mere accident if the device would turn out to be adequate.
What is worse, the user of the formalism would spend a considerable amount of his time trying to overcome its deficiencies.
In other words, the difficulties that face the designer of such a software system is that it is the user of the system, in our case the linguist, who knows the structure of the problem domain, but is very often unable to articulate it until the language for the transfer of domain knowledge has been established.
Although the provision of such a language gives the user the ability to express himself, it normally comes after fundamental decisions regarding the meaning of the language have been frozen into the system architecture.
At this point, it is too late to do anything about it: the architecture will embody a certain theoretical commitment which delimits both what can be said to the system, and how the system can handle what it is told.
This problem is particularly severe when there is not one user, but several, each of whom may have a different approach to the problem that in their own terms is the best one.
This requires a considerable amount of flexibility to be built into the system, not only within a specific instance of the system, but as well across instances, since it is to be expected that during the construction phase of an MT system, a wide variety of theories will be tried (and rejected) as possible candidates.
In order to best suit these apparently conflicting requirements we have taken the following design decisions : 1.
On the one hand, the software to be designed will be oriented towards a class of abstract systems (see below) rather than one specific system.
This class should be so restricted that the decisions to be taken during the linguistic development of the end user system have direct relevance to the linguistic problem domain, while powerful enough to accommodate a variety of linguistic strategies.
2. On the other hand, just specifying a class of systems would be insufficient, given our expectation that the highly experimental nature of the linguistic development phase will give rise to a vast number of experimental instantiations of the system, which should not lead to continuously creating completely new versions of the system.
What is needed is a coherent set of software tools that enable the system developers to adapt the system to changes with a minimal amount of effort, i.e. a system generator.
Thus, we reject the view that the architecture should achieve this flexibility by simply evading theoretical commitment.
Instead it should be capable of displaying a whole range of highly specialized behaviours, and therefore be capable of a high degree of internal reconfiguration according to externally supplied specifications.
In other words we aim at a system which is theory sensitive.
In our philosophy the reconfiguration of the system should be achieved by supplying the new specifications to the system rather than to a team in charge of redesigning the system whenever new needs for the user arise.
Therefore the part of the system that is visible to the linguistic user will be a system generator, rather than an instance of an MT system.
B. Computational Paradigm for MT Software The computational paradigm we have chosen for the systems to be generated is the one of expert systems because the design of software for an MT system of the scope of Eurotra has much in common with the design of a very large expert system.
In both cases successful operation relies as much on the ease with which the specialist knowledge of experts in the problem domain can be communicated to and used by the system as on the programming skill of the software designers and implementers.
Typically, the designers of expert systems accommodate the need to incorporate large amounts of specialist knowledge in a flexible way by attempting to build into the system design a separation between knowledge of a domain and the way in which that knowledge is applied.
The characteristic architecture of an expert system is in the form of a Production system (PS) (cf Davis & King 1977).
A programming scheme is conventionally pictured as having two aspects (&quot;Algorithms + Data = Programs&quot;) -- (cf Wirth 1976); a production system has three : a data base, a set of rules (sometimes called 'productions' -- hence the name), and an interpreter.
Input to the computation is the initial state of the data base.
Rules consist, explicitly or implicitly, of two parts : a pattern and an action.
Computation proceeds by progressive modifications to the data base as the interpreter searches the data base and attempts to match patterns in rules and apply the corresponding actions in the event of a successful match.
The process halts either when the interpreter attempts to apply a halting action or when no more rules can be applied.
This kind of organisation is clearly attractive for knowledge-based computations.
The data base can be set up to model objects in the problem domain.
The rules represent small, modular items of knowledge, whose syntax can be adjusted to reflect formalisms with which expert users are familiar.
And the interpreter embodies a general principle about the appropriate way to apply the expert knowledge coded into the rules.
Given an appropriate problem domain, a good expert system design can make it appear as if the statement of expert knowledge is entirely declarative -- the ideal situation from the user's point of view.
A major aim in designing Eurotra has been to adapt the essential declarative spirit of production systems to the requirements of a system for large scale machine translation.
The reason for adapting the architecture of classical expert systems to our special needs was that the simple production system scheme Is likely to be inadequate for our purposes.
In fact, the success of a classical PS model in a given domain requires that a number of assumptions be satisfied, namely: In machine translation, the domain of knowledge with which we are primarily concerned is that of language.
With respect to assumption (I), we think automatically of rewrite rules as being an obvious way of expressing linguistic knowledge.
Some caution is necessary, however.
First of all, rewrite rules take on a number of different forms and interpretations depending on the linguistic theory with which they are associated.
In the simplest case, they are merely criteria of the well-formedness of strings, and a collection of such rules is simply equivalent to a recognition device.
Usually, however, they are also understood as describing pieces of tree structure, although in some cases -- phrase structure rules in particular -- no tree structure may be explicitly mentioned in the rule: a set of such rules then corresponds to some kind of transducer rather than a simple accepting automaton.
The point is that rules which look the same may mean different things according to what is implicit in the formalism.
When such rules are used to drive a computation, everything which is implicit becomes the responsibility of the interpreter.
This has two consequences : a. if there are different interpretations of rules according to the task which they are supposed to perform, then we need different interpreters to interpret them, which is contrary to assumption (2); an obvious case is the same set of phrase structure rules used to drive a builder of phrase structure trees given a string as input, and to drive an integrity checker given a set of possibly well-formed trees; b. alternatively, in some cases, information which is implicit for one type of interpreter may need to be made explicit for another, causing violation of assumption (3); an obvious case here is the fact that a phrase structure analyser can be written in terms of transductions on trees for a general rewrite interpreter, but at considerable cost in clarity and security.
Secondly, it is not evident that 'rules', in either the pattern-action or the rewrite sense, are necessarily the most appropriate representation for all linguistic description.
Examples where other styles of expression may well be more fitting are in the description of morphological paradigms for highly inflected languages or the formulation of judgements of relative semantic or pragmatic acceptability.
The organisational complexity of Eurotra also poses problems for software design.
Quite separate strategies for analysis and synthesis will be developed independently by language groups working in their own countries, although the results of this decentralised and distributed development will ultimately have to be combinable together into one integrated translation system.
What is more, new languages or sublanguages may be added at any time, requiring new strategies and modes of description.
Finally, the Eurotra software is intended not only as the basis for a single, large MT system, but as a general purpose facility for researchers in MT and computational linguistics in general.
These extra considerations impose requirements of complexity, modularity, extensibility and transparency not commonly expected of today's expert systems.
The conclusion we have drawn from these and similar observations is that the inflexible, monolithic nature of a simple PS is far too rigid to accommodate the variety of diverse tasks involved in machine translation.
The problem, however, is one of size and complexity, rather than of the basic spirit of production systems.
The above considerations have led us to adopt the principle of a controlled production system, that is a PS enhanced with a control language (Georgeff 1982).
The elements of the vocabulary of a control language are names of PSs, and the well-formed strings of the language define just those sequences of PS applications which are allowed.
The user supplies a control 'grammar', which, in a concise and perspicuous way, specifies the class of allowable application sequences.
Our proposal for Eurotra supports an enhanced context free control language, in which names of user-defined processes act as non-terminal symbols.
Since the language is context free, process definitions may refer recursively to other processes, as well as to grammars, whose names are the terminal symbols of the control language.
A grammar specifies a primitive task to be performed.
Like a production system, it consists of a collection of declarative statements about the data for the task, plus details of the interpretation scheme used to apply the declarative information to the data base.
Again, as in a production system, it is important that the information in the declarative part should be homogeneous, and that there should be a single method of application for the whole grammar.
We depart somewhat from conventional productions system philosophy in that our commitment is to declarative expression rather than to production rules.
The device of using a control language to define the organisation of a collection of grammars provides the user with a powerful tool for simulating the procedural knowledge inherent in constructing and testing strategies, without departing radically from an essentially declarative framework.
An important feature of our design methodology is the commitment to interaction with potential users in order to delineate the class of tasks which users themselves feel to be necessary.
In this way, we aim to avoid the error which has often been made in the past, of presenting users with a fixed set of selected generally on computational grounds, which they, the users, must adjust to their own requirements as best they can.