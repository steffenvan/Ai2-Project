<newSection> Abstract Recent work in computer vision has aimed to associate image regions with keywords describing the depicted entities, but actual image ‘understanding’ would also require identifying their attributes, relations and activities.
Since this information cannot be conveyed by simple keywords, we have collected a corpus of “action” photos each associated with five descriptive captions.
In order to obtain a consistent semantic representation for each image, we need to first identify which NPs refer to the same entities.
We present three hierarchical Bayesian models for cross-caption coreference resolution.
We have also created a simple ontology of entity classes that appear in images and evaluate how well these can be recovered.