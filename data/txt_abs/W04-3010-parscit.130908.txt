<newSection> Abstract This paper describes results from several dozen experimental systems, and draws conclusions about the ability of speech recognition models to represent the relationship among syntax, prosody, and segmental acoustics.
Prosody-dependent allophone modeling can reduce the word error rate (WER) of a speech recognizer, but only if both the language model and the acoustic model encode explicit dependence on prosody.
Word error rate is improved mainly because the observed prosody is linguistically unlikely to co-occur with any incorrect word string.
Additional improvements, in both perplexity and WER, can be obtained using a semi-factored language model, in which the relationship between prosody and the word sequence is at least partly mediated by syntactic tags.
Careful analysis of the relationship between prosody and syntax indicates that syntactic phrase boundaries are the most important cue for prosodic phrase boundary recognition, while part of speech is the most important cue for locating pitch accents, but that neither of these cues is entirely sufficient for either classification task.
Experiments to port this system from Radio News to the Switchboard corpus are currently under way, but preliminary results suggest that the prosody of Switchboard is profoundly different from the prosody of Radio News.