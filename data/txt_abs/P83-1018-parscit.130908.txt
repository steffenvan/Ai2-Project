<newSection> ABSTRACT A central goal of linguistic theory is to explain why natural languages are the way they are.
It has often been supposed that computational considerations ought to play a role in this characterization, but rigorous arguments along these lines have been difficult to come by.
In this paper we show how a key &quot;axiom'' of certain theories of grammar, Subjacency, can be explained by appealing to general restrictions on on-line parsing plus natural constraints on the rule-writing vocabulary of grammars.
The explanation avoids the problems with Marcus' [19801 attempt to account for the same constraint.
The argument is robust with respect to machine implementation, and thus avoids the problems that often arise when making detailed claims about parsing efficiency.
It has the added virtue of unifying in the functional domain of parsing certain grammatically disparate phenomena, as well as making a strong claim about the way in which the grammar is actually embedded into an on-line sentence processor.
I INTRODUCTION In its short history, computational linguistics has been driven by two distinct but interrelated goals.
On the one hand, it has aimed at computational explanations of distinctively human linguistic behavior -- that is, accounts of why natural languages are the way they are viewed from the perspective of computation.
On the other hand, it has accumulated a stock of engineering methods for building machines to deal with natural (and artificial) languages.
Sometimes a single body of research has combined both goals.
This was true of the work of Marcus [19801, for example.
But all too often the goals have remained opposed -- even to the extent that current transformational theory has been disparaged as hopelessly &quot;intractable&quot; and no help at all in constructing working parsers.
This paper shows that modern transformational grammar (the &quot;Government-Binding&quot; or &quot;GB&quot; theory as described in Chomsky [1981]) can contribute to both aims of computational linguistics.
We show that by combining simple assumptions about efficient parsability along with some assumptions about just how grammatical theory is to be &quot;embedded&quot; in a model of language processing, one can actually ex flain sonic key constraints of natural languages, such as Subjacency.
(rhe argument is different from that used in Marcus [19801.)
In fact, almost the entire pattern of constraints taken as &quot;axioms&quot; by the GB theory can be accounted for.
Second, contrary to what has sometimes been supposed, by exploiting these constraints we can show that a GB-based theory is particularly compatible wiih efficient parsing designs.
in particular, with extended 1.1((k,t) parsers (of the Sort described by Marcus [19801).
We can extend the 1.1((k.t) design to accommodate such phenomena as antecedent-PRO. and pronominal binding.
rightward fnovement, gapping, and VP deletion.
A. Functional Explanations of!
.ocality Principles Let us consider how to explain locality constraints in natural languages.
First of all, what exactly do we mean by a &quot;locality constraint&quot;?
The paradigm case is that of Subiacency: the distance between a displaced constituent and its &quot;underlying&quot; canonical argument position cannot be too large, where the distance is gauged (in English) in terms of the number of the number of S(entence) or NP phrase boundaries.
For example, in sentence (la) below, John (the so-called &quot;antecedent&quot;) is just one S-boundary away from its presumably &quot;underlying&quot; argument position (denoted &quot;x&quot;, the &quot;trace&quot;)) as the Subject of the embedded clause, and the sentence is fine: (1a) John seems Is x to like ice cream].
However, all we have to do is to make the link between John and x extend over two S's, and the sentence is ill-formed: (lb) John seems [s it is certain (s x to like ice cream This restriction entails a &quot;successive cyclic&quot; analysis of transformational rules (see Chomsky [19731).
In order to derive a sentence like lie) below without violating the Subjacency condition, we must move the NP from its canonical argument position through the empty Subject position in the next higher S and then to its surface slot: (1c) John seems [el to be certain x to get the ice cream.
Since the intermediate subject position is filled in (lb) there is no licit derivation for this sentence.
More precisely, we can state the Subjacency constraint as follows: No rule of grammar can involve X and Y in a configuration like the following, where a and /.3 are bounding nodes (in English, S or NP phrases).
Why should natural languages be designed this way and not some other way?
Why, that is, should a constraint like Subjacency exist at all?
Our general result is that under a certain set of assumptions about grammars and their relationship to human sentence processing one can actually expect the following pattern of syntactic locality constraints: B.
Assumption The assumptions we make are the following: (1) The grammar includes a level of annotated surface structure indicating how constituents have been displaced from their canonical predicate argument positions.
Further, sentence analysis is divided into two stages, along the lines indicated by the theory of Government and Binding: the first stage is a purely syntactic analysis that rebuilds annotated surface structure; the second stage carries out the interpretation of variables, binds them to operators, all making use of the &quot;referential indices&quot; of NPs.
(2) To be &quot;visible&quot; at a stage of analysis a linguistic representation must be written in the vocabulary of that level.
For example, to be affected by syntactic operations, a representation must be expressed in a syntactic vocabulary (in, the usual sense): to be interpreted by operations at the second stage, the NPs in a representation must possess referential indices.
(This assumption is not needed to derive the Subjacency constraint, but may be used to account for another &quot;axiom&quot; of current grammatical theory, the so-called &quot;constituent command&quot; constraint on antecedents and the variables that they bind.)
This &quot;visibility&quot; assumption is a rather natural one.
(3) The rule-writing vocabulary of the grammar cannot make use of arithmetic predicates such as &quot;one&quot;, &quot;two&quot; or &quot;three&quot;, but only such predicates as &quot;adjacent&quot;.
Further, quantificational statements are not allowed in rules.
These two assumptions arc also rather standard.
It has often been noted that grammars &quot;do not count&quot; -- that grammatical predicates are structurally based.
There is no rule of grammar that takes the just the fourth constituent of a sentence and moves it, for example.
In contrast, many different kinds of rules of grammar make reference to adjacent constituents.
(This is a feature found in morphological, phonological, and syntactic rules.) possible derivations in parallel.
In particular, an Earley-type algorithm is ruled out.
To the extent that multiple options about derivations are not pursued, the parse is &quot;deterministic.&quot; (5) The left-context of the parse (as defined in Aho and Ullman [19721) is literally represented, rather than generatively represented (as, e.g., a regular set).
In particular, just the symbols used by the grammar (S, NP, VP...) are part of the left-context vocabulary, and not &quot;complex&quot; symbols serving as proxies for the set of left-context strings.'
In effect, we make the (quite strong) assumption that the sentence processor adopts a direct, transparent embedding of the grammar.
Other theories or parsing methods do not meet these constraints and fail to explain the existence of locality constraints with respect to this particular set of assumptions.
2 For example.
as we show, there is no reason to expect a constraint like Subjacency in the Generalized Phrase Structure Grammars (GPSGs) of Gazdar 119811, because there is no inherent barrier to easily processing a sentence where an antecedent and a trace are unboundedly far from each other.
Similarly, if a parsing method like Farley's algorithm were actually used by people.
thon Subjacency remains a mystery on the functional grounds of efficient parsability.
(It could still be explained on other functional grounds, e.g., that of learnability.)