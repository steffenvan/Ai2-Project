<newSection> 1 Introduction Word Tokenization is regarded as one of major bottlenecks in Chinese Language Processing.
Normally, word tokenization is implemented through word segmentation in Chinese Language Processing literature.
This is also affected in the title of this competition.
There exists two major problems in Chinese word segmentation: ambiguity and unknown word detection.
While ngarm modeling and/or word co-ocurrence has been successfully applied to deal with ambiguity problem, unknown word detection has become major bottleneck in word tokenization.
This paper proposes a HMM-based chunking scheme to cope with unkown words in Chinese word tokenization.
The unknown word detection is re-casted as chunking several words (single-character word or multi-character word) together to form a new word.