<newSection> Abstract This paper describes an approach for the generation of multimodal deixis to be uttered by an anthropomorphic agent in virtual reality.
The proposed algorithm integrates pointing and definite description.
Doing so, the context-dependent discriminatory power of the gesture determines the content-selection for the verbal constituent.
The concept of a pointing cone is used to model the region singled out by a pointing gesture and to distinguish two referential functions called object-pointing and region-pointing.