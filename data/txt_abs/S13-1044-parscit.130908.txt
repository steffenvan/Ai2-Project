<newSection> Abstract We present an approach which uses the similarity in semantic structure of bilingual parallel sentences to bootstrap a pair of semantic role labeling (SRL) models.
The setting is similar to co-training, except for the intermediate model required to convert the SRL structure between the two annotation schemes used for different languages.
Our approach can facilitate the construction of SRL models for resource-poor languages, while preserving the annotation schemes designed for the target language and making use of the limited resources available for it.
We evaluate the model on four language pairs, English vs German, Spanish, Czech and Chinese.
Consistent improvements are observed over the self-training baseline.