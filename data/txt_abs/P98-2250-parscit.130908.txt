<newSection> OutnutUver: 27 NIEI MNIOIRIK Fig.!.
SRN and mechanism of sequence processing.
A character is provided to the input and the next one is used for training.
In turn, it has to be predicted during the test phase.
In the present report, alternative evaluation procedures are proposed.
The network evaluation methods introduced are based on examining the network response to each left context, available in the training corpus.
An effective way to represent and use the complete set of context strings is a treebased data structure.
Therefore, these methods are termed tree-based analysis.
Two possible approaches are proposed for measuring the SRN response accuracy to each left context.
The first uses the idea mentioned above of searching a threshold that distinguishes permitted successors from impossible ones.
An error as a function of the threshold is computed.
Its minimum value corresponds to the SRN learning error rate.
The second approach computes the local proximity between the network response and a vector containing the empirical symbol probabilities that a given symbol would follow the current left context.
Two measures are used: L2 norm and normalised vector multiplication.
The mean of these local proximities measures how close the network responses are to the desired responses.
2 Tree-based corpus representation.
There are diverse methods to represent a given set of words (corpus).
Lists is the simplest, but they are not optimal with regard to the memory complexity and the time complexity of the operations working with the data.
A more effective method is the treebased representation.
Each node in this tree has a maximum of 26 possible children (successors), if we work with orthographic word representations.
The root is empty, it does not represent a symbol.
It is the beginning of a word.
The leaves do not have successors and they always represent the end of a word.
A word can end somewhere between the root and the leaves as well.
This manner of corpus representation, termed trie, is one of the most compact representations and is very effective for different operations with words from the corpus.
In addition to the symbol at each node, we can keep additional information, for example the frequency of a word, if this node is the end of a word.
Another useful piece of information is the frequency of each node C, that is, the frequency of each left context.
It is computed recursively as a sum of the frequencies of all successors and the frequency of the word ending at this node, provided that such a word exists.
These frequencies give us an instant evaluation of the empirical distribution for each successor.
In order to compute the successors' empirical distribution vector r(.), we have to normalise the successors' frequencies with respect to their sum.