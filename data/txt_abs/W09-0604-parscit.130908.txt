<newSection> Abstract Data-driven approaches to sentence compression define the task as dropping any subset of words from the input sentence while retaining important information and grammaticality.
We show that only 16% of the observed compressed sentences in the domain of subtitling can be accounted for in this way.
We argue that part of this is due to evaluation issues and estimate that a deletion model is in fact compatible with approximately 55% of the observed data.
We analyse the remaining problems and conclude that in those cases word order changes and paraphrasing are crucial, and argue for more elaborate sentence compression models which build on NLG work.