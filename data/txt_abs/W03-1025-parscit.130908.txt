<newSection> Abstract 1 Introduction: Why Parsing Characters?
After Linguistic Data Consortium (LDC) released the Chinese Treebank (CTB) developed at UPenn (Xia et al., 2000), various statistical Chinese parsers (Bikel and Chiang, 2000; Xu et al., 2002) have been built.
Techniques used in parsing English have been shown working fairly well when applied to parsing Chinese text.
As there is no word boundary in written Chinese text, CTB is manually segmented into words and then labeled.
Parsers described in (Bikel and Chiang, 2000) and (Xu et al., 2002) operate at word-level with the assumption that input sentences are pre-segmented.
The paper studies the problem of parsing Chinese unsegmented sentences.
The first motivation is that a character-based parser can be used directly in natural language applications that operate at character level, whereas a word-based parser requires a separate word-segmenter.
The second and more important reason is that the availability of CTB, a large corpus with high quality syntactic annotations, provides us with an opportunity to create a highly-accurate word-segmenter.
It is widely known that Chinese word-segmentation is a hard problem.
There are multiple studies (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) showing that the agreement between two (untrained) native speakers is about upper to lower . The agreement between multiple human subjects is even lower (Wu and Fung, 1994).
The reason is that human subjects may differ in segmenting things like personal names (whether family and given names should be one or two words), number and measure units and compound words, although these ambiguities do not change a human being’s understanding of a sentence.
Low agreement between humans affects directly evaluation of machines’ performance (Wu and Fung, 1994) as it is hard to define a gold standard.
It does not necessarily imply that machines cannot do better than humans.
Indeed, if we train a model with consistently segmented data, a machine may do a better job in “remembering” word segmentations.
As will be shown shortly, it is straightforward to encode word-segmentation information in a character-The paper presents a maximum entropy Chinese character-based parser trained on the Chinese Treebank (“CTB” henceforth).
Word-based parse trees in CTB are first converted into character-based trees, where word-level part-ofspeech (POS) tags become constituent labels and character-level tags are derived from word-level POS tags.
A maximum entropy parser is then trained on the character-based corpus.
The parser does word-segmentation, POS-tagging and parsing in a unified framework.
An average label F-measure and word-segmentation F-measure are achieved by the parser.
Our results show that word-level POS tags can improve significantly word-segmentation, but higher-level syntactic strutures are of little use to word segmentation in the maximum entropy parser.
A word-dictionary helps to improve both word-segmentation and parsing accuracy.
based parse tree.
Parsing Chinese character streams therefore does effectively word-segmentation, partof-speech (POS) tagging and constituent labeling at the same time.
Since syntactical information influences directly word-segmentation in the proposed character-based parser, CTB allows us to test whether or not syntactic information is useful for word-segmentation.
A third advantage of parsing Chinese character streams is that Chinese words are more or less an open concept and the out-ofvocabulary (OOV) word rate is high.
As morphology of the Chinese language is limited, extra care is needed to model unknown words when building a word-based model.
Xu et al.
(2002), for example, uses an independent corpus to derive word classes so that unknown words can be parsed reliably.
Chinese characters, on the other hand, are almost closed.
To demonstrate the OOV problem, we collect a word and character vocabulary from the first sentences of CTB, and compute their coverages on the corresponding word and character tokenization of the last of the corpus.
The word-based OOV rate is while the character-based OOV rate is only . The first step of training a character-based parser is to convert word-based parse trees into character-based trees.
We derive character-level tags from word-level POS tags and encode word-boundary information with a positional tag.
Word-level POSs become a constituent label in character-based trees.
A maximum entropy parser (Ratnaparkhi, 1997) parser is then built and tested.
Many language-independent feature templates in the English parser can be reused.
Lexical features, which are language-dependent, are used to further improve the baseline models trained with language-independent features only.
Word-segmentation results will be presented and it will be shown that POSs are very helpful while higher-level syntactic structures are of little use to word-segmentation – at least in the way they are used in the parser.