<newSection> Abstract This paper presents novel improvements to the induction of translation lexicons from monolingual corpora using multilingual dependency parses.
We introduce a dependency-based context model that incorporates long-range dependencies, variable context sizes, and reordering.
It provides a 16% relative improvement over the baseline approach that uses a fixed context window of adjacent words.
Its Top 10 accuracy for noun translation is higher than that of a statistical translation model trained on a Spanish-English parallel corpus containing 100,000 sentence pairs.
We generalize the evaluation to other word-types, and show that the performance can be increased to 18% relative by preserving part-of-speech equivalencies during translation.