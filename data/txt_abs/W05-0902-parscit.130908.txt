<newSection> Abstract We address the issue of human subjectivity when authoring summaries, aiming at a simple, robust evaluation of machine generated summaries.
Applying a cross comprehension test on human authored short summaries from broadcast news, the level of subjectivity is gauged among four authors.
The instruction set is simple, thus there is enough room for subjectivity.
However the approach is robust because the test does not use the absolute score, relying instead on relative comparison, effectively alleviating the subjectivity.
Finally we illustrate the application of the above scheme when evaluating the informativeness of machine generated summaries.