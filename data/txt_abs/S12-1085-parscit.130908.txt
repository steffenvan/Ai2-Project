<newSection> Abstract The paper aims to come up with a system that examines the degree of semantic equivalence between two sentences.
At the core of the paper is the attempt to grade the similarity of two sentences by finding the maximal weighted bipartite match between the tokens of the two sentences.
The tokens include single words, or multiwords in case of Named Entitites, adjectivally and numerically modified words.
Two token similarity measures are used for the task - WordNet based similarity, and a statistical word similarity measure which overcomes the shortcomings of WordNet based similarity.
As part of three systems created for the task, we explore a simple bag of words tokenization scheme, a more careful tokenization scheme which captures named entities, times, dates, monetary entities etc., and finally try to capture context around tokens using grammatical dependencies.