<newSection> 3 Word Graphs It is widely accepted in the community that a significant improvement in translation quality will come from more sophisticated translation and language models.
For example, a language model that goes beyond m-gram dependencies could be used, but this would be difficult to integrate into the search process.
As a step towards the solution of this problem, we determine not only the single best sentence hypothesis, but also other complete sentences that the search algorithm found but that were judged worse.
We can then apply rescoring with a refined model to those hypotheses.
One efficient way to store the different alternatives is a word graph.
Word graphs have been successfully applied in speech recognition, for the search process (Ortmanns et al., 1997) and as an interface to other systems (Oerder and Ney, 1993).
(Knight and Hatzivassiloglou, 1995) and (Langkilde and Knight, 1998) propose the use of word graphs for natural language generation.
In this paper, we are going to present a concept for the generation of word graphs in a machine translation system.
During search, we keep a bookkeeping tree.
It is not necessary to keep all the information that we need for the expansion of hypotheses during search in this structure, thus we store only the following: After the search has finished, i.e. when all source sentence positions have been translated, we trace back the best sentence in the bookkeeping tree.
To generate the N best hypotheses after search, it is not sufficient to simply trace back the complete hypotheses with the highest probabilities in the bookkeeping, because those hypotheses have been recombined.
Thus, many hypotheses with a high probability have not been stored.
To overcome this problem, we enhance the bookkeeping concept and generate a word graph as described in Section 3.3.
If we want to generate a word graph, we have to store both alternatives in the bookkeeping when two hypotheses are recombined.
Thus, an entry in the bookkeeping structure may have several backpointers to different preceding entries.
The bookkeeping structure is no longer a tree but a network where the source is the bookkeeping entry with zero covered source sentence positions and the sink is a node accounting for complete hypotheses (see Figure 3).
This leads us to the concept of word graph nodes and edges containing the following information: — the probabilities according to the different models: the language model and the translation submodels, — the backpointer to the preceding bookkeeping entry.
After the pruning in beam search, all hypotheses that are no longer active do not have to be kept in the bookkeeping structure.
Thus, we can perform garbage collection and remove all those bookkeeping entries that cannot be reached from the backpointers of the active hypotheses.
This reduces the size of the bookkeeping structure significantly.
An example of a word graph can be seen in Figure 3.
To keep the presentation simple, we chose an example without reordering of sentence positions.
The words on the edges are the produced target words, and the bitvectors in the nodes show the covered source sentence positions.
If an edge is labeled with two words, this means that the first English word has no equivalence in the source sentence, like 'just' and 'have' in Figure 3.
The reference translation 'what did you say ?' is contained in the graph, but it has a slightly lower probability than the sentence 'what do you say ?', which is then chosen by the single best search.
The recombination of hypotheses can be seen in the nodes with two or more incoming edges: those hypotheses have been recombined, because they were indistinguishable by translation and language model state.