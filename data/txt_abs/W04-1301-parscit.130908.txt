<newSection> Abstract Naturalistic theories of language acquisition assume learners to be endowed with some innate language knowledge.
The purpose of this innate knowledge is to facilitate language acquisition by constraining a learnerâ€™s hypothesis space.
This paper discusses a naturalistic learning system (a Categorial Grammar Learner (CGL)) that differs from previous learners (such as the Triggering Learning Algorithm (TLA) (Gibson and Wexler, 1994)) by employing a dynamic definition of the hypothesis-space which is driven by the Bayesian Incremental Parameter Setting algorithm (Briscoe, 1999).
We compare the efficiency of the TLA with the CGL when acquiring an independently and identically distributed English-like language in noiseless conditions.
We show that when convergence to the target grammar occurs (which is not guaranteed), the expected number of steps to convergence for the TLA is shorter than that for the CGL initialized with uniform priors.
However, the CGL converges more reliably than the TLA.
We discuss the trade-off of efficiency against more reliable convergence to the target grammar.