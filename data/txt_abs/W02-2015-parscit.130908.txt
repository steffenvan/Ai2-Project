<newSection> Abstract Error analysis is a key step in developing statistical parsers.
In doing this, we manually discover typical cases by examining parser output.
In this paper we argue that the process can be speeded up by considering the output from an ensemble of parsers.
We do this by resampling small proportions (10% and up) from the training data, and exploiting the high diversity of the resulting parsers - resulting from the sparseness of natural-language data.
Varying the sample size, we can trace the gradual learning of each instance and classify instances into a few types.
This division helps in distinguishing instances which are hard for the system, from instances which may be learned in principle.
We suggest that such analysis can yield a qualitative approach to evaluation of statistical parsers.