<newSection> Abstract We propose to use graph-based diffusion techniques with data-dependent kernels to build unigram language models.
Our approach entails building graphs, where each vertex corresponds uniquely to a word from a closed vocabulary, and the existence of an edge (with an appropriate weight) between two words indicates some form of similarity between them.
In one of our constructions, we place an edge between two words if the number of times these words were seen in a training set differs by at most one count.
This graph construction results in a similarity matrix with small intrinsic dimension, since words with the same counts have the same neighbors.
Experimental results from a benchmark task from language modeling show that our method is competitive with the Good-Turing estimator.