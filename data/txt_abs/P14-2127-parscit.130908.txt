<newSection> 1 Introduction Many natural language processing problems including part-of-speech tagging (Collins, 2002), parsing (McDonald et al., 2005), and event extraction (Li et al., 2013) have enjoyed great success using large-scale discriminative training algorithms.
However, a similar success on machine translation has been elusive, where the mainstream methods still tune on small datasets.
What makes large-scale MT training so hard then?
After numerous attempts by various researchers (Liang et al., 2006; Watanabe et al., 2007; Arun and Koehn, 2007; Blunsom et al., 2008; Chiang et al., 2008; Flanigan et al., 2013; Green et al., 2013), the recent work of Yu et al.
(2013) finally reveals a major reason: it is the vast amount of (inevitable) search errors in MT decoding that astray learning.
To alleviate this problem, their work adopts the theoretically-motivated framework of violation-fixing perceptron (Huang et al., 2012) tailed for inexact search, yielding great results on phrase-based MT (outperforming small-scale MERT/PRO by a large margin for the first time).
However, the underlying phrase-based model suffers from limited distortion and thus can only employ a small portion (about 1/3 in their ChEn experiments) of the bitext in training.
To better utilize the large training set, we propose to generalize from phrase-based MT to syntax-based MT, in particular the hierarchical phrase-based translation model (HIERO) (Chiang, 2005), in order to exploit sentence pairs beyond the expressive capacity of phrase-based MT.
The key challenge here is to extend the latent variable violation-fixing perceptron of Yu et al.
(2013) to handle tree-structured derivations and translation hypergraphs.
Luckily, Zhang et al.
(2013) have recently generalized the underlying violation-fixing perceptron of Huang et al.
(2012) from graphs to hypergraphs for bottom-up parsing, which resembles syntax-based decoding.
We just need to further extend it to handle latent variables.
We make the following contributions: