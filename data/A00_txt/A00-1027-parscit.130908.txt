Compound Noun Segmentation Based on Lexical Data
Extracted from Corpus*
Juntae Yoon
jtyoon@linc.cis.upenn.edu
IRCS, University of Pennsylvania,
3401 Walnut St., Suite 400A,
Philadelphia, PA 19104-6228, USA
Abstract
Compound noun analysis is one of the crucial problems in Korean language processing because a series
of nouns in Korean may appear without white space
in real texts, which makes it difficult to identify the
morphological constituents. This paper presents an
effective method of Korean compound noun segmentation based on lexical data extracted from corpus.
The segmentation is done by two steps: First, it is
based on manually constructed built-in dictionary
for segmentation whose data were extracted from 30
million word corpus. Second, a segmentation algorithm using statistical data is proposed, where simple nouns and their frequencies are also extracted
from corpus. The analysis is executed based on CYK
tabular parsing and min-max operation. By experiments, its accuracy is about 97.29%, which turns
out to be very effective.
1 Introduction
Morphological analysis is crucial for processing the
agglutinative language like Korean since words in
such languages have lots of morphological variants.
A sentence is represented by a sequence of eojeols
which are the syntactic unit- delimited by spacing
characters in Korean. Unlike in English, an eojeol
is not one word but composed of a series of words
(content words and functional words). In particular, since an eojeol can often contain more than one
noun, we cannot get proper interpretation of the sentence or phrase without its accurate segmentation.
The problem in compound noun segmentation is
that it is not possible to register all compound nouns
in the dictionary since nouns are in the open set
of words as well as the number of them is very
large. Thus, they must be treated as unseen words
without a segmentation process. Furthermore, accurate compound noun segmentation plays an important role in the application system. Compound
noun segmentation is necessarily required for improving recall and precision in Korean information
* This work was supported by a KOSEF's postdoctoral fellowship grant.
retrieval, and obtaining better translation in machine translation. For example, suppose that a
compound noun `seol'agsan-gugrib-gongwon(Seol'ag
Mountain National Park)' appear in documents.
A user might want to retrieve documents about
`seoPagsan(Seol'ag Mountain)', and then it is likely
that the documents with seol'agsan-gugrib-gongwon'
are also the ones in his interest. Therefore, it
should be exactly segmented before indexing in order for the documents to be retrieved with the query
`seol'agsan). Also, to translate ' seol'agsan-gugribgongwon' to Seol'ag Mountain National Park, the
constituents should be identified first through the
process of segmentation.
This paper presents two methods for segmentation
of compound nouns. First, we extract compound
nouns from a large size of corpus, manually divide
them into simple nouns and construct the hand built
segmentation dictionary with them. The dictionary
includes compound nouns which are frequently used
and need exceptional process. The number of data
are about 100,000.
Second, the segmentation algorithm is applied if
the compound noun does not exist in the built-in
dictionary. Basically, the segmenter is based on frequency of individual nouns extracted from corpus.
However, the problem is that it is difficult to distinguish proper noun and common noun since there
is no clue like capital letters in Korean. Thus, just
a large amount of lexical knowledge does not make
good results if it contains incorrect data and also it is
not appropriate to use frequencies obtained by automatically tagging large corpus. Moreover, sufficient
lexical data cannot be acquired from small amounts
of tagged corpus.
In this paper, we propose a method to get simple nouns and their frequencies from frequently occurring eojeols using repetitiveness of natural language. The amount of eojeols investigated is manually tractable and frequently used nouns extracted
from them are crucial for compound noun segmentation. Furthermore, we propose min-max composition to divide a sequence of syllables, which would
be proven to be an effective method by experiments.
14;
To briefly show the reason that we select the operation, let us consider the following example. Suppose that a compound noun be composed of four
syllables '81828384'. There are several possibilities of
segmentation in the sequence of syllables, where we
consider the following possibilities (si /828384) and
(8182/8384). Assume that '.51' is a frequently appearing word in texts whereas '828354' is a rarely
occurring sequence of syllables as a word. On the
other hand '81.52' and '8384' occurs frequently but
although they don't occur as frequently as In
this case, the more likely segmentation would be
(8182/8384). It means that a sequence of syllables
should not be divided into frequently occurring one
and rarely occurring one. In this sense, min-max is
the appropriate operation for the selection. In other
words, min value is selected between two sequences
of syllables, and then max is taken from min values
selected. To apply the operation repetitively, we use
the CYK tabular parsing style algorithm.
2 Lexical Data Acquisition
Since the compound noun consists of a series of
nouns, the probability model using transition among
parts of speech is not helpful, and rather lexical information is required for the compound noun segmentation. Our segmentation algorithm is based on
a large collection of lexical information that consists
of two kinds of data: One is the hand built segmentation dictionary (HBSD) and the other is the
simple noun dictionary for segmentation (SND).
2.1 Hand-Built Segmentation Dictionary
The first phase of compound noun segmentation uses
the built-in dictionary (HBSD). The advantage of
using the built-in dictionary is that the segmentation could (1) be very accurate by hand-made data
and (2) become more efficient. In Korean compound
noun, one syllable noun is sometimes highly ambiguous between suffix and noun, but human can easily
identify them using semantic knowledge. For example, one syllable noun `.s.se in Korean might be
used either as a suffix or as a noun which means
`Mr/Ms' or 'seed' respectively. Without any semantic information, the best way to distinguish them
is to record all the compound noun examples containing the meaning of seed in the dictionary since
the number of compound nouns containing a meaning of `seed' is even smaller. Besides, we can treat
general spacing errors using the dictionary. By the
spacing rule for Korean, there should be one content
word except noun in an eojeol, but it turns out that
one or more content words of short length sometimes
appear without space in real texts, which causes the
lexical ambiguities. It makes the system inefficient
to deal with all these words on the phase of basic
morphological analysis.
compound nouns analysis information
gajuggudu(leather shoes) gajug(leather)-Fgudu(shoes)
gajugggeun(leather string)
gaguyong(used for furniture)
sagwassi(apple seed)
podossi(graph seed) gagu(furniture)+xyong(used for)
sagwa(apple)±nssi(seed)
gajug(leather)+ggeun(string)
podo(grape)±nssi(seed)
chuggutim(football team) chuggu(football)-1-tim(team)
Table 1: Examples of compound noun and analysis
information in built-in dictionary
To construct the dictionary, compound nouns are
extracted from corpus and manually elaborated.
First, the morphological analyzer analyzes 30 million eojeol corpus using only simple noun dictionary,
and the failed results are candidates for compound
noun. After postpositions, if any, are removed from
the compound noun candidates of the failure eojeols, the candidates are modified and analyzed by
hand. In addition, a collection of compound nouns
of KAIST (Korea Advanced Institute of Science 8z
Technology) is added to the dictionary in order to
supplement them. The number of entries contained
in the built-in dictionary is about 100,000. Table 1
shows some examples in the built-in dictionary. The
italic characters such as 'n' or 'x' in analysis information (right column) of the table is used to make
distinction between noun and suffix.
2.2 Extraction of Lexical Information for
Segmentation from Corpus
As we said earlier, it is impossible for all compound
nouns to be registered in the dictionary, and thus the
built-in dictionary cannot cover all compound nouns
even though it gives more accurate results. We need
some good segmentation model for compound noun,
therefore.
In compound noun segmentation, the thing that
we pay attention to was that lexical information is
crucial for segmenting noun compounds. Since a
compound noun consists only of a sequence of nouns
i.e. {noun}+, the transition probability of parts of
speech is no use. Namely, the frequency of each noun
plays highly important role in compound noun segmentation. Besides, since the parameter space is
huge, we cannot extract enough lexical information
from hundreds of thousands of POS tagged corpus'
even if accurate lexical information can be extracted
from annotated corpus. Thus, a large size of corpus should be used to extract proper frequencies of
nouns. However, it is difficult to look at a large size
of corpus and to assign analyses to it, which makes
it difficult to estimate the frequency distribution of
words. Therefore, we need another approach for obtaining frequencies of nouns.
tit is the size of POS tagged corpus currently publicized
by ETRI (Electronics and Telecommunications Research Institute) project.
197
Figure 1: Distribution of eojeols in Korean corpus
It must be noted here that each noun in compound
nouns could be easily segmented by human in many
cases because it has a prominent figure in the sense
that it is a frequently used word and so familiar with
him. In other words, nouns prominent in documents
can be defined as frequently occurred ones, which we
call distinct nouns. Compound nouns contains these
distinct nouns in many cases, which makes it easier
to segment them and to identify their constituents.
Empirically, it is well-known that too many words
in the dictionary have a bad influence on morphological analysis in Korean. It is because rarely used
nouns result in oversegmentation if they are included
in compound noun segmentation dictionary. Therefore, it is necessary to select distinct nouns, which
leads us to use a part of corpus instead of entire
corpus that consists of frequently used ones in the
corpus.
First, we examined distribution of eojeols in corpus in order to make the subset of corpus to extract
lexical frequencies of nouns. The notable thing in
our experiment is that the number of eojeols in corpus is increased in proportion to the size of corpus,
but a small portion of eojeols takes most parts of the
whole corpus. For instance, 70% of the corpus consists of just 60 thousand types of eojeols which take
7.5 million of frequency from 10 million eojeol corpus
and 20.5 million from 30 million eojeols. The lowest
frequency of the 60,000 eojeols is 49 in 30 million eojeol corpus. We decided to take 60,000 eojeols which
are manually tractable and compose most parts of
corpus (Figure 1).
Second, we made morphological analyses for the
60,000 eojeols by hand. Since Korean is an agglutinative language, an eojeol is represented by a sequence of content words and functional words as
mentioned before. Especially, content words and
functional words often have different distribution
of syllables. In addition, inflectional endings for
predicate and postpositions for nominals also have
quite different distribution for syllables. Hence we
can distinguish the constituents of eojeols in many
cases. Of course, there are also many cases in which
the result of morphological analysis has ambiguities. For example, an eojeol `na-neun' in Korean
has ambiguity of `na/N+ neun/P', `na/PN+ neun/P'
and `nal/V+ neun/E'. In this example, the parts
of speech N, PN, P, V and E mean noun, pronoun, postposition, verb and ending, respectively.
On the other hand, many eojeols which are analyzed as having ambiguities by a morphological analyzer are actually not ambiguous. For instance,
`ga-geora' (go/imperative) has ambiguities by most
morphological analyzer among `ga/V+geora/E' and
`ga/N+i/C+geora/E' (C is copula), but it is actually not ambiguous. Such morphological ambiguity
is caused by overgeneration of the morphological analyzer since the analyzer uses less detailed rules for
robustness of the system. Therefore, if we examine
and correct the results scrupulously, many ambiguities can be removed through the process.
As the result of the manual process, only 15% of
60,000 eojeols remain ambiguous at the mid-level of
part of speech classification2. Then, we extracted
simple nouns and their frequencies from the data.
Despite of manual correction, there must be ambiguities left for the reason mentioned above. There may
be some methods to distribute frequencies in case
of ambiguous words, but we simply assign the equal
distribution to them. For instance, gage has two possibilities of analysis i.e. `gage/N' and `ga/V+ge/E),
and its frequency is 2263, in which the noun 'gage' is
assigned 1132 as its frequency. Table 2 shows examples of manually corrected morphological analyses of
eojeols containing a noun 'gage' and their frequencies. We call the nouns extracted in such a way a
set of distinct nouns.
In addition, we supplement the dictionary with
other nouns not appeased in the words obtained
by the method mentioned above. First, nouns of
more than three syllables are rare in real texts in
Korean, as shown in Lee and Ahn (1996). Their
experiments proved that syllable based bigram indexing model makes much better result than other
n-gram model such as trigram and quadragram in
Korean IR. It follows that two syllable nouns take
an overwhelming majority in nouns. Thus, there are
not many such nouns in the simple nouns extracted
by the manually corrected nouns (a set of distinct
nouns). In particular, since many nouns of more
2At the mid-level of part of speech classification, for example, endings and postpositions are represented just by one
tag e.g. E and P. To identify the sentential or clausal type
(subordinate or declarative) in Korean, the ending should be
subclassified for syntactic analysis more detail which can be
done by statistical process. It is beyond the subject of this
paper.
198
eojeols constituents meaning frequencies
gage gage/NOga/V+ge/E store@go 2263
gage-ga gage/N+ga/P store/SUBJ 165
gage-neun gage/N+neun/P@ga/V+geneun/E store/TOP@go 113
gage-ro gage/N+ro/P to the store 166
gage-reul gage/N+reul/P store/OBJ 535
gage-e gage/N+e/P in the store 312
gage-eseo gage/N+eseo/P in the store 299
gage-yi gage/N+yi/P of the store 132
extracted noun frequency
gage store 2797
Table 2: Example of extraction of distinct nouns. Here N, V, P and E mean tag for noun, verb, postposition
and ending and '@' is marked for representation of ambiguous analysis
than three syllables are derived by a word and suffixes and have some syllable features, they are useful
for distinguishing the boundaries of constituents in
compound nouns. We select nouns of more than
three syllables from morphological dictionary which
is used for basic morphological analysis and consists
of 89,000 words (noun, verb, adverb etc). Second,
simple nouns are extracted from hand-built segmentation dictionary. We selected nouns which do not
exist in a set of distinct nouns.
The frequency is assigned equally with some value
f q. Since the model is based on min-max composition and the nouns extracted in the first phase are
most important, the value does not take an effect on
the system performance.
The nouns extracted in this way are referred to
as a set of supplementary nouns. And the SND for
compound noun segmentation is composed of a set
of distinct nouns and a set of supplementary nouns.
The number of simple nouns for compound noun segmentation is about 50,000.
3 Compound Word Segmentation
Algorithm
3.1 Basic Idea
To simply describe the basic idea of our compound
noun segmentation, we first consider a compound
noun to be segmented into only two nouns. Given a
compound noun, it is segmented by the possibility
that a sequence of syllables inside it forms a word.
The possibility that a sequence of syllables forms a
word is measured by the following formula.
In the formula, fq(si,... sj) is the frequency of
the syllable s,...sj, which is obtained from SND
constructed on the stages of lexical data extraction.
And, f qN is the total sum of frequencies of simple
nouns. Colloquially, the equation (1) estimates how
much the given sequence of syllables are likely to be
word. If a sequence of syllables in the set of distinct
nouns is included in a compound noun, it is more
probable that it is divided around the syllables. If
a compound noun consists of, for any combination
of syllables, sequences of syllables in the set of supplementary nouns, the boundary of segmentation is
somewhat fuzzy. Besides, if a given sequence of syllables is not found in SND, it is not probable that it
is a noun.
Consider a compound noun ' hag-gyo-saenghwal(school life)'. In case that segmentation of
syllables is made into two, there would be four
possibilities of segmentation for the example as
follows:
hag gyo-saeng-hwal
hag-gyo saeng-hwal
hag-gyo-saeng hwal
hag-gyo-saeng-hwal
As we mentioned earlier, it is desirable that the eojeol is segmented in the position where each sequence
of syllables to be divided occurs frequently enough
in training data. As the length of a sequence of syllables is shorter in Korean, it occurs more frequently.
That is, the shorter part usually have higher frequency than the other (longer) part when we divide
syllables into two. Moreover, if the other part is
the syllables that we rarely see in texts, then the
part would not be a word. In the first of the above
example, hag is a sequence of syllable appearing frequently, but gyo-saeng-hwal is not. Actually, gyosaeng-hwal is not a word. On the other hand, both
hag-gyo and saeng-hwal are frequently occurring syllables, and actually they are all words. Put another
way, if it is unlikely that one sequence of syllables is
a word, then it is more likely that the entire syllables
are not segmented. The min-max composition is a
suitable operation for this case. Therefore, we first
f q(si, s j)
W ord(si, s j) =
fqN
(1)
199
take the minimum value from the function Word for
each possibility of segmentation, and then we choose
the maximum from the selected minimums. Also,
the argument taking the maximum is selected as the
most likely segmentation result.
Here, Ward(s,...si) is assigned the frequency of
the syllables s,...si from the dictionary SND. Besides, if two minimums are equal, the entire syllable such as hag-gyo-saeng-hwal, if compared, is preferred, the values of the other sequence of syllables
are compared or the dominant pattern has the priority.
3.2 Segmentation Algorithm
In this section, we generalize the word segmentation
algorithm based on data obtained by the training
method described in the previous section. The basic
idea is to apply min-max operation to each syllable in a compound noun by the bottom-up strategy. That is, if the minimum between Words of
two sequences of syllables is greater than Word of
the combination of them, the syllables should be
segmented. For instance, let us suppose a compound noun consist of two syllable si and s2. If
min(Word(si), Word(s2)) > Word(sis2), then the
compound noun is segmented into s1 and s2. It is
not segmented, otherwise. That is, we take the maximum among minimums. For example, 'hag' is a frequently occurring word, but `gyo' is not in Korean.
In this case, we can hardly regard the sequence of
syllable 'hag-gyo' as the combination of two words
'hag' and `gyo'. The algorithm can be applied recursively from individual syllable to the entire syllable
of the compound noun.
The segmentation algorithm is effectively implemented by borrowing the CYK parsing method.
Since we use the bottom-up strategy, the execution looks like composition rather than segmentation. After all possible segmentation of syllables being checked, the final result is put in the top of the
table. When a compound noun is composed of n
syllables, i.e. s1s2 Sn, the composition is started
from each si (i = 1 n). Thus, the possibility that
the individual syllable forms a word is recorded in
the cell of the first row.
Here, Ci,j is an element of CYK table where the segment result of the syllables s3,...,j+i-1 is stored (Figure 2). For
instance, the segmentation result such that
argmax(min(Word(si), Word(s2)), Word(sis2))
is stored in C1,2- What is interesting here is
that the procedure follows the dynamic programming. Thus, each cell C,j has the most
probable segmentation result for a series of syllables Namely, C1,2 and C2,3 have
the most likely segmentation of s1s2 and .52.53
respectively. When the segmentation of Si 8283 is
about to be checked, min(volue(C2,1), va/ue(C1,3)),
Figure 2: Composition Table
min(vatue(CLI), va/ue(C2,2)) and Word(sis2s3)
are compared to determine the segmentation for
the syllables, because all Ci,j have the most likely
segmentation. Here, value(C,,i) represents the
possibility value of Ci,j.
Then, we can describe the segmentation algorithm
as follows:
When it is about to make the segmentation of syllables si sj, the segmentation results of less length
of syllables like si , si and so forth
would be already stored in the table. In order to
make analysis of si sj, we combine two shorter
length of analyses and the word generation possibilities are computed and checked.
To make it easy to explain the algorithm, let us
take an example compound noun 'hag-gyo-saenghwar (school life) which is segmented with chaggyo'
(school) and `saenghwar (life) (Figure 3). When it
comes up to cell C4,1, we have to make the most
probable segmentation for `hag-gyo-saeng-hwar i.e.
Si s2s3s4. There are three kinds of sequences of syllables, i.e. s1 in C1,1, s152 in C2,1 and sis2.53 in C3,1
that can construct the word consisting of 8182 S3 S4
which would be put in C4,1. For instance, the word
81828384 (hag-gyo-saeng-hwal) is made with Si (hag)
combined with s25384 (gyo-saeng-hwal). Likewise,
it might be made by 8182 combined with 8354 and
.502.93 combined with s4. Since each cell has the
most probable result and its value, it is simple to
find the best segmentation for each syllables. In
addition, four cases, including the whole sequences
of syllables, are compared to make segmentation of
81828384 as follows:
1. min(vcdue(C3,1), va/ue(C3,4))
2. min(value(C2,1), va/ue(C2,3))
3. min(va/ue(Ci,i), va/ue(C3,2))
4. Word(sis2s3s4) = Word(hag-gyo-saeng-hwal)
Again, the most probable segmentation result is
put in C4,1 with the likelihood value for its segmentation. We call it MLS (Most Likely Segmentation)
n-1 ii
1
composition result
for
n-1
1
200
hag
hwal
gyo
gyo-saeng
hag-gyo
saeng
hag-gyo-seeng gyo-seeng-hwal
....._...._________.......„..-,....-.-&apos;')
ag-gyo-saeng-hwal arg max(min(w(hag-gyo-imeng),w(bwitM.
mintwaiag-hyoMv(caeng-hwal)).
mintw(hag),w(gymmengdcwal)).
Word(hag•gyo-siteng-hwal))
heom
-----7-----..Hequenee of syllables in disainet
default immolation pointer
Figure 5: Default segmentation pointer for (geonchug-sa-si-heone where (si-heom' is a very frequently
used noun.
geon chug
arg max(min(wMag Lw(gyo)l,w(hag-gyo))
Figure 3: State of table when analyzing 'hag-gyosaeng-hwal'. Here, w(si si) = value(Ci,j)
which is found in the following way:
MLS(C4,1) =
arg max(min(va/ue(C3,1), va/ue(C3,4)),
min(vaine(C2,1), vcdue(C2,3)),
min(vcdue(C1,1), va/ue(C3,2)),
Word(sis2 S3 S4))
From the four cases, the maximum value and the
segmentation result are selected and recorded in
C4,1. To generalize it, the algorithm is described
as shown in Figure 4.
The algorithm is straightforward. Let Word and
MLS be the likelihood of being a noun and the most
likely segmentation for a sequence of syllables. In the
initialization step, each cell of the table is assigned
Word value for a sequence of syllables si sj+i+i
using its frequency if it is found in SND. In other
words, if the value of Word for the sequence in each
cell is greater than zero, the syllables might be as a
noun a part of a compound noun and so the value is
recorded as MLS. It could be substituted by more
likely one in the segmentation process.
In order to make it efficient, the segmentation result is put as MLS instead of the syllables in case
the sequence of syllables exists in the HBND. The
minimum of each Word for constituents of the result
as Word is recorded.
Then, the segmenter compares possible analyses
to make a larger one as shown in Figure 4. Whenever Word of the entire syllables is less than that of
segmented one, the syllables and value are replaced
with the segmented result and its value. For instance, s1 + s2 and its likelihood substitutes C2,1
if min(Word(si ), Word(s2)) > Word(sis2). When
the entire syllables from the first to nth syllable are
processed, Cno. has the segmentation result.
The overall complexity of the algorithm follows
that of CYK parsing, 0(n3).
3.3 Default Analysis and Tuning
For the final result, we should take into consideration
several issues which are related with the syllables
that left unsegmented. There are several reasons
that the given string remains unsegmented:
1. The first one is a case where the string consists
of several nouns but one of them is a unregistered word. A compound noun `geon-chugsa-si-heom' is composed of (geon-chug-s& and
`si-heorn', which have the meanings of authorized architect and examination. In this case,
the unknown noun is caused by the suffix such
as 'Nil because the suffix derives many words.
However, it is known that it is very difficult to
treat the kinds of suffixes since the suffix like
(sa' is a very frequently used character in Korean and thus prone to make oversegmentation
if included in basic morphological analysis.
2. The string might consist of a proper noun ad a
noun representing a position or geometric information. For instance, a compound noun (kirndae-jung-dae-tong-ryeong' is composed of limdae-jung' and `dae-tong-ryeong' where the former is personal name and the latter means president respectively.
3. The string might be a proper noun itself. For
example, `tviill'ainseu' is a transliterated word
for foreign name 'Williams' and 'hong-gil-dong'
is a personal name in Korean. Generally, since
it has a different sequence of syllables from in
a general Korean word, it often remains unsegmented.
If the basic segmentation is failed, three procedures would be executed for solving three problems
above. For the first issue, we use the set of distinct
nouns. That is, the offset pointer is stored in the initialization step as well as frequency of each noun in
compound noun is recorded in the table. Attention
should be paid to non-frequent sequence of syllables
(ones in the set of supplementary nouns) in the default segmentation because it could be found in any
proper noun such as personal names, place names,
etc or transliterated words. It is known that the performance drops if all nouns in the compound noun
segmentation dictionary are considered for default
segmentation. We save the pointer to the boundary
only when a noun in distinct set appears. For the
above example (geon-chug-sa-si-heone, the default
segmentation would be `geon-chug-sa' and (.5i-hem&
since 'si-heom' is in the set of distinct nouns and the
pointer is set before (si-heorn' (Figure 5).
201
/* initialization step */
for i=1 to n do
for j=1 to n-i+1 do
value(Ci,j) = Word(si
MLS(Ci,j)= si ...sj+i-1; if value(Ci,j) > 0
4); otherwise
for i=2 to n do
for j=1 to i do
value(Ci,j) = max(min(vadue(Ci_i,j), vatue(Ci,j+i-1)),
min(va/ue(Ci-2,j), vattze(C2,i-2)),
min(vcdue(Ci ,i), vatue(Ci—i,j+1)),
Word(si ...si+j))
MLS(Ci,j)= arg max(min(vcaue(Ci—i,j), va/ue(Ci,j+i—i)),
min(uc1ue(Ci-2,i),va1ue(C2,j-2)),
min(valt4e(Ci,j),,vattie(Ci—i,j+i)),
Word(si
Figure 4: The segmentation algorithm
If this procedure is failed, the sequence of syllables
is checked whether it might be proper noun or not.
Since proper noun in Korean could have a kind of
nominal suffix such as `daetongryeong(president)' or
`ssi(Mr/Ms)' as mentioned above, we can identify
it by detaching the nominal suffixes. If there does
not exist any nominal suffix, then the entire syllables
would be regarded just as the transliterated foreign
word or a proper noun like personal or place name.
4 Experimental Results
For the test of compound noun segmentation, we
first extracted compound noun from ETRI POS
tagged corpus3. By the processing, 1774 types of
compound nouns were extracted, which was used as
a gold standard test set.
We evaluated our system by two methods: (1)
the precision and recall rate, and (2) segmentation
accuracy per compound noun which we refer to as
SA. They are defined respectively as follows:
Precision =
number of correct constituents in proposed segment results
total number of constituents in proposed segment results
Recall =
number of correct constituents in proposed segment results
total number of constituents in compoundnouns
SA =
number of correctly segmented compound nouns
total number of cornpoundnouns
3The corpus was constructed by the ETRI (Electronics
and Telecommunications Research Institute) project for standardization of natural language processing technology and the
corpus presented consists of about 270,000 eojeols at present.
What influences on the Korean IR system is
whether words are appropriately segmented or not.
The precision and recall estimate how appropriate
the segmentation results are. They are 98.04% and
97.80% respectively, which shows that our algorithm
is very effective (Table 3).
SA reflects how accurate the segmentation is for a
compound noun at all. We compared two methods:
(1) using only the segmentation algorithm with default analysis which is a baseline of our system and
so is needed to estimate the accuracy of the algorithm. (2) using both the built-in dictionary and the
segmentation algorithm which reflects system accuracy as a whole. As shown in Table 4, the baseline
performance using only distinct nouns and the algorithm is about 94.3% and fairly good. From the
results, we can find that the distinct nouns has great
impact on compound noun segmentation. Also, the
overall segmentation accuracy for the gold standard
is about 97.29% which is a very good result for the
application system. In addition, it shows that the
built-in dictionary supplements the algorithm which
results in better segmentation.
Lastly, we compare our system with the previous
work by (Yun et al. , 1997). It is impossible that we
directly compare our result with theirs, since the test
set is different. It was reported that the accuracy
given in the paper is about 95.6%. When comparing
the performance only in terms of the accuracy, our
system outperforms theirs.
Embeded in the morphological analyzer, the compound noun segmentater is currently being used for
some projects on MT and IE which are worked in
several institutes and it turns out that the system is
very effective.
202
Precision Recall
3553/3637
97.80
Number of correct constituents 3553/3628
Rate 98.04
Table 3: Result 1: Precision and recall rate
Table 4: Result 2: Segmentation accuracy for Compound Noun
SA
Whole System
1726/1774
97.29
Baseline
1673/1774
94.30
Number of correct constituents
Rate
5 Conclusions
In this paper, we presented the new method for
Korean compound noun segmentation. First, we
proposed the lexical acquisition for compound noun
analysis, which consists of the manually constructed
segmentation dictionary (HBSD) and the dictionary
for applying the segmentation algorithm (SND). The
hand-built segmentation dictionary was made manually for compound nouns extracted from corpus. The
simple noun dictionary is based on very frequently
occurring nouns which are called distinct nouns because they are clues for identifying constituents of
compound nouns. Second, the compound noun was
segmented based on the modification of CYK tabular parsing and min-max composition, which was
proven to be the very effective method by experiments. The bottom up approach using min-max
operation guarantees the most likely segmentation,
being applied in the same way as dynamic programming.
With our new method, the result for segmentation is as accurate as 97.29%. Especially, the algorithm made results good enough and the builtin dictionary supplemented the algorithm. Consequently, the methodology is promising and the segmentation system would be helpful for the application system such as machine translation and information retrieval.
6 Acknowledgement
We thank Prof. Mansuk Song at Yonsei Univ. and
Prof. Key-Sun Choi at KAIST to provide data for
experiments.
References
Cha, J., Lee, G. and Lee, J. 1998. Generalized Unknown Morpheme Guessing for Hybrid POS Tagging of Korean. In Proceedings of the 6th Workshop on Very Large Corpora.
Choi, K. S., Han, Y. S., Han, Y. G., and Kwon, 0.
W. 1994. KAIST Tree Bank Project for Korean:
Present and Future Development. In Proceedings
of the International Workshop on Sharable Natural Language Resources.
Elmi, M. A. and Evens, M. 1998. Spelling Correction Using Context. In Proceedings of COLING/ACL 98
Hoperoft, J. E. and Tillman, J. D. 1979. Introduction to Automata Theory, Languages, and Computation.
Jin, W. and Chen, L. 1995. Identifying Unknown
Words in Chinese Corpora In Proceedings of NLPRS 95
Lee, J. H. and Ahn, J. S. 1996. Using n-grams
for Korean Text Retrieval. In Proceedings of 19th
Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval
Li, J. and Wang, K. 1995. Study and Implementation of Nondictionary Chinese Segmentation. In
Proceedings of NLPRS 95
Nagao, M. and Mori, S. 1994. A New Method of
N-gram Statistics for Large Number of N and Automatic Extraction of Words and Phrases from
Large Text Data of Japanese. In Proceedings of
COLING
Park, B, R., Hwang, Y. S. and Rim, H. C. 1997.
Recognizing Korean Unknown Words by Comparatively Analyzing Example Words. In Proceedings
of ICCPOL 97
Sproat, R. W., Shih, W., Gale, W. and Chang,
N. 1994. A Stochastic Finite-State Wordsegmentation Algorithm for Chinese. In Proceedings of the 32nd Annual Meeting of ACL
Yoon, J., Kang, B. and Choi, K. S. 1999. Information Retrieval Based on Compound Noun Analysis
for Exact Term Extraction. Submitted in Journal
of Computer Processing of Orientla Language.
Yoon, J., Lee, W. and Choi, K. S. 1999. Word Segmentation Based on Estimation of Words from
Examples. Technical Report.
Yun, B. H., Cho, M. C. and Rim, H. C. 1997. Segmenting Korean Compound Nouns Using Statistical Information and a Preference Rules. In Proceedings of PA CLING.
203
Compound Noun Segmentation Based on Lexical Data Extracted from Corpus*
Juntae Yoon
jtyoon@linc.cis.upenn.edu
IRCS, University of Pennsylvania,
3401 Walnut St., Suite 400A, Philadelphia, PA 19104-6228, USA
Compound noun analysis is one of the crucial problems in Korean language processing because a series of nouns in Korean may appear without white space in real texts, which makes it difficult to identify the morphological constituents. This paper presents an effective method of Korean compound noun segmentation based on lexical data extracted from corpus. The segmentation is done by two steps: First, it is based on manually constructed built-in dictionary for segmentation whose data were extracted from 30 million word corpus. Second, a segmentation algorithm using statistical data is proposed, where simple nouns and their frequencies are also extracted from corpus. The analysis is executed based on CYK tabular parsing and min-max operation. By experiments, its accuracy is about 97.29%, which turns out to be very effective.
J Cha
G Lee
J Lee
Generalized Unknown Morpheme Guessing for Hybrid POS Tagging of Korean.
1998
In Proceedings of the 6th Workshop on Very Large Corpora.
Cha, Lee, Lee, 1998
Cha, J., Lee, G. and Lee, J. 1998. Generalized Unknown Morpheme Guessing for Hybrid POS Tagging of Korean. In Proceedings of the 6th Workshop on Very Large Corpora.
K S Choi
Y S Han
Y G Han
Kwon
KAIST Tree Bank Project for Korean: Present and Future Development.
1994
In Proceedings of the International Workshop on Sharable Natural Language Resources.
Choi, Han, Han, Kwon, 1994
Choi, K. S., Han, Y. S., Han, Y. G., and Kwon, 0. W. 1994. KAIST Tree Bank Project for Korean: Present and Future Development. In Proceedings of the International Workshop on Sharable Natural Language Resources.
M A Elmi
M Evens
Spelling Correction Using Context.
1998
In Proceedings of COLING/ACL 98
Elmi, Evens, 1998
Elmi, M. A. and Evens, M. 1998. Spelling Correction Using Context. In Proceedings of COLING/ACL 98
J E Hoperoft
J D Tillman
Introduction to Automata Theory, Languages, and Computation.
1979
Hoperoft, Tillman, 1979
Hoperoft, J. E. and Tillman, J. D. 1979. Introduction to Automata Theory, Languages, and Computation.
W Jin
L Chen
Identifying Unknown Words in Chinese Corpora
1995
In Proceedings of NLPRS 95
Jin, Chen, 1995
Jin, W. and Chen, L. 1995. Identifying Unknown Words in Chinese Corpora In Proceedings of NLPRS 95
J H Lee
J S Ahn
Using n-grams for Korean Text Retrieval.
1996
In Proceedings of 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval
distribution to them. For instance, gage has two possibilities of analysis i.e. `gage/N' and `ga/V+ge/E), and its frequency is 2263, in which the noun 'gage' is assigned 1132 as its frequency. Table 2 shows examples of manually corrected morphological analyses of eojeols containing a noun 'gage' and their frequencies. We call the nouns extracted in such a way a set of distinct nouns. In addition, we supplement the dictionary with other nouns not appeased in the words obtained by the method mentioned above. First, nouns of more than three syllables are rare in real texts in Korean, as shown in Lee and Ahn (1996). Their experiments proved that syllable based bigram indexing model makes much better result than other n-gram model such as trigram and quadragram in Korean IR. It follows that two syllable nouns take an overwhelming majority in nouns. Thus, there are not many such nouns in the simple nouns extracted by the manually corrected nouns (a set of distinct nouns). In particular, since many nouns of more 2At the mid-level of part of speech classification, for example, endings and postpositions are represented just by one tag e.g. E and P. To identify the sentential or clausal type (subordinate or d
Lee, Ahn, 1996
Lee, J. H. and Ahn, J. S. 1996. Using n-grams for Korean Text Retrieval. In Proceedings of 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval
J Li
K Wang
Study and Implementation of Nondictionary Chinese Segmentation.
1995
In Proceedings of NLPRS 95
Li, Wang, 1995
Li, J. and Wang, K. 1995. Study and Implementation of Nondictionary Chinese Segmentation. In Proceedings of NLPRS 95
M Nagao
S Mori
A New Method of N-gram Statistics for Large Number of N and Automatic Extraction of Words and Phrases from Large Text Data of Japanese.
1994
In Proceedings of COLING
Nagao, Mori, 1994
Nagao, M. and Mori, S. 1994. A New Method of N-gram Statistics for Large Number of N and Automatic Extraction of Words and Phrases from Large Text Data of Japanese. In Proceedings of COLING
B Park
R Hwang
Y S
H C Rim
Recognizing Korean Unknown Words by Comparatively Analyzing Example Words.
1997
In Proceedings of ICCPOL 97
Park, Hwang, S, Rim, 1997
Park, B, R., Hwang, Y. S. and Rim, H. C. 1997. Recognizing Korean Unknown Words by Comparatively Analyzing Example Words. In Proceedings of ICCPOL 97
R W Sproat
W Shih
W Gale
N Chang
A Stochastic Finite-State Wordsegmentation Algorithm for Chinese.
1994
In Proceedings of the 32nd Annual Meeting of ACL
Sproat, Shih, Gale, Chang, 1994
Sproat, R. W., Shih, W., Gale, W. and Chang, N. 1994. A Stochastic Finite-State Wordsegmentation Algorithm for Chinese. In Proceedings of the 32nd Annual Meeting of ACL
J Yoon
B Kang
K S Choi
Information Retrieval Based on Compound Noun Analysis for Exact Term Extraction. Submitted in
1999
Journal of Computer Processing of Orientla Language.
Yoon, Kang, Choi, 1999
Yoon, J., Kang, B. and Choi, K. S. 1999. Information Retrieval Based on Compound Noun Analysis for Exact Term Extraction. Submitted in Journal of Computer Processing of Orientla Language.
J Yoon
W Lee
K S Choi
Word Segmentation Based on Estimation of Words from Examples.
1999
Technical Report.
Yoon, Lee, Choi, 1999
Yoon, J., Lee, W. and Choi, K. S. 1999. Word Segmentation Based on Estimation of Words from Examples. Technical Report.
B H Yun
M C Cho
H C Rim
Segmenting Korean Compound Nouns Using Statistical Information and a Preference Rules.
1997
In Proceedings of PA CLING.
Yun, Cho, Rim, 1997
Yun, B. H., Cho, M. C. and Rim, H. C. 1997. Segmenting Korean Compound Nouns Using Statistical Information and a Preference Rules. In Proceedings of PA CLING.
