<newSection> Abstract Building on work detecting errors in dependency annotation, we set out to correct local dependency errors.
To do this, we outline the properties of annotation errors that make the task challenging and their existence problematic for learning.
For the task, we define a feature-based model that explicitly accounts for non-relations between words, and then use ambiguities from one model to constrain a second, more relaxed model.
In this way, we are successfully able to correct many errors, in a way which is potentially applicable to dependency parsing more generally.
<newSection> 1 Introduction and Motivation Annotation error detection has been explored for part-of-speech (POS), syntactic constituency, semantic role, and syntactic dependency annotation (see Boyd et al., 2008, and references therein).
Such work is extremely useful, given the harmfulness of annotation errors for training, including the learning of noise (e.g., Hogan, 2007; Habash et al., 2007), and for evaluation (e.g., Padro and Marquez, 1998).
But little work has been done to show the full impact of errors, or what types of cases are the most damaging, important since noise can sometimes be overcome (cf. Osborne, 2002).
Likewise, it is not clear how to learn from consistently misannotated data; studies often only note the presence of errors or eliminate them from evaluation (e.g., Hogan, 2007), and a previous attempt at correction was limited to POS annotation (Dickinson, 2006).
By moving from annotation error detection to error correction, we can more fully elucidate ways in which noise can be overcome and ways it cannot.
We thus explore annotation error correction and its feasibility for dependency annotation, a form of annotation that provides argument relations among words and is useful for training and testing dependency parsers (e.g., Nivre, 2006; McDonald and Pereira, 2006).
A recent innovation in dependency parsing, relevant here, is to use the predictions made by one model to refine another (Nivre and McDonald, 2008; Torres Martins et al., 2008).
This general notion can be employed here, as different models of the data have different predictions about whch parts are erroneous and can highlight the contributions of different features.
Using differences that complement one another, we can begin to sort accurate from inaccurate patterns, by integrating models in such a way as to learn the true patterns and not the errors.
Although we focus on dependency annotation, the methods are potentially applicable for different types of annotation, given that they are based on the similar data representations (see sections 2.1 and 3.2).
In order to examine the effects of errors and to refine one model with another’s information, we need to isolate the problematic cases.
The data representation must therefore be such that it clearly allows for the specific identification of errors between words.
Thus, we explore relatively simple models of the data, emphasizing small sub-structures (see section 3.2).
This simple modeling is not always rich enough for full dependency parsing, but different models can reveal conflicting information and are generally useful as part of a larger system.
Graph-based models of dependency parsing (e.g., McDonald et al., 2006), for example, rely on breaking parsing down into decisions about smaller substructures, and focusing on pairs of words has been used for domain adaptation (Chen et al., 2008) and in memory-based parsing (Canisius et al., 2006).
Exploring annotation error correction in this way can provide insights into more general uses of the annotation, just as previous work on correction for POS annotation (Dickinson, 2006) led to a way to improve POS tagging (Dickinson, 2007).
After describing previous work on error detection and correction in section 2, we outline in section 3 how we model the data, focusing on individual relations between pairs of words.
In section 4, we illustrate the difficulties of error correction and show how simple combinations of local features perform poorly.
Based on the idea that ambiguities from strict, lexical models can constrain more general POS models, we see improvement in error correction in section 5.
<newSection> 2 Background 2.1 Error detection We base our method of error correction on a form of error detection for dependency annotation (Boyd et al., 2008).
The variation n-gram approach was developed for constituency-based treebanks (Dickinson and Meurers, 2003, 2005) and it detects strings which occur multiple times in the corpus with varying annotation, the so-called variation nuclei.
For example, the variation nucleus next Tuesday occurs three times in the Wall Street Journal portion of the Penn Treebank (Tay-lor et al., 2003), twice labeled as NP and once as PP (Dickinson and Meurers, 2003).
Every variation detected in the annotation of a nucleus is classified as either an annotation error or as a genuine ambiguity.
The basic heuristic for detecting errors requires one word of recurring context on each side of the nucleus.
The nucleus with its repeated surrounding context is referred to as a variation n-gram.
While the original proposal expanded the context as far as possible given the repeated n-gram, using only the immediately surrounding words as context is sufficient for detecting errors with high precision (Boyd et al., 2008).
This “shortest” context heuristic receives some support from research on first language acquisition (Mintz, 2006) and unsupervised grammar induction (Klein and Manning, 2002).
The approach can detect both bracketing and la-beling errors in constituency annotation, and we already saw a labeling error for next Tuesday.
As an example of a bracketing error, the variation nucleus last month occurs within the NP its biggest jolt last month once with the label NP and once as a non-constituent, which in the algorithm is handled through a special label NIL.
The method for detecting annotation errors can be extended to discontinuous constituency annotation (Dickinson and Meurers, 2005), making it applicable to dependency annotation, where words in a relation can be arbitrarily far apart.
Specifically, Boyd et al.
(2008) adapt the method by treating dependency pairs as variation nuclei, and they include NIL elements for pairs of words not annotated as a relation.
The method is successful at detecting annotation errors in corpora for three different languages, with precisions of 93% for Swedish, 60% for Czech, and 48% for German.1 Correcting POS annotation errors can be done by applying a POS tagger and altering the input POS tags (Dickinson, 2006).
Namely, ambiguity class information (e.g., IN/RB/RP) is added to each corpus position for training, creating complex ambiguity tags, such as <IN/RB/RP,IN>.
While this results in successful correction, it is not clear how it applies to annotation which is not positional and uses NIL labels.
However, ambiguity class information is relevant when there is a choice between labels; we return to this in section 5.
<newSection> 3 Modeling the data For our data set, we use the written portion (sec-tions P and G) of the Swedish Talbanken05 treebank (Nivre et al., 2006), a reconstruction of the Talbanken76 corpus (Einarsson, 1976) The written data of Talbanken05 consists of 11,431 sentences with 197,123 tokens, annotated using 69 types of dependency relations.
This is a small sample, but it matches the data used for error detection, which results in 634 shortest non-fringe variation n-grams, corresponding to 2490 tokens.
From a subset of 210 nuclei (917 tokens), hand-evaluation reveals error detection precision to be 93% (195/210), with 274 (of the 917) corpus positions in need of correction (Boyd et al., 2008).
This means that 643 positions do not need to be corrected, setting a baseline of 70.1% (643/917) for error correction.2 Following Dickinson (2006), we train our models on the entire corpus, explicitly including NIL relations (see section 3.2); we train on the original annotation, but not the corrections.
Annotation error correction involves overcoming noise in the corpus, in order to learn the true patterns underlying the data.
This is a slightly different goal from that of general dependency parsing methods, which often integrate a variety of features in making decisions about dependency relations (cf., e.g., Nivre, 2006; McDonald and Pereira, 2006).
Instead of maximizing a feature model to improve parsing, we isolate individual pieces of information (e.g., context POS tags), thereby being able to pinpoint, for example, when non-local information is needed for particular types of relations and pointing to cases where pieces of information conflict (cf. also McDonald and Nivre, 2007).
To support this isolation of information, we use dependency pairs as the basic unit of analysis and assign a dependency label to each word pair.
Following Boyd et al.
(2008), we add L or R to the label to indicate which word is the head, the left (L) or the right (R).
This is tantamount to handling pairs of words as single entries in a “lex-icon” and provides a natural way to talk of ambiguities.
Breaking the representation down into strings whch receive a label also makes the method applicable to other annotation types (e.g., Dickin-son and Meurers, 2005).
A major issue in generating a lexicon is how to handle pairs of words which are not dependencies.
We follow Boyd et al.
(2008) and generate NIL labels for those pairs of words which also occur as a true labeled relation.
In other words, only word pairs which can be relations can also be NILs.
For every sentence, then, when we produce feature lists (see section 3.3), we produce them for all word pairs that are related or could potentially be related, but not those which have never been observed as a dependency pair.
This selection of NIL items works because there are no unknown words.
We use the method in Dickinson and Meur-ers (2005) to efficiently calculate the NIL tokens.
Focusing on word pairs and not attempting to build a a whole dependency graph allows us to explore the relations between different kinds of features, and it has the potential benefit of not relying on possibly erroneous sister relations.
From the perspective of error correction, we cannot assume that information from the other relations in the sentence is reliable.3 This representation also fits nicely with previous work, both in error detection (see section 2.1) and in dependency parsing (e.g., Canisius et al., 2006; Chen et al., 2008).
Most directly, Canisius et al.
(2006) integrate such a representation into a memory-based dependency parser, treating each pair individually, with words and POS tags as features.
We employ memory-based learning (MBL) for correction.
MBL stores all corpus instances as vectors of features, and given a new instance, the task of the classifier is to find the most similar cases in memory to deduce the best class.
Given the previous discussion of the goals of correcting errors, what seems to be needed is a way to find patterns which do not fully generalize because of noise appearing in very similar cases in the corpus.
As Zavrel et al.
(1997, p. 137) state about the advantages of MBL: Because language-processing tasks typically can only be described as a complex interaction of regularities, sub-regularities and (families of) exceptions, storing all empirical data as potentially useful in analogical extrapolation works better than extracting the main regularities and forgetting the individual examples (Daelemans, 1996).
By storing all corpus examples, as MBL does, both correct and incorrect data is maintained, allowing us to pinpoint the effect of errors on training.
For our experiments, we use TiMBL, version 6.1 (Daelemans et al., 2007), with the default settings.
We use the default overlap metric, as this maintains a direct connection to majority-based correction.
We could run TiMBL with different values of k, as this should lead to better feature integration.
However, this is difficult to explore without development data, and initial experiments with higher k values were not promising (see section 4.2).
To fully correct every error, one could also experiment with a real dependency parser in the future, in order to look beyond the immediate context and to account for interactions between rela-3We use POS information, which is also prone to errors, but on a different level of annotation.
Still, this has its problems, as discussed in section 4.1. tions.
The approach to correction pursued here, however, isolates problems for assigning dependency structures, highlighting the effectiveness of different features within the same local domain.
Initial experiments with a dependency parser were again not promising (see section 4.2).
When using features for individual relations, we have different options for integrating them.
On the one hand, one can simply additively combine features into a larger vector for training, as described in section 4.2.
On the other hand, one can use one set of features to constrain another set, as described in section 5.
Pulling apart the features commonly employed in dependency parsing can help indicate the contributions each has on the classification.
This general idea is akin to the notion of classifier stacking, and in the realm of dependency parsing, Nivre and McDonald (2008) successfully stack classifiers to improve parsing by “allow[ing] a model to learn relative to the predictions of the other” (p. 951).
The output from one classifier is used as a feature in the next one (see also Tor-res Martins et al., 2008).
Nivre and McDonald (2008) use different kinds of learning paradigms, but the general idea can be carried over to a situation using the same learning mechanism.
Instead of focusing on what one learning algorithm informs another about, we ask what one set of more or less informative features can inform another set about, as described in section 5.1.
<newSection> 4 Performing error correction The task of automatic error correction in some sense seems straightforward, in that there are no unknown words.
Furthermore, we are looking at identical recurring words, which should for the most part have consistent annotation.
But it is precisely this similarity of local contexts that makes the correction task challenging.
Given that variations contain sets of corpus positions with differing labels, it is tempting to take the error detection output and use a heuristic of “majority rules” for the correction cases, i.e., correct the cases to the majority label.
When using only information from the word sequence, this runs into problems quickly, however, in that there are many non-majority labels which are correct.
Some of these non-majority cases pattern in uniform ways and are thus more correctable; others are less tractable in being corrected, as they behave in non-uniform and often non-local ways.
Exploring the differences will highlight what can and cannot be easily corrected, underscoring the difficulties in training from erroneous annotation.
Uniform non-majority cases The first problem with correction to the majority label is an issue of coverage: a large number of variations are ties between two different labels.
Out of 634 shortest non-fringe variation nuclei, 342 (53.94%) have no majority label; for the corresponding 2490 tokens, 749 (30.08%) have no majority tag.
The variation ¨ar v¨ag (’is way’), for example, appears twice with the same local context shown in (1),4 once incorrectly labeled as OO-L (other object [head on the left]) and once correctly as SP-L (subjective predicative complement).
To distinguish these two, more information is necessary than the exact sequence of words.
In this case, for example, looking at the POS categories of the nuclei could potentially lead to accurate correction: AV NN is SP-L 1032 times and OO-L 32 times (AV = the verb “vara” (be), NN = other noun).
While some ties might require non-local information, we can see that local—but more general— information could accurately break this tie.
Secondly, in a surprising number of cases where there is a majority tag (122 out of the 917 tokens we have a correction for), a non-majority label is actually correct.
For the example in (2), the string institution kvarleva (‘institution remnant’) varies between CC-L (sister of first conjunct in binary branching analysis of coordination) and AN-L (apposition).5 CC-L appears 5 times and AN-L 3 times, but the CC-L cases are incorrect and need to be changed to AN-L.
(2) en f¨or˚aldrad institution/NN an obsolete institution kvarleva/NN fr˚an 1800-talets remnant from the 1800s Other cases with a non-majority label have other problems.
In example (3), for instance, the string under h¨agnet (‘under protection’) varies in this context between HD-L (other head, 3 cases) and PA-L (complement of preposition, 5 cases), where the PA-L cases need to be corrected to HD-L.
Both of these categories are new, so part of the issue here could be in the consistency of the conversion.
the other is idiomatic, despite having identical local context.
In these examples, at least the POS labels are different.
Note, though, that in (4) we need to trust the POS labels to overcome the similarity of text, and in (3) we need to distrust them.6 with other words an ¨andam˚alsenlig ...
appropriate (3) fria liv under/PR h¨agnet/ID|NN b.
Med/AB andra ord/ID en form av free life under the protection with other words a form of av/IDJPR ett en g˚ang givet l¨ofte prostitution . of a one time given promise prostitution The additional problem is that there are other, correlated errors in the analysis, as shown in figure 1.
In the case of the correct HD analysis, both h¨agnet and av are POS-annotated as ID (part of idiom (multi-word unit)) and are HD dependents of under, indicating that the three words make up an idiom.
The PA analysis is a non-idiomatic analysis, with h¨agnet as NN.
Without non-local information, some legitimate variations are virtually irresolvable.
Consider (5), for instance: here, we find variation between SS-R (other subject), as in (5a), and FS-R (dummy subject), as in (5b).
Crucially, the POS tags are the same, and the context is the same.
What differen-tiates these cases is that g˚ar has a different set of dependents in the two sentences, as shown in figure 2; to use this information would require us to trust the rest of the dependency structure or to use a dependency parser which accurately derives the structural differences.
Significantly, h¨agnet only appears 10 times in the corpus, all with under as its head, 5 times HD-L and 5 times PA-L.
We will not focus explicitly on correcting these types of cases, but the example serves to emphasize the necessity of correction at all levels of annotation.
Non-uniform non-majority cases All of the above cases have in common that whatever change is needed, it needs to be done for all positions in a variation.
But this is not sound, as error detection precision is not 100%.
Thus, there are variations which clearly must not change.
For example, in (4), there is legitimate variation between PA-L (4a) and HD-L (4b), stemming from the fact that one case is non-idiomatic, and While some variations require non-local information, we have seen that some cases are correctable simply with different kinds of local information (cf. (1)).
In this paper, we will not attempt to directly cover non-local cases or cases with POS annotation problems, instead trying to improve the integration of different pieces of local information.
In our experiments, we trained simple models of the original corpus using TiMBL (see section 3.3) and then tested on the same corpus.
The models we use include words (W) and/or tags (T) for nucleus and/or context positions, where context here refers only to the immediately surrounding words.
These are outlined in table 1, for different models of the nucleus (Nuc.) and the context (Con.).
For instance, the model 6 representation of example (6) (=(1)) consists of all the underlined words and tags.
(6) k¨arlekens v¨ag/NN ¨ar/AV en/EN l˚ang/AJ v¨ag/NN och/++ man g¨or oklokt ...
In table 1, we report the precision figures for different models on the 917 positions we have corrections for.
We report the correction precision for positions the classifier changed the label of (Changed), and the overall correction precision (Overall).
We also report the precision TiMBL has for the whole corpus, with respect to the original tags (instead of the corrected tags).
We can draw a few conclusions from these results.
First, all models using contexual information perform essentially the same—approximately 50% on changed positions and 73% overall.
When not generalizing to new data, simply adding features (i.e., words or tags) to the model is less important than the sheer presence of context.
This is true even for some higher values of k: model 6, for example, has only 73.2% and 72.1% overall precision for k = 2 and k = 3, respectively.
Secondly, these results confirm that the task is difficult, even for a corpus with relatively high error detection precision (see section 2.1).
Despite high similarity of context (e.g., model 6), the best results are only around 73%, and this is given a baseline (no changes) of 70%.
While a more expansive set of features would help, there are other problems here, as the method appears to be overtraining.
There is no question that we are learning the “correct” patterns, i.e., 99.9% similarity to the benchmark in the best cases.
The problem is that, for error correction, we have to overcome noise in the data.
Training and testing with the dependency parser MaltParser (Nivre et al., 2007, default settings) is no better, with 72.1% overall precision (despite a labeled attachment score of 98.3%).
Recall in this light that there are variations for which the non-majority label is the correct one; attempting to get a non-majority label correct using a strict lexical model does not work.
To be able not to learn the erroneous patterns requires a more general model.
Interestingly, a more general model—e.g., treating the corpus as a sequence of tags (model 8)—results in equally good correction, without being a good overall fit to the corpus data (only 92.7%).
This model, too, learns noise, as it misses cases that the lexical models get correct.
Simply combining the features does not help (cf. model 6); what we need is to use information from both stricter and looser models in a way that allows general patterns to emerge without overgeneralizing.
<newSection> 5 Model combination Given the discussion in section 4.1 surrounding examples (1)-(5), it is clear that the information needed for correction is sometimes within the immediate context, although that information is needed, however, is often different.
Consider the more general models, 7 and 8, which only use POS tag information.
While sometimes this general information is effective, at times it is dramatically incorrect.
For example, for (7), the original (incor-rect) relation between finna and erbjuda is CC-L; the model 7 classifier selects OO-L as the correct tag; model 8 selects NIL; and the correct label is +F-L (coordination at main clause level).
(7) f¨ors¨oker try ¨oppna marknaden eller erbjuda/VV andra open market or to offer other arbetsm¨ojligheter . work possibilities The original variation for the nucleus finna erb-juda (‘find offer’) is between CC-L and +F-L, but when represented as the POS tags VV VV (other verb), there are 42 possible labels, with OO-L being the most frequent.
This allows for too much confusion.
If model 7 had more restrictions on the set of allowable tags, it could make a more sensible choice and, in this case, select the correct label.
Previous error correction work (Dickinson, 2006) used ambiguity classes for POS annotation, and this is precisely the type of information we need to constrain the label to one which we know is relevant to the current case.
Here, we investigate ambiguity class information derived from one model integrated into another model.
There are at least two main ways we can use ambiguity classes in our models.
The first is what we have just been describing: an ambiguity class can serve as a constraint on the set of possible outcomes for the system.
If the correct label is in the ambiguity class (as it usually is for error correction), this constraining can do no worse than the original model.
The other way to use an ambiguity class is as a feature in the model.
The success of this approach depends on whether or not each ambiguity class patterns in its own way, i.e., defines a sub-regularity within a feature set.
We consider two different feature models, those containing only tags (models 7 and 8), and add to these ambiguity classes derived from two other models, those containing only words (models 1 and 3).
To correct the labels, we need models which do not strictly adhere to the corpus, and the tag-based models are best at this (see the TiMBL results in table 1).
The ambiguity classes, however, must be fairly constrained, and the word-based models do this best (cf. example (7)).
As described in section 5.1, we can use ambiguity classes to constrain the output of a model.
Specifically, we take models 7 and 8 and constrain each selected tag to be one which is within the ambiguity class of a lexical model, either 1 or 3.
That is, if the TiMBL-determined label is not in the ambiguity class, we select the most likely tag of the ones which are.
If no majority label can be decided from this restricted set, we fall back to the TiMBL-selected tag.
In (7), for instance, if we use model 7, the TiMBL tag is OO-L, but model 3’s ambiguity class restricts this to either CC-L or +F-L.
For the representation VV VV, the label CC-L appears 315 times and +F-L 544 times, so +F-L is correctly selected.7 The results are given in table 2, which can be compared to the the original models 7 and 8 in table 1, i.e., total precisions of 49.5% and 73.2%, respectively.
With these simple constraints, model 8 now outperforms any other model (75.5%), and model 7 begins to approach all the models that use contextual information (68.8%).
Ambiguity classes from one model can also be used as features for another (see section 5.1); in this case, ambiguity class information from lexical models (1 and 3) is used as a feature for POS tag models (7 and 8).
The results are given in table 3, where we can see dramatically improved performance from the original models (cf. table 1) and generally improved performance over using ambiguity classes as constraints (cf. table 2).
If we compare the two results for model 7 (61.9% vs. 72.1%) and then the two results for model 8 (76.4% vs. 73.6%), we observe that the better use of ambiguity classes integrates contextual and non-contextual features.
Model 7 (POS, no context) with model 3 ambiguity classes (lex-ical, with context) is better than using ambiguity classes derived from a non-contextual model.
For model 8, on the other hand, which uses contextual POS features, using the ambiguity class without context (model 1) does better.
In some ways, this combination of model 8 with model 1 ambiguity classes makes the most sense: ambiguity classes are derived from a lexicon, and for dependency annotation, a lexicon can be treated as a set of pairs of words.
It is also noteworthy that model 7, despite not using context directly, achieves comparable results to all the previous models using context, once appropriate ambiguity classes are employed.
<newSection> 5.2.3 Both methods Given that the results of ambiguity classes as features are better than that of constraining, we can now easily combine both methodologies, by constraining the output from section 5.2.2 with the ambiguity class tags.
The results are given in table 4; as we can see, all results are a slight improvement over using ambiguity classes as features without constraining the output (table 3).
Using only local context, the best model here is 3.2% points better than the best original model, representing an improvement in correction.
<newSection> 6 Summary and Outlook After outlining the challenges of error correction, we have shown how to integrate information from different models of dependency annotation in order to perform annotation error correction.
By using ambiguity classes from lexical models, both as features and as constraints on the final output, we saw improvements in POS models that were able to overcome noise, without using non-local information.
A first step in further validating these methods is to correct other dependency corpora; this is limited, of course, by the amount of corpora with corrected data available.
Secondly, because this work is based on features and using ambiguity classes, it can in principle be applied to other types of annotation, e.g., syntactic constituency annotation and semantic role annotation.
In this light, it is interesting to note the connection to annotation error detection: the work here is in some sense an extension of the variation n-gram method.
Whether it can be employed as an error detection system on its own requires future work.
Another way in which this work can be extended is to explore how these representations and integration of features can be used for dependency parsing.
There are several issues to work out, however, in making insights from this work more general.
First, it is not clear that pairs of words are sufficiently general to treat them as a lexicon, when one is parsing new data.
Secondly, we have explicit representations for word pairs not annotated as a dependency relation (i.e., NILs), and these are constrained by looking at those which are the same words as real relations.
Again, one would have to determine which pairs of words need NIL representations in new data.
<newSection> Acknowledgements Thanks to Yvonne Samuelsson for help with the Swedish examples; to Joakim Nivre, Mattias Nilsson, and Eva Pettersson for the evaluation data for Talbanken05; and to the three anonymous reviewers for their insightful comments.
<newSection> References<newSection> Abstract Building on work detecting errors in dependency annotation, we set out to correct local dependency errors.
To do this, we outline the properties of annotation errors that make the task challenging and their existence problematic for learning.
For the task, we define a feature-based model that explicitly accounts for non-relations between words, and then use ambiguities from one model to constrain a second, more relaxed model.
In this way, we are successfully able to correct many errors, in a way which is potentially applicable to dependency parsing more generally.
<newSection> 1 Introduction and Motivation Annotation error detection has been explored for part-of-speech (POS), syntactic constituency, semantic role, and syntactic dependency annotation (see Boyd et al., 2008, and references therein).
Such work is extremely useful, given the harmfulness of annotation errors for training, including the learning of noise (e.g., Hogan, 2007; Habash et al., 2007), and for evaluation (e.g., Padro and Marquez, 1998).
But little work has been done to show the full impact of errors, or what types of cases are the most damaging, important since noise can sometimes be overcome (cf. Osborne, 2002).
Likewise, it is not clear how to learn from consistently misannotated data; studies often only note the presence of errors or eliminate them from evaluation (e.g., Hogan, 2007), and a previous attempt at correction was limited to POS annotation (Dickinson, 2006).
By moving from annotation error detection to error correction, we can more fully elucidate ways in which noise can be overcome and ways it cannot.
We thus explore annotation error correction and its feasibility for dependency annotation, a form of annotation that provides argument relations among words and is useful for training and testing dependency parsers (e.g., Nivre, 2006; McDonald and Pereira, 2006).
A recent innovation in dependency parsing, relevant here, is to use the predictions made by one model to refine another (Nivre and McDonald, 2008; Torres Martins et al., 2008).
This general notion can be employed here, as different models of the data have different predictions about whch parts are erroneous and can highlight the contributions of different features.
Using differences that complement one another, we can begin to sort accurate from inaccurate patterns, by integrating models in such a way as to learn the true patterns and not the errors.
Although we focus on dependency annotation, the methods are potentially applicable for different types of annotation, given that they are based on the similar data representations (see sections 2.1 and 3.2).
In order to examine the effects of errors and to refine one model with another’s information, we need to isolate the problematic cases.
The data representation must therefore be such that it clearly allows for the specific identification of errors between words.
Thus, we explore relatively simple models of the data, emphasizing small sub-structures (see section 3.2).
This simple modeling is not always rich enough for full dependency parsing, but different models can reveal conflicting information and are generally useful as part of a larger system.
Graph-based models of dependency parsing (e.g., McDonald et al., 2006), for example, rely on breaking parsing down into decisions about smaller substructures, and focusing on pairs of words has been used for domain adaptation (Chen et al., 2008) and in memory-based parsing (Canisius et al., 2006).
Exploring annotation error correction in this way can provide insights into more general uses of the annotation, just as previous work on correction for POS annotation (Dickinson, 2006) led to a way to improve POS tagging (Dickinson, 2007).
After describing previous work on error detection and correction in section 2, we outline in section 3 how we model the data, focusing on individual relations between pairs of words.
In section 4, we illustrate the difficulties of error correction and show how simple combinations of local features perform poorly.
Based on the idea that ambiguities from strict, lexical models can constrain more general POS models, we see improvement in error correction in section 5.
<newSection> 2 Background 2.1 Error detection We base our method of error correction on a form of error detection for dependency annotation (Boyd et al., 2008).
The variation n-gram approach was developed for constituency-based treebanks (Dickinson and Meurers, 2003, 2005) and it detects strings which occur multiple times in the corpus with varying annotation, the so-called variation nuclei.
For example, the variation nucleus next Tuesday occurs three times in the Wall Street Journal portion of the Penn Treebank (Tay-lor et al., 2003), twice labeled as NP and once as PP (Dickinson and Meurers, 2003).
Every variation detected in the annotation of a nucleus is classified as either an annotation error or as a genuine ambiguity.
The basic heuristic for detecting errors requires one word of recurring context on each side of the nucleus.
The nucleus with its repeated surrounding context is referred to as a variation n-gram.
While the original proposal expanded the context as far as possible given the repeated n-gram, using only the immediately surrounding words as context is sufficient for detecting errors with high precision (Boyd et al., 2008).
This “shortest” context heuristic receives some support from research on first language acquisition (Mintz, 2006) and unsupervised grammar induction (Klein and Manning, 2002).
The approach can detect both bracketing and la-beling errors in constituency annotation, and we already saw a labeling error for next Tuesday.
As an example of a bracketing error, the variation nucleus last month occurs within the NP its biggest jolt last month once with the label NP and once as a non-constituent, which in the algorithm is handled through a special label NIL.
The method for detecting annotation errors can be extended to discontinuous constituency annotation (Dickinson and Meurers, 2005), making it applicable to dependency annotation, where words in a relation can be arbitrarily far apart.
Specifically, Boyd et al.
(2008) adapt the method by treating dependency pairs as variation nuclei, and they include NIL elements for pairs of words not annotated as a relation.
The method is successful at detecting annotation errors in corpora for three different languages, with precisions of 93% for Swedish, 60% for Czech, and 48% for German.1 Correcting POS annotation errors can be done by applying a POS tagger and altering the input POS tags (Dickinson, 2006).
Namely, ambiguity class information (e.g., IN/RB/RP) is added to each corpus position for training, creating complex ambiguity tags, such as <IN/RB/RP,IN>.
While this results in successful correction, it is not clear how it applies to annotation which is not positional and uses NIL labels.
However, ambiguity class information is relevant when there is a choice between labels; we return to this in section 5.
<newSection> 3 Modeling the data For our data set, we use the written portion (sec-tions P and G) of the Swedish Talbanken05 treebank (Nivre et al., 2006), a reconstruction of the Talbanken76 corpus (Einarsson, 1976) The written data of Talbanken05 consists of 11,431 sentences with 197,123 tokens, annotated using 69 types of dependency relations.
This is a small sample, but it matches the data used for error detection, which results in 634 shortest non-fringe variation n-grams, corresponding to 2490 tokens.
From a subset of 210 nuclei (917 tokens), hand-evaluation reveals error detection precision to be 93% (195/210), with 274 (of the 917) corpus positions in need of correction (Boyd et al., 2008).
This means that 643 positions do not need to be corrected, setting a baseline of 70.1% (643/917) for error correction.2 Following Dickinson (2006), we train our models on the entire corpus, explicitly including NIL relations (see section 3.2); we train on the original annotation, but not the corrections.
Annotation error correction involves overcoming noise in the corpus, in order to learn the true patterns underlying the data.
This is a slightly different goal from that of general dependency parsing methods, which often integrate a variety of features in making decisions about dependency relations (cf., e.g., Nivre, 2006; McDonald and Pereira, 2006).
Instead of maximizing a feature model to improve parsing, we isolate individual pieces of information (e.g., context POS tags), thereby being able to pinpoint, for example, when non-local information is needed for particular types of relations and pointing to cases where pieces of information conflict (cf. also McDonald and Nivre, 2007).
To support this isolation of information, we use dependency pairs as the basic unit of analysis and assign a dependency label to each word pair.
Following Boyd et al.
(2008), we add L or R to the label to indicate which word is the head, the left (L) or the right (R).
This is tantamount to handling pairs of words as single entries in a “lex-icon” and provides a natural way to talk of ambiguities.
Breaking the representation down into strings whch receive a label also makes the method applicable to other annotation types (e.g., Dickin-son and Meurers, 2005).
A major issue in generating a lexicon is how to handle pairs of words which are not dependencies.
We follow Boyd et al.
(2008) and generate NIL labels for those pairs of words which also occur as a true labeled relation.
In other words, only word pairs which can be relations can also be NILs.
For every sentence, then, when we produce feature lists (see section 3.3), we produce them for all word pairs that are related or could potentially be related, but not those which have never been observed as a dependency pair.
This selection of NIL items works because there are no unknown words.
We use the method in Dickinson and Meur-ers (2005) to efficiently calculate the NIL tokens.
Focusing on word pairs and not attempting to build a a whole dependency graph allows us to explore the relations between different kinds of features, and it has the potential benefit of not relying on possibly erroneous sister relations.
From the perspective of error correction, we cannot assume that information from the other relations in the sentence is reliable.3 This representation also fits nicely with previous work, both in error detection (see section 2.1) and in dependency parsing (e.g., Canisius et al., 2006; Chen et al., 2008).
Most directly, Canisius et al.
(2006) integrate such a representation into a memory-based dependency parser, treating each pair individually, with words and POS tags as features.
We employ memory-based learning (MBL) for correction.
MBL stores all corpus instances as vectors of features, and given a new instance, the task of the classifier is to find the most similar cases in memory to deduce the best class.
Given the previous discussion of the goals of correcting errors, what seems to be needed is a way to find patterns which do not fully generalize because of noise appearing in very similar cases in the corpus.
As Zavrel et al.
(1997, p. 137) state about the advantages of MBL: Because language-processing tasks typically can only be described as a complex interaction of regularities, sub-regularities and (families of) exceptions, storing all empirical data as potentially useful in analogical extrapolation works better than extracting the main regularities and forgetting the individual examples (Daelemans, 1996).
By storing all corpus examples, as MBL does, both correct and incorrect data is maintained, allowing us to pinpoint the effect of errors on training.
For our experiments, we use TiMBL, version 6.1 (Daelemans et al., 2007), with the default settings.
We use the default overlap metric, as this maintains a direct connection to majority-based correction.
We could run TiMBL with different values of k, as this should lead to better feature integration.
However, this is difficult to explore without development data, and initial experiments with higher k values were not promising (see section 4.2).
To fully correct every error, one could also experiment with a real dependency parser in the future, in order to look beyond the immediate context and to account for interactions between rela-3We use POS information, which is also prone to errors, but on a different level of annotation.
Still, this has its problems, as discussed in section 4.1. tions.
The approach to correction pursued here, however, isolates problems for assigning dependency structures, highlighting the effectiveness of different features within the same local domain.
Initial experiments with a dependency parser were again not promising (see section 4.2).
When using features for individual relations, we have different options for integrating them.
On the one hand, one can simply additively combine features into a larger vector for training, as described in section 4.2.
On the other hand, one can use one set of features to constrain another set, as described in section 5.
Pulling apart the features commonly employed in dependency parsing can help indicate the contributions each has on the classification.
This general idea is akin to the notion of classifier stacking, and in the realm of dependency parsing, Nivre and McDonald (2008) successfully stack classifiers to improve parsing by “allow[ing] a model to learn relative to the predictions of the other” (p. 951).
The output from one classifier is used as a feature in the next one (see also Tor-res Martins et al., 2008).
Nivre and McDonald (2008) use different kinds of learning paradigms, but the general idea can be carried over to a situation using the same learning mechanism.
Instead of focusing on what one learning algorithm informs another about, we ask what one set of more or less informative features can inform another set about, as described in section 5.1.
<newSection> 4 Performing error correction The task of automatic error correction in some sense seems straightforward, in that there are no unknown words.
Furthermore, we are looking at identical recurring words, which should for the most part have consistent annotation.
But it is precisely this similarity of local contexts that makes the correction task challenging.
Given that variations contain sets of corpus positions with differing labels, it is tempting to take the error detection output and use a heuristic of “majority rules” for the correction cases, i.e., correct the cases to the majority label.
When using only information from the word sequence, this runs into problems quickly, however, in that there are many non-majority labels which are correct.
Some of these non-majority cases pattern in uniform ways and are thus more correctable; others are less tractable in being corrected, as they behave in non-uniform and often non-local ways.
Exploring the differences will highlight what can and cannot be easily corrected, underscoring the difficulties in training from erroneous annotation.
Uniform non-majority cases The first problem with correction to the majority label is an issue of coverage: a large number of variations are ties between two different labels.
Out of 634 shortest non-fringe variation nuclei, 342 (53.94%) have no majority label; for the corresponding 2490 tokens, 749 (30.08%) have no majority tag.
The variation ¨ar v¨ag (’is way’), for example, appears twice with the same local context shown in (1),4 once incorrectly labeled as OO-L (other object [head on the left]) and once correctly as SP-L (subjective predicative complement).
To distinguish these two, more information is necessary than the exact sequence of words.
In this case, for example, looking at the POS categories of the nuclei could potentially lead to accurate correction: AV NN is SP-L 1032 times and OO-L 32 times (AV = the verb “vara” (be), NN = other noun).
While some ties might require non-local information, we can see that local—but more general— information could accurately break this tie.
Secondly, in a surprising number of cases where there is a majority tag (122 out of the 917 tokens we have a correction for), a non-majority label is actually correct.
For the example in (2), the string institution kvarleva (‘institution remnant’) varies between CC-L (sister of first conjunct in binary branching analysis of coordination) and AN-L (apposition).5 CC-L appears 5 times and AN-L 3 times, but the CC-L cases are incorrect and need to be changed to AN-L.
(2) en f¨or˚aldrad institution/NN an obsolete institution kvarleva/NN fr˚an 1800-talets remnant from the 1800s Other cases with a non-majority label have other problems.
In example (3), for instance, the string under h¨agnet (‘under protection’) varies in this context between HD-L (other head, 3 cases) and PA-L (complement of preposition, 5 cases), where the PA-L cases need to be corrected to HD-L.
Both of these categories are new, so part of the issue here could be in the consistency of the conversion.
the other is idiomatic, despite having identical local context.
In these examples, at least the POS labels are different.
Note, though, that in (4) we need to trust the POS labels to overcome the similarity of text, and in (3) we need to distrust them.6 with other words an ¨andam˚alsenlig ...
appropriate (3) fria liv under/PR h¨agnet/ID|NN b.
Med/AB andra ord/ID en form av free life under the protection with other words a form of av/IDJPR ett en g˚ang givet l¨ofte prostitution . of a one time given promise prostitution The additional problem is that there are other, correlated errors in the analysis, as shown in figure 1.
In the case of the correct HD analysis, both h¨agnet and av are POS-annotated as ID (part of idiom (multi-word unit)) and are HD dependents of under, indicating that the three words make up an idiom.
The PA analysis is a non-idiomatic analysis, with h¨agnet as NN.
Without non-local information, some legitimate variations are virtually irresolvable.
Consider (5), for instance: here, we find variation between SS-R (other subject), as in (5a), and FS-R (dummy subject), as in (5b).
Crucially, the POS tags are the same, and the context is the same.
What differen-tiates these cases is that g˚ar has a different set of dependents in the two sentences, as shown in figure 2; to use this information would require us to trust the rest of the dependency structure or to use a dependency parser which accurately derives the structural differences.
Significantly, h¨agnet only appears 10 times in the corpus, all with under as its head, 5 times HD-L and 5 times PA-L.
We will not focus explicitly on correcting these types of cases, but the example serves to emphasize the necessity of correction at all levels of annotation.
Non-uniform non-majority cases All of the above cases have in common that whatever change is needed, it needs to be done for all positions in a variation.
But this is not sound, as error detection precision is not 100%.
Thus, there are variations which clearly must not change.
For example, in (4), there is legitimate variation between PA-L (4a) and HD-L (4b), stemming from the fact that one case is non-idiomatic, and While some variations require non-local information, we have seen that some cases are correctable simply with different kinds of local information (cf. (1)).
In this paper, we will not attempt to directly cover non-local cases or cases with POS annotation problems, instead trying to improve the integration of different pieces of local information.
In our experiments, we trained simple models of the original corpus using TiMBL (see section 3.3) and then tested on the same corpus.
The models we use include words (W) and/or tags (T) for nucleus and/or context positions, where context here refers only to the immediately surrounding words.
These are outlined in table 1, for different models of the nucleus (Nuc.) and the context (Con.).
For instance, the model 6 representation of example (6) (=(1)) consists of all the underlined words and tags.
(6) k¨arlekens v¨ag/NN ¨ar/AV en/EN l˚ang/AJ v¨ag/NN och/++ man g¨or oklokt ...
In table 1, we report the precision figures for different models on the 917 positions we have corrections for.
We report the correction precision for positions the classifier changed the label of (Changed), and the overall correction precision (Overall).
We also report the precision TiMBL has for the whole corpus, with respect to the original tags (instead of the corrected tags).
We can draw a few conclusions from these results.
First, all models using contexual information perform essentially the same—approximately 50% on changed positions and 73% overall.
When not generalizing to new data, simply adding features (i.e., words or tags) to the model is less important than the sheer presence of context.
This is true even for some higher values of k: model 6, for example, has only 73.2% and 72.1% overall precision for k = 2 and k = 3, respectively.
Secondly, these results confirm that the task is difficult, even for a corpus with relatively high error detection precision (see section 2.1).
Despite high similarity of context (e.g., model 6), the best results are only around 73%, and this is given a baseline (no changes) of 70%.
While a more expansive set of features would help, there are other problems here, as the method appears to be overtraining.
There is no question that we are learning the “correct” patterns, i.e., 99.9% similarity to the benchmark in the best cases.
The problem is that, for error correction, we have to overcome noise in the data.
Training and testing with the dependency parser MaltParser (Nivre et al., 2007, default settings) is no better, with 72.1% overall precision (despite a labeled attachment score of 98.3%).
Recall in this light that there are variations for which the non-majority label is the correct one; attempting to get a non-majority label correct using a strict lexical model does not work.
To be able not to learn the erroneous patterns requires a more general model.
Interestingly, a more general model—e.g., treating the corpus as a sequence of tags (model 8)—results in equally good correction, without being a good overall fit to the corpus data (only 92.7%).
This model, too, learns noise, as it misses cases that the lexical models get correct.
Simply combining the features does not help (cf. model 6); what we need is to use information from both stricter and looser models in a way that allows general patterns to emerge without overgeneralizing.
<newSection> 5 Model combination Given the discussion in section 4.1 surrounding examples (1)-(5), it is clear that the information needed for correction is sometimes within the immediate context, although that information is needed, however, is often different.
Consider the more general models, 7 and 8, which only use POS tag information.
While sometimes this general information is effective, at times it is dramatically incorrect.
For example, for (7), the original (incor-rect) relation between finna and erbjuda is CC-L; the model 7 classifier selects OO-L as the correct tag; model 8 selects NIL; and the correct label is +F-L (coordination at main clause level).
(7) f¨ors¨oker try ¨oppna marknaden eller erbjuda/VV andra open market or to offer other arbetsm¨ojligheter . work possibilities The original variation for the nucleus finna erb-juda (‘find offer’) is between CC-L and +F-L, but when represented as the POS tags VV VV (other verb), there are 42 possible labels, with OO-L being the most frequent.
This allows for too much confusion.
If model 7 had more restrictions on the set of allowable tags, it could make a more sensible choice and, in this case, select the correct label.
Previous error correction work (Dickinson, 2006) used ambiguity classes for POS annotation, and this is precisely the type of information we need to constrain the label to one which we know is relevant to the current case.
Here, we investigate ambiguity class information derived from one model integrated into another model.
There are at least two main ways we can use ambiguity classes in our models.
The first is what we have just been describing: an ambiguity class can serve as a constraint on the set of possible outcomes for the system.
If the correct label is in the ambiguity class (as it usually is for error correction), this constraining can do no worse than the original model.
The other way to use an ambiguity class is as a feature in the model.
The success of this approach depends on whether or not each ambiguity class patterns in its own way, i.e., defines a sub-regularity within a feature set.
We consider two different feature models, those containing only tags (models 7 and 8), and add to these ambiguity classes derived from two other models, those containing only words (models 1 and 3).
To correct the labels, we need models which do not strictly adhere to the corpus, and the tag-based models are best at this (see the TiMBL results in table 1).
The ambiguity classes, however, must be fairly constrained, and the word-based models do this best (cf. example (7)).
As described in section 5.1, we can use ambiguity classes to constrain the output of a model.
Specifically, we take models 7 and 8 and constrain each selected tag to be one which is within the ambiguity class of a lexical model, either 1 or 3.
That is, if the TiMBL-determined label is not in the ambiguity class, we select the most likely tag of the ones which are.
If no majority label can be decided from this restricted set, we fall back to the TiMBL-selected tag.
In (7), for instance, if we use model 7, the TiMBL tag is OO-L, but model 3’s ambiguity class restricts this to either CC-L or +F-L.
For the representation VV VV, the label CC-L appears 315 times and +F-L 544 times, so +F-L is correctly selected.7 The results are given in table 2, which can be compared to the the original models 7 and 8 in table 1, i.e., total precisions of 49.5% and 73.2%, respectively.
With these simple constraints, model 8 now outperforms any other model (75.5%), and model 7 begins to approach all the models that use contextual information (68.8%).
Ambiguity classes from one model can also be used as features for another (see section 5.1); in this case, ambiguity class information from lexical models (1 and 3) is used as a feature for POS tag models (7 and 8).
The results are given in table 3, where we can see dramatically improved performance from the original models (cf. table 1) and generally improved performance over using ambiguity classes as constraints (cf. table 2).
If we compare the two results for model 7 (61.9% vs. 72.1%) and then the two results for model 8 (76.4% vs. 73.6%), we observe that the better use of ambiguity classes integrates contextual and non-contextual features.
Model 7 (POS, no context) with model 3 ambiguity classes (lex-ical, with context) is better than using ambiguity classes derived from a non-contextual model.
For model 8, on the other hand, which uses contextual POS features, using the ambiguity class without context (model 1) does better.
In some ways, this combination of model 8 with model 1 ambiguity classes makes the most sense: ambiguity classes are derived from a lexicon, and for dependency annotation, a lexicon can be treated as a set of pairs of words.
It is also noteworthy that model 7, despite not using context directly, achieves comparable results to all the previous models using context, once appropriate ambiguity classes are employed.
<newSection> 5.2.3 Both methods Given that the results of ambiguity classes as features are better than that of constraining, we can now easily combine both methodologies, by constraining the output from section 5.2.2 with the ambiguity class tags.
The results are given in table 4; as we can see, all results are a slight improvement over using ambiguity classes as features without constraining the output (table 3).
Using only local context, the best model here is 3.2% points better than the best original model, representing an improvement in correction.
<newSection> 6 Summary and Outlook After outlining the challenges of error correction, we have shown how to integrate information from different models of dependency annotation in order to perform annotation error correction.
By using ambiguity classes from lexical models, both as features and as constraints on the final output, we saw improvements in POS models that were able to overcome noise, without using non-local information.
A first step in further validating these methods is to correct other dependency corpora; this is limited, of course, by the amount of corpora with corrected data available.
Secondly, because this work is based on features and using ambiguity classes, it can in principle be applied to other types of annotation, e.g., syntactic constituency annotation and semantic role annotation.
In this light, it is interesting to note the connection to annotation error detection: the work here is in some sense an extension of the variation n-gram method.
Whether it can be employed as an error detection system on its own requires future work.
Another way in which this work can be extended is to explore how these representations and integration of features can be used for dependency parsing.
There are several issues to work out, however, in making insights from this work more general.
First, it is not clear that pairs of words are sufficiently general to treat them as a lexicon, when one is parsing new data.
Secondly, we have explicit representations for word pairs not annotated as a dependency relation (i.e., NILs), and these are constrained by looking at those which are the same words as real relations.
Again, one would have to determine which pairs of words need NIL representations in new data.
<newSection> Acknowledgements Thanks to Yvonne Samuelsson for help with the Swedish examples; to Joakim Nivre, Mattias Nilsson, and Eva Pettersson for the evaluation data for Talbanken05; and to the three anonymous reviewers for their insightful comments.
<newSection> References<newSection> Abstract This paper presents six novel approaches to biographic fact extraction that model structural, transitive and latent properties of biographical data.
The ensemble of these proposed models substantially outperforms standard pattern-based biographic fact extraction methods and performance is further improved by modeling inter-attribute correlations and distributions over functions of attributes, achieving an average extraction accuracy of 80% over seven types of biographic attributes.
<newSection> 1 Introduction Extracting biographic facts such as “Birthdate”, “Occupation”, “Nationality”, etc. is a critical step for advancing the state of the art in information processing and retrieval.
An important aspect of web search is to be able to narrow down search results by distinguishing among people with the same name leading to multiple efforts focusing on web person name disambiguation in the literature (Mann and Yarowsky, 2003; Artiles et al., 2007, Cucerzan, 2007).
While biographic facts are certainly useful for disambiguating person names, they also allow for automatic extraction of ency-lopedic knowledge that has been limited to manual efforts such as Britannica, Wikipedia, etc.
Such encyploedic knowledge can advance vertical search engines such as http://www.spock.com that are focused on people searches where one can get an enhanced search interface for searching by various biographic attributes.
Biographic facts are also useful for powerful query mechanisms such as finding what attributes are common between two people (Auer and Lehmann, 2007).
While there are a large quantity of biographic texts available online, there are only a few biographic fact databases available1, and most of them have been created manually, are incomplete and are available primarily in English.
This work presents multiple novel approaches for automatically extracting biographic facts such as “Birthdate”, “Occupation”, “Nationality”, and “Religion”, making use of diverse sources of information present in biographies.
In particular, we have proposed and evaluated the following 6 distinct original approaches to this task with large collective empirical gains: We propose and evaluate techniques for exploiting all of the above classes of information in the next sections.
<newSection> 2 Related Work The literature for biography extraction falls into two major classes.
The first one deals with identifying and extracting biographical sentences and treats the problem as a summarization task (Cowie et al., 2000, Schiffman et al., 2001, Zhou et al., 2004).
The second and more closely related class deals with extracting specific facts such as “birthplace”, “occupation”, etc.
For this task, the primary theme of work in the literature has been to treat the task as a general semantic-class learning problem where one starts with a few seeds of the semantic relationship of interest and learns contextual patterns such as “<NAME> was born in <Birthplace>” or “<NAME> (born <Birthdate>)” (Hearst, 1992; Riloff, 1996; The-len and Riloff, 2002; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Mann and Yarowsky, 2003; Jijkoun et al., 2004; Mann and Yarowsky, 2005; Alfonseca et al., 2006; Pasca et al., 2006).
There has also been some work on extracting biographic facts directly from Wikipedia pages.
Culotta et al.
(2006) deal with learning contextual patterns for extracting family relationships from Wikipedia.
Ruiz-Casado et al.
(2006) learn contextual patterns for biographic facts and apply them to Wikipedia pages.
While the pattern-learning approach extends well for a few biography classes, some of the biographic facts like “Gender” and “Religion” do not have consistent contextual patterns, and only a few of the explicit biographic attributes such as “Birthdate”, “Deathdate”, “Birthplace” and “Oc-cupation” have been shown to work well in the pattern-learning framework (Mann and Yarowsky, 2005; Alfonesca, 2006; Pasca et al., 2006).
Secondly, there is a general lack of work that attempts to utilize the typical information sequencing within biographic texts for fact extraction, and we show how the information structure of biographies can be used to improve upon pattern based models.
Furthermore, we also present additional novel models of attribute correlation and age distribution that aid the extraction process.
<newSection> 3 Approach We first implement the standard pattern-based approach for extracting biographic facts from the raw prose in Wikipedia people pages.
We then present an array of novel techniques exploiting different classes of information including partially-tethered contextual patterns, relative attribute position and sequence, transitive attributes of co-occurring entities, broad-context topical profiles, inter-attribute correlations and likely human age distributions.
For illustrative purposes, we motivate each technique using one or two attributes but in practice they can be applied to a wide range of attributes and empirical results in Table 4 show that they give consistent performance gains across multiple attributes.
<newSection> 4 Contextual Pattern-Based Model A standard model for extracting biographic facts is to learn templatic contextual patterns such as <NAME> “was born in” <Birthplace>.
Such templatic patterns can be learned using seed examples of the attribute in question and, there has been a plethora of work in the seed-based boot-strapping literature which addresses this problem (Ravichandran and Hovy, 2002; Thelen and Riloff, 2002; Mann and Yarowsky, 2005; Alfonseca et al., 2006; Pasca et al., 2006) Thus for our baseline we implemented a standard Ravichandran and Hovy (2002) pattern learning model using 100 seed2 examples from an online biographic database called NNDB (http://www.nndb.com) for each of the biographic attributes: “Birthdate”, “Birthplace”, “Death-date”, “Gender”, “Nationality”, “Occupation” and “Religion”.
Given the seed pairs, patterns for each attribute were learned by searching for seed <Name,Attribute Value> pairs in the Wikipedia page and extracting the left, middle and right contexts as various contextual patterns3.
While the biographic text was obtained from Wikipedia articles, all of the 7 attribute values used as seed and test person names could not be obtained from Wikipedia due to incomplete and unnormalized (for attribute value format) in-foboxes.
Hence, the values for training/evaluation were extracted from NNDB which provides a cleaner set of gold truth, and is similar to an approach utilizing trained annotators for marking up and extracting the factual information in a standard format.
For consistency, only the people names whose articles occur in Wikipedia where selected as part of seed and test sets.
Given the attribute values of the seed names and their text articles, the probability of a relationship r(Attribute Name), given the surrounding context “A1 p A2 q A3”, where p and q are <NAME> and <Attrib Val> respectively, is given using the rote extractor model probability as in (Ravichan-dran and Hovy, 2002; Mann and Yarowsky 2005): 2The seed examples were chosen randomly, with a bias against duplicate attribute values to increase training diversity.
Both the seed and test names and data will be made available online to the research community for replication and extension.
3We implemented a noisy model of coreference resolution by resolving any gender-correct pronoun used in the Wikipedia page to the title person name of the article.
Gender is also extracted automatically as a biographic attribute.
Thus, the probability for each contextual pattern is based on how often it correctly predicts a relationship in the seed set.
And, each extracted attribute value q using the given pattern can thus be ranked according to the above probability.
We tested this approach for extracting values for each of the seven attributes on a test set of 100 held-out names and report Precision, Pseudo-recall and F-score for each attribute which are computed in the standard way as follows, for say Attribute “Birth-place (bplace)”: Since the true values of each attribute are obtained from a cleaner and normalized person-database (NNDB), not all the attribute values maybe present in the Wikipedia article for a given name.
Thus, we also compute accuracy on the subset of names for which the value of a given attribute is also ex-plictly stated in the article.
This is denoted as: # people with bplace correctly extracted Acctruth pres = # of people with true bplace stated in article We further applied a domain model for each attribute to filter noisy targets extracted from lexical patterns.
Our domain models of attributes include lists of acceptable values (such as lists of places, occupations and religions) and structural constraints such as possible date formats for “Birthdate” and “Deathdate”.
The rows with subscript “RH02”in Table 4 shows the performance of this Ravichandran and Hovy (2002) model with additional attribute domain modeling for each attribute, and Table 3 shows the average performance across all attributes.
<newSection> 5 Partially Untethered Templatic Contextual Patterns The pattern-learning literature for fact extraction often consists of patterns with a “hook” and “target” (Mann and Yarowsky, 2005).
For example, in the pattern “<Name> was born in <Birthplace>”, “<NAME>” is the hook and “<Birthplace>” is the target.
The disadvantage of this approach is that the intervening dually-tethered patterns can be quite long and highly variable, such as “<NAME> was highly influential in his role as <Occupation>”.
We overcome this problem by modeling partially untethered variable-length ngram patterns adjacent to only the target, with the only constraint being that the hook entity appear somewhere in the sentence4.
Examples of these new contextual ngram features include “his role as <Occupation>” and ‘role as <Occupation>”.
The pattern probability model here is essentially the same as in Ravichan-dran and Hovy, 2002 and just the pattern representation is changed.
The rows with subscript “RH02imp” in tables 4 and 3 show performance gains using this improved templatic-pattern-based model, yielding an absolute 21% gain in accuracy.
<newSection> 6 Document-Position-Based Model One of the properties of biographic genres is that primary biographic attributes5 tend to appear in characteristic positions, often toward the beginning of the article.
Thus, the absolute position (in percentage) can be modeled explicitly using a Gaussian parametric model as follows for choosing the best candidate value v* for a given attribute A: In the above equation, posnv is the absolute position ratio (position/length) and µA, 0A2 are the sample mean and variance based on the sample of correct position ratios of attribute values in biographies with attribute A.
Figure 2, for example, shows the positional distribution of the seed attribute values for deathdate, nationality and religion in Wikipedia articles, fit to a Gaussian distribution.
Combining this empirically derived position model with a domain model6 of acceptable attribute values is effective enough to serve as a stand-alone model.
value in the Wikipedia pages of the seed names used for learning relative ordering among attributes satisfying the domain model In practice, for attributes such as birthdate, the first text pattern satisfying the domain model is often the correct answer for biographical articles.
Deathdate also tends to occur near the beginning of the article, but almost always some point after the birthdate.
This motivates a second, sequence-based position model based on the rank of the attribute values among other values in the domain of the attribute, as follows: where P(rankv|A) is the fraction of biographies having attribute a with the correct value occuring at rank rankv, where rank is measured according to the relative order in which the values belonging to the attribute domain occur from the beginning of the article.
We use the seed set to learn the relative positions between attributes, that is, in the Wikipedia pages of seed names what is the rank of the correct attribute.
Table 1 shows the most frequent rank of the correct attribute value and Figure 3 shows the distribution of the correct ranks for a sample of attributes.
We can see that 61% of the time the first location mentioned in a biography is the individuals’s birthplace, while 58% of the time the 2nd date in the article is the deathdate.
Thus, “Deathdate” often appears as the second date in a Wikipedia page as expected.
These empirical distributions for the correct rank provide a direct vehicle for scoring hypotheses, and the rows with “rel.
posn” as the subscript in Table 4 shows the improvement in performance using the learned relative ordering.
Averaging across different attributes, table 3 shows an absolute 11% average gain in accuracy of the position-sequence-based models relative to the improved Ravichandran and Hovy results achieved here.
Figure 3: Empirical distribution of the relative position of the correct (seed) answers among all text phrases satisfying the domain model for “birth-place” and “death date”.
<newSection> 7 Implicit Models Some of the biographic attributes such as “Nation-ality”, “Occupation” and “Religion” can be extracted successfully even when the answer is not directly mentioned in the biographic article.
We present two such models for doing so in the following subsections: Attributes such as “Occupation” are transitive in nature, that is, the people names appearing close to the target name will tend to have the same occupation as the target name.
Based on this intution, we implemented a transitive model that predicts occupation based on consensus voting via the extracted occupations of neighboring names7 as follows: # neighboring names with attrib value v # of neighboring names in the article The set of neighboring names is represented as Sneighbors and the best candidate value for an attribute A is chosen based on the the fraction of neighboring names having the same value for the respective attribute.
We rank candidates according to this probability and the row labeled “trans” in Table 4 shows that this model helps in subsantially improving the recall of “Occupation” and “Religion”, yielding a 7% and 3% average improvement in F-measure respectively, on top of the position model described in Section 6.
In addition to modeling cross-entity attribute transitively, attributes such as “Occupation” can also be modeled successfully using a document-wide context or topic model.
For example, the distribution of words occuring in a biography of a politician would be different from that of a scientist.
Thus, even if the occupation is not explicitly mentioned in the article, one can infer it using a bag-of-words topic profile learned from the seed examples.
Given a value v, for an attribute A, (for example v = “Politician” and A = “Occupation”), we learn a centroid weight vector: tft,v is the frequency of word t in the articles of People having attribute A = v |A |is the total number of values of attribute A |t E A |is the total number of values of attribute A, such that the articles of people having one of those values contain the term t N is the total number of People in the seed set Given a biography article of a test name and an attribute in question, we compute a similar word weight vector C0 = [w0 1, w02, ..., w0n] for the test name and measure its cosine similarity to the centroid vector of each value of the given attribute.
Thus, the best value a∗ is chosen as: Tables 3 and 4 show performance using the latent document-wide-context model.
We see that this model by itself gives the top performance on “Occupation”, outperforming the best alternative model by 9% absolute accuracy, indicating the usefulness of implicit attribute modeling via broad-context word frequencies.
This latent model can be further extended using the multilingual nature of Wikipedia.
We take the corresponding German pages of the training names and model the German word distributions characterizing each seed occupation.
Table 4 shows that English attribute classification can be successful using only the words in a parallel German article.
For some attributes, the performance of latent model modeled via cross-language (noted as latentCL) is close to that of English suggesting potential future work by exploiting this multilin-gual dimension.
It is interesting to note that both the transitive model and the latent wide-context model do not rely on the actual “Occupation” being explicitly mentioned in the article, they still outperform explicit pattern-based and position-based models.
This implicit modeling also helps in improving the recall of less-often directly mentioned attributes such as a person’s “Religion”.
<newSection> 8 Model Combination While the pattern-based, position-based, transitive and latent models are all stand-alone models, they can complement each other in combination as they provide relatively orthogonal sources of information.
To combine these models, we perform a simple backoff-based combination for each attribute based on stand-alone model performance, and the rows with subscript “combined” in Tables 3 and 4 shows an average 14% absolute performance gain of the combined model relative to the improved Ravichandran and Hovy 2002 model.
<newSection> 9 Further Extensions: Reducing False Positives Since the position-and-domain-based models will almost always posit an answer, one of the problems is the high number of false positives yielded by these algorithms.
The following subsections introduce further extensions using interesting properties of biographic attributes to reduce the effect of false positives.
One of the ways to filter false positives is by filtering empirically incompatible inter-attribute pairings.
The motivation here is that the attributes are not independent of each other when modeled for the same individual.
For example, P(Religion=Hindu I Nationality=India) is higher than P(Religion=Hindu I Nationality=France) and similarly we can find positive and negative correlations among other attribute pairings.
For implementation, we consider all possible 3-tuples of (“Nationality”, “Birthplace”, “Religion”)8 and search on NNDB for the presence of the tuple for any individual in the database (excluding the test data of course).
As an agressive but effective filter, we filter the tuples for which no name in NNDB was found containing the candidate 3-tuples.
The rows with label “combined+corr” in Table 4 and Table 3 shows substantial performaance gains using inter-attribute correlations, such as the 7% absolute average gain for Birthplace over the Section 8 combined models, and a 3% absolute gain for Nationality and Religion.
Another way to filter out false positives is to consider distributions on meta-attributes, for example: while age is not explicitly extracted, we can use the fact that age is a function of two extracted attributes (<Deathyear>-<Birthyear>) and use the age distribution to filter out false positives for <Birthdate> and <Deathdate>.
Based on the age distribution for famous people9 on the web shown in Figure 5, we can bias against unusual candidate lifespans and filter out completely those outside the range of 25-100, as most of the probability mass is concentrated in this range.
Rows with subscript “comb + age dist” in Table 4 shows the performance gains using this feature, yielding an average 5% absolute accuracy gain for Birthdate.
<newSection> 10 Conclusion This paper has shown six successful novel approaches to biographic fact extraction using structural, transitive and latent properties of biographic data.
We first showed an improvement to the standard Ravichandran and Hovy (2002) model utilizing untethered contextual pattern models, followed by a document position and sequence-based approach to attribute modeling.
Next we showed transitive models exploiting the tendency for individuals occurring together in an article to have related attribute values.
We also showed how latent models of wide document context, both monolingually and translingually, can capture facts that are not stated directly in a text.
Each of these models provide substantial performance gain, and further performance gain is achived via classifier combination.
We also showed how inter-attribution correlations can be of all the models across several biographic attributes.
modeled to filter unlikely attribute combinations, and how models of functions over attributes, such as deathdate-birthdate distributions, can further constrain the candidate space.
These approaches collectively achieve 80% average accuracy on a test set of 7 biographic attribute types, yielding a 37% absolute accuracy gain relative to a standard algorithm on the same data.
<newSection> References<newSection> Abstract This paper addresses the problem of extracting the most important facts from a news article.
Our approach uses syntactic, semantic, and general statistical features to identify the most important sentences in a document.
The importance of the individual features is estimated using generalized iterative scaling methods trained on an annotated newswire corpus.
The performance of our approach is evaluated against 300 unseen news articles and shows that use of these features results in statistically significant improvements over a provenly robust baseline, as measured using metrics such as precision, recall and ROUGE.
<newSection> 1 Introduction The increasing amount of information that is available to both professional users (such as journalists, financial analysts and intelligence analysts) and lay users has called for methods condensing information, in order to make the most important content stand out.
Several methods have been proposed over the last two decades, among which keyword extraction and summarization are the most prominent ones.
Keyword extraction aims to identify the most relevant words or phrases in a document, e.g., (Witten et al., 1999), while summarization aims to provide a short (commonly 100 words), coherent full-text summary of the document, e.g., (McKeown et al., 1999).
Key fact extraction falls in between key word extraction and summarization.
Here, the challenge is to identify the most relevant facts in a document, but not necessarily in a coherent full-text form as is done in summarization.
Evidence of the usefulness of key fact extraction is CNN’s web site which since 2006 has most of its news articles preceded by a list of story highlights, see Figure 1.
The advantage of the news highlights as opposed to full-text summaries is that they are much ‘easier on the eye’ and are better suited for quick skimming.
So far, only CNN.com offers this service and we are interested in finding out to what extent it can be automated and thus applied to any newswire source.
Although these highlights could be easily generated by the respective journalists, many news organization shy away from introducing an additional manual stage into the workflow, where pushback times of minutes are considered unacceptable in an extremely competitive news business which competes in terms of seconds rather than minutes.
Automating highlight generation can help eliminate those delays.
Journalistic training emphasizes that news articles should contain the most important information in the beginning, while less important information, such as background or additional details, appears further down in the article.
This is also the main reason why most summarization systems applied to news articles do not outperform a simple baseline that just uses the first 100 words of an article (Svore et al., 2007; Nenkova, 2005).
On the other hand, most of CNN’s story highlights are not taken from the beginning of the articles.
In fact, more than 50% of the highlights stem from sentences that are not among the first 100 words of the articles.
This makes identifying story highlights a much more challenging task than single-document summarization in the news domain.
In order to automate story highlight identification we automatically extract syntactic, semantic, and purely statistical features from the document.
The weights of the features are estimated using machine learning techniques, trained on an annotated corpus.
In this paper, we focus on identifying the relevant sentences in the news article from which the highlights were generated.
The system we have implemented is named AURUM: AUtomatic Retrieval of Unique information with Machine learning.
A full system would also contain a sentence compression step (Knight and Marcu, 2000), but since both steps are largely independent of each other, existing sentence compression or simplification techniques can be applied to the sentences identified by our approach.
The remainder of this paper is organized as follows: The next section describes the relevant work done to date in keyfact extraction and automatic summarization.
Section 3 lays out our features and explains how they were learned and estimated.
Section 4 presents the experimental setup and our results, and Section 5 concludes with a short discussion.
<newSection> 2 Related Work As mentioned above, the problem of identifying story highlight lies somewhere between keyword extraction and single-document summarization.
The KEA keyphrase extraction system (Witten et al., 1999) mainly relies on purely statistical features such as term frequencies, using the tf.idf measure from Information Retrieval,1 as well as on a term’s position in the text.
In addition to tf.idf scores, Hulth (2004) uses part-of-speech tags and NP chunks and complements this with machine learning; the latter has been used to good results in similar cases (Turney, 2000; Neto et al., 2002).
The B&C system (Barker and Cornacchia, 2000), also used linguistic methods to a very limited extent, identifying NP heads.
INFORMATIONFINDER (Krulwich and Burkey, 1996) requires user feedback to train the system, whereby a user notes whether a given document is of interest to them and specifies their own key-words which are then learned by the system.
Over the last few years, numerous single-as well as multi-document summarization approaches have been developed.
In this paper we will focus mainly on single-document summarization as it is more relevant to the issue we aim to address and traditionally proves harder to accomplish.
A good example of a powerful approach is a method named Maximum Marginal Relevance which extracts a sentence for the summary only if it is different than previously selected ones, thereby striving to reduce redundancy (Carbonell and Goldstein, 1998).
More recently, the work of Svore et al.
(2007) is closely related to our approach as it has also exploited the CNN Story Highlights, although their focus was on summarization and using ROUGE as an evaluation and training measure.
Their approach also heavily relies on additional data resources, mainly indexed Wikipedia articles and Microsoft Live query logs, which are not readily available.
Linguistic features are today used mostly in summarization systems, and include the standard features sentence length, n-gram frequency, sentence position, proper noun identification, similarity to title, tf.idf, and so-called ‘bonus’/‘stigma’ words (Neto et al., 2002; Leite et al., 2007; Pol-lock and Zamora, 1975; Goldstein et al., 1999).
On the other hand, for most of these systems, simple statistical features and tf.idf still turn out to be the most important features.
Attempts to integrate discourse models have also been made (Thione et al., 2004), hand in hand with some of Marcu’s (1995) earlier work.
highlight: 61 percent of those polled now say it was not worth invading Iraq, poll says Text: Now, 61 percent of those surveyed say it was not worth invading Iraq, according to the poll.
Regarding syntax, it seems to be used mainly in sentence compression or trimming.
The algorithm used by Dorr et al.
(2003) removes subordinate clauses, to name one example.
While our approach does not use syntactical features as such, it is worth noting these possible enhancements.
<newSection> 3 Approach In this section we describe which features were used and how the data was annotated to facilitate feature extraction and estimation.
In order to determine the features used for predicting which sentences are the sources for story highlights, we gathered statistics from 1,200 CNN newswire articles.
An additional 300 articles were set aside to serve as a test set later on.
The articles were taken from a wide range of topics: politics, business, sport, health, world affairs, weather, entertainment and technology.
Only articles with story highlights were considered.
For each article we extracted a number of n-gram statistics, where n E {1, 2, 3}.
n-gram score.
We observed the frequency and probability of unigrams, bigrams and trigrams appearing in both the article body and the highlights of a given story.
An important phrase (of length n < 3) in the article would likely be used again in the highlights.
These phrases were ranked and scored according to the probability of their appearing in a given text and its highlights.
Trigger phrases.
These are phrases which cause adjacent words to appear in the highlights.
Over the entire set, such phrases become significant.
We specified a limit of 2 words to the left and 4 words to the right of a phrase.
For example, the word according caused other words in the same sentence to appear in the highlights nearly 25% of the time.
Consider the highlight/sentence pair in Table 1: The word according receives a score of 3 since {invading, Iraq, poll} are all in the highlight.
It should be noted that the trigram {invading Iraq according} would receive an identical score, since {not, worth, poll} are in the highlights as well.
Spawned phrases.
Conversely, spawned phrases occur frequently in the highlights and in close proximity to trigger phrases.
Continuing the example in Table 1, {invading, Iraq, poll, not, worth} are all considered to be spawned phrases.
Of course, simply using the identities of words neglects the issue of lexical paraphrasing, e.g., involving synonyms, which we address to some extent by using WordNet and other features described in this Section.
Table 2 gives an example involving paraphrasing.
Other approaches have tried to select linguistic features which could be useful (Chuang and Yang, 2000), but these gather them under one heading rather than treating them as separate features.
The identification of common verbs has been used both as a positive (Turney, 2000) and as a negative feature (Goldstein et al., 1999) in some systems, whereas we score such terms according to a scale.
Turney also uses a ‘final adjective‘ measure.
Use of a thesaurus has also shown to improve results in automatic summarization, even in multi-document environments (McKeown et al., 1999) and other languages such as Portuguese (Leite et al., 2007).
By manually inspecting the training data, the linguistic features were selected.
AURUM has two types of features: sentence features, such as the position of the sentence or the existence of a negation word, receive the same value for the entire sentence.
On the other hand, word features are evaluated for each of the words in the sentence, normalized over the number of words in the sentence.
Our features resemble those suggested by previous works in keyphrase extraction and automatic summarization, but map more closely to the journalistic characteristics of the corpus, as explained in the following.
Temporal adverbs.
Manually compiled list of phrases, such as after less than, for two weeks and Thursday.
Mention of the news agency’s name.
Journalistic scoops and other exclusive nuggets of information often recall the agency’s name, especially when there is an element of self-advertisement involved, as in “...
The debates are being held by CNN, WMUR and the New Hampshire Union Leader.”
It is interesting to note that an opposite approach has previously been taken (Goldstein et al., 1999), albeit involving a different corpus.
These are the features which apply once for each sentence.
Position of the sentence in the text.
Intuitively, facts of greater importance will be placed at the beginning of the text, and this is supported by the data, as can be seen in Figure 2.
Only half of the highlights stem from sentences in the first fifth of the article.
Nevertheless, selecting sentences from only the first few lines is not a sure-fire approach.
Table 3 presents an article in which none of the first four sentences were in the highlights.
While the baseline found no sentences, AURUM’s performance was better.
The sentence positions score is defined as pi = 1 − (log i/log N), where i is the position of the sentence in the article and N the total number of sentences in the article.
Numbers or dates.
This is especially evident in news reports mentioning figures of casualties, opinion poll results, or financial news.
Source attribution.
Phrasings such as according to a source or officials say.
Negations.
Negations are often used for introducing new or contradictory information: “Kelly is due in a Chicago courtroom Friday for yet another status hearing, but there’s still no trial date in sight.2” We selected a number of typical negation phrases to this end.
Causal adverbs.
Manually compiled list of phrases, including in order to, hoping for and because.
tional Cemetery, honoring U.
S. troops who have fought and died for freedom and expressing his resolve to succeed in the war in Iraq.
21. Elsewhere, Alabama’s Gulf Coast was once again packed with holiday-goers after the damage from hurricanes Ivan and Katrina in 2004 and 2005 kept the tourists away.
These features are tested on each word in the sentence.
‘Bonus’ words.
A list of phrases similar to sensational, badly, ironically, historic, identified from the training data.
This is akin to ‘bonus’/‘stigma’ words (Neto et al., 2002; Leite et al., 2007; Pol-lock and Zamora, 1975; Goldstein et al., 1999).
Verb classes.
After exploring the training data we manually compiled two classes of verbs, each containing 15-20 inflected and uninflected lexemes, talkVerbs and actionVerbs.
talkVerbs include verbs such as {report, mention, accuse} and actionVerbs refer to verbs such as {provoke, spend, use}.
Both lists also contain the WordNet synonyms of each word in the list (Fellbaum, 1998).
Proper nouns.
Proper nouns and other parts of speech were identified running Charniak’s parser (Charniak, 2000) on the news articles.
The overall score of a sentence is computed as the weighted linear combination of the sentence and word scores.
The score σ(s) of sentence s is defined as follows: Each of the sentences s in the article was tested against the position feature ppos(s) and against each of the sentence features fk, see Section 3.2.1, where pos(s) returns the position of sentence s.
Each word j of sentence s is tested against all applicable word features gjk, see Section 3.2.2.
A weight (wpos and wk) is associated with each feature.
How to estimate the weights is discussed next.
There are various optimization methods that allow one to estimate the weights of features, including generalized iterative scaling and quasi-Newton methods (Malouf, 2002).
We opted for generalized iterative scaling as it is commonly used for other NLP tasks and off-the-shelf implementations exist.
Here we used YASMET.3 We used a development set of 240 news articles to train YASMET.
As YASMET is a supervised optimizer, we had to generate annotated data on which it was to be trained.
For each document in the development set, we labeled each sentence as to whether a story highlight was generated from it.
For instance, in the article presented in Figure 3, sentences 5, 6, 7, 9 and 21 were marked as highlight sources, whereas all other sentences in the document were not.4 When annotating, all sentences that were directly relevant to the highlights were marked, with preference given to those appearing earlier in the story or containing more precise information.
At this point it is worth noting that while the overlap between different editors is unknown, the highlights were originally written by a number of different people, ensuring enough variation in the data and helping to avoid over-fitting to a specific editor.
<newSection> 4 Experiments and Results The CNN corpus was divided into a training set and a development and test set.
As we had only 300 manually annotated news articles and we wanted to maximize the number of documents usable for parameter estimation, we applied cross-folding, which is commonly used for situations with limited data.
The dev/test set was randomly partitioned into five folds.
Four of the five folds were used as development data (i.e. for parameter estimation with YASMET), while the remaining fold was used for testing.
The procedure was repeated five times, each time with four folds used for development and a separate one for testing.
Cross-folding is safe to use as long as there are no dependencies between the folds, which is safe to assume here.
Some statistics on our training and development/test data can be found in Table 4.
Most summarization evaluation campaigns, such as NIST’s Document Understanding Conferences (DUC), impose a maximum length on summaries (e.g., 75 characters for the headline generation task or 100 words for the summarization task).
When identifying sentences from which story highlights are generated, the situation is slightly different, as the number of story highlights is not fixed.
On the other hand, most stories have between three and four highlights, and on average between four and five sentences per story from which the highlights were generated.
This variation led to us to carry out two sets of experiments: In the first experiment (fixed), the number of highlight sources is fixed and our system always returns exactly four highlight sources.
In the second experiment (thresh), our system can return between three and six highlight sources, depending on whether a sentence score passes a given threshold.
The threshold 0 was used to allocate sentences si of article a to the highlight list HL by first finding the highest-scoring sentence for that article a(sh).
The threshold score was thus 0 * a(sh) and sentences were judged accordingly.
The algorithm used is given in Figure 3.
All scores were compared to a baseline, which simply returns the first n sentences of a news article.
n = 4 in the fixed experiment.
For the thresh experiment, the baseline always selected the same number of sentences as AURUM-thresh, but from the beginning of the article.
Although this is a very simple baseline, it is worth reiterating that it is also a very competitive baseline, which most single-document summarization systems fail to beat due to the nature of news articles.
Since we are mainly interested in determining to what extent our system is able to correctly identify the highlight sources, we chose precision and recall as evaluation metrics.
Precision is the percentage of all returned highlight sources which are correct: |R| where R is the set of returned highlight sources and T is the set of manually identified true sources in the test set.
Recall is defined as the percentage of all true highlight sources that have been correctly identified by the system: Precision and recall can be combined by using the F-measure, which is the harmonic mean of the two: Table 5 shows the results for both experiments (fixed and thresh) as an average over the folds.
To determine whether the observed differences between two approaches are statistically significant and not just caused by chance, we applied statistical significance testing.
As we did not want to make the assumption that the score differences are normally distributed, we used the bootstrap method, a powerful non-parametric inference test (Efron, 1979).
Improvements at a confidence level of more than 95% are marked with “*”.
We can see that our approach consistently outperforms the baseline, and most of the improvements—in particular the F-measure scores—are statistically significant at the 0.95 level.
As to be expected, AURUM-fixed achieves higher precision gains, while AURUM-thresh achieves higher recall gains.
In addition, for 83.3 percent of the documents, our system’s F-measure score is higher than or equal to that of the baseline.
Figure 4 shows how far down in the documents our system was able to correctly identify highlight sources.
Although the distribution is still heavily skewed towards extracting sentences from the beginning of the document, it is so to a lesser extent than just using positional information as a prior; see Figure 2.
In a third set of experiments we measured the n-gram overlap between the sentences we have identified as highlight sources and the actual story highlights in the ground truth.
To this end we use ROUGE (Lin, 2004), a recall-oriented evaluation package for automatic summarization.
ROUGE operates essentially by comparing n-gram co-occurrences between a candidate summary and a number of reference summaries, and comparing that number in turn to the total number of n-grams in the reference summaries: Where n is the length of the n-gram, with lengths of 1 and 2 words most commonly used in current evaluations.
ROUGE has become the standard tool for evaluating automatic summaries, though it is not the optimal system for this experiment.
This is due to the fact that it is geared towards a different task—as ours is not automatic summarization per se—and that ROUGE works best judging between a number of candidate and model summaries.
The ROUGE scores are shown in Table 6.
Similar to the precision and recall scores, our approach consistently outperforms the baseline, with all but one difference being statistically significant.
Furthermore, in 76.2 percent of the documents, our system’s ROUGE-1 score is higher than or equal to that of the baseline, and likewise for 85.2 percent of ROUGE-2 scores.
Our ROUGE scores and their improvements over the baseline are comparable to the results of Svore et al.
(2007), who optimized their approach towards ROUGE and gained significant improvements from using third-party data resources, both of which our approach does not require.5 Table 7 shows the unique sentences extracted by every system, which are the number of sentences one system extracted correctly while the other did not; this is thus an intuitive measure of how much two systems differ.
Essentially, a system could simply pick the first two sentences of each article and might thus achieve higher precision scores, since it is less likely to return ‘wrong’ sentences.
However, if the scores are similar but there is a difference in the number of unique sentences extracted, this means a system has gone beyond the first 4 sentences and extracted others from deeper down inside the text.
To get a better understanding of the importance of the individual features we examined the weights as determined by YASMET.
Table 8 contains example output from the development sets, with feature selection determined implicitly by the weights the MaxEnt model assigns, where non-discriminative features receive a low weight.
Clearly, sentence position is of highest importance, while trigram ‘trigger’ phrases were quite important as well.
Simple bigrams continued to be a good indicator of data value, as is often the case.
Proper nouns proved to be a valuable pointer to new information, but mention of the news agency’s name had less of an impact than originally thought.
Other particularly significant features included temporal adjectives, superlatives and all n-gram measures.
<newSection> 5 Conclusions A system for extracting essential facts from a news article has been outlined here.
Finding the data nuggets deeper down is a cross between keyphrase extraction and automatic summarization, a task which requires more elaborate features and parameters.
Our approach emphasizes a wide variety of features, including many linguistic features.
These features range from the standard (n-gram frequency), through the essential (sentence position), to the semantic (spawned phrases, verb classes and types of adverbs).
Our experimental results show that a combination of statistical and linguistic features can lead to competitive performance.
Our approach not only outperformed a notoriously difficult baseline but also achieved similar performance to the approach of (Svore et al., 2007), without requiring their third-party data resources.
On top of the statistically significant improvements of our approach over the baseline, we see value in the fact that it does not settle for sentences from the beginning of the articles.
Most single-document automatic summarization systems use other features, ranging from discourse structure to lexical chains.
Considering Marcu’s conclusion (2003) that different approaches should be combined in order to create a good summarization system (aided by machine learning), there seems to be room yet to use basic linguistic cues.
Seeing as how our linguistic features—which are predominantly semantic— aid in this task, it is quite possible that further integration will aid in both automatic summarization and keyphrase extraction tasks.
<newSection> References<newSection> Abstract Large scale annotated corpora are prerequisite to developing high-performance semantic role labeling systems.
Unfortunately, such corpora are expensive to produce, limited in size, and may not be representative.
Our work aims to reduce the annotation effort involved in creating resources for semantic role labeling via semi-supervised learning.
Our algorithm augments a small number of manually labeled instances with unlabeled examples whose roles are inferred automatically via annotation projection.
We formulate the projection task as a generaliza-tion of the linear assignment problem.
We seek to find a role assignment in the un-labeled data such that the argument similarity between the labeled and unlabeled instances is maximized.
Experimental results on semantic role labeling show that the automatic annotations produced by our method improve performance over using hand-labeled instances alone.
<newSection> 1 Introduction Recent years have seen a growing interest in the task of automatically identifying and labeling the semantic roles conveyed by sentential constituents (Gildea and Jurafsky, 2002).
This is partly due to its relevance for applications ranging from information extraction (Surdeanu et al., 2003; Mos-chitti et al., 2003) to question answering (Shen and Lapata, 2007), paraphrase identification (Pad´o and Erk, 2005), and the modeling of textual entailment relations (Tatu and Moldovan, 2005).
Resources like FrameNet (Fillmore et al., 2003) and PropBank (Palmer et al., 2005) have also facilitated the development of semantic role labeling methods by providing high-quality annotations for use in training.
Semantic role labelers are commonly developed using a supervised learning paradigm1 where a classifier learns to predict role labels based on features extracted from annotated training data.
Examples of the annotations provided in FrameNet are given in (1).
Here, the meaning of predicates (usually verbs, nouns, or adjectives) is conveyed by frames, schematic representations of situations.
Semantic roles (or frame elements) are defined for each frame and correspond to salient entities present in the situation evoked by the predicate (or frame evoking element).
Predicates with similar semantics instantiate the same frame and are attested with the same roles.
In our example, the frame Cause harm has three core semantic roles, Agent, Victim, and Body part and can be instantiated with verbs such as punch, crush, slap, and injure.
The frame may also be attested with non-core (peripheral) roles that are more generic and often shared across frames (see the roles Degree, Reason, and Means, in (1c) and (1d)).
The English FrameNet (version 1.3) contains 502 frames covering 5,866 lexical entries.
It also comes with a set of manually annotated example sentences, taken mostly from the British National Corpus.
These annotations are often used as training data for semantic role labeling systems.
However, the applicability of these systems is limited to those words for which labeled data exists, and their accuracy is strongly correlated with the amount of labeled data available.
Despite the substantial annotation effort involved in the creation of FrameNet (spanning approximately twelve years), the number of annotated instances varies greatly across lexical items.
For instance, FrameNet contains annotations for 2,113 verbs; of these 12.3% have five or less annotated examples.
The average number of annotations per verb is 29.2.
Labeled data is thus scarce for individual predicates within FrameNet’s target domain and would presumably be even scarcer across domains.
The problem is more severe for languages other than English, where training data on the scale of FrameNet is virtually non-existent.
Although FrameNets are being constructed for German, Spanish, and Japanese, these resources are substantially smaller than their English counterpart and of limited value for modeling purposes.
One simple solution, albeit expensive and time-consuming, is to manually create more annotations.
A better alternative may be to begin with an initial small set of labeled examples and augment it with unlabeled data sufficiently similar to the original labeled set.
Suppose we have manual annotations for sentence (1a).
We shall try and find in an unlabeled corpus other sentences that are both structurally and semantically similar.
For instance, we may think that Bill will punch me in the face and I punched her hard in the head resemble our initial sentence and are thus good examples to add to our database.
Now, in order to use these new sentences as training data we must somehow infer their semantic roles.
We can probably guess that constituents in the same syntactic position must have the same semantic role, especially if they refer to the same concept (e.g., “body parts”) and thus label in the face and in the head with the role Body part.
Analogously, Bill and I would be labeled as Agent and me and her as Victim.
In this paper we formalize the method sketched above in order to expand a small number of FrameNet-style semantic role annotations with large amounts of unlabeled data.
We adopt a learning strategy where annotations are projected from labeled onto unlabeled instances via maximizing a similarity function measuring syntactic and semantic compatibility.
We formalize the annotation projection problem as a generalization of the linear assignment problem and solve it efficiently using the simplex algorithm.
We evaluate our algorithm by comparing the performance of a semantic role labeler trained on the annotations produced by our method and on a smaller dataset consisting solely of hand-labeled instances.
Results in several experimental settings show that the automatic annotations, despite being noisy, bring significant performance improvements.
<newSection> 2 Related Work The lack of annotated data presents an obstacle to developing many natural language applications, especially when these are not in English.
It is therefore not surprising that previous efforts to reduce the need for semantic role annotation have focused primarily on non-English languages.
Annotation projection is a popular framework for transferring frame semantic annotations from one language to another by exploiting the transla-tional and structural equivalences present in parallel corpora.
The idea here is to leverage the existing English FrameNet and rely on word or constituent alignments to automatically create an annotated corpus in a new language.
Pad´o and Lapata (2006) transfer semantic role annotations from English onto German and Johansson and Nugues (2006) from English onto Swedish.
A different strategy is presented in Fung and Chen (2004), where English FrameNet entries are mapped to concepts listed in HowNet, an on-line ontology for Chinese, without consulting a parallel corpus.
Then, Chinese sentences with predicates instan-tiating these concepts are found in a monolin-gual corpus and their arguments are labeled with FrameNet roles.
Other work attempts to alleviate the data requirements for semantic role labeling either by relying on unsupervised learning or by extending existing resources through the use of unlabeled data.
Swier and Stevenson (2004) present an unsupervised method for labeling the arguments of verbs with their semantic roles.
Given a verb instance, their method first selects a frame from VerbNet, a semantic role resource akin to FrameNet and PropBank, and labels each argument slot with sets of possible roles.
The algorithm proceeds iteratively by first making initial unambiguous role assignments, and then successively updating a probability model on which future assignments are based.
Being unsupervised, their approach requires no manual effort other than creating the frame dictionary.
Unfortunately, existing resources do not have exhaustive coverage and a large number of verbs may be assigned no semantic role information since they are not in the dictionary in the first place.
Pennacchiotti et al.
(2008) address precisely this problem by augmenting FrameNet with new lexical units if they are similar to an existing frame (their notion of similarity combines distributional and WordNet-based measures).
In a similar vein, Gordon and Swanson (2007) attempt to increase the coverage of PropBank.
Their approach leverages existing annotations to handle novel verbs.
Rather than annotating new sentences that contain novel verbs, they find syntactically similar verbs and use their annotations as surrogate training data.
Our own work aims to reduce but not entirely eliminate the annotation effort involved in creating training data for semantic role labeling.
We thus assume that a small number of manual annotations is initially available.
Our algorithm augments these with unlabeled examples whose roles are inferred automatically.
We apply our method in a monolingual setting, and thus do not project annotations between languages but within the same language.
In contrast to Pennacchiotti et al.
(2008) and Gordon and Swanson (2007), we do not aim to handle novel verbs, although this would be a natural extension of our method.
Given a verb and a few labeled instances exemplifying its roles, we wish to find more instances of the same verb in an unlabeled corpus so as to improve the performance of a hypothetical semantic role labeler without having to annotate more data manually.
Although the use of semi-supervised learning is widespread in many natural language tasks, ranging from parsing to word sense disambiguation, its application to FrameNet-style semantic role labeling is, to our knowledge, novel.
<newSection> 3 Semi-Supervised Learning Method Our method assumes that we have access to a small seed corpus that has been manually annotated.
This represents a relatively typical situation where some annotation has taken place but not on a scale that is sufficient for high-performance supervised learning.
For each sentence in the seed corpus we select a number of similar sentences tic role annotations for the frame evoking element (FEE) course in the sentence We can feel the blood coursing through our veins again.
The frame is Fluidic motion, and its roles are Fluid and Path.
Directed edges (without dashes) represent dependency relations between words, edge labels denote types of grammatical relations (e.g., SUBJ, AUX). from an unlabeled expansion corpus.
These are automatically annotated by projecting relevant semantic role information from the labeled sentence.
The similarity between two sentences is opera-tionalized by measuring whether their arguments have a similar structure and whether they express related meanings.
The seed corpus is then enlarged with the k most similar unlabeled sentences to form the expanded corpus.
In what follows we describe in more detail how we measure similarity and project annotations.
Our method operates over labeled dependency graphs.
We show an example in Figure 1 for the sentence We can feel the blood coursing through our veins again.
We represent verbs (i.e., frame evoking elements) in the seed and unlabeled corpora by their predicate-argument structure.
Specifically, we record the direct dependents of the predicate course (e.g., blood or again in Figure 1) and their grammatical roles (e.g., SUBJ, MOD).
Prepositional nodes are collapsed, i.e., we record the preposition’s object and a composite grammatical role (like IOBJ THROUGH, where IOBJ stands for “preposi-tional object” and THROUGH for the preposition itself).
In addition to direct dependents, we also consider nodes coordinated with the predicate as arguments.
Finally, for each argument node we record the semantic roles it carries, if any.
All surface word forms are lemmatized.
An example of the argument structure information we obtain for the predicate course (see Figure 1) is shown in Table 1.
We obtain information about grammatical roles from the output of RASP (Briscoe et al., 2006), a broad-coverage dependency parser.
However, there is nothing inherent in our method that restricts us to this particular parser.
Any other parser with broadly similar dependency output could serve our purposes.
For each frame evoking verb in the seed corpus our method creates a labeled predicate-argument representation.
It also extracts all sentences from the unlabeled corpus containing the same verb.
Not all of these sentences will be suitable instances for adding to our training data.
For example, the same verb may evoke a different frame with different roles and argument structure.
We therefore must select sentences which resemble the seed annotations.
Our hypothesis is that verbs appearing in similar syntactic and semantic contexts will behave similarly in the way they relate to their arguments.
Estimating the similarity between two predicate argument structures amounts to finding the highest-scoring alignment between them.
More formally, given a labeled predicate-argument structure pl with m arguments and an unla-beled predicate-argument structure pu with n arguments, we consider (and score) all possible alignments between these arguments.
A (partial) alignment can be viewed as an injective function σ : Mσ —* {1,...,n} where Mσ C {1, . .
.
, m}.
In other words, an argument i of pl is aligned to argument σ(i) of pu if i E Mσ.
Note that this allows for unaligned arguments on both sides.
We score each alignment σ using a similarity where syn(gli,guσ(i)) denotes the syntactic similarity between grammatical roles gli and guσ(i) and sem(wli, wuσ(i)) the semantic similarity between head words wli and wuσ(i).
Our goal is to find an alignment such that the similarity function is maximized: problem is a generalized version of the linear assignment problem (Dantzig, 1963).
It can be straightforwardly expressed as a linear programming problem by associating each alignment σ with a set of binary indicator variables xij: subject to the following constraints ensuring that σ is an injective function on some Mσ: Figure 2 graphically illustrates the alignment projection problem.
Here, we wish to project semantic role information from the seed blood coursing through our veins again onto the un-labeled sentence Adrenalin was still coursing through her veins.
The predicate course has three arguments in the labeled sentence and four in the unlabeled sentence (represented as rectangles in the figure).
There are 73 possible alignments in this example.
In general, for any m and n arguments, where m < n, the number of alignments is Ek %m!n!
k)!k!
.
Each alignment is scored by taking the sum of the similarity scores of the individual alignment pairs (e.g., between blood and be, vein and still).
In this example, the highest scoring alignment is between blood and adrenalin, vein and vein, and again and still, whereas be is left unaligned (see the non-dotted edges in Figure 2).
Note that only vein and blood carry semantic roles (i.e., Fluid and Path) which are projected onto adrenalin and vein, respectively.
Finding the best alignment crucially depends on estimating the syntactic and semantic similarity between arguments.
We define the syntactic measure on the grammatical relations produced by RASP.
Specifically, we set syn(gli,guσ(i)) to 1 if the relations are identical, to a < 1 if the relations are of the same type but different subtype2 and to 0 otherwise.
To avoid systematic errors, syntactic similarity is also set to 0 if the predicates differ in voice.
We measure the semantic similarity sem(wli, wuσ(i)) with a semantic space model.
The meaning of each word is represented by a vector of its co-occurrences with neighboring words.
The cosine of the angle of the vectors representing wl and wu quantifies their similarity (Section 4 describes the specific model we used in our experiments in more detail).
The parameter A counterbalances the importance of syntactic and semantic information, while the parameter B can be interpreted as the lowest similarity value for which an alignment between two arguments is possible.
An optimal alignment Q* cannot link arguments i0 of pl and j0 of pu, if A · syn(gli0, guj0) + sem(wli0, wuj0) < B (i.e., either i0 V Mσ* or Q*(i0) =� j0).
This is because for an alignment Q with Q(i0) = j0 we can construct a better alignment Q0, which is identical to Q on all i =� i0, but leaves i0 unaligned (i.e., i0 V Mσ0).
By eliminating a negative term from the scoring function, it follows that sim(Q0) > sim(Q).
Therefore, an alignment Q satisfying Q(i0) = j0 cannot be optimal and conversely the optimal alignment Q* can never link two arguments with each other if the sum of their weighted syntactic and semantic similarity scores is below B.
Once we obtain the best alignment Q* between pl and pu, we can simply transfer the role of each role-bearing argument i of pl to the aligned argument Q*(i) of pu, resulting in a labeling of pu.
To increase the accuracy of our method we discard projections if they fail to transfer all roles of the labeled to the unlabeled dependency graph. through our veins again and Adrenalin was still coursing through her veins; non-dotted lines illustrate the highest scoring alignment.
This can either be the case if pl does not cover all roles annotated on the graph (i.e., there are role-bearing nodes which we do not recognize as arguments of the frame evoking verb) or if there are unaligned role-bearing arguments (i.e., i V Mσ* for a role-bearing argument i of pl).
The remaining projections form our expansion corpus.
For each seed instance we select the k most similar neighbors to add to our training data.
The parameter k controls the trade-off between annotation confidence and expansion size.
<newSection> 4 Experimental Setup In this section we discuss our experimental setup for assessing the usefulness of the method presented above.
We give details on our training procedure and parameter estimation, describe the semantic labeler we used in our experiments and explain how its output was evaluated.
Corpora Our seed corpus was taken from FrameNet.
The latter contains approximately 2,000 verb entries out of which we randomly selected a sample of 100.
We next extracted all annotated sentences for each of these verbs.
These sentences formed our gold standard corpus, 20% of which was reserved as test data.
We used the remaining 80% as seeds for training purposes.
We generated seed corpora of various sizes by randomly reducing the number of annotation instances per verb to a maximum of n.
An additional (non-overlapping) random sample of 100 verbs was used as development set for tuning the parameters for our method.
We gathered unla-beled sentences from the BNC.
The seed and unlabeled corpora were parsed with RASP (Briscoe et al., 2006).
The FrameNet annotations in the seed corpus were converted into dependency graphs (see Figure 1) using the method described in F¨urstenau (2008).
Briefly, the method works by matching nodes in the dependency graph with role bearing substrings in FrameNet.
It first finds the node in the graph which most closely matches the frame evoking element in FrameNet.
Next, individual graph nodes are compared against labeled substrings in FrameNet to transfer all roles onto their closest matching graph nodes.
Parameter Estimation The similarity function described in Section 3.2 has three free parameters.
These are the weight A which determines the relative importance of syntactic and semantic information, the parameter B which determines when two arguments cannot be aligned and the syntactic score a for almost identical grammatical roles.
We optimized these parameters on the development set using Powell’s direction set method (Brent, 1973) with F1 as our loss function.
The optimal values for A, B and a were 1.76, 0.41 and 0.67, respectively.
Our similarity function is further parametrized in using a semantic space model to compute the similarity between two words.
Considerable latitude is allowed in specifying the parameters of vector-based models.
These involve the definition of the linguistic context over which co-occurrences are collected, the number of components used (e.g., the k most frequent words in a corpus), and their values (e.g., as raw co-occurrence frequencies or ratios of probabilities).
We created a vector-based model from a lemmatized version of the BNC.
Following previous work (Bullinaria and Levy, 2007), we optimized the parameters of our model on a word-based semantic similarity task.
The task involves examining the degree of linear relationship between the human judgments for two individual words and vector-based similarity values.
We experimented with a variety of dimensions (ranging from 50 to 500,000), vector component definitions (e.g., pointwise mutual information or log likelihood ratio) and similarity measures (e.g., cosine or confusion probability).
We used WordSim353, a benchmark dataset (Finkelstein et al., 2002), consisting of relatedness judgments (on a scale of 0 to 10) for 353 word pairs.
We obtained best results with a model using a context window of five words on either side of the target word, the cosine measure, and 2,000 vector dimensions.
The latter were the most common context words (excluding a stop list of function words).
Their values were set to the ratio of the probability of the context word given the target word to the probability of the context word overall.
This configuration gave high correlations with the WordSim353 similarity judgments using the cosine measure.
Solving the Linear Program A variety of algorithms have been developed for solving the linear assignment problem efficiently.
In our study, we used the simplex algorithm (Dantzig, 1963).
We generate and solve an LP of every unlabeled sentence we wish to annotate.
Semantic role labeler We evaluated our method on a semantic role labeling task.
Specifically, we compared the performance of a generic semantic role labeler trained on the seed corpus and a larger corpus expanded with annotations produced by our method.
Our semantic role labeler followed closely the implementation of Johans-son and Nugues (2008).
We extracted features from dependency parses corresponding to those routinely used in the semantic role labeling literature (see Baker et al. (2007) for an overview).
SVM classifiers were trained to identify the arguments and label them with appropriate roles.
For the latter we performed multi-class classification following the one-versus-one method3 (Friedman, 1996).
For the experiments reported in this paper we used the LIBLINEAR library (Fan et al., 2008).
The misclassification penalty C was set to 0.1.
To evaluate against the test set, we linearized the resulting dependency graphs in order to obtain labeled role bracketings like those in example (1) and measured labeled precision, labeled recall and labeled F1.
(Since our focus is on role labeling and not frame prediction, we let our role labeler make use of gold standard frame annotations, i.e., labeling of frame evoking elements with frame names.)
<newSection> 5 Results The evaluation of our method was motivated by three questions: (1) How do different training set sizes affect semantic role labeling performance?
Training size varies depending on the number of unlabeled sentences added to the seed corpus.
The quality of these sentences also varies depending on their similarity to the seed sentences.
So, we would like to assess whether there is a tradeoff between annotation quality and training size.
(2) How does the size of the seed corpus influence role labeling performance?
Here, we are interested to find out what is the least amount of manual annotation possible for our method to have some positive impact.
(3) And finally, what are the annotation savings our method brings?
Table 2 shows the performance of our semantic role labeler when trained on corpora of different sizes.
The seed corpus was reduced to at most 10 instances per verb.
Each row in the table corresponds to adding the k nearest neighbors of these instances to the training data.
When trained solely on the seed corpus the semantic role labeler yields a (labeled) F1 of 38.5%, (labeled) recall is 42.0% and (labeled) precision is 35.5% (see row 0-NN in the table).
All subsequent expansions yield improved precision and recall.
In all cases except k = 1 the improvement is statistically significant (p < 0.05).
We performed significance testing on F1 using stratified shuffling (Noreen, 1989), an instance of assumption-free approximative ran-domization testing.
As can be seen, the optimal trade-off between the size of the training corpus and annotation quality is reached with two nearest neighbors.
This corresponds roughly to doubling the number of training instances.
(Due to the restrictions mentioned in Section 3.3 a 2-NN expansion does not triple the number of instances.)
We also compared our results against a self-training procedure (see last row in Table 2).
Here, we randomly selected unlabeled sentences corresponding in number to a 2-NN expansion, labeled them with our role labeler, added them to the training set, and retrained.
Self-training resulted in performance inferior to the baseline of adding no un-labeled data at all (see the first row in Table 2).
Performance decreased even more with the addition of more self-labeled instances.
These results indicate that the similarity function is crucial to the success of our method.
An example of the annotations our method produces is given below.
Sentence (2a) is the seed.
Sentences (2b)–(2e) are its most similar neighbors.
The sentences are presented in decreasing order of similarity.
As we can see, sentences (2b) and (2c) accurately identify the semantic roles of the verb come evoking the frame Arriving.
In (2b) He is labeled as Theme, rapidly as Manner, and towards the house as Goal.
Analogously, in (2c) she is the Theme, slowly is Manner and down the stairs is Goal.
The quality of the annotations decreases with less similar instances.
In (2d) then is marked erroneously as Manner, whereas in (2e) only the Theme role is identified correctly.
To answer our second question, we varied the size of the training corpus by varying the number of seeds per verb.
For these experiments we fixed k = 2.
Table 3 shows the performance of the semantic role labeler when the seed corpus has one annotation per verb, five annotations per verb, and so on.
(The results for 10 annotations are repeated from Table 2).
With 1, 5 or 10 instances per verb our method significantly improves labeling performance.
We observe improvements in F1 of 1.5%, 2.1%, and 2.4% respectively when adding the 2 most similar neighbors to these training corpora.
Our method also improves F1 when a 20 seeds ing different numbers of seed instances per verb in the training corpus; the seeds are expanded with their k = 2 nearest neighbors; *: F1 is significantly different from seed corpus (p < 0.05).
corpus or all available seeds are used, however the difference is not statistically significant.
The results in Table 3 also allow us to draw some conclusions regarding the relative quality of manual and automatic annotation.
Expanding a seed corpus with 10 instances per verb improves F1 from 38.5% to 40.9%.
We can compare this to the labeler’s performance when trained solely on the 20 seeds corpus (without any expansion).
The latter has approximately the same size as the expanded 10 seeds corpus.
Interestingly, F1 on this exclusively hand-annotated corpus is only 1.2% better than on the expanded corpus.
So, using our expansion method on a 10 seeds corpus performs almost as well as using twice as many manual annotations.
Even in the case of the 5 seeds corpus, where there is limited information for our method to expand from, we achieve an improvement from 33.5% to 35.6%, compared to 38.5% for manual annotation of about the same number of instances.
In sum, while additional manual annotation is naturally more effective for improving the quality of the training data, we can achieve substantial proportions of these improvements by automatic expansion alone.
This is a promising result suggesting that it is possible to reduce annotation costs without drastically sacrificing quality.
<newSection> 6 Conclusions This paper presents a novel method for reducing the annotation effort involved in creating resources for semantic role labeling.
Our strategy is to expand a manually annotated corpus by projecting semantic role information from labeled onto un-labeled instances.
We formulate the projection problem as an instance of the linear assignment problem.
We seek to find role assignments that maximize the similarity between labeled and un-labeled instances.
Similarity is measured in terms of structural and semantic compatibility between argument structures.
Our method improves semantic role labeling performance in several experimental conditions.
It is especially effective when a small number of annotations is available for each verb.
This is typically the case when creating frame semantic corpora for new languages or new domains.
Our experiments show that expanding such corpora with our method can yield almost the same relative improvement as using exclusively manual annotation.
In the future we plan to extend our method in order to handle novel verbs that are not attested in the seed corpus.
Another direction concerns the systematic modeling of diathesis alternations (Levin, 1993).
These are currently only captured implicitly by our method (when the semantic similarity overrides syntactic dissimilar-ity).
Ideally, we would like to be able to systematically identify changes in the realization of the argument structure of a given predicate.
Although our study focused solely on FrameNet annotations, we believe it can be adapted to related annotation schemes, such as PropBank.
An interesting question is whether the improvements obtained by our method carry over to other role labeling frameworks.
Acknowledgments The authors acknowledge the support of DFG (IRTG 715) and EPSRC (grant GR/T04540/01).
We are grateful to Richard Johansson for his help with the re-implementation of his semantic role labeler.
<newSection> References<newSection> Abstract In this paper, we present an efficient query selection algorithm for the retrieval of web text data to augment a statistical language model (LM).
The number of retrieved relevant documents is optimized with respect to the number of queries submitted.
The querying scheme is applied in the domain of SMS text messages.
Continuous speech recognition experiments are conducted on three languages: English, Spanish, and French.
The web data is utilized for augmenting in-domain LMs in general and for adapting the LMs to a user-specific vocabulary.
Word error rate reductions of up to 6.6 % (in LM augmentation) and 26.0 % (in LM adaptation) are obtained in setups, where the size of the web mixture LM is limited to the size of the baseline in-domain LM.
<newSection> 1 Introduction An automatic speech recognition (ASR) system consists of acoustic models of speech sounds and of a statistical language model (LM).
The LM learns the probabilities of word sequences from text corpora available for training.
The performance of the model depends on the amount and style of the text.
The more text there is, the better the model is, in general.
It is also important that the model be trained on text that matches the style of language used in the ASR application.
Well matching, in-domain, text may be both difficult and expensive to obtain in the large quantities that are needed.
A popular solution is to utilize the World Wide Web as a source of additional text for LM training.
A small in-domain set is used as seed data, and more data of the same kind is retrieved from the web.
A decade ago, Berger and Miller (1998) proposed a just-in-time LM that updated the current LM by retrieving data from the web using recent recognition hypotheses as queries submitted to a search engine.
Perplexity reductions of up to 10 % were reported.1 Many other works have followed.
Zhu and Rosenfeld (2001) retrieved page and phrase counts from the web in order to update the probabilities of infrequent trigrams that occur in N-best lists.
Word error rate (WER) reductions of about 3 % were obtained on TREC-7 data.
In more recent work, the focus has turned to the collection of text rather than n-gram statistics based on page counts.
More effort has been put into the selection of query strings.
Bulyko et al.
(2003; 2007) first extend their baseline vocabulary with words from a small in-domain training corpus.
They then use n-grams with these new words in their web queries in order to retrieve text of a certain genre.
For instance, they succeed in obtaining conversational style phrases, such as “we were friends but we don’t actually have a relationship.”
In a number of experiments, word error rate reductions of 2-3 % are obtained on English data, and 6 % on Mandarin.
The same method for web data collection is applied by C¸ etin and Stolcke (2005) in meeting and lecture transcription tasks.
The web sources reduce perplexity by 10 % and 4.3 %, respectively, and word error rates by 3.5 % and 2.2 %, respectively.
Sarikaya et al.
(2005) chunk the in-domain text into “n-gram islands” consisting of only content words and excluding frequently occurring stop words.
An island such as “stock fund portfolio” is then extended by adding context, producing “my stock fund portfolio”, for instance.
Multiple islands are combined using and and or operations to form web queries.
Significant word error reductions between 10 and 20 % are obtained; however, the in-domain data set is very small, 1700 phrases, which makes (any) new data a much needed addition.
Similarly, Misu and Kawahara (2006) obtain very good word error reductions (20 %) in spoken dialogue systems for software support and sightseeing guidance.
Nouns that have high tf/idf scores in the in-domain documents are used in the web queries.
The existing in-domain data sets poorly match the speaking style of the task and therefore existing dialogue corpora of different domains are included, which improves the performance considerably.
Wan and Hain (2006) select query strings by comparing the n-gram counts within an in-domain topic model to the corresponding counts in an out-of-domain background model.
Topic-specific n-grams are used as queries, and perplexity reductions of 5.4 % are obtained.
It is customary to postprocess and filter the downloaded web texts.
Sentence boundaries are detected using some heuristics.
Text chunks with a high out-of-vocabulary (OOV) rate are discarded.
Additionally, the chunks are often ranked according to their similarity with the in-domain data, and the lowest ranked chunks are discarded.
As a similarity measure, the perplexity of the sentence according to the in-domain LM can be used; for instance, Bulyko et al.
(2007).
Another measure for ranking is relative perplexity (Weilhammer et al., 2006), where the in-domain perplexity is divided by the perplexity given by an LM trained on the web data.
Also the BLEU score familiar from the field of machine translation has been used (Sarikaya et al., 2005).
Some criticism has been raised by Sethy et al.
(2007), who claim that sentence ranking has an inherent bias towards the center of the in-domain distribution.
They propose a data selection algorithm that selects a sentence from the web set, if adding the sentence to the already selected set reduces the relative entropy with respect to the in-domain data distribution.
The algorithm appears efficient in producing a rather small subset (1/11) of the web data, while degrading the WER only marginally.
The current paper describes a new method for query selection and its applications in LM augmentation and adaptation using web data.
The language models are part of a continuous speech recognition system that enables users to use speech as an input modality on mobile devices, such as mobile phones.
The particular domain of interest is personal communication: The user dictates a message that is automatically transcribed into text and sent to a recipient as an SMS text message.
Memory consumption and computational speed are crucial factors in mobile applications.
While most studies ignore the sizes of the LMs when comparing models, we aim at improving the LM without increasing its size when web data is added.
Another aspect that is typically overlooked is that the collection of web data costs time and computational resources.
This applies to the querying, downloading and postprocessing of the data.
The query selection scheme proposed in this paper is economical in the sense that it strives to download as much relevant text from the web as possible using as few queries as possible avoiding overlap between the set of pages found by different queries.
<newSection> 2 Query selection and web data retrieval Our query selection scheme involves multiple steps.
The assumption is that a batch of queries will be created.
These queries are submitted to a search engine and the matching documents are downloaded.
This procedure is repeated for multiple query batches.
In particular, our scheme attempts to maximize the number of retrieved relevant documents, when two restrictions apply: (1) queries are not “free”: each query costs some time or money; for instance, the number of queries submitted within a particular period of time is limited, and (2) the number of documents retrieved for a particular query is limited to a particular number of “top hits”.
Some text reflecting the target domain must be available.
A set of the most frequent n-grams occurring in the text is selected, from unigrams up to five-grams.
Some of these n-grams are characteristic of the domain of interest (such as “Hogwarts School of Witchcraft and Wizardry”), others are just frequent in general (“but they did not say”); we do not know yet which ones.
All n-grams are submitted as queries to the web search engine.
Exact matches of the n-grams are required; different inflections or matches of the words individually are not accepted.
The search engine returns the total number of hits h(qs) for each query qs as well as the URLs of a predefined maximum number of “top hit” web pages.
The top hit pages are downloaded and post-processed into plain text, from which duplicate paragraphs and paragraphs with a high OOV rate are removed.
N-gram language models are then trained separately on the in-domain text and the the filtered web text.
If the amount of web text is very large, only a subset is used, which consists of the parts of the web data that are the most similar to the in-domain text.
As a similarity measure, relative perplexity is used.
The LM trained on web data is called a background LM to distinguish it from the in-domain LM.
Next, the querying is made more specific and targeted on the domain of interest.
New queries are created that consist of n-gram pairs, requiring that a document contain two n-grams (“but they did not say”+“Hogwarts School of Witchcraft and Wizardry”).2 If all possible n-gram pairs are formed from the n-grams selected in Section 2.1, the number of pairs is very large, and we cannot afford using them all as queries.
Typical approaches for query selection include the following: (i) select pairs that include n-grams that are relatively more frequent in the in-domain text than in the background text, (ii) use some extra source of knowledge for selecting the best pairs.
We first tested the second (ii) query selection approach by incorporating some simple linguistic knowledge: In an experiment on English, queries were obtained by combining a highly frequent n-gram with a slightly less frequent n-gram that had to contain a first- or second-person pronoun (I, you, we, me, us, my, your, our).
Such n-grams were thought to capture direct speech, which is characteristic for the desired genre of personal communication.
(Similar techniques are reported in the literature cited in Section 1.)
Although successful for English, this scheme is more difficult to apply to other languages, where person is conveyed as verbal suffixes rather than single words.
Linguistic knowledge is needed for 2Higher order tuples could be used as well, but we have only tested n-gram pairs.
every language, and it turns out that many of the queries are “wasted”, because they are too specific and return only few (if any) documents.
The other proposed query selection technique (i) allows for an automatic identification of the n-grams that are characteristic of the in-domain genre.
If the relative frequency of an n-gram is higher in the in-domain data than in the background data, then the n-gram is potentially valuable.
However, as in the linguistic approach, there is no guarantee that queries are not wasted, since the identified n-gram may be very rare on the Internet.
Pairing it with some other n-gram (which may also be rare) often results in very few hits.
To get out the most of the queries, we propose a query selection algorithm that attempts to optimize the relevance of the query to the target domain, but also takes into account the expected amount of data retrieved by the query.
Thus, the potential queries are ranked according to the expected number of retrieved relevant documents.
Only the highest ranked pairs, which are likely to produce the highest number of relevant web pages, are used as queries.
We denote queries that consist of two n-grams s and t by qs∧t.
The expected number of retrieved relevant documents for the query qs∧t is r(qs∧t): where n(qs∧t) is the expected number of retrieved documents for the query, and p(qs∧t |Q) is the expected proportion of relevant documents within all documents retrieved by the query.
The expected proportion of relevant documents is a value between zero and one, and as explained below, it is dependent on all past queries, the query history Q.
Expected number of retrieved documents n(qs∧t).
From the prospection querying phase (Section 2.1), we know the numbers of hits for the single n-grams s and t, separately: h(qs) and h(qt).
We make the operational, but overly simplifying, assumption that the n-grams occur evenly distributed over the web collection, independently of each other.
The expected size of the intersection qs∧t is then: where N is the size of the web collection that our n-gram selection covers (total number of documents).
N is not known, but different estimates can be used, for instance, N = maxvqs h(qs), where it is assumed that the most frequent n-gram occurs in every document in the collection (prob-ably an underestimate of the actual value).
Ideally, the expected number of retrieved documents equals the expected number of hits, but since the search engine returns a limited maximum number of “top hit” pages, M, we get: Expected proportion of relevant documents ρ(qsnt |Q).
As in the case of n(qsnt), an independence assumption can be applied in the derivation of the expected proportion of relevant documents for the combined query qsnt: We simply put together the chances of obtaining relevant documents by the single n-gram queries qs and qt individually.
The union equals: However, we do not know the values for ρ(qs  |Q) and ρ(qt  |Q).
As mentioned earlier, it is straightforward to obtain a relevance ranking for a set of n-grams: For each n-gram s, the LM probability is computed using both the in-domain and the background LM.
The in-domain probability is divided by the background probability and the n-grams are sorted, highest relative probability first.
The first n-gram is much more prominent in the in-domain than the background data, and we wish to obtain more text with this crucial n-gram.
The opposite is true for the last n-gram.
We need to transform the ranking into ρ(·) values between zero and one.
There is no absolute division into relevant and irrelevant documents from the point of view of LM training.
We use a probabilistic query ranking scheme, such that we define that of all documents containing an x % relevant n-gram, x % are relevant.
When the n-grams have been ranked into a presumed order of relevance, we decide that the most relevant n-gram is 100 % relevant and the least relevant n-gram is 0 % relevant; finally, we scale the relevances of the other n-grams according to rank.
When scoring the remaining n-grams, linear scaling is avoided, because the majority of the n-grams are irrelevant or neutral with respect to our domain of interest, and many of them would obtain fairly high relevance values.
Instead, we fix the relevance value of the “most domain-neutral” n-gram (the one with the relative probability value closest to one); we might assume that only 5 % of all documents containing this n-gram are indeed relevant.
We then fit a polynomial curve through the three points with known values (0, 0.05, and 1) to get the missing ρ(·) values for all qs.
Decay factor δ(s |Q).
We noticed that if constant relevance values are used, the top ranked queries will consist of a rather small set of top ranked n-grams that are paired with each other in all possible combinations.
However, it is likely that each time an n-gram is used in a query, the need for finding more occurrences of this particular n-gram decreases.
Therefore, we introduced a decay factor δ(s  |Q), by which the initial ρ(·) value, written ρ0(qs), is multiplied: E is a small value between zero and one (for instance 0.05), and EvscQ 1 is the number of times the n-gram s has occurred in past queries.
Overlap with previous queries.
Some queries are likely to retrieve the same set of documents as other queries.
This occurs if two queries share one n-gram and there is strong correlation between the second n-grams (for instance, “we wish you”+“Merry Christmas” vs. “we wish you”+ “and a Happy New Year”).
In principle, when assessing the relevance of a query, one should estimate the overlap of that query with all past queries.
We have tested an approximate solution that allows for fast computing.
However, the real effect of this addition was insignificant, and a further description is omitted in this paper.
Optimal order of the queries.
We want to maximize the expected number of retrieved relevant documents while keeping the number of submitted queries as low as possible.
Therefore we sort the queries best first and submit as many queries we can afford from the top of the list.
However, the relevance of a query is dependent on the sequence of past queries (because of the decay factor).
Finding the optimal order of the queries takes O(n2) operations, if n is the total number of queries.
A faster solution is to apply an iterative algorithm: All queries are put in some initial order.
For each query, its r(qs∧t) value is computed according to Equation 1.
The queries are then rearranged into the order defined by the new r(·) values, best first.
These two steps are repeated until convergence.
Repeated focused querying.
Focused querying can be run multiple times.
Some ten thousands of the top ranked queries are submitted to the search engine and the documents matching the queries are downloaded.
A new background LM is trained using the new web data, and a new round of focused querying can take place.
On one language (German), the statical focused querying algorithm (Section 2.2.2) was shown to retrieve 50 % more unique web pages and 70 % more words than the linguistic scheme (Sec-tion 2.2.1) for the same number of queries.
Also results from language modeling and speech recognition experiments favored statistical querying.
For the speech recognition experiments described in the current paper, we have collected web texts for three languages: US English, European Spanish, and Canadian French.
As in-domain data we used 230,000 English text messages (4 million words), 65,000 Spanish messages (2 million words), and 60,000 French messages (1 million words).
These text messages were obtained in data collection projects involving thousand of participants, who used a web interface to enter messages according to different scenarios of personal communication situations.3 A few example messages are shown in Figure 1.
The queries were submitted to Yahoo!’s web search engine.
The web pages that were retrieved by the queries were filtered and cleaned and divided into chunks consisting of single paragraphs.
For English, we obtained 210 million paragraphs and 13 billion words, for Spanish 160 million paragraphs and 12 billion words, and for French 44 million paragraphs and 3 billion words.
3Real messages sent from mobile phones would be the best data, but are hard to get because of privacy protection.
The postprocessing of authentic messages would, however, require proper handling of artifacts resulting from the limited input capacities on keypads of mobile devices, such as specific acronyms: i’ll c u l8er.
In our setup, we did not have to face such issues.
I hope you have a long and happy marriage.
Congratulations!
Remember to pick up Billy at practice at five o’clock!
Hey Eric, how was the trip with the kids over winter vacation?
Did you go to Texas?
The linguistic focused querying method was applied in the US English task (because the statistical method did not yet exist).
The Spanish and Canadian French web collections were obtained using statistical querying.
Since the French set was smaller than the other sets (“only” 3 billion words), web crawling was performed, such that those web sites that had provided us with the most valuable data (measured by relative perplexity) were downloaded entirely.
As a result, the number of paragraphs increased to 110 million and the number of words to 8 billion.
<newSection> 3 Speech Recognition Experiments We have trained language models on the in-domain data together with web data, and these models have been used in speech recognition experiments.
Two kinds of experiments have been performed: (1) the in-domain LM is augmented with web data, and (2) the LM is adapted to a user-specific vocabulary utilizing web data as an additional data source.
One hundred native speakers for each language were recorded reading held-out subsets of the in-domain text data.
The speech data was partitioned into training and test sets, such that around one fourth of the speakers were reserved for testing.
We use a continuous speech recognizer optimized for low memory footprint and fast recognition (Olsen et al., 2008).
The recognizer runs on a server (Core2 2.33 GHz) in about one fourth of real time.
The LM probabilities are quantized and precompiled together with the speaker-independent acoustic models (intra-word triphones) into a finite state transducer (FST).
Each paragraph in the web data is treated as a potential text message and scored according to its similarity to the in-domain data.
Relative perplexity is used as the similarity measure.
The paragraphs are sorted, lowest relative perplexity first, In the tables, the perplexity and word error rate reductions of the web mixtures are computed with respect to the in-domain models of the same size, if such models exist; otherwise the comparison is made to the largest in-domain model available. and the highest ranked paragraphs are used as LM training data.
The optimal size of the set depends on the test, but the largest chosen set contains 15 million paragraphs and 500 million words.
Separate LMs are trained on the in-domain data and web data.
The two LMs are then linearly interpolated into a mixture model.
Roughly the same interpolation weights (0.5) are obtained for the LMs, when the optimal value is chosen based on a held-out in-domain development test set.
In Table 1, the prediction abilities of the in-domain and web mixture language models are compared.
As an evaluation measure we use perplexity calculated on test sets consisting of in-domain text.
The comparison is performed on FSTs of different sizes.
The FSTs contain the acoustic models, language model and lexicon, but the LM makes up for most of the size.
The availability of data varies for the different languages, and therefore the FST sizes are not exactly the same across languages.
The LMs have been created using the SRI LM toolkit (Stolcke, 2002).
Good-Turing smoothing with Katz backoff (Katz, 1987) has been used, and the different model sizes are obtained by pruning down the full models using entropy-based pruning (Stolcke, 1998).
N-gram orders up to five have been tested: 5-grams always work best on the mixture models, whereas the best in-domain models are 4- or 5-grams.
For every language and model size, the web mixture model performs better than the corresponding in-domain model.
The perplexity reductions obtained increase with the size of the model.
Since it is possible to create larger mixture models than in-domain models, there are no in-domain results for the largest model sizes.
Especially if large models can be afforded, the perplexity reductions are considerable.
The largest improvements are observed for French (between 10.2 % and 22.6 % relative).
This is not surprising, as the French in-domain set is the smallest, which leaves much room for improvement.
Speech recognition results for the different LMs are given in Table 2.
The results are consistent in the sense that the web mixture models outperform the in-domain models, and augmentation helps more with larger models.
The largest word error rate reduction is observed for the largest Spanish model (9.7 % relative).
All WER reductions are statistically significant (one-sided Wilcoxon signed-rank test; level 0.05) except the 10 MB Spanish setup.
Although the observed word error rate reductions are mostly smaller than the corresponding perplexity reductions, the results are actually very good, when we consider the fact that considerable reductions in perplexity may typically translate into meager word error reductions; see, for instance, Rosenfeld (2000), Goodman (2001).
This suggests that the web texts are very welcome com-plementary data that improve on the robustness of the recognition.
In the above experiments, Good-Turing (GT) smoothing with Katz backoff was used, although modified Kneser-Ney (KN) interpolation has been shown to outperform other smoothing methods (Chen and Goodman, 1999).
However, as demonstrated by Siivola et al.
(2007), KN smoothing is not compatible with simple pruning methods such as entropy-based pruning.
In order to make a meaningful comparison, we used the revised Kneser pruning and Kneser-Ney growing techniques proposed by Siivola et al.
(2007).
For the three languages, we built KN models that resulted in FSTs of the same sizes as the largest GT in-domain models.
The perplexities decreased 4–8%, but in speech recognition, the improvements were mostly negligible: the error rates were 17.0 for English, 18.7 for Spanish, and 22.5 for French.
For English, we also created web mixture models with KN smoothing.
The error rates were 16.5, 15.9 and 15.7 for the 20 MB, 40 MB and 70 MB models, respectively.
Thus, Kneser-Ney outperformed Good-Turing, but the improvements were small, and a statistically significant difference was measured only for the 40 MB LMs.
This was expected, as it has been observed before that very simple smoothing techniques can perform well on large data sets, such as web data (Brants et al., 2007).
For the purpose of demonstrating the usefulness of our web data retrieval system, we concluded that there was no significant difference between GT and KN smoothing in our current setup.
In the second set of experiments we envisage a system that adapts to the user’s own vocabulary.
Some words that the user needs may not be included in the built-in vocabulary of the device, such as names in the user’s contact list, names of places or words related to some specific hobby or other focus of interest.
Two adaptation techniques have been tested: In our message adaptation, we have not created web queries dynamically on demand.
Instead, we used the large web collections described in Section 2.3, from which we selected paragraphs containing user-specific words.
We have tested both adaptation by pooling (adding the paragraphs to the original training data), and adaptation by in-terpolation (using the new data to train a separate LM, which is interpolated with the original LM).
One million words from the web data were selected for each language.
The adaptation was thought to take place off-line on a server.
For each language, the adaptation takes place on two baseline models, which are the in-domain and web mixture LMs of Section 3.1; however, the amount of in-domain training data is reduced slightly (as explained below).
In order to evaluate the success of the adaptation, a simulated user-specific test set is created.
This set is obtained by selecting a subset of a larger potential test set.
Words that occur both in the training set and the potential test set and that are infrequent in the training set are chosen as the user-specific vocabulary.
For Spanish and French, a training set frequency threshold of one is used, resulting in 606 and 275 user-specific words, respectively.
For English the threshold is 5, which results in 99 words.
All messages in the potential test set containing any of these words are selected into the user-specific test set.
Any message containing user-specific words is removed from the in-domain training set.
In this manner, we obtain a test set with a certain over-representation of a specific vocabulary, without biasing the word frequency distribution of the training set to any noticeable degree.
For comparison, performance is additionally computed on a generic in-domain test set, as be-models have been evaluated on two types of test sets: a user-specific test set with a higher number of user-specific words and a generic in-domain test set.
The numbers in brackets are relative WER reductions [%] compared to the in-domain model.
WER values for the unigram adaptation are rendered in italics, if the improvement obtained is statistically significant compared to the corresponding non-adapted model.
WER values for the message adaptation are in italics, if there is a statistically significant reduction with respect to unigram adaptation. fore.
User-specific and generic development test sets are used for the estimation of optimal interpo-lation weights.
<newSection> 3.2.2 Results The adaptation experiments are summarized in Table 3.
Only medium sized FSTs (21–23 MB) have been tested.
The two baseline models have been adapted using the simple unigram reweight-ing scheme and using selective web message augmentation.
For the in-domain baseline, pooling works the best, that is, adding the web messages to the original in-domain training set.
For the web mixture baseline, a mixture model is the only option; that is, one more layer of interpolation is added.
In the adaptation of the in-domain LMs, message selection is almost twice as effective as unigram adaptation for all data sets.
Also the performance on the generic in-domain test set is slightly improved, because more training data is available.
Except for English, the best results on the user-specific test sets are produced by the adaptation of the web mixture models.
The benefit of using message adaptation instead of simple unigram adaptation is smaller when we have a web mixture model as a baseline rather than an in-domain-only LM.
On the generic test sets, the adaptation of the web mixture makes a difference only for English.
Since there were practically no singleton words in the English in-domain data, the user-specific vocabulary consists of words occurring at most five times.
Thus, the English user-specific words are more frequent than their Spanish and French equivalents, which shows in larger WER reductions for English in all types of adaptation.
<newSection> 4 Discussion and conclusion Mobile applications need to run in small memory, but not much attention is usually paid to memory consumption in related LM work.
We have shown that LM augmentation using web data can be successful, even when the resulting mixture model is not allowed to grow any larger than the initial in-domain model.
Yet, the benefit of the web data is larger, the larger model can be used.
The largest WER reductions were observed in the adaptation to a user-specific vocabulary.
This can be compared to Misu and Kawahara (2006), who obtained similar accuracy improvements with clever selection of web data, when there was initially no in-domain data available with both the correct topic and speaking style.
We used relative perplexity ranking to filter the downloaded web data.
More elaborate algorithms could be exploited, such as the one proposed by Sethy et al.
(2007).
Initially, we have experimented along those lines, but it did not pay off; maybe future refinements will be more successful.
<newSection> References<newSection> Abstract This paper presents a conditional random field-based approach for identifying speaker-produced disfluencies (i.e. if and where they occur) in spontaneous speech transcripts.
We emphasize false start regions, which are often missed in current disfluency identification approaches as they lack lexical or structural similarity to the speech immediately following.
We find that combining lexical, syntactic, and language model-related features with the output of a state-of-the-art disflu-ency identification system improves overall word-level identification of these and other errors.
Improvements are reinforced under a stricter evaluation metric requiring exact matches between cleaned sentences annotator-produced reconstructions, and altogether show promise for general re-construction efforts.
<newSection> 1 Introduction The output of an automatic speech recognition (ASR) system is often not what is required for subsequent processing, in part because speakers themselves often make mistakes (e.g. stuttering, self-correcting, or using filler words).
A cleaner speech transcript would allow for more accurate language processing as needed for natural language processing tasks such as machine translation and conversation summarization which often assume a grammatical sentence as input.
A system would accomplish reconstruction of its spontaneous speech input if its output were to represent, in flawless, fluent, and content-preserving text, the message that the speaker intended to convey.
Such a system could also be applied not only to spontaneous English speech, but to correct common mistakes made by non-native speakers (Lee and Seneff, 2006), and possibly extended to non-English speaker errors.
A key motivation for this work is the hope that a cleaner, reconstructed speech transcript will allow for simpler and more accurate human and natural language processing, as needed for applications like machine translation, question answering, text summarization, and paraphrasing which often assume a grammatical sentence as input.
This benefit has been directly demonstrated for statistical machine translation (SMT).
Rao et al.
(2007) gave evidence that simple disfluency removal from transcripts can improve BLEU (a standard SMT evaluation metric) up to 8% for sentences with disflu-encies.
The presence of disfluencies were found to hurt SMT in two ways: making utterances longer without adding semantic content (and sometimes adding false content) and exacerbating the data mismatch between the spontaneous input and the clean text training data.
While full speech reconstruction would likely require a range of string transformations and potentially deep syntactic and semantic analysis of the errorful text (Fitzgerald, 2009), in this work we will first attempt to resolve less complex errors, corrected by deletion alone, in a given manually-transcribed utterance.
We build on efforts from (Johnson et al., 2004), aiming to improve overall recall – especially of false start or non-copy errors – while concurrently maintaining or improving precision.
Common simple disfluencies in sentence-like utterances (SUs) include filler words (i.e. “um”, “ah”, and discourse markers like “you know”), as well as speaker edits consisting of a reparandum, an interruption point (IP), an optional interregnum (like “I mean”), and a repair region (Shriberg, 1994), as seen in Figure 1. and other examples, reparandum regions are in brackets (’[’, ’]’), interregna are in braces ('I' ' 'f', ’}’), and interruption points are marked by ’+’.
These reparanda, or edit regions, can be classified into three main groups: EX4: [amazon was incorporated by] fuh} well i only knew two people there In simple cleanup (a precursor to full speech reconstruction), all detected filler words are deleted, and the reparanda and interregna are deleted while the repair region is left intact.
This is a strong initial step for speech reconstruction, though more complex and less deterministic changes are often required for generating fluent and grammatical speech text.
In some cases, such as the repetitions mentioned above, simple cleanup is adequate for reconstruction.
However, simply deleting the identified reparandum regions is not always optimal.
We would like to consider preserving these fragments (for false starts in particular) if In the first restart fragment example (EX3 in Section 1.1), the reparandum introduces no new active verbs or new content, and thus can be safely deleted.
The second example (EX4) however demonstrates a case when the reparandum may be considered to have unique and preservable content of its own.
Future work should address how to most appropriately reconstruct speech in this and similar cases; this initial work will for risk information loss as we identify and delete these reparandum regions.
<newSection> 1.2 Related Work Stochastic approaches for simple disfluency detection use features such as lexical form, acoustic cues, and rule-based knowledge.
Most state-ofthe-art methods for edit region detection such as (Johnson and Charniak, 2004; Zhang and Weng, 2005; Liu et al., 2004; Honal and Schultz, 2005) model speech disfluencies as a noisy channel model.
In a noisy channel model we assume that an unknown but fluent string F has passed through a disfluency-adding channel to produce the observed disfluent string D, and we then aim to recover the most likely input string F�, defined as where P(F) represents a language model defining a probability distribution over fluent “source” strings F, and P(D|F) is the channel model defining a conditional probability distribution of observed sentences D which may contain the types of construction errors described in the previous subsection.
The final output is a word-level tagging of the error condition of each word in the sequence, as seen in line 2 of Figure 2.
The Johnson and Charniak (2004) approach, referred to in this document as JC04, combines the noisy channel paradigm with a tree-adjoining grammar (TAG) to capture approximately repeated elements.
The TAG approach models the crossed word dependencies observed when the reparandum incorporates the same or very similar words in roughly the same word order, which JC04 refer to as a rough copy.
Our version of this system does not use external features such as prosodic classes, as they use in Johnson et al.
(2004), but otherwise appears to produce comparable results to those reported.
While much progress has been made in simple disfluency detection in the last decade, even top-performing systems continue to be ineffective at identifying words in reparanda.
To better understand these problems and identify areas class labels, where - denotes a non-error, FL denotes a filler, E generally denotes reparanda, and RC and NC indicate rough copy and non-copy speaker errors, respectively.
for improvement, we used the top-performing1 JC04 noisy channel TAG edit detector to produce edit detection analyses on the test segment of the Spontaneous Speech Reconstruction (SSR) corpus (Fitzgerald and Jelinek, 2008).
Table 1 demonstrates the performance of this system for detecting filled pause fillers, discourse marker fillers, and edit words.
The results of a more granular analysis compared to a hand-refined reference (as shown in line 3 of Figure 2) are shown in Table 2.
The reader will recall that precision P is defined as P = |correct| |correct|+|false |and recall R = |correct| |correct|+|miss|.
We denote the harmonic mean of P and R as F-score F and calculate it F = 2 1/P+1/R.
As expected given the assumptions of the TAG approach, JC04 identifies repetitions and most revisions in the SSR data, but less successfully labels false starts and other speaker self-interruptions which do not have a cross-serial correlations.
These non-copy errors (with a recall of only 43.2%), are hurting the overall edit detection recall score.
Precision (and thus F-score) cannot be calculated for the experiment in Table 2; since the JC04 does not explicitly label edits as rough copies or non-copies, we have no way of knowing whether words falsely labeled as edits would have been considered as false RCs or false NCs.
This will unfortunately hinder us from using JC04 as a direct baseline comparison in our work targeting false starts; however, we consider these results to be further motivation for the work.
Surveying these results, we conclude that there is still much room for improvement in the field of simple disfluency identification, especially the cases of detecting non-copy reparandum and learning how and where to implement non-deletion reconstruction changes.
<newSection> 2 Approach We conducted our experiments on the recently released Spontaneous Speech Reconstruction (SSR) corpus (Fitzgerald and Jelinek, 2008), a medium-sized set of disfluency annotations atop Fisher conversational telephone speech (CTS) data (Cieri et al., 2004).
Advantages of the SSR data include As reconstructions are sometimes non-deterministic (illustrated in EX6 in Section 1.1), the SSR provides two manual reconstruc-tions for each utterance in the data.
We use these dual annotations to learn complementary approaches in training and to allow for more accurate evaluation.
The SSR corpus does not explicitly label all reparandum-like regions, as defined in Section 1.1, but only those which annotators selected to delete.
Thus, for these experiments we must implicitly attempt to replicate annotator decisions regarding whether or not to delete reparandum regions when labeling them as such.
Fortunately, we expect this to have a negligible effect here as we will emphasize utterances which do not require more complex reconstructions in this work.
The Spontaneous Speech Reconstruction corpus is partitioned into three subcorpora: 17,162 training sentences (119,693 words), 2,191 sentences (14,861 words) in the development set, and 2,288 sentences (15,382 words) in the test set.
Approximately 17% of the total utterances contain a reparandum-type error.
The output of the JC04 model ((Johnson and Charniak, 2004) is included as a feature and used as an approximate baseline in the following experiments.
The training of the TAG model within this system requires a very specific data format, so this system is trained not with SSR but with Switchboard (SWBD) (Godfrey et al., 1992) data as described in (Johnson and Charniak, 2004).
Key differences in these corpora, besides the form of their annotations, include: While we hope to adapt the trained JC04 model to SSR data in the future, for now these difference in task, evaluation, and training data will prevent direct comparison between JC04 and our results.
Conditional random fields (Lafferty et al., 2001), or CRFs, are undirected graphical models whose prediction of a hidden variable sequence Y is globally conditioned on a given observation sequence X, as shown in Figure 3.
Each observed field.
For this work, x represents observable inputs for each word as described in Section 3.1 and y represents the error class of each word (Section 3.2).
state xi E X is composed of the corresponding word wi and a set of additional features Fi, detailed in Section 3.1.
The conditional probability of this model can be represented as where Zλ(X) is a global normalization factor and A = (A1 ...
AK) are model parameters related to each feature function Fk(X, Y ).
CRFs have been widely applied to tasks in natural language processing, especially those involving tagging words with labels such as part-of-speech tagging and shallow parsing (Sha and Pereira, 2003), as well as sentence boundary detection (Liu et al., 2005; Liu et al., 2004).
These models have the advantage that they model sequential context (like hidden Markov models (HMMs)) but are discriminative rather than generative and have a less restricted feature set.
Additionally, as compared to HMMs, CRFs offer conditional (versus joint) likelihood, and directly maximizes posterior label probabilities P(E|O).
We used the GRMM package (Sutton, 2006) to implement our CRF models, each using a zero-mean Gaussian prior to reduce over-fitting our model.
No feature reduction is employed, except where indicated.
<newSection> 3 Word-Level ID Experiments We aim to train our CRF model with sets of features with orthogonal analyses of the errorful text, integrating knowledge from multiple sources.
While we anticipate that repetitions and other rough copies will be identified primarily by lexical and local context features, this will not necessarily help for false starts with little or no lexical overlap between reparandum and repair.
To catch these errors, we add both language model features (trained with the SRILM toolkit (Stolcke, 2002) on SWBD data with EDITED reparandum nodes removed), and syntactic features to our model.
We also included the output of the JC04 system – which had generally high precision on the SSR data – in the hopes of building on these results.
Altogether, the following features F were extracted for each observation xi. log ratio of the two (log p(t 1 )) to serve as pW an approximation for mutual information between the token and its history, as defined below.
This aims to capture unexpected n-grams produced by the juxtaposition of the reparan-dum and the repair.
The mutual information feature aims to identify when common words are seen in uncommon context (or, alterna-tively, penalize rare n-grams normalized for rare words).
(Ferreira and Bailey, 2004) and others have found that false starts and repeats tend to end at certain points of phrases, which we also found to be generally true for the annotated data.
Note that the syntactic and POS features we used are extracted from the output of an automatic parser.
While we do not expect the parser to always be accurate, especially when parsing errorful text, we hope that the parser will at least be consistent in the types of structures it assigns to particular error phenomena.
We use these features in the hope of taking advantage of that consistency.
In these experiments, we attempt to label the following word-boundary classes as annotated in SSR corpus: Other labels annotated in the SSR corpus (such as insertions and word reorderings), have been ignored for these error tagging experiments.
We approach our training of CRFs in several ways, detailed in Table 3.
In half of our experiments (#1, 3, and 4), we trained a single model to predict all three annotated classes (as defined at the beginning of Section 3.3), and in the other half (#2, 5, and 6), we trained the model to predict NCs only, NCs and FLs, RCs only, or RCs and FLs (as FLs often serve as interregnum, we predict that these will be a valuable cue for other edits).
We varied the subcorpus utterances used in training.
In some experiments (#1 and 2) we trained with the entire training set3, including sentences without speaker errors, and in others (#3-6) we trained only on those sentences containing the relevant deletion errors (and no additionally complex errors) to produce a densely errorful training set.
Likewise, in some experiments we produced output only for those test sentences which we knew to contain simple errors (#3 and 5).
This was meant to emulate the ideal condition where we could perfectly predict which sentences contain errors before identifying where exactly those errors occurred.
The JC04-edit feature was included to help us build on previous efforts for error classification.
To confirm that the model is not simply replicating these results and is indeed learning on its own with the other features detailed, we also trained models without this JC04-edit feature.
We first evaluate edit detection accuracy on a per-word basis.
To evaluate our progress identifying word-level error classes, we calculate precision, recall and F-scores for each labeled class c in each experimental scenario.
As usual, these metrics are calculated as ratios of correct, false, and missed predictions.
However, to take advantage of the double reconstruction annotations provided in SSR (and more importantly, in recognition of the occasional ambiguities of reconstruction) we modified these calculations slightly as shown below.
where cwi is the hypothesized class for wi and cg1,i and cg2,i are the two reference classes.
Analysis: Experimental results can be seen in Tables 4 and 5.
Table 4 shows the impact of sults: Feature variation.
All models were trained with experimental setup #1 and with the set of features identified.
training models for individual features and of constraining training data to contain only those utterances known to contain errors.
It also demonstrates the potential impact on error classification after prefiltering test data to those SUs with errors.
Table 5 demonstrates the contribution of each group of features to our CRF models.
Our results demonstrate the impact of varying our training data and the number of label classes trained for.
We see in Table 4 from setup #5 experiments that training and testing on error-containing utterances led to a dramatic improvement in F1score.
On the other hand, our results for experiments using setup #6 (where training data was filtered to contain errorful data but test data was fully preserved) are consistently worse than those of either setup #2 (where both train and test data was untouched) or setup #5 (where both train and test data were prefiltered).
The output appears to suffer from sample bias, as the prior of an error occurring in training is much higher than in testing.
This demonstrates that a densely errorful training set alone cannot improve our results when testing data conditions do not match training data conditions.
However, efforts to identify errorful sentences before determining where errors occur in those sentences may be worthwhile in preventing false positives in error-less utterances.
We next consider the impact of the four feature groups on our prediction results.
The CRF model appears competitive even without the advantage of building on JC04 results, as seen in Table 54.
Interestingly and encouragingly, the NT bounds features which indicate the linguistic phrase structures beginning and ending at each word according to an automatic parse were also found to be highly contribututive for both fillers and non-copy identification.
We believe that further pursuit of syntactic features, especially those which can take advantage of the context-free weakness of statistical parsers like (Charniak, 1999) will be promising in future research.
It was unexpected that NC classification would be so sensitive to the loss of lexical features while RC labeling was generally resilient to the dropping of any feature group.
We hypothesize that for rough copies, the information lost from the removal of the lexical items might have been compensated for by the JC04 features as JC04 performed most strongly on this error type.
This should be further investigated in the future.
Depending on the downstream task of speech reconstruction, it could be imperative not only to identify many of the errors in a given spoken utterance, but indeed to identify all errors (and only those errors), yielding the precise cleaned sentence that a human annotator might provide.
In these experiments we apply simple cleanup (as described in Section 1.1) to both JC04 output and the predicted output for each experimental setup in Table 3, deleting words when their right boundary class is a filled pause, rough copy or non-copy.
Taking advantage of the dual annotations for each sentence in the SSR corpus, we can report both single-reference and double-reference evaluation.
Thus, we judge that if a hypothesized cleaned sentence exactly matches either reference sentence cleaned in the same manner, we count the cleaned utterance as correct and otherwise assign no credit.
Analysis: We see the outcome of this set of experiments in Table 6.
While the unfiltered test sets of JC04-1, setup #1 and setup #2 appear to have much higher sentence-level cleanup accuracy than the other experiments, we recall that this is natural also due to the fact that the majority of these sentences should not be cleaned at all, besides culate precision and thus Fl score precisely.
Instead, here we show the resultant Fl for the best case and worst case precision range.
occasional minor filled pause deletions.
Looking specifically on cleanup results for sentences known to contain at least one error, we see, once again, that our system outperforms our baseline JC04 system at this task.
<newSection> 4 Discussion Our first goal in this work was to focus on an area of disfluency detection currently weak in other state-of-the-art speaker error detection systems – false starts – while producing comparable classification on repetition and revision speaker errors.
Secondly, we attempted to quantify how far deleting identified edits (both RC and NC) and filled pauses could bring us to full reconstruction of these sentences.
We’ve shown in Section 3 that by training and testing on data prefiltered to include only utterances with errors, we can dramatically improve our results, not only by improving identification of errors but presumably by reducing the risk of falsely predicting errors.
We would like to further investigate to understand how well we can automatically identify errorful spoken utterances in a corpus.
<newSection> 5 Future Work This work has shown both achievable and demonstrably feasible improvements in the area of identifying and cleaning simple speaker errors.
We believe that improved sentence-level identification of errorful utterances will help to improve our word-level error identification and overall reconstruction accuracy; we will continue to research these areas in the future.
We intend to build on these efforts, adding prosodic and other features to our CRF and maximum entropy models, In addition, as we improve the word-level classification of rough copies and non-copies, we will begin to move forward to better identify more complex speaker errors such as missing arguments, misordered or redundant phrases.
We will also work to apply these results directly to the output of a speech recognition system instead of to transcripts alone.
<newSection> Acknowledgments The authors thank our anonymous reviewers for their valuable comments.
Support for this work was provided by NSF PIRE Grant No. OISE-0530118.
Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the supporting agency.
<newSection> References<newSection> 2 Issues Abstract This paper presents a feasibility study for implementing lexical morphology principles in a machine translation system in order to solve unknown words.
Multilingual symbolic treatment of word-formation is seducing but requires an in-depth analysis of every step that has to be performed.
The construction of a prototype is firstly presented, highlighting the methodological issues of such approach.
Secondly, an evaluation is performed on a large set of data, showing the benefits and the limits of such approach.
<newSection> 1 Introduction Formalising morphological information to deal with morphologically constructed unknown words in machine translation seems attractive, but raises many questions about the resources and the prerequisites (both theoretical and practical) that would make such symbolic treatment efficient and feasible.
In this paper, we describe the prototype we built to evaluate the feasibility of such approach.
We focus on the knowledge required to build such system and on its evaluation.
First, we delimit the issue of neologisms amongst the other unknown words (section 2), and we present the few related work done in NLP research (section 3).
We then explain why implementing morphology in the context of machine translation (MT) is a real challenge and what kind of aspects need to be taken into account (section 4), and we show that translating constructed neologisms is not only a mechanical decomposition but requires more fine-grained analysis.
We then describe the methodology developed to build up a prototyped translator of constructed neologisms (section 5) with all the extensions that have to be made, especially in terms of resources.
Finally, we concentrate on the evaluation of each step of the process and on the global evaluation of the entire approach (section 6).
This last evaluation highlights a set of methodological criteria that are needed to exploit lexical morphology in machine translation.
Unknown words are a problematic issue in any NLP tool.
Depending on the studies (Ren and Perrault 1992; Maurel 2004), it is estimated that between 5 and 10 % of the words of a text written in “standard” language are unknown to lexical resources.
In a MT context (analysis-transfer-generation), unknown words remain not only unanalysed but they cannot be translated, and sometimes they also stop the translation of the whole sentence.
Usually, three main groups of unknown words are distinguished: proper names, errors, and neologisms, and the possible solution highly depends on the type of unknown word to be solved.
In this paper, we concentrate on neologisms which are constructed following a morphological process.
The processing of unknown “constructed neologisms” in NLP can be done by simple guessing (based on the sequence of final letters).
This option can be efficient enough when the task is only tagging, but in a multilingual context (like in MT), dealing with constructed neologisms implies a transfer and a generation process that require a more complex formalisation and implementation.
In the project presented in this paper, we propose to implement lexical morphology phenomena in MT.
<newSection> 3 Related work Implementing lexical morphology in a MT context has seldom been investigated in the past, probably because many researchers share the following view: “Though the idea of providing rules for translating derived words may seem attractive, it raises many problems and so it is currently more of a research goal for MT than a practical possibility” (Arnold, Balkan et al. 1994).
As far as we know, the only related project is described in (Gdaniec, Manandise et al.
2001), where they describe a project of implementation of rules for dealing with constructed words in the IBM MT system.
Even in monolingual contexts, lexical morphology is not very often implemented in NLP.
Morphological analyzers like the ones described in (Porter 1980; Byrd 1983; Byrd, Klavans et al.
1989; Namer 2005) propose more or less deeper lexical analyses, to exploit that dimension of the lexicon.
<newSection> 4 Proposed solution Since morphological processes are regular and exist in many languages, we propose an approach where constructed neologisms in source language (SL) can be analysed and their translation generated in a target language (TL) through the transfer of the constructional information.
For example, a constructed neologism in one language (e.g. ricostruire in Italian) should firstly be analysed, i.e. find (i) the rule that produced it (in this case <reiteration rule>) and (ii) the lexeme-base which it is constructed on (costruire, with all morphosyntactic and transla-tional information).
Secondly, through a transfer mechanism (of both the rule and the base), a translation can be generated by rebuilding a constructed word, (in French reconstruire, Eng: to rebuild).
On a theoretical side, the whole process is formalised into bilingual Lexeme Formation Rules (LFR), as explained below in section 4.3.
Although this approach seems to be simple and attractive, feasibility studies and evaluation should be carefully performed.
To do so, we built a system to translate neologisms from one language into another.
In order to delimit the project and to concentrate on methodological issues, we focused on the prefixation process and on two related languages (Italian and French).
Prefixa-tion is, after suffixation, the most productive process of neologism, and prefixes can be more easily processed in terms of character strings.
Regarding the language, we choose to deal with the translation of Italian constructed neologisms into French.
These two languages are historically and morphologically related and are consequently more “neighbours” in terms of neologism coinage.
In the following, we firstly describe precisely the phenomena that have to be formalized and then the prototype built up for the experiment.
Like in any MT project, the formalisation work has to face different issues of contrastivity, i.e. highlighting the divergences and the similarities between the two languages.
In the two languages chosen for the experiment, few divergences were found in the way they construct prefixed neologisms.
However, in some cases, although the morphosemantic process is similar, the item used to build it up (i.e. the affixes) is not always the same.
For example, to coin nouns of the spatial location “before”, where Italian uses the prefix retro, French uses rétro and arrière.
A deeper analysis shows that Italian retro is used with all types of nouns, whereas in French, rétro only forms processual nouns (derived from verbs, like rétrovision, rétroprojection).
For the other type of nouns (generally locative nouns), arrière is used (ar-rière-cabine, arrière-cour).
Other problematic issues appear when there is more than one prefix for the same LFR.
For example, the rule for “indeterminate plurality” provides in both languages a set of two prefixes (multi/pluri in Italian and multi/pluri in French) with no known restrictions for selecting one or the other (e.g. both pluridimensionnel and multi-dimensionnel are acceptable in French).
For these cases, further empirical research have to be performed to identify restrictions on the rule.
Another important divergence is found in the prefixation of relational adjectives.
Relational adjectives are derived from nouns and designate a relation between the entity denoted by the noun they are derived from and the entity denoted by the noun they modify.
Consequently, in a pre-fixation such as anticostituzionale, the formal base is a relational adjective (costituzionale), but the semantic base is the noun the adjective is derived from (costituzione).
The constructed word anticostituzionale can be paraphrased as “against the constitution”.
Moreover, when the relational adjective does not exist, prefixation is possible on a nominal base to create an adjective (squadra antidroga).
In cases where the adjective does exist, both forms are possible and seem to be equally used, like in the Italian collaborazione interuniversità / collaborazione interuniversi-taria.
From a contrastive point of view, the pre-fixation of relational adjectives exists in both languages (Italian and French) and in both these languages prefixing a noun to create an adjective is also possible (anticostituzione (Adj)).
But we notice an important discrepancy in the possibility of constructing relational adjectives (a rough estimation performed on a large bilingual dictionary (Garzanti IT-FR (2006)) shows that more than 1 000 Italian relational adjectives have no equivalent in French (and are generally translated with a prepositional phrase).
All these divergences require an in-dept analysis but can be overcome only if the formalism and the implementation process are done following a rigorous methodology.
In order to evaluate the approach described above and to concretely investigate the ins and outs of such implementation, we built up a prototype of a machine translation system specialized for constructed neologisms.
This prototype is composed of two modules.
The first one checks every unknown word to see if it is potentially constructed, and if so, performs a morphological analysis to individualise the lexeme-base and the rule that coined it.
The second module is the actual translation module, which analyses the constructed neologism and generates a possible translation.
The whole prototype relies on one hand on lexical resources (two monolingual and one bilingual) and on a set of bilingual Lexeme Formation Rules (LFR).
These two sets of information helps the analysis and the generation steps.
When a neologism is looked-up, the system checks if it is constructed with one of the LFRs and if the lexeme-base is in the lexicon.
If it is the case, the transfer brings the relevant morphological and lexical information in the target language.
The generation step constructs the translation equivalent, using the information provided by the LFR and the lexical resources.
Consequently, the whole system relies on the quality of both the lexical resources and the LFR.
The whole morphological process in the system is formalised through bilingual Lexeme Formation Rules.
Their representation is inspired by (Fradin 2003) as shown in figure 2 in the rule of reiterativity.
Such rules match together two monolingual rules (to be read in columns).
Each monolingual rule describes a process that applies a series of instructions on the different sections of the lexeme : the surface section (G and F), the syntactic category (SX) and the semantic (S) sections.
In this theoretical framework, affixation is only one of the instructions of the rule (the graphemic and phonological modification), and consequently, affixes are called “exponent” of the rule.
This formalisation is particularly useful in a bilingual context for rules that have more than one prefix in both languages: more than one affix can be declared in one single rule, the selection being made according to different constraints or restrictions.
For example, the rule for “indeter-minate plurality” explained in section 4.1 can be formalised as follows: In this kind of rules with “multiple exponents”, the two possible prefixes are declared in the surface section (G and F).
The selection is a monolingual issue and cannot be done at the theoretical level.
Such rules have been formalised and implemented for the 56 productive prefixes of Italian (Iacobini 2004)1, with their French translation equivalent.
However, finding the translation equivalent for each rule requires specific studies of the morphological system of both languages in a contrastive perspective.
The following section briefly summarises the contrastive analysis that has been performed to acquire this type of contrastive knowledge.
As in any MT system, the acquisition of bilingual knowledge is an important issue.
In morphology, the method should be particularly accurate to prevent any methodological bias.
To formalise translation rules for prefixed neologisms, we adopt a meaning-to-form approach, i.e. discovering how a constructed meaning is morpho-logically realised in two languages.
We build up a tertium comparationis (a neutral platform, see (James 1980) for details) that constitute a semantic typology of prefixation processes.
This typology aims to be universal and therefore applicable to all the languages concerned.
On a practical point of view, the typology has been built up by summing up various descriptions of prefixation in various languages (Montermini 2002; Iacobini 2004; Amiot 2005).
We end up with six main classes: location, evaluation, quantitative, modality, negation and ingressive.
The classes are then subdivided according to sub-meanings: for example, location is subdivided in temporal and spatial, and within spatial location, a distinction is made between different positions (before, above, below, in front, ...).
Prefixes of both languages are then literally “projected” (or classified) onto the tertium.
For each terminal sub-class, we have a clear picture of the prefixes involved in both languages.
For example, the LFR presented in figure 1 is the result of the projection of the Italian prefix (ri) and the French one (re) on the sub-class reitera-tivity, which is a sub-class of modality.
At the end of the comparison, we end up with more than 100 LFRs (one rule can be reiterated according the different input and output categories).
From a computing point of view, constraints have to be specified and the lexicon has to be adapted consequently.
<newSection> 5 Implementation Implementation of the LFR is set up as a database, from where the program takes the information to perform the analysis, the transfer and the generation of the neologisms.
In our approach, LFRs are simply declared in a tab format database, easily accessible and modifiable by the user, as shown below: Implemented LFRs describe (i) the surface form of the Italian prefix to be analysed, (ii) the category of the base, (iii) the category of the derived lexeme (the output), (iv) a reference to the rule implied and (v) the French prefix(es) for the generation.
The surface form in (i) should sometimes take into account the different allomorphs of one prefix.
Consequently, the rule has to be reiterated in order to be able to recognize any forms (e.g. the prefix in has different forms according to the initial letter of the base, and four rules have to be implemented for the four allomorphs (in, il, im, ir)).
In some other cases, the initial consonant is doubled, and the algorithm has to take this phenomenon into account.
In (ii), the information of the category of the base has been “overspecified”, to differentiate qualitative and relational adjectives, and deverbal nouns and the other ones (a_rel/a or n_dev/n).
These overspecifications have two objectives: optimizing the analysis performance (reducing the noise of homographic character strings that look like constructed neologisms but that are only misspellings - see below in the evaluation section), and refining the analysis, i.e. selecting the appropriate LFR and, consequently, the appropriate translation.
To identify relational adjectives and deverbal nouns, the monolingual lexicon that supports the analysis step has to be extended.
Thereafter, we present the symbolic method we used to perform such extension.
Our MT prototype relies on lexical resources: it aims at dealing with unknown words that are not in a Reference lexicon and these unknown words are analyzed with lexical material that is in this lexicon.
From a practical point of view, our prototype is based on two very large monolingual databases (Mmorph (Bouillon, Lehmann et al.
1998)) for Italian and French, that contain only morpho-syntactic information, and on one bilingual lexicon that has been built semi-automatically for the use of the experiment.
But the monolingual lexica have to be adapted to provide specific information necessary for dealing with morphological process.
As stated above, identifying the prefix and the base is not enough to provide a proper analysis of constructed neologisms which is detailed enough to be translated.
The main information that is essential for the achievement of the process is the category of the base, which has to be sometimes “overspecified”.
Obviously, the Italian reference lexicon does not contain such information.
Consequently, we looked for a simple way to automatically extend the Italian lexicon.
For example, we looked for a way to automatically link relational adjectives with their noun bases.
Our approach tries to take advantage of only the lexicon, without the use of any larger resources.
To extend the Italian lexicon, we simply built a routine based on the typical suffixes of relational adjectives (in Italian: -ale, -are, -ario, -ano, -ico, -ile, -ino, -ivo, -orio, -esco, -asco, -iero, -izio, -aceo (Wandruszka 2004)).
For every adjective ending with one of these suffixes, the routine looks up if the potential base corresponds to a noun in the rest of the lexicon (modulo some morphographemic variations).
For example, the routine is able to find links between adjectives and base nouns such as ambientale and ambiente, aziendale and azienda, cortisonica and cortisone or contestuale and contesto.
Unfortunately, this kind of automatic implementation does not find links between adjectives made from the learned root of the noun, (prandiale 4 pranzo, bellico guerra).
This automatic extension has been evaluated.
Out of a total of more than 68 000 adjective forms in the lexicon, we identified 8 466 relational adjectives.
From a “recall” perspective, it is not easy to evaluate the coverage of this extension because of the small number of resources containing relational adjectives that could be used as a gold standard.
A similar extension is performed for the deverbal aspect, for the lexicon should also distinguish deverbal noun.
From a morphological point of view, deverbalisation can be done trough two main productive processes: conversion (a command 4 to command) and suffixation.
If the first one is relatively difficult to implement, the second one can be easily captured using the typical suffixes of such processes.
Consequently, we considere that any noun ending with suffixes like ione, aggio,or mento are deverbal.
Thanks to this extended lexicon, overspecified input categories (like a_rel for relational adjective or n_dev for deverbal noun) can be stated and exploited in the implemented LFR as shown in figure 4.
Once the prototyped MT system was built and the lexicon adapted, it was applied to a set of neologisms (see section 6 for details).
For example, unknown Italian neologisms such as arci-contento, ridescrizione, deitalianizzare, were automatically translated in French: archi-content, redescription, désitalianiser.
The divergences existing in the LFR of <loca-tive position before> are correctly dealt with, thanks to the correct analysis of the base.
For example, in the neologism retrobottega, the lexeme-base is correctly identified as a locative noun, and the French equivalent is constructed with the appropriate prefix (arrière-boutique), while in retrodiffusione, the base is analysed as deverbal, and the French equivalent is correctly generated (rétrodiffusion).
For the analysis of relational adjectives, the overspecification of the LFRs and the extension of the lexicon are particularly useful when there is no French equivalent for Italian relational adjectives because the corresponding construction is not possible in the French morphological system.
For example, the Italian relational adjective aziendale (from the noun azienda, Eng: company) has no adjectival equivalent in French.
The Italian prefixed adjective interaziendale can only be translated in French by using a noun as the base (interentreprise).
This translation equivalent can be found only if the base noun of the Italian adjective is found (interaziendale, inter+aziendale 4 azienda, azienda = entreprise, 4 interentreprise).
The same process has been applied for the translation of precongressuale, post-transfuzionale by précongrès, post-transfusion.
Obviously, all the mechanisms formalised in this prototype should be carefully evaluated.
<newSection> 6 Evaluation The advantages of this approach should be carefully evaluated from two points of view: the evaluation of the performance of each step and of the feasibility and portability of the system.
As previously stated, the system is intended to solve neologisms that are unknown from a lexicon with LFRs that exploit information contained in the lexicon.
To evaluate the performance of our system, we built up a corpus of unknown words by confronting a large Italian corpus from journalistic domain (La Repubblica Online (Baroni, Bernardini et al.
2004)) with our reference lexicon for this language (see section 4.1 above).
We obtained a set of unknown words that contains neologisms, but also proper names and erroneous items.
This set is submitted to the various steps of the system, where constructed neologisms are recognised, analysed and translated.
As we previously stated, the analysis step can actually be divided into two tasks.
First of all, the program has to identify, among the unknown words, which of them are morphologically constructed (and so analysable by the LFRs); secondly, the program has to analyse the constructed neologisms, i.e matching them with the correct LFRs and isolating the correct base-words.
For the first task, we obtain a list of 42 673 potential constructed neologisms.
Amongst those, there are a number of erroneous words that are homographic to a constructed neologism.
For example, the item progesso, a misspelling of progresso (Eng: progress), is erroneously analysed as the prefixation of gesso (eng: plaster) with the LFR in pro.
In the second part of the processing, LFRs are concretely applied to the potential neologisms (i.e. constraints on categories and on over-specified category, phonological constraints).
This stage retains 30 376 neologisms.
A manual evaluation is then performed on these outputs.
Globally, 71.18 % of the analysed words are actually neologisms.
But the performance is not the same for every rule.
Most of them are very efficient: among all the rules for the 56 Italian prefixes, only 7 cause too many erroneous analyses, and should be excluded - mainly rules with very short prefixes (like a, di, s), that cause mistakes due to homograph.
As explained above, some of the rules are strongly specified, (i.e. very constrained), so we also evaluate the consequence of some constraints, not only in terms of improved performance but also in terms of loss of information.
Indeed, some of the constraints specified in the rule exclude some neologisms (false negatives).
For example, the modality LFRs with co and ri have been overspecified, requiring deverbal base-noun (and not just a noun).
Adding this constraint improves the performance of the analysis (i.e. the number of correct lexemes analysed), respectively from 69.48 % to 96 % and from 91.21 % to 99.65 %.
Obviously, the number of false negatives (i.e. correct neologisms excluded by the constraint) is very large (between 50 % and 75 % of the excluded items).
In this situation, the question is to decide whether the gain obtained by the constraints (the improved performance) is more important than the un-analysed items.
In this context, we prefer to keep the more constrained rule.
Un-analysed items remain unknown words, and the output of the analysis is almost perfect, which is an important condition for the rest of the process (i.e. transfer and generation).
Generation can also be evaluated according to two points of view: the correctness of the generated items, and the improvement brought by the solved words to the quality of the translated sentence.
To evaluate the first aspect, many procedures can be put in place.
The correctness of constructed words could be evaluated by human judges, but this kind of approach would raise many questions and biases: people that are not expert of morphology would judge the correctness according to their degree of acceptability which varies between judges and is particularly sensitive when neologism is concerned.
Questions of homogeneity in terms of knowledge of the domain and of the language are also raised.
Because of these difficulties, we prefer to centre the evaluation on the existence of the generated neologisms in a corpus.
For neologisms, the most adequate corpus is the Internet, even if the use of such an uncontrolled resource requires some precautions (see (Fradin, Dal et al.
2007) for a complete debate on the use of web resources in morphology).
Concretely, we use the robot Golf (Thomas 2008) that sends each generated neologism automatically as a request on a search engine (here Google©) and reports the number of occurrences as captured by Google.
This robot can be param-eterized, for instance by selecting the appropriate language.
Because of the uncontrolled aspect of the resource, we distinguish three groups of reported frequencies: 0 occurrence, less than 5 occurrences and more than 5.
The threshold of 5 helps to distinguish confirmed existence of neologism (> 5) from unstable appearances (< 5), that are closed to hapax phenomena.
The table below summarizes some results for some prefixed neologisms.
Globally, most of the generated prefixed neologisms have been found in corpus, and most of the time with more than 5 occurrences.
Unfound items are very useful, because they help to point out difficulties or miss-formalised processes.
Most of the unfound neologisms were ill-analysed items in Italian.
Others were due to misuses of hyphens in the generation.
Indeed, in the program, we originally implemented the use of the hyphen in French following the established norm (i.e. a hyphen is required when the prefix ends with a vowel and the base starts with a vowel).
But following this “norm”, some forms were not found in corpus (for example antibra-connier (Eng: antipoacher) reports 0 occurrence).
When re-generated with a hyphen, it reports 63 occurrences.
This last point shows that in neology, usage does not stick always to the norm.
The other problem raised by unknown words is that they decrease the quality of the translation of the entire sentence.
To evaluate the impact of the translated unknown words on the translated sentence, we built up a test-suite of sentences, each of them containing one prefixed neologism (in bold in table 2).
We then submitted the sentences to a commercial MT system (Systran©) and recorded the translation and counted the number of mistakes (FR1 in table 2 below).
On a second step, we “feed” the lexicon of the translation system with the neologisms and their translation (generated by our prototype) and resubmit the same sentences to the system (FR2 in table 2).
For the 60 sentences of the test-suit (21 with an unknown verb, 19 with an unknown adjective and 20 with a unknown noun), we then counted the number of errors before and after the introduction of the neologisms in the lexicon, as shown below (errors are underlined).
For a global view of the evaluation, we classified in the table below the number of sentences according to the number of errors “removed” thanks to the resolution of the unknown word.
Most of the improvements concern only a reduction of 1, i.e. only the unknown word has been solved.
But it should be noticed that improvement is more impressive when the unknown words are nouns or verbs, probably because these categories influence much more items in the sentence in terms of agreement.
In two cases (involving verbs), errors are corrected because of the translation of the unknown words, but at the same time, two other errors are caused by it.
This problem comes from the fact that adding new words in the lexicon of the system requires sometimes more information (such as valency) to provide a proper syntaxctic generation of the sentence.
The relatively good results obtained by the prototype are very encouraging.
They mainly show that if the analysis step is performed correctly, the rest of the process can be done with not much further work.
But at the end of such a feasibility study, it is useful to look objectively for the conditions that make such results possible.
The good quality of the result can be explained by the important preliminary work done (i) in the extension/specialisation of the lexicon, and (ii) in the setting up of the LFRs.
The acquisition of the contrastive knowledge in a MT context is indeed the most essential issue in this kind of approach.
The methodology we proposed here for setting these LFR proves to be useful for the linguist to acquire this specific type of knowledge.
Lexical morphology is often considered as not regular enough to be exploited in NLP.
The evaluation performed in this study shows that it is not the case, especially in neologism.
But in some cases, it is no use to ask for the impossible, and simply give up implementing the most inefficient rules.
We also show that the efficient analysis step is probably the main condition to make the whole system work.
This step should be implemented with as much constraints as possible, to provide an output without errors.
Such implementation requires proper evaluation of the impact of every constraint.
It should also be stated that such implementation (and especially knowledge acquisition) is time-consuming, and one can legitimately ask if machine-learning methods would do the job.
The number of LFRs being relatively restrained in producing neologisms, we can say that the effort of manual formalisation is worthwhile for the benefits that should be valuable on the long term.
Another aspect of the feasibility is closely related to questions of “interoperability”, because such implementation should be done within existing MT programs, and not independently as it was for this feasibility study.
Other questions of portability should also be considered.
As we stated, we chose two morpho-logically related languages on purpose: they present less divergences to deal with and allow con-centrating on the method.
However, the proposed method (especially that contrastive knowledge acquisition) can clearly be ported to another pair of languages (at least inflexional languages).
It should also be noticed that the same approach can be applied to other types of construction.
We mainly think here of suffixation, but one can imagine to use LFRs with other elements of formation (like combining forms, that tend to be very “international”, and consequently the material for many neologisms).
Moreover, the way the rules are formalised and the algorithm designed allow easy reversibility and modification.
<newSection> 7 Conclusion This feasibility study presents the benefit of implementing lexical morphology principles in a MT system.
It presents all the issues raised by formalization and implementation, and shows in a quantitative manner how those principles are useful to partly solve unknown words in machine translation.
From a broader perspective, we show the benefits of such implementation in a MT system, but also the method that should be used to formalise this special kind of information.
We also emphasize the need for in-dept work of knowledge acquisition before actually building up the system, especially because contrastive morphological data are not as obvious as other linguistic dimensions.
Moreover, the evaluation step clearly states that the analysis module is the most important issue in dealing with lexical morphology in multilingual context.
The multilingual approach of morphology also paves the way for other researches, either in representation of word-formation or in exploitation of multilingual dimension in NLP systems.
<newSection> References<newSection> Abstract The present paper is concerned with statistical parsing of constituent structures in German.
The paper presents four experiments that aim at improving parsing performance of coordinate structure: 1) reranking the n-best parses of a PCFG parser, 2) enriching the input to a PCFG parser by gold scopes for any conjunct, 3) reranking the parser output for all possible scopes for conjuncts that are permissible with regard to clause structure.
Experiment 4 reranks a combination of parses from experiments 1 and 3.
The experiments presented show that n-best parsing combined with reranking improves results by a large margin.
Providing the parser with different scope possi-bilities and reranking the resulting parses results in an increase in F-score from 69.76 for the baseline to 74.69.
While the F-score is similar to the one of the first experiment (n-best parsing and reranking), the first experiment results in higher recall (75.48% vs. 73.69%) and the third one in higher precision (75.43% vs. 73.26%).
Combining the two methods results in the best result with an F-score of 76.69.
<newSection> 1 Introduction The present paper is concerned with statistical parsing of constituent structures in German.
German is a language with relatively flexible phrasal ordering, especially of verbal complements and adjuncts.
This makes processing complex cases of coordination particularly challenging and error-prone.
The paper presents four experiments that aim at improving parsing performance of coordinate structures: the first experiment involves reranking of n-best parses produced by a PCFG parser, the second experiment enriches the input to a PCFG parser by offering gold pre-bracketings for any coordinate structures that occur in the sentence.
In the third experiment, the reranker is given all possible pre-bracketed candidate structures for coordinated constituents that are permissible with regard to clause macro- and microstruc-ture.
The parsed candidates are then reranked.
The final experiment combines the parses from the first and the third experiment and reranks them.
Improvements in this final experiment corroborate our hypothesis that forcing the parser to work with pre-bracketed conjuncts provides parsing alternatives that are not present in the n-best parses.
Coordinate structures have been a central issue in both computational and theoretical linguistics for quite some time.
Coordination is one of those phenomena where the simple cases can be accounted for by straightforward empirical gen-eralizations and computational techniques.
More specifically, it is the observation that coordination involves two or more constituents of the same categories.
However, there are a significant number of more complex cases of coordination that defy this generalization and that make the parsing task of detecting the right scope of individual conjuncts and correctly delineating the correct scope of the coordinate structure as a whole difficult.
(1) shows some classical examples of this kind from English.
In (1a), unlike categories (NP and adjective) are conjoined.
(1b) and (1c) are instances of ellipsis (VP ellipsis and gapping).
Yet another difficult set of examples present cases of non-constituent conjunction, as in (2), where the direct and indirect object of a ditransitive verb are conjoined.
(2) Bob gave a book to Sam and a record to Jo.
<newSection> 2 Coordination in German The above phenomena have direct analogues in German.1 Due to the flexible ordering of phrases, their variability is even higher.
For example, due to constituent fronting to clause-initial position in German verb-second main clauses, cases of non-constituent conjunction can involve any two NPs (including the subject) of a ditransitive verb to the exclusion of the third NP complement that appears in clause-initial position.
In addition, German exhibits cases of asymmetric coordination first discussed by H¨ohle (1983; 1990; 1991) and illustrated in (3).2 <newSection> Into Hasen. hare.
Such cases of subject gap coordination are frequently found in text corpora (cf. (4) below) and involve conjunction of a full verb-second clause with a VP whose subject is identical to the subject in the first conjunct.
<newSection> 3 Experimental Setup and Baseline The data source used for the experiments is the T¨ubingen Treebank of Written German (T¨uBa-D/Z) (Telljohann et al., 2005).
T¨uBa-D/Z uses the newspaper ’die tageszeitung’ (taz) as its data source, version 3 comprises approximately 27 000 sentences.
The treebank annotation scheme distinguishes four levels of syntactic constituency: the lexical level, the phrasal level, the level of topological fields, and the clausal level.
The primary ordering principle of a clause is the inventory of topological fields (VF, LK, MF, VC, and NF), which characterize the word order regularities among different clause types of German.
T¨uBaD/Z annotation relies on a context-free backbone (i.e. proper trees without crossing branches) of phrase structure combined with edge labels that specify the grammatical function of the phrase in question.
Conjuncts are generally marked with the function label KONJ.
Figure 1 shows the annotation that sentence (4) received in the treebank.
Syntactic categories are displayed as nodes, grammatical functions as edge labels in gray (e.g. OA: direct object, PRED: predicate).
This is an example of a subject-gap coordination, in which both conjuncts (FKONJ) share the subject (ON) that is realized in the first conjunct.
Bev¨olkerungs-decline in r¨uckgang zwar abgeschw¨acht, ist population though lessened, is so groß so big ’For this reason, although the decline in population has lessened, it is still twice as big as in 1996.’
The syntactic annotation scheme of the T¨uBaD/Z is described in more detail in Telljohann et al.
(2004; 2005).
All experiments reported here are based on a data split of 90% training data and 10% test data.
Two parsers were used to investigate the influence of scope information on parser performance on coordinate structures: BitPar (Schmid, 2004) and LoPar (Schmid, 2000).
BitPar is an efficient implementation of an Earley style parser that uses bit vectors.
However, BitPar cannot handle pre-bracketed input.
For this reason, we used LoPar for the experiments where such input was required.
LoPar, as it is used here, is a pure PCFG parser, which allows the input to be partially bracketed.
We are aware that the results that can be obtained by pure PCFG parsers are not state of the art as reported in the shared task of the ACL 2008 Workshop on Parsing German (K¨ubler, 2008).
While BitPar reaches an F-score of 69.76 (see next section), the best performing parser (Petrov and Klein, 2008) reaches an F-score of 83.97 on T¨uBa-D/Z (but with a different split of training and test data).
However, our experiments require certain features in the parsers, namely the capability to provide n-best analyses and to parse pre-bracketed input.
To our knowledge, the parsers that took part in the shared task do not provide these features.
Should they become available, the methods presented here could be applied to such parsers.
We see no reason why our methods should not be able to improve the results of these parsers further.
Since we are interested in parsing coordina-tions, all experiments are conducted with gold POS tags, so as to abstract away from POS tagging errors.
Although the treebank contains morphological information, this type of information is not used in the experiments presented here.
The reranking experiments were conducted using the reranker by Collins and Koo (2005).
This reranker uses a set of candidate parses for a sentence and reranks them based on a set of features that are extracted from the trees.
The reranker uses a boosting method based on the approach by Freund et al.
(1998).
We used a similar feature set to the one Collins and Koo used; the following types of features were included: rules, bigrams, grandparent rules, grandparent bigrams, lexical bigrams, two-level rules, two-level bigrams, trigrams, head-modifiers, PPs, and distance for head-modifier relations, as well as all feature types involving rules extended by closed class lexicaliza-tion.
For a more detailed description of the rules, the interested reader is referred to Collins and Koo (2005).
For coordination, these features give a wider context than the original parser has and should thus result in improvements for this phenomenon.
When trained on 90% of the approximately 27,000 sentences of the T¨uBa-D/Z treebank, BitPar reaches an F-Score of 69.73 (precision: 68.63%, recall: 70.93%) on the full test set of 2611 sentences.
These results as well as all further results presented here are labeled results, including grammatical functions.
Since German has a relatively free word order, it is impossible to deduce the grammatical function of a noun phrase from the configuration of the sentence.
Consequently, an evaluation based solely on syntactic constituent labels would be meaningless (cf. (K¨ubler, 2008) for a discussion of this point).
The inclusion of grammatical labels in the trees, makes the parsing process significantly more complex.
Looking at sentences with coordination (i.e. sentences that contain a conjunction which is not in sentence-initial position), we find that 34.9% of the 2611 test sentences contain coordinations.
An evaluation of only sentences with coordination shows that there is a noticeable difference: the F-score reaches 67.28 (precision: 66.36%, recall: 68.23%) as compared to 69.73 for the full test set.
The example of a wrong parse shown below illustrates why parsing of complex coordinations is so hard.
Complex coordinations can take up a considerable part of the input string and accordingly of the overall sentence structure.
Such global phenomena are particularly hard for pure PCFG parsing, due to the independence assumption inherent in the statistical models for PCFGs.
Sentence (4) has the following Viterbi parse: The parse shows that the parser did not recognize the coordination.
Instead, the first conjunct including the fronted constituent, Damit hat sich der Bev¨olkerungsr¨uckgang zwar abgeschw¨acht, is treated as a fronted subordinate clause.
<newSection> 4 Experiment 1: n-Best Parsing and Reranking The first hypothesis for improving coordination parsing is based on the assumption that the correct parse may not be the most probable one in Viterbi parsing but may be recovered by n-best parsing and reranking, a technique that has become standard in the last few years.
If this hypothesis holds, we should find the correct parse among the n-best parses.
In order to test this hypothesis, we conducted an experiment with BitPar (Schmid, 2004).
We parsed the test sentences in a 50-best setting.
A closer look at the 50-best parses shows that of the 2611 sentences, 195 (7.5%) were assigned the correct parse as the best parse.
For 325 more sentences (12.4%), the correct parse could be found under the 50 best analyses.
What is more, in 90.2% of these 520 sentences, for which the correct parse was among the 50 best parses, the best parse was among the first 10 parses.
Additionally, only in 4 cases were the correct analyses among the 40-best to 50-best parses, an indication that increasing n may not result in improving the results significantly.
These findings resulted in the decision not to conduct experiments with higher n.
That the 50 best analyses contain valuable information can be seen from an evaluation in which an oracle chooses from the 50 parses.
In this case, we reach an F-score of 80.28.
However, this F-score is also the upper limit for improvement that can be achieved by reranking the 50-best parses.
For reranking, the features of Collins and Koo (2005) were extended in the following way: Since the German treebank used for our experiments includes grammatical function information on almost all levels in the tree, all feature types were also included with grammatical functions attached: All nodes except the root node of the subtree in question were annotated with their grammatical information.
Thus, for the noun phrase (NX) rule with grandparent prepositional phrase (PX) PXGP NX —*ART ADJX NN, we add an additional rule PXGP NX-HD —* ART ADJX NN-HD.
After pruning all features that occurred in the training data with a frequency lower than 5, the extractions produced more than 5 mio.
different features.
The reranker was optimized on the training data, the 50-best parses were produced in a 5-fold cross-validation setting.
A non-exhaustive search for the best value for the α parameter showed that Collins and Koo’s value of 0.0025 produced the best results.
The row for exp.
1 in Table 1 shows the results of this experiment.
The evaluation of the full data set shows an improvement of 4.77 points in the F-score, which reached 74.53.
This is a relative reduction in error rate of 18.73%, which is slightly higher that the error rate reduction reported by Collins and Koo for the Penn Treebank (13%).
However, the results for Collins and Koo’s original parses were higher, and they did not evaluate on grammatical functions.
The evaluation of coordination sentences shows that such sentences profit from reranking to the same degree.
These results prove that while coordination structures profit from reranking, they do not profit more than other phenomena.
We thus conclude that reranking is no cure-all for solving the problem of accurate coordination parsing.
<newSection> 5 Experiment 2: Gold Scope The results of experiment 1 lead to the conclusion that reranking the n-best parses can only result in restricted improvements on coordinations.
The fact that the correct parse often cannot be found in the 50-best analyses suggests that the different possible scopes of a coordination are so different in their probability distribution that not all of the possible scopes are present in the 50-best analyses.
If this hypothesis holds, forcing the parser to consider the different scope readings should increase the accuracy of coordination parsing.
In order to force the parser to use the different scope readings, we first extract these scope readings, and then for each of these scope readings generate a new sentence with partial bracketing that represents the corresponding scope (see below for an example).
LoPar is equipped to parse partially-bracketed input.
Given input sentences with partial brackets, the parser restricts analyses to such cases that do not contradict the brackets in the input.
’Which is correct because they are entertaining, but also triggers wrong associations.’
In order to test the validity of this hypothesis, we conducted an experiment with coordination scopes extracted from the treebank trees.
These scopes were translated into partial brackets that were included in the input sentences.
For the sentence in (5) from the treebank (sic), the input for LoPar would be the following: The round parentheses delineate the conjuncts.
LoPar was then forced to parse sentences containing coordination with the correct scope for the coordination.
The results for this experiment are shown in Table 1 as exp.
2. The introduction of partial brackets that delimit the scope of the coordination improve overall results on the full test set by 4.7 percent points, a rather significant improvement when we consider that only approximately one third of the test sentences were modified.
The evaluation of the set of sentences that contain coordination shows that here, the difference is even higher: 6.7 percent points.
It is also worth noticing that provided with scope information, the parser parses such sentences with the same accuracy as other sentences.
The difference in F-scores between all sentences and only sentences with coordination in this experiment is much lower (0.5 percent points) than for all other experiments (2.5–3.0 percent points).
When comparing the results of experiment 1 (n-best parsing) with the present one, it is evident that the F-scores are very similar: 74.53 for the 50-best reranking setting, and 74.46 for the one where we provided the gold scope.
However, a comparison of precision and recall shows that there are differences: 50-best reranking results in higher recall, providing gold scope for coordinations in higher precision.
The lower recall in the latter experiment indicates that the provided brackets in some cases are not covered by the grammar.
This is corroborated by the fact that in n-best parsing, only 1 sentence could not be parsed; but in parsing with gold scope, 8 sentences could not be parsed.
<newSection> 6 Experiment 3: Extracting Scope The previous experiment has shown that providing the scope of a coordination drastically improves results for sentences with coordination as well as for the complete test set (although to a lower degree).
The question that remains to be answered is whether automatically generated possible scopes can provide enough information for the reranker to improve results.
The first question that needs to be answered is how to find the possible scopes for a coordination.
One possibility is to access the parse forest of a chart parser such as LoPar and extract infor-(5) Was stimmt, weil sie Which is correct, because they unterhaltsam sind, aber auch falsche entertaining are, but also wrong Assoziationen weckt.
associations wakes.
mation about all the possible scope analyses that the parser found.
If the same parser is used for this step and for the final parse, we can be certain that only scopes are extracted that are compatible with the grammar of the final parser.
However, parse forests are generally stored in a highly packed format so that an exhaustive search of the structures is very inefficient and proved impossible with present day computing power.
”There Niederflurbusse, low-floor buses, nicht”, not”, ’”There are indeed a few low-floor buses, but that isn’t enough”, he says.
Another solution consists of generating all possible scopes around the coordination.
Thus, for the sentence in (6), the conjunction is aber.
The shortest possible left conjunct is Niederflurbusse, the next one paar Niederflurbusse, etc.
Clearly, many of these possibilities, such as the last example, are nonsensical, especially when the proposed conjunct crosses into or out of base phrase boundaries.
Another type of boundary that should not be crossed is a clause boundary.
Since the conjunction is part of the subordinated clause in the present example, the right conjunct cannot extend beyond the end of the clause, i.e. beyond nicht.
For this reason, we used KaRoPars (M¨uller and Ule, 2002), a partial parser for German, to parse the sentences.
From the partial parses, we extracted base phrases and clauses.
For (6), the relevant bracketing provided by KaRoPars is the following: The round parentheses mark clause boundaries, the curly braces the one base phrase that is longer than one word.
In the creation of possible con-juncts, only such conjuncts are listed that do not cross base phrase or clause boundaries.
In order to avoid unreasonably high numbers of pre-bracketed versions, we also use higher level phrases, such as coordinated noun phrases.
KaRoPars groups such higher level phrases only in contexts that allow a reliable decision.
While a small percentage of such decisions is wrong, the heuristic used turns out to be reliable and efficient.
For each scope, a partially bracketed version of the input sentence is created, in which only the brackets for the suggested conjuncts are inserted.
Each pre-bracketed version of the sentence is parsed with LoPar.
Then all versions for one sentence are reranked.
The reranker was trained on the data from experiment 1 (n-best parsing).
The results of the reranker show that our restrictions based on the partial parser may have been too restrictive.
Only 375 sentences had more than one pre-bracketed version, and only 328 sentence resulted in more than one parse.
Only the latter set could then profit from reranking.
The results of this experiment are shown in Table 1 as exp.
3. They show that extracting possible scopes for conjuncts from a partial parse is possible.
The difference in F-score between this experiment and the baseline reaches 5.93 percent points.
The F-score is also minimally higher than the F-score for experiment 2 (gold scope), and recall is increased by approximately 1 percent point (even though only 12.5% of the sentences were reranked).
This can be attributed to two factors: First, we provide different scope possibilities.
This means that if the correct scope is not covered by the grammar, the parser may still be able to parse the next closest possibility instead of failing completely.
Second, reranking is not specifically geared towards improving coordinated structures.
Thus, it is possible that a parse is reranked higher because of some other feature.
It is, however, not the case that the improvement results completely from reranking.
This can be deduced from two points: First, while the F-score for experiment 1 (50-best analyses plus reranking) and the present experiment are very close (74.53 vs. 74.69), there are again differences in precision and recall: In experiment 1, recall is higher, and in the present experiment precision.
Second, a look at the evaluation on only sentences with coordination shows that the F-score for the present experiment is higher than the one for experiment 1 (72.14 vs. 71.68).
Additionally, precision for the present experiment is more than 2 percent points higher.
<newSection> 7 Experiment 4: Combining n-Best As described above, the results for reranking the 50-best analyses and for reranking the versions with automatically extracted scope readings are very close.
This raises the question whether the two methods produce similar improvements in the parse trees.
One indicator that this is not the case can be found in the differences in precision and recall.
Another possibility of verifying our assumption that the improvements do not overlap lies in the combination of the 50-best parses with the parses resulting from the automatically extracted scopes.
This increases the number of parses between which the reranker can choose.
In effect, this means a combination of the methods of experiments 1 (n-best) and 3 (automatic scope).
Consequently, if the results from this experiment are very close to the results from experiment 1 (n-best), we can conclude that adding the parses with automatic scope readings does not add new information.
If, however, adding these parses improves results, we can conclude that new information was present in the parses with automatic scope that was not covered in the 50-best parses.
Note that the combination of the two types of input for the reranker should not be regarded as a parser ensemble but rather as a resampling of the n-best search space since both parsers use the same grammar, parsing model, and probability model.
The only difference is that LoPar can accept partially bracketed input, and BitPar can list the n-best analyses.
The results of this experiment are shown in Table 1 as exp.
4. For all sentences, both precision and recall are higher than for experiment 1 and 3, resulting in an F-score of 76.69.
This is more than 2 percent points higher than for the 50-best parses.
This is a very clear indication that the parses contributed by the automatically extracted scopes provide parses that were not present in the 50 best parses from experiment 1 (n-best).
The same trend can be seen in the evaluation of the sentences containing coordination: Here, the improvement in F-score is higher than for the whole set, a clear indication that this method is suitable for improving coordination parsing.
A comparison of the results of the present experiment and experiment 3 (with automatic scope only) shows that the gain in precision is rather small, but the combination clearly improves recall, from 73.96% to 77.23%.
We can conclude that adding the 50 best parses remedies the lacking coverage that was the problem of experiment 3.
More generally, experiment 4 suggests that for the notoriously difficult problem of parsing coordination structures, a hybrid approach that combines parse selection of n best analyses with pre-bracketed scope in the input results in a considerable reduction in error rate compared to each of these methods used in isolation.
<newSection> 8 Related Work Parsing of coordinate structures for English has received considerable attention in computational linguistics.
Collins (1999), among many other authors, reports in the error analysis of his WSJ parsing results that coordination is one of the most frequent cases of incorrect parses, particularly if the conjuncts involved are complex.
He manages to reduce errors for simple cases of NP coordination by introducing a special phrasal category of base NPs.
In the experiments presented above, no explicit distinction is made between simple and complex cases of coordination, and no transformations are performed on the treebank annotations used for training.
Our experiment 1, reranking 50-best parses, is similar to the approaches of Charniak and Johnson (2005) and of Hogan (2007).
However, it differs from their experiments in two crucial ways: 1) Compared to Charniak and Johnson, who use 1.1 mio.
features, our feature set is appr.
five times larger (more than 5 mio. features), with the same threshold of at least five occurrences in the training set.
2) Both Hogan and Charniak and Johnson use special features for coordinate structures, such as a Boolean feature for marking parallelism (Charniak and Johnson) or for distinguishing between coordination of base NPs and coordination of complex conjuncts (Hogan), while our approach refrains from such special-purpose features.
Our experiments using scope information are similar to the approaches of Kurohashi and Na-gao (1994) and Agarwal and Bogges (1992) in that they try to identify coordinate structure bracket-ings.
However, the techniques used by Agarwal and Bogges and in the present paper are quite different.
Agarwal and Bogges and Kurohashi and Nagao rely on shallow parsing techniques to detect parallelism of conjuncts while we use a partial parser only for suggesting possible scopes of conjuncts.
Both of these approaches are limited to coordinate structures with two conjuncts only, while our approach has no such limitation.
Moreover, the goal of Agarwal and Bogges is quite different from ours.
Their goal is robust detection of coordinate structures only (with the intended application of term extraction), while our goal is to improve the performance of a parser that assigns a complete sentence structure to an input sentence.
Finally, our approach at present is restricted to purely syntactic structural properties.
This is in contrast to approaches that incorporate semantic information.
Hogan (2007) uses bi-lexical head-head co-occurrences in order to identify nominal heads of conjuncts more reliably than by syntactic information alone.
Chantree et al.
(2005) resolve attachment ambiguities in coordinate structures, as in (7a) and (7b), by using word frequency information obtained from generic corpora as an effective estimate of the semantic compatibility of a modifier vis-`a-vis the candidate heads.
(7) a.
Project managers and designers b.
Old shoes and boots We view the work by Hogan and by Chantree et al. as largely complementary to, but at the same time as quite compatible with our approach.
We must leave the integration of structural syntactic and lexical semantic information to future research.
<newSection> 9 Conclusion and Future Work We have presented a study on improving the treatment of coordinated structures in PCFG parsing.
While we presented experiments for German, the methods are applicable for any language.
We have chosen German because it is a language with relatively flexible phrasal ordering (cf. Section 2) which makes parsing coordinations particularly challenging.
The experiments presented show that n-best parsing combined with reranking improves results by a large margin.
However, the number of cases in which the correct parse is present in the n-best parses is rather low.
This led us to the assumption that the n-best analyses often do not cover the whole range of different scope possibil-ities but rather present minor variations of parses with few differences in coordination scope.
The experiments in which the parser was forced to assume predefined scopes show that the scope information is important for parsing quality.
Providing the parser with different scope possibilities and reranking the resulting parses results in an increase in F-score from 69.76 for the baseline to 74.69.
One of the major challenges for this approach lies in extracting a list of possible conjuncts.
Forcing the parser to parse all possible sequences results in a prohibitively large number of possibilities, especially for sentences with 3 or more conjunctions.
For this reason, we used chunks above base phases, such as coordinated noun chunks, to restrict the space.
However, an inspection of the lists of bracketed versions of the sentences shows that the definition of base phrases is one of the areas that must be refined.
As mentioned above, the partial parser groups sequences of ”NP KON NP” into a single base phrase.
This may be correct in many cases, but there are exceptions such as (8).
(8) Die 31j¨ahrige Gewerkschaftsmitarbei-The 31-year-old union staff member For (8), the partial parser groups Die 31j¨ahrige Gewerkschaftsmitarbeiterin und ausgebildete In-dustriekauffrau as one noun chunk.
Since our proposed conjuncts cannot cross these boundaries, the correct second conjunct, ausgebildete Indus-triekauffrau aus Oldenburg, cannot be suggested.
However, if we remove these chunk boundaries, the number of possible conjuncts increases dramatically, and parsing times become prohibitive.
As a consequence, we will need to find a good balance between these two needs.
Our plan is to increase flexibility very selectively, for example by enabling the use of wider scopes in cases where the conjunction is preceded and followed by base noun phrases.
For the future, we are planning to repeat experiment 3 (automatic scope) with different phrasal boundaries extracted from the partial parser.
It will be interesting to see if improvements in this experiment will still improve results in experiment 4 (combining 50-best parses with exp. 3).
Another area of improvement is the list of features used for reranking.
At present, we use a feature set that is similar to the one used by Collins and Koo (2005).
However, this feature set does not contain any coordination specific features.
We are planning to extend the feature set by features on structural parallelism as well as on lexical similarity of the conjunct heads.
<newSection> References<newSection>                 Abstract              The paper presents a multi-document summarization system which builds company-specific summaries from a collection of financial news such that the extracted sentences contain novel and relevant information about the corresponding organization.
The user’s familiarity with the com                pany’s profile is assumed.
The goal of                 such summaries is to provide information useful for the short-term trading of the cor                responding company, i.e., to facilitate the inference from news to stock price move                ment in the next day.
We introduce a novel query (i.e., company name) expansion method and a simple unsupervized algorithm for sentence ranking.
The system shows promising results in compari                son with a competitive baseline.
<newSection>                 1 Introduction                              Automatic text summarization has been a field of active research in recent years.
While most methods are extractive, the implementation details differ considerably depending on the goals of a sum                marization system.
Indeed, the intended use of the summaries may help significantly to adapt a par                ticular summarization approach to a specific task whereas the broadly defined goal of preserving rel                evant, although generic, information may turn out                 to be of little use.
In this paper we present a system whose goal is                 to extract sentences from a collection of financial              news to inform about important events concern                ing companies, e.g., to support trading (i.e., buy or                 sell) the corresponding symbol on the next day, or                 managing a portfolio.
For example, a company’s                 announcement of surpassing its earnings’ estimate                 is likely to have a positive short-term effect on its                 stock price, whereas an announcement of job cuts                 is likely to have the reverse effect.
We demonstrate                 how existing methods can be extended to achieve                 precisely this goal.
In a way, the described task can be classified                 as query-oriented multi-document summarization                 because we are mainly interested in information                 related to the company and its sector.
However,                 there are also important differences between the                 two tasks.
• The name of the company is not a query,                 e.g., as it is specified in the context of the DUC competitions1, and requires an exten                sion.
Initially, a query consists exclusively                 of the “symbol”, i.e., the abbreviation of the                 name of a company as it is listed on the stock market.
For example, WPO is the abbrevia                tion used on the stock market to refer to The Washington Post–a large media and education company.
Such symbols are rarely en                countered in the news and cannot be used to                 find all the related information.
• The summary has to provide novel informa                tion related to the company and should avoid general facts about it which the user is sup                posed to know.
This point makes the task                 related to update summarization where one                 has to provide the user with new information                              given some background knowledge2.
In our case, general facts about the company are as                sumed to be known by the user.
Given WPO, we want to distinguish between The Wash                ington Post is owned by The Washington Post                 Company, a diversified education and media                 company and The Post recently went through                 its third round of job cuts and reported an                 11% decline in print advertising revenues for                 its first quarter, the former being an example of background information whereas the lat                ter is what we would like to appear in the                 summary.
Thus, the similarity to the query alone is not the decisive parameter in com                puting sentence relevance.
• While the summaries must be specific for a given organization, important but general fi                nancial events that drive the overall market must be included in the summary.
For example, the recent subprime mortgage crisis af                fected the entire economy regardless of the                 sector.
Our system proceeds in the three steps illus                trated in Figure 1.
First, the company symbol is expanded with terms relevant for the company, ei                ther directly – e.g., iPod is directly related to Apple                 Inc.
– or indirectly – i.e., using information about                 the industry or sector the company operates in.
We                 detail our symbol expansion algorithm in Section 3.
Second, this information is used to rank sen                tences based on their relatedness to the expanded query and their overall importance (Section 4).
Fi                nally, the most relevant sentences are re-ranked                 based on the degree of novelty they carry (Section                 5).
The paper makes the following contributions.
First, we present a new query expansion technique which is useful in the context of company-                dependent news summarization as it helps identify                 sentences important to the company.
Second, we introduce a simple and efficient method for sentence ranking which foregrounds novel informa                tion of interest.
Our system performs well in terms of the ROUGE score (Lin & Hovy, 2003) com                pared with a competitive baseline (Section 6).
<newSection>                 2 Data                              The data we work with is a collection of financial news consolidated and distributed by Yahoo!
Fi-             nance3 from various sources4.
Each story is la                beled as being relevant for a company – i.e., it                 appears in the company’s RSS feed – if the story                 mentions either the company itself or the sector the company belongs to.
Altogether the corpus con                tains 88,974 news articles from a period of about                 5 months (148 days).
Some articles are labeled                 as being relevant for several companies.
The total                 number of (company name, news collection) pairs                 is 46,444.
The corpus is cleaned of HTML tags, embed                ded graphics and unrelated information (e.g., ads,                 frames) with a set of manually devised rules.
The                 filtering is not perfect but removes most of the                 noise.
Each article is passed through a language                 processing pipeline (described in (Atserias et al.,                 2008)).
Sentence boundaries are identified by                 means of simple heuristics.
The text is tokenized according to Penn TreeBank style and each to                ken lemmatized using Wordnet’s morphological                 functions.
Part of speech tags and named entities                 (LOC, PER, ORG, MISC) are identified by means of a publicly available named-entity tagger5 (Cia-                ramita & Altun, 2006, SuperSense).
Apart from                 that, all sentences which are shorter than 5 tokens                 and contain neither nouns nor verbs are sorted out.
We apply the latter filter as we are interested in                 textual information only.
Numeric information                 contained, e.g., in tables can be easily and more                 reliably obtained from the indices tables available                 online.
<newSection>                 3 Query Expansion              In company-oriented summarization query expansion is crucial because, by default, our query con                tains only the symbol, that is the abbreviation of the name of the company.
Unfortunately, exist                ing query expansion techniques which utilize such                 knowledge sources as WordNet or Wikipedia are                 not useful for symbol expansion.
WordNet does                 not include organizations in any systematic way.
Wikipedia covers many companies but it is unclear                 how it can be used for expansion.
Intuitively, a good expansion method should                 provide us with a list of products, or properties, of the company, the field it operates in, the typi                cal customers, etc.
Such information is normally                 found on the profile page of a company at Yahoo!
Finance6.
There, so called “business summaries” provide succinct and financially relevant informa                tion about the company.
Thus, we use business summaries as follows.
For every company sym                bol in our collection, we download its business                 summary, split it into tokens, remove all words                 but nouns and verbs which we then lemmatize.
Since words like company are fairly uninforma-                tive in the context of our task, we do not want to                 include them in the expanded query.
To filter out                 such words, we compute the company-dependent                 TF*IDF score for every word on the collection of                 all business summaries:                              where c is the business summary of a company,                 tfw,c is the frequency of w in c, N is the total                 number of business summaries we have, cfw is                 the number of summaries that contain w.
This formula penalizes words occurring in most sum                maries (e.g., company, produce, offer, operate, found, headquarter, management).
At the mo                ment of running the experiments, N was about 3,000, slightly less than the total number of sym-                6http://finance.yahoo.com/q/pr?s=AAPL                 where the trading symbol of any company can be used                 instead of AAPL.
bols because some companies do not have a business summary on Yahoo!
Finance.
It is impor                tant to point out that companies without a business summary are usually small and are seldom mentioned in news articles: for example, these compa                nies had relevant news articles in only 5% of the                 days monitored in this work.
Table 1 gives the ten high scoring words for                 three companies (Apple Inc.
– the computer and software manufacture, Delta Air Lines – the air                line, and DaVita – dyalisis services).
Table 1                 shows that this approach succeeds in expanding the symbol with terms directly related to the company, e.g., ipod for Apple, but also with more gen                eral information like the industry or the company                 operates in, e.g., software and computer for Apple.
All words whose TF*IDF score is above a certain                 threshold 0 are included in the expanded query (0                 was tuned to a value of 5.0 on the development                 set).
<newSection>                 4 Relatedness to Query                              Once the expanded query is generated, it can be                 used for sentence ranking.
We chose the system of                 Otterbacher et al.
(2005) as a a starting point for                 our approach and also as a competitive baseline because it has been successfully tested in a simi                lar setting–it has been applied to multi-document                 query-focused summarization of news documents.
Given a graph G = (5, E), where 5 is the set                 of all sentences from all input documents, and E is                 the set of edges representing normalized sentence similarities, Otterbacher et al.
(2005) rank all sen             as in the query are stemmed and stopwords are removed from them).
Relevance to the query is de                fined in Equation (6) which has been previously                 used for sentence retrieval (Allan et al., 2003):                              tence nodes based on the inter-sentence relations                 as well as the relevance to the query q.
Sentence                 ranks are found iteratively over the set of graph                 nodes with the following formula:              The first term represents the importance of a sen                tence defined in respect to the query, whereas the                 second term infers the importance of the sentence from its relation to other sentences in the collec                tion.
A ∈ (0, 1) determines the relative importance                 of the two terms and is found empirically.
Another parameter whose value is determined experimen-                tally is the sentence similarity threshold T, which determines the inclusion of a sentence in G.
Ot-                terbacher et al.
(2005) report 0.2 and 0.95 to be                 the optimal values for T and A respectively.
These                 values turned out to produce the best results also                 on our development set and were used in all our experiments.
Similarity between sentences is de                fined as the cosine of their vector representations:                              where tfw,s is the frequency of w in sentence s, |S |is the total number of sentences in the docu                ments from which sentences are to be extracted,                 and sfw is the number of sentences which contain                 the word w (all words in the documents as well                 w∈q where tfw,x stands for the number of times w ap                pears in x, be it a sentence (s) or the query (q).
If                 a sentence shares no words other than stopwords                 with the query, the relevance becomes zero.
Note that without the relevance to the query part Equation 2 takes only inter-sentence similarity into ac                count and computes the weighted PageRank (Brin                 & Page, 1998).
In defining the relevance to the query, in Equa                tion (6), words which do not appear in too many                 sentences in the document collection weigh more.
Indeed, if a word from the query is contained in                 many sentences, it should not count much.
But it                 is also true that not all words from the query are                 equally important.
As it has been mentioned in                 Section 3, words like product or offer appear in                 many business summaries and are equally related                 to any company.
To penalize such words, when                 computing the relevance to the query, we multiply the relevance score of a given word w with the in                verted document frequency of w on the corpus of                 business summaries Q – idfw,Q:                              We also replace tfw,s with the indicator function s(w) since it has been reported to be more ad                equate for sentences, in particular for sentence                 alignment (Nelken & Shieber, 2006):                              Thus, the modified formula we use to compute                 sentence ranks is as follows:                              We call these two ranking algorithms that use                 the formula in (2) OTTERBACHER and QUERY WEIGHTS, the difference being the way the rel                evance to the query is computed: (6) or (9).
We                 use the OTTERBACHER algorithm as a baseline in                 the experiments reported in Section 6.
<newSection>                 5 Novelty Bias              Apart from being related to the query, a good summary should provide the user with novel infor                mation.
According to Equation (2), if there are,                 say, two sentences which are highly similar to the                 query and which share some words, they are likely                 to get a very high score.
Experimenting with the                 development set, we observed that sentences about the company, such as e.g., DaVita, Inc. is a lead                ing provider of kidney care in the United States, providing dialysis services and education for patients with chronic kidney failure and end stage re                nal disease, are ranked high although they do not                 contribute new information.
However, a non-zero                 similarity to the query is indeed a good filter of the information related to the company and to its sec                tor and can be used as a prerequisite of a sentence                 to be included in the summary.
These observations                 motivate our proposal for a ranking method which                 aims at providing relevant and novel information                 at the same time.
Here, we explore two alternative approaches to                 add the novelty bias to the system:                 • The first approach bypasses the relatedness to query step introduced in Section 4 completely.
Instead, this method merges the dis                covery of query relatedness and novelty into                 a single algorithm, which uses a sentence graph that contains edges only between sen                tences related to the query, (i.e., sentences for                 which rel(s|q) > 0).
All edges connecting                 sentences which are unrelated to the query                 are skipped in this graph.
In this way we limit the novelty ranking process to a subset of sen                tences related to the query.
• The second approach models the problem                 in a re-ranking architecture: we take the top ranked sentences after the relatedness-toquery filtering component (Section 4) and re-rank them using the novelty formula intro                duced below.
The main difference between the two approaches                 is that the former uses relatedness-to-query and novelty information but ignores the overall importance of a sentence as given by the PageRank al                gorithm in Section 4, while the latter combines all these aspects –i.e., importance of sentences, relat                edness to query, and novelty– using the re-ranking                 architecture.
To amend the problem of general information ranked inappropriately high, we modify the word-weighting formula (4) so that it implements a nov                elty bias, thus becoming dependent on the query.
A straightforward way to define the novelty weight                 of a word would be to draw a line between the “known” words, i.e., words appearing in the busi                ness summary, and the rest.
In this approach all                 the words from the business summary are equally                 related to the company and get the weight of 0:                              We call this weighting scheme SIMPLE.
As an alternative, we also introduce a more elab                orate weighting procedure which incorporates                 the relatedness-to-query (or rather distance from                 query) in the word weight formula.
Intuitively, the                 more related to the query a word is (e.g., DaVita,                 the name of the company), the more familiar to the                 user it is and the smaller its novelty contribution                 is.
If a word does not appear in the query at all, its                 weight becomes equal to the usual tfw,sidfw,S:                              The overall novelty ranking formula is based                 on the query-dependent PageRank introduced in Equation (2).
However, since we already incorporate the relatedness to the query in these two set                tings, we focus only on related sentences and thus                 may drop the relatedness to the query part from                 (2):                              We set A to the same value as in OTTERBACHER.
We deliberately set the sentence similarity thresh                old T to a very low value (0.05) to prevent the                 graph from becoming exceedingly bushy.
Note                 that this novelty-ranking formula can be equally applied in both scenarios introduced at the begin                ning of this section.
In the first scenario, S stands                 for the set of nodes in the graph that contains only sentences related to the query.
In the second sce                nario, S contains the highest ranking sentences                 detected by the relatedness-to-query component                 (Section 4).
Some sentences are repeated several times in the                 collection.
Such repetitions, which should be avoided in the summary, can be filtered out either before or after the sentence ranking.
We ap                ply a simple repetition check when incrementally adding ranked sentences to the summary.
If a sen                tence to be added is almost identical to the one already included in the summary, we skip it.
Iden                tity check is done by counting the percentage of                 non-stop word lemmas in common between two                 sentences.
95% is taken as the threshold.
We do not filter repetitions before the rank                ing has taken place because often such repetitions carry important and relevant information.
The redundancy filter is applied to all the systems described as they are equally prone to include repe                titions.                              and human annotators jackknife for each (query, summary) pair and computed a macro-average to make human and au                tomatic results comparable (Dang, 2005).
The scores computed on summaries produced by hu                mans are given in the bottom line (MANUAL) and                 serve as upper bound and also as an indicator for                 the inter-annotator agreement.
<newSection>                 6 Evaluation                              We randomly selected 23 company stock names,                 and constructed a document collection for each containing all the news provided in the Yahoo!
Fi                nance news feed for that company in a period of                 two days (the time period was chosen randomly).
The average length of a news collection is about                 600 tokens.
When selecting the company names,                 we took care of not picking those which have only                 a few news articles for that period of time.
This resulted into 9.4 news articles per collection on av                erage.
From each of these, three human annotators independently selected up to ten sentences.
All an                notators had average to good understanding of the                 financial domain.
The annotators were asked to                 choose the sentences which could best help them                 decide whether to buy, sell or retain stock for the                 company the following day and present them in the order of decreasing importance.
The anno                tators compared their summaries of the first four collections and clarified the procedure before proceeding with the other ones.
These four collec                tions were then later used as a development set.
All summaries – manually as well as automat                ically generated – were cut to the first 250 words                 which made the summaries 10 words shorter on average.
We evaluated the performance automat                ically in terms of ROUGE-2 (Lin & Hovy, 2003) using the parameters and following the methodology from the DUC events.
The results are presented in Table 2.
We also report the 95% confi                dence intervals in brackets.
As in DUC, we used                              From Table 2 follows that the modifications we                 applied to the baseline are sensible and indeed bring an improvement.
QUERY WEIGHTS per                forms better than OTTERBACHER and is in turn outperformed by the algorithms biased to novel information (the two NOVELTY systems).
The overlap between the confidence intervals of the baseline and the simple version of the novelty algo                rithm is minimal (0.002).
It is remarkable that the achieved improvement                 is due to a more balanced relatedness to the query ranking (9), as well as to the novelty bias reranking.
The fact that the simpler novelty weight                ing formula (10) produced better results than the more elaborated one (11) requires a deeper anal                ysis and a larger test set to explain the difference.
Our conjecture so far is that the SIMPLE approach                 allows for a better combination of both novelty                 and relatedness to query.
Since the more complex                 novelty ranking formula penalizes terms related                 to the query (Equation (11)), it favors a scenario where novelty is boosted in detriment of related                ness to query, which is not always realistic.
It is important to note that, compared with the                 baseline, we did not do any parameter tuning for                 λ and the inter-sentence similarity threshold.
The                 improvement between the system of Otterbacher                 et al.
(2005) and our best model is statistically                 significant.
Recall from Section 5 that the motivation for pro                moting novel information came from the fact that                 sentences with background information about the company obtained very high scores: they were related but not novel.
The sentences ranked by OT-TERBACHER or QUERY WEIGHTS required a re                ranking to include related and novel sentences in the summary.
We checked whether novelty re                ranking brings an improvement if added on top                 of a system which does not have a novelty bias                 (baseline or QUERY WEIGHTS) and compared it                 with the setting where we simply limit the novelty                 ranking to all the sentences related to the query (NOVELTY SIMPLE and NOVELTY).
In the simi                larity graph, we left only edges between the first                 30 sentences from the ranked list produced by                 one of the two algorithms described in Section 4                 (OTTERBACHER or QUERY WEIGHTS).
Then we                 ranked the sentences biased to novel information the same way as described in Section 5.
The results are presented in Table 3.
What we evalu                ate here is whether a combination of two methods performs better than the simple heuristics of dis                carding edges between sentences unrelated to the                 query.
From the four possible combinations, there is                 an improvement over the baseline only (0.255 vs. 0.280 resp. 0.273).
None of the combinations performs better than the simple novelty bias algorithm on a subset of edges.
This experiment sug                gests that, at least in the scenario investigated here (short-term monitoring of publicly-traded compa                nies), novelty is more important than relatedness to query.
Hence, the simple novelty bias algo                rithm, which emphasizes novelty and incorporates relatedness to query only through a loose constraint (rel(slq) > 0) performs better than com                plex models, which are more constrained by the                 relatedness to query.
<newSection>                 7 Related Work                              Summarization has been extensively investigated in recent years and to date there exists a multi                tude of very different systems.
Here, we review                 those that come closest to ours in respect to the                 task and that concern extractive multi-document                 query-oriented summarization.
We also mention                 some work on using textual news data for stock                 indices prediction which we are aware of.
Stock market prediction: W¨uthrich et al.
(1998) were among the first who introduced an automatic stock indices prediction system which relies on textual information only.
The system gen                erates weighted rules each of which returns the probability of the stock going up, down or remain                ing steady.
The only information used in the rules                 is the presence or absence of certain keyphrases                 provided by a human expert who “judged them                 to be influential factors potentially moving stock markets”.
In this approach, training data is re                quired to measure the usefulness of the keyphrases for each of the three classes.
More recently, Ler-                man et al.
(2008) introduced a forecasting system for prediction markets that combines news analysis with a price trend analysis model.
This approach was shown to be successful for the forecasting of public opinion about political candi                dates in such prediction markets.
Our approach can be seen as a complement to both these approaches, necessary especially for financial mar                kets where the news typically cover many events,                 only some related to the company of interest.
Unsupervized summarization systems extract                 sentences whose relevance can be inferred from the inter-sentence relations in the document col                lection.
In (Radev et al., 2000), the centroid of                 the collection, i.e., the words with the highest                 TF*IDF, is considered and the sentences which contain more words from the centroid are extracted.
Mihalcea & Tarau (2004) explore sev                eral methods developed for ranking documents                 in information retrieval for the single-document                 summarization task.
Similarly, Erkan & Radev                 (2004) apply in-degree and PageRank to build a                 summary from a collection of related documents.
They show that their method, called LexRank,                 achieves good results.
In (Otterbacher et al., 2005;                 Erkan, 2006) the ranking function of LexRank is                 extended to become applicable to query-focused summarization.
The rank of a sentence is deter                mined not just by its relation to other sentences in                              the document collection but also by its relevance                 to the query.
Relevance to the query is defined as the word-based similarity between query and sen                tence.
Query expansion has been used for improving information retrieval (IR) or question answer                ing (QA) systems with mixed results.
One of the                 problems is that the queries are expanded word                 by word, ignoring the context and as a result the                 extensions often become inadequate7.
However, Riezler et al.
(2007) take the entire query into account when adding new words by utilizing tech                niques used in statistical machine translation.
Query expansion for summarization has not yet                 been explored as extensively as in IR or QA.
Nastase (2008) uses Wikipedia and WordNet for                 query expansion and proposes that a concept can be expanded by adding the text of all hyper-                links from the first paragraph of the Wikipedia article about this concept.
The automatic evaluation demonstrates that extracting relevant con                cepts from Wikipedia leads to better performance                 compared with WordNet: both expansion systems                 outperform the no-expansion version in terms of                 the ROUGE score.
Although this method proved                 helpful on the DUC data, it seems less appropriate for expanding company names.
For small compa                nies there are short articles with only a few links;                 the first paragraphs of the articles about larger                 companies often include interesting rather than relevant information.
For example, the text pre                ceding the contents box in the article about Apple                 Inc.
(AAPL) states that “Fortune magazine named                 Apple the most admired company in the United States”8.
The link to the article about the For                tune magazine can be hardly considered relevant                 for the expansion of AAPL.
Wikipedia category                 information, which has been successfully used in                 some NLP tasks (Ponzetto & Strube, 2006, inter                 alia), is too general and does not help discriminate                 between two companies from the same sector.
Our work suggests that query expansion is                 needed for summarization in the financial domain.
In addition to previous work, we also show that an                other key factor for success in this task is detecting                 and modeling the novelty of the target content.
7E.g., see the proceedings of TREC 9, TREC 10: http:                 //trec.nist.gov.
<newSection>                 8 Conclusions                              In this paper we presented a multi-document                 company-oriented summarization algorithm                 which extracts sentences that are both relevant for                 the given organization and novel to the user.
The                 system is expected to be useful in the context of                 stock market monitoring and forecasting, that is,                 to help the trader predict the move of the stock                 price for the given company.
We presented a novel query expansion method which works par                ticularly well in the context of company-oriented                 summarization.
Our sentence ranking method is                 unsupervized and requires little parameter tuning.
An automatic evaluation against a competitive                 baseline showed supportive results, indicating that                 the ranking algorithm is able to select relevant                 sentences and promote novel information at the                 same time.
In the future, we plan to experiment with po                sitional features which have proven useful for                 generic summarization.
We also plan to test the                 system extrinsically.
For example, it would be of                 interest to see if a classifier may predict the move                 of stock prices based on a set of features extracted                 from company-oriented summaries.
Acknowledgments: We would like to thank the                 anonymous reviewers for their helpful feedback.
<newSection>                 References<newSection> Abstract Sense induction seeks to automatically identify word senses directly from a corpus.
A key assumption underlying previous work is that the context surrounding an ambiguous word is indicative of its meaning.
Sense induction is thus typically viewed as an unsupervised clustering problem where the aim is to partition a word’s contexts into different classes, each representing a word sense.
Our work places sense induction in a Bayesian context by modeling the contexts of the ambiguous word as samples from a multinomial distribution over senses which are in turn characterized as distributions over words.
The Bayesian framework provides a principled way to incorporate a wide range of features beyond lexical co-occurrences and to systematically assess their utility on the sense induction task.
The proposed approach yields improvements over state-of-the-art systems on a benchmark dataset.
<newSection> 1 Introduction Sense induction is the task of discovering automatically all possible senses of an ambiguous word.
It is related to, but distinct from, word sense disambiguation (WSD) where the senses are assumed to be known and the aim is to identify the intended meaning of the ambiguous word in context.
Although the bulk of previous work has been devoted to the disambiguation problem1, there are good reasons to believe that sense induction may be able to overcome some of the issues associated with WSD.
Since most disambiguation methods assign senses according to, and with the aid of, dictionaries or other lexical resources, it is difficult to adapt them to new domains or to languages where such resources are scarce.
A related problem concerns the granularity of the sense distinctions which is fixed, and may not be entirely suitable for different applications.
In contrast, when sense distinctions are inferred directly from the data, they are more likely to represent the task and domain at hand.
There is little risk that an important sense will be left out, or that irrelevant senses will influence the results.
Furthermore, recent work in machine translation (Vickrey et al., 2005) and information retrieval (V´eronis, 2004) indicates that induced senses can lead to improved performance in areas where methods based on a fixed sense inventory have previously failed (Carpuat and Wu, 2005; Voorhees, 1993).
Sense induction is typically treated as an unsupervised clustering problem.
The input to the clustering algorithm are instances of the ambiguous word with their accompanying contexts (represented by co-occurrence vectors) and the output is a grouping of these instances into classes corresponding to the induced senses.
In other words, contexts that are grouped together in the same class represent a specific word sense.
In this paper we adopt a novel Bayesian approach and formalize the induction problem in a generative model.
For each ambiguous word we first draw a distribution over senses, and then generate context words according to this distribution.
It is thus assumed that different senses will correspond to distinct lexical distributions.
In this framework, sense distinctions arise naturally through the generative process: our model postulates that the observed data (word contexts) are explicitly intended to communicate a latent structure (their meaning).
Our work is related to Latent Dirichlet Allocation (LDA, Blei et al. 2003), a probabilistic model of text generation.
LDA models each document using a mixture over K topics, which are in turn characterized as distributions over words.
The words in the document are generated by repeatedly sampling a topic according to the topic distribution, and selecting a word given the chosen topic.
Whereas LDA generates words from global topics corresponding to the whole document, our model generates words from local topics chosen based on a context window around the ambiguous word.
Document-level topics resemble general domain labels (e.g., finance, education) and cannot faithfully model more fine-grained meaning distinctions.
In our work, therefore, we create an individual model for every (ambiguous) word rather than a global model for an entire document collection.
We also show how multiple information sources can be straightforwardly integrated without changing the underlying probabilistic model.
For instance, besides lexical information we may want to consider parts of speech or dependencies in our sense induction problem.
This is in marked contrast with previous LDA-based models which mostly take only word-based information into account.
We evaluate our model on a recently released benchmark dataset (Agirre and Soroa, 2007) and demonstrate improvements over the state-of-the-art.
The remainder of this paper is structured as follows.
We first present an overview of related work (Section 2) and then describe our Bayesian model in more detail (Sections 3 and 4).
Section 5 describes the resources and evaluation methodology used in our experiments.
We discuss our results in Section 6, and conclude in Section 7.
<newSection> 2 Related Work Sense induction is typically treated as a clustering problem, where instances of a target word are partitioned into classes by considering their co-occurring contexts.
Considerable latitude is allowed in selecting and representing the co-occurring contexts.
Previous methods have used first or second order co-occurrences (Purandare and Pedersen, 2004; Sch¨utze, 1998), parts of speech (Purandare and Pedersen, 2004), and grammatical relations (Pantel and Lin, 2002; Dorow and Widdows, 2003).
The size of the context window also varies, it can be a relatively small, such as two words before and after the target word (Gauch and Futrelle, 1993), the sentence within which the target is found (Bordag, 2006), or even larger, such as the 20 surrounding words on either side of the target (Purandare and Pedersen, 2004).
In essence, each instance of a target word is represented as a feature vector which subsequently serves as input to the chosen clustering method.
A variety of clustering algorithms have been employed ranging from k-means (Purandare and Pedersen, 2004), to agglomerative clustering (Sch¨utze, 1998), and the Information Bottleneck (Niu et al., 2007).
Graph-based methods have also been applied to the sense induction task.
In this framework words are represented as nodes in the graph and vertices are drawn between the target and its co-occurrences.
Senses are induced by identifying highly dense subgraphs (hubs) in the co-occurrence graph (V´eronis, 2004; Dorow and Widdows, 2003).
Although LDA was originally developed as a generative topic model, it has recently gained popularity in the WSD literature.
The inferred document-level topics can help determine coarse-grained sense distinctions.
Cai et al.
(2007) propose to use LDA’s word-topic distributions as features for training a supervised WSD system.
In a similar vein, Boyd-Graber and Blei (2007) infer LDA topics from a large corpus, however for unsupervised WSD.
Here, LDA topics are integrated with McCarthy et al.’s (2004) algorithm.
For each target word, a topic is sampled from the document’s topic distribution, and a word is generated from that topic.
Also, a distributional neighbor is selected based on the topic and distributional similarity to the generated word.
Then, the word sense is selected based on the word, neighbor, and topic.
Boyd-Graber et al.
(2007) extend the topic modeling framework to include WordNet senses as a latent variable in the word generation process.
In this case the model discovers both the topics of the corpus and the senses assigned to each of its words.
Our own model is also inspired by LDA but crucially performs word sense induction, not disambiguation.
Unlike the work mentioned above, we do not rely on a pre-existing list of senses, and do not assume a correspondence between our automatically derived sense-clusters and those of any given inventory.2 A key element in these previous attempts at adapting LDA for WSD is the tendency to remain at a high level, document-like, setting.
In contrast, we make use of much smaller units of text (a few sentences, rather than a full document), and create an individual model for each (ambiguous) word type.
Our induced senses are few in number (typically less than ten).
This is in marked contrast to tens, and sometimes hundreds, of topics commonly used in document-modeling tasks.
Unlike many conventional clustering methods (e.g., Purandare and Pedersen 2004; Sch¨utze 1998), our model is probabilistic; it specifies a probability distribution over possible values, which makes it easy to integrate and combine with other systems via mixture or product models.
Furthermore, the Bayesian framework allows the incorporation of several information sources in a principled manner.
Our model can easily handle an arbitrary number of feature classes (e.g., parts of speech, dependencies).
This functionality in turn enables us to evaluate which linguistic information matters for the sense induction task.
Previous attempts to handle multiple information sources in the LDA framework (e.g., Griffiths et al. 2005; Barnard et al.
2003) have been task-specific and limited to only two layers of information.
Our model provides this utility in a general framework, and could be applied to other tasks, besides sense induction.
<newSection> 3 The Sense Induction Model The core idea behind sense induction is that contextual information provides important cues regarding a word’s meaning.
The idea dates back to (at least) Firth (1957) (“You shall know a word by the company it keeps”), and underlies most WSD and lexicon acquisition work to date.
Under this premise, we should expect different senses to be signaled by different lexical distributions.
We can place sense induction in a probabilistic setting by modeling the context words around the ambiguous target as samples from a multinomial sense distribution.
More formally, we will write P(s) for the distribution over senses s of an ambiguous target in a specific context window and P(w1s) for the probability distribution over context words w given sense s.
Each word wi in the context window is generated by first sampling a sense from the sense distribution, then choosing a word from the sense-context distribution.
P(si = j) denotes the probability that the jth sense was sampled for the ith word token and P(wi|si = j) the probability of context word wi under sense j.
The model thus specifies a distribution over words within a context window: where S is the number of senses.
We assume that each target word has C contexts and each context c cate conditional dependencies between variables, whereas plates (the rectangles in the figure) refer to repetitions of sampling steps.
The variables in the lower right corner refer to the number of samples.
consists of Nc word tokens.
We shall write �(j) as a shorthand for P(wi|si = j), the multinomial distribution over words for sense j, and 0(c) as a shorthand for the distribution of senses in context c.
Following Blei et al.
(2003) we will assume that the mixing proportion over senses 0 is drawn from a Dirichlet prior with parameters a.
The role of the hyperparameter a is to create a smoothed sense distribution.
We also place a symmetric Dirichlet R on � (Griffiths and Steyvers, 2002).
The hyper-parmeter R can be interpreted as the prior observation count on the number of times context words are sampled from a sense before any word from the corpus is observed.
Our model is represented in graphical notation in Figure 1.
The model sketched above only takes word information into account.
Methods developed for supervised WSD often use a variety of information sources based not only on words but also on lemmas, parts of speech, collocations and syntactic relationships (Lee and Ng, 2002).
The first idea that comes to mind, is to use the same model while treating various features as word-like elements.
In other words, we could simply assume that the contexts we wish to model are the union of all our features.
Although straightforward, this solution is undesirable.
It merges the distributions of distinct feature categories into a single one, and is therefore conceptually incorrect, and can affect the performance of the model.
For instance, parts-ofspeech (which have few values, and therefore high probability), would share a distribution with words (which are much sparser).
Layers containing more elements (e.g. 10 word window) would overwhelm rectangles represent different sources (layers) of information.
All layers share the same, instance-specific, sense distribution (0), but each have their own (multinomial) sense-feature distribution (�).
Shaded nodes represent observed features f; these can be words, parts of speech, collocations or dependencies.
unconditional joint distribution P(s) of the unobserved variables (provided certain criteria are fulfilled).
In our model, each element in each layer is a variable, and is assigned a sense label (see Figure 2, where distinct layers correspond to different representations of the context around the target word).
From these assignments, we must determine the sense distribution of the instance as a whole.
This is the purpose of the Gibbs sampling procedure.
Specifically, in order to derive the update function used in the Gibbs sampler, we must provide the conditional probability of the i-th variable being assigned sense si in layer l, given the feature value fi of the context variable and the current sense assignments of all the other variables in the data (s−i): p(si|s−i, f) — p(fi|s, f −i,R) · p(si|s−i,a) (2) The probability of a single sense assignment, si, is proportional to the product of the likelihood (of feature fi, given the rest of the data) and the prior probability of the assignment.
smaller ones (e.g. 1 word window).
Our solution is to treat each information source (or feature type) individually and then combine all of them together in a unified model.
Our underlying assumption is that the context window around the target word can have multiple representations, all of which share the same sense distribution.
We illustrate this in Figure 2 where each inner rectangle (layer) corresponds to a distinct feature type.
We will naively assume independence between multiple layers, even though this is clearly not the case in our task.
The idea here is to model each layer as faithfully as possible to the empirical data while at the same time combining information from all layers in estimating the sense distribution of each target instance.
<newSection> 4 Inference Our inference procedure is based on Gibbs sampling (Geman and Geman, 1984).
The procedure begins by randomly initializing all unobserved random variables.
At each iteration, each random variable si is sampled from the conditional distribution P(si|s−i) where s−i refers to all variables other than si.
Eventually, the distribution over samples drawn from this process will converge to the f p(fi |l, S, 0) · p(O |f-i, Rt)dO _ #(fi, si) + Rl For the likelihood term p(fi|s, f −i,R), integrating over all possible values of the multinomial feature-sense distribution � gives us the rightmost term in Equation 3, which has an intuitive interpretation.
The term #(fi,si) indicates the number of times the feature-value fi was assigned sense si in the rest of the data.
Similarly, #(si) indicates the number of times the sense assignment si was observed in the data.
Rl is the Dirichlet prior for the feature-sense distribution � in the current layer l, and Vl is the size of the vocabulary of that layer, i.e., the number of possible feature values in the layer.
Intuitively, the probability of a feature-value given a sense is directly proportional to the number of times we have seen that value and that sense-assignment together in the data, taking into account a pseudo-count prior, expressed through R.
This can also be viewed as a form of smoothing.
A similar approach is taken with regards to the prior probability p(si|s−i,a).
In this case, however, all layers must be considered: Here λl is the weight for the contribution of layer l, and αl is the portion of the Dirichlet prior for the sense distribution θ in the current layer.
Treating each layer individually, we integrate over the possible values of θ, obtaining a similar count-based term: where #l(si) indicates the number of elements in layer l assigned the sense si, #l indicates the number of elements in layer l, i.e., the size of the layer and S the number of senses.
To distribute the pseudo counts represented by α in a reasonable fashion among the layers, we define αl = #l #m · α where #m = ∑l #l, i.e., the total size of the instance.
This distributes α according to the relative size of each layer in the instance.
Placing these values in Equation 4 we obtain the following: #m+S·α Putting it all together, we arrive at the final update equation for the Gibbs sampling: Note that when dealing with a single layer, Equation 8 collapses to: where #m(si) indicates the number of elements (e.g., words) in the context window assigned to sense si.
This is identical to the update equation in the original, word-based LDA model.
The sampling algorithm gives direct estimates of s for every context element.
However, in view of our task, we are more interested in estimating θ, the sense-context distribution which can be obtained as in Equation 7, but taking into account all sense assignments, without removing assignment i.
Our system labels each instance with the single, most probable sense.
<newSection> 5 Evaluation Setup In this section we discuss our experimental set-up for assessing the performance of the model presented above.
We give details on our training procedure, describe our features, and explain how our system output was evaluated.
Data In this work, we focus solely on inducing senses for nouns, since they constitute the largest portion of content words.
For example, nouns represent 45% of the content words in the British National Corpus.
Moreover, for many tasks and applications (e.g., web queries, Jansen et al.
2000) nouns are the most frequent and most important part-of-speech.
For evaluation, we used the Semeval-2007 benchmark dataset released as part of the sense induction and discrimination task (Agirre and Soroa, 2007).
The dataset contains texts from the Penn Treebank II corpus, a collection of articles from the first half of the 1989 Wall Street Journal (WSJ).
It is hand-annotated with OntoNotes senses (Hovy et al., 2006) and has 35 nouns.
The average noun ambiguity is 3.9, with a high (almost 80%) skew towards the predominant sense.
This is not entirely surprising since OntoNotes senses are less fine-grained than WordNet senses.
We used two corpora for training as we wanted to evaluate our model’s performance across different domains.
The British National Corpus (BNC) is a 100 million word collection of samples of written and spoken language from a wide range of sources including newspapers, magazines, books (both academic and fiction), letters, and school essays as well as spontaneous conversations.
This served as our out-of-domain corpus, and contained approximately 730 thousand instances of the 35 target nouns in the Semeval lexical sample.
The second, in-domain, corpus was built from selected portions of the Wall Street Journal.
We used all articles (excluding the Penn Treebank II portion used in the Semeval dataset) from the years 1987-89 and 1994 to create a corpus of similar size to the BNC, containing approximately 740 thousand instances of the target words.
Additionally, we used the Senseval 2 and 3 lexical sample data (Preiss and Yarowsky, 2001; Mihalcea and Edmonds, 2004) as development sets, for experimenting with the hyper-parameters of our model (see Section 6).
Evaluation Methodology Agirre and Soroa (2007) present two evaluation schemes for assessing sense induction methods.
Under the first scheme, the system output is compared to the gold standard using standard clustering evaluation metrics (e.g., purity, entropy).
Here, no attempt is made to match the induced senses against the labels of the gold standard.
Under the second scheme, the gold standard is partitioned into a test and training corpus.
The latter is used to derive a mapping of the induced senses to the gold standard labels.
The mapping is then used to calculate the system’s F-Score on the test corpus.
Unfortunately, the first scheme failed to discriminate among participating systems.
The one-cluster-per-word baseline outperformed all systems, except one, which was only marginally better.
The scheme ignores the actual labeling and due to the dominance of the first sense in the data, encourages a single-sense approach which is further amplified by the use of a coarse-grained sense inventory.
For the purposes of this work, therefore, we focused on the second evaluation scheme.
Here, most of the participating systems outperformed the most-frequent-sense baseline, and the rest obtained only slightly lower scores.
Feature Space Our experiments used a feature set designed to capture both immediate local context, wider context and syntactic context.
Specifically, we experimented with six feature categories: ±10-word window (10w), ±5-word window (5w), collocations (1w), word n-grams (ng), part-ofspeech n-grams (pg) and dependency relations (dp).
These features have been widely adopted in various WSD algorithms (see Lee and Ng 2002 for a detailed evaluation).
In all cases, we use the lemmatized version of the word(s).
The Semeval workshop organizers provided a small amount of context for each instance (usually a sentence or two surrounding the sentence containing the target word).
This context, as well as the text in the training corpora, was parsed using RASP (Briscoe and Carroll, 2002), to extract part-of-speech tags, lemmas, and dependency information.
For instances containing more than one occurrence of the target word, we disambiguate the first occurrence.
Instances which were not correctly recognized by the parser (e.g., a target word labeled with the wrong lemma or part-of-speech), were automatically assigned to the largest sense-cluster.3 <newSection> 6 Experiments Model Selection The framework presented in Section 3 affords great flexibility in modeling the empirical data.
This however entails that several parameters must be instantiated.
More precisely, our model is conditioned on the Dirichlet hyper-parameters α and β and the number of senses S.
Additional parameters include the number of iterations for the Gibbs sampler and whether or not the layers are assigned different weights.
Our strategy in this paper is to fix α and β and explore the consequences of varying S.
The value for the α hyperparameter was set to 0.02.
This was optimized in an independent tuning experiment which used the Senseval 2 (Preiss and Yarowsky, 2001) and Senseval 3 (Mihalcea and Edmonds, 2004) datasets.
We experimented with α values ranging from 0.005 to 1.
The β parameter was set to 0.1 (in all layers).
This value is often considered optimal in LDA-related models (Griffiths and Steyvers, 2002).
For simplicity, we used uniform weights for the layers.
The Gibbs sampler was run for 2,000 iterations.
Due to the randomized nature of the inference procedure, all reported results are average scores over ten runs.
Our experiments used the same number of senses for all the words, since tuning this number individually for each word would be prohibitive.
We experimented with values ranging from three to nine senses.
Figure 3 shows the results obtained for different numbers of senses when the model is trained on the WSJ (in-domain) and BNC (out-ofdomain) corpora, respectively.
Here, we are using the optimal combination of layers for each system (which we discuss in the following section in detail).
For the model trained on WSJ, performance peaks at four senses, which is similar to the average ambiguity in the test data.
For the model trained on the BNC, however, the best results are obtained using twice as many senses.
Using fewer senses with the BNC-trained system can result in a drop in accuracy of almost 2%.
This is due to the shift in domain.
As the sense-divisions of the learning domain do not match those of the target domain, finer granularity is required in order to encompass all the relevant distinctions.
Table 1 illustrates the senses inferred for the word drug when using the in-domain and out-ofdomain corpora, respectively.
The most probable words for each sense are also shown.
Firstly, note that the model infers some plausible senses for drug on the WSJ corpus (top half of Table 1).
Sense 1 corresponds to the “enforcement” sense of drug, Sense 2 refers to “medication”, Sense 3 to the “drug industry” and Sense 4 to “drugs research”.
The inferred senses for drug on the BNC (bottom half of Table 1) are more fine grained.
For example, the model finds distinct senses for “medication” (Sense 1 and 7) and “illegal substance” (Senses 2, 4, 6, 7).
It also finds a separate sense for “drug dealing” (Sense 5) and “enforcement” (Sense 8).
Because the BNC has a broader focus, finer distinctions are needed to cover as many senses as possible that are relevant to the target domain (WSJ).
Layer Analysis We next examine which individual feature categories are most informative in our sense induction task.
We also investigate whether their combination, through our layered model (see Figure 2), yields performance improvements.
We used 4 senses for the system trained on WSJ and 8 for the system trained on the BNC (a was set to 0.02 and b to 0.1) Table 2 (left side) shows the performance of our model when using only one layer.
The layer composed of words co-occurring within a ±10-word window (10w), and representing wider, topical, information gives the highest scores on its own.
It is followed by the ±5 (5w) and ±1 (1w) word windows, which represent more immediate, local context.
Part-of-speech n-grams (pg) and word n-grams (ng), on their own, achieve lower scores, largely due to over-generalization and data sparseness, respectively.
The lowest-scoring single layer is the dependency layer (dp), with performance only slightly above the most-frequent-sense baseline (MFS).
Dependency information is very informative when present, but extremely sparse.
Table 2 (middle) also shows the results obtained when running the layered model with all but one of the layers as input.
We can use this information to determine the contribution of each layer by comparing to the combined model with all layers (all).
Because we are dealing with multiple layers, there is an element of overlap involved.
Therefore, each of the word-window layers, despite relatively high informativeness on its own, does not cause as much damage when it is absent, since the other layers compensate for the topical and local information.
The absence of the word n-gram layer, which provides specific local information, does not make a great impact when the 1w and pg layers are present.
Finally, we can see that the extremely sparse dependency layer is detrimental to the multi-layer model as a whole, and its removal increases performance.
The sparsity of the data in this layer means that there is often little information on which to base a decision.
In these cases, the layer contributes a close-to-uniform estimation of the sense distribution, which confuses the combined model.
Other layer combinations obtained similar results.
Table 2 (right side) shows the most informative two and three layer combinations.
Again, dependencies tend to decrease performance.
On the other hand, combining features that have similar performance on their own is beneficial.
We obtain the best performance overall with a two layered model combining topical (+10w) and local (+5w) contexts.
Table 3 replicates the same suite of experiments on the BNC corpus.
The general trends are similar.
Some interesting differences are apparent, however.
The sparser layers, notably word n-grams and dependencies, fare comparatively worse.
This is expected, since the more precise, local, information is likely to vary strongly across domains.
Even when both domains refer to the same sense of a word, it is likely to be used in a different immediate context, and local contextual information learned in one domain will be less effective in the other.
Another observable difference is that the combined model without the dependency layer does slightly better than each of the single layers.
The 1w+pg combination improves over its components, which have similar individual performance.
Finally, the best performing model on the BNC also combines two layers capturing wider (10w) and more local (5w) contextual information (see Table 3, right side).
Comparison to State-of-the-Art Table 4 compares our model against the two best performing sense induction systems that participated in the Semeval-2007 competition.
IR2 (Niu et al., 2007) performed sense induction using the Information Bottleneck algorithm, whereas UMND2 (Pedersen, 2007) used k-means to cluster second order co-occurrence vectors associated with the target word.
These models and our own model significantly outperform the most-frequent-sense baseline (p < 0.01 using a x2 test).
Our best system (10w+5w on WSJ) is significantly better than UMND2 (p < 0.01) and quantitatively better than IR2, although the difference is not statistically significant.
<newSection> 7 Discussion This paper presents a novel Bayesian approach to sense induction.
We formulated sense induction in a generative framework that describes how the contexts surrounding an ambiguous word might be generated on the basis of latent variables.
Our model incorporates features based on lexical information, parts of speech, and dependencies in a principled manner, and outperforms state-of-theart systems.
Crucially, the approach is not specific to the sense induction task and can be adapted for other applications where it is desirable to take multiple levels of information into account.
For example, in document classification, one could consider an accompanying image and its caption as possible additional layers to the main text.
In the future, we hope to explore more rigorous parameter estimation techniques.
Goldwater and Griffiths (2007) describe a method for integrating hyperparameter estimation into the Gibbs sampling procedure using a prior over possible values.
Such an approach could be adopted in our framework, as well, and extended to include the layer weighting parameters, which have strong potential for improving the model’s performance.
In addition, we could allow an infinite number of senses and use an infinite Dirichlet model (Teh et al., 2006) to automatically determine how many senses are optimal.
This provides an elegant solution to the model-order problem, and eliminates the need for external cluster-validation methods.
Acknowledgments The authors acknowledge the support of EPSRC (grant EP/C538447/1).
We are grateful to Sharon Goldwater for her feedback on earlier versions of this work.
<newSection> References<newSection> Abstract This paper examines unsupervised approaches to part-of-speech (POS) tagging for morphologically-rich, resource-scarce languages, with an emphasis on Goldwa-ter and Griffiths’s (2007) fully-Bayesian approach originally developed for English POS tagging.
We argue that existing unsupervised POS taggers unreal-istically assume as input a perfect POS lexicon, and consequently, we propose a weakly supervised fully-Bayesian approach to POS tagging, which relaxes the unrealistic assumption by automatically acquiring the lexicon from a small amount of POS-tagged data.
Since such relaxation comes at the expense of a drop in tagging accuracy, we propose two extensions to the Bayesian framework and demonstrate that they are effective in improving a fully-Bayesian POS tagger for Bengali, our representative morphologically-rich, resource-scarce language.
<newSection> 1 Introduction Unsupervised POS tagging requires neither manual encoding of tagging heuristics nor the availability of data labeled with POS information.
Rather, an unsupervised POS tagger operates by only assuming as input a POS lexicon, which consists of a list of possible POS tags for each word.
As we can see from the partial POS lexicon for English in Figure 1, “the” is unambiguous with respect to POS tagging, since it can only be a determiner (DT), whereas “sting” is ambiguous, since it can be a common noun (NN), a proper noun (NNP) or a verb (VB).
In other words, the lexicon imposes constraints on the possible POS tags of each word, and such constraints are then used by an unsupervised tagger to label a new sentence.
Conceivably, tagging accuracy decreases with the increase in ambiguity: unambiguous words such as “the” will always be tagged correctly; on the other hand, unseen words (or words not present in the POS lexicon) are among the most ambiguous words, since they are not constrained at all and therefore can receive any of the POS tags.
Hence, unsupervised POS tagging can present significant challenges to natural language processing researchers, especially when a large fraction of the words are ambiguous.
Nevertheless, the development of unsupervised taggers potentially allows POS tagging technologies to be applied to a substantially larger number of natural languages, most of which are resource-scarce and, in particular, have little or no POS-tagged data.
The most common approach to unsupervised POS tagging to date has been to train a hidden Markov model (HMM) in an unsupervised manner to maximize the likelihood of an unannotated corpus, using a special instance of the expectation-maximization (EM) algorithm (Dempster et al., 1977) known as Baum-Welch (Baum, 1972).
More recently, a fully-Bayesian approach to unsupervised POS tagging has been developed by Goldwater and Griffiths (2007) [henceforth G&G] as a viable alternative to the traditional maximum-likelihood-based HMM approach.
While unsupervised POS taggers adopting both approaches have demonstrated promising results, it is important to note that they are typically evaluated by assuming the availability of a perfect POS lexicon.
This assumption, however, is fairly unrealistic in practice, as a perfect POS lexicon can only be constructed by having a linguist manually label each word in a language with its possible POS tags.1 In other words, the labor-intensive POS lexicon construction process renders unsupervised POS taggers a lot less unsupervised than they appear.
To make these unsupervised taggers practical, one could attempt to automatically construct a POS lexicon, a task commonly known as POS induction.
However, POS induction is by no means an easy task, and it is not clear how well unsupervised POS taggers work when used in combination with an automatically constructed POS lexicon.
The goals of this paper are three-fold.
First, motivated by the successes of unsupervised approaches to English POS tagging, we aim to investigate whether such approaches, especially G&G’s fully-Bayesian approach, can deliver similar performance for Bengali, our representative resource-scarce language.
Second, to relax the unrealistic assumption of employing a perfect lexicon as in existing unsupervised POS taggers, we propose a weakly supervised fully-Bayesian approach to POS tagging, where we automatically construct a POS lexicon from a small amount of POS-tagged data.
Hence, unlike a perfect POS lexicon, our automatically constructed lexicon is necessarily incomplete, yielding a large number of words that are completely ambiguous.
The high ambiguity rate inherent in our weakly supervised approach substantially complicates the POS tagging process.
Consequently, our third goal of this paper is to propose two potentially performance-enhancing extensions to G&G’s Bayesian POS tagging approach, which exploit morphology and techniques successfully used in supervised POS tagging.
The rest of the paper is organized as follows.
Section 2 presents related work on unsupervised approaches to POS tagging.
Section 3 gives an introduction to G&G’s fully-Bayesian approach to unsupervised POS tagging.
In Section 4, we describe our two extensions to G&G’s approach.
Section 5 presents experimental results on Bengali POS tagging, focusing on evaluating the effective1When evaluating an unsupervised POS tagger, researchers typically construct a pseudo-perfect POS lexicon by collecting the possible POS tags of a word directly from the corpus on which the tagger is to be evaluated.
ness of our two extensions in improving G&G’s approach.
Finally, we conclude in Section 6.
<newSection> 2 Related Work With the notable exception of Synder et al.’s (2008; 2009) recent work on unsupervised multilingual POS tagging, existing approaches to unsupervised POS tagging have been developed and tested primarily on English data.
For instance, Merialdo (1994) uses maximum likelihood estimation to train a trigram HMM.
Sch¨utze (1995) and Clark (2000) apply syntactic clustering and dimensionality reduction in a knowledge-free setting to obtain meaningful clusters.
Haghighi and Klein (2006) develop a prototype-driven approach, which requires just a few prototype examples for each POS tag and exploits these labeled words to constrain the labels of their distributionally similar words.
Smith and Eisner (2005) train an unsupervised POS tagger using contrastive estimation, which seeks to move probability mass to a positive example e from its neighbors (i.e., negative examples are created by perturbing e).
Wang and Schuurmans (2005) improve an unsupervised HMM-based tagger by constraining the learned structure to maintain appropriate marginal tag probabilities and using word similarities to smooth the lexical parameters.
As mentioned before, Goldwater and Griffiths (2007) have recently proposed an unsupervised fully-Bayesian POS tagging framework that operates by integrating over the possible parameter values instead of fixing a set of parameter values for unsupervised sequence learning.
Importantly, this Bayesian approach facilitates the incorpora-tion of sparse priors that result in a more practical distribution of tokens to lexical categories (John-son, 2007).
Similar to Goldwater and Griffiths (2007) and Johnson (2007), Toutanova and Johnson (2007) also use Bayesian inference for POS tagging.
However, their work departs from existing Bayesian approaches to POS tagging in that they (1) introduce a new sparse prior on the distribution over tags for each word, (2) extend the Latent Dirichlet Allocation model, and (3) explicitly model ambiguity class.
While their tagging model, like Goldwater and Griffiths’s, assumes as input an incomplete POS lexicon and a large unlabeled corpus, they consider their approach “semi-supervised” simply because of the human knowledge involved in constructing the POS lexicon.
<newSection> 3 A Fully Bayesian Approach As mentioned in the introduction, the most common approach to unsupervised POS tagging is to train an HMM on an unannotated corpus using the Baum-Welch algorithm so that the likelihood of the corpus is maximized.
To understand what the HMM parameters are, let us revisit how an HMM simultaneously generates an output sequence w = (w0, w1, ..., wn) and the associated hidden state sequence t = (t0, t1, ..., tn).
In the context of POS tagging, each state of the HMM corresponds to a POS tag, the output sequence w is the given word sequence, and the hidden state sequence t is the associated POS tag sequence.
To generate w and t, the HMM begins by guessing a state t0 and then emitting w0 from t0 according to a state-specific output distribution over word tokens.
After that, we move to the next state t1, the choice of which is based on t0’s transition distribution, and emit w1 according to t1’s output distribution.
This generation process repeats until the end of the word sequence is reached.
In other words, the parameters of an HMM, θ, are composed of a set of state-specific (1) output distributions (over word tokens) and (2) transition distributions, both of which can be learned using the EM algorithm.
Once learning is complete, we can use the resulting set of parameters to find the most likely hidden state sequence given a word sequence using the Viterbi algorithm.
Nevertheless, EM sometimes fails to find good parameter values.2 The reason is that EM tries to assign roughly the same number of word tokens to each of the hidden states (Johnson, 2007).
In practice, however, the distribution of word tokens to POS tags is highly skewed (i.e., some POS categories are more populated with tokens than others).
This motivates a fully-Bayesian approach, which, rather than committing to a particular set of parameter values as in an EM-based approach, integrates over all possible values of θ and, most importantly, allows the use of priors to favor the learning of the skewed distributions, through the use of the term P(θ|w) in the following equation: The question, then, is: which priors on θ would allow the acquisition of skewed distributions?
To 2When given good parameter initializations, however, EM can find good parameter values for an HMM-based POS tagger.
See Goldberg et al.
(2008) for details.
answer this question, recall that in POS tagging, θ is composed of a set of tag transition distributions and output distributions.
Each such distribution is a multinomial (i.e., each trial produces exactly one of some finite number of possible outcomes).
For a multinomial with K outcomes, a K-dimensional Dirichlet distribution, which is conjugate to the multinomial, is a natural choice of prior.
For simplicity, we assume that a distribution in θ is drawn from a symmetric Dirichlet with a certain hyper-parameter (see Teh et al. (2006) for details).
The value of a hyperparameter, α, affects the skewness of the resulting distribution, as it assigns different probabilities to different distributions.
For instance, when α < 1, higher probabilities are assigned to sparse multinomials (i.e., multinomials in which only a few entries are non-zero).
Intuitively, the tag transition distributions and the output distributions in an HMM-based POS tagger are sparse multinomials.
As a result, it is logical to choose a Dirichlet prior with α < 1.
By integrating over all possible parameter values, the probability that i-th outcome, yi, takes the value k, given the previous i − 1 outcomes y−i= (y1, y2, ..., yi−1), is where nk is the frequency of k in y−i.
See MacKay and Peto (1995) for the derivation.
Our baseline POS tagging model is a standard trigram HMM with tag transition distributions and output distributions, each of which is a sparse multinomial that is learned by applying a symmetric Dirichlet prior: where wi and ti denote the i-th word and tag.
With a tagset of size T (including a special tag used as sentence delimiter), each of the tag transition distributions has T components.
For the output symbols, each of the ω(ti) has Wti components, where Wti denotes the number of word types that can be emitted from the state corresponding to ti.
From the closed form in Equation 3, given previous outcomes, we can compute the tag transition and output probabilities of the model as follows: where n(ti−2,ti−1,ti) and n(ti,wi) are the frequencies of observing the tag trigram (ti−2,ti−1,ti) and the tag-word pair (ti, wi), respectively.
These counts are taken from the i − 1 tags and words generated previously.
The inference procedure described next exploits the property that trigrams (and outputs) are exchangeable; that is, the probability of a set of trigrams (and outputs) does not depend on the order in which it was generated.
We perform inference using Gibbs sampling (Ge-man and Geman, 1984), using the following posterior distribution to generate samples: Starting with a random assignment of a POS tag to each word (subject to the constraints in the POS lexicon), we resample each POS tag, ti, according to the conditional distribution shown in Figure 2.
Note that the current counts of other trigrams and outputs can be used as “previous” observations due to the property of exchangeability.
Following G&G, we use simulated annealing to find the MAP tag sequence.
The temperature decreases by a factor of exp(log(θ2 N−1 ) after each iter-θ1 ) ation, where θ1 is the initial temperature and θ2 is the temperature after N sampling iterations.
<newSection> 4 Two Extensions In this section, we present two extensions to G&G’s fully-Bayesian framework to unsupervised POS tagging, namely, induced suffix emission and discriminative prediction.
For morphologically-rich languages like Bengali, a lot of grammatical information (e.g., POS) is expressed via suffixes.
In fact, several approaches to unsupervised POS induction for morphologically-rich languages have exploited the observation that some suffixes can only be associated with a small number of POS tags (e.g., Clark (2003), Dasgupta and Ng (2007)).
To exploit suffixes in HMM-based POS tagging, one can (1) convert the word-based POS lexicon to a suffix-based POS lexicon, which lists the possible POS tags for each suffix; and then (2) have the HMM emit suffixes rather than words, subject to the constraints in the suffix-based POS lexicon.
Such a suffix-based HMM, however, may suffer from over-generalization.
To prevent over-generalization and at the same time exploit suffixes, we propose as our first extension to G&G’s framework a hybrid approach to word/suffix emission: a word is emitted if it is present in the word-based POS lexicon; otherwise, its suffix is emitted.
In other words, our approach imposes suffix-based constraints on the tagging of words that are unseen w.r.t. the word-based POS lexicon.
Below we show how to induce the suffix of a word and create the suffix-based POS lexicon.
Inducing suffixes To induce suffixes, we rely on Keshava and Pitler’s (2006) method.
Assume that (1) V is a vocabulary (i.e., a set of distinct words) extracted from a large, unannotated corpus, (2) C1 and C2 are two character sequences, and (3) C1C2 is the concatenation of C1 and C2.
If C1C2 and C1 are found in V , we extract C2 as a suffix.
However, this unsupervised suffix induction method is arguably overly simplistic and hence many of the induced affixes could be spurious.
To identify suffixes that are likely to be correct, we employ a simple procedure: we (1) score each suffix by multiplying its frequency (i.e., the number of distinct words in V to which each suffix attaches) and its length3, and (2) select only those whose score is above a certain threshold.
In our experiments, we set this threshold to 50, and generate our vocabulary from five years of articles taken from the Bengali newspaper Prothom Alo.
This enables us to induce 975 suffixes.
Constructing a suffix-based POS lexicon Next, we construct a suffix-based POS lexicon.
For each word w in the original word-based POS lexicon, we (1) use the induced suffix list obtained in the previous step to identify the longest-matching suffix of w, and then (2) assign all the POS tags associated with w to this suffix.
Incorporating suffix-based output distributions Finally, we extend our trigram model by introducing a state-specific probability distribution over induced suffixes.
Specifically, if the current word is present in the word-based POS lexicon, or if we cannot find any suffix for the word using the induced suffix list, then we emit the word.
Otherwise, we emit its suffix according to a suffix-based output distribution, which is drawn from a symmetric Dirichlet with hyperparameter γ: where si denotes the induced suffix of the i-th word.
The distribution, σ(ti), has Sti components, where Sti denotes the number of induced suffixes that can be emitted from the state corresponding to ti.
We compute the induced suffix emission probabilities of the model as follows: where n(ti,si) is the frequency of observing the tag-suffix pair (ti, si).
This extension requires that we slightly modify the inference procedure.
Specifically, if the current word is unseen (w.r.t. the word-based POS lexicon) and has a suffix (according to the induced suffix list), then we sample from a distribution that is almost identical to the one shown in Figure 2, except that we replace the first fraction (i.e., the fraction involving the emission counts) with the one shown in Equation (6).
Otherwise, we simply sample from the distribution in Figure 2.
As mentioned in the introduction, the (word-based) POS lexicons used in existing approaches to unsupervised POS tagging were created somewhat unrealistically by collecting the possible POS tags of a word directly from the corpus on which the tagger is to be evaluated.
To make the lexicon formation process more realistic, we propose a weakly supervised approach to Bayesian POS tagging, in which we automatically create the word-based POS lexicon from a small set of POS-tagged sentences that is disjoint from the test data.
Adopting a weakly supervised approach has an additional advantage: the presence of POS-tagged sentences makes it possible to exploit techniques developed for supervised POS tagging, which is the idea behind discriminative prediction, our second extension to G&G’s framework.
Given a small set of POS-tagged sentences L, discriminative prediction uses the statistics collected from L to predict the POS of a word in a discriminative fashion whenever possible.
More specifically, discriminative prediction relies on two simple ideas typically exploited by supervised POS tagging algorithms: (1) if the target word (i.e., the word whose POS tag is to be predicted) appears in L, we can label the word with its POS tag in L; and (2) if the target word does not appear in L but its context does, we can use its context to predict its POS tag.
In bigram and trigram POS taggers, the context of a word is represented using the preceding one or two words.
Nevertheless, since L is typically small in a weakly supervised setting, it is common for a target word not to satisfy any of the two conditions above.
Hence, if it is not possible to predict a target word in a discriminative fashion (due to the limited size of L), we resort to the sampling equation in Figure 2.
To incorporate the above discriminative decision steps into G&G’s fully-Bayesian framework for POS tagging, the algorithm estimates three types of probability distributions from L.
First, to capture context, it computes (1) a distribution over the POS tags following a word bigram, (wi−2, wi−1), that appears in L [henceforth D1(wi−2,wi−1)] and (2) a distribution over the POS tags following a word unigram, wi−1, that appears in L [henceforth D2(wi−1)].
Then, to capture the fact that a word can have more than one POS tag, it also estimates a distribution over POS tags for each word wi that appears in L [hence-forth D3(wi)].
Implemented as a set of if-else clauses, the algorithm uses these three types of distributions to tag a target word, wi, in a discriminative manner.
First, it checks whether wi appears in L (line 1).
If so, it tags wi according to D3(wi).
Otherwise, it attempts to label wi based on its context.
Specifically, if (wi−2, wi−1), the word bigram preceding wi, appears in L (line 3), then wi is tagged according to D1(wi−2, wi−1).
Otherwise, it backs off to a unigram distribution: if wi−1, the word preceding wi, appears in L (line 5), then wi is tagged according to D2(wi−1).
Finally, if it is not possible to tag the word discriminatively (i.e., if all the above cases fail), it resorts to the sampling equation (lines 7–8).
We apply simulated annealing to all four cases in this iterative tagging procedure.
<newSection> 5 Evaluation Corpus Our evaluation corpus is the one used in the shared task of the IJCNLP-08 Workshop on NER for South and South East Asian Languages.4 Specifically, we use the portion of the Bengali dataset that is manually POS-tagged.
IIIT Hy-derabad’s POS tagset5, which consists of 26 tags specifically developed for Indian languages, has been used to annotate the data.
The corpus is composed of a training set and a test set with approximately 50K and 30K tokens, respectively.
Importantly, all our POS tagging results will be reported using only the test set; the training set will be used for lexicon construction, as we will see shortly.
Tagset We collapse the set of 26 POS tags into 15 tags.
Specifically, while we retain the tags corresponding to the major POS categories, we merge some of the infrequent tags designed to capture Indian language specific structure (e.g., reduplica-tion, echo words) into a category called OTHERS.
Hyperparameter settings Recall that our tagger consists of three types of distributions — tag transition distributions, word-based output distributions, and suffix-based output distributions — drawn from a symmetric Dirichlet with α, Q, and γ as the underlying hyperparameters, respectively.
We automatically determine the values of these hyperparameters by (1) randomly initializing them and (2) resampling their values by using a Metropolis-Hastings update (Gilks et al., 1996) at the end of each sampling iteration.
Details of this update process can be found in G&G.
Inference Inference is performed by running a Gibbs sampler for 5000 iterations.
The initial temperature is set to 2.0, which is gradually lowered to 0.08 over the iterations.
Owing to the randomness involved in hyperparameter initialization, all reported results are averaged over three runs.
Lexicon construction methods To better understand the role of a POS lexicon in tagging performance, we evaluate each POS tagging model by employing lexicons constructed by three methods.
The first lexicon construction method, arguably the most unrealistic among the three, follows that of G&G: for each word, w, in the test set, we (1) collect from each occurrence of w in the training set and the test set its POS tag, and then (2) insert w and all the POS tags collected for w into the POS lexicon.
This method is unrealistic because (1) in practice, a human needs to list all possible POS tags for each word in order to construct this lexicon, thus rendering the resulting tagger considerably less unsupervised than it appears; and (2) constructing the lexicon using the dataset on which the tagger is to be evaluated implies that there is no unseen word w.r.t. the lexicon, thus un-realistically simplifies the POS tagging task.
To make the method more realistic, G&G also create a set of relaxed lexicons.
Each of these lexicons includes the tags for only the words that appear at least d times in the test corpus, where d ranges from 1 to 10 in our experiments.
Any unseen (i.e., out-of-dictionary) word is ambiguous among the 15 possible tags.
Not surprisingly, both ambiguity and the unseen word rate increase with d.
For instance, the ambiguous token rate increases from 40.0% with 1.7 tags/token (d=1) to 77.7% with 8.1 tags/token (d=10).
Similarly, the unseen word rate increases from 16% (d=2) to 46% (d=10).
We will refer to this set of tag dictionaries as Lexicon 1.
The second method generates a set of relaxed lexicons, Lexicon 2, in essentially the same way as the first method, except that these lexicons include only the words that appear at least d times in the training data.
Importantly, the words that appear solely in the test data are not included in any of these relaxed POS lexicons.
This makes Lexicon 2 a bit more realistic than Lexicon 1 in terms of the way they are constructed.
As a result, in comparison to Lexicon 1, Lexicon 2 has a considerably higher ambiguous token rate and unseen word rate: its ambiguous token rate ranges from 64.3% with 5.3 tags/token (d=1) to 80.5% with 8.6 tags/token (d=10), and its unseen word rate ranges from 25% (d=1) to 50% (d=10).
The third method, arguably the most realistic among the three, is motivated by our proposed weakly supervised approach.
In this method, we (1) form ten different datasets from the (labeled) training data of sizes 5K words, 10K words, ..., 50K words, and then (2) create one POS lexicon from each dataset L by listing, for each word w in L, all the tags associated with w in L.
This set of tag dictionaries, which we will refer to as Lexicon 3, has an ambiguous token rate that ranges from 57.7% with 5.1 tags/token (50K) to 61.5% with 8.1 tags/token (5K), and an unseen word rate that ranges from 25% (50K) to 50% (5K).
We use as our first baseline system G&G’s Bayesian POS tagging model, as our goal is to evaluate the effectiveness of our two extensions in improving their model.
To further gauge the performance of G&G’s model, we employ another baseline commonly used in POS tagging experiments, which is an unsupervised trigram HMM trained by running EM to convergence.
As mentioned previously, we evaluate each tagging model by employing the three POS lexicons described in the previous subsection.
Figure 3(a) shows how the tagging accuracy varies with d when Lexicon 1 is used.
Perhaps not surprisingly, the trigram HMM (MLHMM) and G&G’s Bayesian model (BHMM) achieve almost identical accuracies when d=1 (i.e., the complete lexicon with a zero unseen word rate).
As d increases, both ambiguity and the unseen word rate increase; as a result, the tagging accuracy decreases.
Also, consistent with G&G’s results, BHMM outperforms MLHMM by a large margin (4–7%).
Similar performance trends can be observed when Lexicon 2 is used (see Figure 3(b)).
However, both baselines achieve comparatively lower tagging accuracies, as a result of the higher unseen word rate associated with Lexicon 2.
Results using Lexicon 3 are shown in Figure 4.
Owing to the availability of POS-tagged sentences, we replace MLHMM with its supervised counterpart that is trained on the available labeled data, yielding the SHMM baseline.
The accuracies of SHMM range from 48% to 67%, outperforming BHMM as the amount of labeled data increases.
Next, we augment BHMM with our first extension, induced suffix emission, yielding BHMM+IS.
For Lexicon 1, BHMM+IS achieves the same accuracy as the two baselines when d=1.
The reason is simple: as all the test words are in the POS lexicon, the tagger never emits an induced suffix.
More importantly, BHMM+IS beats BHMM and MLHMM by 4–9% and 10–14%, respectively.
Similar trends are observed for Lexicon 2, where BHMM+IS outperforms BHMM and MLHMM by a larger margin of 5–10% and 12–16%, respectively.
For Lexicon 3, BHMM+IS outperforms SHMM, the stronger baseline, by 6– 11%.
Overall, these results suggest that induced suffix emission is a strong performance-enhancing extension to G&G’s approach.
Finally, we augment BHMM+IS with discriminative prediction, yielding BHMM+IS+DP.
Since this extension requires labeled data, it can only be applied in combination with Lexicon 3.
As seen in Figure 4, BHMM+IS+DP outperforms SHMM by 10–14%.
Its discriminative nature proves to be strong as it even beats BHMM+IS by 3–4%.
<newSection> 5.2.4 Error Analysis Table 1 lists the most common types of errors made by the best-performing tagging model, BHMM+IS+DP (50K-word labeled data).
As we can see, common nouns and proper nouns (row 1) are difficult to distinguish, due in part to the case insensitivity of Bengali.
Also, it is difficult to distinguish Bengali common nouns and adjectives (row 2), as they are distributionally similar to each other.
The confusion between main verbs [VM] and auxiliary verbs [VAUX] (row 3) arises from the fact that certain Bengali verbs can serve as both a main verb and an auxiliary verb, depending on the role the verb plays in the verb sequence.
<newSection> 6 Conclusions While Goldwater and Griffiths’s fully-Bayesian approach and the traditional maximum-likelihood parameter-based approach to unsupervised POS tagging have offered promising results for English, we argued in this paper that such results were obtained under the unrealistic assumption that a perfect POS lexicon is available, which renders these taggers less unsupervised than they appear.
As a result, we investigated a weakly supervised fully-Bayesian approach to POS tagging, which relaxes the unrealistic assumption by automatically acquiring the lexicon from a small amount of POS-tagged data.
Since such relaxation comes at the expense of a drop in tagging accuracy, we proposed two performance-enhancing extensions to the Bayesian framework, namely, induced suffix emission and discriminative prediction, which effectively exploit morphology and techniques from supervised POS tagging, respectively.
<newSection> Acknowledgments We thank the three anonymous reviewers and Sajib Dasgupta for their comments.
We also thank CRBLP, BRAC University, Bangladesh, for providing us with Bengali resources and Taufiq Hasan Al Banna for his MATLAB code.
This work was supported in part by NSF Grant IIS-0812261.
<newSection> References<newSection> Abstract In this paper we propose a new graph-based method that uses the knowledge in a LKB (based on WordNet) in order to perform unsupervised Word Sense Disambiguation.
Our algorithm uses the full graph of the LKB efficiently, performing better than previous approaches in English all-words datasets.
We also show that the algorithm can be easily ported to other languages with good results, with the only requirement of having a wordnet.
In addition, we make an analysis of the performance of the algorithm, showing that it is efficient and that it could be tuned to be faster.
<newSection> 1 Introduction Word Sense Disambiguation (WSD) is a key enabling-technology that automatically chooses the intended sense of a word in context.
Supervised WSD systems are the best performing in public evaluations (Palmer et al., 2001; Snyder and Palmer, 2004; Pradhan et al., 2007) but they need large amounts of hand-tagged data, which is typically very expensive to build.
Given the relatively small amount of training data available, current state-of-the-art systems only beat the simple most frequent sense (MFS) baseline1 by a small margin.
As an alternative to supervised systems, knowledge-based WSD systems exploit the information present in a lexical knowledge base (LKB) to perform WSD, without using any further corpus evidence.
Traditional knowledge-based WSD systems assign a sense to an ambiguous word by comparing each of its senses with those of the surrounding context.
Typically, some semantic similarity metric is used for calculating the relatedness among senses (Lesk, 1986; McCarthy et al., 2004).
One of the major drawbacks of these approaches stems from the fact that senses are compared in a pairwise fashion and thus the number of computations can grow exponentially with the number of words.
Although alternatives like simulated annealing (Cowie et al., 1992) and conceptual density (Agirre and Rigau, 1996) were tried, most of past knowledge based WSD was done in a suboptimal word-by-word process, i.e., disambiguating words one at a time.
Recently, graph-based methods for knowledge-based WSD have gained much attention in the NLP community (Sinha and Mihalcea, 2007; Nav-igli and Lapata, 2007; Mihalcea, 2005; Agirre and Soroa, 2008).
These methods use well-known graph-based techniques to find and exploit the structural properties of the graph underlying a particular LKB.
Because the graph is analyzed as a whole, these techniques have the remarkable property of being able to find globally optimal solutions, given the relations between entities.
Graph-based WSD methods are particularly suited for disambiguating word sequences, and they manage to exploit the interrelations among the senses in the given context.
In this sense, they provide a principled solution to the exponential explosion problem, with excellent performance.
Graph-based WSD is performed over a graph composed by senses (nodes) and relations between pairs of senses (edges).
The relations may be of several types (lexico-semantic, coocurrence relations, etc.) and may have some weight attached to them.
The disambiguation is typically performed by applying a ranking algorithm over the graph, and then assigning the concepts with highest rank to the corresponding words.
Given the computational cost of using large graphs like WordNet, many researchers use smaller subgraphs built online for each target context.
In this paper we present a novel graph-based WSD algorithm which uses the full graph of WordNet efficiently, performing significantly better that previously published approaches in English all-words datasets.
We also show that the algorithm can be easily ported to other languages with good results, with the only requirement of having a wordnet.
The algorithm is publicly available2 and can be applied easily to sense inventories and knowledge bases different from WordNet.
Our analysis shows that our algorithm is efficient compared to previously proposed alternatives, and that a good choice of WordNet versions and relations is fundamental for good performance.
The paper is structured as follows.
We first describe the PageRank and Personalized PageRank algorithms.
Section 3 introduces the graph based methods used for WSD.
Section 4 shows the experimental setting and the main results, and Section 5 compares our methods with related experiments on graph-based WSD systems.
Section 6 shows the results of the method when applied to a Spanish dataset.
Section 7 analyzes the performance of the algorithm.
Finally, we draw some conclusions in Section 8.
<newSection> 2 PageRank and Personalized PageRank The celebrated PageRank algorithm (Brin and Page, 1998) is a method for ranking the vertices in a graph according to their relative structural importance.
The main idea of PageRank is that whenever a link from vi to vj exists in a graph, a vote from node i to node j is produced, and hence the rank of node j increases.
Besides, the strength of the vote from i to j also depends on the rank of node i: the more important node i is, the more strength its votes will have.
Alternatively, PageRank can also be viewed as the result of a random walk process, where the final rank of node i represents the probability of a random walk over the graph ending on node i, at a sufficiently large time.
Let G be a graph with N vertices vi, ...
, vN and di be the outdegree of node i; let M be a In the equation, v is a N × 1 vector whose elements are 1N and c is the so called damping factor, a scalar value between 0 and 1.
The first term of the sum on the equation models the voting scheme described in the beginning of the section.
The second term represents, loosely speaking, the probability of a surfer randomly jumping to any node, e.g. without following any paths on the graph.
The damping factor, usually set in the [0.85..0.95] range, models the way in which these two terms are combined at each step.
The second term on Eq.
(1) can also be seen as a smoothing factor that makes any graph fulfill the property of being aperiodic and irreducible, and thus guarantees that PageRank calculation converges to a unique stationary distribution.
In the traditional PageRank formulation the vector v is a stochastic normalized vector whose element values are all 1N, thus assigning equal probabilities to all nodes in the graph in case of random jumps.
However, as pointed out by (Haveliwala, 2002), the vector v can be non-uniform and assign stronger probabilities to certain kinds of nodes, effectively biasing the resulting PageRank vector to prefer these nodes.
For example, if we concentrate all the probability mass on a unique node i, all random jumps on the walk will return to i and thus its rank will be high; moreover, the high rank of i will make all the nodes in its vicinity also receive a high rank.
Thus, the importance of node i given by the initial distribution of v spreads along the graph on successive iterations of the algorithm.
In this paper, we will use traditional PageRank to refer to the case when a uniform v vector is used in Eq.
(1); and whenever a modified v is used, we will call it Personalized PageRank.
The next section shows how we define a modified v.
PageRank is actually calculated by applying an iterative algorithm which computes Eq.
(1) successively until convergence below a given threshold is achieved, or, more typically, until a fixed number of iterations are executed.
Regarding PageRank implementation details, we chose a damping value of 0.85 and finish the calculation after 30 iterations.
We did not try other damping factors.
Some preliminary experiments with higher iteration counts showed that although sometimes the node ranks varied, the relative order among particular word synsets remained stable after the initial iterations (cf. Section 7 for further details).
Note that, in order to discard the effect of dangling nodes (i.e. nodes without outlinks) we slightly modified Eq.
(1).
For the sake of brevity we omit the details, which the interested reader can check in (Langville and Meyer, 2003).
<newSection> 3 Using PageRank for WSD In this section we present the application of PageRank to WSD.
If we were to apply the traditional PageRank over the whole WordNet we would get a context-independent ranking of word senses, which is not what we want.
Given an input piece of text (typically one sentence, or a small set of contiguous sentences), we want to disambiguate all open-class words in the input taken the rest as context.
In this framework, we need to rank the senses of the target words according to the other words in the context.
Theare two main alternatives to achieve this: The first method has been explored in the literature (cf. Section 5), and we also presented a variant in (Agirre and Soroa, 2008) but the second method is novel in WSD.
In both cases, the algorithms return a list of ranked senses for each target word in the context.
We will see each of them in turn, but first we will present some notation and a preliminary step.
A LKB is formed by a set of concepts and relations among them, and a dictionary, i.e., a list of words (typically, word lemmas) each of them linked to at least one concept of the LKB.
Given any such LKB, we build an undirected graph G = (V, E) where nodes represent LKB concepts (vi), and each relation between concepts vi and vj is represented by an undirected edge ei,j.
In our experiments we have tried our algorithms using three different LKBs: Given an input text, we extract the list Wi i = 1...
m of content words (i.e. nouns, verbs, adjectives and adverbs) which have an entry in the dictionary, and thus can be related to LKB concepts.
Let Conceptsi = {v1, ...
, vi.} be the im associated concepts of word Wi in the LKB graph.
Note that monosemous words will be related to just one concept, whereas polysemous words may be attached to several.
As a result of the disambiguation process, every concept in Conceptsi, i = 1, ...
, m receives a score.
Then, for each target word to be disambiguated, we just choose its associated concept in G with maximal score.
In our experiments we build a context of at least 20 content words for each sentence to be disam-biguated, taking the sentences immediately before and after it in the case that the original sentence was too short.
We follow the algorithm presented in (Agirre and Soroa, 2008), which we explain here for completeness.
The main idea of the subgraph method is to extract the subgraph of GKB whose vertices and relations are particularly relevant for a given input context.
Such a subgraph is called a “disambigua-tion subgraph” GD, and it is built in the following way.
For each word Wi in the input context and each concept vi E Conceptsi, a standard breath-first search (BFS) over GKB is performed, starting at node vi.
Each run of the BFS calculates the minimum distance paths between vi and the rest of concepts of GKB . In particular, we are interested in the minimum distance paths between vi and the concepts associated to the rest of the words in the context, vj E Uj=,4i Conceptsj.
Let mdpvi be the set of these shortest paths.
This BFS computation is repeated for every concept of every word in the input context, storing mdpvi accordingly.
At the end, we obtain a set of minimum length paths each of them having a different concept as a source.
The disambiguation graph GD is then just the union of the vertices and edges of the shortest paths, GD = Umi=1{mdpv,/vj E Conceptsi}.
The disambiguation graph GD is thus a sub-graph of the original GKB graph obtained by computing the shortest paths between the concepts of the words co-occurring in the context.
Thus, we hypothesize that it captures the most relevant concepts and relations in the knowledge base for the particular input context.
Once the GD graph is built, we compute the traditional PageRank algorithm over it.
The intuition behind this step is that the vertices representing the correct concepts will be more relevant in GD than the rest of the possible concepts of the context words, which should have less relations on average and be more isolated.
As usual, the disambiguation step is performed by assigning to each word Wi the associated concept in Conceptsi which has maximum rank.
In case of ties we assign all the concepts with maximum rank.
Note that the standard evaluation script provided in the Senseval competitions treats multiple senses as if one was chosen at random, i.e. for evaluation purposes our method is equivalent to breaking ties at random.
As mentioned before, personalized PageRank allows us to use the full LKB.
We first insert the context words into the graph G as nodes, and link them with directed edges to their respective concepts.
Then, we compute the personalized PageRank of the graph G by concentrating the initial probability mass uniformly over the newly introduced word nodes.
As the words are linked to the concepts by directed edges, they act as source nodes injecting mass into the concepts they are associated with, which thus become relevant nodes, and spread their mass over the LKB graph.
Therefore, the resulting personalized PageRank vector can be seen as a measure of the structural relevance of LKB concepts in the presence of the input context.
One problem with Personalized PageRank is that if one of the target words has two senses which are related by semantic relations, those senses reinforce each other, and could thus dampen the effect of the other senses in the context.
With this observation in mind we devised a variant (dubbed Ppr w2w), where we build the graph for each target word in the context: for each target word Wi, we concentrate the initial probability mass in the senses of the words surrounding Wi, but not in the senses of the target word itself, so that context words increase its relative importance in the graph.
The main idea of this approach is to avoid biasing the initial score of concepts associated to target word Wi, and let the surrounding words decide which concept associated to Wi has more relevance.
Contrary to the other two approaches, Ppr w2w does not disambiguate all target words of the context in a single run, which makes it less efficient (cf. Section 7).
<newSection> 4 Evaluation framework and results In this paper we will use two datasets for comparing graph-based WSD methods, namely, the Senseval-2 (S2AW) and Senseval-3 (S3AW) all words datasets (Snyder and Palmer, 2004; Palmer et al., 2001), which are both labeled with WordNet 1.7 tags.
We did not use the Semeval dataset, for the sake of comparing our results to related work, none of which used Semeval data.
Table 1 shows the results as recall of the graph-based WSD system over these datasets on the different LKBs.
We detail overall results, as well as results per PoS, and the confidence interval for the overall results.
The interval was computed using bootstrap resam-pling with 95% confidence.
The table shows that Ppr w2w is consistently the best method in both datasets and for all LKBs.
Ppr and Spr obtain comparable results, which is remarkable, given the simplicity of the Ppr algo-baseline and the best results of supervised systems at competition time (SMUaw,GAMBL).
rithm, compared to the more elaborate algorithm to construct the graph.
The differences between methods are not statistically significant, which is a common problem on this relatively small datasets (Snyder and Palmer, 2004; Palmer et al., 2001).
Regarding LKBs, the best results are obtained using WordNet 1.7 and eXtended WordNet.
Here the differences are in many cases significant.
These results are surprising, as we would expect that the manually disambiguated gloss relations from WordNet 3.0 would lead to better results, compared to the automatically disam-biguated gloss relations from the eXtended WordNet (linked to version 1.7).
The lower performance of WNet30+gloss can be due to the fact that the Senseval all words data set is tagged using WordNet 1.7 synsets.
When using a different LKB for WSD, a mapping to WordNet 1.7 is required.
Although the mapping is cited as having a correctness on the high 90s (Daude et al., 2000), it could have introduced sufficient noise to counteract the benefits of the hand-disambiguated glosses.
Table 1 also shows the most frequent sense (MFS), as well as the best supervised systems (Snyder and Palmer, 2004; Palmer et al., 2001) that participated in each competition (SMUaw and GAMBL, respectively).
The MFS is a baseline for supervised systems, but it is considered a difficult competitor for unsupervised systems, which rarely come close to it.
In this case the MFS baseline was computed using previously availabel training data like SemCor.
Our best results are close to the MFS in both Senseval-2 and Senseval-3 datasets.
The results for the supervised system are given for reference, and we can see that the gap is relatively small, specially for Senseval-3.
<newSection> 5 Comparison to Related work In this section we will briefly describe some graph-based methods for knowledge-based WSD.
The methods here presented cope with the problem of sequence-labeling, i.e., they disambiguate all the words coocurring in a sequence (typically, all content words of a sentence).
All the methods rely on the information represented on some LKB, which typically is some version of WordNet, sometimes enriched with proprietary relations.
The results on our datasets, when available, are shown in Table 2.
The table also shows the performance of supervised systems.
The TexRank algorithm (Mihalcea, 2005) for WSD creates a complete weighted graph (e.g. a graph where every pair of distinct vertices is connected by a weighted edge) formed by the synsets of the words in the input context.
The weight of the links joining two synsets is calculated by executing Lesk’s algorithm (Lesk, 1986) between them, i.e., by calculating the overlap between the words in the glosses of the correspongind senses.
Once the complete graph is built, the PageRank algorithm is executed over it and words are assigned to the most relevant synset.
In this sense, PageRank is used an alternative to simulated annealing to find the optimal pairwise combinations.
The method was evaluated on the Senseval-3 dataset, as shown in row Mih05 on Table 2.
(Sinha and Mihalcea, 2007) extends their previous work by using a collection of semantic similarity measures when assigning a weight to the links across synsets.
They also compare different graph-based centrality algorithms to rank the vertices of the complete graph.
They use different similarity metrics for different POS types and a voting scheme among the centrality algorithm ranks.
Here, the Senseval-3 corpus was used as a development data set, and we can thus see those results as the upper-bound of their method.
We can see in Table 2 that the methods presented in this paper clearly outperform both Mih05 and Sin07.
This result suggests that analyzing the LKB structure as a whole is preferable than computing pairwise similarity measures over synsets.
The results of various in-house made experiments replicating (Mihalcea, 2005) also confirm this observation.
Note also that our methods are simpler than the combination strategy used in (Sinha and Mihalcea, 2007), and that we did not perform any parameter tuning as they did.
In (Navigli and Velardi, 2005) the authors develop a knowledge-based WSD method based on lexical chains called structural semantic intercon-nections (SSI).
Although the system was first designed to find the meaning of the words in WordNet glosses, the authors also apply the method for labeling text sequences.
Given a text sequence, SSI first identifies monosemous words and assigns the corresponding synset to them.
Then, it iteratively disambiguates the rest of terms by selecting the senses that get the strongest interconnec-tion with the synsets selected so far.
The inter-connection is calculated by searching for paths on the LKB, constrained by some hand-made rules of possible semantic patterns.
The method was evaluated on the Senseval-3 dataset, as shown in row Nav05 on Table 2.
Note that the method labels an instance with the most frequent sense of the word if the algorithm produces no output for that instance, which makes comparison to our system unfair, specially given the fact that the MFS performs better than SSI.
In fact it is not possible to separate the effect of SSI from that of the MFS.
For this reason we place this method close to the MFS baseline in Table 2.
In (Navigli and Lapata, 2007), the authors perform a two-stage process for WSD.
Given an input context, the method first explores the whole LKB in order to find a subgraph which is particularly relevant for the words of the context.
Then, they study different graph-based centrality algorithms for deciding the relevance of the nodes on the sub-graph.
As a result, every word of the context is attached to the highest ranking concept among its possible senses.
The Spr method is very similar to (Navigli and Lapata, 2007), the main difference lying on the initial method for extracting the context subgraph.
Whereas (Navigli and Lapata, 2007) apply a depth-first search algorithm over the LKB graph —and restrict the depth of the subtree to a value of 3—, Spr relies on shortest paths between word synsets.
Navigli and Lapata don’t report overall results and therefore, we can’t directly compare our results with theirs.
However, we can see that on a PoS-basis evaluation our results are consistently better for nouns and verbs (especially the Ppr w2w method) and rather similar for adjectives.
(Tsatsaronis et al., 2007) is another example of a two-stage process, the first one consisting on finding a relevant subgraph by performing a BFS dataset, including MFS and the best supervised system in the competition.
search over the LKB.
The authors apply a spreading activation algorithm over the subgraph for node ranking.
Edges of the subgraph are weighted according to its type, following a tf.idf like approach.
The results show that our methods clearly outperform Tsatsa07.
The fact that the Spr method works better suggests that the traditional PageRank algorithm is a superior method for ranking the subgraph nodes.
As stated before, all methods presented here use some LKB for performing WSD.
(Mihalcea, 2005) and (Sinha and Mihalcea, 2007) use WordNet relations as a knowledge source, but neither of them specify which particular version did they use.
(Tsatsaronis et al., 2007) uses WordNet 1.7 enriched with eXtended WordNet relations, just as we do.
Both (Navigli and Velardi, 2005; Nav-igli and Lapata, 2007) use WordNet 2.0 as the underlying LKB, albeit enriched with several new relations, which are manually created.
Unfortunately, those manual relations are not publicly available, so we can’t directly compare their results with the rest of the methods.
In (Agirre and Soroa, 2008) we experiment with different LKBs formed by combining relations of different MCR versions along with relations extracted from Sem-Cor, which we call supervised and unsupervised relations, respectively.
The unsupervised relations that yielded bests results are also used in this paper (c.f Section 3.1).
<newSection> 6 Experiments on Spanish Our WSD algorithm can be applied over non-english texts, provided that a LKB for this particular language exists.
We have tested the graph-algorithms proposed in this paper on a Spanish dataset, using the Spanish WordNet as knowledge source (Atserias et al., 2004a).
We used the Semeval-2007 Task 09 dataset as evaluation gold standard (M`arquez et al., 2007).
The dataset contains examples of the 150 most frequent nouns in the CESS-ECE corpus, manually annotated with Spanish WordNet synsets.
It is split into a train and test part, and has an “all words” shape i.e. input consists on sentences, each one having at least one occurrence of a target noun.
We ran the experiment over the test part (792 instances), and used the train part for calculating the MFS baseline.
We used the Spanish WordNet as LKB, enriched with eXtended WordNet relations.
It contains 105, 501 nodes and 623,316 relations.
The results in Table 3 are consistent with those for English, with our algorithm approaching MFS performance.
Note that for this dataset the supervised algorithm could barely improve over the MFS, suggesting that for this particular dataset MFS is particularly strong.
<newSection> 7 Performance analysis Table 4 shows the time spent by the different algorithms when applied to the Senseval-2 all words dataset, using the WNet17 + Xwn as LKB.
The dataset consists on 2473 word instances appearing on 476 different sentences.
The experiments were done on a computer with four 2.66 Ghz processors and 16 Gb memory.
The table shows that the time elapsed by the algorithms varies between 30 minutes for the Ppr method (which thus dis-ambiguates circa 82 instances per minute) to almost 3 hours spent by the Ppr w2w method (circa 15 instances per minute).
The Spr method lies in between, requiring 2 hours for completing the task, but its overall performance is well below the PageRank based Ppr w2w method.
Note that the algorithm is coded in C++ for greater efficiency, and uses the Boost Graph Library.
Regarding PageRank calculation, we have tried different numbers of iterations, and analyze the rate of convergence of the algorithm.
Figure 1 depicts the performance of the Ppr w2w method for different iterations of the algorithm.
As before, the algorithm is applied over the MCR17 + Xwn LKB, and evaluated on the Senseval-2 all words dataset.
The algorithm converges very quickly: one sole iteration suffices for achieving a relatively high performance, and 20 iterations are enough for achieving convergence.
The figure shows that, depending on the LKB complexity, the user can tune the algorithm and lower the number of iterations, thus considerably reducing the time required for disambiguation.
<newSection> 8 Conclusions In this paper we propose a new graph-based method that uses the knowledge in a LKB (based on WordNet) in order to perform unsupervised Word Sense Disambuation.
Our algorithm uses the full graph of the LKB efficiently, performing better than previous approaches in English all-words datasets.
We also show that the algorithm can be easily ported to other languages with good results, with the only requirement of having a wordnet.
Both for Spanish and English the algorithm attains performances close to the MFS.
The algorithm is publicly available5 and can be applied easily to sense inventories and knowledge bases different from WordNet.
Our analysis shows that our algorithm is efficient compared to previously proposed alternatives, and that a good choice of WordNet versions and relations is fundamental for good performance.
<newSection> Acknowledgments This work has been partially funded by the EU Commission (project KYOTO ICT-2007-211423) and Spanish Research Department (project KNOW TIN2006-15049-C03-01).
<newSection> References<newSection> Abstract In this paper we compare and contrast two approaches to Machine Translation (MT): the CMU-UKA Syntax Augmented Machine Translation system (SAMT) and UPC-TALP N-gram-based Statistical Machine Translation (SMT).
SAMT is a hierarchical syntax-driven translation system underlain by a phrase-based model and a target part parse tree.
In N-gram-based SMT, the translation process is based on bilingual units related to word-to-word alignment and statistical modeling of the bilingual context following a maximum-entropy framework.
We provide a step-by-step comparison of the systems and report results in terms of automatic evaluation metrics and required computational resources for a smaller Arabic-to-English translation task (1.5M tokens in the training corpus).
Human error analysis clarifies advantages and disadvantages of the systems under consideration.
Finally, we combine the output of both systems to yield significant improvements in translation quality.
<newSection> 1 Introduction There is an ongoing controversy regarding whether or not information about the syntax of language can benefit MT or contribute to a hybrid system.
Classical IBM word-based models were recently augmented with a phrase translation capability, as shown in Koehn et al.
(2003), or in more recent implementation, the MOSES MT system1 (Koehn et al., 2007).
In parallel to the phrase-based approach, the N-gram-based approach appeared (Mariño et al., 2006).
It stemms from the Finite-State Transducers paradigm, and is extended to the log-linear modeling framework, as shown in (Mariño et al., 2006).
A system following this approach deals with bilingual units, called tuples, which are composed of one or more words from the source language and zero or more words from the target one.
The N-gram-based systems allow for linguistically motivated word reordering by implementing word order monotonization.
Prior to the SMT revolution, a major part of MT systems was developed using rule-based algorithms; however, starting from the 1990’s, syntax-driven systems based on phrase hierarchy have gained popularity.
A representative sample of modern syntax-based systems includes models based on bilingual synchronous grammar (Melamed, 2004), parse tree-to-string translation models (Yamada and Knight, 2001) and non-isomorphic tree-to-tree mappings (Eisner, 2003).
The orthodox phrase-based model was enhanced in Chiang (2005), where a hierarchical phrase model allowing for multiple generalizations within each phrase was introduced.
The open-source toolkit SAMT2 (Zollmann and Venu-gopal, 2006) is a further evolution of this approach, in which syntactic categories extracted from the target side parse tree are directly assigned to the hierarchically structured phrases.
Several publications discovering similarities and differences between distinct translation models have been written over the last few years.
In Crego et al.
(2005b), the N-gram-based system is contrasted with a state-of-the-art phrase-based framework, while in DeNeefe et al.
(2007), the authors seek to estimate the advantages, weakest points and possible overlap between syntax-based MT and phrase-based SMT.
In Zollmann et al.
(2008) the comparison of phrase-based, &quot;Chi-ang’s style&quot; hirearchical system and SAMT is provided.
In this study, we intend to compare the differences and similarities of the statistical N-grambased SMT approach and the SAMT system.
The comparison is performed on a small Arabic-toEnglish translation task from the news domain.
<newSection> 2 SAMT system A criticism of phrase-based models is data sparseness.
This problem is even more serious when the source, the target, or both languages are inflectional and rich in morphology.
Moreover, phrase-based models are unable to cope with global reordering because the distortion model is based on movement distance, which may face computa-tional resource limitations (Och and Ney, 2004).
This problem was successfully addressed when the MT system based on generalized hierarchi-cally structured phrases was introduced and discussed in Chiang (2005).
It operates with only two markers (a substantial phrase category and &quot;a glue marker&quot;).
Moreover, a recent work (Zollmann and Venugopal, 2006) reports significant improvement in terms of translation quality if complete or partial syntactic categories (derived from the target side parse tree) are assigned to the phrases.
A formalism for Syntax Augmented Translation is probabilistic synchronous context-free grammar (PSynCFG), which is defined in terms of source and target terminal sets and a set of non-terminals: X ) (-y, α, —, w) where X is a non-terminal, -y is a sequence of source-side terminals and non-terminals, α is a sequence of target-side terminals and non-terminals, — is a one-to-one mapping from non-terminal tokens space in -y to non-terminal space in α, and w is a non-negative weight assigned to the rule.
The non-terminal set is generated from the syntactic categories corresponding to the target-side Penn Treebank set, a set of glue rules and a special marker representing the &quot;Chiang-style&quot; rules, which do not span the parse tree.
Consequently, all lexical mapping rules are covered by the phrases mapping table.
The SAMT system is based on a purely lexical phrase table, which is identified as shown in Koehn et al.
(2003), and word alignment, which is generated by the grow-diag-final-and method (ex-panding the alignment by adding directly neigh-boring alignment points and alignment points in the diagonal neighborhood) (Och and Ney, 2003).
Meanwhile, the target of the training corpus is parsed with Charniak’s parser (Charniak, 2000), and each phrase is annotated with the constituent that spans the target side of the rules.
The set of non-terminals is extended by means of conditional and additive categories according to Combinatory Categorical Grammar (CCG) (Steedman, 1999).
Under this approach, new rules can be formed.
For example, RB+VB, can represent an additive constituent consisting of two synthetically generated adjacent categories 3, i.e., an adverb and a verb.
Furthermore, DT�NP can indicate an incomplete noun phrase with a missing determiner to the left.
The rule recursive generalization procedure coincides with the one proposed in Chiang (2005), but violates the restrictions introduced for single-category grammar; for example, rules that contain adjacent generalized elements are not discarded.
where 1 < i < u < m and 1 < j < v < n, to obtain a new rule where k is an index for the non-terminal M that indicates a one-to-one correspondence between the new M tokens on the two sides.
Figure 1 shows an example of initial rules extraction, which can be further extended using the hierarchical model, as shown in Figure 2 (conse-quently involving more general elements in rule description).
Rules pruning is necessary because the set of generalized rules can be huge.
Pruning is performed according to the relative frequency and the nature of the rules: non-lexical rules that have been seen only once are discarded; source-conditioned rules with a relative frequency of appearance below a threshold are also eliminated.
Rules that do not contain non-terminals are not pruned.
The decoding process is accomplished using a top-down log-linear model.
The source sentence is decoded and enriched with the PSynCFG in such a way that translation quality is represented by a set of feature functions for each rule, i.e.: The decoding process can be represented as a search through the space of neg log probability of the target language terminals.
The set of feature functions is combined with a finite-state target-side n-gram language model (LM), which is used to derive the target language sequence during a parsing decoding.
The feature weights are optimized according to the highest BLEU score.
For more details refer to Zollmann and Venu-gopal (2006).
<newSection> 3 UPC n-gram SMT system A description of the UPC-TALP N-gram translation system can be found in Mariño et al.
(2006).
SMT is based on the principle of translating a source sentence (f) into a sentence in the target language (e).
The problem is formulated in terms of source and target languages; it is defined according to equation (1) and can be reformulated as selecting a translation with the highest probability from a set of target sentences (2): where I and J represent the number of words in the target and source languages, respectively.
Modern state-of-the-art SMT systems operate with the bilingual units extracted from the parallel corpus based on word-to-word alignment.
They are enhanced by the maximum entropy approach and the posterior probability is calculated as a log-linear combination of a set of feature functions (Och and Ney, 2002).
Using this technique, the additional models are combined to determine the translation hypothesis, as shown in (3): where the feature functions hm refer to the system models and the set of am refers to the weights cor-responding to these models.
The N-gram approach to SMT is considered to be an alternative to the phrase-based translation, where a given source word sequence is decomposed into monolingual phrases that are then translated one by one (Marcu and Wong, 2002).
The N-gram-based approach regards translation as a stochastic process that maximizes the joint probability p(f, e), leading to a decomposition based on bilingual n-grams.
The core part of the system constructed in this way is a translation model (TM), which is based on bilingual units, called tuples, that are extracted from a word alignment (performed with GIZA++ tool4) according to certain constraints.
A bilingual TM actually constitutes an n-gram LM of tuples, which approximates the joint probability between the languages under consideration and can be seen here as a LM, where the language is composed of tuples.
The N-gram translation system implements a log-linear combination of five additional models: An extended monotone distortion model based on the automatically learned reordering rules was implemented as described in Crego and Mariño (2006).
Based on the word-to-word alignment, tu-ples were extracted by an unfolding technique.
As a result, the tuples were broken into smaller tuples, and these were sequenced in the order of the target words.
An example of unfolding tuple extraction, contrasted with the SAMT chunk-based rules construction, is presented in Figure 1.
The reordering strategy is additionally supported by a 4-gram LM of reordered source POS tags.
In training, POS tags are reordered according to the extracted reordering patterns and word-toword links.
The resulting sequence of source POS tags is used to train the n-gram LM.
The open-source MARIE5 decoder was used as a search engine for the translation system.
Details can be found in Crego et al.
(2005a).
The decoder implements a beam-search algorithm with pruning capabilities.
All the additional feature models were taken into account during the decoding process.
Given the development set and references, the log-linear combination of weights was adjusted using a simplex optimization method and an n-best re-ranking as described in http://www.statmt.org/jhuws/.
<newSection> 4 Experiments As training corpus, we used the 50K first-lines extraction from the Arabic-English corpus that was provided to the NIST’086 evaluation campaign and belongs to the news domain.
The corpus statistics can be found in Table 1.
The development and test sets were provided with 4 reference translations, belong to the same domain and contain 663 and 500 sentences, respectively.
Evaluation conditions were case-insensitive and sensitive to tokenization.
The word alignment is automatically computed by using GIZA++ (Och and Ney, 2004) in both directions, which are made symmetric by using the grow-diag-final-and operation.
The experiments were done on a dual-processor Pentium IV Intel Xeon Quad Core X5355 2.66 GHz machine with 24 G of RAM.
All computational times and memory size results are approximated.
Arabic is a VSO (SVO in some cases) pro-drop language with rich templatic morphology, where words are made up of roots and affixes and clitics agglutinate to words.
For preprocessing, a similar approach to that shown in Habash and Sadat (2006) was employed, and the MADA+TOKAN system for disambiguation and tokenization was used.
For disambiguation, only diacritic unigram statistics were employed.
For tokenization, the D3 scheme with -TAGBIES option was used.
The scheme splits the following set of clitics: w+, f+, b+, k+, l+, Al+ and pronominal clitics.
The -TAGBIES option produces Bies POS tags on all taggable tokens.
The SAMT guideline was used to perform the experiments and is available on-line: http://www.cs.cmu.edu/∼zollmann/samt/.
Moses MT script was used to create the grow — diag — final word alignment and extract purely lexical phrases, which are then used to induce the SAMT grammar.
The target side (English) of the training corpus was parsed with the Charniak’s parser (Charniak, 2000).
Rule extraction and filtering procedures were restricted to the concatenation of the development and test sets, allowing for rules with a maximal length of 12 elements in the source side and with a zero minimum occurrence criterion for both non-lexical and purely lexical rules.
Moses-style phrases extracted with a phrase-based system were 4.8M, while a number of generalized rules representing the hierarchical model grew dramatically to 22.9M.
10.8M of them were pruned out on the filtering step.
The vocabulary of the English Penn Treebank elementary non-terminals is 72, while a number of generalized elements, including additive and truncated categories, is 35.7K.
The FastTranslateChart beam-search decoder was used as an engine of MER training aiming to tune the feature weight coefficients and produce final n-best and 1-best translations by combining the intensive search with a standard 4-gram LM as shown in Venugopal et al.
(2007).
The iteration limit was set to 10 with 1000-best list and the highest BLEU score as optimization criteria.
We did not use completely abstract rules (with-out any source-side lexical utterance), since these rules significantly slow down the decoding process (noAllowAbstractRules option).
Table 2 shows a summary of computational time and RAM needed at each step of the translation.
The core model of the N-gram-based system is a 4-gram LM of bilingual units containing: 184.345 1-grams7, 552.838 2-grams, 179.466 3-grams and 176.2214-grams.
Along with this model, an N-gram SMT system implements a log-linear combination of a 5-gram target LM estimated on the English portion of the parallel corpus, as well as supporting 4-gram source and target models of POS tags.
Bies POS tags were used for the Arabic portion, as shown in subsection 4.2; a TnT tool was used for English POS tagging (Brants, 2000).
The number of non-unique initially extracted tuples is 1.1M, which were pruned according to the maximum number of translation options per tuple on the source side (30).
Tuples with a NULL on the source side were attached to either the previous or the next unit (Mariño et al., 2006).
The feature models weights were optimized according to the same optimization criteria as in the SAMT experiments (the highest BLEU score).
Stage-by-stage RAM and time requirements are presented in Table 4, while translation quality evaluation results can be found in Table 3.
A statistical significance test based on a bootstrap resampling method, as shown in Koehn (2004), was performed.
For the 98% confidence interval and 1000 set resamples, translations generated by SAMT and N-gram system are significantly different according to BLEU (43.20±1.69 for SAMT vs. 46.42 ± 1.61 for tuple-based system).
Many MT systems generate very different translations of similar quality, even if the models involved into translation process are analogous.
Thus, the outputs of syntax-driven and purely statistical MT systems were combined at the sentence level using 1000-best lists of the most probable translations produced by the both systems.
For system combination, we followed a Minimum Bayes-risk algorithm, as introduced in Ku-mar and Byrne (2004).
Table 3 shows the results of the system combination experiments on the test set, which are contrasted with the oracle translation results, performed as a selection of the translations with the highest BLEU score from the union of two 1000-best lists generated by SAMT and N-gram SMT.
We also analyzed the percentage contribution of each system to the system combination: 55-60% of best translations come from the tuples-based system 1000-best list, both for system combination and oracle experiments on the test set.
In order to understand the obtained results compared to the state-of-the-art SMT, a reference phrase-based factored SMT system was trained and tested on the same data using the MOSES toolkit.
Surface forms of words (factor “0“), POS (factor “1“) and canonical forms of the words (lemmata) (factor “2“) were used as English factors, and surface forms and POS were the Arabic factors.
Word alignment was performed according to the grow-diag-final algorithm with the GIZA++ tool, a msd-bidirectional-fe conditional reordering model was trained; the system had access to the target-side 4-gram LMs of words and POS.
The 0-0,1+0-1,2+0-1 scheme was used on the translation step and 1,2-0,1+1-0,1 to create generation tables.
A detailed description of the model training can be found on the MOSES tutorial web-page8.
The results may be seen in Table 3.
<newSection> 5 Error analysis To understand the strong and weak points of both systems under consideration, a human analysis of the typical translation errors generated by each system was performed following the framework proposed in Vilar et al.
(2006) and contrasting the systems output with four reference translations.
Human evaluation of translation output is a time-consuming process, thus a set of 100 randomly chosen sentences was picked out from the corre-sponding system output and was considered as a representative sample of the automatically generated translation of the test corpus.
According to the proposed error topology, some classes of errors can overlap (for example, an unknown word can lead to a reordering problem), but it allows finding the most prominent source of errors in a reliable way (Vilar et al., 2006; Povovic et al., 2006).
Table 5 presents the comparative statistics of errors generated by the SAMT and the N-gram-based SMT systems.
The average length of the generated translations is 32.09 words for the SAMT translation and 35.30 for the N-gram-based system.
Apart from unknown words, the most important sources of errors of the SAMT system are missing content words and extra words generated by the translation system, causing 17.22 % and 10.60 % of errors, respectively.
A high number of missing content words is a serious problem affecting the translation accuracy.
In some cases, the system is able to construct a grammatically correct translation, but omitting an important content word leads to a significant reduction in translation accuracy: SAMT translation: the ministers of arab environment for the closure of the Israeli dymwnp reactor.
Extra words embedded into the correctly translated phrases are a well-known problem of MT systems based on hierarchical models operating on the small corpora.
For example, in many cases the Arabic expression AlbHr Almyt is translated into English as dead sea side and not as dead sea, since the bilingual instances contain only the whole English phrase, like following: AlbHr Almyt#the dead sea side#@NP The N-gram-based system handles missing words more correctly – only 9.40 % of the errors come from the missing content words; however, it does not handle local and long-term reordering, thus the main problem is phrase reordering (11.41 % and 8.05 % of errors).
In the example below, the underlined block (Circumstantial Complement: from local officials in the tourism sector) is embedded between the verb and the direct object, while in correct translation it must be placed in the end of the sentence.
N-gram translation: the winner received Along with inserting extra words and wrong lexical choice, another prominent source of incorrect translation, generated by the N-gram system, is an erroneous grammatical form selection, i.e., a situation when the system is able to find the correct translation but cannot choose the correct form.
For example, arab environment minister call for closing dymwnp Israeli reactor, where the verb-preposition combination call for was correctly translated on the stem level, but the system was not able to generate a third person conjugation calls for.
In spite of the fact that English is a language with nearly no inflection, 9.40 % of errors stem from poor word form modeling.
This is an example of the weakest point of the SMT systems having access to a small training material; the decoder does not use syntactic information about the subject of the sentence (singular) and makes a choice only concerning the tuple probability.
The difference in total number of errors is negligible, however a subjective evaluation of the systems output shows that the translation generated by the N-gram system is more understandable than the SAMT one, since more content words are translated correctly and the meaning of the sentence is still preserved.
<newSection> 6 Discussion and conclusions In this study two systems are compared: the UPC-TALP N-gram-based and the CMU-UKA SAMT systems, originating from the ideas of Finite-State Transducers and hierarchical phrase translation, respectively.
The comparison was created to be as fair as possible, using the same training material and the same tools on the preprocessing, word-to-word alignment and language modeling steps.
The obtained results were also contrasted with the state-of-the-art phrase-based SMT.
Analyzing the automatic evaluation scores, the N-gram-based approach shows good performance for the small Arabic-to-English task and significantly outperforms the SAMT system.
The results shown by the modern phrase-based SMT (factored MOSES) lie between the two systems under con-sideration.
Considering memory size and compu-tational time, the tuple-based system has obtained significantly better results than SAMT, primarily because of its smaller search space.
Interesting results were obtained for the PER and WER metrics: according to the PER, the UPC-TALP system outperforms the SAMT by 10%, while the WER improvement hardly achieves a 2% difference.
The N-gram-based SMT can translate the context better, but produces more reordering errors than SAMT.
This may be explained by the fact that Arabic and English are languages with high disparity in word order, and the N-gram system deals worse with long-distance reordering because it attempts to use shorter units.
However, by means of introducing the word context into the TM, short-distance bilingual dependencies can be captured effectively.
The main conclusion that can be made from the human evaluation analysis is that the systems commit a comparable number of errors, but they are distributed dissimilarly.
In case of the SAMT system, the frequent errors are caused by missing or incorrectly inserted extra words, while the N-gram-based system suffers from reordering problems and wrong words/word form choice Significant improvement in translation quality was achieved by combining the outputs of the two systems based on different translating principles.
<newSection> 7 Acknowledgments This work has been funded by the Spanish Government under grant TEC2006-13964-C03 (AVI-VAVOZ project).
J. B. Mariño, R. E. Banchs, J. M. Crego, A. de Gispert, P. Lambert, J. A. R. Fonollosa, and M. R. Costa-jussà.
2006. N-gram based machine translation.
Computational Linguistics, 32(4):527–549, December.
<newSection> References<newSection> Abstract We investigate linguistic features that correlate with the readability of texts for adults with intellectual disabilities (ID).
Based on a corpus of texts (including some experimentally measured for comprehension by adults with ID), we analyze the significance of novel discourse-level features related to the cognitive factors underlying our users’ literacy challenges.
We develop and evaluate a tool for automatically rating the readability of texts for these users.
Our experiments show that our discourse-level, cognitively-motivated features improve automatic readability assessment.
<newSection> 1 Introduction Assessing the degree of readability of a text has been a field of research as early as the 1920's.
Dale and Chall define readability as “the sum total (including all the interactions) of all those elements within a given piece of printed material that affect the success a group of readers have with it.
The success is the extent to which they understand it, read it at optimal speed, and find it interesting” (Dale and Chall, 1949).
It has long been acknowledged that readability is a function of text characteristics, but also of the readers themselves.
The literacy skills of the readers, their motivations, background knowledge, and other internal characteristics play an important role in determining whether a text is readable for a particular group of people.
In our work, we investigate how to assess the readability of a text for people with intellectual disabilities (ID).
Previous work in automatic readability assessment has focused on generic features of a text at the lexical and syntactic level.
While such features are essential, we argue that audience-specific features that model the cognitive charac-teristics of a user group can improve the accuracy of a readability assessment tool.
The contri-butions of this paper are: (1) we present a corpus of texts with readability judgments from adults with ID; (2) we propose a set of cognitively-motivated features which operate at the discourse level; (3) we evaluate the utility of these features in predicting readability for adults with ID.
Our framework is to create tools that benefit people with intellectual disabilities (ID), specifically those classified in the “mild level” of mental retardation, IQ scores 55-70.
About 3% of the U.S. population has intelligence test scores of 70 or lower (U.S. Census Bureau, 2000).
People with ID face challenges in reading literacy.
They are better at decoding words (sounding them out) than at comprehending their meaning (Drew & Hardman, 2004), and most read below their mental age-level (Katims, 2000).
Our research addresses two literacy impairments that distinguish people with ID from other low-literacy adults: limitations in (1) working memory and (2) discourse representation.
People with ID have problems remembering and inferring information from text (Fowler, 1998).
They have a slower speed of semantic encoding and thus units are lost from the working memory before they are processed (Perfetti & Lesgold, 1977; Hickson-Bilsky, 1985).
People with ID also have trouble building cohesive representations of discourse (Hickson-Bilsky, 1985).
As less information is integrated into the mental representation of the current discourse, less is comprehended.
Adults with ID are limited in their choice of reading material.
Most texts that they can readily understand are targeted at the level of readability of children.
However, the topics of these texts often fail to match their interests since they are meant for younger readers.
Because of the mismatch between their literacy and their interests, users may not read for pleasure and therefore miss valuable reading-skills practice time.
In a feasibility study we conducted with adults with ID, we asked participants what they enjoyed learning or reading about.
The majority of our subjects mentioned enjoying watching the news, in particular local news.
Many mentioned they were interested in information that would be relevant to their daily lives.
While for some genres, human editors can prepare texts for these users, this is not practical for news sources that are frequently updated and specific to a limited geographic area (like local news).
Our goal is to create an automatic metric to predict the readability of local news articles for adults with ID.
Because of the low levels of written literacy among our target users, we intend to focus on comprehension of texts displayed on a computer screen and read aloud by text-to-speech software; although some users may depend on the text-tospeech software, we use the term readability.
This paper is organized as follows.
Section 2 presents related work on readability assessment.
Section 3 states our research hypotheses and describes our methodology.
Section 4 focuses on the data sets used in our experiments, while section 5 describes the feature set we used for readability assessment along with a corpus-based analysis of each feature.
Section 6 describes a readability assessment tool and reports on evaluation.
Section 7 discusses the implications of the work and proposes direction for future work.
<newSection> 2 Related Work on Readability Metrics Many readability metrics have been established as a function of shallow features of texts, such as the number of syllables per word and number of words per sentence (Flesch, 1948; McLaughlin, 1969; Kincaid et al., 1975).
These so-called traditional readability metrics are still used today in many settings and domains, in part because they are very easy to compute.
Their results, however, are not always representative of the complexity of a text (Davison and Kantor, 1982).
They can easily misrepresent the complexity of technical texts, or reveal themselves un-adapted to a set of readers with particular reading difficulties.
Other formulas rely on lexical information; e.g., the New Dale-Chall readability formula consults a static, manually-built list of “easy” words to determine whether a text contains unfamiliar words (Chall and Dale, 1995).
Researchers in computational linguistics have investigated the use of statistical language models (unigram in particular) to capture the range of vocabulary from one grade level to another (Si and Callan, 2001; Collins-Thompson and Callan, 2004).
These metrics predicted readability better than traditional formulas when tested against a corpus of web pages.
The use of syntactic features was also investigated (Schwarm and Osten-dorf, 2005; Heilman et al., 2007; Petersen and Ostendorf, 2009) in the assessment of text readability for English as a Second Language readers.
While lexical features alone outperform syntactic features in classifying texts according to their reading levels, combining the lexical and syntactic features yields the best results.
Several elegant metrics that focus solely on the syntax of a text have also been developed.
The Yngve (1960) measure, for instance, focuses on the depth of embedding of nodes in the parse tree; others use the ratio of terminal to non-terminal nodes in the parse tree of a sentence (Miller and Chomsky, 1963; Frazier, 1985).
These metrics have been used to analyze the writing of potential Alzheimer's patients to detect mild cognitive impairments (Roark, Mitchell, and Hollingshead, 2007), thereby indicating that cognitively motivated features of text are valuable when creating tools for specific populations.
Barzilay and Lapata (2008) presented early work in investigating the use of discourse to distinguish abridged from original encyclopedia articles.
Their focus, however, is on style detection rather than readability assessment per se.
Coh-Metrix is a tool for automatically calculating text coherence based on features such as repetition of lexical items across sentences and latent semantic analysis (McNamara et al., 2006).
The tool is based on comprehension data collected from children and college students.
Our research differs from related work in that we seek to produce an automatic readability metric that is tailored to the literacy skills of adults with ID.
Because of the specific cognitive cha-racteristics of these users, it is an open question whether existing readability metrics and features are useful for assessing readability for adults with ID.
Many of these earlier metrics have focused on the task of assigning texts to particular elementary school grade levels.
Traditional grade levels may not be the ideal way to score texts to indicate how readable they are for adults with ID.
Other related work has used models of vocabulary (Collins-Thompson and Callan, 2004).
Since we would like to use our tool to give adults with ID access to local news stories, we choose to keep our metric topic-independent.
Another difference between our approach and previous approaches is that we have designed the features used by our readability metric based on the cognitive aspects of our target users.
For example, these users are better at decoding words than at comprehending text meaning (Drew & Hardman, 2004); so, shallow features like “sylla-ble count per word” or unigram models of word frequency (based on texts designed for children) may be less important indicators of reading difficulty.
A critical challenge for our users is to create a cohesive representation of discourse.
Due to their impairments in semantic encoding speed, our users may have particular difficulty with texts that place a significant burden on working memory (items fall out of memory before they can be semantically encoded).
While we focus on readability of texts, other projects have automatically generated texts for people with aphasia (Carroll et al., 1999) or low reading skills (Williams and Reiter, 2005).
<newSection> 3 Research Hypothesis and Methods We hypothesize that the complexity of a text for adults with ID is related to the number of entities referred to in the text overall.
If a paragraph or a text refers to too many entities at once, the reader has to work harder at mapping each entity to a semantic representation and deciding how each entity is related to others.
On the other hand, when a text refers to few entities, less work is required both for semantic encoding and for integrating the entities into a cohesive mental re-presentation.
Section 5.2 discusses some novel discourse-level features (based on the “entity density” of a text) that we believe will correlate to comprehension by adults with ID.
To test our hypothesis, we used the following methodology.
We collected four corpora (as described in Section 4).
Three of them (Britannica, LiteracyNet and WeeklyReader) have been examined in previous work on readability.
The fourth (LocalNews) is novel and results from a user study we conducted with adults with ID.
We then analyzed how significant each feature is on our Britannica and LiteracyNet corpora.
Finally, we combined the significant features into a linear regression model and experimented with several feature combinations.
We evaluated our model on the WeeklyReader and LocalNews corpora.
<newSection> 4 Corpora and Readability Judgments To study how certain linguistic features indicate the readability of a text, we collected a corpus of English text at different levels of readability.
An ideal corpus for our research would contain texts that have been written specifically for our audience of adults with intellectual disabilities – in particular if such texts were paired with alternate versions of each text written for a general audience.
We are not aware of such texts available electronically, and so we have instead mostly collected texts written for an audience of children.
The texts come from online and commercial sources, and some have been analyzed previously by text simplification researchers (Petersen and Ostendorf, 2009).
Our corpus also contains some novel texts produced as part of an experimental study involving adults with ID.
The first section of our corpus (which we refer to as Britannica) has 228 articles from the Encyclopedia Britannica, originally collected by (Barzi-lay and Elhadad, 2003).
This consists of 114 articles in two forms: original articles written for adults and corresponding articles rewritten for an audience of children.
While the texts are paired, the content of the texts is not identical: some details are omitted from the child version, and additional background is sometimes inserted.
The resulting corpus is comparable in content.
Because we are particularly interested in making local news articles accessible to adults with ID, we collected a second paired corpus, which we refer to as LiteracyNet, consisting of 115 news articles made available through (West-ern/Pacific Literacy Network / LiteracyNet, 2008).
The collection of local CNN stories is available in an original and simplified/abridged form (230 total news articles) designed for use in literacy education.
The third corpus we collected (Weekly Reader) was obtained from the Weekly Reader corporation (Weekly Reader, 2008).
It contains articles for students in elementary school.
Each text is labeled with its target grade level (grade 2: 174 articles, grade 3: 289 articles, grade 4: 428 articles, grade 5: 542 articles).
Overall, the corpus has 1433 articles.
(U.S. elementary school grades 2 to 5 generally are for children ages 7 to 10.)
The corpora discussed above are similar to those used by Petersen and Ostendorf (2009).
While the focus of our research is adults with ID, most of the texts discussed in this section have been simplified or written by human authors to be readable for children.
Despite the texts being intended for a different audience than the focus of our research, we still believe these texts to be of value.
It is rare to encounter electronically available corpora in which an original and a simplified version of a text is paired (as in the Britannica and LiteracyNet corpora) or texts labeled as being at specific levels of readability (as in the Weekly Reader corpus).
The final section of our corpus contains local news articles that are labeled with comprehen-sion scores.
These texts were produced for a feasibility study involving adults with ID.
Each text was read by adults with ID, who then answered comprehension questions to measure their under-standing of the texts.
Unlike the previous corpora, LocalNews is novel and was not investigated by previous research in readability.
After obtaining university approval for our experimental protocol and informed consent process, we conducted a study with 14 adults with mild intellectual disabilities who participate in daytime educational programs in the New York area.
Participants were presented with ten articles collected from various local New York based news websites.
Some subjects saw the original form of an article and others saw a simplified form (edited by a human author); no subject saw both versions.
The texts were presented in random order using software that displayed the text on the screen, read it aloud using text-tospeech software, and highlighted each word as it was read.
Afterward, subjects were asked aloud multiple-choice comprehension questions.
We defined the readability score of a story as the percentage of correct answers averaged across the subjects who read that particular story.
A human editor performed the text simplifica-tion with the goal of making the text more readable for adults with mild ID.
The editor made the following types of changes to the original news stories: breaking apart complex sentences, un-embedding information in complex prepositional phrases and reintegrating it as separate sentences, replacing infrequent vocabulary items with more common/colloquial equivalents, omitting sentences and phrases from the story that mention entities and phrases extraneous to the main theme of the article.
For instance, the original sentence “They’re installing an induction loop system in cabs that would allow passengers with hearing aids to tune in specifically to the driver’s voice.” was transformed into “They’re installing a system in cabs.
It would allow passengers with hearing aids to listen to the driver’s voice.”
This corpus of local news articles that have been human edited and scored for comprehen-sion by adults with ID is small in size (20 news articles), but we consider it a valuable resource.
Unlike the texts that have been simplified for children (the rest of our corpus), these texts have been rated for readability by actual adults with ID.
Furthermore, comprehension scores are derived from actual reader comprehension tests, rather than self-perceived comprehension.
Because of the small size of this part of our corpus, however, we primarily use it for evaluation purposes (not for training the readability models).
<newSection> 5 Linguistic Features and Readability We now describe the set of features we investigated for assessing readability automatically.
Table 1 contains a list of the features – including a short code name for each feature which may be used throughout this paper.
We have begun by implementing the simple features used by the Flesh-Kincaid and FOG metrics: average number of words per sentence, average number of syllables per word, and percentage of words in the document with 3+ syllables.
We have also implemented features inspired by earlier research on readability.
Petersen and Os-tendorf (2009) included features calculated from parsing the sentences in their corpus using the Charniak parser (Charniak, 2000): average parse tree height, average number of noun phrases per sentence, average number of verb phrases per sentence, and average number of SBARs per sentence.
We have implemented versions of most of these parse-tree-related features for our project.
We also parse the sentences in our corpus using Charniak’s parser and calculate the following features listed in Table 1: aNP, aN, aVP, aAdj, aSBr, aPP, nNP, nN, nVP, nAdj, nSBr, and nPP.
Because of the special reading characteristics of our target users, we have designed a set of cogni-tively motivated features to predict readability of texts for adults with ID.
We have discussed how working memory limits the semantic encoding of new information by these users; so, our features indicate the number of entities in a text that the reader must keep in mind while reading each sentence and throughout the entire document.
It is our hypothesis that this “entity density” of a text plays an important role in the difficulty of that text for readers with intellectual disabilities.
The first set of features incorporates the Ling-Pipe named entity detection software (Alias-i, 2008), which detects three types of entities: person, location, and organization.
We also use the part-of-speech tagger in LingPipe to identify the common nouns in the document, and we find the union of the common nouns and the named entity noun phrases in the text.
The union of these two sets is our definition of “entity” for this set of features.
We count both the total number of “entity mentions” in a text (each token appearance of an entity) and the total number of unique entities (exact-string-match duplicates only counted once).
Table 1 lists these features: nEM, nUE, aEM, and aUE.
We count the totals per document to capture how many entities the reader must keep track of while reading the document.
We also expect sentences with more entities to be more difficult for our users to semantically encode due to working memory limitations; so, we also count the averages per sentence to capture how many entities the reader must keep in mind to understand each sentence.
To measure the working memory burden of a text, we’d like to capture the number of discourse entities that a reader must keep in mind.
However, the “unique entities” identified by the named entity recognition tool may not be a perfect representation of this – several unique entities may actually refer to the same real-world entity under discussion.
To better model how multiple noun phrases in a text refer to the same entity or concept, we have also built features using lexical chains (Galley and McKeown, 2003).
Lexical chains link nouns in a document connected by relations like synonymy or hyponomy; chains can indicate concepts that recur throughout a text.
A lexical chain has both a length (number of noun phrases it includes) and a span (number of words in the document between the first noun phrase at the beginning of the chain and the last noun phrase that is part of the chain).
We calculate the number of lexical chains in the document (nLC) and those with a span greater than half the document length (nLC2).
We believe these features may indicate the number of entities/concepts that a reader must keep in mind during a document and the subset of very important entities/concepts that are the main topic of the document.
The average length and average span of the lexical chains in a document (aLCL and aLCS) may also indicate how many of the chains in the document are short-lived, which may mean that they are ancillary entities/concepts, not the main topics.
The final two features in Table 1 (aLCw and aLCe) use the concept of an “active” chain.
At a particular location in a text, we define a lexical chain to be “active” if the span (between the first and last noun in the lexical chain) includes the current location.
We expect these features may indicate the total number of concepts that the reader needs to keep in mind during a specific moment in time when reading a text.
Measuring the average number of concepts that the reader of a text must keep in mind may suggest the working memory burden of the text over time.
We were unsure if individual words or individual noun-phrases in the document should be used as the basic unit of “time” for the purpose of averaging the number of active lexical chains; so, we included both features.
To select which features to include in our automatic readability assessment tool (in Section 6), we analyzed the documents in our paired corpora (Britannica and LiteracyNet).
Because they contain a complex and a simplified version of each article, we can examine differences in readability while holding the topic and genre constant.
We calculated the value of each feature for each document, and we used a paired t-test to determine if the difference between the complex and simple documents was significant for that corpus.
Table 2 contains the results of this feature selection process; the columns in the table indicate the values for the following corpora: Britannica complex, Britannica simple, LiteracyNet complex, and LiteracyNet simple.
An asterisk appears in the “Sig” column if the difference between the feature values for the complex vs. simple documents is statistically significant for that corpus (significance level: p<0.00001).
The only two features which did not show a significant difference (p>0.01) between the complex and simple versions of the articles were: average lexical chain length (aLCL) and number of lexical chains with span greater than half the document length (nLC2).
The lack of significance for aLCL may be explained by the vast majority of lexical chains containing few members; complex articles contained more of these chains – but their chains did not contain more members.
In the case of nLC2, over 80% of the articles in each category contained no lexical chains whose span was greater than half the document length.
The rarity of a lexical chain spanning the majority of a document may have led to there being no significant difference between complex/simple.
<newSection> 6 A Readability Assessment Tool After testing the significance of features using paired corpora, we used linear regression and our graded corpus (Weekly Reader) to build a readability assessment tool.
To evaluate the tool’s usefulness for adults with ID, we test the correlation of its scores with the LocalNews corpus.
We began our evaluation by implementing three versions of our automatic readability assessment tool.
The first version uses only those features studied by previous researchers (aWPS, aSPW, %3+S, aNP, aN, aVP, aAdj, aSBr, aPP, nNP, nN, nVP, nAdj, nSBr, nPP).
The second version uses only our novel cognitively motivated features (section 5.2).
The third version uses the union of both sets of features.
By building three versions of the tool, we can compare the relative impact of our novel cognitively-motivated features.
For all versions, we have only included those features that showed a significant difference between the complex and simple articles in our paired corpora (as discussed in section 5.3).
Early work on automatic readability analysis framed the problem as a classification task: creating multiple classifiers for labeling a text as being one of several elementary school grade levels (Collins-Thompson and Callan, 2004).
Because we are focusing on a unique user group with special reading challenges, we do not know a priori what level of text difficulty is ideal for our users.
We would not know where to draw category boundaries for classification.
We also prefer that our assessment tool assign numerical difficulty scores to texts.
Thus, after creating this tool, we can conduct further reading com-prehension experiments with adults with ID to determine what threshold (for readability scores assigned by our tool) is appropriate for our users.
To select features for our model, we used our paired corpora (Britannica and LiteracyNet) to measure the significance of each feature.
Now that we are training a model, we make use of our graded corpus (articles from Weekly Reader).
This corpus contains articles that have each been labeled with an elementary school grade level for which it was written.
We divide this corpus – using 80% of articles as training data and 20% as testing data.
We model the grade level of the articles using linear regression; our model is implemented using R (R Development Core Team, 2008).
We conducted two rounds of training and evaluation of our three regression models.
We also compare our models to a baseline readability assessment tool: the popular Flesh-Kincaid Grade Level index (Kincaid et al., 1975).
In the first round of evaluation, we trained and tested our regression models on the Weekly Reader corpus.
This round of evaluation helped to determine whether our feature-set and regression technique were successfully modeling those aspects of the texts that were relevant to their grade level.
Our results from this round of evaluation are presented in the form of average error scores.
(For each article in the Weekly Reader testing data, we calculate the difference between the output score of the model and the correct grade-level for that article.)
Table 3 presents the average error results for the baseline system and our three regression models.
We can see that the model trained on the shallow and parse-related features out-performs the model trained only on our novel features; however, the best model overall is the one is trained on all of the features.
This model predicts the grade level of Weekly Reader articles to within roughly 0.565 grade levels on average.
In our second round of evaluation, we trained the regression model on the Weekly Reader corpus, but we tested it against the LocalNews corpus.
We measured the correlation between our regression models’ output and the comprehen-sion scores of adults with ID on each text.
For this reason, we do not calculate the “average error”; instead, we simply measure the correlation between the models’ output and the comprehen-sion scores.
(We expect negative correlations because comprehension scores should increase as the predicted grade level of the text goes down.)
Table 4 presents the correlations for our three models and the baseline system in the form of Pearson’s R-values.
We see a surprising result: the model trained only on the cognitively-motivated features is more tightly correlated with the comprehension scores of the adults with ID.
While the model trained on all features was better at assigning grade levels to Weekly Reader articles, when we tested it on the local news articles from our user-study, it was not the top-performing model.
This result suggests that the shallow and parse-related features of texts designed for children (the Weekly Reader articles, our training data) are not the best predictors of text readability for adults with ID.
<newSection> 7 Discussion Based on the cognitive and literacy skills of adults with ID, we designed novel features that were useful in assessing the readability of texts for these users.
The results of our study have supported our hypothesis that the complexity of a text for adults with ID is related to the number of entities referred to in the text.
These “entity density” features enabled us to build models that were better at predicting text readability for adults with intellectual disabilities.
This study has also demonstrated the value of collecting readability judgments from target users when designing a readability assessment tool.
The results in Table 4 suggest that models trained on corpora containing texts designed for children may not always lead to accurate models of the readability of texts for other groups of low-literacy users.
Using features targeting specific aspects of literacy impairment have allowed us to make better use of children’s texts when designing a model for adults with ID.
In order to study more features and models of readability, we will require more testing data for tracking progress of our readability regression models.
Our current study has illustrated the usefulness of texts that have been evaluated by adults with ID, and we therefore plan to increase the size of this corpus in future work.
In addition to using this corpus for evaluation, we may want to use it to train our regression models.
For this study, we trained on Weekly Reader text labeled with elementary school grade levels, but this is not ideal.
Texts designed for children may differ from those that are best for adults with ID, and “grade levels” may not be the best way to rank/rate text readability for these users.
While our user-study comprehension-test corpus is currently too small for training, we intend to grow the size of this corpus in future work.
We also plan on refining our cognitively motivated features for measuring the difficulty of a text for our users.
Currently, we use lexical chain software to link noun phrases in a document that may refer to similar entities/concepts.
In future work, we plan to use co-reference resolution software to model how multiple “entity mentions” may refer to a single discourse entity.
For comparison purposes, we plan to implement other features that have been used in earlier readability assessment systems.
For example, Petersen and Ostendorf (2009) created lists of the most common words from the Weekly Reader articles, and they used the percentage of words in a document not on this list as a feature.
The overall goal of our research is to develop a software system that can automatically simplify the reading level of local news articles and present them in an accessible way to adults with ID.
Our automatic readability assessment tool will be a component in this future text simplifica-tion system.
We have therefore preferred to include features in our tool that focus on aspects of the text that can be modified during a simplifica-tion process.
In future work, we will study how to use our readability assessment tool to guide how a text revision system decides to modify a text to increase its readability for these users.
We have contributed to research on automatic readability assessment by designing a new method for assessing the complexity of a text at the level of discourse.
Our novel “entity density” features are based on named entity and lexical chain software, and they are inspired by the cognitive underpinnings of the literacy challenges of adults with ID – specifically, the role of slow semantic encoding and working memory limitations.
We have demonstrated the usefulness of these novel features in modeling the grade level of elementary school texts and in correlating to readability judgments from adults with ID.
Another contribution of our work is the collection of an initial corpus of texts of local news stories that have been manually simplified by a human editor.
Both the original and the simplified versions of these stories have been evaluated by adults with intellectual disabilities.
We have used these comprehension scores in the evaluation phase of this study, and we have suggested how constructing a larger corpus of such articles could be useful for training readability tools.
More broadly, this project has demonstrated how focusing on a specific user population, analyzing their cognitive skills, and involving them in a user-study has led to new insights in modeling text readability.
As Dale and Chall’s definition (1949) originally argued, characteristics of the reader are central to the issue of readability.
We believe our user-focused research paradigm may be used to drive further advances in readability assessment for other groups of users.
<newSection> Acknowledgements We thank the Weekly Reader Corporation for making its corpus available for our research.
We are grateful to Martin Jansche for his assistance with the statistical data analysis and regression.
<newSection> References<newSection> Abstract We present an algorithm for pronoun-anaphora (in English) that uses Expectation Maximization (EM) to learn virtually all of its parameters in an unsupervised fashion.
While EM frequently fails to find good models for the tasks to which it is set, in this case it works quite well.
We have compared it to several systems available on the web (all we have found so far).
Our program significantly outperforms all of them.
The algorithm is fast and robust, and has been made publically available for downloading.
<newSection> 1 Introduction We present a new system for resolving (per-sonal) pronoun anaphora1.
We believe it is of interest for two reasons.
First, virtually all of its parameters are learned via the expectation-maximization algorithm (EM).
While EM has worked quite well for a few tasks, notably machine translations (starting with the IBM models 1-5 (Brown et al., 1993), it has not had success in most others, such as part-of-speech tagging (Meri-aldo, 1991), named-entity recognition (Collins and Singer, 1999) and context-free-grammar induction (numerous attempts, too many to mention).
Thus understanding the abilities and limitations of EM is very much a topic of interest.
We present this work as a positive data-point in this ongoing discussion.
Secondly, and perhaps more importantly, is the system’s performance.
Remarkably, there are very few systems for actually doing pronoun anaphora available on the web.
By emailing the corpora-list the other members of the list pointed us to four.
We present a head to head evaluation and find that our performance is significantly better than the competition.
<newSection> 2 Previous Work The literature on pronominal anaphora is quite large, and we cannot hope to do justice to it here.
Rather we limit ourselves to particular papers and systems that have had the greatest impact on, and similarity to, ours.
Probably the closest approach to our own is Cherry and Bergsma (2005), which also presents an EM approach to pronoun resolution, and obtains quite successful results.
Our work improves upon theirs in several dimensions.
Firstly, they do not distinguish antecedents of non-reflexive pronouns based on syntax (for instance, subjects and objects).
Both previous work (cf. Tetreault (2001) discussed below) and our present results find these distinctions extremely helpful.
Secondly, their system relies on a separate prepro-cessing stage to classify non-anaphoric pronouns, and mark the gender of certain NPs (Mr., Mrs. and some first names).
This allows the incorpo-ration of external data and learning systems, but conversely, it requires these decisions to be made sequentially.
Our system classifies non-anaphoric pronouns jointly, and learns gender without an external database.
Next, they only handle third-person pronouns, while we handle first and second as well.
Finally, as a demonstration of EM’s capabilities, its evidence is equivocal.
Their EM requires careful initialization — sufficiently careful that the EM version only performs 0.4% better than the initialized program alone.
(We can say nothing about relative performance of their system vs. ours since we have been able to access neither their data nor code.)
A quite different unsupervised approach is Kehler et al.
(2004a), which uses self-training of a discriminative system, initialized with some conservative number and gender heuristics.
The system uses the conventional ranking approach, applying a maximum-entropy classifier to pairs of pronoun and potential antecedent and selecting the best antecedent.
In each iteration of self-training, the system labels the training corpus and its decisions are treated as input for the next training phase.
The system improves substantially over a Hobbs baseline.
In comparison to ours, their feature set is quite similar, while their learning approach is rather different.
In addition, their system does not classify non-anaphoric pronouns, A third paper that has significantly influenced our work is that of (Haghighi and Klein, 2007).
This is the first paper to treat all noun phrase (NP) anaphora using a generative model.
The success they achieve directly inspired our work.
There are, however, many differences between their approach and ours.
The most obvious is our use of EM rather than theirs of Gibbs sampling.
However, the most important difference is the choice of training data.
In our case it is a very large corpus of parsed, but otherwise unannotated text.
Their system is trained on the ACE corpus, and requires explicit annotation of all “markables” — things that are or have antecedents.
For pronouns, only anaphoric pronouns are so marked.
Thus the system does not learn to recognize non-anaphoric pronouns — a significant problem.
More generally it follows from this that the system only works (or at least works with the accuracy they achieve) when the input data is so marked.
These markings not only render the non-anaphoric pronoun situation moot, but also significantly restrict the choice of possible antecedent.
Only perhaps one in four or five NPs are markable (Poesio and Vieira, 1998).
There are also several papers which treat coference as an unsupervised clustering problem (Cardie and Wagstaff, 1999; Angheluta et al., 2004).
In this literature there is no generative model at all, and thus this work is only loosely connected to the above models.
Another key paper is (Ge et al., 1998).
The data annotated for the Ge research is used here for testing and development data.
Also, there are many overlaps between their formulation of the problem and ours.
For one thing, their model is generative, although they do not note this fact, and (with the partial exception we are about to mention) they obtain their probabilities from hand annotated data rather than using EM.
Lastly, they learn their gender information (the probability of that a pronoun will have a particular gender given its antecedent) using a truncated EM procedure.
Once they have derived all of the other parameters from the training data, they go through a larger corpus of unla-beled data collecting estimated counts of how often each word generates a pronoun of a particular gender.
They then normalize these probabilities and the result is used in the final program.
This is, in fact, a single iteration of EM.
Tetreault (2001) is one of the few papers that use the (Ge et al., 1998) corpus used here.
They achieve a very high 80% correct, but this is given hand-annotated number, gender and syntactic binding features to filter candidate antecedents and also ignores non-anaphoric pronouns.
We defer discussion of the systems against which we were able to compare to Section 7 on evaluation.
<newSection> 3 Pronouns We briefly review English pronouns and their properties.
First we only concern ourselves with “personal” pronouns: “I”, “you”, “he”, “she”, “it”, and their variants.
We ignore, e.g., relative pronouns (“who”, “which”, etc.), deictic pronouns (“this”, “that”) and others.
Personal pronouns come in four basic types: subject “I”, “she”, etc.
Used in subject position.
object “me”, “her” etc.
Used in non-subject position.
possessive “my” “her”, and reflexive “myself”, “herself” etc.
Required by English grammar in certain constructions — e.g., “I kicked myself.”
The system described here handles all of these cases.
Note that the type of a pronoun is not connected with its antecedent, but rather is completely determined by the role it plays in it’s sentence.
Personal pronouns are either anaphoric or non-anaphoric.
We say that a pronoun is anaphoric when it is coreferent with another piece of text in the same discourse.
As is standard in the field we distinguish between a referent and an antecedent.
The referent is the thing in the world that the pronoun, or, more generally, noun phrase (NP), denotes.
Anaphora on the other hand is a relation between pieces of text.
It follows from this that non-anaphoric pronouns come in two basic varieties — some have a referent, but because the referent is not mentioned in the text2 there is no anaphoric relation to other text.
Others have no referent (ex-pletive or pleonastic pronouns, as in “It seems that ... ”).
For the purposes of this article we do not distinguish the two.
Personal pronouns have three properties other than their type: gender masculine (“he”), feminine (“she”) or neuter (“they”).
These are critical because it is these properties that our generative model generates.
<newSection> 4 The Generative Model Our generative model ignores the generation of most of the discourse, only generating a pronoun’s person, number,and gender features along with the governor of the pronoun and the syntactic relation between the pronoun and the governor.
(Infor-mally, a word’s governor is the head of the phrase above it.
So the governor of both “I” and “her” in “I saw her” is “saw”.
We first decide if the pronoun is anaphoric based upon a distribution p(anaphoric).
(Actu-ally this is a bit more complex, see the discussion in Section 5.3.)
If the pronoun is anaphoric we then select a possible antecedent.
Any NP in the current or two previous sentences is considered.
We select the antecedent based upon a distribution p(anaphora|context).
The nature of the “context” is discussed below.
Then given the antecedent we generative the pronoun’s person according to p(person|antecedent), the pronoun’s gender according to p(gender|antecedent), number, p(number|antecedent) and governor/relationto-governor from p(governor/relation|antecedent).
To generate a non-anaphoric third person singular “it” we first guess that the non-anaphoric pronouns is “it” according to p(“it”|non-anaphoric).
2Actually, as in most previous work, we only consider referents realized by NPs.
For more general approaches see Byron (2002). and then generate the governor/relation according to p(governor/relation|non-anaphoric-it); Lastly we generate any other non-anaphoric pronouns and their governor with a fixed probability p(other).
(Strictly speaking, this is mathemati-cally invalid, since we do not bother to normalize over all the alternatives; a good topic for future research would be exploring what happens when we make this part of the model truly generative.)
One inelegant part of the model is the need to scale the p(governor/rel|antecedent) probabili-ties.
We smooth them using Kneser-Ney smoothing, but even then their dynamic range (a factor of 106) greatly exceeds those of the other parameters.
Thus we take their nth root.
This n is the last of the model parameters.
<newSection> 5 Model Parameters All of our distributions start with uniform values.
For example, gender distributions start with the probability of each gender equal to one-third.
From this it follows that on the first EM iteration all antecedents will have the same probability of generating a pronoun.
At first glance then, the EM process might seem to be futile.
In this section we hope to give some intuitions as to why this is not the case.
As is typically done in EM learning, we start the process with a much simpler generative model, use a few EM iterations to learn its parameters, and gradually expose the data to more and more complex models, and thus larger and larger sets of parameters.
The first model only learns the probability of an antecedent generating the pronoun given what sentence it is in.
We train this model through four iterations before moving on to more complex ones.
As noted above, all antecedents initially have the same probability, but this is not true after the first iteration.
To see how the probabilities diverge, and diverge correctly, consider the first sentence of a news article.
Suppose it starts “President Bush announced that he ...”
In this situation there is only one possible antecedent, so the expectation that “he” is generated by the NP in the same sentence is 1.0.
Contrast this with the situation in the third and subsequent sentences.
It is only then that we have expectation for sentences two back generating the pronoun.
Furthermore, typically by this point there will be, say, twenty NPs to share the probability mass, so each one will only get an increase of 0.05.
Thus on the first iteration only the first two sentences have the power to move the dis-tributions, but they do, and they make NPs in the current sentence very slightly more likely to generate the pronoun than the sentence one back, which in turn is more likely than the ones two back.
This slight imbalance is reflected when EM readjusts the probability distribution at the end of the first iteration.
Thus for the second iteration everyone contributes to subsequent imbalances, because it is no longer the case the all antecedents are equally likely.
Now the closer ones have higher probability so forth and so on.
To take another example, consider how EM comes to assign gender to various words.
By the time we start training the gender assignment prob-abilities the model has learned to prefer nearer antecedents as well as ones with other desirable properties.
Now suppose we consider a sentence, the first half of which has no pronouns.
Consider the gender of the NPs in this half.
Given no further information we would expect these genders to distribute themselves accord to the prior probability that any NP will be masculine, feminine, etc.
But suppose that the second half of the sentence has a feminine pronoun.
Now the genders will be skewed with the probability of one of them being feminine being much larger.
Thus in the same way these probabilities will be moved from equality, and should, in general be moved correctly.
Virtually all model parameters are learned by EM.
We use the parsed version of the North-American News Corpus.
This is available from the (Mc-Closky et al., 2008).
It has about 800,000 articles, and 500,000,000 words.
The least complicated parameter is the probability of gender given word.
Most words that have a clear gender have this reflected in their probabil-ities.
Some examples are shown in Table 1.
We can see there that EM gets “Paul”, “Paula”, and “Wal-mart” correct.
“Pig” has no obvious gender in English, and the probabilities reflect this.
On the other hand “Piggy” gets feminine gender.
This is no doubt because of “Miss Piggy” the puppet character.
“Waist” the program gets wrong.
Here the probabilities are close to gender-of-pronoun priors.
This happens for a (comparatively small) class of pronouns that, in fact, are probably never an antecedent, but are nearby random pronouns.
Because of their non-antecedent proclivities, this sort of mistake has little effect.
Next consider p(numberlantecedent), that is the probability that a given antecedent will generate a singular or plural pronoun.
This is shown in Table 2.
Since we are dealing with parsed text, we have the antecedent’s part-of-speech, so rather than the antecedent we get the number from the part of speech: “NN” and “NNP” are singular, “NNS” and “NNPS” are plural.
Lastly, we have the probability that an antecedent which is not a noun will have a singular pronoun associated with it.
Note that the probability that a singular antecedent will generate a singular pronoun is not one.
This is correct, although the exact number probably is too low.
For example, “IBM” may be the antecedent of both “we” and “they”, and vice versa.
Next we turn to p(person|antecedent), predicting whether the pronoun is first, second or third person given its antecedent.
We simplify this by noting that we know the person of the antecedent (everything except “I” and “you” and their variants are third person), so we compute p(personjperson).
Actually we condition on one further piece of information, if either the pronoun or the antecedent is being quoted.
The idea is that an “I” in quoted material may be the same person as “John Doe” outside of quotes, if Mr. Doe is speaking.
Indeed, EM picks up on this as is illustrated in Tables 3 and 4.
The first gives the situation when neither antecedent nor pronoun is within a quotation.
The high numbers along the diagonal (0.923, 0.885, and 0.967) show the expected like-goes-to-like preferences.
Contrast this with Table 4 which gives the probabilities when the antecedent is in quotes but the pronoun is not.
Here we see all antecedents being preferentially mapped to third person (0.889, 0.705, and 0.964).
We save p(antecedent|context) till last because it is the most complicated.
Given what we know about the context of the pronoun not all antecedent positions are equally likely.
Some important conditioning events are: the antecedent when holding everything expect the stated feature of the antecedent constant ject and object pronouns may be anywhere.
Possessives may be in previous sentences but this is not as common.
• type of antecedent.
Intuitively other pronouns and proper nouns are more likely to be antecedents than common nouns and NPs headed up by things other than nouns.
All told this comes to 2592 parameters (3 sentences, 6 antecedent word positions, 3 syntactic positions, 4 pronoun positions, 3 pronoun types, and 4 antecedent types).
It is impossible to say if EM is setting all of these correctly.
There are too many of them and we do not have knowledge or intuitions about most all of them.
However, all help performance on the development set, and we can look at a few where we do have strong intuitions.
Table 5 gives some examples.
The first two rows are devoted to the probabilities of particular kind of antecedent (pronouns, proper nouns, and common nouns) generating a pronoun, holding everything constant except the type of antecedent.
The numbers are the geometric mean of the prob-abilities in each case.
The probabilities are ordered according to, at least my, intuition with pronoun being the most likely (0.094), followed by proper nouns (0.057), followed by common nouns (0.032), a fact also noted by (Haghighi and Klein, 2007).
When looking at the probabilities as a function of word position again the EM derived proba-bilities accord with intuition, with bin 0 (the closest) more likely than bin 2 more likely than bin 5.
The last two lines have the only case where we have found the EM probability not in accord with our intuitions.
We would have expected objects of verbs to be more likely to generate a pronoun than the catch-all “other” case.
This proved not to be the case.
On the other hand, the two are much closer in probabilities than any of the other, more intuitive, cases.
There are a few parameters not set by EM.
Several are connected with the well known syntactic constraints on the use of reflexives.
A simple version of this is built in.
Reflexives must have an antecedent in same sentence, and generally cannot be coreferent-referent with the subject of the sentence.
There are three system parameters that we set by hand to optimize performance on the development set.
The first is n.
As noted above, the distribution p(governor/relationIantecedent) has amuch greater dynamic range than the other probability distributions and to prevent it from, in essence, completely determining the answer, we take its nth root.
Secondly, there is a probability of generating a non-anaphoric “it”.
Lastly we have a probability of generating each of the other non-monotonic pronouns along with (the nth root of) their governor.
These parameters are 6, 0.1, and 0.0004 respectively.
<newSection> 6 Definition of Correctness We evaluate all programs according to Mitkov’s “resolution etiquette” scoring metric (also used in Cherry and Bergsma (2005)), which is defined as follows: if N is the number of non-anaphoric pronouns correctly identified, A the number of anaphoric pronouns correctly linked to their antecedent, and P the total number of pronouns, then a pronoun-anaphora program’s percentage correct Most papers dealing with pronoun coreference use this simple ratio, or the variant that ignores non-anaphoric pronouns.
It has appeared under a number of names: success (Yang et al., 2006), accuracy (Kehler et al., 2004a; Angheluta et al., 2004) and success rate (Tetreault, 2001).
The other occasionally-used metric is the MUC score restricted to pronouns, but this has well-known problems (Bagga and Baldwin, 1998).
To make the definition perfectly concrete, however, we must resolve a few special cases.
One is the case in which a pronoun x correctly says that it is coreferent with another pronoun y.
However, the program misidentifies the antecedent of y.
In this case (sometimes called error chaining (Walker, 1989)), both x and y are to be scored as wrong, as they both end up in the wrong corefer-ential chain.
We believe this is, in fact, the standard (Mitkov, personal communication), although there are a few papers (Tetreault, 2001; Yang et al., 2006) which do the opposite and many which simply do not discuss this case.
One more issue arises in the case of a system attempting to perform complete NP anaphora3.
In these cases the coreferential chains they create may not correspond to any of the original chains.
In these cases, we call a pronoun correctly resolved if it is put in a chain including at least one correct non-pronominal antecedent.
This definition cannot be used in general, as putting all NPs into the same set would give a perfect score.
Fortunately, the systems we compare against do not do this – they seem more likely to over-split than under-split.
Furthermore, if they do take some inadvertent advantage of this definition, it helps them and puts our program at a possible disadvantage, so it is a more-than-fair comparison.
<newSection> 7 Evaluation To develop and test our program we use the dataset annotated by Niyu Ge (Ge et al., 1998).
This consists of sections 0 and 1 of the Penn tree-bank.
Ge marked every personal pronoun and all noun phrases that were coreferent with these pronouns.
We used section 0 as our development set, and section 1 for testing.
We reparsed the sentences using the Charniak and Johnson parser (Charniak and Johnson, 2005) rather than using the gold-parses that Ge marked up.
We hope thereby to make the results closer to those a user will experience.
(Generally the gold trees perform about 0.005 higher than the machine parsed version.)
The test set has 1119 personal pronouns of which 246 are non-anaphoric.
Our selection of this dataset, rather than the widely used MUC-6 corpus, is motivated by this large number of pronouns.
We compared our results to four currently-available anaphora programs from the web.
These four were selected by sending a request to a commonly used mailing list (the “corpora-list”) asking for such programs.
We received four leads: JavaRAP, Open-NLP, BART and GuiTAR.
Of course, these systems represent the best available work, not the state of the art.
We presume that more recent supervised systems (Kehler et al., 2004b; Yang et al., 2004; Yang et al., 2006) perform better.
Unfortunately, we were unable to obtain a comparison unsupervised learning system at all.
Only one of the four is explicitly aimed at personal-pronoun anaphora — RAP (Resolu-tion of Anaphora Procedure) (Lappin and Le-ass, 1994).
It is a non-statistical system originally implemented in Prolog.
The version we used is JavaRAP, a later reimplementation in Java (Long Qiu and Chua, 2004).
It only handles third person pronouns.
The other three are more general in that they handle all NP anaphora.
The GuiTAR system (Poesio and Kabadjov, 2004) is designed to work in an “off the shelf” fashion on general text GUITAR resolves pronouns using the algorithm of (Mitkov et al., 2002), which filters candidate antecedents and then ranks them using morphosyn-tactic features.
Due to a bug in version 3, GUITAR does not currently handle possessive pronouns.GUITAR also has an optional discourse-new classification step, which cannot be used as it requires a discontinued Google search API.
OpenNLP (Morton et al., 2005) uses a maximum-entropy classifier to rank potential antecedents for pronouns.
However despite being the best-performing (on pronouns) of the existing systems, there is a remarkable lack of published information on its innards.
BART (Versley et al., 2008) also uses a maximum-entropy model, based on Soon et al.
(2001).
The BART system also provides a more sophisticated feature set than is available in the basic model, including tree-kernel features and a variety of web-based knowledge sources.
Unfor-tunately we were not able to get the basic version working.
More precisely we were able to run the program, but the results we got were substantially lower than any of the other models and we believe that the program as shipped is not working properly.
Some of these systems provide their own pre-processing tools.
However, these were bypassed, so that all systems ran on the Charniak parse trees (with gold sentence segmentation).
Systems with named-entity detectors were allowed to run them as a preprocess.
All systems were run using the models included in their standard distribution; typically these models are trained on annotated news articles (like MUC-6), which should be relatively similar to our WSJ documents.
The performance of the remaining systems is given in Table 6.
The two programs with restrictions were only evaluated on the pronouns the system was capable of handling.
These results should be approached with some caution.
In particular it is possible that the results for the systems other than ours are underes-timated due to errors in the evaluation.
Compli-cations include the fact all of the four programs all have different output conventions.
The better to catch such problems the authors independently wrote two scoring programs.
Nevertheless, given the size of the difference between the results of our system and the others, the conclusion that ours has the best performance is probably solid.
<newSection> 8 Conclusion We have presented a generative model of pronoun-anaphora in which virtually all of the parameters are learned by expectation maximization.
We find it of interest first as an example of one of the few tasks for which EM has been shown to be effective, and second as a useful program to be put in general use.
It is, to the best of our knowledge, the best-performing system available on the web.
To down-load it, go to (to be announced).
The current system has several obvious limitation.
It does not handle cataphora (antecedents occurring after the pronoun), only allows antecedents to be at most two sentences back, does not recognize that a conjoined NP can be the antecedent of a plural pronoun, and has a very limited grasp of pronominal syntax.
Perhaps the largest limitation is the programs inability to recognize the speaker of a quoted segment.
The result is a very large fraction of first person pronouns are given incorrect antecedents.
Fixing these problems would no doubt push the system’s performance up several percent.
However the most critical direction for future research is to push the approach to handle full NP anaphora.
Besides being of the greatest importance in its own right, it would also allow us to add one piece of information we currently neglect in our pronominal system — the more times a document refers to an entity the more likely it is to do so again.
<newSection> 9 Acknowledgements We would like to thank the authors and maintainers of the four systems against which we did our comparison, especially Tom Morton, Mijail Kabadjov and Yannick Versley.
Making your system freely available to other researchers is one of the best ways to push the field forward.
In addition, we thank three anonymous reviewers.
<newSection> References<newSection> Abstract Although a lot of progress has been made recently in word segmentation and POS tagging for Chinese, the output of current state-of-the-art systems is too inaccurate to allow for syntactic analysis based on it.
We present an experiment in improving the output of an off-the-shelf module that performs segmentation and tagging, the tokenizer-tagger from Beijing University (PKU).
Our approach is based on transformation-based learning (TBL).
Unlike in other TBL-based approaches to the problem, however, both obligatory and optional transformation rules are learned, so that the final system can output multiple segmentation and POS tagging analyses for a given input.
By allowing for a small amount of ambiguity in the output of the tokenizer-tagger, we achieve a very considerable improvement in accuracy.
Compared to the PKU tokenizer-tagger, we improve segmentation F-score from 94.18% to 96.74%, tagged word F-score from 84.63% to 92.44%, segmented sentence accuracy from 47.15% to 65.06% and tagged sentence accuracy from 14.07% to 31.47%.
<newSection> 1 Introduction Word segmentation and tagging are the necessary initial steps for almost any language processing system, and Chinese parsers are no exception.
However, automatic Chinese word segmentation and tagging has been recognized as a very difficult task (Sproat and Emerson, 2003), for the following reasons: First, Chinese text provides few cues for word boundaries (Xia, 2000; Wu, 2003) and part-ofspeech (POS) information.
With the exception of punctuation marks, Chinese does not have word delimiters such as the whitespace used in English text, and unlike other languages without whites-paces such as Japanese, Chinese lacks morpholog-ical inflections that could provide cues for word boundaries and POS information.
In fact, the lack of word boundary marks and morphological inflection contributes not only to mistakes in machine processing of Chinese; it has also been identified as a factor for parsing miscues in Chinese children’s reading behavior (Chang et al., 1992).
Second, in addition to the two problems described above, segmentation and tagging also suffer from the fact that the notion of a word is very unclear in Chinese (Xu, 1997; Packard, 2000; Hsu, 2002).
While the word is an intuitive and salient notion in English, it is by no means a clear notion in Chinese.
Instead, for historical reasons, the intuitive and clear notion in Chinese language and culture is the character rather than the word.
Classical Chinese is in general monosyllabic, with each syllable corresponding to an independent morpheme that can be visually rendered with a written character.
In other words, characters did represent the basic syntactic unit in Classical Chinese, and thus became the sociolog-ically intuitive notion.
However, although colloquial Chinese quickly evolved throughout Chinese history to be disyllabic or multi-syllabic, monosyllabic Classical Chinese has been considered more elegant and proper and was commonly used in written text until the early 20th century in China.
Even in Modern Chinese written text, Classical Chinese elements are not rare.
Consequently, even if a morpheme represented by a character is no longer used independently in Modern colloquial Chinese, it might still appear to be a free mor-pheme in modern written text, because it contains Classical Chinese elements.
This fact leads to a phenomenon in which Chinese speakers have difficulty differentiating whether a character represents a bound or free morpheme, which in turn affects their judgment regarding where the word boundaries should be.
As pointed out by Hoosain (Hoosain, 1992), the varying knowledge of Classical Chinese among native Chinese speakers in fact affects their judgments about what is or is not a word.
In summary, due to the influence of Classical Chinese, the notion of a word and the boundary between a bound and free morpheme is very unclear for Chinese speakers, which in turn leads to a fuzzy perception of where word boundaries should be.
Consequently, automatic segmentation and tagging in Chinese faces a serious challenge from prevalent ambiguities.
For example 1, the string “�2�” can be segmented as (1a) or (1b), depending on the context.
yˇouyiji`an have the intention meet The contrast shown in (2) illustrates that even a string that is not ambiguous in terms of segmentation can still be ambiguous in terms of tagging.
(2) a. n/a rUn b´ai hu¯a white flower b. n/d rrJv b´ai hu¯a in vain spend ‘spend (money, time, energy etc.) in vain’ Even Chinese speakers cannot resolve such ambiguities without using further information from a bigger context, which suggests that resolving segmentation and tagging ambiguities probably should not be a task or goal at the word level.
Instead, we should preserve such ambiguities in this level and leave them to be resolved in a later stage, when more information is available.
To summarize, the word as a notion and hence word boundaries are very unclear; segmentation and tagging are prevalently ambiguous in Chinese.
These facts suggest that Chinese segmentation and part-of-speech identification are probably inherently non-deterministic at the word level.
However most of the current segmentation and/or tagging systems output a single result.
While a deterministic approach to Chinese segmentation and POS tagging might be appropriate and necessary for certain tasks or applications, it has been shown to suffer from a problem of low accuracy.
As pointed out by Yu (Yu et al., 2004), although the segmentation and tagging accuracy for certain types of text can reach as high as 95%, the accuracy for open domain text is only slightly higher than 80%.
Furthermore, Chinese segmentation (SIGHAN) bakeoff results also show that the performance of the Chinese segmentation systems has not improved a whole lot since 2003.
This fact also indicates that deterministic approaches to Chinese segmentation have hit a bottleneck in terms of accuracy.
The system for which we improved the output of the Beijing tokenizer-tagger is a hand-crafted Chinese grammar.
For such a system, as probably for any parsing system that presupposes segmented (and tagged) input, the accuracy of the segmentation and POS tagging analyses is critical.
However, as described in detail in the following section, even current state-of-art systems cannot provide satisfactory results for our application.
Based on the experiments presented in section 3, we believe that a proper amount of non-deterministic results can significantly improve the Chinese segmentation and tagging accuracy, which in turn improves the performance of the grammar.
<newSection> 2 Background The improved tokenizer-tagger we developed is part of a larger system, namely a deep Chinese grammar (Fang and King, 2007).
The system is hybrid in that it uses probability estimates for parse pruning (and it is planned to use trained weights for parse ranking), but the “core” grammar is rule-based.
It is written within the framework of Lexical Functional Grammar (LFG) and implemented on the XLE system (Crouch et al., 2006; Maxwell and Kaplan, 1996).
The input to our system is a raw Chinese string such as (3).
xiˇaow´ang zˇou le . XiaoWang leave ASP 2 . ‘XiaoWang left.’
The output of the Chinese LFG consists of a Constituent Structure (c-structure) and a Functional Structure (f-structure) for each sentence.
While c-structure represents phrasal structure and linear word order, f-structure represents various functional relations between parts of sentences.
For example, (4) and (5) are the c-structure and f-structure that the grammar produces for (3).
Both c-structure and f-structure information are carried in syntactic rules in the grammar.
To parse a sentence, the Chinese LFG minimally requires three components: a tokenizer-tagger, a lexicon, and syntactic rules.
The tokenizer-tagger that is currently used in the grammar is developed by Beijing University (PKU)3 and is incorporated as a library transducer (Crouch et al., 2006).
Because the grammar’s syntactic rules are applied based upon the results produced by the tokenizer-tagger, the performance of the latter is critical to overall quality of the system’s output.
However, even though PKU’s tokenizer-tagger is one of the state-of-art systems, its performance is not satisfactory for the Chinese LFG.
This becomes clear from a small-scale evaluation in which the system was tested on a set of 101 gold sentences chosen from the Chinese Treebank 5 (CTB5) (Xue et al., 2002; Xue et al., 2005).
These 101 sentences are 10-20 words long and all of them are chosen from Xinhua sources 4.
Based on the deterministic segmentation and tagging results produced by PKU’s tokenizer-tagger, the Chinese LFG can only parse 80 out of the 101 sentences.
Among the 80 sentences that are parsed, 66 received full parses and 14 received fragmented parses.
Among the 21 completely failed sentences, 20 sentences failed due to segmentation and tagging mistakes.
This simple test shows that in order for the deep Chinese grammar to be practically useful, the performance of the tokenizer-tagger must be improved.
One way to improve the segmentation and tagging accuracy is to allow non-deterministic segmentation and tagging for Chinese for the reasons stated in Section 1.
Therefore, our goal is to find a way to transform PKU’s tokenizer-tagger into a system that produces a proper amount of non-deterministic segmentation and tagging results, one that can significantly improve the system’s accuracy without a substantial sacrifice in terms of efficiency.
Our approach is described in the following section.
<newSection> 3 FST5 Rules for the Improvement of Segmentation and Tagging Output For grammars of other languages implemented on the XLE grammar development platform, the input is usually preprocessed by a cascade of generally non-deterministic finite state transducers that perform tokenization, morphological analysis etc.
Since word segmentation and POS tagging are such hard problems in Chinese, this traditional setup is not an option for the Chinese grammar.
However, finite state rules seem a quite natural approach to improving in XLE the output of a separate segmentation and POS tagging module like PKU’s tokenizer-tagger.
straightforwardly be translated into a cascade of FST rules.
Although the grammar developer had identified PKU’s tokenizer-tagger as the most suitable for the preprocessing of Chinese raw text that is to be parsed with the Chinese LFG, she noticed in the process of development that (i) certain segmentation and/or tagging decisions taken by the tokenizer-tagger systematically go counter her morphosyntactic judgment and that (ii) the tokenizer-tagger (as any software of its kind) makes mistakes.
She therefore decided to develop a set of finite-state rules that transform the output of the module; a set of mostly obligatory rewrite rules adapts the POS-tagged word sequence to the grammar’s standard, and another set of mostly optional rules tries to offer alternative segment and tag sequences for sequences that are frequently processed erroneously by PKU’s tokenizer-tagger.
Given the absence of data segmented and tagged according to the standard the LFG grammar developer desired, the technique of hand-crafting FST rules to postprocess the output of PKU’s tokenizer-tagger worked surprisingly well.
Recall that based on the deterministic segmentation and tagging results produced by PKU’s tokenizer-tagger, our system can only parse 80 out of the 101 sentences, and among the 21 completely failed sentences, 20 sentences failed due to segmentation and tagging mistakes.
In contrast, after the application of the hand-crafted FST rules for post-processing, 100 out of the 101 sentences can be parsed.
However, this approach involved a lot of manual development work (about 3-4 person months) and has reached a stage where it is difficult to systematically work on further improvements.
Since there are large amounts of training data that are close to the segmentation and tagging standard the grammar developer wants to use, the idea of inducing FST rules rather than hand-crafting them comes quite naturally.
The easiest way to do this is to apply transformation-based learning (TBL) to the combined problem of Chinese segmentation and POS tagging, since the cascade of transfor-mational rules learned in a TBL training run can µ-TBL TBL is a machine learning approach that has been employed to solve a number of problems in natural language processing; most famously, it has been used for part-of-speech tagging (Brill, 1995).
TBL is a supervised learning approach, since it relies on gold-annotated training data.
In addition, it relies on a set of templates of transformational rules; learning consists in finding a sequence of in-stantiations of these templates that minimizes the number of errors in a more or less naive base-line output with respect to the gold-annotated training data.
The first attempts to employ TBL to solve the problem of Chinese word segmentation go back to Palmer (1997) and Hockenmaier and Brew (1998).
In more recent work, TBL was used for the adaption of the output of a statistical “general purpose” segmenter to standards that vary depending on the application that requires sentence segmentation (Gao et al., 2004).
TBL approaches to the combined problem of segmenting and POS-tagging Chinese sentences are reported in Florian and Ngai (2001) and Fung et al.
(2004).
Several implementations of the TBL approach are freely available on the web, the most well-known being the so-called Brill tagger, fnTBL, which allows for multi-dimensional TBL, and µ-TBL (Lager, 1999).
Among these, we chose µ-TBL for our experiments because (like fnTBL) it is completely flexible as to whether a sample is a word, a character or anything else and (un-like fnTBL) it allows for the induction of optional rules.
Probably due to its flexibility, µ-TBL has been used (albeit on a small scale for the most part) for tasks as diverse as POS tagging, map tasks, and machine translation.
We started out with a corpus of thirty gold-segmented and -tagged daily editions of the Xin-hua Daily, which were provided by the Institute of Computational Linguistics at Beijing University.
Three daily editions, which comprise 5,054 sentences with 129,377 words and 213,936 characters, were set aside for testing purposes; the remaining 27 editions were used for training.
With the idea of learning both obligatory and optional transformational rules in mind, we then split the training data into two roughly equally sized subsets.
All the data were broken into sentences using a very simple method: The end of a paragraph was always considered a sentence boundary.
Within paragraphs, sentence-final punctuation marks such as periods (which are unambiguous in Chinese), question marks and exclamation marks, potentially followed by a closing parenthesis, bracket or quote mark, were considered sentence boundaries.
We then had to come up with a way of casting the problem of combined segmentation and POS tagging as a TBL problem.
Following a strategy widely used in Chinese word segmentation, we did this by regarding the problem as a character tagging problem.
However, since we intended to learn rules that deal with segmentation and POS tagging simultaneously, we could not adopt the BIO-coding approach.6 Also, since the TBL-induced transformational rules were to be converted into FST rules, we had to keep our character tagging scheme one-dimensional, unlike Florian and Ngai (2001), who used a multi-dimensional TBL approach to solve the problem of combined segmentation and POS tagging.
The character tagging scheme that we finally chose is illustrated in (6), where a. and b. show the character tags that we used for the analyses in (1a) and (1b) respectively.
The scheme consists in tagging the last character of a word with the part-ofspeech of the entire word; all non-final characters are tagged with ‘-’.
The main advantages of this character tagging scheme are that it expresses both word boundaries and parts-of-speech and that, at the same time, it is always consistent; inconsisten-cies between BIO tags indicating word boundaries and part-of-speech tags, which Florian and Ngai (2001), for example, have to resolve, can simply not arise.
Both of the training data subsets were tagged according to our character tagging scheme and converted to the data format expected by µ-TBL.
The first training data subset was used for learning obligatory resegmentation and retagging rules.
The corresponding rule templates, which define the space of possible rules to be explored, are given in Figure 1.
The training parameters of µ-TBL, which are an accuracy threshold and a score threshold, were set to 0.75 and 5 respectively; this means that a potential rule was only retained if at least 75% of the samples to which it would have applied were actually modified in the sense of the gold standard and not in some other way and that the learning process was terminated when no more rule could be found that applied to at least 5 samples in the first training data subset.
With these training parameters, 3,319 obligatory rules were learned by µ-TBL.
Once the obligatory rules had been learned on the first training data subset, they were applied to the second training data subset.
Then, optional rules were learned on this second training data subset.
The rule templates used for optional rules are very similar to the ones used for obligatory rules; a few templates of optional rules are given in Figure 2.
The difference between obligatory rules and optional rules is that the former replace one character tag by another, whereas the latter add character tags.
They hence introduce ambiguity, which is why we call them optional rules.
Like in the learning of the obligatory rules, the accuracy threshold used was 0.75; the score theshold was set to 7 because the training software seemed to hit a bug below that threshold.
753 optional rules were learned.
We did not experiment with the adjustment of the training parameters on a separate held-out set.
Finally, the rule sets learned were converted into the fst (Beesley and Karttunen, 2003) notation for transformational rules, so that they could be tested and used in the FST cascade used for preprocess-ing the input of the Chinese LFG.
For evaluation, the converted rules were applied to our test data set of 5,054 sentences.
A few example rules learned by µ-TBL with the set-up described above are given in Figure 3; we show them both in µ-TBL notation and in fst notation.
<newSection> 3.2.3 Results The results achieved by PKU’s tokenizer-tagger on its own and in combination with the trans-formational rules learned in our experiments are given in Table 1.
We compare the output of PKU’s tokenizer-tagger run in the mode where it returns only the most probable tag for each word (PKU one tag), of PKU’s tokenizer-tagger run in the mode where it returns all possible tags for a given word (PKU all tags), of PKU’s tokenizer-tagger in one-tag mode augmented with the obligatory transformational rules learned on the first part of our training data (PKU one tag + deterministic rule set), and of PKU’s tokenizer-tagger augmented with both the obligatory and optional rules learned on the first and second parts of our training data respectively (PKU one tag + non-deterministic rule set).
We give results in terms of character tag accuracy and ambiguity according to our character tagging scheme.
Then we provide evaluation figures for the word level.
Finally, we give results referring to the sentence level in order to make clear how serious a problem Chinese segmentation and POS tagging still are for parsers, which obviously operate at the sentence level.
These results show that simply switching from the one-tag mode of PKU’s tokenizer-tagger to its all-tags mode is not a solution.
First of all, since the tokenizer-tagger always produces only one segmentation regardless of the mode it is used in, segmentation accuracy would stay completely unaffected by this change, which is particularly serious because there is no way for the grammar to recover from segmentation errors and the tokenizer-tagger produces an entirely correct segmentation only for 47.15% of the sentences.
Second, the improved tagging accuracy would come at a very heavy price in terms of ambiguity; the median number of combined segmentation and POS tagging analyses per sentence would be 1,440.
In contrast, machine-learned transformation rules are an effective means to improve the output of PKU’s tokenizer-tagger.
Applying only the obligatory rules that were learned already improves segmented sentence accuracy from 47.15% to 63.14% and tagged sentence accuracy from 14.07% to 27.21%, and this at no cost in terms of ambiguity.
Adding the optional rules that were learned and hence making the rule set used for post-processing the output of PKU’s tokenizer-tagger non-deterministic makes it possible to improve segmented sentence accuracy and tagged sentence accuracy further to 65.06% and 31.47% respectively, i.e. tagged sentence accuracy is more than doubled with respect to the baseline.
While this last improvement does come at a price in terms of ambiguity, the ambiguity resulting from the application of the non-deterministic rule set is very low in comparison to the ambiguity of the output of PKU’s tokenizer-tagger in all-tags mode; the median number of analyses per sentences only increases to 2.
Finally, it should be noted that the transformational rules provide entirely correct segmentation and POS tagging analyses not only for more sentences, but also for longer sentences.
They increase the average length of a correctly segmented sentence from 18.22 words to 21.94 words and the average length of a correctly segmented and POS-tagged sentence from 9.58 words to 16.33 words.
<newSection> 4 Comparison to related work and Discussion Comparing our results to other results in the literature is not an easy task because segmentation and POS tagging standards vary, and our test data have not been used for a final evaluation before.
Nevertheless, there are of course systems that perform word segmentation and POS tagging for Chinese and have been evaluated on data similar to our test data.
Published results also vary as to the evaluation measures used, in particular when it comes to combined word segmentation and POS tagging.
For word segmentation considered separately, the consensus is to use the (segmentation) F-score (SF).
The quality of systems that perform both segmentation and POS tagging is often expressed in terms of (character) tag accuracy (TA), but this obviously depends on the character tagging scheme adopted.
An alternative measure is POS tagging F-score (TF), which is the geometric mean of precision and recall of correctly segmented and POS-tagged words.
Evaluation measures for the sentence level have not been given in any publication that we are aware of, probably because segmenters and POS taggers are rarely considered as pre-processing modules for parsers, but also because the figures for measures like sentence accuracy are strikingly low.
For systems that perform only word segmentation, we find the following results in the literature: (Gao et al., 2004), who use TBL to adapt a “gen-eral purpose” segmenter to varying standards, report an SF of 95.5% on PKU data and an SF of 90.4% on CTB data.
(Tseng et al., 2005) achieve an SF of 95.0%, 95.3% and 86.3% on PKU data from the Sighan Bakeoff 2005, PKU data from the Sighan Bakeoff 2003 and CTB data from the Sighan Bakeoff 2003 respectively.
Finally, (Zhang et al., 2006) report an SF of 94.8% on PKU data.
For systems that perform both word segmentation and POS tagging, the following results were published: Florian and Ngai (2001) report an SF of 93.55% and a TA of 88.86% on CTB data.
Ng and Low (2004) report an SF of 95.2% and a TA of 91.9% on CTB data.
Finally, Zhang and Clark (2008) achieve an SF of 95.90% and a TF of 91.34% by 10-fold cross validation using CTB data.
Last but not least, there are parsers that operate on characters rather than words and who perform segmentation and POS tagging as part of the parsing process.
Among these, we would like to mention Luo (2003), who reports an SF 96.0% on Chinese Treebank (CTB) data, and (Fung et al., 2004), who achieve “a word segmentation precision/recall performance of 93/94%”.
Both the SF and the TF results achieved by our “PKU one tag + non-deterministic rule set” setup, whose output is slightly ambiguous, compare favorably with all the results mentioned, and even the results achieved by our “PKU one tag + deterministic rule set” setup are competitive.
<newSection> 5 Conclusions and Future Work The idea of carrying some ambiguity from one processing step into the next in order not to prune good solutions is not new.
E.g., Prins and van No-ord (2003) use a probabilistic part-of-speech tagger that keeps multiple tags in certain cases for a hand-crafted HPSG-inspired parser for Dutch, and Curran et al.
(2006) show the benefits of using a multi-tagger rather than a single-tagger for an induced CCG for English.
However, to our knowledge, this idea has not made its way into the field of Chinese parsing so far.
Chinese parsing systems either pass on a single segmentation and POS tagging analysis to the parser proper or they are character-based, i.e. segmentation and tagging are part of the parsing process.
Although several treebank-induced character-based parsers for Chinese have achieved promising results, this approach is impractical in the development of a hand-crafted deep grammar like the Chinese LFG.
We therefore believe that the development of a “multi-tokenizer-tagger” is the way to go for this sort of system (and all systems that can handle a certain amount of ambiguity that may or may not be resolved at later processing stages).
Our results show that we have made an important first step in this direction.
As to future work, we hope to resolve the problem of not having a gold standard that is segmented and tagged exactly according to the guidelines established by the Chinese LFG developer by semi-automatically applying the hand-crafted transformational rules that were developed to the PKU gold standard.
We will then induce obligatory and optional FST rules from this “grammar-compliant” gold standard and hope that these will be able to replace the hand-crafted transformation rules currently used in the grammar.
Finally, we plan to carry out more training runs; in particular, we intend to experiment with lower accuracy (and score) thresholds for optional rules.
The idea is to find the optimal balance between ambiguity, which can probably be higher than with our current set of induced rules without affecting efficiency too adversely, and accuracy, which still needs further improvement, as can easily be seen from the sentence accuracy figures.
<newSection> References<newSection> Abstract We describe a methodology for learning a disambiguation model for deep pragmatic interpretations in the context of situated task-oriented dialogue.
The system accumulates training examples for ambiguity resolution by tracking the fates of alternative interpretations across dialogue, including subsequent clarificatory episodes initiated by the system itself.
We illustrate with a case study building maximum entropy models over abductive in-terpretations in a referential communica-tion task.
The resulting model correctly resolves 81% of ambiguities left unresolved by an initial handcrafted baseline.
A key innovation is that our method draws exclusively on a system’s own skills and experience and requires no human annotation.
<newSection> 1 Introduction In dialogue, the basic problem of interpretation is to identify the contribution a speaker is making to the conversation.
There is much to recognize: the domain objects and properties the speaker is referring to; the kind of action that the speaker is performing; the presuppositions and implicatures that relate that action to the ongoing task.
Nevertheless, since the seminal work of Hobbs et al.
(1993), it has been possible to conceptualize pragmatic in-terpretation as a unified reasoning process that selects a representation of the speaker’s contribution that is most preferred according to a background model of how speakers tend to behave.
In principle, the problem of pragmatic interpretation is qualitatively no different from the many problems that have been tackled successfully by data-driven models in NLP.
However, while researchers have shown that it is sometimes possible to annotate corpora that capture features of in-terpretation, to provide empirical support for theories, as in (Eugenio et al., 2000), or to build classifiers that assist in dialogue reasoning, as in (Jordan and Walker, 2005), it is rarely feasible to fully annotate the interpretations themselves.
The distinctions that must be encoded are subtle, theoretically-loaded and task-specific—and they are not always signaled unambiguously by the speaker.
See (Poesio and Vieira, 1998; Poesio and Artstein, 2005), for example, for an overview of problems of vagueness, underspecification and ambiguity in reference annotation.
As an alternative to annotation, we argue here that dialogue systems can and should prepare their own training data by inference from under-specified models, which provide sets of candidate meanings, and from skilled engagement with their interlocutors, who know which meanings are right.
Our specific approach is based on contribution tracking (DeVault, 2008), a framework which casts linguistic inference in situated, task-oriented dialogue in probabilistic terms.
In contribution tracking, ambiguous utterances may result in alternative possible contexts.
As subsequent utterances are interpreted in those contexts, ambiguities may ramify, cascade, or disappear, giving new insight into the pattern of activity that the interlocutor is engaged in.
For example, consider what happens if the system initiates clarification.
The interlocutor’s answer may indicate not only what they mean now but also what they must have meant earlier when they used the original ambiguous utterance.
Contribution tracking allows a system to accumulate training examples for ambiguity resolution by tracking the fates of alternative interpretations across dialogue.
The system can use these examples to improve its models of pragmatic interpretation.
To demonstrate the feasibility of this approach in realistic situations, we present a system that tracks contributions to a referential com-munication task using an abductive interpretation model: see Section 2.
A user study with this system, described in Section 3, shows that this system can, in the course of interacting with its users, discover the correct interpretations of many potentially ambiguous utterances.
The system thereby automatically acquires a body of training data in its native representations.
We use this data to build a maximum entropy model of pragmatic interpre-tation in our referential communication task.
After training, we correctly resolve 81% of the ambiguities left open in our handcrafted baseline.
<newSection> 2 Contribution tracking We continue a tradition of research that uses simple referential communication tasks to explore the organization and processing of human–computer and mediated human–human conversation, including recently (DeVault and Stone, 2007; Gergle et al., 2007; Healey and Mills, 2006; Schlangen and Fern´andez, 2007).
Our specific task is a two-player object-identification game adapted from the experiments of Clark and Wilkes-Gibbs (1986) and Brennan and Clark (1996); see Section 2.1.
To play this game, our agent, COREF, interprets utterances as performing sequences of task-specific problem-solving acts using a combination of grammar-based constraint inference and abduc-tive plan recognition; see Section 2.2.
Crucially, COREF’s capabilities also include the ambiguity management skills described in Section 2.3, including policies for asking and answering clarifi-cation questions.
The game plays out in a special-purpose graphical interface, which can support either human–human or human–agent interactions.
Two players work together to create a specific configuration of objects, or a scene, by adding objects into the scene one at a time.
Their interfaces display the same set of candidate objects (geometric objects that differ in shape, color and pattern), but their locations are shuffled.
The shuffling undermines the use of spatial expressions such as “the object at bottom left”.
Figures 1 and 2 illustrate the different views.1 1Note that in a human–human game, there are literally two versions of the graphical interface on the separate computers the human participants are using.
In a human–agent interaction, COREF does not literally use the graphical interface, but the information that COREF is provided is limited to the information the graphical interface would provide to a human participant.
For example, COREF is not aware of the locations of objects on its partner’s screen.
As in the experiments of Clark and Wilkes-Gibbs (1986) and Brennan and Clark (1996), one of the players, who plays the role of director, instructs the other player, who plays the role of matcher, which object is to be added next to the scene.
As the game proceeds, the next target object is automatically determined by the interface and privately indicated to the director with a blue arrow, as shown in Figure 1.
(Note that the corresponding matcher’s perspective, shown in Figure 2, does not include the blue arrow.)
The director’s job is then to get the matcher to click on (their version of) this target object.
To achieve agreement about the target, the two players can exchange text through an instant-messaging modality.
(This is the only communi-cation channel.)
Each player’s interface provides a real-time indication that their partner is “Active” while their partner is composing an utterance, but the interface does not show in real-time what is being typed.
Once the Enter key is pressed, the utterance appears to both players at the bottom of a scrollable display which provides full access to all the previous utterances in the dialogue.
When the matcher clicks on an object they believe is the target, their version of that object is privately moved into their scene.
The director has no visible indication that the matcher has clicked on an object.
However, the director needs to click the Continue (next object) button (see Figure 1) in order to move the current target into the director’s scene, and move on to the next target object.
This means that the players need to discuss not just what the target object is, but also whether the matcher has added it, so that they can coordinate on the right moment to move on to the next object.
If this coordination succeeds, then after the director and matcher have completed a series of objects, they will have created the exact same scene in their separate interfaces.
COREF treats interpretation broadly as a problem of abductive intention recognition (Hobbs et al., 1993).2 We give a brief sketch here to highlight the content of COREF’s representations, the sources of information that COREF uses to construct them, and the demands they place on disambiguation.
See DeVault (2008) for full details.
COREF’s utterance interpretations take the form of action sequences that it believes would constitute coherent contributions to the dialogue task in the current context.
Interpretations are constructed abductively in that the initial actions in the sequence need not be directly tied to observable events; they may be tacit in the terminology of Thomason et al.
(2006).
Examples of such tacit actions include clicking an object, initiating a clar-ification, or abandoning a previous question.
As a concrete example, consider utterance (1b) from the dialogue of Figure 1, repeated here as (1): 2In fact, the same reasoning interprets utterances, button presses and the other actions COREF observes!
In interpreting (1b), COREF hypothesizes that the user has tacitly abandoned the agent’s question in (1a).
In fact, COREF identifies two possible inter-pretations for (1b): Both interpretations begin by assuming that user c4 has tacitly abandoned the previous question, and then further analyze the utterance as performing three additional dialogue acts.
When a dialogue act is preceded by tacit actions in an inter-pretation, the speaker of the utterance implicates that the earlier tacit actions have taken place (De-Vault, 2008).
These implicatures are an important part of the interlocutors’ coordination in COREF’s dialogues, but they are a major obstacle to annotating interpretations by hand.
Action sequences such as i2,1 and i2,2 are coherent only when they match the state of the ongoing referential communication game and the semantic and pragmatic status of information in the dialogue.
COREF tracks these connections by maintaining a probability distribution over a set of dialogue states, each of which represents a possible thread that resolves the ambiguities in the dialogue history.
For performance reasons, COREF entertains up to three alternative threads of inter-pretation; COREF strategically drops down to the single most probable thread at the moment each object is completed.
Each dialogue state represents the stack of processes underway in the referential communication game; constituent activities include problem-solving interactions such as identifying an object, information-seeking interactions such as question–answer pairs, and grounding processes such as acknowledgment and clari-fication.
Dialogue states also represent pragmatic information including recent utterances and referents which are salient or in focus.
COREF abductively recognizes the intention I of an actor in three steps.
First, for each dialogue state sk, COREF builds a horizon graph of possible tacit action sequences that could be assumed coherently, given the pending tasks (De-Vault, 2008).
Second, COREF uses the horizon graph and other resources to solve any constraints associated with the observed action.
This step instantiates any free parameters associated with the action to contextually relevant values.
For utterances, the relevant constraints are identified by parsing the utterance using a hand-built, lexicalized tree-adjoining grammar.
In interpreting (1b), the parse yields an ambiguity in the dialogue act associated with the word “brown”, which may mean either of the two shades of brown in Figure 1, which COREF distinguishes using its saddlebrown and sandybrown concepts.
Once COREF has identified a set of interpre-tations {it,1, ..., it,n} for an utterance o at time t, the last step is to assign a probability to each.
In general, we conceive of this following Hobbs et al.
(1993): the agent should weigh the different assumptions that went into constructing each in-terpretation.3 Ultimately, this process should be made sensitive to the rich range of factors that are available from COREF’s deep representation of the dialogue state and the input utterance—this is our project in this paper.
However, in our initial implemented prototype, COREF assigned these probabilities using a simple hand-built model considering only NT, the number of tacit actions ab-ductively assumed to occur in an interpretation: In effect, this is a “null hypothesis” that assigns relatively uniform weights to different abductive hypotheses.
COREF uses its probabilistic model of context in order to tolerate ambiguity as it moves forward with its dialogues and to resolve ambiguity over time.
We have put particular effort into COREF’s skills with three kinds of ambiguity: word-sense ambiguities, where COREF finds multiple resolutions for the domain concept evoked by the use of a lexical item, as in the interaction (1) of Figure 1; referential ambiguities, where COREF takes a noun phrase to be compatible with multiple objects from the display; and speech act ambiguities, where alternative interpretations communicate or implicate different kinds of contributions to the ongoing task.
The resolution of ambiguity may involve some combination of asking questions of the user, aggregating information provided by the user across multiple turns of dialogue, and strategically dropping threads of interpretation.
For example, COREF represents the context resulting from (1b) in terms of two states: one from interpretation i2,1 and one from i2,2.
COREF asks a clarification question (1c); the user’s explicit answer yes allows COREF to discard one of the possible states and allocate all its probability mass to the other one.
The dialogue in (2) shows an alternative case.
The example is taken from the setting shown in Figure 3.
In this case, COREF finds two colors on the screen it thinks the user could intend to evoke with the word orange; the peachy orange of the diamond and circle on the top row and the brighter orange of the solid and empty squares in the middle column.
COREF responds to the ambiguity by introducing two states which track the alternative colors.
Immediately COREF gets an additional description from the user, and adds the constraint that the object is a diamond.
As there is no bright orange diamond, there is no way to interpret the user’s utterance in the bright orange state; COREF discards this state and allocates all its probability mass to the other one.
<newSection> 3 Inferring the fates of interpretations Our approach is based on the observation that COREF’s contribution tracking can be viewed as assigning a fate to every dialogue state it entertains as part of some thread of interpretation.
In particular, if we consider the agent’s contribution tracking retrospectively, every dialogue state can be assigned a fate of correct or incorrect, where a state is viewed as correct if it or some of its descendants eventually capture all the probability mass that COREF is distributing across the viable surviving states, and incorrect otherwise.
In general, there are two ways that a state can end up with fate incorrect.
One way is that the state and all of its descendants are eventually denied any probability mass due to a failure to interpret a subsequent utterance or action as a coherent contribution from any of those states.
In this case, we say that the incorrect state was eliminated.
The second way a state can end up incorrect is if COREF makes a strategic decision to drop the state, or all of its surviving descendants, at a time when the state or its descendants were assigned nonzero probability mass.
In this case we say that the incorrect state was dropped.
Meanwhile, because COREF drops all states but one after each object is completed, there is a single hypothesized state at each time t whose descendants will ultimately capture all of COREF’s probability mass.
Thus, for each time t, COREF will retrospectively classify exactly one state as correct.
Of course, we really want to classify interpre-tations.
Because we seek to estimate P(I = it,j|o, 5t = sk), which conditions the probability assigned to I = it,j on the correctness of state sk, we consider only those interpretations arising in states that are retrospectively identified as correct.
For each such interpretation, we start from the state where that interpretation is adopted and trace forward to a correct state or to its last surviving descendant.
We classify the interpretation the same way as that final state, either correct, eliminated, or dropped.
We harvested a training set using this methodology from the transcripts of a previous evaluation experiment designed to exercise COREF’s ambiguity management skills.
The data comes from 20 subjects—most of them undergraduates par-ticipating for course credit—who interacted with COREF over the web in three rounds of the referential communication each.
The number of objects increased from 4 to 9 to 16 across rounds; the roles of director and matcher alternated in each round, with the initial role assigned at random.
Of the 3275 sensory events that COREF interpreted in these dialogues, from the (retrospec-tively) correct state, COREF hypothesized 0 inter-pretations for 345 events, 1 interpretation for 2612 events, and more than one interpretation for 318 events.
The overall distribution in the number of interpretations hypothesized from the correct state is given in Figure 4.
<newSection> 4 Learning pragmatic interpretation We capture the fate of each interpretation it,j in a discrete variable F whose value is correct, eliminated, or dropped.
We also represent each intention it,j, observation o, and state sk in terms of features.
We seek to learn a function from a set of training examples E = lei,..., enJ where, for l = 1..n, we have: el = ( F = fate(it,j), features(it,j), features(o), features(sk)).
We chose to train maximum entropy models (Berger et al., 1996).
Our learning framework is described in Section 4.1; the results in Section 4.2.
We defined a range of potentially useful features, which we list in Figures 5, 6, and 7.
These features formalize pragmatic distinctions that plausibly provide evidence of the correct interpreta-tion for a user utterance or action.
You might annotate any of these features by hand, but computing them automatically lets us easily explore a much larger range of possibilities.
To allow these various kinds of features (integer-valued, binary-valued, and string-valued) to interface to the maximum entropy model, these features were converted into a much broader class of indicator features taking on a value of either 0.0 or 1.0.
feature set description NumTacitActions The number of tacit actions in it,j.
TaskActions These features represent the action type (function symbol) of each action ak in it,j = (A1 : a1, A2 : a2,..., An : an), as a string.
ActorDoesTaskAction For each Ak : ak in it,j = (A1 : a1, A2 : a2,..., An : an), a feature indicates that Ak (represented as string “Agent” or “User”) has performed action ak (represented as a string action type, as in the TaskActions features).
Presuppositions If o is an utterance, we include a string representation of each presupposition assigned to o by it,j.
The predicate/argument structure is captured in the string, but any gensym identifiers within the string (e.g. target12) are replaced with exemplars for that identifier type (e.g. target).
Assertions If o is an utterance, we include a string representation of each dialogue act assigned to o by it,j.
Gensym identifiers are filtered as in the Presuppositions features.
Syntax If o is an utterance, we include a string representation of the bracketed phrase structure of the syntactic analysis assigned to o by it,j.
This includes the categories of all non-terminals in the structure.
FlexiTaskIntentionActors Given it,j = (A1 : a1, A2 : a2,..., An : an), we include a single string feature capturing the actor sequence (A1, A2,..., An) in it,j (e.g. “User, Agent, Agent”).
feature set description Words If o is an utterance, we include features that indicate the presence of each word that occurs in the utterance.
feature set description NumTasksUnderway The number of tasks underway in sk.
TasksUnderway The name, stack depth, and current task state for each task underway in sk.
NumRemainingReferents The number of objects yet to be identified in sk.
We used the MALLET maximum entropy classifier (McCallum, 2002) as an off-the-shelf, trainable maximum entropy model.
Each run involved two steps.
First, we applied MALLET’s feature selection algorithm, which incrementally selects features (as well as conjunctions of features) that maximize an exponential gain function which represents the value of the feature in predicting in-terpretation fates.
Based on manual experimenta-tion, we chose to have MALLET select about 300 features for each learned model.
In the second step, the selected features were used to train the model to estimate probabilities.
We used MALLET’s implementation of Limited-Memory BFGS (Nocedal, 1980).
We are generally interested in whether COREF’s experience with previous subjects can be leveraged to improve its interactions with new subjects.
Therefore, to evaluate our approach, while making maximal use of our available data set, we performed a hold-one-subject-out cross-validation using our 20 human subjects H = {h1, ..., h20}.
That is, for each subject hi, we trained a model on the training examples associated with subjects H \ {hi}, and then tested the model on the examples associated with subject hi.
To quantify the performance of the learned model in comparison to our baseline, we adapt the mean reciprocal rank statistic commonly used for evaluation in information retrieval (Vorhees, 1999).
We expect that a system will use the probabilities calculated by a disambiguation model to decide which interpretations to pursue and how to follow them up through the most efficient interaction.
What matters is not the absolute probability of the correct interpretation but its rank with respect to competing interpretations.
Thus, we consider each utterance as a query; the disambigua-tion model produces a ranked list of responses for this query (candidate interpretations), ordered by probability.
We find the rank r of the correct in-terpretation in this list and measure the outcome of the query as r.
Because of its weak assumptions, our baseline disambiguation model actually leaves many ties.
So in fact we must compute an expected reciprocal rank (ERR) statistic that averages 1� over all ways of ordering the correct inter-pretation against competitors of equal probability.
Figure 8 shows a histogram of ERR across the ambiguous utterances from the corpus.
The learned models correctly resolve almost 82%, while the baseline model correctly resolves about 21%.
In fact, the learned models get much of this improvement by learning weights to break the ties in our baseline model.
The overall performance measure for a disambiguation model is the mean expected reciprocal rank across all examples in the corpus.
The learned model improves this metric to 0.92 from a baseline of 0.77.
The difference is un-ambiguously significant (Wilcoxon rank sum test W = 23743.5, p < 10−15).
Feature selection during training identified a variety of syntactic, semantic, and pragmatic features as useful in disambiguating correct interpretations.
Selections were made from every feature set in Figures 5, 6, and 7.
It was often possible to identify relevant features as playing a role in successful disambiguation by the learned models.
For example, the learned model trained on H \ {c4} delivered the following probabilities for the two inter-pretations COREF found for c4’s utterance (1b): The correct interpretation, i2,1, hypothesizes that the user means saddlebrown, the darker of the two shades of brown in the display.
Among the features selected in this model is a Presupposi-tions feature (see Figure 5) which is present just in case the word ‘brown’ is interpreted as meaning saddlebrown rather than some other shade.
This feature allows the learned model to prefer to interpret c4’s use of ‘brown’ as meaning this darker shade of brown, based on the observed linguistic behavior of other users.
<newSection> 5 Results in context Our work adds to a body of research learning deep models of language from evidence implicit in an agent’s interactions with its environment.
It shares much of its motivation with co-training (Blum and Mitchell, 1998) in improving initial models by leveraging additional data that is easy to obtain.
However, as the examples of Section 2.3 illustrate, COREF’s interactions with its users offer substan-tially more information about interpretation than the raw text generally used for co-training.
Closer in spirit is AI research on learning vocabulary items by connecting user vocabulary to the agent’s perceptual representations at the time of utterance (Oates et al., 2000; Roy and Pentland, 2002; Cohen et al., 2002; Yu and Ballard, 2004; Steels and Belpaeme, 2005).
Our framework augments this information about utterance context with additional evidence about meaning from linguistic interaction.
In general, dialogue coherence is an important source of evidence for all aspects of language, for both human language learning (Saxton et al., 2005) as well as machine models.
For example, Bohus et al.
(2008) use users’ confirmations of their spoken requests in a multi-modal interface to tune the system’s ASR rankings for recognizing subsequent utterances.
Our work to date has a number of limitations.
First, although 318 ambiguous interpretations did occur, this user study provided a relatively small number of ambiguous interpretations, in machine learning terms; and most (80.2%) of those that did occur were 2-way ambiguities.
A richer domain would require both more data and a generative approach to model-building and search.
Second, this learning experiment has been performed after the fact, and we have not yet investigated the performance of the learned model in a follow-up experiment in which COREF uses the learned model in interactions with its users.
A third limitation lies in the detection of ‘correct’ interpretations.
Our scheme sometimes conflates the user’s actual intentions with COREF’s subsequent assumptions about them.
If COREF decides to strategically drop the user’s actual intended interpretation, our scheme may mark another interpretation as ‘correct’.
Alternative approaches may do better at harvesting meaningful examples of correct and incorrect interpre-tations from an agent’s dialogue experience.
Our approach also depends on having clear evidence about what an interlocutor has said and whether the system has interpreted it correctly—evidence that is often unavailable with spoken input or information-seeking tasks.
Thus, even when spoken language interfaces use probabilistic inference for dialogue management (Williams and Young, 2007), new techniques may be needed to mine their experience for correct interpretations.
<newSection> 6 Conclusion We have implemented a system COREF that makes productive use of its dialogue experience by learning to rank new interpretations based on features it has historically associated with correct utterance interpretations.
We present these results as a proof-of-concept that contribution tracking provides a source of information that an agent can use to improve its statistical interpretation process.
Further work is required to scale these techniques to richer dialogue systems, and to understand the best architecture for extracting evidence from an agent’s interpretive experience and modeling that evidence for future language use.
Nevertheless, we believe that these results showcase how judicious system-building efforts can lead to dialogue capabilities that defuse some of the bottlenecks to learning rich pragmatic interpretation.
In particular, a focus on improving our agents’ basic abilities to tolerate and resolve ambiguities as a dialogue proceeds may prove to be a valuable technique for improving the overall dialogue competence of the agents we build.
<newSection> Acknowledgments This work was sponsored in part by NSF CCF-0541185 and HSD-0624191, and by the U.S. Army Research, Development, and Engineering Command (RDECOM).
Statements and opinions expressed do not necessarily reflect the position or the policy of the Government, and no official endorsement should be inferred.
Thanks to our reviewers, Rich Thomason, David Traum and Jason Williams.
<newSection> References Adam L. Berger, Stephen Della Pietra, and Vincent J. Della Pietra.
1996. A maximum entropy approach to natural language processing.
Computational Linguistics, 22(1):39–71.<newSection> Abstract We present parsing algorithms for various mildly non-projective dependency for-malisms.
In particular, algorithms are presented for: all well-nested structures of gap degree at most 1, with the same complexity as the best existing parsers for constituency formalisms of equivalent generative power; all well-nested structures with gap degree bounded by any constant k; and a new class of structures with gap degree up to k that includes some ill-nested structures.
The third case includes all the gap degree k structures in a number of dependency treebanks.
<newSection> 1 Introduction Dependency parsers analyse a sentence in terms of a set of directed links (dependencies) expressing the head-modifier and head-complement rela-tionships which form the basis of predicate argument structure.
We take dependency structures to be directed trees, where each node corresponds to a word and the root of the tree marks the syntactic head of the sentence.
For reasons of efficiency, many practical implementations of dependency parsing are restricted to projective structures, in which the subtree rooted at each word covers a contiguous substring of the sentence.
However, while free word order languages such as Czech do not satisfy this constraint, parsing without the projectivity constraint is computation-ally complex.
Although it is possible to parse non-projective structures in quadratic time under a model in which each dependency decision is independent of all the others (McDonald et al., 2005), the problem is intractable in the absence of this assumption (McDonald and Satta, 2007).
Nivre and Nilsson (2005) observe that most non-projective dependency structures appearing in practice are “close” to being projective, since they contain only a small proportion of non-projective arcs.
This has led to the study of classes of dependency structures that lie between projective and unrestricted non-projective structures (Kuhlmann and Nivre, 2006; Havelka, 2007).
Kuhlmann (2007) investigates several such classes, based on well-nestedness and gap degree constraints (Bodirsky et al., 2005), relating them to lexicalised constituency grammar formalisms.
Specifically, he shows that: linear context-free rewriting systems (LCFRS) with fan-out k (Vijay-Shanker et al., 1987; Satta, 1992) induce the set of dependency structures with gap degree at most k − 1; coupled context-free grammars in which the maximal rank of a nonterminal is k (Hotz and Pitsch, 1996) induce the set of well-nested dependency structures with gap degree at most k − 1; and LTAGs (Joshi and Schabes, 1997) induce the set of well-nested dependency structures with gap degree at most 1.
These results establish that there must be polynomial-time dependency parsing algorithms for well-nested structures with bounded gap degree, since such parsers exist for their corresponding lexicalised constituency-based formalisms.
However, since most of the non-projective structures in treebanks are well-nested and have a small gap degree (Kuhlmann and Nivre, 2006), developing efficient dependency parsing strategies for these sets of structures has considerable practical interest, since we would be able to parse directly with dependencies in a data-driven manner, rather than indirectly by constructing intermediate constituency grammars and extracting dependencies from constituency parses.
We address this problem with the following contributions: (1) we define a parsing algorithm for well-nested dependency structures of gap degree 1, and prove its correctness.
The parser runs in time O(n7), the same complexity as the best existing algorithms for LTAG (Eisner and Satta, 2000), and can be optimised to O(n6) in the non-lexicalised case; (2) we generalise the previous algorithm to any well-nested dependency structure with gap degree at most k in time O(n5+2k); (3) we generalise the previous parsers to be able to analyse not only well-nested structures, but also ill-nested structures with gap degree at most k satisfying certain constraints1, in time O(n4+3k); and (4) we characterise the set of structures covered by this parser, which we call mildly ill-nested structures, and show that it includes all the trees present in a number of dependency treebanks.
<newSection> 2 Preliminaries A dependency graph for a string w1 ...
wn is a graph G = (V, E), where V = {w1, ...
, wn} and E ⊆ V × V . We write the edge (wi, wj) as wi → wj, meaning that the word wi is a syntactic dependent (or a child) of wj or, conversely, that wj is the governor (parent) of wi.
We write wi →* wj to denote that there exists a (possi-bly empty) path from wi to wj.
The projection of a node wi, denoted bwic, is the set of reflexive-transitive dependents of wi, that is: bwic = {wj ∈ V  |wj →* wi}.
An interval (with endpoints i and j) is a set of the form [i, j] = {wk  |i ≤ k ≤ j}.
A dependency graph is said to be a tree if it is: We now define the concepts of gap degree and well-nestedness (Kuhlmann and Nivre, 2006).
Let T be a (possibly partial) dependency tree for w1 ...
wn: We say that T is projective if an interval for every word wi.
Thus every node in the dependency structure must dominate a contiguous substring in the sentence.
The gap degree of a particular node wk in T is the minimum g ∈ IlV such that bwkc can be written as the union of g +1 intervals; that is, the number of discontinuities in bwkc.
The gap degree of the dependency tree T is the maximum among the gap degrees of its nodes.
Note that T has gap degree 0 if and only if T is projective.
The subtrees induced by nodes wp and wQ are interleaved if bwpc ∩ bwQc = ∅ and there are nodes wi, wj ∈ bwpc and wk, wl ∈ bwQc such that i < k < j < l.
A dependency tree T is well-nested if it does not contain two interleaved subtrees.
A tree that is not well-nested is said to be ill-nested.
Note that projective trees are always well-nested, but well-nested trees are not always projective.
The framework of parsing schemata (Sikkel, 1997) provides a uniform way to describe, analyse and compare parsing algorithms.
Parsing schemata were initially defined for constituency-based grammatical formalisms, but G´omez-Rodriguez et al.
(2008a) define a variant of the framework for dependency-based parsers.
We use these dependency parsing schemata to define parsers and prove their correctness.
Due to space constraints, we only provide brief outlines of the main concepts behind dependency parsing schemata.
The parsing schema approach considers parsing as deduction, generating intermediate results called items.
An initial set of items is obtained from the input sentence, and the parsing process involves deduction steps which produce new items from existing ones.
Each item contains information about the sentence’s structure, and a successful parsing process produces at least one final item providing a full dependency analysis for the sentence or guaranteeing its existence.
In a dependency parsing schema, items are defined as sets of partial dependency trees2.
To define a parser by means of a schema, we must define an item set and provide a set of deduction steps that operate on it.
Given an item set I, the set of final items for strings of length n is the set of items in I that contain a full dependency tree for some arbitrary string of length n.
A final item containing a dependency tree for a particular string w1 ...
wn is said to be a correctfinal item for that string.
These concepts can be used to prove the correctness of a parser: for each input string, a parsing schema’s deduction steps allow us to infer a set of items, called valid items for that string.
A schema is said to be sound if all valid final items it produces for any arbitrary string are correct for that string.
A schema is said to be complete if all correct final items are valid.
A correct parsing schema is one which is both sound and complete.
In constituency-based parsing schemata, deduction steps usually have grammar rules as side conditions.
In the case of dependency parsers it is also possible to use grammars (Eisner and Satta, 1999), but many algorithms use a data-driven approach instead, making individual decisions about which dependencies to create by using probabilis-tic models (Eisner, 1996) or classifiers (Yamada and Matsumoto, 2003).
To represent these algorithms as deduction systems, we use the notion of D-rules (Covington, 1990).
D-rules take the form a → b, which says that word b can have a as a dependent.
Deduction steps in non-grammarbased parsers can be tied to the D-rules associated with the links they create.
In this way, we obtain a representation of the underlying logic of the parser while abstracting away from control structures (the particular model used to create the decisions associated with D-rules).
Furthermore, the choice points in the parsing process and the information we can use to make decisions are made explicit in the steps linked to D-rules.
<newSection> 3 The WG1 parser We define WG1, a parser for well-nested dependency structures of gap degree ≤ 1, as follows: The item set is IWG1 = I1 ∪ I2, with where each item of the form [i, j, h, o, o] represents the set of all well-nested partial dependency trees3 with gap degree at most 1, rooted at wh, and such that bwhc = {wh} ∪ [i, j], and 3In this and subsequent schemata, we use D-rules to express parsing decisions, so partial dependency trees are assumed to be taken from the set of trees licensed by a set of D-rules.
where each item of the form [i, j, h,l, r] represents the set of all well-nested partial dependency trees rooted at wh such that bwhc = {wh} ∪ ([i, j] \ [l, r]), and all the nodes (except possibly h) have gap degree at most 1.
We call items of this form gapped items, and the interval [l, r] the gap of the item.
Note that the constraints h =6 j, h =6 i + 1, h =6 l − 1, h =6 r are added to items to avoid redundancy in the item set.
Since the result of the expression {wh} ∪ ([i, j] \ [l, r]) for a given head can be the same for different sets of values of i, j,l, r, we restrict these values so that we cannot get two different items representing the same dependency structures.
Items t violating these constraints always have an alternative representation that does not violate them, that we can express with a normalising function nm(t) as follows: When defining the deduction steps for this and other parsers, we assume that they always produce normalised items.
For clarity, we do not explicitly write this in the deduction steps, writing t instead of nm(t) as antecedents and consequents of steps.
The set of initial items is defined as the set H = {[h, h, h, o, o]  |h ∈ W,1 ≤ h ≤ n}, where each item [h, h, h, o, o] represents the set containing the trivial partial dependency tree consisting of a single node wh and no links.
This same set of hypotheses can be used for all the parsers, so we do not make it explicit for subsequent schemata.
Note that initial items are separate from the item set IWG1 and not subject to its constraints, so they do not require normalisation.
The set of final items for strings of length n in WG1 is defined as the set which is the set of items in IWG1 containing dependency trees for the complete input string (from position 1 to n), with their head at any word wh.
The deduction steps of the parser can be seen in Figure 1A.
The WG1 parser proceeds bottom-up, by building dependency subtrees and joining them to form larger subtrees, until it finds a complete dependency tree for the input sentence.
The logic of Combine Interleaving Gap L: [i, j, h, l, r] Combine Interleaving Gap R: [i, j, h, l, r] [l, k, h, r + 1, u] [k, m, h, r + 1, j] [i, k, h, j + 1, u] [i, m, h, l, k − 1] such that u > j, such that k > l.
D. General form of the MGk Combine step: the parser can be understood by considering how it infers the item corresponding to the subtree induced by a particular node, given the items for the subtrees induced by the direct dependents of that node.
Suppose that, in a complete dependency analysis for a sentence wi ...
wn, the word wh has wd1 ...
wdp as direct dependents (i.e. we have dependency links wd1 → wh, . .
.
, wdp → wh).
Then, the item corresponding to the subtree induced by wh is obtained from the ones corresponding to the subtrees induced by wd1 ...
wdp by: (1) applying the Link Ungapped or Link Gapped step to each of the items corresponding to the subtrees induced by the direct dependents, and to the hypothesis [h, h, h, o, o].
This allows us to infer p items representing the result of linking each of the dependent subtrees to the new head wh; (2) applying the various Combine steps to join all of the items obtained in the previous step into a single item.
The Combine steps perform a union operation between subtrees.
Therefore, the result is a dependency tree containing all the dependent sub-trees, and with all of them linked to h: this is the subtree induced by wh.
This process is applied repeatedly to build larger subtrees, until, if the parsing process is successful, a final item is found containing a dependency tree for the complete sentence.
The parsing schemata formalism can be used to prove the correctness of a parsing schema.
To prove that WG1 is correct, we need to prove its soundness and completeness.4 Soundness is proven by checking that valid items always contain well-nested trees.
Completeness is proven by induction, taking initial items as the base case and showing that an item containing a correct subtree for a string can always be obtained from items corresponding to smaller subtrees.
In order to prove this induction step, we use the concept of order annotations (Kuhlmann, 2007; Kuhlmann and M¨ohl, 2007), which are strings that lexicalise the precedence relation between the nodes of a dependency tree.
Given a correct subtree, we divide the proof into cases according to the order annotation of its head and we find that, for every possible form of this order annotation, we can find a sequence of Combine steps to infer the relevant item from smaller correct items.
The time complexity of WG1 is O(n7), as the step Combine Shrinking Gap Centre works with 7 free string positions.
This complexity with respect to the length of the input is as expected for this set of structures, since Kuhlmann (2007) shows that they are equivalent to LTAG, and the best existing parsers for this formalism also perform in O(n7) (Eisner and Satta, 2000).
Note that the Combine step which is the bottleneck only uses the 7 indexes, and not any other entities like D-rules, so its O(n7) complexity does not have any additional factors due to grammar size or other variables.
The space complexity of WG1 is O(n5) for recognition, due to the 5 indexes in items, and O(n7) for full parsing.
It is possible to build a variant of this parser with time complexity O(n6), as with parsers for unlexicalised TAG, if we work with unlexicalised D-rules specifying the possibility of dependencies between pairs of categories instead of pairs of words.
In order to do this, we expand the item set with unlexicalised items of the form [i, j, C,l, r], where C is a category, apart from the existing items [i, j, h,l, r].
Steps in the parser are duplicated, to work both with lexicalised and unlex-icalised items, except for the Link steps, which always work with a lexicalised item and an un-lexicalised hypothesis to produce an unlexicalised item, and the Combine Shrinking Gap steps, which can work only with unlexicalised items.
Steps are added to obtain lexicalised items from their unlex-icalised equivalents by binding the head to particular string positions.
Finally, we need certain variants of the Combine Shrinking Gap steps that take 2 unlexicalised antecedents and produce a lexicalised consequent; an example is the following: such that cat(wl)=C Although this version of the algorithm reduces time complexity with respect to the length of the input to O(n6), it also adds a factor related to the number of categories, as well as constant factors due to using more kinds of items and steps than the original WG1 algorithm.
This, together with the advantages of lexicalised dependency parsing, may mean that the original WG1 algorithm is more practical than this version.
<newSection> 4 The WGk parser The WG1 parsing schema can be generalised to obtain a parser for all well-nested dependency structures with gap degree bounded by a constant k(k > 1), which we call WGk parser.
In order to do this, we extend the item set so that it can contain items with up to k gaps, and modify the deduction steps to work with these multi-gapped items.
The item set ZWGk is the set of all [i, j, h, [(l1, r1), ...
, (lg, rg)]] where i, j, h, g E IlV , 0 < g < k, 1 < h < n, 1 < i < j < n , h =� j, h =� i − 1; and for each p E 11,2,...
, g}: lp, rp E W, i < lp < rp < j, rp < lp+1 − 1, h =�lp − 1, h =�rp.
An item [i, j, h, [(l1, r1), ... , (lg, rg)]] represents the set of all well-nested partial dependency trees rooted at wh such that Lwh] = {wh}U([i, j]� Ugp=1[lp, rp]), where each interval [lp, rp] is called a gap.
The constraints h =� j, h =� i + 1, h =� lp − 1, h =� rp are added to avoid redundancy, and normalisation is defined as in WG1.
The set of final items is defined as the set F = {[1, n, h, []] | h E W,1 < h < n}.
Note that this set is the same as in WG1, as these are the items that we denoted [1, n, h, o, o] in the previous parser.
The deduction steps can be seen in Figure 1B.
As expected, the WG1 parser corresponds to WGk when we make k = 1.
WGk works in the same way as WG1, except for the fact that Combine steps can create items with more than one gap5.
The correctness proof is also analogous to that of WG1, but we must take into account that the set of possible order annotations is larger when k > 1, so more cases arise in the completeness proof.
The WGk parser runs in time O(n5+2k): as in the case of WG1, the deduction step with most free variables is Combine Shrinking Gap Centre, and in this case it has 5 + 2k free indexes.
Again, this complexity result is in line with what could be expected from previous research in constituency parsing: Kuhlmann (2007) shows that the set of well-nested dependency structures with gap degree at most k is closely related to coupled context-free grammars in which the maximal rank of a nonterminal is k + 1; and the constituency parser defined by Hotz and Pitsch (1996) for these grammars also adds an n2 factor for each unit increment of k.
Note that a small value of k should be enough to cover the vast majority of the non-projective sentences found in natural language treebanks.
For example, the Prague Dependency Treebank contains no structures with gap degree greater than 4.
Therefore, a WG4 parser would be able to analyse all the well-nested structures in this treebank, which represent 99.89% of the total.
Increasing k beyond 4 would not produce further improvements in coverage.
<newSection> 5 Parsing ill-nested structures The WGk parser analyses dependency structures with bounded gap degree as long as they are well-nested.
This covers the vast majority of 5In all the parsers in this paper, Combine steps may be applied in different orders to produce the same result, causing spurious ambiguity.
In WG1 and WGk, this can be avoided when implementing the schemata, by adding flags to items so as to impose a particular order.
the structures that occur in natural-language tree-banks (Kuhlmann and Nivre, 2006), but there is still a significant minority of sentences that contain ill-nested structures.
Unfortunately, the general problem of parsing ill-nested structures is NP-complete, even when the gap degree is bounded: this set of structures is closely related to LCFRS with bounded fan-out and unbounded production length, and parsing in this formalism has been proven to be NP-complete (Satta, 1992).
The reason for this high complexity is the problem of unrestricted crossing configurations, appearing when dependency subtrees are allowed to interleave in every possible way.
However, just as it has been noted that most non-projective structures appearing in practice are only “slightly” non-projective (Nivre and Nilsson, 2005), we characterise a sense in which the structures appearing in treebanks can be viewed as being only “slightly” ill-nested.
In this section, we generalise the algorithms WG1 and WGk to parse a proper superset of the set of well-nested structures in polynomial time; and give a characterisation of this new set of structures, which includes all the structures in several dependency treebanks.
The WGk parser presented previously is based on a bottom-up process, where Link steps are used to link completed subtrees to a head, and Combine steps are used to join subtrees governed by a common head to obtain a larger structure.
As WGk is a parser for well-nested structures of gap degree up to k, its Combine steps correspond to all the ways in which we can join two sets of sibling subtrees meeting these constraints, and having a common head, into another.
Thus, this parser does not use Combine steps that produce interleaved subtrees, since these would generate items corresponding to ill-nested structures.
We obtain a polynomial parser for a wider set of structures of gap degree at most k, including some ill-nested ones, by having Combine steps representing every way in which two sets of sibling sub-trees of gap degree at most k with a common head can be joined into another, including those producing interleaved subtrees, like the steps for gap degree 1 shown in Figure 1C.
Note that this does not mean that we can build every possible ill-nested structure: some structures with complex crossed configurations have gap degree k, but cannot be built by combining two structures of that gap degree.
More specifically, our algorithm will be able to parse a dependency structure (well-nested or not) if there exists a binarisation of that structure that has gap degree at most k.
The parser implicitly works by finding such a binarisation, since Combine steps are always applied to two items and no intermediate item generated by them can exceed gap degree k (not counting the position of the head in the projection).
More formally, let T be a dependency structure for the string w1 ... wn.
A binarisation of T is a dependency tree T0 over a set of nodes, each of which may be unlabelled or labelled with a word in {w1 ...
wn}, such that the following conditions hold: (1) each node has at most two children, and (2) wz —* wj in T if and only if wz —** wj in T�.
A dependency structure is mildly ill-nested for gap degree k if it has at least one binarisation of gap degree < k.
Otherwise, we say that it is strongly ill-nested for gap degree k.
It is easy to prove that the set of mildly ill-nested structures for gap degree k includes all well-nested structures with gap degree up to k.
We define MG1, a parser for mildly ill-nested structures for gap degree 1, as follows: (1) the item set is the same as that of WG1, except that items can now contain any mildly ill-nested structures for gap degree 1, instead of being restricted to well-nested structures; and (2) deduction steps are the same as in WG1, plus the additional steps shown in Figure 1C.
These extra Combine steps allow the parser to combine interleaved subtrees with simple crossing configurations.
The MG1 parser still runs in O(n7), as these new steps do not use more than 7 string positions.
The proof of correctness for this parser is similar to that of WG1.
Again, we use the concept of order annotations.
The set of mildly ill-nested structures for gap degree k can be defined as those that only contain annotations meeting certain constraints.
The soundness proof involves showing that Combine steps always generate items containing trees with such annotations.
Completeness is proven by induction, by showing that if a subtree is mildly ill-nested for gap degree k, an item for it can be obtained from items for smaller subtrees by applying Combine and Link steps.
In the cases where Combine steps have to be applied, the order in which they may be used to produce a subtree can be obtained from its head’s order annotation.
To generalise this algorithm to mildly ill-nested structures for gap degree k, we need to add a Combine step for every possible way of joining two structures of gap degree at most k into another.
This can be done systematically by considering a set of strings over an alphabet of three symbols: a and b to represent intervals of words in the projection of each of the structures, and g to represent intervals that are not in the projection of either structure, and will correspond to gaps in the joined structure.
The legal combinations of structures for gap degree k will correspond to strings where symbols a and b each appear at most k + 1 times, g appears at most k times and is not the first or last symbol, and there is no more than one consecutive appearance of any symbol.
Given a string of this form, the corresponding Combine step is given by the expression in Figure 1D.
As a particular example, the Combine Interleaving Gap C step in Figure 1C is obtained from the string abgab.
Thus, we define the parsing schema for MGk, a parser for mildly ill-nested structures for gap degree k, as the schema where (1) the item set is like that of WGk, except that items can now contain any mildly ill-nested structures for gap degree k, instead of being restricted to well-nested structures; and (2) the set of deduction steps consists of a Link step as the one in WGk, plus a set of Combine steps obtained as expressed in Figure 1D.
As the string used to generate a Combine step can have length at most 3k + 2, and the resulting step contains an index for each symbol of the string plus two extra indexes, the MGk parser has complexity O(n3k+4).
Note that the item and deduction step sets of an MGk parser are always su-persets of those of WGk.
In particular, the steps for WGk are those obtained from strings that do not contain abab or baba as a scattered substring.
The MGk algorithm defined in the previous section can parse any mildly ill-nested structure for a given gap degree k in polynomial time.
We have characterised the set of mildly ill-nested structures for gap degree k as those having a binarisation of gap degree < k.
Since a binarisation of a dependency structure cannot have lower gap degree than the original structure, this set only contains structures with gap degree at most k.
Furthermore, by the relation between MGk and WGk, we know that it contains all the well-nested structures with gap degree up to k.
Figure 2 shows an example of a structure that has gap degree 1, but is strongly ill-nested for gap degree 1.
This is one of the smallest possible such structures: by generating all the possible trees up to 10 nodes (without counting a dummy root node located at position 0), it can be shown that all the structures of any gap degree k with length smaller than 10 are well-nested or only mildly ill-nested for that gap degree k.
Even if a structure T is strongly ill-nested for a given gap degree, there is always some m ∈ IN such that T is mildly ill-nested for m (since every dependency structure can be binarised, and binari-sations have finite gap degree).
For example, the structure in Figure 2 is mildly ill-nested for gap degree 2.
Therefore, MGk parsers have the property of being able to parse any possible dependency structure as long as we make k large enough.
In practice, structures like the one in Figure 2 do not seem to appear in dependency treebanks.
We have analysed treebanks for nine different languages, obtaining the data presented in Table 1.
None of these treebanks contain structures that are strongly ill-nested for their gap degree.
Therefore, in any of these treebanks, the MGk parser can parse every sentence with gap degree at most k.
<newSection> 6 Conclusions and future work We have defined a parsing algorithm for well-nested dependency structures with bounded gap degree.
In terms of computational complexity, this algorithm is comparable to the best parsers for related constituency-based formalisms: when the gap degree is at most 1, it runs in O(n7), like the fastest known parsers for LTAG, and can be made O(n6) if we use unlexicalised dependencies.
When the gap degree is greater than 1, the time complexity goes up by a factor of n2 for each extra unit of gap degree, as in parsers for coupled context-free grammars.
Most of the non-projective sentences appearing in treebanks are well-nested and have a small gap degree, so this algorithm directly parses the vast majority of the non-projective constructions present in natural languages, without requiring the construction of a constituency grammar as an intermediate step.
Additionally, we have defined a set of structures for any gap degree k which we call mildly ill-nested.
This set includes ill-nested structures verifying certain conditions, and can be parsed in O(n3k+4) with a variant of the parser for well-nested structures.
The practical interest of mildly ill-nested structures can be seen in the data obtained from several dependency treebanks, showing that all of the ill-nested structures in them are mildly ill-nested for their corresponding gap degree.
Therefore, our O(n3k+4) parser can analyse all the gap degree k structures in these treebanks.
The set of mildly ill-nested structures for gap degree k is defined as the set of structures that have a binarisation of gap degree at most k.
This definition is directly related to the way the MGk parser works, since it implicitly finds such a binarisation.
An interesting line of future work would be to find an equivalent characterisation of mildly ill-nested structures which is more grammar-oriented and would provide a more linguistic insight into these structures.
Another research direction, which we are currently working on, is exploring how variants of the MGk parser’s strategy can be applied to the problem of binarising LCFRS (G´omez-Rodriguez et al., 2009).
<newSection> References<newSection> Abstract In this paper, we present an integrated model of the two central tasks of dialog management: interpreting user actions and generating system actions.
We model the interpretation task as a classification problem and the generation task as a prediction problem.
These two tasks are interleaved in an incremental parsing-based dialog model.
We compare three alternative parsing methods for this dialog model using a corpus of human-human spoken dialog from a catalog ordering domain that has been annotated for dialog acts and task/subtask information.
We contrast the amount of context provided by each method and its impact on performance.
<newSection> 1 Introduction Corpora of spoken dialog are now widely available, and frequently come with annotations for tasks/games, dialog acts, named entities and elements of syntactic structure.
These types of information provide rich clues for building dialog models (Grosz and Sidner, 1986).
Dialog models can be built offline (for dialog mining and summariza-tion), or online (for dialog management).
A dialog manager is the component of a dialog system that is responsible for interpreting user actions in the dialog context, and for generating system actions.
Needless to say, a dialog manager operates incrementally as the dialog progresses.
In typical commercial dialog systems, the interpretation and generation processes operate indepen-dently of each other, with only a small amount of shared context.
By contrast, in this paper we describe a dialog model that (1) tightly integrates interpretation and generation, (2) makes explicit the type and amount of shared context, (3) includes the task structure of the dialog in the context, (4) can be trained from dialog data, and (5) runs incrementally, parsing the dialog as it occurs and interleaving generation and interpretation.
At the core of our model is a parser that incrementally builds the dialog task structure as the dialog progresses.
In this paper, we experiment with three different incremental tree-based parsing methods.
We compare these methods using a corpus of human-human spoken dialogs in a catalog ordering domain that has been annotated for dialog acts and task/subtask information.
We show that all these methods outperform a baseline method for recovering the dialog structure.
The rest of this paper is structured as follows: In Section 2, we review related work.
In Section 3, we present our view of the structure of task-oriented human-human dialogs.
In Section 4, we present the parsing approaches included in our experiments.
In Section 5, we describe our data and experiments.
Finally, in Section 6, we present conclusions and describe our current and future work.
<newSection> 2 Related Work There are two threads of research that are relevant to our work: work on parsing (written and spoken) discourse, and work on plan-based dialog models.
Discourse Parsing Discourse parsing is the process of building a hierarchical model of a discourse from its basic elements (sentences or clauses), as one would build a parse of a sentence from its words.
There has now been considerable work on discourse parsing using statistical bottom-up parsing (Soricut and Marcu, 2003), hierarchical agglomerative clustering (Sporleder and Lascarides, 2004), parsing from lexicalized tree-adjoining grammars (Cristea, 2000), and rule-based approaches that use rhetorical relations and discourse cues (Forbes et al., 2003; Polanyi et al., 2004; LeThanh et al., 2004).
With the exception of Cristea (2000), most of this research has been limited to non-incremental parsing of textual monologues where, in contrast to incremental dialog parsing, predicting a system action is not relevant.
The work on discourse parsing that is most similar to ours is that of Baldridge and Lascarides (2005).
They used a probabilistic head-driven parsing method (described in (Collins, 2003)) to construct rhetorical structure trees for a spoken dialog corpus.
However, their parser was not incremental; it used global features such as the number of turn changes.
Also, it focused strictly in interpretation of input utterances; it could not predict actions by either dialog partner.
In contrast to other work on discourse parsing, we wish to use the parsing process directly for di-alog management (rather than for information extraction or summarization).
This influences our approach to dialog modeling in two ways.
First, the subtask tree we build represents the functional task structure of the dialog (rather than the rhetorical structure of the dialog).
Second, our dialog parser must be entirely incremental.
Plan-Based Dialog Models Plan-based approaches to dialog modeling, like ours, operate directly on the dialog’s task structure.
The process of task-oriented dialog is treated as a special case of AI-style plan recognition (Sidner,1985; Litman and Allen, 1987; Rich and Sidner,1997; Carberry, 2001; Bohus and Rudnicky, 2003; Lochbaum, 1998).
Plan-based dialog models are used for both interpretation of user utterances and prediction of agent actions.
In addition to the hand-crafted models listed above, researchers have built stochastic plan recognition models for interaction, including ones based on Hidden Markov Models (Bui, 2003; Blaylock and Allen, 2006) and on probabilistic context-free grammars (Alexandersson and Reithinger,1997; Pynadath and Wellman, 2000).
In this area, the work most closely related to ours is that of Barrett and Weld (Barrett and Weld, 1994), who build an incremental bottom-up parser Figure 2: Sample output (subtask tree) from a parse-based model for the catalog ordering domain.
to parse plans.
Their parser, however, was not probabilistic or targeted at dialog processing.
<newSection> 3 Dialog Structure We consider a task-oriented dialog to be the result of incremental creation of a shared plan by the participants (Lochbaum, 1998).
The shared plan is represented as a single tree T that incorporates the task/subtask structure, dialog acts, syntactic structure and lexical content of the dialog, as shown in Figure 1.
A task is a sequence of sub-tasks ST ∈ S.
A subtask is a sequence of dialog acts DA ∈ D.
Each dialog act corresponds to one clause spoken by one speaker, customer (cu) or agent (ca) (for which we may have acoustic, lexical, syntactic and semantic representations).
Figure 2 shows the subtask tree for a sample di-alog in our domain (catalog ordering).
An order placement task is typically composed of the sequence of subtasks opening, contact-information, order-item, related-offers, summary.
Subtasks can be nested; the nesting can be as deep as five levels in our data.
Most often the nesting is at the leftmost or rightmost frontier of the subtask tree.
As the dialog proceeds, an utterance from a participant is accommodated into the subtask tree in an incremental manner, much like an incremental syntactic parser accommodates the next word into a partial parse tree (Alexandersson and Rei-thinger, 1997).
An illustration of the incremental evolution of dialog structure is shown in Figure 4.
However, while a syntactic parser processes input from a single source, our dialog parser parses user-system exchanges: user utterances are interpreted, while system utterances are generated.
So the steps taken by our dialog parser to incorporate an utterance into the subtask tree depend on whether the utterance was produced by the agent or the user (as shown in Figure 3).
User utterances Each user turn is split into clauses (utterances).
Each clause is supertagged i−k represents the dialog act tags for utterances i − 1 to i − k. and labeled with named entities1.
Interpretation of the clause (cyi ) involves assigning a dialog act label (dayi ) and a subtask label (styi ).
We use ST i−1 i−k to represent the sequence of pre-ceeding k subtask labels, dialog act labels and clauses respectively.
The dialog act label dayi is determined from information about the clause and (a kth order approximation of) the subtask tree so far (Ti−1 = (STi−1 i−k)), as shown in Equation 1 (Table 1).
The subtask label styi is determined from information about the clause, its dialog act and the subtask tree so far, as shown in Equation 2.
Then, the clause is incorporated into the subtask tree.
Agent utterances In contrast, a dialog system starts planning an agent utterance by identifying the subtask to contribute to next, sta , based on the subtask tree so far (Ti−1 = i−k)), as shown in Equation 3 (Table 1) . Then, it chooses the dialog act of the utterance, daa , based on the subtask tree so far and the chosen subtask for the utterance, as shown in Equation 4.
Finally, it generates an utterance, ca , to realize its communicative intent (represented as a subtask and dialog act pair, with associated named entities)2.
Note that the current clause cy i is used in the conditioning context of the interpretation model (for user utterances), but the corresponding clause for the agent utterance ca is to be predicted and hence is not part of conditioning context in the generation model.
<newSection> 4 Dialog Parsing A dialog parser can produce a “shallow” or “deep” tree structure.
A shallow parse is one in which utterances are grouped together into subtasks, but the dominance relations among subtasks are not tracked.
We call this model a chunk-based dia-log model (Bangalore et al., 2006).
The chunk-based model has limitations.
For example, dominance relations among subtasks are important for dialog processes such as anaphora resolution (Grosz and Sidner, 1986).
Also, the chunk-based model is representationally inadequate for center-embedded nestings of subtasks, which do occur in our domain, although less frequently than the more prevalent “tail-recursive” structures.
We use the term parse-based dialog model to refer to deep parsing models for dialog which not only segment the dialog into chunks but also predict dominance relations among chunks.
For this paper, we experimented with three alternative methods for building parse-based models: shift-reduce, start-complete and connection path.
Each of these operates on the subtask tree for the dialog incrementally, from left-to-right, with access only to the preceding dialog context, as shown in Figure 4.
They differ in the parsing actions and the data structures used by the parser; this has implications for robustness to errors.
The instructions to reconstruct the parse are either entirely encoded in the stack (in the shift-reduce method), or entirely in the parsing actions (in the start-complete and connection path methods).
For each of the four types of parsing action required to build the parse tree (see Table 1), we construct a feature vector containing contextual information for the parsing action (see Section 5.1).
These feature vectors and the associated parser actions are used to train maximum entropy models (Berger et al., 1996).
These models are then used to incrementally incorporate the utterances for a new di-alog into that dialog’s subtask tree as the dialog progresses, as shown in Figure 3.
In this method, the subtask tree is recovered through a right-branching shift-reduce parsing process (Hall et al., 2006; Sagae and Lavie, 2006).
The parser shifts each utterance on to the stack.
It then inspects the stack and decides whether to do one or more reduce actions that result in the creation of subtrees in the subtask tree.
The parser maintains two data structures – a stack and a tree.
The actions of the parser change the contents of the stack and create nodes in the dialog tree structure.
The actions for the parser include unary-reduce-X, binary-reduce-X and shift, where X is each of the non-terminals (subtask labels) in the tree.
Shift pushes a token representing the utterance onto the stack; binary-reduce-X pops two tokens off the stack and pushes the non-terminal X; and unary-reduce-X pops one token off the stack and pushes the non-terminal X.
Each type of reduce action creates a constituent X in the dialog tree and the tree(s) associated with the reduced elements as subtree(s) of X.
At the end of the dialog, the output is a binary branching subtask tree.
Consider the example subdialog A: would you like a free magazine?
U: no.
The processing of this dialog using our shift-reduce dialog parser would proceed as follows: the STP model predicts shift for st'; the DAP model predicts YNP(Promotions) for da'; the generator outputs would you like a free magazine?; and the parser shifts a token representing this utterance onto the stack.
Then, the customer says no.
The DAC model classifies dal as No; the STC model classifies st' as shift and binary-reduce-special-offer; and the parser shifts a token representing the utterance onto the stack, before popping the top two elements off the stack and adding the subtree for special-order into the dialog’s subtask tree.
In the shift-reduce method, the dialog tree is constructed as a side effect of the actions performed on the stack: each reduce action on the stack introduces a non-terminal in the tree.
By contrast, in the start-complete method the instructions to build the tree are directly encoded in the parser actions.
A stack is used to maintain the global parse state.
The actions the parser can take are similar to those described in (Ratnaparkhi, 1997).
The parser must decide whether to join each new terminal onto the existing left-hand edge of the tree, or start a new subtree.
The actions for the parser include start-X, n-start-X, complete-X, u-completeX and b-complete-X, where X is each of the non-terminals (subtask labels) in the tree.
Start-X pushes a token representing the current utterance onto the stack; n-start-X pushes non-terminal X onto the stack; complete-X pushes a token representing the current utterance onto the stack, then pops the top two tokens off the stack and pushes the non-terminal X; u-complete-X pops the top token off the stack and pushes the non-terminal X; and b-complete-X pops the top two tokens off the stack and pushes the non-terminal X.
This method produces a dialog subtask tree directly, rather than producing an equivalent binary-branching tree.
Consider the same subdialog as before, A: would you like a free magazine?
U: no.
The processing of this dialog using our start-complete dialog parser would proceed as follows: the STP model predicts start-special-offer for sta; the DAP model predicts YNP(Promotions) for daa; the generator outputs would you like a free magazine?; and the parser shifts a token representing this utterance onto the stack.
Then, the customer says no.
The DAC model classifies dau as No; the STC model classifies stu as complete-special-offer; and the parser shifts a token representing the utterance onto the stack, before popping the top two elements off the stack and adding the subtree for special-order into the dialog’s subtask tree.
In contrast to the shift-reduce and the start-complete methods described above, the connection path method does not use a stack to track the global state of the parse.
Instead, the parser directly predicts the connection path (path from the root to the terminal) for each utterance.
The collection of connection paths for all the utterances in a dialog defines the parse tree.
This encoding was previously used for incremental sentence parsing by (Costa et al., 2001).
With this method, there are many more choices of decision for the parser (195 decisions for our data) compared to the shift-reduce (32) and start-complete (82) methods.
Consider the same subdialog as before, A: would you like a free magazine?
U: no.
The processing of this dialog using our connection path dialog parser would proceed as follows.
First, the STP model predicts S-special-offer for sta; the DAP model predicts YNP(Promotions) for daa; the generator outputs would you like a free magazine?; and the parser adds a subtree rooted at special-offer, with one terminal for the current utterance, into the top of the subtask tree.
Then, the customer says no.
The DAC model classifies dau as No and the STC model classifies stu as S-special-offer.
Since the right frontier of the subtask tree has a subtree matching this path, the parser simply incorporates the current utterance as a terminal of the special-offer subtree.
<newSection> 5 Data and Experiments To evaluate our parse-based dialog model, we used 817 two-party dialogs from the CHILD corpus of telephone-based dialogs in a catalog-purchasing domain.
Each dialog was transcribed by hand; all numbers (telephone, credit card, etc.) were removed for privacy reasons.
The average dialog in this data set had 60 turns.
The dialogs were automatically segmented into utterances and automatically annotated with part-ofspeech tag and supertag information and named entities.
They were annotated by hand for dia-log acts and tasks/subtasks.
The dialog act and task/subtask labels are given in Tables 2 and 3.
In our experiments we used the following features for each utterance: (a) the speaker ID; (b) unigrams, bigrams and trigrams of the words; (c) unigrams, bigrams and trigrams of the part of speech tags; (d) unigrams, bigrams and trigrams of the su-pertags; (e) binary features indicating the presence or absence of particular types of named entity; (f) the dialog act (determined by the parser); (g) the task/subtask label (determined by the parser); and (h) the parser stack at the current utterance (deter-mined by the parser).
Each input feature vector for agent subtask prediction has these features for up to three utterances of left-hand context (see Equation 3).
Each input feature vector for dialog act prediction has the same features as for agent sub-task prediction, plus the actual or predicted sub-task label (see Equation 4).
Each input feature vector for dialog act interpretation has features ah for up to three utterances of left-hand context, plus the current utterance (see Equation 1).
Each input feature vector for user subtask classification has the same features as for user dialog act interpretation, plus the actual or classified dialog act (see Equation 2).
The label for each input feature vector is the parsing action (for subtask classification and prediction) or the dialog act label (for dialog act classification and prediction).
If more than one parsing action takes place on a particular utterance (e.g. a shift and then a reduce), the feature vector is repeated twice with different stack contents.
We randomly selected roughly 90% of the dialogs for training, and used the remainder for testing.
We separately trained models for: user dia-log act classification (DAC, Equation 1); user task/subtask classification (STC, Equation 2); agent task/subtask prediction (STP, Equation 3); and agent dialog act prediction (DAP, Equation 4).
In order to estimate the conditional distributions shown in Table 1, we use the general technique of choosing the MaxEnt distribution that properly estimates the average of each feature over the training data (Berger et al., 1996).
We use the machine learning toolkit LLAMA (Haffner, 2006), which encodes multiclass classification problems using binary MaxEnt classifiers to increase the speed of training and to scale the method to large data sets.
The decoding process for the three parsing methods is illustrated in Figure 3 and has four stages: STP, DAP, DAC, and STC.
As already explained, each of these steps in the decoding process is mod-eled as either a prediction task or a classification task.
The decoder constructs an input feature vector depending on the amount of context being used.
This feature vector is used to query the appropriate classifier model to obtain a vector of labels with weights.
The parser action labels (STP and STC) are used to extend the subtask tree.
For example, in the shift-reduce method, shift results in a push action on the stack, while reduce-X results in popping the top two elements off the stack and pushing X on to the stack.
The dialog act labels (DAP and DAC) are used to label the leaves of the subtask tree (the utterances).
The decoder can use n-best results from the classifier to enlarge the search space.
In order to manage the search space effectively, the decoder uses a beam pruning strategy.
The decoding process proceeds until the end of the dialog is reached.
In this paper, we assume that the end of the dialog is given to the decoder3.
Given that the classifiers are error-prone in their assignment of labels, the parsing step of the decoder needs to be robust to these errors.
We exploit the state of the stack in the different methods to rule out incompatible parser actions (e.g. a reduce-X action when the stack has one element, a shift action on an already shifted utterance).
We also use n-best results to alleviate the impact of classification errors.
Finally, at the end of the di-alog, if there are unattached constituents on the stack, the decoder attaches them as sibling constituents to produce a rooted tree structure.
These constraints contribute to robustness, but cannot be used with the connection path method, since any connection path (parsing action) suggested by the classifier can be incorporated into the incremental parse tree.
Consequently, in the connection path method there are fewer opportunities to correct the errors made by the classifiers.
We evaluate dialog act classification and prediction by comparing the automatically assigned di-alog act tags to the reference dialog act tags.
For these tasks we report accuracy.
We evaluate subtask classification and prediction by comparing the subtask trees output by the different parsing methods to the reference subtask tree.
We use the labeled crossing bracket metric (typically used in the syntactic parsing literature (Harrison et al., 1991)), which computes recall, precision and crossing brackets for the constituents (subtrees) in a hypothesized parse tree given the reference parse tree.
We report F-measure, which is a combination of recall and precision.
For each task, performance is reported for 1, 3, Figure 5 shows the performance of the different methods for determining the subtask tree of the dialog.
Wider beam widths do not lead to improved performance for any method.
One utterance of context is best for shift-reduce and start-join; three is best for the connection path method.
The shift-reduce method performs the best.
With 1 utterance of context, its 1-best f-score is 47.86, as compared with 34.91 for start-complete, 25.13 for the connection path method, and 21.32 for the chunk-based baseline.
These performance differences are statistically significant at p < .001.
However, the best performance for the shift-reduce method is still significantly worse than oracle.
All of the methods are subject to some ‘stickiness’, a certain preference to stay within the current subtask rather than starting a new one.
Also, all of the methods tended to perform poorly on parsing subtasks that occur rarely (e.g. call-forward, order-change) or that occur at many different locations in the dialog (e.g. out-of-domain, order-problem, check-availability).
For example, the shift-reduce method did not make many shift errors but did frequently b-reduce on an incorrect non-terminal (indicating trouble identifying subtask boundaries).
Some non-terminals most likely to be labeled incorrectly by this method (for both agent and user) are: call-forward, order-change, summary, order-problem, opening and out-of-domain.
Similarly, the start-complete method frequently mislabeled a non-terminal in a complete action, e.g. misc-other, check-availability, summary or contact-info.
It also quite frequently mislabeled nonterminals in n-start actions, e.g. order-item, contact-info or summary.
Both of these errors indicate trouble identifying subtask boundaries.
It is harder to analyze the output from the connection path method.
This method is more likely to mislabel tree-internal nodes than those immediately above the leaves.
However, the same non-terminals show up as error-prone for this method as for the others: out-of-domain, check-availability, order-problem and summary.
Figure 6 shows accuracy for classification of user dialog acts.
Wider beam widths do not lead to signficantly improved performance for any method.
Zero utterances of context gives the highest accuracy for all methods.
All methods perform fairly well, but no method significantly outperforms any other: with 0 utterances of context, 1-best accuracy is .681 for the connection path method, .698 for the start-complete method and .698 for the shift-reduce method.
We note that these results are competitive with those reported in the literature (e.g. (Poesio and Mikheev, 1998; Serafin and Eugenio, 2004)), although the dialog corpus and the label sets are different.
The most common errors in dialog act classification occur with dialog acts that occur 40 times or fewer in the testing data (out of 3610 testing utterances), and with Not(Information).
dialog acts pertaining to Order-Info and Product-Info acts are commonly mislabeled, which could potentially indicate that these labels require a subtle distinction between information pertaining to an order and information pertaining to a product.
Table 4 shows the parsing actions performed by each of our methods on the dialog snippet presented in Figure 4.
For this example, the connection path method’s output is correct in all cases. that for dialog act classification because this is a prediction task.
Wider beam widths do not generally lead to improved performance for any method.
Three utterances of context generally gives the best performance.
The shift-reduce method performs significantly better than the connection path method with a beam width of 1 (p < .01), but not at larger beam widths; there are no other significant performance differences between methods at 3 utterances of context.
With 3 utterances of context, 1-best accuracies are .286 for the connection path method, .329 for the start-complete method and .356 for the shift-reduce method.
The most common errors in dialog act prediction occur with rare dialog acts, Not(Information), and the prediction of Acknowledge at the start of a turn (we did not remove grounding acts from the data).
With the shift-reduce method, some YNQ acts are commonly mislabeled.
With all methods, In this paper, we present a parsing-based model of task-oriented dialog that tightly integrates interpretation and generation using a subtask tree representation, can be trained from data, and runs incrementally for use in dialog management.
At the core of this model is a parser that incremen-tally builds the dialog task structure as it interprets user actions and generates system actions.
We experiment with three different incremental parsing methods for our dialog model.
Our proposed shift-reduce method is the best-performing so far, and performance of this method for dialog act classification and task/subtask modeling is good enough to be usable.
However, performance of all the methods for dialog act prediction is too low to be useful at the moment.
In future work, we will explore improved models for this task that make use of global information about the task (e.g. whether each possible subtask has yet been completed; whether required and optional task-related concepts such as shipping address have been filled).
We will also separate grounding and task-related behaviors in our model.
<newSection> References<newSection> Abstract We present a classifier to predict contextual polarity of subjective phrases in a sentence.
Our approach features lexical scoring derived from the Dictionary of Affect in Language (DAL) and extended through WordNet, allowing us to automatically score the vast majority of words in our input avoiding the need for manual labeling.
We augment lexical scoring with n-gram analysis to capture the effect of context.
We combine DAL scores with syntactic constituents and then extract n-grams of constituents from all sentences.
We also use the polarity of all syntactic constituents within the sentence as features.
Our results show significant improvement over a majority class baseline as well as a more difficult baseline consisting of lexical n-grams.
<newSection> 1 Introduction Sentiment analysis is a much-researched area that deals with identification of positive, negative and neutral opinions in text.
The task has evolved from document level analysis to sentence and phrasal level analysis.
Whereas the former is suitable for classifying news (e.g., editorials vs. reports) into positive and negative, the latter is essential for question-answering and recommendation systems.
A recommendation system, for example, must be able to recommend restaurants (or movies, books, etc.) based on a variety of features such as food, service or ambience.
Any single review sentence may contain both positive and negative opinions, evaluating different features of a restaurant.
Consider the following sentence (1) where the writer expresses opposing sentiments towards food and service of a restaurant.
In tasks such as this, therefore, it is important that sentiment analysis be done at the phrase level.
Subjective phrases in a sentence are carriers of sentiments in which an experiencer expresses an attitude, often towards a target.
These subjective phrases may express neutral or polar attitudes depending on the context of the sentence in which they appear.
Context is mainly determined by content and structure of the sentence.
For example, in the following sentence (2), the underlined subjective phrase seems to be negative, but in the larger context of the sentence, it is positive.1 (2) The robber entered the store but his efforts were crushed when the police arrived on time.
Our task is to predict contextual polarity of subjective phrases in a sentence.
A traditional approach to this problem is to use a prior polarity lexicon of words to first set priors on target phrases and then make use of the syntactic and semantic information in and around the sentence to make the final prediction.
As in earlier approaches, we also use a lexicon to set priors, but we explore new uses of a Dictionary of Affect in Language (DAL) (Whissel, 1989) extended using WordNet (Fellbaum, 1998).
We augment this approach with n-gram analysis to capture the effect of context.
We present a system for classification of neutral versus positive versus negative and positive versus negative polarity (as is also done by (Wilson et al., 2005)).
Our approach is novel in the use of following features: et al., 2005).
It contains numeric scores assigned along axes of pleasantness, activeness and concreteness.
We introduce a method for setting numerical priors on words using these three axes, which we refer to as a “scoring scheme” throughout the paper.
This scheme has high coverage of the phrases for classification and requires no manual intervention when tagging words with prior polarities.
We show that classification of subjective phrases using our approach yields better accuracy than two baselines, a majority class baseline and a more difficult baseline of lexical n-gram features.
We also provide an analysis of how the different component DAL scores contribute to our results through the introduction of a “norm” that combines the component scores, separating polar words that are less subjective (e.g., Christmas , murder) from neutral words that are more subjective (e.g., most, lack).
Section 2 presents an overview of previous work, focusing on phrasal level sentiment analysis.
Section 3 describes the corpus and the gold standard we used for our experiments.
In section 4, we give a brief description of DAL, discussing its utility and previous uses for emotion and for sentiment analysis.
Section 5 presents, in detail, our polarity classification framework.
Here we describe our scoring scheme and the features we extract from sentences for classification tasks.
Experimental set-up and results are presented in Section 6.
We conclude with Section 7 where we also look at future directions for this research.
<newSection> 2 Literature Survey The task of sentiment analysis has evolved from document level analysis (e.g., (Turney., 2002); (Pang and Lee, 2004)) to sentence level analysis (e.g., (Hu and Liu., 2004); (Kim and Hovy., 2004); (Yu and Hatzivassiloglou, 2003)).
These researchers first set priors on words using a prior polarity lexicon.
When classifying sentiment at the sentence level, other types of clues are also used, including averaging of word polarities or models for learning sentence sentiment.
Research on contextual phrasal level sentiment analysis was pioneered by Nasukawa and Yi (2003), who used manually developed patterns to identify sentiment.
Their approach had high precision, but low recall.
Wilson et al., (2005) also explore contextual phrasal level sentiment analysis, using a machine learning approach that is closer to the one we present.
Both of these researchers also follow the traditional approach and first set priors on words using a prior polarity lexicon.
Wilson et al.
(2005) use a lexicon of over 8000 subjectivity clues, gathered from three sources ((Riloff and Wiebe, 2003); (Hatzivassiloglou and McKe-own, 1997) and The General Inquirer2).
Words that were not tagged as positive or negative were manually labeled.
Yi et al.
(2003) acquired words from GI, DAL and WordNet.
From DAL, only words whose pleasantness score is one standard deviation away from the mean were used.
Na-sukawa as well as other researchers (Kamps and Marx, 2002)) also manually tag words with prior polarities.
All of these researchers use categorical tags for prior lexical polarity; in contrast, we use quantitative scores, making it possible to use them in computation of scores for the full phrase.
While Wilson et al.
(2005) aim at phrasal level analysis, their system actually only gives “each clue instance its own label” [p. 350].
Their gold standard is also at the clue level and assigns a value based on the clue’s appearance in different expressions (e.g., if a clue appears in a mixture of negative and neutral expressions, its class is negative).
They note that they do not determine subjective expression boundaries and for this reason, they classify at the word level.
This approach is quite different from ours, as we compute the polarity of the full phrase.
The average length of the subjective phrases in the corpus was 2.7 words, with a standard deviation of 2.3.
Like Wilson et al.
(2005) we do not attempt to determine the boundary of subjective expressions; we use the labeled boundaries in the corpus.
<newSection> 3 Corpus We used the Multi-Perspective Question-Answering (MPQA version 1.2) Opinion corpus (Wiebe et al., 2005) for our experiments.
We extracted a total of 17,243 subjective phrases annotated for contextual polarity from the corpus of 535 documents (11,114 sentences).
These subjective phrases are either “direct subjective” or “expressive subjective”.
“Direct subjective” expressions are explicit mentions of a private state (Quirk et al., 1985) and are much easier to classify.
”Expressive subjective” phrases are indirect or implicit mentions of private states and therefore are harder to classify.
Approximately one third of the phrases we extracted were direct subjective with non-neutral expressive intensity whereas the rest of the phrases were expressive subjective.
In terms of polarity, there were 2779 positive, 6471 negative and 7993 neutral expressions.
Our Gold Standard is the manual annotation tag given to phrases in the corpus.
<newSection> 4 DAL DAL is an English language dictionary built to measure emotional meaning of texts.
The samples employed to build the dictionary were gathered from different sources such as interviews, adolescents’ descriptions of their emotions and university students’ essays.
Thus, the 8742 word dictionary is broad and avoids bias from any one particular source.
Each word is given three kinds of scores (pleasantness – also called evaluation, ee, activeness, aa and imagery, ii) on a scale of 1 (low) to 3 (high).
Pleasantness is a measure of polarity.
For example, in Table 1, affection is given a pleasantness score of 2.77 which is closer to 3.0 and is thus a highly positive word.
Likewise, activeness is a measure of the activation or arousal level of a word, which is apparent from the activeness scores of slug and energetic in the table.
The third score, imagery, is a measure of the ease with which a word forms a mental picture.
For example, affect cannot be imagined easily and therefore has a score closer to 1, as opposed to flower which is a very concrete and therefore has an imagery score of 3.
A notable feature of the dictionary is that it has different scores for various inflectional forms of a word ( affect and affection) and thus, morphologi-cal parsing, and the possibility of resulting errors, is avoided.
Moreover, Cowie et al., (2001) showed that the three scores are uncorrelated; this implies that each of the three scores provide complemen-tary information.
The dictionary has previously been used for detecting deceptive speech (Hirschberg et al., 2005) and recognizing emotion in speech (Athanaselis et al., 2006).
<newSection> 5 The Polarity Classification Framework In this section, we present our polarity classification framework.
The system takes a sentence marked with a subjective phrase and identifies the most likely contextual polarity of this phrase.
We use a logistic regression classifier, implemented in Weka, to perform two types of classification: Three way (positive, negative, vs. neutral) and binary (positive vs. negative).
The features we use for classification can be broadly divided into three categories: I.
Prior polarity features computed from DAL and augmented using WordNet (Section 5.1).
II.
lexical features including POS and word n-gram features (Section 5.3), and III.
the combination of DAL scores and syntactic features to allow both n-gram analysis and polarity features of neighbors (Section 5.4).
DAL is used to assign three prior polarity scores to each word in a sentence.
If a word is found in DAL, scores of pleasantness (ee), activeness (aa), and imagery (ii) are assigned to it.
Otherwise, a list of the word’s synonyms and antonyms is created using WordNet.
This list is sequentially traversed until a match is found in DAL or the list ends, in which case no scores are assigned.
For example, astounded, a word absent in DAL, was scored by using its synonym amazed.
Similarly, in-humane was scored using the reverse polarity of its antonym humane, present in DAL.
These scores are Z-Normalized using the mean and standard deviation measures given in the dictionary’s manual (Whissel, 1989).
It should be noted that in our current implementation all function words are given zero scores since they typically do not demonstrate any polarity.
The next step is to boost these normalized scores depending on how far they lie from the mean.
The reason for doing this is to be able to differentiate between phrases like “fairly decent advice” and “excellent advice”.
Without boosting, the pleasantness scores of both phrases are almost the same.
To boost the score, we multiply it by the number of standard deviations it lies from the mean.
After the assignment of scores to individual words, we handle local negations in a sentence by using a simple finite state machine with two states: RETAIN and INVERT.
In the INVERT state, the sign of the pleasantness score of the current word is inverted, while in the RETAIN state the sign of the score stays the same.
Initially, the first word in a given sentence is fed to the RETAIN state.
When a negation (e.g., not, no, never, cannot, didn’t) is encountered, the state changes to the INVERT state.
While in the INVERT state, if ‘but’ is encountered, it switches back to the RETAIN state.
In this machine we also take care of “not only” which serves as an intensifier rather than negation (Wilson et al., 2005).
To handle phrases like “no better than evil” and “could not be clearer”, we also switch states from INVERT to RETAIN when a comparative degree adjective is found after ‘not’.
For example, the words in phrase in Table (2) are given positive pleasantness scores labeled with positive prior polarity.
We observed that roughly 74% of the content words in the corpus were directly found in DAL.
Synonyms of around 22% of the words in the corpus were found to exist in DAL.
Antonyms of only 1% of the words in the corpus were found in DAL.
Our system failed to find prior semantic orientations of roughly 3% of the total words in the corpus.
These were rarely occurring words like apartheid, apocalyptic and ulterior.
We assigned zero scores for these words.
In our system, we assign three DAL scores, using the above scheme, for the subjective phrase in a given sentence.
The features are (1) µ, the mean of the pleasantness scores of the words in the phrase, (2) µaa, the mean of the activeness scores of the words in the phrase, and similarly (3) µZZ, the mean of the imagery scores.
We gave each phrase another score, which we call the norm, that is a combination of the three scores from DAL.
Cowie et al.
(2001) suggest a mechanism of mapping emotional states to a 2-D continuous space using an Activation-Evaluation space (AE) representation.
This representation makes use of the pleasantness and activeness scores from DAL and divides the space into four quadrants: “delightful”, “angry”, “serene”, and “depressed”.
Whissel (2008), observes that tragedies, which are easily imaginable in general, have higher imagery scores than comedies.
Drawing on these approaches and our intuition that neutral expressions tend to be more subjective, we define the norm in the following equation (1).
Words of interest to us may fall into the following four broad categories: It is important to differentiate between these categories of words, because highly subjective words may change orientation depending on context; less subjective words tend to retain their prior orientation.
For instance, in the example sentence from Wilson et al.(2005)., the underlined phrase seems negative, but in the context it is positive.
Since a subjective word like succeed depends on “what” one succeeds in, it may change its polarity accordingly.
In contrast, less subjective words, like angel, do not depend on the context in which they are used; they evoke the same connotation as their prior polarity.
(3) They haven’t succeeded and will never succeed in breaking the will of this valiant people.
As another example, AE space scores of goodies and good turn out to be the same.
What differ-entiates one from the another is the imagery score, which is higher for the former.
Therefore, value of the norm is lower for goodies than for good.
Un-surprisingly, this feature always appears in the top 10 features when the classification task contains neutral expressions as one of the classes.
We extract two types of lexical features, part of speech (POS) tags and n-gram word features.
We count the number of occurrences of each POS in the subjective phrase and represent each POS as an integer in our feature vector.3 For each subjective phrase, we also extract a subset of unigram, bigrams, and trigrams of words (selected automatically, see Section 6).
We represent each n-gram feature as a binary feature.
These types of features were used to approximate standard n-gram language modeling (LM).
In fact, we did experiment with a standard trigram LM, but found that it did not improve performance.
In particular, we trained two LMs, one on the polar subjective phrases and another on the neutral subjective phrases.
Given a sentence, we computed two perplexities of the two LMs on the subjective phrase in the sentence and added them as features in our feature vectors.
This procedure provided us with significant improvement over a chance baseline but did not outperform our current system.
We speculate that this was caused by the split of training data into two parts, one for training the LMs and another for training the classifier.
The resulting small quantity of training data may be the reason for bad performance.
Therefore, we decided to back off to only binary n-gram features as part of our feature vector.
3We use the Stanford Tagger to assign parts of speech tags to sentences.
(Toutanova and Manning, 2000) In this section, we show how we can combine the DAL scores with syntactic constituents.
This process involves two steps.
First, we chunk each sentence to its syntactic constituents (NP, VP, PP, JJP, and Other) using a CRF Chunker.4 If the marked-up subjective phrase does not contain complete chunks (i.e., it partially overlaps with other chunks), we expand the subjective phrase to include the chunks that it overlaps with.
We term this expanded phrase as the target phrase, see Figure 1.
Second, each chunk in a sentence is then assigned a 2-D AE space score as defined by Cowie et al., (2001) by adding the individual AE space scores of all the words in the chunk and then normalizing it by the number of words.
At this point, we are only concerned with the polarity of the chunk (i.e., whether it is positive or negative or neutral) and imagery will not help in this task; the AE space score is determined from pleasantness and activeness alone.
A threshold, determined empirically by analyzing the distributions of positive (pos), negative (neg) and neutral (neu) expressions, is used to define ranges for these classes of expressions.
This enables us to assign each chunk a prior semantic polarity.
Having the semantic orientation (positive, negative, neutral) and phrasal tags, the sentence is then converted to a sequence of encodings [Phrasal − Tag]polarity.
We mark each phrase that we want to classify as a “target” to differentiate it from the other chunks and attach its encoding.
As mentioned, if the target phrase partially overlaps with chunks, it is simply expanded to subsume the chunks.
This encoding is illustrated in Figure 1.
After these two steps, we extract a set of features that are used in classifying the target phrase.
These include n-grams of chunks from the all sentences, minimum and maximum pleasantness scores from the chunks in the target phrase itself, and the syntactic categories that occur in the context of the target phrase.
In the remainder of this section, we describe how these features are extracted.
We extract unigrams, bigrams and trigrams of chunks from all the sentences.
For example, we may extract a bigram from Figure 1 of [V P]neu followed by [PP]target neg . Similar to the lexical n-grams, for the sentence containing the target phrase, we add binary values in our feature vector such that the value is 1 if the sentence contains that chunk n-gram.
We also include two features related to the target phrase.
The target phrase often consists of many chunks.
To detect if a chunk of the target phrase is highly polar, minimum and maximum pleasantness scores over all the chunks in the target phrase are noted.
In addition, we add features which attempt to capture contextual information using the prior semantic polarity assigned to each chunk both within the target phrase itself and within the context of the target phrase.
In cases where the target phrase is in the beginning of the sentence or at the end, we simply assign zero scores.
Then we compute the frequency of each syntactic type (i.e., NP, VP, PP, JJP) and polarity (i.e., positive, negative, neutral) to the left of the target, to the right of the target and for the target.
This additional set of contextual features yields 36 features in total: three polarities: {positive, negative, neutral} * three contexts: {left, target, right} * four chunk syntactic types: {NP, VP, PP, JJP}.
The full set of features captures different types of information.
N-grams look for certain patterns that may be specific to either polar or neutral sentiments.
Minimum and maximum scores capture information about the target phrase standalone.
The last set of features incorporate information about the neighbors of the target phrase.
We performed feature selection on this full set of n-gram related features and thus, a small subset of these n-gram related features, selected automatically (see section 6) were used in the experiments.
<newSection> 6 Experiments and Results Subjective phrases from the MPQA corpus were used in 10-fold cross-validation experiments.
The MPQA corpus includes gold standard tags for each phrase.
A logistic classifier was used for two polarity classification tasks, positive versus negative versus neutral and positive versus negative.
We report accuracy, and F-measure for both balanced and unbalanced data.
Table 3 shows results for a 3-way classifier.
For the balanced data-set, each class has 2799 instances and hence the chance baseline is 33%.
For the unbalanced data-set, there are 2799 instances of positive, 6471 instances of negative and 7993 instances of neutral phrases and thus the baseline is about 46%.
Results show that the accuracy increases as more features are added.
It may be seen from the table that prior polarity scores do not do well alone, but when used in conjunction with other features they play an important role in achieving an accuracy much higher than both baselines (chance and lexical n-grams).
To reconfirm if prior polarity scores add value, we experimented by using all features except the prior polarity scores and noticed a drop in accuracy by about 4%.
This was found to be true for the other classification task as well.
The table shows that parts of speech and lexical n-grams are good features.
A significant improvement in accuracy (over 4%, p-value = 4.2e-15) is observed when chunk features (i.e., n-grams of constituents and polarity of neighboring constituents) are used in conjunction with prior polarity scores and part of speech features.5 This improvement may be explained by the following observation.
The bi-gram “[Other]target [NP]neu” was selected as a neu top feature by the Chi-square feature selector.
So were unigrams, [Other]target neu and [Other]target neg . We thus learned n-gram patterns that are char-acteristic of neutral expressions (the just mentioned bigram and the first of the unigrams) as well as a pattern found mostly in negative expressions (the latter unigram).
It was surprising to find another top chunk feature, the bigram “[Other]target neu [NP]neg” (i.e., a neutral chunk of syntactic type “Other” preceding a negative noun phrase), present in neutral expressions six times more than in polar expressions.
An instance where these chunk features could have been responsible for the correct prediction of a target phrase is shown in Figure 2.
Figure 2(a) shows an example sentence from the MPQA corpus, which has three annotated subjective phrases.
The manually labeled polarity of phrases (A) and (C) is negative and that of (B) is neutral.
Figure 2(b) shows the 5We use the binomial test procedure to test statistical significance throughout the paper.
relevant chunk bigram which is used to predict the contextual polarity of the target phrase (B).
It was interesting to see that the top 10 features consisted of all categories (i.e., prior DAL scores, lexical n-grams and POS, and syntactic) of features.
In this and the other experiment, pleasantness, activation and the norm were among the top 5 features.
We ran a significance test to show the importance of the norm feature in our classification task and observed that it exerted a significant increase in accuracy (2.26%, p-value = 1.45e-5).
Table 4 shows results for positive versus negative classification.
We show results for both balanced and unbalanced data-sets.
For balanced, there are 2779 instances of each class.
For the unbalanced data-set, there are 2779 instances of positive and 6471 instances of neutral, thus our chance baseline is around 70%.
As in the earlier classification, accuracy and F-measure increase as we add features.
While the increase of adding the chunk features, for example, is not as great as in the previous classification, it is nonetheless significant (p-value = 0.0018) in this classification task.
The smaller increase lends support to our hypothesis that polar expressions tend to be less subjective and thus are less likely to be affected by contextual polarity.
Another thing that supports our hypothesis that neutral expressions are more subjective is the fact that the rank of imagery (H), dropped significantly in this classification task as compared to the previous classification task.
This implies that imagery has a much lesser role to play when we are dealing with non-neutral expressions.
<newSection> 7 Conclusion and Future Work We present new features (DAL scores, norm scores computed using DAL, n-gram over chunks with polarity) for phrasal level sentiment analysis.
They work well and help in achieving high accuracy in a three-way classification of positive, negative and neutral expressions.
We do not require any manual intervention during feature selection, and thus our system is fully automated.
We also introduced a 3-D representation that maps different classes to spatial coordinates.
It may seem to be a limitation of our system that it requires accurate expression boundaries.
However, this is not true for the following two reasons: first, Wiebe et al., (2005) declare that while marking the span of subjective expressions and hand annotating the MPQA corpus, the annotators were not trained to mark accurate expression boundaries.
The only constraint was that the subjective expression should be within the mark-ups for all annotators.
Second, we expanded the marked subjective phrase to subsume neighboring phrases at the time of chunking.
A limitation of our scoring scheme is that it does not handle polysemy, since words in DAL are not provided with their parts of speech.
Statistics show, however, that most words occurred with primarily one part of speech only.
For example, “will” occurred as modal 1272 times in the corpus, whereas it appeared 34 times as a noun.
The case is similar for “like” and “just”, which mostly occur as a preposition and an adverb, respectively.
Also, in our state machine, we haven’t accounted for the impact of connectives such as “but” or “although”; we propose drawing on work in argumentative orientation to do so ((Anscombre and Ducrot, 1983); (Elhadad and McKeown, 1990)).
For future work, it would be interesting to do subjectivity and intensity classification using the same scheme and features.
Particularly, for the task of subjectivity analysis, we speculate that the imagery score might be useful for tagging chunks with “subjective” and “objective” instead of positive, negative, and neutral.
<newSection> Acknowledgments This work was supported by the National Science Foundation under the KDD program.
Any opinions, ndings, and conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reect the views of the National Science Foundation. score.
We would like to thank Julia Hirschberg for useful discussion.
We would also like to acknowledge Narayanan Venkiteswaran for implementing parts of the system and Amal El Masri, Ashleigh White and Oliver Elliot for their useful comments.
<newSection> References<newSection> Abstract Sentence fluency is an important component of overall text readability but few studies in natural language processing have sought to understand the factors that define it.
We report the results of an initial study into the predictive power of surface syntactic statistics for the task; we use fluency assessments done for the purpose of evaluating machine translation.
We find that these features are weakly but significantly correlated with fluency.
Machine and human translations can be dis-tinguished with accuracy over 80%.
The performance of pairwise comparison of fluency is also very high—over 90% for a multi-layer perceptron classifier.
We also test the hypothesis that the learned models capture general fluency properties applicable to human-written text.
The results do not support this hypothesis: prediction accuracy on the new data is only 57%.
This finding suggests that developing a dedicated, task-independent corpus of fluency judgments will be beneficial for further in-vestigations of the problem.
<newSection> 1 Introduction Numerous natural language applications involve the task of producing fluent text.
This is a core problem for surface realization in natural language generation (Langkilde and Knight, 1998; Banga-lore and Rambow, 2000), as well as an important step in machine translation.
Considerations of sentence fluency are also key in sentence sim-plification (Siddharthan, 2003), sentence compression (Jing, 2000; Knight and Marcu, 2002; Clarke and Lapata, 2006; McDonald, 2006; Turner and Charniak, 2005; Galley and McKeown, 2007), text re-generation for summarization (Daum´e III and Marcu, 2004; Barzilay and McKeown, 2005; Wan et al., 2005) and headline generation (Banko et al., 2000; Zajic et al., 2007; Soricut and Marcu, 2007).
Despite its importance for these popular applications, the factors contributing to sentence level fluency have not been researched indepth.
Much more attention has been devoted to discourse-level constraints on adjacent sentences indicative of coherence and good text flow (Lapata, 2003; Barzi-lay and Lapata, 2008; Karamanis et al., to appear).
In many applications fluency is assessed in combination with other qualities.
For example, in machine translation evaluation, approaches such as BLEU (Papineni et al., 2002) use n-gram overlap comparisons with a model to judge overall “goodness”, with higher n-grams meant to capture fluency considerations.
More sophisticated ways to compare a system production and a model involve the use of syntax, but even in these cases fluency is only indirectly assessed and the main advantage of the use of syntax is better estimation of the semantic overlap between a model and an output.
Similarly, the metrics proposed for text generation by (Bangalore et al., 2000) (simple accuracy, generation accuracy) are based on string-edit distance from an ideal output.
In contrast, the work of (Wan et al., 2005) and (Mutton et al., 2007) directly sets as a goal the assessment of sentence-level fluency, regardless of content.
In (Wan et al., 2005) the main premise is that syntactic information from a parser can more robustly capture fluency than language models, giving more direct indications of the degree of ungrammaticality.
The idea is extended in (Mutton et al., 2007), where four parsers are used and artificially generated sentences with varying level of fluency are evaluated with impressive success.
The fluency models hold promise for actual improvements in machine translation output quality (Zwarts and Dras, 2008).
In that work, only simple parser features are used for the prediction of fluency, but no actual syntactic properties of the sentences.
But certainly, problems with sentence fluency are expected to be manifested in syntax.
We would expect for example that syntactic tree features that capture common parse configurations and that are used in discrim-inative parsing (Collins and Koo, 2005; Charniak and Johnson, 2005; Huang, 2008) should be useful for predicting sentence fluency as well.
Indeed, early work has demonstrated that syntactic features, and branching properties in particular, are helpful features for automatically distinguish-ing human translations from machine translations (Corston-Oliver et al., 2001).
The exploration of branching properties of human and machine translations was motivated by the observations during failure analysis that MT system output tends to favor right-branching structures over noun compounding.
Branching preference mismatch manifest themselves in the English output when translating from languages whose branching properties are radically different from English.
Accuracy close to 80% was achieved for distinguishing human translations from machine translations.
In our work we continue the investigation of sentence level fluency based on features that capture surface statistics of the syntactic structure in a sentence.
We revisit the task of distinguishing machine translations from human translations, but also further our understanding of fluency by providing comprehensive analysis of the association between fluency assessments of translations and surface syntactic features.
We also demonstrate that based on the same class of features, it is possible to distinguish fluent machine translations from disfluent machine translations.
Finally, we test the models on human written text in order to verify if the classifiers trained on data coming from machine translation evaluations can be used for general predictions of fluency and readability.
For our experiments we use the evaluations of Chinese to English translations distributed by LDC (catalog number LDC2003T17), for which both machine and human translations are available.
Machine translations have been assessed by evaluators for fluency on a five point scale (5: flawless English; 4: good English; 3: non-native English; 2: disfluent English; 1: incomprehen-sible).
Assessments by different annotators were averaged to assign overall fluency assessment for each machine-translated sentence.
For each segment (sentence), there are four human and three machine translations.
In this setting we address four tasks with increasing difficulty: Even for the last most challenging task results are promising, with prediction accuracy almost 10% better than a random baseline.
For the other tasks accuracies are high, exceeding 80%.
It is important to note that the purpose of our study is not evaluation of machine translation per se.
Our goal is more general and the interest is in finding predictors of sentence fluency.
No general corpora exist with fluency assessments, so it seems advantageous to use the assessments done in the context of machine translation for preliminary in-vestigations of fluency.
Nevertheless, our findings are also potentially beneficial for sentence-level evaluation of machine translation.
<newSection> 2 Features Perceived sentence fluency is influenced by many factors.
The way the sentence fits in the context of surrounding sentences is one obvious factor (Barzilay and Lapata, 2008).
Another well-known factor is vocabulary use: the presence of uncommon difficult words are known to pose problems to readers and to render text less readable (Collins-Thompson and Callan, 2004; Schwarm and Ostendorf, 2005).
But these discourse- and vocabulary-level features measure properties at granularities different from the sentence level.
Syntactic sentence level features have not been investigated as a stand-alone class, as has been done for the other types of features.
This is why we constrain our study to syntactic features alone, and do not discuss discourse and language model features that have been extensively studied in prior work on coherence and readability.
In our work, instead of looking at the syntactic structures present in the sentences, e.g. the syntactic rules used, we use surface statistics of phrase length and types of modification.
The sentences were parsed with Charniak’s parser (Char-niak, 2000) in order to calculate these features.
Sentence length is the number of words in a sentence.
Evaluation metrics such as BLEU (Papineni et al., 2002) have a built-in preference for shorter translations.
In general one would expect that shorter sentences are easier to read and thus are perceived as more fluent.
We added this feature in order to test directly the hypothesis for brevity preference.
Parse tree depth is considered to be a measure of sentence complexity.
Generally, longer sentences are syntactically more complex but when sentences are approximately the same length the larger parse tree depth can be indicative of increased complexity that can slow processing and lead to lower perceived fluency of the sentence.
Number offragment tags in the sentence parse Out of the 2634 total sentences, only 165 contained a fragment tag in their parse, indicating the presence of ungrammaticality in the sentence.
Fragments occur in headlines (e.g. “Cheney willing to hold bilateral talks if Arafat observes U.S. cease-fire arrangement”) but in machine translation the presence of fragments can signal a more serious problem.
Phrase type proportion was computed for prepositional phrases (PP), noun phrases (NP) and verb phrases (VP).
The length in number of words of each phrase type was counted, then divided by the sentence length.
Embedded phrases were also included in the calculation: for example a noun phrase (NP1 ...
(NP2)) would contribute length(NP1) + length(NP2) to the phrase length count.
Average phrase length is the number of words comprising a given type of phrase, divided by the number of phrases of this type.
It was computed for PP, NP, VP, ADJP, ADVP.
Two versions of the features were computed—one with embedded phrases included in the calculation and one just for the largest phrases of a given type.
Normalized average phrase length is computed for PP, NP and VP and is equal to the average phrase length of given type divided by the sentence length.
These were computed only for the largest phrases.
Phrase type rate was also computed for PPs, VPs and NPs and is equal to the number of phrases of the given type that appeared in the sentence, divided by the sentence length.
For example, the sentence “The boy caught a huge fish this morning” will have NP phrase number equal to 3/8 and VP phrase number equal to 1/8.
Phrase length The number of words in a PP, NP, VP, without any normalization; it is computed only for the largest phrases.
Normalized phrase length is the average phrase length (for VPs, NPs, PPs) divided by the sentence length.
This was computed both for longest phrase (where embedded phrases of the same type were counted only once) and for each phrase regardless of embedding.
Length of NPs/PPs contained in a VP The average number of words that constitute an NP or PP within a verb phrase, divided by the length of the verb phrase.
Similarly, the length of PP in NP was computed.
Head noun modifiers Noun phrases can be very complex, and the head noun can be modified in variety of ways—pre-modifiers, prepositional phrase modifiers, apposition.
The length in words of these modifiers was calculated.
Each feature also had a variant in which the modifier length was divided by the sentence length.
Finally, two more features on total modification were computed: one was the sum of all modifier lengths, the other the sum of normalized modifier length.
<newSection> 3 Feature analysis In this section, we analyze the association of the features that we described above and fluency.
Note that the purpose of the analysis is not feature selection—all features will be used in the later experiments.
Rather, the analysis is performed in order to better understand which factors are predictive of good fluency.
The distribution of fluency scores in the dataset is rather skewed, with the majority of the sentences rated as being of average fluency 3 as can be seen in Table 1.
Pearson’s correlation between the fluency ratings and features are shown in Table 2.
First of all, fluency and adequacy as given by MT evaluators are highly correlated (0.7).
This is surprisingly high, given that separate fluency and adequacy assessments were elicited with the idea that these are qualities of the translations that are independent of each other.
Fluency was judged directly by the assessors, while adequacy was meant to assess the content of the sentence compared to a human gold-standard.
Yet, the assessments of the two aspects were often the same—readability/fluency of the sentence is important for understanding the sentence.
Only after the assessor has understood the sentence can (s)he judge how it compares to the human model.
One can conclude then that a model of fluency/readability that will allow systems to produce fluent text is key for developing a successful machine translation system.
The next feature most strongly associated with fluency is sentence length.
Shorter sentences are easier and perceived as more fluent than longer ones, which is not surprising.
Note though that the correlation is actually rather weak.
It is only one of various fluency factors and has to be accommodated alongside the possibly conflicting requirements shown by the other features.
Still, length considerations reappear at sub-sentential (phrasal) levels as well.
Noun phrase length for example has almost the same correlation with fluency as sentence length does.
The longer the noun phrases, the less fluent the sentence is.
Long noun phrases take longer to interpret and reduce sentence fluency/readability.
Consider the following example: The long noun phrase is more difficult to read, especially in subject position.
Similarly the length of the verb phrases signal potential fluency problems: VP distance (the average number of words separating two verb phrases) is also negatively correlated with sentence fluency.
In machine translations there is the obvious problem that they might not include a verb for long stretches of text.
But even in human written text, the presence of more verbs can make a difference in fluency (Bailin and Grafstein, 2001).
Consider the following two sentences: tion team of 25 people was formed to attend to those who came to make donations in person.
The next strongest correlation is with unnormal-ized verb phrase length.
In fact in terms of correlations, in turned out that it was best not to normalize the phrase length features at all.
The normalized versions were also correlated with fluency, but the association was lower than for the direct count without normalization.
Parse tree depth is the final feature correlated with fluency with correlation above 0.1.
<newSection> 4 Experiments with machine translation data In this section we use all the features discussed in Section 2 for several classification tasks.
Note that while we discussed the high correlation between fluency and adequacy, we do not use adequacy in the experiments that we report from here on.
For all experiments we used four of the classifiers in Weka—decision tree (J48), logistic regression, support vector machines (SMO), and multi-layer perceptron.
All results are for 10-fold cross validation.
We extracted the 300 sentences with highest fluency scores, 300 sentences with lowest fluency scores among machine translations and 300 randomly chosen human translations.
We then tried the classification task of distinguishing human and machine translations with different fluency quality (highest fluency scores vs. lowest fluency score).
We expect that low fluency MT will be more easily distinguished from human translation in comparison with machine translations rated as having high fluency.
Results are shown in Table 3.
Overall the best classifier is the multi-layer perceptron.
On the task using all available data of machine and human translations, the classification accuracy is 86.99%.
We expected that distinguishing the machine translations from the human ones will be harder when the best translations are used, compared to the worse translations, but this expectation is fulfilled only for the support vector machine classifier.
The results in Table 3 give convincing evidence that the surface structural statistics can distinguish very well between fluent and non-fluent sentences when the examples come from human and machine-produced text respectively.
If this is the case, will it be possible to distinguish between good and bad machine translations as well?
In order to answer this question, we ran one more binary classification task.
The two classes were the 300 machine translations with highest and lowest fluency respectively.
The results are not as good as those for distinguishing machine and human translation, but still significantly outperform a random baseline.
All classifiers performed similarly on the task, and achieved accuracy close to 61%.
We also considered the possibility of pairwise comparisons for fluency: given two sentences, can we distinguish which is the one scored more highly for fluency.
For every two sentences, the feature for the pair is the difference of features of the individual sentences.
There are two ways this task can be set up.
First, we can use all assessed translations and make pairings for every two sentences with different fluency assessment.
In this setting, the question being addressed is Can sentences with differing fluency be distinguished?, without regard to the sources of the sentence.
The harder question is Can a more fluent translation be distinguished from a less fluent translation of the same sentence?
The results from these experiments can be seen in Table 4.
When any two sentences with different fluency assessments are paired, the prediction accuracy is very high: 91.34% for the multi-layer perceptron classifier.
In fact all classifiers have accuracy higher than 80% for this task.
The surface statistics of syntactic form are powerful enough to distinguishing sentences of varying fluency.
The task of pairwise comparison for translations of the same input is more difficult: doing well on this task would be equivalent to having a reliable measure for ranking different possible translation variants.
In fact, the problem is much more difficult as between different translations of the same sentences, “any pair” contains comparisons of sentences with different fluency over the entire data set.
can be seen in the second row of Table 4.
Logistic regression, support vector machines and multi-layer perceptron perform similarly, with support vector machine giving the best accuracy of 71.23%.
This number is impressively high, and significantly higher than baseline performance.
The results are about 20% lower than for prediction of a more fluent sentence when the task is not constrained to translation of the same sentence.
In the previous sections we presented three variations involving fluency predictions based on syntactic phrasing features: distinguishing human from machine translations, distinguishing good machine translations from bad machine translations, and pairwise ranking of sentences with different fluency.
The results differ considerably and it is interesting to know whether the same kind of features are useful in making the three distinctions.
In Table 5 we show the five features with largest weight in the support vector machine model for each task.
In many cases, certain features appear to be important only for particular tasks.
For example the number of prepositional phrases is an important feature only for ranking different versions of the same sentence but is not important for other distinctions.
The number of appositions is helpful in distinguishing human translations from machine translations, but is not that useful in the other tasks.
So the predictive power of the features is very directly related to the variant of fluency distinctions one is interested in making.
<newSection> 5 Applications to human written text The goal we set out in the beginning of this paper was to derive a predictive model of sentence fluency from data coming from MT evaluations.
In the previous sections, we demonstrated that indeed structural features can enable us to perform this task very accurately in the context of machine translation.
But will the models conveniently trained on data from MT evaluation be at all capable to identify sentences in human-written text that are not fluent and are difficult to understand?
To answer this question, we performed an additional experiment on 30 Wall Street Journal articles from the Penn Treebank that were previously used in experiments for assessing overall text quality (Pitler and Nenkova, 2008).
The articles were chosen at random and comprised a total of 290 sentences.
One human assessor was asked to read each sentence and mark the ones that seemed disfluent because they were hard to comprehend.
These were sentences that needed to be read more than once in order to fully understand the information conveyed in them.
There were 52 such sentences.
The assessments served as a gold-standard against which the predictions of the fluency models were compared.
Two models trained on machine translation data were used to predict the status of each sentence in the WSJ articles.
One of the models was that for distinguishing human translations from machine translations (human vs machine MT), the other was the model for distinguishing the 300 best from the 300 worst machine translations (good vs bad MT).
The classifiers used were decision trees for human vs machine distinction and support vector machines for good vs bad MT.
For the first model sentences predicted to belong to the “human translation” class are considered fluent; for the second model fluent sentences are the ones predicted to be in the “best MT” class.
The results are shown in Table 6.
The two models vastly differ in performance.
The model for distinguishing machine translations from human translations is the better one, with accuracy of 57%.
For both, prediction accuracy is much lower than when tested on data from MT evaluations.
These findings indicate that building a new class) for each model when test on WSJ sentences.
The gold-standard is assessment by a single reader of the text.
corpus for the finer fluency distinctions present in human-written text is likely to be more beneficial than trying to leverage data from existing MT evaluations.
Below, we show several example sentences on which the assessor and the model for distinguish-ing human and machine translations (dis)agreed.
Model and assessor agree that sentence is problematic: (1.1) The Soviet legislature approved a 1990 budget yesterday that halves its huge deficit with cuts in defense spending and capital outlays while striving to improve supplies to frustrated consumers.
(1.2) Officials proposed a cut in the defense budget this year to 70.9 billion rubles (US$114.3 billion) from 77.3 billion rubles (US$125 billion) as well as large cuts in outlays for new factories and equipment.
(1.3) Rather, the two closely linked exchanges have been drifting apart for some years, with a nearly five-year-old moratorium on new dual listings, separate and different listing requirements, differing trading and settlement guidelines and diverging national-policy aims.
The model predicts the sentence is good, but the assessor finds it problematic: (2.1) Moody’s Investors Service Inc.
said it lowered the ratings of some $145 million of Pinnacle debt because of ”accelerating deficiency in liquidity,” which it said was evidenced by Pinnacle’s elimination of dividend payments.
(2.2) Sales were higher in all of the company’s business categories, with the biggest growth coming in sales of foodstuffs such as margarine, coffee and frozen food, which rose 6.3%.
(2.3) Ajinomoto predicted sales in the current fiscal year ending next March 31 of 480 billion yen, compared with 460.05 billion yen in fiscal 1989.
The model predicts the sentences are bad, but the assessor considered them fluent: (3.1) The sense grows that modern public bureaucracies simply don’t perform their assigned functions well.
(3.2) Amstrad PLC, a British maker of computer hardware and communications equipment, posted a 52% plunge in pre-tax profit for the latest year.
(3.3) At current allocations, that means EPA will be spending $300 billion on itself.
In our final experiment we focus on the relationship between sentence fluency and overall text quality.
We would expect that the presence of dis-fluent sentences in text will make it appear less well written.
Five annotators had previously assess the overall text quality of each article on a scale from 1 to 5 (Pitler and Nenkova, 2008).
The average of the assessments was taken as a single number describing the article.
The correlation between this number and the percentage of fluent sentences in the article according to the different models is shown in Table 7.
The correlation between the percentage of fluent sentences in the article as given by the human assessor and the overall text quality is rather low, 0.127.
The positive correlation would suggest that the more hard to read sentence appear in a text, the higher the text would be rated overall, which is surprising.
The predictions from the model for distinguishing good and bad machine translations very close to zero, but negative which corresponds better to the intuitive relationship between the two.
Note that none of the correlations are actually significant for the small dataset of 30 points.
<newSection> 6 Conclusion We presented a study of sentence fluency based on data from machine translation evaluations.
These data allow for two types of comparisons: human (fluent) text and (not so good) machine-generated ment of the articles and the percentage of fluent sentences according to different models.
text, and levels of fluency in the automatically produced text.
The distinctions were possible even when based solely on features describing syntactic phrasing in the sentences.
Correlation analysis reveals that the structural features are significant but weakly correlated with fluency.
Interestingly, the features correlated with fluency levels in machine-produced text are not the same as those that distinguish between human and machine translations.
Such results raise the need for caution when using assessments for machine produced text to build a general model of fluency.
The captured phenomena in this case might be different than these from comparing human texts with differing fluency.
For future research it will be beneficial to build a dedicated corpus in which human-produced sentences are assessed for fluency.
Our experiments show that basic fluency distinctions can be made with high accuracy.
Machine translations can be distinguished from human translations with accuracy of 87%; machine translations with low fluency can be distinguished from machine translations with high fluency with accuracy of 61%.
In pairwise comparison of sentences with different fluency, accuracy of predicting which of the two is better is 90%.
Results are not as high but still promising for comparisons in fluency of translations of the same text.
The prediction becomes better when the texts being compared exhibit larger difference in fluency quality.
Admittedly, our pilot experiments with human assessment of text quality and sentence level fluency are small, so no big generalizations can be made.
Still, they allow some useful observations that can guide future work.
They do show that for further research in automatic recognition of fluency, new annotated corpora developed specially for the task will be necessary.
They also give some evidence that sentence-level fluency is only weakly correlated with overall text quality.
Discourse apects and language model features that have been extensively studied in prior work are indeed much more indicative of overall text quality (Pitler and Nenkova, 2008).
We leave direct comparison for future work.
<newSection> References<newSection> Abstract We present a Hebrew to English transliter-ation method in the context of a machine translation system.
Our method uses machine learning to determine which terms are to be transliterated rather than translated.
The training corpus for this purpose includes only positive examples, acquired semi-automatically.
Our classifier reduces more than 38% of the errors made by a baseline method.
The identified terms are then transliterated.
We present an SMT-based transliteration model trained with a parallel corpus extracted from Wikipedia using a fairly simple method which requires minimal knowledge.
The correct result is produced in more than 76% of the cases, and in 92% of the instances it is one of the top-5 results.
We also demonstrate a small improvement in the performance of a Hebrew-to-English MT system that uses our transliteration module.
<newSection> 1 Introduction Transliteration is the process of converting terms written in one language into their approximate spelling or phonetic equivalents in another language.
Transliteration is defined for a pair of languages, a source language and a target language.
The two languages may differ in their script systems and phonetic inventories.
This paper addresses transliteration from Hebrew to English as part of a machine translation system.
Transliteration of terms from Hebrew into English is a hard task, for the most part because of the differences in the phonological and orthographic systems of the two languages.
On the one hand, there are cases where a Hebrew letter can be pronounced in multiple ways.
For example, Hebrew 3 can be pronounced either as [b] or as [v].
On the other hand, two different Hebrew sounds can be mapped into the same English letter.
For example, both n and M are in most cases mapped to [t].
A major difficulty stems from the fact that in the Hebrew orthography (like Arabic), words are represented as sequences of consonants where vowels are only partially and very inconsistently represented.
Even letters that are considered as representing vowels may sometimes represent consonants, specifically 1 [v]/[o]/[u] and' [y]/[i].
As a result, the mapping between Hebrew orthography and phonology is highly ambiguous.
Transliteration has acquired a growing interest recently, particularly in the field of Machine Translation (MT).
It handles those terms where no translation would suffice or even exist.
Failing to recognize such terms would result in poor performance of the translation system.
In the context of an MT system, one has to first identify which terms should be transliterated rather than translated, and then produce a proper transliteration for these terms.
We address both tasks in this work.
Identification of Terms To-be Transliterated (TTT) must not be confused with recognition of Named Entities (NE) (Hermjakob et al., 2008).
On the one hand, many NEs should be translated rather than transliterated, for example:1 m$rd hm$p@im misrad hamishpatim ministry-of the-sentences ‘Ministry of Justice’ 1To facilitate readability, examples are presented with interlinear gloss, including an ASCII representation of Hebrew orthography followed by a broad phonemic transcription, a word-for-word gloss in English where relevant, and the corresponding free text in English.
The following table presents the ASCII encoding of Hebrew used in this paper: N 3 a 'r 17 1 T n M ' n a b g d h w z x @ i k 5 n 1 a v 0 s P -1 m n l m n s & p c q r $ t him htikwn hayam hatichon the-sea the-central ‘the Mediterranean Sea’ On the other hand, there are terms that are not NEs, such as borrowed words or culturally specific terms that are transliterated rather than translated, as shown by the following examples: aqzis@ncializm eqzistentzializm ‘Existentialism’ As these examples show, transliteration cannot be considered the default strategy to handle NEs in MT and translation does not necessarily apply for all other cases.
Candidacy for either transliteration or translation is not necessarily determined by orthographic features.
In contrast to English (and many other languages), proper names in Hebrew are not capitalized.
As a result, the following homographs may be interpreted as either a proper name, a noun, or a verb: One usually distinguishes between two types of transliteration (Knight and Graehl, 1997): Forward transliteration, where an originally Hebrew term is to be transliterated to English; and Backward transliteration, in which a foreign term that has already been transliterated into Hebrew is to be recovered.
Forward transliteration may result in several acceptable alternatives.
This is mainly due to phonetic gaps between the languages and lack of standards for expressing Hebrew phonemes in English.
For example, the Hebrew term cdiq may be transliterated as Tzadik, Tsadik, Tsaddiq, etc.
On the other hand, backward transliteration is restrictive.
There is usually only one acceptable way to express the transliterated term.
So, for example, the name wiliam can be transliterated only to William and not, for example, to Viliem, even though the Hebrew character w may stand for the consonant [v] and the character a may be vow-elized as [e].
We approach the task of transliteration in the context of Machine Translation in two phases.
First, we describe a lightly-supervised classifier that can identify TTTs in the text (section 4).
The identified terms are then transliterated (section 5) using a transliteration model based on Statistical Machine Translation (SMT).
The two modules are combined and integrated in a Hebrew to English MT system (section 6).
The main contribution of this work is the actual transliteration module, which has already been integrated in a Hebrew to English MT system.
The accuracy of the transliteration is comparable with state-of-the-art results for other language pairs, where much more training material is available.
More generally, we believe that the method we describe here can be easily adapted to other language pairs, especially those for which few resources are available.
Specifically, we did not have access to a significant parallel corpus, and most of the resources we used are readily available for many other languages.
<newSection> 2 Previous Work In this section we sketch some related works, focusing on transliteration from Hebrew and Arabic, and on the context of machine translation.
Arbabi et al.
(1994) present a hybrid algorithm for romanization of Arabic names using neural networks and a knowledge based system.
The program applies vowelization rules, based on Arabic morphology and stemming from the knowledge base, to unvowelized names.
This stage, termed the broad approach, exhaustively yields all valid vowelizations of the input.
To solve this over-generation, the narrow approach is then used.
In this approach, the program uses a neural network to filter unreliable names, that is, names whose vowelizations are not in actual use.
The vowelized names are converted into a standard phonetic representation which in turn is used to produce various spellings in languages which use Roman alphabet.
The broad approach covers close to 80% of the names given to it, though with some extraneous vowelization.
The narrow approach covers over 45% of the names presented to it with higher precision than the broad approach.
This approach requires a vast linguistic knowledge in order to create the knowledge base of vow-elization rules.
In addition, these rules are applicable only to names that adhere to the Arabic morphology.
Stalls and Knight (1998) propose a method for back transliteration of names that originate in English and occur in Arabic texts.
The method uses a sequence of probabilistic models to convert names written in Arabic into the English script.
First, an Arabic name is passed through a phonemic model producing a network of possible English sound sequences, where the probability of each sound is location dependent.
Next, phonetic sequences are transformed into English phrases.
Finally, each possible result is scored according to a unigram word model.
This method translates correctly about 32% of the tested names.
Those not translated are frequently not foreign names.
This method uses a pronunciation dictionary and is therefore restricted to transliterating only words of known pronunciation.
Both of the above methods perform only unidirectional translitera-tion, that is, either forward- or backward- translit-eration, while our work handles both.
Al-Onaizan and Knight (2002) describe a system which combines a phonetic based model with a spelling model for transliteration.
The spelling based model directly maps sequences of English letters into sequences of Arabic letters without the need of English pronunciation.
The method uses a translation model based on IBM Model 1 (Brown et al., 1993), in which translation candidates of a phrase are generated by combining translations and transliterations of the phrase components, and matching the result against a large corpus.
The system’s overall accuracy is about 72% for top-1 results and 84% for top-20 results.
This method is restricted to transliterating NEs, and performs best for person names.
As noted above, the TTT problem is not identical to the NER problem.
In addition, the method requires a list of transliteration pairs from which the translit-eration model could be learned.
Yoon et al.
(2007) use phonetic distinctive features and phonology-based pseudo features to learn both language-specific and language-universal transliteration characteristics.
Distinctive features are the characteristics that define the set of phonemic segments (consonants, vowels) in a given language.
Pseudo features capture sound change patterns that involve the position in the syllable.
Distinctive features and pseudo features are extracted from source- and target-language training data to train a linear classifier.
The classifier computes compatibility scores between English source words and target-language words.
When several target-language strings are transliteration candidates for a source word, the one with the highest score is selected as the transliteration.
The method was evaluated using parallel corpora of English with each of four target languages.
NEs were extracted from the English side and were compared with all the words in the target language to find proper transliterations.
The baseline presented for the case of transliteration from English to Arabic achieves Mean Reciprocal Rank (MRR) of 0.66 and this method improves its results by 7%.
This technique involves knowledge about phonological characteristics, such as elision of consonants based on their position in the word, which requires expert knowledge of the language.
In addition, conversion of terms into a phonemic representation poses hurdles in representing short vowels in Arabic and will have similar behavior in Hebrew.
Moreover, English to Arabic transliter-ation is easier than Arabic to English, because in the former, vowels should be deleted whereas in the latter they should be generated.
Matthews (2007) presents a model for translit-eration from Arabic to English based on SMT.
The parallel corpus from which the translation model is acquired contains approximately 2500 pairs, which are part of a bilingual person names corpus (LDC2005G02).
This biases the model toward transliterating person names.
The language model presented for that method consisted of 10K entries of names which is, again, not complete.
This model also uses different settings for maximum phrase length in the translation model and different n-gram order for the language model.
It achieves an accuracy of 43% when transliterating from Arabic to English.
Goldwasser and Roth (2008) introduce a dis-criminative method for identifying NE transliter-ation pairs in English-Hebrew.
Given a word pair (ws, wt), where ws is an English NE, the system determines whether wt, a string in Hebrew, is its transliteration.
The classification is based on pair-wise features: sets of substrings are extracted from each of the words, and substrings from the two sets are then coupled to form the features.
The accuracy of correctly identifying transliteration pairs in top-1 and top-5 was 52% and 88%, respectively.
Whereas this approach selects most suitable transliteration out of a list of candidates, our approach generates a list of possible transliterations ranked by their accuracy.
Despite the importance of identifying TTTs, this task has only been addressed recently.
Gold-berg and Elhadad (2008) present a loosely supervised method for non contextual identification of transliterated foreign words in Hebrew texts.
The method is a Naive-Bayes classifier which learns from noisy data.
Such data are acquired by over-generation of transliterations for a set of words in a foreign script, using mappings from the phonemic representation of words to the Hebrew script.
Precision and recall obtained are 80% and 82%, respectively.
However, although foreign words are indeed often TTTs, many originally Hebrew words should sometimes be transliterated.
As explained in section 4, there are words in Hebrew that may be subject to either translation or transliteration, depending on the context.
A non-contextual approach would not suffice for our task.
Hermjakob et al.
(2008) describe a method for identifying NEs that should be transliterated in Arabic texts.
The method first tries to find a matching English word for each Arabic word in a parallel corpus, and tag the Arabic words as either names or non-names based on a matching algorithm.
This algorithm uses a scoring model which assigns manually-crafted costs to pairs of Arabic and English substrings, allowing for context restrictions.
A number of language specific heuristics, such as considering only capitalized words as candidates and using lists of stop words, are used to enhance the algorithm’s accuracy.
The tagged Arabic corpus is then divided: One part is used to collect statistics about the distribution of name/non-name patterns among tokens, bigrams and trigrams.
The rest of the tagged corpus is used for training using an averaged perceptron.
The precision of the identification task is 92.1% and its recall is 95.9%.
This work also presents a novel transliteration model, which is integrated into a machine translation system.
Its accuracy, measured by the percentage of correctly translated names, is 89.7%.
Our work is very similar in its goals and the overall framework, but in contrast to Hermjakob et al.
(2008) we use much less supervision, and in particular, we do not use a parallel corpus.
We also do not use manually-crafted weights for (hundreds of) bilingual pairs of strings.
More generally, our transliteration model is much more language-pair neutral.
<newSection> 3 Resources and Methodology Our work consists of of two sub-tasks: Identifying TTTs and then transliterating them.
Specifically, we use the following resources for this work: For the identification task we use a large un-annotated corpus of articles from Hebrew press and web-forums (Itai and Wintner, 2008) consisting of 16 million tokens.
The corpus is POS-tagged (Bar-Haim et al., 2008).
We bootstrap a training corpus for one-class SVM (section 4.2) using a list of rare Hebrew character n-grams (section 4.1) to generate a set of positive, high-precision examples for TTTs in the tagged corpus.
POS tags for the positive examples and their surrounding tokens are used as features for the one-class SVM (sec-tion 4.2).
For the transliteration itself we use a list that maps Hebrew consonants to their English counterparts to extract a list of Hebrew-English translation pairs from Wikipedia (section 5.2).
To learn the transliteration model we utilize Moses (sec-tion 5) which is also used for decoding.
Decoding also relies on a target language model, which is trained by applying SRILM to Web 1T corpus (section 5.1).
Importantly, the resources we use for this work are readily available for a large number of languages and can be easily obtained.
None of these require any special expertise in linguistics.
Crucially, no parallel corpus was used.
<newSection> 4 What to transliterate The task in this phase, then, is to determine for each token in a given text whether it should be translated or transliterated.
We developed a set of guidelines to determine which words are to be transliterated.
For example, person names are always transliterated, although many of them have homographs that can be translated.
Foreign words, which retain the sound patterns of their original language with no semantic translation involved, are also (back-)transliterated.
On the other hand, names of countries may be subject to translation or transliteration, as demonstrated in the following examples: We use information obtained from POS tagging (Bar-Haim et al., 2008) to address the problem of identifying TTTs.
Each token is assigned a POS and is additionally marked if it was not found in a lexicon (Itai et al., 2006).
As a baseline, we tag for transliteration Out Of Vocabulary (OOV) tokens.
Our evaluation metric is tagging accuracy, that is, the percentage of correctly tagged tokens.
Many of the TTTs do appear in the lexicon, though, and their number will grow with the availability of more language resources.
As noted above, some TTTs can be identified based on their surface forms; these words are mainly loan words.
For example, the word brwdqsting (broadcasting) contains several sequences of graphemes that are not frequent in Hebrew (e.g., ng in a word-final position).
We manually generated a list of such features to serve as tagging rules.
To create this list we used a few dozens of character bigrams, about a dozen trigrams and a couple of unigrams and four-grams, that are highly unlikely to occur in words of Hebrew origin.
Rules associate n-grams with scores and these scores are summed when applying the rules to tokens.
A typical rule is of the form: if Q1Q2 are the final characters of w, add c to the score of w, where w is a word in Hebrew, Q1 and Q2 are Hebrew characters, and c is an positive integer.
A word is tagged for transliteration if the sum of the scores associated with its substrings is higher than a predefined threshold.
We apply these rules to a large Hebrew corpus and create an initial set of instances of terms that, with high probability, should be be transliterated rather than translated.
Of course, many TTTs, especially those whose surface forms are typical of Hebrew, will be missed when using this tagging technique.
Our solution is to learn the contexts in which TTTs tend to occur, and contrast these contexts with those for translated terms.
The underlying assumption is that the former contexts are syntactically determined, and are independent of the actual surface form of the term (and of whether or not it occurs in the lexicon).
Since the result of the rule-based tagging is considered as examples of TTTs, this automatically-annotated corpus can be used to extract such contexts.
The above process provides us with 40279 examples of TTTs out of a total of more than 16 million tokens.
These examples, however, are only positive examples.
In order to learn from the incomplete data we utilized a One Class Classifier.
Classification problems generally involve two or more classes of objects.
A function separating these classes is to be learned and used by the classifier.
One class classification utilizes only target class objects to learn a function that distinguishes them from any other objects.
SVM (Support Vector Machine) (Vapnik, 1995) is a classification technique which finds a linear separating hyperplane with maximal margins between data instances of two classes.
The separating hyperplane is found for a mapping of data instances into a higher dimension, using a kernel function.
Sch¨olkopf et al.
(2000) introduce an adaptation of the SVM methodology to the problem of one-class classification.
We used one-class SVM as implemented in LIBSVM (Chang and Lin, 2001).
The features selected to represent each TTT were its POS and the POS of the token preceding it in the sentence.
The kernel function which yielded the best results on this problem was a sigmoid with standard parameters.
To evaluate the TTT identification model we created a gold standard, tagged according to the guidelines described above, by a single lexicog-rapher.
The testing corpus consists of 25 sentences from the same sources as the training corpus and contains 518 tokens, of which 98 are TTTs.
We experimented with two different baselines: the naive baseline always decides to translate; a slightly better baseline consults the lexicon, and tags as TTT any token that does not occur in the lexicon.
We measure our performance in error rate reduction of tagging accuracy, compared with the latter baseline.
Our initial approach consisted of consulting only the decision of the one-class SVM.
However, since there are TTTs that can be easily identified using features obtained from their surface form, our method also examines each token using surface-form features, as described in section 4.1.
If a token has no surface features that identify it as a TTT, we take the decision of the one-class SVM.
Table 1 presents different configurations we experimented with, and their results.
The first two columns present the two baselines we used, as explained above.
The third column (OCS) shows the results based only on decisions made by the One Class SVM.
The penultimate column shows the results obtained by our method combining the SVM with surface-based features.
The final column presents the Error Rate Reduction (ERR) achieved when using our method, compared to the baseline of transliterating OOV words.
As can be observed, our method increases classification accuracy: more than 38% of the errors over the baseline are reduced.
The importance of the recognition process is demonstrated in the following example.
The underlined phrase was recognized correctly by our method.
kbwdw habwd $l bn ari kvodo heavud shel ben ari His-honor the-lost of Ben Ari ‘Ben Ari’s lost honor ’ Both the word ben and the word ari have literal meanings in Hebrew (son and lion, respectively), and their combination might be interpreted as a phrase since it is formed as a Hebrew noun construct.
Recognizing them as transliteration candidates is crucial for improving the performance of MT systems.
<newSection> 5 How to transliterate Once a token is classified as a TTT, it is sent to the transliteration module.
Our approach handles the transliteration task as a case of phrase-based SMT, based on the noisy channel model.
According to this model, when translating a string f in the source language into the target language, a string e� is chosen out of all target language strings e if it has the maximal probability given f (Brown et al., 1993): where Pr(f|e) is the translation model and Pr(e) is the target language model.
In phrase-based translation, f is divided into phrases and each source phrase fz is translated into target phrase ez according to a phrase translation model.
Target phrases may then be reordered using a distortion model.
We use SMT for transliteration; this approach views transliteration pairs as aligned sentences and characters are viewed as words.
In the case of phrase-based SMT, phrases are sequences of characters.
We used Moses (Koehn et al., 2007), a phrase-based SMT toolkit, for training the translation model (and later for decoding).
In order to extract phrases, bidirectional word level alignments are first created, both source to target and target to source.
Alignments are merged heuristically if they are consistent, in order to extract phrases.
We created an English target language model from unigrams of Web 1T (Brants and Franz, 2006).
The unigrams are viewed as character n-grams to fit into the SMT system.
We used SRILM (Stol-cke, 2002) with a modified Kneser-Ney smoothing, to generate a language model of order 5.
No parallel corpus of Hebrew-English transliter-ation pairs is available, and compiling one manually is time-consuming and labor-intensive.
Instead, we extracted a parallel list of Hebrew and English terms from Wikipedia and automatically generated such a corpus.
The terms are parallel titles of Wikipedia articles and thus can safely be assumed to denote the same entity.
In many cases these titles are transliterations of one another.
From this list we extracted transliteration pairs according to similarity of consonants in parallel English and Hebrew entries.
The similarity measure is based only on consonants since vowels are often not represented at all in Hebrew.
We constructed a table relating Hebrew and English consonants, based on common knowledge patterns that relate sound to spelling in both languages.
Sound patterns that are not part of the phoneme inventory of Hebrew but are nonetheless represented in Hebrew orthography were also included in the table.
Every entry in the mapping table consists of a Hebrew letter and a possible Latin letter or letter sequences that might match it.
A typical entry is the following: $:SH|S|CH such that SH, S or CH are possible candidates for matching the Hebrew letter $.
Both Hebrew and English titles in Wikipedia may be composed of several words.
However, words composing the entries in each of the languages may be ordered differently.
Therefore, every word in Hebrew is compared with every word in English, assuming that titles are short enough.
The example in Table 2 presents an aligned pair of multi-lingual Wikipedia entries with high similarity of consonants.
This is therefore considered as a transliteration pair.
In contrast, the title empty set which is translated to hqbwch hriqh shows a low similarity of consonants.
This pair is not selected for the training corpus.
gra t e f u l dead g r i i @ pwl d d Out of 41914 Hebrew and English terms retrieved from Wikipedia, more than 20000 were determined as transliteration pairs.
Out of this set, 500 were randomly chosen to serve as a test set, 500 others were chosen to serve as a development set, and the rest are the training set.
Minimum error rate training was done on the development set to optimize translation performance obtained by the training phase.2 For decoding, we prohibited Moses form performing character reordering (distortion).
While reordering may be needed for translation, we want to ensure the monotone nature of transliteration.
We applied Moses to the test set to get a list of top-n transliteration options for each entry in the set.
The results obtained by Moses were further re-ranked to take into account their frequency as reflected in the unigrams of Web 1T (Brants and Franz, 2006).
The re-ranking method first normalizes the scores of Moses’ results to the range of [0, 1].
The respective frequencies of these results in Web1T corpus are also normalized to this range.
The score s of each transliteration option is a linear combination of these two elements: s = αsM + (1 − α)sW, where sM is the normalized score obtained for the transliteration option by Moses, and sW is its normalized frequency.
α is empirically set to 0.75.
Table 3 summarizes the proportion of the terms transliterated correctly across top-n results as achieved by Moses, and their improvement after re-ranking.
We further experimented with two methods for reducing the list of transliteration options to the most prominent ones by taking a variable number of candidates rather than a fixed number.
This is important for limiting the search space of MT systems.
The first method (var1) measures the ratio between the scores of each two consecutive options and generates the option that scored lower only if this ratio exceeds a predefined threshold.
We found that the best setting for the threshold is 0.75, resulting in an accuracy of 88.6% and an average of 2.32 results per token.
Our second method (var2) views the score as a probability mass, and generates all the results whose combined probabilities are at most p.
We found that the best value for p is 0.5, resulting in an accuracy of 87.4% and 1.92 results per token on average.
Both methods outperform the top-2 accuracy.
Table 4 presents a few examples from the test set that were correctly transliterated by our method.
Some incorrect transliterations are demonstrated in Table 5.
Source Transliteration np$ nefesh hlmsbrgr hellmesberger smb@iwn sambation hiprbwlh hyperbola $prd shepard ba$h bachet xt$pswt hatshepsut brgnch berganza ali$r elissar g’wbani giovanni <newSection> 6 Integration with machine translation We have integrated our system as a module in a Machine Translation system, based on Lavie et al.
(2004a).
The system consults the TTT classifier described in section 4 for each token, before translating it.
If the classifier determines that the token should be transliterated, then the transliter-ation procedure described in section 5 is applied to the token to produce the transliteration results.
We provide an external evaluation in the form of BLEU (Papineni et al., 2001) and Meteor (Lavie et al., 2004b) scores for SMT with and without the transliteration module.
When integrating our method in the MT system we use the best transliteration options as obtained when using the re-ranking procedure described in section 5.3.
The translation results for all conditions are presented in Table 6, compared to the basic MT system where no transliteration takes place.
Using the transliteration module yields a statistically significant improvement in METEOR scores (p < 0.05).
METEOR scores are most relevant since they reflect improvement in recall.
The MT system cannot yet take into consideration the weights of the transliteration options.
Translation results are expected to improve once these weights are taken into account.
<newSection> 7 Conclusions We presented a new method for transliteration in the context of Machine Translation.
This method identifies, for a given text, tokens that should be transliterated rather than translated, and applies a transliteration procedure to the identified words.
The method uses only positive examples for learning which words to transliterate and achieves over 38% error rate reduction when compared to the baseline.
In contrast to previous studies this method does not use any parallel corpora for learning the features which define the translit-erated terms.
The simple transliteration scheme is accurate and requires minimal resources which are general and easy to obtain.
The correct transliter-ation is generated in more than 76% of the cases, and in 92% of the instances it is one of the top-5 results.
We believe that some simple extensions could further improve the accuracy of the translitera-tion module, and these are the focus of current and future research.
First, we would like to use available gazetteers, such as lists of place and person names available from the US census bureau, http://world-gazetteer.com/ or http://geonames.org.
Then, we consider utilizing the bigram and trigram parts of Web 1T (Brants and Franz, 2006), to improve the TTT identifier with respect to identifying multi-token expressions which should be transliterated.
In addition, we would like to take into account the weights of the different transliteration options when deciding which to select in the translation.
Finally, we are interested in applying this module to different language pairs, especially ones with limited resources.
<newSection> Acknowledgments We wish to thank Gennadi Lembersky for his help in integrating our work into the MT system, as well as to Erik Peterson and Alon Lavie for providing the code for extracting bilingual article titles from Wikipedia.
We thank Google Inc. and the LDC for making the Web 1T corpus available to us.
Dan Roth provided good advice in early stages of this work.
This research was supported by THE ISRAEL SCIENCE FOUNDATION (grant No. 137/06); by the Israel Internet Association; by the Knowledge Center for Processing Hebrew; and by the Caesarea Rothschild Institute for Interdis-ciplinary Application of Computer Science at the University of Haifa.
<newSection> References<newSection> Abstract This paper discusses computational compositional semantics from the perspective of grammar engineering, in the light of experience with the use of Minimal Recursion Semantics in DELPH-IN grammars.
The relationship between argument indexation and semantic role labelling is explored and a semantic dependency notation (DMRS) is introduced.
<newSection> 1 Introduction The aim of this paper is to discuss work on compositional semantics from the perspective of grammar engineering, which I will take here as the development of (explicitly) linguistically-motivated computational grammars.
The paper was written to accompany an invited talk: it is intended to provide background and further details for those parts of the talk which are not covered in previous publications.
It consists of an brief introduction to our approach to computational compositional semantics, followed by details of two contrasting topics which illustrate the grammar engineering perspective.
The first of these is argument indexing and its relationship to semantic role labelling, the second is semantic dependency structure.
Standard linguistic approaches to compositional semantics require adaptation for use in broad-coverage computational processing.
Although some of the adaptations are relatively trivial, others have involved considerable experimentation by various groups of computational linguists.
Perhaps the most important principle is that semantic representations should be a good match for syntax, in the sense of capturing all and only the information available from syntax and productive morphology, while nevertheless abstracting over semantically-irrelevant idiosyncratic detail.
Compared to much of the linguistics literature, our analyses are relatively superficial, but this is essentially because the broad-coverage computational approach prevents us from over-committing on the basis of the information available from the syntax.
One reflection of this are the formal techniques for scope underspecification which have been developed in computational linguistics.
The im-plementational perspective, especially when combined with a requirement that grammars can be used for generation as well as parsing, also forces attention to details which are routinely ignored in theoretical linguistic studies.
This is particularly true when there are interactions between phenomena which are generally studied separately.
Finally, our need to produce usable systems disallows some appeals to pragmatics, especially those where analyses are radically underspecified to allow for syntactic and morphological effects found only in highly marked contexts.1 In a less high-minded vein, sometimes it is right to be a slacker: life (or at least, project funding) is too short to implement all ideas within a grammar in their full theoretical glory.
Often there is an easy alternative which conveys the necessary information to a consumer of the semantic representations.
Without this, grammars would never stabilise.
Here I will concentrate on discussing work which has used Minimal Recursion Semantics (MRS: Copestake et al.
(2005)) or Robust Minimal Recursion Semantics (RMRS: Copestake (2003)).
The (R)MRS approach has been adopted as a common framework for the DELPH-IN initiative (Deep Linguistic Processing with HPSG: http://www.delph-in.net) and the work discussed here has been done by and in collaboration with researchers involved in DELPH-IN.
The programme of developing computational compositional semantics has a large number of aspects.
It is important that the semantics has a logically-sound interpretation (e.g., Koller and Lascarides (2009), Thater (2007)), is cross-linguistically adequate (e.g., Bender (2008)) and is compatible with generation (e.g., Carroll et al. (1999), Carroll and Oepen (2005)).
Ideally, we want support for shallow as well as deep syntactic analysis (which was the reason for developing RMRS), enrichment by deeper analysis (in-cluding lexical semantics and anaphora resolution, both the subject of ongoing work), and (robust) inference.
The motivation for the development of dependency-style representations (including Dependency MRS (DMRS) discussed in §4) has been to improve ease of use for consumers of the representation and human annotators, as well as use in statistical ranking of analyses/realisations (Fujita et al. (2007), Oepen and L¢nning (2006)).
Integration with distributional semantic techniques is also of interest.
The belated ‘introduction’ to MRS in Copestake et al.
(2005) primarily covered formal representation of complete utterances.
Copestake (2007a) described uses of (R)MRS in applications.
Copes-take et al.
(2001) and Copestake (2007b) concern the algebra for composition.
What I want to do here is to concentrate on less abstract issues in the syntax-semantics interface.
I will discuss two cases where the grammar engineering perspective is important and where there are some conclusions about compositional semantics which are relevant beyond DELPH-IN.
The first, argument indexing (§3), is a relatively clear case in which the constraints imposed by grammar engineering have a significant effect on choice between plausible alternatives.
I have chosen to talk about this both because of its relationship with the currently popular task of semantic role labelling and because the DELPH-IN approach is now fairly stable after a quite considerable degree of experimentation.
What I am reporting is thus a perspective on work done primarily by Flickinger within the English Resource Grammar (ERG: Flickinger (2000)) and by Bender in the context of the Grammar Matrix (Bender et al., 2002), though I’ve been involved in many of the discussions.
The second main topic (§4) is new work on a semantic dependency representation which can be derived from MRS, extending the previous work by Oepen (Oepen and L¢nning, 2006).
Here, the motivation came from an engineering perspective, but the nature of the representation, and indeed the fact that it is possible at all, reveals some interesting aspects of semantic composition in the grammars.
2 The MRS and RMRS languages This paper concerns only representations which are output by deep grammars, which use MRS, but it will be convenient to talk in terms of RMRS and to describe the RMRSs that are constructed under those assumptions.
Such RMRSs are interconvert-ible with MRSs.2 The description is necessarily terse and contains the minimal detail necessary to follow the remainder of the paper.
An RMRS is a description of a set of trees corresponding to scoped logical forms.
Fig 1 shows an example of an RMRS and its corresponding scoped form (only one for this example).
RMRS is a ‘flat’ representation, consisting of a bag of elementary predications (EP), a set of argument relations, and a set of constraints on the possible linkages of the EPs when the RMRS is resolved to scoped form.
Each EP has a predicate, a label and a unique anchor and may have a distin-guished (ARG0) argument (EPs are written here as label:anchor:pred(arg0)).
Label sharing between EPs indicates conjunction (e.g., in Fig 1, big, angry and dog share the label l2).
Argument relations relate non-arg0 arguments to the corresponding EP via the anchor.
Argument names are taken from a fixed set (discussed in §3).
Argument values may be variables (e.g., e8, x4: variables are the only possibility for values of ARG0), constants (strings such as “London”), or holes (e.g. h5), which indicate scopal relationships.
Variables have sortal properties, indicating tense, number and so on, but these are not relevant for this paper.
Variables corresponding to unfilled (syntactically optional) arguments are unique in the RMRS, but otherwise variables must correspond to the ARG0 of an EP (since I am only considering RMRSs from deep grammars here).
Constraints on possible scopal relationships between EPs may be explicitly specified in the grammar via relationships between holes and labels.
In particular qeq constraints (the only type considered here) indicate that, in the scoped forms, a label must either plug a hole directly or be connected to it via a chain of quantifiers.
Hole arguments (other than the BODY of a quantifier) are always linked to a label via a qeq or other constraint (in a deep grammar RMRS).
Variables survive in the models of RMRSs (i.e., the fully scoped trees) whereas holes and labels do not.
l1:a1: some q, BV(a1,x4), RSTR(a1,h5), BODY(a1,h6), h5 qeq l2, l2:a2: big a 1(e8), ARG1(a2,x4), l2:a3: angry a 1(e9), ARG1(a3,x4), l2:a4: dog n 1(x4), l4:a5: bark v 1(e2), ARG1(a5,x4), l4:a6: loud a 1(e10), ARG1(a6,e2) some q(x4, big a 1(e8,x4) n angry a 1(e9, x4) n dog n 1(x4), bark v 1(e2,x4) n loud a 1(e10,e2)) The naming convention for predicates corresponding to lexemes is: stem major sense tag, optionally followed by and minor sense tag (e.g., loud a 1).
Major sense tags correspond roughly to traditional parts of speech.
There are also non-lexical predicates such as ‘poss’ (though none occur in Fig 1).3 MRS varies from RMRS in that the arguments are all directly associated with the EP and thus no anchors are necessary.
I have modified the definition of RMRS given in Copestake (2007b) to make the ARG0 argument optional.
Here I want to add the additional constraint that the ARG0 of an EP is unique to it (i.e., not the ARG0 of any other EP).
I will term this the characteristic variable property.
This means that, for every variable, there is a unique EP which has that variable as its ARG0.
I will assume for this paper that all EPs, apart from quantifier EPs, have such an ARG0.4 The characteristic variable property is one that has emerged from working with large-scale constraint-based grammars.
A few concepts from the MRS algebra are also necessary to the discussion.
Composition can be formalised as functor-argument combination where the argument phrase’s hook fills a slot in the functor phrase, thus instantiating an RMRS argument relation.
The hook consists of an index (a variable), an external argument (also a variable) and an ltop (local top: the label corresponding to the topmost node in the current partial tree, ignoring quantifiers).
The syntax-semantics interface requires that the appropriate hook and slots be set up (mostly lexically in a DELPH-IN grammar) and that each application of a rule specifies the slot to be used (e.g., MOD for modification).
In a lexical entry, the ARG0 of the EP provides the hook 3In fact, most of the choices about semantics made by grammar writers concern the behaviour of constructions and thus these non-lexical predicates, but this would require another paper to discuss.
4I am simplifying for expository convenience.
In current DELPH-IN grammars, quantifiers have an ARG0 which corresponds to the bound variable.
This should not be the charac-teristic variable of the quantifier (it is the characteristic variable of a nominal EP), since its role in the scoped forms is as a notational convenience to avoid lambda expressions.
I will call it the BV argument here.
index, and, apart from quantifiers, the hook ltop is the EP’s label.
In intersective combination, the ltops of the hooks will be equated.
In scopal combination, a hole argument in a slot is specified to be qeq to the ltop of the argument phrase and the ltop of the functor phrase supplies the new hook’s ltop.
By thinking of qeqs as links in an RMRS graph (rather than in terms of their logical behaviour as constraints on the possible scoped forms), an RMRS can be treated as consisting of a set of trees with nodes consisting of EPs grouped via intersec-tive relationships: there will be a backbone tree (headed by the overall ltop and including the main verb if there is one), plus a separate tree for each quantified NP.
For instance, in Fig 1, the third line contains the EPs corresponding to the (single node) backbone tree and the first two lines show the EPs comprising the tree for the quantified NP (one node for the quantifier and one for the N' which it connects to via the RSTR and its qeq).
<newSection> 3 Arguments and roles I will now turn to the representation of arguments in MRS and their relationship to semantic roles.
I want to discuss the approach to argument labelling in some detail, because it is a reasonably clear case where the desiderata for broad-coverage semantics which were discussed in §1 led us to a syntactically-driven approach, as opposed to using semantically richer roles such as AGENT, GOAL and INSTRUMENT.
An MRS can, in fact, be written using a conventional predicate-argument representation.
A representation which uses ordered argument labels can be recovered from this in the obvious way.
E.g., l:like v 1(e,x,y) is equivalent to l:a:like v 1(e), ARG1(a,x), ARG2(a,y).
A fairly large inventory of argument labels is actually used in the DELPH-IN grammars (e.g., RSTR, BODY).
To recover these from the conventional predicate-argument notation requires a look up in a semantic interface component (the SEM-I, Flickinger et al. (2005)).
But open-class predicates use the ARGn convention, where n is 0,1,2,3 or 4 and the discussion here only concerns these.5 Arguably, the DELPH-IN approach is Davidso-nian rather than neo-Davidsonian in that, even in the RMRS form, the arguments are related to the predicate via the anchor which plays no other role in the semantics.
Unlike the neo-Davidsonian use of the event variable to attach arguments, this allows the same style of representation to be used uniformly, including quantifiers, for instance.
Arguments can omitted completely without syntactic ill-formedness of the RMRS, but this is primarily relevant to shallower grammars.
A semantic predicate, such as like v 1, is a logical predicate and as such is expected to have the same arity wherever it occurs in the DELPH-IN grammars.
Thus models for an MRS may be defined in a language with or without argument labels.
The ordering of arguments for open class lexemes is lexically specified on the basis of the syntactic obliqueness hierarchy (Pollard and Sag, 1994).
ARG1 corresponds to the subject in the base (non-passivised) form (‘deep subject’).
Argument numbering is consecutive in the base form, so no predicate with an ARG3 is lexically missing an ARG2, for instance.
An ARG3 may occur without an instantiated ARG2 when a syntactically optional argument is missing (e.g., Kim gave to the library), but this is explicit in the linearised form (e.g., give v(e,x,u,y)).
The full statement of how the obliqueness hierarchy (and thus the labelling) is determined for lexemes has to be made carefully and takes us too far into discussion of syntax to explain in detail here.
While the majority of cases are straightfor-ward, a few are not (e.g., because they depend on decisions about which form is taken as the base in an alternation).
However, all decisions are made at the level of lexical types: adding an entry for a lexeme for a DELPH-IN grammar only requires working out its lexical type(s) (from syntactic behaviour and very constrained semantic notions, e.g., control).
The actual assignment of arguments to an utterance is just a consequence of parsing.
Argument labelling is thus quite different from PropBank (Palmer et al., 2005) role labelling despite the unfortunate similarity of the PropBank naming scheme.
It follows from the fixed arity of predicates that lexemes with different numbers of argu-5ARG4 occurs very rarely, at least in English (the verb bet being perhaps the clearest case).
ments should be given different predicate symbols.
There is usually a clear sense distinction when this occurs.
For instance, we should distinguish between the ‘depart’ and ‘bequeath’ senses of leave because the first takes an ARG1 and an ARG2 (op-tional) and the second ARG1, ARG2 (optional), ARG3.
We do not draw sense distinctions where there is no usage which the grammar could disambiguate.
Of course, there are obvious engineering reasons for preferring a scheme that requires minimal additional information in order to assign argument labels.
Not only does this simplify the job of the grammar writer, but it makes it easier to construct lexical entries automatically and to integrate RMRSs derived from shallower systems.
However, grammar engineers respond to consumers: if more detailed role labelling had a clear utility and required an analysis at the syntax level, we would want to do it in the grammar.
The question is whether it is practically possible.
Detailed discussion of the linguistics literature would be out of place here.
I will assume that Dowty (1991) is right in the assertion that there is no small (say, less than 10) set of role labels which can also be used to link the predicate to its arguments in compositionally constructed semantics (i.e., argument-indexing in Dowty’s terminology) such that each role label can be given a consistent individual semantic interpretation.
For our purposes, a consistent semantic interpretation involves entailment of one or more useful real world propositions (allowing for exceptions to the entailment for unusual individual sentences).
This is not a general argument against rich role labels in semantics, just their use as the means of argument-indexation.
It leaves open uses for grammar-internal purposes, e.g., for defining and controlling alternations.
The earliest versions of the ERG experimented with a version of Davis’s (2001) approach to roles for such reasons: this was not continued, but for reasons irrelevant here.
Roles are still routinely used for argument indexation in linguistics papers (without semantic interpretation).
The case is sometimes made that more mnemonic argument labelling helps human interpretation of the notation.
This may be true of semantics papers in linguistics, which tend to concern groups of similar lexemes.
It is not true of a collaborative computational linguistics project in which broad coverage is being attempted: names can only be mnemonic if they carry some meaning and if the meaning cannot be consistently applied this leads to endless trouble.
What I want to show here is how problems arise even when very limited semantic generalisa-tions are attempted about the nature of just one or two argument labels, when used in broad-coverage grammars.
Take the quite reasonable idea that a semantically consistent labelling for intransitives and related causatives is possible (cf PropBank).
For instance, water might be associated with the same argument label in the following examples: Such an approach was used for a time in the ERG with unaccusatives.
However, it turns out to be impossible to carry through consistently for causative alternations.
Consider the following examples of gallop: 6 If only a single predicate is involved, e.g., gallop v, and the causative has an ARG1 and an ARG2, then what about the two intransitive cases?
If the causative is treated as obligatorily transitive syntactically, then (6) and (7) presumably both have an ARG2 subject.
This leads to Michaela having a different role label in (5) and (7), despite the evident similarity of the real world situation.
Furthermore, the role labels for intransitive movement verbs could only be predicted by a consumer of the semantics who knew whether or not a causative form existed.
The causative may be rare, as with gallop, where the intransitive use is clearly the base case.
Alternatively, if (7) is treated as a causative intransitive, and thus has a subject labelled ARG1, there is a systematic unresolvable ambiguity and the generalisation that the subjects in both intransitive sentences are moving is lost.
Gallop is an not isolated case in having a volitional intransitive use: it applies to most (if not all) motion verbs which undergo the causative alternation.
To rescue this account, we would need to apply it only to true lexical anti-causatives.
It is not clear whether this is doable (even the standard example sink can be used intransitively of deliberate movement) but from a slacker perspective, at this point we should decide to look for an easier approach.
The current ERG captures the causative relationship by using systematic sense labelling: l:a:boil v 1(e), a:ARG1(x), water(x) This is not perfect, but it has clear advantages.
It allows inferences to be made about ARG1 and ARG2 of cause verbs.
In general, inferences about arguments may be made with respect to particular verb classes.
This lends itself to successive refinement in the grammars: the decision to add a standardised sense label, such as cause, does not require changes to the type system, for instance.
If we decide that we can identify true anti-causatives, we can easily make them a distinguished class via this convention.
Conversely, in the situation where causation has not been recognised, and the verb has been treated as a single lexeme having an optional ARG2, the semantics is imperfect but at least the imperfection is local.
In fact, determining argument labelling by the obliqueness hierarchy still allows generalisations to be made for all verbs.
Dowty (1991) argues for the notion of proto-agent (p-agt) and proto-patient (p-pat) as cluster concepts.
Proto-agent properties include volitionality, sentience, causation of an event and movement relative to another participant.
Proto-patient properties include being causally affected and being stationary relative to another participant.
Dowty claims that gener-alisations about which arguments are lexicalised as subject, object and indirect object/oblique can be expressed in terms of relative numbers of p-agt and p-pat properties.
If this is correct, then we can, for example, predict that the ARG1 of any predicate in a DELPH-IN grammar will not have fewer p-agt properties than the ARG2 of that predicate.7 As an extreme alternative, we could use labels which were individual to each predicate, such as LIKER and LIKED (e.g., Pollard and Sag (1994)).
For such role labels to have a consistent meaning, they would have to be lexeme-specific: e.g., LEAVER1 (‘departer’) versus LEAVER2 (‘be-queather’).
However this does nothing for semantic generalisation, blocks the use of argument labels in syntactic generalisations and leads to an extreme proliferation of lexical types when using typed feature structure formalisms (one type would be required per lexeme).
The labels add no additional information and could trivially be added automatically to an RMRS if this were useful for human readers.
Much more interesting is the use of richer lexical semantic generalisations, such as those employed in FrameNet (Baker et al., 1998).
In principle, at least, we could (and should) systematically link the ERG to FrameNet, but this would be a form of semantic enrichment mediated via the SEM-I (cf Roa et al.
(2008)), and not an alternative technique for argument indexation.
<newSection> 4 Dependency MRS The second main topic I want to address is a form of semantic dependency structure (DMRS: see wiki.delph-in.net for the evolving details).
There are good engineering reasons for producing a dependency style representation with links between predicates and no variables: ease of readability for consumers of the representation and for human annotators, parser comparison and integration with distributional lexical semantics being the immediate goals.
Oepen has previously produced elementary dependencies from MRSs but the procedure (partially sketched in Oepen and Lønning (2006)) was not intended to produce complete representations.
It turns out that a DMRS can be constructed which can be demonstrated to be inter-convertible with RMRS, has a simple graph structure and minimises redundancy in the representation.
What is surprising is that this can be done for a particular class of grammars without mak-7Sanfilippo (1990) originally introduced Dowty’s ideas into computational linguistics, but this relative behaviour cannot be correctly expressed simply by using p-agt and p-pat directly for argument indexation as he suggested.
It is incorrect for examples like (2) to be labelled as p-agt, since they have no agentive properties.
ing use of the evident clues to syntax in the predicate names.
The characteristic variable property discussed in §2 is crucial: its availability allows a partial replication of composition, with DMRS links being relatable to functor-argument combinations in the MRS algebra.
I should emphasize that, unlike MRS and RMRS, DMRS is not intended to have a direct logical interpretation.
An example of a DMRS is given in Fig 2.
Links relate nodes corresponding to RMRS predicates.
Nodes have unique identifiers, not shown here.
Directed link labels are of the form ARG/H, ARG/EQ or ARG/NEQ, where ARG corresponds to an RMRS argument label.
H indicates a qeq relationship, EQ label equality and NEQ label inequality, as explained more fully below.
Undirected /EQ arcs also sometimes occur (see §4.3).
The ltop is indicated with a *.
In order to transform an RMRS into a DMRS, we will treat the RMRS as made up of three subgraphs: Label equality graph.
Each EP in an RMRS has a label, which may be shared with any number of other EPs.
This can be captured in DMRS via a graph linking EPs: if this is done exhaustively, there would be n(n − 1)/2 binary non-directional links.
E.g., for the RMRS in Fig 1, we need to link big a 1, angry a 1 and dog n 1 and this takes 3 links.
Obviously the effect of equality could be captured by a smaller number of links, assuming transitivity: but to make the RMRS-to-DMRS conversion deterministic, we need a method for selecting canonical links.
Hole-to-label qeq graph.
A qeq in RMRS links a hole to a label which labels a set of EPs.
There is thus a 1 : 1 mapping between holes and labels which can be converted to a 1 : n mapping between holes and the EPs which share the label.
By taking the EP with the hole as the origin, we can construct an EP-to-EP graph, using the argument name as a label for the link: of course, such links are asymmetric and thus the graph is directed.
e.g., some q has RSTR links to each of big a 1, angry a 1 and dog n 1.
Reducing this to a 1 : 1 mapping between EPs, which we would ideally like for DMRS, requires a canonical method of selecting a head EP from the set of target EPs (as does the selection of the ltop).
Variable graph.
For the conversion to DMRS, we will rely on the characteristic variable property, that every variable has a unique EP associated with it via its ARG0.
Any non-hole argument of an EP will have a value which is the ARG0 of some other EP, or which is unbound (i.e., not found elsewhere in the RMRS) in which case we ignore it.
Thus we can derive a graph between EPs, such that each link is labelled with an argument position and points to a unique EP.
I will talk about an EP’s ‘argument EPs’, to refer to the set of EPs its arguments point to in this graph.
The three EP graphs can be combined to form a dependency structure.
But this has an excessive number of links due to the label equality and qeq components.
We need deterministic techniques for removing the redundancy.
These can utilise the variable graph, since this is already minimal.
The first strategy is to combine the label equality and variable links when they connect the same two EPs.
For instance, we combine the ARG1 link between big a 1, and dog n 1 with the label equality link to give a link labelled ARG1/EQ.
We then test the connectivity of the ARG/EQ links on the assumption of transitivity and remove any redundant links from the label graph.
This usually removes all label equality links: one case where it does not is discussed in §4.3.
Variable graph links with no corresponding label equality are annotated ARG/NEQ, while links arising from the qeq graph are labelled ARG/H.
This retains sufficient information to allow the reconstruction of the three graphs in DMRS-to-RMRS conversion.
In order to reduce the number of links arising from the qeq graph, we make use of the variable graph to select a head from a set of EPs sharing a label.
It is not essential that there should be a unique head, but it is desirable.
The next section outlines how head selection works: despite not using any directly syntactic properties, it generally recovers the syntactic head.
Head selection uses one principle and one heuristic, both of which are motivated by the composi-tional properties of the grammar.
The principle is that qeq links from an EP should parallel any comparable variable links.
If an EP has two arguments, one of which is a variable argument which links to EP' and the other a hole argument which has a value corresponding to a set of EPs including EP', EP' is chosen as the head of that set.
This essentially follows from the composition rules: in an algebra operation giving rise to a qeq, the argument phrase supplies a hook consisting of an index (normally, the ARG0 of the head EP) and an ltop (normally, the label of the head EP).
Thus if a variable argument corresponds to EP', EP' will have been the head of the corresponding phrase and is thus the choice of head in the DMRS.
This most frequently arises with quantifiers, which have both a BV and a RSTR argument: the RSTR argument can be taken as linking to the EP which has an ARG0 equal to the BV (i.e., the head of the N').
If this principle applies, it will select a unique head.
In fact, in this special case, we drop the BV link from the final DMRS because it is entirely predictable from the RSTR link.
In the case where there is no variable argument, we use the heuristic which generally holds in DELPH-IN grammars that the EPs which we wish to distinguish as heads in the DMRS do not share labels with their DMRS argument EPs (in contrast to intersective modifiers, which always share labels with their argument EPs).
Heads may share labels with PPs which are syntactically arguments, but these have a semantics like PP modifiers, where the head is the preposition’s EP argument.
NP arguments are generally quantified and quantifiers scope freely.
AP, VP and S syntactic arguments are always scopal.
PPs which are not modifier-like are either scopal (small clauses) or NP-like (case marking Ps) and free-scoping.
Thus, somewhat counter-intuitively, we can select the head EP from the set of EPs which share a label by looking for an EP which has no argument EPs in that set.
The MRS-to-DMRS procedure deterministically creates a unique DMRS.
A converse DMRS-to-MRS procedure recreates the MRS (up to label, anchor and variable renaming), though requiring the SEMI to add the uninstantiated optional arguments.
I claimed above that DMRSs are an idealisation of semantic composition.
A pure functor-argument application scheme would produce a tree which could be transformed into a structure where no dependent had more than one head.
But in DMRS the notion of functor/head is more complex as determiners and modifiers provide slots in the RMRS algebra but not the index of the result.
Composition of a verb (or any other functor) with an NP argument gives rise to a dependency between the verb and the head noun in the N'.
The head noun provides the index of the NP’s hook in composition, though it does not provide the ltop, which comes from the quantifier.
However, because this ltop is not equated with any label, there is no direct link between the verb and the determiner.
Thus the noun will have a link from the determiner and from the verb.
Similarly, if the constituents in composition were continuous, the adjacency condition would hold, but this does not apply because of the mechanisms for long-distance dependencies and the availability of the external argument in the hook.8 DMRS indirectly preserves the information about constituent structure which is essential for semantic interpretation, unlike some syntactic dependency schemes.
In particular, it retains information about a quantifier’s N', since this forms the restrictor of the generalised quantifier (for instance Most white cats are deaf has different truth conditions from Most deaf cats are white).
An interesting example of nominal modification is shown in Fig 3.
Notice that whose has a decomposed semantics combining two non-lexeme predicates def explicit q and poss.
Unusually, the relative clause has a gap which is not an argument of its semantic head (it’s an argument of poss rather than bite v 1).
This means that when the relative clause 8Given that non-local effects are relatively circumscribed, it is possible to require adjacency in some parts of the DMRS.
This leads to a technique for recording underspecification of noun compound bracketing, for instance. is combined with the gap filler, the label equality and the argument instantiation correspond to different EPs.
Thus there is a label equality which cannot be combined with an argument link and has to be represented by an undirected /EQ arc.
<newSection> 5 Related work and conclusion Hobbs (1985) described a philosophy of computational compositional semantics that is in some respects similar to that presented here.
But, as far as I am aware, the Core Language Engine book (Al-shawi, 1992) provided the first detailed description of a truly computational approach to com-positional semantics: in any case, Steve Pulman provided my own introduction to the idea.
Currently, the ParGram project also undertakes large-scale multilingual grammar engineering work: see Crouch and King (2006) and Crouch (2006) for an account of the semantic composition techniques now being used.
I am not aware of any other current grammar engineering activities on the Par-Gram or DELPH-IN scale which build bidirectional grammars for multiple languages.
Overall, what I have tried to do here is to give a flavour of how compositional semantics and syntax interact in computational grammars.
Analyses which look simple have often taken considerable experimentation to arrive at when working on a large-scale, especially when attempting cross-linguistic generalisations.
The toy examples that can be given in papers like this one do no justice to this, and I would urge readers to try out the grammars and software and, perhaps, to join in.
<newSection> Acknowledgements Particular thanks to Emily Bender, Dan Flickinger and Alex Lascarides for detailed comments at very short notice!
I am also grateful to many other colleagues, especially from DELPH-IN and in the Cambridge NLIP research group.
This work was supported by the Engineering and Physical Sciences Research Council [grant numbers EP/C010035/1, EP/F012950/1].
<newSection> References<newSection> Abstract We describe a method for creating a non-English subjectivity lexicon based on an English lexicon, an online translation service and a general purpose thesaurus: Wordnet.
We use a PageRank-like algorithm to bootstrap from the translation of the English lexicon and rank the words in the thesaurus by polarity using the network of lexical relations in Wordnet.
We apply our method to the Dutch language.
The best results are achieved when using synonymy and antonymy relations only, and ranking positive and negative words simultaneously.
Our method achieves an accuracy of 0.82 at the top 3,000 negative words, and 0.62 at the top 3,000 positive words.
<newSection> 1 Introduction One of the key tasks in subjectivity analysis is the automatic detection of subjective (as opposed to objective, factual) statements in written documents (Mihalcea and Liu, 2006).
This task is essential for applications such as online marketing research, where companies want to know what customers say about the companies, their products, specific products’ features, and whether comments made are positive or negative.
Another application is in political research, where public opinion could be assessed by analyzing user-generated online data (blogs, discussion forums, etc.).
Most current methods for subjectivity identification rely on subjectivity lexicons, which list words that are usually associated with positive or negative sentiments or opinions (i.e., words with polarity).
Such a lexicon can be used, e.g., to classify individual sentences or phrases as subjective or not, and as bearing positive or negative sentiments (Pang et al., 2002; Kim and Hovy, 2004; Wilson et al., 2005a).
For English, manually created subjectivity lexicons have been available for a while, but for many other languages such resources are still missing.
We describe a language-independent method for automatically bootstrapping a subjectivity lexicon, and apply and evaluate it for the Dutch language.
The method starts with an English lexicon of positive and negative words, automatically translated into the target language (Dutch in our case).
A PageRank-like algorithm is applied to the Dutch wordnet in order to filter and expand the set of words obtained through translation.
The Dutch lexicon is then created from the resulting ranking of the wordnet nodes.
Our method has several benefits: We apply our method to the most recent version of Cornetto (Vossen et al., 2007), an extension of the Dutch WordNet, and we experiment with various parameters of the algorithm, in order to arrive at a good setting for porting the method to other languages.
Specifically, we evaluate the quality of the resulting Dutch subjectivity lexicon using different subsets of wordnet relations and information in the glosses (definitions).
We also examine the effect of the number of iterations on the performance of our method.
We find that best performance is achieved when using only synonymy and antonymy relations and, moreover, the algorithm converges after about 10 iterations.
The remainder of the paper is organized as follows.
We summarize related work in section 2, present our method in section 3 and describe the manual assessment of the lexicon in section 4.
We discuss experimental results in section 5 and conclude in section 6.
<newSection> 2 Related work Creating subjectivity lexicons for languages other than English has only recently attracted attention of the research community.
(Mihalcea et al., 2007) describes experiments with subjectivity classification for Romanian.
The authors start with an English subjectivity lexicon with 6,856 entries, Opin-ionFinder (Wiebe and Riloff, 2005), and automatically translate it into Romanian using two bilingual dictionaries, obtaining a Romanian lexicon with 4,983 entries.
A manual evaluation of a sample of 123 entries of this lexicon showed that 50% of the entries do indicate subjectivity.
In (Banea et al., 2008) a different approach based on boostrapping was explored for Romanian.
The method starts with a small seed set of 60 words, which is iteratively (1) expanded by adding synonyms from an online Romanian dictionary, and (2) filtered by removing words which are not similar (at a preset threshold) to the original seed, according to an LSA-based similarity measure computed on a half-million word corpus of Romanian.
The lexicon obtained after 5 iterations of the method was used for sentence-level sentiment classification, indicating an 18% improvement over the lexicon of (Mihalcea et al., 2007).
Both these approaches produce unordered sets of positive and negative words.
Our method, on the other hand, assigns polarity scores to words and produces a ranking of words by polarity, which provides a more flexible experimental framework for applications that will use the lexicon.
Esuli and Sebastiani (Esuli and Sebastiani, 2007) apply an algorithm based on PageRank to rank synsets in English WordNet according to positive and negativite sentiments.
The authors view WordNet as a graph where nodes are synsets and synsets are linked with the synsets of terms used in their glosses (definitions).
The algorithm is initialized with positivity/negativity scores provided in SentiWordNet (Esuli and Sebastiani, 2006), an English sentiment lexicon.
The weights are then distributed through the graph using an the algorithm similar to PageRank.
Authors conclude that larger initial seed sets result in a better ranking produced by the method.
The algorithm is always run twice, once for positivity scores, and once for negativity scores; this is different in our approach, which ranks words from negative to positive in one run.
See section 5.4 for a more detailed comparison between the existing approaches outlined above and our approach.
<newSection> 3 Approach Our approach extends the techniques used in (Esuli and Sebastiani, 2007; Banea et al., 2008) for mining English and Romanian subjectivity lexicons.
We hypothesize that concepts (synsets) that are closely related in a wordnet have similar meaning and thus similar polarity.
To determine relatedness between concepts, we view a wordnet as a graph of lexical relations between words and synsets: Nodes and arcs of such a graph are assigned weights, which are then propagated through the graph by iteratively applying a PageRank-like algorithm.
Initially, weights are assigned to nodes and arcs in the graph using translations from an English polarity lexicon as follows: We use the following notation.
Our algorithm is iterative and k = 0,1, ...
denotes an iteration.
Let aki be the weight of the node i at the k-th iteration.
Let wjm be the weight of the arc that connects node j with node m; we assume the weight is 0 if the arc does not exist.
Finally, α is a damping factor of the PageRank algorithm, set to 0.8.
This factor balances the impact of the initial weight of a node with the impact of weight received through connections to other nodes.
The algorithm proceeds by updating the weights of nodes iteratively as follows: akj · wji m |wjm |+ �1 − α� ·a0 i Furthermore, at each iterarion, all weights ak+1 i are normalized by maxj |ak+1 j |.
The equation above is a straightforward extension of the PageRank method for the case when arcs of the graph are weighted.
Nodes propagate their polarity mass to neighbours through outgoing arcs.
The mass transferred depends on the weight of the arcs.
Note that for arcs with negative weight (in our case, antonymy relation), the polarity of transferred mass is inverted: i.e., synsets with negative polarity will enforce positive polarity in their antonyms.
We iterate the algorithm and read off the resulting weight of the word nodes.
We assume words with the lowest resulting weight to have negative polarity, and word nodes with the highest weight positive polarity.
The output of the algorithm is a list of words ordered by polarity score.
We use an English subjectivity lexicon of Opinion-Finder (Wilson et al., 2005b) as the starting point of our method.
The lexicon contains 2,718 English words with positive polarity and 4,910 words with negative polarity.
We use a free online translation service1 to translate positive and negative polarity words into Dutch, resulting in 974 and 1,523 Dutch words, respectively.
We assumed that a word was translated into Dutch successfully if the translation occurred in the Dutch wordnet (there-fore, the result of the translation is smaller than the original English lexicon).
The Dutch wordnet we used in our experiments is the most recent version of Cornetto (Vossen et al., 2007).
This wordnet contains 103,734 lexical units (words), 70,192 synsets, and 157,679 relations between synsets.
<newSection> 4 Manual assessments To assess the quality of our method we re-used assessments made for earlier work on comparing two resources in terms of their usefulness for automatically generating subjectivity lexicons (Jij-koun and Hofmann, 2008).
In this setting, the goal was to compare two versions of the Dutch Wordnet: the first from 2001 and the other from 2008.
We applied the method described in section 3 to both resources and generated two subjectivity rankings.
From each ranking, we selected the 2000 words ranked as most negative and the 1500 words ranked as most positive, respectively.
More negative than positive words were chosen to reflect the original distribution of positive vs. negative words.
In addition, we selected words for assessment from the remaining parts of the ranked lists, randomly sampling chunks of 3000 words at intervals of 10000 words with a sampling rate of 10%.
The selection was made in this way because we were mostly interested in negative and positive words, i.e., the words near either end of the rankings.
Human annotators were presented with a list of words in random order, for each word its part-ofspeech tag was indicated.
Annotators were asked to identify positive and negative words in this list, i.e., words that indicate positive (negative) emotions, evaluations, or positions.
Annotators were asked to classify each word on the list into one of five classes: ++ the word is positive in most contexts (strongly positive) + the word is positive in some contexts (weakly positive) 0 the word is hardly ever positive or negative (neutral) − the a word is negative in some contexts (weakly negative) −− the word is negative in most contexts (strongly negative) Cases where assessors were unable to assign a word to one of the classes, were separately marked as such.
For the purpose of this study we were only interested in identifying subjective words without considering subjectivity strength.
Furthermore, a pilot study showed assessments of the strength of subjectivity to be a much harder task (54% inter-annotator agreement) than distinguishing between positive, neutral and negative words only (72% agreement).
We therefore collapsed the classes of strongly and weakly subjective words for evaluation.
These results for three classes are reported and used in the remainder of this paper.
The data were annotated by two undergraduate university students, both native speakers of Dutch.
Annotators were recruited through a university mailing list.
Assessment took a total of 32 working hours (annotating at approximately 450-500 words per hour) which were distributed over a total of 8 annotation sessions.
In total, 9,089 unique words were assessed, of which 6,680 words were assessed by both annotators.
For 205 words, one or both assessors could not assign an appropriate class; these words were excluded from the subsequent study, leaving us with 6,475 words with double assessments.
Table 1 shows the number of assessed words and inter-annotator agreement overall and per part-of-speech.
Overall agreement is 69% (Co-hen’s n=0.52).
The highest agreement is for adjectives, at 76% (n=0.62) . This is the same level of agreement as reported in (Kim and Hovy, 2004) for English.
Agreement is lowest for verbs (55%, n=0.29) and adverbs (56%, n=0.18), which is slightly less than the 62% agreement on verbs reported by Kim and Hovy.
Overall we judge agreement to be reasonable.
Table 2 shows the confusion matrix between the two assessors.
We see that one assessor judged more words as subjective overall, and that more words are judged as negative than positive (this <newSection> 5 Experiments and results We evaluated several versions of the method of section 3 in order to find the best setting.
Our baseline is a ranking of all words in the wordnet with the weight -1 assigned to the translations of English negative polarity words, 1 assigned to the translations of positive words, and 0 assigned to the remaining words.
This corresponds to simply translating the English subjectivity lexicon.
In the run all.100 we applied our method to all words, synsets and relations from the Dutch Wordnet to create a graph with 153,386 nodes (70,192 synsets, 83,194 words) and 362,868 directed arcs (103,734 word-to-synset, 103,734 synset-to-word, 155,400 synset-to-synset relations).
We used 100 iterations of the PageRank algorihm for this run (and all runs below, unless indicated otherwise).
In the run syn.100 we only used synset-to-word, word-to-synset relations and 2,850 near-synonymy relations between synsets.
We added 1,459 near-antonym relations to the graph to produce the run syn+ant.100.
In the run syn+hyp.100 we added 66,993 hyponymy and 66,993 hyperonymy relations to those used in run syn.100.
We also experimented with the information provided in the definitions (glosses) of synset.
The glosses were available for 68,122 of the 70,192 synsets.
Following (Esuli and Sebastiani, 2007), we assumed that there is a semantic relationship between a synset and each word used in its gloss.
Thus, the run gloss.100 uses a graph with 70,192 synsets, 83,194 words and 350,855 directed arcs from synsets to lemmas of all words in their glosses.
To create these arcs, glosses were lemmatized and lemmas not found in the wordnet were ignored.
To see if the information in the glosses can complement the wordnet relations, we also generated a hybrid run syn+ant+gloss.100 that used arcs derived from word-to-synset, synset-to-word, synonymy, antonymy relations and glosses.
Finally, we experimented with the number of iterations of PageRank in two setting: using all wordnet relations and using only synonyms and antonyms.
We used several measures to evaluate the quality of the word rankings produced by our method.
We consider the evaluation of a ranking parallel to the evaluation for a binary classification problem, where words are classified as positive (resp.
negative) if the assigned score exceeds a certain threshold value.
We can select a specific threshold and classify all words exceeding this score as positive.
There will be a certain amount of correctly classified words (true positives), and some incorrectly classified words (false positives).
As we move the threshold to include a larger portion of the ranking, both the number of true positives and the number of false positives increase.
We can visualize the quality of rankings by plotting their ROC curves, which show the relation between true positive rate (portion of the data correctly labeled as positive instances) and false positive rate (portion of the data incorrectly labeled as positive instances) at all possible threshold settings.
To compare rankings, we compute the area under the ROC curve (AUC), a measure frequently used to evaluate the performance of ranking classifiers.
The AUC value corresponds to the probability that a randomly drawn positive instance will be ranked higher than a randomly drawn negative instance.
Thus, an AUC of 0.5 corresponds to random performance, a value of 1.0 corresponds to perfect performance.
When evaluating word rankings, we compute AUC− and AUC+ as evaluation measures for the tasks of identifying words with negative (resp., positive) polarity.
Other measures commonly used to evaluate rankings are Kendall’s rank correlation, or Kendall’s tau coefficient, and Kendall’s distance (Fagin et al., 2004; Esuli and Sebastiani, 2007).
When comparing rankings, Kendall’s measures look at the number of pairs of ranked items that agree or disagree with the ordering in the gold standard.
The measures can deal with partially ordered sets (i.e., rankings with ties): only pairs that are ordered in the gold standard are used.
Let T = {(ai, bi)}i denote the set of pairs ordered in the gold standard, i.e., ai �g bi.
Let C = {(a, b) E T |a �r b} be the set of concordant pairs, i.e., pairs ordered the same way in the gold standard and in the ranking.
Let D = {(a, b) E T  |b a} be the set of discordant pairs and U = T (C U D) the set of pairs ordered in the gold standard, but tied in the ranking.
Kendall’s rank correlation coefficient rk and Kendall’s distance Dk are defined as follows: where p is a penalization factor for ties, which we set to 0.5, following (Esuli and Sebastiani, 2007).
The value of rk ranges from -1 (perfect disagreement) to 1 (perfect agreement), with 0 indicating an almost random ranking.
The value of Dk ranges from 0 (perfect agreement) to 1 (per-fect disagreement).
When applying Kendall’s measures we assume that the gold standard defines a partial order: for two words a and b, a �g b holds when a E Ng, b E Ug U Pg or when a E Ug, b E Pg; here Ng, Ug, Pg are sets of words judged as negative, neutral and positive, respectively, by human assessors.
The results in Table 3 indicate that the method performs best when only synonymy and antonymy relations are considered for ranking.
Adding hyponyms and hyperonyms, or adding relations between synsets and words in their glosses substan-tially decrease the performance, according to all four evaluation measures.
With all relations, the performance degrades even further.
Our hypothesis is that with many relations the polarity mass of the seed words is distributed too broadly.
This is supported by the drop in the performance early in the ranking at the “negative” side of runs with all relations and with hyponyms (Figure 1, left).
Another possible explanation can be that words with many incoming arcs (but without strong connections to the seed words) get substantial weights, thereby decreasing the quality of the ranking.
Antonymy relations also prove useful, as using them in addition to synonyms results in a small improvement.
This justifies our modification of the PageRank algorithm, when we allow negative node and arc weights.
In the best setting (syn+ant.100), our method achieves an accuracy of 0.82 at top 3,000 negative words, and 0.62 at top 3,000 positive words (esti-mated from manual assessments of a sample, see section 4).
Moreover, Figure 1 indicates that the accuracy of the seed set (i.e., the baseline translations of the English lexicon) is maintained at the positive and negative ends of the ranking for most variants of the method.
In Figure 2 we plot how the AUC− measure changes when the number of PageRank iterations increases (for positive polarity; the plots are almost identical for negative polarity).
Although the absolute maximum of AUC is achieved at 110 iteration (60 iterations for positive polarity), the AUC clearly converges after 20 iterations.
We conclude that after 20 iterations all useful information has been propagated through the graph.
Moreover, our version of PageRank reaches a stable weight distribution and, at the same time, produces the best ranking.
Although the values in the evaluation results are, obviously, language-dependent, we tried to replicate the methods used in the literature for Romanian and English (section 2), to the degree possible.
Our baseline replicates the method of (Mihal-cea et al., 2007): i.e., a simple translation of the English lexicon into the target language.
The run syn.10 is similar to the iterative method used in (Banea et al., 2008), except that we do not perform a corpus-based filtering.
We run PageRank for 10 iterations, so that polarity is propagated from the seed words to all their 5-step-synonymy neighbours.
Table 3 indicates that increasing the number of iterations in the method of (Banea et The run gloss.100 is similar to the PageRank-based method of (Esuli and Sebastiani, 2007).
The main difference is that Esuli and Sebastiani used the extended English WordNet, where words in all glosses are manually assigned to their correct synsets: the PageRank method then uses relations between synsets and synsets of words in their glosses.
Since such a resource is not available for our target language (Dutch), we used relations between synsets and words in their glosses, instead.
With this simplification, the PageRank method using glosses produces worse results than the method using synonyms.
Further experiments with the extended English WordNet are necessary to investigate whether this decrease can be attributed to the lack of disambiguation for glosses.
An important difference between our method and (Esuli and Sebastiani, 2007) is that the latter produces two independent rankings: one for positive and one for negative words.
To evaluate the effect of this choice, we generated runs gloss.100.N and gloss.100.P that used only negative (resp., only positive) seed words.
We compare these runs with the run gloss.100 (that starts with both positive and negative seeds) in Table 4.
To allow a fair comparison of the generated rankings, the evaluation measures in this case are calculated separately for two binary classification problems: words with negative polarity versus all words, and words with positive polarity versus all.
formation about words of one polarity class helps to identify words of the other polarity: negative words are unlikely to be also positive, and vice versa.
This supports our design choice: ranking words from negative to positive in one run of the method.
<newSection> 6 Conclusion We have presented a PageRank-like algorithm that bootstraps a subjectivity lexicon from a list of initial seed examples (automatic translations of words in an English subjectivity lexicon).
The algorithm views a wordnet as a graph where words and concepts are connected by relations such as synonymy, hyponymy, meronymy etc.
We initialize the algorithm by assigning high weights to positive seed examples and low weights to negative seed examples.
These weights are then propagated through the wordnet graph via the relations.
After a number of iterations words are ranked according to their weight.
We assume that words with lower weights are likely negative and words with high weights are likely positive.
We evaluated several variants of the method for the Dutch language, using the most recent version of Cornetto, an extension of Dutch WordNet.
The evaluation was based on the manual assessment of 9,089 words (with inter-annotator agreement 69%, n=0.52).
Best results were achieved when the method used only synonymy and antonymy relations, and was ranking positive and negative words simultaneously.
In this setting, the method achieves an accuracy of 0.82 at the top 3,000 negative words, and 0.62 at the top 3,000 positive words.
Our method is language-independent and can easily be applied to other languages for which wordnets exist.
We plan to make the implementation of the method publicly available.
An additional important outcome of our experiments is the first (to our knowledge) manually annotated sentiment lexicon for the Dutch language.
The lexicon contains 2,836 negative polarity and 1,628 positive polarity words.
The lexicon will be made publicly available as well.
Our future work will focus on using the lexicon for sentence- and phrase-level sentiment extraction for Dutch.
<newSection> Acknowledgments This work was supported by projects DuO-MAn and Cornetto, carried out within the STEVIN programme which is funded by the Dutch and Flemish Governments (http:// www.stevin-tst.org), and by the Netherlands Organization for Scientific Research (NWO) under project number 612.061.814.
<newSection> References<newSection> Abstract Mobile voice-enabled search is emerging as one of the most popular applications abetted by the exponential growth in the number of mobile devices.
The automatic speech recognition (ASR) output of the voice query is parsed into several fields.
Search is then performed on a text corpus or a database.
In order to improve the robustness of the query parser to noise in the ASR output, in this paper, we investigate two different methods to query parsing.
Both methods exploit multiple hypotheses from ASR, in the form of word confusion networks, in order to achieve tighter coupling between ASR and query parsing and improved accuracy of the query parser.
We also investigate the results of this improvement on search accuracy.
Word confusion-network based query parsing outperforms ASR 1-best based query-parsing by 2.7% absolute and the search performance improves by 1.8% absolute on one of our data sets.
<newSection> 1 Introduction Local search specializes in serving geographically constrained search queries on a structured database of local business listings.
Most text-based local search engines provide two text fields: the “SearchTerm” (e.g. Best Chinese Restaurant) and the “LocationTerm” (e.g. a city, state, street address, neighborhood etc.).
Most voice-enabled local search dialog systems mimic this two-field approach and employ a two-turn dialog strategy.
The dialog system solicits from the user a LocationTerm in the first turn followed by a SearchTerm in the second turn (Wang et al., 2008).
Although the two-field interface has been widely accepted, it has several limitations for mobile voice search.
First, most mobile devices are location-aware which obviates the need to specify the LocationTerm.
Second, it’s not always straightforward for users to be aware of the distinction between these two fields.
It is common for users to specify location information in the SearchTerm field.
For example, “restaurants near Manhattan” for SearchTerm and “NY NY” for LocationTerm.
For voice-based search, it is more natural for users to specify queries in a single utterance1.
Finally, many queries often contain other constraints (assuming LocationTerm is a constraint) such as that deliver in restaurants that deliver or open 24 hours in night clubs open 24 hours.
It would be very cumbersome to enumerate each constraint as a different text field or a dialog turn.
An interface that allows for specifying constraints in a natural language utterance would be most convenient.
In this paper, we introduce a voice-based search system that allows users to specify search requests in a single natural language utterance.
The output of ASR is then parsed by a query parser into three fields: LocationTerm, SearchTerm, and Filler.
We use a local search engine, http://www.yellowpages.com/, which accepts the SearchTerm and LocationTerm as two query fields and returns the search results from a business listings database.
We present two methods for parsing the voice query into different fields with particular emphasis on exploiting the ASR output beyond the 1-best hypothesis.
We demonstrate that by parsing word confusion networks, the accuracy of the query parser can be improved.
We further investigate the effect of this improvement on the search task and demonstrate the benefit of tighter coupling of ASR and the query parser on search accuracy.
The paper outline is as follows.
In Section 2, we discuss some of the related threads of research relevant for our task.
In Section 3, we motivate the need for a query parsing module in voice-based search systems.
We present two different query parsing models in Section 4 and Section 5 and discuss experimental results in Section 6.
We summarize our results in Section 7.
The role of query parsing can be considered as similar to spoken language understanding (SLU) in dialog applications.
However, voice-based search systems currently do not have SLU as a separate module, instead the words in the ASR 1-best output are directly used for search.
Most voice-based search applications apply a conventional vector space model (VSM) used in information retrieval systems for search.
In (Yu et al., 2007), the authors enhanced the VSM by deem-phasizing term frequency in Listing Names and using character level instead of word level uni/bi-gram terms to improve robustness to ASR errors.
While this approach improves recall it does not improve precision.
In other work (Natarajan et al., 2002), the authors proposed a two-state hidden Markov model approach for query understanding and speech recognition in the same step (Natarajan et al., 2002).
There are two other threads of research literature relevant to our work.
Named entity (NE) extraction attempts to identify entities of interest in speech or text.
Typical entities include locations, persons, organizations, dates, times monetary amounts and percentages (Kubala et al., 1998).
Most approaches for NE tasks rely on machine learning approaches using annotated data.
These algorithms include a hidden Markov model, support vector machines, maximum entropy, and conditional random fields.
With the goal of improving robustness to ASR errors, (Favre et al., 2005) described a finite-state machine based approach to take as input ASR n-best strings and extract the NEs.
Although our task of query segmentation has similarity with NE tasks, it is arguable whether the SearchTerm is a well-defined entity, since a user can provide varied expressions as they would for a general web search.
Also, it is not clear how the current best performing NE methods based on maximum entropy or conditional random fields models can be extended to apply on weighted lattices produced by ASR.
The other related literature is natural language interface to databases (NLIDBs), which had been well-studied during 1960s-1980s (Androutsopou-los, 1995).
In this research, the aim is to map a natural language query into a structured query that could be used to access a database.
However, most of the literature pertains to textual queries, not spoken queries.
Although in its full generality the task of NLIDB is significantly more ambitious than our current task, some of the challenging problems (e.g. modifier attachment in queries) can also be seen in our task as well.
<newSection> 3 Voice-based Search System Architecture Figure 1 illustrates the architecture of our voice-based search system.
As expected the ASR and Search components perform speech recognition and search tasks.
In addition to ASR and Search, we also integrate a query parsing module between ASR and Search for a number of reasons.
First, as can be expected the ASR 1-best output is typically error-prone especially when a user query originates from a noisy environment.
However, ASR word confusion networks which compactly encode multiple word hypotheses with their probabilities have the potential to alleviate the errors in a 1-best output.
Our motivation to introduce the understanding module is to rescore the ASR output for the purpose of maximizing search performance.
In this paper, we show promising results using richer ASR output beyond 1-best hypothesis.
Second, as mentioned earlier, the query parser not only provides the search engine “what” and “where” information, but also segments the query to phrases of other concepts.
For the example we used earlier, we segment night club open 24 hours into night club and open 24 hours.
Query segmentation has been considered as a key step to achieving higher retrieval accuracy (Tan and Peng, 2008).
Lastly, we prefer to reuse an existing local search engine http://www.yellowpages.com/, in which many text normalization, task specific tuning, business rules, and scalability issues have been well addressed.
Given that, we need a module to translate ASR output to the query syntax that the local search engine supports.
In the next section, we present our proposed approaches of how we parse ASR output including ASR 1-best string and lattices in a scalable framework.
<newSection> 4 Text Indexing and Search-based Parser (PARIS) As we discussed above, there are many potential approaches such as those for NE extraction we can explore for parsing a query.
In the context of voice local search, users expect overall system response time to be similar to that of web search.
Consequently, the relatively long ASR latency leaves no room for a slow parser.
On the other hand, the parser needs to be tightly synchronized with changes in the listing database, which is updated at least once a day.
Hence, the parser’s training process also needs to be quick to accomodate these changes.
In this section, we propose a probabilistic query parsing approach called PARIS (parsing using indexing and search).
We start by presenting a model for parsing ASR 1-best and extend the approach to consider ASR lattices.
We formulate the query parsing task as follows.
A 1-best ASR output is a sequence of words: Q = q1, q2, ...
, qn.
The parsing task is to segment Q into a sequence of concepts.
Each concept can possibly span multiple words.
Let S = S1, S2, ...
, Sk, ..., Sm be one of the possible segmentations comprising of m segments, where Sk = qij = qi,...qj,1 G i G j G n + 1.
The corresponding concept sequence is represented as C = c1, c2, ...
, ck, ...
, cm.
For a given Q, we are interested in searching for the best segmentation and concept sequence (S∗, C∗) as defined by Equation 1, which is rewritten using Bayes rule as Equation 2.
The prior probability P(C) is approximated using an h-gram model on the concept sequence as shown in Equation 3.
We model the segment sequence generation probability P(S|C) as shown in Equation 4, using independence assumptions.
Finally, the query terms corresponding to a segment and concept are generated using Equations 5 and 6.
To train this model, we only have access to text query logs from two distinct fields (SearchTerm, LocationTerm) and the business listing database.
We built a SearchTerm corpus by including valid queries that users typed to the SearchTerm field and all the unique business listing names in the listing database.
Valid queries are those queries for which the search engine returns at least one business listing result or a business category.
Similarly, we built a corpus for LocationTerm by con-catenating valid LocationTerm queries and unique addresses including street address, city, state, and zip-code in the listing database.
We also built a small corpus for Filler, which contains common carrier phrases and stop words.
The generation probabilities as defined in 6 can be learned from these three corpora.
In the following section, we describe a scalable way of implementation using standard text indexer and searcher.
We use Apache-Lucene (Hatcher and Gospod-netic, 2004), a standard text indexing and search engines for query parsing.
Lucene is an open-source full-featured text search engine library.
Both Lucene indexing and search are efficient enough for our tasks.
It takes a few milliseconds to return results for a common query.
Indexing millions of search logs and listings can be done in minutes.
Reusing text search engines allows a seamless integration between query parsing and search.
We changed the tf.idf based document-term relevancy metric in Lucene to reflect P(qij|ck) using Relevancy as defined below.
where dk is a corpus of examples we collected for the concept ck; tf(qij, dk) is referred as the term frequency, the frequency of qij in dk; N is the number of entries in dk; Q is an empirically determined smoothing factor.
When tf(qij, dk) is zero for all concepts, we loosen the phrase search to be proximity search, which searches words in qij within a specific distance.
For instance, ”burlington west virginia” ∼ 5 will find entries that include these three words within 5 words of each other.
tf(qij, dk) is discounted for proximity search.
For a given qij, we allow a distance of dis(i, j) = (j − i + shift) words.
shift is a parameter that is set empirically.
The discounting formula is given in 8.
Figure 3 shows the procedure we use for parsing.
It enumerates possible segments qij of a given Q.
It then obtains P(qij|ck) using Lucene Search.
We boost pck(qij)) based on the position of qij in Q.
In our case, we simply set: boostck(i, j, n) = 3 if j = n and ck = LocationTerm.
Otherwise, boostck(i, j, n) = 1.
The algorithm searches for the best segmentation using the Viterbi algorithm.
Out-of-vocabulary words are assigned to c3 (Filler).
Word confusion networks (WCNs) is a compact lattice format (Mangu et al., 2000).
It aligns a speech lattice with its top-1 hypothesis, yielding a ”sausage”-like approximation of lattices.
It has been used in applications such as word spotting and spoken document retrieval.
In the following, we present our use of WCNs for query parsing task.
Figure 2 shows a pruned WCN example.
For each word position, there are multiple alternatives and their associated negative log posterior probabilities.
The 1-best path is “Gary Crites Springfield Missouri”.
The reference is “Dairy Queen in Springfield Missouri”.
ASR misrecognized “Dairy Queen” as “Gary Crities”.
However, the correct words “Dairy Queen” do appear in the lattice, though with lower probability.
The challenge is to select the correct words from the lattice by considering both ASR posterior probabilities and parser probabilities.
The hypotheses in WCNs have to be reranked by the Query Parser to prefer those that have meaningful concepts.
Clearly, each business name in the listing database corresponds to a single concept.
However, the long queries from query logs tend to contain multiple concepts.
For example, a frequent query is ”night club for 18 and up”.
We know ”night club” is the main subject.
And ”18 and up” is a constraint.
Without matching ”night club”, any match with ”18 and up” is meaningless.
The data fortunately can tell us which words are more likely to be a subject.
We rarely see ”18 and up” as a complete query.
Given these observations, we propose calculating the probability of a query term to be a subject.
”Subject” here specifically means a complete query or a listing name.
For the example shown in Figure 2, we observe the negative log probability for ”Dairy Queen” to be a subject is 9.3.
”Gary Crites” gets 15.3.
We refer to this probability as subject likelihood.
Given a candidate query term s = w1, w2, ..wm, we represent the subject likelihood as Psb(s).
In our experiments, we estimate Psb using relative frequency normorlized by the length of s.
We use the following formula to combine it with posterior probabilities in WCNs Pcf(s): where A is used to flatten ASR posterior probabilities and nw is the number of words in s.
In our experiments, A is set to 0.5.
We then re-rank ASR outputs based on P(s).
We will report experimental results with this approach.
”Subject” is only related to SearchTerm.
Considering this, we parse the ASR 1-best out first and keep the Location terms extracted as they are.
Only word alternatives corresponding to the search terms are used for reranking.
This also improves speed, since we make the confusion network lattice much smaller.
In our initial investigations, such an approach yields promising results as illustrated in the experiment section.
Another capability that the parser does for both ASR 1-best and lattices is spelling correction.
It corrects words such as restaurants to restaurants.
ASR produces spelling errors because the language model is trained on query logs.
We need to make more efforts to clean up the query log database, though progresses had been made.
<newSection> 5 Finite-state Transducer-based Parser In this section, we present an alternate method for parsing which can transparently scale to take as input word lattices from ASR.
We encode the problem of parsing as a weighted finite-state transducer (FST).
This encoding allows us to apply the parser on ASR 1-best as well as ASR WCNs using the composition operation of FSTs.
We formulate the parsing problem as associating with each token of the input a label indicating whether that token belongs to one of a business listing (bl), city/state (cs) or neither (null).
Thus, given a word sequence (W = w1, ... , wn) output from ASR, we search of the most likely label sequence (T = t1, ... , tn), as shown in Equation 9.
We use the joint probability P(W, T) and approximate it using an k-gram model as shown in Equations 10,11.
A k-gram model can be encoded as a weighted finite-state acceptor (FSA) (Allauzen et al., 2004).
The states of the FSA correspond to the k-gram histories, the transition labels to the pair (wi, ti) and the weights on the arcs are −log(P(wi, ti | arcs for purposes of smoothing with lower order k-grams.
An annotated corpus of words and labels is used to estimate the weights of the FSA.
A sample corpus is shown in Table 1.
The FSA on the joint alphabet is converted into an FST.
The paired symbols (wi7 ti) are reinter-preted as consisting of an input symbol wi and output symbol ti.
The resulting FST (M) is used to parse the 1-best ASR (represented as FSTs (I)), using composition of FSTs and a search for the lowest weight path as shown in Equation 12.
The output symbol sequence (7r2) from the lowest weight path is T*.
Equation 12 shows a method for parsing the 1-best ASR output using the FST.
However, a similar method can be applied for parsing WCNs.
The WCN arcs are associated with a posterior weight that needs to be scaled suitably to be comparable to the weights encoded in M.
We represent the result of scaling the weights in WCN by a factor of A as WCNλ.
The value of the scaling factor is determined empirically.
Thus the process of parsing a WCN is represented by Equation 13.
<newSection> 6 Experiments We have access to text query logs consisting of 18 million queries to the two text fields: SearchTerm and LocationTerm.
In addition to these logs, we have access to 11 million unique business listing names and their addresses.
We use the combined data to train the parameters of the two parsing models as discussed in the previous sections.
We tested our approaches on three data sets, which in total include 2686 speech queries.
These queries were collected from users using mobile devices from different time periods.
Labelers transcribed and annotated the test data using SearchTerm and LocationTerm tags.
We use an ASR with a trigram-based language model trained on the query logs.
Table 2 shows the ASR word accuracies on the three data sets.
The accuracy is the lowest on Test1, in which many users were non-native English speakers and a large percentage of queries are not intended for local search.
We measure the parsing performance in terms of extraction accuracy on the two non-filler slots: SearchTerm and LocationTerm.
Extraction accuracy computes the percentage of the test set where the string identified by the parser for a slot is exactly the same as the annotated string for that slot.
Table 3 reports parsing performance using the PARIS approach for the two slots.
The “Tran-scription” columns present the parser’s performances on human transcriptions (i.e. word accuracy=100%) of the speech.
As expected, the parser’s performance heavily relies on ASR word accuracy.
We achieved lower parsing performance on Test1 compared to other test sets due to lower ASR accuracy on this test set.
The promising aspect is that we consistently improved SearchTerm extraction accuracy when using WCN as input.
The performance under “Oracle path” column shows the upper bound for the parser using the oracle path2 from the WCN.
We pruned the WCN by keeping only those arcs that are within cthresh of the lowest cost arc between two states.
Cthresh = 4 is used in our experiments.
For Test2, the upper bound improvement is 7.6% (82.5%-74.9%) absolute.
Our proposed approach using pruned WCN achieved 2.7% improvement, which is 35% of the maximum potential gain.
We observed smaller improvements on Test1 and Test3.
Our approach did not take advantage of WCN for LocationTerm extraction, hence we obtained the same performance with WCNs as using ASR 1-best.
In Table 4, we report the parsing performance for the FST-based approach.
We note that the FST-based parser on a WCN also improves the SearchTerm and LocationTerm extraction accuracy over ASR 1-best, an improvement of about 1.5%.
The accuracies on the oracle path and the transcription are slightly lower with the FST-based parser than with the PARIS approach.
The performance gap, however, is bigger on ASR 1-best.
The main reason is PARIS has embedded a module for spelling correction that is not included in the FST approach.
For instance, it corrects nieman to neiman.
These improvements from spelling correction don’t contribute much to search performance as we will see below, since the search engine is quite robust to spelling errors.
ASR generates spelling errors because the language model is trained using query logs, where misspellings are frequent.
We evaluated the impact of parsing performance on search accuracy.
In order to measure search accuracy, we need to first collect a reference set of search results for our test utterances.
For this purpose, we submitted the human annotated two-field data to the search engine (http://www.yellowpages.com/) and extracted the top 5 results from the returned pages.
The returned search results are either business categories such as “Chinese Restaurant” or business listings including business names and addresses.
We considered these results as the reference search results for our test utterances.
In order to evaluate our voice search system, we submitted the two fields resulting from the query parser on the ASR output (1-best/WCN) to the search engine.
We extracted the top 5 results from the returned pages and we computed the Precision, Recall and F1 scores between this set of results and the reference search set.
Precision is the ratio of relevant results among the top 5 results the voice search system returns.
Recall refers to the ratio of relevant results to the reference search result set.
F1 combines precision and recall as: (2 * Recall * Precision) / (Recall + Precision) (van Rijsbergen, 1979).
In Table 5 and Table 6, we report the search performance using PARIS and FST approaches.
The overall improvement in search performance is not as large as the improvement in the slot accuracies between using ASR 1-best and WCNs.
On Test1, we obtained higher recall but lower precision with WCN resulting in a slight decrease in F1 score.
For both approaches, we observed that using WCNs consistently improves recall but not precision.
Although this might be counterintu-itive, given that WCNs improve the slot accuracy overall.
One possible explanation is that we have observed errors made by the parser using WCNs are more “severe” in terms of their relationship to the original queries.
For example, in one particular case, the annotated SearchTerm is “book stores”, for which the ASR 1-best-based parser returned “books” (due to ASR error) as the SearchTerm, while the WCN-based parser identified “banks” as the SearchTerm.
As a result, the returned results from the search engine using the 1-best-based parser were more relevant compared to the results returned by the WCN-based parser.
There are few directions that this observation suggests.
First, the weights on WCNs may need to be scaled suitably to optimize the search performance as opposed to the slot accuracy performance.
Second, there is a need for tighter coupling between the parsing and search components as the eventual goal for models of voice search is to improve search accuracy and not just the slot accuracy.
We plan to investigate such questions in future work.
<newSection> 7 Summary This paper describes two methods for query parsing.
The task is to parse ASR output including 1-best and lattices into database or search fields.
In our experiments, these fields are SearchTerm and LocationTerm for local search.
Our first method, referred to as PARIS, takes advantage of a generic search engine (for text indexing and search) for parsing.
All probabilities needed are retrieved on-the-fly.
We used keyword search, phrase search and proximity search.
The second approach, referred to as FST-based parser, which encodes the problem of parsing as a weighted finite-state transduction (FST).
Both PARIS and FST successfully exploit multiple hypotheses and posterior probabilities from ASR encoded as word confusion networks and demonstrate improved accuracy.
These results show the benefits of tightly coupling ASR and the query parser.
Furthermore, we evaluated the effects of this improvement on search performance.
We observed that the search accuracy improves using word confusion networks.
However, the improvement on search is less than the improvement we obtained on parsing performance.
Some improvements the parser achieves do not contribute to search.
This suggests the need of coupling the search module and the query parser as well.
The two methods, namely PARIS and FST, achieved comparable performances on search.
One advantage with PARIS is the fast training process, which takes minutes to index millions of query logs and listing entries.
For the same amount of data, FST needs a number of hours to train.
The other advantage is PARIS can easily use proximity search to loosen the constrain of N-gram models, which is hard to be implemented using FST.
FST, on the other hand, does better smoothing on learning probabilities.
It can also more directly exploit ASR lattices, which essentially are represented as FST too.
For future work, we are interested in ways of harnessing the benefits of the both these approaches.
<newSection> References<newSection> Abstract In this paper we present a human-based evaluation of surface realisation alternatives.
We examine the relative rankings of naturally occurring corpus sentences and automatically generated strings chosen by statistical models (language model, log-linear model), as well as the naturalness of the strings chosen by the log-linear model.
We also investigate to what extent preceding context has an effect on choice.
We show that native speakers do accept quite some variation in word order, but there are also clearly factors that make certain realisation alternatives more natural.
<newSection> 1 Introduction An important component of research on surface realisation (the task of generating strings for a given abstract representation) is evaluation, especially if we want to be able to compare across systems.
There is consensus that exact match with respect to an actually observed corpus sentence is too strict a metric and that BLEU score measured against corpus sentences can only give a rough impression of the quality of the system output.
It is unclear, however, what kind of metric would be most suitable for the evaluation of string realisations, so that, as a result, there have been a range of automatic metrics applied including inter alia exact match, string edit distance, NIST SSA, BLEU, NIST, ROUGE, generation string accuracy, generation tree accuracy, word accuracy (Bangalore et al., 2000; Callaway, 2003; Nakanishi et al., 2005; Velldal and Oepen, 2006; Belz and Reiter, 2006).
It is not always clear how appropriate these metrics are, especially at the level of individual sentences.
Using automatic evaluation metrics cannot be avoided, but ideally, a metric for the evaluation of realisation rankers would rank alternative realisations in the same way as native speakers of the language for which the surface realisation system is developed, and not only globally, but also at the level of individual sentences.
Another major consideration in evaluation is what to take as the gold standard.
The easiest option is to take the original corpus string that was used to produce the abstract representation from which we generate.
However, there may well be other realisations of the same input that are as suitable in the given context.
Reiter and Sripada (2002) argue that while we should take advantage of large corpora in NLG, we also need to take care that we do not introduce errors by learning from incorrect data present in corpora.
In order to better understand what makes good evaluation data (and metrics), we designed and implemented an experiment in which human judges evaluated German string realisations.
The main aims of this experiment were: (i) to establish how much variation in German word order is acceptable for human judges, (ii) to find an automatic evaluation metric that mirrors the findings of the human evaluation, (iii) to provide detailed feedback for the designers of the surface realisation ranking model and (iv) to establish what effect preceding context has on the choice of realisation.
In this paper, we concentrate on points (i) and (iv).
The remainder of the paper is structured as follows: In Section 2 we outline the realisation ranking system that provided the data for the experiment.
In Section 3 we outline the design of the experiment and in Section 4 we present our findings.
In Section 5 we relate this to other work and finally we conclude in Section 6.
<newSection> 2 A Realisation Ranking System for German We take the realisation ranking system for German described in Cahill et al.
(2007) and present the output to human judges.
One goal of this series of experiments is to examine whether the results based on automatic evaluation metrics published in that paper are confirmed in an evaluation by humans.
Another goal is to collect data that will allow us and other researchers1 to explore more fine-grained and reliable automatic evaluation metrics for realisation ranking.
The system presented by Cahill et al.
(2007) ranks the strings generated by a hand-crafted broad-coverage Lexical Functional Grammar (Bresnan, 2001) for German (Rohrer and Forst, 2006) on the basis of a given input f-structure.
In these experiments, we use f-structures from their held-out and test sets, of which 96% can be associated with surface realisations by the grammar.
F-structures are attribute-value matrices representing grammatical functions and morphosyntactic features; roughly speaking, they are predicate-argument structures.
In LFG, f-structures are assumed to be a crosslinguistically relatively parallel syntactic representation level, alongside the more surface-oriented c-structures, which are context-free trees.
Figure 1 shows the f-structure2 associated with TIGER Corpus sentence 8609, glossed in (1), as well as the 4 string realisations that the German LFG generates from this f-structure.
The LFG is reversible, i.e. the same grammar is used for parsing as for generation.
It is a hand-crafted grammar, and has been carefully constructed to only parse (and therefore generate) grammatical strings.3 ¨außerst extremely umstritten. controversial.
‘Williams was extremely controversial in British politics.’
The ranker consists of a log-linear model that is based on linguistically informed structural features as well as a trigram language model, whose score is integrated into the model simply as an additional feature.
The log-linear model is trained on corpus data, in this case sentences from the TIGER Corpus (Brants et al., 2002), for which f-structures are available; the observed corpus sentences are considered as references whose probability is to be maximised during the training process.
The output of the realisation ranker is evaluated in terms of exact match and BLEU score, both measured against the actually observed corpus sentences.
In addition to the figures achieved by the ranker, the corresponding figures achieved by the employed trigram language model on its own are given as a baseline, and the exact match figure of the best possible string selection is given as an upper bound.4 We summarise these figures in Table 1. and log-linear model ranker in Cahill et al.
(2007) By means of these figures, Cahill et al.
(2007) show that a log-linear model based on structural features and a language model score performs considerably better realisation ranking than just a language model.
In our experiments, presented in detail in the following section, we examine whether human judges confirm this and how natural and/or acceptable the selection performed by the realisation ranker under consideration is for German native speakers.
<newSection> 3 Experiment Design The experiment was divided into three parts.
Each part took between 30 and 45 minutes to complete, and participants were asked to leave some time (e.g. a week) between each part.
In total, 24 participants completed the experiment.
All were native German speakers (mostly from South-Western Germany) and almost all had a linguistic background.
Table 2 gives a breakdown of the items in each part of the experiment.5 &quot;Williams war in der britischen Politik äußerst umstritten.&quot; Williams war in der britischen Politik ¨außerst umstritten.
In der britischen Politik war Williams ¨außerst umstritten.
¨Außerst umstritten war Williams in der britischen Politik.
¨Außerst umstritten war in der britischen Politik Williams.
The aim of part 1 of the experiment was twofold.
First, to identify the relative rankings of the systems evaluated in Cahill et al.
(2007) according to the human judges, and second to evaluate the quality of the strings as chosen by the log-linear model of Cahill et al.
(2007).
To these ends, part 1 was further subdivided into two tasks: 1a and b.
Task 1a: During the first task, participants were presented with alternative realisations for an input f-structure (but not shown the original f-structure) and asked to rank them in order of how natural sounding they were, 1 being the best and 3 being the worst.6 Each item contained three alternatives, (i) the original string found in TIGER, (ii) the string chosen as most likely by the trigram language model, and (iii) the string chosen as most likely by the log-linear model.
Only items where each system chose a different alternative were chosen from the evaluation data of Cahill et al.
(2007).
The three alternatives were presented in random order for each item, and the items were presented in random order for each participant.
Some items were presented randomly to participants more than once as a sanity check, and in total for Part 1a, participants made 52 ranking judgements on 44 items.
Figure 2 shows a screen shot of what the participant was presented with for this task.
Task 1b: In the second task of part 1, participants were presented with the string chosen by the log-linear model as being the most likely and asked to evaluate it on a scale from 1 to 5 on how natural sounding it was, 1 being very unnatural or marked and 5 being completely natural.
Figure 3 shows a screen shot of what the participant saw during the experiment.
Again some random items were presented to the participant more than once, and the items themselves were presented in random order.
In total, the participants made 58 judgements on 52 items.
In the second part of the experiment, participants were presented between 4 and 8 alternative surface realisations for an input f-structure, as well as some preceding context.
This preceding context was automatically determined using information from the export release of the TIGER treebank and was not hand-checked for relevance.7 The participants were then asked to choose the realisation that they felt fit best given the preceding sentences.
The items were presented in random order, and the list of alternatives were presented in random order to each participant.
Some items were randomly presented more than once, resulting in 50 judgements on 41 items.
Figure 4 shows a screen shot of what the participant saw.
Part 3 of the experiment was identical to Part 1, except that now, rather than the participants being presented with sentences in isolation, they were given some preceding context.
The context was determined automatically, in the same way as in Part 2.
The items themselves were the same as in Part 1.
The aim of this part of the experiment was to see what effect preceding context had on judgements.
<newSection> 4 Results In this section we present the result and analysis of the experiments outlined above.
The data collected in Experiment 1a showed the overall human relative ranking of the three systems.
We calculate the total numbers of each rank for each system.
Table 3 summarises the results.
The original string is the string found in the TIGER Corpus, the LM String is the string chosen as being most likely by the trigram language model and the LL String is the string chosen as being most likely by the log-linear model.
Table 3 confirms the overall relative rankings of the three systems as determined using BLEU scores.
The original TIGER strings are ranked best (average 1.4), the strings chosen by the log-linear model are ranked better than the strings chosen by the language model (average 2.65 vs 2.04).
In Experiment 1b, the aim was to find out how acceptable the strings chosen by the log-linear model were, although they were not the same as the original string.
Figure 5 summarises the data.
The graph shows that the majority of strings chosen by the log-linear model ranked very highly on the naturalness scale.
In Experiment 2, the aim was to find out how often the human judges chose the same string as the original author (given alternatives generated by the LFG grammar).
Most items had between 4 and 6 alternative strings.
In 70% of all items, the human judges chose the same string as the original author.
However, the remaining 30% of the time, the human judges picked an alternative as being the most fitting in the given context.8 This suggests that there is quite some variation in what native German speakers will accept, but that this variation is by no means random, as indicated by 70% of choices being the same string as the original author’s.
Figure 6 shows for each bin of possible alternatives, the percentage of items with a given number of choices made.
For example, for the items with 4 possible alternatives, over 70% of the time, the judges chose between only 2 of them.
For the items with 5 possible alternatives, in 10% of those items the human judges chose only 1 of those alternatives; in 30% of cases, the human judges all chose the same 2 solutions, and for the remaining 60% they chose between only 3 of the 5 possible alternatives.
These figures indicate that although judges could not always agree on one best string, often they were only choosing between 2 or 3 of the possible alternatives.
This suggests that, on the one hand, native speakers do accept quite some variation, but that, on the other hand, there are clearly factors that make certain realisation alternatives more preferable than others.
8Recall that almost all strings presented to the judges were grammatical.
The graph in Figure 6 shows that only in two cases did the human judges choose from among all possible alternatives.
In one case, there were 4 possible alternatives and in the other 6.
The original sentence that had 4 alternatives is given in (2).
The four alternatives that participants were asked to choose from are given in Table 4, with the frequency of each choice.
The original sentence that had 6 alternatives is given in (3).
The six alternatives generated by the grammar and the frequencies with which they were chosen is given in Table 5.
(2) Die Brandursache blieb zun¨achst unbekannt.
The cause of fire remained initially unknown.
‘The cause of the fire remained unknown initially.’
for (2) and their frequencies Tables 4 and 5 tell different stories.
On the one hand, although each of the 4 alternatives was chosen at least once from Table 4, there is a clear preference for one string (and this is also the original string from the TIGER Corpus).
On the other hand, there is no clear preference9 for any one of the alternatives in Table 5, and, in fact, the alternative that was selected most frequently by the participants is not the original string.
Interestingly, out of the 41 items presented to participants, the original string was chosen by the majority of participants in 36 cases.
Again, this confirms the hypothesis that there is a certain amount of acceptable variation for native speakers but there are clear preferences for certain strings over others.
As explained in Section 3.1, Part 3 of our experiment was identical to Part 1, except that the participants could see some preceding context.
The aim of this part was to investigate to what extent discourse factors influence the way in which human judges evaluate the output of the realisation ranker.
In Task 3a, we expected the original strings to be ranked (even) higher in context than out of context; consequently, the ranks of the realisations selected by the log-linear and the language model would have to go down.
With respect to Task 3b, we had no particular expectation, but were just interested in seeing whether some preceding context would affect the evaluation results for the strings selected as most probable by the log-linear model ranker in any way.
Table 6 summarises the results of Task 3a.
It shows that, at least overall, our expectation that the original corpus sentences would be ranked higher within context than out of context was not borne out.
Actually, they were ranked a bit lower than they were when presented in isolation, and the only realisations that are ranked slightly higher overall are the ones selected by the trigram LM.
The overall results of Task 3b are presented in Figure 7.
Interestingly, although we did not expect any particular effect of preceding context on the way the participants would rate the realisations selected by the log-linear model, the naturalness scores were higher in the condition with context (Task 3b) than in the one without context (Task 1b).
One explanation might be that sentences in some sort of default order are generally rated higher in context than out of context, simply because the context makes sentences less surprising.
Since, contrary to our expectations, we could not detect a clear effect of context in the overall results of Task 3a, we investigated how the average ranks of the three alternatives presented for individual items differ between Task 1a and Task 3a.
An example of an original corpus sentence which many participants ranked higher in context than in isolation is given in (4a.).
The realisations selected by the the log-linear model and the trigram LM are given in (4b.) and (4c.) respectively, and the context shown to the participants is given above these alternatives.
We believe that the context has this effect because it prepares the reader for the structure with the sentence-initial predicative participle entscheidend; usually, these elements appear rather in clause-final position.
In contrast, (5a) is an example of a corpus for strings chosen by log-linear model, presented without and with context sentence which our participants tended to rank lower in context than in isolation.
Actually, the human judges preferred the realisation selected by the trigram LM to the original sentence and the realisation chosen by the log-linear model in both conditions, but this preference was even reinforced when context was available.
One explanation might be that the two preceding sentences are precisely about the decision to which the initial phrase of variant (5b) refers, which ensures a smooth flow of the discourse.
We measure two types of annotator agreement.
First we measure how well each annotator agrees with him/herself.
This is done by evaluating what percentage of the time an annotator made the same choice when presented with the same item choices (recall that as described in Section 3, a number of items were presented randomly more than once to each participant).
The results are given in Table 7.
The results show that in between 70% and 74% of cases, judges make the same decision when presented with the same data.
We found this to be a surprisingly low number and think that it is most likely due to the acceptable variation in word order for speakers.
Another measure of agreement is how well the individual participants agree with each other.
In order to establish this, we calculate an average Spearman’s correlation coefficient (non-parametric Pearson’s correlation coefficient) between each participant for each experiment.
The results are summarised in Table 8.
Although these figures indicate a high level of inter-annotator agreement, more tests are required to establish exactly what these figures mean for each experiment.
<newSection> 5 Related Work The work that is most closely related to what is presented in this paper is that of Velldal (2008).
In his thesis several models of realisation ranking are presented and evaluated against the original corpus text.
Chapter 8 describes a small human-based experiment, where 7 native English speakers rank the output of 4 systems.
One system is the original text, another is a randomly chosen baseline, another is a string chosen by a log-linear model and the fourth is one chosen by a language model.
Joint rankings were allowed.
The results presented in Velldal (2008) mirror our findings in Experiments 1a and 3a, that native speakers rank the original strings higher than the log-linear model strings which are ranked higher than the language model strings.
In both cases, the log-linear models include the language model score as a feature in the log-linear model.
Nakanishi et al.
(2005) report that they achieve the best BLEU scores when they do not include the language model score in their log-linear model, but they also admit that their language model was not trained on enough data.
Belz and Reiter (2006) carry out a comparison of automatic evaluation metrics against human domain experts and human non-experts in the domain of weather forecast statements.
In their evaluations, the NIST score correlated more closely than BLEU or ROUGE to the human judgements.
They conclude that more than 4 reference texts are needed for automatic evaluation of NLG systems.
<newSection> 6 Conclusion and Outlook to Future Work In this paper, we have presented a human-based experiment to evaluate the output of a realisation ranking system for German.
We evaluated the original corpus text, and strings chosen by a language model and a log-linear model.
We found that, at a global level, the human judgements mirrored the relative rankings of the three system according to the BLEU score.
In terms of naturalness, the strings chosen by the log-linear model were generally given 4 or 5, indicating that although the log-linear model might not choose the same string as the original author had written, the strings it was choosing were mostly very natural strings.
When presented with all alternatives generated by the grammar for a given input f-structure, the human judges chose the same string as the original author 70% of the time.
In 5 out of 41 cases, the majority of judges chose a string other than the original string.
These figures show that native speakers accept some variation in word order, and so caution should be exercised when using corpus-derived reference data.
The observed acceptable variation was often linked to information structural considerations, and further experiments will be carried out to investigate this relationship between word order and information structure.
In examining the effect of preceding context, we found that overall context had very little effect.
At the level of individual sentences, however, clear tendencies were observed, but there were some sentences which were judged better in context and others which were ranked lower.
This again indicates that corpus-derived reference data should be used with caution.
An obvious next step is to examine how well automatic metrics correlate with the human judgements collected, not only at an individual sentence level, but also at a global level.
This can be done using statistical techniques to correlate the human judgements with the scores from the automatic metrics.
We will also examine the sentences that were consistently judged to be of poor quality, so that we can provide feedback to the developers of the log-linear model in terms of possible additional features for disambiguation.
<newSection> Acknowledgments We are extremely grateful to all of our participants for taking part in this experiment.
This work was partly funded by the Collaborative Research Centre (SFB 732) at the University of Stuttgart.
<newSection> References<newSection> Abstract As empirically demonstrated by the last SensEval exercises, assigning the appropriate meaning to words in context has resisted all attempts to be successfully addressed.
One possible reason could be the use of inappropriate set of meanings.
In fact, WordNet has been used as a de-facto standard repository of meanings.
However, to our knowledge, the meanings represented by WordNet have been only used for WSD at a very fine-grained sense level or at a very coarse-grained class level.
We suspect that selecting the appropriate level of abstraction could be on between both levels.
We use a very simple method for deriving a small set of appropriate meanings using basic structural properties of WordNet.
We also empirically demonstrate that this automatically derived set of meanings groups senses into an adequate level of abstraction in order to perform class-based Word Sense Disambiguation, allowing accuracy figures over 80%.
<newSection> 1 Introduction Word Sense Disambiguation (WSD) is an intermediate Natural Language Processing (NLP) task which consists in assigning the correct semantic interpretation to ambiguous words in context.
One of the most successful approaches in the last years is the supervised learning from examples, in which statistical or Machine Learning classification models are induced from semantically annotated corpora (M`arquez et al., 2006).
Generally, supervised systems have obtained better results than the unsupervised ones, as shown by experimental work and international evaluation exercises such This paper has been supported by the European Union under the projects QALL-ME (FP6 IST-033860) and KYOTO (FP7 ICT-211423), and the Spanish Government under the project Text-Mess (TIN2006-15265-C06-01) and KNOW (TIN2006-15049-C03-01) as Senseval1.
These annotated corpora are usually manually tagged by lexicographers with word senses taken from a particular lexical semantic resource –most commonly WordNet2 (WN) (Fell-baum, 1998).
WN has been widely criticized for being a sense repository that often provides too fine–grained sense distinctions for higher level applications like Machine Translation or Question & Answering.
In fact, WSD at this level of granularity has resisted all attempts of inferring robust broad-coverage models.
It seems that many word–sense distinctions are too subtle to be captured by automatic systems with the current small volumes of word–sense annotated examples.
Possibly, building class-based classifiers would allow to avoid the data sparseness problem of the word-based approach.
Recently, using WN as a sense repository, the organizers of the English all-words task at SensEval-3 reported an inter-annotation agreement of 72.5% (Snyder and Palmer, 2004).
Interestingly, this result is difficult to outperform by state-of-the-art sense-based WSD systems.
Thus, some research has been focused on deriving different word-sense groupings to overcome the fine–grained distinctions of WN (Hearst and Sch¨utze, 1993), (Peters et al., 1998), (Mihalcea and Moldovan, 2001), (Agirre and LopezDeLa-Calle, 2003), (Navigli, 2006) and (Snow et al., 2007).
That is, they provide methods for grouping senses of the same word, thus producing coarser word sense groupings for better disambiguation.
Wikipedia3 has been also recently used to overcome some problems of automatic learning methods: excessively fine–grained definition of meanings, lack of annotated data and strong domain dependence of existing annotated corpora.
In this way, Wikipedia provides a new very large source of annotated data, constantly expanded (Mihalcea, 2007).
In contrast, some research have been focused on using predefined sets of sense-groupings for learning class-based classifiers for WSD (Segond et al., 1997), (Ciaramita and Johnson, 2003), (Villarejo et al., 2005), (Curran, 2005) and (Ciaramita and Altun, 2006).
That is, grouping senses of different words into the same explicit and comprehensive semantic class.
Most of the later approaches used the original Lexicographical Files of WN (more recently called SuperSenses) as very coarse–grained sense distinctions.
However, not so much attention has been paid on learning class-based classifiers from other available sense–groupings such as WordNet Domains (Magnini and Cavagli`a, 2000), SUMO labels (Niles and Pease, 2001), EuroWordNet Base Concepts (Vossen et al., 1998), Top Concept Ontology labels (Alvez et al., 2008) or Basic Level Concepts (Izquierdo et al., 2007).
Obviously, these resources relate senses at some level of abstraction using different semantic criteria and properties that could be of interest for WSD.
Possibly, their combination could improve the overall results since they offer different semantic perspectives of the data.
Furthermore, to our knowledge, to date no comparative evaluation has been performed on SensEval data exploring different levels of abstraction.
In fact, (Villarejo et al., 2005) studied the performance of class–based WSD comparing only SuperSenses and SUMO by 10–fold cross–validation on SemCor, but they did not provide results for SensEval2 nor SensEval3.
This paper empirically explores on the supervised WSD task the performance of different levels of abstraction provided by WordNet Domains (Magnini and Cavagli`a, 2000), SUMO labels (Niles and Pease, 2001) and Basic Level Concepts (Izquierdo et al., 2007).
We refer to this approach as class–based WSD since the classifiers are created at a class level instead of at a sense level.
Class-based WSD clusters senses of different words into the same explicit and comprehen-sive grouping.
Only those cases belonging to the same semantic class are grouped to train the classifier.
For example, the coarser word grouping obtained in (Snow et al., 2007) only has one remaining sense for “church”.
Using a set of Base Level Concepts (Izquierdo et al., 2007), the three senses of “church” are still represented by faith.n#3, building.n#1 and religious ceremony.n#1.
The contribution of this work is threefold.
We empirically demonstrate that a) Basic Level Concepts group senses into an adequate level of abstraction in order to perform supervised class– based WSD, b) that these semantic classes can be successfully used as semantic features to boost the performance of these classifiers and c) that the class-based approach to WSD reduces dramatically the required amount of training examples to obtain competitive classifiers.
After this introduction, section 2 presents the sense-groupings used in this study.
In section 3 the approach followed to build the class–based system is explained.
Experiments and results are shown in section 4.
Finally some conclusions are drawn in section 5.
<newSection> 2 Semantic Classes WordNet (Fellbaum, 1998) synsets are organized in forty five Lexicographer Files, more recetly called SuperSenses, based on open syntactic categories (nouns, verbs, adjectives and adverbs) and logical groupings, such as person, phenomenon, feeling, location, etc.
There are 26 basic categories for nouns, 15 for verbs, 3 for adjectives and 1 for adverbs.
WordNet Domains4 (Magnini and Cavagli`a, 2000) is a hierarchy of 165 Domain Labels which have been used to label all WN synsets.
Information brought by Domain Labels is complementary to what is already in WN.
First of all a Domain Labels may include synsets of different syntactic categories: for instance MEDICINE groups together senses from nouns, such as doctor and hospital, and from Verbs such as to operate.
Second, a Domain Label may also contain senses from different WordNet subhierarchies.
For example, SPORT contains senses such as athlete, deriving from life form, game equipment, from physical object, sport from act, and playing field, from location.
SUMO5 (Niles and Pease, 2001) was created as part of the IEEE Standard Upper Ontology Working Group.
The goal of this Working Group is to develop a standard upper ontology to promote data interoperability, information search and retrieval, automated inference, and natural language processing.
SUMO consists of a set of concepts, relations, and axioms that formalize an upper ontology.
For these experiments, we used the complete WN1.6 mapping with 1,019 SUMO labels.
Basic Level Concepts6 (BLC) (Izquierdo et al., 2007) are small sets of meanings representing the whole nominal and verbal part of WN.
BLC can be obtained by a very simple method that uses basic structural WN properties.
In fact, the algorithm only considers the relative number of relations of each synset along the hypernymy chain.
The process follows a bottom-up approach using the chain of hypernymy relations.
For each synset in WN, the process selects as its BLC the first local maximum according to the relative number of relations.
The local maximum is the synset in the hypernymy chain having more relations than its immediate hyponym and immediate hypernym.
For synsets having multiple hypernyms, the path having the local maximum with higher number of relations is selected.
Usually, this process finishes having a number of preliminary BLC.
Obviously, while ascending through this chain, more synsets are subsumed by each concept.
The process finishes checking if the number of concepts subsumed by the preliminary list of BLC is higher than a certain threshold.
For those BLC not representing enough concepts according to the threshold, the process selects the next local maximum following the hypernymy hierarchy.
Thus, depending on the type of relations considered to be counted and the threshold established, different sets of BLC can be easily obtained for each WN version.
In this paper, we empirically explore the performance of the different levels of abstraction provided by Basic Level Concepts (BLC) (Izquierdo et al., 2007).
Table 1 presents the total number of BLC and its average depth for WN1.6, varying the threshold and the type of relations considered (all relations or only hyponymy).
<newSection> 3 Class-based WSD We followed a supervised machine learning approach to develop a set of class-based WSD taggers.
Our systems use an implementation of a Support Vector Machine algorithm to train the classifiers (one per class) on semantic annotated corpora for acquiring positive and negative examples of each class and on the definition of a set of features for representing these examples.
The system decides and selects among the possible semantic classes defined for a word.
In the sense approach, one classifier is generated for each word sense, and the classifiers choose between the possible senses for the word.
The examples to train a single classifier for a concrete word are all the examples of this word sense.
In the semantic–class approach, one classifier is generated for each semantic class.
So, when we want to label a word, our program obtains the set of possible semantic classes for this word, and then launch each of the semantic classifiers related with these semantic categories.
The most likely category is selected for the word.
In this approach, contrary to the word sense approach, to train a classifier we can use all examples of all words belonging to the class represented by the classifier.
In table 2 an example for a sense of “church” is shown.
We think that this approach has several advantages.
First, semantic classes reduce the average polysemy degree of words (some word senses are grouped together within the same class).
Moreover, the well known problem of acquisition bottleneck in supervised machine learning algorithms is attenuated, because the number of examples for each classifier is increased.
Support Vector Machines (SVM) have been proven to be robust and very competitive in many NLP tasks, and in WSD in particular (M`arquez et al., 2006).
For these experiments, we used SVM-Light (Joachims, 1998).
SVM are used to learn an hyperplane that separates the positive from the negative examples with the maximum margin.
It means that the hyperplane is located in an intermediate position between positive and negative examples, trying to keep the maximum distance to the closest positive example, and to the closest negative example.
In some cases, it is not possible to get a hyperplane that divides the space linearly, or it is better to allow some errors to obtain a more efficient hyperplane.
This is known as “soft-margin SVM”, and requires the estimation of a parameter (C), that represent the trade-off allowed between training errors and the margin.
We have set this value to 0.01, which has been proved as a good value for SVM in WSD tasks.
When classifying an example, we obtain the value of the output function for each SVM classifier corresponding to each semantic class for the word example.
Our system simply selects the class with the greater value.
Three semantic annotated corpora have been used for training and testing.
SemCor has been used for training while the corpora from the English all-words tasks of SensEval-2 and SensEval-3 has been used for testing.
We also considered SemEval-2007 coarse–grained task corpus for testing, but this dataset was discarded because this corpus is also annotated with clusters of word senses.
SemCor (Miller et al., 1993) is a subset of the Brown Corpus plus the novel The Red Badge of Courage, and it has been developed by the same group that created WordNet.
It contains 253 texts and around 700,000 running words, and more than 200,000 are also lemmatized and sense-tagged according to Princeton WordNet 1.6.
SensEval-27 English all-words corpus (here-inafter SE2) (Palmer et al., 2001) consists on 5,000 words of text from three WSJ articles representing different domains from the Penn TreeBank II.
The sense inventory used for tagging is WordNet 1.7.
Finally, SensEval-38 English all-words corpus (hereinafter SE3) (Snyder and Palmer, 2004), is made up of 5,000 words, extracted from two WSJ articles and one excerpt from the Brown Corpus.
Sense repository of WordNet 1.7.1 was used to tag 2,041 words with their proper senses.
We have defined a set of features to represent the examples according to previous works in WSD and the nature of class-based WSD.
Features widely used in the literature as in (Yarowsky, 1994) have been selected.
These features are pieces of information that occur in the context of the target word, and can be organized as: Local features: bigrams and trigrams that contain the target word, including part-of-speech (PoS), lemmas or word-forms.
Topical features: word–forms or lemmas appearing in windows around the target word.
In particular, our systems use the following basic features: Word–forms and lemmas in a window of 10 words around the target word PoS: the concatenation of the preceding/following three/five PoS Bigrams and trigrams formed by lemmas and word-forms and obtained in a window of 5 words.
We use of all tokens regardless their PoS to build bi/trigrams.
The target word is replaced by X in these features to increase the generalization of them for the semantic classifiers Moreover, we also defined a set of Semantic Features to explode different semantic resources in order to enrich the set of basic features: Most frequent semantic class calculated over SemCor, the most frequent semantic class for the target word.
classes of the monosemous words arround the target word in a window of size 5.
Several types of semantic classes have been considered to create these features.
In particular, two different sets of BLC (BLC20 and BLC509), SuperSenses, WordNet Domains (WND) and SUMO.
In order to increase the generalization capabilities of the classifiers we filter out irrelevant features.
We measure the relevance of a feature10.
f for a class c in terms of the frequency off.
For each class c, and for each feature f of that class, we calculate the frequency of the feature within the class (the number of times that it occurs in examples of the class), and also obtain the total frequency of the feature, for all the classes.
We divide both values (classFreq / totalFreq) and if the result is not greater than a certain threshold t, the feature is removed from the feature list of the class c11.
In this way, we ensure that the features selected for a class are more frequently related with that class than with others.
We set this threshold t to 0.25, obtained empirically with very preliminary versions of the classifiers on SensEval3 test.
<newSection> 4 Experiments and Results To analyze the influence of each feature type in the class-based WSD, we designed a large set of experiments.
An experiment is defined by two sets of semantic classes.
First, the semantic class type for selecting the examples used to build the classifiers (determining the abstraction level of the system).
In this case, we tested: sense12, BLC20, BLC50, WordNet Domains (WND), SUMO and Super-Sense (SS).
Second, the semantic class type used for building the semantic features.
In this case, we tested: BLC20, BLC50, SuperSense, WND and SUMO.
Combining them, we generated the set of experiments described later.
Table 3 presents the average polysemy on SE2 and SE3 of the different semantic classes.
The most frequent classes (MFC) of each word calculated over SemCor are considered to be the baselines of our systems.
Ties between classes on a specific word are solved obtaining the global frequency in SemCor of each of these tied classes, and selecting the more frequent class over the whole training corpus.
When there are no occurrences of a word of the test corpus in SemCor (we are not able to calculate the most frequent class of the word), we obtain again the global frequency for each of its possible semantic classes (obtained 11Depending on the experiment, around 30% of the original features are removed by this filter.
12We included this evaluation for comparison purposes since the current system have been designed for class-based evaluation only. from WN) over SemCor, and we select the most frequent.
<newSection> 4.2 Results Tables 4 and 5 present the F1 measures (harmonic mean of recall and precision) for nouns and verbs respectively when training our systems on Sem-Cor and testing on SE2 and SE3.
Those results showing a statistically significant13 positive difference when compared with the baseline are in marked bold.
Column labeled as “Class” refers to the target set of semantic classes for the classifiers, that is, the desired semantic level for each example.
Column labeled as “Sem.
Feat.”
indicates the class of the semantic features used to train the classifiers.
For example, class BLC20 combined with Semantic Feature BLC20 means that this set of classes were used both to label the test examples and to define the semantic features.
In order to compare their contribution we also performed a “basicFeat” test without including semantic features.
As expected according to most literature in WSD, the performances of the MFC baselines are very high.
In particular, those corresponding to nouns (ranging from 70% to 80%).
While nominal baselines seem to perform similarly in both SE2 and SE3, verbal baselines appear to be consistently much lower for SE2 than for SE3.
In SE2, verbal baselines range from 44% to 68% while in SE3 verbal baselines range from 52% to 79%.
An exception is the results for verbs considering WND: the results are very high due to the low polysemy for verbs according to WND.
As expected, when increasing the level of abstraction (from senses to SuperSenses) the results also increase.
Finally, it also seems that SE2 task is more difficult than SE3 since the MFC baselines are lower.
As expected, the results of the systems increase while augmenting the level of abstraction (from senses to SuperSenses), and almost in every case, the baseline results are reached or outperformed.
This is very relevant since the baseline results are very high.
Regarding nouns, a very different behaviour is observed for SE2 and SE3.
While for SE3 none of the system presents a significant improvement over the baselines, for SE2 a significant improvement is obtained by using several types of semantic features.
In particular, when using WordNet Domains but also BLC20.
In general, BLC20 semantic features seem to be better than BLC50 and SuperSenses.
Regarding verbs, the system obtains significant improvements over the baselines using different types of semantic features both in SE2 and SE3.
In particular, when using again WordNet Domains as semantic features.
In general, the results obtained by BLC20 are not so much different to the results of BLC50 (in a few cases, this difference is greater than 2 points).
For instance, for nouns, if we consider the number of classes within BLC20 (558 classes), BLC50 (253 classes) and SuperSense (24 classes), BLC classifiers obtain high performance rates while maintaining much higher expressive power than SuperSenses.
In fact, using Super-Senses (40 classes for nouns and verbs) we can obtain a very accurate semantic tagger with performances close to 80%.
Even better, we can use BLC20 for tagging nouns (558 semantic classes and F1 over 75%) and SuperSenses for verbs (14 semantic classes and F1 around 75%).
Obviously, the classifiers using WordNet Domains as target grouping obtain very high performances due to its reduced average polysemy.
However, when used as semantic features it seems to improve the results in most of the cases.
In addition, we obtain very competitive classifiers at a sense level.
We also performed a set of experiments for measuring the behaviour of the class-based WSD system when gradually increasing the number of training examples.
These experiments have been carried for nouns and verbs, but only noun results are shown since in both cases, the trend is very similar but more clear for nouns.
The training corpus has been divided in portions of 5% of the total number of files.
That is, complete files are added to the training corpus of each incremental test.
The files were randomly selected to generate portions of 5%, 10%, 15%, etc. of the SemCor corpus14.
Then, we train the system on each of the training portions and we test the system on SE2 and SE3.
Finally, we also compare the 14Each portion contains also the same files than the previous portion.
For example, all files in the 25% portion are also contained in the 30% portion.
resulting system with the baseline computed over the same training portion.
Figures 1 and 2 present the learning curves over SE2 and SE3, respectively, of a class-based WSD system based on BLC20 using the basic features and the semantic features built with WordNet Domains.
Surprisingly, in SE2 the system only improves the F1 measure around 2% while increasing the training corpus from 25% to 100% of SemCor.
In SE3, the system again only improves the F1 measure around 3% while increasing the training corpus from 30% to 100% of SemCor.
That is, most of the knowledge required for the class-based WSD system seems to be already present on a small part of SemCor.
Figures 3 and 4 present the learning curves over SE2 and SE3, respectively, of a class-based WSD system based on SuperSenses using the basic features and the semantic features built with WordNet Domains.
Again, in SE2 the system only improves the F1 measure around 2% while increasing the training corpus from 25% to 100% of SemCor.
In SE3, the system again only improves the F1 measure around 2% while increasing the training corpus from 30% to 100% of SemCor.
That is, with only 25% of the whole corpus, the class-based WSD system reaches a F1 close to the performance using all corpus.
This evaluation seems to indicate that the class-based approach to WSD reduces dramatically the required amount of training examples.
In both cases, when using BLC20 or Super-Senses as semantic classes for tagging, the behaviour of the system is similar to MFC baseline.
This is very interesting since the MFC obtains high results due to the way it is defined, since the MFC over the total corpus is assigned if there are no occurrences of the word in the training corpus.
Without this definition, there would be a large number of words in the test set with no occurrences when using small training portions.
In these cases, the recall of the baselines (and in turn F1) would be much lower.
<newSection> 5 Conclusions and discussion We explored on the WSD task the performance of different levels of abstraction and sense groupings.
We empirically demonstrated that Base Level Concepts are able to group word senses into an adequate medium level of abstraction to perform supervised class–based disambiguation.
We also demonstrated that the semantic classes provide a rich information about polysemous words and can be successfully used as semantic features.
Finally we confirm the fact that the class– based approach reduces dramatically the required amount of training examples, opening the way to solve the well known acquisition bottleneck problem for supervised machine learning algorithms.
In general, the results obtained by BLC20 are not very different to the results of BLC50.
Thus, we can select a medium level of abstraction, without having a significant decrease of the performance.
Considering the number of classes, BLC classifiers obtain high performance rates while maintaining much higher expressive power than SuperSenses.
However, using SuperSenses (46 classes) we can obtain a very accurate semantic tagger with performances around 80%.
Even better, we can use BLC20 for tagging nouns (558 semantic classes and F1 over 75%) and SuperSenses for verbs (14 semantic classes and F1 around 75%).
As BLC are defined by a simple and fully automatic method, they can provide a user–defined level of abstraction that can be more suitable for certain NLP tasks.
Moreover, the traditional set of features used for sense-based classifiers do not seem to be the most adequate or representative for the class-based approach.
We have enriched the usual set of features, by adding semantic information from the monosemous words of the context and the MFC of the target word.
With this new enriched set of features, we can generate robust and competitive class-based classifiers.
To our knowledge, the best results for class– based WSD are those reported by (Ciaramita and Altun, 2006).
This system performs a sequence tagging using a perceptron–trained HMM, using SuperSenses, training on SemCor and testing on SensEval3.
The system achieves an F1–score of 70.54, obtaining a significant improvement from a baseline system which scores only 64.09.
In this case, the first sense baseline is the SuperSense of the most frequent synset for a word, according to the WN sense ranking.
Although this result is achieved for the all words SensEval3 task, including adjectives, we can compare both results since in SE2 and SE3 adjectives obtain very high performance figures.
Using SuperSenses, adjectives only have three classes (WN Lexicographic Files 00, 01 and 44) and more than 80% of them belong to class 00.
This yields to really very high performances for adjectives which usually are over 90%.
As we have seen, supervised WSD systems are very dependent of the corpora used to train and test the system.
We plan to extend our system by selecting new corpora to train or test.
For instance, by using the sense annotated glosses from WordNet.
<newSection> References<newSection> Abstract This paper present an overview of some emerging trends in the application of NLP in the domain of the so-called Digital Humanities and discusses the role and nature of metadata, the annotation layer that is so characteristic of documents that play a role in the scholarly practises of the humanities.
It is explained how metadata are the key to the added value of techniques such as text and link mining, and an outline is given of what measures could be taken to increase the chances for a bright future for the old ties between NLP and the humanities.
There is no data like metadata!
<newSection> 1 Introduction The humanities and the field of natural language processing (NLP) have always had common playgrounds.
The liaison was never constrained to linguistics; also philosophical, philological and literary studies have had their impact on NLP , and there have always been dedicated conferences and journals for the humanities and the NLP community of which the journal Computers and the Humanities (1966-2004) is probably known best.
Among the early ideas on how to use machines to do things with text that had been done manually for ages is the plan to build a concordance for ancient literature, such as the works of St Thomas Aquinas (Schreibman et al., 2004).
which was expressed already in the late 1940s.
Later on humanities researchers started thinking about novel tasks for machines, things that were not feasible without the power of computers, such as authorship discovery.
For NLP the units of processing gradually became more complex and shifted from the character level to units for which string processing is an insufficient basis.
At some stage syntactic parsers and generators were seen as a method to prove the correctness of linguistic theories.
Nowadays semantic layers can be analysed at much more complex levels of granularity.
Not just phrases and sentences are processed, but also entire documents or even document collections including those involving multimodal features.
And in addition to NLP for information carriers, also language-based interaction has grown into a matured field, and applications in other domains than the humanities now seem more dominant.
The impact of the wide range of functionalities that involve NLP in all kinds of information processing tasks is beyond what could be imagined 60 years ago and has given rise to the outreach of NLP in many domains, but during a long period the humanities were one of the few valuable playgrounds.
Even though the humanities have been able to conduct NLP-empowered research that would have been impossible without the the early tools and resources already for many decades, the more recent introduction of statistical methods in langauge is affecting research practises in the humanities at yet another scale.
An important explanation for this development is of course the wide scale digitisation that is taken up in the humanities.
All kinds of initiatives for converting analogue resources into data sets that can be stored in digital repositories have been initiated.
It is widely known that ”There is no data like more data” (Mercer, 1985), and indeed the volumes of digital humanities resources have reached the level required for adequate performance of all kinds of tasks that require the training of statistical models.
In addition, ICT-enabled methodologies and types of collaboration are being developed and have given rise to new epistemic cultures.
Digital Humanities (sometimes also referred to as Computational Humanities) are a trend, and digital scholarship seems a prerequisite for a successful research career.
But in itself the growth of digi-Proceedings of the 12th Conference of the European Chapter of the ACL, pages 10–15, Athens, Greece, 30 March – 3 April 2009.
c�2009 Association for Computational Linguistics tal resources is not the main factor that makes the humanities again a good testbed for NLP.
A key aspect is the nature and role of metadata in the humanities.
In the next section the role of metadata in the humanities and the the ways in which they can facilitate and enhance the application of text and data mining tools will be described in more detail.
The paper takes the position that for the humanities a variant of Mercer’s saying is even more true.
There is no data like metadata!
The relation between NLP and the humanities is worth reviewing, as a closer look into the way in which techniques such as text and link mining can demonstrate that the potential for mutual impact has gained in strength and diversity, and that important lessons can be learned for other application areas than the humanities.
This renewed liaison with the now digital humanities can help NLP to set up an innovative research agenda which covers a wide range of topics including semantic analysis, integration of multimodal information, language-based interaction, performance evaluation, service models, and usability studies.
The further and combined exploration of these topics will help to develop an infrastructure that will also allow content and data-driven research domains in the humanities to renew their field and to exploit the additional potential coming from the ongoing and future digitisation efforts, as well as the richness in terms of available metadata.
To name a few fields of scholarly research: art history, media studies, oral history, archeology, archiving studies, they all have needs that can be served in novel ways by the mature branches that NLP offers today.
After a sketch in section 2 of the role of metadata, so crucial for the interaction between the humanities and NLP, a rough overview of relevant initiatives will be given.
Inspired by some telling examples, it will be outlined what could be done to increase the chances for a bright future for the old ties, and how other domains can benefit as well from the reinvention of the old common playground between NLP and the humanities.
<newSection> 2 Metadata in the Humanities Digital text, but also multimedia content, can be mined for the occurrence of patterns at all kinds of layers, and based on techniques for information extraction and classification, documents can be annotated automatically with a variety of labels, including indications of topic, event types, authorship, stylistics, etc.
Automatically generated annotations can be exploited to support to what is often called the semantic access to content, which is typically seen as more powerful than plain full text search, but in principle also includes conceptual search and navigation.
The data used in research in the domain of the humanities comes from a variety of sources: archives, musea (or in general cultural heritage collections), libraries, etc.
As a testbed for NLP these collections are particularly challenging because of the combination of complexity increasing features, such as language and spelling change over time, diversity in orthography, noisy content (due to errors introduced during data conversion, e.g., OCR or transcription of spoken word material), wider than average stylistic variation and cross-lingual and cross-media links.
They are also particularly attractive because of the available metadata or annotation records, which are the reflection of analytical and comparative scholarly processes.
In addition, there is a wide diversity of annotation types to be found in the domain (cf. the annotation dimensions distinguished by (Mar-shall, 1998)), and the field has developed modelling procedures to exploit this diversity (Mc-Carty, 2005) and visualisation tools (Unsworth, 2005).
For many types of textual data automatically generated annotations are the sole basis for semantic search, navigation and mining.
For humanities and cultural heritage collections, automatically generated annotation is often an addition to the catalogue information traditionally produced by experts in the field.
The latter kind of manually produced metadataa is often specified in accordance to controlled key word lists and meta-data schemata agreed for the domain.
NLP tagging is then an add on to a semantic layer that in itself can already be very rich and of high quality.
More recently initiatives and support tools for so-called social tagging have been proposed that can in principle circumvent the costly annotation by experts, and that could be either based on free text annotation or on the application of so-called folksonomies as a replacement for the traditional taxonomies.
Digital librarians have initiated the development of platforms aiming at the integration of the various annotation processes and at sharing tools that can help to realise an infrastructure for distributed annotation.
But whatever the genesis is of annotations capturing the semantics of an entire document, they are a very valuable source for the training of automatic classifiers.
And traditionally, textual resources in the humanities have lots of it, partly because the mere art of annotating texts has been invented in this domain.
Part of the resources used as basis for scholarly research is non-textual.
Apart from numeric data resources, which are typically strongly structured in database-like environments, there is a growing amount of audiovisual material that is of interest to humanities researchers.
Various kinds of multi-media collections can be a primary source of information for humanities researchers, in particular if there is a substantial amount of spoken word content, e.g., broadcast news archives, and even more prominently: oral history collections.
It is commonly agreed that accessibility of het-erogeneous audiovisual archives can be boosted by indexing not just via the classical metadata, but by enhancing indexing mechanisms through the exploitation of the spoken audio.
For several types of audiovisual data, transcription of the speech segments can be a good basis for a time-coded index.
Research has shown that the quality of the automatically generated speech transcrip-tions, and as a consequence also the index quality, can increase if the language models applied have been optimised to both the available metadata (in particular on the named entities in the annotations) and the collateral sources available (Huijbregts et al., 2007).
‘Collateral data is the term used for secondary information objects that relate to the primary documents, e.g., reviews, program guide summaries, biographies, all kinds of textual publications, etc.
This requires that primary sources have been annotated with links to these secondary materials.
These links can be pointers to source locations within the collection, but also links to related documents from external sources.
In laboratory settings the amount of collateral data is typically scarce, but in real life spoken word archives, experts are available to identify and collect related (textual) content that can help to turn generic language models into domain specific models with higher accuracy.
The quality of automatically generated content annotations in real life settings is lagging behind in comparison to experimental settings.
This is of course an obstacle for the uptake of technology, but a number of pilot projects with collections from the humanities domain show us what can be done to overcome the obstacles.
This can be illustrated again with the situation in the field of spoken document retrieval.
For many A/V collections with a spoken audio track, metadata is not or only sparsely available, which is why this type of collection is often only searchable by linear exploration.
Although there is common agreement that speech-based, automatically generated annotation of audiovisual archives may boost the semantic access to fragments of spoken word archives enormously (Gold-man et al., 2005; Garofolo et al., 2000; Smeaton et al., 2006), success stories for real life archives are scarce.
(Exceptions can be found in research projects in the broadcast news and cultural heritage domains, such as MALACH (Byrne et al., 2004), and systems such as SpeechFind (Hansen et al., 2005).)
In lab conditions the focus is usually on data that (i) have well-known characteristics (e.g, news content), often learned along with annual benchmark evaluations,' (ii) form a relatively homogeneous collection, (iii) are based on tasks that hardly match the needs of real users, and (iv) are annotated in large quantities for training purposes.
In real life however, the exact characteristics of archival data are often unknown, and are far more heterogeneous in nature than those found in laboratory settings.
Language models for realistic audio sets, sometimes referred to as surprise data (Huijbregts, 2008), can benefit from a clever use of this contextual information.
Surprise data sets are increasingly being taken into account in research agendas in the field focusing on multimedia indexing and search (de Jong et al., 2008).
In addition to the fact that they are less homogenous, and may come with links to related documents, real user needs may be available from query logs, and as a consequence they are an interesting challenge for cross-media indexing strategies targeting aggregated collections.
Sur-'E.g., evaluation activities such as those organised by NIST, the National Institute of Standards, e.g., TREC for search tasks involving text, TRECVID for video search, Rich Transcription for the analysis of speech data, etc.
http: //www.nist.gov/ prise data are therefore an ideal source for the development of best practises for the application of tools for exploiting collateral content and meta-data.
The exploitation of available contextual information for surprise content and the organisation of this dual annotation process can be improved, but in principle joining forces between NLP technologies and the capacity of human annotators is attractive.
On the one hand for the improved access to the content, on the other hand for an innovation of the NLP research agenda.
<newSection> 3 Ingredients for a Novel Knowledge-driven Workflow A crucial condition for the revival of the common playground for NLP and the humanities is the availability of representatives of communities that could use the outcome, either in the development of services to their users or as end users.
These representatives may be as diverse and include e.g., archivists, scholars with a research interest in a collection, collection keepers in libraries and musea, developers of educational materials, but in spite of the divergence that can be attributed to such groups, they have a few important characteristics in common: they have a deep understanding of the structure, semantic layers and content of collections, and in developing new road maps and novel ways of working, the pressure they encounter to be cost-effective is modest.
They are the first to understand that the technical solutions and business models of the popular web search engines are not directly applicable to their domain in which the workflow is typically knowledge-driven and labour-intensive.
Though with the introduction of new technologies the traditional role of documentalists as the primary source of high quality annotations may change, the availability of their expertise is likely to remain one of the major success factors in the realisation of a digital in-frastructure that is as rich source as the repositories from the analogue era used to be.
All kinds of coordination bodies and action plans exist to further the field of Digital Humanities, among which The Alliance of Digital Humanities Organizations http://www.
digitalhumanities.org/ and HASTAC (https://www.hastac.org/) and Digital Arts an Humanities www.arts-humanities.
net, and dedicated journals and events have emerged, such as the LaTeCH workshop series.
In part they can build on results of initiatives for collaboration and harmonisation that were started earlier, e.g., as Digital Libraries support actions or as coordinated actions for the international community of cultural heritage institutions.
But in order to reinforce the liaison between NLP and the humanities continued attention, support and funding is needed for the following: Coordination of coherent platforms (both local and international) for the interaction between the communities involved that stimulate the exchange of expertise, tools, experience and guidelines.
Good examples hereof exist already in several domains, e.g., the field of broadcast archiving (IST project PrestoSpace; www.prestospace.
org/), the research area of Oral History, all kinds of communities and platforms targeting the accessibility of cultural heritage collections (e.g., CATCH; http://www.nwo.
nl/catch), but the long-term sustainability of accessible interoperable institutional networks remains a concern.
Infrastructural facilities for the support of researchers and developers of NLP tools; such facilities should support them in finetuning the instruments they develop to the needs of scholarly research.
CLARIN (http:// www.clarin.eu/) is a promising initiative in the EU context that is aiming to cover exactly this (and more) for the social sciences and the humanities.
Open access, source and standards to increase the chances for inter-institutional collabora-tion and exchange of content and tools in accordance with the policies of the de facto leading bodies, such as TEI (http://www. tei-c.org/) and OAI (http://www. openarchives.org/).
Exchange mechanisms for best practices e.g., of building and updating training data, the use of annotation tools and the analysis of query logs.
Protocols and tools for the mark-up of content, the specification of links between collections, the handling of IPR and privacy issues, etc.
Service centers that can offer heavy processing facilities (e.g. named entity extraction or speech transcription) for collections kept in technically modestly equipped environments hereof.
User Interfaces that can flexibly meet the needs of scholarly users for expressing their information needs, and for visualising relationships between interactive information elements (e.g., timelines and maps).
Pilot projects in which researchers from various backgrounds collaborate in analysing a specific digital resource as a central object in order to learn to understand how the interfaces between their fields can be opened up.
An interesting example is the the project Veteran Tapes (http://www.surffoundation.nl/ smartsite.dws?id=14040).
This initiative is linked to the interview collection which is emerging as a result for the Dutch Veterans Interview-project, which aims at collecting 1000 interviews with a representative group of veterans of all conflicts and peace-missions in which The Netherlands were involved.
The research results will be integrated in a web-based fashion to form what is called an enriched publication.
Evaluation frameworks that will trigger contributions to the enhancement en tuning of what NLP has to offer to the needs of the humanities.
These frameworks should include benchmarks addressing tasks and user needs that are more realistic than most of the existing performance evaluation frameworks.
This will require close collaboration between NLP developers and scholars.
<newSection> 4 Conclusion The assumption behind presenting these issues as priorities is that NLP-empowered use of digital content by humanities scholars will be beneficial to both communities.
NLP can use the testbed of the Digital Humanities for the further shaping of that part of the research agenda that covers the role of NLP in information handling, and in particular those avenues that fall under the concept of mining.
By focussing on the integration of meta-data in the models underlying the mining tools and searching for ways to increase the involvement of metadata generators, both experts and ‘amateurs’, important insights are likely to emerge that could help to shape agendas for the role of NLP in other disciplines.
Examples are the role of NLP in the study of recorded meeting content, in the field of social studies, or the organisation and support of tagging communities in the biomedical domain, both areas where manual annotation by experts used to be common practise, and both areas where mining could be done with aggregated collections.
Equally important are the benefits for the humanities.
The added value of metadata-based mining technology for enhanced indexing is not so much in the cost-reduction as in the wider usability of the materials, and in the impulse this may bring for sharing collections that otherwise would too easily be considered as of no general importance.
Furthermore the evolution of digital texts from ‘book surrogates’ towards the rich semantic layers and networks generated by text and/or media mining tools that take all available metadata into account should help the fields involved in not just answering their research questions more efficiently, but also in opening up grey literature for research purposes and in scheduling entirely new questions for which the availability of such networks are a conditio sine qua non.
<newSection> Acknowledgments Part of what is presented in this paper has been inspired by collaborative work with colleagues.
In particular I would like to thank Willemijn Heeren, Roeland Ordelman and Stef Scagliola for their role in the genesis of ideas and insights.
<newSection> References<newSection> Abstract We present a framework for interfacing a PCFG parser with lexical information from an external resource following a different tagging scheme than the treebank.
This is achieved by defining a stochastic mapping layer between the two resources.
Lexical probabilities for rare events are estimated in a semi-supervised manner from a lexicon and large unannotated corpora.
We show that this solution greatly enhances the performance of an unlexicalized Hebrew PCFG parser, resulting in state-of-the-art Hebrew parsing results both when a segmentation oracle is assumed, and in a real-word parsing scenario of parsing unsegmented tokens.
<newSection> 1 Introduction The intuition behind unlexicalized parsers is that the lexicon is mostly separated from the syntax: specific lexical items are mostly irrelevant for accurate parsing, and can be mediated through the use of POS tags and morphological hints.
This same intuition also resonates in highly lexicalized formalism such as CCG: while the lexicon categories are very fine grained and syntactic in nature, once the lexical category for a lexical item is determined, the specific lexical form is not taken into any further consideration.
Despite this apparent separation between the lexical and the syntactic levels, both are usually estimated solely from a single treebank.
Thus, while PCFGs can be accurate, they suffer from vocabulary coverage problems: treebanks are small and lexicons induced from them are limited.
The reason for this treebank-centric view in PCFG learning is 3-fold: the English treebank is fairly large and English morphology is fairly simple, so that in English, the treebank does provide mostly adequate lexical coverage1; Lexicons enumerate analyses, but don’t provide probabilities for them; and, most importantly, the treebank and the external lexicon are likely to follow different annotation schemas, reflecting different linguistic perspectives.
On a different vein of research, current POS tagging technology deals with much larger quantities of training data than treebanks can provide, and lexicon-based unsupervised approaches to POS tagging are practically unlimited in the amount of training data they can use.
POS taggers rely on richer knowledge than lexical estimates derived from the treebank, have evolved sophisticated strategies to handle OOV and can provide distributions p(t|w, context) instead of “best tag” only.
Can these two worlds be combined?
We propose that parsing performance can be greatly improved by using a wide coverage lexicon to suggest analyses for unknown tokens, and estimating the respective lexical probabilities using a semi-supervised technique, based on the training procedure of a lexicon-based HMM POS tagger.
For many resources, this approach can be taken only on the proviso that the annotation schemes of the two resources can be aligned.
We take Modern Hebrew parsing as our case study.
Hebrew is a Semitic language with rich morphological structure.
This rich structure yields a large number of distinct word forms, resulting in a high OOV rate (Adler et al., 2008a).
This poses a serious problem for estimating lexical probabilities from small annotated corpora, such as the Hebrew treebank (Sima’an et al., 2001).
Hebrew has a wide coverage lexicon / morphological-analyzer (henceforth, KC Analyzer) available2, but its tagset is different than the one used by the Hebrew Treebank.
These are not mere technical differences, but derive from different perspectives on the data.
The Hebrew TB tagset is syntactic in nature, while the KC tagset is lexicographic.
This difference in perspective yields different performance for parsers induced from tagged data, and a simple mapping between the two schemes is impossible to define (Sec. 2).
A naive approach for combining the use of the two resources would be to manually re-tag the Treebank with the KC tagset, but we show this approach harms our parser’s performance.
Instead, we propose a novel, layered approach (Sec. 2.1), in which syntactic (TB) tags are viewed as contextual refinements of the lexicon (KC) tags, and conversely, KC tags are viewed as lexical clustering of the syntactic ones.
This layered representation allows us to easily integrate the syntactic and the lexicon-based tagsets, without explicitly requiring the Treebank to be re-tagged.
Hebrew parsing is further complicated by the fact that common prepositions, conjunctions and articles are prefixed to the following word and pronominal elements often appear as suffixes.
The segmentation of prefixes and suffixes can be ambiguous and must be determined in a specific context only.
Thus, the leaves of the syntactic parse trees do not correspond to space-delimited tokens, and the yield of the tree is not known in advance.
We show that enhancing the parser with external lexical information is greatly beneficial, both in an artificial scenario where the token segmentation is assumed to be known (Sec.
4), and in a more realistic one in which parsing and segmentation are handled jointly by the parser (Goldberg and Tsar-faty, 2008) (Sec. 5).
External lexical information enhances unlexicalized parsing performance by as much as 6.67 F-points, an error reduction of 20% over a Treebank-only parser.
Our results are not only the best published results for parsing Hebrew, but also on par with state-of-the-art lexicalized Arabic parsing results assuming gold-standard fine-grained Part-of-Speech (Maamouri et al., 2008).3 <newSection> 2 A Tale of Two Resources Modern Hebrew has 2 major linguistic resources: the Hebrew Treebank (TB), and a wide coverage Lexicon-based morphological analyzer developed and maintained by the Knowledge Center for Processing Hebrew (KC Analyzer).
The Hebrew Treebank consists of sentences manually annotated with constituent-based syntactic information.
The most recent version (V2) (Guthmann et al., 2009) has 6,219 sentences, and covers 28,349 unique tokens and 17,731 unique segments4.
The KC Analyzer assigns morphological analyses (prefixes, suffixes, POS, gender, person, etc.) to Hebrew tokens.
It is based on a lexicon of roughly 25,000 word lemmas and their inflection patterns.
From these, 562,439 unique word forms are derived.
These are then prefixed (subject to constraints) by 73 prepositional prefixes.
It is interesting to note that even with these numbers, the Lexicon’s coverage is far from complete.
Roughly 1,500 unique tokens from the Hebrew Treebank cannot be assigned any analysis by the KC Lexicon, and Adler et al.(2008a) report that roughly 4.5% of the tokens in a 42M tokens corpus of news text are unknown to the Lexicon.
For roughly 400 unique cases in the Treebank, the Lexicon provides some analyses, but not a correct one.
This goes to emphasize the productive nature of Hebrew morphology, and stress that robust lexical probability estimates cannot be derived from an annotated resource as small as the Treebank.
Lexical vs. Syntactic POS Tags The analyses produced by the KC Analyzer are not compatible with the Hebrew TB.
The KC tagset (Adler et al., 2008b; Netzer et al., 2007; Adler, 2007) takes a lexical approach to POS tagging (“a word can assume only POS tags that would be assigned to it in a dictionary”), while the TB takes a syntactic one (“if the word in this particular positions functions as an Adverb, tag it as an Adverb, even though it is listed in the dictionary only as a Noun”).
We present 2 cases that emphasize the difference: Adjectives: the Treebank treats any word in an adjectivial position as an Adjective.
This includes also demonstrative pronouns m -r5, (this boy).
However, from the KC point of view, the fact that a pronoun can be used to modify a noun does not mean it should appear in a dictionary as an adjective.
The MOD tag: similarly, the TB has a special POS-tag for words that perform syntactic modification.
These are mostly adverbs, but almost any Adjective can, in some cir-cumstances, belong to that class as well.
This category is highly syntactic, and does not conform to the lexicon based approach.
In addition, many adverbs and prepositions in Hebrew are lexicalized instances of a preposition followed by a noun (e.g., m5rz, “in+softness”, softly).
These can admit both the lexicalized and the compositional analyses.
Indeed, many words admit the lexicalized analyses in one of the resource but not in the other (e.g., rztv5 “for+benefit” is Prep in the TB but only Prep+Noun in the KC, while for -ren “from+side” it is the other way around).
While the syntactic POS tags annotation of the TB is very useful for assigning the correct tree structure when the correct POS tag is known, there are clear benefits to an annotation scheme that can be easily backed by a dictionary.
We created a unified resource, in which every word occurrence in the Hebrew treebank is assigned a KC-based analysis.
This was done in a semi-automatic manner – for most cases the mapping could be defined deterministically.
The rest (less than a thousand instances) were manually assigned.
Some Treebank tokens had no analyses in the KC lexicon, and some others did not have a correct analysis.
These were marked as “UN-KNOWN” and “MISSING” respectively.5 The result is a Treebank which is morphologically annotated according to two different schemas.
On average, each of the 257 TB tags is mapped to 2.46 of the 273 KC tags.6 While this resource can serve as a basis for many linguistically motivated inquiries, the rest of this paper is 5Another solution would be to add these missing cases to the KC Lexicon.
In our view this act is harmful: we don’t want our Lexicon to artificially overfit our annotated corpora.
6A “tag” in this context means the complete morphological information available for a morpheme in the Treebank: its part of speech, inflectional features and possessive suffixes, but not prefixes or nominative and accusative suffixes, which are taken to be separate morphemes.
devoted to using it for constructing a better parser.
Tagsets Comparison In (Adler et al., 2008b), we hypothesized that due to its syntax-based nature, the Treebank morphological tagset is more suitable than the KC one for syntax related tasks.
Is this really the case?
To verify it, we simulate a scenario in which the complete gold morphological information is available.
We train 2 PCFG grammars, one on each tagged version of the Treebank, and test them on the subset of the development set in which every token is completely covered by the KC Analyzer (351 sentences).7 The input to the parser is the yields and disambiguated pre-terminals of the trees to be parsed.
The parsing results are presented in Table 1.
Note that this scenario does not reflect actual parsing performance, as the gold information is never available in practice, and surface forms are highly ambiguous.
With gold morphological information, the TB tagging scheme is more informative for the parser.
The syntax-oriented annotation scheme of the TB is more informative for parsing than the lexi-cographic KC scheme.
Hence, we would like our parser to use this TB tagset whenever possible, and the KC tagset only for rare or unseen words.
A Layered Representation It seems that learning a treebank PCFG assuming such a different tagset would require a treebank tagged with the alternative annotation scheme.
Rather than assuming the existence of such an alternative resource, we present here a novel approach in which we view the different tagsets as corresponding to different aspects of the morphosyntactic representation of pre-terminals in the parse trees.
Each of these layers captures subtleties and regularities in the data, none of which we would want to (and sometimes, cannot) reduce to the other.
We, therefore, propose to retain both tagsets and learn a fuzzy mapping between them.
In practice, we propose an integrated representation of the tree in which the bottommost layer represents the yield of the tree, the surface forms are tagged with dictionary-based KC POS tags, and syntactic TB POS tags are in turn mapped onto the KC ones (see Figure 1).
This representation helps to retain the information both for the syntactic and the morphological POS tagsets, and can be seen as capturing the interaction between the morphological and syntactic aspects, allowing for a seamless integration of the two levels of representation.
We refer to this intermediate layer of representation as a morphosyntactic-transfer layer and we formally depict it as p(tKC|tTB).
This layered representation naturally gives rise to a generative model in which a phrase level constituent first generates a syntactic POS tag (tTB), and this in turn generates the lexical POS tag(s) (tKC).
The KC tag then ultimately generates the terminal symbols (w).
We assume that a morphological analyzer assigns all possible analyses to a given terminal symbol.
Our terminal symbols are, therefore, pairs: (w, t), and our lexical rules are of the form t —* (w, t).
This gives rise to the following equivalence: p((w, tKC)|tTB) = p(tKC|tTB)p((w, tKC)|tKC) In Sections (4, 5) we use this layered generative process to enable a smooth integration of a PCFG treebank-learned grammar, an external wide-coverage lexicon, and lexical probabilities learned in a semi-supervised manner.
<newSection> 3 Semi-supervised Lexical Probability Estimations A PCFG parser requires lexical probabilities of the form p(w|t) (Charniak et al., 1996).
Such information is not readily available in the lexicon.
However, it can be estimated from the lexicon and large unannotated corpora, by using the well-known Baum-Welch (EM) algorithm to learn a trigram HMM tagging model of the form p(t1, ...
, tn, w1, ...
, wn) = argmax H p(ti|ti_1, ti_2)p(wi|ti), and taking the emission probabilities p(w|t) of that model.
In Hebrew, things are more complicated, as each emission w is not a space delimited token, but rather a smaller unit (a morphological segment, henceforth a segment).
Adler and Elhadad (2006) present a lattice-based modification of the Baum-Welch algorithm to handle this segmentation ambiguity.
Traditionally, such unsupervised EM-trained HMM taggers are thought to be inaccurate, but (Goldberg et al., 2008) showed that by feeding the EM process with sufficiently good initial probabilities, accurate taggers (> 91% accuracy) can be learned for both English and Hebrew, based on a (possibly incomplete) lexicon and large amount of raw text.
They also present a method for automatically obtaining these initial probabilities.
As stated in Section 2, the KC Analyzer (He-brew Lexicon) coverage is incomplete.
Adler et al.(2008a) use the lexicon to learn a Maximum Entropy model for predicting possible analyses for unknown tokens based on their orthography, thus extending the lexicon to cover (even if noisily) any unknown token.
In what follows, we use KCAna-lyzer to refer to this extended version.
Finally, these 3 works are combined to create a state-of-the-art POS-tagger and morphological disambiguator for Hebrew (Adler, 2007): initial lexical probabilities are computed based on the MaxEnt-extended KC Lexicon, and are then fed to the modified Baum-Welch algorithm, which is used to fit a morpheme-based tagging model over a very large corpora.
Note that the emission probabilities P(W|T) of that model cover all the morphemes seen in the unannotated training corpus, even those not covered by the KC Analyzer.8 We hypothesize that such emission probabilities are good estimators for the morpheme-based P(T —* W) lexical probabilities needed by a PCFG parser.
To test this hypothesis, we use it to estimate p(tKC —* w) in some of our models.
<newSection> 4 Parsing with a Segmentation Oracle We now turn to describing our first set of experiments, in which we assume the correct segmentation for each input sentence is known.
This is a strong assumption, as the segmentation stage is ambiguous, and segmentation information provides very useful morphological hints that greatly constrain the search space of the parser.
However, the setting is simpler to understand than the one in which the parser performs both segmentation and POS tagging, and the results show some interesting trends.
Moreover, some recent studies on parsing Hebrew, as well as all studies on parsing Arabic, make this oracle assumption.
As such, the results serve as an interesting comparison.
Note that in real-world parsing situations, the parser is faced with a stream of ambiguous unsegmented tokens, making results in this setting not indicative of real-world parsing performance.
The main question we address is the incorporation of an external lexical resource into the parsing process.
This is challenging as different resources follow different tagging schemes.
One way around it is re-tagging the treebank according to the new tagging scheme.
This will serve as a baseline in our experiment.
The alternative method uses the Layered Representation described above (Sec. 2.1).
We compare the performance of the two approaches, and also compare them against the performance of the original treebank without external information.
We follow the intuition that external lexical resources are needed only when the information contained in the treebank is too sparse.
Therefore, we use treebank-derived estimates for reliable events, and resort to the external resources only in the cases of rare or OOV words, for which the treebank distribution is not reliable.
Grammar and Notation For all our experiments, we use the same grammar, and change only the way lexical probabilities are implemented.
The grammar is an unlexicalized treebank-estimated PCFG with linguistically motivated state-splits.9 In what follows, a lexical event is a word segment which is assigned a single POS thereby functioning as a leaf in a syntactic parse tree.
A rare 9Details of the grammar: all functional information is removed from the non-terminals, finite and non-finite verbs, as well as possessive and other PPs are distinguished, definiteness structure of constituents is marked, and parent annotation is employed.
It is the same grammar as described in (Goldberg and Tsarfaty, 2008).
(lexical) event is an event occurring less than K times in the training data, and a reliable (lexical) event is one occurring at least K times in the training data.
We use OOV to denote lexical events appearing 0 times in the training data.
count(·) is a counting function over the training data, rare stands for any rare event, and wrare is a specific rare event.
KCA(·) is the KC Analyzer function, mapping a lexical event to a set of possible tags (analyses) according to the lexicon.
All our models use relative frequency estimated probabilities for reliable lexical events: p(t → ment of rare (including OOV) events.
In our Baseline, no external resource is used.
We smooth for rare and OOV events using a per-tag probability distribution over rare segments, which we estimate using relative frequency over rare segments in the training data: p(wrare|t) = in treebank grammars are usually estimated.
We experiment with two flavours of lexical models.
In the first, LexFilter, the KC Analyzer is consulted for rare events.
We estimate rare events using the same per-tag distribution as in the baseline, but use the KC Analyzer to filter out any incompatible cases, that is, we force to 0 the probability of any analysis not supported by the lexicon: Our second flavour of lexical models, Lex-Probs, the KC Analyzer is consulted to propose analyses for rare events, and the probability of an analysis is estimated via the HMM emission function described in Section 3, which we denote B: In both LexFilter and LexProbs, we resort to the relative frequency estimation in case the event is not covered in the KC Analyzer.
In this work, we are comparing 3 different representations: TB, which is the original Treebank, KC which is the Treebank converted to use the KC Analyzer tagset, and Layered, which is the layered representation described above.
The details of the lexical models vary according to the representation we choose to work with.
For the TB setting, our lexical rules are of the form ttb → w.
Only the Baseline models are relevant here, as the tagset is not compatible with that of the external lexicon.
For the KC setting, our lexical rules are of the form tkc → w, and their probabilities are estimated as described above.
Note that this setting requires our trees to be tagged with the new (KC) tagset, and parsed sentences are also tagged with this tagset.
For the Layered setting, we use lexical rules of the form ttb → w.
Reliable events are estimated as usual, via relative frequency over the original treebank.
For rare events, we estimate p(ttb → w|ttb) = p(ttb → tkc|ttb)p(tkc → w|tkc), where the transfer probabilities p(ttb → tkc) are estimated via relative frequencies over the layered trees, and the emission probabilities are estimated either based on other rare events (LexFilter) or based on the semi-supervised method described in Section 3 (LexProbs).
The layered setting has several advantages: First, the resulting trees are all tagged with the original TB tagset.
Second, the training procedure does not require a treebank tagged with the KC tagset: Instead of learning the transfer layer from the treebank we could alternatively base our counts on a different parallel resource, estimate it from unannotated data using EM, define it heuristically, or use any other estimation procedure.
We perform all our experiments on Version 2 of the Hebrew Treebank, and follow the train/test/dev split introduced in (Tsarfaty and Sima’an, 2007): section 1 is used for development, sections 2-12 for training, and section 13 is the test set, which we do not use in this work.
All the reported results are on the development set.10 After removal of empty sentences, we have 5241 sentences for training, and 483 for testing.
Due to some changes in the Treebank11, our results are not directly comparable to earlier works.
However, our baseline models are very similar to the models presented in, e.g. (Goldberg and Tsarfaty, 2008).
In order to compare the performance of the model on the various tagset representations (TB tags, KC tags, Layered), we remove from the test set 51 sentences in which at least one token is marked as not having any correct segmentation in the KC Analyzer.
This introduces a slight bias in favor of the KC-tags setting, and makes the test somewhat easier for all the models.
However, it allows for a relatively fair comparison between the various models.12 <newSection> Results and Discussion As expected, all the results are much lower than those with gold fine-grained POS (Table 1).
When not using any external knowledge (Base-line), the TB tagset performs slightly better than the converted treebank (KC).
Note, however, that the difference is less pronounced than in the gold morphology case.
When varying the rare words threshold from 2 to 10, performance drops considerably.
Without external knowledge, the parser is facing difficulties coping with unseen events.
The incorporation of an external lexical knowledge in the form of pruning illegal tag assignments for unseen words based on the KC lexicon (Lex-Filter) substantially improves the results (∼ 72 to ∼ 77).
The additional lexical knowledge clearly improves the parser.
Moreover, varying the rare words threshold in this setting hardly affects the parser performance: the external lexicon suffices to guide the parser in the right direction.
Keeping the rare words threshold high is desirable, as it reduces overfitting to the treebank vocabulary.
We expected the addition of the semi-supervised p(t → w) distribution (LexProbs) to improve the parser, but found it to have an in-significant effect.
The correct segmentation seems to remove enough ambiguity as to let the parser base its decisions on the generic tag distribution for rare events.
In all the settings with a Segmentation Oracle, there is no significant difference between the KC and the Layered representation.
We prefer the layered representation as it provides more flexibility, does not require trees tagged with the KC tagset, and produces parse trees with the original TB POS tags at the leaves.
5 Parsing without a Segmentation Oracle When parsing real world data, correct token segmentation is not known in advance.
For methodological reasons, this issue has either been set-aside (Tsarfaty and Sima’an, 2007), or dealt with in a pipeline model in which a morphological dis-ambiguator is run prior to parsing to determine the correct segmentation.
However, Tsarfaty (2006) argues that there is a strong interaction between syntax and morphological segmentation, and that the two tasks should be modeled jointly, and not in a pipeline model.
Several studies followed this line, (Cohen and Smith, 2007) the most recent of which is Goldberg and Tsarfaty (2008), who presented a model based on unweighted lattice parsing for performing the joint task.
This model uses a morphological analyzer to construct a lattice over all possible morphological analyses of an input sentence.
The arcs of the lattice are (w, t) pairs, and a lattice parser is used to build a parse over the lattice.
The Viterbi parse over the lattice chooses a lattice path, which induces a segmentation over the input sentence.
Thus, parsing and segmentation are performed jointly.
Lexical rules in the model are defined over the lattice arcs (t —* (w, t)|t), and smoothed probabilities for them are estimated from the treebank via relative frequency over terminal/preterminal pairs.
The lattice paths themselves are unweighted, reflecting the intuition that all morphological analyses are a-priori equally likely, and that their perspective strengths should come from the segments they contain and their interaction with the syntax.
Goldberg and Tsarfaty (2008) use a data-driven morphological analyzer derived from the treebank.
Their better models incorporated some external lexical knowledge by use of an Hebrew spell checker to prune some illegal segmentations.
In what follows, we use the layered representation to adapt this joint model to use as its morphological analyzer the wide coverage KC Analyzer in enhancement of a data-driven one.
Then, we further enhance the model with the semi-supervised lexical probabilities described in Sec 3.
The model of Goldberg and Tsarfaty (2008) uses a morphological analyzer to constructs a lattice for each input token.
Then, the sentence lattice is built by concatenating the individual token lattices.
The morphological analyzer used in that work is data driven based on treebank observations, and employs some well crafted heuristics for OOV tokens (for details, see the original paper).
Here, we use instead a morphological analyzer which uses the KC Lexicon for rare and OOV tokens.
We begin by adapting the rare vs. reliable events distinction from Section 4 to cover unsegmented tokens.
We define a reliable token to be a token from the training corpus, which each of its possible segments according to the training corpus was seen in the training corpus at least K times.14 All other tokens are considered to be rare.
Our morphological analyzer works as follows: For reliable tokens, it returns the set of analyses seen for this token in the treebank (each analysis is a sequence of pairs of the form (w, tTB)).
For rare tokens, it returns the set of analyses returned by the KC analyzer (here, analyses are sequences of pairs of the form (w, tKC)).
The lattice arcs, then, can take two possible forms, either (w, tTB) or (w, tKC).
Lexical rules of the form tTB —* (w, tTB) are reliable, and their probabilities estimated via relative frequency over events seen in training.
Lexical rules of the form tTB —* (w, tKC) are estimated in accordance with the transfer layer introduced above: p(tTB —* (w, tKC)) _ p(tKC|tTB)p((w, tKC)|tKC).
The remaining question is how to estimate p((w, tKC)|tKC).
Here, we use either the LexFil-ter (estimated over all rare events) or LexProbs (estimated via the semisupervised emission probabilities)models, as defined in Section 4.1 above.
As our Baseline, we take the best model of (Gold-berg and Tsarfaty, 2008), run against the current version of the Treebank.15 This model uses the same grammar as described in Section 4.1 above, and use some external information in the form of a spell-checker wordlist.
We compare this Baseline with the LexFilter and LexProbs models over the Layered representation.
We use the same test/train splits as described in Section 4.
Contrary to the Oracle segmentation setting, here we evaluate against all sentences, including those containing tokens for which the KC Analyzer does not contain any correct analyses.
Due to token segmentation ambiguity, the resulting parse yields may be different than the gold ones, and evalb can not be used.
Instead, we use the evaluation measure of (Tsarfaty, 2006), also used in (Goldberg and Tsarfaty, 2008), which is an adaptation of parseval to use characters instead of space-delimited tokens as its basic units.
<newSection> Results and Discussion The results are expectedly lower than with the segmentation Oracle, as the joint task is much harder, but the external lexical information greatly benefits the parser also in the joint setting.
While significant, the improvement from the Baseline to LexFilter is quite small, which is due to the Baseline’s own rather strong illegal analyses filtering heuristic.
However, unlike the oracle segmentation case, here the semisupervised lexical probabilities (LexProbs) have a major effect on the parser performance (— 69 to — 73.5 F-score), an overall improvement of — 6.6 F-points over the Baseline, which is the previous state-of-the art for this joint task.
This supports our intuition that rare lexical events are better estimated using a large unannotated corpus, and not using a generic treebank distribution, or sparse treebank based counts, and that lexical probabilities have a crucial role in resolving segmentation ambiguities.
15While we use the same software as (Goldberg and Tsar-faty, 2008), the results reported here are significantly lower.
This is due to differences in annotation scheme between V1 and V2 of the Hebrew TB The parsers with the extended lexicon were unable to assign a parse to about 10 of the 483 test sentences.
We count them as having 0-Fscore in the table results.16 The Baseline parser could not assign a parse to more than twice that many sentences, suggesting its lexical pruning heuristic is quite harsh.
In fact, the unparsed sentences amount to most of the difference between the Baseline and LexFilter parsers.
Here, changing the rare tokens threshold has a significant effect on parsing accuracy, which suggests that the segmentation for rare tokens is highly consistent within the corpus.
When an unknown token is encountered, a clear bias should be taken toward segmentations that were previously seen in the same corpus.
Given that that effect is remedied to some extent by introducing the semi-supervised lexical probabilities, we believe that segmentation accuracy for unseen tokens can be further improved, perhaps using resources such as (Gabay et al., 2008), and techniques for incorporating some document, as opposed to sentence level information, into the parsing process.
<newSection> 6 Conclusions We present a framework for interfacing a parser with an external lexicon following a different annotation scheme.
Unlike other studies (Yang Huang et al., 2005; Szolovits, 2003) in which such interfacing is achieved by a restricted heuristic mapping, we propose a novel, stochastic approach, based on a layered representation.
We show that using an external lexicon for dealing with rare lexical events greatly benefits a PCFG parser for Hebrew, and that results can be further improved by the incorporation of lexical probabilities estimated in a semi-supervised manner using a wide-coverage lexicon and a large unannotated corpus.
In the future, we plan to integrate this framework with a parsing model that is specifically crafted to cope with morphologically rich, free-word order languages, as proposed in (Tsar-faty and Sima’an, 2008).
Apart from Hebrew, our method is applicable in any setting in which there exist a small treebank and a wide-coverage lexical resource.
For example parsing Arabic using the Arabic Treebank and the Buckwalter analyzer, or parsing English biomedical text using a biomedical treebank and the UMLS Specialist Lexicon.
<newSection> References<newSection> Abstract Syntactic Reordering of the source language to better match the phrase structure of the target language has been shown to improve the performance of phrase-based Statistical Machine Translation.
This paper applies syntactic reordering to English-to-Arabic translation.
It introduces reordering rules, and motivates them linguistically.
It also studies the effect of combining reordering with Arabic morphological segmentation, a preprocessing technique that has been shown to improve Arabic-English and English-Arabic translation.
We report on results in the news text domain, the UN text domain and in the spoken travel domain.
<newSection> 1 Introduction Phrase-based Statistical Machine Translation has proven to be a robust and effective approach to machine translation, providing good performance without the need for explicit linguistic information.
Phrase-based SMT systems, however, have limited capabilities in dealing with long distance phenomena, since they rely on local alignments.
Automatically learned reordering models, which can be conditioned on lexical items from both the source and the target, provide some limited reordering capability when added to SMT systems.
One approach that explicitly deals with long distance reordering is to reorder the source side to better match the target side, using predefined rules.
The reordered source is then used as input to the phrase-based SMT system.
This approach indirectly incorporates structure information since the reordering rules are applied on the parse trees of the source sentence.
Obviously, the same reordering has to be applied to both training data and test data.
Despite the added complexity of parsing the data, this technique has shown improvements, especially when good parses of the source side exist.
It has been successfully applied to German-toEnglish and Chinese-to-English SMT (Collins et al., 2005; Wang et al., 2007).
In this paper, we propose the use of a similar approach for English-to-Arabic SMT.
Unlike most other work on Arabic translation, our work is in the direction of the more morphologically complex language, which poses unique challenges.
We propose a set of syntactic reordering rules on the English source to align it better to the Arabic target.
The reordering rules exploit systematic differences between the syntax of Arabic and the syntax of English; they specifically address two syntactic constructs.
The first is the Subject-Verb order in independent sentences, where the preferred order in written Arabic is Verb-Subject.
The second is the noun phrase structure, where many differences exist between the two languages, among them the order of adjectives, compound nouns and genitive constructs, as well as the way definiteness is marked.
The implementation of these rules is fairly straightforward since they are applied to the parse tree.
It has been noted in previous work (Habash, 2007) that syntactic reordering does not improve translation if the parse quality is not good enough.
Since in this paper our source language is English, the parses are more reliable, and result in more correct reorderings.
We show that using the reordering rules results in gains in the translation scores and study the effect of the training data size on those gains.
This paper also investigates the effect of using morphological segmentation of the Arabic target in combination with the reordering rules.
Morphological segmentation has been shown to benefit Arabic-to-English (Habash and Sadat, 2006) and English-to-Arabic (Badr et al., 2008) translation, although the gains tend to decrease with increasing training data size.
Section 2 provides linguistic motivation for the paper.
It describes the rich morphology of Arabic, and its implications on SMT.
It also describes the syntax of the verb phrase and noun phrase in Arabic, and how they differ from their English counterparts.
In Section 3, we describe some of the relevant previous work.
In Section 4, we present the preprocessing techniques used in the experiments.
Section 5 describes the translation system, the data used, and then presents and discusses the experimental results from three domains: news text, UN data and spoken dialogue from the travel domain.
The final section provides a brief summary and conclusion.
<newSection> 2 Arabic Linguistic Issues Arabic has a complex morphology compared to English.
The Arabic noun and adjective are inflected for gender and number; the verb is inflected in addition for tense, voice, mood and person.
Various clitics can attach to words as well: Conjunctions, prepositions and possessive pronouns attach to nouns, and object pronouns attach to verbs.
The example below shows the decompo-sition into stems and clitics of the Arabic verb phrase wsyqAblhm1 and noun phrase wbydh, both of which are written as one word: and will meet-3SM them and he will meet them Although the Arabic language family consists of many dialects, none of them has a standard orthography.
This affects the consistency of the orthography of Modern Standard Arabic (MSA), the only written variety of Arabic.
Certain characters are written inconsistently in different data sources: Final ’y’ is sometimes written as ’Y’ (Alif mqSwrp), and initial Alif hamza (The Buckwal-ter characters ’<’ and ’�’) are written as bare alif (A).
Arabic is usually written without the diacritics that denote short vowels.
This creates an ambiguity at the word level, since a word can have more than one reading.
These factors adversely affect the performance of Arabic-to-English SMT, especially in the English-to-Arabic direction.
Simple pattern matching is not enough to perform morphological analysis and decomposition, since a certain string of characters can, in principle, be either an affixed morpheme or part of the base word itself.
Word-level linguistic information as well as context analysis are needed.
For example the written form wly can mean either ruler or and for me, depending on the context.
Only in the latter case should it be decomposed.
In this section, we describe a number of syntactic facts about Arabic which are relevant to the reordering rules described in Section 4.2.
In Arabic, the main sentence usually has the order Verb-Subject-Object (VSO).
The order Subject-Verb-Object (SVO) also occurs, but is less frequent than VSO.
The verb agrees with the subject in gender and number in the SVO order, but only in gender in the VSO order (Examples 2c and 2d). b. w+ b+ yd +h (2) a.
Akl Alwld AltfAHp and with hand his ate-3SM the-boy the-apple and with his hand An Arabic corpus will, therefore, have more surface forms than an equivalent English corpus, and will also be sparser.
In the LDC news corpora used in this paper (see Section 5.2), the average English sentence length is 33 words compared to the Arabic 25 words.
In a dependent clause, the order must be SVO, as illustrated by the ungrammaticality of Example 3b below.
As we discuss in more detail later, this distinction between dependent and independent clauses has to be taken into account when the syntactic reordering rules are applied.
(3) a.
qAl An Alwld Akl AltfAHp said-3SM that the-boy ate the-apple he said that the boy ate the apple AltfAHp the-apple he said that the boy ate the apple Another pertinent fact is that the negation particle has to always preceed the verb: The Arabic noun phrase can have constructs that are quite different from English.
The adjective in Arabic follows the noun that it modifies, and it is marked with the definite article, if the head noun is definite: The Arabic equivalent of the English possessive, compound nouns and the of-relationship is the Arabic idafa construct, which compounds two or more nouns.
Therefore, N1’s N2 and N2 of N1 are both translated as N2 N1 in Arabic.
As Example 6b shows, this construct can also be chained recursively.
(6) a.
bAb Albyt door the-house the house’s door b.
mftAH bAb Albyt key door the-house The key to the door of the house Example 6 also shows that an idafa construct is made definite by adding the definite article Al- to the last noun in the noun phrase.
Adjectives follow the idafa noun phrase, regardless of which noun in the chain they modify.
Thus, Example 7 is ambiguous in that the adjective kbyr (big) can modify any of the preceding three nouns.
The same is true for relative clauses that modify a noun.
(7) mftAH bAb Albyt Alkbyr key door the-house the-big These and other differences between the Arabic and English syntax are likely to affect the quality of automatic alignments, since corresponding words will occupy positions in the sentence that are far apart, especially when the relevant words (e.g. the verb and its subject) are separated by subordinate clauses.
In such cases, the lexicalized distortion models used in phrase-based SMT do not have the capability of performing reorderings correctly.
This limitation adversely affects the translation quality.
<newSection> 3 Previous Work Most of the work in Arabic machine translation is done in the Arabic-to-English direction.
The other direction, however, is also important, since it opens the wealth of information in different domains that is available in English to the Arabic speaking world.
Also, since Arabic is a morphologically richer language, translating into Arabic poses unique issues that are not present in the opposite direction.
The only works on English-to-Arabic SMT that we are aware of are Badr et al.
(2008), and Sarikaya and Deng (2007).
Badr et al. show that using segmentation and recom-bination as pre- and post- processing steps leads to significant gains especially for smaller training data corpora.
Sarikaya and Deng use Joint Morphological-Lexical Language Models to re-rank the output of an English-to-Arabic MT system.
They use regular expression-based segmentation of the Arabic so as not to run into recombi-nation issues on the output side.
Similarly, for Arabic-to-English, Lee (2004), and Habash and Sadat (2006) show that various segmentation schemes lead to improvements that decrease with increasing parallel corpus size.
They use a trigram language model and the Arabic morphological analyzer MADA (Habash and Rambow, 2005) respectively, to segment the Arabic side of their corpora.
Other work on Arabic-to-English SMT tries to address the word reordering problem.
Habash (2007) automatically learns syntactic reordering rules that are then applied to the Arabic side of the parallel corpora.
The words are aligned in a sentence pair, then the Arabic sentence is parsed to extract reordering rules based on how the constituents in the parse tree are reordered on the English side.
No significant improvement is shown with reordering when compared to a baseline that uses a non-lexicalized distance reordering model.
This is attributed in the paper to the poor quality of parsing.
Syntax-based reordering as a preprocessing step has been applied to many language pairs other than English-Arabic.
Most relevant to the approach in this paper are Collins et al.
(2005) and Wang et al.
(2007).
Both parse the source side and then reorder the sentence based on predefined, linguistically motivated rules.
Significant gain is reported for German-to-English and Chinese-to-English translation.
Both suggest that reordering as a preprocessing step results in better alignment, and reduces the reliance on the distortion model.
Popovic and Ney (2006) use similar methods to reorder German by looking at the POS tags for German-to-English and German-toSpanish.
They show significant improvements on test set sentences that do get reordered as well as those that don’t, which is attributed to the improvement of the extracted phrases.
(Xia and McCord, 2004) present a similar approach, with a notable difference: the re-ordering rules are automatically learned from aligning parse trees for both the source and target sentences.
They report a 10% relative gain for English-to-French translation.
Although target-side parsing is optional in this approach, it is needed to take full advantage of the approach.
This is a bigger issue when no reliable parses are available for the target language, as is the case in this paper.
More generally, the use of automatically-learned rules has the advantage of readily applicable to different language pairs.
The use of deterministic, pre-defined rules, however, has the advantage of being linguistically motivated, since differences between the two languages are addressed explicitly.
Moreover, the implementation of pre-defined transfer rules based on target-side parses is relatively easy and cheap to implement in different language pairs.
Generic approaches for translating from English to more morphologically complex languages have been proposed.
Koehn and Hoang (2007) propose Factored Translation Models, which extend phrase-based statistical machine translation by allowing the integration of additional morphological features at the word level.
They demonstrate improvements for English-to-German and English-to-Czech.
Tighter integration of features is claimed to allow for better modeling of the morphology and hence is better than using pre-processing and post-processing techniques.
Avramidis and Koehn (2008) enrich the English side by adding a feature to the Factored Model that models noun case agreement and verb person conjugation, and show that it leads to a more grammatically correct output for English-to-Greek and English-to-Czech translation.
Although Factored Models are well equipped for handling languages that differ in terms of morphology, they still use the same distortion reordering model as a phrase-based MT system.
<newSection> 4 Preprocessing Techniques It has been shown previously work (Badr et al., 2008; Habash and Sadat, 2006) that morphological segmentation of Arabic improves the translation performance for both Arabic-to-English and English-to-Arabic by addressing the problem of sparsity of the Arabic side.
In this paper, we use segmented and non-segmented Arabic on the target side, and study the effect of the combination of segmentation with reordering.
As mentioned in Section 2.1, simple pattern matching is not enough to decompose Arabic words into stems and affixes.
Lexical information and context are needed to perform the decompo-sition correctly.
We use the Morphological Analyzer MADA (Habash and Rambow, 2005) to decompose the Arabic source.
MADA uses SVM-based classifiers of features (such as POS, number, gender, etc.) to score the different analyses of a given word in context.
We apply morphological decomposition before aligning the training data.
We split the conjunction and preposition prefixes, as well as possessive and object pronoun suffixes.
We then glue the split morphemes into one prefix and one suffix, such that any given word is split into at most three parts: prefix+ stem +suffix.
Note that plural markers and subject pronouns are not split.
For example, the word wlAwlAdh (’and for his children’) is segmented into wl+ AwlAd +P:3MS.
Since training is done on segmented Arabic, the output of the decoder must be recombined into its original surface form.
We follow the approach of Badr et. al (2008) in combining the Arabic output, which is a non-trivial task for several reasons.
First, the ending of a stem sometimes changes when a suffix is attached to it.
Second, word endings are normalized to remove orthographic inconsistency between different sources (Section 2.1).
Finally, some words can recombine into more than one grammatically correct form.
To address these issues, a lookup table is derived from the training data that maps the segmented form of the word to its original form.
The table is also useful in recombining words that are erroneously segmented.
If a certain word does not occur in the table, we back off to a set of manually defined recombina-tion rules.
Word ambiguity is resolved by picking the more frequent surface form.
This section presents the syntax-based rules used for re-ordering the English source to better match the syntax of the Arabic target.
These rules are motivated by the Arabic syntactic facts described in Section 2.2.
Much like Wang et al.
(2007), we parse the English side of our corpora and reorder using predefined rules.
Reordering the English can be done more reliably than other source languages, such as Arabic, Chinese and German, since the state-of-the-art English parsers are considerably better than parsers of other languages.
The following rules for reordering at the sentence level and the noun phrase level are applied to the English parse tree: The transformation rules 1, 2 and 3 are applied in this order, since they interact although they do not conflict.
So, the real value of the Egyptian pound —* value the Egyptian the pound the real The VP reordering rule is independent.
<newSection> 5 Experiments For the English source, we first tokenize using the Stanford Log-linear Part-of-Speech Tagger (Toutanova et al., 2003).
We then proceed to split the data into smaller sentences and tag them using Ratnaparkhi’s Maximum Entropy Tagger (Ratnaparkhi, 1996).
We parse the data using the Collins Parser (Collins, 1997), and then tag person, location and organization names using the Stanford Named Entity Recognizer (Finkel et al., 2005).
On the Arabic side, we normalize the data by changing final ’Y’ to ’y’, and changing the various forms of Alif hamza to bare Alif, since these characters are written inconsistently in some Arabic sources.
We then segment the data using MADA according to the scheme explained in Section 4.1.
The English source is aligned to the segmented Arabic target using the standard MOSES (MOSES, 2007) configuration of GIZA++ (Och and Ney, 2000), which is IBM Model 4, and decoding is done using the phrase-based SMT system MOSES.
We use a maximum phrase length of 15 to account for the increase in length of the segmented Arabic.
We also use a lexicalized bidirectional reordering model conditioned on both the source and target sides, with a distortion limit set to 6.
We tune using Och’s algorithm (Och, 2003) to optimize weights for the distortion model, language model, phrase translation model and word penalty over the BLEU metric (Papineni et al., 2001).
For the segmented Arabic experiments, we experiment with tuning using non-segmented Arabic as a reference.
This is done by recombining the output before each tuning iteration is scored and has been shown by Badr et. al (2008) to perform better than using segmented Arabic as reference.
We report results on three domains: newswire text, UN data and spoken dialogue from the travel domain.
It is important to note that the sentences in the travel domain are much shorter than in the news domain, which simplifies the alignment as well as reordering during decoding.
Also, since the travel domain contains spoken Arabic, it is more biased towards the Subject-Verb-Object sentence order than the Verb-Subject-Object order more common in the news domain.
Also note that since most of our data was originally intended for Arabic-to-English translation, our test and tuning sets have only one reference, and therefore, the BLEU scores we report are lower than typical scores reported in the literature on Arabic-toEnglish.
The news training data consists of several LDC corpora2.
We construct a test set by randomly picking 2000 sentences.
We pick another 2000 sentences randomly for tuning.
Our final training set consists of 3 million English words.
We also test on the NIST MT 05 “test set while tuning on both the NIST MT 03 and 04 test sets.
We use the first English reference of the NIST test sets as the source, and the Arabic source as our reference.
For in terms of the BLEU Metric.
the language model, we use 35 million words from the LDC Arabic Gigaword corpus, plus the Arabic side of the 3 million word training corpus.
Experimentation with different language model orders shows that the optimal model orders are 4-grams for the baseline system and 6-grams for the segmented Arabic.
The average sentence length is 33 for English, 25 for non-segmented Arabic and 36 for segmented Arabic.
To study the effect of syntactic reordering on larger training data sizes, we use the UN English-Arabic parallel text (LDC2003T05).
We experiment with two training data sizes: 30 million and 3 million words.
The test and tuning sets are comprised of 1500 and 500 sentences respectively, chosen at random.
For the spoken domain, we use the BTEC 2007 Arabic-English corpus.
The training set consists of 200K words, the test set has 500 sentences and the tuning set has 500 sentences.
The language model consists of the Arabic side of the training data.
Because of the significantly smaller data size, we use a trigram LM for the baseline, and a 4-gram for segmented Arabic.
In this case, the average sentence length is 9 for English, 8 for Arabic, and 10 for segmented Arabic.
The translation scores for the News domain are shown in Table 1.
The notation used in the table is as follows: The reordering notation is explained in Section 4.2.
All results are in terms of the BLEU metric.
It is important to note that the gain that we report in terms of BLEU are more significant that comparable gains on test sets that have multiple references, since our test sets have only one reference.
Any amount of gain is a result of additional n-gram precision with one reference.
We note that the gain achieved from the reordering of the non-segmented and segmented systems are comparable.
Replicating the before adjectives hurts the scores, possibly because it increases the sentence length noticeably, and thus deteriorates the alignments’ quality.
We note that the gains achieved by reordering on the NIST test set are smaller than the improvements on the random test set.
This is due to the fact that the sentences in the NIST test set are longer, which adversely affects the parsing quality.
The average English sentence length is 33 words in the NIST test set, while the random test set has an average sentence length of 29 words.
Table 2 shows the reordering gains of the non-segmented Arabic by sentence length.
Short sentences are sentences that have less that 40 words of English, while long sentences have more than 40 words.
Out of the 1055 sentence in the NIST test set 719 are short and 336 are long.
We also report oracle scores in Table 3 for combining the baseline system with the reordering systems, as well as the percentage of oracle sentences produced by the reordered system.
The oracle score is computed by starting with the reordered system’s candidate translations and iterating over all the sentences one by one: we replace each sentence with its corresponding baseline system translation then in terms of the BLEU Metric.
compute the total BLEU score of the entire set.
If the score improves, then the sentence in question is replaced with the baseline system’s translation, otherwise it remains unchanged and we move on to the next one.
In Table 4, we report results on the UN corpus for different training data sizes.
It is important to note that although gains from VP reordering stay constant when scaled to larger training sets, gains from NP+PP reordering diminish.
This is due to the fact that NP reordering tend to be more localized then VP reorderings.
Hence with more training data the lexicalized reordering model becomes more effective in reordering NPs.
In Table 5, we report results on the BTEC corpus for different segmentation and reordering scheme combinations.
We should first point out that all sentences in the BTEC corpus are short, simple and easy to align.
Hence, the gain introduced by reordering might not be enough to offset the errors introduced by the parsing.
We also note that spoken Arabic usually prefers the Subject-Verb-Object sentence order, rather than the Verb-Subject-Object sentence order of written Arabic.
This explains the fact that no gain is observed when the verb phrase is reordered.
Noun phrase reordering produces a significant gain with non-segmented Arabic.
Replicating the definite article the in the noun phrase does not create alignment problems as is the case with the newswire data, since the sentences are considerably shorter, and hence the 0.74 point gain observed on the segmented Arabic system.
That gain does not translate to the non-segmented Arabic system since in that case the definite article Al remains attached to its head word.
<newSection> 6 Conclusion This paper presented linguistically motivated rules that reorder English to look like Arabic.
We showed that these rules produce significant gains.
We also studied the effect of the interaction between Arabic morphological segmentation and syntactic reordering on translation results, as well as how they scale to bigger training data sizes.
<newSection> Acknowledgments We would like to thank Michael Collins, Ali Mo-hammad and Stephanie Seneff for their valuable comments.
<newSection> References<newSection> Abstract We propose a system which builds, in a semi-supervised manner, a resource that aims at helping a NER system to annotate corpus-specific named entities.
This system is based on a distributional approach which uses syntactic dependencies for measuring similarities between named entities.
The specificity of the presented method however, is to combine a clique-based approach and a clustering technique that amounts to a soft clustering method.
Our experiments show that the resource constructed by using this clique-based clustering system allows to improve different NER systems.
<newSection> 1 Introduction In Information Extraction domain, named entities (NEs) are one of the most important textual units as they express an important part of the meaning of a document.
Named entity recognition (NER) is not a new domain (see MUC1 and ACE2 conferences) but some new needs appeared concerning NEs processing.
For instance the NE Oxford illustrates the different ambiguity types that are interesting to address: Oxford is also a company unlike Newcastle.
The main goal of our system is to act in a complementary way with an existing NER system, in order to enhance its results.
We address two kinds of issues: first, we want to detect and correctly annotate corpus-specific NEs3 that the NER system could have missed; second, we want to correct some wrong annotations provided by the existing NER system due to ambiguity.
In section 3, we give some examples of such corrections.
The paper is organized as follows.
We present, in section 2, the global architecture of our system and from §2.1 to §2.6, we give details about each of its steps.
In section 3, we present the evaluation of our approach when it is combined with other classic NER systems.
We show that the resulting hybrid systems perform better with respect to F-measure.
In the best case, the latter increased by 4.84 points.
Furthermore, we give examples of successful correction of NEs annotation thanks to our approach.
Then, in section 4, we discuss about related works.
Finally we sum up the main points of this paper in section 5.
<newSection> 2 Description of the system Given a corpus, the main objectives of our system are: to detect potential NEs; to compute the possible annotations for each NE and then; to annotate each occurrence of these NEs with the right annotation by analyzing its local context.
We assume that this corpus dependent approach allows an easier NE annotation.
Indeed, even if a NE such as Oxford can have many annotation types, it will certainly have less annotation possibilities in a specific corpus.
Figure 1 presents the global architecture of our system.
The most important part concerns steps 3 (§2.3) and 4 (§2.4).
The aim of these sub-processes is to group NEs which have the same annotation with respect to a given context.
On the one hand, clique-based methods (see §2.3 for details on cliques) are interesting as they allow the same NE to be in different cliques.
In other words, cliques allow to represent the different possible annotations of a NE.
The clique-based approach drawback however, is the over production of cliques which corresponds to an artificial over production of possible annotations for a NE.
On the other hand, clustering methods aim at structuring a data set and such techniques can be seen as data compression processes.
However, a simple NEs hard clustering doesn’t allow a NE to be in several clusters and thus to express its different annotations.
Then, our proposal is to combine both methods in a clique-based clustering framework.
This combination leads to a soft-clustering approach that we denote CBC system.
The following paragraphs, from 2.1 to 2.6, describe the respective steps mentioned in Figure 1.
relation with a noun as governee argument (e.g. The list of potential NEs extracted from the corpus will be denoted NE and the number of NEs |NE|.
The distributional approach aims at evaluating a distance between words based on their syntactic distribution.
This method assumes that words which appear in the same contexts are semantically similar (Harris, 1951).
To construct the distributional space associated to a corpus, we use a robust parser (in our experiments, we used XIP parser (Ait et al., 2002)) to extract chunks (i.e. nouns, noun phrases, ... ) and syntactic dependencies between these chunks.
Given this parser’s output, we identify triple instances.
Each triple has the form w1.R.w2 where w1 and w2 are chunks and R is a syntactic relation (Lin, 1998), (Kilgarriff et al., 2004).
One triple gives two contexts (1.w1.R and 2.w2.R) and two chunks (w1 and w2).
Then, we only select chunks w which belong to NE.
Each point in the distributional space is a NE and each dimension is a syntactic context.
CT denotes the set of all syntactic contexts and |CT |represents its cardinal.
We illustrate this construction on the sentence “provide Albania with food aid”.
We obtain the three following triples (note that aid and food aid are considered as two different chunks): provide VERB•I-OBJ•Albania NOUN provide VERB•PREP WITH•aid NOUN provide VERB•PREP WITH•food aid NP From these triples, we have the following chunks and contexts4: <newSection> Chunks: Contexts: Different methods exist for detecting potential NEs.
In our system, we used some lexico-syntactic constraints to extract expressions from a corpus because it allows to detect some corpus-specific NEs.
In our approach, a potential NE is a noun starting with an upper-case letter or a noun phrase which is (see (Ehrmann and Jacquet, 2007) for similar use): We also use an heuristic in order to reduce the over production of chunks and contexts: in our experiments for example, each NE and each context should appear more than 10 times in the corpus for being considered.
D is the resulting (|NE |x |CT|) NE-Context matrix where ei : i = 1, ...
, |NE |is a NE and A clique in a graph is a set of pairwise adjacent nodes which is equivalent to a complete subgraph.
A maximal clique is a clique that is not a subset of any other clique.
Maximal cliques computation was already employed for semantic space representation (Ploux and Victorri, 1998).
In this work, cliques of lexical units are used to represent a precise meaning.
Similarly, we compute cliques of NEs in order to represent a precise annotation.
For example, Oxford is an ambiguous NE but a clique such as <Cambridge, Oxford, Edinburgh University, Edinburgh, Oxford University> allows to focus on the specific annotation <organization> (see (Ehrmann and Jacquet, 2007) for similar use).
Given the distributional space described in the previous paragraph, we use a probabilistic framework for computing similarities between NEs.
The approach that we propose is inspired from the language modeling framework introduced in the information retrieval field (see for example (Lavrenko and Croft, 2003)).
Then, we construct cliques of NEs based on these similarities.
We first compute the maximum likelihood estimation for a NE ei to be associated with a conj=1 D(ei, cj) is the total occurrences of the NE ei in the corpus.
This leads to sparse data which is not suitable for measuring similarities.
In order to counter this problem, we use the Jelinek-Mercer smoothing method: D0(ei, cj) = λPml(cj|ei) + (1 − λ)Pml(cj|CORP) where CORP is the corpus and ments we took λ = 0.5.
Given D0, we then use the cross-entropy as a similarity measure between NEs.
Let us denote by 2.3.2 From similarity matrix to adjacency matrix Next, we convert s into an adjacency matrix denoted s.
In a first step, we binarize s as follows.
Let us denote fei1, ...
, ei|NE|}, the list of NEs ranked according to the descending order of their similarity with ei.
Then, L(ei) is the list of NEs which are considered as the nearest neighbors of ei according to the following definition: where a E [0, 1] and b E f1,..., |NE|}.
L(ei) gathers the most significant nearest neighbors of ei by choosing the ones which bring the a most relevant similarities providing that the neighborhood’s size doesn’t exceed b.
This approach can be seen as a flexible k-nearest neighbor method.
In our experiments we chose a = 20% and b = 10.
Finally, we symmetrize the similarity matrix as follows and we obtain s: ~ 1 if ei0 E L(ei) or ei E L(ei0) Given s, the adjacency matrix between NEs, we compute the set of maximal cliques of NEs denoted CLI.
Then, we construct the matrix T of general term: where clik is an element of CLI.
T will be the input matrix for the clustering method.
In the following, we also use clik for denoting the vector represented by (T(clik, e1), ... ,T(clik, e|NE|)).
Figure 2 shows some cliques which contain Oxford that we can obtain with this method.
This figure also illustrates the over production of cliques since at least cli8, cli10 and cli12 can be annotated as <organization>.
We use a clustering technique in order to group cliques of NEs which are mutually highly similar.
The clusters of cliques which contain a NE allow to find the different possible annotations of this NE.
This clustering technique must be able to construct “pure” clusters in order to have precise annotations.
In that case, it is desirable to avoid fixing the number of clusters.
That’s the reason why we propose to use the Relational Analysis approach described below.
We propose to apply the Relational Analysis approach (RA) which is a clustering model that doesn’t require to fix the number of clusters (Michaud and Marcotorchino, 1980), (B´ed´ecarrax and Warnesson, 1989).
This approach takes as input a similarity matrix.
In our context, since we want to cluster cliques of NEs, the corresponding similarity matrix S between cliques is given by the dot products matrix taken from T: S = T · T0.
The general term of this similarity matrix is: S(clik, clik0) = Skk0 = (clik, clik0).
Then, we want to maximize the following clustering function: where S+ = {(clik, clik0) : Skk0 > 0}.
In other words, clik and clik0 have more chances to be in the same cluster providing that their similarity measure, Skk0, is greater or equal to the mean average of positive similarities.
As the objective function is linear with respect to X and as the constraints that X must respect are linear equations, we can solve the clustering problem using an integer linear programming solver.
However, this problem is NP-hard.
As a result, in practice, we use heuristics for dealing with large data sets.
The presented heuristic is quite similar to another algorithm described in (Hartigan, 1975) known as the “leader” algorithm.
But unlike this last approach which is based upon euclidean distances and inertial criteria, the RA heuristic aims at maximizing the criterion given in (6).
A sketch of this heuristic is given in Algorithm 1, (see (Marco-torchino and Michaud, 1981) for further details).
Take the first clique clik as the first element of the first cluster rc = 1 where rc is the current number of cluster for q = 1 to nbitr do for k = 1 to JCLHJ do for l = 1 to rc do Compute the contribution of clique clik with cluster clul: contl = Eclik0 ∈clul (Skk0 − m) clul∗ is the cluster id which has the highest contribution with clique clik and contl∗ is the corresponding We have to provide a number of iterations or/and a delta threshold in order to have an approximate solution in a reasonable processing time.
Besides, it is also required a maximum number of clusters but since we don’t want to fix this parameter, we put by default κmax = |CLI|.
Basically, this heuristic has a O(nbitr xκmax x |CLI|) computation cost.
In general terms, we can assume that nbitr << |CLI|, but not κmax << |CLI|.
Thus, in the worst case, the algorithm has Now, we want to exploit the clusters of cliques in order to annotate NE occurrences.
Then, we need to construct a NE resource where for each pair (NE x syntactic context) we have an annotation.
To this end, we need first, to assign a cluster to each pair (NE x syntactic context) (§2.5.1) and second, to assign each cluster an annotation (§2.5.2).
2.5.1 Cluster assignment to each pair (NE x syntactic context) For each cluster clul we provide a score Fc(cj, clul) for each context cj and a score 5We only represent the NEs and their frequency in the cluster which corresponds to the number of cliques which contain the NEs.
Furthermore, we represent the most relevant contexts for this cluster according to equation (7) introduced in the following.
Fe(ei, clul) for each NE ei.
These scores6 are given by: Given a NE ei and a syntactic context cj, we now introduce the contextual cluster assignment matrix Actxt(ei, cj) as follows: Actxt(ei, cj) = clu* where: clu* = Argmax{clul:clulE)ei;F,(ei,clul)>1}Fc(cj, clul).
In other words, clu* is the cluster for which we find more than one occurrence of ei and the highest score related to the context cj.
Furthermore, we compute a default cluster assignment matrix Adef, which does not depend on the local context: Adef(ei) = clu* where: clu* = Argmax{clul:clulE){clik:clikE)ei}}|clik|.
In other words, clu* is the cluster containing the biggest clique clik containing ei.
So far, the different steps that we have introduced were unsupervised.
In this paragraph, our aim is to give a correct annotation to each cluster (hence, to all NEs in this cluster).
To this end, we need some annotation seeds and we propose two different semi-supervised approaches (regarding the classification given in (Nadeau and Sekine, 2007)).
The first one is the manual annotation of some clusters.
The second one proposes an automatic cluster annotation and assumes that we have some NEs that are already annotated.
Manual annotation of clusters This method is fastidious but it is the best way to match the corpus data with a specific guidelines for annotating NEs.
It also allows to identify new types of annotation.
We used the ACE2007 guidelines for manually annotating each cluster.
However, our CBC system leads to a high number of clusters of cliques and we can’t annotate each of them.
Fortunately, it also leads to a distribution of the clusters’ size (number of cliques by cluster) which is � ei�clul similar to a Zipf distribution.
Consequently, in our experiments, if we annotate the 100 biggest clusters, we annotate around eighty percent of the detected NEs (see §3).
Automatic annotation of clusters We suppose in this context that many NEs in NE are already annotated.
Thus, under this assumption, we have in each cluster provided by the CBC system, both annotated and non-annotated NEs.
Our goal is to exploit the available annotations for refining the annotation of a cluster by implicitly taking into account the syntactic contexts and for propagating the available annotations to NEs which have no annotation.
Given a cluster clue of cliques, #(clue, ei) is the weight of the NE ei in this cluster: it is the number of cliques in clue that contain ei.
For all annotations ap in the set of all possible annotations AN, we compute its associated score in cluster clue: it is the sum of the weights of NEs in clue that is annotated ap.
Then, if the maximal annotation score is greater than a simple majority (half) of the total votes7, we assign the corresponding annotation to the cluster.
We precise that the annotation <none>8 is processed in the same way as any other annotations.
Thus, a cluster can be globally annotated <none>.
The limit of this automatic approach is that it doesn’t allow to annotate new NE types than the ones already available.
In the following, we will denote by Aclu(clue) the annotation of the cluster clue.
The cluster annotation matrix Aclu associated to the contextual cluster assignment matrix Aetxt and the default cluster assignment matrix Adef introduced previously will be called the CBC system’s NE resource (or shortly the NE resource).
2.6 NEs annotation processes using the NE resource In this paragraph, we describe how, given the CBC system’s NE resource, we annotate occurrences of NEs in the studied corpus with respect to its local context.
We precise that for an occurrence of a NE ei its associated local context is the set of syntactical dependencies cj in which ei is involved.
2.6.1 NEs annotation process for the CBC system Given a NE occurrence and its local context we can use Aetxt(ei, cj) and Adef(ei) in order to get the default annotation Aclu(Adef(ei)) and the list of contextual annotations {Aclu(Aetxt(ei, cj))}j.
Then for annotating this NE occurrence using our NE resource, we apply the following rules: 2.6.2 NEs annotation process for an hybrid system We place ourselves into an hybrid situation where we have two NER systems (NER 1 + NER 2) which provide two different lists of annotated NEs.
We want to combine these two systems when annotating NEs occurrences.
Therefore, we resolve any conflicts by applying the following rules: <newSection> 3 Experiments The system described in this paper rather target corpus-specific NE annotation.
Therefore, our experiments will deal with a corpus of recent news articles (see (Shinyama and Sekine, 2004) for motivations regarding our corpus choice) rather than well-known annotated corpora.
Our corpus is constituted of news in English published on the web during two weeks in June 2008.
This corpus is constituted of around 300,000 words (10Mb) which doesn’t represent a very large corpus.
These texts were taken from various press sources and they involve different themes (sports, technology, ...
). We extracted randomly a subset of articles and manually annotated 916 NEs (in our experiments, we deal with three types of annotation namely <person>, <organization> and <location>).
This subset constitutes our test set.
In our experiments, first, we applied the XIP parser (Ait et al., 2002) to the whole corpus in order to construct the frequency matrix D given by (1).
Next, we computed the similarity matrix between NEs according to (2) in order to obtain s defined by (4).
Using the latter, we computed cliques of NEs that allow us to obtain the assignment matrix T given by (5).
Then we applied the clustering heuristic described in Algorithm 1.
At this stage, we want to build the NE resource using the clusters of cliques.
Therefore, as described in §2.5, we applied two kinds of clusters annotations: the manual and the automatic processes.
For the first one, we manually annotated the 100 biggest clusters of cliques.
For the second one, we exploited the annotations provided by XIP NER (Brun and Hag`ege, 2004) and we propagated these annotations to the different clusters (see §2.5.2).
The different materials that we obtained constitute the CBC system’s NE resource.
Our aim now is to exploit this resource and to show that it allows to improve the performances of different classic NER systems.
The different NER systems that we tested are the following ones: corpora (CoNLL, MUC6, MUC7 and ACE): ner-eng-ie.crf-3-all2008-distsim.ser.gz (Finkel et al., 2005) (line 3 in Table 1), the combination of pairs taken among the set of the three last-mentioned NER systems (lines 5 to 7 in Table 1).
Notice that these baseline hybrid systems use the annotation combination process described in §2.6.1.
In Table 1 we first reported in each line, the results given by each system when they are applied alone (figures in italics).
These performances represent our baselines.
Second, we tested for each baseline system, an extended hybrid system that integrates the CBC-NER systems (with respect to the combination process detailed in §2.6.2).
The first two lines of Table 1 show that the two CBC-NER systems alone lead to rather poor results.
However, our aim is to show that the CBC-NER system is, despite its low performances alone, complementary to other basic NER systems.
In other words, we want to show that the exploitation of the CBC system’s NE resource is beneficial and non-redundant compared to other baseline NER systems.
This is actually what we obtained in Table 1 as for each line from 2 to 7, the extended hybrid systems that integrate the CBC-NER systems (M or A) always perform better than the baseline either in terms of precision9 or recall.
For each line, we put in bold the best performance according to the F-measure.
These results allow us to show that the NE resource built using the CBC system is complementary to any baseline NER systems and that it allows to improve the results of the latter.
In order to illustrate why the CBC-NER systems are beneficial, we give below some examples taken from the test corpus for which the CBC system A had allowed to improve the performances by respectively disambiguating or correcting a wrong annotation or detecting corpus-specific NEs.
First, in the sentence “From the start, his parents, Lourdes and Hemery, were with him.”, the baseline hybrid system Stanford + XIP annotated the ambiguous NE “Lourdes” as <location> whereas Stanford + XIP + CBC A gave the correct annotation <person>.
Second, in the sentence “Got 3 percent chance of survival, what ya gonna do?”
The back read, ”A) Fight Through, b) Stay Strong, c) Overcome Because I Am a Warrior.”, the baseline hybrid system Stanford + XIP annotated “Warrior” as <organization> whereas Stanford + XIP + CBC A corrected this annotation with <none>.
Finally, in the sentence “Matthew, also a fa-vorite to win in his fifth and final appearance, was stunningly eliminated during the semifinal round Friday when he misspelled “secernent”.”, the baseline hybrid system Stanford + XIP didn’t give any annotation to “Matthew” whereas Stanford + XIP + CBC A allowed to give the annotation <person>.
<newSection> 4 Related works Many previous works exist in NEs recognition and classification.
However, most of them do not build a NEs resource but exploit external gazetteers (Bunescu and Pasca, 2006), (Cucerzan, 2007).
A recent overview of the field is given in (Nadeau and Sekine, 2007).
According to this paper, we can classify our method in the category of semi-supervised approaches.
Our proposal is close to (Cucchiarelli and Velardi, 2001) as it uses syntactic relations (§2.2) and as it relies on existing NER systems (§2.6.2).
However, the partic-ularity of our method concerns the clustering of cliques of NEs that allows both to represent the different annotations of the NEs and to group the latter with respect to one precise annotation according to a local context.
Regarding this aspect, (Lin and Pantel, 2001) and (Ngomo, 2008) also use a clique computation step and a clique merging method.
However, they do not deal with ambiguity of lexical units nor with NEs.
This means that, in their system, a lexical unit can be in only one merged clique.
From a methodological point of view, our proposal is also close to (Ehrmann and Jacquet, 2007) as the latter proposes a system for NEs fine-grained annotation, which is also corpus dependent.
However, in the present paper we use all syntactic relations for measuring the similarity between NEs whereas in the previous mentioned work, only specific syntactic relations were exploited.
Moreover, we use clustering techniques for dealing with the issue related to over production of cliques.
In this paper, we construct a NE resource from the corpus that we want to analyze.
In that context, (Pasca, 2004) presents a lightly supervised method for acquiring NEs in arbitrary categories from unstructured text of Web documents.
However, Pasca wants to improve web search whereas we aim at annotating specific NEs of an analyzed corpus.
Besides, as we want to focus on corpus-specific NEs, our work is also related to (Shinyama and Sekine, 2004).
In this work, the authors found a significant correlation between the similarity of the time series distribution of a word and the likelihood of being a NE.
This result motivated our choice to test our approach on recent news articles rather than on well-known annotated corpora.
<newSection> 5 Conclusion We propose a system that allows to improve NE recognition.
The core of this system is a clique-based clustering method based upon a distributional approach.
It allows to extract, analyze and discover highly relevant information for corpus-specific NEs annotation.
As we have shown in our experiments, this system combined with another one can lead to strong improvements.
Other applications are currently addressed in our team using this approach.
For example, we intend to use the concept of clique-based clustering as a soft clustering method for other issues.
<newSection> References<newSection> Abstract This paper presents the end-to-end evaluation of an automatic simultaneous translation system, built with state-of-the-art components.
It shows whether, and for which situations, such a system might be advantageous when compared to a human interpreter.
Using speeches in English translated into Spanish, we present the evaluation procedure and we discuss the results both for the recognition and translation components as well as for the overall system.
Even if the translation process remains the Achilles’ heel of the system, the results show that the system can keep at least half of the information, becoming potentially useful for final users.
<newSection> 1 Introduction Anyone speaking at least two different languages knows that translation and especially simultaneous interpretation are very challenging tasks.
A human translator has to cope with the special nature of different languages, comprising phenomena like terminology, compound words, idioms, dialect terms or neologisms, unexplained acronyms or abbreviations, proper names, as well as stylistic and punctuation differences.
Further, translation or interpretation are not a word-by-word rendition of what was said or written in a source language.
Instead, the meaning and intention of a given sentence have to be reexpressed in a natural and fluent way in another language.
Most professional full-time conference interpreters work for international organizations like the United Nations, the European Union, or the African Union, whereas the world’s largest employer of translators and interpreters is currently the European Commission.
In 2006, the European Parliament spent about 300 million Euros, 30% of its budget, on the interpretation and translation of the parliament speeches and EU documents.
Generally, about 1.1 billion Euros are spent per year on the translating and interpreting services within the European Union, which is around 1% of the total EU-Budget (Volker Steinbiss, 2006).
This paper presents the end-to-end evaluation of an automatic simultaneous translation system, built with state-of-the-art components.
It shows whether, and in which cases, such a system might be advantageous compared to human interpreters.
<newSection> 2 Challenges in Human Interpretation According to Al-Khanji et al.
(2000), researchers in the field of psychology, linguistics and interpretation seem to agree that simultaneous interpretation (SI) is a highly demanding cognitive task involving a basic psycholinguistic process.
This process requires the interpreter to monitor, store and retrieve the input of the source language in a continuous manner in order to produce the oral rendition of this input in the target language.
It is clear that this type of difficult linguistic and cognitive operation will force even professional interpreters to elaborate lexical or synthetic search strategies.
Fatigue and stress have a negative effect on the interpreter, leading to a decrease in simultaneous interpretation quality.
In a study by Moser-Mercer et al.
(1998), in which professional speakers were asked to work until they could no longer provide acceptable quality, it was shown that (1) during the first 20 minutes the frequency of errors rose steadily, (2) the interpreters, however, seemed to be unaware of this decline in quality, (3) after 60 minutes, all subjects made a total of 32.5 meaning errors, and (4) in the category of nonsense the number of errors almost doubled after 30 minutes on the task.
Since the audience is only able to evaluate the simultaneously interpreted discourse by its form, the fluency of an interpretation is of utmost importance.
According to a study by Kopczynski (1994), fluency and style were third on a list of priorities (after content and terminology) of elements rated by speakers and attendees as contributing to quality.
Following the overview in (Yagi, 2000), an interpretation should be as natural and as authentic as possible, which means that artificial pauses in the middle of a sentence, hesitations, and false-starts should be avoided, and tempo and intensity of the speaker’s voice should be imitated.
Another point to mention is the time span between a source language chunk and its target language chunk, which is often referred to as ear-voice-span.
Following the summary in (Yagi, 2000), the ear-voice-span is variable in duration depending on some source and target language variables, like speech delivery rate, information density, redundancy, word order, syntactic characteristics, etc.
Short delays are usually preferred for several reasons.
For example, the audience is irritated when the delay is too large and is soon asking whether there is a problem with the interpretation.
<newSection> 3 Automatic Simultaneous Translation Given the explanations above on human interpretation, one has to weigh two factors when considering the use of simultaneous translation systems: translation quality and cost.
The major disadvantage of an automatic system compared to human interpretation is its translation quality, as we will see in the following sections.
Current state-of-the-art systems may reach satisfactory quality for people not understanding the lecturer at all, but are still worse than human interpretation.
Nevertheless, an automatic system may have considerable advantages.
One such advantage is its considerable short-term memory: storing long sequences of words is not a problem for a computer system.
Therefore, compensatory strategies are not necessary, regardless of the speaking rate of the speaker.
However, depending on the system’s translation speed, latency may increase.
While it is possible for humans to compress the length of an utterance without changing its meaning (summarization), it is still a challenging task for automatic systems.
Human simultaneous interpretation is quite expensive, especially due to the fact that usually two interpreters are necessary.
In addition, human interpreters require preparation time to become familiar with the topic.
Moreover, simultaneous interpretation requires a soundproof booth with audio equipment, which adds an overall cost that is unacceptable for all but the most elaborate multilingual events.
On the other hand, a simultaneous translation system also needs time and effort for preparation and adaptation towards the target application, language and domain.
However, once adapted, it can be easily re-used in the same domain, language, etc.
Another advantage is that the transcript of a speech or lecture is produced for free by using an automatic system in the source and target languages.
Figure 1 shows a schematic overview of the simultaneous translation system developed at Uni-versität Karlsruhe (TH) (Fügen et al., 2006b).
The speech of the lecturer is recorded with the help of a close-talk microphone and processed by the speech recognition component (ASR).
The partial hypotheses produced by the ASR module are collected in the resegmentation component, for merging and re-splitting at appropriate “seman-tic” boundaries.
The resegmented hypotheses are then transferred to one or more machine translation components (MT), at least one per language pair.
Different output technologies may be used for presenting the translations to the audience.
For a detailed description of the components as well as the client-server framework used for connecting the components please refer to (Fügen et al., 2006b; Fügen et al., 2006a; Kolss et al., 2006; Fü-gen and Kolss, 2007; Fügen et al., 2001).
The evaluation in speech-to-speech translation jeopardises many concepts and implies a lot of subjectivity.
Three components are involved and an overall system may grow the difficulty of estimating the output quality.
However, two criteria are mainly accepted in the community: measuring the information preservation and determining how much of the translation is understandable.
Several end-to-end evaluations in speech-tospeech translation have been carried out in the last few years, in projects such as JANUS (Gates et al., 1996), Verbmobil (Nübel, 1997) or TC-STAR (Hamon et al., 2007).
Those projects use the main criteria depicted above, and protocols differ in terms of data preparation, rating, procedure, etc.
To our opinion, to evaluate the performance of a complete speech-to-speech translation system, we need to compare the source speech used as input to the translated output speech in the target language.
To that aim, we reused a large part of the evaluation protocol from the TC-STAR project(Hamon et al., 2007).
<newSection> 4 Evaluation Tasks The evaluation is carried out on the simultaneously translated speech of a single speaker’s talks and lectures in the field of speech processing, given in English, and translated into Spanish.
Two data sets were selected from the talks and lectures.
Each set contained three excerpts, no longer than 6 minutes each and focusing on different topics.
The former set deals with speech recognition and the latter with the descriptions of European speech research projects, both from the same speaker.
This represents around 7,200 English words.
The excerpts were manually transcribed to produce the reference for the ASR evaluation.
Then, these transcriptions were manually translated into Spanish by two different translators.
Two reference translations were thus available for the spoken language translation (SLT) evaluation.
Finally, one human interpretation was produced from the excerpts as reference for the end-to-end evaluation.
It should be noted that for the translation system, speech synthesis was used to produce the spoken output.
The system is evaluated as a whole (black box evaluation) and component by component (glass box evaluation): ASR evaluation.
The ASR module is evaluated by computing the Word Error Rate (WER) in case insensitive mode.
SLT evaluation.
For the SLT evaluation, the automatically translated text from the ASR output is compared with two manual reference translations by means of automatic and human metrics.
Two automatic metrics are used: BLEU (Papineni et al., 2001) and mWER (Niessen et al., 2000).
For the human evaluation, each segment is evaluated in relation to adequacy and fluency (White and O’Connell, 1994).
For the evaluation of adequacy, the target segment is compared to a reference segment.
For the evaluation of fluency, the quality of the language is evaluated.
The two types of evaluation are done independently, but each evaluator did both evaluations (first that of fluency, then that of adequacy) for a certain number of segments.
For the evaluation of fluency, evaluators had to answer the question: “Is the text written in good Spanish?”.
For the evaluation of adequacy, evaluators had to answer the question: “How much of the meaning expressed in the reference translation is also expressed in the target translation?”.
For both evaluations, a five-point scale is proposed to the evaluators, where only extreme values are explicitly defined.
Three evaluations are carried out per segment, done by three different evaluators, and segments are divided randomly, because evaluators must not recreate a “story” and thus be influenced by the context.
The total number of judges was 10, with around 100 segments per judge.
Furthermore, the same number of judges was recruited for both categories: experts, from the domain with a knowledge of the technology, and non-experts, without that knowledge.
End-to-End evaluation.
The End-to-End evaluation consists in comparing the speech in the source language to the output speech in the target language.
Two important aspects should be taken into account when assessing the quality of a speech-to-speech system.
First, the information preservation is measured by using “comprehension questionnaires”.
Questions are created from the source texts (the English excerpts), then questions and answers are translated into Spanish by professional translators.
These questions are asked to human judges after they have listened to the output speech in the target language (Spanish).
At a second stage, the answers are analysed: for each answer a Spanish val-idator gives a score according to a binary scale (the information is either correct or incorrect).
This allows us to measure the information preservation.
Three types of questions are used in order to diversify the difficulty of the questions and test the system at different levels: simple Factual (70%), yes/no (20%) and list (10%) questions.
For instance, questions were: What is the larynx responsible for?, Have all sites participating in CHIL built a CHIL room?, Which types of knowledge sources are used by the decoder?, respectively.
The second important aspect of a speech-tospeech system is the quality of the speech output (hereafter quality evaluation).
For assessing the quality of the speech output one question is asked to the judges at the end of each comprehension questionnaire: “Rate the overall quality of this audio sample”, and values go from 1 (“1: Very bad, unusable”) to 5 (“It is very useful”).
Both automatic system and interpreter outputs were evaluated with the same methodology.
Human judges are real users and native Spanish speakers, experts and non-experts, but different from those of the SLT evaluation.
Twenty judges were involved (12 excerpts, 10 evaluations per excerpt and 6 evaluations per judge) and each judge evaluated both automatic and human excerpts on a 50/50 percent basis.
<newSection> 5 Components Results The ASR output has been evaluated using the manual transcriptions of the excerpts.
The overall Word Error Rate (WER) is 11.9%.
Table 1 shows the WER level for each excerpt.
T036 excerpts seem to be easier to recognize automatically than L043 ones, probably due to the more general language of the former.
Each segment within the human evaluation is evaluated 4 times, each by a different judge.
This aims at having a significant number of judgments and measuring the consistency of the human evaluations.
The consistency is measured by computing the Cohen’s Kappa coefficient (Cohen, 1960).
Results show a substantial agreement for fluency (kappa of 0.64) and a moderate agreement for adequacy (0.52).The overall results of the human evaluation are presented in Table 2.
Regarding both experts’ and non-experts’ details, agreement is very similar (0.30 and 0.28, respectively).
Both fluency and adequacy results are over the mean.
They are lower for experts than for non-experts.
This may be due to the fact that experts are more familiar with the domain and therefore more demanding than non experts.
Regarding the detailed evaluation per judge, scores are generally lower for non-experts than for experts.
Scores are rather low, with a mWER of 58.66%, meaning that more than half of the translation is correct.
According to the scoring, the T036 excerpts seem to be easier to translate than the L043 ones, the latter being of a more technical nature.
<newSection> 6 End-to-End Results In this study, ten judges carried out the evaluation for each excerpt.
In order to observe the inter-judges agreement, the global Fleiss’s Kappa coefficient was computed, which allows to measure the agreement between m judges with r criteria of judgment.
This coefficient shows a global agreement between all the judges, which goes beyond Cohen’s Kappa coefficient.
However, a low coefficient requires a more detailed analysis, for instance, by using Kappa for each pair of judges.
Indeed, this allows to see how deviant judges are from the typical judge behaviour.
For m judges, n evaluations and r criteria, the global Kappa is defined as follows: and: Xij is the number of judgments for the ith evaluation and the jth criteria.
Regarding quality evaluation (n = 6, m = 10, r = 5), Kappa values are low for both human interpreters (K = 0.07) and the automatic system (K = 0.01), meaning that judges agree poorly (Landis and Koch, 1977).
This is explained by the extreme subjectivity of the evaluation and the small number of evaluated excerpts.
Looking at each pair of judges and the Kappa coefficients themselves, there is no real agreement, since most of the Kappa values are around zero.
However, some judge pairs show fair agreement, and some others show moderate or substantial agreement.
It is observed, though, that some criteria are not frequently selected by the judges, which limits the statistical significance of the Kappa coefficient.
The limitations are not the same for the comprehension evaluation (n = 60, m = 10, r = 2), since the criteria are binary (i.e. true or false).
Regarding the evaluated excerpts, Kappa values are 0.28 for the automatic system and 0.30 for the interpreter.
According to Landis and Koch (1977), those values mean that judges agree fairly.
In order to go further, the Kappa coefficients were computed for each pair of judges.
Results were slightly better for the interpreter than for the automatic system.
Most of them were between 0.20 and 0.40, implying a fair agreement.
Some judges agreed moderately.
Furthermore, it was also observed that for the 120 available questions, 20 had been answered correctly by all the judges (16 for the interpreter evaluation and 4 for the automatic system one) and 6 had been answered wrongly by all judges (1 for the former and 5 for the latter).
That shows a trend where the interpreter comprehension would be easier than that of the automatic system, or at least where the judgements are less questionable.
Table 4 compares the quality evaluation results of the interpreter to those of the automatic system.
preter and the automatic system [1<5].
As can be seen, with a mean score of 3.03 even for the interpreter, the excerpts were difficult to interpret and translate.
This is particularly so for L043, which is more technical than T036.
The L043-3 excerpt is particularly technical, with formulae and algorithm descriptions, and even a complex description of the human articulatory system.
In fact, L043 provides a typical presentation with an introduction, followed by a deeper description of the topic.
This increasing complexity is reflected on the quality scores of the three excerpts, going from 3.1 to 2.4.
T036 is more fluent due to the less technical nature of the speech and the more general vocabulary used.
However, the T036-2 and T036-3 excerpts get a lower quality score, due to the description of data collections or institutions, and thus the use of named entities.
The interpreter does not seem to be at ease with them and is mispronounc-ing some of them, such as “Grenoble” pronounced like in English instead of in Spanish.
The interpreter seems to be influenced by the speaker, as can also be seen in his use of the neologism “el ce-nario” (“the scenario”) instead of “el escenario”.
Likewise, “Karlsruhe” is pronounced three times differently, showing some inconsistency of the interpreter.
The general trend in quality errors is similar to those of previous evaluations: lengthening words (“seeeeñales”), hesitations, pauses between syllables and catching breath (“caracterís...ticas”), careless mistakes (“probibilidad” instead of “prob-abilidad”), self-correction of wrong interpreting (“reconocien-/reconocimiento”), etc.
An important issue concerns gender and number agreement.
Those errors are explained by the presence of morphological gender in Spanish, like in “estos señales” instead of “estas señales” (“these signals”) together with the speaker’s speed of speech.
The speaker seems to start by default with a masculine determiner (which has no gender in English), adjusting the gender afterward depending on the noun following.
A quick translation may also be the cause for this kind of errors, like “del señal acustico” (“of the acoustic signal”) with a masculine determiner, a feminine substantive and ending in a masculine adjective.
Some translation errors are also present, for instance “computerizar” instead of “calcular” (“compute”).
The errors made by the interpreter help to understand how difficult oral translation is.
This should be taken into account for the evaluation of the automatic system.
The automatic system results, like those of the interpreter, are higher for T036 than for L043.
However, scores are lower, especially for the L043-1 excerpt.
This seems to be due to the type of lexicon used by the speaker for this excerpt, more medical, since the speaker describes the articulatory system.
Moreover, his description is sometimes metaphorical and uses a rather colloquial register.
Therefore, while the interpreter finds it easier to deal with these excerpts (known vocabulary among others) and L043-3 seems to be more complicated (domain-specific, technical aspect), the automatic system finds it more complicated with the former and less with the latter.
In other words, the interpreter has to “understand” what is said in L043-3, contrary to the automatic system, in order to translate.
Scores are higher for the T036 excerpts.
Indeed, there is a high lexical repetition, a large number of named entities, and the quality of the excerpt is very training-dependant.
However, the system runs into trouble to process foreign names, which are very often not understandable.
Differences between T036-1 and the other T036 excerpts are mainly due to the change in topic.
While the former deals with a general vocabulary (i.e. description of projects), the other two excerpts describe the data collection, the evaluation metrics, etc., thus increasing the complexity of translation.
Generally speaking, quality scores of the automatic system are mainly due to the translation component, and to a lesser extent to the recognition component.
Many English words are not translated (“bush”, “keyboards”, “squeaking”, etc.), and word ordering is not always correct.
This is the case for the sentence “how we solve it”, translated into “cómo nos resolvers lo” instead of “cómo lo resolvemos”.
Funnily enough, the problems of gender (“maravillosos aplicaciones” - masc. vs fem.) and number (“pueden real-mente ser aplicado” - plu. vs sing.) the interpreter has, are also found for the automatic system.
Moreover, the translation of compound nouns often shows wrong word ordering, in particular when they are long, i.e. up to three words (e.g. “reconocimiento de habla sistemas” for “speech recognition system” instead of “sistemas de re-conocimiento de habla”).
Finally, some error combinations result in fully non-understandable sentences, such as: “usted tramo se en emacs es squeaking ruido y dries todos demencial” where the following errors take place: The TTS component also contributes to decreasing the output quality.
The prosody module finds it hard to make the sentences sound natural.
Pauses between words are not very frequent, but they do not sound natural (i.e. like catching breath) and they are not placed at specific points, as it would be done by a human.
For instance, the prosody module does not link the noun and its determiner (e.g. “otros aplicaciones”).
Finally, a not user-friendly aspect of the TTS component is the repetition of the same words always pronounced in the same manner, what is quite disturbing for the listener.
Tables 5 and 6 present the results of the comprehension evaluation, for the interpreter and for the automatic system, respectively.
They provide the following information: identifiers of the excerpt: Source data are the same for the interpreter and the automatic system, namely the English speech; subj.
E2E: The subjective results of the end-toend evaluation are done by the same assessors who did the quality evaluation.
This shows the percentage of good answers; fair E2E: The objective verification of the answers.
The audio files are validated to check whether they contain the answers to the questions or not (as the questions were created from the English source).
This shows the maximum percentage of answers an evaluator managed to find from either the interpreter (speaker audio) or the automatic system output (TTS) in Spanish.
For instance, information in English could have been missed by the interpreter because he/she felt that this information was meaningless and could be discarded.
We consider those results as an objective evaluation.
SLT, ASR: Verification of the answers in each component of the end-to-end process.
In order to determine where the information for the automatic system is lost, files from each component (recognised files for ASR, translated files for SLT, and synthesised files for TTS in the “fair E2E” column) are checked.
Regarding Table 5, the interpreter loses 15% of the information (i.e. 15% of the answers were incorrect or not present in the interpreter’s translation) and judges correctly answered 74% of the questions.
Five documents get above 80% of correct results, while judges find almost above 70% of the answers for the six documents.
Regarding the automatic system results (Table 6), the information rate found by judges is just above 50% since, by extension, more than half the questions were correctly answered.
The lowest excerpt, L043-1, gets a rate of 25%, the highest, T036-1, a rate of 76%, which is in agreement with the observation for the quality evaluation.
Information loss can be found in each component, especially for the SLT module (35% of the information is lost here).
It should be noticed that the TTS module made also errors which prevented judges from answering related questions.
Moreover, the ASR module loses 17% of the information.
Those results are certainly due to the specific vocabulary used in this experiment.
So as to objectively compare the interpreter with the automatic system, we selected the questions for which the answers were included in the interpreter files (i.e. those in the “fair E2E” column of Table 5).
The goal was to compare the overall quality of the speech-to-speech translation to interpreters’ quality, without the noise factor of the information missing.
The assumption is that the interpreter translates the “important information” and skips the useless parts of the original speech.
This experiment is to measure the level of this information that is preserved by the automatic system.
So a new subset of results was obtained, on the information kept by the interpreter.
The same study was repeated for the three components and the results are shown in Tables 7 and 8.
tem restricted to the questions for which answers can be found in the interpreter speech [%].
Comparing the automatic system to the interpreter, the automatic system keeps 40% of the information where the interpreter translates the documents correctly.
Those results confirm that ASR loses a lot of information (20%), while SLT loses 10% further, and so does the TTS.
Judges are quite close to the objective validation and found most of the answers they could possibly do. stricted to the questions for which answers can be found in the interpreter speech [%].
Subjective results for the restricted evaluation are similar to the previous results, on the full data (80% vs 74% of the information found by the judges).
Performance is good for the interpreter: 98% of the information correctly translated by the automatic system is also correctly interpreted by the human.
Although we can not compare the performance of the restricted automatic system to that of the restricted interpreter (since data sets of questions are different), it seems that of the interpreter is better.
However, the loss due to subjective evaluation seems to be higher for the interpreter than for the automatic system.
<newSection> 7 Conclusions Regarding the SLT evaluation, the results achieved with the simultaneous translation system are still rather low compared to the results achieved with offline systems for translating European parliament speeches in TC-STAR.
However, the offline systems had almost no latency constraints, and parliament speeches are much easier to recognize and translate when compared to the more spontaneous talks and lectures focused in this paper.
This clearly shows the difficulty of the whole task.
However, the human end-to-end evaluation of the system in which the system is compared with human interpretation shows that the current translation quality allows for understanding of at least half of the content, and therefore, may be already quite helpful for people not understanding the language of the lecturer at all.
<newSection> References<newSection> Abstract We present a method which, given a few words defining a concept in some language, retrieves, disambiguates and extends corresponding terms that define a similar concept in another specified language.
This can be very useful for cross-lingual information retrieval and the preparation of multi-lingual lexical resources.
We automatically obtain term translations from multilingual dictionaries and disambiguate them using web counts.
We then retrieve web snippets with co-occurring translations, and discover additional concept terms from these snippets.
Our term discovery is based on co-appearance of similar words in symmetric patterns.
We evaluate our method on a set of language pairs involving 45 languages, including combinations of very dissimilar ones such as Russian, Chinese, and Hebrew for various concepts.
We assess the quality of the retrieved sets using both human judgments and automatically comparing the obtained categories to corresponding English WordNet synsets.
<newSection> 1 Introduction Numerous NLP tasks utilize lexical databases that incorporate concepts (or word categories): sets of terms that share a significant aspect of their meanings (e.g., terms denoting types of food, tool names, etc).
These sets are useful by themselves for improvement of thesauri and dictionaries, and they are also utilized in various applications including textual entailment and question answering.
Manual development of lexical databases is labor intensive, error prone, and susceptible to arbitrary human decisions.
While databases like WordNet (WN) are invaluable for NLP, for some applications any offline resource would not be extensive enough.
Frequently, an application requires data on some very specific topic or on very recent news-related events.
In these cases even huge and ever-growing resources like Wikipedia may provide insufficient coverage.
Hence applications turn to Web-based on-demand queries to obtain the desired data.
The majority of web pages are written in English and a few other salient languages, hence most of the web-based information retrieval studies are done on these languages.
However, due to the substantial growth of the multilingual web1, queries can be performed and the required information can be found in less common languages, while the query language frequently does not match the language of available information.
Thus, if we are looking for information about some lexical category where terms are given in a relatively uncommon language such as Hebrew, it is likely to find more detailed information and more category instances in a salient language such as English.
To obtain such information, we need to discover a word list that represents the desired category in English.
This list can be used, for instance, in subsequent focused search in order to obtain pages relevant for the given category.
Thus given a few Hebrew words as a description for some category, it can be useful to obtain a similar (and probably more extended) set of English words representing the same category.
In addition, when exploring some lexical category in a common language such as English, it is frequently desired to consider available resources from different countries.
Such resources are likely to be written in languages different from English.
In order to obtain such resources, as before, it would be beneficial, given a concept definition in English, to obtain word lists denoting the same concept in different languages.
In both cases a concept as a set of words should be translated as a whole from one language to another.
In this paper we present an algorithm that given a concept defined as a set of words in some source language discovers and extends a similar set in some specified target language.
Our approach comprises three main stages.
First, given a few terms, we obtain sets of their translations to the target language from multilingual dictionaries, and use web counts to select the appropriate word senses.
Next, we retrieve search engine snippets with the translated terms and extract symmetric patterns that connect these terms.
Finally, we use these patterns to extend the translated concept, by obtaining more terms from the snippets.
We performed thorough evaluation for various concepts involving 45 languages.
The obtained categories were manually verified with two human judges and, when appropriate, automatically compared to corresponding English WN synsets.
In all tested cases we discovered dozens of concept terms with state-of-the-art precision.
Our major contribution is a novel framework for concept translation across languages.
This framework utilizes web queries together with dictionaries for translation, disambiguation and extension of given terms.
While our framework relies on the existence of multilingual dictionaries, we show that even with basic 1000 word dictionaries we achieve good performance.
Modest time and data requirements allow the incorporation of our method in practical applications.
In Section 2 we discuss related work, Section 3 details the algorithm, Section 4 describes the evaluation protocol and Section 5 presents our results.
<newSection> 2 Related work Substantial efforts have been recently made to manually construct and interconnect WN-like databases for different languages (Pease et al., 2008; Charoenporn et al., 2007).
Some studies (e.g., (Amasyali, 2005)) use semi-automated methods based on language-specific heuristics and dictionaries.
At the same time, much work has been done on automatic lexical acquisition, and in particular, on the acquisition of concepts.
The two main algorithmic approaches are pattern-based discovery, and clustering of context feature vectors.
The latter represents word contexts as vectors in some space and use similarity measures and automatic clustering in that space (Deerwester et al., 1990).
Pereira (1993), Curran (2002) and Lin (1998) use syntactic features in the vector definition.
(Pantel and Lin, 2002) improves on the latter by clustering by committee.
Caraballo (1999) uses conjunction and appositive annotations in the vector representation.
While a great effort has focused on improving the computational complexity of these methods (Gorman and Curran, 2006), they still remain data and computation intensive.
The current major algorithmic approach for concept acquisition is to use lexico-syntactic patterns.
Patterns have been shown to produce more accurate results than feature vectors, at a lower computational cost on large corpora (Pantel et al., 2004).
Since (Hearst, 1992), who used a manually prepared set of initial lexical patterns in order to acquire relationships, numerous pattern-based methods have been proposed for the discovery of concepts from seeds (Pantel et al., 2004; Davidov et al., 2007; Pasca et al., 2006).
Most of these studies were done for English, while some show the applicability of their method to some other languages including Russian, Greek, Czech and French.
Many papers directly target specific applications, and build lexical resources as a side effect.
Named Entity Recognition can be viewed as an instance of the concept acquisition problem where the desired categories contain words that are names of entities of a particular kind, as done in (Freitag, 2004) using co-clustering and in (Et-zioni et al., 2005) using predefined pattern types.
Many Information Extraction papers discover relationships between words using syntactic patterns (Riloff and Jones, 1999).
Unlike in the majority of recent studies where the acquisition framework is designed with specific languages in mind, in our task the algorithm should be able to deal well with a wide variety of target languages without any significant manual adaptations.
While some of the proposed frameworks could potentially be language-independent, little research has been done to confirm it yet.
There are a few obstacles that may hinder applying common pattern-based methods to other languages.
Many studies utilize parsing or POS tagging, which frequently depends on the availability and quality of language-specific tools.
Most studies specify seed patterns in advance, and it is not clear whether translated patterns can work well on different languages.
Also, the absence of clear word segmentation in some languages (e.g., Chinese) can make many methods inapplicable.
A few recently proposed concept acquisition methods require only a handful of seed words (Davidov et al., 2007; Pasca and Van Durme, 2008).
While these studies avoid some of the obstacles above, it still remains unconfirmed whether such methods are indeed language-independent.
In the concept extension part of our algorithm we adapt our concept acquisition framework (Davi-dov and Rappoport, 2006; Davidov et al., 2007; Davidov and Rappoport, 2008a; Davidov and Rappoport, 2008b) to suit diverse languages, including ones without explicit word segmentation.
In our evaluation we confirm the applicability of the adapted methods to 45 languages.
Our study is related to cross-language information retrieval (CLIR/CLEF) frameworks.
Both deal with information extracted from a set of languages.
However, the majority of CLIR studies pursue different targets.
One of the main CLIR goals is the retrieval of documents based on explicit queries, when the document language is not the query language (Volk and Buite-laar, 2002).
These frameworks usually develop language-specific tools and algorithms including parsers, taggers and morphology analyzers in order to integrate multilingual queries and documents (Jagarlamudi and Kumaran, 2007).
Our goal is to develop and evaluate a language-independent method for the translation and extension of lexical categories.
While our goals are different from CLIR, CLIR systems can greatly benefit from our framework, since our translated categories can be directly utilized for subsequent document retrieval.
Another field indirectly related to our research is Machine Translation (MT).
Many MT tasks require automated creation or improvement of dictionaries (Koehn and Knight, 2001).
However, MT mainly deals with translation and disambiguation of words at the sentence or document level, while we translate whole concepts defined independently of contexts.
Our primary target is not translation of given words, but the discovery and extension of a concept in a target language when the concept definition is given in some different source language.
<newSection> 3 Cross-lingual Concept Translation Framework Our framework has three main stages: (1) given a set of words in a source language as definition for some concept, we automatically translate them to the target language with multilingual dictionaries, disambiguating translations using web counts; (2) we retrieve from the web snippets where these translations co-appear; (3) we apply a pattern-based concept extension algorithm for discovering additional terms from the retrieved data.
We start from a set of words denoting a category in a source language.
Thus we may use words like (apple, banana, ...) as the definition of fruits or (bear, wolf, fox, ...) as the definition of wild animals2.
Each of these words can be ambiguous.
Multilingual dictionaries usually provide many translations, one or more for each sense.
We need to select the appropriate translation for each term.
In practice, some or even most of the category terms may be absent in available dictionaries.
In these cases, we attempt to extract “chain” translations, i.e., if we cannot find Source→Target translation, we can still find some indirect Source→Intermediate1→Intermediate2→Target paths.
Such translations are generally much more ambiguous, hence we allow up to two intermediate languages in a chain.
We collect all possible translations at the chains having minimal length, and skip category terms for whom this process results in no translations.
Then we use the conjecture that terms of the same concept tend to co-appear more frequently than ones belonging to different concepts3.
Thus, 2In order to reduce noise, we limit the length (in words) of multiword expressions considered as terms.
To calculate this limit for a language we randomly take 100 terms from the appropriate dictionary and set a limit as Lim,,,,,,,,, = round(avg(length(w))) where length(w) is the number of words in term w.
For languages like Chinese without inherent word segmentation, length(w) is the number of characters in w.
While for many languages Lim,,,,,,,,, = 1, some languages like Vietnamese usually require two words or more to express terms.
we select a translation of a term co-appearing most frequently with some translation of a different term of the same concept.
We estimate how well translations of different terms are connected to each other.
Let C = {Ci} be the given seed words for some concept.
Let Tr(Ci, n) be the n-th available translation of word Ci and Cnt(s) denote the web count of string s obtained by a search engine.
Then we select translation Tr(Ci) according to: We utilize the Yahoo!
“x * y” wildcard that allows to count only co-appearances where x and y are separated by a single word.
As a result, we obtain a set of disambiguated term translations.
The number of queries in this stage depends on the ambiguity of concept terms translation to the target language.
Unlike many existing disambiguation methods based on statistics obtained from parallel corpora, we take a rather simplistic query-based approach.
This approach is powerful (as shown in our evaluation) and only relies on a few web queries in a language independent manner.
We need to restrict web mining to specific target languages.
This restriction is straightforward if the alphabet or term translations are language-specific or if the search API supports restriction to this language4.
In case where there are no such natural restrictions, we attempt to detect and add to our queries a few language-specific frequent words.
Using our dictionaries, we find 1–3 of the 15 most frequent words in a desired language that are unique to that language, and we ‘and’ them with the queries to ensure selection of the proper language.
While some languages as Esperanto do not satisfy any of these requirements, more than 60 languages do.
For each pair A, B of disambiguated term translations, we construct and execute the following 2 queries: {“A * B”, “B * A”}5.
When we have 3 or more terms we also add {A B C ...}-like conjunction queries which include 3–5 terms.
For languages with Limmwe > 1, we also construct queries with several “*” wildcards between terms.
For each query we collect snippets containing text fragments of web pages.
Such snippets frequently include the search terms.
Since Yahoo!
allows retrieval of up to the 1000 first results (100 in each query), we collect several thousands snippets.
For most of the target languages and categories, only a few dozen queries (20 on the average) are required to obtain sufficient data.
Thus the relevant data can be downloaded in seconds.
This makes our approach practical for on-demand retrieval tasks.
First we extract from the retrieved snippets contexts where translated terms co-appear, and detect patterns where they co-appear symmetrically.
Then we use the detected patterns to discover additional concept terms.
In order to define word boundaries, for each target language we manually specify boundary characters such as punctuation/space symbols.
This data, along with dictionaries, is the only language-specific data in our framework.
Following (Davidov et al., 2007) we seek symmetric patterns to retrieve concept terms.
We use two meta-pattern types.
First, a Two-Slot pattern type constructed as follows: Ci are slots for concept terms.
We allow up to Limmwe space-separated6 words to be in a single slot.
Infix may contain punctuation, spaces, and up to Limmwe x 4 words.
Prefix and Postfix are limited to contain punctuation characters and/or Limmwe words.
Terms of the same concept frequently co-appear in lists.
To utilize this, we introduce two additional List pattern types7: As in (Widdows and Dorow, 2002; Davidov and Rappoport, 2006), we define a pattern graph.
Nodes correspond to terms and patterns to edges.
If term pair (w1, w2) appears in pattern P, we add nodes Nw1, Nw2 to the graph and a directed edge EP (Nw1, Nw2) between them.
We consider only symmetric patterns.
We define a symmetric pattern as a pattern where some category terms Ci, Cj appear both in left-to-right and right-to-left order.
For example, if we consider the terms {apple, pineapple} we select a List pattern “(one Ci, )+ and Cn.”
if we find both “one apple, one pineapple, one guava and orange.” and “one watermelon, one pineapple and apple.”.
If no such patterns are found, we turn to a weaker definition, considering as symmetric those patterns where the same terms appear in the corpus in at least two different slots.
Thus, we select a pattern “for C1 and C2” if we see both “for apple and guava,” and “for orange and apple,”.
We collect terms in two stages.
First, we obtain “high-quality” core terms and then we retrieve potentially more noisy ones.
In the first stage we collect all terms8 that are bidirectionally connected to at least two different original translations, and call them core concept terms Ccore.
We also add the original ones as core terms.
Then we detect the rest of the terms Crest that appear with more different Ccore terms than with ‘out’ (non-core) terms as follows: where E(Na, Nb) correspond to existence of a graph edge denoting that translated terms a and b co-appear in a pattern in this order.
Our final term set is the union of Ccore and Crest.
For the sake of simplicity, unlike in the majority of current research, we do not attempt to discover more patterns/instances iteratively by reexamining the data or re-querying the web.
If we have enough data, we use windowing to improve result quality.
If we obtain more than 400 snippets for some concept, we randomly divide the data into equal parts, each containing up to 400 snippets.
We apply our algorithm independently to each part and select only the words that appear in more than one part.
<newSection> 4 Experimental Setup We describe here the languages, concepts and dictionaries we used in our experiments.
One of the main goals in this research is to verify that the proposed basic method can be applied to different languages unmodified.
We examined a wide variety of languages and concepts.
Table 3 shows a list of 45 languages used in our experiments, including west European languages, Slavic languages, Semitic languages, and diverse Asian languages.
Our concept set was based on English WN synsets, while concept definitions for evaluation were based on WN glosses.
For automated evaluation we selected as categories 150 synsets/subtrees with at least 10 single-word terms in them.
For manual evaluation we used a subset of 24 of these categories.
In this subset we tried to select generic categories, such that no domain expert knowledge was required to check their correctness.
Ten of these categories were equal to ones used in (Widdows and Dorow, 2002; Davidov and Rap-poport, 2006), which allowed us to indirectly compare to recent work.
Table 1 shows these 10 concepts along with the sample terms.
While the number of tested categories is still modest, it provides a good indication for the quality of our approach.
We developed a set of tools for automatic access to several dictionaries.
We used Wikipedia cross-language links as our main source (60%) for offline translation.
These links include translation of Wikipedia terms into dozens of languages.
The main advantage of using Wikipedia is its wide coverage of concepts and languages.
However, one problem in using it is that it frequently encodes too specific senses and misses common ones.
Thus bear is translated as family Ursidae missing its common “wild animal” sense.
To overcome these difficulties, we also used Wiktionary and complemented these offline resources with a few automated queries to several (20) online dictionaries.
We start with Wikipedia definitions, then if not found, Wiktionary, and then we turn to online dictionaries.
<newSection> 5 Evaluation and Results While there are numerous concept acquisition studies, no framework has been developed so far to evaluate this type of cross-lingual concept discovery, limiting our ability to perform a meaningful comparison to previous work.
Fair estimation of translated concept quality is a challenging task.
For most languages there are no widely accepted concept databases.
Moreover, the contents of the same concept may vary across languages.
Fortunately, when English is taken as a target language, the English WN allows an automated evaluation of concepts.
We conducted evaluation in three different settings, mostly relying on human judges and utilizing the English WN where possible.
guages served as source languages.
In this case human subjects manually provided input terms for 150 concept definitions in each of the target languages using 150 selected English WN glosses.
For each gloss they were requested to provide at least 2 terms.
Then we ran the algorithm on these term lists.
Since the obtained results were English words, we performed both manual evaluation of the 24 categories and automated comparison to the original WN data.
English language pairs for the 24 concepts.
Concept definitions were the same as in (2) and manual evaluation followed the same protocol as in (1).
The absence of exhaustive term lists makes recall estimation problematic.
In all cases we assess the quality of the discovered lists in terms of precision (P) and length of retrieved lists (T).
Each discovered concept was evaluated by two judges.
All judges were fluent English speakers and for each target language, at least one was a fluent speaker of this language.
They were given one-line English descriptions of each category and the full lists obtained by our algorithm for each of the 24 concepts.
Table 2 shows the lists obtained by our algorithm for the category described as Relatives (e.g., grandmother) for several language pairs including Hebrew→French and Chinese→Czech.
We mixed “noise” words into each list of terms10.
These words were automatically and randomly extracted from the same text.
Subjects were required to select all words fitting the provided description.
They were unaware of algorithm details and desired results.
They were instructed to accept common abbreviations, alternative spellings or misspellings like yel¯ow∈color and to accept a term as belonging to a category if at least one of its senses belongs to it, like orange∈color and orange∈fruit.
They were asked to reject terms related or associated but not belonging to the target category, like tasty/∈food, or that are too general, like animal/∈dogs.
The first 4 columns of Table 3 show averaged results of manual evaluation for 24 categories.
In the first two columns English is used as a source language and in the next pair of columns English is used as the target.
In addition we display in parentheses the amount of terms added during the extension stage.
We can see that for all languages, average precision (% of correct terms in concept) is above 80, and frequently above 90, and the average number of extracted terms is above 30.
Internal concept quality is in line with values observed on similarly evaluated tasks for recent concept acquisition studies in English.
As a baseline, only 3% of the inserted 20-40% noise words were incorrectly labeled by judges.
Due to space limitation we do not show the full per-concept behavior; all medians for P and T were close to the average.
We can also observe that the majority (> 60%) of target language terms were obtained during the extension stage.
Thus, even when considering translation from a rich language such as English (where given concepts frequently contain dozens of terms), most of the discovered target language terms are not discovered through translation but during the subsequent concept extension.
In fact, brief examination shows that less than half of source language terms successfully pass translation and disambiguation stage.
However, more than 80% of terms which were skipped due to lack of available translations were re-discovered in the target language during the extension stage, along with the discovery of new correct terms not existing in the given source definition.
The first two columns of Table 4 show similar results for non-English language pairs.
We can see that these results are only slightly inferior to the ones involving English.
We applied our algorithm on 150 concepts with English used as the target language.
Since we want to consider common misspellings and morphological combinations of correct terms as hits, we used a basic speller and stemmer to resolve typos and drop some English endings.
The WN columns in Table 3 display P and T values for this evaluation.
In most cases we obtain > 85% precision.
While these results (P=87,T=17) are lower than in manual evaluation, the task is much harder due to the large number (and hence sparseness) of the utilized 150 WN categories and the incomplete nature of WN data.
For the 10 categories of Table 1 used in previous work, we have obtained (P=92,T=41) which outperforms the seed-based concept acquisition of (Widdows and Dorow, 2002; Davidov and Rappoport, 2006) (P=90,T=35) on the same concepts.
However, it should be noted that our task setting is substantially different since we utilize more seeds and they come from languages different from English.
The first stage in our framework heavily relies on the existence and quality of dictionaries, whose coverage may be insufficient.
In order to check the effect of dictionary coverage on our task, we re-evaluated 10 language pairs using reduced dictionaries containing only the 1000 most frequent words.
The last columns in Table 4 show evaluation results for such reduced dictionaries.
Surprisingly, while we see a difference in coverage and precision, this difference is below 8%, thus even basic 1000-word dictionaries may be useful for some applications.
This may suggest that only a few correct translations are required for successful discovery of the corresponding category.
Hence, even a small dictionary containing translations of the most frequent terms could be enough.
In order to test this hypothesis, we re-evaluated the 10 language pairs using full dictionaries while reducing the initial concept definition to the 3 most frequent words.
The results of this experiment are shown at columns 3–4 of Table 4.
We can see that for most language pairs, 3 seeds were sufficient to achieve equally good results, and providing more extensive concept definitions had little effect on performance.
We obtained high precision.
However, we also observed high variance in the number of terms between different language pairs for the same concept.
There are many possible reasons for this outcome.
Below we briefly discuss some of them; detailed analysis of inter-language and inter-concept variance is a major target for future work.
Web coverage of languages is not uniform (Pao-lillo et al., 2005); e.g. Georgian has much less web hits than English.
Indeed, we observed a correlation between reported web coverage and the number of retrieved terms.
Concept coverage and first column shows the 45 tested languages.
Bold are languages evaluated with at least one native speaker.
P: precision, T: number of retrieved terms.
“[xx]”: number of terms added during the concept extension stage.
Columns 1-4 show results for manual evaluation on 24 concepts.
Columns 5-6 show automated WN-based evaluation on 150 concepts.
For columns 1-2 the input category is given in English, in other columns English served as the target language.
content is also different for each language.
Thus, concepts involving fantasy creatures were found to have little coverage in Arabic and Hindi, and wide coverage in European languages.
For vehicles, Snowmobile was detected in Finnish and number of terms.
“[xx]”: number of terms added in the extension stage.
Columns 1-2 show results for normal experiment settings, 3-4 show data for experiments where the 3 most frequent terms were used as concept definitions, 5-6 describe results for experiment with 1000-word dictionaries.
Swedish while Rickshaw appears in Hindi.
Morphology was completely neglected in this research.
To co-appear in a text, terms frequently have to be in a certain form different from that shown in dictionaries.
Even in English, plurals like spoons, forks co-appear more than spoon, fork.
Hence dictionaries that include morphology may greatly improve the quality of our framework.
We have conducted initial experiments with promising results in this direction, but we do not report them here due to space limitations.
<newSection> 6 Conclusions We proposed a framework that when given a set of terms for a category in some source language uses dictionaries and the web to retrieve a similar category in a desired target language.
We showed that the same pattern-based method can successfully extend dozens of different concepts for many languages with high precision.
We observed that even when we have very few ambiguous translations available, the target language concept can be discovered in a fast and precise manner without relying on any language-specific preprocessing, databases or parallel corpora.
The average concept total processing time, including all web requests, was below 2 minutes11.
The short running time and the absence of language-specific requirements allow processing queries within minutes and makes it possible to apply our method to on-demand cross-language concept mining.
<newSection> References<newSection> Abstract Machine translation (MT) systems have improved significantly; however, their outputs often contain too many errors to communicate the intended meaning to their users.
This paper describes a collaborative approach for mediating between an MT system and users who do not understand the source language and thus cannot easily detect translation mistakes on their own.
Through a visualization of multiple linguistic resources, this approach enables the users to correct difficult translation errors and understand translated passages that were otherwise baffling.
<newSection> 1 Introduction Recent advances in machine translation (MT) have given us some very good translation systems.
They can automatically translate between many languages for a variety of texts; and they are widely accessible to the public via the web.
The quality of the MT outputs, however, is not reliably high.
People who do not understand the source language may be especially baffled by the MT outputs because they have little means to recover from translation mistakes.
The goal of this work is to help monolingual target-language users to obtain better translations by enabling them to identify and overcome errors produced by the MT system.
We argue for a human-computer collaborative approach because both the users and the MT system have gaps in their abilities that the other could compensate.
To facilitate this collaboration, we propose an interface that mediates between the user and the MT system.
It manages additional NLP tools for the source language and translation resources so that the user can explore this extra information to gain enough understanding of the source text to correct MT errors.
The interactions between the users and the MT system may, in turn, offer researchers insights into the translation process and inspirations for better translation models.
We have conducted an experiment in which we asked non-Chinese speakers to correct the outputs of a Chinese-English MT system for several short passages of different genres.
They performed the correction task both with the help of the visual-ization interface and without.
Our experiment addresses the following questions: Through qualitative and quantitative analysis of the user actions and timing statistics, we have found that users of the interface achieved a more accurate understanding of the source texts and corrected more difficult translation mistakes than those who were given the MT outputs alone.
Furthermore, we observed that some users made better use of the interface for certain genres, such as sports news, suggesting that the translation model may be improved by a better integration of document-level contexts.
<newSection> 2 Collaborative Translation The idea of leveraging human-computer collab-orations to improve MT is not new; computer-aided translation, for instance, was proposed by Kay (1980).
The focus of these efforts has been on improving the performance of professional translators.
In contrast, our intended users cannot read the source text.
These users do, however, have the world knowledge and the language model to put together coherent sentences in the target-language.
From the MT research perspective, this raises an interesting question: given that they are missing a translation model, what would it take to make these users into effective “decoders?”
While some translation mistakes are recoverable from a strong language model alone, and some might become readily apparent if one can choose from some possible phrasal translations; the most difficult mistakes may require greater contextual knowledge about the source.
Consider the range of translation resources available to an MT decoder–which ones might the users find informative, handicapped as they are for not knowing the source language?
Studying the users’ interactions with these resources may provide insights into how we might build a better translation model and a better decoder.
In exploring the collaborative approach, the design considerations for facilitating human computer interaction are crucial.
We chose to make available relatively few resources to prevent the users from becoming overwhelmed by the options.
We also need to determine how to present the information from the resources so that the users can easily interpret them.
This is a challenge because the Chinese processing tools and the translation resources are imperfect themselves.
The information should be displayed in such a way that conflicting analyses between different resources are highlighted.
<newSection> 3 Prototype Design We present an overview of our prototype for a collaborative translation interface, named The Chinese Room1.
A screen-shot is shown in Figure 1.
It consists of two main regions.
The left pane is a workspace for users to explore the sentence; the right pane provides multiple tabs that offer additional functionalities. is a graphical environment that supports five main sources of information and functionalities.
The space separates into two regions.
On the left pane is a large workspace for the user to explore the source text one sentence at a time.
On the right pane are tabbed panels that provide the users with access to a document view of the MT outputs as well as additional functionalities for interpreting the source.
In our prototype, the MT output is obtained by querying Google’s Translation API2.
In the interest of exploiting user interactions as a diagnostic tool for improving MT, we chose information sources that are commonly used by modern MT systems.
First, we display the word alignments between MT output and segmented Chinese3.
Even without knowing the Chinese characters, the users can visually detect potential misalignments and poor word reordering.
For instance, the automatic translation shown in Figure 1 begins: Two years ago this month...
It is fluent but incorrect.
The crossed alignments offer users a clue that “two” and “months” should not have been split up.
Users can also explore alternative orderings by dragging the English tokens around.
Second, we make available the glosses for words and characters from a bilingual dictionary4.
the name was nonetheless evocative in that the user requires additional resources to process the input “squiggles.”
The placement of the word gloss presents a challenge because there are often alternative Chinese segmentations.
We place glosses for multi-character words in the column closer to the source.
When the user mouses over each definition, the corresponding characters are highlighted, helping the user to notice potential mis-segmentation in the Chinese.
Third, the Chinese sentence is annotated with its parse structure5.
Constituents are displayed as brackets around the source sentence.
They have been color-coded into four major types (noun phrase, verb phrases, prepositional phrases, and other).
Users can collapse and expand the brackets to keep the workspace uncluttered as they work through the Chinese sentence.
This also indicates to us which fragments held the user’s focus.
Fourth, based on previous studies reporting that automatic translations may improve when given decomposed source inputs (Mellebeek et al., 2005), we allow the users to select a substring from the source text for the MT system to translate.
We display the N-best alternatives in the Translation Tab.
The list is kept short; its purpose is less for reranking but more to give the users a sense of the kinds of hypotheses that the MT system is considering.
Fifth, users can select a substring from the source text and search for source sentences from a bilingual corpus and a monolingual corpus that contain phrases similar to the query6.
The retrieved sentences are displayed in the Example Tab.
For sentences from the bilingual corpus, human translations for the queried phrase are highlighted.
For sentences retrieved from the monolingual corpus, their automatic translations are provided.
If the users wished to examine any of the retrieved translation pairs in detail, they can push it onto the sentence workspace.
<newSection> 4 Experimental Methodology We asked eight non-Chinese speakers to correct the machine translations of four short Chinese pas-con released by the LDC; for a handful of characters that serve as function words, we added the functional definitions using an online dictionary http://www.mandarintools.com/worddict.html.
sages, with an average length of 11.5 sentences.
Two passages are news articles and two are excerpts of a fictional work.
Each participant was instructed to correct the translations for one news article and one fictional passage using all the resources made available by The Chinese Room and the other two passages without.
To keep the experimental conditions as similar as possible, we provided them with a restricted version of the interface (see Figure 2 for a screen-shot) in which all additional functionalities except for the Document View Tab are disabled.
We assigned each person to alternate between working with the full and the restricted versions of the system; half began without, and the others began with.
Thus, every passage received four sets of corrections made collab-oratively with the system and four sets of corrections made based solely on the participants’ internal language models.
All together, there are 184 participant corrected sentences (11.5 sentences x 4 passages x 4 participants) for each condition.
The participants were asked to complete each passage in one sitting.
Within a passage, they could work on the sentences in any arbitrary order.
They could also elect to “pass” any part of a sentence if they found it too difficult to correct.
Timing statistics were automatically collected while they made their corrections.
We interviewed each participant for qualitative feedbacks after all four passages were corrected.
Next, we asked two bilingual speakers to evaluate all the corrected translations.
The outcomes between different groups of users are compared, and the significance of the difference is determined using the two-sample t-test assuming unequal variances.
We require 90% confidence (al-pha=0.1) as the cut-off for a difference to be considered statistically significant; when the difference can be established with higher confidence, we report that value.
In the following subsections, we describe the conditions of this study in more details.
Participants’ Background For this study, we strove to maintain a relatively heterogeneous population; participants were selected to be varied in their exposures to NLP, experiences with foreign languages, as well as their age and gender.
A summary of their backgrounds is shown in Table 1.
Prior to the start of the study, the participants received a 20 minute long presentational tutorial about the basic functionalities supported by our system, but they did not have an opportunity to explore the system on their own.
This helps us to determine whether our interface is intuitive enough for new users to pick up quickly.
Data The four passages used for this study were chosen to span a range of difficulties and genre types.
The easiest of the four is a news article about a new Tamagotchi-like product from Bandai.
It was taken from a webpage that offers bilingual news to help Chinese students to learn English.
A harder news article is taken from a past NIST Chinese-English MT Evaluation; it is about Michael Jordan’s knee injury.
For a different genre, we considered two fictional excerpts from the first chapter of Martin Eden, a novel by Jack London that has been professionally translated into Chinese7.
One excerpt featured a short dialog, while the other one was purely descriptive.
Evaluation of Translations Bilingual human judges are presented with the source text as well as the parallel English text for reference.
Each judge is then shown a set of candidate translations (the original MT output, an alternative translation by a bilingual speaker, and corrected translations by the participants) in a randomized order.
Since the human corrected translations are likely to be fluent, we have instructed the judges to concentrate more on the adequacy of the meaning conveyed.
They are asked to rate each sentence on an abso-for evaluating the translation quality of the MT outputs and the participants’ corrections.
lute scale of 1-10 using the guideline in Table 2.
To reduce the biases in the rating scales of different judges, we normalized the judges’ scores, following standard practices in MT evaluation (Blatz et al., 2003).
Post normalization, the correlation coefficient between the judges is 0.64.
The final assessment score for each translated sentence is the average of judges’ scores, on a scale of 0-1.
<newSection> 5 Results The results of human evaluations for the user experiment are summarized in Table 3, and the corresponding timing statistics (average minutes spent editing a sentence) is shown in Table 4.
We observed that typical MT outputs contain a range of errors.
Some are primarily problems in fluency such that the participants who used the restricted interface, which provided no additional resources other than the Document View Tab, were still able to improve the MT quality from 0.35 to 0.42.
On the other hand, there are also a number of more serious errors that require the participants to gain some level of understanding of the source in order to correct them.
The participants who had access to the full collaborative interface were able to improve the quality from 0.35 to 0.53, closing the gap between the MT and the bilingual translations by 36.9%.
These differences are all statistically significant (with >98% confidence).
The higher quality of corrections does require the participants to put in more time.
Overall, the participants took 2.5 times as long when they have the interface than when they do not.
This may be partly because the participants have more sources of information to explore and partly because the participants tended to “pass” on fewer sentences.
The average Levenshtein edit distance (with words as the atomic unit, and with the score normalized to the interval [0,1]) between the original MT outputs and the corrected sentences made by participants using The Chinese Room is 0.59; in contrast, the edit distance is shorter, at 0.40, when participants correct MT outputs directly.
The timing statistics are informative, but they reflect the interactions of many factors (e.g., the difficulty of the source text, the quality of the machine translation, the background and motivation of the user).
Thus, in the next few subsections, we examine how these factors correlate with the quality of the participant corrections.
Since the quality of MT varies depending on the difficulty and genre of the source text, we investigate how these factors impact our participants’ performances.
Columns 3-6 of Table 3 (and Table 4) compare the corrected translations on a per-document basis.
Of the four documents, the baseline MT system performed the best on the product announcement.
Because the article is straight-forward, participants found it relatively easy to guess the intended translation.
The major obstacle is in detecting and translating Chinese transliteration of Japanese names, which stumped everyone.
The quality difference between the two groups of participants on this document was not statistically significant.
Relatedly, the difference in the amount of time spent is the smallest for this document; participants using The Chinese Room took about 1.5 times longer.
The other news article was much more difficult.
The baseline MT made many mistakes, and both groups of participants spent longer on sentences from this article than the others.
Although sports news is fairly formulaic, participants who only read MT outputs were baffled, whereas those who had access to additional resources were able to recover from MT errors and produced good quality translations.
Finally, as expected, the two fictional excerpts were the most challenging.
Since the participants were not given any information about the story, they also have little context to go on.
In both cases, participants who collaborated with The Chinese Room made higher quality corrections than those who did not.
The difference is statistically significant at 97% confidence for the first excerpt, and 93% confidence for the second.
The differences in time spent between the two groups are greater for these passages because the participants who had to make corrections without help tended to give up more often.
We further analyze the results by separating the participants into two groups according to four factors: whether they were familiar with NLP, whether they studied another language, their gender, and their education level.
Exposure to NLP One of our design objectives for The Chinese Room is accessibility by a diverse population of end-users, many of whom may not be familiar with human language technologies.
To determine how prior knowledge of NLP may impact a user’s experience, we analyze the experimental results with respect to the participants’ background.
In columns 2 and 3 of Table 5, we compare the quality of the corrections made by the two groups.
When making corrections on their own, participants who had been exposed to NLP held a significant edge (0.35 vs. 0.47).
When both groups of participants used The Chinese Room, the difference is reduced (0.51 vs. 0.54) and is not statistically significant.
Because all the participants were given the same short tutorial prior to the start of the study, we are optimistic that the interface is intuitive for many users.
None of the other factors distinguished one MT, corrections by participants without help, corrections by participants using The Chinese Room, and translation produced by a bilingual speaker.
The second column reports score for all documents; columns 3-6 show the per-document scores.
group of participants from the others.
The results are summarized in columns 4-9 of Table 5.
In each case, the two groups had similar levels of performance, and the differences between their corrections were not statistically significant.
This trend holds for both when they were collaborating with the system and when editing on their own.
Prior Knowledge Another factor that may impact the success of the outcome is the user’s knowledge about the domain of the source text.
An example from our study is the sports news article.
Table 6 lists the scores that the four participants who used The Chinese Room received for their corrected translations for that passage (aver-aged over sentences).
User5 and User6 were more familiar with the basketball domain; with the help of the system, they produced translations that were comparable to those from the bilingual translator (the differences are not statistically significant).
Post-experiment, we asked the participants to describe the strategies they developed for collaborat-ing with the system.
Their responses fall into three main categories: Divide and Conquer Some users found the syntactic trees helpful in identifying phrasal units for N-best re-translations or example searches.
For longer sentences, they used the constituent collapse feature to help them reduce clutter and focus on a portion of the sentence.
Example Retrieval Using the search interface, users examined the highlighted query terms to determine whether the MT system made any segmentation errors.
Sometimes, they used the examples to arbitrate whether they should trust any of the dictionary glosses or the MT’s lexical choices.
Typically, though, they did not attempt to inspect the example translations in detail.
Document Coherence and Word Glosses Users often referred to the document view to determine the context for the sentence they are editing.
Together with the word glosses and other resources, the discourse level clues helped to guide users to make better lexical choices than when they made corrections without the full system, relying on sentence coherence alone.
Figure 3 compares the average access counts (per sentence) of different resources (aggregated over all participants and documents).
The option of inspect retrieved examples in detail (i.e., bring them up on the sentence workspace) was rarely used.
The inspiration for this feature was from work on translation memory (Macklovitch et al., 2000); however, it was not as informative for our participants because they experienced a greater degree of uncertainty than professional translators.
<newSection> 6 Discussion The results suggest that collaborative translation is a promising approach.
Participant experiences were generally positive.
Because they felt like they understood the translations better, they did not mind putting in the time to collaborate with the system.
Table 7 shows some of the participants’ outputs.
Although there are some translation errors that cannot be overcome with our current system (e.g., transliterated names), the participants taken as a collective performed surprisingly well.
For many mistakes, even when the users cannot correct them, they recognized a problem; and often, one or two managed to intuit the intended meaning with the help of the available resources.
As an upper-bound for the effectiveness of the system, we construct a combined “oracle” user out of all 4 users that used the interface for each sentence.
The oracle user’s average score is 0.70; in contrast, an oracle of users who did not use the system is 0.54 (cf. the MT’s overall of 0.35 and the bilingual translator’s overall of 0.83).
This suggests The Chinese Room affords a potential for human-human collaboration as well.
The experiment also made clear some limitations of the current resources.
One is domain dependency.
Because NLP technologies are typically trained on news corpora, their bias toward the news domain may mislead our users.
For example, there is a Chinese character (pronounced mei3) that could mean either “beautiful” or “the United States.”
In one of the passages, the intended translation should have been: He was responsive to beauty...
but the corresponding MT output was He was sensitive to the United States...
Although many participants suspected that it was wrong, they were unable to recover from this mistake because the resources (the searchable examples, the part-of-speech tags, and the MT system) did not offer a viable alternative.
This suggests that collaborative translation may serve as a useful diagnostic tool to help MT researchers verify ideas about what types of models and data are useful in translation.
It may also provide a means of data collection for MT training.
To be sure, there are important challenges to be addressed, such as par-ticipation incentive and quality assurance, but similar types of collaborative efforts have been shown fruitful in other domains (Cosley et al., 2007).
Finally, the statistics of user actions may be useful for translation evaluation.
They may be informative features for developing automatic metrics for sentence-level evaluations (Kulesza and Shieber, 2004).
<newSection> 7 Related Work While there have been many successful computer-aided translation systems both for research and as commercial products (Bowker, 2002; Langlais et al., 2000), collaborative translation has not been as widely explored.
Previous efforts such as DerivTool (DeNeefe et al., 2005) and Linear B (Callison-Burch, 2005) placed stronger emphasis on improving MT.
They elicited more in-depth interactions between the users and the MT system’s phrase tables.
These approaches may be more appropriate for users who are MT researchers themselves.
In contrast, our approach focuses on providing intuitive visualization of a variety of information sources for users who may not be MT-savvy.
By tracking the types of information they consulted, the portions of translations they selected to modify, and the portions of the source Score Translation MT 0.34 He is being discovered almost hit an arm in the pile of books on the desktop, just like frightened horse as a Lieju Wangbangbian almost Pengfan the piano stool.
without The Chinese Room 0.26 Startled, he almost knocked over a pile of book on his desk, just like a frightened horse as a Lieju Wangbangbian almost Pengfan the piano stool. with The Chinese Room 0.78 He was nervous, and when one of his arms nearly hit a stack of books on the desktop, he startled like a horse, falling back and almost knocking over the piano stool.
Bilingual Translator 0.93 Feeling nervous, he discovered that one of his arms almost hit the pile of books on the table.
Like a frightened horse, he stumbled aside, almost turning over a piano stool.
MT 0.50 Bandai Group, a spokeswoman for the U.S. to be SIN-West said: “We want to bring women of all ages that ’the flavor of life’.”
without The Chinese Room 0.67 SIN-West, a spokeswoman for the U.S. Bandai Group declared: “We want to bring to women of all ages that ’flavor of life’.” with The Chinese Room 0.68 West, a spokeswoman for the U.S. Toy Manufacturing Group, and soon to be Vice President-said: “We want to bring women of all ages that ’flavor of life’.”
Bilingual Translator 0.75 “We wanted to let women of all ages taste the ’flavor of life’,” said Bandai’s spokeswoman Kasumi Nakanishi.
text they attempted to understand, we may alter the design of our translation model.
Our objective is also related to that of cross-language information retrieval (Resnik et al., 2001).
This work can be seen as providing the next step in helping users to gain some understanding of the information in the documents once they are retrieved.
By facilitating better collaborations between MT and target-language readers, we can naturally increase human annotated data for exploring alternative MT models.
This form of symbiosis is akin to the paradigm proposed by von Ahn and Dabbish (2004).
They designed interactive games in which the player generated data could be used to improve image tagging and other classification tasks (von Ahn, 2006).
While our interface does not have the entertainment value of a game, its application serves a purpose.
Because users are motivated to understand the documents, they may willingly spend time to collaborate and make detailed corrections to MT outputs.
<newSection> 8 Conclusion We have presented a collaborative approach for mediating between an MT system and monolingual target-language users.
The approach encourages users to combine evidences from complementary information sources to infer alternative hypotheses based on their world knowledge.
Experimental evidences suggest that the collaborative effort results in better translations than either the original MT or uninformed human edits.
Moreover, users who are knowledgeable in the document domain were enabled to correct translations with a quality approaching that of a bilingual speaker.
From the participants’ feedbacks, we learned that the factors that contributed to their understanding include: document coherence, syntactic constraints, and re-translation at the phrasal level.
We believe that the collaborative translation approach can provide insights about the translation process and help to gather training examples for future MT development.
<newSection> Acknowledgments This work has been supported by NSF Grants IIS-0710695 and IIS-0745914.
We would like to thank Jarrett Billingsley, Ric Crabbe, Joanna Drum-mund, Nick Farnan, Matt Kaniaris Brian Madden, Karen Thickman, Julia Hockenmaier, Pauline Hwa, and Dorothea Wei for their help with the experiment.
We are also grateful to Chris Callison-Burch for discussions about collaborative translations and to Adam Lopez and the anonymous reviewers for their comments and suggestions on this paper.
<newSection> References<newSection> Abstract We introduce cube summing, a technique that permits dynamic programming algorithms for summing over structures (like the forward and inside algorithms) to be extended with non-local features that violate the classical structural independence assumptions.
It is inspired by cube pruning (Chiang, 2007; Huang and Chiang, 2007) in its computation of non-local features dynamically using scored k-best lists, but also maintains additional residual quantities used in calculating approximate marginals.
When restricted to local features, cube summing reduces to a novel semiring (k-best+residual) that generalizes many of the semirings of Goodman (1999).
When non-local features are included, cube summing does not reduce to any semiring, but is compatible with generic techniques for solving dynamic programming equations.
<newSection> 1 Introduction Probabilistic NLP researchers frequently make independence assumptions to keep inference algorithms tractable.
Doing so limits the features that are available to our models, requiring features to be structurally local.
Yet many problems in NLP—machine translation, parsing, named-entity recognition, and others—have benefited from the addition of non-local features that break classical independence assumptions.
Doing so has required algorithms for approximate inference.
Recently cube pruning (Chiang, 2007; Huang and Chiang, 2007) was proposed as a way to leverage existing dynamic programming algorithms that find optimal-scoring derivations or structures when only local features are involved.
Cube pruning permits approximate decoding with non-local features, but leaves open the question of how the feature weights or probabilities are learned.
Meanwhile, some learning algorithms, like maximum likelihood for conditional log-linear models (Laf-ferty et al., 2001), unsupervised models (Pereira and Schabes, 1992), and models with hidden variables (Koo and Collins, 2005; Wang et al., 2007; Blunsom et al., 2008), require summing over the scores of many structures to calculate marginals.
We first review the semiring-weighted logic programming view of dynamic programming algorithms (Shieber et al., 1995) and identify an intuitive property of a program called proof locality that follows from feature locality in the underlying probability model (§2).
We then provide an analysis of cube pruning as an approximation to the intractable problem of exact optimization over structures with non-local features and show how the use of non-local features with k-best lists breaks certain semiring properties (§3).
The primary contribution of this paper is a novel technique— cube summing—for approximate summing over discrete structures with non-local features, which we relate to cube pruning (§4).
We discuss implementation (§5) and show that cube summing becomes exact and expressible as a semiring when restricted to local features; this semiring generalizes many commonly-used semirings in dynamic programming (§6).
<newSection> 2 Background In this section, we discuss dynamic programming algorithms as semiring-weighted logic programs.
We then review the definition of semirings and important examples.
We discuss the relationship between locally-factored structure scores and proofs in logic programs.
Many algorithms in NLP involve dynamic programming (e.g., the Viterbi, forward-backward, probabilistic Earley’s, and minimum edit distance algorithms).
Dynamic programming (DP) involves solving certain kinds of recursive equations with shared substructure and a topological ordering of the variables.
Shieber et al.
(1995) showed a connection between DP (specifically, as used in parsing) and logic programming, and Goodman (1999) augmented such logic programs with semiring weights, giving an algebraic explanation for the intuitive connections among classes of algorithms with the same logical structure.
For example, in Goodman’s framework, the forward algorithm and the Viterbi algorithm are comprised of the same logic program with different semirings.
Goodman defined other semirings, including ones we will use here.
This formal framework was the basis for the Dyna programming language, which permits a declarative specification of the logic program and compiles it into an efficient, agenda-based, bottom-up procedure (Eisner et al., 2005).
For our purposes, a DP consists of a set of recursive equations over a set of indexed variables.
For example, the probabilistic CKY algorithm (run on sentence w1w2...wn) is written as where N is the nonterminal set and S E N is the start symbol.
Each CX,i,j variable corresponds to the chart value (probability of the most likely subtree) of an X-constituent spanning the substring wi+1...wj. goal is a special variable of greatest interest, though solving for goal correctly may (in general, but not in this example) require solving for all the other values.
We will use the term “in-dex” to refer to the subscript values on variables (X, i, j on CX,i,j).
Where convenient, we will make use of Shieber et al.’s logic programming view of dynamic programming.
In this view, each variable (e.g., CX,i,j in Eq. 1) corresponds to the value of a “theo-rem,” the constants in the equations (e.g., pX→Y Z in Eq.
1) correspond to the values of “axioms,” and the DP defines quantities corresponding to weighted “proofs” of the goal theorem (e.g., finding the maximum-valued proof, or aggregating proof values).
The value of a proof is a combination of the values of the axioms it starts with.
Semirings define these values and define two operators over them, called “aggregation” (max in Eq. 1) and “combination” (x in Eq. 1).
Goodman and Eisner et al. assumed that the values of the variables are in a semiring, and that the equations are defined solely in terms of the two semiring operations.
We will often refer to the “probability” of a proof, by which we mean a non-negative R-valued score defined by the semantics of the dynamic program variables; it may not be a normalized probability.
A semiring is a tuple (A, ®, ®, 0, 1), in which A is a set, ® A x A —* A is the aggregation operation, ® A x A —* A is the combination operation, 0 is the additive identity element (ba E A, a ® 0 = a), and 1 is the multiplica-tive identity element (ba E A, a ® 1 = a).
A semiring requires ® to be associative and commutative, and ® to be associative and to distribute over ®.
Finally, we require a ® 0 = 0 ® a = 0 for all a E A.1 Examples include the inside semiring, (R>0, +, x, 0, 1), and the Viterbi semiring, (R>0, max, x, 0, 1).
The former sums the probabilities of all proofs of each theorem.
The latter (used in Eq. 1) calculates the probability of the most probable proof of each theorem.
Two more examples follow.
Viterbi proof semiring.
We typically need to recover the steps in the most probable proof in addition to its probability.
This is often done using backpointers, but can also be accomplished by representing the most probable proof for each theorem in its entirety as part of the semiring value (Goodman, 1999).
For generality, we define a proof as a string that is constructed from strings associated with axioms, but the particular form of a proof is problem-dependent.
The “Viterbi proof” semiring includes the probability of the most probable proof and the proof itself.
Letting Z C_ E* be the proof language on some symbol set E, this semiring is defined on the set R>0 x Z with 0 element (0, E) and 1 element (1, E).
For two values (u1, U1) and (u2, U2), the aggregation operator returns (max(u1, u2), UargmaxiE{1,21 ui).
1When cycles are permitted, i.e., where the value of one variable depends on itself, infinite sums can be involved.
We must ensure that these infinite sums are well defined under the semiring.
So-called complete semirings satisfy additional conditions to handle infinite sums, but for simplicity we will restrict our attention to DPs that do not involve cycles. of proof U1.
The max-k function returns a sorted list of the top-k proofs from a set.
The * function performs a cross-product on two k-best proof lists (Eq. 2).
The combination operator returns (u1u2, U1.U2), where U1.U2 denotes the string concatenation of U1 and U2.2 k-best proof semiring.
The “k-best proof” semiring computes the values and proof strings of the k most-probable proofs for each theorem.
The set is (R>0 x Z)<k, i.e., sequences (up to length k) of sorted probability/proof pairs.
The aggregation operator ® uses max-k, which chooses the k highest-scoring proofs from its argument (a set of scored proofs) and sorts them in decreasing order.
To define the combination operator ®, we require a cross-product that pairs probabilities and proofs from two k-best lists.
We call this ?, defined on two semiring values u = ((u1, U1), ..., (uk, Uk)) and v = Then, u ® v = max-k(u ? v).
This is similar to the k-best semiring defined by Goodman (1999).
These semirings are summarized in Table 1.
Let X be the space of inputs to our logic program, i.e., x E X is a set of axioms.
Let Z denote the proof language and let � C_ Z denote the set of proof strings that constitute full proofs, i.e., proofs of the special goal theorem.
We assume an exponential probabilistic model such that where each λm > 0 is a parameter of the model and each hm is a feature function.
There is a bijec-tion between � and the space of discrete structures that our model predicts.
Given such a model, DP is helpful for solving two kinds of inference problems.
The first problem, decoding, is to find the highest scoring proof 2We assume for simplicity that the best proof will never be a tie among more than one proof.
Goodman (1999) handles this situation more carefully, though our version is more likely to be used in practice for both the Viterbi proof and k-best proof semirings.
The second is the summing problem, which marginalizes the proof probabilities (without normalization): As defined, the feature functions hm can depend on arbitrary parts of the input axiom set x and the entire output proof y.
An important characteristic of problems suited for DP is that the global calculation (i.e., the value of goal) depend only on local factored parts.
In DP equations, this means that each equation connects a relatively small number of indexed variables related through a relatively small number of indices.
In the logic programming formulation, it means that each step of the proof depends only on the theorems being used at that step, not the full proofs of those theorems.
We call this property proof locality.
In the statistical modeling view of Eq.
3, classical DP requires that the probability model make strong Markovian conditional independence assumptions (e.g., in HMMs, St−1 1 St+1  |St); in exponential families over discrete structures, this corresponds to feature locality.
For a particular proof y of goal consisting of t intermediate theorems, we define a set of proof strings `i E Z for i E {1, ..., t}, where `i corresponds to the proof of the ith theorem.3 We can break the computation of feature function hm into a summation over terms corresponding to each `i: This is simply a way of noting that feature functions “fire” incrementally at specific points in the 3The theorem indexing scheme might be based on a topological ordering given by the proof structure, but is not important for our purposes.
proof, normally at the first opportunity.
Any feature function can be expressed this way.
For local features, we can go farther; we define a function top(E) that returns the proof string corresponding to the antecedents and consequent of the last inference step in E.
Local features have the property: McDonald and Pereira (2006), in which an exact solution to a related decoding problem is found and then modified to fit the problem of interest.
Local features only have access to the most recent deductive proof step (though they may “fire” repeatedly in the proof), while non-local features have access to the entire proof up to a given theorem.
For both kinds of features, the “f” terms are used within the DP formulation.
When taking an inference step to prove theorem i, the value of that theorem’s value, along with the values of the antecedents.
Note that typically only a small number of fm are nonzero for theorem i.
When non-local hm/fm that depend on arbitrary parts of the proof are involved, the decoding and summing inference problems are NP-hard (they instantiate probabilistic inference in a fully connected graphical model).
Sometimes, it is possible to achieve proof locality by adding more indices to the DP variables (for example, consider modifying the bigram HMM Viterbi algorithm for trigram HMMs).
This increases the number of variables and hence computational cost.
In general, it leads to exponential-time inference in the worst case.
There have been many algorithms proposed for approximately solving instances of these decoding and summing problems with non-local features.
Some stem from work on graphical models, including loopy belief propagation (Sutton and McCallum, 2004; Smith and Eisner, 2008), Gibbs sampling (Finkel et al., 2005), sequential Monte Carlo methods such as particle filtering (Levy et al., 2008), and variational inference (Jordan et al., 1999; MacKay, 1997; Kurihara and Sato, 2006).
Also relevant are stacked learning (Cohen and Carvalho, 2005), interpretable as approximation of non-local feature values (Martins et al., 2008), and M-estimation (Smith et al., 2007), which allows training without inference.
Several other approaches used frequently in NLP are approximate methods for decoding only.
These include beam search (Lowerre, 1976), cube pruning, which we discuss in §3, integer linear programming (Roth and Yih, 2004), in which arbitrary features can act as constraints on y, and approximate solutions like Cube pruning (Chiang, 2007; Huang and Chiang, 2007) is an approximate technique for decoding (Eq. 4); it is used widely in machine translation.
Given proof locality, it is essentially an efficient implementation of the k-best proof semiring.
Cube pruning goes farther in that it permits nonlocal features to weigh in on the proof probabilities, at the expense of making the k-best operation approximate.
We describe the two approximations cube pruning makes, then propose cube decoding, which removes the second approximation.
Cube decoding cannot be represented as a semiring; we propose a more general algebraic structure that accommodates it.
Cube pruning is an approximate solution to the decoding problem (Eq. 4) in two ways.
Approximation 1: k < oc.
Cube pruning uses a finite k for the k-best lists stored in each value.
If k = oc, the algorithm performs exact decoding with non-local features (at obviously formidable expense in combinatorial problems).
Approximation 2: lazy computation.
Cube pruning exploits the fact that k < oc to use lazy computation.
When combining the k-best proof lists of d theorems’ values, cube pruning does not enumerate all kd proofs, apply non-local features to all of them, and then return the top k.
Instead, cube pruning uses a more efficient but approximate solution that only calculates the non-local factors on O(k) proofs to obtain the approximate top k.
This trick is only approximate if non-local features are involved.
Approximation 2 makes it impossible to formulate cube pruning using separate aggregation and combination operations, as the use of lazy computation causes these two operations to effectively be performed simultaneously.
To more directly relate our summing algorithm (§4) to cube pruning, we suggest a modified version of cube pruning that does not use lazy computation.
We call this algorithm cube decoding.
This algorithm can be written down in terms of separate aggregation and combination operations, though we will show it is not a semiring.
in place by multiplying in the function result, and returns the modified proof list: We formally describe cube decoding, show that it does not instantiate a semiring, then describe a more general algebraic structure that it does instantiate.
Consider the set 9 of non-local feature functions that map X × Z → R>0.4 Our definitions in §2.2 for the k-best proof semiring can be expanded to accommodate these functions within the semiring value.
Recall that values in the k-best proof semiring fall in Ak = (R>0 ×Z)<k.
For cube decoding, we use a different set Acd defined as where the binary variable indicates whether the value contains a k-best list (0, which we call an “ordinary” value) or a non-local feature function in 9 (1, which we call a “function” value).
We denote a value u ∈ Acd by where each ui ∈ R>0 is a probability and each Ui ∈ Z is a proof string.
We use ⊕k and ⊗k to denote the k-best proof semiring’s operators, defined in §2.2.
We let g0 be such that g0(f) is undefined for all f ∈ Z.
For two values u = hu, gu, usi, v = hv, gv, vsi ∈ Acd, cube decoding’s aggregation operator is: Under standard models, only ordinary values will be operands of ⊕cd, so ⊕cd is undefined when us∨ vs. We define the combination operator ⊗cd: where exec(g, u) executes the function g upon each proof in the proof list u, modifies the scores also be used to implement, e.g., hard constraints or other nonlocal score factors.
Here, max-k is simply used to re-sort the k-best proof list following function evaluation.
The semiring properties fail to hold when introducing non-local features in this way.
In particular, ⊗cd is not associative when 1 < k < ∞.
For example, consider the probabilistic CKY algorithm as above, but using the cube decoding semiring with the non-local feature functions collectively known as “NGramTree” features (Huang, 2008) that score the string of terminals and nonter-minals along the path from word j to word j + 1 when two constituents CY,i,j and CZ,j,k are combined.
The semiring value associated with such a feature is u = hhi, NGramTreeπ(), 1i (for a specific path 7r), and we rewrite Eq.
1 as follows (where ranges for summation are omitted for space): CX,i,k = Lcd pX,Y Z ⊗cd CY,i,j ⊗cd CZ,j,k ⊗cd u The combination operator is not associative since the following will give different answers:5 In Eq.
10, the non-local feature function is executed on the k-best proof list for Z, while in Eq.
11, NGramTreeπ is called on the k-best proof list for the X constructed from Y and Z.
Furthermore, neither of the above gives the desired result, since we actually wish to expand the full set of k2 proofs of X and then apply NGramTreeπ to each of them (or a higher-dimensional “cube” if more operands are present) before selecting the k-best.
The binary operations above retain only the top k proofs of X in Eq.
11 before applying NGramTreeπ to each of them.
We actually would like to redefine combination so that it can operate on arbitrarily-sized sets of values.
We can understand cube decoding through an algebraic structure with two operations ⊕ and ⊗, where ⊗ need not be associative and need not distribute over ⊕, and furthermore where ⊕ and ⊗ are defined on arbitrarily many operands.
We will refer here to such a structure as a generalized semiring.6 To define ®cd on a set of operands with N' ordinary operands and N function operands, we first compute the full O(kN�) cross-product of the ordinary operands, then apply each of the N functions from the remaining operands in turn upon the full N'-dimensional “cube,” finally calling max-k on the result.
<newSection> 4 Cube Summing We present an approximate solution to the summing problem when non-local features are involved, which we call cube summing.
It is an extension of cube decoding, and so we will describe it as a generalized semiring.
The key addition is to maintain in each value, in addition to the k-best list of proofs from Ak, a scalar corresponding to the residual probability (possibly unnormalized) of all proofs not among the k-best.7 The k-best proofs are still used for dynamically computing non-local features but the aggregation and combination operations are redefined to update the residual as appropriate.
We define the set Acs for cube summing as For a proof list u, we use I�uI to denote the sum of all proof scores, Ei:(ui,Ui)Eu ui.
The aggregation operator over operands {ui}Ni=1, all such that uis = 0,8 is defined by: 6Algebraic structures are typically defined with binary operators only, so we were unable to find a suitable term for this structure in the literature.
7Blunsom and Osborne (2008) described a related approach to approximate summing using the chart computed during cube pruning, but did not keep track of the residual terms as we do here.
8We assume that operands ui to ®mss will never be such that uis = 1(non-local feature functions).
This is reasonable in the widely used log-linear model setting we have adopted, where weights A. are factors in a proof’s product score.
where Res returns the “residual” set of scored proofs not in the k-best among its arguments, possibly the empty set.
For a set of N+N' operands {vi}N i=1U{wj}N� such that vis = 1(non-local feature functions) and wjs = 1 (ordinary values), the combination operator ® is shown in Eq.
13 Fig. 1. Note that the case where N' = 0 is not needed in this application; an ordinary value will always be included in combination.
In the special case of two ordinary operands (where us = vs = 0), Eq.
13 reduces to We define 0 as (0, (), g0, 0); an appropriate definition for the combination identity element is less straightforward and of little practical importance; we leave it to future work.
If we use this generalized semiring to solve a DP and achieve goal value of u, the approximate sum of all proof probabilities is given by u0+I�uI.
If all features are local, the approach is exact.
With non-local features, the k-best list may not contain the k-best proofs, and the residual score, while including all possible proofs, may not include all of the non-local features in all of those proofs’ probabilities.
<newSection> 5 Implementation We have so far viewed dynamic programming algorithms in terms of their declarative speci-fications as semiring-weighted logic programs.
Solvers have been proposed by Goodman (1999), by Klein and Manning (2001) using a hypergraph representation, and by Eisner et al.
(2005).
Because Goodman’s and Eisner et al.’s algorithms assume semirings, adapting them for cube summing is non-trivial.9 To generalize Goodman’s algorithm, we suggest using the directed-graph data structure known variously as an arithmetic circuit or computation graph.10 Arithmetic circuits have recently drawn interest in the graphical model community as a 9The bottom-up agenda algorithm in Eisner et al.
(2005) might possibly be generalized so that associativity, distribu-tivity, and binary operators are not required (John Blatz, p.c.).
10This data structure is not specific to any particular set of operations.
We have also used it successfully with the inside semiring.
tool for performing probabilistic inference (Dar-wiche, 2003).
In the directed graph, there are vertices corresponding to axioms (these are sinks in the graph), ® vertices corresponding to theorems, and ® vertices corresponding to summands in the dynamic programming equations.
Directed edges point from each node to the nodes it depends on; ® vertices depend on ® vertices, which depend on ® and axiom vertices.
Arithmetic circuits are amenable to automatic differentiation in the reverse mode (Griewank and Corliss, 1991), commonly used in back-propagation algorithms.
Importantly, this permits us to calculate the exact gradient of the approximate summation with respect to axiom values, following Eisner et al.
(2005).
This is desirable when carrying out the optimization problems involved in parameter estimation.
Another differen-tiation technique, implemented within the semiring, is given by Eisner (2002).
Cube pruning is based on the k-best algorithms of Huang and Chiang (2005), which save time over generic semiring implementations through lazy computation in both the aggregation and combination operations.
Their techniques are not as clearly applicable here, because our goal is to sum over all proofs instead of only finding a small subset of them.
If computing non-local features is a computational bottleneck, they can be computed only for the O(k) proofs considered when choosing the best k as in cube pruning.
Then, the computational requirements for approximate summing are nearly equivalent to cube pruning, but the approximation is less accurate.
<newSection> 6 Semirings Old and New We now consider interesting special cases and variations of cube summing.
When restricted to local features, cube pruning and cube summing can be seen as proper semir-ings.
Cube pruning reduces to an implementation of the k-best semiring (Goodman, 1998), and cube summing reduces to a novel semiring we call the k-best+residual semiring.
Binary instantiations of ® and ® can be iteratively reapplied to give the equivalent formulations in Eqs.
12 and 13.
We define 0 as (0, ()) and 1 as (1, (1, E)).
The ® operator is easily shown to be commutative.
That ® is associative follows from associativity of max-k, shown by Goodman (1998).
Showing that ® is associative and that ® distributes over ® are less straightforward; proof sketches are provided in Appendix A. The k-best+residual semiring generalizes many semirings previously introduced in the literature; see Fig. 2. Once we relax requirements about associativity and distributivity and permit aggregation and combination operators to operate on sets, several extensions to cube summing become possible.
First, when computing approximate summations with non-local features, we may not always be interested in the best proofs for each item.
Since the purpose of summing is often to calculate statistics ignore proof under a model distribution, we may wish instead to sample from that distribution.
We can replace the max-k function with a sample-k function that samples k proofs from the scored list in its argument, possibly using the scores or possibly uniformly at random.
This breaks associativity of ®.
We conjecture that this approach can be used to simulate particle filtering for structured models.
Another variation is to vary k for different theorems.
This might be used to simulate beam search, or to reserve computation for theorems closer to goal, which have more proofs.
<newSection> 7 Conclusion This paper has drawn a connection between cube pruning, a popular technique for approximately solving decoding problems, and the semiring-weighted logic programming view of dynamic programming.
We have introduced a generalization called cube summing, to be used for solving summing problems, and have argued that cube pruning and cube summing are both semirings that can be used generically, as long as the underlying probability models only include local features.
With non-local features, cube pruning and cube summing can be used for approximate decoding and summing, respectively, and although they no longer correspond to semirings, generic algorithms can still be used.
<newSection> Acknowledgments We thank three anonymous EACL reviewers, John Blatz, Pe-dro Domingos, Jason Eisner, Joshua Goodman, and members of the ARK group for helpful comments and feedback that improved this paper.
This research was supported by NSF IIS-0836431 and an IBM faculty award.
A k-best+residual is a Semiring In showing that k-best+residual is a semiring, we will restrict our attention to the computation of the residuals.
The computation over proof lists is identical to that performed in the k-best proof semiring, which was shown to be a semiring by Goodman (1998).
We sketch the proofs that ⊗ is associative and that ⊗ distributes over ⊕; associativity of ⊕ is straightforward.
For a proof list ¯a, k¯ak denotes the sum of proof scores, Pi:(ai Ai)Ea ai.
Note that: Associativity.
Given three semiring values u, v, and w, we need to show that (u⊗v)⊗w = u⊗(v⊗w).
After expanding the expressions for the residuals using Eq.
14, there are 10 terms on each side, five of which are identical and cancel out immediately.
Three more cancel using Eq.
15, leaving: If LHS = RHS, associativity holds.
Using Eq.
15 again, we can rewrite the second term in LHS to obtain The resulting expression is intuitive: the residual of (u⊗v)⊗ w is the difference between the sum of all proof scores and the sum of the k-best.
RHS can be transformed into this same expression with a similar line of reasoning (and using asso-ciativity of *).
Therefore, LHS = RHS and ⊗ is associative.
Distributivity.
To prove that ⊗ distributes over ⊕, we must show left-distributivity,i.e., that u⊗(v⊕w) = (u⊗v)⊕(u⊗ w), and right-distributivity.
We show left-distributivity here.
As above, we expand the expressions, finding 8 terms on the LHS and 9 on the RHS.
Six on each side cancel, leaving: = k¯uk k¯v ∪ ¯wk − kmax-k(¯u * (¯v ∪ ¯w))k = k¯uk k¯v ∪ ¯wk − kmax-k((¯u * ¯v) ∪ (¯u * ¯w))k where the last line follows because * distributes over ∪ (Goodman, 1998).
We now work with the RHS: Since max-k(¯u * ¯v) and max-k(¯u * ¯w) are disjoint (we assume no duplicates; i.e., two different theorems cannot have exactly the same proof), the third term becomes = k¯uk k¯v ∪¯wk − kmax-k((¯u * ¯v) ∪ (¯u * ¯w))k � ‚‚ <newSection> References<newSection> Abstract Spoken Language Understanding aims at mapping a natural language spoken sentence into a semantic representation.
In the last decade two main approaches have been pursued: generative and discriminative models.
The former is more robust to overfitting whereas the latter is more robust to many irrelevant features.
Additionally, the way in which these approaches encode prior knowledge is very different and their relative performance changes based on the task.
In this paper we describe a machine learning framework where both models are used: a generative model produces a list of ranked hypotheses whereas a discriminative model based on structure kernels and Support Vector Machines, re-ranks such list.
We tested our approach on the MEDIA corpus (human-machine dialogs) and on a new corpus (human-machine and human-human dialogs) produced in the European LUNA project.
The results show a large improvement on the state-of-the-art in concept segmentation and labeling.
<newSection> 1 Introduction In Spoken Dialog Systems, the Language Understanding module performs the task of translating a spoken sentence into its meaning representation based on semantic constituents.
These are the units for meaning representation and are often referred to as concepts.
Concepts are instantiated by sequences of words, therefore a Spoken Language Understanding (SLU) module finds the association between words and concepts.
In the last decade two major approaches have been proposed to find this correlation: (i) generative models, whose parameters refer to the joint probability of concepts and constituents; and (ii) discriminative models, which learn a classification function to map words into concepts based on geometric and statistical properties.
An example of generative model is the Hidden Vector State model (HVS) (He and Young, 2005).
This approach extends the discrete Markov model encoding the context of each state as a vector.
State transitions are performed as stack shift operations followed by a push of a preterminal semantic category label.
In this way the model can capture semantic hierarchical structures without the use of tree-structured data.
Another simpler but effective generative model is the one based on Finite State Transducers.
It performs SLU as a translation process from words to concepts using Finite State Transducers (FST).
An example of discriminative model used for SLU is the one based on Support Vector Machines (SVMs) (Vapnik, 1995), as shown in (Raymond and Riccardi, 2007).
In this approach, data are mapped into a vector space and SLU is performed as a classification problem using Maximal Margin Classifiers (Shawe-Taylor and Cristianini, 2004).
Generative models have the advantage to be more robust to overfitting on training data, while discriminative models are more robust to irrelevant features.
Both approaches, used separately, have shown a good performance (Raymond and Riccardi, 2007), but they have very different characteristics and the way they encode prior knowledge is very different, thus designing models able to take into account characteristics of both approaches are particularly promising.
In this paper we propose a method for SLU based on generative and discriminative models: the former uses FSTs to generate a list of SLU hypotheses, which are re-ranked by SVMs.
These exploit all possible word/concept subsequences (with gaps) of the spoken sentence as features (i.e. all possible n-grams).
Gaps allow for the encoding of long distance dependencies between words in relatively small n-grams.
Given the huge size of this feature space, we adopted kernel methods and in particular sequence kernels (Shawe-Taylor and Cristianini, 2004) and tree kernels (Raymond and Riccardi, 2007; Moschitti and Bejan, 2004; Moschitti, 2006) to implicitly encode n-grams and other structural information in SVMs.
We experimented with different approaches for training the discriminative models and two different corpora: the well-known MEDIA corpus (Bonneau-Maynard et al., 2005) and a new corpus acquired in the European project LUNA1 (Ray-mond et al., 2007).
The results show a great improvement with respect to both the FST-based model and the SVM model alone, which are the current state-of-the-art for concept classification on such corpora.
The rest of the paper is organized as follows: Sections 2 and 3 show the generative and discriminative models, respectively.
The experiments and results are reported in Section 4 whereas the conclusions are drawn in Section 5.
<newSection> 2 Generative approach for concept classification In the context of Spoken Language Understanding (SLU), concept classification is the task of associating the best sequence of concepts to a given sentence, i.e. word sequence.
A concept is a class containing all the words carrying out the same semantic meaning with respect to the application domain.
In SLU, concepts are used as semantic units and are represented with concept tags.
The association between words and concepts is learned from an annotated corpus.
The Generative model used in our work for concept classification is the same used in (Raymond and Riccardi, 2007).
Given a sequence of words as input, a translation process based on FST is performed to output a sequence of concept tags.
The translation process involves three steps: (1) the mapping of words into classes (2) the mapping of classes into concepts and (3) the selection of the best concept sequence.
The first step is used to improve the generalization power of the model.
The word classes at this level can be both domain-dependent, e.g. ”Hotel” in MEDIA or ”Software” in the LUNA corpus, or domain-independent, e.g. numbers, dates, months etc.
The class of a word not belonging to any class is the word itself.
In the second step, classes are mapped into concepts.
The mapping is not one-to-one: a class may be associated with more than one concept, i.e. more than one SLU hypothesis can be generated.
In the third step, the best or the m-best hypotheses are selected among those produced in the previous step.
They are chosen according to the maximum probability evaluated by the Conceptual Language Model, described in the next section.
<newSection> 2.1 Stochastic Conceptual Language Model (SCLM) An SCLM is an n-gram language model built on semantic tags.
Using the same notation proposed in (Moschitti et al., 2007) and (Raymond and Ric-cardi, 2007), our SCLM trains joint probability P(W, C) of word and concept sequences from an annotated corpus: where W = w1..wk, C = c1..ck and hi = wi−1ci−1..w1c1.
Since we use a 3-gram conceptual language model, the history hi is {wi−1ci−1, wi−2ci−2}.
All the steps of the translation process described here and above are implemented as Finite State Transducers (FST) using the AT&T FSM/GRM tools and the SRILM (Stolcke, 2002) tools.
In particular the SCLM is trained using SRILM tools and then converted to an FST.
This allows the use of a wide set of stochastic language models (both back-off and interpolated models with several discounting techniques like Good-Turing, Witten-Bell, Natural, Kneser-Ney, Unchanged Kneser-Ney etc).
We represent the combination of all the translation steps as a transducer ASLU (Raymond and Riccardi, 2007) in terms of FST operations: where AW is the transducer representation of the input sentence, AW2C is the transducer mapping words to classes and ASLM is the Semantic Language Model (SLM) described above.
The best SLU hypothesis is given by where bestpathn (in this case n is 1 for the 1-best hypothesis) performs a Viterbi search on the FST quences in common between two sentences, in the space of n-grams (for any n). and outputs the n-best hypotheses and projectC performs a projection of the FST on the output labels, in this case the concepts.
Using the FSTs described above, we can generate m best hypotheses ranked by the joint probability of the SCLM.
After an analysis of the m-best hypotheses of our SLU model, we noticed that many times the hypothesis ranked first by the SCLM is not the closest to the correct concept sequence, i.e. its error rate using the Levenshtein alignment with the manual annotation of the corpus is not the lowest among the m hypotheses.
This means that re-ranking the m-best hypotheses in a convenient way could improve the SLU performance.
The best choice in this case is a discriminative model, since it allows for the use of informative features, which, in turn, can model easily feature dependencies (also if they are infrequent in the training set).
<newSection> 3 Discriminative re-ranking Our discriminative re-ranking is based on SVMs or a perceptron trained with pairs of conceptually annotated sentences.
The classifiers learn to select which annotation has an error rate lower than the others so that the m-best annotations can be sorted based on their correctness.
Kernel Methods refer to a large class of learning algorithms based on inner product vector spaces, among which Support Vector Machines (SVMs) are one of the most well known algorithms.
SVMs and perceptron learn a hyperplane H(x) = wx + b = 0, where x� is the feature vector representation of a classifying object o, w� E Rn (a vector space) and b E R are parameters (Vap-nik, 1995).
The classifying object o is mapped into x� by a feature function 0.
The kernel trick allows us to rewrite the decision hyperplane as to 1 for positive and -1 for negative examples, αi E R+, oiVi E {1..l} are the training instances and the product K(oi, o) = (0(oi)0(o)) is the kernel function associated with the mapping 0.
Note that we do not need to apply the mapping 0, we can use K(oi, o) directly (Shawe-Taylor and Cris-tianini, 2004).
For example, next section shows a kernel function that counts the number of word se-The String Kernels that we consider count the number of substrings containing gaps shared by two sequences, i.e. some of the symbols of the original string are skipped.
Gaps modify the weight associated with the target substrings as shown in the following.
Let E be a finite alphabet, E∗ = U∞n= 0 En is the set of all strings.
Given a string s E E∗, |s |denotes the length of the strings and si its compounding symbols, i.e s = s1..s|s|, whereas s[i : j] selects the substring sisi+1..sj−1sj from the i-th to the j-th character.
u is a subsequence of s if there is a sequence of indexes I = (i1, ..., i|u|), with 1 < i1 < ...
< i|u |< |s|, such that u = si1..si|u| or u = s[I] for short.
d(I) is the distance between the first and last character of the subsequence u in s, i.e. d(I) = i|u |− i1 + 1.
Finally, given s1, s2 E E∗, s1s2 indicates their concatenation.
The set of all substrings of a text corpus forms a feature space denoted by F = {u1, u2, ..} C E∗.
To map a string s in R∞ space, we can use the following functions: φu(s) = P~I:u=s[~I] λd(~I) for some A < 1.
These functions count the number of occurrences of u in the string s and assign them a weight Ad(I) proportional to their lengths.
Hence, the inner product of the feature vectors for two strings s1 and s2 returns the sum of all common subsequences weighted according to their frequency of occurrences and lengths, i.e. where d(.) counts the number of characters in the substrings as well as the gaps that were skipped in the original string.
It is worth noting that: Characters in the sequences can be substituted with any set of symbols.
In our study we preferred to use words so that we can obtain word sequences.
For example, given the sentence: How may I help you ? sample substrings, extracted by the Sequence Kernel (SK), are: How help you ?, How help ?, help you, may help you, etc.
Tree kernels represent trees in terms of their sub-structures (fragments).
The kernel function detects if a tree subpart (common to both trees) belongs to the feature space that we intend to generate.
For such purpose, the desired fragments need to be described.
We consider two important characterizations: the syntactic tree (STF) and the partial tree (PTF) fragments.
An STF is a general subtree whose leaves can be non-terminal symbols.
For example, Figure 1(a) shows 10 STFs (out of 17) of the subtree rooted in VP (of the left tree).
The STFs satisfy the constraint that grammatical rules cannot be broken.
For example, [VP [V NP]] is an STF, which has two non-terminal symbols, V and NP, as leaves whereas [VP [V]] is not an STF.
If we relax the constraint over the STFs, we obtain more general substructures called partial trees fragments (PTFs).
These can be generated by the application of partial production rules of the grammar, consequently [VP [V]] and [VP [NP]] are valid PTFs.
Figure 1(b) shows that the number of PTFs derived from the same tree as before is still higher (i.e. 30 PTs).
The main idea of tree kernels is to compute the number of common substructures between two trees T1 and T2 without explicitly considering the whole fragment space.
To evaluate the above kernels between two T1 and T2, we need to define a set F = {f1, f2,.
.. , f|F|}, i.e. a tree fragment space and an indicator function Ii(n), equal to 1 if the target fi is rooted at node n and equal to 0 otherwise.
A tree-kernel function over T1 and T2 where NT1 and NT2 are the sets of the T1’s and T2’s nodes, respectively and Δ(n1, n2) = P|F| i=1Ii(n1)Ii(n2).
The latter is equal to the number of common fragments rooted in the n1 and n2 nodes.
In the following sections we report the equation for the efficient evaluation of Δ for ST and PT kernels.
The Δ function depends on the type of fragments that we consider as basic features.
For example, to evaluate the fragments of type STF, it can be defined as: where σ ∈ {0, 1}, nc(n1) is the number of children of n1 and cjn is the j-th child of the node n.
Note that, since the productions are the same, nc(n1) = nc(n2).
Δ(n1, n2) evaluates the number of STFs common to n1 and n2 as proved in (Collins and Duffy, 2002).
Moreover, a decay factor λ can be added by modifying steps (2) and (3) as follows2: The computational complexity of Eq.
1 is O(|NT1 |× |NT2|) but as shown in (Moschitti, 2006), the average running time tends to be linear, i.e. O(|NT1 |+ |NT2|), for natural language syntactic trees.
PTFs have been defined in (Moschitti, 2006).
Their computation is carried out by the following Δ function: where ~I1 = (h1,h2,h3,..) and ~I2 = (k1, k2, k3, ..) are index sequences associated with the ordered child sequences cn1 of n1 and cn2 of n2, respectively, ~I1j an d ~I2j point to the j-th child in the corresponding sequence, and, again, l(·) returns the sequence length, i.e. the number of children.
Furthermore, we add two decay factors: µ for the depth of the tree and λ for the length of the child subsequences with respect to the original sequence, i.e. we account for gaps.
It follows that where d(~I1) = ~I1l(~I1) − ~I11 and d(~I2) = ~I2l(~I2) − ~I21.
This way, we penalize both larger trees and child subsequences with gaps.
Eq.
2 is more general than Eq.
1. Indeed, if we only consider the contribution of the longest child sequence from node pairs that have the same children, we implement the STK kernel.
The FST generates the m most likely concept annotations.
These are used to build annotation pairs, (si, sj), which are positive instances if si has a lower concept annotation error than sj, with respect to the manual annotation in the corpus.
Thus, a trained binary classifier can decide if si is more accurate than sj.
Each candidate annotation si is described by a word sequence where each word is followed by its concept annotation.
For example, given the sentence: where NULL, ACTION, RELATIVETIME, and HW are the assigned concepts whereas B and I are the usual begin and internal tags for concept subparts.
The second annotation is less accurate than the first since problema is annotated as an action and ”scheda di rete” is split in three different concepts.
Given the above data, the sequence kernel is used to evaluate the number of common n-grams between si and sj.
Since the string kernel skips some elements of the target sequences, the counted n-grams include: concept sequences, word sequences and any subsequence of words and concepts at any distance in the sentence.
Such counts are used in our re-ranking function as follows: let ei be the pair (s1i, s2 � we evaluate This schema, consisting in summing four different kernels, has been already applied in (Collins and Duffy, 2002) for syntactic parsing re-ranking, where the basic kernel was a tree kernel instead of SK and in (Moschitti et al., 2006), where, to re-rank Semantic Role Labeling annotations, a tree kernel was used on a semantic tree similar to the one introduced in the next section.
Since the aim in concept annotation re-ranking is to exploit innovative and effective source of information, we can use the power of tree kernels to generate correlation between concepts and word structures.
Fig. 2 describes the structural association between the concept and the word level.
This kind of trees allows us to engineer new kernels and consequently new features (Moschitti et al., 2008), e.g. their subparts extracted by STK or PTK, like the tree fragments in figures 1(a) and 1(b).
These can be used in SVMs to learn the classification of words in concepts.
More specifically, in our approach, we use tree fragments to establish the order of correctness between two alternative annotations.
Therefore, given two trees associated with two annotations, a re-ranker based on tree kernel, KR, can be built in the same way of the sequence-based kernel by substituting SK in Eq.
3 with STK or PTK.
<newSection> 4 Experiments In this section, we describe the corpora, parameters, models and results of our experiments of word chunking and concept classification.
Our baseline relates to the error rate of systems based on only FST and SVMs.
The re-ranking models are built on the FST output.
Different ways of producing training data for the re-ranking models determine different results.
We used two different speech corpora: The corpus LUNA, produced in the homonymous European project is the first Italian corpus of spontaneous speech on spoken dialog: it is based on the help-desk conversation in the domain of software/hardware repairing (Raymond et al., 2007).
The data are organized in transcriptions and annotations of speech based on a new multilevel protocol.
Data acquisition is still in progress.
Currently, 250 dialogs acquired with a WOZ approach and 180 Human-Human (HH) dialogs are available.
Statistics on LUNA corpus are reported in Table 1.
The corpus MEDIA was collected within the French project MEDIA-EVALDA (Bonneau-Maynard et al., 2005) for development and evaluation of spoken understanding models and linguistic studies.
The corpus is composed of 1257 dialogs, from 250 different speakers, acquired with a Wizard of Oz (WOZ) approach in the context of hotel room reservations and tourist information.
Statistics on transcribed and conceptually annotated data are reported in Table 2.
We defined two different training sets in the LUNA corpus: one using only the WOZ training dialogs and one merging them with the HH dialogs.
Given the small size of LUNA corpus, we did not carried out parameterization on a development set but we used default or a priori parameters.
We experimented with LUNA WOZ and six re-rankers obtained with the combination of SVMs and perceptron (PCT) with three different types of kernels: Syntactic Tree Kernel (STK), Partial Tree kernels (PTK) and the String Kernel (SK) described in Section 3.3.
Given the high number and the cost of these experiments, we ran only one model, i.e. the one and SVMs with the Sytntactic Tree Kernel (STK) on two different corpora: LUNA WOZ + HH, and MEDIA.
based on SVMs and STK3 , on the largest datasets, i.e. WOZ merged with HH dialogs and Media.
We trained all the SCLMs used in our experiments with the SRILM toolkit (Stolcke, 2002) and we used an interpolated model for probability estimation with the Kneser-Ney discount (Chen and Goodman, 1998).
We then converted the model in an FST as described in Section 2.1.
The model used to obtain the SVM baseline for concept classification was trained using Yam-CHA (Kudo and Matsumoto, 2001).
For the reranking models based on structure kernels, SVMs or perceptron, we used the SVM-Light-TK toolkit (available at dit.unitn.it/moschitti).
For A (see Section 3.2), cost-factor and trade-off parameters, we used, 0.4, 1 and 1, respectively.
The FST model generates the m-best annotations, i.e. the data used to train the re-ranker based on SVMs and perceptron.
Different training approaches can be carried out based on the use of the corpus and the method to generate the m-best.
We apply two different methods for training: Monolithic Training and Split Training.
In the former, FSTs are learned with the whole training set.
The m-best hypotheses generated by such models are then used to train the re-ranker classifier.
In Split Training, the training data are divided in two parts to avoid bias in the FST generation step.
More in detail, we train FSTs on part 1 and generate the m-best hypotheses using part 2.
Then, we re-apply these procedures inverting part 1 with part 2.
Finally, we train the re-ranker on the merged m-best data.
At the classification time, we generate the m-best of the test set using the FST trained on all training data.
cept Error Rate (CER), on the LUNA WOZ corpus using Split Training approach.
The baseline with FST and SVMs used separately are 23.2% and 26.7% respectively.
Regarding the generation of the training instances �si, sj/, we set m to 10 and we choose one of the 10-best hypotheses as the second element of the pair, sj, thus generating 10 different pairs.
The first element instead can be selected according to three different approaches: (A): si is the manual annotation taken from the corpus; (B) si is the most accurate annotation, in terms of the edit distance from the manual annotation, among the 10-best hypotheses of the FST model; (C) as above but si is selected among the 100-best hypotheses.
The pairs are also inverted to generate negative examples.
All the results of our experiments, expressed in terms of concept error rate (CER), are reported in Table 3, 4 and 5.
In Table 3, the corpora, i.e. LUNA (WOZ+HH) and Media, and the training approaches, i.e. Monolithic Training (MT) and Split Training (ST), are reported in the first and second row.
Column 1 shows the concept classification model used, i.e. the baselines FST and SVMs, and the re-ranking models (RR) applied to FST.
A, B and C refer to the three approaches for generating training instances described above.
As already mentioned for these large datasets, SVMs only use STK.
We note that our re-rankers relevantly improve our baselines, i.e. the FST and SVM concept classifiers on both corpora.
For example, SVM reranker using STK, MT and RR-A improves FST concept classifier of 23.2-15.6 = 7.6 points.
Moreover, the monolithic training seems the most appropriate to train the re-rankers whereas approach A is the best in producing training instances for the re-rankers.
This is not surprising since method A considers the manual annotation as a referent gold standard and it always allows comparing candidate annotations with the perfect one.
Tables 4 and 5 have a similar structure of Table 3 but they only show experiments on LUNA WOZ corpus with respect to the monolithic and split training approach, respectively.
In these tables, we also report the result for SVMs and perceptron (PCT) using STK, PTK and SK.
We note that: First, the small size of WOZ training set (only 1,019 turns) impacts on the accuracy of the systems, e.g. FST and SVMs, which achieved a CER of 18.2% and 23.4%, respectively, using also HH dialogs, with only the WOZ data, they obtain 23.2% and 26.7%, respectively.
Second, the perceptron algorithm appears to be ineffective for re-ranking.
This is mainly due to the reduced size of the WOZ data, which clearly prevents an on line algorithm like PCT to adequately refine its model by observing many examples4.
Third, the kernels which produce higher number of substructures, i.e. PTK and SK, improves the kernel less rich in terms of features, i.e. STK.
For example, using split training and approach A, STK is improved by 20.0-16.1=3.9.
This is an interesting result since it shows that (a) richer structures do produce better ranking models and (b) kernel methods give a remarkable help in feature design.
Next, although the training data is small, the re-rankers based on kernels appear to be very effective.
This may also alleviate the burden of annotating a lot of data.
Finally, the experiments of MEDIA show a not so high improvement using re-rankers.
This is due to: (a) the baseline, i.e. the FST model is very accurate since MEDIA is a large corpus thus the re-ranker can only ”correct” small number of errors; and (b) we could only experiment with the less expensive but also less accurate models, i.e. monolithic training and STK.
Media also offers the possibility to compare with the state-of-the-art, which our re-rankers seem to improve.
However, we need to consider that many Media corpus versions exist and this makes such comparisons not completely reliable.
Future work on the paper research line appears to be very interesting: the assessment of our best models on Media and WOZ+HH as well as other corpora is required.
More importantly, the structures that we have proposed for re-ranking are just two of the many possibilities to encode both word/concept statistical distributions and linguistic knowledge encoded in syntactic/semantic parse trees.
<newSection> 5 Conclusions In this paper, we propose discriminative reranking of concept annotation to capitalize from the benefits of generative and discriminative approaches.
Our generative approach is the state-of-the-art in concept classification since we used the same FST model used in (Raymond and Ric-cardi, 2007).
We could improve it by 1% point in MEDIA and 7.6 points (until 30% of relative improvement) on LUNA, where the more limited availability of annotated data leaves a larger room for improvement.
It should be noted that to design the re-ranking model, we only used two different structures, i.e. one sequence and one tree.
Kernel methods show that combinations of feature vectors, sequence kernels and other structural kernels, e.g. on shallow or deep syntactic parse trees, appear to be a promising research line (Moschitti, 2008).
Also, the approach used in (Zanzotto and Mos-chitti, 2006) to define cross pair relations may be exploited to carry out a more effective pair reranking.
Finally, the experimentation with automatic speech transcriptions is interesting to test the robustness of our models to transcription errors.
<newSection> Acknowledgments This work has been partially supported by the European Commission - LUNA project, contract n.
33549. <newSection> References<newSection> Abstract We explore the problem of resolving the second person English pronoun you in multi-party dialogue, using a combination of linguistic and visual features.
First, we distinguish generic and referential uses, then we classify the referential uses as either plural or singular, and finally, for the latter cases, we identify the addressee.
In our first set of experiments, the linguistic and visual features are derived from manual transcriptions and annotations, but in the second set, they are generated through entirely automatic means.
Results show that a multimodal system is often preferable to a unimodal one.
<newSection> 1 Introduction The English pronoun you is the second most frequent word in unrestricted conversation (after I and right before it).1 Despite this, with the exception of Gupta et al.
(2007b; 2007a), its resolution has received very little attention in the literature.
This is perhaps not surprising since the vast amount of work on anaphora and reference resolution has focused on text or discourse - mediums where second-person deixis is perhaps not as prominent as it is in dialogue.
For spoken dialogue pronoun resolution modules however, resolving you is an essential task that has an important impact on the capabilities of dialogue summarization systems.
*We thank the anonymous EACL reviewers, and Surabhi Gupta, John Niekrasz and David Demirdjian for their comments and technical assistance.
This work was supported by the CALO project (DARPA grant NBCH-D-03-0010).
Besides being important for computational implementations, resolving you is also an interesting and challenging research problem.
As for third person pronouns such as it, some uses of you are not strictly referential.
These include discourse marker uses such as you know in example (1), and generic uses like (2), where you does not refer to the addressee as it does in (3).
However, unlike it, you is ambiguous between singular and plural interpretations - an issue that is particularly problematic in multi-party conversations.
While you clearly has a plural referent in (4), in (3) the number of its referent is ambiguous.2 (4) I don’t know if you guys have any questions.
When an utterance contains a singular referential you, resolving the you amounts to identifying the individual to whom the utterance is addressed.
This is trivial in two-person dialogue since the current listener is always the addressee, but in conver-sations with multiple participants, it is a complex problem where different kinds of linguistic and visual information play important roles (Jovanovic, 2007).
One of the issues we investigate here is how this applies to the more concrete problem of resolving the second person pronoun you.
We approach this issue as a three-step problem.
Using the AMI Meeting Corpus (McCowan et al., 2005) of multi-party dialogues, we first discriminate between referential and generic uses of you.
Then, within the referential uses, we distinguish between singular and plural, and finally, we resolve the singular referential instances by identifying the intended addressee.
We use multi-modal features: initially, we extract discourse features from manual transcriptions and use visual information derived from manual annotations, but then we move to a fully automatic approach, using 1-best transcriptions produced by an automatic speech recognizer (ASR) and visual features automatically extracted from raw video.
In the next section of this paper, we give a brief overview of related work.
We describe our data in Section 3, and explain how we extract visual and linguistic features in Sections 4 and 5 respectively.
Section 6 then presents our experiments with manual transcriptions and annotations, while Section 7, those with automatically extracted information.
We end with conclusions in Section 8.
<newSection> 2 Related Work Although the vast majority of work on reference resolution has been with monologic text, some recent research has dealt with the more complex scenario of spoken dialogue (Strube and M¨uller, 2003; Byron, 2004; Arstein and Poesio, 2006; M¨uller, 2007).
There has been work on the identification of non-referential uses of the pronoun it: M¨uller (2006) uses a set of shallow features automatically extracted from manual transcripts of two-party dialogue in order to train a rule-based classifier, and achieves an F-score of 69%.
The only existing work on the resolution of you that we are aware of is Gupta et al.
(2007b; 2007a).
In line with our approach, the authors first disambiguate between generic and referential you, and then attempt to resolve the reference of the referential cases.
Generic uses of you account for 47% of their data set, and for the generic vs. referential disambiguation, they achieve an accuracy of 84% on two-party conversations and 75% on multi-party dialogue.
For the reference resolution task, they achieve 47%, which is 10 points over a baseline that always classifies the next speaker as the addressee.
These results are achieved without visual information, using manual transcripts, and a combination of surface features and manually tagged dialogue acts.
Resolving the referential instances of you amounts to determining the addressee(s) of the utterance containing the pronoun.
Recent years have seen an increasing amount of research on automatic addressee detection.
Much of this work focuses on communication between humans and computational agents (such as robots or ubiquitous computing systems) that interact with users who may be engaged in other activities, including interaction with other humans.
In these situations, it is important for a system to be able to recognize when it is being addressed by a user.
Bakx et al.
(2003) and Turnhout et al.
(2005) studied this issue in the context of mixed human-human and human-computer interaction using facial orientation and utterance length as clues for addressee detection, while Katzenmaier et al.
(2004) investigated whether the degree to which a user utterance fits the language model of a conversational robot can be useful in detecting system-addressed utterances.
This research exploits the fact that humans tend to speak differently to systems than to other humans.
Our research is closer to that of Jovanovic et al.
(2006a; 2007), who studied addressing in human-human multi-party dialogue.
Jovanovic and colleagues focus on addressee identification in face-to-face meetings with four participants.
They use a Bayesian Network classifier trained on several multimodal features (including visual features such as gaze direction, discourse features such as the speaker and dialogue act of preceding utterances, and utterance features such as lexical clues and utterance duration).
Using a combination of features from various resources was found to improve performance (the best system achieves an accuracy of 77% on a portion of the AMI Meeting Corpus).
Although this result is very encouraging, it is achieved with the use of manually produced information - in particular, manual transcriptions, dialogue acts and annotations of visual focus of attention.
One of the issues we aim to investigate here is how automatically extracted multimodal information can help in detecting the addressee(s) of you-utterances.
<newSection> 3 Data Our experiments are performed using the AMI Meeting Corpus (McCowan et al., 2005), a collection of scenario-driven meetings among four participants, manually transcribed and annotated with several different types of information (including dialogue acts, topics, visual focus of attention, and addressee).
We use a sub-corpus of 948 utterances containing you, and these were extracted from 10 different meetings.
The you-utterances are annotated as either discourse marker, generic or referential.
We excluded the discourse marker cases, which account for only 8% of the data, and of the referential cases, selected those with an AMI addressee annotation.3 The addressee of a dialogue act can be unknown, a single meeting participant, two participants, or the whole audience (three participants in the AMI corpus).
Since there are very few instances of two-participant addressee, we distinguish only between singular and plural addressees.
The resulting distribution of classes is shown in Table 1.4 We approach the reference resolution task as a two-step process, first discriminating between plural and singular references, and then resolving the reference of the singular cases.
The latter task requires a classification scheme for distinguishing between the three potential addressees (listeners) for the given you-utterance.
In their four-way classification scheme, Gupta et al.
(2007a) label potential addressees in terms of the order in which they speak after the you-utterance.
That is, for a given you-utterance, the potential addressee who speaks next is labeled 1, the potential addressee who speaks after that is 2, and the remaining participant is 3.
Label 4 is used for group addressing.
However, this results in a very skewed class distribution because the next speaker is the intended addressee 41% of the time, and 38% of instances are plural - the 3Addressee annotations are not provided for some dialogue act types - see (Jovanovic et al., 2006b).
4Note that the percentages of the referential singular and referential plural are relative to the total of referential instances.
remaining two classes therefore make up a small percentage of the data.
We were able to obtain a much less skewed class distribution by identifying the potential addressees in terms of their position in relation to the current speaker.
The meeting setting includes a rectangular table with two participants seated at each of its opposite longer sides.
Thus, for a given you-utterance, we label listeners as either L1, L2 or L3 depending on whether they are sitting opposite, diagonally or laterally from the speaker.
Table 2 shows the resulting class distribution for our dataset.
Such a labelling scheme is more similar to Jo-vanovic (2007), where participants are identified by their seating position.
<newSection> 4 Visual Information We derived per-utterance visual features from the Focus Of Attention (FOA) annotations provided by the AMI corpus.
These annotations track meeting participants’ head orientation and eye gaze during a meeting.5 Our first step was to use the FOA annotations in order to compute what we refer to as Gaze Duration Proportion (GDP) values for each of the utterances of interest - a measure similar to the “Degree of Mean Duration of Gaze” described by (Takemae et al., 2004).
Here a GDP value denotes the proportion of time in utterance u for which subject i is looking at target j: were Tu is the length of utterance u in milliseconds, and T(i, j), the amount of that time that i spends looking at j.
The gazer i can only refer to one of the four meeting participants, but the target j can also refer to the white-board/projector screen present in the meeting room.
For each utterance then, all of the possible values of i and j are used to construct a matrix of GDP values.
From this matrix, we then construct “Highest GDP” features for each of the meeting participants: such features record the target with the highest GDP value and so indicate whom/what the meeting participant spent most time looking at during the utterance.
We also generated a number of additional features for each individual.
These include firstly, three features which record the candidate “gazee” with the highest GDP during each third of the utterance, and which therefore account for gaze transitions.
So as to focus more closely on where participants are looking around the time when you is uttered, another feature records the candidate with the highest GDP -/+ 2 seconds from the start time of the you.
Two further features give some indication of the amount of looking around that the speaker does during an utterance - we hypothesized that participants (especially the speaker) might look around more in utterances with plural addressees.
The first is the ratio of the second highest GDP to the highest, and the second is the ratio of the third highest to the highest.
Finally, there is a highest GDP mutual gaze feature for the speaker, indicating with which other individual, the speaker spent most time engaged in a mutual gaze.
Hence this gives a total of 29 features: seven features for each of the four participants, plus one mutual gaze feature.
They are summarized in Table 3.
These visual features are different to those used by Jovanovic (2007) (see Section 2).
Jo-vanovic’s features record the number of times that each participant looks at each other participant during the utterance, and in addition, the gaze direction of the current speaker.
Hence, they are not highest GDP values, they do not include a mutual gaze feature and they do not record whether participants look at the white-board/projector screen.
To perform automatic visual feature extraction, a six degree-of-freedom head tracker was run over each subject’s video sequence for the utterances containing you.
For each utterance, this gave 4 sequences, one per subject, of the subject’s 3D head orientation and location at each video frame along with 3D head rotational velocities.
From these measurements we computed two types of visual information: participant gaze and mutual gaze.
The 3D head orientation and location of each subject along with camera calibration information was used to compute participant gaze information for each video frame of each sequence in the form of a gaze probability matrix.
More precisely, camera calibration is first used to estimate the 3D head orientation and location of all subjects in the same world coordinate system.
The gaze probability matrix is a 4 x 5 matrix where entry i, j stores the probability that subject i is looking at subject j for each of the four subjects and the last column corresponds to the white-board/projector screen (i.e., entry i, j where j = 5 is the probability that subject i is looking at the screen).
Gaze probability G(i, j) is defined as G(i, j) = G0e−αij2/'Y2 where αij is the angular difference between the gaze of subject i and the direction defined by the location of subjects i and j.
G0 is a normalization factor such that Ej G(i, j) = 1 and -y is a user-defined constant (in our experiments, we chose -y = 15 degrees).
Using the gaze probability matrix, a 4 x 1 per-frame mutual gaze vector was computed that for entry i stores the probability that the speaker and subject i are looking at one another.
In order to create features equivalent to those described in Section 4.1, we first collapse the frame-level probability matrix into a matrix of binary values.
We convert the probability for each frame into a binary judgement of whether subject i is looking at target j: Q is a binary value to evaluate G(i, j) > 0, where 0 is a high-pass thresholding value - or “gaze probability threshold” (GPT) - between 0 and 1.
Once we have a frame-level matrix of binary values, for each subject i, we compute GDP values for the time periods of interest, and in each case, choose the target with the highest GDP as the candidate.
Hence, we compute a candidate target for the utterance overall, for each third of the utterance, and for the period -/+ 2 seconds from the you start time, and in addition, we compute a candidate participant for mutual gaze with the speaker for the utterance overall.
We sought to use the GPT threshold which produces automatic visual features that agree best with the features derived from the FOA annotations.
Hence we experimented with different GPT values in increments of 0.1, and compared the resulting features to the manual features using the kappa statistic.
A threshold of 0.6 gave the best kappa scores, which ranged from 20% to 44%.6 <newSection> 5 Linguistic Information Our set of discourse features is a simplified version of those employed by Galley et al.
(2004) and Gupta et al.
(2007a).
It contains three main types (summarized in Table 4): — Sentential features (1 to 13) encode structural, durational, lexical and shallow syntactic patterns of the you-utterance.
Feature 13 is extracted using the AMI “Named Entity” annotations and indicates whether a particular participant is mentioned in the you-utterance.
Apart from this feature, all other sentential features are automatically extracted, and besides 1, 8, 9, and 10, they are all binary.
— Backward Looking (BL)/Forward Looking (FL) features (14 to 22) are mostly extracted from utterance pairs, namely the you-utterance and the BL/FL (previous/next) utterance by each listener Li (potential addressee).
We also include a few extra features which are not computed in terms of utterance pairs.
These indicate the number of participants that speak during the previous and next 5 utterances, and the BL and FL speaker order.
All of these features are computed automatically.
— Dialogue Act (DA) features (23 to 24) use the manual AMI dialogue act annotations to represent the conversational function of the you-utterance and the BL/FL utterance by each potential addressee.
Along with the sentential feature based on the AMI Named Entity annotations, these are the only discourse features which are not computed automatically.
7 <newSection> 6 First Set of Experiments & Results In this section we report our experiments and results when using manual transcriptions and annotations.
In Section 7 we will present the results obtained using ASR output and automatically extracted visual information.
All experiments (here and in the next section) are performed using a Bayesian Network classifier with 10-fold cross-validation.8 In each task, we give raw overall accuracy results and then F-scores for each of the classes.
We computed measures of information gain in order to assess the predictive power of the various features, and did some experimentation with Correlation-based Feature Selection (CFS) (Hall, 2000).
We first address the task of distinguishing between generic and referential uses of you.
Baseline.
A majority class baseline that classifies all instances of you as referential yields an accuracy of 50.86% (see Table 1).
Results.
A summary of the results is given in Table 5.
Using discourse features only we achieve an accuracy of 77.77%, while using multimodal (MM) yields 79.02%, but this increase is not statistically significant.
In spite of this, visual features do help to distinguish between generic and referential uses note that the visual features alone are able to beat the baseline (p < .005).
The listeners’ gaze is more predictive than the speaker’s: if listeners look mostly at the white-board/projector screen instead of another participant, then the you is more likely to be referential.
More will be said on this in Section 6.2.1 in the analysis of the results for the singular vs. plural referential task.
We found sentential features of the you-utterance to be amongst the best predictors, especially those that refer to surface lexical properties, such as features 1, 11, 12 and 13 in Table 4.
Dialogue act features provide useful information as well.
As pointed out by Gupta et al.
(2007b; 2007a), a you pronoun within a question (e.g. an utterance tagged as elicit-assess or elicit-inform) is more likely to be referential.
Eliminating information about dialogue acts (w/o DA) brings down performance (p < .005), although accuracy remains well above the baseline (p < .001).
Note that the small changes in performance when FL information is taken out (w/o FL) are not statistically significant.
We now turn to the referential instances of you, which can be resolved by determining the addressee(s) of the given utterance.
We start by trying to discriminate singular vs. plural interpretations.
For this, we use a two-way classification scheme that distinguishes between individual and group addressing.
To our knowledge, this is the first attempt at this task using linguistic information.9 Baseline.
A majority class baseline that considers all instances of you as referring to an individual addressee gives 67.92% accuracy (see Table 1).
Results.
A summary of the results is shown in Table 6.
There is no statistically significant difference between the baseline and the results obtained when visual features are used alone (67.92% vs. 66.28%).
However, we found that visual information did contribute to identifying some instances of plural addressing, as shown by the F-score for that class.
Furthermore, the visual features helped to improve results when combined with discourse information: using multimodal (MM) features produces higher results than the discourse-only feature set (p < .005), and increases from 74.24% to 77.05% with CFS.
As in the generic vs. referential task, the white-board/projector screen value for the listeners’ gaze features seems to have discriminative power when listeners’ gaze features take this value, it is often indicative of a plural rather than a singular you.
It seems then, that in our data-set, the speaker often uses the white-board/projector screen when addressing the group, and hence draws the listeners’ gaze in this direction.
We should also note that the ratio features which we thought might be useful here (see Section 4.1) did not prove so.
Amongst the most useful discourse features are those that encode similarity relations between the you-utterance and an utterance by a potential addressee.
Utterances by individual addressees tend to be more lexically cohesive with the you-utterance and so if features such as feature 19 in Table 4 indicate a low level of lexical similarity, then this increases the likelihood of plural addressing.
Sentential features that refer to surface lexical patterns (features 6, 7, 11 and 12) also contribute to improved results, as does feature 21 (number of speakers during the next five utterances) - fewer speaker changes correlates with plural addressing.
Information about dialogue acts also plays a role in distinguishing between singular and plural interpretations.
Questions tend to be addressed to individual participants, while statements show a stronger correlation with plural addressees.
When no DA features are used (w/o DA), the drop in performance for the multimodal classifier to 71.19% is statistically significant (p < .05).
As for the generic vs. referential task, FL information does not have a significant effect on performance.
We now turn to resolving the singular referential uses of you.
Here we must detect the individual addressee of the utterance that contains the pronoun.
Baselines.
Given the distribution shown in Table 2, a majority class baseline yields an accuracy of 35.17%.
An off-line system that has access to future context could implement a next-speaker baseline that always considers the next speaker to be the intended addressee, so yielding a high raw accuracy of 71.03%.
A previous-speaker baseline that does not require access to future context achieves 35% raw accuracy.
Results.
Table 7 shows a summary of the results, and these all outperform the majority class (MC) and previous-speaker baselines.
When all discourse features are available, adding visual information does improve performance (74.48% vs. 60.69%, p < .005), and with CFS, this increases further to 80.34% (p < .005).
Using discourse or visual features alone gives scores that are below the next-speaker baseline (60.69% and 65.52% vs. 71.03%).
Taking all forward-looking (FL) information away reduces performance (p < .05), but the small increase in accuracy caused by taking away dialogue act information is not statistically significant.
When we investigated individual feature contribution, we found that the most predictive features were the FL and backward-looking (BL) speaker order, and the speaker’s visual features (including mutual gaze).
Whomever the speaker spent most time looking at or engaged in a mutual gaze with was more likely to be the addressee.
All of the visual features had some degree of predictive power apart from the ratio features.
Of the other BL/FL discourse features, features 14, 18 and 19 (see Table 4) were more predictive.
These indicate that utterances spoken by the intended addressee are often adjacent to the you-utterance and lexically similar.
<newSection> 7 A Fully Automatic Approach In this section we describe experiments which use features derived from ASR transcriptions and automatically-extracted visual information.
We used SRI’s Decipher (Stolcke et al., 2008)10 in order to generate ASR transcriptions, and applied the head-tracker described in Section 4.2 to the relevant portions of video in order to extract the visual information.
Recall that the Named Entity features (feature 13) and the DA features used in our previous experiments had been manually annotated, and hence are not used here.
We again divide the problem into the same three separate tasks: we first discriminate between generic and referential uses of you, then singular vs. plural referential uses, and finally we resolve the addressee for singular uses.
As before, all experiments are performed using a Bayesian Network classifier and 10-fold cross validation.
<newSection> 7.1 Results For each of the three tasks, Figure 7 compares the accuracy results obtained using the fully-automatic approach with those reported in Section 6.
The figure shows results for the majority class baselines (MCBs), and with discourse-only (Dis), and multimodal (MM) feature sets.
Note that the data set for the automatic approach is smaller, and that the majority class baselines have changed slightly.
This is because of differences in the utterance segmentation, and also because not all of the video sections around the you utterances were processed by the head-tracker.
In all three tasks we are able to significantly outperform the majority class baseline, but the visual features only produce a significant improvement in the individual addressee resolution task.
For the generic vs. referential task, the discourse and multimodal classifiers both outperform the majority class baseline (p < .001), achieving accuracy scores of 68.71% and 68.48% respectively.
In contrast to when using manual transcriptions and annotations (see Section 6.1), removing forward-looking (FL) information reduces performance (p < .05).
For the referential singular vs. plural task, the discourse and multimodal with CFS classifier improve over the majority class baseline (p < .05).
Multimodal with CFS does not improve over the discourse classifier - indeed without feature selection, the addition of visual features causes a drop in performance (p < .05).
Here, taking away FL information does not cause a significant reduction in performance.
Finally, in the individual addressee resolution task, the discourse, visual (60.78%) and multimodal classifiers all outperform the majority class baseline (p < .005, p < .001 and p < .001 respectively).
Here the addition of visual features causes the multimodal classifier to outperform the discourse classifier in raw accuracy by nearly ten percentage points (67.32% vs. 58.17%, p < .05), and with CFS, the score increases further to 74.51% (p < .05).
Taking away FL information does cause a significant drop in performance (p < .05).
<newSection> 8 Conclusions We have investigated the automatic resolution of the second person English pronoun you in multi-party dialogue, using a combination of linguistic and visual features.
We conducted a first set of experiments where our features were derived from manual transcriptions and annotations, and then a second set where they were generated by entirely automatic means.
To our knowledge, this is the first attempt at tackling this problem using automatically extracted multimodal information.
Our experiments showed that visual information can be highly predictive in resolving the addressee of singular referential uses of you.
Visual features significantly improved the performance of both our manual and automatic systems, and the latter achieved an encouraging 75% accuracy.
We also found that our visual features had predictive power for distinguishing between generic and referential uses of you, and between referential sin-gulars and plurals.
Indeed, for the latter task, they significantly improved the manual system’s performance.
The listeners’ gaze features were useful here: in our data set it was apparently the case that the speaker would often use the white-board/projector screen when addressing the group, thus drawing the listeners’ gaze in this direction.
Future work will involve expanding our dataset, and investigating new potentially predictive features.
In the slightly longer term, we plan to integrate the resulting system into a meeting assistant whose purpose is to automatically extract useful information from multi-party meetings.
<newSection> References<newSection> Abstract In this paper, we describe experiments conducted on identifying a person using a novel unique correlated corpus of text and audio samples of the person’s communication in six genres.
The text samples include essays, emails, blogs, and chat.
Audio samples were collected from individual interviews and group discussions and then transcribed to text.
For each genre, samples were collected for six topics.
We show that we can identify the communicant with an accuracy of 71% for six fold cross validation using an average of 22,000 words per individual across the six genres.
For person identification in a particular genre (train on five genres, test on one), an average accuracy of 82% is achieved.
For identification from topics (train on five topics, test on one), an average accuracy of 94% is achieved.
We also report results on identifying a person’s communication in a genre using text genres only as well as audio genres only.
<newSection> 1 Introduction Can one identify a person from samples of his/her communication?
What common patterns of communication can be used to identify people?
Are such patterns consistent across varying genres?
People tend to be interested in subjects and topics that they discuss with friends, family, colleagues and acquaintances.
They can communicate with these people textually via email, text messages and chat rooms.
They can also communicate via verbal conversations.
Other forms of communication could include blogs or even formal writings such as essays or scientific articles.
People communicating in these different “genres” may have different stylistic patterns and we are interested in whether or not we could identify people from their communications in different genres.
The attempt to identify authorship of written text has a long history that predates electronic computing.
The idea that features such as average word length and average sentence length could allow an author to be identified dates to Mendenhall (1887).
Mosteller and Wallace (1964) used function words in a groundbreaking study that identified authors of The Federalist Papers.
Since then many attempts at authorship attribution have used function words and other features, such as word class frequencies and measures derived from syntactic analysis, often combined using multivariable statistical techniques.
Recently, McCarthy (2006) was able to differentiate three authors’ works, and Hill and Provost (2003), using a feature of co-citations, showed that they could successfully identify scientific articles by the same person, achieving 85% accuracy when the person has authored over 100 papers.
Levitan and Argamon (2006) and McCombe (2002) further investigated authorship identification of The Federalist Papers (three authors).
The genre of the text may affect the authorship identification task.
The attempt to characterize genres dates to Biber (1988) who selected 67 linguistic features and analyzed samples of 23 spoken and written genres.
He determined six factors that could be used to identify written text.
Since his study, new “cybergenres” have evolved, including email, blogs, chat, and text messaging.
Efforts have been made to characterize the linguistic features of these genres (Baron, 2003; Crystal, 2001; Herring, 2001; Shepherd and Watters, 1999; Yates, 1996).
The task is complicated by the great diversity that can be exhibited within even a single genre.
Email can be business-related, personal, or spam; the style can be tremendously affected by demographic factors, including gender and age of the sender.
The context of communication influences language style (Thomson and Murachver, 2001; Coupland, et al., 1988).
Some people use ab-breviations to ease the efficiency of communication in informal genres – items that one would not find in a formal essay.
Informal writing may also contain emoticons (e.g., “:-)” or “”) to convey mood.
Successes have been achieved in categorizing web page decriptions (Calvo, et al., 2004) and genre determination (Goldstein-Stewart, et al., 2007; Santini 2007).
Genders of authors have been successfully identified within the British National Corpus (Koppel, et al., 2002).
In authorship identification, recent research has focused on identifying authors within a particular genre: email collections, news stories, scientific papers, listserv forums, and computer programs (de Vel, et al., 2001; Krsul and Spafford, 1997; Madigan, et al., 2005; McCombe, 2002).
In the KDD Cup 2003 Competitive Task, systems attempted to identify successfully scientific articles authored by the same person.
The best system (Hill and Provost, 2003) was able to identify successfully scientific articles by the same person 45% of the time; for authors with over 100 papers, 85% accuracy was achieved.
Are there common features of communication of an individual across and within genres?
Undoubtedly, the lack of corpora has been an impediment to answering this question, as gathering personal communication samples faces considerable privacy and accessibility hurdles.
To our knowledge, all previous studies have focused on individual communications in one or possibly two genres.
To analyze, compare, and contrast the communication of individuals across and within different modalities, we collected a corpus consisting of communication samples of 21 people in six genres on six topics.
We believe this corpus is the first attempt to create such a correlated corpus.
From this corpus, we are able to perform experiments on person identification.
Specifically, this means recognizing which individual of a set of people composed a document or spoke an utterance which was transcribed.
We believe using text and transcribed speech in this manner is a novel research area.
In particular, the following types of experiments can be performed: - Identification of person in a novel genre (using five genres as training) - Identification of person in a novel topic (using five topics as training) - Identification of person in written genres, after training on the two spoken genres - Identification of person in spoken genres, after training on the written genres - Identification of person in written genres, after training on the other written genres In this paper, we discuss the formation and statistics of this corpus and report results for identifying individual people using techniques that utilize several different feature sets.
<newSection> 2 Corpus Collection Our interest was in the research question: can a person be identified from their writing and audio samples?
Since we hypothesize that people communicate about items of interest to them across various genres, we decided to test this theory.
Email and chat were chosen as textual genres (Table 1), since text messages, although very common, were not easy to collect.
We also collected blogs and essays as samples of textual genres.
For audio genres, to simulate conversational speech as much as possible, we collected data from interviews and discussion groups that consisted of sets of subjects participating in the study.
Genres labeled “peer give and take” allowed subjects to interact.
Such a collection of genres allows us to examine both conversational and non-conversational genres, both written and spoken modalities, and both formal and informal writing with the aim of contrasting and comparing computer-mediated and non-computer-mediated genres as well as informal and formal genres.
In order to ensure that the students could produce enough data, we chose six topics that were controversial and politically and/or socially relevant for college students from among whom the subjects would be drawn.
These six topics were chosen from a pilot study consisting of twelve topics, in which we analyzed the amount of information that people tended to “volunteer” on the topics as well as their thoughts about being able to write/speak on such a topic.
The six topics are listed in Table 2.
Topic Question Church Do you feel the Catholic Church needs to change its ways to adapt to life in the 21st Century?
Gay Marriage While some states have legalized gay marriage, others are still opposed to it.
Do you think either side is right or wrong?
Privacy Rights Recently, school officials prevented a school shooting because one of the shooters posted a myspace bulletin.
Do you think this was an invasion of privacy?
Legalization of The city of Denver has decided to Marijuana legalize small amounts of marijuana for persons over 21.
How do you feel about this?
War in Iraq The controversial war in Iraq has made news headlines almost every day since it began.
How do you feel about the war?
Gender Do you feel that gender discrimina-Discrimination tion is still an issue in the present-day United States?
The corpus was created in three phases (Goldstein-Stewart, 2008).
In Phase I, emails, essays and interviews were collected.
In Phase II, blogs and chat and discussion groups were created and samples collected.
For blogs, subjects blogged over a period of time and could read and/or comment on other subjects’ blogs in their own blog.
A graduate research assistant acted as interviewer and discussion and chat group moderator.
Of the 24 subjects who completed Phase I, 7 decided not to continue into Phase II.
Seven additional students were recruited for Phase II.
In Phase III, these replacement students were then asked to provide samples for the Phase I genres.
Four students fully complied, resulting in a corpus with a full set of samples for 21 subjects, 11 women and 10 men.
All audio recordings, interviews and discussions, were transcribed.
Interviewer/moderator comments were removed and, for each discussion, four individual files, one for each participant’s contribution, were produced.
Our data is somewhat homogeneous: it samples only undergraduate university students and was collected in controlled settings.
But we believe that controlling the topics, genres, and demographics of subjects allows the elimination of many variables that effect communicative style and aids the identification of common features.
<newSection> 3 Corpus Statistics The mean word counts for the 21 students per genre and per topic are shown in Figures 1 and 2, respectively.
Figure 1 shows that the students produced more content in the directly interactive genres – interview and discussion (the spoken genres) as well as chat (a written genre).
The email genre had the lowest mean word count, perhaps indicating that it is a genre intended for succinct messaging.
We performed an analysis of the word usage of individuals.
Among the top 20 most frequently occurring words, the most frequent word used by all males was “the”.
For the 11 females, six most frequently used “the”, four used “I”, and one used “like”.
Among abbreviations, 13 individuals used “lol”.
Abbreviations were mainly used in chat.
Other abbreviations were used to varying degrees such as the abbreviation “u”.
Emoti-cons were used by five participants.
<newSection> 4 Classification Frequencies of words in word categories were determined using Linguistic Inquiry and Word Count (LIWC).
LIWC2001 analyzes text and produces 88 output variables, among them word count and average words per sentence.
All others are percentages, including percentage of words that are parts of speech or belong to given dictionaries (Pennebaker, et al., 2001).
Default dictionaries contain categories of words that indicate basic emotional and cognitive dimensions and were used here.
LIWC was designed for both text and speech and has categories, such negations, numbers, social words, and emotion.
Refer to LIWC (www.liwc.net) for a full description of categories.
Here the 88 LIWC features are denoted feature set L.
From the original 24 participants’ documents and the new 7 participants’ documents from Phase II, we aggregated all samples from all genres and computed the top 100 words for males and for females, including stop words.
Six words differed between males and females.
Of these top words, the 64 words with counts that varied by 10% or more between male and female usage were selected.
Excluded from this list were 5 words that appeared frequently but were highly topic-specific: “catholic”, “church”, “ma-rijuana”, “marriage”, and “school.”
Most of these words appeared on a large stop word list (www.webconfs.com/stop-words.php).
Non-stop word terms included the word “feel”, which was used more frequently by females than males, as well as the terms “yea” and “lot” (used more commonly by women) and “uh” (used more commonly by men).
Some stop words were used more by males (“some”, “any”), others by females (“I”, “and”).
Since this set mainly consists of stop words, we refer to it as the functional word features or set F.
The third feature set (T) consisted of the five topic specific words excluded from F.
The fourth feature set (S) consisted of the stop word list of 659 words mentioned above.
The fifth feature set (I) we consider informal features.
It contains nine common words not in set S: “feel”, “lot”, “uh”, “women”, “people”, “men”, “gonna”, “yea” and “yeah”.
This set also contains the abbreviations and emotional expressions “lol”, “ur”, “tru”, “wat”, and “haha”.
Some of the expressions could be characteristic of particular individuals.
For example the term “wat” was consistently used by one individual in the informal chat genre.
Another feature set (E) was built around the emoticons that appeared in the corpus.
These included “:)”, “:(”, “:-(”, “;)”, “:-/”, and “>:o)”.
For our results, we use eight feature set combinations: 1.
All 88 LIWC features (denoted L); 2.
LIWC and functional word features, (L+F); 3.
LIWC plus all functional word features and the topic words (L+F+T); 4.
LIWC plus all functional word features and emoticons (L+F+E); 5.
LIWC plus all stop word features (L+S); 6.
LIWC plus all stop word and informal features (L+S+I); 7.
LIWC supplemented by informal, topic, and stop word features, (L+S+I+T).
Note that, when combined, sets S and I cover set F.
Classification of all samples was performed using four classifiers of the Weka workbench, version 3.5 (Witten and Frank, 2005).
All were used with default settings except the Random Forest classifier (Breiman, 2001), which used 100 trees.
We collected classification results for Naïve-Bayes, J48 (decision tree), SMO (support vector machine) (Cortes and Vapnik, 1995; Platt, 1998) and RF (Random Forests) methods.
<newSection> 5 Person Identification Results To identify a person as the author of a text, six fold cross validation was used.
All 756 samples were divided into 126 “documents,” each consisting of all six samples of a person’s expression in a single genre, regardless of topic.
There is a baseline of approximately 5% accuracy if randomly guessing the person.
Table 3 shows the accuracy results of classification using combinations of the feature sets and classifiers.
The results show that SMO is by far the best classifier of the four and, thus, we used only this classifier on subsequent experiments.
L+S performed better alone than when adding the informal features – a surprising result.
Table 4 shows a comparison of results using feature sets L+F and L+F+T.
The five topic words appear to grant a benefit in the best trained case (SMO).
Table 5 shows a comparison of results using feature sets L+F and L+F+E, and this shows that the inclusion of the individual emoticon features does provide a benefit, which is interesting considering that these are relatively few and are typically concentrated in the chat documents.
The next set of experiments we performed was to identify a person based on knowledge of the person’s communication in other genres.
We first train on five genres, and we then test on one – a “hold out” or test genre.
Again, as in six fold cross validation, a total of 126 “documents” were used: for each genre, 21 samples were constructed, each the concatenation of all text produced by an individual in that genre, across all topics.
Table 6 shows the results of this experiment.
The result of 100% for L+F, L+F+T, and L+F+E in email was surprising, especially since the word counts for email were the lowest.
The lack of difference in L+F and L+F+E results is not surprising since the emoticon features appear only in chat documents, with one exception of a single emoticon in a blog document (“:-/”), which did not appear in any chat documents.
So there was no emoti-con feature that appeared across different genres.
We attempted to determine which genres were most influential in identifying email authorship, by reducing the number of genres in its training set.
Results are reported in Table 7.
The difference between the two sets, which differ only in five topic specific word features, is more marked here.
The lack of these features causes accuracy to drop far more rapidly as the training set is reduced.
It also appears that the chat genre is important when identifying the email genre when topical features are included.
This is probably not just due to the volume of data since discussion groups also have a great deal of data.
We need to investigate further the reason for such a high performance on the email genre.
The results in Table 6 are also interesting for the case of L+S (which has more stop words than L+F).
With this feature set, classification for the interview genre improved significantly, while that of email decreased.
This may indicate that the set of stop words may be very genre specific – a hypothesis we will test in future work.
If this in indeed the case, perhaps certain different sets of stop words may be important for identifying certain genres, genders and individual authorship.
Previous results indicate that the usage of certain stop words as features assists with identifying gender (Sabin, et al., 2008).
Table 6 also shows that, using the informal words (feature set I) decreased performance in two genres: chat (the genre in which the abbrevi-ations are mostly used) and discussion.
We plan to run further experiments to investigate this.
The sections that follow will typically show the results achieved with L+F and L+S features.
Table 8 displays the accuracies when the L+F feature set of single genre is used for training a model tested on one genre.
This generally suggests the contribution of each genre when all are used in training.
When the training and testing sets are the same, 100% accuracy is achieved.
Examining this chart, the highest accuracies are achieved when training and test sets are textual.
Excluding models trained and tested on the same genre, the average accuracy for training and testing within written genres is 36% while the average accuracy for training and testing within spoken genres is 17%.
Even lower are average accuracies of the models trained on spoken and tested on textual genres (9%) and the models trained on textual and tested on spoken genres (6%).
This indicates that the accuracies that feature the same mode (textual or spoken) in training and testing tend to be higher.
Of particular interest here is further examination of the surprising results of testing on email with the L+F feature set.
Of these tests, a model trained on blogs achieved the highest score, perhaps due to a greater stylistic similarity to email than the other genres.
This is also the highest score in the chart apart from cases where train and test genres were the same.
Training on chat and essay genres shows some improvement over the baseline, but models trained with the two spoken genres do not rise above baseline accuracy when tested on the textual email genre.
This set of experiments was designed to determine if there was no training data provided for a certain topic, yet there were samples of communication for an individual across genres for other topics, could an author be determined?
Again a total of 126 “documents” were used: for each topic, 21 samples were constructed, each the concatenation of all text produced by an individual on that topic, across all genres.
One topic was withheld and 105 documents (on the other 5 topics) were used for training.
Table 9 shows that overall the L+S feature set performed better than either the L+F or L+F+T sets.
The most noticeable differences are the drops in the accuracy when the five topic words are added, particularly on the topics of marijuana and privacy rights.
For L+F+T, if “marijuana” is withheld from the topic word features when the marijuana topic is the test set, the accuracy rises to 90%.
Similarly, if “school” is withheld from the topic word features when the privacy rights topic is the test set, the accuracy rises to 100%.
This indicates the topic words are detrimental to determining the communicant, and this appears to be supported by the lack of an accuracy drop in the testing on the Iraq and sexual discrimination topics, both of which featured the fewest uses of the five topic words.
That the results rise when using the L+S features shows that more features that are independent of the topic tend to help distinguish the person (as only the Iraq set experienced a small drop using these features in training and testing, while the others either increased or remained the same).
The similarity here of the results using L+F features when compared to L+F+E is likely due to the small number of emo-ticons observed in the corpus (16 total examples).
One interesting experiment used one speech genre for training, and the other speech genre for testing.
The results (Table 10) show that the additional stop words (S compared to F) make a positive difference in both sets.
We hypothesize that the increased performance of training with discussion data and testing on interview data is due to the larger amount of training data available in discussions.
We will test this in future work.
Table 11 shows the results of training on speech data only and predicting the author of the text genre.
Again, the speech genres alone do not do well at determining the individual author of the text genre.
The best score was 29% for essays.
Table 12 shows the results of training on text data only and predicting authorship for one of the four text genres.
Recognizing the authors in chat is the most difficult, which is not surprising since the blogs, essays and emails are more similar to each other than the chat genre, which uses ab-breviations and more informal language as well as being immediately interactive.
Training on text and classifying speech-based samples by author showed poor results.
Similar to the results for speech genres, using the text genres alone to determine the individual in the speech genre results in a maximum score of 29% for the interview genre (Table 13).
Results for different training and test sets vary considerably.
A key factor in determining which sets can successfully be used to train other sets seems to be the mode, that is, whether or not a set is textual or spoken, as the lowest accuracies tend to be found between genres of different modes.
This suggests that how people write and how they speak may be somewhat distinct.
Typically, more data samples in the training tends to increase the accuracy of the tests, but more features does not guarantee the same result.
An examination of the feature sets revealed further explanations for this apart from any inherent difficulties in recognizing authors between sets.
For many tests, there is a tendency for the same person to be chosen for classification, indicating a bias to that person in the training data.
This is typically caused by features that have mostly, but not all, zero values in training samples, but have many non-zero values in testing.
The most striking examples of this are described in 5.3, where the removal of certain topic-related features was found to dramatically increase the accruacy.
Targetted removal of other features that have the same biasing effect could increase accuracy.
While Weka normalizes the incoming features for SMO, it was also discovered that a simple initial normalization of the feature sets by dividing by the maximum or standardization by subtracting the mean and dividing by the standard deviation of the feature sets could increase the accuracy across the different tests.
<newSection> 6 Conclusion In this paper, we have described a novel unique corpus consisting of samples of communication of 21 individuals in six genres across six topics as well as experiments conducted to identify a person’s samples within the corpus.
We have shown that we can identify individuals with reasonably high accuracy for several cases: (1) when we have samples of their communication across genres (71%), (2) when we have samples of their communication in specific genres other than the one being tested (81%), and (3) when they are communicating on a new topic (94%).
For predicting a person’s communication in one text genre using other text genres only, we were able to achieve a good accuracy for all genres (above 76%) except chat.
We believe this is because chat, due to its “real-time communication” nature is quite different from the other text genres of emails, essays and blogs.
Identifying a person in one speech genre after training with the other speech genre had lower accuracies (less than 48%).
Since these results differed significantly, we hypothesize this is due to the amount of data available for training – a hypothesis we plan to test in the future.
Future plans also include further investigation of some of the suprising results mentioned in this paper as well investigation of stop word lists particular to communicative genres.
We also plan to investigate if it is easier to identify those participants who have produced more data (higher total word count) as well as perform a systematic study the effects of the number of words gathered on person identificaton.
İn addition, we plan to investigate the efficacy of using other features besides those available in LIWC, stopwords and emoticons in person identification.
These include spelling errors, readability measures, complexity measures, suffixes, and content analysis measures.
<newSection> References<newSection> Abstract A common way of describing the senses of ambiguous words in multilingual Word Sense Disambiguation (WSD) is by reference to their translation equivalents in another language.
The theoretical soundness of the senses induced in this way can, however, be doubted.
This type of cross-lingual sense identification has implications for multilingual WSD and MT evaluation as well.
In this article, we first present some arguments in favour of a more thorough analysis of the semantic information that may be induced by the equivalents of ambiguous words found in parallel corpora.
Then, we present an unsupervised WSD method and a lexical selection method that exploit the results of a data-driven sense induction method.
Finally, we show how this automatically acquired information can be exploited for a multilingual WSD and MT evaluation more sensitive to lexical semantics.
<newSection> 1 Word senses in a bi-(mu/ti-)/ingua/ context Determining the senses of ambiguous words by reference to their translational equivalents constitutes a common practice in multilingual WSD: the candidate senses of an ambiguous word, from which one has to be selected during WSD, correspond to its equivalents in another language.
This empirical approach to sense identification circumvents the need for predefined sense inventories and their disadvantages for automatic WSD.1 The first to adopt it were Brown et al.
(1991), who represented the two main senses of a SL word by its two most frequent translations in the target language (TL).
Further promoted by Resnik and Yarowsky (2000) and endorsed in the multilingual tasks of the Senseval (Chklovski et al., 2004) and Semeval (Jin et al., 2007) exercises, this conception of senses is still found in recent works on the integration of WSD in MT.
From these works, only that of Carpuat and Wu (2005) exploits an external hand-crafted sense inventory.
The use of an external resource, not related to the training corpus of their Statistical Machine Translation (SMT) system, turned out to be one of the causes of the observed deterioration of translation quality.
In later works on the subject, which show a more or less important improvement in translation quality, SL word senses are considered as directly reflected in their equivalents found in a parallel training corpus (Cabezas and Resnik, 2005; Carpuat and Wu, 2007; Chan et al., 2007).
Nevertheless, the theoretical soundness of these senses is not really addressed.
Cross-lingual sense induction offers a standard criterion for sense delimitation: the translation equivalents of ambiguous words are supposed to reveal their hidden meanings (Resnik, 2004).
Additional advantages become evident in MT: when the candidate senses of an ambiguous word consist of its possible translations, identifying the sense carried by a new instance of the word coincides with its translation.
Conceiving WSD as lexical selection thus seems natural (Vickrey et al., 2005): it appears that there is no reason to pass through senses in order to arrive to translations.
A correct translation may be attained even without WSD, as in the case of parallel ambiguities where the SL and TL words are similarly ambiguous (Resnik and Yarowsky, 2000).2 However, this conception of senses is not theoretically sound, as translation equivalents do not always constitute valid sense indicators.
This is often neglected in an attempt to render the sense inventory as close as possible to the training corpus of the SMT system.
So, translation equivalents are considered as straightforward indicators of SL senses.
This approach assumes and results in some type of uniformity regarding the nature of the induced senses: clear-cut (e.g. homonymic) and finer sense distinctions are all handled in the same way.
Moreover, senses are enumerated without any description of their possible relations.
For instance, a SL word w having three equivalents (a, b and c) is considered to have three distinct senses (described as `w-a', `w-b' et `w-c)'.
The assumption of biunivocal (one-to-one) correspondences between senses and equivalents disregards the fact that semantically similar equivalents may be used to translate the same sense of a SL word in context.
However, this constitutes a common practice in translation and an advised technique for translators, in order to avoid repetitions in the translated texts.
The phenomenon of translation ambiguity may pose some problems as well: it may not need to be resolved during translation but should be considered in multilingual WSD.
Resolving this kind of ambiguity could also improve the quality of the results of applications such as multilingual information retrieval.
Ignoring the relations between word senses may raise further problems during WSD evaluation, as errors concerning close or distant senses are considered as equally important.
Thus, if a WSD algorithm selects a sense which is slightly 2 A typical example is that of the ambiguous English noun interestwhose &quot;personal&quot; and &quot;financial&quot; senses are translated by the same word in French (interet).
different from the one effectively carried by an instance of an ambiguous word, but not totally wrong, this is directly considered as a false choice.
A differing weighting of WSD errors would be preferable in these cases, if sense distance information was available (Resnik and Yarowsky, 2000).
When WSD coincides with lexical selection in MT, the equivalents of a SL word (w) are perceived to be its candidate senses.
The sense assigned to a new instance of w is considered to be correct if it corresponds to the reference translation (i.e. the translation of that instance in the test corpus).
This strict requirement of exact correspondence constitutes one of the main critics addressed to MT evaluation metrics (Cabezas and Resnik, 2005; Callison-Burch, 2006; Chan et al., 2007) and is one of the main reasons that methods have been developed which go beyond pure string matching (Owczarzak et al., 2007).
A central issue in MT evaluation is the high correlation of the metrics with human judgements of translation quality, which puts the accent on the identification of sense correspondences.
Here too, it is essential to penalize errors relatively to their importance and so information relative to the semantics of the equivalents should be available.
In the next section we will show how this information can be acquired using a data-driven sense induction method.
<newSection> 2 Data-driven semantic ana/ysis in a bi/ingua/ context We propose to explore the semantic relations of the equivalents of ambiguous words using a parallel corpus and to exploit these relations for SL sense induction.
A data-driven sense acquisition method based on this type of relations is presented in Apidianaki (2008).
The theoretical assumptions underlying this approach are the distributional hypotheses of meaning (Harris, 1954) and of semantic similarity (Miller and Charles, 1989), and that of sense correspondence between words in translation relation in real texts.
Our training corpus is the English (EN)— Greek (GR) part of the lemmatized and POS-tagged INTERA corpus (Gavrilidou et al., 2004) which contains approximately four million words.
The corpus has been sentence- and word-aligned at the level of tokens and types (Simard and Langlais, 2003).
Two bilingual lexicons (one for each translation direction: EN—GRiGR—EN are built from the alignment of word types.
In these lexicons, each SL word (&) is associated with the set of the equivalents to which it is aligned, as shown hereafter: The words in parentheses describe the senses of the Greek equivalents.
In order to eliminate the noise present in the lexicons, two filters are used: a POS-filter, that keeps only the correspondences between words of the same category' and an intersection filter, which discards the translation correspondences not found in both translation lexicons.
A lexical sample of 150 ambiguous English nouns having more than two equivalents is then created from the EN—GR lexicon°.
At this stage, the semantic relations possibly existing between the equivalents are not yet evident, and so no conclusions can be extracted concerning the distinctiveness of the senses they can induce on the SL words.
The core component of the sense induction method used is a semantic similarity calculation which aims at discovering the relations between the equivalents of a SL ambiguous word (&).
First, the translation units (TUs)� in which & appears in the SL sentence(s) are extracted from the training corpus and are then grouped by reference to &'s equivalents.
For instance, if & is translated by $, ' and (, three sets of TUs are formed (where & is translated by $ ('&D$' TUs), by ' ('&D'' TUs), etc.).
The SL context features corresponding to each equivalent (i.e. the set of lemmatized content words surrounding & in the SL side of the TUs corresponding to the equivalent) are extracted and treated as a 'bag of words'.
This distributional information serves to calculate the equivalents' similarity using a variation of the Weighted Jaccard coefficient (Grefenstette, 199°).
The similarity calculation is described in detail in Apidianaki (2008).
Each retained context feature is assigned a weight relatively to each equivalent, which ' The noun equivalents of nouns, the verb equivalents of verbs, etc.
° Here we focus on nouns but the method is applicable to words of other POS categories.
s A translation unit contains up to 2 sentences of each language linked by an alignment.
serves to define its relevance for the estimation of the equivalents' similarity.
The equivalents are compared in a pairwise manner and a similarity score is assigned to each pair.
Two equivalents are considered as semantically related if the instances of & they translate in the training corpus occur in &quot;similar enough&quot; contexts.
The pertinence of their relation is judged by comparing its score to a threshold, equal to the mean of the scores assigned to all the pairs of equivalents of &.
The results of this calculation are exploited by a clustering algorithm which takes as input the set of equivalents of & and outputs clusters of similar equivalents illustrating its senses (Apidianaki, 2008).
Clustered equivalents are semantically related' and considered as translating the same SL sense, while isolated ones translate distinct senses.
The same calculation is performed by reference to the TL contexts of the equivalents, i.e. using the lemmatized content words surrounding the equivalents in the TL side of the corresponding TUs sets.
Contrary to the SL results, the TL ones are not used for clustering.
The TL distributional information relative to the clustered equivalents and acquired at this stage will be used for lexical selection, as we will show later in this paper.
The sense clusters created for a word serve to identify its senses.
We describe the senses acquired for the nouns )EF%)($&quot;)G* and H$+)$&quot;)G*: imp/ication: {/0123456, 437389/:}: the &quot;impact&quot; sense {4353;<=>}: the &quot;complication&quot; sense variation: {?56=@A61/:}: the &quot;fluctuation&quot; sense {A486B<;>I 8C<3<3<7:/:}: the &quot;alteration&quot; sense The sense induction method presented above thus permits the automatic creation of a sense inventory from a parallel corpus.
In what follows, we will show how this can be exploited for WSD.
<newSection> 3 Unsupervised WSD based on the semantic c/ustering The method described in section 2 provides, as a by-product, information that can be exploited by an unsupervised WSD classifier.
In the case of a one-equivalent cluster, this information corresponds to the set of the equivalent's ' Most often near-synonyms but they may be linked by other relations (hyperonymy, hyponymy, etc.).
features, retained from the corresponding SL contexts of w.
In the case of bigger clusters, it consists of the SL context features that reveal the equivalents' similarities: for a cluster of two equivalents, it consists of their assimilative contexts (i.e. the features they share)7; for a cluster of more than 2 equivalents, it consists of the intersection of the common features of the pairs of equivalents found in the cluster.
As we have already said, each retained context feature is assigned a weight relatively to each equivalent.
Here are the weighted features characterizing the clusters of variation : {8tax6ttavuqj: significant (2.04), range (0.76), pharmacokinetics(1.89), individual (1.89), affect (1.89), insulin (1.89), woman (1.89), year (1.49), man (1.19), considerable (1.19), member (1.12), old (0.76), Ireland (0.76), case (0.72), increase (0.76), group (0.76), states (0.71), external (0.76), good (0.76), expectancy (0.76), Spain (0.76), pressure (0.76), Europe (0.76) In order to disambiguate a new instance of a word w, cooccurrence information coming from its context is compared to the sets of features characterizing the clusters.
The new context must thus be lemmatized and POS-tagged as well.
Here is an example of a new instance of variation: a.
&quot;Although certain regions have been faced with an exodus of their endogenous population, most of the coastal zones are experiencing an increase in overall demographic pressure, as well as significant seasonal variations in employment, essentially linked to tourism.&quot; The features retained from this context are the lemmas of the content words (nouns, verbs and adjectives) surrounding w.
If common features (CFs) are found between this context and just one cluster of w, this is selected as describing the sense of the new instance.
On the contrary, if CFs with more than one cluster are found, a score is given to each context-cluster association.
This score corresponds to the mean of the weights of the CFs relatively to each equivalent of the cluster and is given by the following formula.
In this formula, e is the number of the equivalents of a cluster and f is the number of its CFs with the new context.
The cluster with the highest score is retained; it describes the sense carried by the new instance of w and could be used as its sense tag.
The only cluster having CFs with the context of variation in (a) and is thus selected is J&axauavorl} ( C F s : increase, pressure, significant).
If any instances remain ambiguous at the end of the WSD process (i.e. no associations are established with the sense clusters), a small modification could increase the method's coverage.
If w has clusters of more than two equivalents, it is possible to use the assimilative contexts of the pairs of equivalents instead of their intersection.
The coverage of the WSD method would be increased in this way, as the sets of assimilative contexts would contain more features than their intersection, and so it would become more probable to find CFs with the new contexts and to establish 'context-cluster' associations.
<newSection> 4 Semantics-sensitive WSD eva/uation In this section, we will present the evaluation of the proposed WSD method and we will show how the clustering information can be exploited at this stage.9 The new instances of the nouns of our lexical sample, used for evaluation, come from our test corpus, the sentence aligned EN— GR part of EUROPARL (Koehn, 2005).
The TUs containing the ambiguous nouns are extracted from the corpus.
Lacking a gold-standard for evaluation, we exploit information relative to translations.
In the multilingual tasks of Senseval and Semeval (Ckhlovski et al., 2004; Jin et al., 2007), the translations of the words in the parallel test corpus are considered as their sense tags.
Here, we consider that the equivalent translating an ambiguous SL word in context (called reference translation) points to a sense described by a cluster.
Consequently, what is being evaluated is the capacity of the WSD method to predict this sense.
The sense proposed for an instance of an ambiguous word is considered as correct if a) a 1-equivalent cluster is selected and the equivalent corresponds to the reference, or b) if a bigger cluster containing the reference is selected.
Otherwise, the proposed sense is fa/se.
In the multilingual tasks where translations are regarded as sense tags, the proposed senses are considered as correct only if they correspond exactly to the reference translation.
This is the principle ofprecision, underlying most of the existing MT evaluation metrics.
From a quantitative point of view, this strict criterion has a negative impact on the WSD evaluation results.
From a qualitative point of view, it ignores the fact that different equivalents may correspond to the same source sense and that an ambiguous word in context can have more than one good translation.
The use of the sense clusters during WSD evaluation offers the possibility of capturing the semantic relations between the equivalents of ambiguous words, acquired during learning.
In this case, the evaluation could be considered as based on a principle of enriched precision that exploits the paradigmatic relations of TL words.
The metrics used for WSD evaluation are the following: number of correct predictions number of correct predictions number of predictions The obtained results are compared to those of a baseline method.
The baseline most often used in Senseval is that of the most frequent sense (i.e. the first sense given for a word in a predefined sense inventory).
This is a very powerful heuristic because of the asymmetric distribution of word senses in real texts.
Our baseline consists of choosing the most frequent equivalent (i.e. the one that translates w most frequently in the training corpus) as illustrating the sense of all its new instances.
The asymmetric distribution of senses is, however, reflected at the level of the equivalents used to translate them: the most frequent equivalent in the training corpus is often the one that translates most of the instances of w in the test corpus.
The baseline score corresponds to both recall and precision, as a prediction is made for all the new instances.
This score is calculated, for each w, on the basis of the number of its instances for which the proposed sense is correct.
This number coincides with the frequency of the most frequent equivalent of w in the test corpus.
In order to facilitate the comparison between our results and the baseline, we use thef-measure (f-score) that combines precision and recall in a unique measure: We evaluate here the performance of our WSD method on the 150 ambiguous nouns of our sample.
We observe that the f-score of our method easily overcomes the results of the baseline.
base/ine 51.42% enriched f-score 76.99% The difference between these scores indicates the positive impact of the clustering information on the WSD results.
As the senses are situated at a higher level of abstraction, the correspondences with the reference are established at a more abstract level than that of exact unigram correspondences.
Our results can be compared to those obtained in the multilingual lexical sample tasks of Senseval and SemEval.
This comparison seems interesting although these tasks concern words of different parts of speech (nouns, verbs and adjectives).
The systems participating at the multilingual English—Hindi lexical sample task of Senseval-3 are all supervised and they all perform better than the baseline (Chklovski et al., 2004).
This is interpreted by the authors as an indication of the clarity of the sense distinctions performed using translations, which provide sufficient information for the training of supervised classifiers.
The systems performed better on the sense-tagged part of the data, showing that sense information may be helpful for the task of targeted word translation.
In the English—Chinese lexical sample task of SemEval the unsupervised systems perform worse than the baseline, contrary to the supervised ones (Jin !&quot; $%., 2007).
<newSection> 5 Capturing semantic simi/arity during trans/ation In the experiments reported here, %!R)($% ,!%!(&quot;)G* refers to the translation of ambiguous SL nouns in context and not to that of whole sentences.
Lexical selection is thus considered as a '%$*MDK)%%)*J task (Vickrey !&quot; $%., 2005): the equivalents translating the SL nouns in the TL sentences of the test TUs are automatically replaced by a blank which has to be filled by the WSD or the lexical selection method.
We give an example of a test TU containing the noun )EF%)($&quot;)G*.
If a one-equivalent cluster is selected by the WSD method, this equivalent is retained as the translation of the SL word (cf. ($), section 3).
On the contrary, when a bigger sense cluster is proposed, the most adequate equivalent for the TL context has to be selected.
This is done by the lexical selection method, which filters the cluster and fills the blank in the TL sentence with the best translation according to the TL context.
The cluster retained during WSD as describing the sense of )EF%)($&quot;)G* in (b) is J/0123456I 437389/:}.
Most often the clustered equivalents are near-synonyms translating the same source sense, but almost never absolute synonyms interchangeable in all TL contexts.
Consequently, the cluster can be filtered by considering their differences.
In order to judge the equivalents' adequacy in the new TL context, the lexical selection method compares information coming from this context to information learned during training.
Given that the training was performed on a lemmatized and POS-tagged corpus, the new TL context must be lemmatized and POS-tagged as well, in order to retain only the lemmas of the content words10.
The information acquired during training and exploited here concerns the context features that differentiate the equivalents in the TL, as shown by the semantic similarity calculation in the TL side of the training corpus (cf. section 2).
The differentiating contexts of the equivalents characterize the sense clusters as well, as was the case with their assimilative contexts.11 The equivalent retained by the lexical selection method for )EF%)($&quot;)G* in the example (b) is /0123456.
This differs from the reference translation (437389/:) but is closely related to it.
Thus, it is a semantically plausible translation that can be used in this TL context.
In a real Statistical Machine Translation (SMT) system, the clusters could be filtered by the language model, on the basis of word sequence probabilities in translations.
In this way, the most probable translation in the TL context, among the semantically pertinent alternatives included in the cluster suggested during WSD, would be selected.
The lexical selection method has been applied to the WSD results on our lexical sample.
The reference translations, found in the test corpus, serve for evaluation here as well.
We calculate the results of this method first using the principle of ,&quot;+)(&quot; F+!(),)G* (i.e. looking for exact correspondences with the reference) and then on the basis of !*+)(L!N F+!(),)G* (i.e. exploiting the clustering information).
The sense clusters serve here to estimate the semantic proximity of the proposed translation to the reference, in cases of no exact correspondence.
Thus, a translation which is semantically similar to the reference is considered to be correct if they are both found in the cluster proposed during WSD.
This renders the evaluation more flexible and significantly increases the quantity of semantically pertinent translations compared to the baseline.
The strict and enrichedKDscores are estimated by considering as correct (score = 1) every translation that is pertinent according to the corresponding evaluation principles.
The results indicate the increase in pertinent translations.
base/ine 52.14% strict -f-score 48.37% enriched f-score 77.79% We observe that the strict f-score is lower than the baseline.
This happens because our method proposes equivalents semantically similar to the reference for some instances for which the baseline predictions are correct.
However, these pertinent predictions are not taken into account by the principle of strict precision.
This is the case in example (b): the baseline prediction (e7chacouq) for this instance o f implication corresponds to the reference while the suggestion of our method (uvve7ceaa), even though semantically pertinent, is not considered as correct according to the principle of strict precision and is not rewarded.
Nevertheless, it would be preferable to weigh differently the predictions related to the reference, by taking into account the strength of their relation.
These predictions could be considered as almost correct and they could be, at the same time, penalized less than translations having a different sense and less rewarded than exact correspondences to the reference.
For this to be done, a measure capable of capturing the semantic distance would be needed.
Using a weighted coefficient is essential in tasks implicating semantics, not only in WSD (Resnik and Yarowsky, 2000) but also in tasks such as the estimation of inter-annotator agreement in semantic annotation (Artstein and Poesio, 2008).
The common element between these tasks is that the distances between the categories (word senses) should be weighted, so that the WSD errors or the divergences between annotators be treated differently.
We envisaged the possibility of weighting differently the proposed translations on the basis of their relation to the reference, by using as distance measure their similarity score in the TL.
A semantically pertinent translation different from the reference was assigned a score equal to the similarity score of the two equivalents in the TL.
A problem that we encountered, and that made us fall back to the solution of a uniform weighting of semantically pertinent translations, is that the comparison of these results to the baseline was not representative of the effective improvement (the great increase in the number of pertinent translation predictions) brought about by exploiting the clustering information.
This happens because all the correct suggestions of the baseline are weighted by a score equal to 1, while the score of translations semantically related to the reference is always lower than 1, given that absolute synonyms are very rare in natural language.
We envisage the elaboration of a more sophisticated coefficient for weighting semantically pertinent translations, that will permit a more conclusive comparison with the baseline.
This coefficient could take into account not only the similarity score between a proposed translation and the reference but also the number of the SL word's candidate translations, the number of its senses and their distinctiveness, as well as the number of the equivalents similar to the reference and their scores.
Before concluding, we would like to take a look at the way the concern for lexical semantics is manifested and taken into account in existing MT evaluation metrics.
Lexical semantic relations are supposed to be captured in BLEU by the use of multiple reference translations (Papineni et al., 2002).
Finding many references for evaluation is, however, rather problematic (Callison-Burch, 2006).
In METEOR (Banerjee and Lavie, 2005), such relations are detected by exploiting WordNet (Miller et al., 1990).
More precisely, the number of pertinent translations is increased using synset information: a translation is correct not only if it corresponds to the reference, but also if it is semantically similar to it, i.e. found in the same synset.
One of the limitations of this metric is that the words being tested for synonymy are not disambiguated; that is what Banerjee and Lavie call &quot;a poor-man's synonymy detection algorithm&quot;.
Consequently, the WN-Synonymy module used maps two unigrams together simply if at least one sense of each word belongs to the same WordNet synset.
Another problem is that the metric is strongly dependent on a predefined sense inventory.
Given that such resources are publicly available for very few languages, the synonymy module often is not operational and is omitted.
Lavie and Agarwal (2007) envisage the possibility of developing new synonymy modules for languages other than English, which would be based on alternative methods and could replace WordNet.
In the previous sections, we showed how the information acquired by an unsupervised sense induction method can help to account for the words' semantic similarity.
The created sense clusters, grouping semantically similar equivalents, can be compared to WordNet synsets.
This kind of semantic information, extracted directly from text data, can constitute an alternative to the use of predefined sense inventories.
A clear advantage of a metric based on the results of unsupervised semantic analysis, in comparison to one dependent on a predefined resource, is that it is language-independent and may be used for evaluation in languages where semantic resources are not available.
<newSection> 6 Conclusion and perspectives In this paper, we have presented the advantages and weaknesses of cross-lingual sense determination, often used in multilingual WSD and MT.
We have put forward some arguments towards a more thorough semantic analysis of the translation equivalents of ambiguous words that serve as sense indicators, and we have shown how it could be of use in multilingual WSD and MT.
The data-driven sense induction method used identifies the senses of ambiguous English nouns by clustering their translation equivalents according to their semantic similarity.
Exploiting the sense inventory built in this way proves of benefit in multilingual WSD and lexical selection in MT.
Their evaluation becomes more flexible as well, as it becomes possible to capture the semantic relations between the translations of ambiguous words.
The problem of strictness of the MT evaluation metrics can thus be overcome without the need for a predefined inventory.
This would allow for a more conclusive estimation of the effect of WSD in SMT.
The integration of the cluster-based WSD method into a real SMT system and the evaluation of its impact on translation quality constitute the main perspectives of the work presented in this article and the object of future work.
<newSection> Acknow/edgments I would like to thank Philippe Langlais for the word alignment and Andy Way for useful comments.
This research is funded by SFI grant 05iINi1732.
<newSection> References<newSection> Abstract The lack of positive results on supervised domain adaptation for WSD have cast some doubts on the utility of hand-tagging general corpora and thus developing generic supervised WSD systems.
In this paper we show for the first time that our WSD system trained on a general source corpus (BNC) and the target corpus, obtains up to 22% error reduction when compared to a system trained on the target corpus alone.
In addition, we show that as little as 40% of the target corpus (when supplemented with the source corpus) is sufficient to obtain the same results as training on the full target data.
The key for success is the use of unlabeled data with SVD, a combination of kernels and SVM.
<newSection> 1 Introduction In many Natural Language Processing (NLP) tasks we find that a large collection of manually-annotated text is used to train and test supervised machine learning models.
While these models have been shown to perform very well when tested on the text collection related to the training data (what we call the source domain), the performance drops considerably when testing on text from other domains (called target domains).
In order to build models that perform well in new (target) domains we usually find two settings (Daum´e III, 2007).
In the semi-supervised setting, the training hand-annotated text from the source domain is supplemented with unlabeled data from the target domain.
In the supervised setting, we use training data from both the source and target domains to test on the target domain.
In (Agirre and Lopez de Lacalle, 2008) we studied semi-supervised Word Sense Disambiguation (WSD) adaptation, and in this paper we focus on supervised WSD adaptation.
We compare the performance of similar supervised WSD systems on three different scenarios.
In the source to target scenario the WSD system is trained on the source domain and tested on the target domain.
In the target scenario the WSD system is trained and tested on the target domain (using cross-validation).
In the adaptation scenario the WSD system is trained on both source and target domain and tested in the target domain (also using cross-validation over the target data).
The source to target scenario represents a weak baseline for domain adaptation, as it does not use any examples from the target domain.
The target scenario represents the hard baseline, and in fact, if the domain adaptation scenario does not yield better results, the adaptation would have failed, as it would mean that the source examples are not useful when we do have hand-labeled target examples.
Previous work shows that current state-of-theart WSD systems are not able to obtain better results on the adaptation scenario compared to the target scenario (Escudero et al., 2000; Agirre and Martinez, 2004; Chan and Ng, 2007).
This would mean that if a user of a generic WSD system (i.e. based on hand-annotated examples from a generic corpus) would need to adapt it to a specific domain, he would be better off throwing away the generic examples and hand-tagging domain examples directly.
This paper will show that domain adaptation is feasible, even for difficult domain-related words, in the sense that generic corpora can be reused when deploying WSD systems in specific domains.
We will also show that, given the source corpus, our technique can save up to 60% of effort when tagging domain-related occurrences.
We performed on a publicly available corpus which was designed to study the effect of domains in WSD (Koeling et al., 2005).
It comprises 41 nouns which are highly relevant in the SPORTS and FINANCES domains, with 300 examples for each.
The use of two target domains strengthens the conclusions of this paper.
Our system uses Singular Value Decomposi-tion (SVD) in order to find correlations between terms, which are helpful to overcome the scarcity of training data in WSD (Gliozzo et al., 2005).
This work explores how this ability of SVD and a combination of the resulting feature spaces improves domain adaptation.
We present two ways to combine the reduced spaces: kernel combination with Support Vector Machines (SVM), and k Nearest-Neighbors (k-NN) combination.
The paper is structured as follows.
Section 2 reviews prior work in the area.
Section 3 presents the data sets used.
In Section 4 we describe the learning features, including the application of SVD, and in Section 5 the learning methods and the combination.
The experimental results are presented in Section 6.
Section 7 presents the discussion and some analysis of this paper and finally Section 8 draws the conclusions.
<newSection> 2 Prior work Domain adaptation is a practical problem attracting more and more attention.
In the supervised setting, a recent paper by Daum´e III (2007) shows that a simple feature augmentation method for SVM is able to effectively use both labeled target and source data to provide the best domain-adaptation results in a number of NLP tasks.
His method improves or equals over previously explored more sophisticated methods (Daum´e III and Marcu, 2006; Chelba and Acero, 2004).
The feature augmentation consists in making three version of the original features: a general, a source-specific and a target-specific versions.
That way the augmented source contains the general and source-specific version and the augmented target data general and specific versions.
The idea behind this is that target domain data has twice the influence as the source when making predictions about test target data.
We reimplemented this method and show that our results are better.
Regarding WSD, some initial works made a basic analysis of domain adaptation issues.
Escud-ero et al.
(2000) tested the supervised adaptation scenario on the DSO corpus, which had examples from the Brown corpus and Wall Street Journal corpus.
They found that the source corpus did not help when tagging the target corpus, showing that tagged corpora from each domain would suffice, and concluding that hand tagging a large general corpus would not guarantee robust broad-coverage WSD.
Agirre and Mart´ınez (2000) used the DSO corpus in the supervised scenario to show that training on a subset of the source corpora that is topically related to the target corpus does allow for some domain adaptation.
More recently, Chan and Ng (2007) performed supervised domain adaptation on a manually selected subset of 21 nouns from the DSO corpus.
They used active learning, count-merging, and predominant sense estimation in order to save target annotation effort.
They showed that adding just 30% of the target data to the source examples the same precision as the full combination of target and source data could be achieved.
They also showed that using the source corpus allowed to significantly improve results when only 10%-30% of the target corpus was used for training.
Unfortunately, no data was given about the target corpus results, thus failing to show that domain-adaptation succeeded.
In followup work (Zhong et al., 2008), the feature augmentation approach was combined with active learning and tested on the OntoNotes corpus, on a large domain-adaptation experiment.
They reduced significantly the effort of hand-tagging, but only obtained domain-adaptation for smaller fractions of the source and target corpus.
Similarly to these works we show that we can save annotation effort on the target corpus, but, in contrast, we do get domain adaptation when using the full dataset.
In a way our approach is complementary, and we could also apply active learning to further reduce the number of target examples to be tagged.
Though not addressing domain adaptation, other works on WSD also used SVD and are closely related to the present paper.
Ando (2006) used Alternative Structured Optimization.
She first trained one linear predictor for each target word, and then performed SVD on 7 carefully selected submatrices of the feature-to-predictor matrix of weights.
The system attained small but consistent improvements (no significance data was given) on the Senseval-3 lexical sample datasets using SVD and unlabeled data.
Gliozzo et al.
(2005) used SVD to reduce the space of the term-to-document matrix, and then computed the similarity between train and test instances using a mapping to the reduced space (similar to our SMA method in Section 4.2).
They combined other knowledge sources into a complex kernel using SVM.
They report improved performance on a number of languages in the Senseval3 lexical sample dataset.
Our present paper differs from theirs in that we propose an additional method to use SVD (the OMT method), and that we focus on domain adaptation.
In the semi-supervised setting, Blitzer et al.
(2006) used Structural Correspondence Learning and unlabeled data to adapt a Part-of-Speech tagger.
They carefully select so-called ‘pivot features’ to learn linear predictors, perform SVD on the weights learned by the predictor, and thus learn correspondences among features in both source and target domains.
Our technique also uses SVD, but we directly apply it to all features, and thus avoid the need to define pivot features.
In preliminary work we unsuccessfully tried to carry along the idea of pivot features to WSD.
On the contrary, in (Agirre and Lopez de Lacalle, 2008) we show that methods closely related to those presented in this paper produce positive semi-supervised domain adaptation results for WSD.
The methods used in this paper originated in (Agirre et al., 2005; Agirre and Lopez de Lacalle, 2007), where SVD over a feature-to-documents matrix improved WSD performance with and without unlabeled data.
The use of several k-NN classifiers trained on a number of reduced and original spaces was shown to get the best results in the Senseval-3 dataset and ranked second in the SemEval 2007 competition.
The present paper extends this work and applies it to domain adaptation.
<newSection> 3 Data sets The dataset we use was designed for domain-related WSD experiments by Koeling et al.
(2005), and is publicly available.
The examples come from the BNC (Leech, 1992) and the SPORTS and FINANCES sections of the Reuters corpus (Rose et al., 2002), comprising around 300 examples (roughly 100 from each of those corpora) for each of the 41 nouns.
The nouns were selected because they were salient in either the SPORTS or FINANCES domains, or because they had senses linked to those domains.
The occurrences were hand-tagged with the senses from WordNet (WN) version 1.7.1 (Fellbaum, 1998).
In our experiments the BNC examples play the role of general source corpora, and the FINANCES and SPORTS examples the role of two specific domain target corpora.
Compared to the DSO corpus used in prior work (cf. Section 2) this corpus has been explicitly created for domain adaptation studies.
DSO contains texts coming from the Brown corpus and the Wall Street Journal, but the texts are not classified according to specific domains (e.g. Sports, Finances), which make DSO less suitable to study domain adaptation.
The fact that the selected nouns are related to the target domain makes the (Koeling et al., 2005) corpus more demanding than the DSO corpus, because one would expect the performance of a generic WSD system to drop when moving to the domain corpus for domain-related words (cf. Table 1), while the performance would be similar for generic words.
In addition to the labeled data, we also use unlabeled data coming from the three sources used in the labeled corpus: the ’written’ part of the BNC (89.7M words), the FINANCES part of Reuters (32.5M words), and the SPORTS part (9.1M words).
<newSection> 4 Original and SVD features In this section, we review the features and two methods to apply SVD over the features.
We relied on the usual features used in previous WSD work, grouped in three main sets.
Local collocations comprise the bigrams and trigrams formed around the target word (using either lemmas, word-forms, or PoS tags) , those formed with the previous/posterior lemma/word-form in the sentence, and the content words in a ±4-word window around the target.
Syntactic dependencies use the object, subject, noun-modifier, preposition, and sibling lemmas, when available.
Finally, Bag-of-words features are the lemmas of the content words in the whole context, plus the salient bigrams in the context (Pedersen, 2001).
We refer to these features as original features.
Apart from the original space of features, we have used the so called SVD features, obtained from the projection of the feature vectors into the reduced space (Deerwester et al., 1990).
Basically, we set a term-by-document or feature-by-example matrix M from the corpus (see section below for more details).
SVD decomposes M into three matrices, M = UEVT . If the desired number of dimensions in the reduced space is p, we select p rows from E and V , yielding Ep and Vp respectively.
We can map any feature vector i� (which represents either a train or test example) into the p-dimensional space as follows: tp = �iT VpE�1 p . Those mapped vectors have p dimensions, and each of the dimensions is what we call a SVD feature.
We have explored two different variants in order to build the reduced matrix and obtain the SVD features, as follows.
Single Matrix for All target words (SVD-SMA).
The method comprises the following steps: (i) extract bag-of-word features (terms in this case) from unlabeled corpora, (ii) build the term-bydocument matrix, (iii) decompose it with SVD, and (iv) map the labeled data (train/test).
This technique is very similar to previous work on SVD (Gliozzo et al., 2005; Zelikovitz and Hirsh, 2001).
The dimensionality reduction is performed once, over the whole unlabeled corpus, and it is then applied to the labeled data of each word.
The reduced space is constructed only with terms, which correspond to bag-of-words features, and thus discards the rest of the features.
Given that the WSD literature shows that all features are necessary for optimal performance (Pradhan et al., 2007), we propose the following alternative to construct the matrix.
One Matrix per Target word (SVD-OMT).
For each word: (i) construct a corpus with its occurrences in the labeled and, if desired, unlabeled corpora, (ii) extract all features, (iii) build the feature-by-example matrix, (iv) decompose it with SVD, and (v) map all the labeled training and test data for the word.
Note that this variant performs one SVD process for each target word separately, hence its name.
When building the SVD-OMT matrices we can use only the training data (TRAIN) or both the train and unlabeled data (+UNLAB).
When building the SVD-SMA matrices, given the small size of the individual word matrices, we always use both the train and unlabeled data (+UNLAB).
Regarding the amount of data, based also on previous work, we used 50% of the available data for OMT, and the whole corpora for SMA.
An important parameter when doing SVD is the number of dimensions in the reduced space (p).
We tried two different values for p (25 and 200) in the BNC domain, and set a dimension for each classifier/matrix combination.
The motivation behind our method is that although the train and test feature vectors overlap sufficiently in the usual WSD task, the domain difference makes such overlap more scarce.
SVD implicitly finds correlations among features, as it maps related features into nearby regions in the reduced space.
In the case of SMA, SVD is applied over the joint term-by-document matrix of labeled (and possibly unlabeled corpora), and it thus can find correlations among closely related words (e.g. cat and dog).
These correlations can help reduce the gap among bag-of-words features from the source and target examples.
In the case of OMT, SVD over the joint feature-by-example matrix of labeled and unlabeled examples of a word allows to find correlations among features that show similar occurrence patterns in the source and target corpora for the target word.
<newSection> 5 Learning methods k-NN is a memory based learning method, where the neighbors are the k most similar labeled examples to the test example.
The similarity among instances is measured by the cosine of their vectors.
The test instance is labeled with the sense obtaining the maximum sum of the weighted vote of the k most similar contexts.
We set k to 5 based on previous results published in (Agirre and Lopez de Lacalle, 2007).
Regarding SVM, we used linear kernels, but also purpose-built kernels for the reduced spaces and the combinations (cf. Section 5.2).
We used the default soft margin (C=0).
In previous experiments we learnt that C is very dependent on the feature set and training data used.
As we will experiment with different features and training datasets, it did not make sense to optimize it across all settings.
We will now detail how we combined the original and SVD features in each of the machine learning methods.
Our k-NN combination method (Agirre et al., 2005; Agirre and Lopez de Lacalle, 2007) takes advantage of the properties of k-NN classifiers and exploit the fact that a classifier can be seen as k points (number of nearest neighbor) each casting one vote.
This makes easy to combine several classifiers, one for each feature space.
For instance, taking two k-NN classifiers of k = 5, C1 and C2, we can combine them into a single k = 10 classifier, where five votes come from C1 and five from C2.
This allows to smoothly combine classifiers from different feature spaces.
In this work we built three single k-NN classifiers trained on OMT, SMA and the original features, respectively.
In order to combine them we weight each vote by the inverse ratio of its position in the rank of the single classifier, (k − ri + 1)/k, where ri is the rank.
The basic idea of kernel methods is to find a suitable mapping function (0) in order to get a better generalization.
Instead of doing this mapping explicitly, kernels give the chance to do it inside the algorithm.
We will formalize it as follows.
First, we define the mapping function 0 : X → F.
Once the function is defined, we can use it in the kernel function in order to become an implicit function K(x, z) = h0(x) · 0(z)i, where h·i denotes a inner product between vectors in the feature space.
This way, we can very easily define mappings representing different information sources and use this mappings in several machine learning algorithm.
In our work we use SVM.
We defined three individual kernels (OMT, SMA and original features) and the combined kernel.
The original feature kernel (KOrig) is given by the identity function over the features 0 : X → X, defining the following kernel: where the denominator is used to normalize and avoid any kind of bias in the combination.
The OMT kernel (KOmt) and SMA kernel (KSma) are defined using OMT and SMA projection matrices, respectively (cf. Section 4.2).
Given the OMT function mapping 0omt : Rm → Rp, where m is the number of the original features and p the reduced dimensionality, then we define KOmt(xi, xj) as follows (KSma is defined similarly): Finally, we define the kernel combination: where n is the number of single kernels explained above, and l the index for the kernel type.
<newSection> 6 Domain adaptation experiments In this section we present the results in our two reference scenarios (source to target, target) and our reference scenario (domain adaptation).
Note that all methods presented here have full coverage, i.e. they return a sense for all test examples, and therefore precision equals recall, and suffices to compare among systems.
In this scenario our supervised WSD systems are trained on the general source corpus (BNC) and tested on the specific target domains separately (SPORTS and FINANCES).
We do not perform any kind of adaptation, and therefore the results are those expected for a generic WSD system when applied to domain-specific texts.
Table 1 shows the results for k-NN and SVM trained with the original features on the BNC.
In addition, we also show the results for the Most Frequent Sense baseline (MFS) taken from the BNC.
The second column denotes the accuracies obtained when testing on SPORTS, and the third column the accuracies for FINANCES.
The low accuracy obtained with MFS, e.g. 39.0 of precision in SPORTS, shows the difficulty of this task.
Both classifiers improve over MFS.
These classifiers are weak baselines for the domain adaptation system.
In this scenario we lay the harder baseline which the domain adaptation experiments should improve on (cf. next section).
The WSD systems are trained and tested on each of the target corpora (SPORTS and FINANCES) using 3-fold cross-validation.
train, +UNLAB denotes that we added unlabeled data related to the source corpus when computing SVD.
The rows denote the classifier and the feature spaces used, which are organized in four sections.
On the top rows we show the three baseline classifiers on the original features.
The two sections below show the results of those classifiers on the reduced dimensions, OMT and SMA (cf. Section 4.2).
Finally, the last rows show the results of the combination strategies (cf. Sections 5.1 and 5.2).
Note that some of the cells have no result, because that combination is not applicable (e.g. using the train and unlabeled data in the original space).
First of all note that the results for the baselines (MFS, SVM, k-NN) are much larger than those in Table 1, showing that this dataset is specially demanding for supervised WSD, and particularly difficult for domain adaptation experiments.
These results seem to indicate that the examples from the source general corpus could be of little use when tagging the target corpora.
Note specially the difference in MFS performance.
The priors of the senses are very different in the source and target corpora, which is a well-known shortcoming for supervised systems.
Note the high results of the baseline classifiers, which leave small room for improvement.
The results for the more sophisticated methods show that SVD and unlabeled data helps slightly, except for k-NN-OMT on SPORTS.
SMA decreases the performance compared to the classifiers trained on original features.
The best improvements come when the three strategies are combined in one, as both the kernel and k-NN combinations obtain improvements over the respective single classifiers.
Note that both the k-NN and SVM combinations perform similarly.
In the combination strategy we show that unlabeled data helps slightly, because instead of only combining OMT and original features we have the opportunity to introduce SMA.
Note that it was not our aim to improve the results of the basic classifiers on this scenario, but given the fact that we are going to apply all these techniques in the domain adaptation scenario, we need to show these results as baselines.
That is, in the next section we will try to obtain results which improve significantly over the best results in this section.
In this last scenario we try to show that our WSD system trained on both source (BNC) and target (SPORTS and FINANCES) data performs better than the one trained on the target data alone.
We also use 3-fold cross-validation for the target data, but the entire source data is used in each turn.
The unlabeled data here refers to the combination of unlabeled source and target data.
The results are presented in table 3.
Again, the columns denote if unlabeled data has been used in the learning process.
The rows correspond to classifiers and the feature spaces involved.
The first rows report the best results in the previous scenarios: BNC → X for the source to target scenario, and X → X for the target scenario.
The rest of the table corresponds to the domain adaptation scenario.
The rows below correspond to MFS and the baseline classifiers, followed by the OMT and SMA results, and the combination results.
The last row shows the results for the feature augmentation algorithm (Daum´e III, 2007).
Focusing on the results, the table shows that MFS decreases with respect to the target scenario (cf. Table 2) when the source data is added, probably caused by the different sense distributions in BNC and the target corpora.
The baseline classifiers (k-NN and SVM) are not able to improve over the baseline classifiers on the target data alone, which is coherent with past research, and shows that straightforward domain adaptation does not work.
The following rows show that our reduction methods on themselves (OMT, SMA used by k-NN and SVM) also fail to perform better than in the target scenario, but the combinations using unlabeled data (k-NN-COMB and specially SVM-COMB) do manage to improve the best results for the target scenario, showing that we were able to attain domain adaptation.
The feature augmentation approach (SVM-AUG) does improve slightly over SVM in the target scenario, but not over the best results in the target scenario, showing the difficulty of domain adaptation for WSD, at least on this dataset.
<newSection> 7 Discussion and analysis Table 4 summarizes the most important results.
The kernel combination method with unlabeled data on the adaptation scenario reduces the error on 22.1% and 17.6% over the baseline SVM on the target scenario (SPORTS and FINANCES respectively), and 12.7% and 9.0% over the k-NN combination method on the target scenario.
These gains are remarkable given the already high baseline, specially taking into consideration that the 41 nouns are closely related to the domains.
The differences, including SVM-AUG, are statistically significant according to the Wilcoxon test with p < 0.01.
In addition, we carried extra experiments to examine the learning curves, and to check, given the source examples, how many additional examples from the target corpus are needed to obtain the same results as in the target scenario using all available examples.
We fixed the source data and used increasing amounts of target data.
We show the original SVM on the target scenario, and SVM-COMB (+UNLAB) and SVM-AUG as the domain adaptation approaches.
The results are shown in figure 1 for SPORTS and figure 2 for FINANCES.
The horizontal line corresponds to the performance of SVM on the target domain.
The point where the learning curves cross the horizontal line show that our domain adaptation method needs only around 40% of the target data in order to get the same performance as the baseline SVM on the target data.
The learning curves also shows that the domain adaptation kernel combination approach, no matter the amount of target data, is always above the rest of the classifiers, showing the robustness of our approach.
<newSection> 8 Conclusion and future work In this paper we explore supervised domain adaptation for WSD with positive results, that is, whether hand-labeling general domain (source) text is worth the effort when training WSD systems that are to be applied to specific domains (targets).
We performed several experiments in three scenarios.
In the first scenario (source to target scenario), the classifiers were trained on source domain data (the BNC) and tested on the target domains, composed by the SPORTS and FINANCES sections of Reuters.
In the second scenario (tar-get scenario) we set the main baseline for our domain adaptation experiment, training and testing our classifiers on the target domain data.
In the last scenario (domain adaptation scenario), we combine both source and target data for training, and test on the target data.
We report results in each scenario for k-NN and SVM classifiers, for reduced features obtained using SVD over the training data, for the use of unlabeled data, and for k-NN and SVM combinations of all.
Our results show that our best domain adaptation strategy (using kernel combination of SVD features and unlabeled data related to the training data) yields statistically significant improvements: up to 22% error reduction compared to SVM on the target domain data alone.
We also show that our domain adaptation method only needs 40% of the target data (in addition to the source data) in order to get the same results as SVM on the target alone.
We obtain coherent results in two target scenarios, and consistent improvement at all levels of the learning curves, showing the robustness or our findings.
We think that our dataset, which comprises examples for 41 nouns that are closely related to the target domains, is specially demanding, as one would expect the performance of a generic WSD system to drop when moving to the domain corpus, specially on domain-related words, while we could expect the performance to be similar for generic or unrelated words.
In the future we would like to evaluate our method on other datasets (e.g. DSO or OntoNotes), to test whether the positive results are confirmed.
We would also like to study word-byword behaviour, in order to assess whether target examples are really necessary for words which are less related to the domain.
<newSection> Acknowledgments This work has been partially funded by the EU Commission (project KYOTO ICT-2007-211423) and Spanish Research Department (project KNOW TIN2006-15049-C03-01).
Oier Lopez de Lacalle has a PhD grant from the Basque Government.
<newSection> References<newSection> Abstract Named entity recognition for morphologically rich, case-insensitive languages, including the majority of semitic languages, Iranian languages, and Indian languages, is inherently more difficult than its English counterpart.
Worse still, progress on machine learning approaches to named entity recognition for many of these languages is currently hampered by the scarcity of annotated data and the lack of an accurate part-of-speech tagger.
While it is possible to rely on manually-constructed gazetteers to combat data scarcity, this gazetteer-centric approach has the potential weakness of creating irreproducible results, since these name lists are not publicly available in general.
Motivated in part by this concern, we present a learning-based named entity recognizer that does not rely on manually-constructed gazetteers, using Bengali as our representative resource-scarce, morphologically-rich language.
Our recognizer achieves a relative improvement of 7.5% in F-measure over a baseline recognizer.
Improvements arise from (1) using induced affixes, (2) extracting information from online lexical databases, and (3) jointly modeling part-of-speech tagging and named entity recognition.
<newSection> 1 Introduction While research in natural language processing has gained a lot of momentum in the past several decades, much of this research effort has been focusing on only a handful of politically-important languages such as English, Chinese, and Arabic.
On the other hand, being the fifth most spoken language1 with more than 200 million native speakers residing mostly in Bangladesh and the Indian state of West Bengal, Bengali has far less electronic resources than the aforementioned languages.
In fact, a major obstacle to the automatic processing of Bengali is the scarcity of annotated corpora.
One potential solution to the problem of data scarcity is to hand-annotate a small amount of data with the desired linguistic information and then develop bootstrapping algorithms for combining this small amount of labeled data with a large amount of unlabeled data.
In fact, co-training (Blum and Mitchell, 1998) has been successfully applied to English named entity recognition (NER) (Collins & Singer [henceforth C&S] (1999)).
In C&S’s approach, consecutive words tagged as proper nouns are first identified as potential NEs, and each such NE is then labeled by combining the outputs of two co-trained classifiers.
Unfortunately, there are practical difficulties in applying this technique to Bengali NER.
First, one of C&S’s co-trained classifiers uses features based on capitalization, but Bengali is case-insensitive.
Second, C&S identify potential NEs based on proper nouns, but unlike English, (1) proper noun identification for Bengali is non-trivial, due to the lack of capitalization; and (2) there does not exist an accurate Bengali part-of-speech (POS) tagger for providing such information, owing to the scarcity of annotated data for training the tagger.
In other words, Bengali NER is complicated not only by the scarcity of annotated data, but also by the lack of an accurate POS tagger.
One could imagine building a Bengali POS tagger using unsupervised induction techniques that have been successfully developed for English (e.g., Sch¨utze (1995), Clark (2003)), including the recently-proposed prototype-driven approach (Haghighi and Klein, 2006) and Bayesian approach (Gold-water and Griffiths, 2007).
The majority of these approaches operate by clustering distributionally similar words, but they are unlikely to work well for Bengali for two reasons.
First, Bengali is a relatively free word order language, and hence the distributional information collected for Bengali words may not be as reliable as that for English words.
Second, many closed-class words that typically appear in the distributional representation of an English word (e.g., prepositions and particles such as “in” and “to”) are realized as inflections in Bengali, and the absence of these informative words implies that the context vector may no longer capture sufficient information for accurately clustering the Bengali words.
In view of the above problems, many learning-based Bengali NE recognizers have relied heavily on manually-constructed name lists for identifying persons, organizations, and locations.
There are at least two weaknesses associated with this gazetteer-centric approach.
First, these name lists are typically not publicly available, making it difficult to reproduce the results of these NE recog-nizers.
Second, it is not clear how comprehen-sive these lists are.
Relying on comprehensive lists that comprise a large portion of the names in the test set essentially reduces the NER problem to a dictionary-lookup problem, which is arguably not very interesting from a research perspective.
In addition, many existing learning-based Bengali NE recognizers have several common weaknesses.
First, they use as features pseudo-affixes, which are created by extracting the first n and the last n characters of a word (where 1 < n < 4) (e.g., Dandapat et al. (2007)).
While affixes encode essential grammatical information in Bengali due to its morphological richness, this extraction method is arguably too ad-hoc and does not cover many useful affixes.
Second, they typically adopt a pipelined NER architecture, performing POS tagging prior to NER and encoding the resulting not-so-accurate POS information as a feature.
In other words, errors in POS tagging are propagated to the NE recognizer via the POS feature, thus limiting its performance.
Motivated in part by these weaknesses, we investigate how to improve a learning-based NE recognizer that does not rely on manually-constructed gazetteers.
Specifically, we investigate two learning architectures for our NER system.
The first one is the aforementioned pipelined architecture in which the NE recognizer uses as features the output of a POS tagger that is trained independently of the recognizer.
Unlike existing Bengali POS and NE taggers, however, we examine two new knowledge sources for training these taggers: (1) affixes induced from an unannotated corpus and (2) semantic class information extracted from Wikipedia.
In the second architecture, we jointly learn the POS tagging and the NER tasks, allowing features for one task to be accessible to the other task during learning.
The goal is to examine whether any benefits can be obtained via joint modeling, which could address the error propagation problem with the pipelined architecture.
While we focus on Bengali NER in this paper, none of the proposed techniques are language-specific.
In fact, we believe that these techniques are of relevance and interest to the EACL community because they can be equally applicable to the numerous resource-scarce European and Middle Eastern languages that share similar linguistic and extra-linguistic properties as Bengali.
For instance, the majority of semitic languages and Iranian languages are, like Bengali, morphologically productive; and many East European languages such as Czech and Polish resemble Bengali in terms of not only their morphological richness, but also their relatively free word order.
The rest of the paper is organized as follows.
In Section 2, we briefly describe the related work.
Sections 3 and 4 show how we induce affixes from an unannotated corpus and extract semantic class information from Wikipedia.
In Sections 5 and 6, we train and evaluate a POS tagger and an NE recognizer independently, augmenting the feature set typically used for these two tasks with our new knowledge sources.
Finally, we describe and evaluate our joint model in Section 7.
<newSection> 2 Related Work Cucerzan and Yarowsky (1999) exploit morphological and contextual patterns to propose a language-independent solution to NER.
They use affixes based on the paradigm that named entities corresponding to a particular class have similar morphological structure.
Their bootstrapping approach is tested on Romanian, English, Greek, Turkish, and Hindi.
The recall for Hindi is the lowest (27.84%) among the five languages, suggesting that the lack of case information can significantly complicate the NER task.
To investigate the role of gazetteers in NER, Mikheev et al.
(1999) combine grammar rules with maximum entropy models and vary the gazetteer size.
Experimental results show that (1) the F-scores for NE classes like person and organization are still high without gazetteers, ranging from 85% to 92%; and (2) a small list of country names can improve the low F-score for locations substantially.
It is worth noting that their recognizer requires that the input data contain POS tags and simple semantic tags, whereas ours automatically acquires such linguistic information.
In addition, their approach uses part of the dataset to extend the gazetteer.
Therefore, the resulting gazetteer list is specific to a particular domain; on the other hand, our approach does not generate a domain-specific list, since it makes use of Wikipedia articles.
Kozareva (2006) generates gazetteer lists for person and location names from unlabeled data using common patterns and a graph exploration algorithm.
The location pattern is essentially a preposition followed by capitalized context words.
However, this approach is inadequate for a morphologically-rich language like Bengali, since prepositions are often realized as inflections.
<newSection> 3 Affix Induction Since Bengali is morphologically productive, a lot of grammatical information about Bengali words is expressed via affixes.
Hence, these affixes could serve as useful features for training POS and NE taggers.
In this section, we show how to induce affixes from an unannotated corpus.
We rely on a simple idea proposed by Keshava and Pitler (2006) for inducing affixes.
Assume that (1) V is a vocabulary (i.e., a set of distinct words) extracted from a large, unannotated corpus, (2) α and Q are two character sequences, and (3) αQ is the concatenation of α and Q. If αQ and α are found in V , we extract Q as a suffix.
Similarly, if αQ and Q are found in V , we extract α as a prefix.
In principle, we can use all of the induced affixes as features for training a POS tagger and an NE recognizer.
However, we choose to use only those features that survive our feature selection process (to be described below), for the following reasons.
First, the number of induced affixes is large, and using only a subset of them as features could make the training process more efficient.
Second, the above affix induction method is arguably overly simplistic and hence many of the induced affixes could be spurious.
Our feature selection process is fairly simple: we (1) score each affix by multiplying its frequency (i.e., the number of distinct words in V to which each affix attaches) and its length2, and (2) select only those whose score is above a certain threshold.
In our experiments, we set this threshold to 50, and generate our vocabulary of 140K words from five years of articles taken from the Bengali newspaper Prothom Alo.
This enables us to induce 979 prefixes and 975 suffixes.
<newSection> 4 Semantic Class Induction from Wikipedia Wikipedia has recently been used as a knowledge source for various language processing tasks, including taxonomy construction (Ponzetto and Strube, 2007a), coreference resolution (Ponzetto and Strube, 2007b), and English NER (e.g., Bunescu and Pas¸ca (2006), Cucerzan (2007), Kazama and Torisawa (2007), Watanabe et al. (2007)).
Unlike previous work on using Wikipedia for NER, our goal here is to (1) generate a list of phrases and tokens that are potentially named entities from the 16914 articles in the Bengali Wikipedia3 and (2) heuristically annotate each of them with one of four classes, namely, PER (per-son), ORG (organization), LOC (location), or OTHERS (i.e., anything other than PER, ORG and LOC).
We employ the steps below to generate our annotated list.
Generating and annotating the titles Recall that each Wikipedia article has been optionally assigned to one or more categories by its creator and/or editors.
We use these categories to help annotate the title of an article.
Specifically, if an article has a category whose name starts with “Born on” or “Death on,” we label the corresponding title with PER.
Similarly, if it has a category whose name starts with “Cities of” or “Countries of,” we label the title as LOC.
If an article does not belong to one of the four categories above, we label its title with the help of a small set of seed key-words shown in Table 1.
Specifically, for each of the three NE classes shown on the left of Table 1, we compute a weighted sum of its keywords: a keyword that appears in the first paragraph has a weight of 3, a keyword that appears elsewhere in the article has a weight of 1, and a keyword that does not appear in the article has a weight of 0.
The rationale behind using different weights is simple: the first paragraph is typically a brief exposition of the title, so it should in principle contain words that correlate more closely with the title than words appearing in the rest of the article.
We then label the title with the class that has the largest weighted sum.
Note, however, that we ignore any article that contains fewer than two key-words, since we do not have reliable evidence for labeling its title as one of the NE classes.
We put all these annotated titles into a title list.
Getting more location names To get more location names, we search for the character sequences “birth place:” and “death place:” in each article, extracting the phrase following any of these sequences and label it as LOC.
We put all such labeled locations into the title list.
Generating and annotating the tokens in the titles Next, we extract the word tokens from each title in the title list and label each token with an NE class.
The reason for doing this is to improve generalization: if “Dhaka University” is labeled as ORG in the title list, then it is desirable to also label the token “University” as ORG, because this could help identify an unseen phrase that contains the term “University” as an organization.
Our token labeling method is fairly simple.
First, we generate the tokens from each title in the title list, assigning to each token the same NE label as that of the title from which it is generated.
For instance, from the title “Anna Frank,” “Anna” will be labeled as PER; and from “Anna University,” “ Anna” will be labeled as LOC.
To resolve such ambiguities (i.e., assigning different labels to the same token), we keep a count of how many times “Anna” is labeled with each NE class, and set its final label to be the most frequent NE class.
We put all these annotated tokens into a token list.
If the title list and the token list have an element in common, we remove the element from the token list, since we have a higher confidence in the labels of the titles.
Merging the lists Finally, we append the token list to the title list.
The resulting title list contains 4885 PERs, 15176 LOCs, and 188 ORGs.
We can now use the title list to annotate a text.
Specifically, we process each word w in the text in a left-to-right manner, using the following steps: These automatic annotations will then be used to derive a set of WIKI features for training our POS tagger and NE recognizer.
Hence, unlike existing Bengali NE recognizers, our “gazetteers” are induced rather than manually created.
<newSection> 5 Part-of-Speech Tagging In this section, we will show how we train and evaluate our POS tagger.
As mentioned before, we hypothesize that introducing our two knowledge sources into the feature set for the tagger could improve its performance: using the induced affixes could improve the extraction of grammatical information from the words, and using the Wikipedia-induced list, which in principle should comprise mostly of names, could help improve the identification of proper nouns.
Corpus Our corpus is composed of 77942 words and is annotated with one of 26 POS tags in the tagset defined by IIIT Hyderabad7.
Using this corpus, we perform 5-fold cross-validation (CV) experiments in our evaluation.
It is worth noting that this dataset has a high unknown word rate of 15% (averaged over the five folds), which is due to the small size of the dataset.
While this rate is comparable to another Bengali POS dataset described in Dandapat et al.
(2007), it is much higher than the 2.6% unknown word rate in the test set for Ratna-parkhi’s (1996) English POS tagging experiments.
Creating training instances Following previous work on POS tagging, we create one training instance for each word in the training set.
The class value of an instance is the POS tag of the corresponding word.
Each instance is represented by a set of linguistic features, as described next.
Features Our feature set consists of (1) baseline features motivated by those used in Danda-pat et al.’s (2007) Bengali POS tagger and Singh et al.’s (2006) Hindi POS tagger, as well as (2) features derived from our induced affixes and the Wikipedia-induced list.
More specifically, the baseline feature set has (1) word unigrams, bigrams and trigrams; (2) pseudo-affix features that are created by taking the first three characters and the last three characters of the current word; and (3) a binary feature that determines whether the current word is a number.
As far as our new features are concerned, we create one induced prefix feature and one induced suffix feature from both the current word and the previous word, as well as two bigrams involving induced prefixes and induced suffixes.
We also create three WIKI features, including the Wikipedia-induced NE tag of the current word and that of the previous word, as well as the combination of these two tags.
Note that the Wikipedia-induced tag of a word can be obtained by annotating the test sentence under consideration using the list generated from the Bengali Wikipedia (see Section 4).
To make the description of these features more concrete, we show the feature templates in Table 2.
Learning algorithm We used CRF++8, a C++ implementation of conditional random fields (Laf-ferty et al., 2001), as our learning algorithm for training a POS tagging model.
Evaluating the model To evaluate the resulting POS tagger, we generate test instances in the same way as the training instances.
5-fold CV results of the POS tagger are shown in Table 3.
Each row consists of three numbers: the overall accuracy, as well as the accuracies on the seen and the unseen words.
Row 1 shows the accuracy when the baseline feature set is used; row 2 shows the accuracy when the baseline feature set is augmented with our two induced affix features; and the last row shows the results when both the induced affix and the WIKI features are incorporated into the baseline feature set.
Perhaps not surprisingly, (1) adding more features improves performance, and (2) accuracies on the seen words are substantially better than those on the unseen words.
In fact, adding the induced affixes to the baseline feature set yields a 7.8% reduction in relative error in overall accuracy.
We also applied a two-tailed paired t-test (p < 0.01), first to the overall accuracies in rows 1 and 2, and then to the overall accuracies in rows 2 and 3.
Both pairs of numbers are statistically significantly different from each other, meaning that incorporating the two induced affix features and then the WIKI features both yields significant improvements.
Error analysis To better understand the results, we examined the errors made by the tagger.
The most frequent errors are shown in Table 4.
From the table, we see that the largest source of errors arises from mislabeling proper nouns as common nouns.
This should be expected, as proper noun identification is difficult due to the lack of capital-ization information.
Unfortunately, failure to identify proper nouns could severely limit the recall of an NE recognizer.
Also, adjectives and common nouns are difficult to distinguish, since these two syntactic categories are morphologically and dis-tributionally similar to each other.
Finally, many errors appear to involve mislabeling a word as a common noun.
The reason is that there is a larger percentage of common nouns (almost 30%) in the training set than other POS tags, thus causing the model to prefer tagging a word as a common noun.
<newSection> 6 Named Entity Recognition In this section, we show how to train and evaluate our NE recognizer.
The recognizer adopts a traditional architecture, assuming that POS tagging is performed prior to NER.
In other words, the NE recognizer will use the POS acquired in Section 5 as one of its features.
As in Section 5, we will focus on examining how our knowledge sources (the induced affixes and the WIKI features) impact the performance of our recognizer.
Corpus The corpus we used for NER evaluation is the same as the one described in the previous section.
Specifically, in addition to POS information, each sentence in the corpus is annotated with NE information.
We focus on recognizing the three major NE types in this paper, namely persons (PER), organizations (ORG), and locations (LOC).
There are 1721 PERs, 104 ORGs, and 686 LOCs in the corpus.
As far as evaluation is concerned, we conduct 5-fold CV experiments, dividing the corpus into the same five folds as in POS tagging.
Creating training instances We view NE recognition as a sequence labeling problem.
In other words, we combine NE identification and classification into one step, labeling each word in a test text with its NE tag.
Any word that does not belong to one of our three NE tags will be labeled as OTHERS.
We adopt the IOB convention, preceding an NE tag with a B if the word is the first word of an NE and an I otherwise.
Now, to train the NE recognizer, we create one training instance from each word in a training text.
The class value of an instance is the NE tag of the corresponding word, or OTHERS if the word is not part of an NE.
Each instance is represented by a set of linguistic features, as described next.
Features Our feature set consists of (1) baseline features motivated by those used in Ekbal et al.’s (2008) Bengali NE recognizer, as well as (2) features derived from our induced affixes and the Wikipedia-induced list.
More specifically, the baseline feature set has (1) word unigrams; (2) pseudo-affix features that are created by taking the first three characters and the last three characters of the current word; (3) a binary feature that determines whether the current word is the first word of a sentence; and (4) a set of POS-related features, including the POS of the current word and its surrounding words, as well as POS bigrams formed from the current and surrounding words.
Our induced affixes and WIKI features are incorporated into the baseline NE feature set in the same manner as in POS tagging.
In essence, the feature templates employed by the NE recognizer are the top 12 templates in Table 2 and those in Table 5.
Learning algorithm We again use CRF++ as our sequence learner for acquiring the recognizer.
Evaluating the model To evaluate the resulting NE tagger, we generate test instances in the same way as the training instances.
To score the output of the recognizer, we use the CoNLL-2000 scoring program9, which reports performance in terms of recall (R), precision (P), and F-measure (F).
All NE results shown in Table 6 are averages of the 5-fold CV experiments.
The first block of the Table 6 shows the overall results when the baseline feature set is used; in addition, we also show results for each of the three NE tags.
As we can see, the baseline achieves an F-measure of 67.05.
The second block shows the results when the baseline feature set is augmented with our two induced affix features.
Somewhat unexpectedly, F-measure drops by 0.8% in comparison to the baseline.
Additional experiments are needed to determine the reason.
Finally, when the WIKI features are incorporated into the augmented feature set, the system achieves an F-measure of 68.70 (see the third block), representing a statistically significant increase of 1.6% in F-measure over the baseline.
As we can see, improvements stem primarily from dramatic gains in recall for locations.
Discussions Several points deserve mentioning.
First, the model performs poorly on the ORGs, owing to the small number of organization names in the corpus.
Worse still, the recall drops after adding the WIKI features.
We examined the list of induced ORG names and found that it is fairly noisy.
This can be attributed in part to the difficulty in forming a set of seed words that can extract ORGs with high precision (e.g., the ORG seed “situate” extracted many LOCs).
Second, using the WIKI features does not help recalling the PERs.
A closer examination of the corpus reveals the reason: many sentences describe fictitious characters, whereas Wikipedia would be most useful for articles that describe famous people.
Overall, while the WIKI features provide our recognizer with a small, but significant, improvement, the usefulness of the Bengali Wikipedia is currently limited by its small size.
Nevertheless, we believe the Bengali Wikipedia will become a useful resource for language processing as its size increases.
<newSection> 7 A Joint Model for POS Tagging and NER The NE recognizer described thus far has adopted a pipelined architecture, and hence its performance could be limited by the errors of the POS tagger.
In fact, as discussed before, the major source of errors made by our POS tagger concerns the confusion between proper nouns and common nouns, and this type of error, when propagated to the NE recognizer, could severely limit its recall.
Also, there is strong empirical support for this argument: the NE recognizers, when given access to the correct POS tags, have F-scores ranging from 76-79%, which are 10% higher on average than those with POS tags that were automatically computed.
Consequently, we hypothesize that modeling POS tagging and NER jointly would yield better performance than learning the two tasks separately.
In fact, many approaches have been developed to jointly model POS tagging and noun phrase chunking, including transformation-based learning (Ngai and Florian, 2001), factorial HMMs (Duh, 2005), and dynamic CRFs (Sutton et al., 2007).
Some of these approaches are fairly sophisticated and also require intensive computations during inference.
For instance, when jointly modeling POS tagging and chunking, Sutton et al.
(2007) reduce the number of POS tags from 45 to 5 when training a factorial dynamic CRF on a small dataset (with only 209 sentences) in order to reduce training and inference time.
In contrast, we propose a relatively simple model for jointly learning Bengali POS tagging and NER, by exploiting the limited dependencies between the two tasks.
Specifically, we make the observation that most of the Bengali words that are part of an NE are also proper nouns.
In fact, based on statistics collected from our evaluation corpus (see Sections 5 and 6), this observation is correct 97.3% of the time.
Note, however, that this observation does not hold for English, since many prepositions and determiners are part of an NE.
On the other hand, this observation largely holds for Bengali because prepositions and determiners are typically realized as noun suffixes.
This limited dependency between the POS tags and the NE tags allows us to develop a simple model for jointly learning the two tasks.
More specifically, we will use CRF++ to learn the joint model.
Training and test instances are generated as described in the previous two subsections (i.e., one instance per word).
The feature set will consist of the union of the features that were used to train the POS tagger and the NE tagger independently, minus the POS-related features that were used in the NE tagger.
The class value of an instance is computed as follows.
If a word is not a proper noun, its class is simply its POS tag.
Otherwise, its class is its NE tag, which can be PER, ORG, LOC, or OTHERS.
In other words, our joint model exploits the observation that we made earlier in the section by assuming that only proper nouns can be part of a named entity.
This allows us to train a joint model without substantially increasing the number of classes.
We again evaluate our joint model using 5-fold CV experiments.
The NE results of the model are shown in Table 7.
The rows here can be interpreted in the same manner as those in Table 6.
Comparing these three experiments with their counterparts in Table 6, we can see that, except for the baseline, jointly modeling offers a significant improvement of 3.3% in overall F-measure.10 In particular, the joint model benefits significantly from our 10The POS tagging results are not shown due to space limitations.
Overall, the POS accuracies drop insignificantly as a result of joint modeling, for the following reason.
Recall from Section 5 that the major source of POS tagging errors arises from the mislabeling of many proper nouns as common nouns, due primarily to the large number of common nouns in the corpus.
The joint model aggravates this problem by subcategorizing the proper nouns into different NE classes, causing the tagger to have an even stronger bias towards labeling a proper noun as a common noun than before.
Nevertheless, as seen from the results in Tables 6 and 7, such a bias has yielded an increase in NER precision.
two knowledge sources, achieving an F-measure of 71.99% when both of them are incorporated.
Finally, to better understand the value of the induced affix features in the joint model as well as the pipelined model described in Section 6, we conducted an ablation experiment, in which we incorporated only the WIKI features into the baseline feature set.
With pipelined modeling, the F-measure for NER is 68.87%, which is similar to the case where both induced affixes and the WIKI features are used.
With joint modeling, however, the F-measure for NER is 70.87%, which is 1% lower than the best joint modeling score.
These results provide suggestive evidence that the induced affix features play a significant role in the improved performance of the joint model.
<newSection> 8 Conclusions We have explored two types of linguistic features, namely the induced affix features and the Wikipedia-related features, to improve a Bengali POS tagger and NE recognizer.
Our experimental results have demonstrated that (1) both types of features significantly improve a baseline POS tagger and (2) the Wikipedia-related features significantly improve a baseline NE recognizer.
Moreover, by exploiting the limited dependencies between Bengali POS tags and NE tags, we proposed a new model for jointly learning the two tasks, which not only avoids the error-propagation problem present in the pipelined system architecture, but also yields statistically significant improvements over the NE recognizer that is trained independently of the POS tagger.
When applied in combination, our three extensions contributed to a relative improvement of 7.5% in F-measure over the baseline NE recognizer.
Most importantly, we believe that these extensions are of relevance and interest to the EACL community because many European and Middle Eastern languages resemble Bengali in terms of not only their morphological richness but also their scarcity of annotated corpora.
We plan to empirically verify our belief in future work.
<newSection> Acknowledgments We thank the three anonymous reviewers for their invaluable comments on the paper.
We also thank CRBLP, BRAC University, Bangladesh, for providing us with Bengali resources.
This work was supported in part by NSF Grant IIS-0812261.
<newSection> References<newSection> Abstract This paper describes a method using morphological rules and heuristics, for the automatic extraction of large-coverage lexicons of stems and root word-forms from a raw text corpus.
We cast the problem of high-coverage lexicon extraction as one of stemming followed by root word-form selection.
We examine the use of POS tagging to improve precision and recall of stemming and thereby the coverage of the lexicon.
We present accuracy, precision and recall scores for the system on a Hindi corpus.
<newSection> 1 Introduction Large-coverage morphological lexicons are an essential component of morphological analysers.
Morphological analysers find application in language processing systems for tasks like tagging, parsing and machine translation.
While raw text is an abundant and easily accessible linguistic resource, high-coverage morphological lexicons are scarce or unavailable in Hindi as in many other languages (Cl´ement et al., 2004).
Thus, the development of better algorithms for the extraction of morphological lexicons from raw text corpora is a task of considerable importance.
A root word-form lexicon is an intermediate stage in the creation of a morphological lexicon.
In this paper, we consider the problem of extracting a large-coverage root word-form lexicon for the Hindi language, a highly inflectional and moderately agglutinative Indo-European language spoken widely in South Asia.
Since a POS tagger, another basic tool, was available along with POS tagged data to train it, and since the error patterns indicated that POS tagging could greatly improve the accuracy of the lexicon, we used the POS tagger in our experiments on lexicon extraction.
Previous work in morphological lexicon extraction from a raw corpus often does not achieve very high precision and recall (de Lima, 1998; Oliver and Tadi´c, 2004).
In some previous work the process of lexicon extraction involves incremental or post-construction manual validation of the entire lexicon (Cl´ement et al., 2004; Sagot, 2005; Fors-berg et al., 2006; Sagot et al., 2006; Sagot, 2007).
Our method attempts to improve on and extend the previous work by increasing the precision and recall of the system to such a point that manual validation might even be rendered unnecessary.
Yet another difference, to our knowledge, is that in our method we cast the problem of lexicon extraction as two subproblems: that of stemming and following it, that of root word-form selection.
The input resources for our system are as follows: a) raw text corpus, b) morphological rules, c) POS tagger and d) word-segmentation labelled data.
We output a stem lexicon and a root word-form lexicon.
We take as input a raw text corpus and a set of morphological rules.
We first run a stemming algorithm that uses the morphological rules and some heuristics to obtain a stem dictionary.
We then create a root dictionary from the stem dictionary.
The last two input resources are optional but when a POS tagger is utilized, the F-score (harmonic mean of precision and recall) of the root lexicon can be as high as 94.6%.
In the rest of the paper, we provide a brief overview of the morphological features of the Hindi language, followed by a description of our method including the specification of rules, the corpora and the heuristics for stemming and root word-form selection.
We then evaluate the system with and without the POS tagger.
<newSection> 2 Hindi Orthography and Morphology There are some features peculiar to Hindi orthography and to the character encoding system that we use.
These need to be compensated for in the system.
It was also found that Hindi’s inflectional morphology has certain characteristics that simplify the word segmentation rules.
Hindi is written in the partially-phonemic Devana-gari script.
Most consonant clusters that occur in the language are represented by characters and ligatures, while a very few are represented as diacrit-ics.
Vowels that follow consonants or consonant clusters are marked with diacritics.
However, each consonant in the Devanagari script also carries an implicit vowel a1 unless its absence is marked by a special diacritic “halant”.
Vowels are represented by vowel characters when they occur at the head of a word or after another vowel.
The y sound sometimes does not surface in the pronunciation when it occurs between two vowels.
So suffixes where the y is followed by e or I can be written in two ways, with or without the y sound in them.
For instance the suffix ie can also be written as iye.
Certain stemming rules will therefore need to be duplicated in order to accommodate the different spelling possibilities and the different vowel representations in Hindi.
The character encoding also plays a small but significant role in the ease of stemming of Hindi word-forms.
We used Unicode to encode Hindi characters.
The Unicode representation of Devanagari treats simple consonants and vowels as separate units and so makes it easier to match substrings at consonant-vowel boundaries.
Ligatures and diacritical forms of consonants are therefore represented by the same character code and they can be equated very simply.
However, when using Unicode as the character encoding, it must be borne in mind that there are different character codes for the vowel diacrit-ics and for the vowel characters for one and the same vowel sound, and that the long and short forms of the vowels are represented by different codes.
These artifacts of the character encoding need to be compensated for when using substring matches to identify the short vowel sound as being part of the corresponding prolonged vowel sound and when stemming.
The inflectional morphology of Hindi does not permit agglutination.
This helps keep the number of inflectional morphological rules manageable.
However, the derivational suffixes are agglu-tinative, leading to an explosion in the number of root word-forms in the inflectional root lexicon.
The example in Table 1 shows that verbs can take one of the two causative suffixes A and vA.
These being derivational suffixes are not stemmed in our system and cause the verb lexicon to be larger than it would have otherwise.
Nouns, verbs and adjectives are the main POS categories that undergo inflection in Hindi according to regular paradigm rules.
For example, Hindi nouns inflect for case and number.
The inflections for the paradigms that the words laDkA (meaning boy) and laDkI (mean-ing girl) belong to are shown in Table 2.
The root word-forms are laDkA and laDkI respectively (the singular and nominative forms).
Hindi verbs are inflected by gender, number, person, mood and tense.
Hindi adjectives take inflections for gender and case.
The number of inflected forms in different POS categories varies considerably, with verbs tending to have a lot more inflections than other POS categories.
<newSection> 3 System Description In order to construct a morphological lexicon, we used a rule-based approach combined with heuristics for stem and root selection.
When used in concert with a POS tagger, they could extract a very accurate morphological lexicon from a raw text corpus.
Our system therefore consists of the following components: The components of the system are described in more detail below.
Rules alone are not always sufficient to identify the best stem or root for a word-form, when the words being stemmed have very few inflectional forms or when a word might be stemmed in one of many ways.
In that case, a raw text corpus can provide important clues for identifying them.
The raw text corpus that we use is the Web-Duniya corpus which consists of 1.4 million sentences of newswire and 21.8 million words.
The corpus, being newswire, is clearly not balanced.
It has a preponderance of third-person forms whereas first and second person inflectional forms are under-represented.
Since Hindi word boundaries are clearly marked with punctuation and spaces, tokenization was an easy task.
The raw text corpus yielded approximately 331000 unique word-forms.
When words beginning with numbers were removed, we were left with about 316000 unique word-forms of which almost half occurred only once in the corpus.
In addition, we needed a corpus of 45,000 words labelled with POS categories using the IL-POST tagset (Sankaran et al., 2008) for the POS tagger.
The morphological rules input into the system are used to recognize word-forms that together belong to a paradigm.
Paradigms can be treated as a set of suffixes that can be used to generate inflectional word-forms from a stem.
The set of suffixes that constitutes a paradigm defines an equivalence class on the set of unique word-forms in the corpus.
For example, the laDkA paradigm in Table 2 would be represented by the set of suffix strings {‘A’, ‘e’, ‘on’} derived from the word-forms laDkA, laDke and laDkon.
A few paradigms are listed in Table 3.
The suffix set formalism of a paradigm closely resembles the one used in a previous attempt at unsupervised paradigm extraction (Zeman, 2007) but differs from it in that Zeman (2007) considers the set of word-forms that match the paradigm to be a part of the paradigm definition.
In our system, we represent the morphological rules by a list of suffix add-delete rules.
Each rule in our method is a five-tuple {α, Q, -y, S, E} where: The sample paradigm rules shown in Table 4 would match the words laDkA, laDkon, laDke and dhoyogI respectively and cause them to be stemmed and assigned roots as shown in Table 5.
The rules by themselves can identify word-andparadigm entries from the raw text corpus if a sufficient number of inflectional forms were present.
For instance, if the words laDkA and laDkon were present in the corpus, by taking the intersection of the paradigms associated with the matching rules in Table 4, it would be possible to infer that the root word-form was laDkA and that the paradigm was N1.
We needed to create about 300 rules for Hindi.
The rules could be stored in a list indexed by the suffix in the case of Hindi because the number of possible suffixes was small.
For highly aggluti-native languages, such as Tamil and Malayalam, which can have thousands of suffixes, it would be necessary to use a Finite State Machine representation of the rules.
We define the term ‘suffix evidence’ for a potential stem as the number of word-forms in the corpus that are composed of a concatenation of the stem and any valid suffix.
For instance, the suffix evidence for the stem laDk is 2 if the word-forms laDkA and laDkon are the only word-forms with the prefix laDk that exist in the corpus and A and on are both valid suffixes.
Table 6 presents word-form counts for different suffix evidence values for the WebDuniya corpus.
Since the real stems for the word-forms were not known, the prefix substring with the highest suffix evidence was used as the stem.
We shall call this heuristically selected stem the best-suffixevidence stem and its suffix evidence as the best-suffix-evidence (BSE).
It will be seen from Table 6 that about 20% of the words have a BSE of only 1.
Altogether about 40% of the words have a BSE of 1 or 2.
Note that all words have a BSE of atleast 1 since the empty string is also considered a valid suffix.
The fraction is even higher for nouns as shown in Table 7.
It must be noted that the number of nouns with a BSE of 5 or more is in the hundreds only because of erroneous concatenations of suffixes with stems.
Nouns in Hindi do not usually have more than four inflectional forms.
The scarcity of suffix evidence for most word-forms poses a huge obstacle to the extraction of a high-coverage lexicon because: The gold standard consists of one thousand word-forms picked at random from the intersection of the unique word-forms in the unlabelled Web-Duniya corpus and the POS labelled corpus.
Each word-form in the gold standard was manually examined and a stem and a root word-form found for it.
For word-forms associated with multiple POS categories, the stem and root of a word-form were listed once for each POS category because the segmentation of a word could depend on its POS category.
There were 1913 word and POS category combinations in the gold standard.
The creation of the stem gold standard needed some arbitrary choices which had to be reflected in the rules as well.
These concerned some words which could be stemmed in multiple ways.
For instance, the noun laDkT meaning ‘girl’ could be segmented into the morphemes laDk and T or allowed to remain unsegmented as laDkT.
This is because by doing the former, the stems of both laDkA and laDkT could be conflated whereas by doing the latter, they could be kept separate from each other.
We arbitrarily made the choice to keep nouns ending in T unsegmented and made our rules reflect that choice.
A second gold standard consisting of 1000 word-forms was also created to be used in evaluation and as training data for supervised algorithms.
The second gold standard contained 1906 word and POS category combinations.
Only word-forms that did not appear in the first gold standard were included in the second one.
Since the list of valid suffixes is given, the stemmer does not need to discover the stems in the language but only learn to apply the right one in the right place.
We experimented with three heuristics for finding the right stem for a word-form.
The heuristics were: In the LSM heuristic, when multiple suffixes can be applied to a word-form to stem it, we choose the longest one.
Since Hindi has concatenative morphology with only postfix inflection, we only need to find one matching suffix to stem it.
It is claimed in the literature that the method of using the longest suffix match works better than random suffix selection (Sarkar and Bandyopadhyay, 2008).
This heuristic was used as the baseline for our experiments.
In the HSE heuristic, which has been applied before to unsupervised morphological segmentation (Goldsmith, 2001), stemming (Pandey and Sid-diqui, 2008), and automatic paradigm extraction (Zeman, 2007), when multiple suffixes can be applied to stem a word-form, the suffix that is picked is the one that results in the stem with the highest suffix evidence.
In our case, when computing the suffix evidence, the following additional constraint is applied: all the suffixes used to compute the suffix evidence score for any stem must be associated with the same POS category.
For example, the suffix yon is only applicable to nouns, whereas the suffix ta is only applicable to verbs.
These two suffixes will therefore never be counted together in computing the suffix evidence for a stem.
The algorithm for determining the suffix evidence computes the suffix evidence once for each POS category and then returns the maximum.
In the absence of this constraint, the accuracy drops as the size of the raw word corpus increases.
<newSection> 3.5.3 HSE and Supervised Rule Selection (HSE + Sup) The problem with the aforementioned heuristics is that there are no weights assigned to rules.
Since the rules for the system were written to be as general and flexible as possible, false positives were commonly encountered.
We propose a very simple supervised learning method to circumvent this problem.
The training data used was a set of 1000 word-forms sampled, like the gold standard, from the unique word-forms in the intersection of the raw text corpus and the POS labelled corpus.
The set of word-forms in the training data was disjoint from the set of word-forms in the gold standard.
The feature set consisted of two features: the last character (or diacritic) of the word-form, and the suffix.
The POS category was an optional feature and used when available.
If the number of incorrect splits exceeded the number of correct splits given a feature set, the rule was assigned a weight of 0, else it was given a weight of 1.
We compare the performance of our rules with the performance of the Lightweight Stemmer for Hindi (Ramanathan and Rao, 2003) with a reported accuracy of 81.5%.
The scores we report in Table 8 are the average of the LSM scores on the two gold standards.
The stemmer using the standard rule-set (Rules1) does not perform as well as the Lightweight Stemmer.
We then handcrafted a different set of rules (Rules2) with adjustments to maximize its performance.
The accuracy was better than Rules1 but not quite equal to the Lightweight Stemmer.
However, since our gold standard is different from that used to evaluate the Lightweight Stemmer, the comparison is not necessarily very meaningful.
As shown in Table 9, in F-score comparisons, HSE seems to outperform LSM and HSE+Sup seems to outperform HSE, but the improvement in performance is not very large in the case of the second gold standard.
In terms of accuracy scores, LSM outperforms HSE and HSE+Sup when evaluated against the second gold standard.
Table 10 lists the number of correct stems, incorrect stems, and finally a count of those incorrect stems that the HSE+Sup heuristic would have gotten right if the POS category had been available.
From the numbers it appears that a sizeable fraction of the errors, especially with noun word-forms, is caused when a suffix of the wrong POS category is applied to a word-form.
Moreover, prior work in Bangla (Sarkar and Bandy-opadhyay, 2008) indicates that POS category information could improve the accuracy of stemming.
Assigning POS categories to word-forms requires a POS tagger and a substantial amount of POS labelled data as described below.
The POS tagset used was the hierarchical tagset IL-POST (Sankaran et al., 2008).
The hierarchical tagset supports broad POS categories like nouns and verbs, less broad POS types like common and proper nouns and finally, at its finest granularity, attributes like gender, number, case and mood.
We found that with a training corpus of about 45,000 tagged words (2366 sentences), it was possible to produce a reasonably accurate POS tagger2, use it to label the raw text corpus with broad POS tags, and consequently improve the accuracy of stemming.
For our experiments, we used both the full training corpus of 45,000 words and a subset of the same consisting of about 20,000 words.
The POS tagging accuracies obtained were approximately 87% and 65% respectively.
The reason for repeating the experiment using the 20,000 word subset of the training data was to demonstrate that a mere 20,000 words of labelled data, which does not take a very great amount of time and effort to create, can produce significant improvements in stemming performance.
In order to assign tags to the words of the gold standard, sentences from the raw text corpus containing word-forms present in the gold standard were tagged using a POS tagger.
The POS categories assigned to each word-form were then read off and stored in a table.
Once POS tags were associated with all the words, a more restrictive criterion for matching a rule to a word-form could be used to calculate the BSE in order to determine the stem of the word-form.
When searching for rules, and consequently the suffixes, to be applied to a word-form, only rules whose -y value matches the word-form’s POS category were considered.
We shall call the HSE heuristic that uses POS information in this way HSE+Pos.
The stem lexicon obtained by the process described above had to be converted into a root word-form lexicon.
A root word-form lexicon is in some cases more useful than a stem lexicon, for the following reasons: The stem lexicon can be converted into a root lexicon using the raw text corpus and the morphological rules that were used for stemming, as follows: Relative frequencies of word-forms have been used in previous work to detect incorrect affix attachments in Bengali and English (Dasgupta and Ng, 2007).
Our evaluation of the system showed that relative frequencies could be very effective predictors of root word-forms when applied within the framework of a rule-based system.
<newSection> 4 Evaluation The goal of our experiment was to build a high-coverage morphological lexicon for Hindi and to evaluate the same.
Having developed a multi-stage system for lexicon extraction with a POS tagging step following by stemming and root word-form discovery, we proceeded to evaluate it as follows.
The stemming and the root discovery module were evaluated against the gold standard of 1000 word-forms.
In the first experiment, the precision and recall of stemming using the HSE+Pos algorithm were measured at different POS tagging accuracies.
In the second experiment the root word-form discovery module was provided the entire raw word corpus to use in determining the best possible candidate for a root and tested using the gold standard.
The scores obtained reflect the performance of the overall system.
For stemming, the recall was calculated as the fraction of stems and suffixes in the gold standard that were returned by the stemmer for each word-form examined.
The precision was calculated as the fraction of stems and suffixes returned by the stemmer that matched the gold standard.
The F-score was calculated as the harmonic mean of the precision and recall.
The recall of the root lexicon was measured as the fraction of gold standard roots that were in the lexicon.
The precision was calculated as the fraction of roots in the lexicon that were also in the gold standard.
Accuracy was the percentage of gold word-forms’ roots that were matched exactly.
In order to approximately estimate the accuracy of a stemmer or morphological analyzer that used such a lexicon, we also calculated the accuracy weighted by the frequency of the word-forms in a small corpus of running text.
The gold standard tokens were seen in this corpus about 4400 times.
We only considered content words (nouns, verbs, adjectives and adverbs) in this calculation.
<newSection> 5 Results The performance of our system using POS tag information is comparable to that obtained by Sarkar and Bandyopadhyay (2008).
Sarkar and Bandy-opadhyay (2008) obtained stemming accuracies of 90.2% for Bangla using gold POS tags.
So in the comparisons in Table 11, we use gold POS tags (row two) and also supervised learning (row three) using the other gold corpus as the labelled training corpus.
We present the scores for the two gold standards separately.
It must be noted that Sarkar and Bandyopadhyay (2008) conducted their experiments on Bangla, and so the results are not exactly comparable.
We also evaluate the performance of stemming using HSE with POS tagging by a real tagger at two different tagging accuracies - approximately 65% and 87% - as shown in Table 12.
We compare the performance with gold POS tags and a baseline system which does not use POS tags.
We do not use labelled training data for this section of the experiments and only evaluate against the first gold standard.
ery at different POS tagging accuracies against a baseline which excludes the use of POS tags altogether.
There seems to be very little prior work that we can use for comparison here.
To our knowledge, the closest comparable work is a system built by Oliver and Tadi´c (2004) in order to enlarge a Croatian Morphological Lexicon.
The overall performance reported by Tadi´c et al was as follows: (precision=86.13%, recall=35.36%, F1=50.14%).
Lastly, Table 14 shows the accuracy of stemming and root finding weighted by the frequencies of the words in a running text corpus.
This was calculated only for content words.
<newSection> 6 Conclusion We have described a system for automatically constructing a root word-form lexicon from a raw text corpus.
The system is rule-based and utilizes a POS tagger.
Though preliminary, our results demonstrate that it is possible, using this method, to extract a high-precision and high-recall root word-form lexicon.
Specifically, we show that with a POS tagger capable of labelling word-forms with POS categories at an accuracy of about 88%, we can extract root word-forms with an accuracy of about 87% and a precision and recall of 94.1% and 95.3% respectively.
Though the system has been evaluated on Hindi, the techniques described herein can probably be applied to other inflectional languages.
The rules selected by the system and applied to the word-forms also contain information that can be used to determine the paradigm membership of each root word-form.
Further work could evaluate the accuracy with which we can accomplish this task.
<newSection> 7 Acknowledgements We would like to thank our colleagues Priyanka Biswas, Kalika Bali and Shalini Hada, of Microsoft Research India, for their assistance in the creation of the Hindi root and stem gold standards.
<newSection> References<newSection> Abstract We describe refinements to hierarchical translation search procedures intended to reduce both search errors and memory usage through modifications to hypothesis expansion in cube pruning and reductions in the size of the rule sets used in translation.
Rules are put into syntactic classes based on the number of non-terminals and the pattern, and various filtering strategies are then applied to assess the impact on translation speed and quality.
Results are reported on the 2008 NIST Arabic-toEnglish evaluation task.
<newSection> 1 Introduction Hierarchical phrase-based translation (Chiang, 2005) has emerged as one of the dominant current approaches to statistical machine translation.
Hiero translation systems incorporate many of the strengths of phrase-based translation systems, such as feature-based translation and strong target language models, while also allowing flexible translation and movement based on hierarchical rules extracted from aligned parallel text.
The approach has been widely adopted and reported to be competitive with other large-scale data driven approaches, e.g. (Zollmann et al., 2008).
Large-scale hierarchical SMT involves automatic rule extraction from aligned parallel text, model parameter estimation, and the use of cube pruning k-best list generation in hierarchical translation.
The number of hierarchical rules extracted far exceeds the number of phrase translations typically found in aligned text.
While this may lead to improved translation quality, there is also the risk of lengthened translation times and increased memory usage, along with possible search errors due to the pruning procedures needed in search.
We describe several techniques to reduce memory usage and search errors in hierarchical translation.
Memory usage can be reduced in cube pruning (Chiang, 2007) through smart memoiza-tion, and spreading neighborhood exploration can be used to reduce search errors.
However, search errors can still remain even when implementing simple phrase-based translation.
We describe a ‘shallow’ search through hierarchical rules which greatly speeds translation without any effect on quality.
We then describe techniques to analyze and reduce the set of hierarchical rules.
We do this based on the structural properties of rules and develop strategies to identify and remove redundant or harmful rules.
We identify groupings of rules based on non-terminals and their patterns and assess the impact on translation quality and computational requirements for each given rule group.
We find that with appropriate filtering strategies rule sets can be greatly reduced in size without impact on translation performance.
<newSection> 1.1 Related Work The search and rule pruning techniques described in the following sections add to a growing literature of refinements to the hierarchical phrase-based SMT systems originally described by Chiang (2005; 2007).
Subsequent work has addressed improvements and extensions to the search procedure itself, the extraction of the hierarchical rules needed for translation, and has also reported contrastive experiments with other SMT architectures.
Hiero Search Refinements Huang and Chiang (2007) offer several refinements to cube pruning to improve translation speed.
Venugopal et al.
(2007) introduce a Hiero variant with relaxed constraints for hypothesis recombination during parsing; speed and results are comparable to those of cube pruning, as described by Chiang (2007).
Li and Khudanpur (2008) report significant improvements in translation speed by taking unseen n-grams into account within cube pruning to minimize language model requests.
Dyer et al.
(2008) extend the translation of source sentences to translation of input lattices following Chappelier et al.
(1999).
Extensions to Hiero Blunsom et al.
(2008) discuss procedures to combine discriminative latent models with hierarchical SMT.
The Syntax-Augmented Machine Translation system (Zoll-mann and Venugopal, 2006) incorporates target language syntactic constituents in addition to the synchronous grammars used in translation.
Shen at al.
(2008) make use of target dependency trees and a target dependency language model during decoding.
Marton and Resnik (2008) exploit shallow correspondences of hierarchical rules with source syntactic constituents extracted from parallel text, an approach also investigated by Chiang (2005).
Zhang and Gildea (2006) propose bina-rization for synchronous grammars as a means to control search complexity arising from more complex, syntactic, hierarchical rules sets.
Hierarchical rule extraction Zhang et al.
(2008) describe a linear algorithm, a modified version of shift-reduce, to extract phrase pairs organized into a tree from which hierarchical rules can be directly extracted.
Lopez (2007) extracts rules on-the-fly from the training bitext during decoding, searching efficiently for rule patterns using suffix arrays.
Analysis and Contrastive Experiments Zollman et al.
(2008) compare phrase-based, hierarchical and syntax-augmented decoders for translation of Arabic, Chinese, and Urdu into English, and they find that attempts to expedite translation by simple schemes which discard rules also degrade translation performance.
Lopez (2008) explores whether lexical reordering or the phrase discontiguity inherent in hierarchical rules explains improvements over phrase-based systems.
Hierarchical translation has also been used to great effect in combination with other translation architectures (e.g. (Sim et al., 2007; Rosti et al., 2007)).
The paper proceeds as follows.
Section 2 describes memoization and spreading neighborhood exploration in cube pruning intended to reduce memory usage and search errors, respectively.
A detailed comparison with a simple phrase-based system is presented.
Section 3 describes pattern-based rule filtering and various procedures to select rule sets for use in translation with an aim to improving translation quality while minimizing rule set size.
Finally, Section 4 concludes.
<newSection> 2 Two Refinements in Cube Pruning Chiang (2007) introduced cube pruning to apply language models in pruning during the generation of k-best translation hypotheses via the application of hierarchical rules in the CYK algorithm.
In the implementation of Hiero described here, there is the parser itself, for which we use a variant of the CYK algorithm closely related to CYK+ (Chap-pelier and Rajman, 1998); it employs hypothesis recombination, without pruning, while maintaining back pointers.
Before k-best list generation with cube pruning, we apply a smart memoiza-tion procedure intended to reduce memory consumption during k-best list expansion.
Within the cube pruning algorithm we use spreading neigh-borhood exploration to improve robustness in the face of search errors.
Each cell in the chart built by the CYK algorithm contains all possible derivations of a span of the source sentence being translated.
After the parsing stage is completed, it is possible to make a very efficient sweep through the backpointers of the CYK grid to count how many times each cell will be accessed by the k-best generation algorithm.
When k-best list generation is running, the number of times each cell is visited is logged so that, as each cell is visited for the last time, the k-best list associated with each cell is deleted.
This continues until the one k-best list remaining at the top of the chart spans the entire sentence.
Memory reductions are substantial for longer sentences: for the longest sentence in the tuning set described later (105 words in length), smart memoization reduces memory usage during the cube pruning stage from 2.1GB to 0.7GB.
For average length sentences of approx. 30 words, memory reductions of 30% are typical.
In generation of a k-best list of translations for a source sentence span, every derivation is transformed into a cube containing the possible translations arising from that derivation, along with their translation and language model scores (Chi-ang, 2007).
These derivations may contain non-terminals which must be expanded based on hypotheses generated by lower cells, which themselves may contain non-terminals.
For efficiency each cube maintains a queue of hypotheses, called here the frontier queue, ranked by translation and language model score; it is from these frontier queues that hypotheses are removed to create the k-best list for each cell.
When a hypothesis is extracted from a frontier queue, that queue is updated by searching through the neighborhood of the extracted item to find novel hypotheses to add; if no novel hypotheses are found, that queue necessarily shrinks.
This shrinkage can lead to search errors.
We therefore require that, when a hypothesis is removed, new candidates must be added by exploring a neighborhood which spreads from the last extracted hypothesis.
Each axis of the cube is searched (here, to a depth of 20) until a novel hypothesis is found.
In this way, up to three new candidates are added for each entry extracted from a frontier queue.
Chiang (2007) describes an initialization procedure in which these frontier queues are seeded with a single candidate per axis; we initialize each frontier queue to a depth of bN-1+1, where Nnt is the number of non-terminals in the derivation and b is a search parameter set throughout to 10.
By starting with deep frontier queues and by forcing them to grow during search we attempt to avoid search errors by ensuring that the universe of items within the frontier queues does not decrease as the k-best lists are filled.
Experiments reported in this paper are based on the NIST MT08 Arabic-to-English translation task.
Alignments are generated over all allowed parallel data, (∼150M words per language).
Features extracted from the alignments and used in translation are in common use: target language model, source-to-target and target-to-source phrase translation models, word and rule penalties, number of usages of the glue rule, source-to-target and target-to-source lexical models, and three rule within a cube, just before and after extraction of the item C.
Grey squares represent the frontier queue; black squares are candidates already extracted.
Chiang (2007) would only consider adding items X to the frontier queue, so the queue would shrink.
Spreading neighborhood exploration adds candidates S to the frontier queue.
count features inspired by Bender et al.
(2007).
MET (Och, 2003) iterative parameter estimation under IBM BLEU is performed on the development set.
The English language used model is a 4-gram estimated over the parallel text and a 965 million word subset of monolingual data from the English Gigaword Third Edition.
In addition to the MT08 set itself, we use a development set mt0205-tune formed from the odd numbered sentences of the NIST MT02 through MT05 evaluation sets; the even numbered sentences form the validation set mt02-05-test.
The mt02-05-tune set has 2,075 sentences.
We first compare the cube pruning decoder to the TTM (Kumar et al., 2006), a phrase-based SMT system implemented with Weighted Finite-State Tansducers (Allauzen et al., 2007).
The system implements either a monotone phrase order translation, or an MJ1 (maximum phrase jump of 1) reordering model (Kumar and Byrne, 2005).
Relative to the complex movement and translation allowed by Hiero and other models, MJ1 is clearly inferior (Dreyer et al., 2007); MJ1 was developed with efficiency in mind so as to run with a minimum of search errors in translation and to be easily and exactly realized via WFSTs.
Even for the large models used in an evaluation task, the TTM system is reported to run largely without pruning (Blackwood et al., 2008).
The Hiero decoder can easily be made to implement MJ1 reordering by allowing only a restricted set of reordering rules in addition to the usual glue rule, as shown in left-hand column of Table 1, where T is the set of terminals.
Constraining Hiero in this way makes it possible to compare its performance to the exact WFST TTM implementation and to identify any search errors made by Hiero.
Table 2 shows the lowercased IBM BLEU scores obtained by the systems for mt02-05-tune with monotone and reordered search, and with MET-optimised parameters for MJ1 reordering.
For Hiero, an N-best list depth of 10,000 is used throughout.
In the monotone case, all phrase-based systems perform similarly although Hiero does make search errors.
For simple MJ1 reordering, the basic Hiero search procedure makes many search errors and these lead to degradations in BLEU.
Spreading neighborhood expansion reduces the search errors and improves BLEU score significantly but search errors remain a problem.
Search errors are even more apparent after MET.
This is not surprising, given that mt02-05-tune is the set over which MET is run: MET drives up the likelihood of good hypotheses at the expense of poor hypotheses, but search errors often increase due to the expanded dynamic range of the hypothesis scores.
Our aim in these experiments was to demonstrate that spreading neighborhood exploration can aid in avoiding search errors.
We emphasize that we are not proposing that Hiero should be used to implement reordering models such as MJ1 which were created for completely different search procedures (e.g. WFST composition).
However these experiments do suggest that search errors may be an issue, particularly as the search space grows to include the complex long-range movement allowed by the hierarchical rules.
We next study various filtering procedures to reduce hierarchical rule sets to find a balance between translation speed, memory usage, and performance.
<newSection> 3 Rule Filtering by Pattern Hierarchical rules X → (γ,α) are composed of sequences of terminals and non-terminals, which mance on mt02-05-tune for TTM (a), Hiero (b), Hiero with spreading neighborhood exploration (c).
SE is the number of Hiero hypotheses with search errors.
we call elements.
In the source, a maximum of two non-adjacent non-terminals is allowed (Chi-ang, 2007).
Leaving aside rules without non-terminals (i.e. phrase pairs as used in phrase-based translation), rules can be classed by their number of non-terminals, N,,,t, and their number of elements, Ne.
There are 5 possible classes: N,,,t.Ne= 1.2,1.3, 2.3, 2.4, 2.5.
During rule extraction we search each class separately to control memory usage.
Furthermore, we extract from alignments only those rules which are relevant to our given test set; for computation of backward translation probabilities we log general counts of target-side rules but discard unneeded rules.
Even with this restriction, our initial ruleset for mt02-05-tune exceeds 175M rules, of which only 0.62M are simple phrase pairs.
The question is whether all these rules are needed for translation.
If the rule set can be reduced without reducing translation quality, both memory efficiency and translation speed can be increased.
Previously published approaches to reducing the rule set include: enforcing a minimum span of two words per non-terminal (Lopez, 2008), which would reduce our set to 115M rules; or a minimum count (mincount) threshold (Zoll-mann et al., 2008), which would reduce our set to 78M (mincount=2) or 57M (mincount=3) rules.
Shen et al.
(2008) describe the result of filtering rules by insisting that target-side rules are well-formed dependency trees.
This reduces their rule set from 140M to 26M rules.
This filtering leads to a degradation in translation performance (see Table 2 of Shen et al.
(2008)), which they counter by adding a dependency LM in translation.
As another reference point, Chiang (2007) reports Chinese-to-English translation experiments based on 5.5M rules.
Zollmann et al.
(2008) report that filtering rules en masse leads to degradation in translation performance.
Rather than apply a coarse filtering, such as a mincount for all rules, we follow a more syntactic approach and further classify our rules according to their pattern and apply different filters to each pattern depending on its value in translation.
The premise is that some patterns are more important than others.
number of non-terminals, Nnt, number of elements Ne, source and target patterns, and types in the rule set extracted for mt02-05-tune.
Given a rule set, we define source patterns and target patterns by replacing every sequence of non-terminals by a single symbol ‘w’ (indicating word, i.e. terminal string, w E T+).
Each hierarchical rule has a unique source and target pattern which together define the rule pattern.
By ignoring the identity and the number of adjacent terminals, the rule pattern represents a natural generalization of any rule, capturing its structure and the type of reordering it encodes.
In total, there are 66 possible rule patterns.
Table 3 presents a few examples extracted for mt02-05tune, showing that some patterns are much more diverse than others.
For example, patterns with two non-terminals (Nnt=2) are richer than patterns with Nnt=1, as they cover many more distinct rules.
Additionally, patterns with two non-terminals which also have a monotonic relationship between source and target non-terminals are much more diverse than their reordered counterparts.
Some examples of extracted rules and their corresponding pattern follow, where Arabic is shown in Buckwalter encoding.
We describe a greedy approach to building a rule set in which rules belonging to a pattern are added to the rule set guided by the improvements they yield on mt02-05-tune relative to the monotone Hiero system described in the previous section.
We find that certain patterns seem not to contribute to any improvement.
This is particularly significant as these patterns often encompass large numbers of rules, as with patterns with matching source and target patterns.
For instance, we found no improvement when adding the pattern (X1w,X1w), of which there were 1.2M instances (Table 3).
Since concatenation is already possible under the general glue rule, rules with this pattern are redundant.
By contrast, the much less frequent reordered counterpart, i.e. the (wX1,X1w) pattern (0.01M instances), provides substantial gains.
The situation is analogous for rules with two non-terminals (Nnt=2).
Based on exploratory analyses (not reported here, for space) an initial rule set was built by excluding patterns reported in Table 4.
In total, 171.5M rules are excluded, for a remaining set of 4.2M rules, 3.5M of which are hierarchical.
We acknowledge that adding rules in this way, by greedy search, is less than ideal and inevitably raises questions with respect to generality and re-peatability.
However in our experience this is a robust approach, mainly because the initial translation system runs very fast; it is possible to run many exploratory experiments in a short time.
In measuring the effectiveness of rules in translation, we also investigate whether a ‘fully hierarchical’ search is needed or whether a shallow search is also effective.
In constrast to full Hiero, in the shallow search, only phrases are allowed to be substituted into non-terminals.
The rules used in each case can be expressed as shown in the 2nd and 3rd columns of Table 1.
Shallow search can be considered (loosely) to be a form of rule filtering.
As can be seen in Table 5 there is no impact on BLEU, while translation speed increases by a factor of 7.
Of course, these results are specific to this Arabic-to-English translation task, and need not be expected to carry over to other language pairs, such as Chinese-to-English translation.
However, the impact of this search simplification is easy to measure, and the gains can be significant enough, that it may be worth investigation even for languages with complex long distance movement.
Results applying these filters with various thresholds are given in Table 6, including number of rules and decoding time.
As shown, all filters achieve at least a 50% speed-up in decoding time by discarding 15% to 25% of the baseline rules.
Remarkably, performance is unaffected when applying the simple NT and NRT filters with a threshold of 20 translations.
Finally, the CM filter behaves slightly worse for thresholds of 90% for the same decoding time.
For this reason, we select NRT=20 as our general filter.
tion (IBM BLEU), time (in seconds per word) and number of rules (in millions).
We now filter rules individually (not by class) according to their number of translations.
For each fixed -y V T+ (i.e. with at least 1 non-terminal), we define the following filters over rules X --+ (-y,α): In this section we first reconsider whether reintro-ducing the monotonic rules (originally excluded as described in rows ’b’, ’c’, ’d’ in Table 4) affects performance.
Results are given in the upper rows of Table 7.
For all classes, we find that reintroduc-ing these rules increases the total number of rules substantially, despite the NRT=20 filter, but leads to degradation in translation performance.
We next reconsider the mincount threshold values for Nnt.Ne classes 1.3, 2.3, 2.4 and 2.5 originally described in Table 4 (rows ’e’ to ’h’).
Results under various mincount cutoffs for each class are given in Table 7 (middle five rows).
For classes 2.3 and 2.5, the mincount cutoff can be reduced to 1 (i.e. all rules are kept) with slight translation improvements.
In contrast, reducing the cutoff for classes 1.3 and 2.4 to 3 and 5, respectively, adds many more rules with no increase in performance.
We also find that increasing the cutoff to 15 for class 2.4 yields the same results with a smaller rule set.
Finally, we consider further filtering applied to class 1.2 with mincount 5 and 10 (final two rows in Table 7).
The number of rules is largely unchanged, but translation performance drops consistently as more rules are removed.
Based on these experiments, we conclude that it is better to apply separate mincount thresholds to the classes to obtain optimal performance with a minimum size rule set.
Finally, in this section we report results of our shallow hierarchical system with the 2.5 min-count=1 configuration from Table 7, after including the following N-best list rescoring steps.
specific zero-cutoff stupid-backoff (Brants et al., 2007) 5-gram language models, estimated using ∼4.7B words of English newswire text, and apply them to rescore each 10000-best list.
• Minimum Bayes Risk (MBR).
We then rescore the first 1000-best hypotheses with MBR, taking the negative sentence level BLEU score as the loss function to minimise (Ku-mar and Byrne, 2004).
Table 8 shows results for mt02-05-tune, mt0205-test, the NIST subsets from the MT06 evaluation (mt06-nist-nw for newswire data and mt06nist-ng for newsgroup) and mt08, as measured by lowercased IBM BLEU and TER (Snover et al., 2006).
Mixed case NIST BLEU for this system on mt08 is 42.5.
This is directly comparable to official MT08 evaluation results1.
<newSection> 4 Conclusions This paper focuses on efficient large-scale hierarchical translation while maintaining good translation quality.
Smart memoization and spreading neighborhood exploration during cube pruning are described and shown to reduce memory consumption and Hiero search errors using a simple phrase-based system as a contrast.
We then define a general classification of hierarchical rules, based on their number of non-terminals, elements and their patterns, for refined extraction and filtering.
For a large-scale Arabic-to-English task, we show that shallow hierarchical decoding is as good as fully hierarchical search and that decoding time is dramatically decreased.
In addition, we describe individual rule filters based on the distribution of translations with further time reductions at no cost in translation scores.
This is in direct contrast to recent reported results in which other filtering strategies lead to degraded performance (Shen et al., 2008; Zollmann et al., 2008).
We find that certain patterns are of much greater value in translation than others and that separate minimum count filters should be applied accordingly.
Some patterns were found to be redundant or harmful, in particular those with two monotonic non-terminals.
Moreover, we show that the value of a pattern is not directly related to the number of rules it encompasses, which can lead to discarding large numbers of rules as well as to dramatic speed improvements.
Although reported experiments are only for Arabic-to-English translation, we believe the approach will prove to be general.
Pattern relevance will vary for other language pairs, but we expect filtering strategies to be equally worth pursuing.
<newSection> Acknowledgments This work was supported in part by the GALE program of the Defense Advanced Research Projects Agency, Contract No. HR0011- 06-C-0022.
G. Iglesias supported by Spanish Government research grant BES-2007-15956 (project TEC2006-13694-C03-03).
<newSection> References<newSection> Abstract We present a simple and effective method for extracting parallel sentences from comparable corpora.
We employ a statistical machine translation (SMT) system built from small amounts of parallel texts to translate the source side of the nonparallel corpus.
The target side texts are used, along with other corpora, in the language model of this SMT system.
We then use information retrieval techniques and simple filters to create French/English parallel data from a comparable news corpora.
We evaluate the quality of the extracted data by showing that it significantly improves the performance of an SMT systems.
<newSection> 1 Introduction Parallel corpora have proved be an indispensable resource in Statistical Machine Translation (SMT).
A parallel corpus, also called bitext, consists in bilingual texts aligned at the sentence level.
They have also proved to be useful in a range of natural language processing applications like automatic lexical acquisition, cross language information retrieval and annotation projection.
Unfortunately, parallel corpora are a limited resource, with insufficient coverage of many language pairs and application domains of interest.
The performance of an SMT system heavily depends on the parallel corpus used for training.
Generally, more bitexts lead to better performance.
Current resources of parallel corpora cover few language pairs and mostly come from one domain (proceedings of the Canadian or European Parliament, or of the United Nations).
This becomes specifically problematic when SMT systems trained on such corpora are used for general translations, as the language jargon heavily used in these corpora is not appropriate for everyday life translations or translations in some other domain.
One option to increase this scarce resource could be to produce more human translations, but this is a very expensive option, in terms of both time and money.
In recent work less expensive but very productive methods of creating such sentence aligned bilingual corpora were proposed.
These are based on generating “parallel” texts from already available “almost parallel” or “not much parallel” texts.
The term “comparable corpus” is often used to define such texts.
A comparable corpus is a collection of texts composed independently in the respective languages and combined on the basis of similarity of content (Yang and Li, 2003).
The raw material for comparable documents is often easy to obtain but the alignment of individual documents is a challenging task (Oard, 1997).
Multilingual news reporting agencies like AFP, Xinghua, Reuters, CNN, BBC etc.
serve to be reliable producers of huge collections of such comparable corpora.
Such texts are widely available from LDC, in particular the Gigaword corpora, or over the WEB for many languages and domains, e.g. Wikipedia.
They often contain many sentences that are reasonable translations of each other, thus potential parallel sentences to be identified and extracted.
There has been considerable amount of work on bilingual comparable corpora to learn word translations as well as discovering parallel sentences.
Yang and Lee (2003) use an approach based on dynamic programming to identify potential parallel sentences in title pairs.
Longest common sub sequence, edit operations and match-based score functions are subsequently used to determine confidence scores.
Resnik and Smith (2003) propose their STRAND web-mining based system and show that their approach is able to find large numbers of similar document pairs.
Works aimed at discovering parallel sentences French: Au total, 1,634 million d’´electeurs doivent d´esigner les 90 d´eput´es de la prochaine l´egislature parmi 1.390 candidats pr´esent´es par 17 partis, dont huit sont repr´esent´es au parlement.
Query: In total, 1,634 million voters will designate the 90 members of the next parliament among 1.390 candidates presented by 17 parties, eight of which are represented in parliament.
Result: Some 1.6 million voters were registered to elect the 90 members of the legislature from 1,390 candidates from 17 parties, eight of which are represented in parliament, several civilian organisations and independent lists.
include (Utiyama and Isahara, 2003), who use cross-language information retrieval techniques and dynamic programming to extract sentences from an English-Japanese comparable corpus.
They identify similar article pairs, and then, treating these pairs as parallel texts, align their sentences on a sentence pair similarity score and use DP to find the least-cost alignment over the document pair.
Fung and Cheung (2004) approach the problem by using a cosine similarity measure to match foreign and English documents.
They work on “very non-parallel corpora”.
They then generate all possible sentence pairs and select the best ones based on a threshold on cosine similarity scores.
Using the extracted sentences they learn a dictionary and iterate over with more sentence pairs.
Recent work by Munteanu and Marcu (2005) uses a bilingual lexicon to translate some of the words of the source sentence.
These translations are then used to query the database to find matching translations using information retrieval (IR) techniques.
Candidate sentences are determined based on word overlap and the decision whether a sentence pair is parallel or not is performed by a maximum entropy classifier trained on parallel sentences.
Bootstrapping is used and the size of the learned bilingual dictionary is increased over iterations to get better results.
Our technique is similar to that of (Munteanu and Marcu, 2005) but we bypass the need of the bilingual dictionary by using proper SMT translations and instead of a maximum entropy classifier we use simple measures like the word error rate (WER) and the translation error rate (TER) to decide whether sentences are parallel or not.
Using the full SMT sentences, we get an added advantage of being able to detect one of the major errors of this technique, also identified by (Munteanu and Marcu, 2005), i.e, the cases where the initial sentences are identical but the retrieved sentence has a tail of extra words at sentence end.
We try to counter this problem as detailed in 4.1.
We apply this technique to create a parallel corpus for the French/English language pair using the LDC Gigaword comparable corpus.
We show that we achieve significant improvements in the BLEU score by adding our extracted corpus to the already available human-translated corpora.
This paper is organized as follows.
In the next section we first describe the baseline SMT system trained on human-provided translations only.
We then proceed by explaining our parallel sentence selection scheme and the post-processing.
Section 4 summarizes our experimental results and the paper concludes with a discussion and perspectives of this work.
<newSection> 2 Baseline SMT system The goal of SMT is to produce a target sentence e from a source sentence f.
Among all possible target language sentences the one with the highest probability is chosen: where Pr(f|e) is the translation model and Pr(e) is the target language model (LM).
This approach is usually referred to as the noisy source-channel approach in SMT (Brown et al., 1993).
Bilingual corpora are needed to train the translation model and monolingual texts to train the target language model.
It is today common practice to use phrases as translation units (Koehn et al., 2003; Och and Ney, 2003) instead of the original word-based approach.
A phrase is defined as a group of source words f˜ that should be translated together into a group of target words ˜e.
The translation model in phrase-based systems includes the phrase translation probabilities in both directions, i.e. P(˜e| and P(˜f|˜e).
The use of a maximum entropy approach simplifies the introduction of several additional models explaining the translation process : e* = arg max Pr(e|f) Aihi(e,f))} (3) �= arg max {exp( e i The feature functions hi are the system models and the Ai weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002).
In our system fourteen features functions were used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty, and a target language model.
The system is based on the Moses SMT toolkit (Koehn et al., 2007) and constructed as follows.
First, Giza++ is used to perform word alignments in both directions.
Second, phrases and lexical reorderings are extracted using the default settings of the Moses SMT toolkit.
The 4-gram back-off target LM is trained on the English part of the bitexts and the Gigaword corpus of about 3.2 billion words.
Therefore, it is likely that the target language model includes at least some of the translations of the French Gigaword corpus.
We argue that this is a key factor to obtain good quality translations.
The translation model was trained on the news-commentary corpus (1.56M words)1 and a bilingual dictionary of about 500k entries.2 This system uses only a limited amount of human-translated parallel texts, in comparison to the bitexts that are available in NIST evaluations.
In a different versions of this system, the Europarl (40M words) and the Canadian Hansard corpus (72M words) were added.
In the framework of the EuroMatrix project, a test set of general news data was provided for the shared translation task of the third workshop on SMT (Callison-Burch et al., 2008), called new-stest2008 in the following.
The size of this corpus amounts to 2051 lines and about 44 thousand words.
This data was randomly split into two parts for development and testing.
Note that only one reference translation is available.
We also noticed several spelling errors in the French source texts, mainly missing accents.
These were mostly automatically corrected using the Linux spell checker.
This increased the BLEU score by about 1 BLEU point in comparison to the results reported in the official evaluation (Callison-Burch et al., 2008).
The system tuned on this development data is used translate large amounts of text of French Gigaword corpus (see Figure 2).
These translations will be then used to detect potential parallel sentences in the English Gigaword corpus.
<newSection> 3 System Architecture The general architecture of our parallel sentence extraction system is shown in figure 3.
Starting from comparable corpora for the two languages, French and English, we propose to translate French to English using an SMT system as described above.
These translated texts are then used to perform information retrieval from the English corpus, followed by simple metrics like WER and TER to filter out good sentence pairs and eventually generate a parallel corpus.
We show that a parallel corpus obtained using this technique helps considerably to improve an SMT system.
We shall also be trying to answer the following question over the course of this study: do we need to use the best possible SMT systems to be able to retrieve the correct parallel sentences or any ordinary SMT system will serve the purpose ? <newSection> 3.1 System for Extracting Parallel Sentences from Comparable Corpora LDC provides large collections of texts from multilingual news reporting agencies.
We identified agencies that provided news feeds for the languages of our interest and chose AFP for our study.3 We start by translating the French AFP texts to English using the SMT systems discussed in section 2.
In our experiments we considered only the most recent texts (2002-2006, 5.5M sentences; about 217M French words).
These translations are then treated as queries for the IR process.
The design of our sentence extraction process is based on the heuristic that considering the corpus at hand, we can safely say that a news item reported on day X in the French corpus will be most probably found in the day X-5 and day X+5 time period.
We experimented with several window sizes and found the window size of ±5 days to be the most accurate in terms of time and the quality of the retrieved sentences.
Using the ID and date information for each sentence of both corpora, we first collect all sentences from the SMT translations corresponding to the same day (query sentences) and then the corresponding articles from the English Gigaword corpus (search space for IR).
These day-specific files are then used for information retrieval using a robust information retrieval system.
The Lemur IR toolkit (Ogilvie and Callan, 2001) was used for sentence extraction.
The top 5 scoring sentences are returned by the IR process.
We found no evidence that retrieving more than 5 top scoring sentences helped get better sentences.
At the end of this step, we have for each query sentence 5 potentially matching sentences as per the IR score.
The information retrieval step is the most time consuming task in the whole system.
The time taken depends upon various factors like size of the index to search in, length of the query sentence etc.
To give a time estimate, using a ±5 day window required 9 seconds per query vs 15 seconds per query when a ±7 day window was used.
The number of results retrieved per sentence also had an impact on retrieval time with 20 results taking 19 seconds per query, whereas 5 results taking 9 seconds per query.
Query length also affected the speed of the sentence extraction process.
But with the problem at we could differentiate among important and unimportant words as nouns, verbs and sometimes even numbers (year, date) could be the keywords.
We, however did place a limit of approximately 90 words on the queries and the indexed sentences.
This choice was motivated by the fact that the word alignment toolkit Giza++ does not process longer sentences.
A Krovetz stemmer was used while building the index as provided by the toolkit.
English stop words, i.e. frequently used words, such as “a” or “the”, are normally not indexed because they are so common that they are not useful to query on.
The stop word list provided by the IR Group of University of Glasgow4 was used.
The resources required by our system are minimal : translations of one side of the comparable corpus.
We will be showing later in section 4.2 of this paper that with an SMT system trained on small amounts of human-translated data we can ’retrieve’ potentially good parallel sentences.
Once we have the results from information retrieval, we proceed on to decide whether sentences are parallel or not.
At this stage we choose the best scoring sentence as determined by the toolkit and pass the sentence pair through further filters.
Gale and Church (1993) based their align program on the fact that longer sentences in one language tend to be translated into longer sentences in the other language, and that shorter sentences tend to be translated into shorter sentences.
We also use the same logic in our initial selection of the sentence pairs.
A sentence pair is selected for further processing if the length ratio is not more than 1.6.
A relaxed factor of 1.6 was chosen keeping in consideration the fact that French sentences are longer than their respective English translations.
Finally, we discarded all sentences that contain a large fraction of numbers.
Typically, those are tables of sport results that do not carry useful information to train an SMT.
Sentences pairs conforming to the previous criteria are then judged based on WER (Levenshtein distance) and translation error rate (TER).
WER measures the number of operations required to transform one sentence into the other (insertions, deletions and substitutions).
A zero WER would mean the two sentences are identical, subsequently lower WER sentence pairs would be sharing most of the common words.
However two correct translations may differ in the order in which the words appear, something that WER is incapable of taking into account as it works on word to word basis.
This shortcoming is addressed by TER which allows block movements of words and thus takes into account the reorderings of words and phrases in translation (Snover et al., 2006).
We used both WER and TER to choose the most suitable sentence pairs.
<newSection> 4 Experimental evaluation Our main goal was to be able to create an additional parallel corpus to improve machine translation quality, especially for the domains where we have less or no parallel data available.
In this section we report the results of adding these extracted parallel sentences to the already available human-translated parallel sentences.
We conducted a range of experiments by adding our extracted corpus to various combinations of already available human-translated parallel corpora.
We experimented with WER and TER as filters to select the best scoring sentences.
Generally, sentences selected based on TER filter showed better BLEU and TER scores than their WER counter parts.
So we chose TER filter as standard for our experiments with limited amounts of human translated corpus.
Figure 4 shows this WER vs TER comparison based on BLEU and TER scores on the test data in function of the size of training data.
These experiments were performed with only 1.56M words of human-provided translations (news-commentary corpus).
Two main classes of errors common in such tasks: firstly, cases where the two sentences share many common words but actually convey different meaning, and secondly, cases where the two sentences are (exactly) parallel except at sentence ends where one sentence has more information than the other.
This second case of errors can be detected using WER as we have both the sentences in English.
We detected the extra insertions at the end of the IR result sentence and removed them.
Some examples of such sentences along with tails detected and removed are shown in figure 1.
This resulted in an improvement in the SMT scores as shown in table 1.
This technique worked perfectly for sentences having TER greater than 30%.
Evidently these are the sentences which have longer tails which result in a lower TER score and removing them improves performance significantly.
Removing sentence tails evidently improved the scores especially for larger data, for example for the data size of 12.5M we see an improvement of 0.65 and 0.98 BLEU points on dev and test data respectively and 1.00 TER points on test data (last line table 1).
The best BLEU score on the development data is obtained when adding 9.4M words of automatically aligned bitexts (11M in total).
This corresponds to an increase of about 2.88 points BLEU on the development set and an increase of 2.46 BLEU points on the test set (19.53 → 21.99) as shown in table 2, first two lines.
The TER decreased by 3.07%.
Adding the dictionary improves the baseline system (second line in Table 2), but it is not necessary any more once we have the automatically extracted data.
Having had very promising results with our previous experiments, we proceeded onto experimentation with larger human-translated data sets.
We added our extracted corpus to the collection of News-commentary (1.56M) and Europarl (40.1M) bitexts.
The corresponding SMT experiments yield an improvement of about 0.2 BLEU points on the Dev and Test set respectively (see table 2).
Our motivation for this approach was to be able to improve SMT performance by ’creating’ parallel texts for domains which do not have enough or any parallel corpora.
Therefore only the news-commentary bitext and the bilingual dictionary were used to train an SMT system that produced the queries for information retrieval.
To investigate the impact of the SMT quality on our system, we built another SMT system trained on large amounts of human-translated corpora (116M), as detailed in section 2.
Parallel sentence extraction was done using the translations performed by this big SMT system as IR queries.
We found no experimental evidence that the improved automatic translations yielded better alignments of the comaprable corpus.
It is however interesting to note that we achieve almost the same performance when we add 9.4M words of autoamticallly extracted sentence as with 40M of human-provided (out-of domain) translations (second versus fifth line in Table 2).
<newSection> 5 Conclusion and discussion Sentence aligned parallel corpora are essential for any SMT system.
The amount of in-domain parallel corpus available accounts for the quality of the translations.
Not having enough or having no in-domain corpus usually results in bad translations for that domain.
This need for parallel corpora, has made the researchers employ new techniques and methods in an attempt to reduce the dire need of this crucial resource of the SMT systems.
Our study also contributes in this regard by employing an SMT itself and information retrieval techniques to produce additional parallel corpora from easily available comparable corpora.
We use automatic translations of comparable corpus of one language (source) to find the corresponding parallel sentence from the comparable corpus in the other language (target).
We only used a limited amount of human-provided bilingual resources.
Starting with about a total 2.6M words of sentence aligned bilingual data and a bilingual dictionary, large amounts of monolingual data are translated.
These translations are then employed to find the corresponding matching sentences in the target side corpus, using information retrieval methods.
Simple filters are used to determine whether the retrieved sentences are parallel or not.
By adding these retrieved parallel sentences to already available human translated parallel corpora we were able to improve the BLEU score on the test set by almost 2.5 points.
Almost one point BLEU of this improvement was obtained by removing additional words at the end of the aligned sentences in the target language.
Contrary to the previous approaches as in (Munteanu and Marcu, 2005) which used small amounts of in-domain parallel corpus as an initial resource, our system exploits the target language side of the comparable corpus to attain the same goal, thus the comparable corpus itself helps to better extract possible parallel sentences.
The Gigaword comparable corpora were used in this paper, but the same approach can be extended to extract parallel sentences from huge amounts of corpora available on the web by identifying comparable articles using techniques such as (Yang and Li, 2003) and (Resnik and Y, 2003).
This technique is particularly useful for language pairs for which very little parallel corpora exist.
Other probable sources of comparable corpora to be exploited include multilingual encyclopedias like Wikipedia, encyclopedia Encarta etc.
There also exist domain specific comparable corpora (which are probably potentially parallel), like the documentations that are done in the national/regional language as well as English, or the translations of many English research papers in French or some other language used for academic proposes.
We are currently working on several extensions of the procedure described in this paper.
We will investigate whether the same findings hold for other tasks and language pairs, in particular translating from Arabic to English, and we will try to compare our approach with the work of Munteanu and Marcu (2005).
The simple filters that we are currently using seem to be effective, but we will also test other criteria than the WER and TER.
Finally, another interesting direction is to iterate the process.
The extracted additional bitexts could be used to build an SMT system that is better optimized on the Gigaword corpus, to translate again all the sentence from French to English, to perform IR and the filtering and to extract new, potentially improved, parallel texts.
Starting with some million words of bitexts, this process may allow to build at the end an SMT system that achieves the same performance than we obtained using about 40M words of human-translated bi-texts (news-commentary + Europarl).
<newSection> 6 Acknowledgments This work was partially supported by the Higher Education Commission, Pakistan through the HEC Overseas Scholarship 2005 and the French Government under the project INSTAR (ANR JCJC06 143038).
Some of the baseline SMT systems used in this work were developed in a cooperation between the University of Le Mans and the company SYSTRAN.
<newSection> References<newSection> Abstract In this paper, we explore ways of improving an inference rule collection and its application to the task of recognizing textual entailment.
For this purpose, we start with an automatically acquired collection and we propose methods to refine it and obtain more rules using a hand-crafted lexical resource.
Following this, we derive a dependency-based structure representation from texts, which aims to provide a proper base for the inference rule application.
The evaluation of our approach on the recognizing textual entailment data shows promising results on precision and the error analysis suggests possible improvements.
<newSection> 1 Introduction Textual inference plays an important role in many natural language processing (NLP) tasks.
In recent years, the recognizing textual entailment (RTE) (Dagan et al., 2006) challenge, which focuses on detecting semantic inference, has attracted a lot of attention.
Given a text T (several sentences) and a hypothesis H (one sentence), the goal is to detect if H can be inferred from T.
Studies such as (Clark et al., 2007) attest that lexical substitution (e.g. synonyms, antonyms) or simple syntactic variation account for the entailment only in a small number of pairs.
Thus, one essential issue is to identify more complex expressions which, in appropriate contexts, convey the same (or similar) meaning.
However, more generally, we are also interested in pairs of expressions in which only a uni-directional inference relation holds1.
A typical example is the following RTE pair in which accelerate to in H is used as an alternative formulation for reach speed of in T.
T: The high-speed train, scheduled for a trial run on Tuesday, is able to reach a maximum speed of up to 430 kilometers per hour, or 119 meters per second.
H: The train accelerates to 430 kilometers per hour.
One way to deal with textual inference is through rule representation, for example X wrote Y Pz� X is author of Y.
However, manually building collections of inference rules is time-consuming and it is unlikely that humans can exhaustively enumerate all the rules encoding the knowledge needed in reasoning with natural language.
Instead, an alternative is to acquire these rules automatically from large corpora.
Given such a rule collection, the next step to focus on is how to successfully use it in NLP applications.
This paper tackles both aspects, acquiring inference rules and using them for the task of recognizing textual entailment.
For the first aspect, we extend and refine an existing collection of inference rules acquired based on the Distributional Hypothesis (DH).
One of the main advantages of using the DH is that the only input needed is a large corpus of (parsed) text2.
For the extension and refinement, a hand-crafted lexical resource is used for augmenting the original inference rule collection and exclude some of the incorrect rules.
For the second aspect, we focus on applying these rules to the RTE task.
In particular, we use a structure representation derived from the dependency parse trees of T and H, which aims to capture the essential information they convey.
The rest of the paper is organized as follows: Section 2 introduces the inference rule collection we use, based on the Discovery of Inference Rules from Text (henceforth DIRT) algorithm and discusses previous work on applying it to the RTE task.
Section 3 focuses on the rule collection itself and on the methods in which we use an external lexical resource to extend and refine it.
Section 4 discusses the application of the rules for the RTE data, describing the structure representation we use to identify the appropriate context for the rule application.
The experimental results will be presented in Section 5, followed by an error analysis and discussions in Section 6.
Finally Section 7 will conclude the paper and point out future work directions.
<newSection> 2 Background A number of automatically acquired inference rule/paraphrase collections are available, such as (Szpektor et al., 2004), (Sekine, 2005).
In our work we use the DIRT collection because it is the largest one available and it has a relatively good accuracy (in the 50% range for top generated paraphrases, (Szpektor et al., 2007)).
In this section, we describe the DIRT algorithm for acquiring inference rules.
Following that, we will overview the RTE systems which take DIRT as an external knowledge resource.
The DIRT algorithm has been introduced by (Lin and Pantel, 2001) and it is based on what is called the Extended Distributional Hypothesis.
The original DH states that words occurring in similar contexts have similar meaning, whereas the extended version hypothesizes that phrases occurring in similar contexts are similar.
An inference rule in DIRT is a pair of binary relations ( pattern1(X, Y ), pattern2(X, Y ) ) which stand in an inference relation.
pattern1 and pattern2 are chains in dependency trees3 while X and Y are placeholders for nouns at the end of this chain.
The two patterns will constitute a candidate paraphrase if the sets of X and Y values exhibit relevant overlap.
In the following example, the two patterns are prevent and provide protection against.
Such rules can be informally defined (Szpek-tor et al., 2007) as directional relations between two text patterns with variables.
The left-handside pattern is assumed to entail the right-handside pattern in certain contexts, under the same variable instantiation.
The definition relaxes the intuition of inference, as we only require the entailment to hold in some and not all contexts, motivated by the fact that such inferences occur often in natural text.
The algorithm does not extract directional inference rules, it can only identify candidate paraphrases; many of the rules are however uni-directional.
Besides syntactic rewriting or lexical rules, rules in which the patterns are rather complex phrases are also extracted.
Some of the rules encode lexical relations which can also be found in resources such as WordNet while others are lexical-syntactic variations that are unlikely to occur in hand-crafted resources (Lin and Pan-tel, 2001).
Table 1 gives a few examples of rules present in DIRT4.
Current work on inference rules focuses on making such resources more precise.
(Basili et al., 2007) and (Szpektor et al., 2008) propose attaching selectional preferences to inference rules.
These are semantic classes which correspond to the anchor values of an inference rule and have the role of making precise the context in which the rule can be applied 5.
This aspect is very important and we plan to address it in our future work.
However in this paper we investigate the first and more basic issue: how to successfully use rules in their current form.
<newSection> 2.2 Related Work Intuitively such inference rules should be effective for recognizing textual entailment.
However, only a small number of systems have used DIRT as a resource in the RTE-3 challenge, and the experimental results have not fully shown it has an important contribution.
In (Clark et al., 2007)’s approach, semantic parsing to clause representation is performed and true entailment is decided only if every clause in the semantic representation of T semantically matches some clause in H.
The only variation allowed consists of rewritings derived from WordNet and DIRT.
Given the preliminary stage of this system, the overall results show very low improvement over a random classification baseline.
(Bar-Haim et al., 2007) implement a proof system using rules for generic linguistic structures, lexical-based rules, and lexical-syntactic rules (these obtained with a DIRT-like algorithm on the first CD of the Reuters RCV1 corpus).
The entailment considers not only the strict notion of proof but also an approximate one.
Given premise p and hypothesis h, the lexical-syntactic component marks all lexical noun alignments.
For every pair of alignment, the paths between the two nouns are extracted, and the DIRT algorithm is applied to obtain a similarity score.
If the score is above a threshold the rule is applied.
However these lexical-syntactic rules are only used in about 3% of the attempted proofs and in most cases there is no lexical variation.
(Iftene and Balahur-Dobrescu, 2007) use DIRT in a more relaxed manner.
A DIRT rule is employed in the system if at least one of the anchors match in T and H, i.e. they use them as unary rules.
However, the detailed analysis of the system that they provide shows that the DIRT component is the least relevant one (adding 0.4% of precision).
In (Marsi et al., 2007), the focus is on the usefulness of DIRT.
In their system a paraphrase substitution step is added on top of a system based on a tree alignment algorithm.
The basic paraphrase substitution method follows three steps.
Initially, the two patterns of a rule are matched in T and H (instantiations of the anchors X, Y do not have to match).
The text tree is transformed by applying the paraphrase substitution.
Following this, the transformed text tree and hypothesis trees are aligned.
The coverage (proportion of aligned content words) is computed and if above some threshold, entailment is true.
The paraphrase component adds 1.0% to development set results and only 0.5% to test sets, but a more detailed analysis on the results of the interaction with the other system components is not given.
<newSection> 3 Extending and refining DIRT Based on observations of using the inference rule collection on the real data, we discover that 1) some of the needed rules still lack even in a very large collection such as DIRT and 2) some systematic errors in the collection can be excluded.
On both aspects, we use WordNet as additional lexical resource.
A closer look into the RTE data reveals that DIRT lacks many of the rules that entailment pairs require.
Table 2 lists a selection of such rules.
The first rows contain rules which are structurally very simple.
These, however, are missing from DIRT and most of them also from other hand-crafted resources such as WordNet (i.e. there is no short path connecting the two verbs).
This is to be expected as they are rules which hold in specific contexts, but difficult to be captured by a sense distinction of the lexical items involved.
The more complex rules are even more difficult to capture with a DIRT-like algorithm.
Some of these do not occur frequently enough even in large amounts of text to permit acquiring them via the DH.
In order to address the issue of missing rules, we investigate the effects of combining DIRT with an exact hand-coded lexical resource in order to create new rules.
For this we extended the DIRT rules by adding based on DIRT rule X face threat of Y —* X at risk of Y rules in which any of the lexical items involved in the patterns can be replaced by WordNet synonyms.
In the example above, we consider the DIRT rule X face threat of Y —* X, at risk of Y (Table 3).
Of course at this moment due to the lack of sense disambiguation, our method introduces lots of rules that are not correct.
As one can see, expressions such as front scourge do not make any sense, therefore any rules containing this will be incorrect.
However some of the new rules created in this example, such as X face threat of Y Pz� X, at danger of Y are reasonable ones and the rules which are incorrect often contain patterns that are very unlikely to occur in natural text.
The idea behind this is that a combination of various lexical resources is needed in order to cover the vast variety of phrases which humans can judge to be in an inference relation.
The method just described allows us to identify the first four rules listed in Table 2.
We also acquire the rule X face menace of Y Pz� X endangered by Y (via X face threat of Y Pz� X threatened by Y, menace Pz� threat, threaten Pz� endanger).
Our extension is application-oriented therefore it is not intended to be evaluated as an independent rule collection, but in an application scenario such as RTE (Section 6).
In our experiments we also made a step towards removing the most systematic errors present in DIRT.
DH algorithms have the main disadvantage that not only phrases with the same meaning are extracted but also phrases with opposite meaning.
In order to overcome this problem and since such errors are relatively easy to detect, we applied a filter to the DIRT rules.
This eliminates inference rules which contain WordNet antonyms.
For such a rule to be eliminated the two patterns have to be identical (with respect to edge labels and content words) except from the antonymous words; an example of a rule eliminated this way is X have confidence in Y Pz� X lack confidence in Y.
As pointed out by (Szpektor et al., 2007) a thorough evaluation of a rule collection is not a trivial task; however due to our methodology we can assume that the percentage of rules eliminated this way that are indeed contradictions gets close to 100%.
<newSection> 4 Applying DIRT on RTE In this section we point out two issues that are encountered when applying inference rules for textual entailment.
The first issue is concerned with correctly identifying the pairs in which the knowledge encoded in these rules is needed.
Following this, another non-trivial task is to determine the way this knowledge interacts with the rest of information conveyed in an entailment pair.
In order to further investigate these issues, we apply the rule collection on a dependency-based representation of text and hypothesis, namely Tree Skeleton.
A straightforward experiment can reveal the number of pairs in the RTE data which contain rules present in DIRT.
For all the experiments in this paper, we use the DIRT collection provided by (Lin and Pantel, 2001), derived from the DIRT algorithm applied on 1GB of news text.
The results we report here use only the most confident rules amounting to more than 4 million rules (top 40 following (Lin and Pantel, 2001)).6 Following the definition of an entailment rule, we identify RTE pairs in which patterns(w1, w2) and pattern2(w1, w2) are matched one in T and the other one in H and (patterns(X, Y ), pattern2(X, Y)) is an inference rule.
The pair bellow is an example of this.
On average, only 2% of the pairs in the RTE data is subject to the application of such inference rules.
Out of these, approximately 50% are lexical rules (one verb entailing the other).
Out of these lexical rules, around 50% are present in WordNet in a synonym, hypernym or sister relation.
At a manual analysis, close to 80% of these are correct rules; this is higher than the estimated accuracy of DIRT, probably due to the bias of the data which consists of pairs which are entailment candidates.
However, given the small number of inference rules identified this way, we performed another analysis.
This aims at determining an upper bound of the number of pairs featuring entailment phrases present in a collection.
Given DIRT and the RTE data, we compute in how many pairs the two patterns of a paraphrase can be matched irrespective of their anchor values.
An example is the following pair, This is a case in which the rule is correct and the entailment is positive.
In order to determine this, a system will have to know that Libya’s case against Britain and the US in T entails one case in H.
Similarly, in this context, the dispute over their demand for extradition of Libyans charged with blowing up a Pan Am jet over Lockerbie in 1988 in T can be replaced with the extradition of Libyan suspects in the Pan Am Lockerbie bombing preserving the meaning.
Altogether in around 20% of the pairs, patterns of a rule can be found this way, many times with more than one rule found in a pair.
However, in many of these pairs, finding the patterns of an inference rule does not imply that the rule is truly present in that pair.
Considering a system is capable of correctly identifying the cases in which an inference rule is needed, subsequent issues arise from the way these fragments of text interact with the surrounding context.
Assuming we have a correct rule present in an entailment pair, the cases in which the pair is still not a positive case of entailment can be summarized as follows: • The entailment rule is present in parts of the text which are not relevant to the entailment value of the pair.
• The rule is relevant, however the sentences in which the patterns are embedded block the entailment (e.g. through negative markers, modifiers, embedding verbs not preserving entailment)7 • The rule is correct in a limited number of contexts, but the current context is not the correct one.
To sum up, making use of the knowledge encoded with such rules is not a trivial task.
If rules are used strictly in concordance with their definition, their utility is limited to a very small number of entailment pairs.
For this reason, 1) instead of forcing the anchor values to be identical as most previous work, we allow more flexible rule matching (similar to (Marsi et al., 2007)) and 2) furthermore, we control the rule application process using a text representation based on dependency structure.
The Tree Skeleton (TS) structure was proposed by (Wang and Neumann, 2007), and can be viewed as an extended version of the predicate-argument structure.
Since it contains not only the predicate and its arguments, but also the dependency paths in-between, it captures the essential part of the sentence.
Following their algorithm, we first preprocess the data using a dependency parser8 and then select overlapping topic words (i.e. nouns) in T and H.
By doing so, we use fuzzy match at the substring level instead of full match.
Starting with these nouns, we traverse the dependency tree to identify the lowest common ancestor node (named as root node).
This sub-tree without the inner yield is defined as a Tree Skeleton.
Figure 1 shows the TS of T of the following positive example, paths contained in a TS should also be two.
In practice, among all the 800 T-H pairs of the RTE-2 test set, we successfully extracted tree skeletons in 296 text pairs, i.e., 37% of the test data is covered by this step and results on other data sets are similar.
Applying DIRT on a TS Dependency representations like the tree skeleton have been explored by many researchers, e.g. (Zanzotto and Moschitti, 2006) have utilized a tree kernel method to calculate the similarity between T and H, and (Wang and Neumann, 2007) chose subsequence kernel to reduce the computational complexity.
However, the focus of this paper is to evaluate the application of inference rules on RTE, instead of exploring methods of tackling the task itself.
Therefore, we performed a straightforward matching algorithm to apply the inference rules on top of the tree skeleton structure.
Given tree skeletons of T and H, we check if the two left dependency paths, the two right ones or the two root nodes contain the patterns of a rule.
In the example above, the rule X obj ←−− su bj obj2 o receive −−−→ Y ≈ X ← award −−−→bj1 Y satisfies this criterion, as it is matched at the root nodes.
Notice that the rule is correct only in restricted contexts, in which the object of receive is something which is conferred on the basis of merit.
However in this pair, the context is indeed the correct one.
<newSection> 5 Experiments Our experiments consist in predicting positive entailment in a very straightforward rule-based manner (Table 4 summarizes the results using three different rule collections).
For each collection we select the RTE pairs in which we find a tree skeleton and match an inference rule.
The first number in our table entries represents how many of such pairs we have identified, out the 1600 of development and test pairs.
For these pairs we simply predict positive entailment and the second entry represents what percentage of these pairs are indeed positive entailment.
Our work does not focus on building a complete RTE system; however, we also combine our method with a bag of words baseline to see the effects on the whole data set.
In the first two columns (DirtTS and Dirt+WNTS) we consider DIRT in its original state and DIRT with rules generated with WordNet as described in Section 3; all precisions are higher than 67%9.
After adding WordNet, approximately in twice as many pairs, tree skeletons and rules are matched, while the precision is not harmed.
This may indicate that our method of adding rules does not decrease precision of an RTE system.
In the third column we report the results of using a set of rules containing only the trivial identity ones (IdTS).
For our current system, this can be seen as a precision upper bound for all the other collections, in concordance with the fact that identical rules are nothing but inference rules of highest possible confidence.
The fourth column (Dirt+Id+WNTS) contains what can be considered our best setting.
In this setting considerably more pairs are covered using a collection containing DIRT and identity rules with WordNet extension.
Although the precision results with this setting are encouraging (65% for RTE2 data and 72% for RTE3 data), the coverage is still low, 8% for RTE2 and 6% for RTE3.
This aspect together with an error analysis we performed are the focus of Section 7.
The last column (Dirt+Id+WN) gives the precision we obtain if we simply decide a pair is true entailment if we have an inference rule matched in it (irrespective of the values of the anchors or of the existence of tree skeletons).
As expected, only identifying the patterns of a rule in a pair irrespective of tree skeletons does not give any indication of the entailment value of the pair.
At last, we also integrate our method with a bag of words baseline, which calculates the ratio of overlapping words in T and H.
For the pairs that our method covers, we overrule the baseline’s decision.
The results are shown in Table 6 (Main stands for the Dirt + Id + WNTS configuration).
On the full data set, the improvement is still small due to the low coverage of our method, however on the pairs that are covered by our method (Ta-ble 5), there is a significant improvement over the overlap baseline.
<newSection> 6 Discussion In this section we take a closer look at the data in order to better understand how does our method of combining tree skeletons and inference rules work.
We will first perform error analysis on what we have considered our best setting so far.
Following this, we analyze data to identify the main reasons which cause the low coverage.
For error analysis we consider the pairs incorrectly classified in the RTE3 test data set, consisting of a total of 25 pairs.
We classify the errors into three main categories: rule application errors, inference rule errors, and other errors (Table 7).
In the first category, the tree skeleton fails to match the corresponding anchors of the inference rules.
For instance, if someone founded the Institute of Mathematics (Instituto di Matematica) at the University ofMilan, it does not follow that they founded The University of Milan.
The Institute of Mathematics should be aligned with the University of Milan, which should avoid applying the in-A rather small portion of the errors (16%) are caused by incorrect inference rules.
Out of these, two are correct in some contexts but not in the entailment pairs in which they are found.
For example, the following rule X generate Y Pt� X earn Y is used incorrectly, however in the restricted context of money or income, the two verbs have similar meaning.
An example of an incorrect rule is X issue Y Pt� X hit Y since it is difficult to find a context in which this holds.
The last category contains all the other errors.
In all these cases, the additional information conveyed by the text or the hypothesis which cannot be captured by our current approach, affects the entailment.
For example an imitation diamond is not a diamond, and more than 1,000 members of the Russian and foreign media does not entail more than 1,000 members from Russia; these are not trivial, since lexical semantics and fine-grained analysis of the restrictors are needed.
For the second part of our analysis we discuss the coverage issue, based on an analysis of uncovered pairs.
A main factor in failing to detect pairs in which entailment rules should be applied is the fact that the tree skeleton does not find the corresponding lexical items of two rule patterns.
Issues will occur even if the tree skeleton structure is modified to align all the corresponding fragments together.
Consider cases such as threaten to boycott and boycott or similar constructions with other embedding verbs such as manage, forget, attempt.
Our method can detect if the two embedded verbs convey a similar meaning, however not how the embedding verbs affect the implication.
Independent of the shortcomings of our tree skeleton structure, a second factor in failing to detect true entailment still lies in lack of rules.
For instance, the last two examples in Table 2 are entailment pair fragments which can be formulated as inference rules, but it is not straightforward to acquire them via the DH.
<newSection> 7 Conclusion Throughout the paper we have identified important issues encountered in using inference rules for textual entailment and proposed methods to solve them.
We explored the possibility of combining a collection obtained in a statistical, unsupervised manner, DIRT, with a hand-crafted lexical resource in order to make inference rules have a larger contribution to applications.
We also investigated ways of effectively applying these rules.
The experiment results show that although coverage is still not satisfying, the precision is promising.
Therefore our method has the potential to be successfully integrated in a larger entailment detection framework.
The error analysis points out several possible future directions.
The tree skeleton representation we used needs to be enhanced in order to capture more accurately the relevant fragments of the text.
A different issue remains the fact that a lot of rules we could use for textual entailment detection are still lacking.
A proper study of the limitations of the DH as well as a classification of the knowledge we want to encode as inference rules would be a step forward towards solving this problem.
Furthermore, although all the inference rules we used aim at recognizing positive entailment cases, it is natural to use them for detecting negative cases of entailment as well.
In general, we can identify pairs in which the patterns of an inference rule are present but the anchors are mismatched, or they are not the correct hypernym/hyponym relation.
This can be the base of a principled method for detecting structural contradictions (de Marn-effe et al., 2008).
<newSection> 8 Acknowledgments We thank Dekang Lin and Patrick Pantel for providing the DIRT collection and to Grzegorz Chrupała, Alexander Koller, Manfred Pinkal and Stefan Thater for very useful discussions.
Geor-giana Dinu and Rui Wang are funded by the IRTG and PIRE PhD scholarship programs.
<newSection> References<newSection> Abstract Building on work detecting errors in dependency annotation, we set out to correct local dependency errors.
To do this, we outline the properties of annotation errors that make the task challenging and their existence problematic for learning.
For the task, we define a feature-based model that explicitly accounts for non-relations between words, and then use ambiguities from one model to constrain a second, more relaxed model.
In this way, we are successfully able to correct many errors, in a way which is potentially applicable to dependency parsing more generally.
<newSection> 1 Introduction and Motivation Annotation error detection has been explored for part-of-speech (POS), syntactic constituency, semantic role, and syntactic dependency annotation (see Boyd et al., 2008, and references therein).
Such work is extremely useful, given the harmfulness of annotation errors for training, including the learning of noise (e.g., Hogan, 2007; Habash et al., 2007), and for evaluation (e.g., Padro and Marquez, 1998).
But little work has been done to show the full impact of errors, or what types of cases are the most damaging, important since noise can sometimes be overcome (cf. Osborne, 2002).
Likewise, it is not clear how to learn from consistently misannotated data; studies often only note the presence of errors or eliminate them from evaluation (e.g., Hogan, 2007), and a previous attempt at correction was limited to POS annotation (Dickinson, 2006).
By moving from annotation error detection to error correction, we can more fully elucidate ways in which noise can be overcome and ways it cannot.
We thus explore annotation error correction and its feasibility for dependency annotation, a form of annotation that provides argument relations among words and is useful for training and testing dependency parsers (e.g., Nivre, 2006; McDonald and Pereira, 2006).
A recent innovation in dependency parsing, relevant here, is to use the predictions made by one model to refine another (Nivre and McDonald, 2008; Torres Martins et al., 2008).
This general notion can be employed here, as different models of the data have different predictions about whch parts are erroneous and can highlight the contributions of different features.
Using differences that complement one another, we can begin to sort accurate from inaccurate patterns, by integrating models in such a way as to learn the true patterns and not the errors.
Although we focus on dependency annotation, the methods are potentially applicable for different types of annotation, given that they are based on the similar data representations (see sections 2.1 and 3.2).
In order to examine the effects of errors and to refine one model with another’s information, we need to isolate the problematic cases.
The data representation must therefore be such that it clearly allows for the specific identification of errors between words.
Thus, we explore relatively simple models of the data, emphasizing small sub-structures (see section 3.2).
This simple modeling is not always rich enough for full dependency parsing, but different models can reveal conflicting information and are generally useful as part of a larger system.
Graph-based models of dependency parsing (e.g., McDonald et al., 2006), for example, rely on breaking parsing down into decisions about smaller substructures, and focusing on pairs of words has been used for domain adaptation (Chen et al., 2008) and in memory-based parsing (Canisius et al., 2006).
Exploring annotation error correction in this way can provide insights into more general uses of the annotation, just as previous work on correction for POS annotation (Dickinson, 2006) led to a way to improve POS tagging (Dickinson, 2007).
After describing previous work on error detection and correction in section 2, we outline in section 3 how we model the data, focusing on individual relations between pairs of words.
In section 4, we illustrate the difficulties of error correction and show how simple combinations of local features perform poorly.
Based on the idea that ambiguities from strict, lexical models can constrain more general POS models, we see improvement in error correction in section 5.
<newSection> 2 Background 2.1 Error detection We base our method of error correction on a form of error detection for dependency annotation (Boyd et al., 2008).
The variation n-gram approach was developed for constituency-based treebanks (Dickinson and Meurers, 2003, 2005) and it detects strings which occur multiple times in the corpus with varying annotation, the so-called variation nuclei.
For example, the variation nucleus next Tuesday occurs three times in the Wall Street Journal portion of the Penn Treebank (Tay-lor et al., 2003), twice labeled as NP and once as PP (Dickinson and Meurers, 2003).
Every variation detected in the annotation of a nucleus is classified as either an annotation error or as a genuine ambiguity.
The basic heuristic for detecting errors requires one word of recurring context on each side of the nucleus.
The nucleus with its repeated surrounding context is referred to as a variation n-gram.
While the original proposal expanded the context as far as possible given the repeated n-gram, using only the immediately surrounding words as context is sufficient for detecting errors with high precision (Boyd et al., 2008).
This “shortest” context heuristic receives some support from research on first language acquisition (Mintz, 2006) and unsupervised grammar induction (Klein and Manning, 2002).
The approach can detect both bracketing and la-beling errors in constituency annotation, and we already saw a labeling error for next Tuesday.
As an example of a bracketing error, the variation nucleus last month occurs within the NP its biggest jolt last month once with the label NP and once as a non-constituent, which in the algorithm is handled through a special label NIL.
The method for detecting annotation errors can be extended to discontinuous constituency annotation (Dickinson and Meurers, 2005), making it applicable to dependency annotation, where words in a relation can be arbitrarily far apart.
Specifically, Boyd et al.
(2008) adapt the method by treating dependency pairs as variation nuclei, and they include NIL elements for pairs of words not annotated as a relation.
The method is successful at detecting annotation errors in corpora for three different languages, with precisions of 93% for Swedish, 60% for Czech, and 48% for German.1 Correcting POS annotation errors can be done by applying a POS tagger and altering the input POS tags (Dickinson, 2006).
Namely, ambiguity class information (e.g., IN/RB/RP) is added to each corpus position for training, creating complex ambiguity tags, such as <IN/RB/RP,IN>.
While this results in successful correction, it is not clear how it applies to annotation which is not positional and uses NIL labels.
However, ambiguity class information is relevant when there is a choice between labels; we return to this in section 5.
<newSection> 3 Modeling the data For our data set, we use the written portion (sec-tions P and G) of the Swedish Talbanken05 treebank (Nivre et al., 2006), a reconstruction of the Talbanken76 corpus (Einarsson, 1976) The written data of Talbanken05 consists of 11,431 sentences with 197,123 tokens, annotated using 69 types of dependency relations.
This is a small sample, but it matches the data used for error detection, which results in 634 shortest non-fringe variation n-grams, corresponding to 2490 tokens.
From a subset of 210 nuclei (917 tokens), hand-evaluation reveals error detection precision to be 93% (195/210), with 274 (of the 917) corpus positions in need of correction (Boyd et al., 2008).
This means that 643 positions do not need to be corrected, setting a baseline of 70.1% (643/917) for error correction.2 Following Dickinson (2006), we train our models on the entire corpus, explicitly including NIL relations (see section 3.2); we train on the original annotation, but not the corrections.
Annotation error correction involves overcoming noise in the corpus, in order to learn the true patterns underlying the data.
This is a slightly different goal from that of general dependency parsing methods, which often integrate a variety of features in making decisions about dependency relations (cf., e.g., Nivre, 2006; McDonald and Pereira, 2006).
Instead of maximizing a feature model to improve parsing, we isolate individual pieces of information (e.g., context POS tags), thereby being able to pinpoint, for example, when non-local information is needed for particular types of relations and pointing to cases where pieces of information conflict (cf. also McDonald and Nivre, 2007).
To support this isolation of information, we use dependency pairs as the basic unit of analysis and assign a dependency label to each word pair.
Following Boyd et al.
(2008), we add L or R to the label to indicate which word is the head, the left (L) or the right (R).
This is tantamount to handling pairs of words as single entries in a “lex-icon” and provides a natural way to talk of ambiguities.
Breaking the representation down into strings whch receive a label also makes the method applicable to other annotation types (e.g., Dickin-son and Meurers, 2005).
A major issue in generating a lexicon is how to handle pairs of words which are not dependencies.
We follow Boyd et al.
(2008) and generate NIL labels for those pairs of words which also occur as a true labeled relation.
In other words, only word pairs which can be relations can also be NILs.
For every sentence, then, when we produce feature lists (see section 3.3), we produce them for all word pairs that are related or could potentially be related, but not those which have never been observed as a dependency pair.
This selection of NIL items works because there are no unknown words.
We use the method in Dickinson and Meur-ers (2005) to efficiently calculate the NIL tokens.
Focusing on word pairs and not attempting to build a a whole dependency graph allows us to explore the relations between different kinds of features, and it has the potential benefit of not relying on possibly erroneous sister relations.
From the perspective of error correction, we cannot assume that information from the other relations in the sentence is reliable.3 This representation also fits nicely with previous work, both in error detection (see section 2.1) and in dependency parsing (e.g., Canisius et al., 2006; Chen et al., 2008).
Most directly, Canisius et al.
(2006) integrate such a representation into a memory-based dependency parser, treating each pair individually, with words and POS tags as features.
We employ memory-based learning (MBL) for correction.
MBL stores all corpus instances as vectors of features, and given a new instance, the task of the classifier is to find the most similar cases in memory to deduce the best class.
Given the previous discussion of the goals of correcting errors, what seems to be needed is a way to find patterns which do not fully generalize because of noise appearing in very similar cases in the corpus.
As Zavrel et al.
(1997, p. 137) state about the advantages of MBL: Because language-processing tasks typically can only be described as a complex interaction of regularities, sub-regularities and (families of) exceptions, storing all empirical data as potentially useful in analogical extrapolation works better than extracting the main regularities and forgetting the individual examples (Daelemans, 1996).
By storing all corpus examples, as MBL does, both correct and incorrect data is maintained, allowing us to pinpoint the effect of errors on training.
For our experiments, we use TiMBL, version 6.1 (Daelemans et al., 2007), with the default settings.
We use the default overlap metric, as this maintains a direct connection to majority-based correction.
We could run TiMBL with different values of k, as this should lead to better feature integration.
However, this is difficult to explore without development data, and initial experiments with higher k values were not promising (see section 4.2).
To fully correct every error, one could also experiment with a real dependency parser in the future, in order to look beyond the immediate context and to account for interactions between rela-3We use POS information, which is also prone to errors, but on a different level of annotation.
Still, this has its problems, as discussed in section 4.1. tions.
The approach to correction pursued here, however, isolates problems for assigning dependency structures, highlighting the effectiveness of different features within the same local domain.
Initial experiments with a dependency parser were again not promising (see section 4.2).
When using features for individual relations, we have different options for integrating them.
On the one hand, one can simply additively combine features into a larger vector for training, as described in section 4.2.
On the other hand, one can use one set of features to constrain another set, as described in section 5.
Pulling apart the features commonly employed in dependency parsing can help indicate the contributions each has on the classification.
This general idea is akin to the notion of classifier stacking, and in the realm of dependency parsing, Nivre and McDonald (2008) successfully stack classifiers to improve parsing by “allow[ing] a model to learn relative to the predictions of the other” (p. 951).
The output from one classifier is used as a feature in the next one (see also Tor-res Martins et al., 2008).
Nivre and McDonald (2008) use different kinds of learning paradigms, but the general idea can be carried over to a situation using the same learning mechanism.
Instead of focusing on what one learning algorithm informs another about, we ask what one set of more or less informative features can inform another set about, as described in section 5.1.
<newSection> 4 Performing error correction The task of automatic error correction in some sense seems straightforward, in that there are no unknown words.
Furthermore, we are looking at identical recurring words, which should for the most part have consistent annotation.
But it is precisely this similarity of local contexts that makes the correction task challenging.
Given that variations contain sets of corpus positions with differing labels, it is tempting to take the error detection output and use a heuristic of “majority rules” for the correction cases, i.e., correct the cases to the majority label.
When using only information from the word sequence, this runs into problems quickly, however, in that there are many non-majority labels which are correct.
Some of these non-majority cases pattern in uniform ways and are thus more correctable; others are less tractable in being corrected, as they behave in non-uniform and often non-local ways.
Exploring the differences will highlight what can and cannot be easily corrected, underscoring the difficulties in training from erroneous annotation.
Uniform non-majority cases The first problem with correction to the majority label is an issue of coverage: a large number of variations are ties between two different labels.
Out of 634 shortest non-fringe variation nuclei, 342 (53.94%) have no majority label; for the corresponding 2490 tokens, 749 (30.08%) have no majority tag.
The variation ¨ar v¨ag (’is way’), for example, appears twice with the same local context shown in (1),4 once incorrectly labeled as OO-L (other object [head on the left]) and once correctly as SP-L (subjective predicative complement).
To distinguish these two, more information is necessary than the exact sequence of words.
In this case, for example, looking at the POS categories of the nuclei could potentially lead to accurate correction: AV NN is SP-L 1032 times and OO-L 32 times (AV = the verb “vara” (be), NN = other noun).
While some ties might require non-local information, we can see that local—but more general— information could accurately break this tie.
Secondly, in a surprising number of cases where there is a majority tag (122 out of the 917 tokens we have a correction for), a non-majority label is actually correct.
For the example in (2), the string institution kvarleva (‘institution remnant’) varies between CC-L (sister of first conjunct in binary branching analysis of coordination) and AN-L (apposition).5 CC-L appears 5 times and AN-L 3 times, but the CC-L cases are incorrect and need to be changed to AN-L.
(2) en f¨or˚aldrad institution/NN an obsolete institution kvarleva/NN fr˚an 1800-talets remnant from the 1800s Other cases with a non-majority label have other problems.
In example (3), for instance, the string under h¨agnet (‘under protection’) varies in this context between HD-L (other head, 3 cases) and PA-L (complement of preposition, 5 cases), where the PA-L cases need to be corrected to HD-L.
Both of these categories are new, so part of the issue here could be in the consistency of the conversion.
the other is idiomatic, despite having identical local context.
In these examples, at least the POS labels are different.
Note, though, that in (4) we need to trust the POS labels to overcome the similarity of text, and in (3) we need to distrust them.6 with other words an ¨andam˚alsenlig ...
appropriate (3) fria liv under/PR h¨agnet/ID|NN b.
Med/AB andra ord/ID en form av free life under the protection with other words a form of av/IDJPR ett en g˚ang givet l¨ofte prostitution . of a one time given promise prostitution The additional problem is that there are other, correlated errors in the analysis, as shown in figure 1.
In the case of the correct HD analysis, both h¨agnet and av are POS-annotated as ID (part of idiom (multi-word unit)) and are HD dependents of under, indicating that the three words make up an idiom.
The PA analysis is a non-idiomatic analysis, with h¨agnet as NN.
Without non-local information, some legitimate variations are virtually irresolvable.
Consider (5), for instance: here, we find variation between SS-R (other subject), as in (5a), and FS-R (dummy subject), as in (5b).
Crucially, the POS tags are the same, and the context is the same.
What differen-tiates these cases is that g˚ar has a different set of dependents in the two sentences, as shown in figure 2; to use this information would require us to trust the rest of the dependency structure or to use a dependency parser which accurately derives the structural differences.
Significantly, h¨agnet only appears 10 times in the corpus, all with under as its head, 5 times HD-L and 5 times PA-L.
We will not focus explicitly on correcting these types of cases, but the example serves to emphasize the necessity of correction at all levels of annotation.
Non-uniform non-majority cases All of the above cases have in common that whatever change is needed, it needs to be done for all positions in a variation.
But this is not sound, as error detection precision is not 100%.
Thus, there are variations which clearly must not change.
For example, in (4), there is legitimate variation between PA-L (4a) and HD-L (4b), stemming from the fact that one case is non-idiomatic, and While some variations require non-local information, we have seen that some cases are correctable simply with different kinds of local information (cf. (1)).
In this paper, we will not attempt to directly cover non-local cases or cases with POS annotation problems, instead trying to improve the integration of different pieces of local information.
In our experiments, we trained simple models of the original corpus using TiMBL (see section 3.3) and then tested on the same corpus.
The models we use include words (W) and/or tags (T) for nucleus and/or context positions, where context here refers only to the immediately surrounding words.
These are outlined in table 1, for different models of the nucleus (Nuc.) and the context (Con.).
For instance, the model 6 representation of example (6) (=(1)) consists of all the underlined words and tags.
(6) k¨arlekens v¨ag/NN ¨ar/AV en/EN l˚ang/AJ v¨ag/NN och/++ man g¨or oklokt ...
In table 1, we report the precision figures for different models on the 917 positions we have corrections for.
We report the correction precision for positions the classifier changed the label of (Changed), and the overall correction precision (Overall).
We also report the precision TiMBL has for the whole corpus, with respect to the original tags (instead of the corrected tags).
We can draw a few conclusions from these results.
First, all models using contexual information perform essentially the same—approximately 50% on changed positions and 73% overall.
When not generalizing to new data, simply adding features (i.e., words or tags) to the model is less important than the sheer presence of context.
This is true even for some higher values of k: model 6, for example, has only 73.2% and 72.1% overall precision for k = 2 and k = 3, respectively.
Secondly, these results confirm that the task is difficult, even for a corpus with relatively high error detection precision (see section 2.1).
Despite high similarity of context (e.g., model 6), the best results are only around 73%, and this is given a baseline (no changes) of 70%.
While a more expansive set of features would help, there are other problems here, as the method appears to be overtraining.
There is no question that we are learning the “correct” patterns, i.e., 99.9% similarity to the benchmark in the best cases.
The problem is that, for error correction, we have to overcome noise in the data.
Training and testing with the dependency parser MaltParser (Nivre et al., 2007, default settings) is no better, with 72.1% overall precision (despite a labeled attachment score of 98.3%).
Recall in this light that there are variations for which the non-majority label is the correct one; attempting to get a non-majority label correct using a strict lexical model does not work.
To be able not to learn the erroneous patterns requires a more general model.
Interestingly, a more general model—e.g., treating the corpus as a sequence of tags (model 8)—results in equally good correction, without being a good overall fit to the corpus data (only 92.7%).
This model, too, learns noise, as it misses cases that the lexical models get correct.
Simply combining the features does not help (cf. model 6); what we need is to use information from both stricter and looser models in a way that allows general patterns to emerge without overgeneralizing.
<newSection> 5 Model combination Given the discussion in section 4.1 surrounding examples (1)-(5), it is clear that the information needed for correction is sometimes within the immediate context, although that information is needed, however, is often different.
Consider the more general models, 7 and 8, which only use POS tag information.
While sometimes this general information is effective, at times it is dramatically incorrect.
For example, for (7), the original (incor-rect) relation between finna and erbjuda is CC-L; the model 7 classifier selects OO-L as the correct tag; model 8 selects NIL; and the correct label is +F-L (coordination at main clause level).
(7) f¨ors¨oker try ¨oppna marknaden eller erbjuda/VV andra open market or to offer other arbetsm¨ojligheter . work possibilities The original variation for the nucleus finna erb-juda (‘find offer’) is between CC-L and +F-L, but when represented as the POS tags VV VV (other verb), there are 42 possible labels, with OO-L being the most frequent.
This allows for too much confusion.
If model 7 had more restrictions on the set of allowable tags, it could make a more sensible choice and, in this case, select the correct label.
Previous error correction work (Dickinson, 2006) used ambiguity classes for POS annotation, and this is precisely the type of information we need to constrain the label to one which we know is relevant to the current case.
Here, we investigate ambiguity class information derived from one model integrated into another model.
There are at least two main ways we can use ambiguity classes in our models.
The first is what we have just been describing: an ambiguity class can serve as a constraint on the set of possible outcomes for the system.
If the correct label is in the ambiguity class (as it usually is for error correction), this constraining can do no worse than the original model.
The other way to use an ambiguity class is as a feature in the model.
The success of this approach depends on whether or not each ambiguity class patterns in its own way, i.e., defines a sub-regularity within a feature set.
We consider two different feature models, those containing only tags (models 7 and 8), and add to these ambiguity classes derived from two other models, those containing only words (models 1 and 3).
To correct the labels, we need models which do not strictly adhere to the corpus, and the tag-based models are best at this (see the TiMBL results in table 1).
The ambiguity classes, however, must be fairly constrained, and the word-based models do this best (cf. example (7)).
As described in section 5.1, we can use ambiguity classes to constrain the output of a model.
Specifically, we take models 7 and 8 and constrain each selected tag to be one which is within the ambiguity class of a lexical model, either 1 or 3.
That is, if the TiMBL-determined label is not in the ambiguity class, we select the most likely tag of the ones which are.
If no majority label can be decided from this restricted set, we fall back to the TiMBL-selected tag.
In (7), for instance, if we use model 7, the TiMBL tag is OO-L, but model 3’s ambiguity class restricts this to either CC-L or +F-L.
For the representation VV VV, the label CC-L appears 315 times and +F-L 544 times, so +F-L is correctly selected.7 The results are given in table 2, which can be compared to the the original models 7 and 8 in table 1, i.e., total precisions of 49.5% and 73.2%, respectively.
With these simple constraints, model 8 now outperforms any other model (75.5%), and model 7 begins to approach all the models that use contextual information (68.8%).
Ambiguity classes from one model can also be used as features for another (see section 5.1); in this case, ambiguity class information from lexical models (1 and 3) is used as a feature for POS tag models (7 and 8).
The results are given in table 3, where we can see dramatically improved performance from the original models (cf. table 1) and generally improved performance over using ambiguity classes as constraints (cf. table 2).
If we compare the two results for model 7 (61.9% vs. 72.1%) and then the two results for model 8 (76.4% vs. 73.6%), we observe that the better use of ambiguity classes integrates contextual and non-contextual features.
Model 7 (POS, no context) with model 3 ambiguity classes (lex-ical, with context) is better than using ambiguity classes derived from a non-contextual model.
For model 8, on the other hand, which uses contextual POS features, using the ambiguity class without context (model 1) does better.
In some ways, this combination of model 8 with model 1 ambiguity classes makes the most sense: ambiguity classes are derived from a lexicon, and for dependency annotation, a lexicon can be treated as a set of pairs of words.
It is also noteworthy that model 7, despite not using context directly, achieves comparable results to all the previous models using context, once appropriate ambiguity classes are employed.
<newSection> 5.2.3 Both methods Given that the results of ambiguity classes as features are better than that of constraining, we can now easily combine both methodologies, by constraining the output from section 5.2.2 with the ambiguity class tags.
The results are given in table 4; as we can see, all results are a slight improvement over using ambiguity classes as features without constraining the output (table 3).
Using only local context, the best model here is 3.2% points better than the best original model, representing an improvement in correction.
<newSection> 6 Summary and Outlook After outlining the challenges of error correction, we have shown how to integrate information from different models of dependency annotation in order to perform annotation error correction.
By using ambiguity classes from lexical models, both as features and as constraints on the final output, we saw improvements in POS models that were able to overcome noise, without using non-local information.
A first step in further validating these methods is to correct other dependency corpora; this is limited, of course, by the amount of corpora with corrected data available.
Secondly, because this work is based on features and using ambiguity classes, it can in principle be applied to other types of annotation, e.g., syntactic constituency annotation and semantic role annotation.
In this light, it is interesting to note the connection to annotation error detection: the work here is in some sense an extension of the variation n-gram method.
Whether it can be employed as an error detection system on its own requires future work.
Another way in which this work can be extended is to explore how these representations and integration of features can be used for dependency parsing.
There are several issues to work out, however, in making insights from this work more general.
First, it is not clear that pairs of words are sufficiently general to treat them as a lexicon, when one is parsing new data.
Secondly, we have explicit representations for word pairs not annotated as a dependency relation (i.e., NILs), and these are constrained by looking at those which are the same words as real relations.
Again, one would have to determine which pairs of words need NIL representations in new data.
<newSection> Acknowledgements Thanks to Yvonne Samuelsson for help with the Swedish examples; to Joakim Nivre, Mattias Nilsson, and Eva Pettersson for the evaluation data for Talbanken05; and to the three anonymous reviewers for their insightful comments.
<newSection> References<newSection> Abstract Many different types of features have been shown to improve accuracy in parse reranking.
A class of features that thus far has not been considered is based on a projection of the syntactic structure of a translation of the text to be parsed.
The intuition for using this type of bitext projection feature is that ambiguous structures in one language often correspond to unambiguous structures in another.
We show that reranking based on bitext projection features increases parsing accuracy significantly.
<newSection> 1 Introduction Parallel text or bitext is an important knowledge source for solving many problems such as machine translation, cross-language information retrieval, and the projection of linguistic resources from one language to another.
In this paper, we show that bitext-based features are effective in addressing another NLP problem, increasing the accuracy of statistical parsing.
We pursue this approach for a number of reasons.
First, one limiting factor for syntactic approaches to statistical machine translation is parse quality (Quirk and Corston-Oliver, 2006).
Improved parses of bitext should result in improved machine translation.
Second, as more and more texts are available in several languages, it will be increasingly the case that a text to be parsed is itself part of a bitext.
Third, we hope that the improved parses of bitext will serve as higher quality training data for improving monolingual parsing using a process similar to self-training (McClosky et al., 2006).
It is well known that different languages encode different types of grammatical information (agree-ment, case, tense etc.) and that what can be left unspecified in one language must be made explicit in another.
This information can be used for syntactic disambiguation.
However, it is surprisingly hard to do this well.
We use parses and alignments that are automatically generated and hence imperfect.
German parse quality is considered to be worse than English parse quality, and the annotation style is different, e.g., NP structure in German is flatter.
We conduct our research in the framework of N-best parse reranking, but apply it to bitext and add only features based on syntactic projection from German to English.
We test the idea that, generally, English parses with more isomorphism with respect to the projected German parse are better.
The system takes as input (i) English sentences with a list of automatically generated syntactic parses, (ii) a translation of the English sentences into German, (iii) an automatically generated parse of the German translation, and (iv) an automatically generated word alignment.
We achieve a significant improvement of 0.66 F1 (ab-solute) on test data.
The paper is organized as follows.
Section 2 outlines our approach and section 3 introduces the model.
Section 4 describes training and section 5 presents the data and experimental results.
In section 6, we discuss previous work.
Section 7 analyzes our results and section 8 concludes.
<newSection> 2 Approach Consider the English sentence “He saw a baby and a woman who had gray hair”.
Suppose that the baseline parser generates two parses, containing the NPs shown in figures 1 and 2, respectively, and that the semantically more plausible second parse in figure 2 is correct.
How can we determine that the second parse should be favored?
Since we are parsing bitext, we can observe the German translation which is “Er sah ein Baby und eine Frau, die graue Haare hatte” (glossed: “he saw a baby and a woman, who gray hair had”).
The singular verb in the subordinate clause (“hatte”: “had”) indicates that the subordinate S must be attached low to “woman” (“Frau”) as shown in figure 3.
We follow Collins’ (2000) approach to discriminative reranking (see also (Riezler et al., 2002)).
Given a new sentence to parse, we first select the best N parse trees according to a generative model.
Then we use new features to learn discriminatively how to rerank the parses in this N-best list.
We use features derived using projections of the 1-best German parse onto the hypothesized English parse under consideration.
In more detail, we take the 100 best English parses from the BitPar parser (Schmid, 2004) and rerank them.
We have a good chance of finding the optimal parse among the 100-best1.
An automatically generated word alignment determines trans-lational correspondence between German and English.
We use features which measure syntactic di1Using an oracle to select the best parse results in an Fl of 95.90, an improvement of 8.01 absolute over the baseline.
vergence between the German and English trees to try to rank the English trees which have less divergence higher.
Our test set is 3718 sentences from the English Penn treebank (Marcus et al., 1993) which were translated into German.
We hold out these sentences, and train BitPar on the remaining Penn treebank training sentences.
The average Fi parsing accuracy of BitPar on this test set is 87.89%, which is our baseline2.
We implement features based on projecting the German parse to each of the English 100-best parses in turn via the word alignment.
By performing cross-validation and measuring test performance within each fold, we compare our new system with the baseline on the 3718 sentence set.
The overall test accuracy we reach is 88.55%, a statistically significant improvement over baseline of 0.66.
Given a word alignment of the bitext, the system performs the following steps for each English sentence to be parsed: <newSection> 3 Model We use a log-linear model to choose the best English parse.
The feature functions are functions on the hypothesized English parse e, the German parse g, and the word alignment a, and they assign a score (varying between 0 and infinity) that measures syntactic divergence.
The alignment of a sentence pair is a function that, for each English word, returns a set of German words that the English word is aligned with as shown here for the sentence pair from section 2: Er sah ein Baby und eine Frau , die graue Haare hatte Feature function values are calculated either by taking the negative log of a probability, or by using a heuristic function which scales in a similar fashion3.
The form of the log-linear model is shown in eq.
1. There are M feature functions h1, ...
, hM.
The vector λ is used to control the contribution of each feature function.
Given a vector of weights λ, the best English parse eˆ can be found by solving eq.
2. The model is trained by finding the weight vector λ which maximizes accuracy (see section 4).
The basic idea behind our feature functions is that any constituent in a sentence should play approximately the same syntactic role and have a similar span as the corresponding constituent in a translation.
If there is an obvious disagreement, it is probably caused by wrong attachment or other syntactic mistakes in parsing.
Sometimes in translation the syntactic role of a given semantic consti-tutent changes; we assume that our model penalizes all hypothesized parses equally in this case.
For the initial experiments, we used a set of 34 probabilistic and heuristic feature functions.
BitParLogProb (the only monolingual feature) is the negative log probability assigned by BitPar to the English parse.
If we set λ1 = 1 and λi = 0 for all i =� 1 and evaluate eq.
2, we will select the parse ranked best by BitPar.
In order to define our feature functions, we first introduce auxiliary functions operating on individual word positions or sets of word positions.
Alignment functions take an alignment a as an argument.
In the descriptions of these functions we omit a as it is held constant for a sentence pair (i.e., an English sentence and its German translation).
f(i) returns the set of word positions of German words aligned with an English word at position i. f′(i) returns the leftmost word position of the German words aligned with an English word at position i, or zero if the English word is unaligned.
f−1(i) returns the set of positions of English 3For example, a probability of 1 is a feature value of 0, while a low probability is a feature value which is ≫ 0.
words aligned with a German word at position i. f′−1(i) returns the leftmost word position of the English words aligned with a German word at position i, or zero if the German word is unaligned.
We overload the above functions to allow the argument i to be a set, in which case union is used, for example, f(i) = UjEif(j).
Positions in a tree are denoted with integers.
First, the POS tags are numbered from 1 to the length of the sentence (i.e., the same as the word positions).
Constituents higher in the tree are also indexed using consecutive integers.
We refer to the constituent that has been assigned index i in the tree t as “constituent i in tree t” or simply as “constituent i”.
The following functions have the English and German trees as an implicit argument; it should be obvious from the argument to the function whether the index i refers to the German tree or the English tree.
When we say “constituents”, we include nodes on the POS level of the tree.
Our syntactic trees are annotated with a syntactic head for each constituent.
Finally, the tag at position 0 is NULL.
mid2sib(i) returns 0 if i is 0, returns 1 if i has exactly two siblings, one on the left of i and one on the right, and otherwise returns 0.
head(i) returns the index of the head of i.
The head of a POS tag is its own position.
tag(i) returns the tag of i. left(i) returns the index of the leftmost sibling of i. right(i) returns the index of the rightmost sibling.
up(i) returns the index of i’s parent.
A(i) returns the set of word positions covered by i.
If i is a set, A returns all word positions between the leftmost position covered by any constituent in the set and the rightmost position covered by any constituent in the set (inclusive).
n(A) returns the size of the set A. c(A) returns the number of characters (including punctuation and excluding spaces) covered by the constituents in set A.
Q7r� is 1 if 7r is true, and 0 otherwise.
l and m are the lengths in words of the English and German sentences, respectively.
Feature CrdBin counts binary events involving the heads of coordinated phrases.
If in the English parse we have a coordination where the English CC is aligned only with a German KON, and both have two siblings, then the value contributed to CrdBin is 1 (indicating a constraint violation) unless the head of the English left conjunct is aligned with the head of the German left conjunct and likewise the right conjuncts are aligned.
Eq.
3 calculates the value of CrdBin.
Feature Q simply captures a mismatch between questions and statements.
If an English sentence is parsed as a question but the parallel German sentence is not, or vice versa, the feature value is 1; otherwise the value is 0.
Span projection features calculate the percentage difference between a constituent’s span and the span of its projection.
Span size is measured in characters or words.
To project a constituent in a parse, we use the word alignment to project all word positions covered by the constituent and then look for the smallest covering constituent in the parse of the parallel sentence.
CrdPrj is a feature that measures the divergence in the size of coordination constituents and their projections.
If we have a constituent (XP1 CC XP2) in English that is projected to a German coordination, we expect the English and German left conjuncts to span a similar percentage of their respective sentences, as should the right conjuncts.
The feature computes a character-based percentage difference as shown in eq.
4. r and s are the lengths in characters of the English and German sentences, respectively.
In the English parse in figure 1, the left conjunct has 5 characters and the right conjunct has 6, while in figure 2 the left conjunct has 5 characters and the right conjunct has 20.
In the German parse (fig-ure 3) the left conjunct has 7 characters and the right conjunct has 27.
Finally, r = 33 and s = 42.
Thus, the value of CrdPrj is 0.48 for the first hypothesized parse and 0.05 for the second, which captures the higher divergence of the first English parse from the German parse.
POSParentPrj is based on computing the span difference between all the parent constituents of POS tags in a German parse and their respective coverage in the corresponding hypothesized parse.
The feature value is the sum of all the differences.
POSPar(i) is true if i immediately dominates a POS tag.
The projection direction is from German to English, and the feature computes a percentage difference which is character-based.
The value of the feature is calculated in eq.
5, where M is the number of constituents (including POS tags) in the German tree.
The right conjunct in figure 3 is a POSParent that corresponds to the coordination NP in figure 1, contributing a score of 0.21, and to the right conjunct in figure 2, contributing a score of 0.04.
For the two parses of the full sentences containing the NPs in figure 1 and figure 2, we sum over 7 POSParents and get a value of 0.27 for parse 1 and 0.11 for parse 2.
The lower value for parse 2 correctly captures the fact that the first English parse has higher divergence than the second due to incorrect high attachment.
AbovePOSPrj is similar to POSParentPrj, but it is word-based and the projection direction is from English to German.
Unlike POSParentPrj the feature value is calculated over all constituents above the POS level in the English tree.
Another span projection feature function is DTNNPrj, which projects English constituents of the form (NP(DT)(NN)).
DTNN(i) is true if i is an NP immediately dominating only DT and NN.
The feature computes a percentage difference which is word-based, shown in eq.
6. L is the number of constituents in the English tree.
This feature is designed to disprefer parses where constituents starting with “DT NN”, e.g., (NP (DT NN NN NN)), are incorrectly split into two NPs, e.g., (NP (DT NN)) and (NP (NN NN)).
This feature fires in this case, and projects the (NP (DT NN)) into German.
If the German projection is a surprisingly large number of words (as should be the case if the German also consists of a determiner followed by several nouns) then the penalty paid by this feature is large.
This feature is important as (NP (DT NN)) is a very common construction.
We use Europarl (Koehn, 2005), from which we extract a parallel corpus of approximately 1.22 million sentence pairs, to estimate the probabilistic feature functions described in this section.
For the PDepth feature, we estimate English parse depth probability conditioned on German parse depth from Europarl by calculating a simple probability distribution over the 1-best parse pairs for each parallel sentence.
A very deep German parse is unlikely to correspond to a flat English parse and we can penalize such a parse using PDepth.
The index i refers to a sentence pair in Europarl, as does j.
Let li and mi be the depths of the top BitPar ranked parses of the English and German sentences, respectively.
We calculate the probability of observing an English tree of depth l′ given German tree of depth m′ as the maximum likelihood estimate, shown in eq.
7, where δ(z, z′) = 1 if z = z′ and 0 otherwise.
To avoid noisy feature values due to outliers and parse errors, we bound the value of PDepth at 5 as shown in eq.
84. The full parse of the sentence containing the English high attachment has a parse depth of 8 while the full parse of the sentence containing the English low attachment has a depth of 9.
Their feature values given the German parse depth of 6 are −log10(0.12) = 0.93 and − log10(0.14) = 0.84.
The wrong parse is assigned a higher feature value indicating its higher divergence.
The feature PTagEParentGPOSGParent measures tagging inconsistency based on estimating the probability that for an English word at position i, the parent of its POS tag has a particular label.
The feature value is calculated in eq.
10. Consider (S(NP(NN fruit))(VP(V flies))) and (NP(NN fruit)(NNS flies)) with the translation (NP(NNS Fruchtfliegen)).
Assume that “fruit” and “flies” are aligned with the German compound noun “Fruchtfliegen”.
In the incorrect English parse the parent of the POS of “fruit” is NP and the parent of the POS of “flies” is VP, while in the correct parse the parent of the POS of “fruit” is NP and the parent of the POS of “flies” is NP.
In the German parse the compound noun is POS-tagged as an NNS and the parent is an NP.
The probabilities considered for the two English parses are p(NP|NNS, NP) for “fruit” in both parses, p(VP|NNS, NP) for “flies” in the incorrect parse, and p(NP|NNS, NP) for “flies” in the correct parse.
A German NNS in an NP has a higher probability of being aligned with a word in an English NP than with a word in an English VP, so the second parse will be preferred.
As with the PDepth feature, we use relative frequency to estimate this feature.
When an English word is aligned with two words, estimation is more complex.
We heuristically give each English and German pair one count.
The value calculated by the feature function is the geometric mean5 of the pairwise probabilities, see eq.
10. Our best system uses the nine features we have described in detail so far.
In addition, we implemented the following 25 other features, which did not improve performance (see section 7): (i) 7 “ptag” features similar to PTagEParentGPOSG-Parent but predicting and conditioning on different combinations of tags (POS tag, parent of POS, grandparent of POS) (ii) 10 “prj” features similar to POSParentPrj measuring different combinations of character and word percentage differences at the POS parent and POS grandparent levels, projecting from both English and German (iii) 3 variants of the DTNN feature function <newSection> 4 Training Log-linear models are often trained using the Maximum Entropy criterion, but we train our model directly to maximize F1.
We score F1 by comparing hypothesized parses for the discriminative training set with the gold standard.
To try to find the optimal A vector, we perform direct accuracy maximization, meaning that we search for the A vector which directly optimizes F1 on the training set.
Och (2003) has described an efficient exact one-dimensional accuracy maximization technique for a similar search problem in machine translation.
The technique involves calculating an explicit representation of the piecewise constant function gm(x) which evaluates the accuracy of the hypotheses which would be picked by eq.
2 from a set of hypotheses if we hold all weights constant, except for the weight Am, which is set to x.
This is calculated in one pass over the data.
The algorithm for training is initialized with a choice for A and is described in figure 4.
The function F1(A) returns F1 of the parses selected using A.
Due to space we do not describe step 8 in detail (see (Och, 2003)).
In step 9 the algorithm performs approximate normalization, where feature weights are forced towards zero.
The implementation of step 9 is straight-forward given the M explicit functions gm(x) created in step 8.
<newSection> 5 Data and Experiments We used the subset of the Wall Street Journal investigated in (Atterer and Sch¨utze, 2007) for our experiments, which consists of all sentences that have at least one prepositional phrase attachment ambiguity.
This difficult subset of sentences seems particularly interesting when investigating the potential of information in bitext for improving parsing performance.
The first 500 sentences of this set were translated from English to German by a graduate student and an additional 3218 sentences by a translation bureau.
We withheld these 3718 English sentences (and an additional 1000 reserved sentences) when we trained BitPar on the Penn treebank.
Parses.
We use the BitPar parser (Schmid, 2004) which is based on a bit-vector implementation (cf. (Graham et al., 1980)) of the Cocke-Younger-Kasami algorithm (Kasami, 1965; Younger, 1967).
It computes a compact parse forest for all possible analyses.
As all possible analyses are computed, any number of best parses can be extracted.
In contrast, other treebank parsers use sophisticated search strategies to find the most probable analysis without examining the set of all possible analyses (Charniak et al., 1998; Klein and Manning, 2003).
BitPar is particularly useful for N-best parsing as the N-best parses can be computed efficiently.
For the 3718 sentences in the translated set, we created 100-best English parses and 1-best German parses.
The German parser was trained on the TIGER treebank.
For the Europarl corpus, we created 1-best parses for both languages.
Word Alignment.
We use a word alignment of the translated sentences from the Penn treebank, as well as a word alignment of the Europarl corpus.
We align these two data sets together with data from the JRC Acquis (Steinberger et al., 2006) to try to obtain better quality alignments (it is well known that alignment quality improves as the amount of data increases (Fraser and Marcu, 2007)).
We aligned approximately 3.08 million sentence pairs.
We tried to obtain better alignment quality as alignment quality is a problem in many cases where syntactic projection would otherwise work well (Fossum and Knight, 2008).
To generate the alignments, we used Model 4 (Brown et al., 1993), as implemented in GIZA++ (Och and Ney, 2003).
As is standard practice, we trained Model 4 with English as the source language, and then trained Model 4 with German as the source language, resulting in two Viterbi alignments.
These were combined using the Grow Diag Final And symmetrization heuristic (Koehn et al., 2003).
Experiments.
We perform 7-way cross-validation on 3718 sentences.
In each fold of the cross-validation, the training set is 3186 sentences, while the test set is 532 sentences.
Our results are shown in table 1.
In row 1, we take the hypothesis ranked best by BitPar.
In row 2, we train using the algorithm outlined in section 4.
To cancel out any effect caused by a particularly effective or ineffective starting A value, we perform 5 trials each time.
Columns 3 and 5 report the improvement over the baseline on train and test respectively.
We reach an improvement of 0.56 over the baseline using the algorithm as described in section 4.
Our initial experiments used many highly correlated features.
For our next experiment we use greedy feature selection.
We start with a A vector that is zero for all features, and then run the error minimization without the random generation of vectors (figure 4, line 4).
This means that we add one feature at a time.
This greedy algorithm winds up producing a vector with many zero weights.
In row 3 of table 1, we used the greedy feature selection algorithm and trained using F1, resulting in a performance of 0.66 over the baseline which is our best result.
We performed a planned one-tailed paired t-test on the F1 scores of the parses selected by the baseline and this system for the 3718 sentences (parses were taken from the test portion of each fold).
We found that there is a significant difference with the baseline (t(3717) = 6.42, p < .01).
We believe that using the full set of 34 features (many of which are very similar to one another) made the training problem harder without improving the fit to the training data, and that greedy feature selection helps with this (see also section 7).
<newSection> 6 Previous Work As we mentioned in section 2, work on parse reranking is relevant, but a vital difference is that we use features based only on syntactic projection of the two languages in a bitext.
For an overview of different types of features that have been used in parse reranking see Charniak and Johnson (2005).
Like Collins (2000) we use cross-validation to train our model, but we have access to much less data (3718 sentences total, which is less than 1/10 of the data Collins used).
We use rich feature functions which were designed by hand to specifically address problems in English parses which can be disambiguated using the German translation.
Syntactic projection has been used to bootstrap treebanks in resource poor languages.
Some examples of projection of syntactic parses from English to a resource poor language for which no parser is available are the works of Yarowsky and Ngai (2001), Hwa et al.
(2005) and Goyal and Chatterjee (2006).
Our work differs from theirs in that we are performing a parse reranking task in English using knowledge gained from German parses, and parsing accuracy is generally thought to be worse in German than in English.
Hopkins and Kuhn (2006) conducted research with goals similar to ours.
They showed how to build a powerful generative model which flexibly incorporates features from parallel text in four languages, but were not able to show an improvement in parsing performance.
After the submission of our paper for review, two papers outlining relevant work were published.
Burkett and Klein (2008) describe a system for simultaneously improving Chinese and English parses of a Chinese/English bitext.
This work is complementary to ours.
The system is trained using gold standard trees in both Chinese and English, in contrast with our system which only has access to gold standard trees in English.
Their system uses a tree alignment which varies within training, but this does not appear to make a large difference in performance.
They use coarsely defined features which are language independent.
We use several features similar to their two best performing sets of features, but in contrast with their work, we also define features which are specifically aimed at English disambiguation problems that we have observed can be resolved using German parses.
They use an in-domain Chinese parser and out-of-domain English parser, while for us the English parser is in-domain and the German parser is out-of-domain, both of which make improving the English parse more difficult.
Their Maximum Entropy training is more appropriate for their numerous coarse features, while we use Minimum Error Rate Training, which is much faster.
Finally, we are projecting from a single German parse which is a more difficult problem.
Fossum and Knight (2008) outline a system for using Chinese/English word alignments to determine ambiguous English PP-attachments.
They first use an oracle to choose PP-attachment decisions which are ambiguous in the English side of a Chinese/English bitext, and then build a classifier which uses information from a word alignment to make PP-attachment decisions.
No Chinese syntactic information is required.
We use automatically generated German parses to improve English syntactic parsing, and have not been able to find a similar phenomenon for which only a word alignment would suffice.
<newSection> 7 Analysis We looked at the weights assigned during the cross-validation performed to obtain our best result.
The weights of many of the 34 features we defined were frequently set to zero.
We sorted the features by the number of times the relevant λ scalar was zero (i.e., the number of folds of the cross-validation for which they were zero; the greedy feature selection is deterministic and so we do not run multiple trials).
We then reran the same greedy feature selection algorithm as was used in table 1, row 3, but this time using only the top 9 feature values, which were the features which were active on 4 or more folds6.
The result was an improvement on train of 0.84 and an improvement on test of 0.73.
This test result may be slightly overfit, but the result supports the inference that these 9 feature functions are the most important.
We chose these feature functions to be described in detail in section 3.
We observed that the variants of the similar features POSParentPrj and Above-POSPrj projected in opposite directions and measured character and word differences, respectively, and this complementarity seems to help.
We also tried to see if our results depended strongly on the log-linear model and training algorithm, by using the SVM-Light ranker (Joachims, 2002).
In order to make the experiment tractable, we limited ourselves to the 8-best parses (rather than 100-best).
Our training algorithm and model was 0.74 better than the baseline on train and 0.47 better on test, while SVM-Light was 0.54 better than baseline on train and 0.49 better on test (us-ing linear kernels).
We believe that the results are not unduly influenced by the training algorithm.
<newSection> 8 Conclusion We have shown that rich bitext projection features can improve parsing accuracy.
This confirms the hypothesis that the divergence in what information different languages encode grammatically can be exploited for syntactic disambiguation.
Improved parsing due to bitext projection features should be helpful in syntactic analysis of bitexts (by way of mutual syntactic disambiguation) and in computing syntactic analyses of texts that have translations in other languages available.
<newSection> Acknowledgments This work was supported in part by Deutsche Forschungsgemeinschaft Grant SFB 732.
We would like to thank Helmut Schmid for support of BitPar and for his many helpful comments on our work.
We would also like to thank the anonymous reviewers.
<newSection> References<newSection> Abstract In this paper, we first demonstrate the interest of the Loopy Belief Propagation algorithm to train and use a simple alignment model where the expected marginal values needed for an efficient EM-training are not easily computable.
We then improve this model with a distortion model based on structure conservation.
<newSection> 1 Introduction and Related Work Automatic word alignment of parallel corpora is an important step for data-oriented Machine translation (whether Statistical or Example-Based) as well as for automatic lexicon acquisition.
Many algorithms have been proposed in the last twenty years to tackle this problem.
One of the most suc-cessfull alignment procedure so far seems to be the so-called “IBM model 4” described in (Brown et al., 1993).
It involves a very complex distortion model (here and in subsequent usages “dis-tortion” will be a generic term for the reordering of the words occurring in the translation process) with many parameters that make it very complex to train.
By contrast, the first alignment model we are going to propose is fairly simple.
But this simplicity will allow us to try and experiment different ideas for making a better use of the sentence structures in the alignment process.
This model (and even more so its subsequents variations), although simple, do not have a computationally efficient procedure for an exact EM-based training.
However, we will give some theoretical and empirical evidences that Loopy Belief Propagation can give us a good approximation procedure.
Although we do not have the space to review the many alignment systems that have already been proposed, we will shortly refer to works that share some similarities with our approach.
In particular, the first alignment model we will present has already been described in (Melamed, 2000).
We differ however in the training and decoding procedure we propose.
The problem of making use of syntactic trees for alignment (and translation), which is the object of our second alignment model has already received some attention, notably by (Yamada and Knight, 2001) and (Gildea, 2003) . <newSection> 2 Factor Graphs and Belief Propagation In this paper, we will make several use of Factor Graphs.
A Factor Graph is a graphical model, much like a Bayesian Network.
The three most common types of graphical models (Factor Graphs, Bayesian Network and Markov Network) share the same purpose: intuitively, they allow to represent the dependencies among random variables; mathematically, they represent a factoriza-tion of the joint probability of these variables.
Formally, a factor graph is a bipartite graph with 2 kinds of nodes.
On one side, the Variable Nodes (abbreviated as V-Node from here on), and on the other side, the Factor Nodes (abbreviated as F-Node).
If a Factor Graph represents a given joint distribution, there will be one V-Node for every random variable in this joint distribution.
Each F-Node is associated with a function of the V-Nodes to which it is connected (more precisely, a function of the values of the random variables associated with the V-Nodes, but for brevity, we will frequently mix the notions of V-Node, Random Variables and their values).
The joint distribution is then the product of these functions (and of a normalizing constant).
Therefore, each F-Node actually represent a factor in the factorization of the joint distribution.
As a short example, let us consider a problem classically used to introduce Bayesian Network.
We want to model the joint probability of the Weather(W) being sunny or rainy, the Sprinkle(S) being on or off, and the Lawn(L) being wet or dry.
Figure 1 show the dependencies of the variables represented with a Factor Graph and with a Bayesian Network.
Mathematically, the Bayesian Network imply that the joint probability has the following factorization: P(W, L, 5) = P(W) · P(5|W) · P(L|W, 5).
The Factor Graph imply there exist two functions co1 and co2 as well as a normalization constant C such that we have the factorization: P(W, L, 5) = C · co2(W, 5) · co1(L, W, 5).
If we set C = 1, co2(W, 5) = P(W) · P(5|W) and co1(L, W, 5) = P(L|W, 5), the Factor Graph express exactly the same factor-ization as the Bayesian Network.
A reason to use Graphical Models is that we can use with them an algorithm called Belief Propagation (abbreviated as BP from here on) (Pearl, 1988).
The BP algorithm comes in two flavors: sum-product BP and max-product BP.
Each one respectively solve two problems that arise often (and are often intractable) in the use of a probabilistic model: “what are the marginal probabilities of each individual variable?” and “what is the set of values with the highest probability?”.
More precisely, the BP algorithm will give the correct answer to these questions if the graph representing the distribution is a forest.
If it is not the case, the BP algorithm is not even guaranteed to converge.
It has been shown, however, that the BP algorithm do converge in many practical cases, and that the results it produces are often surprisingly good approximations (see, for example, (Murphy et al., 1999) or (Weiss and Freeman, 2001) ).
(Yedidia et al., 2003) gives a very good presentation of the sum-product BP algorithm, as well as some theoretical justifications for its success.
We will just give an outline of the algorithm.
The BP algorithm is a message-passing algorithm.
Messages are sent during several iterations until convergence.
At each iteration, each V-Node sends to its neighboring F-Nodes a message representing an estimation of its own marginal values.
The message sent by the V-Node Vi to the F-Node Fj estimating the marginal probability of Vi to take the value x is : (N(Vi) represent the set of the neighbours of Vi) Also, every F-Node send a message to its neighboring V-Nodes that represent its estimates of the marginal values of the V-Node: , bi being normalized so that Ex bi(x) = 1.
The belief bi(x) is expected to converge to the marginal probability (or an approximation of it) of Vi taking the value x . An interesting point to note is that each message can be “scaled” (that is, multiplied by a constant) by any factor at any point without changing the result of the algorithm.
This is very useful both for preventing overflow and underflow during computation, and also sometimes for simplifying the algorithm (we will use this in section 3.2).
Also, damping schemes such as the ones proposed in (Murphy et al., 1999) or (Heskes, 2003) are useful for decreasing the cases of non-convergence.
As for the max-product BP, it is best explained as “sum-product BP where each sum is replaced by a maximization”.
<newSection> 3 The monolink model We are now going to present a simple alignment model that will serve both to illustrate the efficiency of the BP algorithm and as basis for further improvement.
As previously mentioned, this model is mostly identical to one already proposed in (Melamed, 2000).
The training and decoding procedures we propose are however different.
Following the usual convention, we will designate the two sides of a sentence pair as French and English.
A sentence pair will be noted (e, f).
ei represents the word at position i in e.
In this first simple model, we will pay little attention to the structure of the sentence pair we want to align.
Actually, each sentence will be reduced to a bag of words.
Intuitively, the two sides of a sentence pair express the same set of meanings.
What we want to do in the alignment process is find the parts of the sentences that originate from the same meaning.
We will suppose here that each meaning generate at most one word on each side, and we will name concept the pair of words generated by a meaning.
It is possible for a meaning to be expressed in only one side of the sentence pair.
In that case, we will have a “one-sided” concept consisting of only one word.
In this view, a sentence pair appears “superficially” as a pair of bag of words, but the bag of words are themselves the visible part of an underlying bag of concepts.
We propose a simple generative model to describe the generation of a sentence pair (or rather, its underlying bag of concepts): The probability of a bag of concepts C is then: We can alternatively represent a bag of concepts as a pair of sentence (e, f), plus an alignment a. a is a set of links, a link being represented as a pair of positions in each side of the sentence pair (the special position -1 indicating the empty side of a one-sided concept).
This alternative representation has the advantage of better separating what is observed (the sentence pair) and what is hidden (the alignment).
It is not a strictly equivalent representation (it also contains information about the word positions) but this will not be relevant here.
The joint distribution of e,f and a is then: This model only take into consideration one-to-one alignments.
Therefore, from now on, we will call this model “monolink”.
Considering only one-to-one alignments can be seen as a limitation compared to others models that can often produce at least one-to-many alignments, but on the good side, this allow the monolink model to be nicely symmetric.
Additionally, as already argued in (Melamed, 2000), there are ways to determine the boundaries of some multi-words phrases (Melamed, 2002), allowing to treat several words as a single token.
Alternatively, a procedure similar to the one described in (Cromieres, 2006), where substrings instead of single words are aligned (thus considering every segmentation possible) could be used.
With the monolink model, we want to do two things: first, we want to find out good values for the distributions Psize and Pconcept.
Then we want to be able to find the most likely alignment a given the sentence pair (e, f).
We will consider Psize to be a uniform distribution over the integers up to a sufficiently big value (since it is not possible to have a uniform distribution over an infinite discrete set).
We will not need to determine the exact value of Psize . The assumption that it is uniform is actually enough to “remove” it of the computations that follow.
In order to determine the Pconcept distribution, we can use an EM procedure.
It is easy to show that, at every iteration, the EM procedure will require to set Pconcept(we, wf) proportional to the sum of the expected counts of the concept (we, wf) over the training corpus.
This, in turn, mean we have to compute the conditional expectation: for every sentence pair (e, f).
This computation require a sum over all the possible alignments, whose numbers grow exponentially with the size of the sentences.
As noted in (Melamed, 2000), it does not seem possible to compute this expectation efficiently with dynamic programming tricks like the one used in the IBM models 1 and 2 (as a passing remark, these “tricks” can actually be seen as instances of the BP algorithm).
We propose to solve this problem by applying the BP algorithm to a Factor Graph representing the conditional distribution P(a|e, f).
Given a sentence pair (e, f), we build this graph as follows.
We create a V-node Vie for every position i in the English sentence.
This V-Node can take for ij nodes are noted Fri-j) value any position in the french sentence, or the special position −1 (meaning this position is not aligned, corresponding to a one-sided concept).
We create symmetrically a V-node Vj f for every position in the french sentence.
We have to enforce a “reciprocal love” condition: if a V-Node at position i choose a position j on the opposite side, the opposite V-Node at position j must choose the position i.
This is done by adding a F-Node F rec i,j between every opposite node V e We then connect a “translation probability” F-Node F tp.e i to every V-Node Vie associated with the function: We add symmetrically on the French side F-Nodes Ftp.f j to the V-Nodes Vjf . It should be fairly easy to see that such a Factor Graph represents P(a|e, f).
See figure 2 for an example.
Using the sum-product BP, the beliefs of every V-Node Vie to take the value j and of every node Vjf to take the value i should converge to the marginal expectation E((i, j) ∈ a|e, f) (or rather, a hopefully good approximation of it).
We can also use max-product BP on the same graph to decode the most likely alignment.
In the monolink case, decoding is actually an instance of the “assignment problem”, for which efficient algorithms are known.
However this will not be the case for the more complex model of the next section.
Actually, (Bayati et al., 2005) has recently proved that max-product BP always give the optimal solution to the assignment problem.
Applying naively the BP algorithm would lead us to a complexity of O(|e|2 · |f|2) per BP iteration.
While this is not intractable, it could turn out to be a bit slow.
Fortunately, we found it is possible to reduce this complexity to O(|e |· |f|) by making two useful observations.
Let us note meij the resulting message from Vie to Vjf (that is the message sent by F rec i,j to Vjf after it received its own message from V i e).
meij(x) has the same value for every x different from i: b� mij (x =6 i) = Pk�j m�((k).
We can divide all the messages meij by meij(x =6 i), so that meij(x) = 1 except if x = i; and the same can be done for the messages coming from the French side mfij.
It follows that meij(x =6 i) = Pk�j bei (k) = 1 − bei (j) if the bei are kept normalized.
Therefore, at every step, we only need to compute meij(j), not meij(x =6 j).
Hence the following algorithm (meij(j) will be here abbreviated to meij since it is the only value of the message we need to compute).
We describe the process for computing the English-side messages and beliefs (meij and bei) , but the process must also be done symmetrically for the French-side messages and beliefs (mf ijand bfi ) at every iteration.
0- Initialize all messages and beliefs with: mij = 1 and be(0) A similar algorithm can be found for the max-product BP.
We evaluated the monolink algorithm with two languages pairs: French-English and Japanese-English.
For the English-French Pair, we used 200,000 sentence pairs extracted from the Hansard corpus (Germann, 2001).
Evaluation was done with the scripts and gold standard provided during the workshop HLT-NAACL 20031 (Mihalcea and Pedersen, 2003).
Null links are not considered for the evaluation.
For the English-Japanese evaluation, we used 100,000 sentence pairs extracted from a corpus of English/Japanese news.
We used 1000 sentence pairs extracted from pre-aligned data(Utiyama and Isahara, 2003) as a gold standard.
We segmented all the Japanese data with the automatic segmenter Juman (Kurohashi and Nagao, 1994).
There is a caveat to this evaluation, though.
The reason is that the segmentation and alignment scheme used in our gold standard is not very fine-grained: mostly, big chunks of the Japanese sentence covering several words are aligned to big chunks of the English sentence.
For the evaluation, we had to consider that when two chunks are aligned, there is a link between every pair of words belonging to each chunk.
A consequence is that our gold standard will contain a lot more links than it should, some of them not relevants.
This means that the recall will be largely underestimated and the precision will be overestimated.
For the BP/EM training, we used 10 BP iterations for each sentences, and 5 global EM iterations.
By using a damping scheme for the BP algorithm, we never observed a problem of non-convergence (such problems do commonly appears without damping).
With our python/C implementation, training time approximated 1 hour.
But with a better implementation, it should be possible to reduce this time to something comparable to the model 1 training time with Giza++.
For the decoding, although the max-product BP should be the algorithm of choice, we found we could obtain slightly better results (by between 1 and 2 AER points) by using the sum-product BP, choosing links with high beliefs, and cutting-off links with very small beliefs (the cut-off was chosen roughly by manually looking at a few aligned sentences not used in the evaluation, so as not to create too much bias).
Due to space constraints, all of the results of this section and the next one are summarized in two tables (tables 1 and 2) at the end of this paper.
In order to compare the efficiency of the BP training procedure to a more simple one, we reim-plemented the Competitive Link Algorithm (ab-breviated as CLA from here on) that is used in (Melamed, 2000) to train an identical model.
This algorithm starts with some relatively good estimates found by computing correlation score (we used the G-test score) between words based on their number of co-occurrences.
A greedy Viterbi training is then applied to improve this initial guess.
In contrast, our BP/EM training do not need to compute correlation scores and start the training with uniform parameters.
We only evaluated the CLA on the French/English pair.
The first iteration of CLA did improve alignment quality, but subsequent ones decreased it.
The reported score for CLA is therefore the one obtained during the best iteration.
The BP/EM training demonstrate a clear superiority over the CLA here, since it produce almost 7 points of AER improvement over CLA.
In order to have a comparison with a well-known and state-of-the-art system, we also used the GIZA++ program (Och and Ney, 1999) to align the same data.
We tried alignments in both direction and provide the results for the direction that gave the best results.
The settings used were the ones used by the training scripts of the Moses system2, which we assumed to be fairly optimal.
We tried alignment with the default Moses settings (5 iterations of model 1, 5 of Hmm, 3 of model 3, 3 of model 4) and also tried with increased number of iterations for each model (up to 10 per model).
We are aware that the score we obtained for model 4 in English-French is slightly worse than what is usually reported for a similar size of training data.
At the time of this paper, we did not have the time to investigate if it is a problem of non-optimal settings in GIZA++, or if the training data we used was “difficult to learn from” (it is common to extract sentences of moderate length for the training data but we didn’t, and some sentences of our training corpus do have more than 200 words; also, we did not use any kind of preprocessing).
In any case, Giza++ is compared here with an algorithm trained on the same data and with no possibilities for fine-tuning; therefore the comparison should be fair.
The comparison show that performance-wise, the monolink algorithm is between the model 2 and the model 3 for English/French.
Considering our model has the same number of parameters as the model 1 (namely, the word translation probabilities, or concept probabilities in our model), these are pretty good results.
Overall, the mono-link model tend to give better precision and worse recall than the Giza++ models, which was to be expected given the different type of alignments produced (1-to-1 and 1-to-many).
For English/Japanese, monolink is at just about the level of model 1, but model 1,2 and 3 have very close performances for this language pair (inter-estingly, this is different from the English/French pair).
Incidentally, these performances are very poor.
Recall was expected to be low, due to the previously mentioned problem with the gold standard.
But precision was expected to be better.
It could be the algorithms are confused by the very fine-grained segmentation produced by Juman.
<newSection> 4 Adding distortion through structure While the simple monolink model gives interesting results, it is somehow limited in that it do not use any model of distortion.
We will now try to add a distortion model; however, rather than directly modeling the movement of the positions of the words, as is the case in the IBM models, we will try to design a distortion model based on the structures of the sentences.
In particular, we are interested in using the trees produced by syntactic parsers.
The intuition we want to use is that, much like there is a kind of “lexical conservation” in the translation process, meaning that a word on one side has usually an equivalent on the other side, there should also be a kind of “structure conservation”, with most structures on one side having an equivalent on the other.
Before going further, we should precise the idea of “structure” we are going to use.
As we said, our prime (but not only) interest will be to make use of the syntactic trees of the sentences to be aligned.
However these kind of trees come in very different shapes depending on the language and the type of parser used (dependency, constituents,...
). This is why we decided the only information we would keep from a syntactic tree is the set of its sub-nodes.
More specifically, for every sub-node, we will only consider the set of positions it cover in the underlying sentence.
We will call such a set of positions a P-set.
This simplification will allow us to process dependency trees, constituents trees and other structures in a uniformized way.
Figure 3 gives an example of a constituents tree and the P-sets it generates.
According to our intuition about the “conserva-tion of structure”, some (not all) of the P-sets on one side should have an equivalent on the other side.
We can model this in a way similar to how we represented equivalence between words with concepts.
We postulate that, in addition to a bag of concepts, sentence pairs are underlaid by a set of P-concepts.
P-concepts being actually pairs of P-sets (a P-set for each side of the sentence pair).
We also allow the existence of one-sided P-concepts.
In the previous model, sentence pairs where just bag of words underlaid by a or bag of concepts, and there was no modeling of the position of the words.
P-concepts bring a notion of word position to the model.
Intuitively, there should be coherency between P-concepts and concepts.
This coherence will come from a compatibility constraint: if a sentence contains a two-sided P-concept (PSe, PSf), and if a word we covered by PSe come from a two-sided concept (we, wf), then wf must be covered by PSf.
Let us describe the model more formally.
In the view of this model, a sentence pair is fully described by: e and f (the sentences themselves), a (the word alignment giving us the underlying bag of concept), se and sf (the sets of P-sets on each side of the sentence) and as (the P-set alignment that give us the underlying set of P-concepts).
e,f,se,sf are considered to be observed (even if we will need parsing tools to observe se and sf); a and as are hidden.
The probability of a sentence pair is given by the joint probability of these variables :P (e, f, se, sf, a, as).
By making some simple independence assumptions, we can write: P(a, as, e, f,se, sf) = Pml(a, e, f)' Pml(a, e, f) is taken to be identical to the mono-link model (see equation (1)).
We are not interested in P(se, sf|e, f) (parsers will deal with it for us).
In our model, P(as|a, se, sf) will be equal to: where comp(a, as, se, sf) is equal to 1 if the compatibility constraint is verified, and 0 else.
C is a normalizing constant.
Ppc describe the probability of each P-concept.
Although it would be possible to learn parameters for the distribution Ppc depending on the characteristics of each P-concepts, we want to keep our model simple.
Therefore, Ppc will have only two different values.
One for the one-sided P-concepts, and one for the two-sided ones.
Considering the constraint of normalization, we then have actually one parameter: α = Pp,(1−sided) Pp,(2−sided).
Although it would be possible to learn the parameter α during the EM-training, we choose to set it at a preset value.
Intuitively, we should have 0 < α < 1, because if α is greater than 1, then the one-sided P-concepts will be favored by the model, which is not what we want.
Some empirical experiments showed that all values of α in the range [0.5,0.9] were giving good results, which lead to think that α can be set mostly independently from the training corpus.
We still need to train the concepts probabilities (used in Pml(a, e, f)), and to be able to decode the most probable alignments.
This is why we are again going to represent P(a, as|e, f, se, sf) as a Factor Graph.
This Factor Graph will contain two instances of the monolink Factor Graph as subgraph: one for a, the other for as (see figure 4).
More precisely, we create again a V-Node for every position on each side of the sentence pair.
We will call these V-Nodes “Word V-Nodes”, to differentiate them from the new “P-set V-Nodes”.
We will create a “P-set V-Node” Vps.e ifor every P-set in se, and a “P-set V-Node” V ps.f j for every P-set in sj.
We inter-connect all of the Word V-Nodes so that we have a subgraph identical to the Factor Graph used in the monolink case.
We also create a “monolink subgraph” for the P-set V-Nodes.
We now have 2 disconnected subgraphs.
However, we need to add F-Nodes between them to enforce the compatibility constraint between as and Vpse k , and for every position i that the corresponding P-set cover, we add a F-Node Fcomp.e We proceed symmetrically on the French side.
Messages inside each monolink subgraph can still be computed with the efficient procedure described in section 3.2.
We do not have the space to describe in details the messages sent between P-set V-Nodes and Word V-Nodes, but they are easily computed from the principles of the BP algorithm.
Let NE = PpsEse |ps |and NF = PpsEsf |ps|.
Then the complexity of one BP iteration will be O(NG · ND + |e |· |f|).
An interesting aspect of this model is that it is flexible towards enforcing the respect of the structures by the alignment, since not every P-set need to have an equivalent in the opposite sentence.
(Gildea, 2003) has shown that too strict an enforcement can easily degrade alignment quality and that good balance was difficult to find.
Another interesting aspect is the fact that we have a somehow “parameterless” distortion model.
There is only one real-valued parameter to control the distortion: α.
And even this parameter is actually pre-set before any training on real data.
The distortion is therefore totally controlled by the two sets of P-sets on each side of the sentence.
Finally, although we introduced the P-sets as being generated from a syntactic tree, they do not need to.
In particular, we found interesting to use P-sets consisting of every pair of adjacent positions in a sentence.
For example, with a sentence of length 5, we generate the P-sets {1,2},{2,3},{3,4} and {4,5}.
The underlying intuition is that “adjacency” is often preserved in translation (we can see this as another case of “conservation of structure”).
Practically, using P-sets of adjacent positions create a distortion model where permutation of words are not penalized, but gaps are penalized.
The evaluation setting is the same as in the previous section.
We created syntactic trees for every sentences.
For English,we used the Dan Bikel implementation of the Collins parser (Collins, 2003).
For French, the SYGMART parser (Chauch´e, 1984) and for Japanese, the KNP parser (Kuro-hashi and Nagao, 1994).
The line SDM:Parsing (SDM standing for “Structure-based Distortion Monolink”) shows the results obtained by using P-sets from the trees produced by these parsers.
The line SDM:Adjacency shows results obtained by using adjacent positions P-sets ,as described at the end of the previous section (therefore, SDM:Adjacency do not use any parser).
Several interesting observations can be made from the results.
First, our structure-based distortion model did improve the results of the mono-link model.
There are however some surprising results.
In particular, SDM:Adjacency produced surprisingly good results.
It comes close to the results of the IBM model 4 in both language pairs, while it actually uses exactly the same parameters as model 1.
The fact that an assumption as simple as “allow permutations, penalize gaps” can produce results almost on par with the complicated distortion model of model 4 might be an indication that this model is unnecessarily complex for languages with similar structure.Another surprising result is the fact that SDM:Adjacency gives better results for the English-French language pair than SDM:Parsing, while we expected that information provided by parsers would have been more relevant for the distortion model.
It might be an indication that the structure of English and French is so close that knowing it provide only moderate information for word reordering.
The contrast with the English-Japanese pair is, in this respect, very interesting.
For this language pair, SDM:Adjacency did provide a strong improvement, but significantly less so than SDM:Parsing.
This tend to show that for language pairs that have very different structures, the information provided by syntactic tree is much more relevant.
<newSection> 5 Conclusion and Future Work We will summarize what we think are the 4 more interesti ng contributions of this paper.
BP algorithm has been shown to be useful and flexible for training and decoding complex alignment models.
An original mostly non-parametrical distortion model based on a simplified structure of the sentences has been described.
Adjacence constraints have been shown to produce very efficient distortion model.
Empirical performances differences in the task of aligning Japanese and English to French hint that considering different paradigms depending on language pairs could be an improvement on the “one-size-fits-all” approach generally used in Statistical alignment and translation.
Several interesting improvement could also be made on the model we presented.
Especially, a more elaborated Pp,, that would take into account the nature of the nodes (NP, VP, head,..) to parametrize the P-set alignment probability, and would use the EM-algorithm to learn those parameters.
U. Germann.
2001. Aligned hansards of the 36th parliament of canada.
http://www.isi.edu/naturallanguage/download/hansard/.
<newSection> References<newSection> Abstract Parallel Multiple Context-Free Grammar (PMCFG) is an extension of context-free grammar for which the recognition problem is still solvable in polynomial time.
We describe a new parsing algorithm that has the advantage to be incremental and to support PMCFG directly rather than the weaker MCFG formalism.
The algorithm is also top-down which allows it to be used for grammar based word prediction.
<newSection> 1 Introduction Parallel Multiple Context-Free Grammar (PMCFG) (Seki et al., 1991) is one of the grammar formalisms that have been proposed for the syntax of natural languages.
It is an extension of context-free grammar (CFG) where the right hand side of the production rule is a tuple of strings instead of only one string.
Using tuples the grammar can model discontinuous constituents which makes it more powerful than context-free grammar.
In the same time PMCFG has the advantage to be parseable in polynomial time which makes it attractive from computational point of view.
A parsing algorithm is incremental if it reads the input one token at the time and calculates all possible consequences of the token, before the next token is read.
There is substantial evidence showing that humans process language in an incremental fashion which makes the incremental algorithms attractive from cognitive point of view.
If the algorithm is also top-down then it is possible to predict the next word from the sequence of preceding words using the grammar.
This can be used for example in text based dialog systems or text editors for controlled language where the user might not be aware of the grammar coverage.
In this case the system can suggest the possible continuations.
A restricted form of PMCFG that is still stronger than CFG is Multiple Context-Free Grammar (MCFG).
In Seki and Kato (2008) it has been shown that MCFG is equivalent to string-based Linear Context-Free Rewriting Systems and Finite-Copying Tree Transducers and it is stronger than Tree Adjoining Grammars (Joshi and Schabes, 1997).
Efficient recognition and parsing algorithms for MCFG have been described in Nakanishi et al.
(1997), Ljungl¨of (2004) and Burden and Ljungl¨of (2005).
They can be used with PMCFG also but it has to be approximated with over-generating MCFG and post processing is needed to filter out the spurious parsing trees.
We present a parsing algorithm that is incremental, top-down and supports PMCFG directly.
The algorithm exploits a view of PMCFG as an infinite context-free grammar where new context-free categories and productions are generated during parsing.
It is trivial to turn the algorithm into statistical by attaching probabilities to each rule.
In Ljungl¨of (2004) it has been shown that the Grammatical Framework (GF) formalism (Ranta, 2004) is equivalent to PMCFG.
The algorithm was implemented as part of the GF interpreter and was evaluated with the resource grammar library (Ranta, 2008) which is the largest collection of grammars written in this formalism.
The incrementality was used to build a help system which suggests the next possible words to the user.
Section 2 gives a formal definition of PMCFG.
In section 3 the procedure for “linearization” i.e. the derivation of string from syntax tree is defined.
The definition is needed for better understanding of the formal proofs in the paper.
The algorithm introduction starts with informal description of the idea in section 4 and after that the formal rules are given in section 5.
The implementation details are outlined in section 6 and after that there are some comments on the evaluation in section 7.
Section 8 gives a conclusion.
<newSection> 2 PMCFG definition Proceedings of the 12th Conference of the European Chapter of the ACL, pages 69–76, Athens, Greece, 30 March – 3 April 2009.
c�2009 Association for Computational Linguistics Here αi is a sequence of terminals and hk; li pairs, where 1 ≤ k ≤ a(f) is called argument index and 1 ≤ l ≤ dk(f) is called constituent index.
where A ∈ N is called result category, A1, A2, ...
, Aa(f) ∈ N are called argument categories and f ∈ F is the function symbol.
For the production to be well formed the conditions We use the same definition of PMCFG as is used by Seki and Kato (2008) and Seki et al.
(1993) with the minor difference that they use variable names like xkl while we use hk; li to refer to the function arguments.
As an example we will use the anbncn language: Here the dimensions are d(S) = 1 and d(N) = 3 and the arities are a(c) = a(s) = 1 and a(z) = 0.
a is the empty string.
<newSection> 3 Derivation The derivation of a string in PMCFG is a two-step process.
First we have to build a syntax tree of a category S and after that to linearize this tree to string.
The definition of a syntax tree is recursive: Definition 2 (f t1 ...
ta(f)) is a tree of category A if ti is a tree of category Bi and there is a production: A → f[B1 ...
Ba(f)] The abstract notation for “t is a tree of category A” is t : A.
When a(f) = 0 then the tree does not have children and the node is called leaf.
The linearization is bottom-up.
The functions in the leaves do not have arguments so the tuples in their definitions already contain constant strings.
If the function has arguments then they have to be linearized and the results combined.
Formally this can be defined as a function L applied to the syntax tree: L(f t1 t2 ... ta(f)) = (x1, x2 ...
xr(f)) where xi = K(L(t1), L(t2) ...
L(ta(f))) αi and f := (α1, α2 ...
αr(f)) ∈ F The function uses a helper function K which takes the already linearized arguments and a sequence αi of terminals and hk; li pairs and returns a string.
The string is produced by simple substitution of each hk; li with the string for constituent l from argument k: where βi ∈ T*.
The recursion in L terminates when a leaf is reached.
In the example anbncn language the function z does not have arguments and it corresponds to the base case when n = 0.
Every application of s over another tree t : N increases n by one.
For example the syntax tree (s (s z)) will produce the tuple (aa, bb, cc).
Finally the application of c combines all elements in the tuple in a single string i.e. c (s (s z)) will produce the string aabbcc.
<newSection> 4 The Idea Although PMCFG is not context-free it can be approximated with an overgenerating context-free grammar.
The problem with this approach is that the parser produces many spurious parse trees that have to be filtered out.
A direct parsing algorithm for PMCFG should avoid this and a careful look at the difference between PMCFG and CFG gives an idea.
The context-free approximation of anbncn is the language a*b*c* with grammar: The string &quot;aabbcc&quot; is in the language and it can be derived with the following steps: The grammar is only an approximation because there is no enforcement that we will use only equal number of reductions for A, B and C.
This can be guaranteed if we replace B and C with new categories B' and C' after the derivation of A: In this case the only possible derivation from aaB'C' is aabbcc.
The PMCFG parser presented in this paper works like context-free parser, except that during the parsing it generates fresh categories and rules which are spe-cializations of the originals.
The newly generated rules are always versions of already existing rules where some category is replaced with new more specialized category.
The generation of specialized categories prevents the parser from recognizing phrases that are otherwise withing the scope of the context-free approximation of the original grammar.
<newSection> 5 Parsing The algorithm is described as a deductive process in the style of (Shieber et al., 1995).
The process derives a set of items where each item is a statement about the grammatical status of some substring in the input.
The inference rules are in natural deduction style: where the premises Xi are some items and Y is the derived item.
We assume that w1 ...
wn is the input string.
The deduction system deals with three types of items: active, passive and production items.
Productions In Shieber’s deduction systems the grammar is a constant and the existence of a given production is specified as a side condition.
In our case the grammar is incrementally extended at runtime, so the set of productions is part of the deduction set.
The productions from the original grammar are axioms and are included in the initial deduction set.
Active Items The active items represent the partial parsing result: such that the tree (f t1 ... ta(f)) will produce the substring wj+1 ...
wk as a prefix in constituent l for any sequence of arguments ti : Bi.
The sequence α is the part that produced the substring: JC(G(t1), G(t2) ... G(ta(f))) α = wj+1 ...
wk and β is the part that is not processed yet.
Passive Items The passive items are of the form: [kjA;l;N] , j < k and state that there exists at least one production: and a tree (f t1 ...
ta(f)) : A such that the constituent with index l in the linearization of the tree is equal to wj+1 ... wk.
Contrary to the active items in the passive the whole constituent is matched: JC(G(t1), G(t2) ... G(ta(f))) γl = wj+1 ...
wk Each time when we complete an active item, a passive item is created and at the same time we create a new category N which accumulates all productions for A that produce the wj+1 ...
wk substring from constituent l.
All trees of category N must produce wj+1 ...
wk in the constituent l.
There are six inference rules (see figure 1).
The INITIAL PREDICT rule derives one item spanning the 0 − 0 range for each production with the start category S on the left hand side.
The rhs(f, l) function returns the constituent with index l of function f.
In the PREDICT rule, for each active item with dot before a (d; r) pair and for each production for Bd, a new active item is derived where the dot is in the beginning of constituent r in g.
When the dot is before some terminal s and s is equal to the current terminal wk then the SCAN rule derives a new item where the dot is moved to the next position.
When the dot is at the end of an active item then it is converted to passive item in the COMPLETE rule.
The category N in the passive item is a fresh category created for each unique (A, l, j, k) quadruple.
A new production is derived for N which has the same function and arguments as in the active item.
The item in the premise of COMPLETE was at some point predicted in PREDICT from some other item.
The COMBINE rule will later replace the occurence A in the original item (the premise of PREDICT) with the special-ization N.
The COMBINE rule has two premises: one active item and one passive.
The passive item starts from position u and the only inference rule that can derive items with different start positions is PREDICT.
Also the passive item must have been predicted from active item where the dot is before (d; r), the category for argument number d must have been Bd and the item ends at u.
The active item in the premise of COMBINE is such an item so it was one of the items used to predict the passive one.
This means that we can move the dot after (d; r) and the d-th argument is replaced with its specialization N.
If the string 0 contains another reference to the d-th argument then the next time when it has to be predicted the rule PREDICT will generate active items, only for those productions that were successfully used to parse the previous constituents.
If a context-free approximation was used this would have been equivalent to unification of the redundant subtrees.
Instead this is done at runtime which also reduces the search space.
The parsing is successful if we had derived the [o S;1; S'] item, where n is the length of the text, S is the start category and S' is the newly created category.
The parser is incremental because all active items span up to position k and the only way to move to the next position is the SCAN rule where a new symbol from the input is consumed.
The parsing system is sound if every derivable item represents a valid grammatical statement under the interpretation given to every type of item.
The derivation in INITIAL PREDICT and PREDICT is sound because the item is derived from existing production and the string before the dot is empty so: 1CQe=e The rationale for SCAN is that if If the item in the premise is valid then it is based on existing production and function and so will be the item in the consequent.
In the COMPLETE rule the dot is at the end of the string.
This means that wj+1 ...
wk will be not just a prefix in constituent l of the linearization but the full string.
This is exactly what is required in the semantics of the passive item.
The passive item is derived from a valid active item so there is at least one production for A.
The category N is unique for each (A, l, j, k) quadruple so it uniquely identifies the passive item in which it is placed.
There might be many productions that can produce the passive item but all of them should be able to generate wj+1 ...
wk and they are exactly the productions that are added to N.
From all this arguments it follows that COMPLETE is sound.
The COMBINE rule is sound because from the active item in the premise we know that: From the passive item we know that every production for N produces the wu+1 ...
wk in r.
From that follows that where u' is the same as u except that Bd is replaced with N.
Note that the last conclusion will not hold if we were using the original context because Bd is a more general category and can contain productions that does not derive wu+1 ... wk.
The parsing system is complete if it derives an item for every valid grammatical statement.
In our case we have to prove that for every possible parse tree the corresponding items will be derived.
The proof for completeness requires the following lemma: The proof is by induction on the depth of the tree.
If the tree has only one level then the function f does not have arguments and from the linearization definition and from the premise in the lemma it follows that αl = wj+1 ... wk.
From the active item in the lemma by applying iteratively the SCAN rule and finally the COMPLETE rule the system will derive the requested item.
If the tree has subtrees then we assume that the lemma is true for every subtree and we prove it for the whole tree.
We know that Since the function K does simple substitution it is possible for each (d; s) pair in al to find a new range in the input string j0−k0 such that the lemma to be applicable for the corresponding subtree td : Bd.
The terminals in al will be processed by the SCAN rule.
Rule PREDICT will generate the active items required for the subtrees and the COMBINE rule will consume the produced passive items.
Finally the COMPLETE rule will derive the requested item for the whole tree.
From the lemma we can prove the completeness of the parsing system.
For every possible tree t : S such that G(t) = (w1 ...
wn) we have to prove that the [n0 S;1; S0] item will be derived.
Since the top-level function of the tree must be from production for S the INITIAL PREDICT rule will generate the active item in the premise of the lemma.
From this and from the assumptions for t it follows that the requested passive item will be derived.
The algorithm is very similar to the Earley (1970) algorithm for context-free grammars.
The similarity is even more apparent when the inference rules in this paper are compared to the inference rules for the Earley algorithm presented in Shieber et al.
(1995) and Ljungl¨of (2004).
This suggests that the space and time complexity of the PMCFG parser should be similar to the complexity of the Earley parser which is O(n2) for space and O(n3) for time.
However we generate new categories and productions at runtime and this have to be taken into account.
Let the P(j) function be the maximal number of productions generated from the beginning up to the state where the parser has just consumed terminal number j.
P(j) is also the upper limit for the number of categories created because in the worst case there will be only one production for each new category.
The active items have two variables that directly depend on the input size - the start index j and the end index k.
If an item starts at position j then there are (n − j + 1) possible values for k because j < k < n.
The item also contains a production and there are P(j) possible choices for it.
In total there are: possible choices for one active item.
The possibilities for all other variables are only a constant factor.
The P(j) function is monotonic because the algorithm only adds new productions and never removes.
From that follows the inequality: which gives the approximation for the upper limit: The same result applies to the passive items.
The only difference is that the passive items have only a category instead of a full production.
However the upper limit for the number of categories is the same.
Finally the upper limit for the total number of active, passive and production items is: The expression for P(n) is grammar dependent but we can estimate that it is polynomial because the set of productions corresponds to the compact representation of all parse trees in the context-free approximation of the grammar.
The exponent however is grammar dependent.
From this we can expect that asymptotic space complexity will be O(ne) where a is some parameter for the grammar.
This is consistent with the results in Nakanishi et al.
(1997) and Ljungl¨of (2004) where the exponent also depends on the grammar.
The time complexity is proportional to the number of items and the time needed to derive one item.
The time is dominated by the most complex rule which in this algorithm is COMBINE.
All variables that depend on the input size are present both in the premises and in the consequent except u.
There are n possible values for u so the time complexity is O(ne+1).
If the parsing is successful we need a way to extract the syntax trees.
Everything that we need is already in the set of newly generated productions.
If the goal item is [n0 S; 0; S0] then every tree t of category S0 that can be constructed is a syntax tree for the input sentence (see definition 2 in section 3 again).
Note that the grammar can be erasing; i.e., there might be productions like this: There are three arguments but only two of them are used.
When the string is parsed this will generate a new specialized production: S0 , f[B01, B2, B03] Here S,B1 and B3 are specialized to S0, B01 and B03 but the B2 category is still the same.
This is correct because actually any subtree for the second argument will produce the same result.
Despite this it is sometimes useful to know which parts of the tree were used and which were not.
In the GF interpreter such unused branches are replaced by meta variables.
In this case the tree extractor should check whether the category also exists in the original set of categories N in the grammar.
Just like with the context-free grammars the parsing algorithm is polynomial but the chart can contain exponential or even infinite number of trees.
Despite this the chart is a compact finite representation of the set of trees.
<newSection> 6 Implementation Every implementation requires a careful design of the data structures in the parser.
For efficient access the set of items is split into four subsets: A, Sj, C and P.
A is the agenda i.e. the set of active items that have to be analyzed.
Sj contains items for which the dot is before an argument reference and which span up to position j.
C is the set of possible continuations i.e. a set of items for which the dot is just after a terminal.
P is the set of productions.
In addition the set F is used internally for the generatation of fresh categories.
The sets C, Sj and F are used as association maps.
They contain associations like k �--> v where k is the key and v is the value.
All maps except F can contain more than one value for one and the same key.
The pseudocode of the implementation is given in figure 2.
There are two procedures Init and Compute.
Init computes the initial values of S, P and A.
The initial agenda A is the set of all items that can be predicted from the start category S (INITIAL PREDICT rule).
Compute consumes items from the current agenda and applies the SCAN, PREDICT, COMBINE or COMPLETE rule.
The case statement matches the current item against the patterns of the rules and selects the proper rule.
The PREDICT and COMBINE rules have two premises so they are used in two places.
In both cases one of the premises is related to the current item and a loop is needed to find item matching the other premis.
The passive items are not independent entities but are just the combination of key and value in the set F.
Only the start position of every item is kept because the end position for the interesting passive items is always the current position and the active items are either in the agenda if they end at the current position or they are in the Sj set if they end at position j.
The active items also keep only the dot position in the constituent because the constituent definition can be retrieved from the grammar.
For this reason the runtime representation of the items is [j; A → f[B]; l; p] where j is the start position of the item and p is the dot position inside the constituent.
The Compute function returns the updated S and P sets and the set of possible continuations C.
The set of continuations is a map indexed by a terminal and the values are active items.
The parser computes the set of continuations at each step and if the current terminal is one of the keys the set of values for it is taken as an agenda for the next step.
<newSection> 7 Evaluation The algorithm was evaluated with four languages from the GF resource grammar library (Ranta, 2008): Bulgarian, English, German and Swedish.
These grammars are not primarily intended for parsing but as a resource from which smaller domain dependent grammars are derived for every application.
Despite this, the resource grammar library is a good benchmark for the parser because these are the biggest GF grammars.
The compiler converts a grammar written in the high-level GF language to a low-level PMCFG grammar which the parser can use directly.
The sizes of the grammars in terms of number of productions and number of unique discontinuous constituents are given on table 1.
The number of constituents roughly corresponds to the number of productions in the context-free approximation of the grammar.
The parser performance in terms of miliseconds per token is shown in figure 3.
In the evaluation 34272 sentences were parsed and the average time for parsing a given number of tokens is drawn in the chart.
As it can be seen, although the theoretical complexity is polynomial, the real-time performance for practically interesting grammars tends to be linear.
<newSection> 8 Conclusion The algorithm has proven useful in the GF system.
It accomplished the initial goal to provide suggestions in text based dialog systems and in editors for controlled languages.
Additionally the algorithm has properties that were not envisaged in the beginning.
It works with PMCFG directly rather that by approximation with MCFG or some other weaker formalism.
Since the Linear Context-Free Rewriting Systems, Finite-Copying Tree Transducers and Tree Adjoining Grammars can be converted to PMCFG, the algorithm presented in this paper can be used with the converted grammar.
The approach to represent context-dependent grammar as infinite context-free grammar might be applicable to other formalisms as well.
This will make it very attractive in applications where some of the other formalisms are already in use.
<newSection> References<newSection> Abstract This paper presents an application of finite state transducers weighted with feature structure descriptions, following Amtrup (2003), to the morphology of the Semitic language Tigrinya.
It is shown that feature-structure weights provide an efficient way of handling the templatic morphology that characterizes Semitic verb stems as well as the long-distance dependencies characterizing the complex Tigrinya verb morphotactics.
A relatively complete computational implementation of Tigrinya verb morphology is described.
<newSection> 1 Introduction Morphological analysis is the segmentation of words into their component morphemes and the assignment of grammatical morphemes to grammatical categories and lexical morphemes to lexemes.
For example, the English noun parties could be analyzed as party+PLURAL.
Morphological generation is the reverse process.
Both processes relate a surface level to a lexical level.
The relationship between these levels has concerned many phonologists and morphologists over the years, and traditional descriptions, since the pioneering work of Chomsky and Halle (1968), have characterized it in terms of a series of ordered content-sensitive rewrite rules, which apply in the generation, but not the analysis, direction.
Within computational morphology, a very significant advance came with the demonstration that phonological rules could be implemented as finite state transducers (Johnson, 1972; Kaplan and Kay, 1994) (FSTs) and that the rule ordering could be dispensed with using FSTs that relate the surface and lexical levels directly (Koskenniemi, 1983).
Because of the invertibility of FSTs, “two-level” phonology and morphology permitted the creation of systems of FSTs that implemented both analysis (surface input, lexical output) and generation (lexical input, surface output).
In addition to inversion, FSTs are closed under composition.
A second important advance in computational morphology was the recognition by Karttunen et al.
(1992) that a cascade of composed FSTs could implement the two-level model.
This made possible quite complex finite state systems, including ordered alternation rules representing context-sensitive variation in the phonological or orthographic shape of morphemes, the morpho-tactics characterizing the possible sequences of morphemes (in canonical form) for a given word class, and one or more sublexicons.
For example, to handle written English nouns, we could create a cascade of FSTs covering the rules that insert an e in words like bushes and parties and relate lexical y to surface i in words like buggies and parties and an FST that represents the possible sequences of morphemes in English nouns, including all of the noun stems in the English lexicon.
The key feature of such systems is that, even though the FSTs making up the cascade must be composed in a particular order, the result of composition is a single FST relating surface and lexical levels directly, as in two-level morphology.
<newSection> 1.2 FSTs for non-concatenative morphology These ideas have revolutionized computational morphology, making languages with complex word structure, such as Finnish and Turkish, far more amenable to analysis by traditional computational techniques.
However, finite state morphology is inherently biased to view morphemes as sequences of characters or phones and words as concatenations of morphemes.
This presents problems in the case of non-concatenative morphology: discontinuous morphemes (circumfix-ation); infixation, which breaks up a morpheme by inserting another within it; reduplication, by which part or all of some morpheme is copied; and the template morphology (also called stem-pattern morphology, intercalation, and interdigi-tation) that characterizes Semitic languages, and which is the focus of much of this paper.
The stem of a Semitic verb consists of a root, essentially a sequence of consonants, and a pattern, a sort of template which inserts other segments between the root consonants and possibly copies certain of them (see Tigrinya examples in the next section).
Researchers within the finite state framework have proposed a number of ways to deal with Semitic template morphology.
One approach is to make use of separate tapes for root and pattern at the lexical level (Kiraz, 2000).
A transition in such a system relates a single surface character to multiple lexical characters, one for each of the distinct sublexica.
Another approach is to have the transducers at the lexical level relate an upper abstract charac-terization of a stem to a lower string that directly represents the merging of a particular root and pattern.
This lower string can then be compiled into an FST that yields a surface expression (Beesley and Karttunen, 2003).
Given the extra compile-and-replace operation, this resulting system maps directly between abstract lexical expressions and surface strings.
In addition to Arabic, this approach has been applied to a portion of the verb morphology system of the Ethio-Semitic language Amharic (Amsalu and Demeke, 2006), which is characterized by all of the same sorts of complexity as Tigrinya.
A third approach makes use of a finite set of registers that the FST can write to and read from (Cohen-Sygal and Wintner, 2006).
Because it can remember relevant previous states, a “finite-state registered transducer” for template morphology can keep the root and pattern separate as it processes a stem.
This paper proposes an approach which is closest to this last framework, one that starts with familiar extension to FSTs, weights on the transitions.
The next section gives an overview of Tigrinya verb morphology.
The following section discusses weighted FSTs, in particular, with weights consisting of feature structure descriptions.
Then I describe a system that applies this approach to Tigrinya verb morphology.
<newSection> 2 Tigrinya Verb Morphology Tigrinya is an Ethio-Semitic language spoken by 5-6 million people in northern Ethiopia and central Eritrea.
There has been almost no computational work on the language, and there are effectively no corpora or digitized dictionaries containing roots.
For a language with the morphological complexity of Tigrinya, a crucial early step in computational linguistic work must be the development of morphological analyzers and generators.
A Tigrinya verb (Leslau, 1941 is a standard reference for Tigrinya grammar) consists of a stem and one or more prefixes and suffixes.
Most of the complexity resides in the stem, which can be described in terms of three dimensions: root (the only strictly lexical component of the verb), tense-aspect-mood (TAM), and derivational category.
Table 1 illustrates the possible combinations of TAM and derivational category for a single root.1 A Tigrinya verb root consists of a sequence of three, four, or five consonants.
In addition, as in other Ethio-Semitic languages, certain roots include inherent vowels and/or gemination (length-ening) of particular consonants.
Thus among the three-consonant roots, there are three subclasses: CCC, CaCC, CC C.
As we have seen, the stem of a Semitic verb can be viewed as the result of the insertion of pattern vowels between root consonants and the copying of root consonants in particular positions.
For Tigrinya, each combination of root class, TAM, and derivational category is charac-terized by a particular pattern.
With respect to TAM, there are four possibilities, as shown in Table 1, conventionally referred to in English as PERFECTIVE, IMPERFECTIVE, JUSSIVE-IMPERATIVE, and GERUNDIVE.
Word-forms within these four TAM categories combine with auxiliaries to yield the full range of possbil-ities in the complex Tigrinya tense-aspect-mood system.
Since auxiliaries are written as separate words or separated from the main verbs by an apostrophe, they will not be discussed further.
Within each of the TAM categories, a Tigrinya verb root can appear in up to eight different deriva-1I use i for the high central vowel of Tigrinya, a for the mid central vowel, q for the velar ejective, a dot under a character to represent other ejectives, a right quote to represent a glottal stop, a left quote to represent the voiced pharyngeal fricative, and to represent gemination.
Other symbols are conventional International Phonetic Alphabet.
tional categories, which can can be characterized in terms of four binary features, each with particular morphological consequences.
These features will be referred to in this paper as “ps” (“passive”), “tr” (“transitive”), “it” (“iterative”), and “rc” (“re-ciprocal”).
The eight possible combinations of these features (see Table 1 for examples) are SIMPLE [-ps,-tr,-it,-rc], PASSIVE/REFLEXIVE [+ps,-tr,-it,-rc], TRANSITIVE/CAUSATIVE: [-ps,+tr,-it,-rc], FREQUENTATIVE [-ps,-tr,+it,-rc], RECIPROCAL 1 [+ps,-tr,-it,+rc], CAUSATIVE RECIPROCAL 1 [-ps,+tr,-it,+rc], RECIPROCAL 2 [+ps,-tr,+it,-rc], CAUSATIVE RECIPROCAL 2 [-ps,+tr,+it,-rc].
Notice that the [+ps,+it] and [+tr,+it] combinations are roughly equivalent semantically to the [+ps,+rc] and [+tr,+rc] combinations, though this is not true for all verb roots.
The affixes closest to the stem represent subject agreement; there are ten combinations of person, number, and gender in the Tigrinya pronominal and verb-agreement system.
For imperfective and jussive verbs, as in the corresponding TAM categories in other Semitic languages, subject agreement takes the form of prefixes and sometimes also suffixes, for example, y1flet� ‘that he know’, y1flet�u ‘that they (mas.) know’.
In the perfective, imperative, and gerundive, subject agreement is expressed by suffixes alone, for example, feletki ‘you (sg., fem.) knew’, feletu ‘they (mas.) knew!’.
Following the subject agreement suffix (if there is one), a transitive Tigrinya verb may also include an object suffix (or object agreement marker), again in one of the same set of ten possible combinations of person, number, and gender.
There are two sets of object suffixes, a plain set representing direct objects and a prepositional set representing various sorts of dative, benefactive, locative, and instrumental complements, for example, y1felt�en i ‘he knows me’, y1felt�el ey ‘he knows for me’.
Preceding the subject prefix of an imperfective or jussive verb or the stem of a perfective, imperative, or gerundive verb, there may be the prefix indicating negative polarity, ay-.
Non-finite negative verbs also require the suffix -n: y1felt�en i ‘he knows me’; ay 1felt�en 1n ‘he doesn’t know me’.
Preceding the negative prefix (if there is one), an imperfective or perfective verb may also include the prefix marking relativization, (z)1-, for example, zifelten i ‘(he) who knows me’.
The rel-ativizer can in turn be preceded by one of a set of seven prepositions, for example, kabzifelten i ‘from him who knows me’.
Finally, in the perfective, imperfective, and gerundive, there is the possibility of one or the other of several conjunctive prefixes at the beginning of the verb (with-out the relativizer), for example, kifelten i ‘so that he knows me’ and one of several conjunctive suffixes at the end of the verb, for example, y1felt�en 1n ‘and he knows me’.
Given up to 32 possible stem templates (com-binations of four tense-aspect-mood and eight derivational categories) and the various possible combinations of agreement, polarity, rela-tivization, preposition, and conjunction affixes, a Tigrinya verb root can appear in well over 100,000 different wordforms.
Tigrinya shares with other Semitic languages complex variations in the stem patterns when the root contains glottal or pharyngeal consonants or semivowels.
These and a range of other regular language-specific morphophonemic processes can be captured in alternation rules.
As in other Semitic languages, reduplication also plays a role in some of the stem patterns (as seen in Table 1).
Furthermore, the second consonant of the most important conjugation class, as well as the consonant of most of the object suffixes, geminates in certain environments and not others (Buckley, 2000), a process that depends on syllable weight.
The morphotactics of the Tigrinya verb is replete with dependencies which span the verb stem: (1) the negative circumfix ay-n, (2) absence of the negative suffix -n following a subordinating prefix, (3) constraints on combinations of subject agreement prefixes and suffixes in the imperfective and jussive, (4) constraints on combinations of subject agreement affixes and object suffixes.
There is also considerable ambiguity in the system.
For example, the second person and third person feminine plural imperfective and jussive subject suffix is identical to one allomorph of the third person feminine singular object suffix (y1fE t�a) ’he knows her; they (fem.) know’).
Tigrinya is written in the Ge’ez (Ethiopic) syllabary, which fails to mark gemination and to distinguish between syllable final consonants and consonants followed by the vowel 1.
This introduces further ambiguity.
In sum, the complexity of Tigrinya verbs presents a challenge to any computational morphology framework.
In the next section I consider an augmentation to finite state morphology offering clear advantages for this language.
<newSection> 3 FSTs with Feature Structures A weighted FST (Mohri et al., 2000) is a finite state transducer whose transitions are augmented with weights.
The weights must be elements of a semiring, an algebraic structure with an “addition” operation, a “multiplication” operation, identity elements for each operation, and the constraint that multiplication distributes over addition.
Weights on a path of transitions through a transducer are “multiplied”, and the weights associated with alternate paths through a transducer are “added”.
Weighted FSTs are closed under the same operations as unweighted FSTs; in particular, they can be composed.
Weighted FSTs are familiar in speech processing, where the semiring elements usually represent probabilities, with “mul-tiplication” and “addition” in their usual senses.
Amtrup (2003) recognized the advantages that would accrue to morphological analyzers and generators if they could accommodate structured representations.
One familiar approach to representing linguistic structure is feature structures (FSs) (Carpenter, 1992; Copestake, 2002).
A feature structure consists of a set of attribute-value pairs, for which values are either atomic properties, such as FALSE or FEMININE, or feature structures.
For example, we might represent the morphological structure of the Tigrinya noun gEzay ‘my house’ as [lex=gEza, num=sing, poss=[pers=1, num=sg]].
The basic operation over FSs is unification.
Loosely speaking, two FSs unify if their attribute-values pairs are compatible; the resulting unification combines the features of the FSs.
For example, the two FSs [lex=gEza, num=sg] and [poss=[pers=1, num=sg]] unify to yield the FS [lex=gEza, num=sg, poss=[pers=1, num=sg]].
The distinguished FS TOP unifies with any other FS.
Amtrup shows that sets of FSs constitute a semiring, with pairwise unification as the multi-plication operator, set union as the addition operator, TOP as the identity element for multiplication, and the empty set as the identity element for addition.
Thus FSTs can be weighted with FSs.
In an FST with FS weights, traversing a path through the network for a given input string yields an FS set, in addition to the usual output string.
The FS set is the result of repeated unification of the FS sets on the arcs in the path, starting with an initial input FS set.
A path through the network fails not only if the current input character fails to match the input character on the arc, but also if the current accumulated FS set fails to unify with the FS set on an arc.
Using examples from Persian, Amtrup demonstrates two advantages of FSTs weighted with FS sets.
First, long-distance dependencies within words present notorious problems for finite state techniques.
For generation, the usual approach is to overgenerate and then filter out the illegal strings below, but this may result in a much larger network because of the duplication of state descriptions.
Using FSs, enforcing long-distance constraints is straightforward.
Weights on the relevant transitions early in the word specify values for features that must agree with similar feature specifications on transitions later in the word (see the Tigrinya examples in the next section).
Second, many NLP applications, such a machine translation, work with the sort of structured representations that are elegantly handled by FS descriptions.
Thus it is often desirable to have the output of a morphological analyzer exhibit this richness, in contrast to the string representations that are the output of an unweighted finite state analyzer.
<newSection> 4 Weighted FSTs for Tigrinya Verbs As we have seen, Tigrinya verbs exhibit various sorts of long-distance dependencies.
The cir-cumfix that marks the negative of non-subordinate verbs, ay...n, is one example.
Figure 1 shows how this constraint can be handled naturally using an FST weighted with FS sets.
In place of the separate negative and affirmative subnetworks that would have to span the entire FST in the abs-cence of weighted arcs, we have simply the negative and affirmative branches at the beginning and end of the weighted FST.
In the analysis direction, this FST will accept forms such as ay 1fElt˙un ‘they don’t know’ and y1fElt˙u ‘they know’ and reject forms such as ay 1fElt˙u.
In the generation direction, the FST will correctly generate a form such as ay 1fElt˙un given a initial FS that includes the feature [pol=neg].
Now consider the source of most of the complexity of the Tigrinya verb, the stem.
The stem may be thought of as conveying three types of information: lexical (the root of the verb), derivational, and TAM.
However, unlike the former two types, the TAM category of the verb is redundantly coded for by the combination of subject agreement affixes.
Thus, analysis of a stem should return at least the root and the derivational category, and generation should start with a root and a derivational category and return a stem.
We can represent each root as a sequence of consonants, separated in some cases by the vowel a or the gemination character ( ).
Given a particular derivational pattern and a TAM category, extracting the root from the stem is a straightforward matter with an FST.
For example, for the imperfective passive, the CC C root pattern appears in the template C1C EC, and the root is what is left if the two vowels in the stem are skipped over.
However, we want to extract both the derivational pattern and the root, and the problem for finite state methods, as discussed in Section 1.2, is that both are spread throughout the stem.
The analyzer needs to alternate between recording elements of the root and clues about the derivational pattern as it traverses the stem, and the generator needs to alternate between outputting characters that represent root elements and characters that depend on the derivational pattern as it produces the stem.
The process is complicated further because some stem characters, such as the gemination character, may be either lexical (that is, a root element) or derivational, and others may provide information about both components.
For example, a stem with four consonants and a separating the second and third consonants represents the fre-quentative of a three-consonant root if the third and fourth consonants are identical (e.g., fElalEt˙ ’knew repeatedly’, root: flt˙) and a four-consonant root (CCaCC root pattern) in the simple derivational category if they are not (e.g., kElakEl ’pre-vented’, root klakl).
As discussed in Section 1.2, one of the familiar approaches to this problem, that of Beesley and Karttunen (2003), precompiles all of the combinations of roots and derivational patterns into stems.
The problem with this approach for Tigrinya is that we do not have anything like a complete list of roots; that is, we expect many stems to be novel and will need to be able to analyze them on the fly.
The other two approaches discussed in 1.2, that of Kiraz (2000) and that of Cohen-Sygal & Wintner (2006), are closer to what is proposed here.
Each has an explicit mechanism for keeping the root and pattern distinct: separate tapes in the case of Kiraz (2000) and separate memory registers in the case of Cohen-Sygal & Wintner (2006).
The present approach also divides the work of processing the root and the derivational patterns between two components of the system.
However, instead of the additional overhead required for implementing a multi-tape system or registers, this system makes use of the FSTs weighted with FSs that are already motivated for other aspects of morphology, as argued above.
In this approach, the lexical aspects of morphology are handled by the ordinary input-output character correspondences, and the grammatical aspects of morphology, in particular the derivational patterns, are handled by the FS weights on the FST arcs and the unification that takes place as accumulated weights are matched against the weights on FST arcs.
As explained in Section 2, we can represent the eight possible derivational categories for a Tigrinya verb stem in terms of four binary features (ps, tr, rc, it).
Each of these features is reflected more or less directly in the stem form (though differently for different root classes and for different TAM categories).
However, they are sometimes distributed across the stem: different parts of a stem may be constrained by the presence of a particular feature.
For example, the feature +ps (abbreviating [ps=True]) causes the gemination of the stem-initial consonant under various circum-stances and also controls the final vowel in the stem in the imperfective, and the feature +tr is marked by the vowel a before the first root consonant and, in the imperfective, by the nature of the vowel that follows the first root consonant (E where we would otherwise expect 1, 1 where we would otherwise expect E.)
That is, as with the verb affixes, there are long-distance dependencies within the verb stem.
Figure 2 illustrates this division of labor for the portion of the stem FST that covers the CC C root pattern for the imperfective.
This FST (including the subnetwork not shown that is responsible for the reduplicated portion of the +it patterns) handles all eight possible derivational categories.
For the root Vfs.m ’finish’, the stems are [-ps,-tr,-rc,-it]: f1s˙ 1m, [+ps,-tr,-rc,-it]: f1s˙ Em, [-ps,+tr,-rc,-it]: afEs˙ 1m, [-ps,-tr,-rc,+it]: fEs˙ as˙ 1m, [+ps,-tr,+rc,-it]: f as˙ Em, [-ps,+tr,+rc,-it]: af as˙ 1m, [+ps,-tr,-rc,+it]: f Es˙ as˙ Em, [-ps,+tr,-rc,+it]: af Es˙ as˙ 1m.
What is notable is the relatively small number of states that are required; among the consonant and vowel positions in the stems, all but the first are shared among the various derivational categories.
Of course the full stem FST, applying to all combinations of the eight root classes, the eight derivational categories, and the four TAM categories, is much larger, but the FS weights still permit a good deal of sharing, including sharing across the root classes and across the TAM categories.
The full verb morphology processing system (see Figure 3) consists of analysis and generation FSTs for both orthographic and phonemically represented words, four FSTs in all.
Eleven FSTs are composed to yield the phonemic analysis FST (de-noted by the dashed border in Figure 3), and two additional FSTs are composed onto this FST to yield the orthographic FST (denoted by the large solid rectangle).
The generation FSTs are created by inverting the analysis FSTs.
Only the orthographic FSTs are discussed in the remainder of this paper.
At the most abstract (lexical) end is the heart of the system, the morphotactic FST, and the heart of this FST is the stem FST described above.
The stem FST is composed from six FSTs, including three that handle the morphotactics of the stem, one that handles root constraints, and two that handle phonological processes that apply only to the stem.
A prefix FST and a suffix FST are then concatenated onto the composed stem FST to create the full verb morphotactic FST.
Within the whole FST, it is only the morphotactic FSTs (the yellow rectangles in Figure 3) that have FS weights.2 In the analysis direction, the morphotactic FST takes as input words in an abstract canonical form and an initial weight of TOP; that is, at this point in analysis, no grammatical information has been extracted.
The output of the morphotactic FST is either the empty list if the form is unanalyz-able, or one or more analyses, each consisting of a root string and a fully specified grammatical description in the form of an FS.
For example, given the form ’ayt1f1l et˙un, the morpho-tactic FST would output the root flt. and the FS [tam=imprf, der=[+ps,-tr,-rc,-it], sbj=[+2p,+plr,-fem], +neg, obj=nil, -rel] (see Figure 3).
That is, this word represents the imperfective, negative, non-relativized passive of the verb root A (‘know’) with second person plural masculine subject and no object: ’you (plr., mas.) are not known’.
The system has no actual lexicon, so it outputs all roots that are compatible with the input, even if such roots do not exist in the language.
In the generation direction, the opposite happens.
In this case, the input root can be any legal sequence of characters that matches one of the eight 2The reduplication that characterizes [+it] stems and the “anti-reduplication” that prevents sequences of identical root consonants in some positions are handled with separate transitions for each consonant pair.
shown, which handles the reduplicated portion of +it stems, for example, fesas 1m.
root patterns (there are some constraints on what can constitute a root), though not necessarily an actual root in the language.
The highest FST below the morphotactic FST handles one case of allomorphy: the two allo-morphs of the relativization prefix.
Below this are nine FSTs handling phonology; for example, one of these converts the sequence a1 to e.
At the bottom end of the cascade are two orthographic FSTs which are required when the input to analysis or the output of generation is in standard Tigrinya orthography.
One of these is responsible for the insertion of the vowel 1 and for consonant gemination (neither of which is indicated in the orthography); the other inserts a glottal stop before a word-initial vowel.
The full orthographic FST consists of 22,313 states and 118,927 arcs.
The system handles verbs in all of the root classes discussed by Leslau (1941), including those with laryngeals and semivowels in different root positions and the three common irregular verbs, and all grammatical combinations of subject, object, negation, rel-ativization, preposition, and conjunction affixes.
For the orthographic version of the analyzer, a word is entered in Ge’ez script (UTF-8 encoding).
The program romanizes the input using the SERA transcription conventions (Firdyiwek and Yaqob, 1997), which represent Ge’ez characters with the ASCII character set, before handing it to the orthographic analysis FST.
For each possible analysis, the output consists of a (romanized) root and a FS set.
Where a set contains more than one FS, the interpretation is that any of the FS elements constitutes a possible analysis.
Input to the generator consists of a romanized root and a single feature structure.
The output of the orthographic generation FST is an orthographic representation, using SERA conventions, of each possible form that is compatible with the input root and FS.
These forms are then converted to Ge’ez orthography.
The analyzer and generator are publicly accessible on the Internet at Systematic evaluation of the system is difficult since no Tigrinya corpora are currently available.
One resource that is useful, however, is the Tigrinya word list compiled by Biniam Gebremichael, available on the Internet at www.cs.ru.nl/ biniam/geez/crawl.php.
Biniam extracted 227,984 distinct wordforms from Tigrinya texts by crawling the Internet.
As a first step toward evaluating the morphological analyzer, the orthographic analyzer was run on 400 word-forms selected randomly from the list compiled by Biniam, and the results were evaluated by a human reader.
Of the 400 wordforms, 329 were unambigu-ously verbs.
The program correctly analyzed 308 of these.
The 21 errors included irregular verbs and orthographic/phonological variants that had not been built into the FST; these will be straightforward to add.
Fifty other words were not verbs.
The program again responded appropriately, given its knowledge, either rejecting the word or analyzing it as a verb based on a non-existent root.
Thirteen other words appeared to be verb forms containing a simple typographical error, and I was unable to identify the remaining eight words.
For the latter two categories, the program again responded by rejecting the word or treating it as a verb based on a non-existent root.
To test the morphological generator, the program was run on roots belonging to all 21 of the major classes discussed by Leslau (1941), including those with glottal or pharyngeal consonants or semivowels in different positions within the roots.
For each of these classes, the program was asked to generate all possible derivational patterns (in the third person singular masculine form).
In addition, for a smaller set of four root classes in the simple derivational pattern, the program was tested on all relevant combinations of the subject and object affixes3 and, for the imperfective and perfective, on 13 combinations of the relativization, negation, prepositional, and conjunctive affixes.
For each of the 272 tests, the generation FST succeeded in outputting the correct form (and in some cases a phonemic and/or orthographic alternative).
In conclusion, the orthographic morphological analyzer and generator provide good coverage of 3With respect to their morphophonological behavior, the subject affixes and object suffixes each group into four categories.
Tigrinya verbs.
One weakness of the present system results from its lack of a root dictionary.
The analyzer produces as many as 15 different analyses of words, when in many cases only one contains a root that exists in the language.
The number could be reduced somewhat by a more extensive filter on possible root segment sequences; however, root internal phonotactics is an area that has not been extensively studied for Tigrinya.
In any case, once a Tigrinya root dictionary becomes available, it will be straightforward to compose a lexical FST onto the existing FSTs that will reject all but acceptable roots.
Even a relatively small root dictionary should also permit inferences about possible root segment sequences in the language, enabling the construction of a stricter filter for roots that are not yet contained in the dictionary.
<newSection> 5 Conclusion Progress in all applications for a language such as Tigrinya is held back when verb morphology is not dealt with adequately.
Tigrinya morphology is complex in two senses.
First, like other Semitic languages, it relies on template morphology, presenting unusual challenges to any computational framework.
This paper presents a new answer to these challenges, one which has the potential to integrate morphological processing into other knowledge-based applications through the inclusion of the powerful and flexible feature structure framework.
This approach should extend to other Semitic languages, such as Arabic, Hebrew, and Amharic.
Second, Tigrinya verbs are simply very elaborate.
In addition to the stems resulting from the intercalation of eight root classes, eight derivational patterns and four TAM categories, there are up to four prefix slots and four suffix slots; various sorts of prefix-suffix dependencies; and a range of interacting phonological processes, including those sensitive to syllable structure, as well as segmental context.
Just putting together all of these constraints in a way that works is significant.
Since the motivation for this project is primarily practical rather than theoretical, the main achievement of the paper is the demonstration that, with some effort, a system can be built that actually handles Tigrinya verbs in great detail.
Future work will focus on fine-tuning the verb FST, developing an FST for nouns, and applying this same approach to other Semitic languages.
Mehryar Mohri, Fernando Pereira, and Michael Riley.
2000. Weighted finite-state transducers in speech recognition.
In Proceedings of ISCA ITRW on Automatic Speech Recognition: Challenges for the Mil-lenium, pages 97–106, Paris.
<newSection> References Saba Amsalu and Girma A. Demeke.
2006. Non-concatenative finite-state morphotactics of Amharic simple verbs.
ELRC Working Papers, 2(3).