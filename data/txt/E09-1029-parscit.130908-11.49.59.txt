<newSection>                 Abstract              The paper presents a multi-document summarization system which builds company-specific summaries from a collection of financial news such that the extracted sentences contain novel and relevant information about the corresponding organization.
The user’s familiarity with the com                pany’s profile is assumed.
The goal of                 such summaries is to provide information useful for the short-term trading of the cor                responding company, i.e., to facilitate the inference from news to stock price move                ment in the next day.
We introduce a novel query (i.e., company name) expansion method and a simple unsupervized algorithm for sentence ranking.
The system shows promising results in compari                son with a competitive baseline.
<newSection>                 1 Introduction                              Automatic text summarization has been a field of active research in recent years.
While most methods are extractive, the implementation details differ considerably depending on the goals of a sum                marization system.
Indeed, the intended use of the summaries may help significantly to adapt a par                ticular summarization approach to a specific task whereas the broadly defined goal of preserving rel                evant, although generic, information may turn out                 to be of little use.
In this paper we present a system whose goal is                 to extract sentences from a collection of financial              news to inform about important events concern                ing companies, e.g., to support trading (i.e., buy or                 sell) the corresponding symbol on the next day, or                 managing a portfolio.
For example, a company’s                 announcement of surpassing its earnings’ estimate                 is likely to have a positive short-term effect on its                 stock price, whereas an announcement of job cuts                 is likely to have the reverse effect.
We demonstrate                 how existing methods can be extended to achieve                 precisely this goal.
In a way, the described task can be classified                 as query-oriented multi-document summarization                 because we are mainly interested in information                 related to the company and its sector.
However,                 there are also important differences between the                 two tasks.
• The name of the company is not a query,                 e.g., as it is specified in the context of the DUC competitions1, and requires an exten                sion.
Initially, a query consists exclusively                 of the “symbol”, i.e., the abbreviation of the                 name of a company as it is listed on the stock market.
For example, WPO is the abbrevia                tion used on the stock market to refer to The Washington Post–a large media and education company.
Such symbols are rarely en                countered in the news and cannot be used to                 find all the related information.
• The summary has to provide novel informa                tion related to the company and should avoid general facts about it which the user is sup                posed to know.
This point makes the task                 related to update summarization where one                 has to provide the user with new information                              given some background knowledge2.
In our case, general facts about the company are as                sumed to be known by the user.
Given WPO, we want to distinguish between The Wash                ington Post is owned by The Washington Post                 Company, a diversified education and media                 company and The Post recently went through                 its third round of job cuts and reported an                 11% decline in print advertising revenues for                 its first quarter, the former being an example of background information whereas the lat                ter is what we would like to appear in the                 summary.
Thus, the similarity to the query alone is not the decisive parameter in com                puting sentence relevance.
• While the summaries must be specific for a given organization, important but general fi                nancial events that drive the overall market must be included in the summary.
For example, the recent subprime mortgage crisis af                fected the entire economy regardless of the                 sector.
Our system proceeds in the three steps illus                trated in Figure 1.
First, the company symbol is expanded with terms relevant for the company, ei                ther directly – e.g., iPod is directly related to Apple                 Inc.
– or indirectly – i.e., using information about                 the industry or sector the company operates in.
We                 detail our symbol expansion algorithm in Section 3.
Second, this information is used to rank sen                tences based on their relatedness to the expanded query and their overall importance (Section 4).
Fi                nally, the most relevant sentences are re-ranked                 based on the degree of novelty they carry (Section                 5).
The paper makes the following contributions.
First, we present a new query expansion technique which is useful in the context of company-                dependent news summarization as it helps identify                 sentences important to the company.
Second, we introduce a simple and efficient method for sentence ranking which foregrounds novel informa                tion of interest.
Our system performs well in terms of the ROUGE score (Lin & Hovy, 2003) com                pared with a competitive baseline (Section 6).
<newSection>                 2 Data                              The data we work with is a collection of financial news consolidated and distributed by Yahoo!
Fi-             nance3 from various sources4.
Each story is la                beled as being relevant for a company – i.e., it                 appears in the company’s RSS feed – if the story                 mentions either the company itself or the sector the company belongs to.
Altogether the corpus con                tains 88,974 news articles from a period of about                 5 months (148 days).
Some articles are labeled                 as being relevant for several companies.
The total                 number of (company name, news collection) pairs                 is 46,444.
The corpus is cleaned of HTML tags, embed                ded graphics and unrelated information (e.g., ads,                 frames) with a set of manually devised rules.
The                 filtering is not perfect but removes most of the                 noise.
Each article is passed through a language                 processing pipeline (described in (Atserias et al.,                 2008)).
Sentence boundaries are identified by                 means of simple heuristics.
The text is tokenized according to Penn TreeBank style and each to                ken lemmatized using Wordnet’s morphological                 functions.
Part of speech tags and named entities                 (LOC, PER, ORG, MISC) are identified by means of a publicly available named-entity tagger5 (Cia-                ramita & Altun, 2006, SuperSense).
Apart from                 that, all sentences which are shorter than 5 tokens                 and contain neither nouns nor verbs are sorted out.
We apply the latter filter as we are interested in                 textual information only.
Numeric information                 contained, e.g., in tables can be easily and more                 reliably obtained from the indices tables available                 online.
<newSection>                 3 Query Expansion              In company-oriented summarization query expansion is crucial because, by default, our query con                tains only the symbol, that is the abbreviation of the name of the company.
Unfortunately, exist                ing query expansion techniques which utilize such                 knowledge sources as WordNet or Wikipedia are                 not useful for symbol expansion.
WordNet does                 not include organizations in any systematic way.
Wikipedia covers many companies but it is unclear                 how it can be used for expansion.
Intuitively, a good expansion method should                 provide us with a list of products, or properties, of the company, the field it operates in, the typi                cal customers, etc.
Such information is normally                 found on the profile page of a company at Yahoo!
Finance6.
There, so called “business summaries” provide succinct and financially relevant informa                tion about the company.
Thus, we use business summaries as follows.
For every company sym                bol in our collection, we download its business                 summary, split it into tokens, remove all words                 but nouns and verbs which we then lemmatize.
Since words like company are fairly uninforma-                tive in the context of our task, we do not want to                 include them in the expanded query.
To filter out                 such words, we compute the company-dependent                 TF*IDF score for every word on the collection of                 all business summaries:                              where c is the business summary of a company,                 tfw,c is the frequency of w in c, N is the total                 number of business summaries we have, cfw is                 the number of summaries that contain w.
This formula penalizes words occurring in most sum                maries (e.g., company, produce, offer, operate, found, headquarter, management).
At the mo                ment of running the experiments, N was about 3,000, slightly less than the total number of sym-                6http://finance.yahoo.com/q/pr?s=AAPL                 where the trading symbol of any company can be used                 instead of AAPL.
bols because some companies do not have a business summary on Yahoo!
Finance.
It is impor                tant to point out that companies without a business summary are usually small and are seldom mentioned in news articles: for example, these compa                nies had relevant news articles in only 5% of the                 days monitored in this work.
Table 1 gives the ten high scoring words for                 three companies (Apple Inc.
– the computer and software manufacture, Delta Air Lines – the air                line, and DaVita – dyalisis services).
Table 1                 shows that this approach succeeds in expanding the symbol with terms directly related to the company, e.g., ipod for Apple, but also with more gen                eral information like the industry or the company                 operates in, e.g., software and computer for Apple.
All words whose TF*IDF score is above a certain                 threshold 0 are included in the expanded query (0                 was tuned to a value of 5.0 on the development                 set).
<newSection>                 4 Relatedness to Query                              Once the expanded query is generated, it can be                 used for sentence ranking.
We chose the system of                 Otterbacher et al.
(2005) as a a starting point for                 our approach and also as a competitive baseline because it has been successfully tested in a simi                lar setting–it has been applied to multi-document                 query-focused summarization of news documents.
Given a graph G = (5, E), where 5 is the set                 of all sentences from all input documents, and E is                 the set of edges representing normalized sentence similarities, Otterbacher et al.
(2005) rank all sen             as in the query are stemmed and stopwords are removed from them).
Relevance to the query is de                fined in Equation (6) which has been previously                 used for sentence retrieval (Allan et al., 2003):                              tence nodes based on the inter-sentence relations                 as well as the relevance to the query q.
Sentence                 ranks are found iteratively over the set of graph                 nodes with the following formula:              The first term represents the importance of a sen                tence defined in respect to the query, whereas the                 second term infers the importance of the sentence from its relation to other sentences in the collec                tion.
A ∈ (0, 1) determines the relative importance                 of the two terms and is found empirically.
Another parameter whose value is determined experimen-                tally is the sentence similarity threshold T, which determines the inclusion of a sentence in G.
Ot-                terbacher et al.
(2005) report 0.2 and 0.95 to be                 the optimal values for T and A respectively.
These                 values turned out to produce the best results also                 on our development set and were used in all our experiments.
Similarity between sentences is de                fined as the cosine of their vector representations:                              where tfw,s is the frequency of w in sentence s, |S |is the total number of sentences in the docu                ments from which sentences are to be extracted,                 and sfw is the number of sentences which contain                 the word w (all words in the documents as well                 w∈q where tfw,x stands for the number of times w ap                pears in x, be it a sentence (s) or the query (q).
If                 a sentence shares no words other than stopwords                 with the query, the relevance becomes zero.
Note that without the relevance to the query part Equation 2 takes only inter-sentence similarity into ac                count and computes the weighted PageRank (Brin                 & Page, 1998).
In defining the relevance to the query, in Equa                tion (6), words which do not appear in too many                 sentences in the document collection weigh more.
Indeed, if a word from the query is contained in                 many sentences, it should not count much.
But it                 is also true that not all words from the query are                 equally important.
As it has been mentioned in                 Section 3, words like product or offer appear in                 many business summaries and are equally related                 to any company.
To penalize such words, when                 computing the relevance to the query, we multiply the relevance score of a given word w with the in                verted document frequency of w on the corpus of                 business summaries Q – idfw,Q:                              We also replace tfw,s with the indicator function s(w) since it has been reported to be more ad                equate for sentences, in particular for sentence                 alignment (Nelken & Shieber, 2006):                              Thus, the modified formula we use to compute                 sentence ranks is as follows:                              We call these two ranking algorithms that use                 the formula in (2) OTTERBACHER and QUERY WEIGHTS, the difference being the way the rel                evance to the query is computed: (6) or (9).
We                 use the OTTERBACHER algorithm as a baseline in                 the experiments reported in Section 6.
<newSection>                 5 Novelty Bias              Apart from being related to the query, a good summary should provide the user with novel infor                mation.
According to Equation (2), if there are,                 say, two sentences which are highly similar to the                 query and which share some words, they are likely                 to get a very high score.
Experimenting with the                 development set, we observed that sentences about the company, such as e.g., DaVita, Inc. is a lead                ing provider of kidney care in the United States, providing dialysis services and education for patients with chronic kidney failure and end stage re                nal disease, are ranked high although they do not                 contribute new information.
However, a non-zero                 similarity to the query is indeed a good filter of the information related to the company and to its sec                tor and can be used as a prerequisite of a sentence                 to be included in the summary.
These observations                 motivate our proposal for a ranking method which                 aims at providing relevant and novel information                 at the same time.
Here, we explore two alternative approaches to                 add the novelty bias to the system:                 • The first approach bypasses the relatedness to query step introduced in Section 4 completely.
Instead, this method merges the dis                covery of query relatedness and novelty into                 a single algorithm, which uses a sentence graph that contains edges only between sen                tences related to the query, (i.e., sentences for                 which rel(s|q) > 0).
All edges connecting                 sentences which are unrelated to the query                 are skipped in this graph.
In this way we limit the novelty ranking process to a subset of sen                tences related to the query.
• The second approach models the problem                 in a re-ranking architecture: we take the top ranked sentences after the relatedness-toquery filtering component (Section 4) and re-rank them using the novelty formula intro                duced below.
The main difference between the two approaches                 is that the former uses relatedness-to-query and novelty information but ignores the overall importance of a sentence as given by the PageRank al                gorithm in Section 4, while the latter combines all these aspects –i.e., importance of sentences, relat                edness to query, and novelty– using the re-ranking                 architecture.
To amend the problem of general information ranked inappropriately high, we modify the word-weighting formula (4) so that it implements a nov                elty bias, thus becoming dependent on the query.
A straightforward way to define the novelty weight                 of a word would be to draw a line between the “known” words, i.e., words appearing in the busi                ness summary, and the rest.
In this approach all                 the words from the business summary are equally                 related to the company and get the weight of 0:                              We call this weighting scheme SIMPLE.
As an alternative, we also introduce a more elab                orate weighting procedure which incorporates                 the relatedness-to-query (or rather distance from                 query) in the word weight formula.
Intuitively, the                 more related to the query a word is (e.g., DaVita,                 the name of the company), the more familiar to the                 user it is and the smaller its novelty contribution                 is.
If a word does not appear in the query at all, its                 weight becomes equal to the usual tfw,sidfw,S:                              The overall novelty ranking formula is based                 on the query-dependent PageRank introduced in Equation (2).
However, since we already incorporate the relatedness to the query in these two set                tings, we focus only on related sentences and thus                 may drop the relatedness to the query part from                 (2):                              We set A to the same value as in OTTERBACHER.
We deliberately set the sentence similarity thresh                old T to a very low value (0.05) to prevent the                 graph from becoming exceedingly bushy.
Note                 that this novelty-ranking formula can be equally applied in both scenarios introduced at the begin                ning of this section.
In the first scenario, S stands                 for the set of nodes in the graph that contains only sentences related to the query.
In the second sce                nario, S contains the highest ranking sentences                 detected by the relatedness-to-query component                 (Section 4).
Some sentences are repeated several times in the                 collection.
Such repetitions, which should be avoided in the summary, can be filtered out either before or after the sentence ranking.
We ap                ply a simple repetition check when incrementally adding ranked sentences to the summary.
If a sen                tence to be added is almost identical to the one already included in the summary, we skip it.
Iden                tity check is done by counting the percentage of                 non-stop word lemmas in common between two                 sentences.
95% is taken as the threshold.
We do not filter repetitions before the rank                ing has taken place because often such repetitions carry important and relevant information.
The redundancy filter is applied to all the systems described as they are equally prone to include repe                titions.                              and human annotators jackknife for each (query, summary) pair and computed a macro-average to make human and au                tomatic results comparable (Dang, 2005).
The scores computed on summaries produced by hu                mans are given in the bottom line (MANUAL) and                 serve as upper bound and also as an indicator for                 the inter-annotator agreement.
<newSection>                 6 Evaluation                              We randomly selected 23 company stock names,                 and constructed a document collection for each containing all the news provided in the Yahoo!
Fi                nance news feed for that company in a period of                 two days (the time period was chosen randomly).
The average length of a news collection is about                 600 tokens.
When selecting the company names,                 we took care of not picking those which have only                 a few news articles for that period of time.
This resulted into 9.4 news articles per collection on av                erage.
From each of these, three human annotators independently selected up to ten sentences.
All an                notators had average to good understanding of the                 financial domain.
The annotators were asked to                 choose the sentences which could best help them                 decide whether to buy, sell or retain stock for the                 company the following day and present them in the order of decreasing importance.
The anno                tators compared their summaries of the first four collections and clarified the procedure before proceeding with the other ones.
These four collec                tions were then later used as a development set.
All summaries – manually as well as automat                ically generated – were cut to the first 250 words                 which made the summaries 10 words shorter on average.
We evaluated the performance automat                ically in terms of ROUGE-2 (Lin & Hovy, 2003) using the parameters and following the methodology from the DUC events.
The results are presented in Table 2.
We also report the 95% confi                dence intervals in brackets.
As in DUC, we used                              From Table 2 follows that the modifications we                 applied to the baseline are sensible and indeed bring an improvement.
QUERY WEIGHTS per                forms better than OTTERBACHER and is in turn outperformed by the algorithms biased to novel information (the two NOVELTY systems).
The overlap between the confidence intervals of the baseline and the simple version of the novelty algo                rithm is minimal (0.002).
It is remarkable that the achieved improvement                 is due to a more balanced relatedness to the query ranking (9), as well as to the novelty bias reranking.
The fact that the simpler novelty weight                ing formula (10) produced better results than the more elaborated one (11) requires a deeper anal                ysis and a larger test set to explain the difference.
Our conjecture so far is that the SIMPLE approach                 allows for a better combination of both novelty                 and relatedness to query.
Since the more complex                 novelty ranking formula penalizes terms related                 to the query (Equation (11)), it favors a scenario where novelty is boosted in detriment of related                ness to query, which is not always realistic.
It is important to note that, compared with the                 baseline, we did not do any parameter tuning for                 λ and the inter-sentence similarity threshold.
The                 improvement between the system of Otterbacher                 et al.
(2005) and our best model is statistically                 significant.
Recall from Section 5 that the motivation for pro                moting novel information came from the fact that                 sentences with background information about the company obtained very high scores: they were related but not novel.
The sentences ranked by OT-TERBACHER or QUERY WEIGHTS required a re                ranking to include related and novel sentences in the summary.
We checked whether novelty re                ranking brings an improvement if added on top                 of a system which does not have a novelty bias                 (baseline or QUERY WEIGHTS) and compared it                 with the setting where we simply limit the novelty                 ranking to all the sentences related to the query (NOVELTY SIMPLE and NOVELTY).
In the simi                larity graph, we left only edges between the first                 30 sentences from the ranked list produced by                 one of the two algorithms described in Section 4                 (OTTERBACHER or QUERY WEIGHTS).
Then we                 ranked the sentences biased to novel information the same way as described in Section 5.
The results are presented in Table 3.
What we evalu                ate here is whether a combination of two methods performs better than the simple heuristics of dis                carding edges between sentences unrelated to the                 query.
From the four possible combinations, there is                 an improvement over the baseline only (0.255 vs. 0.280 resp. 0.273).
None of the combinations performs better than the simple novelty bias algorithm on a subset of edges.
This experiment sug                gests that, at least in the scenario investigated here (short-term monitoring of publicly-traded compa                nies), novelty is more important than relatedness to query.
Hence, the simple novelty bias algo                rithm, which emphasizes novelty and incorporates relatedness to query only through a loose constraint (rel(slq) > 0) performs better than com                plex models, which are more constrained by the                 relatedness to query.
<newSection>                 7 Related Work                              Summarization has been extensively investigated in recent years and to date there exists a multi                tude of very different systems.
Here, we review                 those that come closest to ours in respect to the                 task and that concern extractive multi-document                 query-oriented summarization.
We also mention                 some work on using textual news data for stock                 indices prediction which we are aware of.
Stock market prediction: W¨uthrich et al.
(1998) were among the first who introduced an automatic stock indices prediction system which relies on textual information only.
The system gen                erates weighted rules each of which returns the probability of the stock going up, down or remain                ing steady.
The only information used in the rules                 is the presence or absence of certain keyphrases                 provided by a human expert who “judged them                 to be influential factors potentially moving stock markets”.
In this approach, training data is re                quired to measure the usefulness of the keyphrases for each of the three classes.
More recently, Ler-                man et al.
(2008) introduced a forecasting system for prediction markets that combines news analysis with a price trend analysis model.
This approach was shown to be successful for the forecasting of public opinion about political candi                dates in such prediction markets.
Our approach can be seen as a complement to both these approaches, necessary especially for financial mar                kets where the news typically cover many events,                 only some related to the company of interest.
Unsupervized summarization systems extract                 sentences whose relevance can be inferred from the inter-sentence relations in the document col                lection.
In (Radev et al., 2000), the centroid of                 the collection, i.e., the words with the highest                 TF*IDF, is considered and the sentences which contain more words from the centroid are extracted.
Mihalcea & Tarau (2004) explore sev                eral methods developed for ranking documents                 in information retrieval for the single-document                 summarization task.
Similarly, Erkan & Radev                 (2004) apply in-degree and PageRank to build a                 summary from a collection of related documents.
They show that their method, called LexRank,                 achieves good results.
In (Otterbacher et al., 2005;                 Erkan, 2006) the ranking function of LexRank is                 extended to become applicable to query-focused summarization.
The rank of a sentence is deter                mined not just by its relation to other sentences in                              the document collection but also by its relevance                 to the query.
Relevance to the query is defined as the word-based similarity between query and sen                tence.
Query expansion has been used for improving information retrieval (IR) or question answer                ing (QA) systems with mixed results.
One of the                 problems is that the queries are expanded word                 by word, ignoring the context and as a result the                 extensions often become inadequate7.
However, Riezler et al.
(2007) take the entire query into account when adding new words by utilizing tech                niques used in statistical machine translation.
Query expansion for summarization has not yet                 been explored as extensively as in IR or QA.
Nastase (2008) uses Wikipedia and WordNet for                 query expansion and proposes that a concept can be expanded by adding the text of all hyper-                links from the first paragraph of the Wikipedia article about this concept.
The automatic evaluation demonstrates that extracting relevant con                cepts from Wikipedia leads to better performance                 compared with WordNet: both expansion systems                 outperform the no-expansion version in terms of                 the ROUGE score.
Although this method proved                 helpful on the DUC data, it seems less appropriate for expanding company names.
For small compa                nies there are short articles with only a few links;                 the first paragraphs of the articles about larger                 companies often include interesting rather than relevant information.
For example, the text pre                ceding the contents box in the article about Apple                 Inc.
(AAPL) states that “Fortune magazine named                 Apple the most admired company in the United States”8.
The link to the article about the For                tune magazine can be hardly considered relevant                 for the expansion of AAPL.
Wikipedia category                 information, which has been successfully used in                 some NLP tasks (Ponzetto & Strube, 2006, inter                 alia), is too general and does not help discriminate                 between two companies from the same sector.
Our work suggests that query expansion is                 needed for summarization in the financial domain.
In addition to previous work, we also show that an                other key factor for success in this task is detecting                 and modeling the novelty of the target content.
7E.g., see the proceedings of TREC 9, TREC 10: http:                 //trec.nist.gov.
<newSection>                 8 Conclusions                              In this paper we presented a multi-document                 company-oriented summarization algorithm                 which extracts sentences that are both relevant for                 the given organization and novel to the user.
The                 system is expected to be useful in the context of                 stock market monitoring and forecasting, that is,                 to help the trader predict the move of the stock                 price for the given company.
We presented a novel query expansion method which works par                ticularly well in the context of company-oriented                 summarization.
Our sentence ranking method is                 unsupervized and requires little parameter tuning.
An automatic evaluation against a competitive                 baseline showed supportive results, indicating that                 the ranking algorithm is able to select relevant                 sentences and promote novel information at the                 same time.
In the future, we plan to experiment with po                sitional features which have proven useful for                 generic summarization.
We also plan to test the                 system extrinsically.
For example, it would be of                 interest to see if a classifier may predict the move                 of stock prices based on a set of features extracted                 from company-oriented summaries.
Acknowledgments: We would like to thank the                 anonymous reviewers for their helpful feedback.
<newSection>                 References