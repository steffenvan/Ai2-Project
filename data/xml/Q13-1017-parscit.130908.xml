<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.997683">
Dual Coordinate Descent Algorithms for Efficient
Large Margin Structured Prediction
</title>
<author confidence="0.976398">
Ming-Wei Chang Wen-tau Yih
</author>
<affiliation confidence="0.954809">
Microsoft Research
</affiliation>
<address confidence="0.93083">
Redmond, WA 98052, USA
</address>
<email confidence="0.999243">
{minchang,scottyih}@microsoft.com
</email>
<sectionHeader confidence="0.998601" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998493714285714">
Due to the nature of complex NLP problems,
structured prediction algorithms have been
important modeling tools for a wide range of
tasks. While there exists evidence showing
that linear Structural Support Vector Machine
(SSVM) algorithm performs better than struc-
tured Perceptron, the SSVM algorithm is still
less frequently chosen in the NLP community
because of its relatively slow training speed.
In this paper, we propose a fast and easy-to-
implement dual coordinate descent algorithm
for SSVMs. Unlike algorithms such as Per-
ceptron and stochastic gradient descent, our
method keeps track of dual variables and up-
dates the weight vector more aggressively. As
a result, this training process is as efficient as
existing online learning methods, and yet de-
rives consistently better models, as evaluated
on four benchmark NLP datasets for part-of-
speech tagging, named-entity recognition and
dependency parsing.
</bodyText>
<sectionHeader confidence="0.999475" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999929651162791">
Complex natural language processing tasks are in-
herently structured. From sequence labeling prob-
lems like part-of-speech tagging and named entity
recognition to tree construction tasks like syntactic
parsing, strong dependencies exist among the la-
bels of individual components. By modeling such
relations in the output space, structured output pre-
diction algorithms have been shown to outperform
significantly simple binary or multi-class classi-
fiers (Lafferty et al., 2001; Collins, 2002; McDonald
et al., 2005).
Among the existing structured output prediction
algorithms, the linear Structural Support Vector
Machine (SSVM) algorithm (Tsochantaridis et al.,
2004; Joachims et al., 2009) has shown outstanding
performance in several NLP tasks, such as bilingual
word alignment (Moore et al., 2007), constituency
and dependency parsing (Taskar et al., 2004b; Koo
et al., 2007), sentence compression (Cohn and La-
pata, 2009) and document summarization (Li et al.,
2009). Nevertheless, as a learning method for NLP,
the SSVM algorithm has been less than popular al-
gorithms such as the structured Perceptron (Collins,
2002). This may be due to the fact that cur-
rent SSVM implementations often suffer from sev-
eral practical issues. First, state-of-the-art imple-
mentations of SSVM such as cutting plane meth-
ods (Joachims et al., 2009) are typically compli-
cated.1 Second, while methods like stochastic gradi-
ent descent are simple to implement, tuning learning
rates can be difficult. Finally, while SSVM mod-
els can achieve superior accuracy, this often requires
long training time.
In this paper, we propose a novel optimiza-
tion method for efficiently training linear SSVMs.
Our method not only is easy to implement, but
also has excellent training speed, competitive with
both structured Perceptron (Collins, 2002) and
MIRA (Crammer et al., 2005). When evaluated on
several NLP tasks, including POS tagging, NER and
dependency parsing, this optimization method also
outperforms other approaches in terms of prediction
accuracy. Our final algorithm is a dual coordinate
</bodyText>
<footnote confidence="0.998597">
1Our algorithm is easy to implement mainly because we use
the square hinge loss function.
</footnote>
<page confidence="0.96073">
207
</page>
<bodyText confidence="0.967881820512821">
Transactions of the Association for Computational Linguistics, 1 (2013) 207–218. Action Editor: Ben Taskar.
Submitted 10/2012; Revised 3/2013; Published 5/2013. c�2013 Association for Computational Linguistics.
descent (DCD) algorithm for solving a structured
output SVM problem with a 2-norm hinge loss func-
tion. The algorithm consists of two main compo-
nents. One component behaves analogously to on-
line learning methods and updates the weight vector
immediately after inference is performed. The other
component is similar to the cutting plane method
and updates the dual variables (and the weight vec-
tor) without running inference. Conceptually, this
hybrid approach operates at a balanced trade-off
point between inference and weight update, per-
forming better than with either component alone.
Our contributions in this work can be summarized
as follows. Firstly, our proposed algorithm shows
that even for structured output prediction, an SSVM
model can be trained as efficiently as a structured
Perceptron one. Secondly, we conducted a careful
experimental study on three NLP tasks using four
different benchmark datasets. When compared with
previous methods for training SSVMs (Joachims
et al., 2009), our method achieves similar perfor-
mance using less training time. When compared to
commonly used learning algorithms such as Percep-
tron and MIRA, the model trained by our algorithm
performs consistently better when given the same
amount of training time. We believe our method can
be a powerful tool for many different NLP tasks.
The rest of our paper is organized as follows.
We first describe our approach by formally defining
the problem and notation in Sec. 2, where we also
review some existing, closely-related structured-
output learning algorithms and optimization tech-
niques. We introduce the detailed algorithmic de-
sign in Sec. 3. The experimental comparisons of
variations of our approach and the existing methods
on several NLP benchmark tasks and datasets are re-
ported in Sec. 4. Finally, Sec. 5 concludes the paper.
</bodyText>
<sectionHeader confidence="0.972525" genericHeader="introduction">
2 Background and Related Work
</sectionHeader>
<bodyText confidence="0.999974230769231">
We first introduce notations used throughout this pa-
per. An input example is denoted by x and an out-
put structure is denoted by y. The feature vector
Φ(x, y) is a function defined over an input-output
pair (x, y). We focus on linear models with predic-
tions made by solving the decoding problem:
The set Y(xi) represents all possible (exponentially
many) structures that can be generated from the ex-
ample xi. Let yi be the true structured label of xi.
The difference between the feature vectors of the
correct label yi and y is denoted as ΦYi,Y(xi) �
Φ(xi, yi) − Φ(xi, y). We define Δ(yi, y) as a dis-
tance function between two structures.
</bodyText>
<subsectionHeader confidence="0.987578">
2.1 Perceptron and MIRA
</subsectionHeader>
<bodyText confidence="0.995481722222222">
Structured Perceptron First introduced by
Collins (2002), the structured Perceptron algorithm
runs two steps iteratively: first, it finds the best
structured prediction y for an example with the
current weight vector using Eq. (1); then the
weight vector is updated according to the difference
between the feature vectors of the true label and
the prediction: w +- w + ΦYi,Y(xi). Inspired by
Freund and Schapire (1999), Collins (2002) also
proposed the averaged structured Perceptron, which
maintains an averaged weight vector throughout the
training procedure. This technique has been shown
to improve the generalization ability of the model.
MIRA The Margin Infused Relaxed Algo-
rithm (MIRA), which was introduced by Crammer
et al. (2005), explicitly uses the notion of margin to
update the weight vector. The MIRA updates the
weight vector by calculating the step size using
</bodyText>
<equation confidence="0.850785333333333">
211w − w0112
1
S.T. wT ΦYi,Y(xi) &gt; Δ(y, yi), by E Wk,
</equation>
<bodyText confidence="0.999861714285714">
where Wk is a set containing the best-k structures
according to the weight vector w0. MIRA is a
very popular method in the NLP community and has
been applied to NLP tasks like word segmentation
and part-of-speech tagging (Kruengkrai et al., 2009),
NER and chunking (Mejer and Crammer, 2010) and
dependency parsing (McDonald et al., 2005).
</bodyText>
<subsectionHeader confidence="0.998122">
2.2 Structural SVM
</subsectionHeader>
<bodyText confidence="0.9992615">
Structural SVM (SSVM) is a maximum margin
model for the structured output prediction setting.
Training SSVM is equivalent to solving the follow-
ing global optimization problem:
</bodyText>
<equation confidence="0.6968945">
min
W
arg max wTΦ(xi, y). (1) min 11w112 �l L(xi, yi, w), (2)
Y∈Y(Xi) W 2 + C i=1
</equation>
<page confidence="0.986265">
208
</page>
<bodyText confidence="0.982029">
where l is the number of labeled examples and
</bodyText>
<equation confidence="0.991944">
L(xi, yi, w) = e (max [A(yi, y) − w TΦ
Yxi,Y(xi)1
</equation>
<bodyText confidence="0.999662166666667">
The typical choice of e is e(a) = at. If t = 2 is used,
we refer to the SSVM defined in Eq. (2) as the L2-
Loss SSVM. If hinge loss (t = 1) is used in Eq. (2),
we refer to it as the L1-Loss SSVM. Note that the
function A is not only necessary,2 but also enables
us to use more information on the differences be-
tween the structures in the training phase. For ex-
ample, using Hamming distance for sequence label-
ing is a reasonable choice, as the model can express
finer distinctions between structures yi and y.
When training an SSVM model, we often need to
solve the loss-augmented inference problem,
</bodyText>
<equation confidence="0.9874975">
arg max 1wT Φ(xi, y) + A(yi, y)] . (3)
Y∈Y(Xi)
</equation>
<bodyText confidence="0.989081653846154">
Note that it is a different inference problem than the
decoding problem in Eq. (1).
Algorithms for training SSVM Cutting
plane (CP) methods (Tsochantaridis et al., 2004;
Joachims et al., 2009) have been the dominant
method for learning the L1-Loss SSVM. Eq. (2)
contains an exponential number of constraints.
The cutting plane (CP) methods iteratively select
a subset of active constraints for each example
then solve a sub-problem which contains active
constraints to improve the model. CP has proven
useful for solving SSVMs. For instance, Yu and
Joachims (2009) proposed using CP methods to
solve a 1-slack variable formulation, and showed
that solving for a 1-slack variable formulation
is much faster than solving the l-slack variable
one (Eq. (2)). Chang et al. (2010) also proposed
a variant of cutting plane method for solving the
L2-Loss SSVM. This method uses a dual coordinate
descent algorithm to solve the sub-problems. We
call their approach the CPD method.
Several other algorithms also aim at solv-
ing the L1-Loss SSVM. Stochastic gradient de-
scent (SGD) (Bottou, 2004; Shalev-Shwartz et al.,
2007) is a technique for optimizing general con-
vex functions and has been applied to solving the
</bodyText>
<footnote confidence="0.842795">
2Without Δ(y, yi) in Eq. 2, the optimal w would be zero.
</footnote>
<bodyText confidence="0.995455127659575">
L1-Loss SSVM (Ratliff et al., 2007). Taskar et
al. (2004a) proposed a structured SMO algorithm.
Because the algorithm solves the dual formulation
of the L1-Loss SSVM, it requires picking a vio-
lation pair for each update. In contrast, because
each dual variable can be updated independently in
our DCD algorithm, the implementation is relatively
simple. The extragradient algorithm has also been
applied to solving the L1-Loss SSVM (Taskar et al.,
2005). Unlike our DCD algorithm, the extragradient
method requires the learning rate to be specified.
The connections between dual methods and the
online algorithms have been previously discussed.
Specifically, Shalev-Shwartz and Singer (2006) con-
nects the dual methods to a wide range of online
learning algorithms. In (Martins et al., 2010), the au-
thors apply similar techniques on L1-Loss SSVMs
and show that the proposed algorithm can be faster
than the SGD algorithm.
Exponentiated Gradient (EG) descent (Kivinen
and Warmuth, 1995; Collins et al., 2008) has re-
cently been applied to solving the L1-Loss SSVM.
Compared to other SSVM learners, EG requires
manual tuning of the step size. In addition, EG re-
quires solution of the sum-product inference prob-
lem, which can be more expensive than solving
Eq. (3) (Taskar et al., 2006). Very recently, Lacoste-
Julien et al. (2013) proposed a block-coordinate de-
scent algorithm for the L1-Loss SSVM based on the
Frank-Wolfe algorithm (FW-Struct), which has been
shown to outperform the EG algorithm significantly.
Similar to our DCD algorithm, FW calculates the
optimal learning rate when updating the dual vari-
ables.
The Sequential Dual Method (SDM) (Shevade et
al., 2011) is probably the most related to this paper.
SDM solves the L1-Loss SSVM problem using mul-
tiple updating policies, which is similar to our ap-
proach. However, there are several important differ-
ences in the detailed algorithmic design. As will be
clear in Sec. 3, our dual coordinate descent (DCD)
algorithm is very simple, while SDM (which is not
a DCD algorithm) uses a complicated procedure to
balance different update policies. By targeting the
L2-Loss SSVM formulation, our methods can up-
date the weight vector more efficiently, since there
are no equality constraints in the dual.
</bodyText>
<page confidence="0.998367">
209
</page>
<sectionHeader confidence="0.6081315" genericHeader="method">
3 Dual Coordinate Descent Algorithms for
Structural SVM
</sectionHeader>
<bodyText confidence="0.985337571428571">
In this work, we focus on solving the dual of linear
L2-Loss SSVM, which can be written as follows:
Working Set The number of non-zero variables in
the optimal α can be small when solving Eq. (4).
Hence, it is often feasible to use a small working set
)/Vi for each example to keep track of the structures
for non-zero α’s. More formally,
</bodyText>
<equation confidence="0.9782765">
min 1 X αi,yΦyi,y(xi)112 (4)
αi,y&gt;0 211
i,y
+ 1 X X Xαi,y)2 _ A(y, yi)αi,y.
4C ( i,y
i yEY(xi)
</equation>
<bodyText confidence="0.998574571428571">
In the above equation, a dual variable αi,y is asso-
ciated with a structure y E Y(xi). Therefore, the
total number of dual variables can be quite large: its
upper bound is lB, where B = maxi |Y(xi)|.
The connection between the dual variables and
the weight vector w at optimal solutions is through
the following equation:
</bodyText>
<equation confidence="0.625253">
αi,yΦyi,y(xi). (5)
</equation>
<bodyText confidence="0.988086909090909">
Advantages of L2-Loss SSVM The use of the
2-norm hinge loss function eliminates the need of
equality constraints3; only non-negative constraints
(αi,y &gt; 0) remain. This is important because now
each dual variable can be updated without changing
values of the other dual variables. We can then up-
date one single dual variable at a time. As a result,
this dual formulation allows us to design a simple
and principled dual coordinate descent (DCD) opti-
mization method.
DCD algorithms consist of two iterative steps:
</bodyText>
<listItem confidence="0.916493">
1. Pick a dual variable αi,y.
2. Update the dual variable and the weight vector.
Go to 1.
</listItem>
<bodyText confidence="0.999378">
In the normal binary classification case, how to
select dual variables to solve is not an issue as
choosing them randomly works effectively in prac-
tice (Hsieh et al., 2008). However, this is not a prac-
tical scheme for training SSVM models given that
the number of dual variables in Eq. (4) can be very
large because of the exponentially many legitimate
output structures. To address this issue, we intro-
duce the concept of working set below.
</bodyText>
<footnote confidence="0.535751">
3For L1-Loss SSVM, there are the equality constraints:
</footnote>
<equation confidence="0.9748865">
Ey∈Y(x,) αi,y = C, ∀i.
)/Vi = {y  |Vy E Y(xi), αi,y &gt; 01.
</equation>
<bodyText confidence="0.99966925">
Intuitively, the working set )/Vi records the output
structures that are similar to the true structure yi.We
set all dual variables to be zero initially (therefore,
w = 0 as well), so )/Vi = 0 for all i. Then the algo-
rithm starts to build the working set in the training
procedure. After training, the weight vector is com-
puted using dual variables in the working set and
thus equivalent to
</bodyText>
<equation confidence="0.847408">
αi,yΦyi,y(xi). (6)
</equation>
<bodyText confidence="0.9987875">
Connections to Structured Perceptron The pro-
cess of updating a dual variable is in fact very simi-
lar to the update rule used in Perceptron and MIRA.
Take structured Perceptron for example, its weight
vector can be determined using the following equa-
tion:
</bodyText>
<equation confidence="0.882844">
Oi,yΦyi,y(xi), (7)
</equation>
<bodyText confidence="0.973683615384615">
where h(xi) is the set containing all structures Per-
ceptron predicts for xi during training, and Oi,y is
the number of times Perceptron predicts y for xi
during training. By comparing Eq. (6) and Eq. (7), it
is clear that SSVM is just a more principled way to
update the weight vector, as α’s are computed based
on the notion of margin.4
Updating Dual Variables and Weights After
picking a dual variable αi,¯y, we first show how to
update it optimally. Recall that a dual variable αi,¯y
is associated with the i-th example and a structure ¯y.
The optimal update size d for αi,¯y can be calculated
analytically from the following optimization prob-
</bodyText>
<footnote confidence="0.7105905">
4Of course, Wi could be very different from r(xi), the con-
struction of the working sets will be discussed in Sec. 3.1.
</footnote>
<equation confidence="0.999724583333333">
X
yEY(xi)
XL
i=1
w =
XL
i=1
w =
X
yEWi
wperc = XL X
i=1 yEr(xi)
</equation>
<page confidence="0.976717">
210
</page>
<bodyText confidence="0.87816975">
Algorithm 1 UPDATEWEIGHT(i, w):
Update the weight vector w and the dual variables
in the working set of the i-th example. C is the reg-
ularization parameter defined in Eq. (2).
</bodyText>
<listItem confidence="0.864706363636364">
1: Shuffle the elements in Wi (but retain the newest
member of the working set to be updated first.
See Theorem 1 for the reasons.)
2: for y¯ ∈ Wi do
Δ(Y,Yi)−w ΦYi,�Y(xi)−AY∈ C� αi,Y
3: d ← kΦYi,�Y(xi)k2+1
2C
4: α0 ← max(αi,¯y + d, 0)
5: w ← w + (α0 − αi,¯y)Φyi,¯y(xi)
6: αi,¯y ← α0
7: end for
</listItem>
<bodyText confidence="0.999982466666667">
where the w is defined in Eq. (6). Compared to
stochastic gradient descent, DCD algorithms keep
track of dual variables and do not need to tune the
learning rate.
Instead of updating one dual variable at a time,
our algorithm updates all dual variables once in the
working set. This step is important for the conver-
gence of the DCD algorithms.5 The exact update
algorithm is presented in Algorithm 1. Line 3 cal-
culates the optimal step size (the analytical solution
to the above optimization problem). Line 4 makes
sure that dual variables are non-negative. Lines 5
and 6 update the weight vectors and the dual vari-
ables. Note that every update ensures Eq. (4) to be
no greater than the original value.
</bodyText>
<subsectionHeader confidence="0.997895">
3.1 Two DCD Optimization Algorithms
</subsectionHeader>
<bodyText confidence="0.999584">
Now we are ready to present two novel DCD algo-
rithms for L2-Loss SSVM: DCD-Light and DCD-
SSVM.
</bodyText>
<subsectionHeader confidence="0.897501">
3.1.1 DCD-Light
</subsectionHeader>
<bodyText confidence="0.985413">
The basic idea of DCD-Light is just like online
learning algorithms. Instead of doing inference for
5Specifically, updating all of the structures in the working
set is a necessary condition for our algorithms to converge.
the whole batch of examples before updating the
weight vector in each iteration, as done in CPD and
1-slack variable formulation of SVM-Struct, DCD-
Light updates the model weights after solving the in-
ference problem for each individual example. Algo-
rithm 2 depicts the detailed steps. In Line 5, the loss-
augmented inference (Eq. (3)) is performed; then
the weight vector is updated in Line 9 – all of the
structures and dual variables in the working set are
used to update the weight vector. Note that there is
a δ parameter in Line 6 to control how precise we
would like to solve this SSVM problem. As sug-
gested in (Hsieh et al., 2008), we shuffle the exam-
ples in each iteration (Line 3) as it helps the algo-
rithm converge faster.
DCD-Light has several noticeable differences
when compared to the most popular online learn-
ing method, averaged Perceptron. First, DCD-Light
performs the loss-augmented inference (Eq. (3)) at
Line 5 instead of the argmax inference (Eq. (1)).
Second, the algorithm updates the weight vector
with all structures in the working set. Finally, DCD-
light does not average the weight vectors.
</bodyText>
<subsectionHeader confidence="0.659181">
3.1.2 DCD-SSVM
</subsectionHeader>
<bodyText confidence="0.999924347826087">
Observing that DCD-Light does not fully utilize
the saved dual variables in the working set, we pro-
pose a hybrid approach called DCD-SSVM, which
combines ideas from DCD-Light and cutting plane
methods. In short, after running the updates on a
batch of examples, we refine the model by solving
the dual variables further in the current working sets.
The key advantage of keeping track of these dual
variables is that it allows us to update the saved dual
variables without performing any inference, which
is often an expensive step in structured prediction
algorithms.
DCD-SSVM is summarized in Algorithm 3.
Lines 10 to 16 are from DCD-Light. In Lines 3 to
8, we grab the idea from cutting plane methods by
updating the weight vector using the saved dual vari-
ables in the working sets without any inference (note
that Lines 3 to 8 do not have any effect at the first
iteration). By revisiting the dual variables, we can
derive a better intermediate model, resulting in run-
ning the inference procedure less frequently. Similar
to DCD-Light, we also shuffle the examples in each
iteration.
</bodyText>
<equation confidence="0.9405885">
lem (derived from Eq. (4)):
min
d≥−cai,Y
y∈Wi
2kw + dΦyi,¯y(x)k2+
1
14C (d +
αi,y)2 − dΔ(yi, ¯y), (8)
</equation>
<page confidence="0.990054">
211
</page>
<bodyText confidence="0.96017">
Algorithm 2 DCD-Light: The lightweight dual co-
ordinate descent algorithm for optimizing Eq. (4).
</bodyText>
<listItem confidence="0.954609166666667">
1: w ← 0, Wi ← ∅, ∀i
2: for t = 1 ... T do
3: Shuffle the order of the training examples
4: for i = 1... l do
5: Y¯ ← arg maxy wT Φ(Xi, Y) + Δ(Y, Yi)
6: if Δ(¯Y, Yi)−wT Φyi,¯y(Xi)
then
7: Wi ← Wi ∪ {¯Y}
8: end if
9: UPDATEWEIGHT(i, w) {Algo. 1}
10: end for
11: end for
</listItem>
<bodyText confidence="0.997385666666667">
DCD algorithms are similar to column generation
algorithms for linear programming (Desrosiers and
L¨ubbecke, 2005), where the master problem is to
solve the dual problem that focuses on the variables
in the working sets, and the subproblem is to find
new variables for the working sets. In Sec. 4, we
will demonstrate the importance of balancing these
two problems by comparing DCD-SSVM and DCD-
Light.
</bodyText>
<subsectionHeader confidence="0.999843">
3.2 Convergence Analysis
</subsectionHeader>
<bodyText confidence="0.989046454545455">
We now present the theoretic analysis of both DCD-
Light and DCD-SSVM, and address two main top-
ics: (1) whether the working sets will grow expo-
nentially and (2) the convergence rate. Due to the
lack of space, we show only the main theorems.
Leveraging Theorem 5 in (Joachims et al., 2009),
we can prove that the DCD algorithms only add a
limited number of variables in the working sets, and
have the following theorem.
Theorem 1. The number of times that DCD-Light
or DCD-SSVM /adds structures into working sets is
bounded by O12(R2+ 22 )lCΔ21 where R2 is de-
fined as maxi,¯y\kΦyi,¯y(Xi)k2, and Δ is the upper
bound of Δ(yi, y&apos;), ∀yi, y&apos; ∈ Y(xi).
We discuss next the convergence rates of our
DCD algorithms under two different conditions –
when the working sets are fixed and the general case.
If the working sets are fixed in DCD algorithms,
they become cyclic dual coordinate descent meth-
Algorithm 3 DCD-SSVM: a hybrid dual coor-
dinate descent algorithm that combines ideas from
DCD-Light and cutting plane algorithms.
</bodyText>
<listItem confidence="0.965051333333333">
1: w ← 0, Wi ← ∅,∀i
2: fort = 1 ... T do
3: for j = 1 ... r do
4: Shuffle the order of the training examples
5: for i = 1... l do
6: UPDATEWEIGHT(i, w) {Algo. 1}
7: end for
8: end for
9: Shuffle the order of the training examples
10: for i = 1... l do
11: Y¯ ← arg maxy wT Φ(Xi, Y) + Δ(Y, Yi)
12: if Δ(¯Y,Yi)−wTΦyi,¯y(Xi)
then
13: Wi ← Wi ∪ {¯Y}
14: end if
15: UPDATEWEIGHT(i, w) {Algo. 1}
16: end for
17: end for
</listItem>
<bodyText confidence="0.983094454545455">
ods. In this case, we denote the minimization prob-
lem Eq. (4) as F(α). For fixed working sets {Wi},
we denote FS(α) as the minimization problem that
focuses on the dual variables in the working set only.
By applying the results from (Luo and Tseng, 1993;
Wang and Lin, 2013) to L2-Loss SSVM, we have
the following theorem.
Theorem 2. For any given non-empty working sets
{Wi}, if the DCD algorithms do not extend the
working sets (i.e., line 6-8 in Algorithm 2 are not ex-
ecuted), then the DCD algorithms will obtain the E-
optimal solution for FS(α) in O(log(1� )) iterations.
Based on Theorem 1 and Theorem 2, we have the
following theorem.
Theorem 3. DCD-SSVM obtains an E-optimal solu-
tion in O(E log(1�)) iterations.
To the best of our knowledge, this is the first con-
vergence analysis result for L2-Loss SSVM. Com-
pared to other theoretic analysis results for L1-Loss
SSVM, a tighter bound might exist given a better
theoretic analysis. We leave this for future work.6
6Noticeably, the use of working sets complicates the theo-
</bodyText>
<figure confidence="0.92846414">
αi,Y
E
YEWi
2C ≥ δ
αi,Y
E
YEWi
2C ≥ δ
212
Training Time (seconds)
(a) Test F1 vs. Time in NER-CoNLL
Training Time (seconds)
(b) Test Acc vs. Time in POS
0 2,000 4,000 6,000
Training Time (seconds)
(c) Test Acc vs. Time in DP-WSJ
0 20 40 60 80 100
0 100 200 300 400
Test Acc 97.2
97
96.8
96.6
Test F1 86
84
82
80
78
Test Acc 90
88
86
Primal Objective Value
6,000
4,000
2,000
1.5
1
Primal Objective Value
2 ·104
Primal Objective Value
4
6 ·104
2
DCD-SSVM
DCD-Light
CPD
0 2,000 4,000 6,000
0 20 40 60 80 100
0 100 200 300 400 500
Training Time (seconds) Training Time (seconds) Training Time (seconds)
(d) Primal Objective Value in NER-CoNLL (e) Primal Objective Value in POS (f) Primal Objective Value in DP-WSJ
</figure>
<figureCaption confidence="0.758733666666667">
Figure 1: We plot the testing performance (top row) and the primal objective function (bottom row) versus training
time for three optimization methods for learning the L2-Loss SSVM. In general, DCD-SSVM is the best algorithm for
both the objective function and the testing performance.
</figureCaption>
<sectionHeader confidence="0.99927" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9999772">
In order to verify the effectiveness of the proposed
algorithm, we conduct a set of experiments on dif-
ferent optimization and learning algorithms. Before
going to the experimental results, we first introduce
the tasks and settings used in the experiments.
</bodyText>
<subsectionHeader confidence="0.99923">
4.1 Tasks and Data
</subsectionHeader>
<bodyText confidence="0.992407647058823">
We evaluated our method and existing structured
output learning approaches on named entity recog-
nition (NER), part-of-speech tagging (POS) and de-
pendency parsing (DP) on four benchmark datasets.
NER-MUC7 MUC-7 data contains a subset of
North American News Text Corpora annotated with
many types of entities. We followed the settings
in (Ratinov and Roth, 2009) and consider three main
entities categories: PER, LOC and ORG. We evalu-
ated the results using phrase-level F1.
retic analysis significantly. Also note that Theorem 2 shows
that if we put all possible structures in the working sets (i.e.,
F(α) = FS(α)), then the DCD algorithms can obtain e-optimal
solution in O(log(E )) iterations.
NER-CoNLL This is the English dataset from the
CoNLL 2003 shared task (T. K. Sang and De Meul-
der, 2003). The data set labels sentences from the
Reuters Corpus, Volume 1 (Lewis et al., 2004) with
four different entity types: PER, LOC, ORG and
MISC. We evaluated the results using phrase-level
F1.
POS-WSJ The standard set for evaluating the per-
formance of a part-of-speech tagger. The training,
development and test sets consist of sections 0-18,
19-21 and 22-24 of the Penn Treebank data (Marcus
et al., 1993), respectively. We evaluated the results
by token-level accuracy.
DP-WSJ We took sections 02-21 of Penn Tree-
bank as the training set, section 00 as the develop-
ment set and section 23 as the test set. We imple-
ment a simple version of hash kernel to speed up of
training procedure for this task (Bohnet, 2010). We
reported the unlabeled attachment accuracy for this
task (McDonald et al., 2005).
</bodyText>
<page confidence="0.99828">
213
</page>
<figure confidence="0.994886777777778">
80
79
Test F1
78
77
76
0 10 20 30
Test F1
86
84
82
80
0 20 40 60 80 100 120
Test Acc
97
96
95
DCD-SSVM
FW-Struct
SVM-Struct
0 200 400 600
Training Time (seconds)
(a) Test F1 vs. Time in NER-MUC7
Training Time (seconds)
(b) Test F1 vs. Time in NER-CoNLL
Training Time (seconds)
(c) Test Acc vs. Time in POS-WSJ
</figure>
<figureCaption confidence="0.929434">
Figure 2: Comparisons between the testing performance of DCD-SSVM, FW-Struct and SVM-Struct. Note that
DCD-SSVM often obtain a better model with much less training time when comparing to SVM-Struct.
</figureCaption>
<subsectionHeader confidence="0.919036">
4.2 Features and Inference Algorithms
</subsectionHeader>
<bodyText confidence="0.999741666666667">
For the sequence labeling tasks, NER and POS,
we followed the discriminative HMM settings used
in (Joachims et al., 2009) and defined the features as
</bodyText>
<equation confidence="0.99867675">
demi(xi, yi)
[yi = 1][yi−1 = 1]
[yi = 1][yi−1 = 2]
[yi = k][yi−1 = k]
</equation>
<bodyText confidence="0.999956733333333">
where demi is the feature vector dedicated to the i-th
token (or, the emission features), N represents the
number of tokens in this sequence, yi represents the
i-th token in the sequence y, [yi = 1] is the indictor
variable and k is the number of tags.
The inference problems are solved by the Viterbi
algorithm. The emission features used in both POS
and NER are the standard ones, including word fea-
tures, word-shape features, etc. For NER, we used
additional simple gazetteer features7 and word clus-
ter features (Turian et al., 2010)
For dependency parsing, we followed the setting
described in (McDonald et al., 2005) and used sim-
ilar features. The decoding algorithm is the first-
order Eisner’s algorithm (Eisner, 1997).
</bodyText>
<subsectionHeader confidence="0.999371">
4.3 Algorithms and Implementation Detail
</subsectionHeader>
<bodyText confidence="0.99974025">
For all SSVM algorithms (including SGD), C was
chosen among the set 10.01, 0.05,0.1,0.5, 1, 5} ac-
cording to the accuracy/F1 on the development set.
For each task, the same features were used by all
</bodyText>
<footnote confidence="0.642084">
7Adding Wikipedia gazetteers would likely increase the per-
formance significantly (Ratinov and Roth, 2009).
</footnote>
<bodyText confidence="0.984589931034483">
algorithms. For NER-MUC7, NER-CoNLL and
POS-WSJ, we ran the online algorithms and DCD-
SSVM for 25 iterations. For DP-WSJ, we only let
the algorithms run for 10 iterations as the inference
procedure is very expensive computationally. The
algorithms in the experiments are:
DCD Our dual coordinate descent method on the
L2-Loss SSVM. For DCD-SSVM, r is set to be 5.
For both DCD-Light and DCD-SSVM , we follow
the suggestion in (Joachims et al., 2009): if the value
of a dual variable becomes zero, its corresponding
structure will be removed from the working set to
improve the speed.
SVM-Struct We used the latest (v3.10) of SVM-
HMM.8 This version uses the cutting plane method
on a 1-slack variable formulation (Joachims et al.,
2009) for the L1-Loss SSVM. SVM-Struct was im-
plemented in C and all the other algorithms are im-
plemented in C#. We did not apply SVM-Struct to
DP-WSJ because there is no native implementation.
Perceptron This refers to the averaged structured
Perceptron method introduced by Collins (2002). To
speed up the convergence rate, we shuffle the train-
ing examples at each iteration.
MIRA Margin Infused Relaxed Algorithm
(MIRA) (Crammer et al., 2005) is the online
learning algorithm that explicitly uses the notion
of margin to update the weight vector. We use
1-best MIRA in our experiments. To increase
</bodyText>
<footnote confidence="0.853228">
8http://www.cs.cornell.edu/People/tj/
svm_light/svm_hmm.html
</footnote>
<equation confidence="0.8873625">
⎡
⎢ ⎢ ⎢ ⎢ ⎣
d(X, y) =
N
i=l
,
⎤
⎦⎥⎥⎥⎥
</equation>
<page confidence="0.996872">
214
</page>
<bodyText confidence="0.998995666666667">
the convergence speed, we shuffle the training
examples at each iteration. Following (McDonald et
al., 2005), we did not tune the C parameter for the
MIRA algorithm.
SGD Stochastic gradient descent (SGD) (Bottou,
2004) is a technique for optimizing general convex
functions. In this paper, we use SGD as an alterna-
tive baseline for optimizing the L1-Loss SSVM ob-
jective function (Eq. (2) with higne loss).9 When us-
ing SGD, the learning rate must be carefully tuned.
Following (Bottou, 2004), the learning rate is ob-
tained by
</bodyText>
<equation confidence="0.9854605">
η0
(1.0 + (η0T/C))0.75,
</equation>
<bodyText confidence="0.999658235294117">
where C is the regularization parameter, T is the
number of updates so far and η0 is the initial step
size. The parameter η0 was selected among the set
{2−1,2−2,2−3,2−4,2−5} by running the SGD al-
gorithm on a set of 1000 randomly sampled exam-
ples, and then choosing the η0 with lowest primal
objective function on these examples.
FW-Struct FW-Struct represents the Frank-Wolfe
algorithm for the L1-Loss SSVM (Lacoste-Julien et
al., 2013).
In order to improve the training speed, we cached
all the feature vectors generated by the gold la-
beled data once computed. This applied to all al-
gorithms except SVM-Struct, which has its own
caching mechanism. We report the performance
of the averaged weight vectors for Perceptron and
MIRA.
</bodyText>
<subsectionHeader confidence="0.935688">
4.4 Results
</subsectionHeader>
<bodyText confidence="0.999906">
We present the experimental results below on com-
paring different dual coordinate descent methods,
as well as comparing our main algorithm, DCD-
SSVM, with other structured learning approaches.
</bodyText>
<subsectionHeader confidence="0.972467">
4.4.1 Comparisons of DCD Methods
</subsectionHeader>
<bodyText confidence="0.999935333333333">
We compared three DCD methods: DCD-Light,
DCD-SSVM and CPD. CPD is a cutting plane
method proposed by Chang et al. (2010), which uses
</bodyText>
<footnote confidence="0.818422333333333">
9To compare with SGD using its best setting, we report only
the results of SGD on the L1-Loss SSVM as we found tuning
the step size for the L2-Loss SSVM is more difficult.
</footnote>
<bodyText confidence="0.998141769230769">
a dual coordinate descent algorithm to solve the in-
ternal sub-problems. We specifically included CPD
as it also targets at the L2-Loss SSVM.
Because different optimization strategies will
reach the same objective values eventually, compar-
ing them on prediction accuracy of the final models
is not meaningful. Instead, here we compare how
fast each algorithm converges as shown in Figure 1.
Each marker on the line in this figure represents one
iteration of the corresponding algorithm. Generally
speaking, CPD improves the model very slowly in
the early stages, but much faster after several iter-
ations. In comparison, DCD-Light often behaves
much better initially, and DCD-SSVM is generally
the most efficient algorithm here.
The reason behind the slow performance of CPD
is clear. During early rounds of the algorithm,
the weight vector is far from optimal, so it spends
too much time using “bad” weight vectors to find
the most violated structures. On the other hand,
DCD-Light updates the weight vector more fre-
quently, so it behaves much better in general. DCD-
SSVM spends more time on updating models during
each batch, but keeps the same amount of time doing
inference as DCD-Light. As a result, it finds a better
trade-off between inference and learning.
</bodyText>
<subsectionHeader confidence="0.847888">
4.4.2 DCD-SSVM, SVM-Struct and FW-Struct
</subsectionHeader>
<bodyText confidence="0.99996565">
Joachims et al. (2009) proposed a 1-slack vari-
able method for the L1-Loss SSVM. They showed
that solving a 1-slack variable formulation is an
order-of-magnitude faster than solving the original
formulation (l-slack variables formulation). Nev-
ertheless, from Figure 2, we can see the clear ad-
vantage of DCD-SSVM over SVM-Struct. Al-
though using 1-slack variable has improved the
learning speed, SVM-Struct still converges slower
than DCD-SSVM. In addition, the performance of
models trained by SVM-Struct in the early stage is
quite unstable, which makes early stopping an in-
effective strategy in practice when training time is
limited.
We also compared our algorithms to FW-Struct.
Our results agree with (Lacoste-Julien et al., 2013),
which shows that the FW-Struct outperforms the
SVM-Struct. In our experiments, we found that our
DCD algorithms were competitive, sometimes con-
verged faster than the FW-Struct.
</bodyText>
<page confidence="0.996696">
215
</page>
<figure confidence="0.998191555555556">
Test F1
80
79
78
77
76
0 10 20 30
Training Time (seconds)
(a) Test F1 vs. Time in NER-MUC7
0 100 200 300 400
Training Time (seconds)
(b) Test Acc vs. Time in POS-WSJ
0 2,000 4,000 6,000
Training Time (seconds)
(c) Test Acc vs. Time in DP-WSJ
DCD-SSVM
PERP
MIRA
SGD
Test Acc 97.2
97
96.8
96.6
Test Acc 91
90
89
88
</figure>
<figureCaption confidence="0.99409">
Figure 3: Comparisons between DCD-SSVM and popular online learning algorithms. Note that the results diverge
when comparing Perceptron and MIRA. In general, DCD-SSVM is the most stable algorithm.
</figureCaption>
<table confidence="0.9993732">
Task/Data DCD Percep MIRA SGD
NER-MUC7 79.4 78.5 78.8 77.8
NER-CoNLL 85.6 85.3 85.1 84.2
POS-WSJ 97.1 96.9 96.9 96.9
DP-WSJ 90.8 90.3 90.2 90.9
</table>
<tableCaption confidence="0.980137666666667">
Table 1: Performance of online learning algorithms and
the DCD-SSVM algorithm on the testing sets. NER is
measured by F, while others by accuracy.
</tableCaption>
<sectionHeader confidence="0.865612" genericHeader="method">
4.4.3 DCD-SSVM, MIRA, Perceptron and
SGD
</sectionHeader>
<bodyText confidence="0.999873428571429">
As in binary classification, large-margin methods
like SVMs often perform better than algorithms like
Perceptron and SGD (Hsieh et al., 2008; Shalev-
Shwartz and Zhang, 2013), here we observe similar
behaviors in the structured output domain. Table 1
shows the final test accuracy numbers or F1 scores of
models trained by algorithms including Perceptron,
MIRA and SGD, compared to those of the SSVM
models trained by DCD-SSVM. Among the bench-
mark datasets and tasks we have experimented with,
DCD-SSVM derived the most accurate models, ex-
cept for DP-WSJ when compared to SGD.
Perhaps a more interesting comparison is on
the training speed, which can be observed in Fig-
ure 3. Compared to other online algorithms, DCD-
SSVM can take advantage of cached dual variables
and structures. We show that the training speed of
DCD-SSVM can be competitive to that of the on-
line learning algorithms, unlike SVM-Struct. Note
that SGD is not very stable for NER-MUC7, even
though we tuned the step size very carefully.
</bodyText>
<sectionHeader confidence="0.999643" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999984916666667">
In this paper, we present a novel approach for learn-
ing the L2-Loss SSVM model. By combining the
ideas of dual coordinate descent and cutting plane
methods, the hybrid approach, DCD-SSVM outper-
forms other SSVM training methods both in terms
of objective value reduction and testing error rate
reduction. As demonstrated in our experiments
on several NLP tasks, our approach also tends to
learn more accurate models than commonly used
structured learning algorithms, including structured
Perceptron, MIRA and SGD. Perhaps more inter-
estingly, our SSVM learning method is very effi-
cient: the model training time is competitive to on-
line learning algorithms such as structured Percep-
tron and MIRA. These unique qualities make DCD-
SSVM an excellent choice for solving a variety of
complex NLP problems.
In the future, we would like to compare our algo-
rithm to other structured prediction approaches, such
as conditional random fields (Lafferty et al., 2001)
and exponential gradient descent methods (Collins
et al., 2008). Expediting the learning process fur-
ther by leveraging approximate inference is also an
interesting direction to investigate.
</bodyText>
<sectionHeader confidence="0.998813" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9911228">
We sincerely thank John Platt, Lin Xiao and Kaiwei Chang for
the discussions and feedback. We are grateful to Po-Wei Wang
and Chih-Jen Lin for providing their work on convergence rate
analysis on feasible descent methods. We also thank the review-
ers for their detailed comments on this paper.
</bodyText>
<page confidence="0.998333">
216
</page>
<sectionHeader confidence="0.996217" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999835390476191">
B. Bohnet. 2010. Very high accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings of
the 23rd International Conference on Computational
Linguistics, Proceedings the International Conference
on Computational Linguistics (COLING).
L. Bottou. 2004. Stochastic learning. In Olivier Bous-
quet and Ulrike von Luxburg, editors, Advanced Lec-
tures on Machine Learning, Lecture Notes in Artifi-
cial Intelligence, LNAI 3176, pages 146–168. Springer
Verlag, Berlin.
M. Chang, V. Srikumar, D. Goldwasser, and D. Roth.
2010. Structured output learning with indirect super-
vision. In Proceedings of the International Conference
on Machine Learning (ICML).
T. Cohn and M. Lapata. 2009. Sentence compression
as tree transduction. Journal of AI Research, 34:637–
674, April.
M. Collins, A. Globerson, T. Koo, X. Carreras, and P. L.
Bartlett. 2008. Exponentiated gradient algorithms
for conditional random fields and max-margin Markov
networks. Journal of Machine Learning Research, 9.
M. Collins. 2002. Discriminative training methods for
hidden Markov models: Theory and experiments with
perceptron algorithms. In Proceedings of the Confer-
ence on Empirical Methods for Natural Language Pro-
cessing (EMNLP).
K. Crammer, R. Mcdonald, and F. Pereira. 2005. Scal-
able large-margin online learning for structured clas-
sification. Technical report, Department of Computer
and Information Science, University of Pennsylvania.
J. Desrosiers and M. E. L¨ubbecke. 2005. A primer in
column generation. In Column Generation, pages 1–
32. Springer.
J. M. Eisner. 1997. Three new probabilistic models for
dependency parsing: An exploration. In Proceedings
the International Conference on Computational Lin-
guistics (COLING), pages 340–345.
Y. Freund and R. Schapire. 1999. Large margin clas-
sification using the Perceptron algorithm. Machine
Learning, 37(3):277–296.
C.-J. Hsieh, K.-W. Chang, C.-J. Lin, S. S. Keerthi, and
S. Sundararajan. 2008. A dual coordinate descent
method for large-scale linear SVM. In Proceedings
of the International Conference on Machine Learning
(ICML), New York, NY, USA. ACM.
T. Joachims, T. Finley, and Chun-Nam Yu. 2009.
Cutting-plane training of structural SVMs. Machine
Learning, 77(1):27–59.
J. Kivinen and M. K. Warmuth. 1995. Exponentiated
gradient versus gradient descent for linear predictors.
In ACM Symp. of the Theory of Computing.
T. Koo, A. Globerson, X. Carreras, and M. Collins. 2007.
Structured prediction models via the matrix-tree theo-
rem. In Proceedings of the 2007 Joint Conference of
EMNLP-CoNLL, pages 141–150.
C. Kruengkrai, K. Uchimoto, J. Kazama, Y. Wang,
K. Torisawa, and H. Isahara. 2009. An error-driven
word-character hybrid model for joint chinese word
segmentation and pos tagging. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 513–521.
S. Lacoste-Julien, M. Jaggi, M. W. Schmidt, and
P. Pletscher. 2013. Stochastic block-coordinate Frank-
Wolfe optimization for structural SVMs. In Pro-
ceedings of the International Conference on Machine
Learning (ICML).
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of the International Conference on Machine Learning
(ICML).
D. D. Lewis, Y. Yang, T. Rose, and F. Li. 2004. RCV1:
A new benchmark collection for text categorization
research. Journal of Machine Learning Research,
5:361–397.
L. Li, K. Zhou, G.-R. Xue, H. Zha, and Y. Yu. 2009.
Enhancing diversity, coverage and balance for summa-
rization through structure learning. In Proceedings of
the 18th international conference on World wide web,
The International World Wide Web Conference, pages
71–80, New York, NY, USA. ACM.
Z.-Q. Luo and P. Tseng. 1993. Error bounds and conver-
gence analysis of feasible descent methods: A general
approach. Annals of Operations Research, 46(1):157–
178.
M. P. Marcus, B. Santorini, and M. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313–330, June.
A. F. Martins, K. Gimpel, N. A. Smith, E. P. Xing, M. A.
Figueiredo, and P. M. Aguiar. 2010. Learning struc-
tured classifiers with dual coordinate ascent. Technical
report, Technical report CMU-ML-10-109.
R. McDonald, K. Crammer, and F. Pereira. 2005. Online
large-margin training of dependency parsers. In Pro-
ceedings of the Annual Meeting of the Association for
Computational Linguistics (ACL), pages 91–98, Ann
Arbor, Michigan.
A. Mejer and K. Crammer. 2010. Confidence in
structured-prediction using confidence-weighted mod-
els. In Proceedings of the 2010 Conference on Em-
pirical Methods in Natural Language Processing, Pro-
ceedings of the Conference on Empirical Methods for
Natural Language Processing (EMNLP), pages 971–
981.
</reference>
<page confidence="0.975558">
217
</page>
<reference confidence="0.999897176470588">
R. C. Moore, W. Yih, and A. Bode. 2007. Improved dis-
criminative bilingual word alignment. In Proceedings
of the Annual Meeting of the Association for Compu-
tational Linguistics (ACL).
L. Ratinov and D. Roth. 2009. Design challenges and
misconceptions in named entity recognition. In Proc.
of the Annual Conference on Computational Natural
Language Learning (CoNLL), Jun.
N. Ratliff, J. Andrew (Drew) Bagnell, and M. Zinkevich.
2007. (Online) subgradient methods for structured
prediction. In Proceedings of the International Work-
shop on Artificial Intelligence and Statistics, March.
S. Shalev-Shwartz and Y. Singer. 2006. Online learn-
ing meets optimization in the dual. In Proceedings of
the Annual ACM Workshop on Computational Learn-
ing Theory (COLT).
S. Shalev-Shwartz and T. Zhang. 2013. Stochastic dual
coordinate ascent methods for regularized loss min-
imization. Journal of Machine Learning Research,
14:567–599.
S. Shalev-Shwartz, Y. Singer, and N. Srebro. 2007. Pe-
gasos: primal estimated sub-gradient solver for SVM.
In Zoubin Ghahramani, editor, Proceedings of the In-
ternational Conference on Machine Learning (ICML),
pages 807–814. Omnipress.
S. Shevade, P. Balamurugan, S. Sundararajan, and
S. Keerthi. 2011. A sequential dual method for struc-
tural SVMs. In IEEE International Conference on
Data Mining(ICDM).
E. F. T. K. Sang and F. De Meulder. 2003. Introduction to
the CoNLL-2003 shared task: Language-independent
named entity recognition. In Walter Daelemans and
Miles Osborne, editors, Proceedings of CoNLL-2003,
pages 142–147. Edmonton, Canada.
B. Taskar, C. Guestrin, and D. Koller. 2004a. Max-
margin markov networks. In The Conference on
Advances in Neural Information Processing Systems
(NIPS).
B. Taskar, D. Klein, M. Collins, D. Koller, and C. Man-
ning. 2004b. Max-margin parsing. In Proceedings
of the Conference on Empirical Methods for Natural
Language Processing (EMNLP).
B. Taskar, S. Lacoste-julien, and M. I. Jordan. 2005.
Structured prediction via the extragradient method. In
The Conference on Advances in Neural Information
Processing Systems (NIPS).
B. Taskar, S. Lacoste-Julien, and M. I Jordan. 2006.
Structured prediction, dual extragradient and bregman
projections. Journal of Machine Learning Research,
7:1627–1653.
I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.
2004. Support vector machine learning for interde-
pendent and structured output spaces. In Proceedings
of the International Conference on Machine Learning
(ICML).
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: a simple and general method for semi-
supervised learning. In Proceedings of the Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 384–394, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Po-Wei Wang and Chih-Jen Lin. 2013. Iteration com-
plexity of feasible descent methods for convex opti-
mization. Technical report, National Taiwan Univer-
sity.
C. Yu and T. Joachims. 2009. Learning structural SVMs
with latent variables. In Proceedings of the Interna-
tional Conference on Machine Learning (ICML).
</reference>
<page confidence="0.997159">
218
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.910042">
<title confidence="0.9981175">Dual Coordinate Descent Algorithms for Large Margin Structured Prediction</title>
<author confidence="0.990249">Ming-Wei Chang Wen-tau</author>
<affiliation confidence="0.945977">Microsoft</affiliation>
<address confidence="0.9986">Redmond, WA 98052,</address>
<abstract confidence="0.998679727272727">Due to the nature of complex NLP problems, prediction algorithms been important modeling tools for a wide range of tasks. While there exists evidence showing that linear Structural Support Vector Machine (SSVM) algorithm performs better than structured Perceptron, the SSVM algorithm is still less frequently chosen in the NLP community because of its relatively slow training speed. In this paper, we propose a fast and easy-toimplement dual coordinate descent algorithm for SSVMs. Unlike algorithms such as Perceptron and stochastic gradient descent, our method keeps track of dual variables and updates the weight vector more aggressively. As a result, this training process is as efficient as existing online learning methods, and yet derives consistently better models, as evaluated on four benchmark NLP datasets for part-ofspeech tagging, named-entity recognition and dependency parsing.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B Bohnet</author>
</authors>
<title>Very high accuracy and fast dependency parsing is not a contradiction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics, Proceedings the International Conference on Computational Linguistics (COLING).</booktitle>
<contexts>
<context position="25231" citStr="Bohnet, 2010" startWordPosition="4272" endWordPosition="4273">ferent entity types: PER, LOC, ORG and MISC. We evaluated the results using phrase-level F1. POS-WSJ The standard set for evaluating the performance of a part-of-speech tagger. The training, development and test sets consist of sections 0-18, 19-21 and 22-24 of the Penn Treebank data (Marcus et al., 1993), respectively. We evaluated the results by token-level accuracy. DP-WSJ We took sections 02-21 of Penn Treebank as the training set, section 00 as the development set and section 23 as the test set. We implement a simple version of hash kernel to speed up of training procedure for this task (Bohnet, 2010). We reported the unlabeled attachment accuracy for this task (McDonald et al., 2005). 213 80 79 Test F1 78 77 76 0 10 20 30 Test F1 86 84 82 80 0 20 40 60 80 100 120 Test Acc 97 96 95 DCD-SSVM FW-Struct SVM-Struct 0 200 400 600 Training Time (seconds) (a) Test F1 vs. Time in NER-MUC7 Training Time (seconds) (b) Test F1 vs. Time in NER-CoNLL Training Time (seconds) (c) Test Acc vs. Time in POS-WSJ Figure 2: Comparisons between the testing performance of DCD-SSVM, FW-Struct and SVM-Struct. Note that DCD-SSVM often obtain a better model with much less training time when comparing to SVM-Struct. </context>
</contexts>
<marker>Bohnet, 2010</marker>
<rawString>B. Bohnet. 2010. Very high accuracy and fast dependency parsing is not a contradiction. In Proceedings of the 23rd International Conference on Computational Linguistics, Proceedings the International Conference on Computational Linguistics (COLING).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Bottou</author>
</authors>
<title>Stochastic learning.</title>
<date>2004</date>
<booktitle>Advanced Lectures on Machine Learning, Lecture Notes in Artificial Intelligence, LNAI 3176,</booktitle>
<pages>146--168</pages>
<editor>In Olivier Bousquet and Ulrike von Luxburg, editors,</editor>
<publisher>Springer Verlag,</publisher>
<location>Berlin.</location>
<contexts>
<context position="9383" citStr="Bottou, 2004" startWordPosition="1497" endWordPosition="1498">mprove the model. CP has proven useful for solving SSVMs. For instance, Yu and Joachims (2009) proposed using CP methods to solve a 1-slack variable formulation, and showed that solving for a 1-slack variable formulation is much faster than solving the l-slack variable one (Eq. (2)). Chang et al. (2010) also proposed a variant of cutting plane method for solving the L2-Loss SSVM. This method uses a dual coordinate descent algorithm to solve the sub-problems. We call their approach the CPD method. Several other algorithms also aim at solving the L1-Loss SSVM. Stochastic gradient descent (SGD) (Bottou, 2004; Shalev-Shwartz et al., 2007) is a technique for optimizing general convex functions and has been applied to solving the 2Without Δ(y, yi) in Eq. 2, the optimal w would be zero. L1-Loss SSVM (Ratliff et al., 2007). Taskar et al. (2004a) proposed a structured SMO algorithm. Because the algorithm solves the dual formulation of the L1-Loss SSVM, it requires picking a violation pair for each update. In contrast, because each dual variable can be updated independently in our DCD algorithm, the implementation is relatively simple. The extragradient algorithm has also been applied to solving the L1-</context>
<context position="28809" citStr="Bottou, 2004" startWordPosition="4875" endWordPosition="4876">onvergence rate, we shuffle the training examples at each iteration. MIRA Margin Infused Relaxed Algorithm (MIRA) (Crammer et al., 2005) is the online learning algorithm that explicitly uses the notion of margin to update the weight vector. We use 1-best MIRA in our experiments. To increase 8http://www.cs.cornell.edu/People/tj/ svm_light/svm_hmm.html ⎡ ⎢ ⎢ ⎢ ⎢ ⎣ d(X, y) = N i=l , ⎤ ⎦⎥⎥⎥⎥ 214 the convergence speed, we shuffle the training examples at each iteration. Following (McDonald et al., 2005), we did not tune the C parameter for the MIRA algorithm. SGD Stochastic gradient descent (SGD) (Bottou, 2004) is a technique for optimizing general convex functions. In this paper, we use SGD as an alternative baseline for optimizing the L1-Loss SSVM objective function (Eq. (2) with higne loss).9 When using SGD, the learning rate must be carefully tuned. Following (Bottou, 2004), the learning rate is obtained by η0 (1.0 + (η0T/C))0.75, where C is the regularization parameter, T is the number of updates so far and η0 is the initial step size. The parameter η0 was selected among the set {2−1,2−2,2−3,2−4,2−5} by running the SGD algorithm on a set of 1000 randomly sampled examples, and then choosing the </context>
</contexts>
<marker>Bottou, 2004</marker>
<rawString>L. Bottou. 2004. Stochastic learning. In Olivier Bousquet and Ulrike von Luxburg, editors, Advanced Lectures on Machine Learning, Lecture Notes in Artificial Intelligence, LNAI 3176, pages 146–168. Springer Verlag, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Chang</author>
<author>V Srikumar</author>
<author>D Goldwasser</author>
<author>D Roth</author>
</authors>
<title>Structured output learning with indirect supervision.</title>
<date>2010</date>
<booktitle>In Proceedings of the International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="9075" citStr="Chang et al. (2010)" startWordPosition="1445" endWordPosition="1448"> et al., 2004; Joachims et al., 2009) have been the dominant method for learning the L1-Loss SSVM. Eq. (2) contains an exponential number of constraints. The cutting plane (CP) methods iteratively select a subset of active constraints for each example then solve a sub-problem which contains active constraints to improve the model. CP has proven useful for solving SSVMs. For instance, Yu and Joachims (2009) proposed using CP methods to solve a 1-slack variable formulation, and showed that solving for a 1-slack variable formulation is much faster than solving the l-slack variable one (Eq. (2)). Chang et al. (2010) also proposed a variant of cutting plane method for solving the L2-Loss SSVM. This method uses a dual coordinate descent algorithm to solve the sub-problems. We call their approach the CPD method. Several other algorithms also aim at solving the L1-Loss SSVM. Stochastic gradient descent (SGD) (Bottou, 2004; Shalev-Shwartz et al., 2007) is a technique for optimizing general convex functions and has been applied to solving the 2Without Δ(y, yi) in Eq. 2, the optimal w would be zero. L1-Loss SSVM (Ratliff et al., 2007). Taskar et al. (2004a) proposed a structured SMO algorithm. Because the algor</context>
<context position="30228" citStr="Chang et al. (2010)" startWordPosition="5107" endWordPosition="5110">g speed, we cached all the feature vectors generated by the gold labeled data once computed. This applied to all algorithms except SVM-Struct, which has its own caching mechanism. We report the performance of the averaged weight vectors for Perceptron and MIRA. 4.4 Results We present the experimental results below on comparing different dual coordinate descent methods, as well as comparing our main algorithm, DCDSSVM, with other structured learning approaches. 4.4.1 Comparisons of DCD Methods We compared three DCD methods: DCD-Light, DCD-SSVM and CPD. CPD is a cutting plane method proposed by Chang et al. (2010), which uses 9To compare with SGD using its best setting, we report only the results of SGD on the L1-Loss SSVM as we found tuning the step size for the L2-Loss SSVM is more difficult. a dual coordinate descent algorithm to solve the internal sub-problems. We specifically included CPD as it also targets at the L2-Loss SSVM. Because different optimization strategies will reach the same objective values eventually, comparing them on prediction accuracy of the final models is not meaningful. Instead, here we compare how fast each algorithm converges as shown in Figure 1. Each marker on the line i</context>
</contexts>
<marker>Chang, Srikumar, Goldwasser, Roth, 2010</marker>
<rawString>M. Chang, V. Srikumar, D. Goldwasser, and D. Roth. 2010. Structured output learning with indirect supervision. In Proceedings of the International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Cohn</author>
<author>M Lapata</author>
</authors>
<title>Sentence compression as tree transduction.</title>
<date>2009</date>
<journal>Journal of AI Research,</journal>
<volume>34</volume>
<pages>674</pages>
<contexts>
<context position="2044" citStr="Cohn and Lapata, 2009" startWordPosition="289" endWordPosition="293">ions in the output space, structured output prediction algorithms have been shown to outperform significantly simple binary or multi-class classifiers (Lafferty et al., 2001; Collins, 2002; McDonald et al., 2005). Among the existing structured output prediction algorithms, the linear Structural Support Vector Machine (SSVM) algorithm (Tsochantaridis et al., 2004; Joachims et al., 2009) has shown outstanding performance in several NLP tasks, such as bilingual word alignment (Moore et al., 2007), constituency and dependency parsing (Taskar et al., 2004b; Koo et al., 2007), sentence compression (Cohn and Lapata, 2009) and document summarization (Li et al., 2009). Nevertheless, as a learning method for NLP, the SSVM algorithm has been less than popular algorithms such as the structured Perceptron (Collins, 2002). This may be due to the fact that current SSVM implementations often suffer from several practical issues. First, state-of-the-art implementations of SSVM such as cutting plane methods (Joachims et al., 2009) are typically complicated.1 Second, while methods like stochastic gradient descent are simple to implement, tuning learning rates can be difficult. Finally, while SSVM models can achieve superi</context>
</contexts>
<marker>Cohn, Lapata, 2009</marker>
<rawString>T. Cohn and M. Lapata. 2009. Sentence compression as tree transduction. Journal of AI Research, 34:637– 674, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>A Globerson</author>
<author>T Koo</author>
<author>X Carreras</author>
<author>P L Bartlett</author>
</authors>
<title>Exponentiated gradient algorithms for conditional random fields and max-margin Markov networks.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>9</volume>
<contexts>
<context position="10567" citStr="Collins et al., 2008" startWordPosition="1682" endWordPosition="1685"> also been applied to solving the L1-Loss SSVM (Taskar et al., 2005). Unlike our DCD algorithm, the extragradient method requires the learning rate to be specified. The connections between dual methods and the online algorithms have been previously discussed. Specifically, Shalev-Shwartz and Singer (2006) connects the dual methods to a wide range of online learning algorithms. In (Martins et al., 2010), the authors apply similar techniques on L1-Loss SSVMs and show that the proposed algorithm can be faster than the SGD algorithm. Exponentiated Gradient (EG) descent (Kivinen and Warmuth, 1995; Collins et al., 2008) has recently been applied to solving the L1-Loss SSVM. Compared to other SSVM learners, EG requires manual tuning of the step size. In addition, EG requires solution of the sum-product inference problem, which can be more expensive than solving Eq. (3) (Taskar et al., 2006). Very recently, LacosteJulien et al. (2013) proposed a block-coordinate descent algorithm for the L1-Loss SSVM based on the Frank-Wolfe algorithm (FW-Struct), which has been shown to outperform the EG algorithm significantly. Similar to our DCD algorithm, FW calculates the optimal learning rate when updating the dual varia</context>
</contexts>
<marker>Collins, Globerson, Koo, Carreras, Bartlett, 2008</marker>
<rawString>M. Collins, A. Globerson, T. Koo, X. Carreras, and P. L. Bartlett. 2008. Exponentiated gradient algorithms for conditional random fields and max-margin Markov networks. Journal of Machine Learning Research, 9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods for Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="1610" citStr="Collins, 2002" startWordPosition="229" endWordPosition="230">ed on four benchmark NLP datasets for part-ofspeech tagging, named-entity recognition and dependency parsing. 1 Introduction Complex natural language processing tasks are inherently structured. From sequence labeling problems like part-of-speech tagging and named entity recognition to tree construction tasks like syntactic parsing, strong dependencies exist among the labels of individual components. By modeling such relations in the output space, structured output prediction algorithms have been shown to outperform significantly simple binary or multi-class classifiers (Lafferty et al., 2001; Collins, 2002; McDonald et al., 2005). Among the existing structured output prediction algorithms, the linear Structural Support Vector Machine (SSVM) algorithm (Tsochantaridis et al., 2004; Joachims et al., 2009) has shown outstanding performance in several NLP tasks, such as bilingual word alignment (Moore et al., 2007), constituency and dependency parsing (Taskar et al., 2004b; Koo et al., 2007), sentence compression (Cohn and Lapata, 2009) and document summarization (Li et al., 2009). Nevertheless, as a learning method for NLP, the SSVM algorithm has been less than popular algorithms such as the struct</context>
<context position="2930" citStr="Collins, 2002" startWordPosition="431" endWordPosition="432">r from several practical issues. First, state-of-the-art implementations of SSVM such as cutting plane methods (Joachims et al., 2009) are typically complicated.1 Second, while methods like stochastic gradient descent are simple to implement, tuning learning rates can be difficult. Finally, while SSVM models can achieve superior accuracy, this often requires long training time. In this paper, we propose a novel optimization method for efficiently training linear SSVMs. Our method not only is easy to implement, but also has excellent training speed, competitive with both structured Perceptron (Collins, 2002) and MIRA (Crammer et al., 2005). When evaluated on several NLP tasks, including POS tagging, NER and dependency parsing, this optimization method also outperforms other approaches in terms of prediction accuracy. Our final algorithm is a dual coordinate 1Our algorithm is easy to implement mainly because we use the square hinge loss function. 207 Transactions of the Association for Computational Linguistics, 1 (2013) 207–218. Action Editor: Ben Taskar. Submitted 10/2012; Revised 3/2013; Published 5/2013. c�2013 Association for Computational Linguistics. descent (DCD) algorithm for solving a st</context>
<context position="6064" citStr="Collins (2002)" startWordPosition="928" endWordPosition="929">structure is denoted by y. The feature vector Φ(x, y) is a function defined over an input-output pair (x, y). We focus on linear models with predictions made by solving the decoding problem: The set Y(xi) represents all possible (exponentially many) structures that can be generated from the example xi. Let yi be the true structured label of xi. The difference between the feature vectors of the correct label yi and y is denoted as ΦYi,Y(xi) � Φ(xi, yi) − Φ(xi, y). We define Δ(yi, y) as a distance function between two structures. 2.1 Perceptron and MIRA Structured Perceptron First introduced by Collins (2002), the structured Perceptron algorithm runs two steps iteratively: first, it finds the best structured prediction y for an example with the current weight vector using Eq. (1); then the weight vector is updated according to the difference between the feature vectors of the true label and the prediction: w +- w + ΦYi,Y(xi). Inspired by Freund and Schapire (1999), Collins (2002) also proposed the averaged structured Perceptron, which maintains an averaged weight vector throughout the training procedure. This technique has been shown to improve the generalization ability of the model. MIRA The Mar</context>
<context position="28177" citStr="Collins (2002)" startWordPosition="4773" endWordPosition="4774">low the suggestion in (Joachims et al., 2009): if the value of a dual variable becomes zero, its corresponding structure will be removed from the working set to improve the speed. SVM-Struct We used the latest (v3.10) of SVMHMM.8 This version uses the cutting plane method on a 1-slack variable formulation (Joachims et al., 2009) for the L1-Loss SSVM. SVM-Struct was implemented in C and all the other algorithms are implemented in C#. We did not apply SVM-Struct to DP-WSJ because there is no native implementation. Perceptron This refers to the averaged structured Perceptron method introduced by Collins (2002). To speed up the convergence rate, we shuffle the training examples at each iteration. MIRA Margin Infused Relaxed Algorithm (MIRA) (Crammer et al., 2005) is the online learning algorithm that explicitly uses the notion of margin to update the weight vector. We use 1-best MIRA in our experiments. To increase 8http://www.cs.cornell.edu/People/tj/ svm_light/svm_hmm.html ⎡ ⎢ ⎢ ⎢ ⎢ ⎣ d(X, y) = N i=l , ⎤ ⎦⎥⎥⎥⎥ 214 the convergence speed, we shuffle the training examples at each iteration. Following (McDonald et al., 2005), we did not tune the C parameter for the MIRA algorithm. SGD Stochastic gradi</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M. Collins. 2002. Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. In Proceedings of the Conference on Empirical Methods for Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crammer</author>
<author>R Mcdonald</author>
<author>F Pereira</author>
</authors>
<title>Scalable large-margin online learning for structured classification.</title>
<date>2005</date>
<tech>Technical report,</tech>
<institution>Department of Computer and Information Science, University of Pennsylvania.</institution>
<contexts>
<context position="2962" citStr="Crammer et al., 2005" startWordPosition="435" endWordPosition="438">issues. First, state-of-the-art implementations of SSVM such as cutting plane methods (Joachims et al., 2009) are typically complicated.1 Second, while methods like stochastic gradient descent are simple to implement, tuning learning rates can be difficult. Finally, while SSVM models can achieve superior accuracy, this often requires long training time. In this paper, we propose a novel optimization method for efficiently training linear SSVMs. Our method not only is easy to implement, but also has excellent training speed, competitive with both structured Perceptron (Collins, 2002) and MIRA (Crammer et al., 2005). When evaluated on several NLP tasks, including POS tagging, NER and dependency parsing, this optimization method also outperforms other approaches in terms of prediction accuracy. Our final algorithm is a dual coordinate 1Our algorithm is easy to implement mainly because we use the square hinge loss function. 207 Transactions of the Association for Computational Linguistics, 1 (2013) 207–218. Action Editor: Ben Taskar. Submitted 10/2012; Revised 3/2013; Published 5/2013. c�2013 Association for Computational Linguistics. descent (DCD) algorithm for solving a structured output SVM problem with</context>
<context position="6747" citStr="Crammer et al. (2005)" startWordPosition="1032" endWordPosition="1035">ly: first, it finds the best structured prediction y for an example with the current weight vector using Eq. (1); then the weight vector is updated according to the difference between the feature vectors of the true label and the prediction: w +- w + ΦYi,Y(xi). Inspired by Freund and Schapire (1999), Collins (2002) also proposed the averaged structured Perceptron, which maintains an averaged weight vector throughout the training procedure. This technique has been shown to improve the generalization ability of the model. MIRA The Margin Infused Relaxed Algorithm (MIRA), which was introduced by Crammer et al. (2005), explicitly uses the notion of margin to update the weight vector. The MIRA updates the weight vector by calculating the step size using 211w − w0112 1 S.T. wT ΦYi,Y(xi) &gt; Δ(y, yi), by E Wk, where Wk is a set containing the best-k structures according to the weight vector w0. MIRA is a very popular method in the NLP community and has been applied to NLP tasks like word segmentation and part-of-speech tagging (Kruengkrai et al., 2009), NER and chunking (Mejer and Crammer, 2010) and dependency parsing (McDonald et al., 2005). 2.2 Structural SVM Structural SVM (SSVM) is a maximum margin model fo</context>
<context position="28332" citStr="Crammer et al., 2005" startWordPosition="4796" endWordPosition="4799">rking set to improve the speed. SVM-Struct We used the latest (v3.10) of SVMHMM.8 This version uses the cutting plane method on a 1-slack variable formulation (Joachims et al., 2009) for the L1-Loss SSVM. SVM-Struct was implemented in C and all the other algorithms are implemented in C#. We did not apply SVM-Struct to DP-WSJ because there is no native implementation. Perceptron This refers to the averaged structured Perceptron method introduced by Collins (2002). To speed up the convergence rate, we shuffle the training examples at each iteration. MIRA Margin Infused Relaxed Algorithm (MIRA) (Crammer et al., 2005) is the online learning algorithm that explicitly uses the notion of margin to update the weight vector. We use 1-best MIRA in our experiments. To increase 8http://www.cs.cornell.edu/People/tj/ svm_light/svm_hmm.html ⎡ ⎢ ⎢ ⎢ ⎢ ⎣ d(X, y) = N i=l , ⎤ ⎦⎥⎥⎥⎥ 214 the convergence speed, we shuffle the training examples at each iteration. Following (McDonald et al., 2005), we did not tune the C parameter for the MIRA algorithm. SGD Stochastic gradient descent (SGD) (Bottou, 2004) is a technique for optimizing general convex functions. In this paper, we use SGD as an alternative baseline for optimizin</context>
</contexts>
<marker>Crammer, Mcdonald, Pereira, 2005</marker>
<rawString>K. Crammer, R. Mcdonald, and F. Pereira. 2005. Scalable large-margin online learning for structured classification. Technical report, Department of Computer and Information Science, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Desrosiers</author>
<author>M E L¨ubbecke</author>
</authors>
<title>A primer in column generation.</title>
<date>2005</date>
<booktitle>In Column Generation,</booktitle>
<pages>1--32</pages>
<publisher>Springer.</publisher>
<marker>Desrosiers, L¨ubbecke, 2005</marker>
<rawString>J. Desrosiers and M. E. L¨ubbecke. 2005. A primer in column generation. In Column Generation, pages 1– 32. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1997</date>
<booktitle>In Proceedings the International Conference on Computational Linguistics (COLING),</booktitle>
<pages>340--345</pages>
<contexts>
<context position="26812" citStr="Eisner, 1997" startWordPosition="4550" endWordPosition="4551">ber of tokens in this sequence, yi represents the i-th token in the sequence y, [yi = 1] is the indictor variable and k is the number of tags. The inference problems are solved by the Viterbi algorithm. The emission features used in both POS and NER are the standard ones, including word features, word-shape features, etc. For NER, we used additional simple gazetteer features7 and word cluster features (Turian et al., 2010) For dependency parsing, we followed the setting described in (McDonald et al., 2005) and used similar features. The decoding algorithm is the firstorder Eisner’s algorithm (Eisner, 1997). 4.3 Algorithms and Implementation Detail For all SSVM algorithms (including SGD), C was chosen among the set 10.01, 0.05,0.1,0.5, 1, 5} according to the accuracy/F1 on the development set. For each task, the same features were used by all 7Adding Wikipedia gazetteers would likely increase the performance significantly (Ratinov and Roth, 2009). algorithms. For NER-MUC7, NER-CoNLL and POS-WSJ, we ran the online algorithms and DCDSSVM for 25 iterations. For DP-WSJ, we only let the algorithms run for 10 iterations as the inference procedure is very expensive computationally. The algorithms in th</context>
</contexts>
<marker>Eisner, 1997</marker>
<rawString>J. M. Eisner. 1997. Three new probabilistic models for dependency parsing: An exploration. In Proceedings the International Conference on Computational Linguistics (COLING), pages 340–345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Freund</author>
<author>R Schapire</author>
</authors>
<title>Large margin classification using the Perceptron algorithm.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context position="6426" citStr="Freund and Schapire (1999)" startWordPosition="985" endWordPosition="988"> difference between the feature vectors of the correct label yi and y is denoted as ΦYi,Y(xi) � Φ(xi, yi) − Φ(xi, y). We define Δ(yi, y) as a distance function between two structures. 2.1 Perceptron and MIRA Structured Perceptron First introduced by Collins (2002), the structured Perceptron algorithm runs two steps iteratively: first, it finds the best structured prediction y for an example with the current weight vector using Eq. (1); then the weight vector is updated according to the difference between the feature vectors of the true label and the prediction: w +- w + ΦYi,Y(xi). Inspired by Freund and Schapire (1999), Collins (2002) also proposed the averaged structured Perceptron, which maintains an averaged weight vector throughout the training procedure. This technique has been shown to improve the generalization ability of the model. MIRA The Margin Infused Relaxed Algorithm (MIRA), which was introduced by Crammer et al. (2005), explicitly uses the notion of margin to update the weight vector. The MIRA updates the weight vector by calculating the step size using 211w − w0112 1 S.T. wT ΦYi,Y(xi) &gt; Δ(y, yi), by E Wk, where Wk is a set containing the best-k structures according to the weight vector w0. M</context>
</contexts>
<marker>Freund, Schapire, 1999</marker>
<rawString>Y. Freund and R. Schapire. 1999. Large margin classification using the Perceptron algorithm. Machine Learning, 37(3):277–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-J Hsieh</author>
<author>K-W Chang</author>
<author>C-J Lin</author>
<author>S S Keerthi</author>
<author>S Sundararajan</author>
</authors>
<title>A dual coordinate descent method for large-scale linear SVM.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Conference on Machine Learning (ICML),</booktitle>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="13422" citStr="Hsieh et al., 2008" startWordPosition="2169" endWordPosition="2172">remain. This is important because now each dual variable can be updated without changing values of the other dual variables. We can then update one single dual variable at a time. As a result, this dual formulation allows us to design a simple and principled dual coordinate descent (DCD) optimization method. DCD algorithms consist of two iterative steps: 1. Pick a dual variable αi,y. 2. Update the dual variable and the weight vector. Go to 1. In the normal binary classification case, how to select dual variables to solve is not an issue as choosing them randomly works effectively in practice (Hsieh et al., 2008). However, this is not a practical scheme for training SSVM models given that the number of dual variables in Eq. (4) can be very large because of the exponentially many legitimate output structures. To address this issue, we introduce the concept of working set below. 3For L1-Loss SSVM, there are the equality constraints: Ey∈Y(x,) αi,y = C, ∀i. )/Vi = {y |Vy E Y(xi), αi,y &gt; 01. Intuitively, the working set )/Vi records the output structures that are similar to the true structure yi.We set all dual variables to be zero initially (therefore, w = 0 as well), so )/Vi = 0 for all i. Then the algor</context>
<context position="17497" citStr="Hsieh et al., 2008" startWordPosition="2901" endWordPosition="2904">of examples before updating the weight vector in each iteration, as done in CPD and 1-slack variable formulation of SVM-Struct, DCDLight updates the model weights after solving the inference problem for each individual example. Algorithm 2 depicts the detailed steps. In Line 5, the lossaugmented inference (Eq. (3)) is performed; then the weight vector is updated in Line 9 – all of the structures and dual variables in the working set are used to update the weight vector. Note that there is a δ parameter in Line 6 to control how precise we would like to solve this SSVM problem. As suggested in (Hsieh et al., 2008), we shuffle the examples in each iteration (Line 3) as it helps the algorithm converge faster. DCD-Light has several noticeable differences when compared to the most popular online learning method, averaged Perceptron. First, DCD-Light performs the loss-augmented inference (Eq. (3)) at Line 5 instead of the argmax inference (Eq. (1)). Second, the algorithm updates the weight vector with all structures in the working set. Finally, DCDlight does not average the weight vectors. 3.1.2 DCD-SSVM Observing that DCD-Light does not fully utilize the saved dual variables in the working set, we propose </context>
<context position="33613" citStr="Hsieh et al., 2008" startWordPosition="5660" endWordPosition="5663"> online learning algorithms. Note that the results diverge when comparing Perceptron and MIRA. In general, DCD-SSVM is the most stable algorithm. Task/Data DCD Percep MIRA SGD NER-MUC7 79.4 78.5 78.8 77.8 NER-CoNLL 85.6 85.3 85.1 84.2 POS-WSJ 97.1 96.9 96.9 96.9 DP-WSJ 90.8 90.3 90.2 90.9 Table 1: Performance of online learning algorithms and the DCD-SSVM algorithm on the testing sets. NER is measured by F, while others by accuracy. 4.4.3 DCD-SSVM, MIRA, Perceptron and SGD As in binary classification, large-margin methods like SVMs often perform better than algorithms like Perceptron and SGD (Hsieh et al., 2008; ShalevShwartz and Zhang, 2013), here we observe similar behaviors in the structured output domain. Table 1 shows the final test accuracy numbers or F1 scores of models trained by algorithms including Perceptron, MIRA and SGD, compared to those of the SSVM models trained by DCD-SSVM. Among the benchmark datasets and tasks we have experimented with, DCD-SSVM derived the most accurate models, except for DP-WSJ when compared to SGD. Perhaps a more interesting comparison is on the training speed, which can be observed in Figure 3. Compared to other online algorithms, DCDSSVM can take advantage of</context>
</contexts>
<marker>Hsieh, Chang, Lin, Keerthi, Sundararajan, 2008</marker>
<rawString>C.-J. Hsieh, K.-W. Chang, C.-J. Lin, S. S. Keerthi, and S. Sundararajan. 2008. A dual coordinate descent method for large-scale linear SVM. In Proceedings of the International Conference on Machine Learning (ICML), New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
<author>T Finley</author>
<author>Chun-Nam Yu</author>
</authors>
<title>Cutting-plane training of structural SVMs.</title>
<date>2009</date>
<booktitle>Machine Learning,</booktitle>
<volume>77</volume>
<issue>1</issue>
<contexts>
<context position="1810" citStr="Joachims et al., 2009" startWordPosition="254" endWordPosition="257">d. From sequence labeling problems like part-of-speech tagging and named entity recognition to tree construction tasks like syntactic parsing, strong dependencies exist among the labels of individual components. By modeling such relations in the output space, structured output prediction algorithms have been shown to outperform significantly simple binary or multi-class classifiers (Lafferty et al., 2001; Collins, 2002; McDonald et al., 2005). Among the existing structured output prediction algorithms, the linear Structural Support Vector Machine (SSVM) algorithm (Tsochantaridis et al., 2004; Joachims et al., 2009) has shown outstanding performance in several NLP tasks, such as bilingual word alignment (Moore et al., 2007), constituency and dependency parsing (Taskar et al., 2004b; Koo et al., 2007), sentence compression (Cohn and Lapata, 2009) and document summarization (Li et al., 2009). Nevertheless, as a learning method for NLP, the SSVM algorithm has been less than popular algorithms such as the structured Perceptron (Collins, 2002). This may be due to the fact that current SSVM implementations often suffer from several practical issues. First, state-of-the-art implementations of SSVM such as cutti</context>
<context position="4486" citStr="Joachims et al., 2009" startWordPosition="660" endWordPosition="663">and the weight vector) without running inference. Conceptually, this hybrid approach operates at a balanced trade-off point between inference and weight update, performing better than with either component alone. Our contributions in this work can be summarized as follows. Firstly, our proposed algorithm shows that even for structured output prediction, an SSVM model can be trained as efficiently as a structured Perceptron one. Secondly, we conducted a careful experimental study on three NLP tasks using four different benchmark datasets. When compared with previous methods for training SSVMs (Joachims et al., 2009), our method achieves similar performance using less training time. When compared to commonly used learning algorithms such as Perceptron and MIRA, the model trained by our algorithm performs consistently better when given the same amount of training time. We believe our method can be a powerful tool for many different NLP tasks. The rest of our paper is organized as follows. We first describe our approach by formally defining the problem and notation in Sec. 2, where we also review some existing, closely-related structuredoutput learning algorithms and optimization techniques. We introduce th</context>
<context position="8493" citStr="Joachims et al., 2009" startWordPosition="1353" endWordPosition="1356">on A is not only necessary,2 but also enables us to use more information on the differences between the structures in the training phase. For example, using Hamming distance for sequence labeling is a reasonable choice, as the model can express finer distinctions between structures yi and y. When training an SSVM model, we often need to solve the loss-augmented inference problem, arg max 1wT Φ(xi, y) + A(yi, y)] . (3) Y∈Y(Xi) Note that it is a different inference problem than the decoding problem in Eq. (1). Algorithms for training SSVM Cutting plane (CP) methods (Tsochantaridis et al., 2004; Joachims et al., 2009) have been the dominant method for learning the L1-Loss SSVM. Eq. (2) contains an exponential number of constraints. The cutting plane (CP) methods iteratively select a subset of active constraints for each example then solve a sub-problem which contains active constraints to improve the model. CP has proven useful for solving SSVMs. For instance, Yu and Joachims (2009) proposed using CP methods to solve a 1-slack variable formulation, and showed that solving for a 1-slack variable formulation is much faster than solving the l-slack variable one (Eq. (2)). Chang et al. (2010) also proposed a v</context>
<context position="20264" citStr="Joachims et al., 2009" startWordPosition="3386" endWordPosition="3389">ers and L¨ubbecke, 2005), where the master problem is to solve the dual problem that focuses on the variables in the working sets, and the subproblem is to find new variables for the working sets. In Sec. 4, we will demonstrate the importance of balancing these two problems by comparing DCD-SSVM and DCDLight. 3.2 Convergence Analysis We now present the theoretic analysis of both DCDLight and DCD-SSVM, and address two main topics: (1) whether the working sets will grow exponentially and (2) the convergence rate. Due to the lack of space, we show only the main theorems. Leveraging Theorem 5 in (Joachims et al., 2009), we can prove that the DCD algorithms only add a limited number of variables in the working sets, and have the following theorem. Theorem 1. The number of times that DCD-Light or DCD-SSVM /adds structures into working sets is bounded by O12(R2+ 22 )lCΔ21 where R2 is defined as maxi,¯y\kΦyi,¯y(Xi)k2, and Δ is the upper bound of Δ(yi, y&apos;), ∀yi, y&apos; ∈ Y(xi). We discuss next the convergence rates of our DCD algorithms under two different conditions – when the working sets are fixed and the general case. If the working sets are fixed in DCD algorithms, they become cyclic dual coordinate descent met</context>
<context position="25990" citStr="Joachims et al., 2009" startWordPosition="4404" endWordPosition="4407">4 82 80 0 20 40 60 80 100 120 Test Acc 97 96 95 DCD-SSVM FW-Struct SVM-Struct 0 200 400 600 Training Time (seconds) (a) Test F1 vs. Time in NER-MUC7 Training Time (seconds) (b) Test F1 vs. Time in NER-CoNLL Training Time (seconds) (c) Test Acc vs. Time in POS-WSJ Figure 2: Comparisons between the testing performance of DCD-SSVM, FW-Struct and SVM-Struct. Note that DCD-SSVM often obtain a better model with much less training time when comparing to SVM-Struct. 4.2 Features and Inference Algorithms For the sequence labeling tasks, NER and POS, we followed the discriminative HMM settings used in (Joachims et al., 2009) and defined the features as demi(xi, yi) [yi = 1][yi−1 = 1] [yi = 1][yi−1 = 2] [yi = k][yi−1 = k] where demi is the feature vector dedicated to the i-th token (or, the emission features), N represents the number of tokens in this sequence, yi represents the i-th token in the sequence y, [yi = 1] is the indictor variable and k is the number of tags. The inference problems are solved by the Viterbi algorithm. The emission features used in both POS and NER are the standard ones, including word features, word-shape features, etc. For NER, we used additional simple gazetteer features7 and word clu</context>
<context position="27608" citStr="Joachims et al., 2009" startWordPosition="4677" endWordPosition="4680">development set. For each task, the same features were used by all 7Adding Wikipedia gazetteers would likely increase the performance significantly (Ratinov and Roth, 2009). algorithms. For NER-MUC7, NER-CoNLL and POS-WSJ, we ran the online algorithms and DCDSSVM for 25 iterations. For DP-WSJ, we only let the algorithms run for 10 iterations as the inference procedure is very expensive computationally. The algorithms in the experiments are: DCD Our dual coordinate descent method on the L2-Loss SSVM. For DCD-SSVM, r is set to be 5. For both DCD-Light and DCD-SSVM , we follow the suggestion in (Joachims et al., 2009): if the value of a dual variable becomes zero, its corresponding structure will be removed from the working set to improve the speed. SVM-Struct We used the latest (v3.10) of SVMHMM.8 This version uses the cutting plane method on a 1-slack variable formulation (Joachims et al., 2009) for the L1-Loss SSVM. SVM-Struct was implemented in C and all the other algorithms are implemented in C#. We did not apply SVM-Struct to DP-WSJ because there is no native implementation. Perceptron This refers to the averaged structured Perceptron method introduced by Collins (2002). To speed up the convergence r</context>
<context position="31738" citStr="Joachims et al. (2009)" startWordPosition="5355" endWordPosition="5358">ficient algorithm here. The reason behind the slow performance of CPD is clear. During early rounds of the algorithm, the weight vector is far from optimal, so it spends too much time using “bad” weight vectors to find the most violated structures. On the other hand, DCD-Light updates the weight vector more frequently, so it behaves much better in general. DCDSSVM spends more time on updating models during each batch, but keeps the same amount of time doing inference as DCD-Light. As a result, it finds a better trade-off between inference and learning. 4.4.2 DCD-SSVM, SVM-Struct and FW-Struct Joachims et al. (2009) proposed a 1-slack variable method for the L1-Loss SSVM. They showed that solving a 1-slack variable formulation is an order-of-magnitude faster than solving the original formulation (l-slack variables formulation). Nevertheless, from Figure 2, we can see the clear advantage of DCD-SSVM over SVM-Struct. Although using 1-slack variable has improved the learning speed, SVM-Struct still converges slower than DCD-SSVM. In addition, the performance of models trained by SVM-Struct in the early stage is quite unstable, which makes early stopping an ineffective strategy in practice when training time</context>
</contexts>
<marker>Joachims, Finley, Yu, 2009</marker>
<rawString>T. Joachims, T. Finley, and Chun-Nam Yu. 2009. Cutting-plane training of structural SVMs. Machine Learning, 77(1):27–59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kivinen</author>
<author>M K Warmuth</author>
</authors>
<title>Exponentiated gradient versus gradient descent for linear predictors.</title>
<date>1995</date>
<booktitle>In ACM Symp. of the Theory of Computing.</booktitle>
<contexts>
<context position="10544" citStr="Kivinen and Warmuth, 1995" startWordPosition="1678" endWordPosition="1681">extragradient algorithm has also been applied to solving the L1-Loss SSVM (Taskar et al., 2005). Unlike our DCD algorithm, the extragradient method requires the learning rate to be specified. The connections between dual methods and the online algorithms have been previously discussed. Specifically, Shalev-Shwartz and Singer (2006) connects the dual methods to a wide range of online learning algorithms. In (Martins et al., 2010), the authors apply similar techniques on L1-Loss SSVMs and show that the proposed algorithm can be faster than the SGD algorithm. Exponentiated Gradient (EG) descent (Kivinen and Warmuth, 1995; Collins et al., 2008) has recently been applied to solving the L1-Loss SSVM. Compared to other SSVM learners, EG requires manual tuning of the step size. In addition, EG requires solution of the sum-product inference problem, which can be more expensive than solving Eq. (3) (Taskar et al., 2006). Very recently, LacosteJulien et al. (2013) proposed a block-coordinate descent algorithm for the L1-Loss SSVM based on the Frank-Wolfe algorithm (FW-Struct), which has been shown to outperform the EG algorithm significantly. Similar to our DCD algorithm, FW calculates the optimal learning rate when </context>
</contexts>
<marker>Kivinen, Warmuth, 1995</marker>
<rawString>J. Kivinen and M. K. Warmuth. 1995. Exponentiated gradient versus gradient descent for linear predictors. In ACM Symp. of the Theory of Computing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>A Globerson</author>
<author>X Carreras</author>
<author>M Collins</author>
</authors>
<title>Structured prediction models via the matrix-tree theorem.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference of EMNLP-CoNLL,</booktitle>
<pages>141--150</pages>
<contexts>
<context position="1998" citStr="Koo et al., 2007" startWordPosition="283" endWordPosition="286">vidual components. By modeling such relations in the output space, structured output prediction algorithms have been shown to outperform significantly simple binary or multi-class classifiers (Lafferty et al., 2001; Collins, 2002; McDonald et al., 2005). Among the existing structured output prediction algorithms, the linear Structural Support Vector Machine (SSVM) algorithm (Tsochantaridis et al., 2004; Joachims et al., 2009) has shown outstanding performance in several NLP tasks, such as bilingual word alignment (Moore et al., 2007), constituency and dependency parsing (Taskar et al., 2004b; Koo et al., 2007), sentence compression (Cohn and Lapata, 2009) and document summarization (Li et al., 2009). Nevertheless, as a learning method for NLP, the SSVM algorithm has been less than popular algorithms such as the structured Perceptron (Collins, 2002). This may be due to the fact that current SSVM implementations often suffer from several practical issues. First, state-of-the-art implementations of SSVM such as cutting plane methods (Joachims et al., 2009) are typically complicated.1 Second, while methods like stochastic gradient descent are simple to implement, tuning learning rates can be difficult.</context>
</contexts>
<marker>Koo, Globerson, Carreras, Collins, 2007</marker>
<rawString>T. Koo, A. Globerson, X. Carreras, and M. Collins. 2007. Structured prediction models via the matrix-tree theorem. In Proceedings of the 2007 Joint Conference of EMNLP-CoNLL, pages 141–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Kruengkrai</author>
<author>K Uchimoto</author>
<author>J Kazama</author>
<author>Y Wang</author>
<author>K Torisawa</author>
<author>H Isahara</author>
</authors>
<title>An error-driven word-character hybrid model for joint chinese word segmentation and pos tagging.</title>
<date>2009</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>513--521</pages>
<contexts>
<context position="7185" citStr="Kruengkrai et al., 2009" startWordPosition="1110" endWordPosition="1113">ocedure. This technique has been shown to improve the generalization ability of the model. MIRA The Margin Infused Relaxed Algorithm (MIRA), which was introduced by Crammer et al. (2005), explicitly uses the notion of margin to update the weight vector. The MIRA updates the weight vector by calculating the step size using 211w − w0112 1 S.T. wT ΦYi,Y(xi) &gt; Δ(y, yi), by E Wk, where Wk is a set containing the best-k structures according to the weight vector w0. MIRA is a very popular method in the NLP community and has been applied to NLP tasks like word segmentation and part-of-speech tagging (Kruengkrai et al., 2009), NER and chunking (Mejer and Crammer, 2010) and dependency parsing (McDonald et al., 2005). 2.2 Structural SVM Structural SVM (SSVM) is a maximum margin model for the structured output prediction setting. Training SSVM is equivalent to solving the following global optimization problem: min W arg max wTΦ(xi, y). (1) min 11w112 �l L(xi, yi, w), (2) Y∈Y(Xi) W 2 + C i=1 208 where l is the number of labeled examples and L(xi, yi, w) = e (max [A(yi, y) − w TΦ Yxi,Y(xi)1 The typical choice of e is e(a) = at. If t = 2 is used, we refer to the SSVM defined in Eq. (2) as the L2- Loss SSVM. If hinge los</context>
</contexts>
<marker>Kruengkrai, Uchimoto, Kazama, Wang, Torisawa, Isahara, 2009</marker>
<rawString>C. Kruengkrai, K. Uchimoto, J. Kazama, Y. Wang, K. Torisawa, and H. Isahara. 2009. An error-driven word-character hybrid model for joint chinese word segmentation and pos tagging. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 513–521.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lacoste-Julien</author>
<author>M Jaggi</author>
<author>M W Schmidt</author>
<author>P Pletscher</author>
</authors>
<title>Stochastic block-coordinate FrankWolfe optimization for structural SVMs.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="29576" citStr="Lacoste-Julien et al., 2013" startWordPosition="5002" endWordPosition="5005">VM objective function (Eq. (2) with higne loss).9 When using SGD, the learning rate must be carefully tuned. Following (Bottou, 2004), the learning rate is obtained by η0 (1.0 + (η0T/C))0.75, where C is the regularization parameter, T is the number of updates so far and η0 is the initial step size. The parameter η0 was selected among the set {2−1,2−2,2−3,2−4,2−5} by running the SGD algorithm on a set of 1000 randomly sampled examples, and then choosing the η0 with lowest primal objective function on these examples. FW-Struct FW-Struct represents the Frank-Wolfe algorithm for the L1-Loss SSVM (Lacoste-Julien et al., 2013). In order to improve the training speed, we cached all the feature vectors generated by the gold labeled data once computed. This applied to all algorithms except SVM-Struct, which has its own caching mechanism. We report the performance of the averaged weight vectors for Perceptron and MIRA. 4.4 Results We present the experimental results below on comparing different dual coordinate descent methods, as well as comparing our main algorithm, DCDSSVM, with other structured learning approaches. 4.4.1 Comparisons of DCD Methods We compared three DCD methods: DCD-Light, DCD-SSVM and CPD. CPD is a </context>
<context position="32449" citStr="Lacoste-Julien et al., 2013" startWordPosition="5463" endWordPosition="5466">a 1-slack variable formulation is an order-of-magnitude faster than solving the original formulation (l-slack variables formulation). Nevertheless, from Figure 2, we can see the clear advantage of DCD-SSVM over SVM-Struct. Although using 1-slack variable has improved the learning speed, SVM-Struct still converges slower than DCD-SSVM. In addition, the performance of models trained by SVM-Struct in the early stage is quite unstable, which makes early stopping an ineffective strategy in practice when training time is limited. We also compared our algorithms to FW-Struct. Our results agree with (Lacoste-Julien et al., 2013), which shows that the FW-Struct outperforms the SVM-Struct. In our experiments, we found that our DCD algorithms were competitive, sometimes converged faster than the FW-Struct. 215 Test F1 80 79 78 77 76 0 10 20 30 Training Time (seconds) (a) Test F1 vs. Time in NER-MUC7 0 100 200 300 400 Training Time (seconds) (b) Test Acc vs. Time in POS-WSJ 0 2,000 4,000 6,000 Training Time (seconds) (c) Test Acc vs. Time in DP-WSJ DCD-SSVM PERP MIRA SGD Test Acc 97.2 97 96.8 96.6 Test Acc 91 90 89 88 Figure 3: Comparisons between DCD-SSVM and popular online learning algorithms. Note that the results div</context>
</contexts>
<marker>Lacoste-Julien, Jaggi, Schmidt, Pletscher, 2013</marker>
<rawString>S. Lacoste-Julien, M. Jaggi, M. W. Schmidt, and P. Pletscher. 2013. Stochastic block-coordinate FrankWolfe optimization for structural SVMs. In Proceedings of the International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="1595" citStr="Lafferty et al., 2001" startWordPosition="225" endWordPosition="228">tter models, as evaluated on four benchmark NLP datasets for part-ofspeech tagging, named-entity recognition and dependency parsing. 1 Introduction Complex natural language processing tasks are inherently structured. From sequence labeling problems like part-of-speech tagging and named entity recognition to tree construction tasks like syntactic parsing, strong dependencies exist among the labels of individual components. By modeling such relations in the output space, structured output prediction algorithms have been shown to outperform significantly simple binary or multi-class classifiers (Lafferty et al., 2001; Collins, 2002; McDonald et al., 2005). Among the existing structured output prediction algorithms, the linear Structural Support Vector Machine (SSVM) algorithm (Tsochantaridis et al., 2004; Joachims et al., 2009) has shown outstanding performance in several NLP tasks, such as bilingual word alignment (Moore et al., 2007), constituency and dependency parsing (Taskar et al., 2004b; Koo et al., 2007), sentence compression (Cohn and Lapata, 2009) and document summarization (Li et al., 2009). Nevertheless, as a learning method for NLP, the SSVM algorithm has been less than popular algorithms suc</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D Lewis</author>
<author>Y Yang</author>
<author>T Rose</author>
<author>F Li</author>
</authors>
<title>RCV1: A new benchmark collection for text categorization research.</title>
<date>2004</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>5--361</pages>
<contexts>
<context position="24604" citStr="Lewis et al., 2004" startWordPosition="4163" endWordPosition="4166">pora annotated with many types of entities. We followed the settings in (Ratinov and Roth, 2009) and consider three main entities categories: PER, LOC and ORG. We evaluated the results using phrase-level F1. retic analysis significantly. Also note that Theorem 2 shows that if we put all possible structures in the working sets (i.e., F(α) = FS(α)), then the DCD algorithms can obtain e-optimal solution in O(log(E )) iterations. NER-CoNLL This is the English dataset from the CoNLL 2003 shared task (T. K. Sang and De Meulder, 2003). The data set labels sentences from the Reuters Corpus, Volume 1 (Lewis et al., 2004) with four different entity types: PER, LOC, ORG and MISC. We evaluated the results using phrase-level F1. POS-WSJ The standard set for evaluating the performance of a part-of-speech tagger. The training, development and test sets consist of sections 0-18, 19-21 and 22-24 of the Penn Treebank data (Marcus et al., 1993), respectively. We evaluated the results by token-level accuracy. DP-WSJ We took sections 02-21 of Penn Treebank as the training set, section 00 as the development set and section 23 as the test set. We implement a simple version of hash kernel to speed up of training procedure f</context>
</contexts>
<marker>Lewis, Yang, Rose, Li, 2004</marker>
<rawString>D. D. Lewis, Y. Yang, T. Rose, and F. Li. 2004. RCV1: A new benchmark collection for text categorization research. Journal of Machine Learning Research, 5:361–397.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Li</author>
<author>K Zhou</author>
<author>G-R Xue</author>
<author>H Zha</author>
<author>Y Yu</author>
</authors>
<title>Enhancing diversity, coverage and balance for summarization through structure learning.</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th international conference on World wide web, The International World Wide Web Conference,</booktitle>
<pages>71--80</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="2089" citStr="Li et al., 2009" startWordPosition="297" endWordPosition="300">ion algorithms have been shown to outperform significantly simple binary or multi-class classifiers (Lafferty et al., 2001; Collins, 2002; McDonald et al., 2005). Among the existing structured output prediction algorithms, the linear Structural Support Vector Machine (SSVM) algorithm (Tsochantaridis et al., 2004; Joachims et al., 2009) has shown outstanding performance in several NLP tasks, such as bilingual word alignment (Moore et al., 2007), constituency and dependency parsing (Taskar et al., 2004b; Koo et al., 2007), sentence compression (Cohn and Lapata, 2009) and document summarization (Li et al., 2009). Nevertheless, as a learning method for NLP, the SSVM algorithm has been less than popular algorithms such as the structured Perceptron (Collins, 2002). This may be due to the fact that current SSVM implementations often suffer from several practical issues. First, state-of-the-art implementations of SSVM such as cutting plane methods (Joachims et al., 2009) are typically complicated.1 Second, while methods like stochastic gradient descent are simple to implement, tuning learning rates can be difficult. Finally, while SSVM models can achieve superior accuracy, this often requires long trainin</context>
</contexts>
<marker>Li, Zhou, Xue, Zha, Yu, 2009</marker>
<rawString>L. Li, K. Zhou, G.-R. Xue, H. Zha, and Y. Yu. 2009. Enhancing diversity, coverage and balance for summarization through structure learning. In Proceedings of the 18th international conference on World wide web, The International World Wide Web Conference, pages 71–80, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z-Q Luo</author>
<author>P Tseng</author>
</authors>
<title>Error bounds and convergence analysis of feasible descent methods: A general approach.</title>
<date>1993</date>
<journal>Annals of Operations Research,</journal>
<volume>46</volume>
<issue>1</issue>
<pages>178</pages>
<contexts>
<context position="21662" citStr="Luo and Tseng, 1993" startWordPosition="3654" endWordPosition="3657">j = 1 ... r do 4: Shuffle the order of the training examples 5: for i = 1... l do 6: UPDATEWEIGHT(i, w) {Algo. 1} 7: end for 8: end for 9: Shuffle the order of the training examples 10: for i = 1... l do 11: Y¯ ← arg maxy wT Φ(Xi, Y) + Δ(Y, Yi) 12: if Δ(¯Y,Yi)−wTΦyi,¯y(Xi) then 13: Wi ← Wi ∪ {¯Y} 14: end if 15: UPDATEWEIGHT(i, w) {Algo. 1} 16: end for 17: end for ods. In this case, we denote the minimization problem Eq. (4) as F(α). For fixed working sets {Wi}, we denote FS(α) as the minimization problem that focuses on the dual variables in the working set only. By applying the results from (Luo and Tseng, 1993; Wang and Lin, 2013) to L2-Loss SSVM, we have the following theorem. Theorem 2. For any given non-empty working sets {Wi}, if the DCD algorithms do not extend the working sets (i.e., line 6-8 in Algorithm 2 are not executed), then the DCD algorithms will obtain the Eoptimal solution for FS(α) in O(log(1� )) iterations. Based on Theorem 1 and Theorem 2, we have the following theorem. Theorem 3. DCD-SSVM obtains an E-optimal solution in O(E log(1�)) iterations. To the best of our knowledge, this is the first convergence analysis result for L2-Loss SSVM. Compared to other theoretic analysis resu</context>
</contexts>
<marker>Luo, Tseng, 1993</marker>
<rawString>Z.-Q. Luo and P. Tseng. 1993. Error bounds and convergence analysis of feasible descent methods: A general approach. Annals of Operations Research, 46(1):157– 178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="24924" citStr="Marcus et al., 1993" startWordPosition="4215" endWordPosition="4218">ing sets (i.e., F(α) = FS(α)), then the DCD algorithms can obtain e-optimal solution in O(log(E )) iterations. NER-CoNLL This is the English dataset from the CoNLL 2003 shared task (T. K. Sang and De Meulder, 2003). The data set labels sentences from the Reuters Corpus, Volume 1 (Lewis et al., 2004) with four different entity types: PER, LOC, ORG and MISC. We evaluated the results using phrase-level F1. POS-WSJ The standard set for evaluating the performance of a part-of-speech tagger. The training, development and test sets consist of sections 0-18, 19-21 and 22-24 of the Penn Treebank data (Marcus et al., 1993), respectively. We evaluated the results by token-level accuracy. DP-WSJ We took sections 02-21 of Penn Treebank as the training set, section 00 as the development set and section 23 as the test set. We implement a simple version of hash kernel to speed up of training procedure for this task (Bohnet, 2010). We reported the unlabeled attachment accuracy for this task (McDonald et al., 2005). 213 80 79 Test F1 78 77 76 0 10 20 30 Test F1 86 84 82 80 0 20 40 60 80 100 120 Test Acc 97 96 95 DCD-SSVM FW-Struct SVM-Struct 0 200 400 600 Training Time (seconds) (a) Test F1 vs. Time in NER-MUC7 Trainin</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. P. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A F Martins</author>
<author>K Gimpel</author>
<author>N A Smith</author>
<author>E P Xing</author>
<author>M A Figueiredo</author>
<author>P M Aguiar</author>
</authors>
<title>Learning structured classifiers with dual coordinate ascent.</title>
<date>2010</date>
<tech>Technical report, Technical report CMU-ML-10-109.</tech>
<contexts>
<context position="10351" citStr="Martins et al., 2010" startWordPosition="1648" endWordPosition="1651">t requires picking a violation pair for each update. In contrast, because each dual variable can be updated independently in our DCD algorithm, the implementation is relatively simple. The extragradient algorithm has also been applied to solving the L1-Loss SSVM (Taskar et al., 2005). Unlike our DCD algorithm, the extragradient method requires the learning rate to be specified. The connections between dual methods and the online algorithms have been previously discussed. Specifically, Shalev-Shwartz and Singer (2006) connects the dual methods to a wide range of online learning algorithms. In (Martins et al., 2010), the authors apply similar techniques on L1-Loss SSVMs and show that the proposed algorithm can be faster than the SGD algorithm. Exponentiated Gradient (EG) descent (Kivinen and Warmuth, 1995; Collins et al., 2008) has recently been applied to solving the L1-Loss SSVM. Compared to other SSVM learners, EG requires manual tuning of the step size. In addition, EG requires solution of the sum-product inference problem, which can be more expensive than solving Eq. (3) (Taskar et al., 2006). Very recently, LacosteJulien et al. (2013) proposed a block-coordinate descent algorithm for the L1-Loss SS</context>
</contexts>
<marker>Martins, Gimpel, Smith, Xing, Figueiredo, Aguiar, 2010</marker>
<rawString>A. F. Martins, K. Gimpel, N. A. Smith, E. P. Xing, M. A. Figueiredo, and P. M. Aguiar. 2010. Learning structured classifiers with dual coordinate ascent. Technical report, Technical report CMU-ML-10-109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>91--98</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="1634" citStr="McDonald et al., 2005" startWordPosition="231" endWordPosition="234">hmark NLP datasets for part-ofspeech tagging, named-entity recognition and dependency parsing. 1 Introduction Complex natural language processing tasks are inherently structured. From sequence labeling problems like part-of-speech tagging and named entity recognition to tree construction tasks like syntactic parsing, strong dependencies exist among the labels of individual components. By modeling such relations in the output space, structured output prediction algorithms have been shown to outperform significantly simple binary or multi-class classifiers (Lafferty et al., 2001; Collins, 2002; McDonald et al., 2005). Among the existing structured output prediction algorithms, the linear Structural Support Vector Machine (SSVM) algorithm (Tsochantaridis et al., 2004; Joachims et al., 2009) has shown outstanding performance in several NLP tasks, such as bilingual word alignment (Moore et al., 2007), constituency and dependency parsing (Taskar et al., 2004b; Koo et al., 2007), sentence compression (Cohn and Lapata, 2009) and document summarization (Li et al., 2009). Nevertheless, as a learning method for NLP, the SSVM algorithm has been less than popular algorithms such as the structured Perceptron (Collins</context>
<context position="7276" citStr="McDonald et al., 2005" startWordPosition="1124" endWordPosition="1127">RA The Margin Infused Relaxed Algorithm (MIRA), which was introduced by Crammer et al. (2005), explicitly uses the notion of margin to update the weight vector. The MIRA updates the weight vector by calculating the step size using 211w − w0112 1 S.T. wT ΦYi,Y(xi) &gt; Δ(y, yi), by E Wk, where Wk is a set containing the best-k structures according to the weight vector w0. MIRA is a very popular method in the NLP community and has been applied to NLP tasks like word segmentation and part-of-speech tagging (Kruengkrai et al., 2009), NER and chunking (Mejer and Crammer, 2010) and dependency parsing (McDonald et al., 2005). 2.2 Structural SVM Structural SVM (SSVM) is a maximum margin model for the structured output prediction setting. Training SSVM is equivalent to solving the following global optimization problem: min W arg max wTΦ(xi, y). (1) min 11w112 �l L(xi, yi, w), (2) Y∈Y(Xi) W 2 + C i=1 208 where l is the number of labeled examples and L(xi, yi, w) = e (max [A(yi, y) − w TΦ Yxi,Y(xi)1 The typical choice of e is e(a) = at. If t = 2 is used, we refer to the SSVM defined in Eq. (2) as the L2- Loss SSVM. If hinge loss (t = 1) is used in Eq. (2), we refer to it as the L1-Loss SSVM. Note that the function A </context>
<context position="25316" citStr="McDonald et al., 2005" startWordPosition="4283" endWordPosition="4286">phrase-level F1. POS-WSJ The standard set for evaluating the performance of a part-of-speech tagger. The training, development and test sets consist of sections 0-18, 19-21 and 22-24 of the Penn Treebank data (Marcus et al., 1993), respectively. We evaluated the results by token-level accuracy. DP-WSJ We took sections 02-21 of Penn Treebank as the training set, section 00 as the development set and section 23 as the test set. We implement a simple version of hash kernel to speed up of training procedure for this task (Bohnet, 2010). We reported the unlabeled attachment accuracy for this task (McDonald et al., 2005). 213 80 79 Test F1 78 77 76 0 10 20 30 Test F1 86 84 82 80 0 20 40 60 80 100 120 Test Acc 97 96 95 DCD-SSVM FW-Struct SVM-Struct 0 200 400 600 Training Time (seconds) (a) Test F1 vs. Time in NER-MUC7 Training Time (seconds) (b) Test F1 vs. Time in NER-CoNLL Training Time (seconds) (c) Test Acc vs. Time in POS-WSJ Figure 2: Comparisons between the testing performance of DCD-SSVM, FW-Struct and SVM-Struct. Note that DCD-SSVM often obtain a better model with much less training time when comparing to SVM-Struct. 4.2 Features and Inference Algorithms For the sequence labeling tasks, NER and POS, w</context>
<context position="26710" citStr="McDonald et al., 2005" startWordPosition="4532" endWordPosition="4535"> where demi is the feature vector dedicated to the i-th token (or, the emission features), N represents the number of tokens in this sequence, yi represents the i-th token in the sequence y, [yi = 1] is the indictor variable and k is the number of tags. The inference problems are solved by the Viterbi algorithm. The emission features used in both POS and NER are the standard ones, including word features, word-shape features, etc. For NER, we used additional simple gazetteer features7 and word cluster features (Turian et al., 2010) For dependency parsing, we followed the setting described in (McDonald et al., 2005) and used similar features. The decoding algorithm is the firstorder Eisner’s algorithm (Eisner, 1997). 4.3 Algorithms and Implementation Detail For all SSVM algorithms (including SGD), C was chosen among the set 10.01, 0.05,0.1,0.5, 1, 5} according to the accuracy/F1 on the development set. For each task, the same features were used by all 7Adding Wikipedia gazetteers would likely increase the performance significantly (Ratinov and Roth, 2009). algorithms. For NER-MUC7, NER-CoNLL and POS-WSJ, we ran the online algorithms and DCDSSVM for 25 iterations. For DP-WSJ, we only let the algorithms ru</context>
<context position="28699" citStr="McDonald et al., 2005" startWordPosition="4855" endWordPosition="4858">on. Perceptron This refers to the averaged structured Perceptron method introduced by Collins (2002). To speed up the convergence rate, we shuffle the training examples at each iteration. MIRA Margin Infused Relaxed Algorithm (MIRA) (Crammer et al., 2005) is the online learning algorithm that explicitly uses the notion of margin to update the weight vector. We use 1-best MIRA in our experiments. To increase 8http://www.cs.cornell.edu/People/tj/ svm_light/svm_hmm.html ⎡ ⎢ ⎢ ⎢ ⎢ ⎣ d(X, y) = N i=l , ⎤ ⎦⎥⎥⎥⎥ 214 the convergence speed, we shuffle the training examples at each iteration. Following (McDonald et al., 2005), we did not tune the C parameter for the MIRA algorithm. SGD Stochastic gradient descent (SGD) (Bottou, 2004) is a technique for optimizing general convex functions. In this paper, we use SGD as an alternative baseline for optimizing the L1-Loss SSVM objective function (Eq. (2) with higne loss).9 When using SGD, the learning rate must be carefully tuned. Following (Bottou, 2004), the learning rate is obtained by η0 (1.0 + (η0T/C))0.75, where C is the regularization parameter, T is the number of updates so far and η0 is the initial step size. The parameter η0 was selected among the set {2−1,2−</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>R. McDonald, K. Crammer, and F. Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 91–98, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mejer</author>
<author>K Crammer</author>
</authors>
<title>Confidence in structured-prediction using confidence-weighted models.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference on Empirical Methods for Natural Language Processing (EMNLP),</booktitle>
<pages>971--981</pages>
<contexts>
<context position="7229" citStr="Mejer and Crammer, 2010" startWordPosition="1117" endWordPosition="1120">prove the generalization ability of the model. MIRA The Margin Infused Relaxed Algorithm (MIRA), which was introduced by Crammer et al. (2005), explicitly uses the notion of margin to update the weight vector. The MIRA updates the weight vector by calculating the step size using 211w − w0112 1 S.T. wT ΦYi,Y(xi) &gt; Δ(y, yi), by E Wk, where Wk is a set containing the best-k structures according to the weight vector w0. MIRA is a very popular method in the NLP community and has been applied to NLP tasks like word segmentation and part-of-speech tagging (Kruengkrai et al., 2009), NER and chunking (Mejer and Crammer, 2010) and dependency parsing (McDonald et al., 2005). 2.2 Structural SVM Structural SVM (SSVM) is a maximum margin model for the structured output prediction setting. Training SSVM is equivalent to solving the following global optimization problem: min W arg max wTΦ(xi, y). (1) min 11w112 �l L(xi, yi, w), (2) Y∈Y(Xi) W 2 + C i=1 208 where l is the number of labeled examples and L(xi, yi, w) = e (max [A(yi, y) − w TΦ Yxi,Y(xi)1 The typical choice of e is e(a) = at. If t = 2 is used, we refer to the SSVM defined in Eq. (2) as the L2- Loss SSVM. If hinge loss (t = 1) is used in Eq. (2), we refer to it</context>
</contexts>
<marker>Mejer, Crammer, 2010</marker>
<rawString>A. Mejer and K. Crammer. 2010. Confidence in structured-prediction using confidence-weighted models. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference on Empirical Methods for Natural Language Processing (EMNLP), pages 971– 981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Moore</author>
<author>W Yih</author>
<author>A Bode</author>
</authors>
<title>Improved discriminative bilingual word alignment.</title>
<date>2007</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="1920" citStr="Moore et al., 2007" startWordPosition="271" endWordPosition="274">tasks like syntactic parsing, strong dependencies exist among the labels of individual components. By modeling such relations in the output space, structured output prediction algorithms have been shown to outperform significantly simple binary or multi-class classifiers (Lafferty et al., 2001; Collins, 2002; McDonald et al., 2005). Among the existing structured output prediction algorithms, the linear Structural Support Vector Machine (SSVM) algorithm (Tsochantaridis et al., 2004; Joachims et al., 2009) has shown outstanding performance in several NLP tasks, such as bilingual word alignment (Moore et al., 2007), constituency and dependency parsing (Taskar et al., 2004b; Koo et al., 2007), sentence compression (Cohn and Lapata, 2009) and document summarization (Li et al., 2009). Nevertheless, as a learning method for NLP, the SSVM algorithm has been less than popular algorithms such as the structured Perceptron (Collins, 2002). This may be due to the fact that current SSVM implementations often suffer from several practical issues. First, state-of-the-art implementations of SSVM such as cutting plane methods (Joachims et al., 2009) are typically complicated.1 Second, while methods like stochastic gra</context>
</contexts>
<marker>Moore, Yih, Bode, 2007</marker>
<rawString>R. C. Moore, W. Yih, and A. Bode. 2007. Improved discriminative bilingual word alignment. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ratinov</author>
<author>D Roth</author>
</authors>
<title>Design challenges and misconceptions in named entity recognition.</title>
<date>2009</date>
<booktitle>In Proc. of the Annual Conference on Computational Natural Language Learning (CoNLL),</booktitle>
<contexts>
<context position="24081" citStr="Ratinov and Roth, 2009" startWordPosition="4074" endWordPosition="4077">verify the effectiveness of the proposed algorithm, we conduct a set of experiments on different optimization and learning algorithms. Before going to the experimental results, we first introduce the tasks and settings used in the experiments. 4.1 Tasks and Data We evaluated our method and existing structured output learning approaches on named entity recognition (NER), part-of-speech tagging (POS) and dependency parsing (DP) on four benchmark datasets. NER-MUC7 MUC-7 data contains a subset of North American News Text Corpora annotated with many types of entities. We followed the settings in (Ratinov and Roth, 2009) and consider three main entities categories: PER, LOC and ORG. We evaluated the results using phrase-level F1. retic analysis significantly. Also note that Theorem 2 shows that if we put all possible structures in the working sets (i.e., F(α) = FS(α)), then the DCD algorithms can obtain e-optimal solution in O(log(E )) iterations. NER-CoNLL This is the English dataset from the CoNLL 2003 shared task (T. K. Sang and De Meulder, 2003). The data set labels sentences from the Reuters Corpus, Volume 1 (Lewis et al., 2004) with four different entity types: PER, LOC, ORG and MISC. We evaluated the r</context>
<context position="27158" citStr="Ratinov and Roth, 2009" startWordPosition="4602" endWordPosition="4605">used additional simple gazetteer features7 and word cluster features (Turian et al., 2010) For dependency parsing, we followed the setting described in (McDonald et al., 2005) and used similar features. The decoding algorithm is the firstorder Eisner’s algorithm (Eisner, 1997). 4.3 Algorithms and Implementation Detail For all SSVM algorithms (including SGD), C was chosen among the set 10.01, 0.05,0.1,0.5, 1, 5} according to the accuracy/F1 on the development set. For each task, the same features were used by all 7Adding Wikipedia gazetteers would likely increase the performance significantly (Ratinov and Roth, 2009). algorithms. For NER-MUC7, NER-CoNLL and POS-WSJ, we ran the online algorithms and DCDSSVM for 25 iterations. For DP-WSJ, we only let the algorithms run for 10 iterations as the inference procedure is very expensive computationally. The algorithms in the experiments are: DCD Our dual coordinate descent method on the L2-Loss SSVM. For DCD-SSVM, r is set to be 5. For both DCD-Light and DCD-SSVM , we follow the suggestion in (Joachims et al., 2009): if the value of a dual variable becomes zero, its corresponding structure will be removed from the working set to improve the speed. SVM-Struct We u</context>
</contexts>
<marker>Ratinov, Roth, 2009</marker>
<rawString>L. Ratinov and D. Roth. 2009. Design challenges and misconceptions in named entity recognition. In Proc. of the Annual Conference on Computational Natural Language Learning (CoNLL), Jun.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ratliff</author>
<author>J Andrew Bagnell</author>
<author>M Zinkevich</author>
</authors>
<title>(Online) subgradient methods for structured prediction.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Workshop on Artificial Intelligence and Statistics,</booktitle>
<contexts>
<context position="9597" citStr="Ratliff et al., 2007" startWordPosition="1533" endWordPosition="1536">ble formulation is much faster than solving the l-slack variable one (Eq. (2)). Chang et al. (2010) also proposed a variant of cutting plane method for solving the L2-Loss SSVM. This method uses a dual coordinate descent algorithm to solve the sub-problems. We call their approach the CPD method. Several other algorithms also aim at solving the L1-Loss SSVM. Stochastic gradient descent (SGD) (Bottou, 2004; Shalev-Shwartz et al., 2007) is a technique for optimizing general convex functions and has been applied to solving the 2Without Δ(y, yi) in Eq. 2, the optimal w would be zero. L1-Loss SSVM (Ratliff et al., 2007). Taskar et al. (2004a) proposed a structured SMO algorithm. Because the algorithm solves the dual formulation of the L1-Loss SSVM, it requires picking a violation pair for each update. In contrast, because each dual variable can be updated independently in our DCD algorithm, the implementation is relatively simple. The extragradient algorithm has also been applied to solving the L1-Loss SSVM (Taskar et al., 2005). Unlike our DCD algorithm, the extragradient method requires the learning rate to be specified. The connections between dual methods and the online algorithms have been previously di</context>
</contexts>
<marker>Ratliff, Bagnell, Zinkevich, 2007</marker>
<rawString>N. Ratliff, J. Andrew (Drew) Bagnell, and M. Zinkevich. 2007. (Online) subgradient methods for structured prediction. In Proceedings of the International Workshop on Artificial Intelligence and Statistics, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Shalev-Shwartz</author>
<author>Y Singer</author>
</authors>
<title>Online learning meets optimization in the dual.</title>
<date>2006</date>
<booktitle>In Proceedings of the Annual ACM Workshop on Computational Learning Theory (COLT).</booktitle>
<contexts>
<context position="10252" citStr="Shalev-Shwartz and Singer (2006)" startWordPosition="1630" endWordPosition="1633"> proposed a structured SMO algorithm. Because the algorithm solves the dual formulation of the L1-Loss SSVM, it requires picking a violation pair for each update. In contrast, because each dual variable can be updated independently in our DCD algorithm, the implementation is relatively simple. The extragradient algorithm has also been applied to solving the L1-Loss SSVM (Taskar et al., 2005). Unlike our DCD algorithm, the extragradient method requires the learning rate to be specified. The connections between dual methods and the online algorithms have been previously discussed. Specifically, Shalev-Shwartz and Singer (2006) connects the dual methods to a wide range of online learning algorithms. In (Martins et al., 2010), the authors apply similar techniques on L1-Loss SSVMs and show that the proposed algorithm can be faster than the SGD algorithm. Exponentiated Gradient (EG) descent (Kivinen and Warmuth, 1995; Collins et al., 2008) has recently been applied to solving the L1-Loss SSVM. Compared to other SSVM learners, EG requires manual tuning of the step size. In addition, EG requires solution of the sum-product inference problem, which can be more expensive than solving Eq. (3) (Taskar et al., 2006). Very rec</context>
</contexts>
<marker>Shalev-Shwartz, Singer, 2006</marker>
<rawString>S. Shalev-Shwartz and Y. Singer. 2006. Online learning meets optimization in the dual. In Proceedings of the Annual ACM Workshop on Computational Learning Theory (COLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Shalev-Shwartz</author>
<author>T Zhang</author>
</authors>
<title>Stochastic dual coordinate ascent methods for regularized loss minimization.</title>
<date>2013</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>14--567</pages>
<marker>Shalev-Shwartz, Zhang, 2013</marker>
<rawString>S. Shalev-Shwartz and T. Zhang. 2013. Stochastic dual coordinate ascent methods for regularized loss minimization. Journal of Machine Learning Research, 14:567–599.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Shalev-Shwartz</author>
<author>Y Singer</author>
<author>N Srebro</author>
</authors>
<title>Pegasos: primal estimated sub-gradient solver for SVM.</title>
<date>2007</date>
<booktitle>Proceedings of the International Conference on Machine Learning (ICML),</booktitle>
<pages>807--814</pages>
<editor>In Zoubin Ghahramani, editor,</editor>
<publisher>Omnipress.</publisher>
<contexts>
<context position="9413" citStr="Shalev-Shwartz et al., 2007" startWordPosition="1499" endWordPosition="1502">el. CP has proven useful for solving SSVMs. For instance, Yu and Joachims (2009) proposed using CP methods to solve a 1-slack variable formulation, and showed that solving for a 1-slack variable formulation is much faster than solving the l-slack variable one (Eq. (2)). Chang et al. (2010) also proposed a variant of cutting plane method for solving the L2-Loss SSVM. This method uses a dual coordinate descent algorithm to solve the sub-problems. We call their approach the CPD method. Several other algorithms also aim at solving the L1-Loss SSVM. Stochastic gradient descent (SGD) (Bottou, 2004; Shalev-Shwartz et al., 2007) is a technique for optimizing general convex functions and has been applied to solving the 2Without Δ(y, yi) in Eq. 2, the optimal w would be zero. L1-Loss SSVM (Ratliff et al., 2007). Taskar et al. (2004a) proposed a structured SMO algorithm. Because the algorithm solves the dual formulation of the L1-Loss SSVM, it requires picking a violation pair for each update. In contrast, because each dual variable can be updated independently in our DCD algorithm, the implementation is relatively simple. The extragradient algorithm has also been applied to solving the L1-Loss SSVM (Taskar et al., 2005</context>
</contexts>
<marker>Shalev-Shwartz, Singer, Srebro, 2007</marker>
<rawString>S. Shalev-Shwartz, Y. Singer, and N. Srebro. 2007. Pegasos: primal estimated sub-gradient solver for SVM. In Zoubin Ghahramani, editor, Proceedings of the International Conference on Machine Learning (ICML), pages 807–814. Omnipress.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Shevade</author>
<author>P Balamurugan</author>
<author>S Sundararajan</author>
<author>S Keerthi</author>
</authors>
<title>A sequential dual method for structural SVMs.</title>
<date>2011</date>
<booktitle>In IEEE International Conference on Data Mining(ICDM).</booktitle>
<contexts>
<context position="11228" citStr="Shevade et al., 2011" startWordPosition="1789" endWordPosition="1792">e L1-Loss SSVM. Compared to other SSVM learners, EG requires manual tuning of the step size. In addition, EG requires solution of the sum-product inference problem, which can be more expensive than solving Eq. (3) (Taskar et al., 2006). Very recently, LacosteJulien et al. (2013) proposed a block-coordinate descent algorithm for the L1-Loss SSVM based on the Frank-Wolfe algorithm (FW-Struct), which has been shown to outperform the EG algorithm significantly. Similar to our DCD algorithm, FW calculates the optimal learning rate when updating the dual variables. The Sequential Dual Method (SDM) (Shevade et al., 2011) is probably the most related to this paper. SDM solves the L1-Loss SSVM problem using multiple updating policies, which is similar to our approach. However, there are several important differences in the detailed algorithmic design. As will be clear in Sec. 3, our dual coordinate descent (DCD) algorithm is very simple, while SDM (which is not a DCD algorithm) uses a complicated procedure to balance different update policies. By targeting the L2-Loss SSVM formulation, our methods can update the weight vector more efficiently, since there are no equality constraints in the dual. 209 3 Dual Coor</context>
</contexts>
<marker>Shevade, Balamurugan, Sundararajan, Keerthi, 2011</marker>
<rawString>S. Shevade, P. Balamurugan, S. Sundararajan, and S. Keerthi. 2011. A sequential dual method for structural SVMs. In IEEE International Conference on Data Mining(ICDM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F T K Sang</author>
<author>F De Meulder</author>
</authors>
<title>Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition.</title>
<date>2003</date>
<booktitle>In Walter Daelemans</booktitle>
<pages>142--147</pages>
<editor>and Miles Osborne, editors,</editor>
<location>Edmonton, Canada.</location>
<marker>Sang, De Meulder, 2003</marker>
<rawString>E. F. T. K. Sang and F. De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. In Walter Daelemans and Miles Osborne, editors, Proceedings of CoNLL-2003, pages 142–147. Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>C Guestrin</author>
<author>D Koller</author>
</authors>
<title>Maxmargin markov networks.</title>
<date>2004</date>
<booktitle>In The Conference on Advances in Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="1978" citStr="Taskar et al., 2004" startWordPosition="279" endWordPosition="282">ong the labels of individual components. By modeling such relations in the output space, structured output prediction algorithms have been shown to outperform significantly simple binary or multi-class classifiers (Lafferty et al., 2001; Collins, 2002; McDonald et al., 2005). Among the existing structured output prediction algorithms, the linear Structural Support Vector Machine (SSVM) algorithm (Tsochantaridis et al., 2004; Joachims et al., 2009) has shown outstanding performance in several NLP tasks, such as bilingual word alignment (Moore et al., 2007), constituency and dependency parsing (Taskar et al., 2004b; Koo et al., 2007), sentence compression (Cohn and Lapata, 2009) and document summarization (Li et al., 2009). Nevertheless, as a learning method for NLP, the SSVM algorithm has been less than popular algorithms such as the structured Perceptron (Collins, 2002). This may be due to the fact that current SSVM implementations often suffer from several practical issues. First, state-of-the-art implementations of SSVM such as cutting plane methods (Joachims et al., 2009) are typically complicated.1 Second, while methods like stochastic gradient descent are simple to implement, tuning learning rat</context>
<context position="9618" citStr="Taskar et al. (2004" startWordPosition="1537" endWordPosition="1540"> faster than solving the l-slack variable one (Eq. (2)). Chang et al. (2010) also proposed a variant of cutting plane method for solving the L2-Loss SSVM. This method uses a dual coordinate descent algorithm to solve the sub-problems. We call their approach the CPD method. Several other algorithms also aim at solving the L1-Loss SSVM. Stochastic gradient descent (SGD) (Bottou, 2004; Shalev-Shwartz et al., 2007) is a technique for optimizing general convex functions and has been applied to solving the 2Without Δ(y, yi) in Eq. 2, the optimal w would be zero. L1-Loss SSVM (Ratliff et al., 2007). Taskar et al. (2004a) proposed a structured SMO algorithm. Because the algorithm solves the dual formulation of the L1-Loss SSVM, it requires picking a violation pair for each update. In contrast, because each dual variable can be updated independently in our DCD algorithm, the implementation is relatively simple. The extragradient algorithm has also been applied to solving the L1-Loss SSVM (Taskar et al., 2005). Unlike our DCD algorithm, the extragradient method requires the learning rate to be specified. The connections between dual methods and the online algorithms have been previously discussed. Specifically</context>
</contexts>
<marker>Taskar, Guestrin, Koller, 2004</marker>
<rawString>B. Taskar, C. Guestrin, and D. Koller. 2004a. Maxmargin markov networks. In The Conference on Advances in Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>D Klein</author>
<author>M Collins</author>
<author>D Koller</author>
<author>C Manning</author>
</authors>
<title>Max-margin parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Empirical Methods for Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="1978" citStr="Taskar et al., 2004" startWordPosition="279" endWordPosition="282">ong the labels of individual components. By modeling such relations in the output space, structured output prediction algorithms have been shown to outperform significantly simple binary or multi-class classifiers (Lafferty et al., 2001; Collins, 2002; McDonald et al., 2005). Among the existing structured output prediction algorithms, the linear Structural Support Vector Machine (SSVM) algorithm (Tsochantaridis et al., 2004; Joachims et al., 2009) has shown outstanding performance in several NLP tasks, such as bilingual word alignment (Moore et al., 2007), constituency and dependency parsing (Taskar et al., 2004b; Koo et al., 2007), sentence compression (Cohn and Lapata, 2009) and document summarization (Li et al., 2009). Nevertheless, as a learning method for NLP, the SSVM algorithm has been less than popular algorithms such as the structured Perceptron (Collins, 2002). This may be due to the fact that current SSVM implementations often suffer from several practical issues. First, state-of-the-art implementations of SSVM such as cutting plane methods (Joachims et al., 2009) are typically complicated.1 Second, while methods like stochastic gradient descent are simple to implement, tuning learning rat</context>
<context position="9618" citStr="Taskar et al. (2004" startWordPosition="1537" endWordPosition="1540"> faster than solving the l-slack variable one (Eq. (2)). Chang et al. (2010) also proposed a variant of cutting plane method for solving the L2-Loss SSVM. This method uses a dual coordinate descent algorithm to solve the sub-problems. We call their approach the CPD method. Several other algorithms also aim at solving the L1-Loss SSVM. Stochastic gradient descent (SGD) (Bottou, 2004; Shalev-Shwartz et al., 2007) is a technique for optimizing general convex functions and has been applied to solving the 2Without Δ(y, yi) in Eq. 2, the optimal w would be zero. L1-Loss SSVM (Ratliff et al., 2007). Taskar et al. (2004a) proposed a structured SMO algorithm. Because the algorithm solves the dual formulation of the L1-Loss SSVM, it requires picking a violation pair for each update. In contrast, because each dual variable can be updated independently in our DCD algorithm, the implementation is relatively simple. The extragradient algorithm has also been applied to solving the L1-Loss SSVM (Taskar et al., 2005). Unlike our DCD algorithm, the extragradient method requires the learning rate to be specified. The connections between dual methods and the online algorithms have been previously discussed. Specifically</context>
</contexts>
<marker>Taskar, Klein, Collins, Koller, Manning, 2004</marker>
<rawString>B. Taskar, D. Klein, M. Collins, D. Koller, and C. Manning. 2004b. Max-margin parsing. In Proceedings of the Conference on Empirical Methods for Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>S Lacoste-julien</author>
<author>M I Jordan</author>
</authors>
<title>Structured prediction via the extragradient method.</title>
<date>2005</date>
<booktitle>In The Conference on Advances in Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="10014" citStr="Taskar et al., 2005" startWordPosition="1598" endWordPosition="1601">wartz et al., 2007) is a technique for optimizing general convex functions and has been applied to solving the 2Without Δ(y, yi) in Eq. 2, the optimal w would be zero. L1-Loss SSVM (Ratliff et al., 2007). Taskar et al. (2004a) proposed a structured SMO algorithm. Because the algorithm solves the dual formulation of the L1-Loss SSVM, it requires picking a violation pair for each update. In contrast, because each dual variable can be updated independently in our DCD algorithm, the implementation is relatively simple. The extragradient algorithm has also been applied to solving the L1-Loss SSVM (Taskar et al., 2005). Unlike our DCD algorithm, the extragradient method requires the learning rate to be specified. The connections between dual methods and the online algorithms have been previously discussed. Specifically, Shalev-Shwartz and Singer (2006) connects the dual methods to a wide range of online learning algorithms. In (Martins et al., 2010), the authors apply similar techniques on L1-Loss SSVMs and show that the proposed algorithm can be faster than the SGD algorithm. Exponentiated Gradient (EG) descent (Kivinen and Warmuth, 1995; Collins et al., 2008) has recently been applied to solving the L1-Lo</context>
</contexts>
<marker>Taskar, Lacoste-julien, Jordan, 2005</marker>
<rawString>B. Taskar, S. Lacoste-julien, and M. I. Jordan. 2005. Structured prediction via the extragradient method. In The Conference on Advances in Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>S Lacoste-Julien</author>
<author>M I Jordan</author>
</authors>
<title>Structured prediction, dual extragradient and bregman projections.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>7--1627</pages>
<contexts>
<context position="10842" citStr="Taskar et al., 2006" startWordPosition="1730" endWordPosition="1733">alev-Shwartz and Singer (2006) connects the dual methods to a wide range of online learning algorithms. In (Martins et al., 2010), the authors apply similar techniques on L1-Loss SSVMs and show that the proposed algorithm can be faster than the SGD algorithm. Exponentiated Gradient (EG) descent (Kivinen and Warmuth, 1995; Collins et al., 2008) has recently been applied to solving the L1-Loss SSVM. Compared to other SSVM learners, EG requires manual tuning of the step size. In addition, EG requires solution of the sum-product inference problem, which can be more expensive than solving Eq. (3) (Taskar et al., 2006). Very recently, LacosteJulien et al. (2013) proposed a block-coordinate descent algorithm for the L1-Loss SSVM based on the Frank-Wolfe algorithm (FW-Struct), which has been shown to outperform the EG algorithm significantly. Similar to our DCD algorithm, FW calculates the optimal learning rate when updating the dual variables. The Sequential Dual Method (SDM) (Shevade et al., 2011) is probably the most related to this paper. SDM solves the L1-Loss SSVM problem using multiple updating policies, which is similar to our approach. However, there are several important differences in the detailed </context>
</contexts>
<marker>Taskar, Lacoste-Julien, Jordan, 2006</marker>
<rawString>B. Taskar, S. Lacoste-Julien, and M. I Jordan. 2006. Structured prediction, dual extragradient and bregman projections. Journal of Machine Learning Research, 7:1627–1653.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Tsochantaridis</author>
<author>T Hofmann</author>
<author>T Joachims</author>
<author>Y Altun</author>
</authors>
<title>Support vector machine learning for interdependent and structured output spaces.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="1786" citStr="Tsochantaridis et al., 2004" startWordPosition="250" endWordPosition="253">asks are inherently structured. From sequence labeling problems like part-of-speech tagging and named entity recognition to tree construction tasks like syntactic parsing, strong dependencies exist among the labels of individual components. By modeling such relations in the output space, structured output prediction algorithms have been shown to outperform significantly simple binary or multi-class classifiers (Lafferty et al., 2001; Collins, 2002; McDonald et al., 2005). Among the existing structured output prediction algorithms, the linear Structural Support Vector Machine (SSVM) algorithm (Tsochantaridis et al., 2004; Joachims et al., 2009) has shown outstanding performance in several NLP tasks, such as bilingual word alignment (Moore et al., 2007), constituency and dependency parsing (Taskar et al., 2004b; Koo et al., 2007), sentence compression (Cohn and Lapata, 2009) and document summarization (Li et al., 2009). Nevertheless, as a learning method for NLP, the SSVM algorithm has been less than popular algorithms such as the structured Perceptron (Collins, 2002). This may be due to the fact that current SSVM implementations often suffer from several practical issues. First, state-of-the-art implementatio</context>
<context position="8469" citStr="Tsochantaridis et al., 2004" startWordPosition="1349" endWordPosition="1352">ss SSVM. Note that the function A is not only necessary,2 but also enables us to use more information on the differences between the structures in the training phase. For example, using Hamming distance for sequence labeling is a reasonable choice, as the model can express finer distinctions between structures yi and y. When training an SSVM model, we often need to solve the loss-augmented inference problem, arg max 1wT Φ(xi, y) + A(yi, y)] . (3) Y∈Y(Xi) Note that it is a different inference problem than the decoding problem in Eq. (1). Algorithms for training SSVM Cutting plane (CP) methods (Tsochantaridis et al., 2004; Joachims et al., 2009) have been the dominant method for learning the L1-Loss SSVM. Eq. (2) contains an exponential number of constraints. The cutting plane (CP) methods iteratively select a subset of active constraints for each example then solve a sub-problem which contains active constraints to improve the model. CP has proven useful for solving SSVMs. For instance, Yu and Joachims (2009) proposed using CP methods to solve a 1-slack variable formulation, and showed that solving for a 1-slack variable formulation is much faster than solving the l-slack variable one (Eq. (2)). Chang et al. </context>
</contexts>
<marker>Tsochantaridis, Hofmann, Joachims, Altun, 2004</marker>
<rawString>I. Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun. 2004. Support vector machine learning for interdependent and structured output spaces. In Proceedings of the International Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Turian</author>
<author>L Ratinov</author>
<author>Y Bengio</author>
</authors>
<title>Word representations: a simple and general method for semisupervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>384--394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="26625" citStr="Turian et al., 2010" startWordPosition="4519" endWordPosition="4522">e features as demi(xi, yi) [yi = 1][yi−1 = 1] [yi = 1][yi−1 = 2] [yi = k][yi−1 = k] where demi is the feature vector dedicated to the i-th token (or, the emission features), N represents the number of tokens in this sequence, yi represents the i-th token in the sequence y, [yi = 1] is the indictor variable and k is the number of tags. The inference problems are solved by the Viterbi algorithm. The emission features used in both POS and NER are the standard ones, including word features, word-shape features, etc. For NER, we used additional simple gazetteer features7 and word cluster features (Turian et al., 2010) For dependency parsing, we followed the setting described in (McDonald et al., 2005) and used similar features. The decoding algorithm is the firstorder Eisner’s algorithm (Eisner, 1997). 4.3 Algorithms and Implementation Detail For all SSVM algorithms (including SGD), C was chosen among the set 10.01, 0.05,0.1,0.5, 1, 5} according to the accuracy/F1 on the development set. For each task, the same features were used by all 7Adding Wikipedia gazetteers would likely increase the performance significantly (Ratinov and Roth, 2009). algorithms. For NER-MUC7, NER-CoNLL and POS-WSJ, we ran the onlin</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>J. Turian, L. Ratinov, and Y. Bengio. 2010. Word representations: a simple and general method for semisupervised learning. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 384–394, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Po-Wei Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Iteration complexity of feasible descent methods for convex optimization.</title>
<date>2013</date>
<tech>Technical report,</tech>
<institution>National Taiwan University.</institution>
<contexts>
<context position="21683" citStr="Wang and Lin, 2013" startWordPosition="3658" endWordPosition="3661">ffle the order of the training examples 5: for i = 1... l do 6: UPDATEWEIGHT(i, w) {Algo. 1} 7: end for 8: end for 9: Shuffle the order of the training examples 10: for i = 1... l do 11: Y¯ ← arg maxy wT Φ(Xi, Y) + Δ(Y, Yi) 12: if Δ(¯Y,Yi)−wTΦyi,¯y(Xi) then 13: Wi ← Wi ∪ {¯Y} 14: end if 15: UPDATEWEIGHT(i, w) {Algo. 1} 16: end for 17: end for ods. In this case, we denote the minimization problem Eq. (4) as F(α). For fixed working sets {Wi}, we denote FS(α) as the minimization problem that focuses on the dual variables in the working set only. By applying the results from (Luo and Tseng, 1993; Wang and Lin, 2013) to L2-Loss SSVM, we have the following theorem. Theorem 2. For any given non-empty working sets {Wi}, if the DCD algorithms do not extend the working sets (i.e., line 6-8 in Algorithm 2 are not executed), then the DCD algorithms will obtain the Eoptimal solution for FS(α) in O(log(1� )) iterations. Based on Theorem 1 and Theorem 2, we have the following theorem. Theorem 3. DCD-SSVM obtains an E-optimal solution in O(E log(1�)) iterations. To the best of our knowledge, this is the first convergence analysis result for L2-Loss SSVM. Compared to other theoretic analysis results for L1-Loss SSVM,</context>
</contexts>
<marker>Wang, Lin, 2013</marker>
<rawString>Po-Wei Wang and Chih-Jen Lin. 2013. Iteration complexity of feasible descent methods for convex optimization. Technical report, National Taiwan University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Yu</author>
<author>T Joachims</author>
</authors>
<title>Learning structural SVMs with latent variables.</title>
<date>2009</date>
<booktitle>In Proceedings of the International Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="8865" citStr="Yu and Joachims (2009)" startWordPosition="1411" endWordPosition="1414">e problem, arg max 1wT Φ(xi, y) + A(yi, y)] . (3) Y∈Y(Xi) Note that it is a different inference problem than the decoding problem in Eq. (1). Algorithms for training SSVM Cutting plane (CP) methods (Tsochantaridis et al., 2004; Joachims et al., 2009) have been the dominant method for learning the L1-Loss SSVM. Eq. (2) contains an exponential number of constraints. The cutting plane (CP) methods iteratively select a subset of active constraints for each example then solve a sub-problem which contains active constraints to improve the model. CP has proven useful for solving SSVMs. For instance, Yu and Joachims (2009) proposed using CP methods to solve a 1-slack variable formulation, and showed that solving for a 1-slack variable formulation is much faster than solving the l-slack variable one (Eq. (2)). Chang et al. (2010) also proposed a variant of cutting plane method for solving the L2-Loss SSVM. This method uses a dual coordinate descent algorithm to solve the sub-problems. We call their approach the CPD method. Several other algorithms also aim at solving the L1-Loss SSVM. Stochastic gradient descent (SGD) (Bottou, 2004; Shalev-Shwartz et al., 2007) is a technique for optimizing general convex functi</context>
</contexts>
<marker>Yu, Joachims, 2009</marker>
<rawString>C. Yu and T. Joachims. 2009. Learning structural SVMs with latent variables. In Proceedings of the International Conference on Machine Learning (ICML).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>