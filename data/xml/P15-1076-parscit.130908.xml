<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.991298">
Trans-dimensional Random Fields for Language Modeling
</title>
<author confidence="0.99955">
Bin Wang1, Zhijian Ou1, Zhiqiang Tan2
</author>
<affiliation confidence="0.995706">
1Department of Electronic Engineering, Tsinghua University, Beijing 100084, China
2Department of Statistics, Rutgers University, Piscataway, NJ 08854, USA
</affiliation>
<email confidence="0.899926">
wangbin12@mails.tsinghua.edu.cn, ozj@tsinghua.edu.cn,
ztan@stat.rutgers.edu
</email>
<sectionHeader confidence="0.997358" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998298">
Language modeling (LM) involves
determining the joint probability of
words in a sentence. The conditional
approach is dominant, representing the
joint probability in terms of conditionals.
Examples include n-gram LMs and neural
network LMs. An alternative approach,
called the random field (RF) approach, is
used in whole-sentence maximum entropy
(WSME) LMs. Although the RF approach
has potential benefits, the empirical
results of previous WSME models are
not satisfactory. In this paper, we revisit
the RF approach for language modeling,
with a number of innovations. We
propose a trans-dimensional RF (TDRF)
model and develop a training algorithm
using joint stochastic approximation and
trans-dimensional mixture sampling. We
perform speech recognition experiments
on Wall Street Journal data, and find that
our TDRF models lead to performances as
good as the recurrent neural network LMs
but are computationally more efficient in
computing sentence probability.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.945345923076923">
Language modeling is crucial for a variety
of computational linguistic applications, such
as speech recognition, machine translation,
handwriting recognition, information retrieval and
so on. It involves determining the joint probability
p(x) of a sentence x, which can be denoted as
a pair x = (l, xl), where l is the length and
xl = (x1, ... , xl) is a sequence of l words.
Currently, the dominant approach is conditional
modeling, which decomposes the joint probability
of xl into a product of conditional probabilities 1
1And the joint probability of x is modeled as p(x) =
by using the chain rule,
</bodyText>
<equation confidence="0.986442666666667">
l
p(x1, ... , xl) = p(xi|x1,...,xi−1). (1)
i=1
</equation>
<bodyText confidence="0.994979">
To avoid degenerate representation of the con-
ditionals, the history of xi, denoted as hi =
</bodyText>
<equation confidence="0.696043333333333">
(x1, · · · , xi−1), is reduced to equivalence classes
through a mapping φ(hi) with the assumption
p(xi|hi) ≈ p(xi|φ(hi)). (2)
</equation>
<bodyText confidence="0.998344647058823">
Language modeling in this conditional
approach consists of finding suitable mappings
φ(hi) and effective methods to estimate
p(xi|φ(hi)). A classic example is the traditional
n-gram LMs with φ(hi) = (xi−n+1, ... , xi−1).
Various smoothing techniques are used for
parameter estimation (Chen and Goodman, 1999).
Recently, neural network LMs, which have begun
to surpass the traditional n-gram LMs, also follow
the conditional modeling approach, with φ(hi)
determined by a neural network (NN), which can
be either a feedforward NN (Schwenk, 2007) or a
recurrent NN (Mikolov et al., 2011).
Remarkably, an alternative approach is used in
whole-sentence maximum entropy (WSME) lan-
guage modeling (Rosenfeld et al., 2001). Specifi-
cally, a WSME model has the form:
</bodyText>
<equation confidence="0.9988035">
p(x; λ) = Z exp{λT f(x)}
1 (3)
</equation>
<bodyText confidence="0.999119">
Here f(x) is a vector of features, which can be
arbitrary computable functions of x, λ is the cor-
responding parameter vector, and Z is the global
normalization constant. Although WSME mod-
els have the potential benefits of being able to
naturally express sentence-level phenomena and
integrate features from a variety of knowledge
</bodyText>
<footnote confidence="0.847832666666667">
p(x&apos;)p((EOS)1x1), where (EOS) is a special token placed
at the end of every sentence. Thus the distribution of the
sentence length is implicitly modeled.
</footnote>
<page confidence="0.92934">
785
</page>
<note confidence="0.975941666666667">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 785–794,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999964000000001">
sources, their performance results ever reported
are not satisfactory (Rosenfeld et al., 2001; Amaya
and Benedi, 2001; Ruokolainen et al., 2010).
The WSME model defined in (3) is basically a
Markov random field (MRF). A substantial chal-
lenge in fitting MRFs is that evaluating the gradi-
ent of the log likelihood requires high-dimensional
integration and hence is difficult even for mod-
erately sized models (Younes, 1989), let alone
the language model (3). The sampling methods
previously tried for approximating the gradient are
the Gibbs sampling, the Independence Metropolis-
Hasting sampling and the importance sampling
(Rosenfeld et al., 2001). Simple applications of
these methods are hardly able to work efficient-
ly for the complex, high-dimensional distribution
such as (3), and hence the WSME models are in
fact poorly fitted to the data. This is one of the
reasons for the unsatisfactory results of previous
WSME models.
In this paper, we propose a new language
model, called the trans-dimensional random
field (TDRF) model, by explicitly taking
account of the empirical distributions of lengths.
This formulation subsequently enables us to
develop a powerful Markov chain Monte Carlo
(MCMC) technique, called trans-dimensional
mixture sampling and then propose an effective
training algorithm in the framework of stochastic
approximation (SA) (Benveniste et al., 1990;
Chen, 2002). The SA algorithm involves jointly
updating the model parameters and normalization
constants, in conjunction with trans-dimensional
MCMC sampling. Section 2 and 3 present the
model definition and estimation respectively.
Furthermore, we make several additional in-
novations, as detailed in Section 4, to enable
successful training of TDRF models. First, the
diagonal elements of hessian matrix are estimat-
ed during SA iterations to rescale the gradient,
which significantly improves the convergence of
the SA algorithm. Second, word classing is in-
troduced to accelerate the sampling operation and
also improve the smoothing behavior of the mod-
els through sharing statistical strength between
similar words. Finally, multiple CPUs are used to
parallelize the training of our RF models.
In Section 5, speech recognition experiments
are conducted to evaluate our TDRF LMs, com-
pared with the traditional 4-gram LMs and the re-
current neural network LMs (RNNLMs) (Mikolov
et al., 2011) which have emerged as a new state-
of-art of language modeling. We explore the use
of a variety of features based on word and class
information in TDRF LMs. In terms of word error
rates (WERs) for speech recognition, our TDRF
LMs alone can outperform the KN-smoothing 4-
gram LM with 9.1% relative reduction, and per-
form comparably to the RNNLM with a slight
0.5% relative reduction. To our knowledge, this
result represents the first strong empirical evidence
supporting the power of using the whole-sentence
language modeling approach. Our open-source
TDRF toolkit is released publicly 2.
</bodyText>
<sectionHeader confidence="0.954664" genericHeader="introduction">
2 Model Definition
</sectionHeader>
<bodyText confidence="0.999750333333333">
Throughout, we denote 3 by xl = (x1, ... , xl) a
sentence (i.e., word sequence) of length l ranging
from 1 to m. Each element of xl corresponds to
a single word. For l = 1, ... , m, we assume
that sentences of length l are distributed from an
exponential family model:
</bodyText>
<equation confidence="0.9980125">
pl(xl;λ) = 1
Zl(λ)eλT f(xl)
</equation>
<bodyText confidence="0.99954425">
where f(xl) = (f1(xl), f2(xl), ... , fd(xl))T is
the feature vector and λ = (λ1, λ2,... , λd)T is
the corresponding parameter vector, and Zl(λ) is
the normalization constant:
</bodyText>
<equation confidence="0.957072">
Zl(λ) = � eλT f(xl) (5)
xl
</equation>
<bodyText confidence="0.999273333333333">
Moreover, we assume that length l is associated
with probability πl for l = 1, ... , m. Therefore,
the pair (l, xl) is jointly distributed as
</bodyText>
<equation confidence="0.995638">
p(l, xl; λ) = πl pl(xl; λ). (6)
</equation>
<bodyText confidence="0.999788090909091">
We provide several comments on the above
model definition. First, by making explicit the
role of lengths in model definition, it is clear that
the model in (6) is a mixture of random fields
on sentences of different lengths (namely on sub-
spaces of different dimensions), and hence will be
called a trans-dimensional random field (TDRF).
Different from the WSME model (3), a crucial
aspect of the TDRF model (6) is that the mixture
weights πl can be set to the empirical length
probabilities in the training data. The WSME
</bodyText>
<footnote confidence="0.998583">
2http://oa.ee.tsinghua.edu.cn/
˜ouzhijian/software.htm
3We add sup or subscript l, e.g. in xl, pl(), to make clear
that the variables and distributions depend on length l.
</footnote>
<equation confidence="0.946753">
, (4)
</equation>
<page confidence="0.9797">
786
</page>
<bodyText confidence="0.994444166666667">
model (3) is essentially also a mixture of RFs, but
the mixture weights implied are proportional to the
normalizing constants Zl(λ):
estimating normalizing constants from multiple
RFs of the same dimension, and (Green, 1995) on
trans-dimensional MCMC.
</bodyText>
<equation confidence="0.999436666666667">
p(l,xl; λ) = Zl(λ) Zl(λ)eλT f(xl), (7)
1
Z(λ)
</equation>
<bodyText confidence="0.995044142857143">
where Z(λ) = Eml=1 Zl(λ).
A motivation for proposing (6) is that it is
very difficult to sample from (3), namely (7),
as a mixture distribution with unknown weights
which typically differ from each other by orders of
magnitudes, e.g. 1040 or more in our experiments.
Setting mixture weights to the known, empirical
length probabilities enables us to develop a very
effective learning algorithm, as introduced in Sec-
tion 3. Basically, the empirical weights serve as a
control device to improve sampling from multiple
distributions (Liang et al., 2007; Tan, 2015) .
Second, it can be shown that if we incorporate
the length features 4 in the vector of features f(x)
in (3), then the distribution p(x; λ) in (3) under
the maximum entropy (ME) principle will take the
form of (6) and the probabilities (π1, ... , πm) in
(6) implied by the parameters for the length fea-
tures are exactly the empirical length probabilities.
Third, a feature fi(xl), 1 ≤ i ≤ d, can be any
computable function of the sentence xl, such as
n-grams. In our current experiments, the features
fi(xl) and their corresponding parameters λi are
defined to be position-independent and length-
independent. For example, fi(xl) = Ek fi(xl,k),
where fi(xl, k) is a binary function of xl evaluated
at position k. As a result, the feature fi(xl) takes
values in the non-negative integers.
</bodyText>
<sectionHeader confidence="0.969786" genericHeader="method">
3 Model Estimation
</sectionHeader>
<bodyText confidence="0.999713916666667">
We develop a stochastic approximation algorith-
m using Markov chain Monte Carlo to estimate
the parameters λ and the normalization constants
Z1(λ),..., Zm(λ) (Benveniste et al., 1990; Chen,
2002). The core algorithms newly designed in
this paper are the joint SA for simultaneously
estimating parameters and normalizing constants
(Section 3.2) and trans-dimensional mixture sam-
pling (Section 3.3) which is used as Step I of the
joint SA. The most relevant previous works that
we borrowed from are (Gu and Zhu, 2001) on SA
for fitting a single RF, (Tan, 2015) on sampling and
</bodyText>
<footnote confidence="0.986530666666667">
4The length feature corresponding to length l is a binary
feature that takes one if the sentence x is of length l, and
otherwise takes zero.
</footnote>
<subsectionHeader confidence="0.998779">
3.1 Maximum likelihood estimation
</subsectionHeader>
<bodyText confidence="0.999227142857143">
Suppose that the training dataset consists of nl
sentences of length l for l = 1, ... , m. First,
the maximum likelihood estimate of the length
probability πl is easily shown to be nl/n, where
n = Eml=1 nl. By abuse of notation, we set
πl = nl/n hereafter. Next, the log-likelihood of
λ given the empirical length probabilities is
</bodyText>
<equation confidence="0.994429">
log pl(xl; λ), (8)
</equation>
<bodyText confidence="0.9933586">
where Dl is the collection of sentences of length l
in the training set. By setting to 0 the derivative of
(8) with respect to λ, we obtain that the maximum
likelihood estimate of λ is determined by the
following equation:
</bodyText>
<equation confidence="0.963194">
∂L(λ) ˜p[f] − pλ[f] = 0, (9)
∂λ
</equation>
<bodyText confidence="0.9853695">
where ˜p[f] is the expectation of the feature vector
f with respect to the empirical distribution:
</bodyText>
<equation confidence="0.838375">
f(xl), (10)
</equation>
<bodyText confidence="0.93365">
and pλ[f] is the expectation of f with respect to
the joint distribution (6) with πl = nl/n:
pλ,l[f], (11)
and pλ,l[f] = Exl f(xl)pl(xl; λ). Eq.(9) has
the form of equating empirical expectations ˜p[f]
with theoretical expectations pλ[f], as similarly
found in maximum likelihood estimation of single
random field models.
</bodyText>
<subsectionHeader confidence="0.999835">
3.2 Joint stochastic approximation
</subsectionHeader>
<bodyText confidence="0.999930166666667">
Training random field models is challenging due
to numerical intractability of the normalizing con-
stants Zl(λ) and expectations pλ,l[f]. We propose
a novel SA algorithm for estimating the parame-
ters λ by (9) and, simultaneously, estimating the
log ratios of normalization constants:
</bodyText>
<equation confidence="0.977140818181818">
Zl(λ)
ζ∗ l (λ) = log , l = 1, ... , m (12)
Z1(λ)
1
L(λ) =
n
�m
l=1
�
xl∈Dl
1
˜p[f] =
n
�m
l=1
�
xl∈Dl
nl
pλ[f] =
n
�m
l=1
</equation>
<page confidence="0.955169">
787
</page>
<figure confidence="0.884008294117647">
Algorithm 1 Joint stochastic approximation
Input: training set
1: set initial values λ(0) = (0, ... , 0)T and
ζ(0) = ζ∗(λ(0)) − ζ∗1 (λ(0))
2: fort = 1, 2,. . . , tmax do
3: set B(t) = 0
4: set (L(t,0), X(t,0)) = (L(t−1,K), X(t−1,K))
Step L: MCMC sampling
5: for k = 1 --+ K do
6: sampling (See Algorithm 3)
(L(t,k), X(t,k)) = SAMPLE(L(t,k−1), X(t,k−1))
7: set B(t) = B(t) U {(L(t,k), X(t,k))}
8: end for
Step LL: SA updating
9: Compute λ(t) based on (14)
10: Compute ζ(t) based on (15) and (16)
11: end for
</figure>
<bodyText confidence="0.999959111111111">
where Z1(λ) is chosen as the reference value and
can be calculated exactly. The algorithm can be
obtained by combining the standard SA algorithm
for training single random fields (Gu and Zhu,
2001) and a trans-dimensional extension of the
self-adjusted mixture sampling algorithm (Tan,
2015).
Specifically, consider the following joint distri-
bution of the pair (l, xl):
</bodyText>
<equation confidence="0.991866">
p(l, xl; λ, ζ) a πl
eζl eλT f(xl), (13)
</equation>
<bodyText confidence="0.979051866666667">
where πl is set to nl/n for l = 1, ... , m, but
ζ = (ζ1, ... , ζm)T with ζ1 = 0 are hypothesized
values of the truth ζ∗(λ) = (ζ∗1(λ), ... , ζ∗m(λ))T
with ζ∗1(λ) = 0. The distribution p(l, xl; λ,ζ)
reduces to p(l, xl; λ) in (6) if ζ were identical
to ζ∗(λ). In general, p(l, xl; λ,ζ) differs from
p(l, xl; λ) in that the marginal probability of
length l is not necessarily πl.
The joint SA algorithm, whose pseudo-code is
shown in Algorithm 1, consists of two steps at
each time t as follows.
Step I: MCMC sampling. Generate a sample
set B(t) with p(l, xl; λ(t−1), ζ(t−1)) as the station-
ary distribution (see Section 3.3).
Step II: SA updating. Compute
</bodyText>
<equation confidence="0.940898142857143">
� λ(t−1) + γλ˜p[f] −E(l,xl) ∈B(t) f(xl) l (14)
K
where γλ is a learning rate of λ; compute
ζ(t−2 ) = ζ(t) + γζ1 δ1(B(t)) ... δm(B(t)) l (15)
π1 πm f
ζ(t) = ζ(t− 12) − ζ(t− 12) (16)
1
</equation>
<bodyText confidence="0.9989645">
where γζ is a learning rate of ζ, and δl(B(t)) is the
relative frequency of length l appearing in B(t):
</bodyText>
<equation confidence="0.999292">
δl(B(t)) = E(j,xj)∈B(t) 1(j = l). (17)
K
</equation>
<bodyText confidence="0.943118166666667">
The rationale in (15) is to adjust ζ based on
how the relative frequencies of lengths δl(B(t))
are compared with the desired length probabili-
ties πl. Intuitively, if the relative frequency of
some length l in the sample set B(t) is greater
(or respectively smaller) than the desired length
probability πl, then the hypothesized value ζ(t−1)
l
is an underestimate (or overestimate) of ζ∗l (λ(t−1))
and hence should be increased (or decreased).
Following Gu &amp; Zhu (2001) and Tan (2015), we
set the learning rates in two stages:
</bodyText>
<equation confidence="0.982431333333333">
� t−βλ if t &lt; t0
γλ = 1(18)
if t &gt; t0
t−t0+tβλ
0
�
(0.1t)−βζ if t &lt; t0
γζ = 1 if t &gt; t0 (19)
0.1(t−t0)+(0.1t0)βζ
</equation>
<bodyText confidence="0.9999825">
where 0.5 &lt; βλ, βζ &lt; 1. In the first stage (t &lt; t0),
a slow-decaying rate of t−β is used to introduce
large adjustments. This forces the estimates λ(t)
and ζ(t) to fall reasonably fast into the true values.
In the second stage (t &gt; t0), a fast-decaying
rate of t−1 is used. The iteration number t is
multiplied by 0.1 in (19), to make the the learning
rate of ζ decay more slowly than λ. Commonly,
t0 is selected to ensure there is no more significant
adjustment observed in the first stage.
</bodyText>
<subsectionHeader confidence="0.998569">
3.3 Trans-dimensional mixture sampling
</subsectionHeader>
<bodyText confidence="0.999965529411765">
We describe a trans-dimensional mixture sam-
pling algorithm to simulate from the joint distri-
bution p(l, xl; λ, ζ), which is used with (λ, ζ) =
(λ(t−1), ζ(t−1)) at time t for MCMC sampling in
the joint SA algorithm. The name “mixture sam-
pling” reflects the fact that p(l, xl; λ, ζ) represents
a labeled mixture, because l is a label indicating
that xl is associated with the distribution pl(xl; ζ).
With fixed (λ, ζ), this sampling algorithm can
be seen as formally equivalent to reversible jump
MCMC (Green, 1995), which was originally pro-
posed for Bayes model determination.
The trans-dimensional mixture sampling algo-
rithm consists of two steps at each time t: local
jump between lengths and Markov move of sen-
tences for a given length. In the following, we de-
note by L(t−1) and X(t−1) the length and sequence
</bodyText>
<page confidence="0.985923">
788
</page>
<bodyText confidence="0.990037866666667">
before sampling, but use the short notation (λ, ζ)
for (λ(t−1), ζ(t−1)).
Step I: Local jump. The Metropolis-Hastings
method is used in this step to sample the length.
Assuming L(t−1) = k, first we draw a new length
j ∼ Γ(k, ·). The jump distribution Γ(k, l) is
defined to be uniform at the neighborhood of k :
where m is the maximum length. Eq.(20) restricts
the difference between j and k to be no more than
one. If j = k, we retain the sequence and perform
the next step directly, i.e. set L(t) = k and X(t) =
X(t−1). If j = k + 1 or j = k − 1, the two cases
are processed differently.
If j = k + 1, we first draw an element
(i.e., word) Y from a proposal distribution:
</bodyText>
<equation confidence="0.9785648125">
Y ∼ gk+1(y|X(t−1)). Then we set
L(t) = j (= k + 1) and X(t) = {X(t−1),Y } with
probability
min{1, Γ(j, k) p(j, {X (t−1) Y }; λ, ζ) }
Γ(k, j) p(k, X(t−1); λ, ζ)gk+1(Y |X(t−1))
where {X(t−1), Y } denotes a sequence of length
k + 1 whose first k elements are X(t−1) and the
last element is Y .
If j = k − 1, we set L(t) = j (= k − 1) and
X(t) = X(t−1)
1:j with probability
J Γ(j k) p(j, Xi:j1); λ, ζ)gk(Xkt−1) |X1 tj 1))
min 1, Γ(k&apos;:j) p(k, X(t−1); λ, ζ)
+ 1, {xk, y};
gk+1(y|xk)=p(k
λ,ζ) � w p(k + 1, {xk, w}; λ, ζ).(23)
</equation>
<bodyText confidence="0.903936">
Step II: Markov move. After the step of local
jump, we obtain
</bodyText>
<equation confidence="0.931972333333333">
if L(t) = k
{ X(t−1)
Y } if L(t) = k + 1
{X(t−1),
X(t−1) if L(t) = k − 1
1:k−1
</equation>
<bodyText confidence="0.960767333333333">
Then we perform Gibbs sampling on
from
the first element to the last element (Algori
</bodyText>
<equation confidence="0.795903142857143">
X(t),
thm 2)
1: for i = 1
L(t) do
2: draw W
{X(t)
w,
3: set
→
∼p(L(t),
1:i−1,
X(t)
i+1:L(t)};λ,ζ)
X(t) i= W
</equation>
<sectionHeader confidence="0.756156666666667" genericHeader="method">
4: end for
4 Algorithm Optimization and
Acceleration
</sectionHeader>
<bodyText confidence="0.89993875">
slow convergence, especially when
is high-
dimensional. We introduce several techniques for
improving the convergence of the algorithm an
λ
d
reducing computational cost.
Taking the second derivatives of
</bodyText>
<equation confidence="0.994033777777778">
λ.
L(λ) yields
d2L(λ)
Hi = −dλ2
i
K
H(t−21)1 � =i
(l,xl)∈B(t)
(22)
</equation>
<bodyText confidence="0.997944125">
where
is the first j elements of
and
is the kth element of
In (21) and (22),
can be flexibly
specified as a proper density function in y. In our
application, we find the foll
</bodyText>
<equation confidence="0.943361857142857">
X(t−1)
1:j
X(t−1)
X(t−1)
k
X(t−1).
gk+1(y|xk)
</equation>
<table confidence="0.91037475">
owing choice works
reasonably well:
Algorithm 2 Markov Move
The joint SA algorithm may still suffer from
</table>
<subsectionHeader confidence="0.940871">
4.1 Improving SA recursion
</subsectionHeader>
<bodyText confidence="0.960819">
We propose two techniques to effectively improve
the convergence of SA recursion.
The first technique is to incorporate Hessian
information, similarly as in related works on s-
tochastic approximation (Gu and Zhu, 2001) and
stochastic gradient descent algorithms (Byrd et al.,
2014). But we only use the diagonal elements of
the Hessian matrix to re-scale the gradient, due to
high-dimensionality of
where Hi denotes the ith diagonal element of
Hessian matrix. At time t, before updating the
parameter λ (Step II in Section 3.2), we compute
</bodyText>
<equation confidence="0.959232">
πl(¯pl[fi])2,
(26)
H(t)
(27)
</equation>
<bodyText confidence="0.7652246">
where
fi(xl), and
is the subset, of size
containing all
sentences of length l in B(t).
The second technique is to introduce the
on the training set. At each iteration, a
subset D(t) of K sentences are randomly selected
from the training set. Then the gradient is approx-
imated with the overall empirical expectation
being replaced by the empirical expectation over
the subset
This technique is reminiscent of
stochastic gradient descent using a random sub-
sample of tr
</bodyText>
<equation confidence="0.975579714285714">
i=H(t−1)i+γH(H(t−21)−H(t−1)
i ),
i
¯pl[fi]=|B(t)
l |−1E(l,xl)∈B(t)
l
B(t)
l
|B(t)
l|,
“mini-
batch”
˜p[f]
D(t).
</equation>
<bodyText confidence="0.792516">
aining data to achieve fast convergence
</bodyText>
<equation confidence="0.924865083333333">
Γ(k, l) = I
3
2,
1 if k = 1, l ∈ [1, 2] or k = m, l ∈ [m − 1, m]
0, otherwise
1,ifk∈ [2,m−1],l ∈ [k−1,k+1]
X(t) =
(24)
= p[f2i ] − �m πl(pl[fi])2 (25)
l=1
fi(xl)2 − �m
l=1
</equation>
<page confidence="0.975837">
789
</page>
<figureCaption confidence="0.984335666666667">
Figure 1: Examples of convergence curves on
training set after introducing hessian and training
set mini-batching.
</figureCaption>
<bodyText confidence="0.986879">
of optimization algorithms (Bousquet and Bottou,
2008).
By combining the two techniques, we revise the
updating equation (14) of A to
</bodyText>
<equation confidence="0.9987192">
+ γλ x
max(H(t)
i , h)
r E(l,xl)∈D(t) fi(xl) — P(l,xl)∈B(t) fi(xl) 1 (28)
(Il K K 1
</equation>
<bodyText confidence="0.990123">
where 0 &lt; h &lt; 1 is a threshold to avoid H(t)
</bodyText>
<equation confidence="0.499412">
i
</equation>
<bodyText confidence="0.998522666666667">
being too small or even zero. Moreover, a constant
t, is added to the denominator of (18), to avoid too
large adjustment of A, i.e.
</bodyText>
<equation confidence="0.9871765">
� 1 if t ≤ t0,
tc+tβλ
γa = 1 (29)
if t &gt; t0.
tc+t−t0+tβλ
0
</equation>
<bodyText confidence="0.998699666666667">
Fig.1(a) shows the result after introducing hessian
estimation, and Fig.1(b) shows the effect of train-
ing set mini-batching.
</bodyText>
<subsectionHeader confidence="0.998167">
4.2 Sampling acceleration
</subsectionHeader>
<bodyText confidence="0.9994193">
For MCMC sampling in Section 3.3, the Gibbs
sampling operation of drawing X(t)
i (Step 2 in Al-
gorithms 2) involves calculating the probabilities
of all the possible elements in position i. This
is computationally costly, because the vocabulary
size |V |is usually 10 thousands or more in prac-
tice. As a result, the Gibbs sampling operation
presents a bottleneck limiting the efficiency of
sampling algorithms.
We propose a novel method of using class in-
formation to effectively reduce the computational
cost of Gibbs sampling. Suppose that each word
in vocabulary V is assigned to a single class.
If the total class number is |C|, then there are,
on average, |V|/|C |words in each class. With
the class information, we can first draw the class
of X(t)
i , denoted by c(t)
i , and then draw a word
</bodyText>
<figure confidence="0.915310888888889">
Algorithm 3 Class-based MCMC sampling
1: function SAMPLE((L(t−1), X(t−1)))
2: set k = L(t−1)
3: init (L(t), X(t)) = (k, X(t−1))
Step I. Local jump
4: generate j - P(k, ·) (Eq.(20))
5: if j = k + 1 then
6: generate C - Qk+1(c)
7: generate Y - ˘gk+1(yJX(t−1), C) (Eq.31)
8: set L(t) = j and X(t) = {X(t−1), Y } with
probability (Eq.21) and (Eq.32)
9: end if
10: if j = k - 1 then
11: set L(t) = j and X(t) = X(t−1)
1:k−1 with probabil-
ity Eq.(22) and (Eq.32)
12: end if
Step II. Markov move
13: for i = 1 --+ L(t) do
14: draw C Qi(c)
15: set ci(t) = C with probability (Eq.30)
16: draw W E V(t)
ci
17: set X(t) i= W
18: end for
19: return (L(t), X(t))
20: end function
</figure>
<figureCaption confidence="0.432527">
belonging to class c(t)
</figureCaption>
<bodyText confidence="0.998035666666667">
i . The computational cost is
reduced from |V |to |C |+ |V|/|C |on average.
The idea of using class information to accel-
erate training has been proposed in various con-
texts of language modeling, such as maximum
entropy models (Goodman, 2001b) and RNN LMs
(Mikolov et al., 2011). However, the realization of
this idea is different for training our models.
The pseudo-code of the new sampling method is
shown in Algorithm 3. Denote by V, the subset of
V containing all the words belonging to class c. In
the Markov move step (Step 13 to 18 in Algorithm
3), at each position i, we first generate a class C
from a proposal distribution Qi(c) and then accept
C as the new c(t)
</bodyText>
<equation confidence="0.979816714285714">
i with probability
( )
1, Qi(c(t)
i ) pi(C)
min (30)
Qi(C) pi(c(t)
i )
</equation>
<bodyText confidence="0.84508125">
where p(L(t), {X(t)
�pi(c) = 1:i−1, w, X(t)i+1:L(t)}; A, ζ).
wEVc
The probabilities Qi(c) and pi(c) depend on
</bodyText>
<equation confidence="0.8560585">
{X(t)
1:i−1, X(t)
</equation>
<bodyText confidence="0.945677666666667">
i+1:L(t)}, but this is suppressed in the
notation. Then we normalize the probabilities of
words belonging to class c(t)
i and draw a word as
the new X(t)
i from the class c(t)
i .
Similarly, in the local jump step with k =
L(t−1), if the proposal j = k + 1 (Step 5 to 9
</bodyText>
<figure confidence="0.9996071">
(a)
(b)
200
Hessian+mini−batch
Hessian
150
100
50
0 500 1000 1500 2000
t/10
t/10
− log−likelihood
200
180
160
140
1200 20 40 60 80 100
without hessian
with hessian
negabw log−likelihood
</figure>
<page confidence="0.987687">
790
</page>
<bodyText confidence="0.9980755">
in Algorithm 3), we first generate C ti Qk+1(c)
and then generate Y from class C by
</bodyText>
<equation confidence="0.997667142857143">
p(k + 1, {xk, y}; λ, ζ)
˘gk+1(y|xk, C) = (31)
EwEVC p(k + 1, {xk, w}; λ, ζ)
with xk = X(t−1). Then we set L(t) = j and
X(t) = {X(t−1), Y } with probability as defined
in (21), by setting
gk+1(y|xk) = Qk+1(C)˘gk+1(y|xk,C). (32)
</equation>
<bodyText confidence="0.999305444444444">
If the proposal j = k − 1, similarly we use
acceptance probability (22) with (32).
In our application, we construct Qi(c) dynami-
cally as follows. Write xl for {X(t−1), Y } in Step
8 or for X(t) in Step 11 of Algorithm 3. First,
we construct a reduced model pcl (xl), by including
only the features that depend on xli through its
class and retaining the corresponding parameters
in pl(xl; λ, ζ). Then we define the distribution
</bodyText>
<equation confidence="0.987357">
Qi(c) = pcl({xl1:i−1, c, xli+1:l}),
</equation>
<bodyText confidence="0.8663275">
which can be directly calculated without knowing
the value of xli.
</bodyText>
<subsectionHeader confidence="0.999869">
4.3 Parallelization of sampling
</subsectionHeader>
<bodyText confidence="0.9999749">
The sampling operation can be easily parallelized
in SA Algorithm 1. At each time t, both the
parameters λ and log normalization constants ζ
are fixed at λ(t−1) and ζ(t−1). Instead of simu-
lating one Markov Chain, we simulate J Markov
Chains on J CPU cores separately. As a result, to
generate a sample set B(t) of size K, only K/J
sampling steps need to be performed on each CPU
core. By parallelization, the sampling operation is
completed J times faster than before.
</bodyText>
<sectionHeader confidence="0.99983" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999392">
5.1 PTB perplexity results
</subsectionHeader>
<bodyText confidence="0.997498083333333">
In this section, we evaluate the performance of
LMs by perplexity (PPL). We use the Wall Street
Journal (WSJ) portion of Penn Treebank (PTB).
Sections 0-20 are used as the training data (about
930K words), sections 21-22 as the development
data (74K) and section 23-24 as the test data
(82K). The vocabulary is limited to 10K words,
with one special token (UNK) denoting words
not in the vocabulary. This setting is the same as
that used in other studies (Mikolov et al., 2011).
The baseline is a 4-gram LM with modified
Kneser-Ney smoothing (Chen and Goodman,
</bodyText>
<table confidence="0.999513">
Type Features
w (w−3w−2w−1w0)(w−2w−1w0)(w−1w0)(w0)
c (c−3c−2c−1c0)(c−2c−1c0)(c−1c0)(c0)
ws (w−3w0)(w−3w−2w0)(w−3w−1w0)(w−2w0)
cs (c−3c0)(c−3c−2c0)(c−3c−1c0)(c−2c0)
wsh (w−4w0) (w−5w0)
csh (c−4c0) (c−5c0)
cpw (c−3c−2c−1w0) (c−2c−1w0)(c−1w0)
</table>
<tableCaption confidence="0.999805">
Table 1: Feature definition in TDRF LMs
</tableCaption>
<bodyText confidence="0.999359108108108">
1999), denoted by KN4. We use the RNNLM
toolkit5 to train a RNNLM (Mikolov et al., 2011).
The number of hidden units is 250 and other
configurations are set by default6.
Word classing has been shown to be useful in
conditional ME models (Chen, 2009). For our
TDRF models, we consider a variety of features
as shown in Table 1, mainly based on word and
class information. Each word is deterministically
assigned to a single class, by running the automat-
ic clustering algorithm proposed in (Martin et al.,
1998) on the training data.
In Table 1, wi, ci, i = 0, −1, ... , −5 denote the
word and its class at different position offset i,
e.g. w0, c0 denotes the current word and its class.
We first introduce the classic word/class n-gram
features (denoted by “w”/“c”) and the word/class
skipping n-gram features (denoted by “ws”/“cs”)
(Goodman, 2001a). Second, to demonstrate that
long-span features can be naturally integrated in
TDRFs, we introduce higher-order features “w-
sh”/“csh”, by considering two words/classes sep-
arated with longer distance. Third, as an example
of supporting heterogenous features that combine
different information, the crossing features “cp-
w” (meaning class-predict-word) are introduced.
Note that for all the feature types in Table 1, only
the features observed in the training data are used.
The joint SA (Algorithm 1) is used to train the
TDRF models, with all the acceleration methods
described in Section 4 applied. The minibatch
size K = 300. The learning rates γλ and γζ
are configured as (29) and (19) respectively with
βλ = βζ = 0.6 and tc = 3000. For t0, it is first
initialized to be 104. During iterations, we monitor
the smoothed log-likelihood (moving average of
1000 iterations) on the PTB development data.
</bodyText>
<footnote confidence="0.998786142857143">
5http://rnnlm.org/
6Minibatch size=10, learning rate=0.1, BPTT steps=5. 17
sweeps are performed before stopping, which takes about 25
hours. No word classing is used, since classing in RNNLMs
reduces computation but at cost of accuracy. RNNLMs were
experimented with varying numbers of hidden units (100-
500). The best result from using 250 hidden units is reported.
</footnote>
<page confidence="0.993966">
791
</page>
<figure confidence="0.99584172">
models PPL (± std. dev.)
KN4 142.72
RNN 128.81
TDRF w+c 130.69±1.64
WER
8.71
7.96
8.87
8.82
PPL (± std. dev.)
295.41
256.15
≈ 2.8 x 1012
≈ 6.7 x 1012
model
KN4
RNN
WSMEs (200c)
w+c+ws+cs
w+c+ws+cs+cpw
#feat
1.6M
5.1M
5.2M
6.4M
</figure>
<tableCaption confidence="0.849106">
Table 2: The PPLs on the PTB test data. The class
</tableCaption>
<bodyText confidence="0.98738485">
number is 200.
We set t0 to the current iteration number once the
rising percentage of the smoothed log-likelihoods
within 100 iterations is below 20%, and then
continue 5000 further iterations before stopping.
The configuration of hessian estimation (Section
4.1) is &apos;yH = &apos;y,, and h = 10−4. L2 regularization
with constant 10−5 is used to avoid over-fitting. 8
CPU cores are used to parallelize the algorithm, as
described in Section 4.3, and the training of each
TDRF model takes less than 20 hours.
The perplexity results on the PTB test data are
given in Table 2. As the normalization constants
of TDRF models are estimated stochastically, we
report the Monte Carlo mean and standard devi-
ation from the last 1000 iterations for each PPL.
The TDRF model using the basic “w+c” features
performs close to the RNNLM in perplexity. To be
compact, results with more features are presented
in the following WSJ experiment.
</bodyText>
<subsectionHeader confidence="0.998932">
5.2 WSJ speech recognition results
</subsectionHeader>
<bodyText confidence="0.999852857142857">
In this section, we continue to use the LMs ob-
tained above (using PTB training and develop-
ment data), and evaluate their performance mea-
sured by WERs in speech recognition, by re-
scoring 1000-best lists from WSJ’92 test data (330
sentences). The oracle WER of the 1000-best lists
is 3.4%, which are generated from using the Kaldi
toolkit7 with a DNN-based acoustic model.
TDRF LMs using a variety of features and
different number of classes are tested. The results
are shown in Table 3. Different types of features,
like the skipping features, the higher-order fea-
tures and the crossing features can all be easily
supported in TDRF LMs, and the performance
is improved to varying degrees. Particularly, the
TDRF using the “w+c+ws+cs+cpw” features with
class number 200 performs comparable to the
RNNLM in both perplexity and WER. Numerical-
ly, the relative reduction is 9.1% compared with
the KN4 LMs, and 0.5% compared with the RNN
LM.
</bodyText>
<footnote confidence="0.948732">
7http://kaldi.sourceforge.net/
</footnote>
<table confidence="0.970145615384615">
TDRFs (100c)
w+c
w+c+ws+cs
w+c+ws+cs+cpw
w+c+ws+cs+wsh+csh
TDRFs (200c)
w+c
w+c+ws+cs
w+c+ws+cs+cpw
w+c+ws+cs+wsh+csh
TDRFs (500c)
w+c 8.72 261.02±2.94 2.8M
w+c+ws+cs 8.29 266.34±6.13 5.9M
</table>
<tableCaption confidence="0.997686">
Table 3: The WERs and PPLs on the WSJ’92 test
</tableCaption>
<bodyText confidence="0.978627">
data. “#feat” denotes the feature number. Differ-
ent TDRF models with class number 100/200/500
are reported (denoted by “100c”/“200c”/“500c”)
</bodyText>
<subsectionHeader confidence="0.99374">
5.3 Comparison and discussion
</subsectionHeader>
<bodyText confidence="0.999993413793104">
TDRF vs WSME. For comparison, Table 3 also
presents the results from our implementation of
the WSME model (3), using the same features as
in Table 1. This WSME model is the same as in
(Rosenfeld, 1997), but different from (Rosenfeld
et al., 2001), which uses the traditional n-gram
LM as the priori distribution p0.
For the WSME model (3), we can still use a
SA training algorithm, similar to that developed in
Section 3.2, to estimate the parameters A. But in
this case, there is no need to introduce Cl, because
the normalizing constants Zl(A) are canceled out
as seen from (7). Specifically, the learning rate &apos;y,,
and the L2 regularization are configured the same
as in TDRF training. A fixed number of iterations
with t0 = 5000 is performed. The total iteration
number is 10000, which is similar to the iteration
number used in TDRF training.
In order to calculate perplexity, we need to
estimate the global normalizing constant Z(A) =
E�l=1 Zl(A) for the WSME model. Similarly
as in (Tan, 2015), we apply the SA algorithm
in Section 3.2 to estimate the log normalizing
constants C, while fixing the parameters A to be
those already estimated from the WSME model
and using uniform probabilities Trl ≡ m−1.
The resulting PPLs of these WSME models are
extremely poor. The average test log-likelihoods
per sentence for these two WSME models are
</bodyText>
<figure confidence="0.9995615">
8.56 268.25±3.52 2.2M
8.16 265.81±4.30 4.5M
8.05 265.63±7.93 5.6M
8.03 276.90±5.00
5.2M
8.46 257.78±3.13 2.5M
8.05 257.80±4.29 5.2M
7.92 264.86±8.55 6.4M
7.94 266.42±7.48
5.9M
</figure>
<page confidence="0.990315">
792
</page>
<bodyText confidence="0.999947722222222">
−494 and −509 respectively. However, the W-
ERs from using the trained WSME models in
hypothesis re-ranking are not as poor as would be
expected from their PPLs. This appears to indicate
that the estimated WSME parameters are not so
bad for relative ranking. Moreover, when the
estimated λ and ζ are substituted into our TDRF
model (6) with the empirical length probabilities
πl, the “corrected” average test log-likelihoods
per sentence for these two sets of parameters are
improved to be −152 and −119 respectively. The
average test log-likelihoods are both −96 for the
two corresponding TDRF models in Table 3. This
is some evidence for the model deficiency of the
WSME distribution as defined in (3), and intro-
ducing the empirical length probabilities gives a
more reasonable model assumption.
TDRF vs conditional ME. After training, TDRF
models are computationally more efficient in com-
puting sentence probability, simply summing up
weights for the activated features in the sentence.
The conditional ME models (Khudanpur and Wu,
2000; Roark et al., 2004) suffer from the expen-
sive computation of local normalization factors.
This computational bottleneck hinders their use
in practice (Goodman, 2001b; Rosenfeld et al.,
2001). Partly for this reason, although building
conditional ME models with sophisticated features
as in Table 1 is theoretically possible, such work
has not been pursued so far.
TDRF vs RNN. The RNN models suffer from
the expensive softmax computation in the output
layer 8. Empirically in our experiments, the aver-
age time costs for re-ranking of the 1000-best list
for a sentence are 0.16 sec vs 40 sec, based on
TDRF and RNN respectively (no GPU used).
</bodyText>
<sectionHeader confidence="0.999988" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999873">
While there has been extensive research on con-
ditional LMs, there has been little work on the
whole-sentence LMs, mainly in (Rosenfeld et al.,
2001; Amaya and Benedi, 2001; Ruokolainen et
al., 2010). Although the whole-sentence approach
has potential benefits, the empirical results of pre-
vious WSME models are not satisfactory, almost
the same as traditional n-gram models. After
incorporating lexical and syntactic information,
a mere relative improvement of 1% and 0.4%
</bodyText>
<footnote confidence="0.722483">
8This deficiency could be partly alleviated with
</footnote>
<bodyText confidence="0.946103347826087">
some speed-up methods, e.g. using word clustering
(Mikolov, 2012) or noise contrastive estimation (Mnih and
Kavukcuoglu, 2013).
respectively in perplexity and in WER is reported
for the resulting WSEM (Rosenfeld et al., 2001).
Subsequent studies of using WSEMs with gram-
matical features, as in (Amaya and Benedi, 2001)
and (Ruokolainen et al., 2010), report perplexity
improvement above 10% but no WER improve-
ment when using WSEMs alone.
Most RF modeling has been restricted to fixed-
dimensional spaces 9. Despite recent progress,
fitting RFs of moderate or large dimensions re-
mains to be challenging (Koller and Friedman,
2009; Mizrahi et al., 2013). In particular, the
work of (Pietra et al., 1997) is inspiring to us,
but the improved iterative scaling (IIS) method
for parameter estimation and the Gibbs sampler
are not suitable for even moderately sized models.
Our TDRF model, together with the joint SA al-
gorithm and trans-dimensional mixture sampling,
are brand new and lead to encouraging results for
language modeling.
</bodyText>
<sectionHeader confidence="0.999673" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9936031">
In summary, we have made the following contri-
butions, which enable us to successfully train T-
DRF models and obtain encouraging performance
improvement.
• The new TDRF model and the joint SA train-
ing algorithm, which simultaneously updates
the model parameters and normalizing con-
stants while using trans-dimensional mixture
sampling.
• Several additional innovations including ac-
celerating SA iterations by using Hessian
information, introducing word classing to ac-
celerate the sampling operation and improve
the smoothing behavior of the models, and
parallelization of sampling.
In this work, we mainly explore the use of fea-
tures based on word and class information. Future
work with other knowledge sources and larger-
scale experiments is needed to fully exploit the
advantage of TDRFs to integrate richer features.
</bodyText>
<sectionHeader confidence="0.999504" genericHeader="acknowledgments">
8 Acknowledgments
</sectionHeader>
<bodyText confidence="0.996969">
This work is supported by Toshiba Corporation,
National Natural Science Foundation of China
(NSFC) via grant 61473168, and Tsinghua Ini-
tiative. We thank the anonymous reviewers for
helpful comments on this paper.
</bodyText>
<footnote confidence="0.888864333333333">
9Using local fixed-dimensional RFs in sequential models
was once explored, e.g. temporal restricted Boltzmann
machine (TRBM) (Sutskever and Hinton, 2007).
</footnote>
<page confidence="0.998597">
793
</page>
<sectionHeader confidence="0.998323" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999216403846154">
Fredy Amaya and Jos´e Miguel Benedi. 2001. Im-
provement of a whole sentence maximum entropy
language model using grammatical features. In
Association for Computational Linguistics (ACL).
Albert Benveniste, Michel M´etivier, and Pierre
Priouret. 1990. Adaptive algorithms and stochastic
approximations. New York: Springer.
Olivier Bousquet and Leon Bottou. 2008. The
tradeoffs of large scale learning. In NIPS, pages
161–168.
Richard H Byrd, SL Hansen, Jorge Nocedal, and
Yoram Singer. 2014. A stochastic quasi-newton
method for large-scale optimization. arXiv preprint
arXiv:1401.7020.
Stanley F. Chen and Joshua Goodman. 1999. An em-
pirical study of smoothing techniques for language
modeling. Computer Speech &amp; Language, 13:359–
394.
Hanfu Chen. 2002. Stochastic approximation and its
applications. Springer Science &amp; Business Media.
Stanley F. Chen. 2009. Shrinking exponential lan-
guage models. In Proc. of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics.
Joshua Goodman. 2001a. A bit of progress in language
modeling. Computer Speech &amp; Language, 15:403–
434.
Joshua Goodman. 2001b. Classes for fast maximum
entropy training. In Proc. of International Confer-
ence on Acoustics, Speech, and Signal Processing
(ICASSP).
Peter J. Green. 1995. Reversible jump markov
chain monte carlo computation and bayesian model
determination. Biometrika, 82:711–732.
Ming Gao Gu and Hong-Tu Zhu. 2001. Maxi-
mum likelihood estimation for spatial models by
markov chain monte carlo stochastic approximation.
Journal of the Royal Statistical Society: Series B
(Statistical Methodology), 63:339–355.
Sanjeev Khudanpur and Jun Wu. 2000. Maximum en-
tropy techniques for exploiting syntactic, semantic
and collocational dependencies in language model-
ing. Computer Speech &amp; Language, 14:355–372.
Daphne Koller and Nir Friedman. 2009. Probabilistic
graphical models: principles and techniques. MIT
press.
Faming Liang, Chuanhai Liu, and Raymond J Carroll.
2007. Stochastic approximation in monte carlo
computation. Journal of the American Statistical
Association, 102(477):305–320.
Sven Martin, J¨org Liermann, and Hermann Ney. 1998.
Algorithms for bigram and trigram word clustering.
Speech Communication, 24:19–37.
Tomas Mikolov, Stefan Kombrink, Lukas Burget,
Jan H Cernocky, and Sanjeev Khudanpur. 2011.
Extensions of recurrent neural network language
model. In Proc. of International Conference on
Acoustics, Speech and Signal Processing (ICASSP).
Tom´aˇs Mikolov. 2012. Statistical language models
based on neural networks. Ph.D. thesis, Brno
University of Technology.
Yariv Dror Mizrahi, Misha Denil, and Nando de Fre-
itas. 2013. Linear and parallel learning of markov
random fields. arXiv preprint arXiv:1308.6342.
Andriy Mnih and Koray Kavukcuoglu. 2013. Learning
word embeddings efficiently with noise-contrastive
estimation. In Neural Information Processing Sys-
tems (NIPS).
Stephen Della Pietra, Vincent Della Pietra, and John
Lafferty. 1997. Inducing features of random fields.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 19:380–393.
Brian Roark, Murat Saraclar, Michael Collins, and
Mark Johnson. 2004. Discriminative language
modeling with conditional random fields and the
perceptron algorithm. In Proceedings of the 42nd
Annual Meeting on Association for Computational
Linguistics (ACL), page 47.
Ronald Rosenfeld, Stanley F. Chen, and Xiaojin Zhu.
2001. Whole-sentence exponential language mod-
els: a vehicle for linguistic-statistical integration.
Computer Speech &amp; Language, 15:55–73.
Ronald Rosenfeld. 1997. A whole sentence maximum
entropy language model. In Proc. of Automatic
Speech Recognition and Understanding (ASRU).
Teemu Ruokolainen, Tanel Alum¨ae, and Marcus Do-
brinkat. 2010. Using dependency grammar features
in whole sentence maximum entropy language mod-
el for speech recognition. In Baltic HLT.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech &amp; Language, 21:492–
518.
Ilya Sutskever and Geoffrey E Hinton. 2007. Learn-
ing multilevel distributed representations for high-
dimensional sequences. In International Confer-
ence on Artificial Intelligence and Statistics (AIS-
TATS).
Zhiqiang Tan. 2015. Optimally adjusted mixture sam-
pling and locally weighted histogram. In Technical
Report, Department of Statistics, Rutgers University.
Laurent Younes. 1989. Parametric inference for
imperfectly observed gibbsian fields. Probability
theory and related fields, 82:625–645.
</reference>
<page confidence="0.998344">
794
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.468979">
<title confidence="0.99987">Trans-dimensional Random Fields for Language Modeling</title>
<author confidence="0.997333">Zhijian Zhiqiang</author>
<address confidence="0.7447155">of Electronic Engineering, Tsinghua University, Beijing 100084, of Statistics, Rutgers University, Piscataway, NJ 08854,</address>
<email confidence="0.9487275">wangbin12@mails.tsinghua.edu.cn,ztan@stat.rutgers.edu</email>
<abstract confidence="0.999492884615385">Language modeling (LM) involves determining the joint probability of words in a sentence. The conditional approach is dominant, representing the joint probability in terms of conditionals. Examples include n-gram LMs and neural network LMs. An alternative approach, called the random field (RF) approach, is used in whole-sentence maximum entropy (WSME) LMs. Although the RF approach has potential benefits, the empirical results of previous WSME models are not satisfactory. In this paper, we revisit the RF approach for language modeling, with a number of innovations. We propose a trans-dimensional RF (TDRF) model and develop a training algorithm using joint stochastic approximation and trans-dimensional mixture sampling. We perform speech recognition experiments on Wall Street Journal data, and find that our TDRF models lead to performances as good as the recurrent neural network LMs but are computationally more efficient in computing sentence probability.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Fredy Amaya</author>
<author>Jos´e Miguel Benedi</author>
</authors>
<title>Improvement of a whole sentence maximum entropy language model using grammatical features.</title>
<date>2001</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="3835" citStr="Amaya and Benedi, 2001" startWordPosition="572" endWordPosition="575">y express sentence-level phenomena and integrate features from a variety of knowledge p(x&apos;)p((EOS)1x1), where (EOS) is a special token placed at the end of every sentence. Thus the distribution of the sentence length is implicitly modeled. 785 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 785–794, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics sources, their performance results ever reported are not satisfactory (Rosenfeld et al., 2001; Amaya and Benedi, 2001; Ruokolainen et al., 2010). The WSME model defined in (3) is basically a Markov random field (MRF). A substantial challenge in fitting MRFs is that evaluating the gradient of the log likelihood requires high-dimensional integration and hence is difficult even for moderately sized models (Younes, 1989), let alone the language model (3). The sampling methods previously tried for approximating the gradient are the Gibbs sampling, the Independence MetropolisHasting sampling and the importance sampling (Rosenfeld et al., 2001). Simple applications of these methods are hardly able to work efficient</context>
<context position="33295" citStr="Amaya and Benedi, 2001" startWordPosition="5706" endWordPosition="5709">s reason, although building conditional ME models with sophisticated features as in Table 1 is theoretically possible, such work has not been pursued so far. TDRF vs RNN. The RNN models suffer from the expensive softmax computation in the output layer 8. Empirically in our experiments, the average time costs for re-ranking of the 1000-best list for a sentence are 0.16 sec vs 40 sec, based on TDRF and RNN respectively (no GPU used). 6 Related Work While there has been extensive research on conditional LMs, there has been little work on the whole-sentence LMs, mainly in (Rosenfeld et al., 2001; Amaya and Benedi, 2001; Ruokolainen et al., 2010). Although the whole-sentence approach has potential benefits, the empirical results of previous WSME models are not satisfactory, almost the same as traditional n-gram models. After incorporating lexical and syntactic information, a mere relative improvement of 1% and 0.4% 8This deficiency could be partly alleviated with some speed-up methods, e.g. using word clustering (Mikolov, 2012) or noise contrastive estimation (Mnih and Kavukcuoglu, 2013). respectively in perplexity and in WER is reported for the resulting WSEM (Rosenfeld et al., 2001). Subsequent studies of </context>
</contexts>
<marker>Amaya, Benedi, 2001</marker>
<rawString>Fredy Amaya and Jos´e Miguel Benedi. 2001. Improvement of a whole sentence maximum entropy language model using grammatical features. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Albert Benveniste</author>
<author>Michel M´etivier</author>
<author>Pierre Priouret</author>
</authors>
<title>Adaptive algorithms and stochastic approximations.</title>
<date>1990</date>
<publisher>Springer.</publisher>
<location>New York:</location>
<marker>Benveniste, M´etivier, Priouret, 1990</marker>
<rawString>Albert Benveniste, Michel M´etivier, and Pierre Priouret. 1990. Adaptive algorithms and stochastic approximations. New York: Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olivier Bousquet</author>
<author>Leon Bottou</author>
</authors>
<title>The tradeoffs of large scale learning.</title>
<date>2008</date>
<booktitle>In NIPS,</booktitle>
<pages>161--168</pages>
<contexts>
<context position="19580" citStr="Bousquet and Bottou, 2008" startWordPosition="3353" endWordPosition="3356"> empirical expectation over the subset This technique is reminiscent of stochastic gradient descent using a random subsample of tr i=H(t−1)i+γH(H(t−21)−H(t−1) i ), i ¯pl[fi]=|B(t) l |−1E(l,xl)∈B(t) l B(t) l |B(t) l|, “minibatch” ˜p[f] D(t). aining data to achieve fast convergence Γ(k, l) = I 3 2, 1 if k = 1, l ∈ [1, 2] or k = m, l ∈ [m − 1, m] 0, otherwise 1,ifk∈ [2,m−1],l ∈ [k−1,k+1] X(t) = (24) = p[f2i ] − �m πl(pl[fi])2 (25) l=1 fi(xl)2 − �m l=1 789 Figure 1: Examples of convergence curves on training set after introducing hessian and training set mini-batching. of optimization algorithms (Bousquet and Bottou, 2008). By combining the two techniques, we revise the updating equation (14) of A to + γλ x max(H(t) i , h) r E(l,xl)∈D(t) fi(xl) — P(l,xl)∈B(t) fi(xl) 1 (28) (Il K K 1 where 0 &lt; h &lt; 1 is a threshold to avoid H(t) i being too small or even zero. Moreover, a constant t, is added to the denominator of (18), to avoid too large adjustment of A, i.e. � 1 if t ≤ t0, tc+tβλ γa = 1 (29) if t &gt; t0. tc+t−t0+tβλ 0 Fig.1(a) shows the result after introducing hessian estimation, and Fig.1(b) shows the effect of training set mini-batching. 4.2 Sampling acceleration For MCMC sampling in Section 3.3, the Gibbs sam</context>
</contexts>
<marker>Bousquet, Bottou, 2008</marker>
<rawString>Olivier Bousquet and Leon Bottou. 2008. The tradeoffs of large scale learning. In NIPS, pages 161–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard H Byrd</author>
<author>SL Hansen</author>
<author>Jorge Nocedal</author>
<author>Yoram Singer</author>
</authors>
<title>A stochastic quasi-newton method for large-scale optimization. arXiv preprint arXiv:1401.7020.</title>
<date>2014</date>
<contexts>
<context position="18330" citStr="Byrd et al., 2014" startWordPosition="3135" endWordPosition="3138">rst j elements of and is the kth element of In (21) and (22), can be flexibly specified as a proper density function in y. In our application, we find the foll X(t−1) 1:j X(t−1) X(t−1) k X(t−1). gk+1(y|xk) owing choice works reasonably well: Algorithm 2 Markov Move The joint SA algorithm may still suffer from 4.1 Improving SA recursion We propose two techniques to effectively improve the convergence of SA recursion. The first technique is to incorporate Hessian information, similarly as in related works on stochastic approximation (Gu and Zhu, 2001) and stochastic gradient descent algorithms (Byrd et al., 2014). But we only use the diagonal elements of the Hessian matrix to re-scale the gradient, due to high-dimensionality of where Hi denotes the ith diagonal element of Hessian matrix. At time t, before updating the parameter λ (Step II in Section 3.2), we compute πl(¯pl[fi])2, (26) H(t) (27) where fi(xl), and is the subset, of size containing all sentences of length l in B(t). The second technique is to introduce the on the training set. At each iteration, a subset D(t) of K sentences are randomly selected from the training set. Then the gradient is approximated with the overall empirical expectati</context>
</contexts>
<marker>Byrd, Hansen, Nocedal, Singer, 2014</marker>
<rawString>Richard H Byrd, SL Hansen, Jorge Nocedal, and Yoram Singer. 2014. A stochastic quasi-newton method for large-scale optimization. arXiv preprint arXiv:1401.7020.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1999</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>13</volume>
<pages>394</pages>
<contexts>
<context position="2490" citStr="Chen and Goodman, 1999" startWordPosition="365" endWordPosition="368">of x is modeled as p(x) = by using the chain rule, l p(x1, ... , xl) = p(xi|x1,...,xi−1). (1) i=1 To avoid degenerate representation of the conditionals, the history of xi, denoted as hi = (x1, · · · , xi−1), is reduced to equivalence classes through a mapping φ(hi) with the assumption p(xi|hi) ≈ p(xi|φ(hi)). (2) Language modeling in this conditional approach consists of finding suitable mappings φ(hi) and effective methods to estimate p(xi|φ(hi)). A classic example is the traditional n-gram LMs with φ(hi) = (xi−n+1, ... , xi−1). Various smoothing techniques are used for parameter estimation (Chen and Goodman, 1999). Recently, neural network LMs, which have begun to surpass the traditional n-gram LMs, also follow the conditional modeling approach, with φ(hi) determined by a neural network (NN), which can be either a feedforward NN (Schwenk, 2007) or a recurrent NN (Mikolov et al., 2011). Remarkably, an alternative approach is used in whole-sentence maximum entropy (WSME) language modeling (Rosenfeld et al., 2001). Specifically, a WSME model has the form: p(x; λ) = Z exp{λT f(x)} 1 (3) Here f(x) is a vector of features, which can be arbitrary computable functions of x, λ is the corresponding parameter vec</context>
</contexts>
<marker>Chen, Goodman, 1999</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1999. An empirical study of smoothing techniques for language modeling. Computer Speech &amp; Language, 13:359– 394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanfu Chen</author>
</authors>
<title>Stochastic approximation and its applications.</title>
<date>2002</date>
<publisher>Springer Science &amp; Business Media.</publisher>
<contexts>
<context position="5106" citStr="Chen, 2002" startWordPosition="767" endWordPosition="768">), and hence the WSME models are in fact poorly fitted to the data. This is one of the reasons for the unsatisfactory results of previous WSME models. In this paper, we propose a new language model, called the trans-dimensional random field (TDRF) model, by explicitly taking account of the empirical distributions of lengths. This formulation subsequently enables us to develop a powerful Markov chain Monte Carlo (MCMC) technique, called trans-dimensional mixture sampling and then propose an effective training algorithm in the framework of stochastic approximation (SA) (Benveniste et al., 1990; Chen, 2002). The SA algorithm involves jointly updating the model parameters and normalization constants, in conjunction with trans-dimensional MCMC sampling. Section 2 and 3 present the model definition and estimation respectively. Furthermore, we make several additional innovations, as detailed in Section 4, to enable successful training of TDRF models. First, the diagonal elements of hessian matrix are estimated during SA iterations to rescale the gradient, which significantly improves the convergence of the SA algorithm. Second, word classing is introduced to accelerate the sampling operation and als</context>
<context position="9930" citStr="Chen, 2002" startWordPosition="1567" endWordPosition="1568"> d, can be any computable function of the sentence xl, such as n-grams. In our current experiments, the features fi(xl) and their corresponding parameters λi are defined to be position-independent and lengthindependent. For example, fi(xl) = Ek fi(xl,k), where fi(xl, k) is a binary function of xl evaluated at position k. As a result, the feature fi(xl) takes values in the non-negative integers. 3 Model Estimation We develop a stochastic approximation algorithm using Markov chain Monte Carlo to estimate the parameters λ and the normalization constants Z1(λ),..., Zm(λ) (Benveniste et al., 1990; Chen, 2002). The core algorithms newly designed in this paper are the joint SA for simultaneously estimating parameters and normalizing constants (Section 3.2) and trans-dimensional mixture sampling (Section 3.3) which is used as Step I of the joint SA. The most relevant previous works that we borrowed from are (Gu and Zhu, 2001) on SA for fitting a single RF, (Tan, 2015) on sampling and 4The length feature corresponding to length l is a binary feature that takes one if the sentence x is of length l, and otherwise takes zero. 3.1 Maximum likelihood estimation Suppose that the training dataset consists of</context>
</contexts>
<marker>Chen, 2002</marker>
<rawString>Hanfu Chen. 2002. Stochastic approximation and its applications. Springer Science &amp; Business Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
</authors>
<title>Shrinking exponential language models.</title>
<date>2009</date>
<booktitle>In Proc. of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="25424" citStr="Chen, 2009" startWordPosition="4419" endWordPosition="4420">The baseline is a 4-gram LM with modified Kneser-Ney smoothing (Chen and Goodman, Type Features w (w−3w−2w−1w0)(w−2w−1w0)(w−1w0)(w0) c (c−3c−2c−1c0)(c−2c−1c0)(c−1c0)(c0) ws (w−3w0)(w−3w−2w0)(w−3w−1w0)(w−2w0) cs (c−3c0)(c−3c−2c0)(c−3c−1c0)(c−2c0) wsh (w−4w0) (w−5w0) csh (c−4c0) (c−5c0) cpw (c−3c−2c−1w0) (c−2c−1w0)(c−1w0) Table 1: Feature definition in TDRF LMs 1999), denoted by KN4. We use the RNNLM toolkit5 to train a RNNLM (Mikolov et al., 2011). The number of hidden units is 250 and other configurations are set by default6. Word classing has been shown to be useful in conditional ME models (Chen, 2009). For our TDRF models, we consider a variety of features as shown in Table 1, mainly based on word and class information. Each word is deterministically assigned to a single class, by running the automatic clustering algorithm proposed in (Martin et al., 1998) on the training data. In Table 1, wi, ci, i = 0, −1, ... , −5 denote the word and its class at different position offset i, e.g. w0, c0 denotes the current word and its class. We first introduce the classic word/class n-gram features (denoted by “w”/“c”) and the word/class skipping n-gram features (denoted by “ws”/“cs”) (Goodman, 2001a).</context>
</contexts>
<marker>Chen, 2009</marker>
<rawString>Stanley F. Chen. 2009. Shrinking exponential language models. In Proc. of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>A bit of progress in language modeling.</title>
<date>2001</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>15</volume>
<pages>434</pages>
<contexts>
<context position="21860" citStr="Goodman, 2001" startWordPosition="3792" endWordPosition="3793">bability (Eq.21) and (Eq.32) 9: end if 10: if j = k - 1 then 11: set L(t) = j and X(t) = X(t−1) 1:k−1 with probability Eq.(22) and (Eq.32) 12: end if Step II. Markov move 13: for i = 1 --+ L(t) do 14: draw C Qi(c) 15: set ci(t) = C with probability (Eq.30) 16: draw W E V(t) ci 17: set X(t) i= W 18: end for 19: return (L(t), X(t)) 20: end function belonging to class c(t) i . The computational cost is reduced from |V |to |C |+ |V|/|C |on average. The idea of using class information to accelerate training has been proposed in various contexts of language modeling, such as maximum entropy models (Goodman, 2001b) and RNN LMs (Mikolov et al., 2011). However, the realization of this idea is different for training our models. The pseudo-code of the new sampling method is shown in Algorithm 3. Denote by V, the subset of V containing all the words belonging to class c. In the Markov move step (Step 13 to 18 in Algorithm 3), at each position i, we first generate a class C from a proposal distribution Qi(c) and then accept C as the new c(t) i with probability ( ) 1, Qi(c(t) i ) pi(C) min (30) Qi(C) pi(c(t) i ) where p(L(t), {X(t) �pi(c) = 1:i−1, w, X(t)i+1:L(t)}; A, ζ). wEVc The probabilities Qi(c) and pi(</context>
<context position="26021" citStr="Goodman, 2001" startWordPosition="4522" endWordPosition="4523">els (Chen, 2009). For our TDRF models, we consider a variety of features as shown in Table 1, mainly based on word and class information. Each word is deterministically assigned to a single class, by running the automatic clustering algorithm proposed in (Martin et al., 1998) on the training data. In Table 1, wi, ci, i = 0, −1, ... , −5 denote the word and its class at different position offset i, e.g. w0, c0 denotes the current word and its class. We first introduce the classic word/class n-gram features (denoted by “w”/“c”) and the word/class skipping n-gram features (denoted by “ws”/“cs”) (Goodman, 2001a). Second, to demonstrate that long-span features can be naturally integrated in TDRFs, we introduce higher-order features “wsh”/“csh”, by considering two words/classes separated with longer distance. Third, as an example of supporting heterogenous features that combine different information, the crossing features “cpw” (meaning class-predict-word) are introduced. Note that for all the feature types in Table 1, only the features observed in the training data are used. The joint SA (Algorithm 1) is used to train the TDRF models, with all the acceleration methods described in Section 4 applied.</context>
<context position="32631" citStr="Goodman, 2001" startWordPosition="5595" endWordPosition="5596">RF models in Table 3. This is some evidence for the model deficiency of the WSME distribution as defined in (3), and introducing the empirical length probabilities gives a more reasonable model assumption. TDRF vs conditional ME. After training, TDRF models are computationally more efficient in computing sentence probability, simply summing up weights for the activated features in the sentence. The conditional ME models (Khudanpur and Wu, 2000; Roark et al., 2004) suffer from the expensive computation of local normalization factors. This computational bottleneck hinders their use in practice (Goodman, 2001b; Rosenfeld et al., 2001). Partly for this reason, although building conditional ME models with sophisticated features as in Table 1 is theoretically possible, such work has not been pursued so far. TDRF vs RNN. The RNN models suffer from the expensive softmax computation in the output layer 8. Empirically in our experiments, the average time costs for re-ranking of the 1000-best list for a sentence are 0.16 sec vs 40 sec, based on TDRF and RNN respectively (no GPU used). 6 Related Work While there has been extensive research on conditional LMs, there has been little work on the whole-sentenc</context>
</contexts>
<marker>Goodman, 2001</marker>
<rawString>Joshua Goodman. 2001a. A bit of progress in language modeling. Computer Speech &amp; Language, 15:403– 434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Classes for fast maximum entropy training.</title>
<date>2001</date>
<booktitle>In Proc. of International Conference on Acoustics, Speech, and Signal Processing (ICASSP).</booktitle>
<contexts>
<context position="21860" citStr="Goodman, 2001" startWordPosition="3792" endWordPosition="3793">bability (Eq.21) and (Eq.32) 9: end if 10: if j = k - 1 then 11: set L(t) = j and X(t) = X(t−1) 1:k−1 with probability Eq.(22) and (Eq.32) 12: end if Step II. Markov move 13: for i = 1 --+ L(t) do 14: draw C Qi(c) 15: set ci(t) = C with probability (Eq.30) 16: draw W E V(t) ci 17: set X(t) i= W 18: end for 19: return (L(t), X(t)) 20: end function belonging to class c(t) i . The computational cost is reduced from |V |to |C |+ |V|/|C |on average. The idea of using class information to accelerate training has been proposed in various contexts of language modeling, such as maximum entropy models (Goodman, 2001b) and RNN LMs (Mikolov et al., 2011). However, the realization of this idea is different for training our models. The pseudo-code of the new sampling method is shown in Algorithm 3. Denote by V, the subset of V containing all the words belonging to class c. In the Markov move step (Step 13 to 18 in Algorithm 3), at each position i, we first generate a class C from a proposal distribution Qi(c) and then accept C as the new c(t) i with probability ( ) 1, Qi(c(t) i ) pi(C) min (30) Qi(C) pi(c(t) i ) where p(L(t), {X(t) �pi(c) = 1:i−1, w, X(t)i+1:L(t)}; A, ζ). wEVc The probabilities Qi(c) and pi(</context>
<context position="26021" citStr="Goodman, 2001" startWordPosition="4522" endWordPosition="4523">els (Chen, 2009). For our TDRF models, we consider a variety of features as shown in Table 1, mainly based on word and class information. Each word is deterministically assigned to a single class, by running the automatic clustering algorithm proposed in (Martin et al., 1998) on the training data. In Table 1, wi, ci, i = 0, −1, ... , −5 denote the word and its class at different position offset i, e.g. w0, c0 denotes the current word and its class. We first introduce the classic word/class n-gram features (denoted by “w”/“c”) and the word/class skipping n-gram features (denoted by “ws”/“cs”) (Goodman, 2001a). Second, to demonstrate that long-span features can be naturally integrated in TDRFs, we introduce higher-order features “wsh”/“csh”, by considering two words/classes separated with longer distance. Third, as an example of supporting heterogenous features that combine different information, the crossing features “cpw” (meaning class-predict-word) are introduced. Note that for all the feature types in Table 1, only the features observed in the training data are used. The joint SA (Algorithm 1) is used to train the TDRF models, with all the acceleration methods described in Section 4 applied.</context>
<context position="32631" citStr="Goodman, 2001" startWordPosition="5595" endWordPosition="5596">RF models in Table 3. This is some evidence for the model deficiency of the WSME distribution as defined in (3), and introducing the empirical length probabilities gives a more reasonable model assumption. TDRF vs conditional ME. After training, TDRF models are computationally more efficient in computing sentence probability, simply summing up weights for the activated features in the sentence. The conditional ME models (Khudanpur and Wu, 2000; Roark et al., 2004) suffer from the expensive computation of local normalization factors. This computational bottleneck hinders their use in practice (Goodman, 2001b; Rosenfeld et al., 2001). Partly for this reason, although building conditional ME models with sophisticated features as in Table 1 is theoretically possible, such work has not been pursued so far. TDRF vs RNN. The RNN models suffer from the expensive softmax computation in the output layer 8. Empirically in our experiments, the average time costs for re-ranking of the 1000-best list for a sentence are 0.16 sec vs 40 sec, based on TDRF and RNN respectively (no GPU used). 6 Related Work While there has been extensive research on conditional LMs, there has been little work on the whole-sentenc</context>
</contexts>
<marker>Goodman, 2001</marker>
<rawString>Joshua Goodman. 2001b. Classes for fast maximum entropy training. In Proc. of International Conference on Acoustics, Speech, and Signal Processing (ICASSP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter J Green</author>
</authors>
<title>Reversible jump markov chain monte carlo computation and bayesian model determination.</title>
<date>1995</date>
<journal>Biometrika,</journal>
<pages>82--711</pages>
<contexts>
<context position="8295" citStr="Green, 1995" startWordPosition="1295" endWordPosition="1296">random field (TDRF). Different from the WSME model (3), a crucial aspect of the TDRF model (6) is that the mixture weights πl can be set to the empirical length probabilities in the training data. The WSME 2http://oa.ee.tsinghua.edu.cn/ ˜ouzhijian/software.htm 3We add sup or subscript l, e.g. in xl, pl(), to make clear that the variables and distributions depend on length l. , (4) 786 model (3) is essentially also a mixture of RFs, but the mixture weights implied are proportional to the normalizing constants Zl(λ): estimating normalizing constants from multiple RFs of the same dimension, and (Green, 1995) on trans-dimensional MCMC. p(l,xl; λ) = Zl(λ) Zl(λ)eλT f(xl), (7) 1 Z(λ) where Z(λ) = Eml=1 Zl(λ). A motivation for proposing (6) is that it is very difficult to sample from (3), namely (7), as a mixture distribution with unknown weights which typically differ from each other by orders of magnitudes, e.g. 1040 or more in our experiments. Setting mixture weights to the known, empirical length probabilities enables us to develop a very effective learning algorithm, as introduced in Section 3. Basically, the empirical weights serve as a control device to improve sampling from multiple distributi</context>
<context position="15547" citStr="Green, 1995" startWordPosition="2600" endWordPosition="2601">no more significant adjustment observed in the first stage. 3.3 Trans-dimensional mixture sampling We describe a trans-dimensional mixture sampling algorithm to simulate from the joint distribution p(l, xl; λ, ζ), which is used with (λ, ζ) = (λ(t−1), ζ(t−1)) at time t for MCMC sampling in the joint SA algorithm. The name “mixture sampling” reflects the fact that p(l, xl; λ, ζ) represents a labeled mixture, because l is a label indicating that xl is associated with the distribution pl(xl; ζ). With fixed (λ, ζ), this sampling algorithm can be seen as formally equivalent to reversible jump MCMC (Green, 1995), which was originally proposed for Bayes model determination. The trans-dimensional mixture sampling algorithm consists of two steps at each time t: local jump between lengths and Markov move of sentences for a given length. In the following, we denote by L(t−1) and X(t−1) the length and sequence 788 before sampling, but use the short notation (λ, ζ) for (λ(t−1), ζ(t−1)). Step I: Local jump. The Metropolis-Hastings method is used in this step to sample the length. Assuming L(t−1) = k, first we draw a new length j ∼ Γ(k, ·). The jump distribution Γ(k, l) is defined to be uniform at the neighbo</context>
</contexts>
<marker>Green, 1995</marker>
<rawString>Peter J. Green. 1995. Reversible jump markov chain monte carlo computation and bayesian model determination. Biometrika, 82:711–732.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ming Gao Gu</author>
<author>Hong-Tu Zhu</author>
</authors>
<title>Maximum likelihood estimation for spatial models by markov chain monte carlo stochastic approximation.</title>
<date>2001</date>
<journal>Journal of the Royal Statistical Society: Series B (Statistical Methodology),</journal>
<pages>63--339</pages>
<contexts>
<context position="10250" citStr="Gu and Zhu, 2001" startWordPosition="1617" endWordPosition="1620">ition k. As a result, the feature fi(xl) takes values in the non-negative integers. 3 Model Estimation We develop a stochastic approximation algorithm using Markov chain Monte Carlo to estimate the parameters λ and the normalization constants Z1(λ),..., Zm(λ) (Benveniste et al., 1990; Chen, 2002). The core algorithms newly designed in this paper are the joint SA for simultaneously estimating parameters and normalizing constants (Section 3.2) and trans-dimensional mixture sampling (Section 3.3) which is used as Step I of the joint SA. The most relevant previous works that we borrowed from are (Gu and Zhu, 2001) on SA for fitting a single RF, (Tan, 2015) on sampling and 4The length feature corresponding to length l is a binary feature that takes one if the sentence x is of length l, and otherwise takes zero. 3.1 Maximum likelihood estimation Suppose that the training dataset consists of nl sentences of length l for l = 1, ... , m. First, the maximum likelihood estimate of the length probability πl is easily shown to be nl/n, where n = Eml=1 nl. By abuse of notation, we set πl = nl/n hereafter. Next, the log-likelihood of λ given the empirical length probabilities is log pl(xl; λ), (8) where Dl is the</context>
<context position="12671" citStr="Gu and Zhu, 2001" startWordPosition="2060" endWordPosition="2063">0) = (0, ... , 0)T and ζ(0) = ζ∗(λ(0)) − ζ∗1 (λ(0)) 2: fort = 1, 2,. . . , tmax do 3: set B(t) = 0 4: set (L(t,0), X(t,0)) = (L(t−1,K), X(t−1,K)) Step L: MCMC sampling 5: for k = 1 --+ K do 6: sampling (See Algorithm 3) (L(t,k), X(t,k)) = SAMPLE(L(t,k−1), X(t,k−1)) 7: set B(t) = B(t) U {(L(t,k), X(t,k))} 8: end for Step LL: SA updating 9: Compute λ(t) based on (14) 10: Compute ζ(t) based on (15) and (16) 11: end for where Z1(λ) is chosen as the reference value and can be calculated exactly. The algorithm can be obtained by combining the standard SA algorithm for training single random fields (Gu and Zhu, 2001) and a trans-dimensional extension of the self-adjusted mixture sampling algorithm (Tan, 2015). Specifically, consider the following joint distribution of the pair (l, xl): p(l, xl; λ, ζ) a πl eζl eλT f(xl), (13) where πl is set to nl/n for l = 1, ... , m, but ζ = (ζ1, ... , ζm)T with ζ1 = 0 are hypothesized values of the truth ζ∗(λ) = (ζ∗1(λ), ... , ζ∗m(λ))T with ζ∗1(λ) = 0. The distribution p(l, xl; λ,ζ) reduces to p(l, xl; λ) in (6) if ζ were identical to ζ∗(λ). In general, p(l, xl; λ,ζ) differs from p(l, xl; λ) in that the marginal probability of length l is not necessarily πl. The joint S</context>
<context position="18267" citStr="Gu and Zhu, 2001" startWordPosition="3126" endWordPosition="3129">) Hi = −dλ2 i K H(t−21)1 � =i (l,xl)∈B(t) (22) where is the first j elements of and is the kth element of In (21) and (22), can be flexibly specified as a proper density function in y. In our application, we find the foll X(t−1) 1:j X(t−1) X(t−1) k X(t−1). gk+1(y|xk) owing choice works reasonably well: Algorithm 2 Markov Move The joint SA algorithm may still suffer from 4.1 Improving SA recursion We propose two techniques to effectively improve the convergence of SA recursion. The first technique is to incorporate Hessian information, similarly as in related works on stochastic approximation (Gu and Zhu, 2001) and stochastic gradient descent algorithms (Byrd et al., 2014). But we only use the diagonal elements of the Hessian matrix to re-scale the gradient, due to high-dimensionality of where Hi denotes the ith diagonal element of Hessian matrix. At time t, before updating the parameter λ (Step II in Section 3.2), we compute πl(¯pl[fi])2, (26) H(t) (27) where fi(xl), and is the subset, of size containing all sentences of length l in B(t). The second technique is to introduce the on the training set. At each iteration, a subset D(t) of K sentences are randomly selected from the training set. Then th</context>
<context position="14332" citStr="Gu &amp; Zhu (2001)" startWordPosition="2369" endWordPosition="2372">1 where γζ is a learning rate of ζ, and δl(B(t)) is the relative frequency of length l appearing in B(t): δl(B(t)) = E(j,xj)∈B(t) 1(j = l). (17) K The rationale in (15) is to adjust ζ based on how the relative frequencies of lengths δl(B(t)) are compared with the desired length probabilities πl. Intuitively, if the relative frequency of some length l in the sample set B(t) is greater (or respectively smaller) than the desired length probability πl, then the hypothesized value ζ(t−1) l is an underestimate (or overestimate) of ζ∗l (λ(t−1)) and hence should be increased (or decreased). Following Gu &amp; Zhu (2001) and Tan (2015), we set the learning rates in two stages: � t−βλ if t &lt; t0 γλ = 1(18) if t &gt; t0 t−t0+tβλ 0 � (0.1t)−βζ if t &lt; t0 γζ = 1 if t &gt; t0 (19) 0.1(t−t0)+(0.1t0)βζ where 0.5 &lt; βλ, βζ &lt; 1. In the first stage (t &lt; t0), a slow-decaying rate of t−β is used to introduce large adjustments. This forces the estimates λ(t) and ζ(t) to fall reasonably fast into the true values. In the second stage (t &gt; t0), a fast-decaying rate of t−1 is used. The iteration number t is multiplied by 0.1 in (19), to make the the learning rate of ζ decay more slowly than λ. Commonly, t0 is selected to ensure there </context>
</contexts>
<marker>Gu, Zhu, 2001</marker>
<rawString>Ming Gao Gu and Hong-Tu Zhu. 2001. Maximum likelihood estimation for spatial models by markov chain monte carlo stochastic approximation. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 63:339–355.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanjeev Khudanpur</author>
<author>Jun Wu</author>
</authors>
<title>Maximum entropy techniques for exploiting syntactic, semantic and collocational dependencies in language modeling.</title>
<date>2000</date>
<journal>Computer Speech &amp; Language,</journal>
<pages>14--355</pages>
<contexts>
<context position="32465" citStr="Khudanpur and Wu, 2000" startWordPosition="5569" endWordPosition="5572">lihoods per sentence for these two sets of parameters are improved to be −152 and −119 respectively. The average test log-likelihoods are both −96 for the two corresponding TDRF models in Table 3. This is some evidence for the model deficiency of the WSME distribution as defined in (3), and introducing the empirical length probabilities gives a more reasonable model assumption. TDRF vs conditional ME. After training, TDRF models are computationally more efficient in computing sentence probability, simply summing up weights for the activated features in the sentence. The conditional ME models (Khudanpur and Wu, 2000; Roark et al., 2004) suffer from the expensive computation of local normalization factors. This computational bottleneck hinders their use in practice (Goodman, 2001b; Rosenfeld et al., 2001). Partly for this reason, although building conditional ME models with sophisticated features as in Table 1 is theoretically possible, such work has not been pursued so far. TDRF vs RNN. The RNN models suffer from the expensive softmax computation in the output layer 8. Empirically in our experiments, the average time costs for re-ranking of the 1000-best list for a sentence are 0.16 sec vs 40 sec, based </context>
</contexts>
<marker>Khudanpur, Wu, 2000</marker>
<rawString>Sanjeev Khudanpur and Jun Wu. 2000. Maximum entropy techniques for exploiting syntactic, semantic and collocational dependencies in language modeling. Computer Speech &amp; Language, 14:355–372.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daphne Koller</author>
<author>Nir Friedman</author>
</authors>
<title>Probabilistic graphical models: principles and techniques.</title>
<date>2009</date>
<publisher>MIT press.</publisher>
<contexts>
<context position="34272" citStr="Koller and Friedman, 2009" startWordPosition="5853" endWordPosition="5856">ome speed-up methods, e.g. using word clustering (Mikolov, 2012) or noise contrastive estimation (Mnih and Kavukcuoglu, 2013). respectively in perplexity and in WER is reported for the resulting WSEM (Rosenfeld et al., 2001). Subsequent studies of using WSEMs with grammatical features, as in (Amaya and Benedi, 2001) and (Ruokolainen et al., 2010), report perplexity improvement above 10% but no WER improvement when using WSEMs alone. Most RF modeling has been restricted to fixeddimensional spaces 9. Despite recent progress, fitting RFs of moderate or large dimensions remains to be challenging (Koller and Friedman, 2009; Mizrahi et al., 2013). In particular, the work of (Pietra et al., 1997) is inspiring to us, but the improved iterative scaling (IIS) method for parameter estimation and the Gibbs sampler are not suitable for even moderately sized models. Our TDRF model, together with the joint SA algorithm and trans-dimensional mixture sampling, are brand new and lead to encouraging results for language modeling. 7 Conclusion In summary, we have made the following contributions, which enable us to successfully train TDRF models and obtain encouraging performance improvement. • The new TDRF model and the join</context>
</contexts>
<marker>Koller, Friedman, 2009</marker>
<rawString>Daphne Koller and Nir Friedman. 2009. Probabilistic graphical models: principles and techniques. MIT press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Faming Liang</author>
<author>Chuanhai Liu</author>
<author>Raymond J Carroll</author>
</authors>
<title>Stochastic approximation in monte carlo computation.</title>
<date>2007</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>102</volume>
<issue>477</issue>
<contexts>
<context position="8918" citStr="Liang et al., 2007" startWordPosition="1394" endWordPosition="1397">rans-dimensional MCMC. p(l,xl; λ) = Zl(λ) Zl(λ)eλT f(xl), (7) 1 Z(λ) where Z(λ) = Eml=1 Zl(λ). A motivation for proposing (6) is that it is very difficult to sample from (3), namely (7), as a mixture distribution with unknown weights which typically differ from each other by orders of magnitudes, e.g. 1040 or more in our experiments. Setting mixture weights to the known, empirical length probabilities enables us to develop a very effective learning algorithm, as introduced in Section 3. Basically, the empirical weights serve as a control device to improve sampling from multiple distributions (Liang et al., 2007; Tan, 2015) . Second, it can be shown that if we incorporate the length features 4 in the vector of features f(x) in (3), then the distribution p(x; λ) in (3) under the maximum entropy (ME) principle will take the form of (6) and the probabilities (π1, ... , πm) in (6) implied by the parameters for the length features are exactly the empirical length probabilities. Third, a feature fi(xl), 1 ≤ i ≤ d, can be any computable function of the sentence xl, such as n-grams. In our current experiments, the features fi(xl) and their corresponding parameters λi are defined to be position-independent an</context>
</contexts>
<marker>Liang, Liu, Carroll, 2007</marker>
<rawString>Faming Liang, Chuanhai Liu, and Raymond J Carroll. 2007. Stochastic approximation in monte carlo computation. Journal of the American Statistical Association, 102(477):305–320.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sven Martin</author>
<author>J¨org Liermann</author>
<author>Hermann Ney</author>
</authors>
<title>Algorithms for bigram and trigram word clustering. Speech Communication,</title>
<date>1998</date>
<pages>24--19</pages>
<contexts>
<context position="25684" citStr="Martin et al., 1998" startWordPosition="4461" endWordPosition="4464">w−4w0) (w−5w0) csh (c−4c0) (c−5c0) cpw (c−3c−2c−1w0) (c−2c−1w0)(c−1w0) Table 1: Feature definition in TDRF LMs 1999), denoted by KN4. We use the RNNLM toolkit5 to train a RNNLM (Mikolov et al., 2011). The number of hidden units is 250 and other configurations are set by default6. Word classing has been shown to be useful in conditional ME models (Chen, 2009). For our TDRF models, we consider a variety of features as shown in Table 1, mainly based on word and class information. Each word is deterministically assigned to a single class, by running the automatic clustering algorithm proposed in (Martin et al., 1998) on the training data. In Table 1, wi, ci, i = 0, −1, ... , −5 denote the word and its class at different position offset i, e.g. w0, c0 denotes the current word and its class. We first introduce the classic word/class n-gram features (denoted by “w”/“c”) and the word/class skipping n-gram features (denoted by “ws”/“cs”) (Goodman, 2001a). Second, to demonstrate that long-span features can be naturally integrated in TDRFs, we introduce higher-order features “wsh”/“csh”, by considering two words/classes separated with longer distance. Third, as an example of supporting heterogenous features that</context>
</contexts>
<marker>Martin, Liermann, Ney, 1998</marker>
<rawString>Sven Martin, J¨org Liermann, and Hermann Ney. 1998. Algorithms for bigram and trigram word clustering. Speech Communication, 24:19–37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Stefan Kombrink</author>
<author>Lukas Burget</author>
<author>Jan H Cernocky</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Extensions of recurrent neural network language model.</title>
<date>2011</date>
<booktitle>In Proc. of International Conference on Acoustics, Speech and Signal Processing (ICASSP).</booktitle>
<contexts>
<context position="2766" citStr="Mikolov et al., 2011" startWordPosition="409" endWordPosition="412">sumption p(xi|hi) ≈ p(xi|φ(hi)). (2) Language modeling in this conditional approach consists of finding suitable mappings φ(hi) and effective methods to estimate p(xi|φ(hi)). A classic example is the traditional n-gram LMs with φ(hi) = (xi−n+1, ... , xi−1). Various smoothing techniques are used for parameter estimation (Chen and Goodman, 1999). Recently, neural network LMs, which have begun to surpass the traditional n-gram LMs, also follow the conditional modeling approach, with φ(hi) determined by a neural network (NN), which can be either a feedforward NN (Schwenk, 2007) or a recurrent NN (Mikolov et al., 2011). Remarkably, an alternative approach is used in whole-sentence maximum entropy (WSME) language modeling (Rosenfeld et al., 2001). Specifically, a WSME model has the form: p(x; λ) = Z exp{λT f(x)} 1 (3) Here f(x) is a vector of features, which can be arbitrary computable functions of x, λ is the corresponding parameter vector, and Z is the global normalization constant. Although WSME models have the potential benefits of being able to naturally express sentence-level phenomena and integrate features from a variety of knowledge p(x&apos;)p((EOS)1x1), where (EOS) is a special token placed at the end </context>
<context position="6085" citStr="Mikolov et al., 2011" startWordPosition="912" endWordPosition="915">diagonal elements of hessian matrix are estimated during SA iterations to rescale the gradient, which significantly improves the convergence of the SA algorithm. Second, word classing is introduced to accelerate the sampling operation and also improve the smoothing behavior of the models through sharing statistical strength between similar words. Finally, multiple CPUs are used to parallelize the training of our RF models. In Section 5, speech recognition experiments are conducted to evaluate our TDRF LMs, compared with the traditional 4-gram LMs and the recurrent neural network LMs (RNNLMs) (Mikolov et al., 2011) which have emerged as a new stateof-art of language modeling. We explore the use of a variety of features based on word and class information in TDRF LMs. In terms of word error rates (WERs) for speech recognition, our TDRF LMs alone can outperform the KN-smoothing 4- gram LM with 9.1% relative reduction, and perform comparably to the RNNLM with a slight 0.5% relative reduction. To our knowledge, this result represents the first strong empirical evidence supporting the power of using the whole-sentence language modeling approach. Our open-source TDRF toolkit is released publicly 2. 2 Model De</context>
<context position="21897" citStr="Mikolov et al., 2011" startWordPosition="3797" endWordPosition="3800">9: end if 10: if j = k - 1 then 11: set L(t) = j and X(t) = X(t−1) 1:k−1 with probability Eq.(22) and (Eq.32) 12: end if Step II. Markov move 13: for i = 1 --+ L(t) do 14: draw C Qi(c) 15: set ci(t) = C with probability (Eq.30) 16: draw W E V(t) ci 17: set X(t) i= W 18: end for 19: return (L(t), X(t)) 20: end function belonging to class c(t) i . The computational cost is reduced from |V |to |C |+ |V|/|C |on average. The idea of using class information to accelerate training has been proposed in various contexts of language modeling, such as maximum entropy models (Goodman, 2001b) and RNN LMs (Mikolov et al., 2011). However, the realization of this idea is different for training our models. The pseudo-code of the new sampling method is shown in Algorithm 3. Denote by V, the subset of V containing all the words belonging to class c. In the Markov move step (Step 13 to 18 in Algorithm 3), at each position i, we first generate a class C from a proposal distribution Qi(c) and then accept C as the new c(t) i with probability ( ) 1, Qi(c(t) i ) pi(C) min (30) Qi(C) pi(c(t) i ) where p(L(t), {X(t) �pi(c) = 1:i−1, w, X(t)i+1:L(t)}; A, ζ). wEVc The probabilities Qi(c) and pi(c) depend on {X(t) 1:i−1, X(t) i+1:L(</context>
<context position="24811" citStr="Mikolov et al., 2011" startWordPosition="4333" endWordPosition="4336"> on each CPU core. By parallelization, the sampling operation is completed J times faster than before. 5 Experiments 5.1 PTB perplexity results In this section, we evaluate the performance of LMs by perplexity (PPL). We use the Wall Street Journal (WSJ) portion of Penn Treebank (PTB). Sections 0-20 are used as the training data (about 930K words), sections 21-22 as the development data (74K) and section 23-24 as the test data (82K). The vocabulary is limited to 10K words, with one special token (UNK) denoting words not in the vocabulary. This setting is the same as that used in other studies (Mikolov et al., 2011). The baseline is a 4-gram LM with modified Kneser-Ney smoothing (Chen and Goodman, Type Features w (w−3w−2w−1w0)(w−2w−1w0)(w−1w0)(w0) c (c−3c−2c−1c0)(c−2c−1c0)(c−1c0)(c0) ws (w−3w0)(w−3w−2w0)(w−3w−1w0)(w−2w0) cs (c−3c0)(c−3c−2c0)(c−3c−1c0)(c−2c0) wsh (w−4w0) (w−5w0) csh (c−4c0) (c−5c0) cpw (c−3c−2c−1w0) (c−2c−1w0)(c−1w0) Table 1: Feature definition in TDRF LMs 1999), denoted by KN4. We use the RNNLM toolkit5 to train a RNNLM (Mikolov et al., 2011). The number of hidden units is 250 and other configurations are set by default6. Word classing has been shown to be useful in conditional ME models</context>
</contexts>
<marker>Mikolov, Kombrink, Burget, Cernocky, Khudanpur, 2011</marker>
<rawString>Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan H Cernocky, and Sanjeev Khudanpur. 2011. Extensions of recurrent neural network language model. In Proc. of International Conference on Acoustics, Speech and Signal Processing (ICASSP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom´aˇs Mikolov</author>
</authors>
<title>Statistical language models based on neural networks.</title>
<date>2012</date>
<tech>Ph.D. thesis,</tech>
<institution>Brno University of Technology.</institution>
<contexts>
<context position="33711" citStr="Mikolov, 2012" startWordPosition="5767" endWordPosition="5768"> GPU used). 6 Related Work While there has been extensive research on conditional LMs, there has been little work on the whole-sentence LMs, mainly in (Rosenfeld et al., 2001; Amaya and Benedi, 2001; Ruokolainen et al., 2010). Although the whole-sentence approach has potential benefits, the empirical results of previous WSME models are not satisfactory, almost the same as traditional n-gram models. After incorporating lexical and syntactic information, a mere relative improvement of 1% and 0.4% 8This deficiency could be partly alleviated with some speed-up methods, e.g. using word clustering (Mikolov, 2012) or noise contrastive estimation (Mnih and Kavukcuoglu, 2013). respectively in perplexity and in WER is reported for the resulting WSEM (Rosenfeld et al., 2001). Subsequent studies of using WSEMs with grammatical features, as in (Amaya and Benedi, 2001) and (Ruokolainen et al., 2010), report perplexity improvement above 10% but no WER improvement when using WSEMs alone. Most RF modeling has been restricted to fixeddimensional spaces 9. Despite recent progress, fitting RFs of moderate or large dimensions remains to be challenging (Koller and Friedman, 2009; Mizrahi et al., 2013). In particular,</context>
</contexts>
<marker>Mikolov, 2012</marker>
<rawString>Tom´aˇs Mikolov. 2012. Statistical language models based on neural networks. Ph.D. thesis, Brno University of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yariv Dror Mizrahi</author>
<author>Misha Denil</author>
<author>Nando de Freitas</author>
</authors>
<title>Linear and parallel learning of markov random fields. arXiv preprint arXiv:1308.6342.</title>
<date>2013</date>
<marker>Mizrahi, Denil, de Freitas, 2013</marker>
<rawString>Yariv Dror Mizrahi, Misha Denil, and Nando de Freitas. 2013. Linear and parallel learning of markov random fields. arXiv preprint arXiv:1308.6342.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Koray Kavukcuoglu</author>
</authors>
<title>Learning word embeddings efficiently with noise-contrastive estimation.</title>
<date>2013</date>
<booktitle>In Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="33772" citStr="Mnih and Kavukcuoglu, 2013" startWordPosition="5773" endWordPosition="5776">extensive research on conditional LMs, there has been little work on the whole-sentence LMs, mainly in (Rosenfeld et al., 2001; Amaya and Benedi, 2001; Ruokolainen et al., 2010). Although the whole-sentence approach has potential benefits, the empirical results of previous WSME models are not satisfactory, almost the same as traditional n-gram models. After incorporating lexical and syntactic information, a mere relative improvement of 1% and 0.4% 8This deficiency could be partly alleviated with some speed-up methods, e.g. using word clustering (Mikolov, 2012) or noise contrastive estimation (Mnih and Kavukcuoglu, 2013). respectively in perplexity and in WER is reported for the resulting WSEM (Rosenfeld et al., 2001). Subsequent studies of using WSEMs with grammatical features, as in (Amaya and Benedi, 2001) and (Ruokolainen et al., 2010), report perplexity improvement above 10% but no WER improvement when using WSEMs alone. Most RF modeling has been restricted to fixeddimensional spaces 9. Despite recent progress, fitting RFs of moderate or large dimensions remains to be challenging (Koller and Friedman, 2009; Mizrahi et al., 2013). In particular, the work of (Pietra et al., 1997) is inspiring to us, but th</context>
</contexts>
<marker>Mnih, Kavukcuoglu, 2013</marker>
<rawString>Andriy Mnih and Koray Kavukcuoglu. 2013. Learning word embeddings efficiently with noise-contrastive estimation. In Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
<author>John Lafferty</author>
</authors>
<title>Inducing features of random fields.</title>
<date>1997</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>19--380</pages>
<contexts>
<context position="34345" citStr="Pietra et al., 1997" startWordPosition="5866" endWordPosition="5869">rastive estimation (Mnih and Kavukcuoglu, 2013). respectively in perplexity and in WER is reported for the resulting WSEM (Rosenfeld et al., 2001). Subsequent studies of using WSEMs with grammatical features, as in (Amaya and Benedi, 2001) and (Ruokolainen et al., 2010), report perplexity improvement above 10% but no WER improvement when using WSEMs alone. Most RF modeling has been restricted to fixeddimensional spaces 9. Despite recent progress, fitting RFs of moderate or large dimensions remains to be challenging (Koller and Friedman, 2009; Mizrahi et al., 2013). In particular, the work of (Pietra et al., 1997) is inspiring to us, but the improved iterative scaling (IIS) method for parameter estimation and the Gibbs sampler are not suitable for even moderately sized models. Our TDRF model, together with the joint SA algorithm and trans-dimensional mixture sampling, are brand new and lead to encouraging results for language modeling. 7 Conclusion In summary, we have made the following contributions, which enable us to successfully train TDRF models and obtain encouraging performance improvement. • The new TDRF model and the joint SA training algorithm, which simultaneously updates the model parameter</context>
</contexts>
<marker>Pietra, Pietra, Lafferty, 1997</marker>
<rawString>Stephen Della Pietra, Vincent Della Pietra, and John Lafferty. 1997. Inducing features of random fields. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19:380–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Murat Saraclar</author>
<author>Michael Collins</author>
<author>Mark Johnson</author>
</authors>
<title>Discriminative language modeling with conditional random fields and the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics (ACL),</booktitle>
<pages>47</pages>
<contexts>
<context position="32486" citStr="Roark et al., 2004" startWordPosition="5573" endWordPosition="5576"> these two sets of parameters are improved to be −152 and −119 respectively. The average test log-likelihoods are both −96 for the two corresponding TDRF models in Table 3. This is some evidence for the model deficiency of the WSME distribution as defined in (3), and introducing the empirical length probabilities gives a more reasonable model assumption. TDRF vs conditional ME. After training, TDRF models are computationally more efficient in computing sentence probability, simply summing up weights for the activated features in the sentence. The conditional ME models (Khudanpur and Wu, 2000; Roark et al., 2004) suffer from the expensive computation of local normalization factors. This computational bottleneck hinders their use in practice (Goodman, 2001b; Rosenfeld et al., 2001). Partly for this reason, although building conditional ME models with sophisticated features as in Table 1 is theoretically possible, such work has not been pursued so far. TDRF vs RNN. The RNN models suffer from the expensive softmax computation in the output layer 8. Empirically in our experiments, the average time costs for re-ranking of the 1000-best list for a sentence are 0.16 sec vs 40 sec, based on TDRF and RNN respe</context>
</contexts>
<marker>Roark, Saraclar, Collins, Johnson, 2004</marker>
<rawString>Brian Roark, Murat Saraclar, Michael Collins, and Mark Johnson. 2004. Discriminative language modeling with conditional random fields and the perceptron algorithm. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics (ACL), page 47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Rosenfeld</author>
<author>Stanley F Chen</author>
<author>Xiaojin Zhu</author>
</authors>
<title>Whole-sentence exponential language models: a vehicle for linguistic-statistical integration.</title>
<date>2001</date>
<journal>Computer Speech &amp; Language,</journal>
<pages>15--55</pages>
<contexts>
<context position="2895" citStr="Rosenfeld et al., 2001" startWordPosition="427" endWordPosition="430"> and effective methods to estimate p(xi|φ(hi)). A classic example is the traditional n-gram LMs with φ(hi) = (xi−n+1, ... , xi−1). Various smoothing techniques are used for parameter estimation (Chen and Goodman, 1999). Recently, neural network LMs, which have begun to surpass the traditional n-gram LMs, also follow the conditional modeling approach, with φ(hi) determined by a neural network (NN), which can be either a feedforward NN (Schwenk, 2007) or a recurrent NN (Mikolov et al., 2011). Remarkably, an alternative approach is used in whole-sentence maximum entropy (WSME) language modeling (Rosenfeld et al., 2001). Specifically, a WSME model has the form: p(x; λ) = Z exp{λT f(x)} 1 (3) Here f(x) is a vector of features, which can be arbitrary computable functions of x, λ is the corresponding parameter vector, and Z is the global normalization constant. Although WSME models have the potential benefits of being able to naturally express sentence-level phenomena and integrate features from a variety of knowledge p(x&apos;)p((EOS)1x1), where (EOS) is a special token placed at the end of every sentence. Thus the distribution of the sentence length is implicitly modeled. 785 Proceedings of the 53rd Annual Meeting</context>
<context position="4363" citStr="Rosenfeld et al., 2001" startWordPosition="653" endWordPosition="656">ance results ever reported are not satisfactory (Rosenfeld et al., 2001; Amaya and Benedi, 2001; Ruokolainen et al., 2010). The WSME model defined in (3) is basically a Markov random field (MRF). A substantial challenge in fitting MRFs is that evaluating the gradient of the log likelihood requires high-dimensional integration and hence is difficult even for moderately sized models (Younes, 1989), let alone the language model (3). The sampling methods previously tried for approximating the gradient are the Gibbs sampling, the Independence MetropolisHasting sampling and the importance sampling (Rosenfeld et al., 2001). Simple applications of these methods are hardly able to work efficiently for the complex, high-dimensional distribution such as (3), and hence the WSME models are in fact poorly fitted to the data. This is one of the reasons for the unsatisfactory results of previous WSME models. In this paper, we propose a new language model, called the trans-dimensional random field (TDRF) model, by explicitly taking account of the empirical distributions of lengths. This formulation subsequently enables us to develop a powerful Markov chain Monte Carlo (MCMC) technique, called trans-dimensional mixture sa</context>
<context position="30147" citStr="Rosenfeld et al., 2001" startWordPosition="5190" endWordPosition="5193">c+ws+cs+cpw w+c+ws+cs+wsh+csh TDRFs (200c) w+c w+c+ws+cs w+c+ws+cs+cpw w+c+ws+cs+wsh+csh TDRFs (500c) w+c 8.72 261.02±2.94 2.8M w+c+ws+cs 8.29 266.34±6.13 5.9M Table 3: The WERs and PPLs on the WSJ’92 test data. “#feat” denotes the feature number. Different TDRF models with class number 100/200/500 are reported (denoted by “100c”/“200c”/“500c”) 5.3 Comparison and discussion TDRF vs WSME. For comparison, Table 3 also presents the results from our implementation of the WSME model (3), using the same features as in Table 1. This WSME model is the same as in (Rosenfeld, 1997), but different from (Rosenfeld et al., 2001), which uses the traditional n-gram LM as the priori distribution p0. For the WSME model (3), we can still use a SA training algorithm, similar to that developed in Section 3.2, to estimate the parameters A. But in this case, there is no need to introduce Cl, because the normalizing constants Zl(A) are canceled out as seen from (7). Specifically, the learning rate &apos;y,, and the L2 regularization are configured the same as in TDRF training. A fixed number of iterations with t0 = 5000 is performed. The total iteration number is 10000, which is similar to the iteration number used in TDRF training</context>
<context position="32657" citStr="Rosenfeld et al., 2001" startWordPosition="5597" endWordPosition="5600">le 3. This is some evidence for the model deficiency of the WSME distribution as defined in (3), and introducing the empirical length probabilities gives a more reasonable model assumption. TDRF vs conditional ME. After training, TDRF models are computationally more efficient in computing sentence probability, simply summing up weights for the activated features in the sentence. The conditional ME models (Khudanpur and Wu, 2000; Roark et al., 2004) suffer from the expensive computation of local normalization factors. This computational bottleneck hinders their use in practice (Goodman, 2001b; Rosenfeld et al., 2001). Partly for this reason, although building conditional ME models with sophisticated features as in Table 1 is theoretically possible, such work has not been pursued so far. TDRF vs RNN. The RNN models suffer from the expensive softmax computation in the output layer 8. Empirically in our experiments, the average time costs for re-ranking of the 1000-best list for a sentence are 0.16 sec vs 40 sec, based on TDRF and RNN respectively (no GPU used). 6 Related Work While there has been extensive research on conditional LMs, there has been little work on the whole-sentence LMs, mainly in (Rosenfel</context>
</contexts>
<marker>Rosenfeld, Chen, Zhu, 2001</marker>
<rawString>Ronald Rosenfeld, Stanley F. Chen, and Xiaojin Zhu. 2001. Whole-sentence exponential language models: a vehicle for linguistic-statistical integration. Computer Speech &amp; Language, 15:55–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Rosenfeld</author>
</authors>
<title>A whole sentence maximum entropy language model.</title>
<date>1997</date>
<booktitle>In Proc. of Automatic Speech Recognition and Understanding (ASRU).</booktitle>
<contexts>
<context position="30102" citStr="Rosenfeld, 1997" startWordPosition="5185" endWordPosition="5186">rge.net/ TDRFs (100c) w+c w+c+ws+cs w+c+ws+cs+cpw w+c+ws+cs+wsh+csh TDRFs (200c) w+c w+c+ws+cs w+c+ws+cs+cpw w+c+ws+cs+wsh+csh TDRFs (500c) w+c 8.72 261.02±2.94 2.8M w+c+ws+cs 8.29 266.34±6.13 5.9M Table 3: The WERs and PPLs on the WSJ’92 test data. “#feat” denotes the feature number. Different TDRF models with class number 100/200/500 are reported (denoted by “100c”/“200c”/“500c”) 5.3 Comparison and discussion TDRF vs WSME. For comparison, Table 3 also presents the results from our implementation of the WSME model (3), using the same features as in Table 1. This WSME model is the same as in (Rosenfeld, 1997), but different from (Rosenfeld et al., 2001), which uses the traditional n-gram LM as the priori distribution p0. For the WSME model (3), we can still use a SA training algorithm, similar to that developed in Section 3.2, to estimate the parameters A. But in this case, there is no need to introduce Cl, because the normalizing constants Zl(A) are canceled out as seen from (7). Specifically, the learning rate &apos;y,, and the L2 regularization are configured the same as in TDRF training. A fixed number of iterations with t0 = 5000 is performed. The total iteration number is 10000, which is similar </context>
</contexts>
<marker>Rosenfeld, 1997</marker>
<rawString>Ronald Rosenfeld. 1997. A whole sentence maximum entropy language model. In Proc. of Automatic Speech Recognition and Understanding (ASRU).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Teemu Ruokolainen</author>
<author>Tanel Alum¨ae</author>
<author>Marcus Dobrinkat</author>
</authors>
<title>Using dependency grammar features in whole sentence maximum entropy language model for speech recognition.</title>
<date>2010</date>
<booktitle>In Baltic HLT.</booktitle>
<marker>Ruokolainen, Alum¨ae, Dobrinkat, 2010</marker>
<rawString>Teemu Ruokolainen, Tanel Alum¨ae, and Marcus Dobrinkat. 2010. Using dependency grammar features in whole sentence maximum entropy language model for speech recognition. In Baltic HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
</authors>
<title>Continuous space language models.</title>
<date>2007</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>21</volume>
<pages>518</pages>
<contexts>
<context position="2725" citStr="Schwenk, 2007" startWordPosition="403" endWordPosition="404">hrough a mapping φ(hi) with the assumption p(xi|hi) ≈ p(xi|φ(hi)). (2) Language modeling in this conditional approach consists of finding suitable mappings φ(hi) and effective methods to estimate p(xi|φ(hi)). A classic example is the traditional n-gram LMs with φ(hi) = (xi−n+1, ... , xi−1). Various smoothing techniques are used for parameter estimation (Chen and Goodman, 1999). Recently, neural network LMs, which have begun to surpass the traditional n-gram LMs, also follow the conditional modeling approach, with φ(hi) determined by a neural network (NN), which can be either a feedforward NN (Schwenk, 2007) or a recurrent NN (Mikolov et al., 2011). Remarkably, an alternative approach is used in whole-sentence maximum entropy (WSME) language modeling (Rosenfeld et al., 2001). Specifically, a WSME model has the form: p(x; λ) = Z exp{λT f(x)} 1 (3) Here f(x) is a vector of features, which can be arbitrary computable functions of x, λ is the corresponding parameter vector, and Z is the global normalization constant. Although WSME models have the potential benefits of being able to naturally express sentence-level phenomena and integrate features from a variety of knowledge p(x&apos;)p((EOS)1x1), where (E</context>
</contexts>
<marker>Schwenk, 2007</marker>
<rawString>Holger Schwenk. 2007. Continuous space language models. Computer Speech &amp; Language, 21:492– 518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya Sutskever</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>Learning multilevel distributed representations for highdimensional sequences.</title>
<date>2007</date>
<booktitle>In International Conference on Artificial Intelligence and Statistics (AISTATS).</booktitle>
<marker>Sutskever, Hinton, 2007</marker>
<rawString>Ilya Sutskever and Geoffrey E Hinton. 2007. Learning multilevel distributed representations for highdimensional sequences. In International Conference on Artificial Intelligence and Statistics (AISTATS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiqiang Tan</author>
</authors>
<title>Optimally adjusted mixture sampling and locally weighted histogram. In</title>
<date>2015</date>
<tech>Technical Report,</tech>
<institution>Department of Statistics, Rutgers University.</institution>
<contexts>
<context position="8930" citStr="Tan, 2015" startWordPosition="1398" endWordPosition="1399">C. p(l,xl; λ) = Zl(λ) Zl(λ)eλT f(xl), (7) 1 Z(λ) where Z(λ) = Eml=1 Zl(λ). A motivation for proposing (6) is that it is very difficult to sample from (3), namely (7), as a mixture distribution with unknown weights which typically differ from each other by orders of magnitudes, e.g. 1040 or more in our experiments. Setting mixture weights to the known, empirical length probabilities enables us to develop a very effective learning algorithm, as introduced in Section 3. Basically, the empirical weights serve as a control device to improve sampling from multiple distributions (Liang et al., 2007; Tan, 2015) . Second, it can be shown that if we incorporate the length features 4 in the vector of features f(x) in (3), then the distribution p(x; λ) in (3) under the maximum entropy (ME) principle will take the form of (6) and the probabilities (π1, ... , πm) in (6) implied by the parameters for the length features are exactly the empirical length probabilities. Third, a feature fi(xl), 1 ≤ i ≤ d, can be any computable function of the sentence xl, such as n-grams. In our current experiments, the features fi(xl) and their corresponding parameters λi are defined to be position-independent and lengthinde</context>
<context position="10293" citStr="Tan, 2015" startWordPosition="1628" endWordPosition="1629">ues in the non-negative integers. 3 Model Estimation We develop a stochastic approximation algorithm using Markov chain Monte Carlo to estimate the parameters λ and the normalization constants Z1(λ),..., Zm(λ) (Benveniste et al., 1990; Chen, 2002). The core algorithms newly designed in this paper are the joint SA for simultaneously estimating parameters and normalizing constants (Section 3.2) and trans-dimensional mixture sampling (Section 3.3) which is used as Step I of the joint SA. The most relevant previous works that we borrowed from are (Gu and Zhu, 2001) on SA for fitting a single RF, (Tan, 2015) on sampling and 4The length feature corresponding to length l is a binary feature that takes one if the sentence x is of length l, and otherwise takes zero. 3.1 Maximum likelihood estimation Suppose that the training dataset consists of nl sentences of length l for l = 1, ... , m. First, the maximum likelihood estimate of the length probability πl is easily shown to be nl/n, where n = Eml=1 nl. By abuse of notation, we set πl = nl/n hereafter. Next, the log-likelihood of λ given the empirical length probabilities is log pl(xl; λ), (8) where Dl is the collection of sentences of length l in the</context>
<context position="12765" citStr="Tan, 2015" startWordPosition="2074" endWordPosition="2075"> set (L(t,0), X(t,0)) = (L(t−1,K), X(t−1,K)) Step L: MCMC sampling 5: for k = 1 --+ K do 6: sampling (See Algorithm 3) (L(t,k), X(t,k)) = SAMPLE(L(t,k−1), X(t,k−1)) 7: set B(t) = B(t) U {(L(t,k), X(t,k))} 8: end for Step LL: SA updating 9: Compute λ(t) based on (14) 10: Compute ζ(t) based on (15) and (16) 11: end for where Z1(λ) is chosen as the reference value and can be calculated exactly. The algorithm can be obtained by combining the standard SA algorithm for training single random fields (Gu and Zhu, 2001) and a trans-dimensional extension of the self-adjusted mixture sampling algorithm (Tan, 2015). Specifically, consider the following joint distribution of the pair (l, xl): p(l, xl; λ, ζ) a πl eζl eλT f(xl), (13) where πl is set to nl/n for l = 1, ... , m, but ζ = (ζ1, ... , ζm)T with ζ1 = 0 are hypothesized values of the truth ζ∗(λ) = (ζ∗1(λ), ... , ζ∗m(λ))T with ζ∗1(λ) = 0. The distribution p(l, xl; λ,ζ) reduces to p(l, xl; λ) in (6) if ζ were identical to ζ∗(λ). In general, p(l, xl; λ,ζ) differs from p(l, xl; λ) in that the marginal probability of length l is not necessarily πl. The joint SA algorithm, whose pseudo-code is shown in Algorithm 1, consists of two steps at each time t a</context>
<context position="14347" citStr="Tan (2015)" startWordPosition="2374" endWordPosition="2375">ning rate of ζ, and δl(B(t)) is the relative frequency of length l appearing in B(t): δl(B(t)) = E(j,xj)∈B(t) 1(j = l). (17) K The rationale in (15) is to adjust ζ based on how the relative frequencies of lengths δl(B(t)) are compared with the desired length probabilities πl. Intuitively, if the relative frequency of some length l in the sample set B(t) is greater (or respectively smaller) than the desired length probability πl, then the hypothesized value ζ(t−1) l is an underestimate (or overestimate) of ζ∗l (λ(t−1)) and hence should be increased (or decreased). Following Gu &amp; Zhu (2001) and Tan (2015), we set the learning rates in two stages: � t−βλ if t &lt; t0 γλ = 1(18) if t &gt; t0 t−t0+tβλ 0 � (0.1t)−βζ if t &lt; t0 γζ = 1 if t &gt; t0 (19) 0.1(t−t0)+(0.1t0)βζ where 0.5 &lt; βλ, βζ &lt; 1. In the first stage (t &lt; t0), a slow-decaying rate of t−β is used to introduce large adjustments. This forces the estimates λ(t) and ζ(t) to fall reasonably fast into the true values. In the second stage (t &gt; t0), a fast-decaying rate of t−1 is used. The iteration number t is multiplied by 0.1 in (19), to make the the learning rate of ζ decay more slowly than λ. Commonly, t0 is selected to ensure there is no more sign</context>
<context position="30901" citStr="Tan, 2015" startWordPosition="5322" endWordPosition="5323">to that developed in Section 3.2, to estimate the parameters A. But in this case, there is no need to introduce Cl, because the normalizing constants Zl(A) are canceled out as seen from (7). Specifically, the learning rate &apos;y,, and the L2 regularization are configured the same as in TDRF training. A fixed number of iterations with t0 = 5000 is performed. The total iteration number is 10000, which is similar to the iteration number used in TDRF training. In order to calculate perplexity, we need to estimate the global normalizing constant Z(A) = E�l=1 Zl(A) for the WSME model. Similarly as in (Tan, 2015), we apply the SA algorithm in Section 3.2 to estimate the log normalizing constants C, while fixing the parameters A to be those already estimated from the WSME model and using uniform probabilities Trl ≡ m−1. The resulting PPLs of these WSME models are extremely poor. The average test log-likelihoods per sentence for these two WSME models are 8.56 268.25±3.52 2.2M 8.16 265.81±4.30 4.5M 8.05 265.63±7.93 5.6M 8.03 276.90±5.00 5.2M 8.46 257.78±3.13 2.5M 8.05 257.80±4.29 5.2M 7.92 264.86±8.55 6.4M 7.94 266.42±7.48 5.9M 792 −494 and −509 respectively. However, the WERs from using the trained WSME</context>
</contexts>
<marker>Tan, 2015</marker>
<rawString>Zhiqiang Tan. 2015. Optimally adjusted mixture sampling and locally weighted histogram. In Technical Report, Department of Statistics, Rutgers University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurent Younes</author>
</authors>
<title>Parametric inference for imperfectly observed gibbsian fields. Probability theory and related fields,</title>
<date>1989</date>
<pages>82--625</pages>
<contexts>
<context position="4138" citStr="Younes, 1989" startWordPosition="623" endWordPosition="624">ational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 785–794, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics sources, their performance results ever reported are not satisfactory (Rosenfeld et al., 2001; Amaya and Benedi, 2001; Ruokolainen et al., 2010). The WSME model defined in (3) is basically a Markov random field (MRF). A substantial challenge in fitting MRFs is that evaluating the gradient of the log likelihood requires high-dimensional integration and hence is difficult even for moderately sized models (Younes, 1989), let alone the language model (3). The sampling methods previously tried for approximating the gradient are the Gibbs sampling, the Independence MetropolisHasting sampling and the importance sampling (Rosenfeld et al., 2001). Simple applications of these methods are hardly able to work efficiently for the complex, high-dimensional distribution such as (3), and hence the WSME models are in fact poorly fitted to the data. This is one of the reasons for the unsatisfactory results of previous WSME models. In this paper, we propose a new language model, called the trans-dimensional random field (T</context>
</contexts>
<marker>Younes, 1989</marker>
<rawString>Laurent Younes. 1989. Parametric inference for imperfectly observed gibbsian fields. Probability theory and related fields, 82:625–645.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>