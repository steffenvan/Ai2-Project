<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000726">
<title confidence="0.977052">
XRCE-M: A Hybrid System for Named Entity Metonymy Resolution
</title>
<author confidence="0.993314">
*Caroline Brun *Maud Ehrmann *Guillaume Jacquet
</author>
<affiliation confidence="0.982992">
* Xerox Research Centre Europe
</affiliation>
<address confidence="0.957852">
6, chemin de Maupertuis
38240 Meylan France
</address>
<email confidence="0.997067">
*{Caroline.Brun, Maud.Ehrmann, Guillaume.Jacquet}@xrce.xerox.com
</email>
<sectionHeader confidence="0.993825" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999719555555555">
This paper describes our participation to the
Metonymy resolution at SemEval 2007 (task
#8). In order to perform named entity me-
tonymy resolution, we developed a hybrid
system based on a robust parser that extracts
deep syntactic relations combined with a
non-supervised distributional approach, also
relying on the relations extracted by the
parser.
</bodyText>
<sectionHeader confidence="0.67279" genericHeader="method">
1 Description of our System
</sectionHeader>
<bodyText confidence="0.9997176">
SemEval 2007 introduces a task aiming at resolving
metonymy for named entities, for location and or-
ganization names (Markert and Nissim 2007). Our
system addresses this task by combining a symbolic
approach based on robust deep parsing and lexical
semantic information, with a distributional method
using syntactic context similarities calculated on
large corpora. Our system is completely unsuper-
vised, as opposed to state-of-the-art systems (see
(Market and Nissim, 2005)).
</bodyText>
<subsectionHeader confidence="0.994812">
1.1 Robust and Deep Parsing Using XIP
</subsectionHeader>
<bodyText confidence="0.999736">
We use the Xerox Incremental Parser (XIP, (Aït et
al., 2002)) to perform robust and deep syntactic
analysis. Deep syntactic analysis consists here in the
construction of a set of syntactic relations1 from an
input text. These relations, labeled with deep syn-
tactic functions, link lexical units of the input text
and/or more complex syntactic domains that are
constructed during the processing (mainly chunks,
see (Abney, 1991)).
</bodyText>
<footnote confidence="0.444009">
1 inspired from dependency grammars, see (Mel’čuk,
1998), and (Tesnière, 1959).
</footnote>
<bodyText confidence="0.9797685">
Moreover, together with surface syntactic relations,
the parser calculates more sophisticated relations
using derivational morphologic properties, deep
syntactic properties2, and some limited lexical se-
mantic coding (Levin&apos;s verb class alternations, see
(Levin, 1993)), and some elements of the Framenet3
classification, (Ruppenhofer et al., 2006)). These
deep syntactic relations correspond roughly to the
agent-experiencer roles that is subsumed by the
SUBJ-N relation and to the patient-theme role sub-
sumed by the OBJ-N relation, see (Brun and Ha-
gège, 2003). Not only verbs bear these relations but
also deverbal nouns with their corresponding argu-
ments.
Here is an example of an output (chunks and
deep syntactic relations):
Lebanon still wanted to see the implementation of a UN
resolution
</bodyText>
<equation confidence="0.965496666666667">
TOP{SC{NP{Lebanon} FV{still wanted}} IV{to see} NP{the
implementation} PP{of NP{a UN resolution}} .}
MOD_PRE(wanted,still)
MOD_PRE(resolution,UN)
MOD_POST(implementation,resolution)
COUNTRY(Lebanon)
ORGANISATION(UN)
EXPERIENCER_PRE(wanted,Lebanon)
EXPERIENCER(see,Lebanon)
CONTENT(see,implementation)
EMBED_INFINIT(see,wanted)
OBJ-N(implement,resolution)
</equation>
<subsectionHeader confidence="0.991488">
1.2 Adaptation to the Task
</subsectionHeader>
<bodyText confidence="0.9994825">
Our parser includes a module for “standard”
named entity recognition, but needs to be adapted to
handle named entity metonymy. Following the
guidelines of the SemEval task #8, we performed a
</bodyText>
<footnote confidence="0.998082">
2 Subject and object of infinitives in the context of con-
trol verbs.
3 http://framenet.icsi.berkeley.edu/
</footnote>
<page confidence="0.964149">
488
</page>
<bodyText confidence="0.9785872">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 488–491,
Prague, June 2007. c�2007 Association for Computational Linguistics
corpus study on the trial data in order to detect lexi-
cal and syntactic regularities triggering a metonymy,
for both location names and organization names.
For example, we examined the subject relation be-
tween organizations or locations and verbs and we
then classify the verbs accordingly: we draw hy-
pothesis like “if a location name is the subject of a
verb referring to an economic action, like import,
provide, refund, repay, etc., then it is a place-for-
people”. We adapted our parser by adding dedicated
lexicons that encode the information collected from
the corpus and develop rules modifying the interpre-
tation of the entity, for example:
</bodyText>
<equation confidence="0.997357">
If (LOCATION(#1) &amp; SUBJ-N(#2[v_econ],#1))4
Î PLACE-FOR-PEOPLE(#1)
</equation>
<bodyText confidence="0.995258428571429">
We focus our study on relations like subject, object,
experiencer, content, modifiers (nominal and prepo-
sitional) and attributes. We also capitalize on the
already-encoded lexical information attached to
verbs by the parser, like communication verbs like
say, deny, comment, or categories of the FrameNet
Experiencer subject frame, i.e. verbs like feel, sense,
see. This information was very useful since experi-
encers denote persons, therefore all organizations or
locations having an experiencer role can be consid-
ered as organization-for-members or place-for-
people. Here is an example of output5, when apply-
ing the modified parser on the following sentence:
“It was the largest Fiat everyone had ever seen”.
</bodyText>
<equation confidence="0.954959">
ORG-FOR-PRODUCT(Fiat)
MOD_PRE(seen,ever)
SUBJ-N_PRE(was,It)
EXPERIENCER_PRE(seen,everyone)
SUBJATTR(It,Fiat)
QUALIF(Fiat,largest)
</equation>
<bodyText confidence="0.984879636363636">
Here, the relation QUALIF(Fiat, largest) triggers
the metonymical interpretation of “Fiat” as org-for-
product.
This first development step is the starting point of
our methodology, which is completed by a non-
supervised distributional approach described in the
next section.
4 Which read as “if the parser has detected a location
name (#1), which is the subject of a verb (#2) bearing the
feature “v-econ”, then create a PLACE-FOR-PEOPLE
unary predicate on #1.
</bodyText>
<sectionHeader confidence="0.567668" genericHeader="method">
5 Only dependencies are shown.
</sectionHeader>
<subsectionHeader confidence="0.995223">
1.3 Hybridizing with a Distributional Approach
</subsectionHeader>
<bodyText confidence="0.999847333333333">
The distributional approach proposes to establish a
distance between words depending on there syntac-
tic distribution.
The distributional hypothesis is that words that ap-
pear in similar contexts are semantically similar
(Harris, 1951): the more two words have the same
distribution, i.e. are found in the same syntactic con-
texts, the more they are semantically close.
We propose to apply this principle for metonymy
resolution. Traditionally, the distributional approach
groups words like USA, Britain, France, Germany
because there are in the same syntactical contexts:
</bodyText>
<listItem confidence="0.99991225">
(1) Someone live in Germany.
(2) Someone works in Germany.
(3) Germany declares something.
(4) Germany signs something.
</listItem>
<bodyText confidence="0.999951419354839">
The metonymy resolution task implies to distin-
guish the literal cases, (1) &amp; (2), from the meto-
nymic ones, (3) &amp; (4). Our method establishes these
distinctions using the syntactic context distribution.
We group contexts occurring with the same words:
the syntactic contexts live in and work in are occur-
ring with Germany, France, country, city, place,
when syntactic contexts subject-of-declare and sub-
ject-of-sign are occurring with Germany, France,
someone, government, president.
For each Named Entity annotation, the hybrid
method consists in using symbolic annotation if
there is (§1.2), else using distributional annotation
(§1.3) as presented below.
Method: We constructed a distributional space with
the 100M-word BNC. We prepared the corpus by
lemmatizing and then parsing with the same robust
parser than for the symbolic approach (XIP, see sec-
tion 3.1). It allows us to identify triple instances.
Each triple have the form w1.R.w2 where w1 and
w2 are lexical units and R is a syntactic relation
(Lin, 1998; Kilgarriff &amp; al. 2004).
Our approach can be distinguished from classical
distributional approach by different points.
First, we use triple occurrences to build a distribu-
tional space (one triple implies two contexts and
two lexical units), but we use the transpose of the
classical space: each point xi of this space is a syn-
tactical context (with the form R.w.), each dimen-
sion j is a lexical units, and each value xi(j) is the
frequency of corresponding triple occurrences. Sec-
</bodyText>
<page confidence="0.996579">
489
</page>
<bodyText confidence="0.839894">
ond, our lexical units are words but also complex
nominal groups or verbal groups. Third, contexts
can be simple contexts or composed contexts6.
We illustrate these three points on the phrase pro-
vide Albania with food aid. The XIP parser gives
the following triples where for example, food aid is
considered as a lexical unit:
OBJ-N(&apos;VERB:provide&apos;,&apos;NOUN: Albania&apos;).
PREP_WITH(&apos;VERB: provide &apos;,&apos;NOUN:aid&apos;).
PREP_WITH(&apos;VERB: provide &apos;,&apos;NP:food aid&apos;).
From these triples, we create the following lexical
units and contexts (in the context 1.VERB: provide.
OBJ-N, “1” mean that the verb provide is the gov-
ernor of the relation OBJ-N):
</bodyText>
<equation confidence="0.708216333333333">
Words: Contexts:
VERB:provide 1.VERB: provide. OBJ-N
NOUN:Albania 1.VERB: provide.PREP_WITH
NOUN:aid 2.NOUN: Albania.OBJ-N
NP:food aid 2.NOUN: aid. PREP_WITH
2.NP: food aid. PREP_WITH
1.VERB:provide.OBJ-N+2.NOUN:aid. PREP_WITH
1.VERB:provide.OBJ-N+2.NP:food aid. PREP_WITH
1.VERB:provide.PREP_WITH +2.NO:Albania.OBJ-N
</equation>
<bodyText confidence="0.990766524590164">
We use a heuristic to control the high productivity
of these lexical units and contexts. Each lexical unit
and each context should appear more than 100 times
in the corpus. From the 100M-word BNC we ob-
tained 60,849 lexical units and 140,634 contexts.
Then, our distributional space has 140,634 units and
60,849 dimensions.
Using the global space to compute distances be-
tween each context is too consuming and would
induce artificial ambiguity (Jacquet, Venant, 2005).
If any named entity can be used in a metonymic
reading, in a given corpus each named entity has not
the same distribution of metonymic readings. The
country Vietnam is more frequently used as an event
than France or Germany, so, knowing that a context
is employed with Vietnam allow to reduce the meto-
nymic ambiguity.
For this, we construct a singular sub-space de-
pending to the context and to the lexical unit (the
ambiguous named entity):
For a given couple context i + lexical unit j we
construct a subspace as follows:
Sub_contexts = list of contexts which are occur-
ring with the word i. If there are more than k con-
texts, we take only the k more frequents.
Sub_dimension = list of lexical units which are
occurring with at least one of the contexts from the
6 For our application, one context can be composed by
two simple contexts.
Sub_contexts list. If there are more than n words,
we take only the n more frequents (relative fre-
quency) with the Sub_contexts list (for this applica-
tion, k = 100 and n = 1,000).
We reduce dimensions of this sub-space to 10
dimensions with a PCA (Principal Components
Analysis).
In this new reduced space (k*10), we compute
the closest context of the context j with the Euclid-
ian distance.
At this point, we use the results of the symbolic
approach described before as starting point. We at-
tribute to each context of the Sub_contexts list, the
annotation, if there is, attributed by symbolic rules.
Each kind of annotation (literal, place-for-people,
place-for-event, etc) is attributed a score corre-
sponding to the sum of the scores obtained by each
context annotated with this category. The score of a
context i decreases in inverse proportion to its dis-
tance from the context j: score(context i) =
1/d(context i, context j) where d(i,j) is the Euclidian
distance between i and j.
We illustrate this process with the sentence pro-
vide Albania with food aid. The unit Albania is
found in 384 different contexts (|Sub_contexts |=
384) and 54,183 lexical units are occurring with at
least one of the contexts from the Sub_contexts list
(|Sub_dimension |= 54,183).
After reducing dimension with PCA, we obtain
the context list below ordered by closeness with the
given context (1.VERB:provide.OBJ-N):
Contexts d symb. annot.
</bodyText>
<equation confidence="0.8453452">
1.VERB:provide.OBJ-N 0.00
1.VERB:allow.OBJ-N 0.76 place-for-people
1.VERB:include.OBJ-N 0.96
2.ADJ:new.MOD_PRE 1.02
1.VERB:be.SUBJ-N 1.43
1.VERB:supply.SUBJ-N_PRE 1.47 literal
1.VERB:become.SUBJ-N_PRE 1.64
1.VERB:come.SUBJ-N_PRE 1.69
1.VERB:support.SUBJ-N_PRE 1.70 place-for-people
etc.
</equation>
<bodyText confidence="0.8724065">
Score for each metonymic annotation of Albania:
Æ place-for-people 3.11
literal 1.23
place-for-event 0.00
... 0.00
The score obtained by each annotation type al-
lows annotating this occurrence of Albania as a
place-for-people metonymic reading. If we can’t
choose only one annotation (all score = 0 or equal-
ity between two annotations) we do not annotate.
</bodyText>
<page confidence="0.994716">
490
</page>
<bodyText confidence="0.8133755">
Uncovered contexts: some of the syntactico-
semantic contexts triggering a metonymy are not
covered by the system at the moment.
2 Evaluation and Results
The following tables show the results on the test
corpus:
</bodyText>
<table confidence="0.998643375">
type Nb. accuracy coverage Baseline Baseline
samp accuracy coverage
Loc/coarse 908 0.851 1 0.794 1
Loc/medium 908 0.848 1 0.794 1
Loc /fine 908 0.841 1 0.794 1
Org/coarse 842 0.732 1 0.618 1
Org/medium 842 0.711 1 0.618 1
Org/fine 842 0.700 1 0.618 1
</table>
<tableCaption confidence="0.980292">
Table 1: Global Results
</tableCaption>
<table confidence="0.9999011">
Nb Prec. Recall F-score
occ.
Literal 721 0.867 0.960 0.911
Place-for-people 141 0.651 0.490 0.559
Place-for-event 10 0.5 0.1 0.166
Place-for-product 1 _ 0 0
Object-for-name 4 1 0.5 0.666
Object-for-representation 0 _ _ _
Othermet 11 _ 0 0
mixed 20 _ 0 0
</table>
<tableCaption confidence="0.909677">
Table 2: Detailed Results for Locations
</tableCaption>
<table confidence="0.999952454545454">
Nb Prec. Recall F-score
occ.
Literal 520 0.730 0.906 0.808
Organization-for-members 161 0.622 0.522 0.568
Organization-for-event 1 _ 0 0
Organization-for-product 67 0.550 0.418 0.475
Organization-for-facility 16 0.5 0.125 0.2
Organization-for-index 3 _ 0 0
Object-for-name 6 1 0.666 0.8
Othermet 8 _ 0 0
Mixed 60 _ 0 0
</table>
<tableCaption confidence="0.999563">
Table 3: Detailed Results for Organizations
</tableCaption>
<bodyText confidence="0.99982525">
The results obtained on the test corpora are above
the baseline for both location and organization
names and therefore are very encouraging for the
method we developed. However, our results on the
test corpora are below the ones we get on the train
corpora, which indicates that there is room for im-
provement for our methodology.
Identified errors are of different nature:
Parsing errors: For example in the sentence “Many
galleries in the States, England and France de-
clined the invitation.”, because the analysis of the
coordination is not correct, France is calculated as
subject of declined, a context triggering a place-for-
people interpretation, which is wrong here.
Mixed cases: These phenomena, while relatively
frequent in the corpora, are not properly treated.
</bodyText>
<sectionHeader confidence="0.9992" genericHeader="conclusions">
3 Conclusion
</sectionHeader>
<bodyText confidence="0.999996333333333">
This paper describes a system combining a sym-
bolic and a non-supervised distributional approach,
developed for resolving location and organization
names metonymy. We plan to pursue this work in
order to improve the system on the already-covered
phenomenon as well as on different names entities.
</bodyText>
<sectionHeader confidence="0.999192" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998923942857143">
Abney S. 1991. Parsing by Chunks. In Robert Berwick, Steven
Abney and Carol Teny (eds.). Principle-based Parsing, Klu-
wer Academics Publishers.
Aït-Mokhtar S., Chanod, J.P., Roux, C. 2002. Robustness be-
yond Shallowness: Incremental Dependency Parsing. Spe-
cial issue of NLE journal.
Brun, C., Hagège C., 2003. Normalization and Paraphrasing
Using Symbolic Methods, Proceeding of the Second Interna-
tional Workshop on Paraphrasing. ACL 2003, Vol. 16, Sap-
poro, Japan.
Harris Z. 1951. Structural Linguistics, University of Chicago
Press.
Jacquet G.,Venant F. 2003. Construction automatique de clas-
ses de sélection distributionnelle, In Proc. TALN 2003,
Dourdan.
Kilgarriff A., Rychly P., Smrz P., Tugwell D. 2004. The sketch
engine. In Proc. EURALEX, pages 105-116.
Levin, B. 1993. English Verb Classes and Alternations – A
preliminary Investigation. The University of Chicago Press.
Nissim, M. and Markert, K. 2005. Learning to buy a Renault
and to talk to a BMW: A supervised approach to conven-
tional metonymy. Proceedings of the 6th International Work-
shop on Computational Semantics, Tilburg.
Nissim, M. and Markert, K. 2007. SemEval-2007 Task 08: Me-
tonymy Resolution at SemEval-2007. In Proceedings of Se-
mEval-2007.
Lin D. 1998. Automatic retrieval and clustering of similar
words. In COLING-ACL, pages 768-774.
Mel’čuk I. 1988. Dependency Syntax. State University of New
York, Albany.
Ruppenhofer, J. Michael Ellsworth, Miriam R. L. Petruck,
Christopher R Johnson and Jan Scheffczyk. 2006. Framenet
II: Extended Theory and Practice.
Tesnière L. 1959. Eléments de Syntaxe Structurale. Klincksiek
Eds. (Corrected edition Paris 1969).
</reference>
<page confidence="0.998747">
491
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.019199">
<title confidence="0.999753">XRCE-M: A Hybrid System for Named Entity Metonymy Resolution</title>
<author confidence="0.999888">Caroline Brun Maud Ehrmann Guillaume Jacquet</author>
<affiliation confidence="0.966909">Xerox Research Centre Europe</affiliation>
<address confidence="0.9016585">6, chemin de Maupertuis 38240 Meylan France</address>
<email confidence="0.967841">*{Caroline.Brun,Maud.Ehrmann,Guillaume.Jacquet}@xrce.xerox.com</email>
<abstract confidence="0.992063438596491">This paper describes our participation to the Metonymy resolution at SemEval 2007 (task tonymy resolution, we developed a hybrid system based on a robust parser that extracts deep syntactic relations combined with a non-supervised distributional approach, also relying on the relations extracted by the parser. 1 Description of our System SemEval 2007 introduces a task aiming at resolving metonymy for named entities, for location and organization names (Markert and Nissim 2007). Our system addresses this task by combining a symbolic approach based on robust deep parsing and lexical semantic information, with a distributional method using syntactic context similarities calculated on large corpora. Our system is completely unsupervised, as opposed to state-of-the-art systems (see (Market and Nissim, 2005)). 1.1 Robust and Deep Parsing Using XIP We use the Xerox Incremental Parser (XIP, (Aït et al., 2002)) to perform robust and deep syntactic analysis. Deep syntactic analysis consists here in the of a set of syntactic from an input text. These relations, labeled with deep syntactic functions, link lexical units of the input text and/or more complex syntactic domains that are constructed during the processing (mainly chunks, see (Abney, 1991)). 1inspired from dependency grammars, see 1998), and (Tesnière, 1959). Moreover, together with surface syntactic relations, the parser calculates more sophisticated relations using derivational morphologic properties, deep and some limited lexical semantic coding (Levin&apos;s verb class alternations, see 1993)), and some elements of the classification, (Ruppenhofer et al., 2006)). These deep syntactic relations correspond roughly to the agent-experiencer roles that is subsumed by the SUBJ-N relation and to the patient-theme role subsumed by the OBJ-N relation, see (Brun and Hagège, 2003). Not only verbs bear these relations but also deverbal nouns with their corresponding arguments. Here is an example of an output (chunks and deep syntactic relations): Lebanon still wanted to see the implementation of a UN resolution TOP{SC{NP{Lebanon} FV{still wanted}} IV{to see} NP{the implementation} PP{of NP{a UN resolution}} .} MOD_PRE(wanted,still) MOD_PRE(resolution,UN) MOD_POST(implementation,resolution) COUNTRY(Lebanon) ORGANISATION(UN) EXPERIENCER_PRE(wanted,Lebanon) EXPERIENCER(see,Lebanon) CONTENT(see,implementation) EMBED_INFINIT(see,wanted) OBJ-N(implement,resolution) 1.2 Adaptation to the Task Our parser includes a module for “standard” named entity recognition, but needs to be adapted to handle named entity metonymy. Following the guidelines of the SemEval task #8, we performed a 2Subject and object of infinitives in the context of control verbs. 3http://framenet.icsi.berkeley.edu/ 488 of the 4th International Workshop on Semantic Evaluations pages 488–491, June 2007. Association for Computational Linguistics corpus study on the trial data in order to detect lexical and syntactic regularities triggering a metonymy, for both location names and organization names. For example, we examined the subject relation between organizations or locations and verbs and we then classify the verbs accordingly: we draw hypothesis like “if a location name is the subject of a referring to an economic action, like etc., then it is a place-forpeople”. We adapted our parser by adding dedicated lexicons that encode the information collected from the corpus and develop rules modifying the interpretation of the entity, for example: (LOCATION(#1) &amp; We focus our study on relations like subject, object, experiencer, content, modifiers (nominal and prepositional) and attributes. We also capitalize on the already-encoded lexical information attached to verbs by the parser, like communication verbs like or categories of the FrameNet subject frame, i.e. verbs like This information was very useful since experiencers denote persons, therefore all organizations or locations having an experiencer role can be considered as organization-for-members or place-for- Here is an example of when applying the modified parser on the following sentence: was the largest had ever seen”. ORG-FOR-PRODUCT(Fiat) MOD_PRE(seen,ever) SUBJ-N_PRE(was,It) EXPERIENCER_PRE(seen,everyone) SUBJATTR(It,Fiat) QUALIF(Fiat,largest) Here, the relation QUALIF(Fiat, largest) triggers the metonymical interpretation of “Fiat” as org-forproduct. This first development step is the starting point of our methodology, which is completed by a nonsupervised distributional approach described in the next section. read as “if the parser has detected a location name (#1), which is the subject of a verb (#2) bearing the feature “v-econ”, then create a PLACE-FOR-PEOPLE unary predicate on #1. 5Only dependencies are shown. 1.3 Hybridizing with a Distributional Approach The distributional approach proposes to establish a distance between words depending on there syntactic distribution. The distributional hypothesis is that words that appear in similar contexts are semantically similar (Harris, 1951): the more two words have the same distribution, i.e. are found in the same syntactic contexts, the more they are semantically close. We propose to apply this principle for metonymy resolution. Traditionally, the distributional approach words like because there are in the same syntactical contexts: Someone live in (2) Someone works in Germany. Germany declares (4) Germany signs something. The metonymy resolution task implies to distinguish the literal cases, (1) &amp; (2), from the metonymic ones, (3) &amp; (4). Our method establishes these distinctions using the syntactic context distribution. We group contexts occurring with the same words: syntactic contexts in in occurwith syntactic contexts suboccurring with For each Named Entity annotation, the hybrid method consists in using symbolic annotation if there is (§1.2), else using distributional annotation (§1.3) as presented below. constructed a distributional space with the 100M-word BNC. We prepared the corpus by lemmatizing and then parsing with the same robust parser than for the symbolic approach (XIP, see section 3.1). It allows us to identify triple instances. Each triple have the form w1.R.w2 where w1 and w2 are lexical units and R is a syntactic relation 1998; Kilgarriff &amp; Our approach can be distinguished from classical distributional approach by different points. First, we use triple occurrences to build a distributional space (one triple implies two contexts and two lexical units), but we use the transpose of the space: each point this space is a syntactical context (with the form R.w.), each dimena lexical units, and each value is the of corresponding triple occurrences. Sec- 489 ond, our lexical units are words but also complex nominal groups or verbal groups. Third, contexts be simple contexts or composed illustrate these three points on the phrase pro- Albania with food The XIP parser gives following triples where for example, aid considered as a lexical unit: OBJ-N(&apos;VERB:provide&apos;,&apos;NOUN: Albania&apos;). PREP_WITH(&apos;VERB: provide &apos;,&apos;NOUN:aid&apos;). PREP_WITH(&apos;VERB: provide &apos;,&apos;NP:food aid&apos;). From these triples, we create the following lexical and contexts (in the context provide. that the verb the governor of the relation OBJ-N): Words: Contexts: VERB:provide 1.VERB: provide. OBJ-N NOUN:Albania 1.VERB: provide.PREP_WITH NOUN:aid 2.NOUN: Albania.OBJ-N NP:food aid 2.NOUN: aid. PREP_WITH 2.NP: food aid. PREP_WITH 1.VERB:provide.OBJ-N+2.NOUN:aid. PREP_WITH 1.VERB:provide.OBJ-N+2.NP:food aid. PREP_WITH 1.VERB:provide.PREP_WITH +2.NO:Albania.OBJ-N We use a heuristic to control the high productivity of these lexical units and contexts. Each lexical unit and each context should appear more than 100 times in the corpus. From the 100M-word BNC we obtained 60,849 lexical units and 140,634 contexts. Then, our distributional space has 140,634 units and 60,849 dimensions. Using the global space to compute distances between each context is too consuming and would induce artificial ambiguity (Jacquet, Venant, 2005). If any named entity can be used in a metonymic reading, in a given corpus each named entity has not the same distribution of metonymic readings. The more frequently used as an event so, knowing that a context employed with to reduce the metonymic ambiguity. For this, we construct a singular sub-space depending to the context and to the lexical unit (the ambiguous named entity): a given couple context lexical unit construct a subspace as follows: Sub_contexts = list of contexts which are occurwith the word If there are more than k conwe take only the frequents. Sub_dimension = list of lexical units which are occurring with at least one of the contexts from the 6For our application, one context can be composed by two simple contexts. list. If there are more than take only the frequents (relative frequency) with the Sub_contexts list (for this applica- 100 and 1,000). We reduce dimensions of this sub-space to 10 dimensions with a PCA (Principal Components Analysis). this new reduced space we compute closest context of the context the Euclidian distance. At this point, we use the results of the symbolic approach described before as starting point. We attribute to each context of the Sub_contexts list, the annotation, if there is, attributed by symbolic rules. Each kind of annotation (literal, place-for-people, place-for-event, etc) is attributed a score corresponding to the sum of the scores obtained by each context annotated with this category. The score of a in inverse proportion to its disfrom the context score(context = context where is the Euclidian between illustrate this process with the sentence pro- Albania with food aid. unit found in 384 different contexts (|Sub_contexts |= 384) and 54,183 lexical units are occurring with at least one of the contexts from the Sub_contexts list (|Sub_dimension |= 54,183). After reducing dimension with PCA, we obtain the context list below ordered by closeness with the context Contexts d symb. annot. 1.VERB:provide.OBJ-N 0.00 1.VERB:allow.OBJ-N 0.76 place-for-people 1.VERB:include.OBJ-N 0.96 2.ADJ:new.MOD_PRE 1.02 1.VERB:be.SUBJ-N 1.43 1.VERB:supply.SUBJ-N_PRE 1.47 literal 1.VERB:become.SUBJ-N_PRE 1.64 1.VERB:come.SUBJ-N_PRE 1.69 1.VERB:support.SUBJ-N_PRE 1.70 place-for-people etc. for each metonymic annotation of 3.11 literal 1.23 place-for-event 0.00 ... 0.00 The score obtained by each annotation type alannotating this occurrence of a reading. If we can’t choose only one annotation (all score = 0 or equality between two annotations) we do not annotate. 490 contexts:some of the syntacticosemantic contexts triggering a metonymy are not covered by the system at the moment. 2 Evaluation and Results following tables show the results on the corpus: type Nb. samp accuracy coverage Baseline accuracy Baseline coverage Loc/coarse 908 0.851 1 0.794 1 Loc/medium 908 0.848 1 0.794 1 Loc /fine 908 0.841 1 0.794 1 Org/coarse 842 0.732 1 0.618 1 Org/medium 842 0.711 1 0.618 1 Org/fine 842 0.700 1 0.618 1</abstract>
<note confidence="0.907908045454545">Table 1: Global Results Nb occ. Prec. Recall F-score Literal 721 0.867 0.960 0.911 Place-for-people 141 0.651 0.490 0.559 Place-for-event 10 0.5 0.1 0.166 Place-for-product 1 _ 0 0 Object-for-name 4 1 0.5 0.666 Object-for-representation 0 _ _ _ Othermet 11 _ 0 0 mixed 20 _ 0 0 Table 2: Detailed Results for Locations Nb occ. Prec. Recall F-score Literal 520 0.730 0.906 0.808 Organization-for-members 161 0.622 0.522 0.568 Organization-for-event 1 _ 0 0 Organization-for-product 67 0.550 0.418 0.475 Organization-for-facility 16 0.5 0.125 0.2 Organization-for-index 3 _ 0 0 Object-for-name 6 1 0.666 0.8 Othermet 8 _ 0 0 Mixed 60 _ 0 0 Table 3: Detailed Results for Organizations</note>
<abstract confidence="0.996287826086957">The results obtained on the test corpora are above the baseline for both location and organization names and therefore are very encouraging for the method we developed. However, our results on the test corpora are below the ones we get on the train corpora, which indicates that there is room for improvement for our methodology. Identified errors are of different nature: errors:For example in the sentence galleries in the States, England and France dethe because the analysis of the is not correct, calculated as of a context triggering a place-forpeople interpretation, which is wrong here. cases:These phenomena, while relatively frequent in the corpora, are not properly treated. 3 Conclusion This paper describes a system combining a symbolic and a non-supervised distributional approach, developed for resolving location and organization names metonymy. We plan to pursue this work in order to improve the system on the already-covered phenomenon as well as on different names entities.</abstract>
<note confidence="0.96464854054054">References S. 1991. by In Robert Berwick, Steven Abney and Carol Teny (eds.). Principle-based Parsing, Kluwer Academics Publishers. S., Chanod, J.P., Roux, C. 2002. be- Shallowness: Incremental Dependency Special issue of NLE journal. C., Hagège C., 2003. and Paraphrasing Symbolic Proceeding of the Second International Workshop on Paraphrasing. ACL 2003, Vol. 16, Sapporo, Japan. Z. 1951. University of Chicago Press. G.,Venant F. 2003. automatique de clasde sélection distributionnelle, Proc. TALN 2003, Dourdan. A., Rychly P., Smrz P., Tugwell D. 2004. sketch In Proc. EURALEX, pages 105-116. B. 1993. Verb Classes and Alternations – A The University of Chicago Press. M. and Markert, K. 2005. to buy a Renault and to talk to a BMW: A supervised approach to conven- Proceedings of the 6th International Workshop on Computational Semantics, Tilburg. M. and Markert, K. 2007. Task 08: Me- Resolution at In Proceedings of SemEval-2007. D. 1998. retrieval and clustering of similar In COLING-ACL, pages 768-774. I. 1988. State University of New York, Albany. Ruppenhofer, J. Michael Ellsworth, Miriam R. L. Petruck, R Johnson and Jan Scheffczyk. 2006. Extended Theory and L. 1959. de Syntaxe Klincksiek Eds. (Corrected edition Paris 1969). 491</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abney</author>
</authors>
<title>Parsing by Chunks.</title>
<date>1991</date>
<editor>In Robert Berwick, Steven Abney and Carol Teny (eds.). Principle-based Parsing,</editor>
<publisher>Kluwer Academics Publishers.</publisher>
<contexts>
<context position="1578" citStr="Abney, 1991" startWordPosition="230" endWordPosition="231">rities calculated on large corpora. Our system is completely unsupervised, as opposed to state-of-the-art systems (see (Market and Nissim, 2005)). 1.1 Robust and Deep Parsing Using XIP We use the Xerox Incremental Parser (XIP, (Aït et al., 2002)) to perform robust and deep syntactic analysis. Deep syntactic analysis consists here in the construction of a set of syntactic relations1 from an input text. These relations, labeled with deep syntactic functions, link lexical units of the input text and/or more complex syntactic domains that are constructed during the processing (mainly chunks, see (Abney, 1991)). 1 inspired from dependency grammars, see (Mel’čuk, 1998), and (Tesnière, 1959). Moreover, together with surface syntactic relations, the parser calculates more sophisticated relations using derivational morphologic properties, deep syntactic properties2, and some limited lexical semantic coding (Levin&apos;s verb class alternations, see (Levin, 1993)), and some elements of the Framenet3 classification, (Ruppenhofer et al., 2006)). These deep syntactic relations correspond roughly to the agent-experiencer roles that is subsumed by the SUBJ-N relation and to the patient-theme role subsumed by the </context>
</contexts>
<marker>Abney, 1991</marker>
<rawString>Abney S. 1991. Parsing by Chunks. In Robert Berwick, Steven Abney and Carol Teny (eds.). Principle-based Parsing, Kluwer Academics Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Aït-Mokhtar</author>
<author>J P Chanod</author>
<author>C Roux</author>
</authors>
<title>Robustness beyond Shallowness: Incremental Dependency Parsing.</title>
<date>2002</date>
<journal>Special issue of NLE journal.</journal>
<marker>Aït-Mokhtar, Chanod, Roux, 2002</marker>
<rawString>Aït-Mokhtar S., Chanod, J.P., Roux, C. 2002. Robustness beyond Shallowness: Incremental Dependency Parsing. Special issue of NLE journal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Brun</author>
<author>C Hagège</author>
</authors>
<title>Normalization and Paraphrasing Using Symbolic Methods,</title>
<date>2003</date>
<booktitle>Proceeding of the Second International Workshop on Paraphrasing. ACL 2003,</booktitle>
<volume>16</volume>
<location>Sapporo, Japan.</location>
<contexts>
<context position="2221" citStr="Brun and Hagège, 2003" startWordPosition="316" endWordPosition="320">pendency grammars, see (Mel’čuk, 1998), and (Tesnière, 1959). Moreover, together with surface syntactic relations, the parser calculates more sophisticated relations using derivational morphologic properties, deep syntactic properties2, and some limited lexical semantic coding (Levin&apos;s verb class alternations, see (Levin, 1993)), and some elements of the Framenet3 classification, (Ruppenhofer et al., 2006)). These deep syntactic relations correspond roughly to the agent-experiencer roles that is subsumed by the SUBJ-N relation and to the patient-theme role subsumed by the OBJ-N relation, see (Brun and Hagège, 2003). Not only verbs bear these relations but also deverbal nouns with their corresponding arguments. Here is an example of an output (chunks and deep syntactic relations): Lebanon still wanted to see the implementation of a UN resolution TOP{SC{NP{Lebanon} FV{still wanted}} IV{to see} NP{the implementation} PP{of NP{a UN resolution}} .} MOD_PRE(wanted,still) MOD_PRE(resolution,UN) MOD_POST(implementation,resolution) COUNTRY(Lebanon) ORGANISATION(UN) EXPERIENCER_PRE(wanted,Lebanon) EXPERIENCER(see,Lebanon) CONTENT(see,implementation) EMBED_INFINIT(see,wanted) OBJ-N(implement,resolution) 1.2 Adapta</context>
</contexts>
<marker>Brun, Hagège, 2003</marker>
<rawString>Brun, C., Hagège C., 2003. Normalization and Paraphrasing Using Symbolic Methods, Proceeding of the Second International Workshop on Paraphrasing. ACL 2003, Vol. 16, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Harris</author>
</authors>
<title>Structural Linguistics,</title>
<date>1951</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="5631" citStr="Harris, 1951" startWordPosition="791" endWordPosition="792">nt of our methodology, which is completed by a nonsupervised distributional approach described in the next section. 4 Which read as “if the parser has detected a location name (#1), which is the subject of a verb (#2) bearing the feature “v-econ”, then create a PLACE-FOR-PEOPLE unary predicate on #1. 5 Only dependencies are shown. 1.3 Hybridizing with a Distributional Approach The distributional approach proposes to establish a distance between words depending on there syntactic distribution. The distributional hypothesis is that words that appear in similar contexts are semantically similar (Harris, 1951): the more two words have the same distribution, i.e. are found in the same syntactic contexts, the more they are semantically close. We propose to apply this principle for metonymy resolution. Traditionally, the distributional approach groups words like USA, Britain, France, Germany because there are in the same syntactical contexts: (1) Someone live in Germany. (2) Someone works in Germany. (3) Germany declares something. (4) Germany signs something. The metonymy resolution task implies to distinguish the literal cases, (1) &amp; (2), from the metonymic ones, (3) &amp; (4). Our method establishes th</context>
</contexts>
<marker>Harris, 1951</marker>
<rawString>Harris Z. 1951. Structural Linguistics, University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Jacquet</author>
<author>F Venant</author>
</authors>
<title>Construction automatique de classes de sélection distributionnelle,</title>
<date>2003</date>
<booktitle>In Proc. TALN 2003,</booktitle>
<location>Dourdan.</location>
<marker>Jacquet, Venant, 2003</marker>
<rawString>Jacquet G.,Venant F. 2003. Construction automatique de classes de sélection distributionnelle, In Proc. TALN 2003, Dourdan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilgarriff</author>
<author>P Rychly</author>
<author>P Smrz</author>
<author>D Tugwell</author>
</authors>
<title>The sketch engine.</title>
<date>2004</date>
<booktitle>In Proc. EURALEX,</booktitle>
<pages>105--116</pages>
<marker>Kilgarriff, Rychly, Smrz, Tugwell, 2004</marker>
<rawString>Kilgarriff A., Rychly P., Smrz P., Tugwell D. 2004. The sketch engine. In Proc. EURALEX, pages 105-116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Levin</author>
</authors>
<title>English Verb Classes and Alternations – A preliminary Investigation.</title>
<date>1993</date>
<publisher>The University of Chicago Press.</publisher>
<contexts>
<context position="1928" citStr="Levin, 1993" startWordPosition="274" endWordPosition="275">n of a set of syntactic relations1 from an input text. These relations, labeled with deep syntactic functions, link lexical units of the input text and/or more complex syntactic domains that are constructed during the processing (mainly chunks, see (Abney, 1991)). 1 inspired from dependency grammars, see (Mel’čuk, 1998), and (Tesnière, 1959). Moreover, together with surface syntactic relations, the parser calculates more sophisticated relations using derivational morphologic properties, deep syntactic properties2, and some limited lexical semantic coding (Levin&apos;s verb class alternations, see (Levin, 1993)), and some elements of the Framenet3 classification, (Ruppenhofer et al., 2006)). These deep syntactic relations correspond roughly to the agent-experiencer roles that is subsumed by the SUBJ-N relation and to the patient-theme role subsumed by the OBJ-N relation, see (Brun and Hagège, 2003). Not only verbs bear these relations but also deverbal nouns with their corresponding arguments. Here is an example of an output (chunks and deep syntactic relations): Lebanon still wanted to see the implementation of a UN resolution TOP{SC{NP{Lebanon} FV{still wanted}} IV{to see} NP{the implementation} P</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Levin, B. 1993. English Verb Classes and Alternations – A preliminary Investigation. The University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Nissim</author>
<author>K Markert</author>
</authors>
<title>Learning to buy a Renault and to talk to a BMW: A supervised approach to conventional metonymy.</title>
<date>2005</date>
<booktitle>Proceedings of the 6th International Workshop on Computational Semantics,</booktitle>
<location>Tilburg.</location>
<marker>Nissim, Markert, 2005</marker>
<rawString>Nissim, M. and Markert, K. 2005. Learning to buy a Renault and to talk to a BMW: A supervised approach to conventional metonymy. Proceedings of the 6th International Workshop on Computational Semantics, Tilburg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Nissim</author>
<author>K Markert</author>
</authors>
<date>2007</date>
<booktitle>SemEval-2007 Task 08: Metonymy Resolution at SemEval-2007. In Proceedings of SemEval-2007.</booktitle>
<marker>Nissim, Markert, 2007</marker>
<rawString>Nissim, M. and Markert, K. 2007. SemEval-2007 Task 08: Metonymy Resolution at SemEval-2007. In Proceedings of SemEval-2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In COLING-ACL,</booktitle>
<pages>768--774</pages>
<contexts>
<context position="7104" citStr="Lin, 1998" startWordPosition="1019" endWordPosition="1020">of-sign are occurring with Germany, France, someone, government, president. For each Named Entity annotation, the hybrid method consists in using symbolic annotation if there is (§1.2), else using distributional annotation (§1.3) as presented below. Method: We constructed a distributional space with the 100M-word BNC. We prepared the corpus by lemmatizing and then parsing with the same robust parser than for the symbolic approach (XIP, see section 3.1). It allows us to identify triple instances. Each triple have the form w1.R.w2 where w1 and w2 are lexical units and R is a syntactic relation (Lin, 1998; Kilgarriff &amp; al. 2004). Our approach can be distinguished from classical distributional approach by different points. First, we use triple occurrences to build a distributional space (one triple implies two contexts and two lexical units), but we use the transpose of the classical space: each point xi of this space is a syntactical context (with the form R.w.), each dimension j is a lexical units, and each value xi(j) is the frequency of corresponding triple occurrences. Sec489 ond, our lexical units are words but also complex nominal groups or verbal groups. Third, contexts can be simple co</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Lin D. 1998. Automatic retrieval and clustering of similar words. In COLING-ACL, pages 768-774.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mel’čuk</author>
</authors>
<date>1988</date>
<institution>Dependency Syntax. State University of</institution>
<location>New York, Albany.</location>
<marker>Mel’čuk, 1988</marker>
<rawString>Mel’čuk I. 1988. Dependency Syntax. State University of New York, Albany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Michael Ellsworth Ruppenhofer</author>
<author>Miriam R L Petruck</author>
<author>Christopher R Johnson</author>
<author>Jan Scheffczyk</author>
</authors>
<title>Framenet II: Extended Theory and Practice.</title>
<date>2006</date>
<contexts>
<context position="2008" citStr="Ruppenhofer et al., 2006" startWordPosition="283" endWordPosition="286">ons, labeled with deep syntactic functions, link lexical units of the input text and/or more complex syntactic domains that are constructed during the processing (mainly chunks, see (Abney, 1991)). 1 inspired from dependency grammars, see (Mel’čuk, 1998), and (Tesnière, 1959). Moreover, together with surface syntactic relations, the parser calculates more sophisticated relations using derivational morphologic properties, deep syntactic properties2, and some limited lexical semantic coding (Levin&apos;s verb class alternations, see (Levin, 1993)), and some elements of the Framenet3 classification, (Ruppenhofer et al., 2006)). These deep syntactic relations correspond roughly to the agent-experiencer roles that is subsumed by the SUBJ-N relation and to the patient-theme role subsumed by the OBJ-N relation, see (Brun and Hagège, 2003). Not only verbs bear these relations but also deverbal nouns with their corresponding arguments. Here is an example of an output (chunks and deep syntactic relations): Lebanon still wanted to see the implementation of a UN resolution TOP{SC{NP{Lebanon} FV{still wanted}} IV{to see} NP{the implementation} PP{of NP{a UN resolution}} .} MOD_PRE(wanted,still) MOD_PRE(resolution,UN) MOD_PO</context>
</contexts>
<marker>Ruppenhofer, Petruck, Johnson, Scheffczyk, 2006</marker>
<rawString>Ruppenhofer, J. Michael Ellsworth, Miriam R. L. Petruck, Christopher R Johnson and Jan Scheffczyk. 2006. Framenet II: Extended Theory and Practice.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Tesnière</author>
</authors>
<title>Eléments de Syntaxe Structurale. Klincksiek Eds. (Corrected edition</title>
<date>1959</date>
<location>Paris</location>
<contexts>
<context position="1659" citStr="Tesnière, 1959" startWordPosition="241" endWordPosition="242">opposed to state-of-the-art systems (see (Market and Nissim, 2005)). 1.1 Robust and Deep Parsing Using XIP We use the Xerox Incremental Parser (XIP, (Aït et al., 2002)) to perform robust and deep syntactic analysis. Deep syntactic analysis consists here in the construction of a set of syntactic relations1 from an input text. These relations, labeled with deep syntactic functions, link lexical units of the input text and/or more complex syntactic domains that are constructed during the processing (mainly chunks, see (Abney, 1991)). 1 inspired from dependency grammars, see (Mel’čuk, 1998), and (Tesnière, 1959). Moreover, together with surface syntactic relations, the parser calculates more sophisticated relations using derivational morphologic properties, deep syntactic properties2, and some limited lexical semantic coding (Levin&apos;s verb class alternations, see (Levin, 1993)), and some elements of the Framenet3 classification, (Ruppenhofer et al., 2006)). These deep syntactic relations correspond roughly to the agent-experiencer roles that is subsumed by the SUBJ-N relation and to the patient-theme role subsumed by the OBJ-N relation, see (Brun and Hagège, 2003). Not only verbs bear these relations </context>
</contexts>
<marker>Tesnière, 1959</marker>
<rawString>Tesnière L. 1959. Eléments de Syntaxe Structurale. Klincksiek Eds. (Corrected edition Paris 1969).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>