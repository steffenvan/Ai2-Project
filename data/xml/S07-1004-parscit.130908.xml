<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004815">
<title confidence="0.602041">
SemEval-2007 Task 5: Multilingual Chinese-English Lexical Sample
</title>
<author confidence="0.985459">
Peng Jin, Yunfang Wu and Shiwen Yu
</author>
<affiliation confidence="0.912981">
Institute of Computational Linguistics
Peking University, Beijing China
</affiliation>
<email confidence="0.98911">
ljandp, wuyf, yusw}@pku.edu.cn
</email>
<sectionHeader confidence="0.998586" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999350166666667">
The Multilingual Chinese-English lexical
sample task at SemEval-2007 provides a
framework to evaluate Chinese word sense
disambiguation and to promote research.
This paper reports on the task preparation
and the results of six participants.
</bodyText>
<sectionHeader confidence="0.999385" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999237818181818">
The Multilingual Chinese-English lexical sample
task is designed following the leading ideas of the
Senseval-3 Multilingual English-Hindi lexical
sample task (Chklovski et al., 2004). The “sense
tags” for the ambiguous Chinese target words are
given in the form of their English translations.
The data preparation is introduced in the second
section. And then the participating systems are
briefly described and their scores are listed.
In the conclusions we bring forward some sug-
gestion for the next campaign.
</bodyText>
<sectionHeader confidence="0.967074" genericHeader="method">
2 Chinese Word Sense Annotated Corpus
</sectionHeader>
<bodyText confidence="0.999649416666667">
All the training and test data come from the
People’s Daily in January, February and March of
2000. The People’s Daily is the most popular
newspaper in China and is open domain. Before
manually sense annotating, the texts have been
word-segmented and part of speech (PoS) tagged
according to the PoS tagging scheme of Institute of
Computational Linguistics in Peking University
(ICL/PKU). The corpus had been used as one of
the gold-standard data set for the second
international Chinese word segmentation bakeoff
in 2005.1
</bodyText>
<subsectionHeader confidence="0.92213">
2.1 Manual Annotation
</subsectionHeader>
<bodyText confidence="0.999980741935484">
The sense annotated corpus is manually con-
structed with the help of a word sense annotating
interface developed in Java. Three native annota-
tors, two major in Chinese linguistics and one ma-
jor in computer science took part in the construc-
tion of the sense-annotated corpus. A text generally
is first annotated by one annotator and then veri-
fied by two checkers. Checking is of course a nec-
essary procedure to keep the consistency. Inspired
by the observation that checking all the instances
of a word in a specific time frame will greatly im-
prove the precision and accelerate the speed, a
software tool is designed in Java to gather all the
occurrences of a word in the corpus into a check-
ing file with the sense KWIC (Key Word in Con-
text) format in sense tags order. The inter-
annotator agreement gets to 84.8% according to
Wu. et al. (2006).
The sense entries are specified in the Chinese
Semantic Dictionary (CSD) developed by
ICL/PKU. The sense distinctions are made mainly
according to the Contemporary Chinese Dictionary,
the most widely used dictionary in mandarin Chi-
nese, with necessary adjustment and improvement
is implemented according to words usage in real
texts. Word senses are described using the feature-
based formalism. The features, which appear in
the form “Attribute =Value”, can incorporate ex-
tensive distributional information about a word
sense. The feature set constitutes the representation
of a sense, while the verbal definitions of meaning
</bodyText>
<footnote confidence="0.958067">
1 http://sighan.cs.uchicago.edu/bakeoff2005/
</footnote>
<page confidence="0.993407">
19
</page>
<bodyText confidence="0.966429777777778">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 19–23,
Prague, June 2007. c�2007 Association for Computational Linguistics
serve only as references for human use. The Eng-
lish translation is assigned to each sense in the at-
tribute “English translation” in CSD.
Based on the sense-annotated corpus, a sense is
replaced by its English translation, which might
group different senses together under the same
English word.
</bodyText>
<subsectionHeader confidence="0.997168">
2.2 Instances selection
</subsectionHeader>
<bodyText confidence="0.999910555555556">
In this task together 40 Chinese ambiguous words:
19 nouns and 21 verbs are selected for the evalua-
tion. Each sense of one word is provided at least 15
instances and at most 40 instances, in which
around 2/3 is used as the training data and 1/3 as
the test data. Table 1 presents the number of words
under each part of speech, the average number of
senses for each PoS and the number of instances
respectively in the training and test set.
</bodyText>
<table confidence="0.997418">
# Average # training # test
senses instances instances
19 2.58 1019 364
nouns
21 3.57 1667 571
verbs
</table>
<tableCaption confidence="0.997941">
Table 1: Summary of the sense inventory and
</tableCaption>
<bodyText confidence="0.938005142857143">
number of training data and test set
In order to escape from the sense-skewed distri-
bution that really exists in the corpus of People’s
Daily, many instances of some senses have been
removed from the sense annotated corpus. So the
sense distribution of the ambiguous words in this
task does not reflect the usages in real texts.
</bodyText>
<sectionHeader confidence="0.888815" genericHeader="method">
3 Participating Systems
</sectionHeader>
<bodyText confidence="0.999664">
In order to facilitate participators to select the fea-
tures, we gave a specification for the PoS-tag set.
Both word-segmented and un-segmented context
are provided.
Two kinds of precisions are evaluated. One is
micro-average:
</bodyText>
<equation confidence="0.98752475">
N N
P mir = ∑ m i n
/ ∑ i
i=1 i=1
</equation>
<bodyText confidence="0.910206571428572">
N is the number of all target word-types. is
mi
the number of labeled correctly to one specific tar-
get word-type and is the number of all test in-
ni
stances for this word-type.
The other is macro-average:
</bodyText>
<equation confidence="0.98819025">
N
Pmar =∑= pi N
/
i 1
</equation>
<bodyText confidence="0.9999056">
All teams attempted all test instances. So the re-
call is the same with the precision. The precision
baseline is obtained by the most frequent sense.
Because the corpus is not reflected the real usage,
the precision is very low.
Six teams participated in this word sense disam-
biguation task. Four of them used supervised learn-
ing algorithms and two used un-supervised method.
For each team two kinds of precision are given as
in table 2.
</bodyText>
<table confidence="0.999424625">
Team Micro-average Macro-average
SRCB-WSD 0.716578 0.749236
I2R 0.712299 0.746824
CITYU-HIF 0.710160 0.748761
SWAT 0.657754 0.692487
TorMd 0.375401 0.431243
HIT 0.336898 0.395993
baseline 0.4053 0.4618
</table>
<tableCaption confidence="0.999759">
Table 2: The scores of all participating systems
</tableCaption>
<bodyText confidence="0.998665066666667">
As follow the participating systems are briefly
introduced.
SRCB-WSD system exploited maximum entropy
model as the classifier from OpenNLP2 The fol-
lowing features are used in this WSD system:
· All the verbs and nouns in the context, that is,
the words with tags “n, nr, ns, nt, nz, v, vd, vn”
· PoS of the left word and the right word
·noun phrase, verb phrase, adjective phrase,
time phrase, place phrase and quantity phrase.
These phrases are considered as constituents of
context, as well as words and punctuations which
do not belong to any phrase.
·the type of these phrases which are around the
target phrases
</bodyText>
<footnote confidence="0.778023">
2 http:// maxent.sourceforge.net/
</footnote>
<note confidence="0.206532">
, pi = mi / ni
</note>
<page confidence="0.917085">
20
</page>
<bodyText confidence="0.999181451612903">
· word category information comes from Chi-
nese thesaurus
I2R system used a semi-supervised classification
algorithm (label propagation algorithm) (Niu, et al.,
2005). They used three types of features: PoS of
neighboring words with position information, un-
ordered single words in topical context, and local
collocations.
In the label propagation algorithm (LP) (Zhu
and Ghahramani, 2002), label information of any
vertex in a graph is propagated to nearby vertices
through weighted edges until a global stable stage
is achieved. Larger edge weights allow labels to
travel through easier. Thus the closer the examples,
the more likely they have similar labels (the global
consistency assumption). In label propagation
process, the soft label of each initial labeled exam-
ple is clamped in each iteration to replenish label
sources from these labeled data. Thus the labeled
data act like sources to push out labels through
unlabeled data. With this push from labeled exam-
ples, the class boundaries will be pushed through
edges with large weights and settle in gaps along
edges with small weights. If the data structure fits
the classification goal, then LP algorithm can use
these unlabeled data to help learning classification
plane.
CITYU-HIF system was a fully supervised one
based on a Naïve Bayes classifier with simple fea-
ture selection for each target word. The features
used are as follows:
</bodyText>
<listItem confidence="0.974405333333333">
· Local features at specified positions:
PoS of word at w-2, w-1, w1, w2
Word at w-2, w-1, w1, w2
· Topical features within a given window:
Content words appearing within w-10 to w10
· Syntactic features:
</listItem>
<equation confidence="0.909296">
PoS bi-gram at w-2w0 , w-1w0 , w0w1 , w0w2
PoS tri-gram at w-2 w-1w0 and w0w1w2
</equation>
<bodyText confidence="0.999971959183673">
One characteristic of this system is the incorpo-
ration of the intrinsic nature of each target word in
disambiguation. It is assumed that WSD is highly
lexically sensitive and each word is best character-
ized by different lexical information. Human
judged to consider for each target word the type of
disambiguation information if they found useful.
During disambiguation, they run two Naïve Bayes
classifiers, one on all features above, and the other
only on the type of information deemed useful by
the human judges. When the probability of the best
guess from the former is under a certain threshold,
the best guess from the latter was used instead.
SWAT system uses a weighted vote from three
different classifiers to make the prediction. The
three systems are: a Naïve Bayes classifier that
compares similarities based on Bayes&apos; Rule, a clas-
sifier that creates a decision list of context features,
and a classifier that compares the angles between
vectors of the features found most commonly with
each sense. The features include bigrams, and tri-
grams, and unigrams are weighted by distance
from the ambiguous word.
TorMd used an unsupervised naive Bayes classi-
fier. They combine Chinese text and an English
thesaurus to create a `Chinese word&apos;--`English
category&apos; co-occurrence matrix. This system gener-
ated the prior-probabilities and likelihoods of a
Naïve Bayes word sense classifier not from sense-
annotated (in this case English translation anno-
tated) data, but from this word--category co-
occurrence matrix. They used the Macquarie The-
saurus as very coarse sense inventory.
They asked a native speaker of Chinese to map
the English translations of the target words to ap-
propriate thesaurus categories. Once the Naïve
Bayes classifier identifies a particular category as
the intended sense, the mapping file is used to label
the target word with the corresponding English
translation. They rely simply on the bag of words
that co-occur with the target word (window size of
5 words on either side).
HIT is a fully unsupervised WSD system, which
puts bag of words of Chinese sentences and the
English translations of target ambiguous word to
search engine (Google and Baidu). Then they
could get all kinds of statistic data. The correct
translation was found through comparing their
cross entropy.
</bodyText>
<sectionHeader confidence="0.997719" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.99979">
The goal of this task is to create a framework to
evaluate Chinese word sense disambiguation and
to promote research.
</bodyText>
<page confidence="0.996816">
21
</page>
<table confidence="0.998878">
Target Sen Train Test Base- Scores
Word se # ing # # line
SRCB I2R CITY SWA TOR HIT
-WSD U-HIF T-MP MD
F 3 63 20 .50 .70 .80 .75 .75 .55 .55
成立 3 73 27 .370 .778 .815 .741 .778 .481 .407
吃 4 69 23 .435 .696 .609 .696 .696 .174 .174
出 9 222 77 .130 .506 .506 .481 .532 .169 .091
带 8 197 67 .150 .567 .552 .537 .433 .119 .104
动 4 58 20 .50 .60 .50 .55 .60 .30 .30
动摇 2 47 16 .625 .875 .875 .875 .563 .50 .438
发 5 105 36 .278 .694 .667 .611 .889 .25 .139
赶 3 56 18 .50 .667 .722 .667 .667 .389 .333
nq 4 106 39 .256 .718 .615 .641 .538 .256 .256
进 5 132 44 .227 .659 .75 .727 .568 .25 .114
开通 2 56 20 .50 .90 .95 .95 .60 .50 .50
看 4 103 34 .294 .765 .706 .765 .559 .294 .294
平息 2 20 8 .50 .75 .75 .75 .625 .375 .50
使 2 46 16 .625 .938 .813 .813 .875 .563 .438
说明 2 60 18 .556 .667 .722 .778 .722 .444 .556
挑 2 40 14 .429 .571 .643 .571 .571 .143 .286
推翻 2 29 10 .60 .80 .70 .90 .80 .30 .30
望 2 37 13 .769 .769 .769 .769 .769 .462 .462
想 4 110 37 .270 .730 .676 .676 .541 .216 .216
ff惊 2 38 14 .714 .930 1.0 .929 .786 .714 .571
Ave. 3.5 1667 571 .342/ .685/ .676/ .671/ .618/ .30/ .263/
7 .44 .728 .721 .723 .66 .355 .335
</table>
<tableCaption confidence="0.999932">
Table 3: Performance on verbs. Micro / macro average precisions are spitted by “/” at the last row.
</tableCaption>
<bodyText confidence="0.999639368421053">
Together six teams participate in this WSD task,
four of them adopt supervised learning methods
and two of them used unsupervised algorithms. All
of the four supervised learning systems exceed ob-
viously the baseline obtained by the most frequent
sense. It is noted that the performances of the first
three systems are very close. Two unsupervised
methods’ scores are below the baseline. More
unlabeled data maybe improve their performance.
Although the SRCB-WSD system got the high-
est scores among the six participants, it does not
perform always better than other system from table
2 and table 3. But to each word, the four super-
vised systems always predict correctly more in-
stances than the two un-supervised systems.
Besides the corpus, we provide a specification of
the PoS tag set. Only SRCB-WSD system utilized
this knowledge in feature selection. We will pro-
vide more instances in the next campaign.
</bodyText>
<page confidence="0.996096">
22
</page>
<table confidence="0.98384704">
Target Sen Train Test Base- Scores
Word se # ing # # line
SRCB I2R CITY SWA TOR HIT
-WSD U-HIF T-MP MD
本 3 68 25 .40 .88 .84 .88 .76 .72 .32
表面 2 53 18 .611 .611 .722 .722 .833 .556 .333
菜 2 56 19 .526 .842 .842 .684 .789 .474 .632
长城 3 48 21 .476 .571 .591 .619 .619 .429 .619
单位 2 50 17 .588 .824 .824 .824 .647 .706 .529
道 3 53 18 .50 .778 .722 .778 .611 .50 .222
队伍 3 64 22 .455 .591 .591 .636 .545 .318 .364
JL女 2 60 20 .50 1.0 .95 1.0 1.0 .50 .50
机组 2 38 14 .714 1.0 1.0 1.0 1.0 .643 .571
镜头 2 45 15 .533 .733 .733 .60 .467 .467 .467
面 3 67 23 .435 .783 .783 .739 .696 .348 .696
牌子 2 44 17 .353 .529 .589 .588 .588 .353 .529
旗gyp 3 50 18 .556 .611 .611 .722 .722 .50 .111
气息 2 39 14 .714 .929 .786 .714 .786 .857 .571
气象 2 47 16 .625 .813 .813 .938 1.0 .438 .563
日子 3 88 32 .313 .656 .563 .625 .656 .281 .344
天地 3 65 25 .40 .88 1.0 .92 .60 .56 .44
HR光 2 41 14 .714 .786 .714 .786 .643 .714 .50
中医 2 43 16 .625 .875 .938 1.0 .875 .438 .50
Ave. 2.4 1019 364 .506/ .766/ .761/ .772/ .72/ .50/ .456/
5 .528 .773 .769 .778 .728 .516 .464
</table>
<tableCaption confidence="0.998926">
Table 4: Performance on nouns. Micro / macro average precisions are spitted by “/” at the last row.
</tableCaption>
<bodyText confidence="0.4389415">
Timothy Chklovski, Rada Mihalcea, Ted Pedersen and
Amruta Purandare. 2004. The Senseval-3 Multilin-
</bodyText>
<sectionHeader confidence="0.998741" genericHeader="acknowledgments">
5 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9974441">
This research is supported by Humanity and Social
Science Research Project of China State Education
Ministry (No. 06JC740001) and National Basic
Research Program of China (No. 2004CB318102).
We would like to thank Tao Guo and Yulai Pei
for their hard work to guarantee the quality of the
corpus. Huiming Duan provides us the corpus
which has been word-segmented and PoS-tagged
and gives some suggestions during the manual an-
notation.
</bodyText>
<sectionHeader confidence="0.99964" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998776875">
Rada Mihalcea, Timothy Chklovski and Adam Kilgar-
riff. 2004. The Senseval-3 English lexical sample
task. Proceedings of SENSEVAL-3. 25-28.
gual English-Hindi lexical sample task. Proceedings of
SENSEVAL-3. 5-8.
Xiaojin Zhu, Zoubin Ghahramani. 2002. Learning from
Labeled and Unlabeled Data with Label Propagation.
CMU CALD tech report CMU-CALD-02-107.
Yunfang Wu, Peng Jin, Yangsen Zhang, and Shiwen Yu.
2006. A Chinese Corpus with Word Sense Annota-
tion. Proceedings of ICCPOL, Singapore, 414-421.
Zhen-Yu Niu, Dong-Hong Ji and Chew-Lim Tan. 2005.
Word Sense Disambiguation Using Label Propaga-
tion Based Semi Supervised Learning. Proceedings
of the 43rd Annual Meeting of the Association for
Computational Linguistics.395-402
</reference>
<page confidence="0.998926">
23
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.921978">
<title confidence="0.971559">SemEval-2007 Task 5: Multilingual Chinese-English Lexical Sample</title>
<author confidence="0.998678">Peng Jin</author>
<author confidence="0.998678">Yunfang Wu</author>
<author confidence="0.998678">Shiwen Yu</author>
<affiliation confidence="0.9870805">Institute of Computational Linguistics Peking University, Beijing China</affiliation>
<email confidence="0.984398">ljandp,wuyf,yusw}@pku.edu.cn</email>
<abstract confidence="0.997831428571429">The Multilingual Chinese-English lexical sample task at SemEval-2007 provides a framework to evaluate Chinese word sense disambiguation and to promote research. This paper reports on the task preparation and the results of six participants.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Timothy Chklovski</author>
<author>Adam Kilgarriff</author>
</authors>
<title>The Senseval-3 English lexical sample task.</title>
<date>2004</date>
<booktitle>Proceedings of SENSEVAL-3.</booktitle>
<pages>25--28</pages>
<marker>Mihalcea, Chklovski, Kilgarriff, 2004</marker>
<rawString>Rada Mihalcea, Timothy Chklovski and Adam Kilgarriff. 2004. The Senseval-3 English lexical sample task. Proceedings of SENSEVAL-3. 25-28.</rawString>
</citation>
<citation valid="false">
<title>gual English-Hindi lexical sample task.</title>
<booktitle>Proceedings of SENSEVAL-3.</booktitle>
<pages>5--8</pages>
<marker></marker>
<rawString>gual English-Hindi lexical sample task. Proceedings of SENSEVAL-3. 5-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
<author>Zoubin Ghahramani</author>
</authors>
<date>2002</date>
<booktitle>Learning from Labeled and Unlabeled Data with Label Propagation. CMU CALD tech report</booktitle>
<pages>02--107</pages>
<contexts>
<context position="6734" citStr="Zhu and Ghahramani, 2002" startWordPosition="1090" endWordPosition="1093">se phrases are considered as constituents of context, as well as words and punctuations which do not belong to any phrase. ·the type of these phrases which are around the target phrases 2 http:// maxent.sourceforge.net/ , pi = mi / ni 20 · word category information comes from Chinese thesaurus I2R system used a semi-supervised classification algorithm (label propagation algorithm) (Niu, et al., 2005). They used three types of features: PoS of neighboring words with position information, unordered single words in topical context, and local collocations. In the label propagation algorithm (LP) (Zhu and Ghahramani, 2002), label information of any vertex in a graph is propagated to nearby vertices through weighted edges until a global stable stage is achieved. Larger edge weights allow labels to travel through easier. Thus the closer the examples, the more likely they have similar labels (the global consistency assumption). In label propagation process, the soft label of each initial labeled example is clamped in each iteration to replenish label sources from these labeled data. Thus the labeled data act like sources to push out labels through unlabeled data. With this push from labeled examples, the class bou</context>
</contexts>
<marker>Zhu, Ghahramani, 2002</marker>
<rawString>Xiaojin Zhu, Zoubin Ghahramani. 2002. Learning from Labeled and Unlabeled Data with Label Propagation. CMU CALD tech report CMU-CALD-02-107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yunfang Wu</author>
<author>Peng Jin</author>
<author>Yangsen Zhang</author>
<author>Shiwen Yu</author>
</authors>
<title>A Chinese Corpus with Word Sense Annotation.</title>
<date>2006</date>
<booktitle>Proceedings of ICCPOL, Singapore,</booktitle>
<pages>414--421</pages>
<marker>Wu, Jin, Zhang, Yu, 2006</marker>
<rawString>Yunfang Wu, Peng Jin, Yangsen Zhang, and Shiwen Yu. 2006. A Chinese Corpus with Word Sense Annotation. Proceedings of ICCPOL, Singapore, 414-421.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhen-Yu Niu</author>
<author>Dong-Hong Ji</author>
<author>Chew-Lim Tan</author>
</authors>
<title>Word Sense Disambiguation Using Label Propagation Based Semi Supervised Learning.</title>
<date>2005</date>
<booktitle>Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics.395-402</booktitle>
<contexts>
<context position="6512" citStr="Niu, et al., 2005" startWordPosition="1057" endWordPosition="1060">s in the context, that is, the words with tags “n, nr, ns, nt, nz, v, vd, vn” · PoS of the left word and the right word ·noun phrase, verb phrase, adjective phrase, time phrase, place phrase and quantity phrase. These phrases are considered as constituents of context, as well as words and punctuations which do not belong to any phrase. ·the type of these phrases which are around the target phrases 2 http:// maxent.sourceforge.net/ , pi = mi / ni 20 · word category information comes from Chinese thesaurus I2R system used a semi-supervised classification algorithm (label propagation algorithm) (Niu, et al., 2005). They used three types of features: PoS of neighboring words with position information, unordered single words in topical context, and local collocations. In the label propagation algorithm (LP) (Zhu and Ghahramani, 2002), label information of any vertex in a graph is propagated to nearby vertices through weighted edges until a global stable stage is achieved. Larger edge weights allow labels to travel through easier. Thus the closer the examples, the more likely they have similar labels (the global consistency assumption). In label propagation process, the soft label of each initial labeled </context>
</contexts>
<marker>Niu, Ji, Tan, 2005</marker>
<rawString>Zhen-Yu Niu, Dong-Hong Ji and Chew-Lim Tan. 2005. Word Sense Disambiguation Using Label Propagation Based Semi Supervised Learning. Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics.395-402</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>