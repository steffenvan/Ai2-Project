<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002694">
<title confidence="0.998562">
USFD: Preliminary Exploration of Features
and Classifiers for the TempEval-2007 Tasks
</title>
<author confidence="0.998518">
Mark Hepple
</author>
<affiliation confidence="0.998538">
Dept of Computer Science
University of Sheffield
</affiliation>
<address confidence="0.852059333333333">
Regent Court
211 Portobello Street
Sheffield S1 4DP, UK
</address>
<email confidence="0.998806">
hepple@dcs.shef.ac.uk
</email>
<author confidence="0.99693">
Andrea Setzer
</author>
<affiliation confidence="0.998531">
Dept of Computer Science
University of Sheffield
</affiliation>
<address confidence="0.852060333333333">
Regent Court
211 Portobello Street
Sheffield S1 4DP, UK
</address>
<email confidence="0.998819">
andrea@dcs.shef.ac.uk
</email>
<author confidence="0.99515">
Rob Gaizauskas
</author>
<affiliation confidence="0.9985225">
Dept of Computer Science
University of Sheffield
</affiliation>
<address confidence="0.852137333333333">
Regent Court
211 Portobello Street
Sheffield S1 4DP, UK
</address>
<email confidence="0.99913">
robertg@dcs.shef.ac.uk
</email>
<sectionHeader confidence="0.995643" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.985522117647059">
We describe the Sheffield system used
in TempEval-2007. Our system takes
a machine-learning (ML) based approach,
treating temporal relation assignment as a
simple classification task and using features
easily derived from the TempEval data, i.e.
which do not require ‘deeper’ NLP analy-
sis. We aimed to explore three questions:
(1) How well would a ‘lite’ approach of
this kind perform? (2) Which features con-
tribute positively to system performance?
(3) Which ML algorithm is better suited for
the TempEval tasks? We used the Weka
ML workbench to facilitate experimenting
with different ML algorithms. The paper de-
scribes our system and supplies preliminary
answers to the above questions.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999132">
The Sheffield team were involved in TempEval as
co-proposers/co-organisers of the task.1 For our par-
ticipation in the task, we decided to pursue an ML-
based approach, the benefits of which have been ex-
plored elsewhere (Boguraev and Ando, 2005; Mani
et al., 2006). For the TempEval tasks, this is easily
done by treating the assignment of temporal relation
types as a simple classification task, using readily
available information for the instance features. More
specifically, the features used were ones provided as
</bodyText>
<footnote confidence="0.974187">
1We maintained a strict separation between persons assisting
in annotation of the test corpus and those involved in system
development.
</footnote>
<bodyText confidence="0.999013617647059">
attributes in the TempEval data annotation for the
events/times being related, plus some additional fea-
tures that could be straightforwardly computed from
documents, i.e. without the use of more heavily ‘en-
gineered’ NLP components. The aims of this work
were three-fold. First, we wanted to see whether a
‘lite’ approach of this kind could yield reasonable
performance, before pursuing possibilities that re-
lied on using ‘deeper’ NLP analysis methods. Sec-
ondly, we were interested to see which of the fea-
tures considered would contribute positively to sys-
tem performance. Thirdly, rather than selecting a
single ML approach (e.g. one of those currently in
vogue within NLP), we wanted to look across ML
algorithms to see if any approach was better suited
to the TempEval tasks than any other, and conse-
quently we used the Weka workbench (Witten and
Frank, 2005) in our ML experiments.
In what follows, we will first describe how our
system was constructed, before going on to discuss
our main observations around the key aims men-
tioned above. For example, in regard to our ‘lite’ ap-
proach, we would observe (c.f. the results reported
in the Task Description paper) that although some
other systems scored more highly, the score differ-
ences were relatively small. Regarding features, we
found for example that the system performed better
for Task A when, surprisingly, the tense attribute
of EVENTs was excluded. Regarding ML algo-
rithms, we found not only that there was substantial
variation between the effectiveness of different algo-
rithms for assigning relations (as one might expect),
but also that there was considerable differences in
the relative effectiveness of algorithms across tasks,
</bodyText>
<page confidence="0.984562">
438
</page>
<bodyText confidence="0.949416">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 438–441,
Prague, June 2007. c�2007 Association for Computational Linguistics
i.e. so that an algorithm performing well on one task
(compared to the alternatives), might perform rather
poorly on another task. The paper closes with some
comments about future research directions.
</bodyText>
<sectionHeader confidence="0.938137" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.999717853658537">
The TempEval training and test data is marked up
to identify all event and time expressions occurring
within documents, and also to record the TLINK re-
lations that are relevant for each task (except that
TLINK relation types are absent in the test data).
These annotations provide additional information
about these entities in the form of XML attributes,
e.g. for EVENT annotations we find attributes such
as tense, aspect, part-of-speech and so on.
Our system consists of a suite of Perl scripts
that create the input files required for Weka, and
handle its output. These include firstly an ‘ex-
traction’ script, which extracts information about
EVENT, TIMEXs and TLINKs from the data files, and
secondly a ‘feature selection/reformatting’ script,
which allows the information that is to be supplied
to Weka to be selected, and recasts it into the format
that Weka requires for its training/test files. A final
script takes Weka’s output over the test files and con-
nects it back to the original test documents to pro-
duce the final output files required for scoring.
The information that the first extraction script ex-
tracts for each EVENT, TIMEX and TLINK largely
corresponds to attributes/values associated with the
annotations of these items in the initial data files
(although not all such attributes are of use for ma-
chine learning purposes). In addition, the script de-
termines for each EVENT expression whether it is
one deemed relevant by the Event Target List (ETL)
for Tasks A and B. This script also maps EVENTs
and TIMEXs into sequential order – intra-sentential
order for task A and inter-sentential order for task
C. This information can be used to compute various
‘order’ features, such as:
event-first: do a related EVENT and TIMEX
(for Task A) appear with the EVENT before or after
the TIMEX?
adjacent: do a related EVENT and TIMEX (again
for Task A) appear adjacently in the sequence of
temporal entities or not? (Note that this allows an
EVENT and TIMEX to be adjacent if there tokens
</bodyText>
<table confidence="0.998561625">
Type Attribute A Task C
B
EVENT aspect ✓ ✓ ✓
EVENT polarity ✓ ✓ ×
EVENT POS ✓ ✓ ✓
EVENT stem ✓ × ×
EVENT string × × ×
EVENT class × ✓ ✓
EVENT tense × ✓ ✓
ORDER adjacent ✓ N/A N/A
ORDER event-first ✓ N/A N/A
ORDER event-between × N/A N/A
ORDER timex-between × N/A N/A
TIMEX3 mod ✓ × N/A
TIMEX3 type ✓ × N/A
TLINK reltype ✓ ✓ ✓
</table>
<tableCaption confidence="0.993401">
Table 1: Features
</tableCaption>
<bodyText confidence="0.978293571428571">
that intervene, but not any other temporal entities.)
event-between: for a related EVENT/TIMEX
pair, do any other events appear between them?
timex-between: for a related EVENT/TIMEX
pair, do any other timexes appear between them?
Table 1 lists all the features that we tried using
for any of the three tasks. Aside from the OR-
DER features (as designated in the leftmost col-
umn), which were computed as just described, and
the EVENT string feature (which is the literal
tagged expression from the text), all other features
correspond to annotation attributes. Note that the
TLINK reltype is extracted from the training data
to provide the target attribute for training (a dummy
value is provided for this in test data).
The output of the extraction script is converted to
a format suitable for use by Weka by a second script.
This script also allows a manual selection to be made
as to the features that are included. For each of the
three tasks, a rough-and-ready process was followed
to find a ‘good’ set of features for use with that
task, which proceeded as follows. Firstly, the maxi-
mal set of features considered for the task was tried
with a few ML algorithms in Weka (using a 10-fold
cross-validation over the training data) to find one
that seemed to work quite well for the task. Then
using only that algorithm, we checked whether the
string feature could be dropped (since this fea-
</bodyText>
<page confidence="0.997061">
439
</page>
<bodyText confidence="0.999975578947369">
ture’s value set was always of quite high cardinality),
i.e. if its omission improved performance, which for
all three tasks was the case. Next, we tried dropping
each of the remaining features in turn, to identify
those whose exclusion improved performance, and
then for those features so identified, tried dropping
them in combination to arrive at a final ‘optimal’ fea-
ture set. Table 1 shows for each of the tasks which
of the features were considered for inclusion (those
marked N/A were not), and which of these remained
in the final optimal feature set (✓).
Having determined the set of features for use with
each task, we tried out a range of ML algorithms
(again with a 10-fold cross-validation over the train-
ing data), to arrive at the final feature-set/ML algo-
rithm combination that was used for the task in the
competitive evaluation. This was trained over the
entire training data and applied to the test data to
produce the final submitted results.
</bodyText>
<sectionHeader confidence="0.999509" genericHeader="method">
3 Discussion
</sectionHeader>
<bodyText confidence="0.999971269230769">
Looking to Table 1, and the features that were con-
sidered for each task and then included in the final
set, various observations can be made. First, note
that the string feature was omitted for all tasks,
which is perhaps not surprising, since its values will
be sparsely distributed, so that there will be very few
training instances for most of its individual values.
However, the stem feature was found to be use-
ful for Task A, which can be interpreted as evidence
for a ‘lexical effect’ on local event-timex relations,
e.g. perhaps with different verbs displaying different
trends in how they relate to timexes. No correspond-
ing effects were observed for Tasks B and C.
The use of ORDER features for Task A was found
to be useful – specifically the features indicating
whether the event or timex appeared linearly first in
the sentence and whether the two were adjacent or
not. The more elaborate ORDER features, address-
ing more specific cases of what might intervene be-
tween the related timex and event expression, were
not found to be helpful.
Perhaps the most striking observation to be made
regarding the table is that it was found beneficial to
exclude the feature tense for Task A, whilst the
feature aspect was retained. We have no expla-
nation to offer for this result. Likewise, the event
</bodyText>
<figure confidence="0.7078125">
Task
Algorithm A B C
</figure>
<tableCaption confidence="0.9613095">
Table 2: Comparing different algorithms (%-acc.
scores, from cross-validation over training data)
</tableCaption>
<bodyText confidence="0.999559466666667">
class feature, which distinguishes e.g. perception
vs. reporting vs. aspectual etc verbs, was excluded
for Task A, although it was retained for Task B.
In regard to the use of different ML algorithms for
the classification tasks addressed in TempEval, we
observed considerable variation between algorithms
as to their performance, and this was not unexpected.
However, given the seemingly high similarity of the
three tasks, we were rather more surprised to see that
there was considerable variation between the perfor-
mance of algorithms across tasks, i.e. so that an al-
gorithm performing well on one task (compared to
the alternatives), might perform rather poorly on an-
other task. This is illustrated by the results in Table 2
for a selected subset of the algorithms considered,
which shows %-accuracy scores that were computed
by cross-validation over the training data, using the
feature set chosen as ‘optimal’ for each task.2 The
algorithm names in the left-hand column are the
ones used in WEKA (of which functions.SMO
is the WEKA implementation of support-vector ma-
chines or SVM). The first row of results give a ‘base-
line’ for performance, corresponding to the assign-
ment of the most common label for the task. (These
were produced using WEKA’s rules.ZeroR al-
gorithm, which does exactly that.)
The best results observed for each task are shown
in bold in the table. These best performing al-
gorithms were used for the corresponding tasks in
the competition. Observe that the lazy.KStar
</bodyText>
<footnote confidence="0.932853833333333">
2These scores are computed under the ‘strict’ requirement
that key and response labels should be identical. The TempE-
val competition also uses a ‘relaxed’ metric which gives par-
tial credit when one (or both) label is disjunctive and there is a
partial match, e.g. between labels AFTER and OVERLAP-OR-
AFTER. See (Verhagen et al., 2007) for details.
</footnote>
<table confidence="0.922897944444444">
baseline
lazy.KStar
rules.DecisionTable
functions.SMO (svm)
rules.JRip
bayes.NaiveBayes
49.8 62.1 42.0
58.2 76.7 54.0
53.3 79.0 52.9
55.1 78.1 55.5
50.7 78.6 53.4
56.3 76.2 50.7
440
Task A Task B Task C
FS FR FS FR FS FR
USFD 0.59 0.60 0.73 0.74 0.54 0.59
ave. 0.56 0.59 0.74 0.75 0.51 0.60
max. 0.62 0.64 0.80 0.81 0.55 0.66
</table>
<tableCaption confidence="0.732081666666667">
Table 3: Competition task scores for Sheffield sys-
tem (USFD), plus average/max scores across all
competing systems
</tableCaption>
<bodyText confidence="0.999985444444444">
method, which gives the best performance for Task
A, gives a rather ‘middling’ performance for Task
B. Similarly, the SVM method that gives the best
results for Task C falls quite a way below the per-
formance of KStar on Task A. A more extreme
case is seen with the results for rules.JRip
(Weka’s implementation of the RIPPER algorithm),
whose score for Task B is close to that of the best-
performing system, but which scores only slightly
above baseline on Task A.
The competition scores for our system are given
in Table 3, shown as (harmonic) F-measures under
both strict (FS) and relaxed (FR) metrics (see foot-
note 2). The table also shows the average score for
each task/metric across all systems taking part in the
competition, as well as the maximum score returned
by any system. See (Verhagen et al., 2007) for a full
tabulation of results for all systems.3
</bodyText>
<sectionHeader confidence="0.9992" genericHeader="method">
4 Future Directions
</sectionHeader>
<bodyText confidence="0.874856">
SIGNALs and SLINKs are possible candidates as
additional features – signals obviously so, whereas
the benefits of exploiting subordination information
are less clear. Our initial exploratory efforts in
this direction involved pulling information regard-
ing SIGNALs and SLINKs across from TimeBank4
(Pustejovsky et al., 2003) so as to make this avail-
3The TempEval test data identifies precisely the temporal
entity pairs to which a relation label must be assigned. When
a fixed set of items is classified, the scores for precision, recall
and F-measure will be identical, being the same as the score for
simple accuracy. However, not all the participating systems fol-
low this pattern of assigning labels to ‘all and only’ the entity
pairs identified in the test data, i.e. some systems decide which
entity pairs to label, as well as which label to assign. Accord-
ingly, the performance results given in (Verhagen et al., 2007)
are reported using metrics of precision, recall and F-measure.
4This was possible because both the trial and training data
were derived from TimeBank.
able for use with the TempEval tasks, in the hope
that this would allow us to determine if this informa-
tion would be useful without first facing the cost of
developing SIGNAL and SLINK recognisers. Re-
garding SIGNALs, however, we ran into the prob-
lem that there are many TLINKs in the TempEval
data for which no corresponding TLINK appears
in TimeBank, and hence for which SIGNAL infor-
mation could not be imported. We were unable to
progress this work sufficiently in the time available
for there to be any useful results to report here.
</bodyText>
<sectionHeader confidence="0.999198" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999978">
We have explored using a ML-based approach to
the TempEval tasks, which does not rely on the use
of deeper NLP-analysis components. We observe
that although some other systems in the competi-
tion have produced higher scores for the tasks, the
score differences are relatively small. In the course
of this work, we have made some interesting ob-
servations regarding the performance variability of
different ML algorithms when applied to the diffent
TempEval tasks, and regarding the features that con-
tribute to the system’s performance.
</bodyText>
<sectionHeader confidence="0.999263" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999476826086957">
B. Boguraev and R. Kubota Ando. 2005. TimeML-
Compliant Text Analysis for Temporal Reasoning. In
Proceedings ofIJCAI-05, pages 997–1003.
Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong Min
Lee, and James Pustejovsky. 2006. Machine Learning
of Temporal Relations. In ACL ’06: Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the ACL,
pages 753–760, Morristown, NJ, USA. Association for
Computational Linguistics.
J. Pustejovsky, D. Day, L. Ferro, R. Gaizauskas, P. Hanks,
M. Lazo, D. Radev, R. Sauri, A. See, A. Setzer, and
B. Sundheim. 2003. The TIMEBANK Corpus. In
Proceedings of Corpus Linguistics 2003, pages 647–
656, Lancaster, March.
M. Verhagen, R. Gaizauskas, F. Schilder, M. Hepple,
G. Katz, and J. Pustejovsky. 2007. SemEval-2007
Task 15: TempEval Temporal Relation Identification.
In Proceedings of SemEval-2007: 4th International
Workshop on Semantic Evaluations.
I.H. Witten and E. Frank, editors. 2005. Data Mining:
Practical Machine Learning Tools and Techniques.
Morgan Kaufmann, San Francisco, second edition.
</reference>
<page confidence="0.998688">
441
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.880979">
<title confidence="0.99445">USFD: Preliminary Exploration of Features and Classifiers for the TempEval-2007 Tasks</title>
<author confidence="0.99998">Mark Hepple</author>
<affiliation confidence="0.995639333333333">Dept of Computer Science University of Sheffield Regent Court</affiliation>
<address confidence="0.995702">211 Portobello Street Sheffield S1 4DP, UK</address>
<email confidence="0.991367">hepple@dcs.shef.ac.uk</email>
<author confidence="0.999966">Andrea Setzer</author>
<affiliation confidence="0.995641333333333">Dept of Computer Science University of Sheffield Regent Court</affiliation>
<address confidence="0.995216">211 Portobello Street Sheffield S1 4DP, UK</address>
<email confidence="0.979624">andrea@dcs.shef.ac.uk</email>
<author confidence="0.999218">Rob Gaizauskas</author>
<affiliation confidence="0.99564">Dept of Computer Science University of Sheffield Regent Court</affiliation>
<address confidence="0.995865">211 Portobello Street Sheffield S1 4DP, UK</address>
<email confidence="0.998314">robertg@dcs.shef.ac.uk</email>
<abstract confidence="0.998952388888889">We describe the Sheffield system used in TempEval-2007. Our system takes a machine-learning (ML) based approach, treating temporal relation assignment as a simple classification task and using features easily derived from the TempEval data, i.e. which do not require ‘deeper’ NLP analysis. We aimed to explore three questions: (1) How well would a ‘lite’ approach of this kind perform? (2) Which features contribute positively to system performance? (3) Which ML algorithm is better suited for the TempEval tasks? We used the Weka ML workbench to facilitate experimenting with different ML algorithms. The paper describes our system and supplies preliminary answers to the above questions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B Boguraev</author>
<author>R Kubota Ando</author>
</authors>
<title>TimeMLCompliant Text Analysis for Temporal Reasoning.</title>
<date>2005</date>
<booktitle>In Proceedings ofIJCAI-05,</booktitle>
<pages>997--1003</pages>
<contexts>
<context position="1464" citStr="Boguraev and Ando, 2005" startWordPosition="214" endWordPosition="217">ns: (1) How well would a ‘lite’ approach of this kind perform? (2) Which features contribute positively to system performance? (3) Which ML algorithm is better suited for the TempEval tasks? We used the Weka ML workbench to facilitate experimenting with different ML algorithms. The paper describes our system and supplies preliminary answers to the above questions. 1 Introduction The Sheffield team were involved in TempEval as co-proposers/co-organisers of the task.1 For our participation in the task, we decided to pursue an MLbased approach, the benefits of which have been explored elsewhere (Boguraev and Ando, 2005; Mani et al., 2006). For the TempEval tasks, this is easily done by treating the assignment of temporal relation types as a simple classification task, using readily available information for the instance features. More specifically, the features used were ones provided as 1We maintained a strict separation between persons assisting in annotation of the test corpus and those involved in system development. attributes in the TempEval data annotation for the events/times being related, plus some additional features that could be straightforwardly computed from documents, i.e. without the use of</context>
</contexts>
<marker>Boguraev, Ando, 2005</marker>
<rawString>B. Boguraev and R. Kubota Ando. 2005. TimeMLCompliant Text Analysis for Temporal Reasoning. In Proceedings ofIJCAI-05, pages 997–1003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
<author>Marc Verhagen</author>
<author>Ben Wellner</author>
<author>Chong Min Lee</author>
<author>James Pustejovsky</author>
</authors>
<title>Machine Learning of Temporal Relations.</title>
<date>2006</date>
<booktitle>In ACL ’06: Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the ACL,</booktitle>
<pages>753--760</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1484" citStr="Mani et al., 2006" startWordPosition="218" endWordPosition="221">‘lite’ approach of this kind perform? (2) Which features contribute positively to system performance? (3) Which ML algorithm is better suited for the TempEval tasks? We used the Weka ML workbench to facilitate experimenting with different ML algorithms. The paper describes our system and supplies preliminary answers to the above questions. 1 Introduction The Sheffield team were involved in TempEval as co-proposers/co-organisers of the task.1 For our participation in the task, we decided to pursue an MLbased approach, the benefits of which have been explored elsewhere (Boguraev and Ando, 2005; Mani et al., 2006). For the TempEval tasks, this is easily done by treating the assignment of temporal relation types as a simple classification task, using readily available information for the instance features. More specifically, the features used were ones provided as 1We maintained a strict separation between persons assisting in annotation of the test corpus and those involved in system development. attributes in the TempEval data annotation for the events/times being related, plus some additional features that could be straightforwardly computed from documents, i.e. without the use of more heavily ‘engin</context>
</contexts>
<marker>Mani, Verhagen, Wellner, Lee, Pustejovsky, 2006</marker>
<rawString>Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong Min Lee, and James Pustejovsky. 2006. Machine Learning of Temporal Relations. In ACL ’06: Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the ACL, pages 753–760, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
<author>D Day</author>
<author>L Ferro</author>
<author>R Gaizauskas</author>
<author>P Hanks</author>
<author>M Lazo</author>
<author>D Radev</author>
<author>R Sauri</author>
<author>A See</author>
<author>A Setzer</author>
<author>B Sundheim</author>
</authors>
<title>The TIMEBANK Corpus.</title>
<date>2003</date>
<booktitle>In Proceedings of Corpus Linguistics</booktitle>
<pages>647--656</pages>
<location>Lancaster,</location>
<contexts>
<context position="13593" citStr="Pustejovsky et al., 2003" startWordPosition="2239" endWordPosition="2242">relaxed (FR) metrics (see footnote 2). The table also shows the average score for each task/metric across all systems taking part in the competition, as well as the maximum score returned by any system. See (Verhagen et al., 2007) for a full tabulation of results for all systems.3 4 Future Directions SIGNALs and SLINKs are possible candidates as additional features – signals obviously so, whereas the benefits of exploiting subordination information are less clear. Our initial exploratory efforts in this direction involved pulling information regarding SIGNALs and SLINKs across from TimeBank4 (Pustejovsky et al., 2003) so as to make this avail3The TempEval test data identifies precisely the temporal entity pairs to which a relation label must be assigned. When a fixed set of items is classified, the scores for precision, recall and F-measure will be identical, being the same as the score for simple accuracy. However, not all the participating systems follow this pattern of assigning labels to ‘all and only’ the entity pairs identified in the test data, i.e. some systems decide which entity pairs to label, as well as which label to assign. Accordingly, the performance results given in (Verhagen et al., 2007)</context>
</contexts>
<marker>Pustejovsky, Day, Ferro, Gaizauskas, Hanks, Lazo, Radev, Sauri, See, Setzer, Sundheim, 2003</marker>
<rawString>J. Pustejovsky, D. Day, L. Ferro, R. Gaizauskas, P. Hanks, M. Lazo, D. Radev, R. Sauri, A. See, A. Setzer, and B. Sundheim. 2003. The TIMEBANK Corpus. In Proceedings of Corpus Linguistics 2003, pages 647– 656, Lancaster, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Verhagen</author>
<author>R Gaizauskas</author>
<author>F Schilder</author>
<author>M Hepple</author>
<author>G Katz</author>
<author>J Pustejovsky</author>
</authors>
<title>SemEval-2007 Task 15: TempEval Temporal Relation Identification.</title>
<date>2007</date>
<booktitle>In Proceedings of SemEval-2007: 4th International Workshop on Semantic Evaluations.</booktitle>
<contexts>
<context position="11931" citStr="Verhagen et al., 2007" startWordPosition="1961" endWordPosition="1964"> for the task. (These were produced using WEKA’s rules.ZeroR algorithm, which does exactly that.) The best results observed for each task are shown in bold in the table. These best performing algorithms were used for the corresponding tasks in the competition. Observe that the lazy.KStar 2These scores are computed under the ‘strict’ requirement that key and response labels should be identical. The TempEval competition also uses a ‘relaxed’ metric which gives partial credit when one (or both) label is disjunctive and there is a partial match, e.g. between labels AFTER and OVERLAP-ORAFTER. See (Verhagen et al., 2007) for details. baseline lazy.KStar rules.DecisionTable functions.SMO (svm) rules.JRip bayes.NaiveBayes 49.8 62.1 42.0 58.2 76.7 54.0 53.3 79.0 52.9 55.1 78.1 55.5 50.7 78.6 53.4 56.3 76.2 50.7 440 Task A Task B Task C FS FR FS FR FS FR USFD 0.59 0.60 0.73 0.74 0.54 0.59 ave. 0.56 0.59 0.74 0.75 0.51 0.60 max. 0.62 0.64 0.80 0.81 0.55 0.66 Table 3: Competition task scores for Sheffield system (USFD), plus average/max scores across all competing systems method, which gives the best performance for Task A, gives a rather ‘middling’ performance for Task B. Similarly, the SVM method that gives the b</context>
<context position="13198" citStr="Verhagen et al., 2007" startWordPosition="2182" endWordPosition="2185">w the performance of KStar on Task A. A more extreme case is seen with the results for rules.JRip (Weka’s implementation of the RIPPER algorithm), whose score for Task B is close to that of the bestperforming system, but which scores only slightly above baseline on Task A. The competition scores for our system are given in Table 3, shown as (harmonic) F-measures under both strict (FS) and relaxed (FR) metrics (see footnote 2). The table also shows the average score for each task/metric across all systems taking part in the competition, as well as the maximum score returned by any system. See (Verhagen et al., 2007) for a full tabulation of results for all systems.3 4 Future Directions SIGNALs and SLINKs are possible candidates as additional features – signals obviously so, whereas the benefits of exploiting subordination information are less clear. Our initial exploratory efforts in this direction involved pulling information regarding SIGNALs and SLINKs across from TimeBank4 (Pustejovsky et al., 2003) so as to make this avail3The TempEval test data identifies precisely the temporal entity pairs to which a relation label must be assigned. When a fixed set of items is classified, the scores for precision</context>
</contexts>
<marker>Verhagen, Gaizauskas, Schilder, Hepple, Katz, Pustejovsky, 2007</marker>
<rawString>M. Verhagen, R. Gaizauskas, F. Schilder, M. Hepple, G. Katz, and J. Pustejovsky. 2007. SemEval-2007 Task 15: TempEval Temporal Relation Identification. In Proceedings of SemEval-2007: 4th International Workshop on Semantic Evaluations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I H Witten</author>
<author>E Frank</author>
<author>editors</author>
</authors>
<date>2005</date>
<booktitle>Data Mining: Practical Machine Learning Tools and Techniques.</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco,</location>
<note>second edition.</note>
<marker>Witten, Frank, editors, 2005</marker>
<rawString>I.H. Witten and E. Frank, editors. 2005. Data Mining: Practical Machine Learning Tools and Techniques. Morgan Kaufmann, San Francisco, second edition.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>