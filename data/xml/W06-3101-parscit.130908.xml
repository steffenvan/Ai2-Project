<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.069764">
<title confidence="0.9988575">
Morpho-syntactic Information for Automatic Error Analysis of Statistical
Machine Translation Output
</title>
<author confidence="0.816569">
Maja Popovi´c* Adri`a de Gispert† Deepa Gupta⊥ Patrik Lambert†
Hermann Ney* Jos´e B. Mari˜no† Marcello Federico⊥ Rafael Banchs†
</author>
<affiliation confidence="0.965972">
� Lehrstuhl f¨ur Informatik VI - Computer Science Department, RWTH Aachen University, Aachen, Germany
† TALP Research Center, Universitat Polit`ecnica de Catalunya (UPC), Barcelona, Spain
</affiliation>
<address confidence="0.797092">
1 ITC-irst, Centro per la Ricerca Scientifica e Tecnologica, Trento, Italy
</address>
<email confidence="0.981753">
{popovic,ney}@informatik.rwth-aachen.de {agispert,canton}@gps.tsc.upc.es
{gupta,federico}@itc.it {lambert,banchs}@gps.tsc.upc.es
</email>
<sectionHeader confidence="0.995388" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999972791666667">
Evaluation of machine translation output
is an important but difficult task. Over the
last years, a variety of automatic evalua-
tion measures have been studied, some of
them like Word Error Rate (WER), Posi-
tion Independent Word Error Rate (PER)
and BLEU and NIST scores have become
widely used tools for comparing different
systems as well as for evaluating improve-
ments within one system. However, these
measures do not give any details about
the nature of translation errors. Therefore
some analysis of the generated output is
needed in order to identify the main prob-
lems and to focus the research efforts. On
the other hand, human evaluation is a time
consuming and expensive task. In this
paper, we investigate methods for using
of morpho-syntactic information for auto-
matic evaluation: standard error measures
WER and PER are calculated on distinct
word classes and forms in order to get a
better idea about the nature of translation
errors and possibilities for improvements.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999895076923077">
The evaluation of the generated output is an impor-
tant issue for all natural language processing (NLP)
tasks, especially for machine translation (MT). Au-
tomatic evaluation is preferred because human eval-
uation is a time consuming and expensive task.
A variety of automatic evaluation measures have
been proposed and studied over the last years, some
of them are shown to be a very useful tool for com-
paring different systems as well as for evaluating
improvements within one system. The most widely
used are Word Error Rate (WER), Position Indepen-
dent Word Error Rate (PER), the BLEU score (Pap-
ineni et al., 2002) and the NIST score (Doddington,
2002). However, none of these measures give any
details about the nature of translation errors. A rela-
tionship between these error measures and the actual
errors in the translation outputs is not easy to find.
Therefore some analysis of the translation errors is
necessary in order to define the main problems and
to focus the research efforts. A framework for hu-
man error analysis and error classification has been
proposed in (Vilar et al., 2006), but like human eval-
uation, this is also a time consuming task.
The goal of this work is to present a framework
for automatic error analysis of machine translation
output based on morpho-syntactic information.
</bodyText>
<sectionHeader confidence="0.999833" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999589818181818">
There is a number of publications dealing with
various automatic evaluation measures for machine
translation output, some of them proposing new
measures, some proposing improvements and exten-
sions of the existing ones (Doddington, 2002; Pap-
ineni et al., 2002; Babych and Hartley, 2004; Ma-
tusov et al., 2005). Semi-automatic evaluation mea-
sures have been also investigated, for example in
(Nießen et al., 2000). An automatic metric which
uses base forms and synonyms of the words in or-
der to correlate better to human judgements has been
</bodyText>
<page confidence="0.736188">
1
</page>
<subsectionHeader confidence="0.793439">
Proceedings of the Workshop on Statistical Machine Translation, pages 1–6,
New York City, June 2006. @2006 Association for Computational Linguistics
</subsectionHeader>
<bodyText confidence="0.999402166666667">
proposed in (Banerjee and Lavie, 2005). However,
error analysis is still a rather unexplored area. A
framework for human error analysis and error clas-
sification has been proposed in (Vilar et al., 2006)
and a detailed analysis of the obtained results has
been carried out. Automatic methods for error anal-
ysis to our knowledge have not been studied yet.
Many publications propose the use of morpho-
syntactic information for improving the perfor-
mance of a statistical machine translation system.
Various methods for treating morphological and
syntactical differences between German and English
are investigated in (Nießen and Ney, 2000; Nießen
and Ney, 2001a; Nießen and Ney, 2001b). Mor-
phological analysis has been used for improving
Arabic-English translation (Lee, 2004), for Serbian-
English translation (Popovi´c et al., 2005) as well as
for Czech-English translation (Goldwater and Mc-
Closky, 2005). Inflectional morphology of Spanish
verbs is dealt with in (Popovi´c and Ney, 2004; de
Gispert et al., 2005). To the best of our knowledge,
the use of morpho-syntactic information for error
analysis of translation output has not been investi-
gated so far.
</bodyText>
<sectionHeader confidence="0.954421" genericHeader="method">
3 Morpho-syntactic Information and
Automatic Evaluation
</sectionHeader>
<bodyText confidence="0.998546857142857">
We propose the use of morpho-syntactic informa-
tion in combination with the automatic evaluation
measures WER and PER in order to get more details
about the translation errors.
We investigate two types of potential problems for
the translation with the Spanish-English language
pair:
</bodyText>
<listItem confidence="0.998903">
• syntactic differences between the two lan-
guages considering nouns and adjectives
• inflections in the Spanish language considering
mainly verbs, adjectives and nouns
</listItem>
<bodyText confidence="0.990576">
As any other automatic evaluation measures,
these novel measures will be far from perfect. Pos-
sible POS-tagging errors may introduce additional
noise. However, we expect this noise to be suffi-
ciently small and the new measures to be able to give
sufficiently clear ideas about particular errors.
</bodyText>
<subsectionHeader confidence="0.999257">
3.1 Syntactic differences
</subsectionHeader>
<bodyText confidence="0.999985214285714">
Adjectives in the Spanish language are usually
placed after the corresponding noun, whereas in En-
glish is the other way round. Although in most cases
the phrase based translation system is able to han-
dle these local permutations correctly, some errors
are still present, especially for unseen or rarely seen
noun-adjective groups. In order to investigate this
type of errors, we extract the nouns and adjectives
from both the reference translations and the sys-
tem output and then calculate WER and PER. If the
difference between the obtained WER and PER is
large, this indicates reordering errors: a number of
nouns and adjectives is translated correctly but in the
wrong order.
</bodyText>
<subsectionHeader confidence="0.99891">
3.2 Spanish inflections
</subsectionHeader>
<bodyText confidence="0.999984625">
Spanish has a rich inflectional morphology, espe-
cially for verbs. Person and tense are expressed
by the suffix so that many different full forms of
one verb exist. Spanish adjectives, in contrast to
English, have four possible inflectional forms de-
pending on gender and number. Therefore the er-
ror rates for those word classes are expected to be
higher for Spanish than for English. Also, the er-
ror rates for the Spanish base forms are expected to
be lower than for the full forms. In order to investi-
gate potential inflection errors, we compare the PER
for verbs, adjectives and nouns for both languages.
For the Spanish language, we also investigate differ-
ences between full form PER and base form PER:
the larger these differences, more inflection errors
are present.
</bodyText>
<sectionHeader confidence="0.991843" genericHeader="method">
4 Experimental Settings
</sectionHeader>
<subsectionHeader confidence="0.995994">
4.1 Task and Corpus
</subsectionHeader>
<bodyText confidence="0.9997873">
The corpus analysed in this work is built in the
framework of the TC-Star project. It contains more
than one million sentences and about 35 million run-
ning words of the Spanish and English European
Parliament Plenary Sessions (EPPS). A description
of the EPPS data can be found in (Vilar et al., 2005).
In order to analyse effects of data sparseness, we
have randomly extracted a small subset referred to
as 13k containing about thirteen thousand sentences
and 370k running words (about 1% of the original
</bodyText>
<page confidence="0.993496">
2
</page>
<table confidence="0.999917052631579">
Training corpus: Spanish English
full Sentences 1281427
Running Words 36578514 34918192
Vocabulary 153124 106496
Singletons [%] 35.2 36.2
13k Sentences 13360
Running Words 385198 366055
Vocabulary 22425 16326
Singletons [%] 47.6 43.7
Dev: Sentences 1008
Running Words 25778 26070
Distinct Words 3895 3173
OOVs (full) [%] 0.15 0.09
OOVs (13k) [%] 2.7 1.7
Test: Sentences 840 1094
Running Words 22774 26917
Distinct Words 4081 3958
OOVs (full) [%] 0.14 0.25
OOVs (13k) [%] 2.8 2.6
</table>
<tableCaption confidence="0.9991832">
Table 1: Corpus statistics for the Spanish-English
EPPS task (running words include punctuation
marks)
corpus). The statistics of the corpora can be seen in
Table 1.
</tableCaption>
<subsectionHeader confidence="0.991699">
4.2 Translation System
</subsectionHeader>
<bodyText confidence="0.999982">
The statistical machine translation system used in
this work is based on a log-linear combination of
seven different models. The most important ones are
phrase based models in both directions, additionally
IBM1 models at the phrase level in both directions
as well as phrase and length penalty are used. A
more detailed description of the system can be found
in (Vilar et al., 2005; Zens et al., 2005).
</bodyText>
<subsectionHeader confidence="0.965818">
4.3 Experiments
</subsectionHeader>
<bodyText confidence="0.999819333333333">
The translation experiments have been done in both
translation directions on both sizes of the corpus. In
order to examine improvements of the baseline sys-
tem, a new system with POS-based word reorderings
of nouns and adjectives as proposed in (Popovi´c and
Ney, 2006) is also analysed. Adjectives in the Span-
ish language are usually placed after the correspond-
ing noun, whereas for English it is the other way
round. Therefore, local reorderings of nouns and ad-
</bodyText>
<table confidence="0.9997576">
Spanish-*English WER PER BLEU
full baseline 34.5 25.5 54.7
reorder 33.5 25.2 56.4
13k baseline 41.8 30.7 43.2
reorder 38.9 29.5 48.5
English-*Spanish WER PER BLEU
full baseline 39.7 30.6 47.8
reorder 39.6 30.5 48.3
13k baseline 49.6 37.4 36.2
reorder 48.1 36.5 37.7
</table>
<tableCaption confidence="0.998833">
Table 2: Translation Results [%]
</tableCaption>
<bodyText confidence="0.9998447">
jective groups in the source language have been ap-
plied. If the source language is Spanish, each noun is
moved behind the corresponding adjective group. If
the source language is English, each adjective group
is moved behind the corresponding noun. An adverb
followed by an adjective (e.g. ”more important”) or
two adjectives with a coordinate conjunction in be-
tween (e.g. ”economic and political”) are treated as
an adjective group. Standard translation results are
presented in Table 2.
</bodyText>
<sectionHeader confidence="0.999163" genericHeader="method">
5 Error Analysis
</sectionHeader>
<subsectionHeader confidence="0.970522">
5.1 Syntactic errors
</subsectionHeader>
<bodyText confidence="0.9997839">
As explained in Section 3.1, reordering errors due
to syntactic differences between two languages have
been measured by the relative difference between
WER and PER calculated on nouns and adjectives.
Corresponding relative differences are calculated
also for verbs as well as adjectives and nouns sep-
arately.
Table 3 presents the relative differences for the
English and Spanish output. It can be seen that
the PER/WER difference for nouns and adjectives
is relatively high for both language pairs (more than
20%), and for the English output is higher than for
the Spanish one. This corresponds to the fact that
the Spanish language has a rather free word order:
although the adjective usually is placed behind the
noun, this is not always the case. On the other hand,
adjectives in English are always placed before the
corresponding noun. It can also be seen that the
difference is higher for the reduced corpus for both
outputs indicating that the local reordering problem
</bodyText>
<page confidence="0.997539">
3
</page>
<table confidence="0.999475846153846">
English output 1 − PER
WER
full nouns+adjectives 24.7
+reordering 20.8
verbs 4.1
adjectives 10.2
nouns 20.1
13k nouns+adjectives 25.7
+reordering 20.1
verbs 4.6
adjectives 8.4
nouns 19.1
Spanish output PER
1
−
WER
full nouns+adjectives 21.5
+reordering 20.3
verbs 3.3
adjectives 5.6
nouns 16.9
13k nouns+adjectives 22.9
+reordering 19.8
verbs 3.9
adjectives 5.4
nouns 19.3
</table>
<tableCaption confidence="0.855573">
Table 3: Relative difference between PER and
WER [%] for different word classes
</tableCaption>
<bodyText confidence="0.9996495">
is more important when only small amount of train-
ing data is available. As mentioned in Section 3.1,
the phrase based translation system is able to gen-
erate frequent noun-adjective groups in the correct
word order, but unseen or rarely seen groups intro-
duce difficulties.
Furthermore, the results show that the POS-based
reordering of adjectives and nouns leads to a de-
crease of the PER/WER difference for both out-
puts and for both corpora. Relative decrease of the
PER/WER difference is larger for the small corpus
than for the full corpus. It can also be noted that the
relative decrease for both corpora is larger for the
English output than for the Spanish one due to free
word order - since the Spanish adjective group is not
always placed behind the noun, some reorderings in
English are not really needed.
For the verbs, PER/WER difference is less than
5% for both outputs and both training corpora, in-
dicating that the word order of verbs is not an im-
</bodyText>
<table confidence="0.996257642857143">
English output PER
full verbs 44.8
adjectives 27.3
nouns 23.0
13k verbs 56.1
adjectives 38.1
nouns 31.7
Spanish output PER
full verbs 61.4
adjectives 41.8
nouns 28.5
13k verbs 73.0
adjectives 50.9
nouns 37.0
</table>
<tableCaption confidence="0.999794">
Table 4: PER [%] for different word classes
</tableCaption>
<bodyText confidence="0.999819428571429">
portant issue for the Spanish-English language pair.
PER/WER difference for adjectives and nouns is
higher than for verbs, for the nouns being signifi-
cantly higher than for adjectives. The reason for this
is probably the fact that word order differences in-
volving only the nouns are also present, for example
“export control = control de exportaci´on”.
</bodyText>
<subsectionHeader confidence="0.995301">
5.2 Inflectional errors
</subsectionHeader>
<bodyText confidence="0.99968535">
Table 4 presents the PER for different word classes
for the English and Spanish output respectively. It
can be seen that all PERs are higher for the Spanish
output than for the English one due to the rich in-
flectional morphology of the Spanish language. It
can be also seen that the Spanish verbs are espe-
cially problematic (as stated in (Vilar et al., 2006))
reaching 60% of PER for the full corpus and more
than 70% for the reduced corpus. Spanish adjectives
also have a significantly higher PER than the English
ones, whereas for the nouns this difference is not so
high.
Results of the further analysis of inflectional er-
rors are presented in Table 5. Relative difference
between full form PER and base form PER is sig-
nificantly lower for adjectives and nouns than for
verbs, thus showing that the verb inflections are the
main source of translation errors into the Spanish
language.
Furthermore, it can be seen that for the small cor-
</bodyText>
<page confidence="0.992158">
4
</page>
<table confidence="0.997159">
Spanish output 1 − ����
����
full verbs 26.9
adjectives 9.3
nouns 8.4
13k verbs 23.7
adjectives 15.1
nouns 6.5
</table>
<tableCaption confidence="0.802989">
Table 5: Relative difference between PER of base
forms and PER of full forms [%] for the Spanish
output
</tableCaption>
<bodyText confidence="0.9983781">
pus base/full PER difference for verbs and nouns is
basically the same as for the full corpus. Since nouns
in Spanish only have singular and plural form as in
English, the number of unseen forms is not partic-
ularly enlarged by the reduction of the training cor-
pus. On the other hand, base/full PER difference of
adjectives is significantly higher for the small corpus
due to an increased number of unseen adjective full
forms.
As for verbs, intuitively it might be expected that
the number of inflectional errors for this word class
also increases by reducing the training corpus, even
more than for adjectives. However, the base/full
PER difference is not larger for the small corpus,
but even smaller. This is indicating that the problem
of choosing the right inflection of a Spanish verb ap-
parently is not related to the number of unseen full
forms since the number of inflectional errors is very
high even when the translation system is trained on
a very large corpus.
</bodyText>
<sectionHeader confidence="0.995019" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999986434782609">
In this work, we presented a framework for auto-
matic analysis of translation errors based on the use
of morpho-syntactic information. We carried out a
detailed analysis which has shown that the results
obtained by our method correspond to those ob-
tained by human error analysis in (Vilar et al., 2006).
Additionally, it has been shown that the improve-
ments of the baseline system can be adequately mea-
sured as well.
This work is just a first step towards the devel-
opment of linguistically-informed evaluation mea-
sures which provide partial and more specific infor-
mation of certain translation problems. Such mea-
sures are very important to understand what are the
weaknesses of a statistical machine translation sys-
tem, and what are the best ways and methods for
improvements.
For our future work, we plan to extend the pro-
posed measures in order to carry out a more de-
tailed error analysis, for example examinating dif-
ferent types of inflection errors for Spanish verbs.
We also plan to investigate other types of translation
errors and other language pairs.
</bodyText>
<sectionHeader confidence="0.994731" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99540475">
This work was partly supported by the TC-STAR
project by the European Community (FP6-506738)
and partly by the Generalitat de Catalunya and the
European Social Fund.
</bodyText>
<sectionHeader confidence="0.998842" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997439">
Bogdan Babych and Anthony Hartley. 2004. Extending
bleu mt evaluation method with frequency weighting.
In Proc. of the 42nd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), Barcelona,
Spain, July.
Satanjeev Banerjee and Alon Lavie. 2005. Meteor:
An automatic metric for mt evaluation with improved
correlation with human judgements. In 43rd Annual
Meeting of the Assoc. for Computational Linguistics:
Proc. Workshop on Intrinsic and Extrinsic Evaluation
Measuresfor MT and/or Summarization, pages 65–72,
Ann Arbor, MI, June.
Adri`a de Gispert, Jos´e B. Mari˜no, and Josep M. Crego.
2005. Improving statistical machine translation by
classifying and generalizing inflected verb forms. In
Proc. of the 9th European Conf. on Speech Commu-
nication and Technology (Interspeech), pages 3185–
3188, Lisbon, Portugal, September.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proc. ARPA Workshop on Human Lan-
guage Technology, pages 128–132, San Diego.
Sharon Goldwater and David McClosky. 2005. Improv-
ing stastistical machine translation through morpho-
logical analysis. In Proc. of the Conf. on Empirical
Methods for Natural Language Processing (EMNLP),
Vancouver, Canada, October.
Young-suk Lee. 2004. Morphological analysis for statis-
tical machine translation. In Proc. 2004 Meeting of the
North American chapter of the Association for Compu-
tational Linguistics (HLT-NAACL), Boston, MA, May.
</reference>
<page confidence="0.956704">
5
</page>
<reference confidence="0.999053873015873">
Evgeny Matusov, Gregor Leusch, Oliver Bender, and
Hermann Ney. 2005. Evaluating machine transla-
tion output with automatic sentence segmentation. In
Proceedings of the International Workshop on Spoken
Language Translation (IWSLT), pages 148–154, Pitts-
burgh, PA, October.
Sonja Nießen and Hermann Ney. 2000. Improving SMT
quality with morpho-syntactic analysis. In COLING
’00: The 18th Int. Conf. on Computational Linguistics,
pages 1081–1085, Saarbr¨ucken, Germany, July.
Sonja Nießen and Hermann Ney. 2001a. Morpho-
syntactic analysis for reordering in statistical machine
translation. In Proc. MT Summit VIII, pages 247–252,
Santiago de Compostela, Galicia, Spain, September.
Sonja Nießen and Hermann Ney. 2001b. Toward hier-
archical models for statistical machine translation of
inflected languages. In Data-Driven Machine Trans-
lation Workshop, pages 47–54, Toulouse, France, July.
Sonja Nießen, Franz J. Och, Gregor Leusch, and Her-
mann Ney. 2000. An evaluation tool for ma-
chine translation: Fast evaluation for mt research. In
Proc. Second Int. Conf. on Language Resources and
Evaluation (LREC), pages 39–45, Athens, Greece,
May.
Kishore Papineni, Salim Roukos, Todd Ward, and Wie-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proc. of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 311–318, Philadelphia, PA,
July.
Maja Popovi´c and Hermann Ney. 2004. Towards the use
of word stems &amp; suffixes for statistical machine trans-
lation. In Proc. 4th Int. Conf. on Language Resources
and Evaluation (LREC), pages 1585–1588, Lissabon,
Portugal, May.
Maja Popovi´c and Hermann Ney. 2006. POS-based
word reorderings for statistical machine translation. In
Proc. of the Fifth Int. Conf. on Language Resources
and Evaluation (LREC), Genova, Italy, May.
Maja Popovi´c, David Vilar, Hermann Ney, Slobodan
Joviˇci´c, and Zoran ˇSari´c. 2005. Augmenting a small
parallel text with morpho-syntactic language resources
for Serbian–English statistical machine translation. In
43rd Annual Meeting of the Assoc. for Computational
Linguistics: Proc. Workshop on Building and Using
Parallel Texts: Data-Driven Machine Translation and
Beyond, pages 41–48, Ann Arbor, MI, June.
David Vilar, Evgeny Matusov, Saˇsa Hasan, Richard Zens,
and Hermann Ney. 2005. Statistical machine transla-
tion of european parliamentary speeches. In Proc. MT
Summit X, pages 259–266, Phuket, Thailand, Septem-
ber.
David Vilar, Jia Xu, Luis Fernando D’Haro, and Her-
mann Ney. 2006. Error analysis of statistical machine
translation output. In Proc. of the Fifth Int. Conf. on
Language Resources and Evaluation (LREC), page to
appear, Genova, Italy, May.
Richard Zens, Oliver Bender, Saˇsa Hasan, Shahram
Khadivi, Evgeny Matusov, Jia Xu, Yuqi Zhang, and
Hermann Ney. 2005. The RWTH phrase-based statis-
tical machine translation system. In Proceedings of the
International Workshop on Spoken Language Transla-
tion (IWSLT), pages 155–162, Pittsburgh, PA, October.
</reference>
<page confidence="0.998723">
6
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.160373">
<title confidence="0.7473">Morpho-syntactic Information for Automatic Error Analysis of Machine Translation Output de</title>
<author confidence="0.337877">B</author>
<affiliation confidence="0.987151">f¨ur Informatik VI - Computer Science Department, RWTH Aachen University, Aachen,</affiliation>
<address confidence="0.7369">Research Center, Universitat Polit`ecnica de Catalunya (UPC), Barcelona, Centro per la Ricerca Scientifica e Tecnologica, Trento, Italy</address>
<abstract confidence="0.99898812">Evaluation of machine translation output is an important but difficult task. Over the last years, a variety of automatic evaluation measures have been studied, some of them like Word Error Rate (WER), Position Independent Word Error Rate (PER) and BLEU and NIST scores have become widely used tools for comparing different systems as well as for evaluating improvements within one system. However, these measures do not give any details about the nature of translation errors. Therefore some analysis of the generated output is needed in order to identify the main problems and to focus the research efforts. On the other hand, human evaluation is a time consuming and expensive task. In this paper, we investigate methods for using of morpho-syntactic information for automatic evaluation: standard error measures WER and PER are calculated on distinct word classes and forms in order to get a better idea about the nature of translation errors and possibilities for improvements.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Bogdan Babych</author>
<author>Anthony Hartley</author>
</authors>
<title>Extending bleu mt evaluation method with frequency weighting.</title>
<date>2004</date>
<booktitle>In Proc. of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<location>Barcelona, Spain,</location>
<contexts>
<context position="3230" citStr="Babych and Hartley, 2004" startWordPosition="491" endWordPosition="494">h efforts. A framework for human error analysis and error classification has been proposed in (Vilar et al., 2006), but like human evaluation, this is also a time consuming task. The goal of this work is to present a framework for automatic error analysis of machine translation output based on morpho-syntactic information. 2 Related Work There is a number of publications dealing with various automatic evaluation measures for machine translation output, some of them proposing new measures, some proposing improvements and extensions of the existing ones (Doddington, 2002; Papineni et al., 2002; Babych and Hartley, 2004; Matusov et al., 2005). Semi-automatic evaluation measures have been also investigated, for example in (Nießen et al., 2000). An automatic metric which uses base forms and synonyms of the words in order to correlate better to human judgements has been 1 Proceedings of the Workshop on Statistical Machine Translation, pages 1–6, New York City, June 2006. @2006 Association for Computational Linguistics proposed in (Banerjee and Lavie, 2005). However, error analysis is still a rather unexplored area. A framework for human error analysis and error classification has been proposed in (Vilar et al.,</context>
</contexts>
<marker>Babych, Hartley, 2004</marker>
<rawString>Bogdan Babych and Anthony Hartley. 2004. Extending bleu mt evaluation method with frequency weighting. In Proc. of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL), Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor: An automatic metric for mt evaluation with improved correlation with human judgements.</title>
<date>2005</date>
<booktitle>In 43rd Annual Meeting of the Assoc. for Computational Linguistics: Proc. Workshop on Intrinsic and Extrinsic Evaluation Measuresfor MT and/or Summarization,</booktitle>
<pages>65--72</pages>
<location>Ann Arbor, MI,</location>
<contexts>
<context position="3672" citStr="Banerjee and Lavie, 2005" startWordPosition="561" endWordPosition="564">slation output, some of them proposing new measures, some proposing improvements and extensions of the existing ones (Doddington, 2002; Papineni et al., 2002; Babych and Hartley, 2004; Matusov et al., 2005). Semi-automatic evaluation measures have been also investigated, for example in (Nießen et al., 2000). An automatic metric which uses base forms and synonyms of the words in order to correlate better to human judgements has been 1 Proceedings of the Workshop on Statistical Machine Translation, pages 1–6, New York City, June 2006. @2006 Association for Computational Linguistics proposed in (Banerjee and Lavie, 2005). However, error analysis is still a rather unexplored area. A framework for human error analysis and error classification has been proposed in (Vilar et al., 2006) and a detailed analysis of the obtained results has been carried out. Automatic methods for error analysis to our knowledge have not been studied yet. Many publications propose the use of morphosyntactic information for improving the performance of a statistical machine translation system. Various methods for treating morphological and syntactical differences between German and English are investigated in (Nießen and Ney, 2000; Nie</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved correlation with human judgements. In 43rd Annual Meeting of the Assoc. for Computational Linguistics: Proc. Workshop on Intrinsic and Extrinsic Evaluation Measuresfor MT and/or Summarization, pages 65–72, Ann Arbor, MI, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adri`a de Gispert</author>
<author>Jos´e B Mari˜no</author>
<author>Josep M Crego</author>
</authors>
<title>Improving statistical machine translation by classifying and generalizing inflected verb forms.</title>
<date>2005</date>
<booktitle>In Proc. of the 9th European Conf. on Speech Communication and Technology (Interspeech),</booktitle>
<pages>3185--3188</pages>
<location>Lisbon, Portugal,</location>
<marker>de Gispert, Mari˜no, Crego, 2005</marker>
<rawString>Adri`a de Gispert, Jos´e B. Mari˜no, and Josep M. Crego. 2005. Improving statistical machine translation by classifying and generalizing inflected verb forms. In Proc. of the 9th European Conf. on Speech Communication and Technology (Interspeech), pages 3185– 3188, Lisbon, Portugal, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram co-occurrence statistics.</title>
<date>2002</date>
<booktitle>In Proc. ARPA Workshop on Human Language Technology,</booktitle>
<pages>128--132</pages>
<location>San Diego.</location>
<contexts>
<context position="2277" citStr="Doddington, 2002" startWordPosition="339" endWordPosition="340">tput is an important issue for all natural language processing (NLP) tasks, especially for machine translation (MT). Automatic evaluation is preferred because human evaluation is a time consuming and expensive task. A variety of automatic evaluation measures have been proposed and studied over the last years, some of them are shown to be a very useful tool for comparing different systems as well as for evaluating improvements within one system. The most widely used are Word Error Rate (WER), Position Independent Word Error Rate (PER), the BLEU score (Papineni et al., 2002) and the NIST score (Doddington, 2002). However, none of these measures give any details about the nature of translation errors. A relationship between these error measures and the actual errors in the translation outputs is not easy to find. Therefore some analysis of the translation errors is necessary in order to define the main problems and to focus the research efforts. A framework for human error analysis and error classification has been proposed in (Vilar et al., 2006), but like human evaluation, this is also a time consuming task. The goal of this work is to present a framework for automatic error analysis of machine tran</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. 2002. Automatic evaluation of machine translation quality using n-gram co-occurrence statistics. In Proc. ARPA Workshop on Human Language Technology, pages 128–132, San Diego.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>David McClosky</author>
</authors>
<title>Improving stastistical machine translation through morphological analysis.</title>
<date>2005</date>
<booktitle>In Proc. of the Conf. on Empirical Methods for Natural Language Processing (EMNLP),</booktitle>
<location>Vancouver, Canada,</location>
<contexts>
<context position="4533" citStr="Goldwater and McClosky, 2005" startWordPosition="691" endWordPosition="695">t. Automatic methods for error analysis to our knowledge have not been studied yet. Many publications propose the use of morphosyntactic information for improving the performance of a statistical machine translation system. Various methods for treating morphological and syntactical differences between German and English are investigated in (Nießen and Ney, 2000; Nießen and Ney, 2001a; Nießen and Ney, 2001b). Morphological analysis has been used for improving Arabic-English translation (Lee, 2004), for SerbianEnglish translation (Popovi´c et al., 2005) as well as for Czech-English translation (Goldwater and McClosky, 2005). Inflectional morphology of Spanish verbs is dealt with in (Popovi´c and Ney, 2004; de Gispert et al., 2005). To the best of our knowledge, the use of morpho-syntactic information for error analysis of translation output has not been investigated so far. 3 Morpho-syntactic Information and Automatic Evaluation We propose the use of morpho-syntactic information in combination with the automatic evaluation measures WER and PER in order to get more details about the translation errors. We investigate two types of potential problems for the translation with the Spanish-English language pair: • syn</context>
</contexts>
<marker>Goldwater, McClosky, 2005</marker>
<rawString>Sharon Goldwater and David McClosky. 2005. Improving stastistical machine translation through morphological analysis. In Proc. of the Conf. on Empirical Methods for Natural Language Processing (EMNLP), Vancouver, Canada, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young-suk Lee</author>
</authors>
<title>Morphological analysis for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proc. 2004 Meeting of the North American chapter of the Association for Computational Linguistics (HLT-NAACL),</booktitle>
<location>Boston, MA,</location>
<contexts>
<context position="4405" citStr="Lee, 2004" startWordPosition="675" endWordPosition="676">has been proposed in (Vilar et al., 2006) and a detailed analysis of the obtained results has been carried out. Automatic methods for error analysis to our knowledge have not been studied yet. Many publications propose the use of morphosyntactic information for improving the performance of a statistical machine translation system. Various methods for treating morphological and syntactical differences between German and English are investigated in (Nießen and Ney, 2000; Nießen and Ney, 2001a; Nießen and Ney, 2001b). Morphological analysis has been used for improving Arabic-English translation (Lee, 2004), for SerbianEnglish translation (Popovi´c et al., 2005) as well as for Czech-English translation (Goldwater and McClosky, 2005). Inflectional morphology of Spanish verbs is dealt with in (Popovi´c and Ney, 2004; de Gispert et al., 2005). To the best of our knowledge, the use of morpho-syntactic information for error analysis of translation output has not been investigated so far. 3 Morpho-syntactic Information and Automatic Evaluation We propose the use of morpho-syntactic information in combination with the automatic evaluation measures WER and PER in order to get more details about the tran</context>
</contexts>
<marker>Lee, 2004</marker>
<rawString>Young-suk Lee. 2004. Morphological analysis for statistical machine translation. In Proc. 2004 Meeting of the North American chapter of the Association for Computational Linguistics (HLT-NAACL), Boston, MA, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeny Matusov</author>
<author>Gregor Leusch</author>
<author>Oliver Bender</author>
<author>Hermann Ney</author>
</authors>
<title>Evaluating machine translation output with automatic sentence segmentation.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation (IWSLT),</booktitle>
<pages>148--154</pages>
<location>Pittsburgh, PA,</location>
<contexts>
<context position="3253" citStr="Matusov et al., 2005" startWordPosition="495" endWordPosition="499"> human error analysis and error classification has been proposed in (Vilar et al., 2006), but like human evaluation, this is also a time consuming task. The goal of this work is to present a framework for automatic error analysis of machine translation output based on morpho-syntactic information. 2 Related Work There is a number of publications dealing with various automatic evaluation measures for machine translation output, some of them proposing new measures, some proposing improvements and extensions of the existing ones (Doddington, 2002; Papineni et al., 2002; Babych and Hartley, 2004; Matusov et al., 2005). Semi-automatic evaluation measures have been also investigated, for example in (Nießen et al., 2000). An automatic metric which uses base forms and synonyms of the words in order to correlate better to human judgements has been 1 Proceedings of the Workshop on Statistical Machine Translation, pages 1–6, New York City, June 2006. @2006 Association for Computational Linguistics proposed in (Banerjee and Lavie, 2005). However, error analysis is still a rather unexplored area. A framework for human error analysis and error classification has been proposed in (Vilar et al., 2006) and a detailed a</context>
</contexts>
<marker>Matusov, Leusch, Bender, Ney, 2005</marker>
<rawString>Evgeny Matusov, Gregor Leusch, Oliver Bender, and Hermann Ney. 2005. Evaluating machine translation output with automatic sentence segmentation. In Proceedings of the International Workshop on Spoken Language Translation (IWSLT), pages 148–154, Pittsburgh, PA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonja Nießen</author>
<author>Hermann Ney</author>
</authors>
<title>Improving SMT quality with morpho-syntactic analysis.</title>
<date>2000</date>
<booktitle>In COLING ’00: The 18th Int. Conf. on Computational Linguistics,</booktitle>
<pages>1081--1085</pages>
<location>Saarbr¨ucken, Germany,</location>
<contexts>
<context position="4267" citStr="Nießen and Ney, 2000" startWordPosition="653" endWordPosition="656">Banerjee and Lavie, 2005). However, error analysis is still a rather unexplored area. A framework for human error analysis and error classification has been proposed in (Vilar et al., 2006) and a detailed analysis of the obtained results has been carried out. Automatic methods for error analysis to our knowledge have not been studied yet. Many publications propose the use of morphosyntactic information for improving the performance of a statistical machine translation system. Various methods for treating morphological and syntactical differences between German and English are investigated in (Nießen and Ney, 2000; Nießen and Ney, 2001a; Nießen and Ney, 2001b). Morphological analysis has been used for improving Arabic-English translation (Lee, 2004), for SerbianEnglish translation (Popovi´c et al., 2005) as well as for Czech-English translation (Goldwater and McClosky, 2005). Inflectional morphology of Spanish verbs is dealt with in (Popovi´c and Ney, 2004; de Gispert et al., 2005). To the best of our knowledge, the use of morpho-syntactic information for error analysis of translation output has not been investigated so far. 3 Morpho-syntactic Information and Automatic Evaluation We propose the use of </context>
</contexts>
<marker>Nießen, Ney, 2000</marker>
<rawString>Sonja Nießen and Hermann Ney. 2000. Improving SMT quality with morpho-syntactic analysis. In COLING ’00: The 18th Int. Conf. on Computational Linguistics, pages 1081–1085, Saarbr¨ucken, Germany, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonja Nießen</author>
<author>Hermann Ney</author>
</authors>
<title>Morphosyntactic analysis for reordering in statistical machine translation.</title>
<date>2001</date>
<booktitle>In Proc. MT Summit VIII,</booktitle>
<pages>247--252</pages>
<location>Santiago de Compostela, Galicia, Spain,</location>
<contexts>
<context position="4289" citStr="Nießen and Ney, 2001" startWordPosition="657" endWordPosition="660">05). However, error analysis is still a rather unexplored area. A framework for human error analysis and error classification has been proposed in (Vilar et al., 2006) and a detailed analysis of the obtained results has been carried out. Automatic methods for error analysis to our knowledge have not been studied yet. Many publications propose the use of morphosyntactic information for improving the performance of a statistical machine translation system. Various methods for treating morphological and syntactical differences between German and English are investigated in (Nießen and Ney, 2000; Nießen and Ney, 2001a; Nießen and Ney, 2001b). Morphological analysis has been used for improving Arabic-English translation (Lee, 2004), for SerbianEnglish translation (Popovi´c et al., 2005) as well as for Czech-English translation (Goldwater and McClosky, 2005). Inflectional morphology of Spanish verbs is dealt with in (Popovi´c and Ney, 2004; de Gispert et al., 2005). To the best of our knowledge, the use of morpho-syntactic information for error analysis of translation output has not been investigated so far. 3 Morpho-syntactic Information and Automatic Evaluation We propose the use of morpho-syntactic infor</context>
</contexts>
<marker>Nießen, Ney, 2001</marker>
<rawString>Sonja Nießen and Hermann Ney. 2001a. Morphosyntactic analysis for reordering in statistical machine translation. In Proc. MT Summit VIII, pages 247–252, Santiago de Compostela, Galicia, Spain, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonja Nießen</author>
<author>Hermann Ney</author>
</authors>
<title>Toward hierarchical models for statistical machine translation of inflected languages.</title>
<date>2001</date>
<booktitle>In Data-Driven Machine Translation Workshop,</booktitle>
<pages>47--54</pages>
<location>Toulouse, France,</location>
<contexts>
<context position="4289" citStr="Nießen and Ney, 2001" startWordPosition="657" endWordPosition="660">05). However, error analysis is still a rather unexplored area. A framework for human error analysis and error classification has been proposed in (Vilar et al., 2006) and a detailed analysis of the obtained results has been carried out. Automatic methods for error analysis to our knowledge have not been studied yet. Many publications propose the use of morphosyntactic information for improving the performance of a statistical machine translation system. Various methods for treating morphological and syntactical differences between German and English are investigated in (Nießen and Ney, 2000; Nießen and Ney, 2001a; Nießen and Ney, 2001b). Morphological analysis has been used for improving Arabic-English translation (Lee, 2004), for SerbianEnglish translation (Popovi´c et al., 2005) as well as for Czech-English translation (Goldwater and McClosky, 2005). Inflectional morphology of Spanish verbs is dealt with in (Popovi´c and Ney, 2004; de Gispert et al., 2005). To the best of our knowledge, the use of morpho-syntactic information for error analysis of translation output has not been investigated so far. 3 Morpho-syntactic Information and Automatic Evaluation We propose the use of morpho-syntactic infor</context>
</contexts>
<marker>Nießen, Ney, 2001</marker>
<rawString>Sonja Nießen and Hermann Ney. 2001b. Toward hierarchical models for statistical machine translation of inflected languages. In Data-Driven Machine Translation Workshop, pages 47–54, Toulouse, France, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonja Nießen</author>
<author>Franz J Och</author>
<author>Gregor Leusch</author>
<author>Hermann Ney</author>
</authors>
<title>An evaluation tool for machine translation: Fast evaluation for mt research.</title>
<date>2000</date>
<booktitle>In Proc. Second Int. Conf. on Language Resources and Evaluation (LREC),</booktitle>
<pages>39--45</pages>
<location>Athens, Greece,</location>
<contexts>
<context position="3355" citStr="Nießen et al., 2000" startWordPosition="511" endWordPosition="514">n evaluation, this is also a time consuming task. The goal of this work is to present a framework for automatic error analysis of machine translation output based on morpho-syntactic information. 2 Related Work There is a number of publications dealing with various automatic evaluation measures for machine translation output, some of them proposing new measures, some proposing improvements and extensions of the existing ones (Doddington, 2002; Papineni et al., 2002; Babych and Hartley, 2004; Matusov et al., 2005). Semi-automatic evaluation measures have been also investigated, for example in (Nießen et al., 2000). An automatic metric which uses base forms and synonyms of the words in order to correlate better to human judgements has been 1 Proceedings of the Workshop on Statistical Machine Translation, pages 1–6, New York City, June 2006. @2006 Association for Computational Linguistics proposed in (Banerjee and Lavie, 2005). However, error analysis is still a rather unexplored area. A framework for human error analysis and error classification has been proposed in (Vilar et al., 2006) and a detailed analysis of the obtained results has been carried out. Automatic methods for error analysis to our know</context>
</contexts>
<marker>Nießen, Och, Leusch, Ney, 2000</marker>
<rawString>Sonja Nießen, Franz J. Och, Gregor Leusch, and Hermann Ney. 2000. An evaluation tool for machine translation: Fast evaluation for mt research. In Proc. Second Int. Conf. on Language Resources and Evaluation (LREC), pages 39–45, Athens, Greece, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WieJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of the 40th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="2239" citStr="Papineni et al., 2002" startWordPosition="330" endWordPosition="334">oduction The evaluation of the generated output is an important issue for all natural language processing (NLP) tasks, especially for machine translation (MT). Automatic evaluation is preferred because human evaluation is a time consuming and expensive task. A variety of automatic evaluation measures have been proposed and studied over the last years, some of them are shown to be a very useful tool for comparing different systems as well as for evaluating improvements within one system. The most widely used are Word Error Rate (WER), Position Independent Word Error Rate (PER), the BLEU score (Papineni et al., 2002) and the NIST score (Doddington, 2002). However, none of these measures give any details about the nature of translation errors. A relationship between these error measures and the actual errors in the translation outputs is not easy to find. Therefore some analysis of the translation errors is necessary in order to define the main problems and to focus the research efforts. A framework for human error analysis and error classification has been proposed in (Vilar et al., 2006), but like human evaluation, this is also a time consuming task. The goal of this work is to present a framework for au</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WieJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 311–318, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maja Popovi´c</author>
<author>Hermann Ney</author>
</authors>
<title>Towards the use of word stems &amp; suffixes for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proc. 4th Int. Conf. on Language Resources and Evaluation (LREC),</booktitle>
<pages>1585--1588</pages>
<location>Lissabon, Portugal,</location>
<marker>Popovi´c, Ney, 2004</marker>
<rawString>Maja Popovi´c and Hermann Ney. 2004. Towards the use of word stems &amp; suffixes for statistical machine translation. In Proc. 4th Int. Conf. on Language Resources and Evaluation (LREC), pages 1585–1588, Lissabon, Portugal, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maja Popovi´c</author>
<author>Hermann Ney</author>
</authors>
<title>POS-based word reorderings for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proc. of the Fifth Int. Conf. on Language Resources and Evaluation (LREC),</booktitle>
<location>Genova, Italy,</location>
<marker>Popovi´c, Ney, 2006</marker>
<rawString>Maja Popovi´c and Hermann Ney. 2006. POS-based word reorderings for statistical machine translation. In Proc. of the Fifth Int. Conf. on Language Resources and Evaluation (LREC), Genova, Italy, May.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Maja Popovi´c</author>
<author>David Vilar</author>
<author>Hermann Ney</author>
<author>Slobodan Joviˇci´c</author>
<author>Zoran ˇSari´c</author>
</authors>
<title>Augmenting a small parallel text with morpho-syntactic language resources for Serbian–English statistical machine translation.</title>
<date>2005</date>
<booktitle>In 43rd Annual Meeting of the Assoc. for Computational Linguistics: Proc. Workshop on Building and Using Parallel Texts: Data-Driven Machine Translation and Beyond,</booktitle>
<pages>41--48</pages>
<location>Ann Arbor, MI,</location>
<marker>Popovi´c, Vilar, Ney, Joviˇci´c, ˇSari´c, 2005</marker>
<rawString>Maja Popovi´c, David Vilar, Hermann Ney, Slobodan Joviˇci´c, and Zoran ˇSari´c. 2005. Augmenting a small parallel text with morpho-syntactic language resources for Serbian–English statistical machine translation. In 43rd Annual Meeting of the Assoc. for Computational Linguistics: Proc. Workshop on Building and Using Parallel Texts: Data-Driven Machine Translation and Beyond, pages 41–48, Ann Arbor, MI, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vilar</author>
<author>Evgeny Matusov</author>
<author>Saˇsa Hasan</author>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Statistical machine translation of european parliamentary speeches.</title>
<date>2005</date>
<booktitle>In Proc. MT Summit X,</booktitle>
<pages>259--266</pages>
<location>Phuket, Thailand,</location>
<contexts>
<context position="7437" citStr="Vilar et al., 2005" startWordPosition="1161" endWordPosition="1164">estigate potential inflection errors, we compare the PER for verbs, adjectives and nouns for both languages. For the Spanish language, we also investigate differences between full form PER and base form PER: the larger these differences, more inflection errors are present. 4 Experimental Settings 4.1 Task and Corpus The corpus analysed in this work is built in the framework of the TC-Star project. It contains more than one million sentences and about 35 million running words of the Spanish and English European Parliament Plenary Sessions (EPPS). A description of the EPPS data can be found in (Vilar et al., 2005). In order to analyse effects of data sparseness, we have randomly extracted a small subset referred to as 13k containing about thirteen thousand sentences and 370k running words (about 1% of the original 2 Training corpus: Spanish English full Sentences 1281427 Running Words 36578514 34918192 Vocabulary 153124 106496 Singletons [%] 35.2 36.2 13k Sentences 13360 Running Words 385198 366055 Vocabulary 22425 16326 Singletons [%] 47.6 43.7 Dev: Sentences 1008 Running Words 25778 26070 Distinct Words 3895 3173 OOVs (full) [%] 0.15 0.09 OOVs (13k) [%] 2.7 1.7 Test: Sentences 840 1094 Running Words </context>
<context position="8693" citStr="Vilar et al., 2005" startWordPosition="1363" endWordPosition="1366">OVs (full) [%] 0.14 0.25 OOVs (13k) [%] 2.8 2.6 Table 1: Corpus statistics for the Spanish-English EPPS task (running words include punctuation marks) corpus). The statistics of the corpora can be seen in Table 1. 4.2 Translation System The statistical machine translation system used in this work is based on a log-linear combination of seven different models. The most important ones are phrase based models in both directions, additionally IBM1 models at the phrase level in both directions as well as phrase and length penalty are used. A more detailed description of the system can be found in (Vilar et al., 2005; Zens et al., 2005). 4.3 Experiments The translation experiments have been done in both translation directions on both sizes of the corpus. In order to examine improvements of the baseline system, a new system with POS-based word reorderings of nouns and adjectives as proposed in (Popovi´c and Ney, 2006) is also analysed. Adjectives in the Spanish language are usually placed after the corresponding noun, whereas for English it is the other way round. Therefore, local reorderings of nouns and adSpanish-*English WER PER BLEU full baseline 34.5 25.5 54.7 reorder 33.5 25.2 56.4 13k baseline 41.8 </context>
</contexts>
<marker>Vilar, Matusov, Hasan, Zens, Ney, 2005</marker>
<rawString>David Vilar, Evgeny Matusov, Saˇsa Hasan, Richard Zens, and Hermann Ney. 2005. Statistical machine translation of european parliamentary speeches. In Proc. MT Summit X, pages 259–266, Phuket, Thailand, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vilar</author>
<author>Jia Xu</author>
<author>Luis Fernando D’Haro</author>
<author>Hermann Ney</author>
</authors>
<title>Error analysis of statistical machine translation output.</title>
<date>2006</date>
<booktitle>In Proc. of the Fifth Int. Conf. on Language Resources and Evaluation (LREC),</booktitle>
<pages>page</pages>
<location>Genova, Italy,</location>
<marker>Vilar, Xu, D’Haro, Ney, 2006</marker>
<rawString>David Vilar, Jia Xu, Luis Fernando D’Haro, and Hermann Ney. 2006. Error analysis of statistical machine translation output. In Proc. of the Fifth Int. Conf. on Language Resources and Evaluation (LREC), page to appear, Genova, Italy, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Oliver Bender</author>
<author>Saˇsa Hasan</author>
<author>Shahram Khadivi</author>
<author>Evgeny Matusov</author>
<author>Jia Xu</author>
<author>Yuqi Zhang</author>
<author>Hermann Ney</author>
</authors>
<title>The RWTH phrase-based statistical machine translation system.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation (IWSLT),</booktitle>
<pages>155--162</pages>
<location>Pittsburgh, PA,</location>
<contexts>
<context position="8713" citStr="Zens et al., 2005" startWordPosition="1367" endWordPosition="1370">0.25 OOVs (13k) [%] 2.8 2.6 Table 1: Corpus statistics for the Spanish-English EPPS task (running words include punctuation marks) corpus). The statistics of the corpora can be seen in Table 1. 4.2 Translation System The statistical machine translation system used in this work is based on a log-linear combination of seven different models. The most important ones are phrase based models in both directions, additionally IBM1 models at the phrase level in both directions as well as phrase and length penalty are used. A more detailed description of the system can be found in (Vilar et al., 2005; Zens et al., 2005). 4.3 Experiments The translation experiments have been done in both translation directions on both sizes of the corpus. In order to examine improvements of the baseline system, a new system with POS-based word reorderings of nouns and adjectives as proposed in (Popovi´c and Ney, 2006) is also analysed. Adjectives in the Spanish language are usually placed after the corresponding noun, whereas for English it is the other way round. Therefore, local reorderings of nouns and adSpanish-*English WER PER BLEU full baseline 34.5 25.5 54.7 reorder 33.5 25.2 56.4 13k baseline 41.8 30.7 43.2 reorder 38</context>
</contexts>
<marker>Zens, Bender, Hasan, Khadivi, Matusov, Xu, Zhang, Ney, 2005</marker>
<rawString>Richard Zens, Oliver Bender, Saˇsa Hasan, Shahram Khadivi, Evgeny Matusov, Jia Xu, Yuqi Zhang, and Hermann Ney. 2005. The RWTH phrase-based statistical machine translation system. In Proceedings of the International Workshop on Spoken Language Translation (IWSLT), pages 155–162, Pittsburgh, PA, October.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>