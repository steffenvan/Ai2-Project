<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.056399">
<title confidence="0.830513">
Optimality Theory: Universal Grammar, Learning and
Parsing Algorithms, and Connectionist Foundations
(Abstract)
</title>
<author confidence="0.985802">
Paul Smolensky and Bruce Tesar
</author>
<affiliation confidence="0.99991">
Department of Computer Science and Institute of Cognitive Science
University of Colorado, Boulder USA
</affiliation>
<bodyText confidence="0.9966459">
We present a recently proposed theory of grammar,
Optimality Theory (OT; Prince Az Smolensky 1991,
1993). The principles of OT derive in large part from
the high-level principles governing computation in con-
nectionist networks. The talk proceeds as follows: (1)
we summarize OT and its applications to UG. The we
present (2) learning and (3) parsing algorithms for OT.
Finally, (4) we show how crucial elements of OT emerge
from connectionism, and discuss the one central feature
of OT which so far eludes connectionist explanation.
</bodyText>
<listItem confidence="0.9330875">
(1) In OT, UG provides a set of highly general univer-
sal constraints which apply in parallel to assess the well-
formedness of possible structural descriptions of linguis-
tic inputs. The constraints may conflict, and for most
</listItem>
<bodyText confidence="0.9176814">
inputs no structural description meets them all. The
grammatical structure is the one that optimally meets
the conflicting constraint sets. Optimality is defined on
a language-particular basis: each language&apos;s grammar
ranks the universal constraints in a dominance hierar-
chy such that each constraint has absolute priority over
all lower-ranked constraints. Given knowledge of UG,
the job of the learner is to determine the constraint
ranking which is particular to his or her language. [The
explanatory power of OT as a theory of UG has now
been attested for phonology in over two dozen papers
and books (e.g., McCarthy &amp; Prince 1993; Rutgers Op-
timality Workshop, 1993); applications of OT to syntax
are now being explored (e.g. Legendre, Raymond, &amp;
Smolensky 1993; Grimshaw 1993).]
</bodyText>
<listItem confidence="0.906564">
(2) Learnability of OT (Tesar &amp; Smolensky, 1993). The-
ories of UG can be used to address questions of learn-
ability via the formal universal principles they provide,
or via their substantive universals. We will show that
OT endows UG with sufficiently tight formal struc-
ture to yield a number of strong learnability results at
the formal level. We will present a family of closely
</listItem>
<bodyText confidence="0.964741875">
related algorithms for learning, from positive exam-
ples only, language-particular grammars on the basis
of prior knowledge of the universal principles. We will
sketch our proof of the correctness of these algorithms
and demonstrate their low computational complexity.
(More precisely, the learning time in the worst case,
measured in terms of &apos;informative examples&apos;, grows only
as n2, where n is the number of constraints in UG, even
though the number of possible grammars grows as n!,
i.e., faster than exponentially.) Because these results
depend only on the formal universals of OT, and not on
the content of the universal constraints which provide
the substantive universals of the theory, the conclusion
that OT grammars are highly learnable applies equally
to OT grammars in phonology, syntax, or any other
grammar component.
</bodyText>
<listItem confidence="0.839685">
(3) Parsing in OT is assumed by many to be problem-
atic. For OT is often described as follows: take an
input form, generate all possible parses of it (generally,
</listItem>
<bodyText confidence="0.987785625">
infinite in number), evaluate all the constraints against
all the parses, filter the parses by descending the con-
straints in the dominance hierarchy. While this cor-
rectly characterizes the input/output function which is
an OT grammar, it hardly provides an efficient pars-
ing procedure. We will show, however, that efficient,
provably correct parsing by dynamic programming is
possible, at least when the set of candidate parses is
sufficiently simple (Tesar, 1994).
(4) OT is built from a set of principles, most of which
derive from high-level principles of connectionist com-
putation. The most central of these assert that, given
an input representation, connectionist networks tend to
compute an output representation which best satisfies
a set of conflicting soft constraints, with constraint con-
flicts handled via a notion of differential strength. For-
malized through Harmony Theory (Smolensky, 1986)
and Harmonic Grammar (Legendre, Miyata, &amp; Smolen-
sky 1990), this conception of computation yields a the-
ory of grammar based on optimization. Optimality
Theory introduces to a non-numerical form of optimiza-
tion, made possible by a property as yet unexplained
from the connectionist perspective: in grammars, con-
straints fall into strict domination hierarchies.
</bodyText>
<page confidence="0.99639">
271
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.220417">
<title confidence="0.939979333333333">Optimality Theory: Universal Grammar, Learning and Parsing Algorithms, and Connectionist Foundations (Abstract)</title>
<author confidence="0.999842">Paul Smolensky</author>
<author confidence="0.999842">Bruce Tesar</author>
<affiliation confidence="0.9964025">Department of Computer Science and Institute of Cognitive Science University of Colorado, Boulder USA</affiliation>
<abstract confidence="0.974934341772152">We present a recently proposed theory of grammar, Theory (OT; Prince 1991, 1993). The principles of OT derive in large part from the high-level principles governing computation in connectionist networks. The talk proceeds as follows: (1) we summarize OT and its applications to UG. The we present (2) learning and (3) parsing algorithms for OT. Finally, (4) we show how crucial elements of OT emerge from connectionism, and discuss the one central feature of OT which so far eludes connectionist explanation. (1) In OT, UG provides a set of highly general universal constraints which apply in parallel to assess the wellformedness of possible structural descriptions of linguistic inputs. The constraints may conflict, and for most inputs no structural description meets them all. The grammatical structure is the one that optimally meets the conflicting constraint sets. Optimality is defined on a language-particular basis: each language&apos;s grammar ranks the universal constraints in a dominance hierarchy such that each constraint has absolute priority over all lower-ranked constraints. Given knowledge of UG, the job of the learner is to determine the constraint ranking which is particular to his or her language. [The explanatory power of OT as a theory of UG has now been attested for phonology in over two dozen papers books (e.g., McCarthy 1993; Rutgers Optimality Workshop, 1993); applications of OT to syntax now being explored (e.g. Legendre, Raymond, Smolensky 1993; Grimshaw 1993).] (2) Learnability of OT (Tesar &amp; Smolensky, 1993). Theories of UG can be used to address questions of learnability via the formal universal principles they provide, or via their substantive universals. We will show that OT endows UG with sufficiently tight formal structure to yield a number of strong learnability results at the formal level. We will present a family of closely related algorithms for learning, from positive examples only, language-particular grammars on the basis of prior knowledge of the universal principles. We will sketch our proof of the correctness of these algorithms and demonstrate their low computational complexity. (More precisely, the learning time in the worst case, measured in terms of &apos;informative examples&apos;, grows only where n is the number of constraints in UG, even though the number of possible grammars grows as n!, i.e., faster than exponentially.) Because these results depend only on the formal universals of OT, and not on the content of the universal constraints which provide the substantive universals of the theory, the conclusion that OT grammars are highly learnable applies equally to OT grammars in phonology, syntax, or any other grammar component. (3) Parsing in OT is assumed by many to be problematic. For OT is often described as follows: take an input form, generate all possible parses of it (generally, infinite in number), evaluate all the constraints against all the parses, filter the parses by descending the constraints in the dominance hierarchy. While this correctly characterizes the input/output function which is an OT grammar, it hardly provides an efficient parsing procedure. We will show, however, that efficient, provably correct parsing by dynamic programming is possible, at least when the set of candidate parses is sufficiently simple (Tesar, 1994). (4) OT is built from a set of principles, most of which derive from high-level principles of connectionist computation. The most central of these assert that, given an input representation, connectionist networks tend to compute an output representation which best satisfies a set of conflicting soft constraints, with constraint conflicts handled via a notion of differential strength. For- Harmony Theory (Smolensky, 1986) Grammar (Legendre, Miyata, Smolensky 1990), this conception of computation yields a theory of grammar based on optimization. Optimality Theory introduces to a non-numerical form of optimization, made possible by a property as yet unexplained from the connectionist perspective: in grammars, constraints fall into strict domination hierarchies.</abstract>
<intro confidence="0.820387">271</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>