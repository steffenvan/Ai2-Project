<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000027">
<title confidence="0.9951025">
General Indexation of Weighted Automata –
Application to Spoken Utterance Retrieval
</title>
<author confidence="0.634449">
Cyril Allauzen and Mehryar Mohri and Murat Saraclar
</author>
<affiliation confidence="0.539711">
AT&amp;T Labs - Research
</affiliation>
<address confidence="0.813192">
180 Park Avenue, Florham Park, NJ 07932
</address>
<email confidence="0.989517">
{allauzen, mohri, murat}@research.att.com
</email>
<sectionHeader confidence="0.969827" genericHeader="abstract">
1 Motivation
</sectionHeader>
<bodyText confidence="0.999723131578947">
Much of the massive quantities of digitized data widely
available is highly variable or uncertain. This uncertainty
affects the interpretation of the data and its computational
processing at various levels, e.g., natural language texts
are abundantly ambiguous, speech and hand-written se-
quences are highly variable and hard to recognize in pres-
ence of noise, biological sequences may be altered or in-
complete.
Searching or indexing such data requires dealing with
a large number of ranked or weighted alternatives. These
may be for example the different parses of an input text,
the various responses to a search engine or information
extraction query, or the best hypotheses of a speech or
hand-written recognition system. In most cases, alterna-
tive sequences can be compactly represented by weighted
automata. The weights may be probabilities or some
other weights used to rank different hypotheses.
This motivates our study of the general problem of
indexation of weighted automata. This is more general
than the classical indexation problems since, typically,
there are many distinct hypotheses or alternatives asso-
ciated with the same index, e.g., a specific input speech
or hand-written sequence may have a large number of dif-
ferent transcriptions according to the system and models
used. Moreover, the problem requires taking into con-
sideration the weight of each alternative, which does not
have a counterpart in classical indexation problems.
We describe a general indexation algorithm for
weighted automata. The resulting index is represented
by a deterministic weighted transducer that is optimal for
search: the search for an input string takes time linear
in the sum of the size of that string and the number of
indices of the weighted automata where it appears.
In some cases, one may wish to search using sequences
in some level, e.g. word sequences, different from the
level of the sequences of the index, e.g. phonemic se-
quences. One may also wish to search for complex se-
quences including both words and parts-of-speech, or re-
</bodyText>
<sectionHeader confidence="0.66878" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.999887306818182">
Much of the massive quantities of digitized
data widely available, e.g., text, speech, hand-
written sequences, are either given directly,
or, as a result of some prior processing, as
weighted automata. These are compact rep-
resentations of a large number of alternative
sequences and their weights reflecting the un-
certainty or variability of the data. Thus,
the indexation of such data requires indexing
weighted automata.
We present a general algorithm for the index-
ation of weighted automata. The resulting in-
dex is represented by a deterministic weighted
transducer that is optimal for search: the search
for an input string takes time linear in the sum
of the size of that string and the number of
indices of the weighted automata where it ap-
pears. We also introduce a general framework
based on weighted transducers that general-
izes this indexation to enable the search for
more complex patterns including syntactic in-
formation or for different types of sequences,
e.g., word sequences instead of phonemic se-
quences. The use of this framework is illus-
trated with several examples.
We applied our general indexation algorithm
and framework to the problem of indexation of
speech utterances and report the results of our
experiments in several tasks demonstrating that
our techniques yield comparable results to pre-
vious methods, while providing greater gener-
ality, including the possibility of searching for
arbitrary patterns represented by weighted au-
tomata.
strict the search by either restricting the weights or proba-
bilities or the lengths or types of sequences. We describe
a general indexation framework covering all these cases.
Our framework is based on the use of filtering weighted
transducers for restriction or other transducers mapping
between distinct information levels or knowledge struc-
tures. We illustrate the use of this framework with sev-
eral examples that demonstrate its relevance to a number
of indexation tasks.
We applied our framework and algorithms to the par-
ticular problem of speech indexation. In recent years,
spoken document retrieval systems have made large
archives of broadcast news searchable and browsable.
Most of these systems use automatic speech recognition
to convert speech into text, which is then indexed us-
ing standard methods. When a user presents the system
with a query, documents that are relevant to the query are
found using text-based information retrieval techniques.
As speech indexation and retrieval systems move be-
yond the domain of broadcast news to more challenging
spoken communications, the importance for the indexed
material to contain more than just a simple text represen-
tation of the communication is becoming clear. Index-
ation and retrieval techniques must be extended to han-
dle more general representations including for example
syntactic information. In addition to the now familiar re-
trieval systems or search engines, other applications such
as data mining systems can be used to automatically iden-
tify useful patterns in large collections of spoken commu-
nications. Information extraction systems can be used to
gather high-level information such as named-entities.
For a given input speech utterance, a large-vocabulary
speech recognition system often generates a lattice, a
weighted automaton representing a range of alternative
hypotheses with some associated weights or probabilities
used to rank them. When the accuracy of a system is rel-
atively low as in many conversational speech recognition
tasks, it is not safe to rely only on the best hypothesis out-
put by the system. It is then preferable to use instead the
full lattice output by the recognizer.
We report the results of our experiments in sev-
eral tasks demonstrating that our techniques yield
comparable results to the previous methods of
Saraclar and Sproat (2004), while providing greater
generality, including the possibility of searching for
arbitrary patterns represented by weighted automata.
The paper is organized as follows. Section 2 introduces
the notation and the definitions used in the rest of the pa-
per. Section 3 describes our general indexation algorithm
for weighted automata. The algorithm for searching that
index is presented in Section 4 and our general indexa-
tion framework is described and illustrated in Section 5.
Section 6 reports the results of our experiments in several
tasks.
</bodyText>
<sectionHeader confidence="0.958467" genericHeader="introduction">
2 Preliminaries
</sectionHeader>
<construct confidence="0.789228">
Definition 1 A system (K, G, ®, 0,1) is a semiring
(Kuich and Salomaa, 1986) if: (K, G, 0) is a commuta-
tive monoid with identity element 0; (K, ®,1) is a monoid
with identity element 1; ® distributes over G; and 0 is an
annihilatorfor ®: for all a E K, a ® 0 = 0 ® a = 0.
</construct>
<bodyText confidence="0.763889727272727">
Thus, a semiring is a ring that may lack negation. Two
semirings often used in speech processing are: the log
semiring L = (R U fool, Glog, +, oo, 0) (Mohri, 2002)
which is isomorphic to the familiar real or probability
semiring (R+, +, x, 0,1) via a log morphism with, for
all a,b E R U fool:
a Glog b = − log(exp(−a) + exp(−b))
and the convention that: exp(−oo) = 0 and
− log(0) = oo, and the tropical semiring T = (R+ U
fool, min, +, oo, 0) which can be derived from the log
semiring using the Viterbi approximation.
</bodyText>
<construct confidence="0.806665777777778">
Definition 2 A weighted finite-state transducer T over a
semiring K is an 8-tuple T = (E, A, Q, I, F, E, λ, ρ)
where: E is the finite input alphabet of the transducer;
A is the finite output alphabet; Q is a finite set of states;
I C_ Q the set of initial states; F C_ Q the set offinal
states; EC_Qx(EUfel)x(AUfcl)xKxQafinite
set of transitions; λ : I → K the initial weight function;
and ρ : F → K the final weight function mapping F to
K.
</construct>
<bodyText confidence="0.997715769230769">
A Weighted automaton A = (E, Q, I, F, E, λ, ρ) is de-
fined in a similar way by simply omitting the output la-
bels. We denote by L(A) C_ E* the set of strings ac-
cepted by an automaton A and similarly by L(X) the
strings described by a regular expression X. We denote
by JAJ = JQJ + JEJ the size of A.
Given a transition e E E, we denote by i[e] its input
label, p[e] its origin or previous state and n[e] its desti-
nation state or next state, w[e] its weight, o[e] its output
label (transducer case). Given a state q E Q, we denote
by E[q] the set of transitions leaving q.
A path π = e1 · · · ek is an element of E* with con-
secutive transitions: n[ei_1] = p[ei], i = 2, ... , k. We
extend n and p to paths by setting: n[π] = n[ek] and
p[π] = p[e1]. A cycle π is a path whose origin and
destination states coincide: n[π] = p[π]. We denote by
P(q, q&apos;) the set of paths from q to q&apos; and by P(q, x, q&apos;)
and P(q, x, y, q&apos;) the set of paths from q to q&apos; with in-
put label x E E* and output label y (transducer case).
These definitions can be extended to subsets R, R&apos; C_ Q,
by: P(R, x, R&apos;) = UqER, q&apos;ERIP(q, x, q&apos;). The label-
ing functions i (and similarly o) and the weight func-
tion w can also be extended to paths by defining the la-
bel of a path as the concatenation of the labels of its
constituent transitions, and the weight of a path as the
®-product of the weights of its constituent transitions:
</bodyText>
<equation confidence="0.965418666666667">
i[π] = i[e1] ··· i[ek], w[π] = w[e1] ® ··· ® w[ek]. We
also extend w to any finite set of paths Π by setting:
w[Π] = ®wErl w[π]. The output weight associated by
A to each input string x E Σ* is:
[A](x) = � λ(p[π]) ® w[π] ® ρ(n[π])
wEP(I,x,F)
</equation>
<bodyText confidence="0.857286333333333">
[A](x) is defined to be 0 when P(I, x, F) = 0. Simi-
larly, the output weight associated by a transducer T to a
pair of input-output string (x, y) is:
</bodyText>
<equation confidence="0.998009">
[T](x, y) = � λ(p[π]) ® w[π] ® ρ(n[π])
wEP(I,x,y,F)
[T](x, y) = 0 when P(I, x, y, F) = 0. A successful
</equation>
<bodyText confidence="0.999798263157895">
path in a weighted automaton or transducer M is a path
from an initial state to a final state. M is unambiguous if
for any string x E Σ* there is at most one successful path
labeled with x. Thus, an unambiguous transducer defines
a function.
For any transducer T, denote by Π2(T) the automaton
obtained by projecting T on its output, that is by omitting
its input labels.
Note that the second operation of the tropical semiring
and the log semiring as well as their identity elements are
identical. Thus the weight of a path in an automaton A
over the tropical semiring does not change if A is viewed
as a weighted automaton over the log semiring or vice-
versa.
Given two strings u and v in Σ*, v is a factor of u if
u = xvy for some x and y in Σ*; if y = e then v is also
a suffix of u. More generally, v is a factor (resp. suffix) of
L C_ Σ* if v is a suffix (resp. factor) of some u E L. We
denote by |x |the length of a string x E Σ*.
</bodyText>
<sectionHeader confidence="0.995783" genericHeader="method">
3 Indexation Algorithm
</sectionHeader>
<bodyText confidence="0.998863409090909">
This section presents an algorithm for the construction of
an efficient index for a large set of speech utterances.
We assume that for each speech utterance ui of the
dataset considered, i = 1, ... , n, a weighted automaton
Ai over the alphabet Σ and the log semiring, e.g., phone
or word lattice output by an automatic speech recognizer,
is given. The problem consists of creating a full index,
that is one that can be used to search directly any factor
of any string accepted by these automata. Note that this
problem crucially differs from classical indexation prob-
lems in that the input data is uncertain. Our algorithm
must make use of the weights associated to each string
by the input automata.
The main idea behind the design of the algorithm de-
scribed is that the full index can be represented by a
weighted finite-state transducer T mapping each factor
x to the set of indices of the automata in which x appears
and the negative log of the expected count of x. More
precisely, let Pi be the probability distribution defined by
the weighted automaton Ai over the set of strings Σ* and
let Cx(u) denote the number of occurrences of a factor
x in u, then, for any factor x E Σ* and automaton index
</bodyText>
<equation confidence="0.9623735">
i E 11, ... , n}:
[T](x, i) = − log(EPi[Cx]) (1)
</equation>
<bodyText confidence="0.999772071428571">
Our algorithm for the construction of the index is simple,
it is based on general weighted automata and transducer
algorithms. We describe the consecutive stages of the al-
gorithm.
This algorithm can be seen as a generalization to
weighted automata of the notion of suffix automaton and
factor automaton for strings. The suffix (factor) automa-
ton of a string u is the minimal deterministic finite au-
tomata recognizing exactly the set of suffixes (resp. fac-
tors) of u (Blumer et al., 1985; Crochemore, 1986). The
size of both automata is linear in the length of u and both
can be built in linear time. These are classical repre-
sentations used in text indexation (Blumer et al., 1987;
Crochemore, 1986).
</bodyText>
<subsectionHeader confidence="0.998933">
3.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.9999435">
When the automata Ai are word or phone lattices out-
put by a speech recognition or other natural language
processing system, the path weights correspond to joint
probabilities. We can apply to Ai a general weight-
pushing algorithm in the log semiring (Mohri, 1997)
which converts these weights into the desired (negative
log of) posterior probabilities. More generally, the path
weights in the resulting automata can be interpreted as
log-likelihoods. We denote by Pi the corresponding
probability distribution. When the input automaton Ai is
acyclic, the complexity of the weight-pushing algorithm
is linear in its size (O(|Ai|)). Figures 1(b)(d) illustrates
the application of the algorithm to the automata of Fig-
ures 1(a)(c).
</bodyText>
<subsectionHeader confidence="0.999976">
3.2 Construction of Transducer Index T
</subsectionHeader>
<bodyText confidence="0.999574846153846">
Let Bi = (Σ, Qi, Ii, Fi, Ei, λi, ρi) denote the result of
the application of the weight pushing algorithm to the au-
tomaton Ai. The weight associated by Bi to each string
it accepts can be interpreted as the log-likelihood of that
string for the utterance ui given the models used to gen-
erate the automata. More generally, Bi defines a proba-
bility distribution Pi over all strings x E Σ* which is just
the sum of the probability of all paths of Bi in which x
appears.
For each state q E Qi, denote by d[q] the shortest dis-
tance from Ii to q (or -log of the forward probability) and
by f[q] the shortest distance from q to F (or -log of the
backward probability):
</bodyText>
<equation confidence="0.989085666666667">
d[q] = � (λi(p[π]) + w[π]) (2)
log
wEP(Ii,q)
</equation>
<figureCaption confidence="0.9983285">
Figure 1: Weighted automata over the real semiring (a) A1, (b) B1 obtained by applying weight pushing to A1, (c) A2
and (d) B2 obtained by applying weight pushing to A2.
</figureCaption>
<figure confidence="0.977079230769231">
a
1
1
b/1
a/0.5
b
0
b
a
2 3 0
b/0.5
2
a/1
3/1
(a) (b)
b/1 1
a/1
b/0.333 1
a/1
b/1
3/1
0
2
a/2
a/0.666
b/1
</figure>
<equation confidence="0.9516994">
2 3/1 0
(c) (d)
f[q] = � (w[π] + ρi(n[π])) (3)
log
π∈P(q,Fi)
</equation>
<bodyText confidence="0.999593333333333">
The shortest distances d[q] and f[q] can be computed for
all states q E Qi in linear time (O(|Bi|)) when Bi is
acyclic (Mohri, 2002). Then,
</bodyText>
<equation confidence="0.971110333333333">
− log(EPi[Cx]) = � d[p[π]] + w[π] + f[n[π]] (4)
log
i[π]=x
</equation>
<bodyText confidence="0.9417015">
From the weighted automaton Bi, one can derive a
weighted transducer Ti in two steps:
</bodyText>
<listItem confidence="0.96965005882353">
1. Factor Selection. In the general case we select all
the factors to be indexed in the following way:
• Replace each transition (p, a, w, q) E Qi xΣx
RxQi by (p,a,a,w,q) E QixΣxΣxRxQi;
• Create a new state s E� Qi and make s the
unique initial state;
• Create a new state e E� Qi and make e the
unique final state;
• Create a new transition (s, e, e, d[q], q) for each
state q E Qi;
• Create a new transition (q, e, i, f[q], e) for each
state q E Qi;
2. Optimization. The resulting transducer can be op-
timized by applying weighted e-removal, weighted
determinization, and minimization over the log
semiring by viewing it as an acceptor, i.e., input-
output labels are encoded a single labels.
</listItem>
<bodyText confidence="0.806812">
It is clear from Equation 4 that for any factor x E Σ∗:
</bodyText>
<equation confidence="0.986828">
[Ti](x, i) = − log(EPi[Cx]) (5)
</equation>
<bodyText confidence="0.971716">
This construction is illustrated by Figures 2(a)(b). Our
full index transducer T is the constructed by
</bodyText>
<listItem confidence="0.9902805">
• taking the ®log-sum (or union) of all the transducers
Ti, i = 1,...,n;
• defining T as the result of determinization (in the
log semiring) applied to that transducer.
</listItem>
<figureCaption confidence="0.9731095">
Figure 3 is illustrating this construction and optimization.
Figure 2: Construction of T1 index of the weighted au-
tomata B1 given Figure 1(b): (a) intermediary result after
factor selection and (b) resulting weighted transducer T1.
</figureCaption>
<sectionHeader confidence="0.991413" genericHeader="method">
4 Search
</sectionHeader>
<bodyText confidence="0.999949066666667">
The full index represented by the weighted finite-state
transducer T is optimal. Indeed, T contains no transi-
tion with input e other than the final transitions labeled
with an output index and it is deterministic. Thus, the
set of indices Ix of the weighted automata containing a
factor x can be obtained in O(|x |+ |Ix|) by reading in T
the unique path with input label x and then the transitions
with input e which have each a distinct output label.
The user’s query is typically an unweighted string, but
it can be given as an arbitrary weighted automaton X.
This covers the case of Boolean queries or regular expres-
sions which can be compiled into automata. The response
to a query X is computed using the general algorithm of
composition of weighted transducers (Mohri et al., 1996)
followed by projection on the output:
</bodyText>
<equation confidence="0.974409">
Π2(X o T) (6)
</equation>
<bodyText confidence="0.974022">
which is then e-removed and determinized to give di-
rectly the list of all indices and their corresponding log-
</bodyText>
<figure confidence="0.999208083333333">
s:1/1
4
s:s/1
s:s/0.5
s:s/1
0 a:s/0.5
b:s/0.5
1
b:s/1
2
s:1/1
s:1/1
a:s/1
3
s:1/1
5/1
s:s/1
a:s/1.5 s:1/1
0 2
b:s/0.333 s:1/1 1/1
b:s/1
3 a:s/1 s:1/1
4
s:1/3.5
</figure>
<figureCaption confidence="0.983514666666667">
Figure 3: Weighted transducer T obtained by index-
ing the weighted automata B1 and B2 given in Fig-
ures 1(b)(d)
</figureCaption>
<bodyText confidence="0.990533333333333">
likelihoods. The final result can be pruned to include only
the most likely responses. The pruning threshold may be
used to vary the number of responses.
</bodyText>
<sectionHeader confidence="0.994872" genericHeader="method">
5 General Indexation Framework
</sectionHeader>
<bodyText confidence="0.999833">
The indexation technique just outlined can be easily ex-
tended to include many of the techniques used for speech
indexation. This can be done by introducing a transducer
F that converts between different levels of information
sources or structures, or that filters out or reweights index
entries. The filter F can be applied (i) before, (ii) during
or (iii) after the construction of the index. For case (i), the
filter is used directly on the input and the indexation algo-
rithm is applied to the weighted automata (F oAi)1&lt;i&lt;n.
For case (ii), filtering is done after the factor selection
step of the algorithm and the filter applies to the factors,
typically to restrict the factors that will be indexed. For
case (iii), the filter is applied to the index. Obviously
different filters can be used in combination at different
stages.
When such a filter is used, the response to a query X is
obtained using another transducer F&apos; 1 and the following
composition and projection:
</bodyText>
<equation confidence="0.607606">
H2(X o F&apos; o T) (7)
</equation>
<bodyText confidence="0.999503">
Since composition is associative, it does not impose a
specific order to its application. However, in practice,
it is often advantageous to compute X o F&apos; before appli-
cation of T. The following are examples of some filter
transducers that can be of interest in many applications.
</bodyText>
<footnote confidence="0.81826">
1In most cases, F&apos; is the inverse of F.
</footnote>
<listItem confidence="0.942908959183673">
• Pronunciation Dictionary: a pronunciation dic-
tionary can be used to map word sequences into
their phonemic transcriptions, thus transform word
lattices into equivalent phone lattices. This map-
ping can represented by a weighted transducer F.
Using an index based on phone lattices allows a
user to search for words that are not in the ASR
vocabulary. In this case, the inverse transduc-
tion F&apos; is a grapheme to phoneme converter, com-
monly present in TTS front-ends. Among others,
Witbrock and Hauptmann (1997) present a system
where a phonetic transcript is obtained from the
word transcript and retrieval is performed using both
word and phone indices.
• Vocabulary Restriction: in some cases using a full
index can be prohibitive and unnecessary. It might
be desirable to do partial indexing by ignoring some
words (or phones) in the input. For example, we
might wish to index only “named entities”, or just
the consonants. This is mostly motivated by the
reduction of the size of the index while retaining
the necessary information. A similar approach is
to apply a many to one mapping to index groups of
phones, or metaphones (Amir et al., 2001), to over-
come phonetic errors.
• Reweighting: a weighted transducer can be used
to emphasize some words in the input while de-
emphasizing other. The weights, for example might
correspond to TF-IDF weights. Another reweight-
ing method might involve edit distance or confusion
statistics.
• Classification: an extreme form of summarizing the
information contained in the indexed material is to
assign a class label, such as a topic label, to each
input. The query would also be classified and all
answers with the same class label would be returned
as relevant.
• Length Restriction: a common way of indexing
phone strings is to index fixed length overlapping
phone strings (Logan et al., 2002). This results in a
partial index with only fixed length strings. More
generally a minimum and maximum string length
may be imposed on the index. An example restric-
tion automaton is given in Figure 4. In this case,
the filter applies to the factors and has to be applied
during or after indexation. The restricted index will
be smaller in size but contains less information and
may result in degradation in retrieval performance,
especially for long queries.
</listItem>
<bodyText confidence="0.996258333333333">
The length restriction filter requires a modification of
the search procedure. Assume a fixed – say r – length
restriction filter and a string query of length k. If k &lt; r,
</bodyText>
<figure confidence="0.998041407407407">
e:2/0.571
0
b:e/2.333
a:e/2.5
2
1
b:e/0.600
e:2/3.333
a:e/0.571
e:1/3.5
e:1/0.428
e:1/0.600
e:2/0.400
4
e:1/0.75
5
e:2/0.25
e:1/0.333
e:2/0.666
a:e/0.333
b:e/0.25 7
3/1
6
e:2/1
e:1/1
9/1
8/1
</figure>
<figureCaption confidence="0.990649666666667">
Figure 4: (a) Filter F restricting to strings of length 2. (b)
Restricted index F o T, where T is the weighted trans-
ducer given in Figure 3(b).
</figureCaption>
<bodyText confidence="0.9965138">
then we need to pad the input to length r with Er−k. If
k &gt; r, then we must search for all substrings of length r
in the index. A string is present in a certain lattice if all its
substrings are (and not vice versa). So, the results of each
substring search must be intersected. The probability of
each substring xi+r−1 for i E 11, ... , k + 1 � r} is an
i
upper bound on the probability of the string xk1, and the
count of each substring is an upper bound on the count of
the string, so for i E 11,. .. , k + 1 — r}
</bodyText>
<equation confidence="0.957009">
EP[C(xk1)] — EP[C(xi+r−1
i )].
</equation>
<bodyText confidence="0.9999806">
Since, we take the system described there as our base-
line, we give a brief review of the basic indexation al-
gorithm used there. The algorithm uses the same pre-
processing step. For each label in E, an index file is
constructed. For each arc a that appears in the prepro-
cessed weighted automaton Bi, the following informa-
tion is stored: (i, p[a], n[a], d[p[a]], w[a]). Since the pre-
processing ensures that f[q] = 0 for all q in Bi, it is pos-
sible to compute — log(EP,[Cx]) as in Equation 4 using
the information stored in the index.
</bodyText>
<subsectionHeader confidence="0.997647">
6.1 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999971666666667">
For evaluating retrieval performance we use precision
and recall with respect to manual transcriptions. Let
Correct(q) be the number of times the query q is found
correctly, Answer(q) be the number of answers to the
query q, and Reference(q) be the number of times q is
found in the reference.
</bodyText>
<equation confidence="0.9984804">
Correct(q)
Precision(q) =
Answer(q)
Correct(q)
Recall(q) = Reference(q)
</equation>
<bodyText confidence="0.9997325">
We compute precision and recall rates for each query and
report the average over all queries. The set of queries Q
includes all the words seen in the reference except for a
stoplist of 100 most common words.
</bodyText>
<figure confidence="0.969829434782609">
a:x/2.5
b:x/2.333
b:x/0.600
a:x/0.571
x:1/0.333
x:2/0.666
x:1/0.75
x:2/0.25
1
2
0
4
5
3/1
(b)
a a
0 1 2
b b
(a)
1 � Precision = Precision(q)
|Q |qEQ
Recall = 1 Recall(q)
|Q |qEQ
</figure>
<bodyText confidence="0.998447">
Therefore, the intersection operation must use minimum
for combining the expected counts of substrings. In other
words, the expected count of the string is approximated
by the minimum of the probabilities of each of its sub-
strings,
</bodyText>
<equation confidence="0.9666945">
EP[C(xk1)] ,: min
1&lt;i&lt;k+1−r
</equation>
<bodyText confidence="0.99987675">
In addition to a filter transducer, pruning can be ap-
plied at different stages of the algorithm to reduce the
size of the index. Pruning eliminates least likely paths in
a weighted automaton or transducer. Applying pruning
to Ai can be seen as part of the process that generates the
uncertain input data. When pruning is applied to Bi, only
the more likely alternatives will be indexed. If pruning is
applied to Ti, or to T, pruning takes the expected counts
into consideration and not the probabilities. Note that the
threshold used for this type of pruning is directly compa-
rable to the threshold used for pruning the search results
in Section 4 since both are thresholds on expected counts.
</bodyText>
<sectionHeader confidence="0.994388" genericHeader="evaluation">
6 Experimental Results
</sectionHeader>
<bodyText confidence="0.999550333333333">
Our task is retrieving the utterances (or short audio seg-
ments) that a given query appears in. The experimental
setup is identical to that of Saraclar and Sproat (2004).
For lattice based retrieval methods, different operating
points can be obtained by changing the threshold. The
precision and recall at these operating points can be plot-
ted as a curve.
In addition to individual precision-recall values we
also compute the F-measure defined as
</bodyText>
<equation confidence="0.815383333333333">
2 x Precision x Recall
F=
Precision + Recall
</equation>
<bodyText confidence="0.9999185">
and report the maximum F-measure (maxF) to summa-
rize the information in a precision-recall curve.
</bodyText>
<subsectionHeader confidence="0.986254">
6.2 Corpora
</subsectionHeader>
<bodyText confidence="0.999954571428571">
We use three different corpora to assess the effectiveness
of different retrieval techniques.
The first corpus is the DARPA Broadcast News cor-
pus consisting of excerpts from TV or radio programs
including various acoustic conditions. The test set is
the 1998 Hub-4 Broadcast News (hub4e98) evaluation
test set (available from LDC, Catalog no. LDC2000S86)
</bodyText>
<equation confidence="0.6819585">
EP [C(xi+r−1
i )].
</equation>
<bodyText confidence="0.999835">
which is 3 hours long and was manually segmented into
940 segments. It contains 32411 word tokens and 4885
word types. For ASR we use a real-time system (Saraclar
et al., 2002). Since the system was designed for SDR,
the recognition vocabulary of the system has over 200K
words.
The second corpus is the Switchboard corpus consist-
ing of two party telephone conversations. The test set is
the RT02 evaluation test set which is 5 hours long, has
120 conversation sides and was manually segmented into
6266 segments. It contains 65255 word tokens and 3788
word types. For ASR we use the first pass of the evalua-
tion system (Ljolje et al., 2002). The recognition vocab-
ulary of the system has over 45K words.
The third corpus is named Teleconferences since it con-
sists of multi-party teleconferences on various topics. A
test set of six teleconferences (about 3.5 hours) was tran-
scribed. It contains 31106 word tokens and 2779 word
types. Calls are automatically segmented into a total of
1157 segments prior to ASR. We again use the first pass
of the Switchboard evaluation system for ASR.
We use the AT&amp;T DCD Library (Allauzen et al., 2003)
as our ASR decoder and our implementation of the algo-
rithm is based on the AT&amp;T FSM Library (Mohri et al.,
2000), both of which are available for download.
</bodyText>
<subsectionHeader confidence="0.541286">
6.3 Results
</subsectionHeader>
<bodyText confidence="0.998769766666667">
We implemented some of the proposed techniques and
made comparisons with the previous method used by
Saraclar and Sproat (2004). The full indexing method
consumed too much time while indexing Broadcast News
lattices and used too much memory while indexing phone
lattices for Teleconferences. In the other cases, we con-
firmed that the new method yields identical results. In
Table 1 we compare the index sizes for full indexing and
partial indexing with the previous method. In both cases,
the input lattices are pruned so that the cost (negative log
probability) difference between two paths is less than six.
Although the new method results in much smaller index
sizes for the string case (i.e. nbest=1), it can result in very
large index sizes for full indexing of lattices (cost=6).
However, partial indexing by length restriction solves this
problem. For the results reported in Table 1, the length of
the word strings to be indexed was restricted to be less
than or equal to four, and the length of the phone strings
to be indexed was restricted to be exactly four.
In Saraclar and Sproat (2004), it was shown that using
word lattices yields a relative gain of 3-5% in maxF over
using best word hypotheses. Furthermore, it was shown
that a “search cascade” strategy for using both word and
phone indices increases the relative gain over the baseline
to 8-12%. In this strategy, we first search the word index
for the given query, if no matches are found we search
the phone index. Using the partial indices, we obtained
a precision recall performance that is almost identical to
the one obtained with the previous method. Comparison
of the maximum F-measure for both methods is given in
</bodyText>
<tableCaption confidence="0.966605">
Table 2.
</tableCaption>
<table confidence="0.99948175">
Task Previous Method Partial Index
Broadcast News 86.0 86.1
Switchboard 60.5 60.8
Teleconferences 52.8 52.7
</table>
<tableCaption confidence="0.9487715">
Table 2: Comparison of maximum F-measure for three
corpora.
</tableCaption>
<bodyText confidence="0.997780857142857">
As an example, we used a filter that indexes only con-
sonants (i.e. maps the vowels to e). The resulting index
was used instead of the full phone index. The size of
the consonants only index was 370MB whereas the size
of the full index was 431MB. In Figure 5 we present the
precision recall performance of this consonant only in-
dex.
</bodyText>
<figure confidence="0.339473">
Precision
</figure>
<figureCaption confidence="0.9059435">
Figure 5: Comparison of Precision vs Recall Perfor-
mance for Switchboard.
</figureCaption>
<sectionHeader confidence="0.997431" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999981857142857">
We described a general framework for indexing uncer-
tain input data represented as weighted automata. The
indexation algorithm utilizes weighted finite-state algo-
rithms to obtain an index represented as a weighted finite-
state transducer. We showed that many of the techniques
used for speech indexing can be implemented within this
framework. We gave comparative results to a previous
method for lattice indexing.
The same idea and framework can be used for indexa-
tion in natural language processing or other areas where
uncertain input data is given as weighted automata. The
complexity of the index construction algorithm can be
improved in some general cases using techniques simi-
lar to classical string matching ones (Blumer et al., 1985;
</bodyText>
<figure confidence="0.986965636363637">
Recall
40
80
70
60
50
3030 40 50 60
Word Index
Word and Phone Index
Word and Phone (consonants only) Index
70 80
</figure>
<table confidence="0.9627">
Task Type Pruning Previous Method Full Index Partial Index
Broadcast News word nbest=1 29 2.7 –
Broadcast News word cost=6 91 – 25
Broadcast News phone cost=6 27 – 14
Switchboard word nbest=1 18 4.7 –
Switchboard word cost=6 90 99 88
Switchboard phone cost=6 97 431 41
Teleconferences word nbest=1 16 2.6 –
Teleconferences word cost=6 142 352 184
Teleconferences phone cost=6 146 – 69
</table>
<tableCaption confidence="0.999958">
Table 1: Comparison of Index Sizes in MegaBytes.
</tableCaption>
<bodyText confidence="0.9992952">
Crochemore, 1986; Blumer et al., 1987). Various prun-
ing techniques can be applied to reduce the size of the
index without significantly degrading performance. Fi-
nally, other types of filters that make use of the general
framework can be investigated.
</bodyText>
<sectionHeader confidence="0.998929" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999541666666667">
We wish to thank our colleague Richard Sproat for useful
discussions and the use of the lattice indexing software
(lctools) used in our baseline experiments.
</bodyText>
<sectionHeader confidence="0.999071" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994732161290322">
Cyril Allauzen, Mehryar Mohri, and Michael Ri-
ley. 2003. DCD Library - Decoder Library.
http://www.research.att.com/sw/tools/dcd.
Arnon Amir, Alon Efrat, and Savitha Srinivasan. 2001.
Advances in phonetic word spotting. In Proceedings
of the Tenth International Conference on Information
and Knowledge Management, pages 580–582, Atlanta,
Georgia, USA.
Anselm Blumer, Janet Blumer, Andrzej Ehrenfeucht,
David Haussler, and Joel Seiferas. 1985. The smallest
automaton recognizing the subwords of a text. Theo-
retical Computer Science, 40(1):31–55.
Anselm Blumer, Janet Blumer, David Haussler, Ross Mc-
Connel, and Andrzej Ehrenfeucht. 1987. Complete
inverted files for efficient text retrieval and analysis.
Journal of the ACM, 34(3):578–595.
Maxime Crochemore. 1986. Transducers and repeti-
tions. Theoretical Computer Science, 45(1):63–86.
Werner Kuich and Arto Salomaa. 1986. Semirings,
Automata, Languages. Number 5 in EATCS Mono-
graphs on Theoretical Computer Science. Springer-
Verlag, Berlin, Germany.
Andrej Ljolje, Murat Saraclar, Michiel Bacchiani,
Michael Collins, and Brian Roark. 2002. The AT&amp;T
RT-02 STT system. In Proc. RT02 Workshop, Vienna,
Virginia.
Beth Logan, Pedro Moreno, and Om Deshmukh. 2002.
Word and sub-word indexing approaches for reducing
the effects of OOV queries on spoken audio. In Proc.
HLT.
Mehryar Mohri, Fernando C. N. Pereira, and Michael Ri-
ley. 1996. Weighted Automata in Text and Speech
Processing. In Proceedings of the 12th biennial Euro-
pean Conference on Artificial Intelligence (ECAI-96),
Workshop on Extendedfinite state models of language,
Budapest, Hungary.
Mehryar Mohri, Fernando C. N. Pereira, and Michael
Riley. 2000. The Design Principles of a
Weighted Finite-State Transducer Library. The-
oretical Computer Science, 231:17–32, January.
http://www.research.att.com/sw/tools/fsm.
Mehryar Mohri. 1997. Finite-State Transducers in Lan-
guage and Speech Processing. Computational Lin-
guistics, 23:2.
Mehryar Mohri. 2002. Semiring Frameworks and Algo-
rithms for Shortest-Distance Problems. Journal ofAu-
tomata, Languages and Combinatorics, 7(3):321–350.
Murat Saraclar and Richard Sproat. 2004. Lattice-based
search for spoken utterance retrieval. In Proc. HLT-
NAACL.
Murat Saraclar, Michael Riley, Enrico Bocchieri, and
Vincent Goffin. 2002. Towards automatic closed cap-
tioning: Low latency real-time broadcast news tran-
scription. In Proceedings of the International Confer-
ence on Spoken Language Processing (ICSLP), Den-
ver, Colorado, USA.
Michael Witbrock and Alexander Hauptmann. 1997. Us-
ing words and phonetic strings for efficient informa-
tion retrieval from imperfectly transcribed spoken doc-
uments. In 2nd ACMInternational Conference on Dig-
ital Libraries (DL’97), pages 30–35, Philadelphia, PA,
July.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.9946595">General Indexation of Weighted Automata – Application to Spoken Utterance Retrieval</title>
<author confidence="0.994426">Allauzen Mohri</author>
<affiliation confidence="0.981531">AT&amp;T Labs -</affiliation>
<address confidence="0.997104">180 Park Avenue, Florham Park, NJ</address>
<abstract confidence="0.980255630094044">mohri, 1 Motivation Much of the massive quantities of digitized data widely is highly variable or This uncertainty affects the interpretation of the data and its computational processing at various levels, e.g., natural language texts are abundantly ambiguous, speech and hand-written sequences are highly variable and hard to recognize in presence of noise, biological sequences may be altered or incomplete. Searching or indexing such data requires dealing with a large number of ranked or weighted alternatives. These may be for example the different parses of an input text, the various responses to a search engine or information extraction query, or the best hypotheses of a speech or hand-written recognition system. In most cases, alternasequences can be compactly represented by The weights may be probabilities or some other weights used to rank different hypotheses. This motivates our study of the general problem of indexation of weighted automata. This is more general than the classical indexation problems since, typically, there are many distinct hypotheses or alternatives associated with the same index, e.g., a specific input speech or hand-written sequence may have a large number of different transcriptions according to the system and models used. Moreover, the problem requires taking into consideration the weight of each alternative, which does not have a counterpart in classical indexation problems. We describe a general indexation algorithm for weighted automata. The resulting index is represented a deterministic weighted transducer that is search: the search for an input string takes time linear in the sum of the size of that string and the number of indices of the weighted automata where it appears. In some cases, one may wish to search using sequences in some level, e.g. word sequences, different from the level of the sequences of the index, e.g. phonemic sequences. One may also wish to search for complex seincluding both words and parts-of-speech, or re- Abstract Much of the massive quantities of digitized data widely available, e.g., text, speech, handwritten sequences, are either given directly, or, as a result of some prior processing, as weighted automata. These are compact representations of a large number of alternative sequences and their weights reflecting the uncertainty or variability of the data. Thus, the indexation of such data requires indexing weighted automata. We present a general algorithm for the indexation of weighted automata. The resulting index is represented by a deterministic weighted that is search: the search for an input string takes time linear in the sum of the size of that string and the number of indices of the weighted automata where it appears. We also introduce a general framework based on weighted transducers that generalizes this indexation to enable the search for more complex patterns including syntactic information or for different types of sequences, e.g., word sequences instead of phonemic sequences. The use of this framework is illustrated with several examples. We applied our general indexation algorithm and framework to the problem of indexation of speech utterances and report the results of our experiments in several tasks demonstrating that our techniques yield comparable results to previous methods, while providing greater generality, including the possibility of searching for arbitrary patterns represented by weighted automata. strict the search by either restricting the weights or probabilities or the lengths or types of sequences. We describe a general indexation framework covering all these cases. Our framework is based on the use of filtering weighted transducers for restriction or other transducers mapping between distinct information levels or knowledge structures. We illustrate the use of this framework with several examples that demonstrate its relevance to a number of indexation tasks. We applied our framework and algorithms to the particular problem of speech indexation. In recent years, spoken document retrieval systems have made large archives of broadcast news searchable and browsable. Most of these systems use automatic speech recognition to convert speech into text, which is then indexed using standard methods. When a user presents the system with a query, documents that are relevant to the query are found using text-based information retrieval techniques. As speech indexation and retrieval systems move beyond the domain of broadcast news to more challenging spoken communications, the importance for the indexed material to contain more than just a simple text representation of the communication is becoming clear. Indexation and retrieval techniques must be extended to handle more general representations including for example syntactic information. In addition to the now familiar retrieval systems or search engines, other applications such as data mining systems can be used to automatically identify useful patterns in large collections of spoken communications. Information extraction systems can be used to gather high-level information such as named-entities. For a given input speech utterance, a large-vocabulary recognition system often generates a a weighted automaton representing a range of alternative hypotheses with some associated weights or probabilities used to rank them. When the accuracy of a system is relatively low as in many conversational speech recognition tasks, it is not safe to rely only on the best hypothesis output by the system. It is then preferable to use instead the full lattice output by the recognizer. We report the results of our experiments in several tasks demonstrating that our techniques yield comparable results to the previous methods of Saraclar and Sproat (2004), while providing greater generality, including the possibility of searching for arbitrary patterns represented by weighted automata. The paper is organized as follows. Section 2 introduces the notation and the definitions used in the rest of the paper. Section 3 describes our general indexation algorithm for weighted automata. The algorithm for searching that index is presented in Section 4 and our general indexation framework is described and illustrated in Section 5. Section 6 reports the results of our experiments in several tasks. 2 Preliminaries 1 system a and Salomaa, 1986) if: a commutamonoid with identity element a monoid identity element over and an for all a = 0 Thus, a semiring is a ring that may lack negation. Two often used in speech processing are: the 2002) which is isomorphic to the familiar real or probability a with, for + the convention that: = 0 = and the semiring can be derived from the log semiring using the Viterbi approximation. 2 finite-state transducer a an 8-tuple Q, I, F, E, λ, the finite input alphabet of the transducer; the finite output alphabet; a finite set of states; set of initial states; set offinal of transitions; initial weight function; final weight function mapping automaton Q, I, F, E, λ, defined in a similar way by simply omitting the output la- We denote by set of strings acby an automaton similarly by described by a regular expression We denote size of a transition we denote by input origin or previous state and destistate or next state, weight, o[e] its output (transducer case). Given a state we denote set of transitions leaving · · an element of contransitions: = ... , We paths by setting: = = A cycle a path whose origin and states coincide: = We denote by set of paths from by x, x, y, set of paths from inlabel output label case). definitions can be extended to subsets x, = x, The labelfunctions similarly and the weight funcalso be extended to paths by defining the label of a path as the concatenation of the labels of its constituent transitions, and the weight of a path as the of the weights of its constituent transitions: = = ··· ® We extend any finite set of paths setting: = The output weight associated by each input string is: = defined to be x, = Simithe output weight associated by a transducer a of input-output string = = 0 x, y, = A a weighted automaton or transducer a path an initial state to a final state. any string there is at most one successful path with Thus, an unambiguous transducer defines a function. any transducer denote by automaton by projecting its output, that is by omitting its input labels. Note that the second operation of the tropical semiring and the log semiring as well as their identity elements are Thus the weight of a path in an automaton the tropical semiring does not change if viewed as a weighted automaton over the log semiring or viceversa. two strings a some if also More generally, a of if a suffix (resp. factor) of some We by length of a string 3 Indexation Algorithm This section presents an algorithm for the construction of an efficient index for a large set of speech utterances. assume that for each speech utterance the considered, ... , a weighted automaton the alphabet the log semiring, e.g., phone or word lattice output by an automatic speech recognizer, is given. The problem consists of creating a full index, that is one that can be used to search directly any factor of any string accepted by these automata. Note that this problem crucially differs from classical indexation problems in that the input data is uncertain. Our algorithm must make use of the weights associated to each string by the input automata. The main idea behind the design of the algorithm described is that the full index can be represented by a finite-state transducer each factor the set of indices of the automata in which the negative log of the expected count of More let the probability distribution defined by weighted automaton the set of strings and the number of occurrences of a factor then, for any factor and automaton index ... , = Our algorithm for the construction of the index is simple, it is based on general weighted automata and transducer algorithms. We describe the consecutive stages of the algorithm. This algorithm can be seen as a generalization to automata of the notion of automaton automaton strings. The suffix (factor) automaof a string the minimal deterministic finite automata recognizing exactly the set of suffixes (resp. facof et al., 1985; Crochemore, 1986). The of both automata is linear in the length of both can be built in linear time. These are classical representations used in text indexation (Blumer et al., 1987; Crochemore, 1986). 3.1 Preprocessing the automata word or phone lattices output by a speech recognition or other natural language processing system, the path weights correspond to joint We can apply to general weightpushing algorithm in the log semiring (Mohri, 1997) which converts these weights into the desired (negative log of) posterior probabilities. More generally, the path weights in the resulting automata can be interpreted as We denote by corresponding distribution. When the input automaton acyclic, the complexity of the weight-pushing algorithm linear in its size Figures 1(b)(d) illustrates the application of the algorithm to the automata of Figures 1(a)(c). Construction of Transducer Index the result of the application of the weight pushing algorithm to the au- The weight associated by each string it accepts can be interpreted as the log-likelihood of that for the utterance the models used to genthe automata. More generally, a probadistribution all strings which is just sum of the probability of all paths of which appears. each state denote by shortest disfrom the forward probability) and shortest distance from the backward probability): = + log 1: Weighted automata over the real semiring (a) (b) by applying weight pushing to (c) (d) by applying weight pushing to a 1 1 b/1 a/0.5 b 0 b a 2 3 0 b/0.5 2 a/1 3/1 (a) (b) b/1 1 a/1 b/0.333 1 a/1 b/1 3/1 0 2 a/2 a/0.666 b/1 2 3/1 0 (c) (d) = + log shortest distances be computed for states linear time when acyclic (Mohri, 2002). Then, = + + log the weighted automaton one can derive a transducer two steps: 1. Factor Selection. In the general case we select all the factors to be indexed in the following way: Replace each transition a, w, Create a new state make unique initial state; Create a new state make unique final state; Create a new transition e, e, each Create a new transition e, i, each 2. Optimization. The resulting transducer can be opby applying weighted weighted determinization, and minimization over the log semiring by viewing it as an acceptor, i.e., inputoutput labels are encoded a single labels. is clear from Equation 4 that for any factor = This construction is illustrated by Figures 2(a)(b). Our index transducer the constructed by taking the (or union) of all the transducers defining the result of determinization (in the log semiring) applied to that transducer. Figure 3 is illustrating this construction and optimization. 2: Construction of of the weighted au- Figure 1(b): (a) intermediary result after selection and (b) resulting weighted transducer 4 Search The full index represented by the weighted finite-state optimal. Indeed, no transiwith input than the final transitions labeled with an output index and it is deterministic. Thus, the of indices of the weighted automata containing a be obtained in reading in unique path with input label then the transitions input have each a distinct output label. The user’s query is typically an unweighted string, but can be given as an arbitrary weighted automaton This covers the case of Boolean queries or regular expressions which can be compiled into automata. The response a query computed using the general algorithm of composition of weighted transducers (Mohri et al., 1996) followed by projection on the output: is then and determinized to give dithe list of all indices and their corresponding log- 4 s:s/1 s:s/0.5 s:s/1 1 2 3 5/1 s:s/1 0 2 1/1 4 3: Weighted transducer by indexthe weighted automata in Figures 1(b)(d) likelihoods. The final result can be pruned to include only the most likely responses. The pruning threshold may be used to vary the number of responses. 5 General Indexation Framework The indexation technique just outlined can be easily extended to include many of the techniques used for speech indexation. This can be done by introducing a transducer converts between different levels of information sources or structures, or that filters out or reweights index The filter be applied (i) before, (ii) during or (iii) after the construction of the index. For case (i), the filter is used directly on the input and the indexation algois applied to the weighted automata For case (ii), filtering is done after the factor selection step of the algorithm and the filter applies to the factors, typically to restrict the factors that will be indexed. For case (iii), the filter is applied to the index. Obviously different filters can be used in combination at different stages. such a filter is used, the response to a query using another transducer the following composition and projection: o Since composition is associative, it does not impose a specific order to its application. However, in practice, is often advantageous to compute before appliof The following are examples of some filter transducers that can be of interest in many applications. most cases, the inverse of Pronunciation Dictionary: pronunciation dictionary can be used to map word sequences into their phonemic transcriptions, thus transform word lattices into equivalent phone lattices. This mapcan represented by a weighted transducer Using an index based on phone lattices allows a user to search for words that are not in the ASR vocabulary. In this case, the inverse transducis a grapheme to phoneme converter, commonly present in TTS front-ends. Among others, Witbrock and Hauptmann (1997) present a system where a phonetic transcript is obtained from the word transcript and retrieval is performed using both word and phone indices. Vocabulary Restriction: some cases using a full index can be prohibitive and unnecessary. It might be desirable to do partial indexing by ignoring some words (or phones) in the input. For example, we might wish to index only “named entities”, or just the consonants. This is mostly motivated by the reduction of the size of the index while retaining the necessary information. A similar approach is to apply a many to one mapping to index groups of phones, or metaphones (Amir et al., 2001), to overcome phonetic errors. Reweighting: weighted transducer can be used to emphasize some words in the input while deemphasizing other. The weights, for example might correspond to TF-IDF weights. Another reweighting method might involve edit distance or confusion statistics. Classification: extreme form of summarizing the information contained in the indexed material is to assign a class label, such as a topic label, to each input. The query would also be classified and all answers with the same class label would be returned as relevant. Length Restriction: common way of indexing phone strings is to index fixed length overlapping phone strings (Logan et al., 2002). This results in a partial index with only fixed length strings. More generally a minimum and maximum string length may be imposed on the index. An example restriction automaton is given in Figure 4. In this case, the filter applies to the factors and has to be applied during or after indexation. The restricted index will be smaller in size but contains less information and may result in degradation in retrieval performance, especially for long queries. The length restriction filter requires a modification of search procedure. Assume a fixed – say length filter and a string query of length If &lt; 0 2 1 4 5 3/1 6 9/1 8/1 4: (a) Filter to strings of length 2. (b) index where the weighted transducer given in Figure 3(b). we need to pad the input to length If then we must search for all substrings of length in the index. A string is present in a certain lattice if all its substrings are (and not vice versa). So, the results of each substring search must be intersected. The probability of substring ... , k 1 an i bound on the probability of the string and the count of each substring is an upper bound on the count of string, so for .. , k 1 Since, we take the system described there as our baseline, we give a brief review of the basic indexation algorithm used there. The algorithm uses the same prestep. For each label in an index file is For each arc appears in the preproweighted automaton the following informais stored: Since the preensures that = 0 all it is posto compute in Equation 4 using the information stored in the index. 6.1 Evaluation Metrics For evaluating retrieval performance we use precision and recall with respect to manual transcriptions. Let the number of times the query found the number of answers to the and the number of times found in the reference. = = We compute precision and recall rates for each query and the average over all queries. The set of queries includes all the words seen in the reference except for a stoplist of 100 most common words. 1 2 0 4 5 3/1 (b) a a 0 1 2 b b (a) �Precision = Recall = Therefore, the intersection operation must use minimum for combining the expected counts of substrings. In other words, the expected count of the string is approximated by the minimum of the probabilities of each of its substrings, In addition to a filter transducer, pruning can be applied at different stages of the algorithm to reduce the size of the index. Pruning eliminates least likely paths in a weighted automaton or transducer. Applying pruning be seen as part of the process that generates the input data. When pruning is applied to only the more likely alternatives will be indexed. If pruning is to or to pruning takes the expected counts into consideration and not the probabilities. Note that the threshold used for this type of pruning is directly comparable to the threshold used for pruning the search results in Section 4 since both are thresholds on expected counts. 6 Experimental Results Our task is retrieving the utterances (or short audio segments) that a given query appears in. The experimental setup is identical to that of Saraclar and Sproat (2004). For lattice based retrieval methods, different operating points can be obtained by changing the threshold. The precision and recall at these operating points can be plotted as a curve. In addition to individual precision-recall values we also compute the F-measure defined as Precision + Recall and report the maximum F-measure (maxF) to summarize the information in a precision-recall curve. 6.2 Corpora We use three different corpora to assess the effectiveness of different retrieval techniques. The first corpus is the DARPA Broadcast News corpus consisting of excerpts from TV or radio programs including various acoustic conditions. The test set is the 1998 Hub-4 Broadcast News (hub4e98) evaluation test set (available from LDC, Catalog no. LDC2000S86) which is 3 hours long and was manually segmented into 940 segments. It contains 32411 word tokens and 4885 word types. For ASR we use a real-time system (Saraclar et al., 2002). Since the system was designed for SDR, the recognition vocabulary of the system has over 200K words. The second corpus is the Switchboard corpus consisting of two party telephone conversations. The test set is the RT02 evaluation test set which is 5 hours long, has 120 conversation sides and was manually segmented into 6266 segments. It contains 65255 word tokens and 3788 word types. For ASR we use the first pass of the evaluation system (Ljolje et al., 2002). The recognition vocabulary of the system has over 45K words. third corpus is named it consists of multi-party teleconferences on various topics. A test set of six teleconferences (about 3.5 hours) was transcribed. It contains 31106 word tokens and 2779 word types. Calls are automatically segmented into a total of 1157 segments prior to ASR. We again use the first pass of the Switchboard evaluation system for ASR. We use the AT&amp;T DCD Library (Allauzen et al., 2003) as our ASR decoder and our implementation of the algorithm is based on the AT&amp;T FSM Library (Mohri et al., 2000), both of which are available for download. 6.3 Results We implemented some of the proposed techniques and made comparisons with the previous method used by Saraclar and Sproat (2004). The full indexing method consumed too much time while indexing Broadcast News lattices and used too much memory while indexing phone lattices for Teleconferences. In the other cases, we confirmed that the new method yields identical results. In Table 1 we compare the index sizes for full indexing and partial indexing with the previous method. In both cases, the input lattices are pruned so that the cost (negative log probability) difference between two paths is less than six. Although the new method results in much smaller index sizes for the string case (i.e. nbest=1), it can result in very large index sizes for full indexing of lattices (cost=6). However, partial indexing by length restriction solves this problem. For the results reported in Table 1, the length of the word strings to be indexed was restricted to be less than or equal to four, and the length of the phone strings to be indexed was restricted to be exactly four. In Saraclar and Sproat (2004), it was shown that using word lattices yields a relative gain of 3-5% in maxF over using best word hypotheses. Furthermore, it was shown that a “search cascade” strategy for using both word and phone indices increases the relative gain over the baseline to 8-12%. In this strategy, we first search the word index for the given query, if no matches are found we search the phone index. Using the partial indices, we obtained a precision recall performance that is almost identical to the one obtained with the previous method. Comparison of the maximum F-measure for both methods is given in Table 2. Task Previous Method Partial Index Broadcast News 86.0 86.1 Switchboard 60.5 60.8 Teleconferences 52.8 52.7 Table 2: Comparison of maximum F-measure for three corpora. As an example, we used a filter that indexes only con- (i.e. maps the vowels to The resulting index was used instead of the full phone index. The size of the consonants only index was 370MB whereas the size of the full index was 431MB. In Figure 5 we present the precision recall performance of this consonant only index. Precision Figure 5: Comparison of Precision vs Recall Performance for Switchboard. 7 Conclusion We described a general framework for indexing uncertain input data represented as weighted automata. The indexation algorithm utilizes weighted finite-state algorithms to obtain an index represented as a weighted finitestate transducer. We showed that many of the techniques used for speech indexing can be implemented within this framework. We gave comparative results to a previous method for lattice indexing. The same idea and framework can be used for indexation in natural language processing or other areas where uncertain input data is given as weighted automata. The complexity of the index construction algorithm can be improved in some general cases using techniques simi-</abstract>
<note confidence="0.950266142857143">lar to classical string matching ones (Blumer et al., 1985; Recall 40 80 70 60 50</note>
<phone confidence="0.847468">40 50 60</phone>
<title confidence="0.836910666666667">Word Index Word and Phone Index Word and Phone (consonants only) Index</title>
<phone confidence="0.746989">70 80</phone>
<note confidence="0.862052666666667">Task Type Pruning Previous Method Full Index Partial Index Broadcast News word nbest=1 29 2.7 – Broadcast News word cost=6 91 – 25 Broadcast News phone cost=6 27 – 14 Switchboard word nbest=1 18 4.7 – Switchboard word cost=6 90 99 88</note>
<abstract confidence="0.848302214285715">Switchboard phone cost=6 97 431 41 Teleconferences word nbest=1 16 2.6 – Teleconferences word cost=6 142 352 184 Teleconferences phone cost=6 146 – 69 Table 1: Comparison of Index Sizes in MegaBytes. Crochemore, 1986; Blumer et al., 1987). Various pruning techniques can be applied to reduce the size of the index without significantly degrading performance. Finally, other types of filters that make use of the general framework can be investigated. Acknowledgments We wish to thank our colleague Richard Sproat for useful discussions and the use of the lattice indexing software used in our baseline experiments.</abstract>
<note confidence="0.707331666666667">References Allauzen, Mehryar Mohri, and Michael Riley. 2003. DCD Library - Decoder</note>
<web confidence="0.708823">http://www.research.att.com/sw/tools/dcd.</web>
<note confidence="0.846860581395349">Arnon Amir, Alon Efrat, and Savitha Srinivasan. 2001. in phonetic word spotting. In of the Tenth International Conference on Information Knowledge pages 580–582, Atlanta, Georgia, USA. Anselm Blumer, Janet Blumer, Andrzej Ehrenfeucht, David Haussler, and Joel Seiferas. 1985. The smallest recognizing the subwords of a text. Theo- Computer 40(1):31–55. Anselm Blumer, Janet Blumer, David Haussler, Ross Mc- Connel, and Andrzej Ehrenfeucht. 1987. Complete inverted files for efficient text retrieval and analysis. of the 34(3):578–595. Maxime Crochemore. 1986. Transducers and repeti- Computer 45(1):63–86. Kuich and Arto Salomaa. 1986. Number 5 in EATCS Monographs on Theoretical Computer Science. Springer- Verlag, Berlin, Germany. Andrej Ljolje, Murat Saraclar, Michiel Bacchiani, Michael Collins, and Brian Roark. 2002. The AT&amp;T STT system. In RT02 Vienna, Virginia. Beth Logan, Pedro Moreno, and Om Deshmukh. 2002. Word and sub-word indexing approaches for reducing effects of OOV queries on spoken audio. In Mehryar Mohri, Fernando C. N. Pereira, and Michael Riley. 1996. Weighted Automata in Text and Speech In of the 12th biennial European Conference on Artificial Intelligence (ECAI-96), Workshop on Extendedfinite state models of language, Mehryar Mohri, Fernando C. N. Pereira, and Michael Riley. 2000. The Design Principles of a Finite-State Transducer Library. Computer 231:17–32, January. http://www.research.att.com/sw/tools/fsm. Mehryar Mohri. 1997. Finite-State Transducers in Lanand Speech Processing. Lin- 23:2. Mehryar Mohri. 2002. Semiring Frameworks and Algofor Shortest-Distance Problems. ofAu- Languages and 7(3):321–350. Murat Saraclar and Richard Sproat. 2004. Lattice-based</note>
<title confidence="0.757879">for spoken utterance retrieval. In HLT-</title>
<author confidence="0.754862">Towards automatic closed cap-</author>
<affiliation confidence="0.798721666666667">tioning: Low latency real-time broadcast news tran- In of the International Conferon Spoken Language Processing Den-</affiliation>
<address confidence="0.98612">ver, Colorado, USA.</address>
<author confidence="0.949438">Us-</author>
<abstract confidence="0.862024333333333">ing words and phonetic strings for efficient information retrieval from imperfectly transcribed spoken doc- In ACMInternational Conference on Dig-</abstract>
<note confidence="0.4880275">Libraries pages 30–35, Philadelphia, PA, July.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Cyril Allauzen</author>
<author>Mehryar Mohri</author>
<author>Michael Riley</author>
</authors>
<date>2003</date>
<booktitle>DCD Library - Decoder Library. http://www.research.att.com/sw/tools/dcd.</booktitle>
<contexts>
<context position="26579" citStr="Allauzen et al., 2003" startWordPosition="4674" endWordPosition="4677">t contains 65255 word tokens and 3788 word types. For ASR we use the first pass of the evaluation system (Ljolje et al., 2002). The recognition vocabulary of the system has over 45K words. The third corpus is named Teleconferences since it consists of multi-party teleconferences on various topics. A test set of six teleconferences (about 3.5 hours) was transcribed. It contains 31106 word tokens and 2779 word types. Calls are automatically segmented into a total of 1157 segments prior to ASR. We again use the first pass of the Switchboard evaluation system for ASR. We use the AT&amp;T DCD Library (Allauzen et al., 2003) as our ASR decoder and our implementation of the algorithm is based on the AT&amp;T FSM Library (Mohri et al., 2000), both of which are available for download. 6.3 Results We implemented some of the proposed techniques and made comparisons with the previous method used by Saraclar and Sproat (2004). The full indexing method consumed too much time while indexing Broadcast News lattices and used too much memory while indexing phone lattices for Teleconferences. In the other cases, we confirmed that the new method yields identical results. In Table 1 we compare the index sizes for full indexing and </context>
</contexts>
<marker>Allauzen, Mohri, Riley, 2003</marker>
<rawString>Cyril Allauzen, Mehryar Mohri, and Michael Riley. 2003. DCD Library - Decoder Library. http://www.research.att.com/sw/tools/dcd.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arnon Amir</author>
<author>Alon Efrat</author>
<author>Savitha Srinivasan</author>
</authors>
<title>Advances in phonetic word spotting.</title>
<date>2001</date>
<booktitle>In Proceedings of the Tenth International Conference on Information and Knowledge Management,</booktitle>
<pages>580--582</pages>
<location>Atlanta, Georgia, USA.</location>
<contexts>
<context position="19980" citStr="Amir et al., 2001" startWordPosition="3527" endWordPosition="3530">e a phonetic transcript is obtained from the word transcript and retrieval is performed using both word and phone indices. • Vocabulary Restriction: in some cases using a full index can be prohibitive and unnecessary. It might be desirable to do partial indexing by ignoring some words (or phones) in the input. For example, we might wish to index only “named entities”, or just the consonants. This is mostly motivated by the reduction of the size of the index while retaining the necessary information. A similar approach is to apply a many to one mapping to index groups of phones, or metaphones (Amir et al., 2001), to overcome phonetic errors. • Reweighting: a weighted transducer can be used to emphasize some words in the input while deemphasizing other. The weights, for example might correspond to TF-IDF weights. Another reweighting method might involve edit distance or confusion statistics. • Classification: an extreme form of summarizing the information contained in the indexed material is to assign a class label, such as a topic label, to each input. The query would also be classified and all answers with the same class label would be returned as relevant. • Length Restriction: a common way of inde</context>
</contexts>
<marker>Amir, Efrat, Srinivasan, 2001</marker>
<rawString>Arnon Amir, Alon Efrat, and Savitha Srinivasan. 2001. Advances in phonetic word spotting. In Proceedings of the Tenth International Conference on Information and Knowledge Management, pages 580–582, Atlanta, Georgia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anselm Blumer</author>
<author>Janet Blumer</author>
<author>Andrzej Ehrenfeucht</author>
<author>David Haussler</author>
<author>Joel Seiferas</author>
</authors>
<title>The smallest automaton recognizing the subwords of a text.</title>
<date>1985</date>
<journal>Theoretical Computer Science,</journal>
<volume>40</volume>
<issue>1</issue>
<contexts>
<context position="12492" citStr="Blumer et al., 1985" startWordPosition="2201" endWordPosition="2204"> of occurrences of a factor x in u, then, for any factor x E Σ* and automaton index i E 11, ... , n}: [T](x, i) = − log(EPi[Cx]) (1) Our algorithm for the construction of the index is simple, it is based on general weighted automata and transducer algorithms. We describe the consecutive stages of the algorithm. This algorithm can be seen as a generalization to weighted automata of the notion of suffix automaton and factor automaton for strings. The suffix (factor) automaton of a string u is the minimal deterministic finite automata recognizing exactly the set of suffixes (resp. factors) of u (Blumer et al., 1985; Crochemore, 1986). The size of both automata is linear in the length of u and both can be built in linear time. These are classical representations used in text indexation (Blumer et al., 1987; Crochemore, 1986). 3.1 Preprocessing When the automata Ai are word or phone lattices output by a speech recognition or other natural language processing system, the path weights correspond to joint probabilities. We can apply to Ai a general weightpushing algorithm in the log semiring (Mohri, 1997) which converts these weights into the desired (negative log of) posterior probabilities. More generally,</context>
<context position="29785" citStr="Blumer et al., 1985" startWordPosition="5209" endWordPosition="5212">ithm utilizes weighted finite-state algorithms to obtain an index represented as a weighted finitestate transducer. We showed that many of the techniques used for speech indexing can be implemented within this framework. We gave comparative results to a previous method for lattice indexing. The same idea and framework can be used for indexation in natural language processing or other areas where uncertain input data is given as weighted automata. The complexity of the index construction algorithm can be improved in some general cases using techniques similar to classical string matching ones (Blumer et al., 1985; Recall 40 80 70 60 50 3030 40 50 60 Word Index Word and Phone Index Word and Phone (consonants only) Index 70 80 Task Type Pruning Previous Method Full Index Partial Index Broadcast News word nbest=1 29 2.7 – Broadcast News word cost=6 91 – 25 Broadcast News phone cost=6 27 – 14 Switchboard word nbest=1 18 4.7 – Switchboard word cost=6 90 99 88 Switchboard phone cost=6 97 431 41 Teleconferences word nbest=1 16 2.6 – Teleconferences word cost=6 142 352 184 Teleconferences phone cost=6 146 – 69 Table 1: Comparison of Index Sizes in MegaBytes. Crochemore, 1986; Blumer et al., 1987). Various pru</context>
</contexts>
<marker>Blumer, Blumer, Ehrenfeucht, Haussler, Seiferas, 1985</marker>
<rawString>Anselm Blumer, Janet Blumer, Andrzej Ehrenfeucht, David Haussler, and Joel Seiferas. 1985. The smallest automaton recognizing the subwords of a text. Theoretical Computer Science, 40(1):31–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anselm Blumer</author>
<author>Janet Blumer</author>
<author>David Haussler</author>
<author>Ross McConnel</author>
<author>Andrzej Ehrenfeucht</author>
</authors>
<title>Complete inverted files for efficient text retrieval and analysis.</title>
<date>1987</date>
<journal>Journal of the ACM,</journal>
<volume>34</volume>
<issue>3</issue>
<contexts>
<context position="12686" citStr="Blumer et al., 1987" startWordPosition="2236" endWordPosition="2239"> is based on general weighted automata and transducer algorithms. We describe the consecutive stages of the algorithm. This algorithm can be seen as a generalization to weighted automata of the notion of suffix automaton and factor automaton for strings. The suffix (factor) automaton of a string u is the minimal deterministic finite automata recognizing exactly the set of suffixes (resp. factors) of u (Blumer et al., 1985; Crochemore, 1986). The size of both automata is linear in the length of u and both can be built in linear time. These are classical representations used in text indexation (Blumer et al., 1987; Crochemore, 1986). 3.1 Preprocessing When the automata Ai are word or phone lattices output by a speech recognition or other natural language processing system, the path weights correspond to joint probabilities. We can apply to Ai a general weightpushing algorithm in the log semiring (Mohri, 1997) which converts these weights into the desired (negative log of) posterior probabilities. More generally, the path weights in the resulting automata can be interpreted as log-likelihoods. We denote by Pi the corresponding probability distribution. When the input automaton Ai is acyclic, the complex</context>
</contexts>
<marker>Blumer, Blumer, Haussler, McConnel, Ehrenfeucht, 1987</marker>
<rawString>Anselm Blumer, Janet Blumer, David Haussler, Ross McConnel, and Andrzej Ehrenfeucht. 1987. Complete inverted files for efficient text retrieval and analysis. Journal of the ACM, 34(3):578–595.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maxime Crochemore</author>
</authors>
<title>Transducers and repetitions.</title>
<date>1986</date>
<journal>Theoretical Computer Science,</journal>
<volume>45</volume>
<issue>1</issue>
<contexts>
<context position="12511" citStr="Crochemore, 1986" startWordPosition="2205" endWordPosition="2206">factor x in u, then, for any factor x E Σ* and automaton index i E 11, ... , n}: [T](x, i) = − log(EPi[Cx]) (1) Our algorithm for the construction of the index is simple, it is based on general weighted automata and transducer algorithms. We describe the consecutive stages of the algorithm. This algorithm can be seen as a generalization to weighted automata of the notion of suffix automaton and factor automaton for strings. The suffix (factor) automaton of a string u is the minimal deterministic finite automata recognizing exactly the set of suffixes (resp. factors) of u (Blumer et al., 1985; Crochemore, 1986). The size of both automata is linear in the length of u and both can be built in linear time. These are classical representations used in text indexation (Blumer et al., 1987; Crochemore, 1986). 3.1 Preprocessing When the automata Ai are word or phone lattices output by a speech recognition or other natural language processing system, the path weights correspond to joint probabilities. We can apply to Ai a general weightpushing algorithm in the log semiring (Mohri, 1997) which converts these weights into the desired (negative log of) posterior probabilities. More generally, the path weights i</context>
</contexts>
<marker>Crochemore, 1986</marker>
<rawString>Maxime Crochemore. 1986. Transducers and repetitions. Theoretical Computer Science, 45(1):63–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Werner Kuich</author>
<author>Arto Salomaa</author>
</authors>
<date>1986</date>
<journal>Semirings, Automata, Languages. Number</journal>
<booktitle>in EATCS Monographs on Theoretical Computer Science.</booktitle>
<volume>5</volume>
<publisher>SpringerVerlag,</publisher>
<location>Berlin, Germany.</location>
<contexts>
<context position="6746" citStr="Kuich and Salomaa, 1986" startWordPosition="1052" endWordPosition="1055">oviding greater generality, including the possibility of searching for arbitrary patterns represented by weighted automata. The paper is organized as follows. Section 2 introduces the notation and the definitions used in the rest of the paper. Section 3 describes our general indexation algorithm for weighted automata. The algorithm for searching that index is presented in Section 4 and our general indexation framework is described and illustrated in Section 5. Section 6 reports the results of our experiments in several tasks. 2 Preliminaries Definition 1 A system (K, G, ®, 0,1) is a semiring (Kuich and Salomaa, 1986) if: (K, G, 0) is a commutative monoid with identity element 0; (K, ®,1) is a monoid with identity element 1; ® distributes over G; and 0 is an annihilatorfor ®: for all a E K, a ® 0 = 0 ® a = 0. Thus, a semiring is a ring that may lack negation. Two semirings often used in speech processing are: the log semiring L = (R U fool, Glog, +, oo, 0) (Mohri, 2002) which is isomorphic to the familiar real or probability semiring (R+, +, x, 0,1) via a log morphism with, for all a,b E R U fool: a Glog b = − log(exp(−a) + exp(−b)) and the convention that: exp(−oo) = 0 and − log(0) = oo, and the tropical </context>
</contexts>
<marker>Kuich, Salomaa, 1986</marker>
<rawString>Werner Kuich and Arto Salomaa. 1986. Semirings, Automata, Languages. Number 5 in EATCS Monographs on Theoretical Computer Science. SpringerVerlag, Berlin, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrej Ljolje</author>
<author>Murat Saraclar</author>
<author>Michiel Bacchiani</author>
<author>Michael Collins</author>
<author>Brian Roark</author>
</authors>
<date>2002</date>
<booktitle>The AT&amp;T RT-02 STT system. In Proc. RT02 Workshop,</booktitle>
<location>Vienna, Virginia.</location>
<contexts>
<context position="26083" citStr="Ljolje et al., 2002" startWordPosition="4590" endWordPosition="4593"> and was manually segmented into 940 segments. It contains 32411 word tokens and 4885 word types. For ASR we use a real-time system (Saraclar et al., 2002). Since the system was designed for SDR, the recognition vocabulary of the system has over 200K words. The second corpus is the Switchboard corpus consisting of two party telephone conversations. The test set is the RT02 evaluation test set which is 5 hours long, has 120 conversation sides and was manually segmented into 6266 segments. It contains 65255 word tokens and 3788 word types. For ASR we use the first pass of the evaluation system (Ljolje et al., 2002). The recognition vocabulary of the system has over 45K words. The third corpus is named Teleconferences since it consists of multi-party teleconferences on various topics. A test set of six teleconferences (about 3.5 hours) was transcribed. It contains 31106 word tokens and 2779 word types. Calls are automatically segmented into a total of 1157 segments prior to ASR. We again use the first pass of the Switchboard evaluation system for ASR. We use the AT&amp;T DCD Library (Allauzen et al., 2003) as our ASR decoder and our implementation of the algorithm is based on the AT&amp;T FSM Library (Mohri et a</context>
</contexts>
<marker>Ljolje, Saraclar, Bacchiani, Collins, Roark, 2002</marker>
<rawString>Andrej Ljolje, Murat Saraclar, Michiel Bacchiani, Michael Collins, and Brian Roark. 2002. The AT&amp;T RT-02 STT system. In Proc. RT02 Workshop, Vienna, Virginia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Logan</author>
<author>Pedro Moreno</author>
<author>Om Deshmukh</author>
</authors>
<title>Word and sub-word indexing approaches for reducing the effects of OOV queries on spoken audio.</title>
<date>2002</date>
<booktitle>In Proc. HLT.</booktitle>
<contexts>
<context position="20670" citStr="Logan et al., 2002" startWordPosition="3640" endWordPosition="3643"> be used to emphasize some words in the input while deemphasizing other. The weights, for example might correspond to TF-IDF weights. Another reweighting method might involve edit distance or confusion statistics. • Classification: an extreme form of summarizing the information contained in the indexed material is to assign a class label, such as a topic label, to each input. The query would also be classified and all answers with the same class label would be returned as relevant. • Length Restriction: a common way of indexing phone strings is to index fixed length overlapping phone strings (Logan et al., 2002). This results in a partial index with only fixed length strings. More generally a minimum and maximum string length may be imposed on the index. An example restriction automaton is given in Figure 4. In this case, the filter applies to the factors and has to be applied during or after indexation. The restricted index will be smaller in size but contains less information and may result in degradation in retrieval performance, especially for long queries. The length restriction filter requires a modification of the search procedure. Assume a fixed – say r – length restriction filter and a strin</context>
</contexts>
<marker>Logan, Moreno, Deshmukh, 2002</marker>
<rawString>Beth Logan, Pedro Moreno, and Om Deshmukh. 2002. Word and sub-word indexing approaches for reducing the effects of OOV queries on spoken audio. In Proc. HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
<author>Fernando C N Pereira</author>
<author>Michael Riley</author>
</authors>
<title>Weighted Automata in Text and Speech Processing.</title>
<date>1996</date>
<booktitle>In Proceedings of the 12th biennial European Conference on Artificial Intelligence (ECAI-96), Workshop on Extendedfinite state models of language,</booktitle>
<location>Budapest, Hungary.</location>
<contexts>
<context position="16900" citStr="Mohri et al., 1996" startWordPosition="2998" endWordPosition="3001">with an output index and it is deterministic. Thus, the set of indices Ix of the weighted automata containing a factor x can be obtained in O(|x |+ |Ix|) by reading in T the unique path with input label x and then the transitions with input e which have each a distinct output label. The user’s query is typically an unweighted string, but it can be given as an arbitrary weighted automaton X. This covers the case of Boolean queries or regular expressions which can be compiled into automata. The response to a query X is computed using the general algorithm of composition of weighted transducers (Mohri et al., 1996) followed by projection on the output: Π2(X o T) (6) which is then e-removed and determinized to give directly the list of all indices and their corresponding logs:1/1 4 s:s/1 s:s/0.5 s:s/1 0 a:s/0.5 b:s/0.5 1 b:s/1 2 s:1/1 s:1/1 a:s/1 3 s:1/1 5/1 s:s/1 a:s/1.5 s:1/1 0 2 b:s/0.333 s:1/1 1/1 b:s/1 3 a:s/1 s:1/1 4 s:1/3.5 Figure 3: Weighted transducer T obtained by indexing the weighted automata B1 and B2 given in Figures 1(b)(d) likelihoods. The final result can be pruned to include only the most likely responses. The pruning threshold may be used to vary the number of responses. 5 General Inde</context>
</contexts>
<marker>Mohri, Pereira, Riley, 1996</marker>
<rawString>Mehryar Mohri, Fernando C. N. Pereira, and Michael Riley. 1996. Weighted Automata in Text and Speech Processing. In Proceedings of the 12th biennial European Conference on Artificial Intelligence (ECAI-96), Workshop on Extendedfinite state models of language, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
<author>Fernando C N Pereira</author>
<author>Michael Riley</author>
</authors>
<title>The Design Principles of a Weighted Finite-State Transducer Library. Theoretical Computer Science,</title>
<date>2000</date>
<pages>231--17</pages>
<note>http://www.research.att.com/sw/tools/fsm.</note>
<contexts>
<context position="26692" citStr="Mohri et al., 2000" startWordPosition="4696" endWordPosition="4699">al., 2002). The recognition vocabulary of the system has over 45K words. The third corpus is named Teleconferences since it consists of multi-party teleconferences on various topics. A test set of six teleconferences (about 3.5 hours) was transcribed. It contains 31106 word tokens and 2779 word types. Calls are automatically segmented into a total of 1157 segments prior to ASR. We again use the first pass of the Switchboard evaluation system for ASR. We use the AT&amp;T DCD Library (Allauzen et al., 2003) as our ASR decoder and our implementation of the algorithm is based on the AT&amp;T FSM Library (Mohri et al., 2000), both of which are available for download. 6.3 Results We implemented some of the proposed techniques and made comparisons with the previous method used by Saraclar and Sproat (2004). The full indexing method consumed too much time while indexing Broadcast News lattices and used too much memory while indexing phone lattices for Teleconferences. In the other cases, we confirmed that the new method yields identical results. In Table 1 we compare the index sizes for full indexing and partial indexing with the previous method. In both cases, the input lattices are pruned so that the cost (negativ</context>
</contexts>
<marker>Mohri, Pereira, Riley, 2000</marker>
<rawString>Mehryar Mohri, Fernando C. N. Pereira, and Michael Riley. 2000. The Design Principles of a Weighted Finite-State Transducer Library. Theoretical Computer Science, 231:17–32, January. http://www.research.att.com/sw/tools/fsm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
</authors>
<date>1997</date>
<booktitle>Finite-State Transducers in Language and Speech Processing. Computational Linguistics,</booktitle>
<pages>23--2</pages>
<contexts>
<context position="12987" citStr="Mohri, 1997" startWordPosition="2286" endWordPosition="2287">nimal deterministic finite automata recognizing exactly the set of suffixes (resp. factors) of u (Blumer et al., 1985; Crochemore, 1986). The size of both automata is linear in the length of u and both can be built in linear time. These are classical representations used in text indexation (Blumer et al., 1987; Crochemore, 1986). 3.1 Preprocessing When the automata Ai are word or phone lattices output by a speech recognition or other natural language processing system, the path weights correspond to joint probabilities. We can apply to Ai a general weightpushing algorithm in the log semiring (Mohri, 1997) which converts these weights into the desired (negative log of) posterior probabilities. More generally, the path weights in the resulting automata can be interpreted as log-likelihoods. We denote by Pi the corresponding probability distribution. When the input automaton Ai is acyclic, the complexity of the weight-pushing algorithm is linear in its size (O(|Ai|)). Figures 1(b)(d) illustrates the application of the algorithm to the automata of Figures 1(a)(c). 3.2 Construction of Transducer Index T Let Bi = (Σ, Qi, Ii, Fi, Ei, λi, ρi) denote the result of the application of the weight pushing </context>
</contexts>
<marker>Mohri, 1997</marker>
<rawString>Mehryar Mohri. 1997. Finite-State Transducers in Language and Speech Processing. Computational Linguistics, 23:2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
</authors>
<title>Semiring Frameworks and Algorithms for Shortest-Distance Problems.</title>
<date>2002</date>
<journal>Journal ofAutomata, Languages and Combinatorics,</journal>
<volume>7</volume>
<issue>3</issue>
<contexts>
<context position="7105" citStr="Mohri, 2002" startWordPosition="1132" endWordPosition="1133"> in Section 4 and our general indexation framework is described and illustrated in Section 5. Section 6 reports the results of our experiments in several tasks. 2 Preliminaries Definition 1 A system (K, G, ®, 0,1) is a semiring (Kuich and Salomaa, 1986) if: (K, G, 0) is a commutative monoid with identity element 0; (K, ®,1) is a monoid with identity element 1; ® distributes over G; and 0 is an annihilatorfor ®: for all a E K, a ® 0 = 0 ® a = 0. Thus, a semiring is a ring that may lack negation. Two semirings often used in speech processing are: the log semiring L = (R U fool, Glog, +, oo, 0) (Mohri, 2002) which is isomorphic to the familiar real or probability semiring (R+, +, x, 0,1) via a log morphism with, for all a,b E R U fool: a Glog b = − log(exp(−a) + exp(−b)) and the convention that: exp(−oo) = 0 and − log(0) = oo, and the tropical semiring T = (R+ U fool, min, +, oo, 0) which can be derived from the log semiring using the Viterbi approximation. Definition 2 A weighted finite-state transducer T over a semiring K is an 8-tuple T = (E, A, Q, I, F, E, λ, ρ) where: E is the finite input alphabet of the transducer; A is the finite output alphabet; Q is a finite set of states; I C_ Q the se</context>
<context position="14667" citStr="Mohri, 2002" startWordPosition="2599" endWordPosition="2600">probability) and by f[q] the shortest distance from q to F (or -log of the backward probability): d[q] = � (λi(p[π]) + w[π]) (2) log wEP(Ii,q) Figure 1: Weighted automata over the real semiring (a) A1, (b) B1 obtained by applying weight pushing to A1, (c) A2 and (d) B2 obtained by applying weight pushing to A2. a 1 1 b/1 a/0.5 b 0 b a 2 3 0 b/0.5 2 a/1 3/1 (a) (b) b/1 1 a/1 b/0.333 1 a/1 b/1 3/1 0 2 a/2 a/0.666 b/1 2 3/1 0 (c) (d) f[q] = � (w[π] + ρi(n[π])) (3) log π∈P(q,Fi) The shortest distances d[q] and f[q] can be computed for all states q E Qi in linear time (O(|Bi|)) when Bi is acyclic (Mohri, 2002). Then, − log(EPi[Cx]) = � d[p[π]] + w[π] + f[n[π]] (4) log i[π]=x From the weighted automaton Bi, one can derive a weighted transducer Ti in two steps: 1. Factor Selection. In the general case we select all the factors to be indexed in the following way: • Replace each transition (p, a, w, q) E Qi xΣx RxQi by (p,a,a,w,q) E QixΣxΣxRxQi; • Create a new state s E� Qi and make s the unique initial state; • Create a new state e E� Qi and make e the unique final state; • Create a new transition (s, e, e, d[q], q) for each state q E Qi; • Create a new transition (q, e, i, f[q], e) for each state q E</context>
</contexts>
<marker>Mohri, 2002</marker>
<rawString>Mehryar Mohri. 2002. Semiring Frameworks and Algorithms for Shortest-Distance Problems. Journal ofAutomata, Languages and Combinatorics, 7(3):321–350.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Murat Saraclar</author>
<author>Richard Sproat</author>
</authors>
<title>Lattice-based search for spoken utterance retrieval.</title>
<date>2004</date>
<booktitle>In Proc. HLTNAACL.</booktitle>
<contexts>
<context position="6112" citStr="Saraclar and Sproat (2004)" startWordPosition="952" endWordPosition="955">large-vocabulary speech recognition system often generates a lattice, a weighted automaton representing a range of alternative hypotheses with some associated weights or probabilities used to rank them. When the accuracy of a system is relatively low as in many conversational speech recognition tasks, it is not safe to rely only on the best hypothesis output by the system. It is then preferable to use instead the full lattice output by the recognizer. We report the results of our experiments in several tasks demonstrating that our techniques yield comparable results to the previous methods of Saraclar and Sproat (2004), while providing greater generality, including the possibility of searching for arbitrary patterns represented by weighted automata. The paper is organized as follows. Section 2 introduces the notation and the definitions used in the rest of the paper. Section 3 describes our general indexation algorithm for weighted automata. The algorithm for searching that index is presented in Section 4 and our general indexation framework is described and illustrated in Section 5. Section 6 reports the results of our experiments in several tasks. 2 Preliminaries Definition 1 A system (K, G, ®, 0,1) is a </context>
<context position="24635" citStr="Saraclar and Sproat (2004)" startWordPosition="4350" endWordPosition="4353">cess that generates the uncertain input data. When pruning is applied to Bi, only the more likely alternatives will be indexed. If pruning is applied to Ti, or to T, pruning takes the expected counts into consideration and not the probabilities. Note that the threshold used for this type of pruning is directly comparable to the threshold used for pruning the search results in Section 4 since both are thresholds on expected counts. 6 Experimental Results Our task is retrieving the utterances (or short audio segments) that a given query appears in. The experimental setup is identical to that of Saraclar and Sproat (2004). For lattice based retrieval methods, different operating points can be obtained by changing the threshold. The precision and recall at these operating points can be plotted as a curve. In addition to individual precision-recall values we also compute the F-measure defined as 2 x Precision x Recall F= Precision + Recall and report the maximum F-measure (maxF) to summarize the information in a precision-recall curve. 6.2 Corpora We use three different corpora to assess the effectiveness of different retrieval techniques. The first corpus is the DARPA Broadcast News corpus consisting of excerpt</context>
<context position="26875" citStr="Saraclar and Sproat (2004)" startWordPosition="4725" endWordPosition="4728">opics. A test set of six teleconferences (about 3.5 hours) was transcribed. It contains 31106 word tokens and 2779 word types. Calls are automatically segmented into a total of 1157 segments prior to ASR. We again use the first pass of the Switchboard evaluation system for ASR. We use the AT&amp;T DCD Library (Allauzen et al., 2003) as our ASR decoder and our implementation of the algorithm is based on the AT&amp;T FSM Library (Mohri et al., 2000), both of which are available for download. 6.3 Results We implemented some of the proposed techniques and made comparisons with the previous method used by Saraclar and Sproat (2004). The full indexing method consumed too much time while indexing Broadcast News lattices and used too much memory while indexing phone lattices for Teleconferences. In the other cases, we confirmed that the new method yields identical results. In Table 1 we compare the index sizes for full indexing and partial indexing with the previous method. In both cases, the input lattices are pruned so that the cost (negative log probability) difference between two paths is less than six. Although the new method results in much smaller index sizes for the string case (i.e. nbest=1), it can result in very</context>
</contexts>
<marker>Saraclar, Sproat, 2004</marker>
<rawString>Murat Saraclar and Richard Sproat. 2004. Lattice-based search for spoken utterance retrieval. In Proc. HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Murat Saraclar</author>
<author>Michael Riley</author>
<author>Enrico Bocchieri</author>
<author>Vincent Goffin</author>
</authors>
<title>Towards automatic closed captioning: Low latency real-time broadcast news transcription.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing (ICSLP),</booktitle>
<location>Denver, Colorado, USA.</location>
<contexts>
<context position="25618" citStr="Saraclar et al., 2002" startWordPosition="4509" endWordPosition="4512">) to summarize the information in a precision-recall curve. 6.2 Corpora We use three different corpora to assess the effectiveness of different retrieval techniques. The first corpus is the DARPA Broadcast News corpus consisting of excerpts from TV or radio programs including various acoustic conditions. The test set is the 1998 Hub-4 Broadcast News (hub4e98) evaluation test set (available from LDC, Catalog no. LDC2000S86) EP [C(xi+r−1 i )]. which is 3 hours long and was manually segmented into 940 segments. It contains 32411 word tokens and 4885 word types. For ASR we use a real-time system (Saraclar et al., 2002). Since the system was designed for SDR, the recognition vocabulary of the system has over 200K words. The second corpus is the Switchboard corpus consisting of two party telephone conversations. The test set is the RT02 evaluation test set which is 5 hours long, has 120 conversation sides and was manually segmented into 6266 segments. It contains 65255 word tokens and 3788 word types. For ASR we use the first pass of the evaluation system (Ljolje et al., 2002). The recognition vocabulary of the system has over 45K words. The third corpus is named Teleconferences since it consists of multi-par</context>
</contexts>
<marker>Saraclar, Riley, Bocchieri, Goffin, 2002</marker>
<rawString>Murat Saraclar, Michael Riley, Enrico Bocchieri, and Vincent Goffin. 2002. Towards automatic closed captioning: Low latency real-time broadcast news transcription. In Proceedings of the International Conference on Spoken Language Processing (ICSLP), Denver, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Witbrock</author>
<author>Alexander Hauptmann</author>
</authors>
<title>Using words and phonetic strings for efficient information retrieval from imperfectly transcribed spoken documents.</title>
<date>1997</date>
<booktitle>In 2nd ACMInternational Conference on Digital Libraries (DL’97),</booktitle>
<pages>30--35</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="19340" citStr="Witbrock and Hauptmann (1997)" startWordPosition="3418" endWordPosition="3421">amples of some filter transducers that can be of interest in many applications. 1In most cases, F&apos; is the inverse of F. • Pronunciation Dictionary: a pronunciation dictionary can be used to map word sequences into their phonemic transcriptions, thus transform word lattices into equivalent phone lattices. This mapping can represented by a weighted transducer F. Using an index based on phone lattices allows a user to search for words that are not in the ASR vocabulary. In this case, the inverse transduction F&apos; is a grapheme to phoneme converter, commonly present in TTS front-ends. Among others, Witbrock and Hauptmann (1997) present a system where a phonetic transcript is obtained from the word transcript and retrieval is performed using both word and phone indices. • Vocabulary Restriction: in some cases using a full index can be prohibitive and unnecessary. It might be desirable to do partial indexing by ignoring some words (or phones) in the input. For example, we might wish to index only “named entities”, or just the consonants. This is mostly motivated by the reduction of the size of the index while retaining the necessary information. A similar approach is to apply a many to one mapping to index groups of p</context>
</contexts>
<marker>Witbrock, Hauptmann, 1997</marker>
<rawString>Michael Witbrock and Alexander Hauptmann. 1997. Using words and phonetic strings for efficient information retrieval from imperfectly transcribed spoken documents. In 2nd ACMInternational Conference on Digital Libraries (DL’97), pages 30–35, Philadelphia, PA, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>