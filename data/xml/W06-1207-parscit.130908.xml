<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002203">
<title confidence="0.9814">
Classifying Particle Semantics in English Verb-Particle Constructions
</title>
<author confidence="0.998066">
Paul Cook
</author>
<affiliation confidence="0.9983595">
Department of Computer Science
University of Toronto
</affiliation>
<address confidence="0.8960065">
Toronto, ON M5S 3G4
Canada
</address>
<email confidence="0.999428">
pcook@cs.toronto.edu
</email>
<author confidence="0.993574">
Suzanne Stevenson
</author>
<affiliation confidence="0.9983425">
Department of Computer Science
University of Toronto
</affiliation>
<address confidence="0.895912">
Toronto, ON M5S 3G4
Canada
</address>
<email confidence="0.999295">
suzanne@cs.toronto.edu
</email>
<sectionHeader confidence="0.993899" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998622">
Previous computational work on learning
the semantic properties of verb-particle
constructions (VPCs) has focused on their
compositionality, and has left unaddressed
the issue of which meaning of the compo-
nent words is being used in a given VPC.
We develop a feature space for use in clas-
sification of the sense contributed by the
particle in a VPC, and test this on VPCs
using the particle up. The features that
capture linguistic properties of VPCs that
are relevant to the semantics of the par-
ticle outperform linguistically uninformed
word co-occurrence features in our exper-
iments on unseen test VPCs.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999924366666667">
A challenge in learning the semantics of mul-
tiword expressions (MWEs) is their varying de-
grees of compositionality—the contribution of
each component word to the overall semantics
of the expression. MWEs fall on a range from
fully compositional (i.e., each component con-
tributes its meaning, as in frying pan) to non-
compositional or idiomatic (as in hit the roof). Be-
cause of this variation, researchers have explored
automatic methods for learning whether, or the de-
gree to which, an MWE is compositional (e.g.,
Lin, 1999; Bannard et al., 2003; McCarthy et al.,
2003; Fazly et al., 2005).
However, such work leaves unaddressed the ba-
sic issue of which of the possible meanings of a
component word is contributed when the MWE is
(at least partly) compositional. Words are notori-
ously ambiguous, so that even if it can be deter-
mined that an MWE is compositional, its meaning
is still unknown, since the actual semantic contri-
bution of the components is yet to be determined.
We address this problem in the domain of verb-
particle constructions (VPCs) in English, a rich
source of MWEs.
VPCs combine a verb with any of a finite set
of particles, as in jump up, figure out, or give in.
Particles such as up, out, or in, with their literal
meaning based in physical spatial relations, show
a variety of metaphorical and aspectual meaning
extensions, as exemplified here for the particle up:
</bodyText>
<listItem confidence="0.92670225">
(1a) The sun just came up. [vertical spatial movement]
(1b) She walked up to him. [movement toward a goal]
(1c) Drink up your juice! [completion]
(1d) He curled up into a ball. [reflexive movement]
</listItem>
<bodyText confidence="0.99910135">
Cognitive linguistic analysis, as in Lindner (1981),
can provide the basis for elaborating this type of
semantic variation.
Given such a sense inventory for a particle,
our goal is to automatically determine its mean-
ing when used with a given verb in a VPC. We
classify VPCs according to their particle sense,
using statistical features that capture the seman-
tic and syntactic properties of verbs and particles.
We contrast these with simple word co-occurrence
features, which are often used to indicate the se-
mantics of a target word. In our experiments, we
focus on VPCs using the particle up because it is
highly frequent and has a wide range of meanings.
However, it is worth emphasizing that our feature
space draws on general properties of VPCs, and is
not specific to this particle.
A VPC may be ambiguous, with its particle oc-
curring in more than one sense; in contrast to (1a),
come up may use up in a goal-oriented sense as in
</bodyText>
<page confidence="0.992247">
45
</page>
<note confidence="0.6949245">
Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, pages 45–53,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999915454545454">
The deadline is coming up. While our long-term
goal is token classification (disambiguation) of a
VPC in context, following other work on VPCs
(e.g., Bannard et al., 2003; McCarthy et al., 2003),
we begin here with the task of type classification.
Given our use of features which capture the statis-
tical behaviour relevant to a VPC across a corpus,
we assume that the outcome of type classification
yields the predominant sense of the particle in the
VPC. Predominant sense identification is a useful
component of sense disambiguation of word to-
kens (McCarthy et al., 2004), and we presume our
VPC type classification work will form the basis
for later token disambiguation.
Section 2 continues the paper with a discussion
of the features we developed for particle sense
classification. Section 3 first presents some brief
cognitive linguistic background, followed by the
sense classes of up used in our experiments. Sec-
tions 4 and 5 discuss our experimental set-up and
results, Section 6 related work, and Section 7 our
conclusions.
</bodyText>
<sectionHeader confidence="0.951854" genericHeader="method">
2 Features Used in Classification
</sectionHeader>
<bodyText confidence="0.9999088">
The following subsections describe the two sets of
features we investigated. The linguistic features
are motivated by specific semantic and syntactic
properties of verbs and VPCs, while the word co-
occurrence features are more general.
</bodyText>
<subsectionHeader confidence="0.9928815">
2.1 Linguistically Motivated Features
2.1.1 Slot Features
</subsectionHeader>
<bodyText confidence="0.997401151515152">
We hypothesize that the semantic contribution
of a particle when combined with a given verb is
related to the semantics of that verb. That is, the
particle contributes the same meaning when com-
bining with any of a semantic class of verbs.&apos; For
example, the VPCs drink up, eat up and gobble up
all draw on the completion sense of up; the VPCs
puff out, spread out and stretch out all draw on the
extension sense of out. The prevalence of these
patterns suggests that features which have been
shown to be effective for the semantic classifica-
tion of verbs may be useful for our task.
We adopt simple syntactic “slot” features which
have been successfully used in automatic seman-
tic classification of verbs (Joanis and Stevenson,
&apos;Villavicencio (2005) observes that verbs from a seman-
tic class will form VPCs with similar sets of particles. Here
we are hypothesizing further that VPCs formed from verbs
of a semantic class draw on the same meaning of the given
particle.
2003). The features are motivated by the fact
that semantic properties of a verb are reflected
in the syntactic expression of the participants in
the event the verb describes. The slot features
encode the relative frequencies of the syntactic
slots—subject, direct and indirect object, object of
a preposition—that the arguments and adjuncts of
a verb appear in. We calculate the slot features
over three contexts: all uses of a verb; all uses of
the verb in a VPC with the target particle (up in our
experiments); all uses of the verb in a VPC with
any of a set of high frequency particles (to capture
its semantics when used in VPCs in general).
</bodyText>
<subsectionHeader confidence="0.85378">
2.1.2 Particle Features
</subsectionHeader>
<bodyText confidence="0.99936465">
Two types of features are motivated by proper-
ties specific to the semantics and syntax of par-
ticles and VPCs. First, Wurmbrand (2000) notes
that compositional particle verbs in German (a
somewhat related phenomenon to English VPCs)
allow the replacement of their particle with seman-
tically similar particles. We extend this idea, hy-
pothesizing that when a verb combines with a par-
ticle such as up in a particular sense, the pattern
of usage of that verb in VPCs using all other par-
ticles may be indicative of the sense of the target
particle (in this case up) when combined with that
verb. To reflect this observation, we count the rel-
ative frequency of any occurrence of the verb used
in a VPC with each of a set of high frequency par-
ticles.
Second, one of the striking syntactic properties
of VPCs is that they can often occur in either the
joined configuration (2a) or the split configuration
(2b):
</bodyText>
<listItem confidence="0.9925355">
(2a) Drink up your milk! He walked out quickly.
(2b) Drink your milk up! He walked quickly out.
</listItem>
<bodyText confidence="0.996689714285714">
Bolinger (1971) notes that the joined construction
may be more favoured when the sense of the par-
ticle is not literal. To encode this, we calculate the
relative frequency of the verb co-occurring with
the particle up with each of – words between
the verb and up, reflecting varying degrees of verb-
particle separation.
</bodyText>
<subsectionHeader confidence="0.999583">
2.2 Word Co-occurrence Features
</subsectionHeader>
<bodyText confidence="0.9997855">
We also explore the use of general context fea-
tures, in the form of word co-occurrence frequency
vectors, which have been used in numerous ap-
proaches to determining the semantics of a target
</bodyText>
<page confidence="0.998805">
46
</page>
<bodyText confidence="0.999929">
word. Note, however, that unlike the task of word
sense disambiguation, which examines the context
of a target word token to be disambiguated, here
we are looking at aggregate contexts across all in-
stances of a target VPC, in order to perform type
classification.
We adopt very simple word co-occurrence fea-
tures (WCFs), calculated as the frequency of any
(non-stoplist) word within a certain window left
and right of the target. We noted above that the
target particle semantics is related both to the se-
mantics of the verb it co-occurs with, and to the
occurrence of the verb across VPCs with different
particles. Thus we not only calculate the WCFs of
the target VPC (a given verb used with the parti-
cle up), but also the WCFs of the verb itself, and
the verb used in a VPC with any of the high fre-
quency particles. These WCFs give us a very gen-
eral means for determining semantics, whose per-
formance we can contrast with our linguistic fea-
tures.
</bodyText>
<sectionHeader confidence="0.718765" genericHeader="method">
3 Particle Semantics and Sense Classes
</sectionHeader>
<bodyText confidence="0.999952">
We give some brief background on cognitive
grammar and its relation to particle semantics, and
then turn to the semantic analysis of up that we
draw on as the basis for the sense classes in our
experiments.
</bodyText>
<subsectionHeader confidence="0.999923">
3.1 Cognitive Grammar and Schemas
</subsectionHeader>
<bodyText confidence="0.99940705">
Some linguistic studies consider many VPCs to be
idiomatic, but do not give a detailed account of
the semantic similarities between them (Bolinger,
1971; Fraser, 1976; Jackendoff, 2002). In con-
trast, work in cognitive linguistics has claimed that
many so-called idiomatic expressions draw on the
compositional contribution of (at least some of)
their components (Lindner, 1981; Morgan, 1997;
Hampe, 2000). In cognitive grammar (Langacker,
1987), non-spatial concepts are represented as spa-
tial relations. Key terms from this framework are:
Trajector (TR) The object which is conceptually
foregrounded.
Landmark (LM) The object against which the
TR is foregrounded.
Schema An abstract conceptualization of an ex-
perience. Here we focus on schemas depict-
ing a TR, LM and their relationship in both
the initial configuration and the final config-
uration communicated by some expression.
</bodyText>
<figure confidence="0.9825005">
TR
TR
LM LM
Initial Final
</figure>
<figureCaption confidence="0.999978">
Figure 1: Schema for Vertical up.
</figureCaption>
<bodyText confidence="0.99868825">
The semantic contribution of a particle in a VPC
corresponds to a schema. For example, in sen-
tence (3), the TR is the balloon and the LM is the
ground the balloon is moving away from.
</bodyText>
<listItem confidence="0.520166">
(3) The balloon floated up.
</listItem>
<bodyText confidence="0.9999268">
The schema describing the semantic contribution
of the particle in the above sentence is shown
in Figure 1, which illustrates the relationship be-
tween the TR and LM in the initial and final con-
figurations.
</bodyText>
<subsectionHeader confidence="0.999536">
3.2 The Senses of up
</subsectionHeader>
<bodyText confidence="0.9999394">
Lindner (1981) identifies a set of schemas for each
of the particles up and out, and groups VPCs ac-
cording to which schema is contributed by their
particle. Here we describe the four senses of up
identified by Lindner.
</bodyText>
<subsectionHeader confidence="0.841113">
3.2.1 Vertical up (Vert-up)
</subsectionHeader>
<bodyText confidence="0.999335714285714">
In this schema (shown above in Figure 1), the
TR moves away from the LM in the direction of
increase along a vertically oriented axis. This in-
cludes prototypical spatial upward movement such
as that in sentence (3), as well as upward move-
ment along an abstract vertical axis as in sen-
tence (4).
</bodyText>
<listItem confidence="0.57612">
(4) The price of gas jumped up.
</listItem>
<bodyText confidence="0.998873888888889">
In Lindner’s analysis, this sense also includes ex-
tensions of upward movement where a vertical
path or posture is still salient. Note that in some of
these senses, the notion of verticality is metaphor-
ical; the contribution of such senses to a VPC may
not be considered compositional in a traditional
analysis. Some of the most common sense exten-
sions are given below, with a brief justification as
to why verticality is still salient.
</bodyText>
<page confidence="0.994804">
47
</page>
<figure confidence="0.998894333333333">
TR
TR
LM = goal LM = goal
LM = TR LM = TR
Initial Final
Initial Final
</figure>
<figureCaption confidence="0.996283">
Figure 2: Schema for Goal-Oriented up.
Figure 3: Schema for Reflexive up.
</figureCaption>
<figure confidence="0.990939333333333">
Vertical up Goal-Oriented up
Completive up
Reflexive up
</figure>
<bodyText confidence="0.995545444444444">
Up as a path into perceptual field. Spatially
high objects are generally easier to perceive.
Examples: show up, spring up, whip up.
Up as a path into mental field. Here up encodes
a path for mental as opposed to physical objects.
Examples: dream up, dredge up, think up.
Up as a path into a state of activity. Activity is
prototypically associated with an erect posture.
Examples: get up, set up, start up.
</bodyText>
<subsubsectionHeader confidence="0.590983">
3.2.2 Goal-Oriented up (Goal-up)
</subsubsectionHeader>
<bodyText confidence="0.983367666666667">
Here the TR approaches a goal LM; movement
is not necessarily vertical (see Figure 2). Proto-
typical examples are walk up and march up. This
category also includes extensions into the social
domain (kiss up and suck up), as well as exten-
sions into the domain of time (come up and move
up), as in:
(5a) The intern kissed up to his boss.
(5b) The deadline is coming up quickly.
</bodyText>
<subsectionHeader confidence="0.878613">
3.2.3 Completive up (Cmpl-up)
</subsectionHeader>
<bodyText confidence="0.999966714285714">
Cmpl-up is a sub-sense of Goal-up in which the
goal represents an action being done to comple-
tion. This sense shares its schema with Goal-up
(Figure 2), but it is considered as a separate sense
since it corresponds to uses of up as an aspectual
marker. Examples of Cmpl-up are: clean up, drink
up, eat up, finish up and study up.
</bodyText>
<subsectionHeader confidence="0.815092">
3.2.4 Reflexive up (Refl-up)
</subsectionHeader>
<bodyText confidence="0.998562666666666">
Reflexive up is a sub-sense of Goal-up in which
the sub-parts of the TR are approaching each other.
The schema for Refl-up is shown in Figure 3; it is
unique in that the TR and LM are the same object.
Examples of Refl-up are: bottle up, connect up,
couple up, curl up and roll up.
</bodyText>
<figureCaption confidence="0.993748">
Figure 4: Simplified schematic network for up.
</figureCaption>
<subsectionHeader confidence="0.995763">
3.3 The Sense Classes for Our Study
</subsectionHeader>
<bodyText confidence="0.99998835">
Adopting a cognitive linguistic perspective, we as-
sume that all uses of a particle make some compo-
sitional contribution of meaning to a VPC. In this
work, we classify target VPCs according to which
of the above senses of up is contributed to the ex-
pression. For example, the expressions jump up
and pick up are designated as being in the class
Vert-up since up in these VPCs has the vertical
sense, while clean up and drink up are designated
as being in the class Cmpl-up since up here has
the completive sense. The relations among the
senses of up can be shown in a “schematic net-
work” (Langacker, 1987). Figure 4 shows a sim-
plification of such a network in which we connect
more similar senses with shorter edges. This type
of analysis allows us to alter the granularity of our
classification in a linguistically motivated fashion
by combining closely related senses. Thus we can
explore the effect of different sense granularities
on classification.
</bodyText>
<sectionHeader confidence="0.996194" genericHeader="method">
4 Materials and Methods
</sectionHeader>
<subsectionHeader confidence="0.940242">
4.1 Experimental Expressions
</subsectionHeader>
<bodyText confidence="0.999938272727273">
We created a list of English VPCs using up, based
on a list of VPCs made available by McIntyre
(2001) and a list of VPCs compiled by two human
judges. The judges then filtered this list to include
only VPCs which they both agreed were valid, re-
sulting in a final list of 389 VPCs. From this list,
training, verification and test sets of sixty VPCs
each are randomly selected. Note that the expense
of manually annotating the data (as described be-
low) prevents us from using larger datasets in this
initial investigation. The experimental sets are
</bodyText>
<page confidence="0.996269">
48
</page>
<bodyText confidence="0.999921636363636">
chosen such that each includes the same propor-
tion of verbs across three frequency bands, so that
the sets do not differ in frequency distribution of
the verbs. (We use frequency of the verbs, rather
than the VPCs, since many of our features are
based on the verb of the expression, and moreover,
VPC frequency is approximate.) The verification
data is used in exploration of the feature space and
selection of final features to use in testing; the test
set is held out for final testing of the classifiers.
Each VPC in each dataset is annotated by the
two human judges according to which of the four
senses of up identified in Section 3.2 is contributed
to the VPC. As noted in Section 1, VPCs may
be ambiguous with respect to their particle sense.
Since our task here is type classification, the
judges identify the particle sense of a VPC in its
predominant usage, in their assessment. The ob-
served inter-annotator agreement is for each
dataset. The unweighted observed kappa scores
are ,and , for the training, verifica-
tion and test sets respectively.
</bodyText>
<subsectionHeader confidence="0.998015">
4.2 Calculation of the Features
</subsectionHeader>
<bodyText confidence="0.998366074074074">
We extract our features from the 100M word
British National Corpus (BNC, Burnard, 2000).
VPCs are identified using a simple heuristic based
on part-of-speech tags, similar to one technique
used by Baldwin (2005). A use of a verb is con-
sidered a VPC if it occurs with a particle (tagged
AVP) within a six word window to the right. Over
a random sample of 113 VPCs thus extracted, we
found 88% to be true VPCs, somewhat below the
performance of Baldwin’s (2005) best extraction
method, indicating potential room for improve-
ment.
The slot and particle features are calculated us-
ing a modified version of the ExtractVerb software
provided by Joanis and Stevenson (2003), which
runs over the BNC pre-processed using Abney’s
(1991) Cass chunker.
To compute the word co-occurrence features
(WCFs), we first determine the relative frequency
of all words which occur within a five word win-
dow left and right of any of the target expressions
in the training data. From this list we eliminate
the most frequent 1% of words as a stoplist and
then use the next most frequent words as “fea-
ture words”. For each “feature word”, we then cal-
culate its relative frequency of occurrence within
the same five word window of the target expres-
</bodyText>
<table confidence="0.999091333333333">
Sense Class #VPCs in Sense Class
Train Verification Test
Vert-up 24 33 27
Goal-up 1 1 3
Cmpl-up 20 23 22
Refl-up 15 3 8
</table>
<tableCaption confidence="0.991323">
Table 1: Frequency of items in each sense class.
</tableCaption>
<table confidence="0.999281">
Sense Class #VPCs in Sense Class
Train Verification Test
Vert-up 24 33 27
Goal-up 21 24 25
Cmpl-up
Refl-up 15 3 8
</table>
<tableCaption confidence="0.993835">
Table 2: Frequency of items in each class for the
3-way task.
</tableCaption>
<bodyText confidence="0.827524666666667">
sions in all datasets. We use and
to create feature sets WCF and WCF respec-
tively.
</bodyText>
<subsectionHeader confidence="0.993502">
4.3 Experimental Classes
</subsectionHeader>
<bodyText confidence="0.9999808125">
Table 1 shows the distribution of senses in each
dataset. Each of the training and verification sets
has only one VPC corresponding to Goal-up. Re-
call that Goal-up shares a schema with Cmpl-up,
and is therefore very close to it in meaning, as in-
dicated spatially in Figure 4. We therefore merge
Goal-up and Cmpl-up into a single sense, to pro-
vide more balanced classes.
Since we want to see how our features per-
form on differing granularities of sense classes, we
run each experiment as both a 3-way and 2-way
classification task. In the 3-way task, the sense
classes correspond to the meanings Vert-up, Goal-
up merged with Cmpl-up (as noted above), and
Refl-up, as shown in Table 2. In the 2-way task, we
further merge the classes corresponding to Goal-
</bodyText>
<table confidence="0.995363666666667">
Sense Class #VPCs in Sense Class
Train Verification Test
Vert-up 24 33 27
Goal-up 36 27 33
Cmpl-up
Refl-up
</table>
<tableCaption confidence="0.975165">
Table 3: Frequency of items in each class for the
2-way task.
</tableCaption>
<page confidence="0.999514">
49
</page>
<bodyText confidence="0.9999395">
up/Cmpl-up with that of Refl-up, as shown in Ta-
ble 3. We choose to merge these classes because
(as illustrated in Figure 4) Refl-up is a sub-sense of
Goal-up, and moreover, all three of these senses
contrast with Vert-up, in which increase along a
vertical axis is the salient property. It is worth em-
phasizing that the 2-way task is not simply a clas-
sification between literal and non-literal up—Vert-
up includes extensions of up in which the increase
along a vertical axis is metaphorical.
</bodyText>
<subsectionHeader confidence="0.9941525">
4.4 Evaluation Metrics and Classifier
Software
</subsectionHeader>
<bodyText confidence="0.999978576923077">
The variation in the frequency of the sense classes
of up across the datasets makes the true distri-
bution of the classes difficult to estimate. Fur-
thermore, there is no obvious informed baseline
for this task. Therefore, we make the assumption
that the true distribution of the classes is uniform,
and use the chance accuracy as the baseline
(where is the number of classes—in our exper-
iments, either or ). Accordingly, our measure
of classification accuracy should weight each class
evenly. Therefore, we report the average per class
accuracy, which gives equal weight to each class.
For classification we use LIBSVM (Chang and
Lin, 2001), an implementation of a support-vector
machine. We set the input parameters, cost
and gamma, using 10-fold cross-validation on the
training data. In addition, we assign a weight of
to each class to eliminate the ef-
fects of the variation in class size on the classifier.
Note that our choice of accuracy measure and
weighting of classes in the classifier is necessary
given our assumption of a uniform random base-
line. Since the accuracy values we report incorpo-
rate this weighting, these results cannot be com-
pared to a baseline of always choosing the most
frequent class.
</bodyText>
<sectionHeader confidence="0.997643" genericHeader="evaluation">
5 Experimental Results
</sectionHeader>
<bodyText confidence="0.997852833333333">
We present experimental results for both
Ver(ification) and unseen Test data, on each
set of features, individually and in combination.
All experiments are run on both the 2-way and
3-way sense classification, which have a chance
baseline of 50% and 33%, respectively.
</bodyText>
<table confidence="0.9992608">
Features 3-way Task 2-way Task
Ver Test Ver Test
Slots 41 51 53 67
Particles 37 33 65 47
Slots Particles 54 54 59 63
</table>
<tableCaption confidence="0.999625">
Table 4: Accuracy (%) using linguistic features.
</tableCaption>
<subsectionHeader confidence="0.89777">
5.1 Experiments Using the Linguistic
Features
</subsectionHeader>
<bodyText confidence="0.9991755">
The results for experiments using the features that
capture semantic and syntactic properties of verbs
and VPCs are summarized in Table 4, and dis-
cussed in turn below.
</bodyText>
<subsubsectionHeader confidence="0.929378">
5.1.1 Slot Features
</subsubsectionHeader>
<bodyText confidence="0.999991470588236">
Experiments using the slot features alone test
whether features that tap into semantic informa-
tion about a verb are sufficient to determine the
appropriate sense class of a particle when that verb
combines with it in a VPC. Although accuracy on
the test data is well above the baseline in both the
2-way and 3-way tasks, for verification data the
increase over the baseline is minimal. The class
corresponding to sense Refl-up in the 3-way task
is relatively small, which means that a small vari-
ation in classification on these verbs may lead to
a large variation in accuracy. However, we find
that the difference in accuracy across the datasets
is not due to performance on VPCs in this sense
class. Although these features show promise for
our task, the variation across the datasets indicates
the limitations of our small sample sizes.
</bodyText>
<subsubsectionHeader confidence="0.897296">
5.1.2 Particle Features
</subsubsectionHeader>
<bodyText confidence="0.999802333333333">
We also examine the performance of the parti-
cle features on their own, since to the best of our
knowledge, no such features have been used be-
fore in investigating VPCs. The results are dis-
appointing, with only the verification data on the
2-way task showing substantially higher accuracy
than the baseline. An analysis of errors reveals no
consistent explanation, suggesting again that the
variation may be due to small sample sizes.
</bodyText>
<subsectionHeader confidence="0.707064">
5.1.3 Slot + Particle Features
</subsectionHeader>
<bodyText confidence="0.999991166666667">
We hypothesize that the combination of the slot
features with the particle features will give an in-
crease in performance over either set of linguis-
tic features used individually, given that they tap
into differing properties of verbs and VPCs. We
find that the combination does indeed give more
</bodyText>
<page confidence="0.983167">
50
</page>
<table confidence="0.9927345">
Features 3-way Task 2-way Task
Ver Test Ver Test
WCF 45 42 59 51
WCF 38 34 55 48
</table>
<tableCaption confidence="0.999129">
Table 5: Accuracy (%) using WCFs.
</tableCaption>
<bodyText confidence="0.999675777777778">
consistent performance across verification and test
data than either feature set used individually. We
analyze the errors made using slot and particle fea-
tures separately, and find that they tend to classify
different sets of verbs incorrectly. Therefore, we
conclude that these feature sets are at least some-
what complementary. By combining these com-
plementary feature sets, the classifier is better able
to generalise across different datasets.
</bodyText>
<subsectionHeader confidence="0.995946">
5.2 Experiments Using WCFs
</subsectionHeader>
<bodyText confidence="0.999726666666667">
Our goal was to compare the more knowledge-rich
slot and particle features to an alternative feature
set, the WCFs, which does not rely on linguistic
analysis of the semantics and syntax of verbs and
VPCs. Recall that we experiment with both 200
feature words, WCF , and 500 feature words,
WCF , as shown in Table 5. Most of the exper-
iments using WCFs perform worse than the cor-
responding experiment using all the linguistic fea-
tures. It appears that the linguistically motivated
features are better suited to our task than simple
word context features.
</bodyText>
<subsectionHeader confidence="0.99966">
5.3 Linguistic Features and WCFs Combined
</subsectionHeader>
<bodyText confidence="0.999882210526316">
Although the WCFs on their own perform worse
than the linguistic features, we find that the lin-
guistic features and WCFs are at least somewhat
complementary since they tend to classify differ-
ent verbs incorrectly. We hypothesize that, as with
the slot and particle features, the different types
of information provided by the linguistic features
and WCFs may improve performance in combina-
tion. We therefore combine the linguistic features
with each of the WCF and WCF features;
see Table 6. However, contrary to our hypothesis,
for the most part, the experiments using the full
combination of features give accuracies the same
or below that of the corresponding experiment us-
ing just the linguistic features. We surmise that
these very different types of features—the linguis-
tic features and WCFs—must be providing con-
flicting rather than complementary information to
the classifier, so that no improvement is attained.
</bodyText>
<table confidence="0.983799">
Features 3-way Task 2-way Task
Ver Test Ver Test
Combined 53 45 63 53
Combined 54 46 65 49
</table>
<tableCaption confidence="0.9884235">
Table 6: Accuracy (%) combining linguistic fea-
tures with WCFs.
</tableCaption>
<subsectionHeader confidence="0.998309">
5.4 Discussion of Results
</subsectionHeader>
<bodyText confidence="0.999987902439024">
The best performance across the datasets is at-
tained using all the linguistic features. The lin-
guistically uninformed WCFs perform worse on
their own, and do not consistently help (and in
some cases hurt) the performance of the linguis-
tic features when combined with them. We con-
clude then that linguistically based features are
motivated for this task. Note that the features are
still quite simple, and straightforward to extract
from a corpus—i.e., linguistically informed does
not mean expensive (although the slot features do
require access to chunked text).
Interestingly, in determining the semantic near-
est neighbor of German particle verbs, Schulte im
Walde (2005) found that WCFs that are restricted
to the arguments of the verb outperform simple
window-based co-occurrence features. Although
her task is quite different from ours, similarly re-
stricting our WCFs may enable them to encode
more linguistically-relevant information.
The accuracies we achieve with the linguistic
features correspond to a 30–31% reduction in er-
ror rate over the chance baseline for the 3-way
task, and an 18–26% reduction in error rate for
the 2-way task. Although we expected that the
2-way task may be easier, since it requires less
fine-grained distinctions, it is clear that combining
senses that have some motivation for being treated
separately comes at a price.
The reductions in error rate that we achieve with
our best features are quite respectable for a first
attempt at addressing this problem, but more work
clearly remains. There is a relatively high variabil-
ity in performance across the verification and test
sets, indicating that we need a larger number of
experimental expressions to be able to draw firmer
conclusions. Even if our current results extend to
larger datasets, we intend to explore other feature
approaches, such as word co-occurrence features
for specific syntactic slots as suggested above, in
order to improve the performance.
</bodyText>
<page confidence="0.998012">
51
</page>
<sectionHeader confidence="0.99992" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.997809098039216">
The semantic compositionality of VPC types has
recently received increasing attention. McCarthy
et al. (2003) use several measures to automati-
cally rate the overall compositionality of a VPC.
Bannard (2005), extending work by Bannard et al.
(2003), instead considers the extent to which the
verb and particle each contribute semantically to
the VPC. In contrast, our work assumes that the
particle of every VPC contributes composition-
ally to its meaning. We draw on cognitive lin-
guistic analysis that posits a rich set of literal and
metaphorical meaning possibilities of a particle,
which has been previously overlooked in compu-
tational work on VPCs.
In this first investigation of particle meaning in
VPCs, we choose to focus on type-based clas-
sification, partly due to the significant extra ex-
pense of manually annotating sufficient numbers
of tokens in text. As noted earlier, though, VPCs
can take on different meanings, indicating a short-
coming of type-based work. Patrick and Fletcher
(2005) classify VPC tokens, considering each as
compositional, non-compositional or not a VPC.
Again, however, it is important to recognize which
of the possible meaning components is being con-
tributed. In this vein, Uchiyama et al. (2005)
tackle token classification of Japanese compound
verbs (similar to VPCs) as aspectual, spatial, or
adverbial. In the future, we aim to extend the
scope of our work, to determine the meaning of
a particle in a VPC token, along the lines of our
sense classes here. This will almost certainly re-
quire semantic classification of the verb token (La-
pata and Brew, 2004), similar to our approach here
of using the semantic class of a verb type as indica-
tive of the meaning of a particle type.
Particle semantics has clear relations to prepo-
sition semantics. Some research has focused on
the sense disambiguation of specific prepositions
(e.g., Alam, 2004), while other work has classi-
fied preposition tokens according to their seman-
tic role (O’Hara and Wiebe, 2003). Moreover,
two large lexical resources of preposition senses
are currently under construction, The Preposi-
tion Project (Litkowski, 2005) and PrepNet (Saint-
Dizier, 2005). These resources were not suitable
as the basis for our sense classes because they do
not address the range of metaphorical extensions
that a preposition/particle can take on, but future
work may enable larger scale studies of the type
needed to adequately address VPC semantics.
</bodyText>
<sectionHeader confidence="0.997831" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999989590909091">
While progress has recently been made in tech-
niques for assessing the compositionality of VPCs,
work thus far has left unaddressed the problem of
determining the particular meaning of the compo-
nents. We focus here on the semantic contribution
of the particle—a part-of-speech whose seman-
tic complexity and range of metaphorical mean-
ing extensions has been largely overlooked in prior
computational work. Drawing on work within
cognitive linguistics, we annotate a set of 180
VPCs according to the sense class of the particle
up, our experimental focus in this initial investiga-
tion. We develop features that capture linguistic
properties of VPCs that are relevant to the seman-
tics of particles, and show that they outperform
linguistically uninformed word co-occurrence fea-
tures, achieving around 20–30% reduction in er-
ror rate over a chance baseline. Areas of on-going
work include development of a broader range of
features, consideration of methods for token-based
semantic determination, and creation of larger ex-
perimental datasets.
</bodyText>
<sectionHeader confidence="0.998189" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9997945">
S. Abney. 1991. Parsing by chunks. In R. Berwick,
S. Abney, and C. Tenny, editors, Principle-
Based Parsing: Computation and Psycholin-
guistics, p. 257–278. Kluwer Academic Pub-
lishers.
Y. S. Alam. 2004. Decision trees for sense dis-
ambiguation of prepositions: Case of over. In
HLT-NAACL 2004: Workshop on Computa-
tional Lexical Semantics, p. 52–59.
T. Baldwin. 2005. The deep lexical acquisition of
English verb-particle constructions. Computer
Speech and Language, Special Issue on Multi-
word Expressions, 19(4):398–414.
C. Bannard. 2005. Learning about the meaning of
verb-particle constructions from corpora. Com-
puter Speech and Language, Special Issue on
Multiword Expressions, 19(4):467–478.
C. Bannard, T. Baldwin, and A. Lascarides. 2003.
A statistical approach to the semantics of verb-
particles. In Proceedings of the ACL-2003
Workshop on Multiword Expressions: Analysis,
Acquisition and Treatment, p. 65–72.
D. Bolinger. 1971. The Phrasal Verb in English.
Harvard University Press.
</reference>
<page confidence="0.973651">
52
</page>
<reference confidence="0.99983691">
L. Burnard. 2000. The British National Cor-
pus Users Reference Guide. Oxford University
Computing Services.
C.-C. Chang and C.-J. Lin. 2001. LIBSVM: a
library for support vector machines. Soft-
ware available at http://www.csie.ntu.
edu.tw/˜cjlin/libsvm.
A. Fazly, R. North, and S. Stevenson. 2005. Au-
tomatically distinguishing literal and figurative
usages of highly polysemous verbs. In Proceed-
ings of the ACL-2005 Workshop on Deep Lexi-
cal Acquisition.
B. Fraser. 1976. The Verb-Particle Combination in
English. Academic Press.
B. Hampe. 2000. Facing up to the meaning of
‘face up to’: A cognitive semantico-pragmatic
analysis of an English verb-particle construc-
tion. In A. Foolen and F. van der Leek, edi-
tors, Constructions in Cognitive Linguistics. Se-
lected Papers from the fifth International Cog-
nitive Linguistics Conference, p. 81–101. John
Benjamins Publishing Company.
R. Jackendoff. 2002. English particle construc-
tions, the lexicon, and the autonomy of syntax.
In N. Dehe, R. Jackendoff, A. McIntyre, and
S. Urban, editors, Verb-Particle Explorations.
Mouton de Gruyter.
E. Joanis and S. Stevenson. 2003. A general fea-
ture space for automatic verb classification. In
Proceedings of the Conference of the European
Chapter of the Association for Computational
Linguistics (EACL-2003), p. 163–170.
R. W. Langacker. 1987. Foundations of Cognitive
Grammar: Theoretical Prerequisites, volume 1.
Stanford University Press, Stanford.
M. Lapata and C. Brew. 2004. Verb class disam-
biguation using informative priors. Computa-
tional Linguistics, 30(1):45–73.
D. Lin. 1999. Automatic identification of non-
compositional phrases. In Proceedings of the
37th Annual Meeting of the Association for
Computational Linguistics, p. 317–324.
S. Lindner. 1981. A lexico-semantic analysis of
English verb particle constructions with out and
up. Ph.D. thesis, University of California, San
Diego.
K. C. Litkowski. 2005. The Preposition Project. In
Proceedings of the Second ACL-SIGSEM Work-
shop on the Linguistic Dimensions of Preposi-
tions and their Use in Computational Linguis-
tics Formalisms and Applications.
D. McCarthy, B. Keller, and J. Carroll. 2003.
Detecting a continuum of compositionality in
phrasal verbs. In Proceedings of the ACL-
SIGLEX Workshop on Multiword Expressions:
Analysis, Acquisition and Treatment.
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll.
2004. Finding predominant word senses in un-
tagged text. In Proceedings of the 42nd Annual
Meeting of the Association for Computational
Linguistics, p. 280–287.
A. McIntyre. 2001. The particle verb list.
http://www.uni-leipzig.de/
˜angling/mcintyre/pv.list.pdf.
P. S. Morgan. 1997. Figuring out figure out:
Metaphor and the semantics of the English
verb-particle construction. Cognitive Linguis-
tics, 8(4):327–357.
T. O’Hara and J. Wiebe. 2003. Preposition se-
mantic classification via Penn Treebank and
FrameNet. In Proceedings of CoNLL-2003, p.
79–86.
J. Patrick and J. Fletcher. 2005. Classifying verb-
particle constructions by verb arguments. In
Proceedings of the Second ACL-SIGSEM Work-
shop on the Linguistic Dimensions of Preposi-
tions and their use in Computational Linguistics
Formalisms and Applications, p. 200–209.
P. Saint-Dizier. 2005. PrepNet: a framework for
describing prepositions: Preliminary investiga-
tion results. In Proceedings of the Sixth Interna-
tional Workshop on Computational Semantics
(IWCS’05), p. 145–157.
S. Schulte im Walde. 2005. Exploring features to
identify semantic nearest neighbours: A case
study on German particle verbs. In Proceed-
ings of the International Conference on Recent
Advances in Natural Language Processing.
K. Uchiyama, T. Baldwin, and S. Ishizaki.
2005. Disambiguating Japanese compound
verbs. Computer Speech and Language, Special
Issue on Multiword Expressions, 19(4):497–
512.
A. Villavicencio. 2005. The availability of verb-
particle constructions in lexical resources: How
much is enough? Computer Speech and Lan-
guage, Special Issue on Multiword Expressions,
19(4):415–432.
S. Wurmbrand. 2000. The structure(s) of particle
verbs. Master’s thesis, McGill University.
</reference>
<page confidence="0.999351">
53
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.794538">
<title confidence="0.999953">Classifying Particle Semantics in English Verb-Particle Constructions</title>
<author confidence="0.998652">Paul</author>
<affiliation confidence="0.9998435">Department of Computer University of</affiliation>
<address confidence="0.940937">Toronto, ON M5S</address>
<email confidence="0.996283">pcook@cs.toronto.edu</email>
<author confidence="0.939865">Suzanne</author>
<affiliation confidence="0.9998095">Department of Computer University of</affiliation>
<address confidence="0.940878">Toronto, ON M5S</address>
<email confidence="0.999446">suzanne@cs.toronto.edu</email>
<abstract confidence="0.9974148125">Previous computational work on learning the semantic properties of verb-particle constructions (VPCs) has focused on their compositionality, and has left unaddressed issue of of the component words is being used in a given VPC. We develop a feature space for use in classification of the sense contributed by the particle in a VPC, and test this on VPCs the particle The features that capture linguistic properties of VPCs that are relevant to the semantics of the particle outperform linguistically uninformed word co-occurrence features in our experiments on unseen test VPCs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abney</author>
</authors>
<title>Parsing by chunks. In</title>
<date>1991</date>
<booktitle>PrincipleBased Parsing: Computation and Psycholinguistics,</booktitle>
<pages>257--278</pages>
<editor>R. Berwick, S. Abney, and C. Tenny, editors,</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<marker>Abney, 1991</marker>
<rawString>S. Abney. 1991. Parsing by chunks. In R. Berwick, S. Abney, and C. Tenny, editors, PrincipleBased Parsing: Computation and Psycholinguistics, p. 257–278. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y S Alam</author>
</authors>
<title>Decision trees for sense disambiguation of prepositions: Case of over.</title>
<date>2004</date>
<booktitle>In HLT-NAACL 2004: Workshop on Computational Lexical Semantics,</booktitle>
<pages>52--59</pages>
<contexts>
<context position="29024" citStr="Alam, 2004" startWordPosition="4891" endWordPosition="4892">se compound verbs (similar to VPCs) as aspectual, spatial, or adverbial. In the future, we aim to extend the scope of our work, to determine the meaning of a particle in a VPC token, along the lines of our sense classes here. This will almost certainly require semantic classification of the verb token (Lapata and Brew, 2004), similar to our approach here of using the semantic class of a verb type as indicative of the meaning of a particle type. Particle semantics has clear relations to preposition semantics. Some research has focused on the sense disambiguation of specific prepositions (e.g., Alam, 2004), while other work has classified preposition tokens according to their semantic role (O’Hara and Wiebe, 2003). Moreover, two large lexical resources of preposition senses are currently under construction, The Preposition Project (Litkowski, 2005) and PrepNet (SaintDizier, 2005). These resources were not suitable as the basis for our sense classes because they do not address the range of metaphorical extensions that a preposition/particle can take on, but future work may enable larger scale studies of the type needed to adequately address VPC semantics. 7 Conclusions While progress has recentl</context>
</contexts>
<marker>Alam, 2004</marker>
<rawString>Y. S. Alam. 2004. Decision trees for sense disambiguation of prepositions: Case of over. In HLT-NAACL 2004: Workshop on Computational Lexical Semantics, p. 52–59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Baldwin</author>
</authors>
<title>The deep lexical acquisition of English verb-particle constructions.</title>
<date>2005</date>
<journal>Computer Speech and Language, Special Issue on Multiword Expressions,</journal>
<volume>19</volume>
<issue>4</issue>
<contexts>
<context position="16432" citStr="Baldwin (2005)" startWordPosition="2796" endWordPosition="2797">n 1, VPCs may be ambiguous with respect to their particle sense. Since our task here is type classification, the judges identify the particle sense of a VPC in its predominant usage, in their assessment. The observed inter-annotator agreement is for each dataset. The unweighted observed kappa scores are ,and , for the training, verification and test sets respectively. 4.2 Calculation of the Features We extract our features from the 100M word British National Corpus (BNC, Burnard, 2000). VPCs are identified using a simple heuristic based on part-of-speech tags, similar to one technique used by Baldwin (2005). A use of a verb is considered a VPC if it occurs with a particle (tagged AVP) within a six word window to the right. Over a random sample of 113 VPCs thus extracted, we found 88% to be true VPCs, somewhat below the performance of Baldwin’s (2005) best extraction method, indicating potential room for improvement. The slot and particle features are calculated using a modified version of the ExtractVerb software provided by Joanis and Stevenson (2003), which runs over the BNC pre-processed using Abney’s (1991) Cass chunker. To compute the word co-occurrence features (WCFs), we first determine t</context>
</contexts>
<marker>Baldwin, 2005</marker>
<rawString>T. Baldwin. 2005. The deep lexical acquisition of English verb-particle constructions. Computer Speech and Language, Special Issue on Multiword Expressions, 19(4):398–414.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Bannard</author>
</authors>
<title>Learning about the meaning of verb-particle constructions from corpora.</title>
<date>2005</date>
<journal>Computer Speech and Language, Special Issue on Multiword Expressions,</journal>
<volume>19</volume>
<issue>4</issue>
<contexts>
<context position="27350" citStr="Bannard (2005)" startWordPosition="4618" endWordPosition="4619">lity in performance across the verification and test sets, indicating that we need a larger number of experimental expressions to be able to draw firmer conclusions. Even if our current results extend to larger datasets, we intend to explore other feature approaches, such as word co-occurrence features for specific syntactic slots as suggested above, in order to improve the performance. 51 6 Related Work The semantic compositionality of VPC types has recently received increasing attention. McCarthy et al. (2003) use several measures to automatically rate the overall compositionality of a VPC. Bannard (2005), extending work by Bannard et al. (2003), instead considers the extent to which the verb and particle each contribute semantically to the VPC. In contrast, our work assumes that the particle of every VPC contributes compositionally to its meaning. We draw on cognitive linguistic analysis that posits a rich set of literal and metaphorical meaning possibilities of a particle, which has been previously overlooked in computational work on VPCs. In this first investigation of particle meaning in VPCs, we choose to focus on type-based classification, partly due to the significant extra expense of m</context>
</contexts>
<marker>Bannard, 2005</marker>
<rawString>C. Bannard. 2005. Learning about the meaning of verb-particle constructions from corpora. Computer Speech and Language, Special Issue on Multiword Expressions, 19(4):467–478.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Bannard</author>
<author>T Baldwin</author>
<author>A Lascarides</author>
</authors>
<title>A statistical approach to the semantics of verbparticles.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL-2003 Workshop on Multiword Expressions: Analysis, Acquisition and Treatment,</booktitle>
<pages>65--72</pages>
<contexts>
<context position="1477" citStr="Bannard et al., 2003" startWordPosition="224" endWordPosition="227">-occurrence features in our experiments on unseen test VPCs. 1 Introduction A challenge in learning the semantics of multiword expressions (MWEs) is their varying degrees of compositionality—the contribution of each component word to the overall semantics of the expression. MWEs fall on a range from fully compositional (i.e., each component contributes its meaning, as in frying pan) to noncompositional or idiomatic (as in hit the roof). Because of this variation, researchers have explored automatic methods for learning whether, or the degree to which, an MWE is compositional (e.g., Lin, 1999; Bannard et al., 2003; McCarthy et al., 2003; Fazly et al., 2005). However, such work leaves unaddressed the basic issue of which of the possible meanings of a component word is contributed when the MWE is (at least partly) compositional. Words are notoriously ambiguous, so that even if it can be determined that an MWE is compositional, its meaning is still unknown, since the actual semantic contribution of the components is yet to be determined. We address this problem in the domain of verbparticle constructions (VPCs) in English, a rich source of MWEs. VPCs combine a verb with any of a finite set of particles, a</context>
<context position="3812" citStr="Bannard et al., 2003" startWordPosition="619" endWordPosition="622">orth emphasizing that our feature space draws on general properties of VPCs, and is not specific to this particle. A VPC may be ambiguous, with its particle occurring in more than one sense; in contrast to (1a), come up may use up in a goal-oriented sense as in 45 Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, pages 45–53, Sydney, July 2006. c�2006 Association for Computational Linguistics The deadline is coming up. While our long-term goal is token classification (disambiguation) of a VPC in context, following other work on VPCs (e.g., Bannard et al., 2003; McCarthy et al., 2003), we begin here with the task of type classification. Given our use of features which capture the statistical behaviour relevant to a VPC across a corpus, we assume that the outcome of type classification yields the predominant sense of the particle in the VPC. Predominant sense identification is a useful component of sense disambiguation of word tokens (McCarthy et al., 2004), and we presume our VPC type classification work will form the basis for later token disambiguation. Section 2 continues the paper with a discussion of the features we developed for particle sense</context>
<context position="27391" citStr="Bannard et al. (2003)" startWordPosition="4623" endWordPosition="4626">ification and test sets, indicating that we need a larger number of experimental expressions to be able to draw firmer conclusions. Even if our current results extend to larger datasets, we intend to explore other feature approaches, such as word co-occurrence features for specific syntactic slots as suggested above, in order to improve the performance. 51 6 Related Work The semantic compositionality of VPC types has recently received increasing attention. McCarthy et al. (2003) use several measures to automatically rate the overall compositionality of a VPC. Bannard (2005), extending work by Bannard et al. (2003), instead considers the extent to which the verb and particle each contribute semantically to the VPC. In contrast, our work assumes that the particle of every VPC contributes compositionally to its meaning. We draw on cognitive linguistic analysis that posits a rich set of literal and metaphorical meaning possibilities of a particle, which has been previously overlooked in computational work on VPCs. In this first investigation of particle meaning in VPCs, we choose to focus on type-based classification, partly due to the significant extra expense of manually annotating sufficient numbers of </context>
</contexts>
<marker>Bannard, Baldwin, Lascarides, 2003</marker>
<rawString>C. Bannard, T. Baldwin, and A. Lascarides. 2003. A statistical approach to the semantics of verbparticles. In Proceedings of the ACL-2003 Workshop on Multiword Expressions: Analysis, Acquisition and Treatment, p. 65–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bolinger</author>
</authors>
<title>The Phrasal Verb in English.</title>
<date>1971</date>
<publisher>Harvard University Press.</publisher>
<contexts>
<context position="7656" citStr="Bolinger (1971)" startWordPosition="1271" endWordPosition="1272">n a particular sense, the pattern of usage of that verb in VPCs using all other particles may be indicative of the sense of the target particle (in this case up) when combined with that verb. To reflect this observation, we count the relative frequency of any occurrence of the verb used in a VPC with each of a set of high frequency particles. Second, one of the striking syntactic properties of VPCs is that they can often occur in either the joined configuration (2a) or the split configuration (2b): (2a) Drink up your milk! He walked out quickly. (2b) Drink your milk up! He walked quickly out. Bolinger (1971) notes that the joined construction may be more favoured when the sense of the particle is not literal. To encode this, we calculate the relative frequency of the verb co-occurring with the particle up with each of – words between the verb and up, reflecting varying degrees of verbparticle separation. 2.2 Word Co-occurrence Features We also explore the use of general context features, in the form of word co-occurrence frequency vectors, which have been used in numerous approaches to determining the semantics of a target 46 word. Note, however, that unlike the task of word sense disambiguation,</context>
<context position="9567" citStr="Bolinger, 1971" startWordPosition="1603" endWordPosition="1604">in a VPC with any of the high frequency particles. These WCFs give us a very general means for determining semantics, whose performance we can contrast with our linguistic features. 3 Particle Semantics and Sense Classes We give some brief background on cognitive grammar and its relation to particle semantics, and then turn to the semantic analysis of up that we draw on as the basis for the sense classes in our experiments. 3.1 Cognitive Grammar and Schemas Some linguistic studies consider many VPCs to be idiomatic, but do not give a detailed account of the semantic similarities between them (Bolinger, 1971; Fraser, 1976; Jackendoff, 2002). In contrast, work in cognitive linguistics has claimed that many so-called idiomatic expressions draw on the compositional contribution of (at least some of) their components (Lindner, 1981; Morgan, 1997; Hampe, 2000). In cognitive grammar (Langacker, 1987), non-spatial concepts are represented as spatial relations. Key terms from this framework are: Trajector (TR) The object which is conceptually foregrounded. Landmark (LM) The object against which the TR is foregrounded. Schema An abstract conceptualization of an experience. Here we focus on schemas depicti</context>
</contexts>
<marker>Bolinger, 1971</marker>
<rawString>D. Bolinger. 1971. The Phrasal Verb in English. Harvard University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Burnard</author>
</authors>
<title>The British National Corpus Users Reference Guide.</title>
<date>2000</date>
<institution>Oxford University Computing Services.</institution>
<contexts>
<context position="16308" citStr="Burnard, 2000" startWordPosition="2777" endWordPosition="2778">n judges according to which of the four senses of up identified in Section 3.2 is contributed to the VPC. As noted in Section 1, VPCs may be ambiguous with respect to their particle sense. Since our task here is type classification, the judges identify the particle sense of a VPC in its predominant usage, in their assessment. The observed inter-annotator agreement is for each dataset. The unweighted observed kappa scores are ,and , for the training, verification and test sets respectively. 4.2 Calculation of the Features We extract our features from the 100M word British National Corpus (BNC, Burnard, 2000). VPCs are identified using a simple heuristic based on part-of-speech tags, similar to one technique used by Baldwin (2005). A use of a verb is considered a VPC if it occurs with a particle (tagged AVP) within a six word window to the right. Over a random sample of 113 VPCs thus extracted, we found 88% to be true VPCs, somewhat below the performance of Baldwin’s (2005) best extraction method, indicating potential room for improvement. The slot and particle features are calculated using a modified version of the ExtractVerb software provided by Joanis and Stevenson (2003), which runs over the </context>
</contexts>
<marker>Burnard, 2000</marker>
<rawString>L. Burnard. 2000. The British National Corpus Users Reference Guide. Oxford University Computing Services.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-C Chang</author>
<author>C-J Lin</author>
</authors>
<title>LIBSVM: a library for support vector machines. Software available at http://www.csie.ntu.</title>
<date>2001</date>
<note>edu.tw/˜cjlin/libsvm.</note>
<contexts>
<context position="19996" citStr="Chang and Lin, 2001" startWordPosition="3418" endWordPosition="3421">e frequency of the sense classes of up across the datasets makes the true distribution of the classes difficult to estimate. Furthermore, there is no obvious informed baseline for this task. Therefore, we make the assumption that the true distribution of the classes is uniform, and use the chance accuracy as the baseline (where is the number of classes—in our experiments, either or ). Accordingly, our measure of classification accuracy should weight each class evenly. Therefore, we report the average per class accuracy, which gives equal weight to each class. For classification we use LIBSVM (Chang and Lin, 2001), an implementation of a support-vector machine. We set the input parameters, cost and gamma, using 10-fold cross-validation on the training data. In addition, we assign a weight of to each class to eliminate the effects of the variation in class size on the classifier. Note that our choice of accuracy measure and weighting of classes in the classifier is necessary given our assumption of a uniform random baseline. Since the accuracy values we report incorporate this weighting, these results cannot be compared to a baseline of always choosing the most frequent class. 5 Experimental Results We </context>
</contexts>
<marker>Chang, Lin, 2001</marker>
<rawString>C.-C. Chang and C.-J. Lin. 2001. LIBSVM: a library for support vector machines. Software available at http://www.csie.ntu. edu.tw/˜cjlin/libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Fazly</author>
<author>R North</author>
<author>S Stevenson</author>
</authors>
<title>Automatically distinguishing literal and figurative usages of highly polysemous verbs.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL-2005 Workshop on Deep Lexical Acquisition.</booktitle>
<contexts>
<context position="1521" citStr="Fazly et al., 2005" startWordPosition="232" endWordPosition="235">seen test VPCs. 1 Introduction A challenge in learning the semantics of multiword expressions (MWEs) is their varying degrees of compositionality—the contribution of each component word to the overall semantics of the expression. MWEs fall on a range from fully compositional (i.e., each component contributes its meaning, as in frying pan) to noncompositional or idiomatic (as in hit the roof). Because of this variation, researchers have explored automatic methods for learning whether, or the degree to which, an MWE is compositional (e.g., Lin, 1999; Bannard et al., 2003; McCarthy et al., 2003; Fazly et al., 2005). However, such work leaves unaddressed the basic issue of which of the possible meanings of a component word is contributed when the MWE is (at least partly) compositional. Words are notoriously ambiguous, so that even if it can be determined that an MWE is compositional, its meaning is still unknown, since the actual semantic contribution of the components is yet to be determined. We address this problem in the domain of verbparticle constructions (VPCs) in English, a rich source of MWEs. VPCs combine a verb with any of a finite set of particles, as in jump up, figure out, or give in. Partic</context>
</contexts>
<marker>Fazly, North, Stevenson, 2005</marker>
<rawString>A. Fazly, R. North, and S. Stevenson. 2005. Automatically distinguishing literal and figurative usages of highly polysemous verbs. In Proceedings of the ACL-2005 Workshop on Deep Lexical Acquisition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Fraser</author>
</authors>
<title>The Verb-Particle Combination in English.</title>
<date>1976</date>
<publisher>Academic Press.</publisher>
<contexts>
<context position="9581" citStr="Fraser, 1976" startWordPosition="1605" endWordPosition="1606">y of the high frequency particles. These WCFs give us a very general means for determining semantics, whose performance we can contrast with our linguistic features. 3 Particle Semantics and Sense Classes We give some brief background on cognitive grammar and its relation to particle semantics, and then turn to the semantic analysis of up that we draw on as the basis for the sense classes in our experiments. 3.1 Cognitive Grammar and Schemas Some linguistic studies consider many VPCs to be idiomatic, but do not give a detailed account of the semantic similarities between them (Bolinger, 1971; Fraser, 1976; Jackendoff, 2002). In contrast, work in cognitive linguistics has claimed that many so-called idiomatic expressions draw on the compositional contribution of (at least some of) their components (Lindner, 1981; Morgan, 1997; Hampe, 2000). In cognitive grammar (Langacker, 1987), non-spatial concepts are represented as spatial relations. Key terms from this framework are: Trajector (TR) The object which is conceptually foregrounded. Landmark (LM) The object against which the TR is foregrounded. Schema An abstract conceptualization of an experience. Here we focus on schemas depicting a TR, LM an</context>
</contexts>
<marker>Fraser, 1976</marker>
<rawString>B. Fraser. 1976. The Verb-Particle Combination in English. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Hampe</author>
</authors>
<title>Facing up to the meaning of ‘face up to’: A cognitive semantico-pragmatic analysis of an English verb-particle construction.</title>
<date>2000</date>
<booktitle>Constructions in Cognitive Linguistics. Selected Papers from the fifth International Cognitive Linguistics Conference,</booktitle>
<pages>81--101</pages>
<editor>In A. Foolen and F. van der Leek, editors,</editor>
<publisher>John Benjamins Publishing Company.</publisher>
<contexts>
<context position="9819" citStr="Hampe, 2000" startWordPosition="1639" endWordPosition="1640">cognitive grammar and its relation to particle semantics, and then turn to the semantic analysis of up that we draw on as the basis for the sense classes in our experiments. 3.1 Cognitive Grammar and Schemas Some linguistic studies consider many VPCs to be idiomatic, but do not give a detailed account of the semantic similarities between them (Bolinger, 1971; Fraser, 1976; Jackendoff, 2002). In contrast, work in cognitive linguistics has claimed that many so-called idiomatic expressions draw on the compositional contribution of (at least some of) their components (Lindner, 1981; Morgan, 1997; Hampe, 2000). In cognitive grammar (Langacker, 1987), non-spatial concepts are represented as spatial relations. Key terms from this framework are: Trajector (TR) The object which is conceptually foregrounded. Landmark (LM) The object against which the TR is foregrounded. Schema An abstract conceptualization of an experience. Here we focus on schemas depicting a TR, LM and their relationship in both the initial configuration and the final configuration communicated by some expression. TR TR LM LM Initial Final Figure 1: Schema for Vertical up. The semantic contribution of a particle in a VPC corresponds t</context>
</contexts>
<marker>Hampe, 2000</marker>
<rawString>B. Hampe. 2000. Facing up to the meaning of ‘face up to’: A cognitive semantico-pragmatic analysis of an English verb-particle construction. In A. Foolen and F. van der Leek, editors, Constructions in Cognitive Linguistics. Selected Papers from the fifth International Cognitive Linguistics Conference, p. 81–101. John Benjamins Publishing Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Jackendoff</author>
</authors>
<title>English particle constructions, the lexicon, and the autonomy of syntax.</title>
<date>2002</date>
<contexts>
<context position="9600" citStr="Jackendoff, 2002" startWordPosition="1607" endWordPosition="1608">frequency particles. These WCFs give us a very general means for determining semantics, whose performance we can contrast with our linguistic features. 3 Particle Semantics and Sense Classes We give some brief background on cognitive grammar and its relation to particle semantics, and then turn to the semantic analysis of up that we draw on as the basis for the sense classes in our experiments. 3.1 Cognitive Grammar and Schemas Some linguistic studies consider many VPCs to be idiomatic, but do not give a detailed account of the semantic similarities between them (Bolinger, 1971; Fraser, 1976; Jackendoff, 2002). In contrast, work in cognitive linguistics has claimed that many so-called idiomatic expressions draw on the compositional contribution of (at least some of) their components (Lindner, 1981; Morgan, 1997; Hampe, 2000). In cognitive grammar (Langacker, 1987), non-spatial concepts are represented as spatial relations. Key terms from this framework are: Trajector (TR) The object which is conceptually foregrounded. Landmark (LM) The object against which the TR is foregrounded. Schema An abstract conceptualization of an experience. Here we focus on schemas depicting a TR, LM and their relationshi</context>
</contexts>
<marker>Jackendoff, 2002</marker>
<rawString>R. Jackendoff. 2002. English particle constructions, the lexicon, and the autonomy of syntax.</rawString>
</citation>
<citation valid="false">
<editor>In N. Dehe, R. Jackendoff, A. McIntyre, and S. Urban, editors, Verb-Particle Explorations. Mouton de Gruyter.</editor>
<marker></marker>
<rawString>In N. Dehe, R. Jackendoff, A. McIntyre, and S. Urban, editors, Verb-Particle Explorations. Mouton de Gruyter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Joanis</author>
<author>S Stevenson</author>
</authors>
<title>A general feature space for automatic verb classification.</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics (EACL-2003),</booktitle>
<pages>163--170</pages>
<contexts>
<context position="16886" citStr="Joanis and Stevenson (2003)" startWordPosition="2874" endWordPosition="2877">word British National Corpus (BNC, Burnard, 2000). VPCs are identified using a simple heuristic based on part-of-speech tags, similar to one technique used by Baldwin (2005). A use of a verb is considered a VPC if it occurs with a particle (tagged AVP) within a six word window to the right. Over a random sample of 113 VPCs thus extracted, we found 88% to be true VPCs, somewhat below the performance of Baldwin’s (2005) best extraction method, indicating potential room for improvement. The slot and particle features are calculated using a modified version of the ExtractVerb software provided by Joanis and Stevenson (2003), which runs over the BNC pre-processed using Abney’s (1991) Cass chunker. To compute the word co-occurrence features (WCFs), we first determine the relative frequency of all words which occur within a five word window left and right of any of the target expressions in the training data. From this list we eliminate the most frequent 1% of words as a stoplist and then use the next most frequent words as “feature words”. For each “feature word”, we then calculate its relative frequency of occurrence within the same five word window of the target expresSense Class #VPCs in Sense Class Train Verif</context>
</contexts>
<marker>Joanis, Stevenson, 2003</marker>
<rawString>E. Joanis and S. Stevenson. 2003. A general feature space for automatic verb classification. In Proceedings of the Conference of the European Chapter of the Association for Computational Linguistics (EACL-2003), p. 163–170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R W Langacker</author>
</authors>
<title>Foundations of Cognitive Grammar: Theoretical Prerequisites,</title>
<date>1987</date>
<volume>1</volume>
<publisher>Stanford University Press, Stanford.</publisher>
<contexts>
<context position="9859" citStr="Langacker, 1987" startWordPosition="1644" endWordPosition="1645">o particle semantics, and then turn to the semantic analysis of up that we draw on as the basis for the sense classes in our experiments. 3.1 Cognitive Grammar and Schemas Some linguistic studies consider many VPCs to be idiomatic, but do not give a detailed account of the semantic similarities between them (Bolinger, 1971; Fraser, 1976; Jackendoff, 2002). In contrast, work in cognitive linguistics has claimed that many so-called idiomatic expressions draw on the compositional contribution of (at least some of) their components (Lindner, 1981; Morgan, 1997; Hampe, 2000). In cognitive grammar (Langacker, 1987), non-spatial concepts are represented as spatial relations. Key terms from this framework are: Trajector (TR) The object which is conceptually foregrounded. Landmark (LM) The object against which the TR is foregrounded. Schema An abstract conceptualization of an experience. Here we focus on schemas depicting a TR, LM and their relationship in both the initial configuration and the final configuration communicated by some expression. TR TR LM LM Initial Final Figure 1: Schema for Vertical up. The semantic contribution of a particle in a VPC corresponds to a schema. For example, in sentence (3)</context>
<context position="14181" citStr="Langacker, 1987" startWordPosition="2415" endWordPosition="2416">r Our Study Adopting a cognitive linguistic perspective, we assume that all uses of a particle make some compositional contribution of meaning to a VPC. In this work, we classify target VPCs according to which of the above senses of up is contributed to the expression. For example, the expressions jump up and pick up are designated as being in the class Vert-up since up in these VPCs has the vertical sense, while clean up and drink up are designated as being in the class Cmpl-up since up here has the completive sense. The relations among the senses of up can be shown in a “schematic network” (Langacker, 1987). Figure 4 shows a simplification of such a network in which we connect more similar senses with shorter edges. This type of analysis allows us to alter the granularity of our classification in a linguistically motivated fashion by combining closely related senses. Thus we can explore the effect of different sense granularities on classification. 4 Materials and Methods 4.1 Experimental Expressions We created a list of English VPCs using up, based on a list of VPCs made available by McIntyre (2001) and a list of VPCs compiled by two human judges. The judges then filtered this list to include o</context>
</contexts>
<marker>Langacker, 1987</marker>
<rawString>R. W. Langacker. 1987. Foundations of Cognitive Grammar: Theoretical Prerequisites, volume 1. Stanford University Press, Stanford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lapata</author>
<author>C Brew</author>
</authors>
<title>Verb class disambiguation using informative priors.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>1</issue>
<contexts>
<context position="28739" citStr="Lapata and Brew, 2004" startWordPosition="4841" endWordPosition="4845">trick and Fletcher (2005) classify VPC tokens, considering each as compositional, non-compositional or not a VPC. Again, however, it is important to recognize which of the possible meaning components is being contributed. In this vein, Uchiyama et al. (2005) tackle token classification of Japanese compound verbs (similar to VPCs) as aspectual, spatial, or adverbial. In the future, we aim to extend the scope of our work, to determine the meaning of a particle in a VPC token, along the lines of our sense classes here. This will almost certainly require semantic classification of the verb token (Lapata and Brew, 2004), similar to our approach here of using the semantic class of a verb type as indicative of the meaning of a particle type. Particle semantics has clear relations to preposition semantics. Some research has focused on the sense disambiguation of specific prepositions (e.g., Alam, 2004), while other work has classified preposition tokens according to their semantic role (O’Hara and Wiebe, 2003). Moreover, two large lexical resources of preposition senses are currently under construction, The Preposition Project (Litkowski, 2005) and PrepNet (SaintDizier, 2005). These resources were not suitable </context>
</contexts>
<marker>Lapata, Brew, 2004</marker>
<rawString>M. Lapata and C. Brew. 2004. Verb class disambiguation using informative priors. Computational Linguistics, 30(1):45–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic identification of noncompositional phrases.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>317--324</pages>
<contexts>
<context position="1455" citStr="Lin, 1999" startWordPosition="222" endWordPosition="223">med word co-occurrence features in our experiments on unseen test VPCs. 1 Introduction A challenge in learning the semantics of multiword expressions (MWEs) is their varying degrees of compositionality—the contribution of each component word to the overall semantics of the expression. MWEs fall on a range from fully compositional (i.e., each component contributes its meaning, as in frying pan) to noncompositional or idiomatic (as in hit the roof). Because of this variation, researchers have explored automatic methods for learning whether, or the degree to which, an MWE is compositional (e.g., Lin, 1999; Bannard et al., 2003; McCarthy et al., 2003; Fazly et al., 2005). However, such work leaves unaddressed the basic issue of which of the possible meanings of a component word is contributed when the MWE is (at least partly) compositional. Words are notoriously ambiguous, so that even if it can be determined that an MWE is compositional, its meaning is still unknown, since the actual semantic contribution of the components is yet to be determined. We address this problem in the domain of verbparticle constructions (VPCs) in English, a rich source of MWEs. VPCs combine a verb with any of a fini</context>
</contexts>
<marker>Lin, 1999</marker>
<rawString>D. Lin. 1999. Automatic identification of noncompositional phrases. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, p. 317–324.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lindner</author>
</authors>
<title>A lexico-semantic analysis of English verb particle constructions with out and up.</title>
<date>1981</date>
<tech>Ph.D. thesis,</tech>
<institution>University of California,</institution>
<location>San Diego.</location>
<contexts>
<context position="2568" citStr="Lindner (1981)" startWordPosition="414" endWordPosition="415">rticle constructions (VPCs) in English, a rich source of MWEs. VPCs combine a verb with any of a finite set of particles, as in jump up, figure out, or give in. Particles such as up, out, or in, with their literal meaning based in physical spatial relations, show a variety of metaphorical and aspectual meaning extensions, as exemplified here for the particle up: (1a) The sun just came up. [vertical spatial movement] (1b) She walked up to him. [movement toward a goal] (1c) Drink up your juice! [completion] (1d) He curled up into a ball. [reflexive movement] Cognitive linguistic analysis, as in Lindner (1981), can provide the basis for elaborating this type of semantic variation. Given such a sense inventory for a particle, our goal is to automatically determine its meaning when used with a given verb in a VPC. We classify VPCs according to their particle sense, using statistical features that capture the semantic and syntactic properties of verbs and particles. We contrast these with simple word co-occurrence features, which are often used to indicate the semantics of a target word. In our experiments, we focus on VPCs using the particle up because it is highly frequent and has a wide range of me</context>
<context position="9791" citStr="Lindner, 1981" startWordPosition="1635" endWordPosition="1636">ive some brief background on cognitive grammar and its relation to particle semantics, and then turn to the semantic analysis of up that we draw on as the basis for the sense classes in our experiments. 3.1 Cognitive Grammar and Schemas Some linguistic studies consider many VPCs to be idiomatic, but do not give a detailed account of the semantic similarities between them (Bolinger, 1971; Fraser, 1976; Jackendoff, 2002). In contrast, work in cognitive linguistics has claimed that many so-called idiomatic expressions draw on the compositional contribution of (at least some of) their components (Lindner, 1981; Morgan, 1997; Hampe, 2000). In cognitive grammar (Langacker, 1987), non-spatial concepts are represented as spatial relations. Key terms from this framework are: Trajector (TR) The object which is conceptually foregrounded. Landmark (LM) The object against which the TR is foregrounded. Schema An abstract conceptualization of an experience. Here we focus on schemas depicting a TR, LM and their relationship in both the initial configuration and the final configuration communicated by some expression. TR TR LM LM Initial Final Figure 1: Schema for Vertical up. The semantic contribution of a par</context>
</contexts>
<marker>Lindner, 1981</marker>
<rawString>S. Lindner. 1981. A lexico-semantic analysis of English verb particle constructions with out and up. Ph.D. thesis, University of California, San Diego.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K C Litkowski</author>
</authors>
<title>The Preposition Project.</title>
<date>2005</date>
<booktitle>In Proceedings of the Second ACL-SIGSEM Workshop on the Linguistic Dimensions of Prepositions and their Use in Computational Linguistics Formalisms and Applications.</booktitle>
<contexts>
<context position="29271" citStr="Litkowski, 2005" startWordPosition="4927" endWordPosition="4928"> certainly require semantic classification of the verb token (Lapata and Brew, 2004), similar to our approach here of using the semantic class of a verb type as indicative of the meaning of a particle type. Particle semantics has clear relations to preposition semantics. Some research has focused on the sense disambiguation of specific prepositions (e.g., Alam, 2004), while other work has classified preposition tokens according to their semantic role (O’Hara and Wiebe, 2003). Moreover, two large lexical resources of preposition senses are currently under construction, The Preposition Project (Litkowski, 2005) and PrepNet (SaintDizier, 2005). These resources were not suitable as the basis for our sense classes because they do not address the range of metaphorical extensions that a preposition/particle can take on, but future work may enable larger scale studies of the type needed to adequately address VPC semantics. 7 Conclusions While progress has recently been made in techniques for assessing the compositionality of VPCs, work thus far has left unaddressed the problem of determining the particular meaning of the components. We focus here on the semantic contribution of the particle—a part-of-spee</context>
</contexts>
<marker>Litkowski, 2005</marker>
<rawString>K. C. Litkowski. 2005. The Preposition Project. In Proceedings of the Second ACL-SIGSEM Workshop on the Linguistic Dimensions of Prepositions and their Use in Computational Linguistics Formalisms and Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McCarthy</author>
<author>B Keller</author>
<author>J Carroll</author>
</authors>
<title>Detecting a continuum of compositionality in phrasal verbs.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACLSIGLEX Workshop on Multiword Expressions: Analysis, Acquisition and Treatment.</booktitle>
<contexts>
<context position="1500" citStr="McCarthy et al., 2003" startWordPosition="228" endWordPosition="231">n our experiments on unseen test VPCs. 1 Introduction A challenge in learning the semantics of multiword expressions (MWEs) is their varying degrees of compositionality—the contribution of each component word to the overall semantics of the expression. MWEs fall on a range from fully compositional (i.e., each component contributes its meaning, as in frying pan) to noncompositional or idiomatic (as in hit the roof). Because of this variation, researchers have explored automatic methods for learning whether, or the degree to which, an MWE is compositional (e.g., Lin, 1999; Bannard et al., 2003; McCarthy et al., 2003; Fazly et al., 2005). However, such work leaves unaddressed the basic issue of which of the possible meanings of a component word is contributed when the MWE is (at least partly) compositional. Words are notoriously ambiguous, so that even if it can be determined that an MWE is compositional, its meaning is still unknown, since the actual semantic contribution of the components is yet to be determined. We address this problem in the domain of verbparticle constructions (VPCs) in English, a rich source of MWEs. VPCs combine a verb with any of a finite set of particles, as in jump up, figure ou</context>
<context position="3836" citStr="McCarthy et al., 2003" startWordPosition="623" endWordPosition="626">our feature space draws on general properties of VPCs, and is not specific to this particle. A VPC may be ambiguous, with its particle occurring in more than one sense; in contrast to (1a), come up may use up in a goal-oriented sense as in 45 Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, pages 45–53, Sydney, July 2006. c�2006 Association for Computational Linguistics The deadline is coming up. While our long-term goal is token classification (disambiguation) of a VPC in context, following other work on VPCs (e.g., Bannard et al., 2003; McCarthy et al., 2003), we begin here with the task of type classification. Given our use of features which capture the statistical behaviour relevant to a VPC across a corpus, we assume that the outcome of type classification yields the predominant sense of the particle in the VPC. Predominant sense identification is a useful component of sense disambiguation of word tokens (McCarthy et al., 2004), and we presume our VPC type classification work will form the basis for later token disambiguation. Section 2 continues the paper with a discussion of the features we developed for particle sense classification. Section</context>
<context position="27253" citStr="McCarthy et al. (2003)" startWordPosition="4601" endWordPosition="4604">rst attempt at addressing this problem, but more work clearly remains. There is a relatively high variability in performance across the verification and test sets, indicating that we need a larger number of experimental expressions to be able to draw firmer conclusions. Even if our current results extend to larger datasets, we intend to explore other feature approaches, such as word co-occurrence features for specific syntactic slots as suggested above, in order to improve the performance. 51 6 Related Work The semantic compositionality of VPC types has recently received increasing attention. McCarthy et al. (2003) use several measures to automatically rate the overall compositionality of a VPC. Bannard (2005), extending work by Bannard et al. (2003), instead considers the extent to which the verb and particle each contribute semantically to the VPC. In contrast, our work assumes that the particle of every VPC contributes compositionally to its meaning. We draw on cognitive linguistic analysis that posits a rich set of literal and metaphorical meaning possibilities of a particle, which has been previously overlooked in computational work on VPCs. In this first investigation of particle meaning in VPCs, </context>
</contexts>
<marker>McCarthy, Keller, Carroll, 2003</marker>
<rawString>D. McCarthy, B. Keller, and J. Carroll. 2003. Detecting a continuum of compositionality in phrasal verbs. In Proceedings of the ACLSIGLEX Workshop on Multiword Expressions: Analysis, Acquisition and Treatment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McCarthy</author>
<author>R Koeling</author>
<author>J Weeds</author>
<author>J Carroll</author>
</authors>
<title>Finding predominant word senses in untagged text.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>280--287</pages>
<contexts>
<context position="4215" citStr="McCarthy et al., 2004" startWordPosition="686" endWordPosition="689">c�2006 Association for Computational Linguistics The deadline is coming up. While our long-term goal is token classification (disambiguation) of a VPC in context, following other work on VPCs (e.g., Bannard et al., 2003; McCarthy et al., 2003), we begin here with the task of type classification. Given our use of features which capture the statistical behaviour relevant to a VPC across a corpus, we assume that the outcome of type classification yields the predominant sense of the particle in the VPC. Predominant sense identification is a useful component of sense disambiguation of word tokens (McCarthy et al., 2004), and we presume our VPC type classification work will form the basis for later token disambiguation. Section 2 continues the paper with a discussion of the features we developed for particle sense classification. Section 3 first presents some brief cognitive linguistic background, followed by the sense classes of up used in our experiments. Sections 4 and 5 discuss our experimental set-up and results, Section 6 related work, and Section 7 our conclusions. 2 Features Used in Classification The following subsections describe the two sets of features we investigated. The linguistic features are </context>
</contexts>
<marker>McCarthy, Koeling, Weeds, Carroll, 2004</marker>
<rawString>D. McCarthy, R. Koeling, J. Weeds, and J. Carroll. 2004. Finding predominant word senses in untagged text. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, p. 280–287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McIntyre</author>
</authors>
<title>The particle verb list.</title>
<date>2001</date>
<note>http://www.uni-leipzig.de/ ˜angling/mcintyre/pv.list.pdf.</note>
<contexts>
<context position="14684" citStr="McIntyre (2001)" startWordPosition="2497" endWordPosition="2498">e completive sense. The relations among the senses of up can be shown in a “schematic network” (Langacker, 1987). Figure 4 shows a simplification of such a network in which we connect more similar senses with shorter edges. This type of analysis allows us to alter the granularity of our classification in a linguistically motivated fashion by combining closely related senses. Thus we can explore the effect of different sense granularities on classification. 4 Materials and Methods 4.1 Experimental Expressions We created a list of English VPCs using up, based on a list of VPCs made available by McIntyre (2001) and a list of VPCs compiled by two human judges. The judges then filtered this list to include only VPCs which they both agreed were valid, resulting in a final list of 389 VPCs. From this list, training, verification and test sets of sixty VPCs each are randomly selected. Note that the expense of manually annotating the data (as described below) prevents us from using larger datasets in this initial investigation. The experimental sets are 48 chosen such that each includes the same proportion of verbs across three frequency bands, so that the sets do not differ in frequency distribution of t</context>
</contexts>
<marker>McIntyre, 2001</marker>
<rawString>A. McIntyre. 2001. The particle verb list. http://www.uni-leipzig.de/ ˜angling/mcintyre/pv.list.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P S Morgan</author>
</authors>
<title>Figuring out figure out: Metaphor and the semantics of the English verb-particle construction.</title>
<date>1997</date>
<journal>Cognitive Linguistics,</journal>
<volume>8</volume>
<issue>4</issue>
<contexts>
<context position="9805" citStr="Morgan, 1997" startWordPosition="1637" endWordPosition="1638">background on cognitive grammar and its relation to particle semantics, and then turn to the semantic analysis of up that we draw on as the basis for the sense classes in our experiments. 3.1 Cognitive Grammar and Schemas Some linguistic studies consider many VPCs to be idiomatic, but do not give a detailed account of the semantic similarities between them (Bolinger, 1971; Fraser, 1976; Jackendoff, 2002). In contrast, work in cognitive linguistics has claimed that many so-called idiomatic expressions draw on the compositional contribution of (at least some of) their components (Lindner, 1981; Morgan, 1997; Hampe, 2000). In cognitive grammar (Langacker, 1987), non-spatial concepts are represented as spatial relations. Key terms from this framework are: Trajector (TR) The object which is conceptually foregrounded. Landmark (LM) The object against which the TR is foregrounded. Schema An abstract conceptualization of an experience. Here we focus on schemas depicting a TR, LM and their relationship in both the initial configuration and the final configuration communicated by some expression. TR TR LM LM Initial Final Figure 1: Schema for Vertical up. The semantic contribution of a particle in a VPC</context>
</contexts>
<marker>Morgan, 1997</marker>
<rawString>P. S. Morgan. 1997. Figuring out figure out: Metaphor and the semantics of the English verb-particle construction. Cognitive Linguistics, 8(4):327–357.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T O’Hara</author>
<author>J Wiebe</author>
</authors>
<title>Preposition semantic classification via Penn Treebank and FrameNet.</title>
<date>2003</date>
<booktitle>In Proceedings of CoNLL-2003,</booktitle>
<pages>79--86</pages>
<marker>O’Hara, Wiebe, 2003</marker>
<rawString>T. O’Hara and J. Wiebe. 2003. Preposition semantic classification via Penn Treebank and FrameNet. In Proceedings of CoNLL-2003, p. 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Patrick</author>
<author>J Fletcher</author>
</authors>
<title>Classifying verbparticle constructions by verb arguments.</title>
<date>2005</date>
<booktitle>In Proceedings of the Second ACL-SIGSEM Workshop on the Linguistic Dimensions of Prepositions and their use in Computational Linguistics Formalisms and Applications,</booktitle>
<pages>200--209</pages>
<contexts>
<context position="28142" citStr="Patrick and Fletcher (2005)" startWordPosition="4743" endWordPosition="4746"> assumes that the particle of every VPC contributes compositionally to its meaning. We draw on cognitive linguistic analysis that posits a rich set of literal and metaphorical meaning possibilities of a particle, which has been previously overlooked in computational work on VPCs. In this first investigation of particle meaning in VPCs, we choose to focus on type-based classification, partly due to the significant extra expense of manually annotating sufficient numbers of tokens in text. As noted earlier, though, VPCs can take on different meanings, indicating a shortcoming of type-based work. Patrick and Fletcher (2005) classify VPC tokens, considering each as compositional, non-compositional or not a VPC. Again, however, it is important to recognize which of the possible meaning components is being contributed. In this vein, Uchiyama et al. (2005) tackle token classification of Japanese compound verbs (similar to VPCs) as aspectual, spatial, or adverbial. In the future, we aim to extend the scope of our work, to determine the meaning of a particle in a VPC token, along the lines of our sense classes here. This will almost certainly require semantic classification of the verb token (Lapata and Brew, 2004), s</context>
</contexts>
<marker>Patrick, Fletcher, 2005</marker>
<rawString>J. Patrick and J. Fletcher. 2005. Classifying verbparticle constructions by verb arguments. In Proceedings of the Second ACL-SIGSEM Workshop on the Linguistic Dimensions of Prepositions and their use in Computational Linguistics Formalisms and Applications, p. 200–209.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Saint-Dizier</author>
</authors>
<title>PrepNet: a framework for describing prepositions: Preliminary investigation results.</title>
<date>2005</date>
<booktitle>In Proceedings of the Sixth International Workshop on Computational Semantics (IWCS’05),</booktitle>
<pages>145--157</pages>
<marker>Saint-Dizier, 2005</marker>
<rawString>P. Saint-Dizier. 2005. PrepNet: a framework for describing prepositions: Preliminary investigation results. In Proceedings of the Sixth International Workshop on Computational Semantics (IWCS’05), p. 145–157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Schulte im Walde</author>
</authors>
<title>Exploring features to identify semantic nearest neighbours: A case study on German particle verbs.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Conference on Recent Advances in Natural Language Processing.</booktitle>
<contexts>
<context position="25847" citStr="Walde (2005)" startWordPosition="4382" endWordPosition="4383">l the linguistic features. The linguistically uninformed WCFs perform worse on their own, and do not consistently help (and in some cases hurt) the performance of the linguistic features when combined with them. We conclude then that linguistically based features are motivated for this task. Note that the features are still quite simple, and straightforward to extract from a corpus—i.e., linguistically informed does not mean expensive (although the slot features do require access to chunked text). Interestingly, in determining the semantic nearest neighbor of German particle verbs, Schulte im Walde (2005) found that WCFs that are restricted to the arguments of the verb outperform simple window-based co-occurrence features. Although her task is quite different from ours, similarly restricting our WCFs may enable them to encode more linguistically-relevant information. The accuracies we achieve with the linguistic features correspond to a 30–31% reduction in error rate over the chance baseline for the 3-way task, and an 18–26% reduction in error rate for the 2-way task. Although we expected that the 2-way task may be easier, since it requires less fine-grained distinctions, it is clear that comb</context>
</contexts>
<marker>Walde, 2005</marker>
<rawString>S. Schulte im Walde. 2005. Exploring features to identify semantic nearest neighbours: A case study on German particle verbs. In Proceedings of the International Conference on Recent Advances in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Uchiyama</author>
<author>T Baldwin</author>
<author>S Ishizaki</author>
</authors>
<title>Disambiguating Japanese compound verbs.</title>
<date>2005</date>
<journal>Computer Speech and Language, Special Issue on Multiword Expressions,</journal>
<volume>19</volume>
<issue>4</issue>
<pages>512</pages>
<contexts>
<context position="28375" citStr="Uchiyama et al. (2005)" startWordPosition="4779" endWordPosition="4782"> overlooked in computational work on VPCs. In this first investigation of particle meaning in VPCs, we choose to focus on type-based classification, partly due to the significant extra expense of manually annotating sufficient numbers of tokens in text. As noted earlier, though, VPCs can take on different meanings, indicating a shortcoming of type-based work. Patrick and Fletcher (2005) classify VPC tokens, considering each as compositional, non-compositional or not a VPC. Again, however, it is important to recognize which of the possible meaning components is being contributed. In this vein, Uchiyama et al. (2005) tackle token classification of Japanese compound verbs (similar to VPCs) as aspectual, spatial, or adverbial. In the future, we aim to extend the scope of our work, to determine the meaning of a particle in a VPC token, along the lines of our sense classes here. This will almost certainly require semantic classification of the verb token (Lapata and Brew, 2004), similar to our approach here of using the semantic class of a verb type as indicative of the meaning of a particle type. Particle semantics has clear relations to preposition semantics. Some research has focused on the sense disambigu</context>
</contexts>
<marker>Uchiyama, Baldwin, Ishizaki, 2005</marker>
<rawString>K. Uchiyama, T. Baldwin, and S. Ishizaki. 2005. Disambiguating Japanese compound verbs. Computer Speech and Language, Special Issue on Multiword Expressions, 19(4):497– 512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Villavicencio</author>
</authors>
<title>The availability of verbparticle constructions in lexical resources: How much is enough?</title>
<date>2005</date>
<journal>Computer Speech and Language, Special Issue on Multiword Expressions,</journal>
<volume>19</volume>
<issue>4</issue>
<contexts>
<context position="5751" citStr="Villavicencio (2005)" startWordPosition="937" endWordPosition="938">verb. That is, the particle contributes the same meaning when combining with any of a semantic class of verbs.&apos; For example, the VPCs drink up, eat up and gobble up all draw on the completion sense of up; the VPCs puff out, spread out and stretch out all draw on the extension sense of out. The prevalence of these patterns suggests that features which have been shown to be effective for the semantic classification of verbs may be useful for our task. We adopt simple syntactic “slot” features which have been successfully used in automatic semantic classification of verbs (Joanis and Stevenson, &apos;Villavicencio (2005) observes that verbs from a semantic class will form VPCs with similar sets of particles. Here we are hypothesizing further that VPCs formed from verbs of a semantic class draw on the same meaning of the given particle. 2003). The features are motivated by the fact that semantic properties of a verb are reflected in the syntactic expression of the participants in the event the verb describes. The slot features encode the relative frequencies of the syntactic slots—subject, direct and indirect object, object of a preposition—that the arguments and adjuncts of a verb appear in. We calculate the </context>
</contexts>
<marker>Villavicencio, 2005</marker>
<rawString>A. Villavicencio. 2005. The availability of verbparticle constructions in lexical resources: How much is enough? Computer Speech and Language, Special Issue on Multiword Expressions, 19(4):415–432.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Wurmbrand</author>
</authors>
<title>The structure(s) of particle verbs. Master’s thesis,</title>
<date>2000</date>
<institution>McGill University.</institution>
<contexts>
<context position="6776" citStr="Wurmbrand (2000)" startWordPosition="1115" endWordPosition="1116">ures encode the relative frequencies of the syntactic slots—subject, direct and indirect object, object of a preposition—that the arguments and adjuncts of a verb appear in. We calculate the slot features over three contexts: all uses of a verb; all uses of the verb in a VPC with the target particle (up in our experiments); all uses of the verb in a VPC with any of a set of high frequency particles (to capture its semantics when used in VPCs in general). 2.1.2 Particle Features Two types of features are motivated by properties specific to the semantics and syntax of particles and VPCs. First, Wurmbrand (2000) notes that compositional particle verbs in German (a somewhat related phenomenon to English VPCs) allow the replacement of their particle with semantically similar particles. We extend this idea, hypothesizing that when a verb combines with a particle such as up in a particular sense, the pattern of usage of that verb in VPCs using all other particles may be indicative of the sense of the target particle (in this case up) when combined with that verb. To reflect this observation, we count the relative frequency of any occurrence of the verb used in a VPC with each of a set of high frequency p</context>
</contexts>
<marker>Wurmbrand, 2000</marker>
<rawString>S. Wurmbrand. 2000. The structure(s) of particle verbs. Master’s thesis, McGill University.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>