<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000815">
<title confidence="0.9991655">
Parser Evaluation Using Derivation Trees:
A Complement to evalb
</title>
<author confidence="0.996848">
Seth Kulick and Ann Bies and Justin Mott
</author>
<affiliation confidence="0.99296">
Linguistic Data Consortium, University of Pennsylvania, Philadelphia, PA 19104
</affiliation>
<email confidence="0.996845">
{skulick,bies,jmott}@ldc.upenn.edu
</email>
<author confidence="0.995023">
Anthony Kroch and Mark Liberman and Beatrice Santorini
</author>
<affiliation confidence="0.999818">
Department of Linguistics, University of Pennsylvania, Philadelphia, PA 19104
</affiliation>
<email confidence="0.999385">
{kroch,myl,beatrice}@ling.upenn.edu
</email>
<sectionHeader confidence="0.997397" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999967">
This paper introduces a new technique for
phrase-structure parser analysis, catego-
rizing possible treebank structures by inte-
grating regular expressions into derivation
trees. We analyze the performance of the
Berkeley parser on OntoNotes WSJ and
the English Web Treebank. This provides
some insight into the evalb scores, and
the problem of domain adaptation with the
web data. We also analyze a “test-on-
train” dataset, showing a wide variance in
how the parser is generalizing from differ-
ent structures in the training material.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999979357142857">
Phrase-structure parsing is usually evaluated using
evalb (Sekine and Collins, 2008), which provides
a score based on matching brackets. While this
metric serves a valuable purpose in pushing parser
research forward, it has limited utility for under-
standing what sorts of errors a parser is making.
This is the case even if the score is broken down
by brackets (NP, VP, etc.), because the brackets
can represent different types of structures. We
would also like to have answers to such questions
as “How does the parser do on non-recursive NPs,
separate from NPs resulting from modification?
On PP attachment?” etc.
Answering such questions is the goal of this
work, which combines two strands of research.
First, inspired by the tradition of Tree Adjoin-
ing Grammar-based research (Joshi and Schabes,
1997; Bangalore and Joshi, 2010), we use a de-
composition of the full trees into “elementary
trees” (henceforth “etrees”), with a derivation tree
that records how the etrees relate to each other,
as in Kulick et al. (2011). In particular, we use
the “spinal” structure approach of (Shen et al.,
2008; Shen and Joshi, 2008), where etrees are con-
strained to be unary-branching.
Second, we use a set of regular expressions
(henceforth “regexes”) that categorize the possible
structures in the treebank. These are best thought
of as an extension of head-finding rules, which not
only find a head but simultaneously identify each
parent/children relation as one of a limited number
of types of structures (right-modification, etc.).
The crucial step is that we integrate these
regexes into the spinal etrees. The derivation trees
provide elements of a dependency analysis, which
allow us to calculate scores for head identification
and attachment for different projections (e.g., PP).
The regexes allow us to also provide scores based
on spans of different construction types. Together
these two aspects break down the evalb brackets
into more meaningful categories, and the simulta-
neous head and span scoring allows us to separate
these aspects in the analysis.
After describing in more detail the basic frame-
work, we show some aspects of the resulting anal-
ysis of the performance of the Berkeley parser
(Petrov et al., 2008) on three datasets: (a)
OntoNotes WSJ sections 2-21 (Weischedel et al.,
2011)1, (b) OntoNotes WSJ section 22, and (c)
the “Answers” section of the English Web Tree-
bank (Bies et al., 2012). We trained the parser on
sections 2-21, and so (a) is “test-on-train”. These
three results together show how the parser is gen-
eralizing from the training data, and what aspects
of the “domain adaptation” problem to the web
material are particularly important.2
</bodyText>
<sectionHeader confidence="0.9951305" genericHeader="method">
2 Framework for analyzing parsing
performance
</sectionHeader>
<bodyText confidence="0.999926">
We first describe the use of the regexes in tree de-
composition, and then give some examples of in-
</bodyText>
<footnote confidence="0.9997315">
1We refer only to the WSJ treebank portion of OntoNotes,
which is roughly a subset of the Penn Treebank (Marcus et
al., 1999) with annotation revisions including the addition of
NML nodes.
2We parse (c) while training on (a) to follow the procedure
in Petrov and McDonald (2012)
</footnote>
<page confidence="0.924448">
668
</page>
<bodyText confidence="0.846033666666667">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 668–673,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
corporating these regexes into the derivation trees.
</bodyText>
<subsectionHeader confidence="0.993806">
2.1 Use of regular expressions
</subsectionHeader>
<bodyText confidence="0.999929307692308">
Decomposing the original phrase-structure tree
into the smaller components requires some
method of determining the “head” of a nonter-
minal, from among its children nodes, similar to
parsing work such as Collins (1999). As described
above, we are also interested in the type of lin-
guistic construction represented by that one-level
structure, each of which instantiates one of a few
types - recursive coordination, simple head-and-
sister, etc. We address both tasks together with the
regexes. In contrast to the sort of head rules in
(Collins, 1999), these refer as little as possible to
specific POS tags. Instead of explicitly listing the
POS tags of possible heads, the heads are in most
cases determined by their location in the structure.
Sample regexes are shown in Figure 1. There
are 49 regexes used.3 Regexes ADJP-t and
ADVP-t in (a) identify their terminal head to
be the rightmost terminal, possibly preceded by
some number of terminals or nonterminals, rely-
ing on a mapping that maps all terminals (except
CC, which is mapped to CONJ) to TAG and all
nonterminals (except CONJP and NML) to NT.
Structures with a CONJ/CONJP/NML child do
not match this rule and are handled by different
regexes, which are all mutually exclusive. In some
cases, we need to search for particular nonterminal
heads, such as with the (b) regexes S-vp and SQ-
vp, which identify the rightmost VP among the
children of a S or SQ as the head. (c) NP-modr
is a regex for a recursive NP with a right modifier.
In this case, the NP on the left is identified as the
head. (d) VP-crd is also a regex for a recursive
structure, in this case for VP coordination, pick-
ing out the leftmost conjunct as the head of the
structure. The regex names roughly describe their
purpose - “mod” for right-modification, “crd” for
coordination, etc. The suffix “-t” is for the simple
non-recursive case in which the head is a terminal.
</bodyText>
<subsectionHeader confidence="0.993593">
2.2 Regexes in the derivation trees
</subsectionHeader>
<bodyText confidence="0.826313375">
The names of these regexes are incorporated into
the etrees themselves, as labels of the nontermi-
nals. This allows an etree to contain information
3Some among the 49 are duplicates, used for different
nonterminals, as with (a) and (b) in Figure 1. We derived
the regexes via an iterative process of inspection of tree de-
composition on dataset (a), together with taking advantage of
the treebanking experience from some of the co-authors.
</bodyText>
<equation confidence="0.542871333333333">
(a)ADJP-t,ADVP-t:
&amp;quot;(TAG|NT|NML)*(head:TAG) (NT)*$
(b)S-vp, SQ-vp: &amp;quot;([&amp;quot; ]+)*(head:VP)$
(c)NP-modr:
&amp;quot;(head:NP)(SBAR|S|VP|ADJP|PP|ADVP|NP)+$
(d)VP-crd: &amp;quot;(head:VP) (VP)* CONJ VP$
</equation>
<figureCaption confidence="0.998881">
Figure 1: Some sample regexes
</figureCaption>
<bodyText confidence="0.999684333333333">
such as “this node represents right modification”.
For example, Figure 2 shows the derivation tree
resulting from the decomposition of the tree in
Figure 4. Each structure within a circle is one
etree, and the derivation as a whole indicates how
these etrees are combined. Here we indicate with
arrows that point to the relevant regex. For ex-
ample, the PP-t etree #a6 points to the NP-modr
regex, which consists of the NP-t together with
the PP-t. The nonterminals of the spinal etrees are
the names of the regexes, with the simpler non-
terminal labels trivially derivable from the regex
names.4
The tree in Figure 5 is the parser output corre-
sponding to the gold tree in Figure 4, and in this
case gets the PP-t attachment wrong, while every-
thing else is the same as the gold.5 This is reflected
in the derivation tree in Figure 3, in which the NP-
modr regex is absent, with the NP-t and PP-t etrees
#b5 and #b6 both pointing to the VP-t regex in
#b3. We show in Section 2.3 how this derivation
tree representation is used to score this attachment
error directly, rather than obscuring it as an NP
bracket error as evalb would do.
</bodyText>
<subsectionHeader confidence="0.998633">
2.3 Scoring
</subsectionHeader>
<bodyText confidence="0.999981583333333">
We decompose both the gold and parser output
trees into derivation trees with spinal etrees, and
score based on the regexes projected by each word.
There is a match for a regex if the corresponding
words in gold/parser files project to that regex, a
precision error if the parser file does but the gold
does not, and a recall error if the gold does but the
parser file does not.
For example, comparing the trees in Figures 4
and 5 via their derivation trees in Figures 2 and
Figures 3, the word “trip” has a match for the regex
NP-t, but a recall error for NP-modr. The word
</bodyText>
<footnote confidence="0.617027166666667">
4We do not have space here to discuss the data structure
in complete detail, but multiple regex names at a node, such a
VP-aux and VP-t at tree a3 in Figure 2, indicate multiple VP
nonterminals.
5We leave function tags aside for future work. The gold
tree is shown without the SBJ function tag.
</footnote>
<page confidence="0.995465">
669
</page>
<figureCaption confidence="0.9999935">
Figure 2: Derivation Tree for Figure 4
Figure 3: Derivation Tree for Figure 5)
</figureCaption>
<figure confidence="0.999857666666666">
#a3
S-vp
#a1
#a2
VP-aux
VP-t
#a5
NP-t
will
#a6
They
make
NP-modr
NP-t
#a7
NP-t
#a4
PP-t
the
trip
to
Florida
#b3
S-vp
#b1
NP-t
#b2
VP-aux
VP-t
#b6
They
will
make
#b5
NP-t
PP-t
P
#b7
NP-t
#b4
the
trip
tri
to
Florida
NP
VP
They
NP
the trip
PP
to NP
Florida
make NP
the trip
PP
to NP
Florida
S
VP
will VP
make NP
NP
They
S
will VP
</figure>
<figureCaption confidence="0.999723">
Figure 4: Gold tree Figure 5: Parser output tree
</figureCaption>
<table confidence="0.999150666666667">
Corpus tokens brackets coverage % evalb
S
2-21 g 650877 578597 571243 98.7
p 575744 569480 98.9 93.8
N VP
22 g 32092 24819 24532 98.8
Thp wl 24801 24528 98.9 90.1
Ans g 53960 48492 47348 97.6
p 48750 47423 97.3 80.8
</table>
<tableCaption confidence="0.999739">
Table 1: Corpus information for gold(g) and
</tableCaption>
<sectionHeader confidence="0.226926" genericHeader="method">
NP
</sectionHeader>
<bodyText confidence="0.985875277777778">
parsed(p) sections of each corpus
“make” has a match for the regexes VP-t, VP-
aux, and S-vp, and so on. Summing such scores
over the corresponding gold/parser trees gives us
F-scores for each regex.
There are two modifications/extensions to these
F-scores that we also use:
(1) For each regex match, we score whether it
matches based on the span as well. For exam-
ple, “make” is a match for VP-t in Figures 2
and 3, and is also a match for the span as well,
since in both derivation trees it includes the words
“make...Florida”. It is this matching for span as
well as head that allows us to compare our results
to evalb. We call the match just for the head the “F-
h” score and the match that also includes the span
information the “F-s” score. The F-s score roughly
corresponds to the evalb score. However, the F-
</bodyText>
<figureCaption confidence="0.684625666666667">
s score is for separate syntactic constructions (in-
cluding also head identification), although we can
also sum it over all the structures, as done later in
Figure 6. The simultaneous F-h and F-s scores let
us identify constructions where the parser has the
head projection correct, but gets the span wrong.
</figureCaption>
<equation confidence="0.484385">
1
</equation>
<bodyText confidence="0.996419333333333">
(2) Since the derivation tree is really a depen-
dency tree with more complex nodes (Rambow
and Joshi, 1997; Kulick et al., 2012), we can also
score each regex for attachment.6 For example,
while “to” is a match for PP-t, its attachment is
not, since in Figure 2 it is a child of the “trip” etree
(#a5) and in Figure 3 it is a child of the “make”
etree (#b3). Therefore our analysis results in an
attachment score for every regex.
</bodyText>
<subsectionHeader confidence="0.999254">
2.4 Comparison with previous work
</subsectionHeader>
<bodyText confidence="0.999905666666667">
This work is in the same basic line of research
as the inter-annotator agreement analysis work in
Kulick et al. (2013). However, that work did
not utilize regexes, and focused on comparing se-
quences of identical strings. The current work
scores on general categories of structures, without
</bodyText>
<footnote confidence="0.9989908">
6A regex intermediate in a etree, such as VP-t above, is
considered to have a default null attachment. Also, the at-
tachment score is not relevant for regexes that already express
a recursive structure, such as NP-modr. In Figure 2, NP-t in
etree #a5 is considered as having the attachment to #a3.
</footnote>
<page confidence="0.991393">
670
</page>
<table confidence="0.999797888888889">
Sections 2-21 (Ontonotes) Section 22 (Ontonotes) Answers (English Web Treebank)
regex %gold F-h F-s att spanR %gold F-h F-s att spanR %gold F-h F-s att spanR
NP-t 30.7 98.9 97.6 96.5 99.6 31.1 98.0 95.8 94.4 99.6 28.5 95.4 91.5 90.9 99.3
VP-t 13.5 98.8 94.5 98.4 95.8 13.4 98.1 91.7 97.3 93.7 16.0 96.7 81.7 96.1 85.4
PP-t 12.2 99.2 91.0 90.5 92.0 12.1 98.7 86.4 86.1 88.2 8.4 96.4 80.5 80.7 84.7
S-vp 12.2 97.9 92.8 96.8 96.3 11.9 96.5 89.1 95.4 95.0 14.2 94.1 72.9 88.0 84.1
NP-modr 8.6 88.4 80.3 - 91.5 8.5 82.9 71.8 - 87.9 4.4 69.0 54.2 - 80.5
VP-aux 5.5 97.9 94.0 - 96.1 5.0 96.5 91.0 - 94.6 6.2 94.4 81.7 - 86.7
SBAR-s 3.7 96.1 91.1 91.8 95.3 3.5 94.3 87.2 86.4 93.5 4.0 84.8 68.2 81.9 81.9
ADVP-t 2.7 95.2 93.3 93.9 98.6 3.0 89.6 84.5 88.0 95.9 4.5 84.0 78.2 80.3 96.8
NML-t 2.3 91.6 90.3 97.6 99.8 2.6 85.6 82.2 93.5 99.8 0.7 42.1 37.7 88.8 100.0
ADJP-t 1.9 94.6 88.4 95.5 94.6 1.8 86.8 77.0 93.6 90.7 2.5 84.7 67.0 88.1 84.2
QP-t 1.0 95.3 93.8 98.3 99.6 1.2 91.0 89.0 97.1 100.0 0.2 57.7 57.7 94.4 100.0
NP-crd 0.8 80.3 73.7 - 92.4 0.6 68.6 58.4 - 86.1 0.5 55.3 47.8 - 88.1
VP-crd 0.4 84.3 82.8 - 98.2 0.4 75.3 73.5 - 97.6 0.8 65.5 58.3 - 89.8
S-crd 0.3 83.7 83.2 - 99.6 0.4 70.9 68.6 - 96.7 0.8 68.5 63.0 - 93.4
SQ-v 0.1 88.3 82.0 93.3 97.8 0.1 66.7 66.7 88.9 100.0 0.9 81.9 72.4 93.4 95.8
FRAG-nt 0.1 49.9 48.6 95.4 97.9 0.1 28.6 28.6 100.0 100.0 0.8 22.7 21.3 96.3 96.3
</table>
<tableCaption confidence="0.93672">
Table 2: Scores for the most frequent categories of brackets in the three datasets of corpora, as determined
</tableCaption>
<bodyText confidence="0.60619525">
by the regexes. % gold is the frequency of this regex type compared to all the brackets in the gold. F-h
is the score based on matching heads, F-s also incorporates the span information, att is the attachment
accuracy for words that match in F-h, and spanR is the span-right accuracy for words that match in F-h.
the reliance on sequences of individual strings.7
</bodyText>
<sectionHeader confidence="0.824438" genericHeader="method">
3 Analysis of parsing results
</sectionHeader>
<bodyText confidence="0.999880125">
We worked with the three datasets as described
in the introduction. We trained the parser on sec-
tions 2-21 of OntoNotes WSJ, and parsed the three
datasets with the gold tags, since at present we
wish to analyze the parser performance in isola-
tion from Part-of-Speech tagging errors. Table 1
shows the sizes of the three corpora in terms of
tokens and brackets, for both the gold and parsed
versions, with the evalb scores for the parsed ver-
sions. The score is lower for Answers, as also
found by Petrov and McDonald (2012).
To facilitate comparison of our analysis with
evalb, we used corpora versions with the same
bracket deletion (empty yields and most punctua-
tion) as evalb. We ran the gold and parsed versions
through our regex decomposition and derivation
tree creation. Table 1 shows the number and per-
centage of brackets handled by our regexes. The
high coverage (%) reinforces the point that there is
a limited number of core structures in the treebank.
In the results below in Table 2 and Figure 6 we
combine the nonterminals that are not covered by
one of the regexes with the simple non-recursive
regex case for that nonterminal.8
</bodyText>
<footnote confidence="0.9978906">
7In future work we will compare our approach to that
of Kummerfeld et al. (2012), who also move beyond evalb
scores in an effort to provide more meaningful error analysis.
8We also combine a few other non-recursive regexes to-
gether with NP-t, such as the special one for possessives.
</footnote>
<bodyText confidence="0.9999927">
We present the results in two ways. Table 2 lists
the most frequent categories in the three datasets,
with their percentage of the overall number of
brackets (%gold), their score based just on the
head identification (F-h), their score based on head
identification and (left and right) span (F-s), and
the attachment (att) and span-right (spanR) scores
for those that match based on the head.9
The two graphs in Figure 6 show the cumu-
lative results based on F-h and F-s, respectively.
These show the cumulative score in order of the
frequency of categories. For example, for sections
2-21, the score for NP-t is shown first, with 30.7%
of the brackets, and then together with the VP-t
category, they cover 45.2% of the brackets, etc.10
The benefit of the approach described here is that
now we can see the contribution to the evalb score
of the particular types of constructions, and within
those constructions, how well the parser is doing
at getting the same head projection, but failing or
</bodyText>
<footnote confidence="0.9527418">
9The score for the left edge is almost always very high for
every category, and we just list here the right edge score. The
attachment score does not apply to the recursive categories,
as mentioned above.
10The final F-s value is lower than the evalb score - e.g.
92.5 for sections 2-21 (the rightmost point in the graph for
sections 2-21 in the F-s graph in Figure 6) compared to the
93.8 evalb score. Space prevents full explanation, but there
are two reasons for this. One is that there are cases in which
bracket spans match, but the head, as found by our regexes, is
different in the gold and parser trees. The other cases is when
brackets match, and may even have the same head, but their
regex is different. In future work we will provide a full ac-
counting of such cases, but they do not affect the main aspects
of the analysis.
</footnote>
<page confidence="0.99587">
671
</page>
<figure confidence="0.998061741573033">
F-scores by head identification
cumulative % of all brackets
F-scores by head identification and span
0 5 10 20 30 40 50 60 70 80 90 100
cumulative % of all backets
2-211 2
3 4
3
22 1 2 4
2
4 3 6
5 7
911
1
3
6
101
7
5
6
9
11
1
answers
13
10
1412
412
15
15
5
113 12
14
15 9
11
1:NP-t 2:VP-t
3:PP-t 4:S-vp
5:NP-modr 6:VP-aux
7:SHAR-s 8:ADVP-t
9:NML-t 10:ADJP-t
11:QP-t 12:SQ-vp
13:S-crd 14:VP-crd
15:FRAG-nt
0 5 10 20 30 40 50 60 70 80 90 100
89.2 91.2 93.2 95 97 99.0
78.0 82.0 86.0 90.2 .0 94.2
5 7
9
1
2-21
2
1
22
2
3 4
3
4
6
101
1 1312
41
5
answers 1
2
5
6
7
9 11 13
1
101
� 1
2
5
1:NP-t 2:VP-t
3:PP-t 4:S-vp
5:NP-modr 6:VP-aux
7:SBAR-s 8:ADVP-t
9:NML-t 10:ADJP-t
11:QP-t 12:SQ-vp
13:S-crd 14:VP-crd
15:FRAG-nt
4 3 6
e
5
10 13 12 14
7
15 9
11
.0 97.6
</figure>
<figureCaption confidence="0.873086">
Figure 6: Cumulative scores based on F-h (left) and F-s (right). These graphs are both cumulative in
exactly the same way, in that each point represents the total percentage of brackets accounted for so far.
So for the 2-21 line, point 1, meaning the NP non-recursive regex, accounts for 30.7% of the brackets,
point 2, meaning the VP non-recursive regex, accounts for another 13.5%, so 44.2% cumulatively, etc.
</figureCaption>
<bodyText confidence="0.941046">
not on the spans.
</bodyText>
<subsectionHeader confidence="0.998796">
3.1 Analysis and future work
</subsectionHeader>
<bodyText confidence="0.993108791666667">
As this is work-in-progress, the analysis is not yet
complete. We highlight a few points here.
(1) The high performance on the OntoNotes WSJ
material is in large part due to the score on the
non-recursive regexes of NP-t, VP-t, S-vp, and the
auxiliaries (points 1, 2, 4, 6 in the graphs). Critical
to this is the fact that the parser does well on deter-
mining the right edge of verbal structures, which
affects the F-s score for VP-t (non-recursive), VP-
aux, and S-vp. The spanR score for VP-t is 95.8
for Sections 2-21 and 93.7 for Section 22.
(2) We wouldn’t expect the test-on-training evalb
score to be 100%, since it has to back off from
the training data, but the results for the different
categories vary widely, with e.g., the NP-modr F-
h score much lower than other frequent regexes.
This variance from the test-on-training dataset car-
ries over almost exactly to Section 22.
(3) The different distribution of structures in
Answers hurts performance. For example, the
mediocre performance of the parser on SQ-vp
barely affects the score with OntoNotes, but has
a larger negative effect with Answers, due to its
increased frequency in the latter.
</bodyText>
<listItem confidence="0.727627">
(4) While the different distribution of construc-
</listItem>
<bodyText confidence="0.999966875">
tions is a problem for Answers, more critical is
the poor performance of the parser on determin-
ing the right edge of verbal constructions. This is
only 85.4 for VP-t in Answers, compared to the
OntoNotes results mentioned in (1). Since this af-
fects the F-s scores for VP-t, VP-aux, and S-vp,
the negative effect is large. Preliminary investi-
gation shows that this is due in part to incorrect
PP and SBAR placement (the PP-t and SBAR-s
attachment scores (80.7 and 81.9) are worse for
Answers compared to Section 22 (86.1 and 86.4)),
and coordinated S-clauses with no conjunction.
In sum, there is a wealth of information from
this new type of analysis that we will use in our on-
going work to better understand what the parser is
learning and how it works on different genres.
</bodyText>
<sectionHeader confidence="0.998145" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998277666666667">
This material is based upon work supported by Na-
tional Science Foundation Grant # BCS-114749
(first, fourth, and sixth authors) and by the Defense
Advanced Research Projects Agency (DARPA)
under Contract No. HR0011-11-C-0145 (first,
second, and third authors). The content does not
necessarily reflect the position or the policy of the
Government, and no official endorsement should
be inferred.
</bodyText>
<sectionHeader confidence="0.995784" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9996345125">
Srinivas Bangalore and Aravind K. Joshi, editors.
2010. Supertagging: Using Complex Lexical De-
scriptions in Natural Language Processing. MIT
Press.
Ann Bies, Justin Mott, Colin Warner, and Seth Kulick.
2012. English Web Treebank. LDC2012T13. Lin-
guistic Data Consortium.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
Department of Computer and Information Sciences,
University of Pennsylvania.
A.K. Joshi and Y. Schabes. 1997. Tree-adjoining
grammars. In G. Rozenberg and A. Salomaa, ed-
itors, Handbook of Formal Languages, Volume 3:
Beyond Words, pages 69–124. Springer, New York.
Seth Kulick, Ann Bies, and Justin Mott. 2011. Using
derivation trees for treebank error detection. Asso-
ciation for Computational Linguistics.
Seth Kulick, Ann Bies, and Justin Mott. 2012. Using
supertags and encoded annotation principles for im-
proved dependency to phrase structure conversion.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 305–314, Montr´eal, Canada, June. Associa-
tion for Computational Linguistics.
Seth Kulick, Ann Bies, Justin Mott, Mohamed
Maamouri, Beatrice Santorini, and Anthony Kroch.
2013. Using derivation trees for informative tree-
bank inter-annotator agreement evaluation. In Pro-
ceedings of the 2013 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
550–555, Atlanta, Georgia, June. Association for
Computational Linguistics.
Jonathan K. Kummerfeld, David Hall, James R. Cur-
ran, and Dan Klein. 2012. Parser showdown at the
wall street corral: An empirical investigation of error
types in parser output. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pages 1048–1059, Jeju Island,
Korea, July. Association for Computational Linguis-
tics.
Mitchell P. Marcus, Beatrice Santorini, Mary Ann
Marcinkiewicz, and Ann Taylor. 1999. Treebank-3.
LDC99T42, Linguistic Data Consortium, Philadel-
phia.
Slav Petrov and Ryan McDonald. 2012. Overview of
the 2012 shared task on parsing the web. In Pro-
ceedings of the First Workshop on Syntactic Analysis
of Non-Canonical Language.
Slav Petrov, Leon Barrett, Romain Thibaux, and
Dan Klein. 2008. The Berkeley Parser.
https://code.google.com/p/berkeleyparser/.
Owen Rambow and Aravind Joshi. 1997. A formal
look at dependency grammars and phrase-structure
grammars, with special consideration of word-order
phenomena. In L. Wanner, editor, Recent Trends
in Meaning-Text Theory, pages 167–190. John Ben-
jamins, Amsterdam and Philadelphia.
Satoshi Sekine and Michael Collins. 2008. Evalb.
http://nlp.cs.nyu.edu/evalb/.
Libin Shen and Aravind Joshi. 2008. LTAG depen-
dency parsing with bidirectional incremental con-
struction. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 495–504, Honolulu, Hawaii, October. As-
sociation for Computational Linguistics.
Libin Shen, Lucas Champollion, and Aravind Joshi.
2008. LTAG-spinal and the Treebank: A new
resource for incremental, dependency and seman-
tic parsing. Language Resources and Evaluation,
42(1):1–19.
Ralph Weischedel, Martha Palmer, Mitchell Marcus,
Eduard Hovy, Sameer Pradhan, Lance Ramshaw,
Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle
Franchini, Mohammed El-Bachouti, Robert Belvin,
and Ann Houston. 2011. OntoNotes 4.0. Linguistic
Data Consortium LDC2011T03.
</reference>
<page confidence="0.999271">
673
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.611842">
<title confidence="0.9948115">Parser Evaluation Using Derivation A Complement to evalb</title>
<author confidence="0.861944333333333">Kulick Bies Mott Linguistic Data Consortium</author>
<author confidence="0.861944333333333">University of Pennsylvania</author>
<author confidence="0.861944333333333">PA Kroch Liberman Philadelphia</author>
<affiliation confidence="0.974048">Department of Linguistics, University of Pennsylvania, Philadelphia, PA</affiliation>
<abstract confidence="0.998249">This paper introduces a new technique for phrase-structure parser analysis, categorizing possible treebank structures by integrating regular expressions into derivation trees. We analyze the performance of the Berkeley parser on OntoNotes WSJ and the English Web Treebank. This provides some insight into the evalb scores, and the problem of domain adaptation with the web data. We also analyze a “test-ontrain” dataset, showing a wide variance in how the parser is generalizing from different structures in the training material.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Aravind K Joshi</author>
<author>editors</author>
</authors>
<date>2010</date>
<booktitle>Supertagging: Using Complex Lexical Descriptions in Natural Language Processing.</booktitle>
<publisher>MIT Press.</publisher>
<marker>Bangalore, Joshi, editors, 2010</marker>
<rawString>Srinivas Bangalore and Aravind K. Joshi, editors. 2010. Supertagging: Using Complex Lexical Descriptions in Natural Language Processing. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Bies</author>
<author>Justin Mott</author>
<author>Colin Warner</author>
<author>Seth Kulick</author>
</authors>
<date>2012</date>
<booktitle>English Web Treebank. LDC2012T13. Linguistic Data Consortium.</booktitle>
<contexts>
<context position="3351" citStr="Bies et al., 2012" startWordPosition="515" endWordPosition="518">s allow us to also provide scores based on spans of different construction types. Together these two aspects break down the evalb brackets into more meaningful categories, and the simultaneous head and span scoring allows us to separate these aspects in the analysis. After describing in more detail the basic framework, we show some aspects of the resulting analysis of the performance of the Berkeley parser (Petrov et al., 2008) on three datasets: (a) OntoNotes WSJ sections 2-21 (Weischedel et al., 2011)1, (b) OntoNotes WSJ section 22, and (c) the “Answers” section of the English Web Treebank (Bies et al., 2012). We trained the parser on sections 2-21, and so (a) is “test-on-train”. These three results together show how the parser is generalizing from the training data, and what aspects of the “domain adaptation” problem to the web material are particularly important.2 2 Framework for analyzing parsing performance We first describe the use of the regexes in tree decomposition, and then give some examples of in1We refer only to the WSJ treebank portion of OntoNotes, which is roughly a subset of the Penn Treebank (Marcus et al., 1999) with annotation revisions including the addition of NML nodes. 2We p</context>
</contexts>
<marker>Bies, Mott, Warner, Kulick, 2012</marker>
<rawString>Ann Bies, Justin Mott, Colin Warner, and Seth Kulick. 2012. English Web Treebank. LDC2012T13. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer and Information Sciences, University of Pennsylvania.</institution>
<contexts>
<context position="4552" citStr="Collins (1999)" startWordPosition="705" endWordPosition="706"> nodes. 2We parse (c) while training on (a) to follow the procedure in Petrov and McDonald (2012) 668 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 668–673, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics corporating these regexes into the derivation trees. 2.1 Use of regular expressions Decomposing the original phrase-structure tree into the smaller components requires some method of determining the “head” of a nonterminal, from among its children nodes, similar to parsing work such as Collins (1999). As described above, we are also interested in the type of linguistic construction represented by that one-level structure, each of which instantiates one of a few types - recursive coordination, simple head-andsister, etc. We address both tasks together with the regexes. In contrast to the sort of head rules in (Collins, 1999), these refer as little as possible to specific POS tags. Instead of explicitly listing the POS tags of possible heads, the heads are in most cases determined by their location in the structure. Sample regexes are shown in Figure 1. There are 49 regexes used.3 Regexes A</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, Department of Computer and Information Sciences, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Joshi</author>
<author>Y Schabes</author>
</authors>
<title>Tree-adjoining grammars.</title>
<date>1997</date>
<booktitle>Handbook of Formal Languages, Volume 3: Beyond Words,</booktitle>
<pages>69--124</pages>
<editor>In G. Rozenberg and A. Salomaa, editors,</editor>
<publisher>Springer,</publisher>
<location>New York.</location>
<contexts>
<context position="1748" citStr="Joshi and Schabes, 1997" startWordPosition="256" endWordPosition="259">n pushing parser research forward, it has limited utility for understanding what sorts of errors a parser is making. This is the case even if the score is broken down by brackets (NP, VP, etc.), because the brackets can represent different types of structures. We would also like to have answers to such questions as “How does the parser do on non-recursive NPs, separate from NPs resulting from modification? On PP attachment?” etc. Answering such questions is the goal of this work, which combines two strands of research. First, inspired by the tradition of Tree Adjoining Grammar-based research (Joshi and Schabes, 1997; Bangalore and Joshi, 2010), we use a decomposition of the full trees into “elementary trees” (henceforth “etrees”), with a derivation tree that records how the etrees relate to each other, as in Kulick et al. (2011). In particular, we use the “spinal” structure approach of (Shen et al., 2008; Shen and Joshi, 2008), where etrees are constrained to be unary-branching. Second, we use a set of regular expressions (henceforth “regexes”) that categorize the possible structures in the treebank. These are best thought of as an extension of head-finding rules, which not only find a head but simultane</context>
</contexts>
<marker>Joshi, Schabes, 1997</marker>
<rawString>A.K. Joshi and Y. Schabes. 1997. Tree-adjoining grammars. In G. Rozenberg and A. Salomaa, editors, Handbook of Formal Languages, Volume 3: Beyond Words, pages 69–124. Springer, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Seth Kulick</author>
<author>Ann Bies</author>
<author>Justin Mott</author>
</authors>
<title>Using derivation trees for treebank error detection. Association for Computational Linguistics.</title>
<date>2011</date>
<contexts>
<context position="1965" citStr="Kulick et al. (2011)" startWordPosition="293" endWordPosition="296">represent different types of structures. We would also like to have answers to such questions as “How does the parser do on non-recursive NPs, separate from NPs resulting from modification? On PP attachment?” etc. Answering such questions is the goal of this work, which combines two strands of research. First, inspired by the tradition of Tree Adjoining Grammar-based research (Joshi and Schabes, 1997; Bangalore and Joshi, 2010), we use a decomposition of the full trees into “elementary trees” (henceforth “etrees”), with a derivation tree that records how the etrees relate to each other, as in Kulick et al. (2011). In particular, we use the “spinal” structure approach of (Shen et al., 2008; Shen and Joshi, 2008), where etrees are constrained to be unary-branching. Second, we use a set of regular expressions (henceforth “regexes”) that categorize the possible structures in the treebank. These are best thought of as an extension of head-finding rules, which not only find a head but simultaneously identify each parent/children relation as one of a limited number of types of structures (right-modification, etc.). The crucial step is that we integrate these regexes into the spinal etrees. The derivation tre</context>
</contexts>
<marker>Kulick, Bies, Mott, 2011</marker>
<rawString>Seth Kulick, Ann Bies, and Justin Mott. 2011. Using derivation trees for treebank error detection. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Seth Kulick</author>
<author>Ann Bies</author>
<author>Justin Mott</author>
</authors>
<title>Using supertags and encoded annotation principles for improved dependency to phrase structure conversion.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>305--314</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="10890" citStr="Kulick et al., 2012" startWordPosition="1835" endWordPosition="1838">l the match just for the head the “Fh” score and the match that also includes the span information the “F-s” score. The F-s score roughly corresponds to the evalb score. However, the Fs score is for separate syntactic constructions (including also head identification), although we can also sum it over all the structures, as done later in Figure 6. The simultaneous F-h and F-s scores let us identify constructions where the parser has the head projection correct, but gets the span wrong. 1 (2) Since the derivation tree is really a dependency tree with more complex nodes (Rambow and Joshi, 1997; Kulick et al., 2012), we can also score each regex for attachment.6 For example, while “to” is a match for PP-t, its attachment is not, since in Figure 2 it is a child of the “trip” etree (#a5) and in Figure 3 it is a child of the “make” etree (#b3). Therefore our analysis results in an attachment score for every regex. 2.4 Comparison with previous work This work is in the same basic line of research as the inter-annotator agreement analysis work in Kulick et al. (2013). However, that work did not utilize regexes, and focused on comparing sequences of identical strings. The current work scores on general categori</context>
</contexts>
<marker>Kulick, Bies, Mott, 2012</marker>
<rawString>Seth Kulick, Ann Bies, and Justin Mott. 2012. Using supertags and encoded annotation principles for improved dependency to phrase structure conversion. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 305–314, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Seth Kulick</author>
<author>Ann Bies</author>
<author>Justin Mott</author>
<author>Mohamed Maamouri</author>
<author>Beatrice Santorini</author>
<author>Anthony Kroch</author>
</authors>
<title>Using derivation trees for informative treebank inter-annotator agreement evaluation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>550--555</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="11344" citStr="Kulick et al. (2013)" startWordPosition="1919" endWordPosition="1922">on correct, but gets the span wrong. 1 (2) Since the derivation tree is really a dependency tree with more complex nodes (Rambow and Joshi, 1997; Kulick et al., 2012), we can also score each regex for attachment.6 For example, while “to” is a match for PP-t, its attachment is not, since in Figure 2 it is a child of the “trip” etree (#a5) and in Figure 3 it is a child of the “make” etree (#b3). Therefore our analysis results in an attachment score for every regex. 2.4 Comparison with previous work This work is in the same basic line of research as the inter-annotator agreement analysis work in Kulick et al. (2013). However, that work did not utilize regexes, and focused on comparing sequences of identical strings. The current work scores on general categories of structures, without 6A regex intermediate in a etree, such as VP-t above, is considered to have a default null attachment. Also, the attachment score is not relevant for regexes that already express a recursive structure, such as NP-modr. In Figure 2, NP-t in etree #a5 is considered as having the attachment to #a3. 670 Sections 2-21 (Ontonotes) Section 22 (Ontonotes) Answers (English Web Treebank) regex %gold F-h F-s att spanR %gold F-h F-s att</context>
</contexts>
<marker>Kulick, Bies, Mott, Maamouri, Santorini, Kroch, 2013</marker>
<rawString>Seth Kulick, Ann Bies, Justin Mott, Mohamed Maamouri, Beatrice Santorini, and Anthony Kroch. 2013. Using derivation trees for informative treebank inter-annotator agreement evaluation. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 550–555, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jonathan K Kummerfeld</author>
<author>David Hall</author>
<author>James R Curran</author>
<author>Dan Klein</author>
</authors>
<title>Parser showdown at the wall street corral: An empirical investigation of error types in parser output.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1048--1059</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="14924" citStr="Kummerfeld et al. (2012)" startWordPosition="2580" endWordPosition="2583">ns with the same bracket deletion (empty yields and most punctuation) as evalb. We ran the gold and parsed versions through our regex decomposition and derivation tree creation. Table 1 shows the number and percentage of brackets handled by our regexes. The high coverage (%) reinforces the point that there is a limited number of core structures in the treebank. In the results below in Table 2 and Figure 6 we combine the nonterminals that are not covered by one of the regexes with the simple non-recursive regex case for that nonterminal.8 7In future work we will compare our approach to that of Kummerfeld et al. (2012), who also move beyond evalb scores in an effort to provide more meaningful error analysis. 8We also combine a few other non-recursive regexes together with NP-t, such as the special one for possessives. We present the results in two ways. Table 2 lists the most frequent categories in the three datasets, with their percentage of the overall number of brackets (%gold), their score based just on the head identification (F-h), their score based on head identification and (left and right) span (F-s), and the attachment (att) and span-right (spanR) scores for those that match based on the head.9 Th</context>
</contexts>
<marker>Kummerfeld, Hall, Curran, Klein, 2012</marker>
<rawString>Jonathan K. Kummerfeld, David Hall, James R. Curran, and Dan Klein. 2012. Parser showdown at the wall street corral: An empirical investigation of error types in parser output. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1048–1059, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Ann Taylor</author>
</authors>
<date>1999</date>
<booktitle>Treebank-3. LDC99T42, Linguistic Data Consortium,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="3882" citStr="Marcus et al., 1999" startWordPosition="605" endWordPosition="608">section 22, and (c) the “Answers” section of the English Web Treebank (Bies et al., 2012). We trained the parser on sections 2-21, and so (a) is “test-on-train”. These three results together show how the parser is generalizing from the training data, and what aspects of the “domain adaptation” problem to the web material are particularly important.2 2 Framework for analyzing parsing performance We first describe the use of the regexes in tree decomposition, and then give some examples of in1We refer only to the WSJ treebank portion of OntoNotes, which is roughly a subset of the Penn Treebank (Marcus et al., 1999) with annotation revisions including the addition of NML nodes. 2We parse (c) while training on (a) to follow the procedure in Petrov and McDonald (2012) 668 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 668–673, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics corporating these regexes into the derivation trees. 2.1 Use of regular expressions Decomposing the original phrase-structure tree into the smaller components requires some method of determining the “head” of a nonterminal, from am</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, Taylor, 1999</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, Mary Ann Marcinkiewicz, and Ann Taylor. 1999. Treebank-3. LDC99T42, Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Ryan McDonald</author>
</authors>
<title>Overview of the 2012 shared task on parsing the web.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Workshop on Syntactic Analysis of Non-Canonical Language.</booktitle>
<contexts>
<context position="4035" citStr="Petrov and McDonald (2012)" startWordPosition="630" endWordPosition="633">est-on-train”. These three results together show how the parser is generalizing from the training data, and what aspects of the “domain adaptation” problem to the web material are particularly important.2 2 Framework for analyzing parsing performance We first describe the use of the regexes in tree decomposition, and then give some examples of in1We refer only to the WSJ treebank portion of OntoNotes, which is roughly a subset of the Penn Treebank (Marcus et al., 1999) with annotation revisions including the addition of NML nodes. 2We parse (c) while training on (a) to follow the procedure in Petrov and McDonald (2012) 668 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 668–673, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics corporating these regexes into the derivation trees. 2.1 Use of regular expressions Decomposing the original phrase-structure tree into the smaller components requires some method of determining the “head” of a nonterminal, from among its children nodes, similar to parsing work such as Collins (1999). As described above, we are also interested in the type of linguistic construction</context>
<context position="14223" citStr="Petrov and McDonald (2012)" startWordPosition="2460" endWordPosition="2463">ords that match in F-h. the reliance on sequences of individual strings.7 3 Analysis of parsing results We worked with the three datasets as described in the introduction. We trained the parser on sections 2-21 of OntoNotes WSJ, and parsed the three datasets with the gold tags, since at present we wish to analyze the parser performance in isolation from Part-of-Speech tagging errors. Table 1 shows the sizes of the three corpora in terms of tokens and brackets, for both the gold and parsed versions, with the evalb scores for the parsed versions. The score is lower for Answers, as also found by Petrov and McDonald (2012). To facilitate comparison of our analysis with evalb, we used corpora versions with the same bracket deletion (empty yields and most punctuation) as evalb. We ran the gold and parsed versions through our regex decomposition and derivation tree creation. Table 1 shows the number and percentage of brackets handled by our regexes. The high coverage (%) reinforces the point that there is a limited number of core structures in the treebank. In the results below in Table 2 and Figure 6 we combine the nonterminals that are not covered by one of the regexes with the simple non-recursive regex case fo</context>
</contexts>
<marker>Petrov, McDonald, 2012</marker>
<rawString>Slav Petrov and Ryan McDonald. 2012. Overview of the 2012 shared task on parsing the web. In Proceedings of the First Workshop on Syntactic Analysis of Non-Canonical Language.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
</authors>
<location>Romain Thibaux, and</location>
<marker>Petrov, Barrett, </marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
</authors>
<title>The Berkeley Parser.</title>
<date>2008</date>
<note>https://code.google.com/p/berkeleyparser/.</note>
<marker>Klein, 2008</marker>
<rawString>Dan Klein. 2008. The Berkeley Parser. https://code.google.com/p/berkeleyparser/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Owen Rambow</author>
<author>Aravind Joshi</author>
</authors>
<title>A formal look at dependency grammars and phrase-structure grammars, with special consideration of word-order phenomena.</title>
<date>1997</date>
<booktitle>Recent Trends in Meaning-Text Theory,</booktitle>
<pages>167--190</pages>
<editor>In L. Wanner, editor,</editor>
<location>Amsterdam and Philadelphia.</location>
<contexts>
<context position="10868" citStr="Rambow and Joshi, 1997" startWordPosition="1831" endWordPosition="1834">results to evalb. We call the match just for the head the “Fh” score and the match that also includes the span information the “F-s” score. The F-s score roughly corresponds to the evalb score. However, the Fs score is for separate syntactic constructions (including also head identification), although we can also sum it over all the structures, as done later in Figure 6. The simultaneous F-h and F-s scores let us identify constructions where the parser has the head projection correct, but gets the span wrong. 1 (2) Since the derivation tree is really a dependency tree with more complex nodes (Rambow and Joshi, 1997; Kulick et al., 2012), we can also score each regex for attachment.6 For example, while “to” is a match for PP-t, its attachment is not, since in Figure 2 it is a child of the “trip” etree (#a5) and in Figure 3 it is a child of the “make” etree (#b3). Therefore our analysis results in an attachment score for every regex. 2.4 Comparison with previous work This work is in the same basic line of research as the inter-annotator agreement analysis work in Kulick et al. (2013). However, that work did not utilize regexes, and focused on comparing sequences of identical strings. The current work scor</context>
</contexts>
<marker>Rambow, Joshi, 1997</marker>
<rawString>Owen Rambow and Aravind Joshi. 1997. A formal look at dependency grammars and phrase-structure grammars, with special consideration of word-order phenomena. In L. Wanner, editor, Recent Trends in Meaning-Text Theory, pages 167–190. John Benjamins, Amsterdam and Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
<author>Michael Collins</author>
</authors>
<date>2008</date>
<note>Evalb. http://nlp.cs.nyu.edu/evalb/.</note>
<contexts>
<context position="1027" citStr="Sekine and Collins, 2008" startWordPosition="137" endWordPosition="140">paper introduces a new technique for phrase-structure parser analysis, categorizing possible treebank structures by integrating regular expressions into derivation trees. We analyze the performance of the Berkeley parser on OntoNotes WSJ and the English Web Treebank. This provides some insight into the evalb scores, and the problem of domain adaptation with the web data. We also analyze a “test-ontrain” dataset, showing a wide variance in how the parser is generalizing from different structures in the training material. 1 Introduction Phrase-structure parsing is usually evaluated using evalb (Sekine and Collins, 2008), which provides a score based on matching brackets. While this metric serves a valuable purpose in pushing parser research forward, it has limited utility for understanding what sorts of errors a parser is making. This is the case even if the score is broken down by brackets (NP, VP, etc.), because the brackets can represent different types of structures. We would also like to have answers to such questions as “How does the parser do on non-recursive NPs, separate from NPs resulting from modification? On PP attachment?” etc. Answering such questions is the goal of this work, which combines tw</context>
</contexts>
<marker>Sekine, Collins, 2008</marker>
<rawString>Satoshi Sekine and Michael Collins. 2008. Evalb. http://nlp.cs.nyu.edu/evalb/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Aravind Joshi</author>
</authors>
<title>LTAG dependency parsing with bidirectional incremental construction.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>495--504</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<contexts>
<context position="2065" citStr="Shen and Joshi, 2008" startWordPosition="310" endWordPosition="313">ow does the parser do on non-recursive NPs, separate from NPs resulting from modification? On PP attachment?” etc. Answering such questions is the goal of this work, which combines two strands of research. First, inspired by the tradition of Tree Adjoining Grammar-based research (Joshi and Schabes, 1997; Bangalore and Joshi, 2010), we use a decomposition of the full trees into “elementary trees” (henceforth “etrees”), with a derivation tree that records how the etrees relate to each other, as in Kulick et al. (2011). In particular, we use the “spinal” structure approach of (Shen et al., 2008; Shen and Joshi, 2008), where etrees are constrained to be unary-branching. Second, we use a set of regular expressions (henceforth “regexes”) that categorize the possible structures in the treebank. These are best thought of as an extension of head-finding rules, which not only find a head but simultaneously identify each parent/children relation as one of a limited number of types of structures (right-modification, etc.). The crucial step is that we integrate these regexes into the spinal etrees. The derivation trees provide elements of a dependency analysis, which allow us to calculate scores for head identifica</context>
</contexts>
<marker>Shen, Joshi, 2008</marker>
<rawString>Libin Shen and Aravind Joshi. 2008. LTAG dependency parsing with bidirectional incremental construction. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 495–504, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Lucas Champollion</author>
<author>Aravind Joshi</author>
</authors>
<title>LTAG-spinal and the Treebank: A new resource for incremental, dependency and semantic parsing.</title>
<date>2008</date>
<journal>Language Resources and Evaluation,</journal>
<volume>42</volume>
<issue>1</issue>
<contexts>
<context position="2042" citStr="Shen et al., 2008" startWordPosition="306" endWordPosition="309">uch questions as “How does the parser do on non-recursive NPs, separate from NPs resulting from modification? On PP attachment?” etc. Answering such questions is the goal of this work, which combines two strands of research. First, inspired by the tradition of Tree Adjoining Grammar-based research (Joshi and Schabes, 1997; Bangalore and Joshi, 2010), we use a decomposition of the full trees into “elementary trees” (henceforth “etrees”), with a derivation tree that records how the etrees relate to each other, as in Kulick et al. (2011). In particular, we use the “spinal” structure approach of (Shen et al., 2008; Shen and Joshi, 2008), where etrees are constrained to be unary-branching. Second, we use a set of regular expressions (henceforth “regexes”) that categorize the possible structures in the treebank. These are best thought of as an extension of head-finding rules, which not only find a head but simultaneously identify each parent/children relation as one of a limited number of types of structures (right-modification, etc.). The crucial step is that we integrate these regexes into the spinal etrees. The derivation trees provide elements of a dependency analysis, which allow us to calculate sco</context>
</contexts>
<marker>Shen, Champollion, Joshi, 2008</marker>
<rawString>Libin Shen, Lucas Champollion, and Aravind Joshi. 2008. LTAG-spinal and the Treebank: A new resource for incremental, dependency and semantic parsing. Language Resources and Evaluation, 42(1):1–19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Weischedel</author>
<author>Martha Palmer</author>
<author>Mitchell Marcus</author>
<author>Eduard Hovy</author>
<author>Sameer Pradhan</author>
<author>Lance Ramshaw</author>
</authors>
<date>2011</date>
<booktitle>OntoNotes 4.0. Linguistic Data Consortium LDC2011T03.</booktitle>
<location>Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle Franchini, Mohammed El-Bachouti, Robert</location>
<contexts>
<context position="3241" citStr="Weischedel et al., 2011" startWordPosition="495" endWordPosition="498">allow us to calculate scores for head identification and attachment for different projections (e.g., PP). The regexes allow us to also provide scores based on spans of different construction types. Together these two aspects break down the evalb brackets into more meaningful categories, and the simultaneous head and span scoring allows us to separate these aspects in the analysis. After describing in more detail the basic framework, we show some aspects of the resulting analysis of the performance of the Berkeley parser (Petrov et al., 2008) on three datasets: (a) OntoNotes WSJ sections 2-21 (Weischedel et al., 2011)1, (b) OntoNotes WSJ section 22, and (c) the “Answers” section of the English Web Treebank (Bies et al., 2012). We trained the parser on sections 2-21, and so (a) is “test-on-train”. These three results together show how the parser is generalizing from the training data, and what aspects of the “domain adaptation” problem to the web material are particularly important.2 2 Framework for analyzing parsing performance We first describe the use of the regexes in tree decomposition, and then give some examples of in1We refer only to the WSJ treebank portion of OntoNotes, which is roughly a subset o</context>
</contexts>
<marker>Weischedel, Palmer, Marcus, Hovy, Pradhan, Ramshaw, 2011</marker>
<rawString>Ralph Weischedel, Martha Palmer, Mitchell Marcus, Eduard Hovy, Sameer Pradhan, Lance Ramshaw, Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle Franchini, Mohammed El-Bachouti, Robert Belvin, and Ann Houston. 2011. OntoNotes 4.0. Linguistic Data Consortium LDC2011T03.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>