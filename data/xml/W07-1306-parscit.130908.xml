<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005216">
<title confidence="0.951923">
Can Corpus Based Measures be Used for Comparative Study of Languages?
</title>
<author confidence="0.963983">
Anil Kumar Singh
</author>
<affiliation confidence="0.906204">
Language Tech. Research Centre
Int’l Inst. of Information Tech.
Hyderabad, India
</affiliation>
<email confidence="0.97833">
anil@research.iiit.net
</email>
<author confidence="0.981753">
Harshit Surana
</author>
<affiliation confidence="0.909444666666667">
Language Tech. Research Centre
Int’l Inst. of Information Tech.
Hyderabad, India
</affiliation>
<email confidence="0.993215">
surana.h@gmail.com
</email>
<sectionHeader confidence="0.998572" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999829764705882">
Quantitative measurement of inter-language
distance is a useful technique for studying
diachronic and synchronic relations between
languages. Such measures have been used
successfully for purposes like deriving lan-
guage taxonomies and language reconstruc-
tion, but they have mostly been applied to
handcrafted word lists. Can we instead
use corpus based measures for comparative
study of languages? In this paper we try to
answer this question. We use three corpus
based measures and present the results ob-
tained from them and show how these results
relate to linguistic and historical knowledge.
We argue that the answer is yes and that such
studies can provide or validate linguistic and
computational insights.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999972238095238">
Crosslingual and multilingual processing is acquir-
ing importance in the computational linguistics
community. As a result, semi-automatic crosslin-
gual comparison of languages is also becoming
a fruitful area of study. Among the fundamen-
tal tools for crosslingual comparison are measures
of inter-language distances. In linguistics, the
study of inter-language distances, especially for lan-
guage classification, has a long history (Swadesh,
1952; Ellison and Kirby, 2006). Basically, the
work on this problem has been along linguistic,
archaeological and computational streams. Like
in other disciplines, computational methods are in-
creasingly being combined with other more conven-
tional approaches (Dyen et al., 1992; Nerbonne and
Heeringa, 1997; Kondrak, 2002; Ellison and Kirby,
2006). The work being presented in this paper be-
longs to the computational stream.
Even in the computational stream, most of the
previous work on inter-language distances had a
strong linguistic dimension. For example, most
of the quantitative measures of inter-language dis-
tance have been applied on handcrafted word
lists (Swadesh, 1952; Dyen et al., 1992). However,
with increasing use of computational techniques and
the availability of electronic data, a natural ques-
tion arises: Can languages be linguistically com-
pared based on word lists extracted from corpora.
A natural counter-question is whether such compar-
ison will be valid from linguistic and psycholinguis-
tic points of view. The aim of this paper is to exam-
ine such questions.
To calculate inter-language distances on the basis
of words in corpora, we propose two corpus based
distance measures. They internally use a more lin-
guistically grounded distance measure for compar-
ing strings. We also present the results obtained with
one purely statistical measure, just to show that even
naive corpus based measures can be useful. The
main contribution is to show that even noisy corpora
can be used for comparative study of languages. Dif-
ferent measures can give different kinds of insights.
</bodyText>
<sectionHeader confidence="0.999952" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999459">
Typology or history of languages can be studied us-
ing spoken data or text. There has been work on
the former (Remmel, 1980; Kondrak, 2002), but we
</bodyText>
<page confidence="0.981197">
40
</page>
<note confidence="0.606953">
Proceedings of Ninth Meeting of the ACL Special Interest Group in Computational Morphology and Phonology, pages 40–47,
Prague, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.99871425">
will focus only on text. An example of a major work
on text based similarity is the paper by Kondrak and
Sherif (Kondrak and Sherif, 2006). They have evalu-
ated various phonetic similarity algorithms for align-
ing cognates. They found that learning based al-
gorithms outperform manually constructed schemes,
but only when large training data is used.
A recent work on applications of such techniques
for linguistic study is by Heeringa et al. (Heeringa
et al., 2006). They performed a study on differ-
ent variations of string distance algorithms for di-
alectology and concluded that order sensitivity is
important while scaling with length is not. It may
be noted that Ellison and Kirby (Ellison and Kirby,
2006) have shown that scaling by distance does give
significantly better results. Nakleh et al. (Nakleh
et al., 2005) have written about using phyloge-
netic techniques in historical linguistics as men-
tioned by Nerbonne (Nerbonne, 2005) in the review
of the book titled ‘Language Classification by Num-
bers’ by McMahon and McMahon (McMahon and
McMahon, 2005). All these works are about using
quantitative techniques for language typology and
classification etc.
</bodyText>
<sectionHeader confidence="0.999456" genericHeader="method">
3 Inter-Language Comparison
</sectionHeader>
<bodyText confidence="0.999903695652174">
Inter-language comparison is more general than
measuring inter-language distance. In addition to
the overall linguistic distance, the comparison can
be of more specific characteristics like the propor-
tion of cognates derived vertically and horizontally.
Or it can be of specific phonetic features (Nerbonne,
2005; McMahon and McMahon, 2005). Quantita-
tive measures for comparing languages can first be
classified according to the form of data being com-
pared, i.e., speech, written text or electronic text.
Assuming that the text is in electronic form, the most
common measures are based on word lists. These
lists are usually prepared by linguists and they are
often in some special notation, e.g. more or less a
phonetic transcription.
The measures can be based on inter-lingual or on
intra-lingual comparison of phonetic forms (Ellison
and Kirby, 2006). They may or may not use statis-
tical techniques like measures of distributional sim-
ilarity (cross entropy, KL-divergence, etc.). These
characteristics of measures may imply some linguis-
tic or psycholinguistic assumptions. One of these is
about a common phonetic space.
</bodyText>
<sectionHeader confidence="0.962826" genericHeader="method">
4 Common Phonetic Space
</sectionHeader>
<bodyText confidence="0.999977837209303">
Language distance can be calculated through
crosslingual as well as intra-lingual comparison.
Many earlier attempts (Nerbonne and Heeringa,
1997; Kondrak, 2002) were based on crosslingual
comparison of phonetic forms, but some researchers
have argued against the possibility of obtaining
meaningful results from crosslingual comparison of
phonetic forms. This is related to the idea of a
common phonetic space. Port and Leary (Port and
Leary, 2005) have argued against it. Ellison and
Kirby (Ellison and Kirby, 2006) argue that even if
there is a common space, language specific catego-
rization of sound often restructures this space. They
conclude that if there is no language-independent
common phonetic space with an equally common
similarity measure, there can be no principled ap-
proach to comparing forms in one language with
another. They suggest that language-internal com-
parison of forms is better and psychologically more
well-grounded.
This may be true, but should we really abandon
the approach based on crosslingual comparison? As
even Ellison and Kirby say, it is possible to argue
that there is a common phonetic space. After all,
the sounds produced by humans are determined by
human physiology. The only matter of debate is
whether common phonetic space makes sense from
the cognitive point of view. We argue that it does.
In psychology, there has been a long debate about
a similar problem which can be stated in terms of a
common chromatic space. Do humans in different
cultures see the same colors? There is still no con-
clusive answer, but many computational techniques
have been tried to solve real world problems like
classifying human faces, seemingly with the implicit
assumption that there is a common chromatic space.
Such techniques have shown some success (sheng
Chen and kai Liu, 2003).
Could it be that we are defining the notion of a
common chromatic (or phonetic) space too strictly?
Or that the way we define it is not relevant for com-
putational techniques? In our view the answer is
yes. We will give a simple, not very novel, exam-
</bodyText>
<page confidence="0.998904">
41
</page>
<bodyText confidence="0.9999609">
ple. The phoneme t as in the English word battery is
not present in many languages of the world. When
a Thai speaker can not say battery, with the correct
t, he will say battery with t as in the French word
entre. Such substitution will be very regular. The
point is that even if phonetic space is restructured
for a particular language, we can still find which
segments or sections of two differently structured
phonetic spaces are close. Cyan may span different
ranges (on the spectrum) in different cultures, but the
ranges are likely to be near to one another. Even if
some culture has no color which can be called cyan,
one or two of the colors that it does have will be
closer to cyan than the others. The same is true
for all the other colors and also for sounds. If we
use fuzzy similarity measures to take care of such
differently structured cognitive spaces, cross-lingual
comparison may still be meaningful for certain pur-
poses. This argument is in defence of cross-lingual
comparison, not against intra-lingual comparison.
</bodyText>
<sectionHeader confidence="0.962479" genericHeader="method">
5 Common Orthographic Space
</sectionHeader>
<bodyText confidence="0.9995596">
Writing systems used by languages differ very
widely. This can be taken to mean that there
is no common orthographic space for meaning-
ful crosslingual comparison of orthographic forms.
This may be true in general, but for sets of languages
using related scripts, we can assume a similar ortho-
graphic space. For example, most of the major South
Asian languages use scripts derived from Brahmi.
The similarity among these scripts is so much that
crosslingual comparison of text is possible for var-
ious purposes such as identifying cognates without
any phonetic transcription. This is in spite of the fact
that the letter shapes differ so much that they are not
mutually identifiable. Such similarity is relevant for
corpus based measures.
</bodyText>
<sectionHeader confidence="0.946238" genericHeader="method">
6 Corpus Based Measures
</sectionHeader>
<bodyText confidence="0.999968304347826">
Since we use (non-parallel) corpora of the two lan-
guages for finding out the cognates and hence com-
paring two languages, the validity of the results de-
pends on how representative the corpora are. How-
ever, if they are of enough size, we might still be
able to make meaningful, even if limited, compar-
ison among languages. We restrict ourselves to
word list based comparison. In such a case, cor-
pus based measures can be effective if the corpora
contain a representative portion of the vocabulary,
or even of word segments. The second case (of seg-
ments) is relevant for the n-gram measure described
in section-7.
This category of measures have to incorporate
more linguistic information if they are to provide
good results. Designing such measures can be a
challenging problem as we will be mainly relying
on the corpus for our information. Knowledge about
similarities and differences of writing systems can
play an important role here. The two cognate based
measures described in sections 9 and 10 are an at-
tempt at this. But first we describe a simple n-gram
based measure.
</bodyText>
<sectionHeader confidence="0.99789" genericHeader="method">
7 Symmetric Cross Entropy (SCE)
</sectionHeader>
<bodyText confidence="0.999980166666667">
The first measure is purely a letter n-gram based
measure similar to the one used by Singh (Singh,
2006b) for language and encoding identification. To
calculate the distance, we first prepare letter 5-gram
models from the corpora of the languages to be com-
pared. Then we combine n-grams of all orders and
rank them according to their probability in descend-
ing order. Only the top N n-grams are retained and
the rest are pruned. 1 Now we have two probability
distributions which can be compared by a measure
of distributional similarity. We have used symmetric
cross entropy as such a measure:
</bodyText>
<equation confidence="0.922891">
�dsce = (p(gl) log q(gm) + q(gm) log p(gl))
gl=gm
(1)
</equation>
<bodyText confidence="0.999909363636364">
where p and q are the probability distributions for
the two languages and gl and gm are n-grams in lan-
guages l and m, respectively.
The disadvantage of this measure is that it does
not use any linguistic (e.g., phonetic) information,
but the advantage is that it can measure the similar-
ity of distributions of n-grams. Such measures have
proved to be very effective in automatically iden-
tifying languages of text, with accuracies nearing
100% for fairly small amounts of training and test
data (Adams and Resnik, 1997; Singh, 2006b).
</bodyText>
<footnote confidence="0.95625475">
1This is based on the results obtained by Cavnar (Cavnar and
Trenkle, 1994) and our own studies, which show that the top N
(300 according to Cavnar) n-grams have a high correlation with
the identity of the language.
</footnote>
<page confidence="0.999178">
42
</page>
<sectionHeader confidence="0.992919" genericHeader="method">
8 Method for Cognate Identification
</sectionHeader>
<bodyText confidence="0.999886206896552">
The other two measures are based on cognates, in-
herited as well as borrowed. Both of them use an
algorithm for identification of cognates. Many such
algorithms have been proposed. Estimates of sur-
face similarity can be used for finding cognate words
across languages for related languages. By surface
similarity we mean the orthographic, phonetic and
(possibly) morphological similarity of two words or
strings. In spite of the name, surface similarity is
deeper than string similarity as calculated by edit
distances. Ribeiro et al. (Ribeiro et al., 2001) have
surveyed some of the algorithms for cognate align-
ment. However, since they studied methods based
on parallel text, we cannot use them directly.
For identifying cognates, we are using the compu-
tational model of scripts or CPMS (Singh, 2006a).
This model takes into account the characteristics of
Brahmi origin scripts and calculates surface simi-
larity in a fuzzy way. This is achieved by using
a stepped distance function (SDF) and a dynamic
programming (DP) algorithm. We have adapted the
CPMS for identifying cognates.
Different researchers have argued about the im-
portance of order sensitivity and scaling in using
string comparison algorithms (Heeringa et al., 2006;
Ellison and Kirby, 2006). The CPMS takes both
of these into account, as well as using knowledge
about the script. In general, the distance between
two strings can be defined as:
</bodyText>
<equation confidence="0.966013">
clm = fp(wl, wm) (2)
</equation>
<bodyText confidence="0.9999416">
where fp is the function which calculates surface
similarity based cost between the word wl of lan-
guage l and the word wm of language m.
Those word pairs are identified as cognates which
have the least cost.
</bodyText>
<sectionHeader confidence="0.979292" genericHeader="method">
9 Cognate Coverage Distance (CCD)
</sectionHeader>
<bodyText confidence="0.999903333333333">
The second measure used by us is a corpus based
estimate of the coverage of cognates across two lan-
guages. Cognate coverage is defined as the num-
ber of words (out of the vocabularies of the two lan-
guages) which are of the same origin. The decision
about whether two words are cognates or not is made
on the basis of surface similarity of the two words
as described in the previous section. We use (non-
parallel) corpora of the two languages for identify-
ing the cognates.
The normalized distance between two languages
is defined as:
</bodyText>
<equation confidence="0.9960555">
′ tlm (3 )
tlm = 1 − max(t)
</equation>
<bodyText confidence="0.999756833333333">
where tlm and tml are the number of cognates found
when comparing from language l to m and from lan-
guage m to l, respectively.
Since the CPMS based measure of surface lexical
similarity is asymmetric, we calculate the average
number of unidirectional cognates:
</bodyText>
<equation confidence="0.974358">
dccd = t′lm + t′ml (4)
2
</equation>
<sectionHeader confidence="0.969345" genericHeader="method">
10 Phonetic Distance of Cognates (PDC)
</sectionHeader>
<bodyText confidence="0.999799">
Simply finding the coverage of cognates may in-
dicate the distance between two languages, but a
measure based solely on this information does not
take into account the variation between the cognates
themselves. To include this variation into the esti-
mate of distance, we use another measure based on
the sum of the CPMS based cost of n cognates found
between two languages:
</bodyText>
<equation confidence="0.552089">
clm (5)
</equation>
<bodyText confidence="0.991933">
where n is the minimum of tlm for all the language
pairs compared.
The normalized distance can be defined as:
</bodyText>
<equation confidence="0.86352125">
Cpdc
C′lm
lm (6)
max(Cpdc)
A symmetric version of this cost is then calcu-
lated:
C′ lm + C′ ml
dpdc = (7) 2
</equation>
<sectionHeader confidence="0.989862" genericHeader="method">
11 Experimental Setup
</sectionHeader>
<bodyText confidence="0.9998578">
For synchronic comparison, we selected ten lan-
guages for our experiment (table-1), mainly be-
cause sufficient corpora were available for these lan-
guages. These languages, though belonging to two
different families (Indo-Iranian and Dravidian), have
</bodyText>
<equation confidence="0.988248">
n
E
i = 0
Cpdc =
lm
</equation>
<page confidence="0.993319">
43
</page>
<figure confidence="0.972517">
Combined11
CCD11 PDC11
</figure>
<figureCaption confidence="0.969772">
Figure 1: Graphical view of synchronic comparison among ten major South Asian languages using CCD
and PDC measures. The layout of the graph is modeled on the geographical locations of these languages.
The connections among the nodes of the graph are obtained by joining each node to its two closest neighbors
in terms of the values obtained by using the two measures.
</figureCaption>
<figure confidence="0.998441264150943">
PAE
PAE
PAE
0.2011
EINE
HIE
0.0211
ASE
0.1611
0.1211
ASE
0.5211 0.3211
HIE
0.3711
EINE
ASE
EINE
HIE
0.0711 0.2011
0.0511 0.1111
0.4211
0.1611
ORE
ORE
ORE
MRE
0.6111
MRE
MRE
0.6111
0.1711
TEE
TEE
0.1711
TEE
KNE
0.5311
0.2511
KNE
KNE
0.7211
0.8511
0.3111
0.8111
0.4511
MLE
TAE
0.5611
TAE
TAE
MLE
0.6211
MLE
</figure>
<bodyText confidence="0.997049879310345">
a lot of similarities (Emeneau, 1956). The cognate
words among them are loanwords as well as inher-
ited words. In fact, the similarity among these lan-
guages is due to common origin (intra-family) as
well as contact and borrowing over thousands of
years (intra- and inter-family). Moreover, they also
use scripts derived from the same origin (Brahmi),
which allows us to use the CPMS for identifying
cognates. The corpora used for these ten languages
are all part of the CIIL (Central Institute of Indian
Languages) multilingual corpus. This corpus is a
collection of documents from different domains and
is one of best known corpora for Indian languages.
Still, the representativeness of this corpus may be a
matter of debate as it is not as large and diverse as
the BNC (British National Corpus) corpus for En-
glish.
For the cognate measures (CCD and PDC), the
only information we are extracting from the cor-
pora are the word types and their frequencies.
Thus, in a way, we are also working with word
lists, but our word lists are extracted from cor-
pora. Word lists handcrafted by linguists may be
very useful, but they are not always available for
all kinds of inter-language or inter-dialectal compar-
ison, whereas electronic corpora are more likely to
be available. Currently we are not doing any prepro-
cessing or stemming on the word lists before running
the cognate extraction algorithm. For SCE, n-gram
models are being prepared as described in section-
7. For all three measures, we calculate the distances
among all possible pairs of the languages.
For diachronic comparison, we selected modern
standard Hindi, medieval Hindi (actually, Avadhi)
and Sanskrit. The corpus for modern Hindi was the
same as that used for synchronic comparison. The
medieval Hindi we have experimented with is of two
different periods. These are the varieties used by
two great poets of that period, namely Jaayasi (1477-
1542 A.D.) and Tulsidas (1532-1623 A.D.). We took
some of their major works available in electronic
form as the corpora. For Sanskrit, we used the elec-
tronic version of Mahabharata (compiled during the
period 1000 B.C. to 500 A.D. approximately) as the
corpus. We calculate the distances among all pos-
sible pairs of the four varieties using the three mea-
sures. We also compare the ten modern languages
with Sanskrit using the same Mahabharata corpus.
For synchronic comparison, we first extract the
list of word types with frequencies from the corpus.
Then we rank them according to frequency. Top N
of these are retained. This is done because other-
wise a lot of less relevant word types like proper
nouns get included. We are interested in compar-
ing the core vocabulary of languages. The assump-
tion is that words in the core vocabulary are likely
to be more frequent. Another reason for restricting
the experiments to the top N word types is that there
</bodyText>
<page confidence="0.996907">
44
</page>
<table confidence="0.999640035714285">
BN HI KN ML MR OR PA TA TE
AS 0.02 0.39 0.71 0.86 0.61 0.20 0.61 0.93 0.73
0.12 0.25 0.39 0.61 0.45 0.11 0.58 0.95 0.46
0.05 0.30 0.51 0.50 0.43 0.18 0.42 0.70 0.64
BN 0.32 0.68 0.86 0.57 0.07 0.56 0.96 0.70
0.29 0.42 0.64 0.42 0.05 0.56 0.90 0.50
0.29 0.47 0.45 0.43 0.14 0.42 0.74 0.43
HI 0.61 0.81 0.42 0.40 0.20 0.93 0.61
0.17 0.56 0.16 0.27 0.16 0.87 0.38
0.43 0.46 0.16 0.33 0.20 0.74 0.34
KN 0.77 0.68 0.75 0.73 0.88 0.53
0.45 0.17 0.31 0.50 0.82 0.25
0.18 0.38 0.52 0.58 0.42 0.09
ML 0.89 0.88 0.88 0.62 0.72
0.65 0.59 0.77 0.56 0.31
0.42 0.53 0.55 0.07 0.19
MR 0.64 0.52 0.95 0.68
0.40 0.37 0.94 0.46
0.34 0.39 0.60 0.30
OR 0.63 0.98 0.74
0.45 0.89 0.44
0.65 0.83 0.64
PA 0.90 0.71
0.90 0.59
0.92 0.48
TA 0.85
0.81
0.39
</table>
<tableCaption confidence="0.998476">
Table 1: Inter-language comparison among ten ma-
</tableCaption>
<bodyText confidence="0.9725838">
jor South Asian languages using three corpus based
measures. The values have been normalized and
scaled to be somewhat comparable. Each cell con-
tains three values: by CCD, PDC and SCE.
are huge differences in sizes of corpora of different
languages. In the next step we identify the cognates
among these word lists. No language specific fea-
tures or thresholds are used. Only common thresh-
olds are used. We now branch out to using either
CCD or PDC.
The method used for diachronic comparison is
similar except that N is much smaller because the
amount of classical corpus being used (Jaayasi, Tul-
sidas) is also much smaller. Two letter codes are
used for ten languages and four varieties2.
</bodyText>
<subsectionHeader confidence="0.979665">
12 Analysis of Results
</subsectionHeader>
<bodyText confidence="0.999798">
The results of our experiments are shown tables 1
to 3 and figures 1 and 2. Table-1 shows the dis-
tances among pairs of languages using the three
</bodyText>
<footnote confidence="0.826797">
2AS: Assamese, BN: Bengali, HI: Hindi, KN: Kannada,
ML: Malayalam, MR: Marathi, OR: Oriya, PA: Punjabi,
TA: Tamil, TE: Telugu, TL: Avadhi (Tulsidas), JY: Avadhi
(Jaayasi), MB: Sanskrit (Mahabharata)
</footnote>
<bodyText confidence="0.999948">
measures. Figure-1 shows a graph showing the dis-
tances according to CCD and PDC. Figure-2 shows
the effect of the size of word lists (N) on com-
parison for three linguistically close language pairs.
Table-2 shows the comparison of ten languages with
Sanskrit. Table-3 gives the diachronic comparison
among four historical varieties.
</bodyText>
<subsectionHeader confidence="0.626673">
12.1 Synchronic Comparison
</subsectionHeader>
<bodyText confidence="0.999384307692308">
As table-1 shows, all three measures give results
which correspond well to the linguistic knowledge
about differences among these languages. Cognate
based measures give better results, but even the n-
gram based measure gives good results. However,
there are some differences among the values ob-
tained with different measures. These differences
are also in accordance with linguistic insights. For
example, the distance between Hindi and Telugu
was given as 0.61 by CCD and 0.38 by PDC. Simi-
larly, the distance between Hindi and Kannada was
given as 0.61 by CCD and 0.17 by PDC. These val-
ues, in relative terms, indicate that the number of
cognates between these languages is in the medium
range as compared to other pairs. But less PDC cost
shows that top N cognates are very similar. This
is because most cognates are tatsam words directly
borrowed from Sanskrit without any change.
The results presented in the table have been nor-
malized on all language pairs using the maximum
and minimum cost. The results would be differ-
ent and more comparable if we normalize over lan-
guage families (Indo-Iranian and Dravidian). With
such normalization, Punjabi-Oriya and Marathi-
Assamese are identified as the farthest language
pairs with costs of 0.92 and 0.90, respectively. This
corresponds well with the actual geographical and
linguistic distances.
While comparing with Sanskrit, it is clear that
different languages have different levels of cognate
coverage. However, except for Punjabi and Tamil,
all languages have very similar PDC cost with the
Mahabharata corpus. This again shows that the
closest cognates among these languages are tatsam
words. These results agree well with linguistic
knowledge, even though the Sanskrit corpus (Ma-
habharata) is highly biased.
Figure-1 makes the results clearer. It shows that
just by connecting each node to its nearest two
</bodyText>
<page confidence="0.996253">
45
</page>
<table confidence="0.978237333333333">
Distance AS BN HI KN ML MR OR PA TA TE
CCD 0.71 0.70 0.65 0.78 0.87 0.73 0.71 0.78 0.94 0.77
PDC 0.37 0.38 0.40 0.43 0.37 0.41 0.37 0.50 0.63 0.30
</table>
<tableCaption confidence="0.99988">
Table 2: Comparison with Sanskrit (Mahabharata)
</tableCaption>
<figureCaption confidence="0.986615">
Figure 2: Effect of the size of word lists on inter-
language comparison.
</figureCaption>
<table confidence="0.9991799">
TL JY MB
HI 0.45 0.54 0.82
0.45 0.42 0.70
0.64 0.56 0.49
TL 0.01 0.84
0.02 0.72
0.16 0.91
JY 0.98
0.95
0.81
</table>
<tableCaption confidence="0.9486015">
Table 3: Diachronic comparison among four histor-
ical varieties.
</tableCaption>
<bodyText confidence="0.999882846153846">
neighbors we can get a very good graphical repre-
sentation of the differences among languages. It also
shows that different measures capture different as-
pects. For example, CCD fails to connect Marathi
with Kannada and Kannada with Malayalam. Sim-
ilarly, PDC fails to connect Bengali with Hindi.
We get this missing information by combining the
graphs obtained with the two measures. More so-
phisticated methods for creating such graphs may
give better results. Note that the Hindi-Telugu and
Marathi-Kannada connections are valid as these lan-
guage pairs are close, even though they are not ge-
netically related. The results indicate closeness be-
tween two languages, but they do not distinguish be-
tween inheritance and borrowing.
We also experimented with several word list sizes.
In figure-2 the CCD values are plotted against word
list sizes for three close language pairs. There is
variation for Hindi-Punjabi and Malayalam-Telugu,
but not for Assamese-Bengali. The following obser-
vations can be derived from the three lines on the
plot. Malayalam-Telugu share a lot of common core
words but not less common words. Hindi-Punjabi
share a lot of less common words, but core words
are not exactly similar. Finally, Assamese-Bengali
share both core as well as less common words.
</bodyText>
<sectionHeader confidence="0.732582" genericHeader="method">
12.2 Diachronic Comparison
</sectionHeader>
<bodyText confidence="0.999963333333333">
Table-4 shows the results. We can see that Hindi is
closer to Tulsidas than to Jaayasi by the CCD mea-
sure. PDC gives almost similar results for both. Tul-
sidas and Jaayasi are the nearest. Tulsidas is much
nearer to Mahabharata than Jaayasi, chiefly because
Tulsidas’ language has more Sanskrit origin words.
Our results put Tulsidas nearest to Hindi, followed
by Jaayasi and then Sanskrit. This is historically as
well as linguistically correct.
</bodyText>
<sectionHeader confidence="0.91646" genericHeader="conclusions">
13 Conclusions and Further Work
</sectionHeader>
<bodyText confidence="0.999157875">
In this paper we first discussed the possibility and
validity of using corpus based measures for compar-
ative study of languages. We presented some ar-
guments in favor of this possibility. We then de-
scribed three corpus based measures for comparative
study of languages. The first measure was symmet-
ric cross entropy of letter n-grams. This measure
uses the least amount of linguistic information. The
second and third measures were cognate coverage
distance and phonetic distance of cognates, respec-
tively. These two are more linguistically grounded.
Using these measures, we presented a synchronic
comparison of ten major South Asian languages and
a diachronic comparison of four historical varieties.
The results of our experiments show that even these
simple measures based on crosslingual comparison
</bodyText>
<page confidence="0.99769">
46
</page>
<bodyText confidence="0.999976">
and on the data extracted from not very representa-
tive and noisy corpora can be used for obtaining or
validating useful linguistic insights about language
divergence, classification etc.
These measures can be tried for more languages
to see whether they have any validity for less related
languages than the languages we experimented with.
We can also try to design measures and find meth-
ods for distinguishing between borrowed and inher-
ited words. Proper combination of synchronic and
diachronic comparison might help us in doing this.
Other possible applications could be for language re-
construction, classification, dialectology etc.
Better versions of the two cognate based measures
can be defined by using the idea of confusion prob-
abilities (Ellison and Kirby, 2006) and the idea of
distributional similarity. If intra-lingual comparison
is more meaningful than inter-lingual comparison,
then these modified versions should be even more
useful for comparative study of languages.
</bodyText>
<sectionHeader confidence="0.999453" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999905287671233">
Gary Adams and Philip Resnik. 1997. A language
identification application built on the Java client-server
platform. In Jill Burstein and Claudia Leacock, ed-
itors, From Research to Commercial Applications:
Making NLP Work in Practice, pages 43–47. Associa-
tion for Computational Linguistics.
William B. Cavnar and John M. Trenkle. 1994. N-gram-
based text categorization. In Proceedings of SDAIR-
94, 3rd Annual Symposium on Document Analysis and
Information Retrieval, pages 161–175, Las Vegas, US.
I. Dyen, J.B. Kruskal, and P. Black. 1992. An
indo-european classification: A lexicostatistical exper-
iment. In Transactions of the American Philosophical
Society, 82:1-132.
T. Mark Ellison and Simon Kirby. 2006. Measuring lan-
guage divergence by intra-lexical comparison. In Pro-
ceedings of the 21st International Conference on Com-
putational Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, Sydney,
Australia. Association for Computational Linguistics.
M. B. Emeneau. 1956. India as a linguistic area. In
Linguistics 32:3-16.
W. Heeringa, P. Kleiweg, C. Gooskens, and J. Nerbonne.
2006. Evaluation of String Distance Algorithms for
Dialectology. In Proc. ofACL Workshop on Linguistic
Distances.
G. Kondrak and T. Sherif. 2006. Evaluation of Several
Phonetic Similarity Algorithms on the Task of Cognate
Identification. In Proc. ofACL Workshop on Linguistic
Distances.
Grzegorz Kondrak. 2002. Algorithms for language re-
construction. Ph.D. thesis. Adviser-Graeme Hirst.
April McMahon and Robert McMahon. 2005. Lan-
guage Classification by the Numbers. Oxford Univer-
sity Press, Oxford.
Luay Nakleh, Don Ringe, and Tandy Warnow. 2005.
Perfect phylogentic networks: A new methodology for
reconstructing the evolutionary history of natural lan-
guages. pages 81–2:382–420.
J. Nerbonne and W. Heeringa. 1997. Measuring dialect
distance phonetically. In Proceedings of SIGPHON-
97: 3rd Meeting of the ACL Special Interest Group in
Computational Phonology.
J. Nerbonne. 2005. Review of ‘language classification
by the numbers’ by april mcmahon and robert mcma-
hon.
B. Port and A. Leary. 2005. Against formal phonology.
pages 81(4):927–964.
M. Remmel. 1980. Computers in the historical phonetics
and phonology of Balto-Finnic languages: problems
and perspectives. In Communication pr´esent´ee au 5th
International Finno-Ugric Congress, Turku.
A. Ribeiro, G. Dias, G. Lopes, and J. Mexia. 2001. Cog-
nates alignment. Machine Translation Summit VIII,
Machine Translation in The Information Age, pages
287–292.
Duan sheng Chen and Zheng kai Liu. 2003. A novel
approach to detect and correct highlighted face re-
gion in color image. In AVSS ’03: Proceedings of
the IEEE Conference on Advanced Video and Signal
Based Surveillance, page 7, Washington, DC, USA.
IEEE Computer Society.
Anil Kumar Singh. 2006a. A computational phonetic
model for indian language scripts. In Constraints on
Spelling Changes: Fifth International Workshop on
Writing Systems, Nijmegen, The Netherlands.
Anil Kumar Singh. 2006b. Study of some distance mea-
sures for language and encoding identification. In Pro-
ceedings of ACL 2006 Workshop on Linguistic Dis-
tance, Sydney, Australia.
M. Swadesh. 1952. Lexico-dating of prehistoric ethnic
contacts. In Proceedings of the American philosophi-
cal society, 96(4).
</reference>
<page confidence="0.999492">
47
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.180428">
<title confidence="0.993413">Can Corpus Based Measures be Used for Comparative Study of Languages?</title>
<author confidence="0.988219">Anil Kumar</author>
<affiliation confidence="0.983923">Language Tech. Research Int’l Inst. of Information</affiliation>
<address confidence="0.800288">Hyderabad,</address>
<email confidence="0.987115">anil@research.iiit.net</email>
<affiliation confidence="0.7276115">Harshit Language Tech. Research Int’l Inst. of Information Hyderabad,</affiliation>
<email confidence="0.999858">surana.h@gmail.com</email>
<abstract confidence="0.998091333333333">Quantitative measurement of inter-language distance is a useful technique for studying diachronic and synchronic relations between languages. Such measures have been used successfully for purposes like deriving language taxonomies and language reconstruction, but they have mostly been applied to handcrafted word lists. Can we instead use corpus based measures for comparative study of languages? In this paper we try to answer this question. We use three corpus based measures and present the results obtained from them and show how these results relate to linguistic and historical knowledge. We argue that the answer is yes and that such studies can provide or validate linguistic and computational insights.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Gary Adams</author>
<author>Philip Resnik</author>
</authors>
<title>A language identification application built on the Java client-server platform.</title>
<date>1997</date>
<booktitle>In Jill Burstein and Claudia Leacock, editors, From Research to Commercial Applications: Making NLP Work in Practice,</booktitle>
<pages>43--47</pages>
<publisher>Association for Computational Linguistics.</publisher>
<contexts>
<context position="11927" citStr="Adams and Resnik, 1997" startWordPosition="1910" endWordPosition="1913">ve used symmetric cross entropy as such a measure: �dsce = (p(gl) log q(gm) + q(gm) log p(gl)) gl=gm (1) where p and q are the probability distributions for the two languages and gl and gm are n-grams in languages l and m, respectively. The disadvantage of this measure is that it does not use any linguistic (e.g., phonetic) information, but the advantage is that it can measure the similarity of distributions of n-grams. Such measures have proved to be very effective in automatically identifying languages of text, with accuracies nearing 100% for fairly small amounts of training and test data (Adams and Resnik, 1997; Singh, 2006b). 1This is based on the results obtained by Cavnar (Cavnar and Trenkle, 1994) and our own studies, which show that the top N (300 according to Cavnar) n-grams have a high correlation with the identity of the language. 42 8 Method for Cognate Identification The other two measures are based on cognates, inherited as well as borrowed. Both of them use an algorithm for identification of cognates. Many such algorithms have been proposed. Estimates of surface similarity can be used for finding cognate words across languages for related languages. By surface similarity we mean the orth</context>
</contexts>
<marker>Adams, Resnik, 1997</marker>
<rawString>Gary Adams and Philip Resnik. 1997. A language identification application built on the Java client-server platform. In Jill Burstein and Claudia Leacock, editors, From Research to Commercial Applications: Making NLP Work in Practice, pages 43–47. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William B Cavnar</author>
<author>John M Trenkle</author>
</authors>
<title>N-grambased text categorization.</title>
<date>1994</date>
<booktitle>In Proceedings of SDAIR94, 3rd Annual Symposium on Document Analysis and Information Retrieval,</booktitle>
<pages>161--175</pages>
<location>Las Vegas, US.</location>
<contexts>
<context position="12019" citStr="Cavnar and Trenkle, 1994" startWordPosition="1925" endWordPosition="1928">(gl)) gl=gm (1) where p and q are the probability distributions for the two languages and gl and gm are n-grams in languages l and m, respectively. The disadvantage of this measure is that it does not use any linguistic (e.g., phonetic) information, but the advantage is that it can measure the similarity of distributions of n-grams. Such measures have proved to be very effective in automatically identifying languages of text, with accuracies nearing 100% for fairly small amounts of training and test data (Adams and Resnik, 1997; Singh, 2006b). 1This is based on the results obtained by Cavnar (Cavnar and Trenkle, 1994) and our own studies, which show that the top N (300 according to Cavnar) n-grams have a high correlation with the identity of the language. 42 8 Method for Cognate Identification The other two measures are based on cognates, inherited as well as borrowed. Both of them use an algorithm for identification of cognates. Many such algorithms have been proposed. Estimates of surface similarity can be used for finding cognate words across languages for related languages. By surface similarity we mean the orthographic, phonetic and (possibly) morphological similarity of two words or strings. In spite</context>
</contexts>
<marker>Cavnar, Trenkle, 1994</marker>
<rawString>William B. Cavnar and John M. Trenkle. 1994. N-grambased text categorization. In Proceedings of SDAIR94, 3rd Annual Symposium on Document Analysis and Information Retrieval, pages 161–175, Las Vegas, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dyen</author>
<author>J B Kruskal</author>
<author>P Black</author>
</authors>
<title>An indo-european classification: A lexicostatistical experiment.</title>
<date>1992</date>
<journal>In Transactions of the American Philosophical Society,</journal>
<pages>82--1</pages>
<contexts>
<context position="1758" citStr="Dyen et al., 1992" startWordPosition="248" endWordPosition="251">community. As a result, semi-automatic crosslingual comparison of languages is also becoming a fruitful area of study. Among the fundamental tools for crosslingual comparison are measures of inter-language distances. In linguistics, the study of inter-language distances, especially for language classification, has a long history (Swadesh, 1952; Ellison and Kirby, 2006). Basically, the work on this problem has been along linguistic, archaeological and computational streams. Like in other disciplines, computational methods are increasingly being combined with other more conventional approaches (Dyen et al., 1992; Nerbonne and Heeringa, 1997; Kondrak, 2002; Ellison and Kirby, 2006). The work being presented in this paper belongs to the computational stream. Even in the computational stream, most of the previous work on inter-language distances had a strong linguistic dimension. For example, most of the quantitative measures of inter-language distance have been applied on handcrafted word lists (Swadesh, 1952; Dyen et al., 1992). However, with increasing use of computational techniques and the availability of electronic data, a natural question arises: Can languages be linguistically compared based on </context>
</contexts>
<marker>Dyen, Kruskal, Black, 1992</marker>
<rawString>I. Dyen, J.B. Kruskal, and P. Black. 1992. An indo-european classification: A lexicostatistical experiment. In Transactions of the American Philosophical Society, 82:1-132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mark Ellison</author>
<author>Simon Kirby</author>
</authors>
<title>Measuring language divergence by intra-lexical comparison.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia.</location>
<contexts>
<context position="1512" citStr="Ellison and Kirby, 2006" startWordPosition="213" endWordPosition="216"> historical knowledge. We argue that the answer is yes and that such studies can provide or validate linguistic and computational insights. 1 Introduction Crosslingual and multilingual processing is acquiring importance in the computational linguistics community. As a result, semi-automatic crosslingual comparison of languages is also becoming a fruitful area of study. Among the fundamental tools for crosslingual comparison are measures of inter-language distances. In linguistics, the study of inter-language distances, especially for language classification, has a long history (Swadesh, 1952; Ellison and Kirby, 2006). Basically, the work on this problem has been along linguistic, archaeological and computational streams. Like in other disciplines, computational methods are increasingly being combined with other more conventional approaches (Dyen et al., 1992; Nerbonne and Heeringa, 1997; Kondrak, 2002; Ellison and Kirby, 2006). The work being presented in this paper belongs to the computational stream. Even in the computational stream, most of the previous work on inter-language distances had a strong linguistic dimension. For example, most of the quantitative measures of inter-language distance have been</context>
<context position="4137" citStr="Ellison and Kirby, 2006" startWordPosition="629" endWordPosition="632"> paper by Kondrak and Sherif (Kondrak and Sherif, 2006). They have evaluated various phonetic similarity algorithms for aligning cognates. They found that learning based algorithms outperform manually constructed schemes, but only when large training data is used. A recent work on applications of such techniques for linguistic study is by Heeringa et al. (Heeringa et al., 2006). They performed a study on different variations of string distance algorithms for dialectology and concluded that order sensitivity is important while scaling with length is not. It may be noted that Ellison and Kirby (Ellison and Kirby, 2006) have shown that scaling by distance does give significantly better results. Nakleh et al. (Nakleh et al., 2005) have written about using phylogenetic techniques in historical linguistics as mentioned by Nerbonne (Nerbonne, 2005) in the review of the book titled ‘Language Classification by Numbers’ by McMahon and McMahon (McMahon and McMahon, 2005). All these works are about using quantitative techniques for language typology and classification etc. 3 Inter-Language Comparison Inter-language comparison is more general than measuring inter-language distance. In addition to the overall linguisti</context>
<context position="5472" citStr="Ellison and Kirby, 2006" startWordPosition="832" endWordPosition="835">ically and horizontally. Or it can be of specific phonetic features (Nerbonne, 2005; McMahon and McMahon, 2005). Quantitative measures for comparing languages can first be classified according to the form of data being compared, i.e., speech, written text or electronic text. Assuming that the text is in electronic form, the most common measures are based on word lists. These lists are usually prepared by linguists and they are often in some special notation, e.g. more or less a phonetic transcription. The measures can be based on inter-lingual or on intra-lingual comparison of phonetic forms (Ellison and Kirby, 2006). They may or may not use statistical techniques like measures of distributional similarity (cross entropy, KL-divergence, etc.). These characteristics of measures may imply some linguistic or psycholinguistic assumptions. One of these is about a common phonetic space. 4 Common Phonetic Space Language distance can be calculated through crosslingual as well as intra-lingual comparison. Many earlier attempts (Nerbonne and Heeringa, 1997; Kondrak, 2002) were based on crosslingual comparison of phonetic forms, but some researchers have argued against the possibility of obtaining meaningful results</context>
<context position="13455" citStr="Ellison and Kirby, 2006" startWordPosition="2154" endWordPosition="2157">they studied methods based on parallel text, we cannot use them directly. For identifying cognates, we are using the computational model of scripts or CPMS (Singh, 2006a). This model takes into account the characteristics of Brahmi origin scripts and calculates surface similarity in a fuzzy way. This is achieved by using a stepped distance function (SDF) and a dynamic programming (DP) algorithm. We have adapted the CPMS for identifying cognates. Different researchers have argued about the importance of order sensitivity and scaling in using string comparison algorithms (Heeringa et al., 2006; Ellison and Kirby, 2006). The CPMS takes both of these into account, as well as using knowledge about the script. In general, the distance between two strings can be defined as: clm = fp(wl, wm) (2) where fp is the function which calculates surface similarity based cost between the word wl of language l and the word wm of language m. Those word pairs are identified as cognates which have the least cost. 9 Cognate Coverage Distance (CCD) The second measure used by us is a corpus based estimate of the coverage of cognates across two languages. Cognate coverage is defined as the number of words (out of the vocabularies </context>
</contexts>
<marker>Ellison, Kirby, 2006</marker>
<rawString>T. Mark Ellison and Simon Kirby. 2006. Measuring language divergence by intra-lexical comparison. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, Sydney, Australia. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M B Emeneau</author>
</authors>
<title>India as a linguistic area.</title>
<date>1956</date>
<booktitle>In Linguistics</booktitle>
<pages>32--3</pages>
<contexts>
<context position="16380" citStr="Emeneau, 1956" startWordPosition="2676" endWordPosition="2677">D and PDC measures. The layout of the graph is modeled on the geographical locations of these languages. The connections among the nodes of the graph are obtained by joining each node to its two closest neighbors in terms of the values obtained by using the two measures. PAE PAE PAE 0.2011 EINE HIE 0.0211 ASE 0.1611 0.1211 ASE 0.5211 0.3211 HIE 0.3711 EINE ASE EINE HIE 0.0711 0.2011 0.0511 0.1111 0.4211 0.1611 ORE ORE ORE MRE 0.6111 MRE MRE 0.6111 0.1711 TEE TEE 0.1711 TEE KNE 0.5311 0.2511 KNE KNE 0.7211 0.8511 0.3111 0.8111 0.4511 MLE TAE 0.5611 TAE TAE MLE 0.6211 MLE a lot of similarities (Emeneau, 1956). The cognate words among them are loanwords as well as inherited words. In fact, the similarity among these languages is due to common origin (intra-family) as well as contact and borrowing over thousands of years (intra- and inter-family). Moreover, they also use scripts derived from the same origin (Brahmi), which allows us to use the CPMS for identifying cognates. The corpora used for these ten languages are all part of the CIIL (Central Institute of Indian Languages) multilingual corpus. This corpus is a collection of documents from different domains and is one of best known corpora for I</context>
</contexts>
<marker>Emeneau, 1956</marker>
<rawString>M. B. Emeneau. 1956. India as a linguistic area. In Linguistics 32:3-16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Heeringa</author>
<author>P Kleiweg</author>
<author>C Gooskens</author>
<author>J Nerbonne</author>
</authors>
<title>Evaluation of String Distance Algorithms for Dialectology. In</title>
<date>2006</date>
<booktitle>Proc. ofACL Workshop on Linguistic Distances.</booktitle>
<contexts>
<context position="3893" citStr="Heeringa et al., 2006" startWordPosition="589" endWordPosition="592"> of the ACL Special Interest Group in Computational Morphology and Phonology, pages 40–47, Prague, June 2007. c�2007 Association for Computational Linguistics will focus only on text. An example of a major work on text based similarity is the paper by Kondrak and Sherif (Kondrak and Sherif, 2006). They have evaluated various phonetic similarity algorithms for aligning cognates. They found that learning based algorithms outperform manually constructed schemes, but only when large training data is used. A recent work on applications of such techniques for linguistic study is by Heeringa et al. (Heeringa et al., 2006). They performed a study on different variations of string distance algorithms for dialectology and concluded that order sensitivity is important while scaling with length is not. It may be noted that Ellison and Kirby (Ellison and Kirby, 2006) have shown that scaling by distance does give significantly better results. Nakleh et al. (Nakleh et al., 2005) have written about using phylogenetic techniques in historical linguistics as mentioned by Nerbonne (Nerbonne, 2005) in the review of the book titled ‘Language Classification by Numbers’ by McMahon and McMahon (McMahon and McMahon, 2005). All </context>
<context position="13429" citStr="Heeringa et al., 2006" startWordPosition="2150" endWordPosition="2153">gnment. However, since they studied methods based on parallel text, we cannot use them directly. For identifying cognates, we are using the computational model of scripts or CPMS (Singh, 2006a). This model takes into account the characteristics of Brahmi origin scripts and calculates surface similarity in a fuzzy way. This is achieved by using a stepped distance function (SDF) and a dynamic programming (DP) algorithm. We have adapted the CPMS for identifying cognates. Different researchers have argued about the importance of order sensitivity and scaling in using string comparison algorithms (Heeringa et al., 2006; Ellison and Kirby, 2006). The CPMS takes both of these into account, as well as using knowledge about the script. In general, the distance between two strings can be defined as: clm = fp(wl, wm) (2) where fp is the function which calculates surface similarity based cost between the word wl of language l and the word wm of language m. Those word pairs are identified as cognates which have the least cost. 9 Cognate Coverage Distance (CCD) The second measure used by us is a corpus based estimate of the coverage of cognates across two languages. Cognate coverage is defined as the number of words</context>
</contexts>
<marker>Heeringa, Kleiweg, Gooskens, Nerbonne, 2006</marker>
<rawString>W. Heeringa, P. Kleiweg, C. Gooskens, and J. Nerbonne. 2006. Evaluation of String Distance Algorithms for Dialectology. In Proc. ofACL Workshop on Linguistic Distances.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Kondrak</author>
<author>T Sherif</author>
</authors>
<title>Evaluation of Several Phonetic Similarity Algorithms on the Task of Cognate Identification.</title>
<date>2006</date>
<booktitle>In Proc. ofACL Workshop on Linguistic Distances.</booktitle>
<contexts>
<context position="3568" citStr="Kondrak and Sherif, 2006" startWordPosition="538" endWordPosition="541"> show that even noisy corpora can be used for comparative study of languages. Different measures can give different kinds of insights. 2 Related Work Typology or history of languages can be studied using spoken data or text. There has been work on the former (Remmel, 1980; Kondrak, 2002), but we 40 Proceedings of Ninth Meeting of the ACL Special Interest Group in Computational Morphology and Phonology, pages 40–47, Prague, June 2007. c�2007 Association for Computational Linguistics will focus only on text. An example of a major work on text based similarity is the paper by Kondrak and Sherif (Kondrak and Sherif, 2006). They have evaluated various phonetic similarity algorithms for aligning cognates. They found that learning based algorithms outperform manually constructed schemes, but only when large training data is used. A recent work on applications of such techniques for linguistic study is by Heeringa et al. (Heeringa et al., 2006). They performed a study on different variations of string distance algorithms for dialectology and concluded that order sensitivity is important while scaling with length is not. It may be noted that Ellison and Kirby (Ellison and Kirby, 2006) have shown that scaling by dis</context>
</contexts>
<marker>Kondrak, Sherif, 2006</marker>
<rawString>G. Kondrak and T. Sherif. 2006. Evaluation of Several Phonetic Similarity Algorithms on the Task of Cognate Identification. In Proc. ofACL Workshop on Linguistic Distances.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grzegorz Kondrak</author>
</authors>
<title>Algorithms for language reconstruction.</title>
<date>2002</date>
<journal>Ph.D. thesis. Adviser-Graeme Hirst.</journal>
<contexts>
<context position="1802" citStr="Kondrak, 2002" startWordPosition="256" endWordPosition="257">ual comparison of languages is also becoming a fruitful area of study. Among the fundamental tools for crosslingual comparison are measures of inter-language distances. In linguistics, the study of inter-language distances, especially for language classification, has a long history (Swadesh, 1952; Ellison and Kirby, 2006). Basically, the work on this problem has been along linguistic, archaeological and computational streams. Like in other disciplines, computational methods are increasingly being combined with other more conventional approaches (Dyen et al., 1992; Nerbonne and Heeringa, 1997; Kondrak, 2002; Ellison and Kirby, 2006). The work being presented in this paper belongs to the computational stream. Even in the computational stream, most of the previous work on inter-language distances had a strong linguistic dimension. For example, most of the quantitative measures of inter-language distance have been applied on handcrafted word lists (Swadesh, 1952; Dyen et al., 1992). However, with increasing use of computational techniques and the availability of electronic data, a natural question arises: Can languages be linguistically compared based on word lists extracted from corpora. A natural</context>
<context position="3231" citStr="Kondrak, 2002" startWordPosition="486" endWordPosition="487">of words in corpora, we propose two corpus based distance measures. They internally use a more linguistically grounded distance measure for comparing strings. We also present the results obtained with one purely statistical measure, just to show that even naive corpus based measures can be useful. The main contribution is to show that even noisy corpora can be used for comparative study of languages. Different measures can give different kinds of insights. 2 Related Work Typology or history of languages can be studied using spoken data or text. There has been work on the former (Remmel, 1980; Kondrak, 2002), but we 40 Proceedings of Ninth Meeting of the ACL Special Interest Group in Computational Morphology and Phonology, pages 40–47, Prague, June 2007. c�2007 Association for Computational Linguistics will focus only on text. An example of a major work on text based similarity is the paper by Kondrak and Sherif (Kondrak and Sherif, 2006). They have evaluated various phonetic similarity algorithms for aligning cognates. They found that learning based algorithms outperform manually constructed schemes, but only when large training data is used. A recent work on applications of such techniques for </context>
<context position="5926" citStr="Kondrak, 2002" startWordPosition="899" endWordPosition="900">.g. more or less a phonetic transcription. The measures can be based on inter-lingual or on intra-lingual comparison of phonetic forms (Ellison and Kirby, 2006). They may or may not use statistical techniques like measures of distributional similarity (cross entropy, KL-divergence, etc.). These characteristics of measures may imply some linguistic or psycholinguistic assumptions. One of these is about a common phonetic space. 4 Common Phonetic Space Language distance can be calculated through crosslingual as well as intra-lingual comparison. Many earlier attempts (Nerbonne and Heeringa, 1997; Kondrak, 2002) were based on crosslingual comparison of phonetic forms, but some researchers have argued against the possibility of obtaining meaningful results from crosslingual comparison of phonetic forms. This is related to the idea of a common phonetic space. Port and Leary (Port and Leary, 2005) have argued against it. Ellison and Kirby (Ellison and Kirby, 2006) argue that even if there is a common space, language specific categorization of sound often restructures this space. They conclude that if there is no language-independent common phonetic space with an equally common similarity measure, there </context>
</contexts>
<marker>Kondrak, 2002</marker>
<rawString>Grzegorz Kondrak. 2002. Algorithms for language reconstruction. Ph.D. thesis. Adviser-Graeme Hirst.</rawString>
</citation>
<citation valid="true">
<authors>
<author>April McMahon</author>
<author>Robert McMahon</author>
</authors>
<title>Language Classification by the Numbers.</title>
<date>2005</date>
<publisher>Oxford University Press,</publisher>
<location>Oxford.</location>
<contexts>
<context position="4487" citStr="McMahon and McMahon, 2005" startWordPosition="684" endWordPosition="687">a et al. (Heeringa et al., 2006). They performed a study on different variations of string distance algorithms for dialectology and concluded that order sensitivity is important while scaling with length is not. It may be noted that Ellison and Kirby (Ellison and Kirby, 2006) have shown that scaling by distance does give significantly better results. Nakleh et al. (Nakleh et al., 2005) have written about using phylogenetic techniques in historical linguistics as mentioned by Nerbonne (Nerbonne, 2005) in the review of the book titled ‘Language Classification by Numbers’ by McMahon and McMahon (McMahon and McMahon, 2005). All these works are about using quantitative techniques for language typology and classification etc. 3 Inter-Language Comparison Inter-language comparison is more general than measuring inter-language distance. In addition to the overall linguistic distance, the comparison can be of more specific characteristics like the proportion of cognates derived vertically and horizontally. Or it can be of specific phonetic features (Nerbonne, 2005; McMahon and McMahon, 2005). Quantitative measures for comparing languages can first be classified according to the form of data being compared, i.e., spee</context>
</contexts>
<marker>McMahon, McMahon, 2005</marker>
<rawString>April McMahon and Robert McMahon. 2005. Language Classification by the Numbers. Oxford University Press, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luay Nakleh</author>
<author>Don Ringe</author>
<author>Tandy Warnow</author>
</authors>
<title>Perfect phylogentic networks: A new methodology for reconstructing the evolutionary history of natural languages.</title>
<date>2005</date>
<pages>81--2</pages>
<contexts>
<context position="4249" citStr="Nakleh et al., 2005" startWordPosition="647" endWordPosition="650">for aligning cognates. They found that learning based algorithms outperform manually constructed schemes, but only when large training data is used. A recent work on applications of such techniques for linguistic study is by Heeringa et al. (Heeringa et al., 2006). They performed a study on different variations of string distance algorithms for dialectology and concluded that order sensitivity is important while scaling with length is not. It may be noted that Ellison and Kirby (Ellison and Kirby, 2006) have shown that scaling by distance does give significantly better results. Nakleh et al. (Nakleh et al., 2005) have written about using phylogenetic techniques in historical linguistics as mentioned by Nerbonne (Nerbonne, 2005) in the review of the book titled ‘Language Classification by Numbers’ by McMahon and McMahon (McMahon and McMahon, 2005). All these works are about using quantitative techniques for language typology and classification etc. 3 Inter-Language Comparison Inter-language comparison is more general than measuring inter-language distance. In addition to the overall linguistic distance, the comparison can be of more specific characteristics like the proportion of cognates derived verti</context>
</contexts>
<marker>Nakleh, Ringe, Warnow, 2005</marker>
<rawString>Luay Nakleh, Don Ringe, and Tandy Warnow. 2005. Perfect phylogentic networks: A new methodology for reconstructing the evolutionary history of natural languages. pages 81–2:382–420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nerbonne</author>
<author>W Heeringa</author>
</authors>
<title>Measuring dialect distance phonetically.</title>
<date>1997</date>
<booktitle>In Proceedings of SIGPHON97: 3rd Meeting of the ACL Special Interest Group in Computational Phonology.</booktitle>
<contexts>
<context position="1787" citStr="Nerbonne and Heeringa, 1997" startWordPosition="252" endWordPosition="255">ult, semi-automatic crosslingual comparison of languages is also becoming a fruitful area of study. Among the fundamental tools for crosslingual comparison are measures of inter-language distances. In linguistics, the study of inter-language distances, especially for language classification, has a long history (Swadesh, 1952; Ellison and Kirby, 2006). Basically, the work on this problem has been along linguistic, archaeological and computational streams. Like in other disciplines, computational methods are increasingly being combined with other more conventional approaches (Dyen et al., 1992; Nerbonne and Heeringa, 1997; Kondrak, 2002; Ellison and Kirby, 2006). The work being presented in this paper belongs to the computational stream. Even in the computational stream, most of the previous work on inter-language distances had a strong linguistic dimension. For example, most of the quantitative measures of inter-language distance have been applied on handcrafted word lists (Swadesh, 1952; Dyen et al., 1992). However, with increasing use of computational techniques and the availability of electronic data, a natural question arises: Can languages be linguistically compared based on word lists extracted from cor</context>
<context position="5910" citStr="Nerbonne and Heeringa, 1997" startWordPosition="895" endWordPosition="898">n in some special notation, e.g. more or less a phonetic transcription. The measures can be based on inter-lingual or on intra-lingual comparison of phonetic forms (Ellison and Kirby, 2006). They may or may not use statistical techniques like measures of distributional similarity (cross entropy, KL-divergence, etc.). These characteristics of measures may imply some linguistic or psycholinguistic assumptions. One of these is about a common phonetic space. 4 Common Phonetic Space Language distance can be calculated through crosslingual as well as intra-lingual comparison. Many earlier attempts (Nerbonne and Heeringa, 1997; Kondrak, 2002) were based on crosslingual comparison of phonetic forms, but some researchers have argued against the possibility of obtaining meaningful results from crosslingual comparison of phonetic forms. This is related to the idea of a common phonetic space. Port and Leary (Port and Leary, 2005) have argued against it. Ellison and Kirby (Ellison and Kirby, 2006) argue that even if there is a common space, language specific categorization of sound often restructures this space. They conclude that if there is no language-independent common phonetic space with an equally common similarity</context>
</contexts>
<marker>Nerbonne, Heeringa, 1997</marker>
<rawString>J. Nerbonne and W. Heeringa. 1997. Measuring dialect distance phonetically. In Proceedings of SIGPHON97: 3rd Meeting of the ACL Special Interest Group in Computational Phonology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nerbonne</author>
</authors>
<title>Review of ‘language classification by the numbers’ by april mcmahon and robert mcmahon.</title>
<date>2005</date>
<contexts>
<context position="4366" citStr="Nerbonne, 2005" startWordPosition="666" endWordPosition="667">rge training data is used. A recent work on applications of such techniques for linguistic study is by Heeringa et al. (Heeringa et al., 2006). They performed a study on different variations of string distance algorithms for dialectology and concluded that order sensitivity is important while scaling with length is not. It may be noted that Ellison and Kirby (Ellison and Kirby, 2006) have shown that scaling by distance does give significantly better results. Nakleh et al. (Nakleh et al., 2005) have written about using phylogenetic techniques in historical linguistics as mentioned by Nerbonne (Nerbonne, 2005) in the review of the book titled ‘Language Classification by Numbers’ by McMahon and McMahon (McMahon and McMahon, 2005). All these works are about using quantitative techniques for language typology and classification etc. 3 Inter-Language Comparison Inter-language comparison is more general than measuring inter-language distance. In addition to the overall linguistic distance, the comparison can be of more specific characteristics like the proportion of cognates derived vertically and horizontally. Or it can be of specific phonetic features (Nerbonne, 2005; McMahon and McMahon, 2005). Quant</context>
</contexts>
<marker>Nerbonne, 2005</marker>
<rawString>J. Nerbonne. 2005. Review of ‘language classification by the numbers’ by april mcmahon and robert mcmahon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Port</author>
<author>A Leary</author>
</authors>
<title>Against formal phonology.</title>
<date>2005</date>
<pages>81--4</pages>
<contexts>
<context position="6214" citStr="Port and Leary, 2005" startWordPosition="941" endWordPosition="944">, etc.). These characteristics of measures may imply some linguistic or psycholinguistic assumptions. One of these is about a common phonetic space. 4 Common Phonetic Space Language distance can be calculated through crosslingual as well as intra-lingual comparison. Many earlier attempts (Nerbonne and Heeringa, 1997; Kondrak, 2002) were based on crosslingual comparison of phonetic forms, but some researchers have argued against the possibility of obtaining meaningful results from crosslingual comparison of phonetic forms. This is related to the idea of a common phonetic space. Port and Leary (Port and Leary, 2005) have argued against it. Ellison and Kirby (Ellison and Kirby, 2006) argue that even if there is a common space, language specific categorization of sound often restructures this space. They conclude that if there is no language-independent common phonetic space with an equally common similarity measure, there can be no principled approach to comparing forms in one language with another. They suggest that language-internal comparison of forms is better and psychologically more well-grounded. This may be true, but should we really abandon the approach based on crosslingual comparison? As even E</context>
</contexts>
<marker>Port, Leary, 2005</marker>
<rawString>B. Port and A. Leary. 2005. Against formal phonology. pages 81(4):927–964.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Remmel</author>
</authors>
<title>Computers in the historical phonetics and phonology of Balto-Finnic languages: problems and perspectives.</title>
<date>1980</date>
<booktitle>In Communication pr´esent´ee au 5th International Finno-Ugric Congress,</booktitle>
<location>Turku.</location>
<contexts>
<context position="3215" citStr="Remmel, 1980" startWordPosition="484" endWordPosition="485"> on the basis of words in corpora, we propose two corpus based distance measures. They internally use a more linguistically grounded distance measure for comparing strings. We also present the results obtained with one purely statistical measure, just to show that even naive corpus based measures can be useful. The main contribution is to show that even noisy corpora can be used for comparative study of languages. Different measures can give different kinds of insights. 2 Related Work Typology or history of languages can be studied using spoken data or text. There has been work on the former (Remmel, 1980; Kondrak, 2002), but we 40 Proceedings of Ninth Meeting of the ACL Special Interest Group in Computational Morphology and Phonology, pages 40–47, Prague, June 2007. c�2007 Association for Computational Linguistics will focus only on text. An example of a major work on text based similarity is the paper by Kondrak and Sherif (Kondrak and Sherif, 2006). They have evaluated various phonetic similarity algorithms for aligning cognates. They found that learning based algorithms outperform manually constructed schemes, but only when large training data is used. A recent work on applications of such</context>
</contexts>
<marker>Remmel, 1980</marker>
<rawString>M. Remmel. 1980. Computers in the historical phonetics and phonology of Balto-Finnic languages: problems and perspectives. In Communication pr´esent´ee au 5th International Finno-Ugric Congress, Turku.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ribeiro</author>
<author>G Dias</author>
<author>G Lopes</author>
<author>J Mexia</author>
</authors>
<title>Cognates alignment. Machine Translation Summit VIII, Machine Translation in The Information Age,</title>
<date>2001</date>
<pages>287--292</pages>
<contexts>
<context position="12755" citStr="Ribeiro et al., 2001" startWordPosition="2044" endWordPosition="2047">identity of the language. 42 8 Method for Cognate Identification The other two measures are based on cognates, inherited as well as borrowed. Both of them use an algorithm for identification of cognates. Many such algorithms have been proposed. Estimates of surface similarity can be used for finding cognate words across languages for related languages. By surface similarity we mean the orthographic, phonetic and (possibly) morphological similarity of two words or strings. In spite of the name, surface similarity is deeper than string similarity as calculated by edit distances. Ribeiro et al. (Ribeiro et al., 2001) have surveyed some of the algorithms for cognate alignment. However, since they studied methods based on parallel text, we cannot use them directly. For identifying cognates, we are using the computational model of scripts or CPMS (Singh, 2006a). This model takes into account the characteristics of Brahmi origin scripts and calculates surface similarity in a fuzzy way. This is achieved by using a stepped distance function (SDF) and a dynamic programming (DP) algorithm. We have adapted the CPMS for identifying cognates. Different researchers have argued about the importance of order sensitivit</context>
</contexts>
<marker>Ribeiro, Dias, Lopes, Mexia, 2001</marker>
<rawString>A. Ribeiro, G. Dias, G. Lopes, and J. Mexia. 2001. Cognates alignment. Machine Translation Summit VIII, Machine Translation in The Information Age, pages 287–292.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Duan sheng Chen</author>
<author>Zheng kai Liu</author>
</authors>
<title>A novel approach to detect and correct highlighted face region in color image.</title>
<date>2003</date>
<booktitle>In AVSS ’03: Proceedings of the IEEE Conference on Advanced Video and Signal Based Surveillance, page 7,</booktitle>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<marker>Chen, Liu, 2003</marker>
<rawString>Duan sheng Chen and Zheng kai Liu. 2003. A novel approach to detect and correct highlighted face region in color image. In AVSS ’03: Proceedings of the IEEE Conference on Advanced Video and Signal Based Surveillance, page 7, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anil Kumar Singh</author>
</authors>
<title>A computational phonetic model for indian language scripts.</title>
<date>2006</date>
<booktitle>In Constraints on Spelling Changes: Fifth International Workshop on Writing Systems,</booktitle>
<location>Nijmegen, The Netherlands.</location>
<contexts>
<context position="10866" citStr="Singh, 2006" startWordPosition="1731" endWordPosition="1732"> section-7. This category of measures have to incorporate more linguistic information if they are to provide good results. Designing such measures can be a challenging problem as we will be mainly relying on the corpus for our information. Knowledge about similarities and differences of writing systems can play an important role here. The two cognate based measures described in sections 9 and 10 are an attempt at this. But first we describe a simple n-gram based measure. 7 Symmetric Cross Entropy (SCE) The first measure is purely a letter n-gram based measure similar to the one used by Singh (Singh, 2006b) for language and encoding identification. To calculate the distance, we first prepare letter 5-gram models from the corpora of the languages to be compared. Then we combine n-grams of all orders and rank them according to their probability in descending order. Only the top N n-grams are retained and the rest are pruned. 1 Now we have two probability distributions which can be compared by a measure of distributional similarity. We have used symmetric cross entropy as such a measure: �dsce = (p(gl) log q(gm) + q(gm) log p(gl)) gl=gm (1) where p and q are the probability distributions for the </context>
<context position="12999" citStr="Singh, 2006" startWordPosition="2086" endWordPosition="2087"> of surface similarity can be used for finding cognate words across languages for related languages. By surface similarity we mean the orthographic, phonetic and (possibly) morphological similarity of two words or strings. In spite of the name, surface similarity is deeper than string similarity as calculated by edit distances. Ribeiro et al. (Ribeiro et al., 2001) have surveyed some of the algorithms for cognate alignment. However, since they studied methods based on parallel text, we cannot use them directly. For identifying cognates, we are using the computational model of scripts or CPMS (Singh, 2006a). This model takes into account the characteristics of Brahmi origin scripts and calculates surface similarity in a fuzzy way. This is achieved by using a stepped distance function (SDF) and a dynamic programming (DP) algorithm. We have adapted the CPMS for identifying cognates. Different researchers have argued about the importance of order sensitivity and scaling in using string comparison algorithms (Heeringa et al., 2006; Ellison and Kirby, 2006). The CPMS takes both of these into account, as well as using knowledge about the script. In general, the distance between two strings can be de</context>
</contexts>
<marker>Singh, 2006</marker>
<rawString>Anil Kumar Singh. 2006a. A computational phonetic model for indian language scripts. In Constraints on Spelling Changes: Fifth International Workshop on Writing Systems, Nijmegen, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anil Kumar Singh</author>
</authors>
<title>Study of some distance measures for language and encoding identification.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL 2006 Workshop on Linguistic Distance,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="10866" citStr="Singh, 2006" startWordPosition="1731" endWordPosition="1732"> section-7. This category of measures have to incorporate more linguistic information if they are to provide good results. Designing such measures can be a challenging problem as we will be mainly relying on the corpus for our information. Knowledge about similarities and differences of writing systems can play an important role here. The two cognate based measures described in sections 9 and 10 are an attempt at this. But first we describe a simple n-gram based measure. 7 Symmetric Cross Entropy (SCE) The first measure is purely a letter n-gram based measure similar to the one used by Singh (Singh, 2006b) for language and encoding identification. To calculate the distance, we first prepare letter 5-gram models from the corpora of the languages to be compared. Then we combine n-grams of all orders and rank them according to their probability in descending order. Only the top N n-grams are retained and the rest are pruned. 1 Now we have two probability distributions which can be compared by a measure of distributional similarity. We have used symmetric cross entropy as such a measure: �dsce = (p(gl) log q(gm) + q(gm) log p(gl)) gl=gm (1) where p and q are the probability distributions for the </context>
<context position="12999" citStr="Singh, 2006" startWordPosition="2086" endWordPosition="2087"> of surface similarity can be used for finding cognate words across languages for related languages. By surface similarity we mean the orthographic, phonetic and (possibly) morphological similarity of two words or strings. In spite of the name, surface similarity is deeper than string similarity as calculated by edit distances. Ribeiro et al. (Ribeiro et al., 2001) have surveyed some of the algorithms for cognate alignment. However, since they studied methods based on parallel text, we cannot use them directly. For identifying cognates, we are using the computational model of scripts or CPMS (Singh, 2006a). This model takes into account the characteristics of Brahmi origin scripts and calculates surface similarity in a fuzzy way. This is achieved by using a stepped distance function (SDF) and a dynamic programming (DP) algorithm. We have adapted the CPMS for identifying cognates. Different researchers have argued about the importance of order sensitivity and scaling in using string comparison algorithms (Heeringa et al., 2006; Ellison and Kirby, 2006). The CPMS takes both of these into account, as well as using knowledge about the script. In general, the distance between two strings can be de</context>
</contexts>
<marker>Singh, 2006</marker>
<rawString>Anil Kumar Singh. 2006b. Study of some distance measures for language and encoding identification. In Proceedings of ACL 2006 Workshop on Linguistic Distance, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Swadesh</author>
</authors>
<title>Lexico-dating of prehistoric ethnic contacts.</title>
<date>1952</date>
<booktitle>In Proceedings of the American philosophical society,</booktitle>
<pages>96--4</pages>
<contexts>
<context position="1486" citStr="Swadesh, 1952" startWordPosition="211" endWordPosition="212"> linguistic and historical knowledge. We argue that the answer is yes and that such studies can provide or validate linguistic and computational insights. 1 Introduction Crosslingual and multilingual processing is acquiring importance in the computational linguistics community. As a result, semi-automatic crosslingual comparison of languages is also becoming a fruitful area of study. Among the fundamental tools for crosslingual comparison are measures of inter-language distances. In linguistics, the study of inter-language distances, especially for language classification, has a long history (Swadesh, 1952; Ellison and Kirby, 2006). Basically, the work on this problem has been along linguistic, archaeological and computational streams. Like in other disciplines, computational methods are increasingly being combined with other more conventional approaches (Dyen et al., 1992; Nerbonne and Heeringa, 1997; Kondrak, 2002; Ellison and Kirby, 2006). The work being presented in this paper belongs to the computational stream. Even in the computational stream, most of the previous work on inter-language distances had a strong linguistic dimension. For example, most of the quantitative measures of inter-l</context>
</contexts>
<marker>Swadesh, 1952</marker>
<rawString>M. Swadesh. 1952. Lexico-dating of prehistoric ethnic contacts. In Proceedings of the American philosophical society, 96(4).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>