<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000702">
<title confidence="0.979891">
Partially Supervised Coreference Resolution for Opinion Summarization
through Structured Rule Learning
</title>
<author confidence="0.994449">
Veselin Stoyanov and Claire Cardie
</author>
<affiliation confidence="0.874673333333333">
Department of Computer Science
Cornell University
Ithaca, NY 14850, USA
</affiliation>
<email confidence="0.99915">
{ves,cardie}@cs.cornell.edu
</email>
<sectionHeader confidence="0.994807" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998332615384615">
Combining fine-grained opinion informa-
tion to produce opinion summaries is im-
portant for sentiment analysis applica-
tions. Toward that end, we tackle the
problem of source coreference resolution
– linking together source mentions that re-
fer to the same entity. The partially super-
vised nature of the problem leads us to de-
fine and approach it as the novel problem
of partially supervised clustering. We pro-
pose and evaluate a new algorithm for the
task of source coreference resolution that
outperforms competitive baselines.
</bodyText>
<sectionHeader confidence="0.998792" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999940722222223">
Sentiment analysis is concerned with extracting
attitudes, opinions, evaluations, and sentiment
from text. Work in this area has been motivated
by the desire to provide information analysis ap-
plications in the arenas of government, business,
and politics (e.g. Coglianese (2004)). Addition-
ally, sentiment analysis can augment existing NLP
applications such as question answering, informa-
tion retrieval, summarization, and clustering by
providing information about sentiment (e.g. Stoy-
anov et al. (2005), Riloff et al. (2005)). To date,
research in the area (see Related Work section)
has focused on the problem of extracting senti-
ment both at the document level (coarse-grained
sentiment information), and at the level of sen-
tences, clauses, or individual expressions (fine-
grained sentiment information).
In contrast, our work concerns the summa-
rization of fine-grained information about opin-
ions. In particular, while recent research ef-
forts have shown that fine-grained opinions (e.g.
Riloff and Wiebe (2003), Bethard et al. (2004),
Wiebe and Riloff (2005)) as well as their sources
(e.g. Bethard et al. (2004), Choi et al. (2005),
Kim and Hovy (2005)) can be extracted auto-
matically, little has been done to create opin-
ion summaries, where opinions from the same
source/target are combined, statistics are com-
puted for each source/target and multiple opinions
from the same source to the same target are ag-
gregated. A simple opinion summary is shown in
figure 1.1 We expect that this type of opinion sum-
mary, based on fine-grained opinion information,
will be important for information analysis applica-
tions in any domain where the analysis of opinions
is critical.
This paper addresses the problem of opinion
summarization by considering the creation of sim-
ple opinion summaries like those of figure 1. We
propose source coreference resolution — the task
of determining which mentions of opinion sources
refer to the same entity — as the primary mecha-
nism for identifying the set of opinions attributed
to each real-world source. For this type of sum-
mary, source coreference resolution constitutes an
integral step in the process of generating full opin-
ion summaries. For example, given the opinion
expressions of figure 1, their polarity, and the asso-
ciated opinion sources and targets, the bulk of the
resulting summary can be produced by recogniz-
ing that source mentions “Zacarias Moussaoui”,
“he”, “my”, and “Mr. Moussaoui” all refer to
the same person; and that source mentions “Mr.
Zerkin” and “Zerkin” refer to the same person.2
</bodyText>
<footnote confidence="0.9974435">
1For simplicity, the example summary does not contain
any source/target statistics.
2In addition, the summary would require the closely re-
lated task of target coreference resolution and a means for ag-
gregating the conflicting opinions from Zerkin toward Mous-
saoui.
</footnote>
<page confidence="0.959451">
336
</page>
<note confidence="0.85823">
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 336–344,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999906837837838">
At first glance, source coreference resolution
appears equivalent to the task of noun phrase
coreference resolution and therefore amenable to
traditional coreference resolution techniques (e.g.
Ng and Cardie (2002), Morton (2000)). We hy-
pothesize in Section 3, however, that the task is
likely to succumb to a better solution by treating
it in the context of a new machine learning set-
ting that we refer to as partially supervised clus-
tering. In particular, due to high coreference an-
notation costs, data sets that are annotated with
opinion information (like ours) do not typically in-
clude supervisory coreference information for all
noun phrases in a document (as would be required
for the application of traditional coreference reso-
lution techniques), but only for noun phrases that
act as opinion sources (or targets).
As a result, we define the task of partially su-
pervised clustering, the goal of which is to learn
a clustering function from a set of partially spec-
ified clustering examples (Section 4). We are not
aware of prior work on the problem of partially
supervised clustering and argue that it differs sub-
stantially from that of semi-supervised clustering.
We propose an algorithm for partially supervised
clustering that extends a rule learner with structure
information and is generally applicable to prob-
lems that fit the partially supervised clustering def-
inition (Section 5). We apply the algorithm to
the source coreference resolution task and evalu-
ate its performance on a standard sentiment analy-
sis data set that includes source coreference chains
(Section 6). We find that our algorithm outper-
forms highly competitive baselines by a consid-
erable margin – B3 score of 83.2 vs. 81.8 and
67.1 vs. 60.9 F1 score for the identification of
positive source coreference links.
</bodyText>
<sectionHeader confidence="0.999764" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.984103147058824">
Work relevant to our problem can be split into
three main areas – sentiment analysis, traditional
noun phrase coreference resolution, and super-
vised and weakly supervised clustering. Related
work in the former two areas is summarized briefly
below. Supervised and weakly supervised cluster-
ing approaches are discussed in Section 4.
Sentiment analysis. Much of the relevant re-
search in sentiment analysis addresses sentiment
classification, a text categorization task of extract-
ing opinion at the coarse-grained document level.
The goal in sentiment classification is to assign to
[Source Zacarias Moussaoui] [_ complained] at length today
about [Target his own lawyer], telling a federal court jury that
[Target he] was [_ more interested in achievingfame than sav-
ing Moussaoui’s life].
Mr. Moussaoui said he was appearing on the witness stand to
tell the truth. And one part of the truth, [Source he] said, is that
[Target sending him to prison for life] would be “[_ a greater
punishment] than being sentenced to death.”
“[_ [Target You] have put your interest ahead of [Source my]
life],” [Source Mr. Moussaoui] told his court-appointed lawyer
Gerald T. Zerkin.
...
But, [Source Mr. Zerkin] pressed [Target Mr. Moussaoui], was
it [_ not true] that he told his lawyers earlier not to involve
any Muslims in the defense, not to present any evidence that
might persuade the jurors to spare his life?
...
[Source Zerkin] seemed to be trying to show the jurors
that while [Target the defendant] is generally [+ an honest
individual], his conduct shows [Target he] is [_ not stable
mentally], and thus [_ undeserving] of [Target the ultimate
punishment].
</bodyText>
<equation confidence="0.9328555">
−
Moussaoui prison for life
− −/I
ultimate punishment
</equation>
<figureCaption confidence="0.857489714285714">
Figure 1: Example text containing opinions
(above) and a summary of the opinions (be-
low). Sources and targets of opinions are brack-
eted; opinion expressions are shown in italics and
bracketed with associated polarity, either positive
(+) or negative (-). The underlined phrase will be
explained later in the paper.
</figureCaption>
<bodyText confidence="0.99937947368421">
a document either positive (“thumbs up”) or nega-
tive (“thumbs down”) polarity (e.g. Das and Chen
(2001), Pang et al. (2002), Turney (2002), Dave
et al. (2003)). Other research has concentrated
on analyzing fine-grained opinions at, or below,
the sentence level. Recent work, for example, in-
dicates that systems can be trained to recognize
opinions and their polarity, strength, and sources
to a reasonable degree of accuracy (e.g. Dave et
al. (2003), Riloff and Wiebe (2003), Bethard et
al. (2004), Wilson et al. (2004), Yu and Hatzivas-
siloglou (2003), Choi et al. (2005), Kim and Hovy
(2005), Wiebe and Riloff (2005)). Our work ex-
tends research on fine-grained opinion extraction
by augmenting the opinions with additional infor-
mation that allows the creation of concise opinion
summaries. In contrast to the opinion extracts pro-
duced by Pang and Lee (2004), our summaries are
not text extracts, but rather explicitly identify and
</bodyText>
<equation confidence="0.960494">
−
Zerkin
</equation>
<page confidence="0.990765">
337
</page>
<bodyText confidence="0.998701035714286">
characterize the relations between opinions and
their sources.
Coreference resolution. Coreference resolution
is a relatively well studied NLP problem (e.g.
Morton (2000), Ng and Cardie (2002), Iida et al.
(2003), McCallum and Wellner (2003)). Corefer-
ence resolution is defined as the problem of decid-
ing which noun phrases in the text (mentions) re-
fer to the same real world entities (are coreferent).
Generally, successful approaches to coreference
resolution have relied on supervised classification
followed by clustering. For supervised classifica-
tion these approaches learn a pairwise function to
predict whether a pair of noun phrases is corefer-
ent. Subsequently, when making coreference res-
olution decisions on unseen documents, the learnt
pairwise NP coreference classifier is run, followed
by a clustering step to produce the final clusters
(coreference chains) of coreferent NPs. For both
training and testing, coreference resolution algo-
rithms rely on feature vectors for pairs of noun
phrases that encode linguistic information about
the NPs and their local context. Our general ap-
proach to source coreference resolution is inspired
by the state-of-the-art performance of one such ap-
proach to coreference resolution, which relies on a
rule learner and single-link clustering as described
in Ng and Cardie (2002).
</bodyText>
<sectionHeader confidence="0.976035" genericHeader="method">
3 Source Coreference Resolution
</sectionHeader>
<bodyText confidence="0.99970716">
In this section we introduce the problem of source
coreference resolution in the context of opinion
summarization and argue for the need for novel
methods for the task.
The task of source coreference resolution is to
decide which mentions of opinion sources refer to
the same entity. Much like traditional coreference
resolution, we employ a learning approach; how-
ever, our approach differs from traditional coref-
erence resolution in its definition of the learn-
ing task. Motivated by the desire to utilize unla-
beled examples (discussed later), we define train-
ing as an integrated task in which pairwise NP
coreference decisions are learned together with
the clustering function as opposed to treating each
NP pair as a training example. Thus, our train-
ing phase takes as input a set of documents with
manually annotated opinion sources together with
coreference annotations for the sources; it outputs
a classifier that can produce source coreference
chains for previously unseen documents contain-
ing marked (manually or automatically) opinion
sources. More specifically, the source coreference
resolution training phase proceeds through the fol-
lowing steps:
</bodyText>
<listItem confidence="0.890680222222222">
1. Source-to-NP mapping: We preprocess
each document by running a tokenizer, sen-
tence splitter, POS tagger, parser, and an NP
finder. Subsequently, we augment the set of
NPs found by the NP finder with the help of
a system for named entity detection. We then
map the sources to the NPs. Since there is
no one-to-one correspondence, we use a set
of heuristics to create the mapping. More de-
tails about why heuristics are needed and the
process used to map sources to NPs can be
found in Stoyanov and Cardie (2006).
2. Feature vector creation: We extract a fea-
ture vector for every pair of NPs from the pre-
processed corpus. We use the features intro-
duced by Ng and Cardie (2002) for the task
of coreference resolution.
3. Classifier construction: Using the feature
</listItem>
<bodyText confidence="0.951894">
vectors from step 2, we construct a training
set containing one training example per doc-
ument. Each training example consists of the
feature vectors for all pairs of NPs in the doc-
ument, including those that do not map to
sources, together with the available corefer-
ence information for the source noun phrases
(i.e. the noun phrases to which sources are
mapped). The training instances are pro-
vided as input to a learning algorithm (see
Section 5), which constructs a classifier that
can take the instances associated with a new
(previously unseen) document and produce a
clustering over all NPs in the document.
The testing phase employs steps 1 and 2 as de-
scribed above, but replaces step 3 by a straightfor-
ward application of the learnt classifier. Since we
are interested in coreference information only for
the source NPs, we simply discard the non-source
NPs from the resulting clustering.
The approach to source coreference resolution
described here would be identical to traditional
coreference resolution when provided with train-
ing examples containing coreference information
for all NPs. However, opinion corpora in general,
and our corpus in particular, contain no corefer-
ence information about general NPs. Neverthe-
less, after manual sources are mapped to NPs in
</bodyText>
<page confidence="0.994018">
338
</page>
<bodyText confidence="0.999977902439024">
step 1 above, our approach can rely on the avail-
able coreference information for the source NPs.
Due to the high cost of coreference annotation, we
desire methods that can work in the presence of
only this limited amount of coreference informa-
tion.
A possible workaround the absence of full NP
coreference information is to train a traditional
coreference system only on the labeled part of the
data (indeed that is one of the baselines against
which we compare). However, we believe that
an effective approach to source coreference res-
olution has to utilize the unlabeled noun phrases
because links between sources might be realized
through non-source mentions. This problem is il-
lustrated in figure 1. The underlined Moussaoui
is coreferent with all of the Moussaoui references
marked as sources, but, because it is used in an
objective sentence rather than as the source of
an opinion, the reference would be omitted from
the Moussaoui source chain. Unfortunately, this
proper noun phrase might be critical in establish-
ing the coreference of the final source reference he
with the other mentions of the source Moussaoui.
As mentioned previously, in order to utilize
the unlabeled data, our approach differs from tra-
ditional coreference resolution, which uses NP
pairs as training instances. We instead follow the
framework of supervised clustering (Finley and
Joachims, 2005; Li and Roth, 2005) and consider
each document as a training example. As in super-
vised clustering, this framework has the additional
advantage that the learning algorithm can consider
the clustering algorithm when making decisions
about pairwise classification, which could lead to
improvements in the classifier. In the next section
we describe our approach to classifier construction
for step 3 and compare our problem to traditional
weakly supervised clustering, characterizing it as
an instance of the novel problem of partially su-
pervised clustering.
</bodyText>
<sectionHeader confidence="0.980096" genericHeader="method">
4 Partially Supervised Clustering
</sectionHeader>
<bodyText confidence="0.999980898305085">
In our desire to perform effective source corefer-
ence resolution we arrive at the following learning
problem – the learning algorithm is presented with
a set of partially specified examples of clusterings
and acquires a function that can cluster accurately
an unseen set of items, while taking advantage of
the unlabeled information in the examples.
This setting is to be contrasted with semi-
supervised clustering (or clustering with con-
straints), which has received much research at-
tention (e.g. Demiriz et al. (1999), Wagstaff and
Cardie (2000), Basu (2005), Davidson and Ravi
(2005)). Semi-supervised clustering can be de-
fined as the problem of clustering a set of items
in the presence of limited supervisory informa-
tion such as pairwise constraints (e.g. two items
must/cannot be in the same cluster) or labeled
points. In contrast to our setting, in the semi-
supervised case there is no training phase – the
algorithm receives all examples (labeled and un-
labeled) at the same time together with some dis-
tance or cost function and attempts to find a clus-
tering that optimizes a given measure (usually
based on the distance or cost function).
Source coreference resolution might alterna-
tively be approached as a supervised clustering
problem. Traditionally, approaches to supervised
clustering have treated the pairwise link decisions
as a classification problem. These approaches first
learn a distance metric that optimizes the pairwise
decisions; and then follow the pairwise classifica-
tion with a clustering step. However, these tradi-
tional approaches have no obvious way of utilizing
the available unlabeled information.
In contrast, we follow recent approaches to su-
pervised clustering that propose ways to learn
the distance measure in the context of the clus-
tering decisions (Li and Roth, 2005; Finley and
Joachims, 2005; McCallum and Wellner, 2003).
This provides two advantages for the problem of
source coreference resolution. First, it allows the
algorithm to take advantage of the complexity of
the rich structural dependencies introduced by the
clustering problem. Viewed traditionally as a hur-
dle, the structural complexity of clustering may be
beneficial in the partially supervised case. We be-
lieve that provided with a few partially specified
clustering examples, an algorithm might be able
to generalize from the structural dependencies to
infer correctly the whole clustering of the items.
In addition, considering pairwise decisions in the
context of the clustering can arguably lead to more
accurate classifiers.
Unfortunately, none of the supervised cluster-
ing approaches is readily applicable to the partially
supervised case. However, by adapting the for-
mal supervised clustering definition, which we do
next, we can develop approaches to partially su-
pervised clustering that take advantage of the un-
</bodyText>
<page confidence="0.996217">
339
</page>
<bodyText confidence="0.998180411764706">
labeled portions of the data.
Formal definition. For partially supervised
clustering we extend the formal definition of su-
pervised clustering given by Finley and Joachims
(2005). In the fully supervised setting, an al-
gorithm is given a set 5 of n training examples
(x1, y1), ..., (xam,, yam,) E X x Y , where X is the
set of all possible sets of items and Y is the set of
all possible clusterings of these sets. For a train-
ing example (x, y), x = {x1, x2, ..., xk} is a set
of k items and y = {y1, y2,..., yr} is a clustering
of the items in x with each yz C_ x. Addition-
ally, each item can be in no more than one cluster
(vi, j.yz n yj = 0) and in the fully supervised case
each item is in at least one cluster (x = U yz).
The goal of the learning algorithm is to acquire a
function h : X —* Y that can accurately cluster a
(previously unseen) set of items.
In the context of source coreference resolution
the training set contains one example for each doc-
ument. The items in each training example are the
NPs and the clustering over the items is the equiv-
alence relation defined by the coreference infor-
mation. For source coreference resolution, how-
ever, clustering information is unavailable for the
non-source NPs. Thus, to be able to deal with this
unlabeled component of the data we arrive to the
setting of partially supervised clustering, in which
we relax the condition that each item is in at least
one cluster (x = U yz) and replace it with the con-
dition x _D U yz. The items with no linking infor-
mation (items in x \ U yz) constitute the unlabeled
(unsupervised) component of the partially super-
vised clustering.
</bodyText>
<sectionHeader confidence="0.986274" genericHeader="method">
5 Structured Rule Learner
</sectionHeader>
<bodyText confidence="0.999935714285714">
We develop a novel method for partially super-
vised clustering, which is motivated by the success
of a rule learner (RIPPER) for coreference resolu-
tion (Ng and Cardie, 2002). We extend RIPPER
so that it can learn rules in the context of single-
link clustering, which both suits our task (i.e. pro-
nouns link to their single antecedent) and has ex-
hibited good performance for coreference resolu-
tion (Ng and Cardie, 2002). We begin with a brief
overview of RIPPER followed by a description of
the modifications that we implemented. For ease
of presentation, we assume that we are in the fully
supervised case. We end this section by describing
the changes for the partially supervised case.
</bodyText>
<table confidence="0.5615">
procedure StRip(TrainData){
GrowData, PruneData = Split(TrainData);
//Keep instances from the same document together
while(there are positive uncovered instances) {
</table>
<equation confidence="0.9191354">
r = growRule(GrowData);
r = pruneRule(r, PruneData);
DL = relativeDL(Ruleset);
if(DL &lt; minDL + d bits)
Ruleset.add(r);
</equation>
<figure confidence="0.868524757575758">
Mark examples covered by r as +;
else
exit loop with Ruleset
I
I
procedure growRule(growData){
r = empty rule;
for(every unused feature f){
if (f is nominal feature) {
for(every possible value v of f) {
mark all instances that have values of v for f with +;
compute the transitive closure of the positive instances
//(including instances marked + from previous rules);
compute the infoGain for the future/value combination;
I
I else{ //Numeric feature
create one bag for each feature value and split the instances into bags;
do a forward and a backward pass over the bags keeping a running
clustering and compute the information gain for each value;
I
I
add the future/value pair with the best infoGain to r;
growData = growData - all negative instances;
return r;
I
procedure pruneRule(r, pruneData){
for(all antecedents a in the rule){
apply all antecedents in r up to a to pruneData;
compute the transitive closure of the positive instances;
compute A(a) – the accuracy of the rule up to antecedent a;
I
Remove all antecedents after the antecedent for which A(a) is maximum.
I
</figure>
<figureCaption confidence="0.997634">
Figure 2: The StRip algorithm. Additions to RIP-
PER are shown in bold.
</figureCaption>
<subsectionHeader confidence="0.98207">
5.1 The RIPPER Algorithm
</subsectionHeader>
<bodyText confidence="0.99978185">
RIPPER (for Repeated Incremental Pruning to
Produce Error Reduction) was introduced by Co-
hen (1995) as an extension of an existing rule
induction algorithm. Cohen (1995) showed that
RIPPER produces error rates competitive with
C4.5, while exhibiting better running times. RIP-
PER consists of two phases – a ruleset is grown
and then optimized.
The ruleset creation phase begins by ran-
domly splitting the training data into a rule-
growing set (2/3 of the training data) and a pruning
set (the remaining 1/3). A rule is then grown on
the former set by repeatedly adding the antecedent
(the feature value test) with the largest information
gain until the accuracy of the rule becomes 1.0 or
there are no remaining potential antecedents. Next
the rule is applied to the pruning data and any rule-
final sequence that reduces the accuracy of the rule
is removed.
The optimization phase uses the full training
</bodyText>
<page confidence="0.993977">
340
</page>
<bodyText confidence="0.999968833333333">
set to first grow a replacement rule and a revised
rule for each rule in the ruleset. For each rule,
the algorithm then considers the original rule, the
replacement rule, and the revised rule, and keeps
the rule with the smallest description length in the
context of the ruleset. After all rules are con-
sidered, RIPPER attempts to grow residual rules
that cover data not already covered by the rule-
set. Finally, RIPPER deletes any rules from the
ruleset that reduce the overall minimum descrip-
tion length of the data plus the ruleset. RIPPER
performs two rounds of this optimization phase.
</bodyText>
<subsectionHeader confidence="0.999661">
5.2 The StRip Algorithm
</subsectionHeader>
<bodyText confidence="0.999970559322034">
The property of partially supervised clustering that
we want to explore is the structured nature of the
decisions. That is, each decision of whether two
items (say a and b) belong to the same cluster has
an implication for all items a&apos; that belong to a’s
cluster and all items b&apos; that belong to b’s cluster.
We target modifications to RIPPER that will al-
low StRip (for Structured RIPPER) to learn rules
that produce good clusterings in the context of
single-link clustering. We extend RIPPER so that
every time it makes a decision about a rule, it con-
siders the effect of the rule on the overall clus-
tering of items (as opposed to considering the in-
stances that the rule classifies as positive/negative
in isolation). More precisely, we precede every
computation of rule performance (e.g. informa-
tion gain or description length) by a transitive clo-
sure (i.e. single link clustering) of the data w.r.t. to
the pairwise classifications. Following the transi-
tive closure, all pairs of items that are in the same
cluster are considered covered by the rule for per-
formance computation.
The StRip algorithm is given in figure 2, with
modifications to the original RIPPER algorithm
shown in bold. Due to space limitations the op-
timization stage of the algorithm is omitted. Our
modifications to the optimization stage of RIPPER
are in the spirit of the rest of the StRip algorithm.
Partially supervised case. So far we described
StRip only for the fully supervised case. We
use a very simple modification to handle the par-
tially supervised setting: we exclude the unla-
beled pairs when computing the performance of
the rules. Thus, the unlabeled items do not count
as correct or incorrect classifications when acquir-
ing or pruning a rule, although they do participate
in the transitive closure. Links in the unlabeled
data are inferred entirely through the indirect links
between items in the labeled component that they
introduce. In the example of figure 1, the two
problematic unlabeled links are the link between
the source mention “he” and the underlined non-
source NP “Mr. Moussaoui” and the link between
the underlined “Mr. Moussaoui” to any source
mention of Moussaoui. While StRip will not re-
ward any rule (or rule set) that covers these two
links directly, such rules will be rewarded indi-
rectly since they put the source he in the chain for
the source Moussaoui.
StRip running time. StRip’s running time is
generally comparable to that of RIPPER. We com-
pute transitive closure by using a Union-Find
structure, which runs in time O(log*n), which for
practical purposes can be considered linear (O(n))
3. However, when computing the best information
gain for a nominal feature, StRip has to make a
pass over the data for each value that the feature
takes, while RIPPER can split the data into bags
and perform the computation in one pass.
</bodyText>
<sectionHeader confidence="0.974717" genericHeader="evaluation">
6 Evaluation and Results
</sectionHeader>
<bodyText confidence="0.999982333333333">
This section describes the source coreference data
set, the baselines, our implementation of StRip,
and the results of our experiments.
</bodyText>
<subsectionHeader confidence="0.999313">
6.1 Data set
</subsectionHeader>
<bodyText confidence="0.999870294117647">
For evaluation we use the MPQA corpus (Wiebe
et al., 2005).4 The corpus consists of 535 doc-
uments from the world press. All documents in
the collection are manually annotated with phrase-
level opinion information following the annota-
tion scheme of Wiebe et al. (2005). Discussion
of the annotation scheme is beyond the scope of
this paper; for our purposes it suffices to say that
the annotations include the source of each opin-
ion and coreference information for the sources
(e.g. source coreference chains). The corpus con-
tains no additional noun phrase coreference infor-
mation.
For our experiments, we randomly split the data
set into a training set consisting of 400 documents
and a test set consisting of the remaining 135 doc-
uments. We use the same test set for all experi-
</bodyText>
<footnote confidence="0.994152857142857">
3For the transitive closure, n is the number of items in a
�
document, which is O( k), where k is the number of NP
pairs. Thus, transitive closure is sublinear in the number of
training instances.
4The MPQA corpus is available at
http://nrrc.mitre.org/NRRC/publications.htm.
</footnote>
<page confidence="0.997565">
341
</page>
<bodyText confidence="0.999984571428571">
ments, although some learning runs were trained
on 200 training documents (see next Subsection).
The test set contains a total of 4736 source NPs
(average of 35.34 source NPs per document) split
into 1710 total source NP chains (average of 12.76
chains per document) for an average of 2.77 source
NPs per chain.
</bodyText>
<subsectionHeader confidence="0.998911">
6.2 Implementation
</subsectionHeader>
<bodyText confidence="0.999892272727273">
We implemented the StRip algorithm by modify-
ing JRip – the java implementation of RIPPER in-
cluded in the WEKA toolkit (Witten and Frank,
2000). The WEKA implementation follows the
original RIPPER specification. We changed the
implementation to incorporate the modifications
suggested by the StRip algorithm; we also mod-
ified the underlying data representations and data
handling techniques for efficiency. Also due to ef-
ficiency considerations, we train StRip only on the
200-document training set.
</bodyText>
<subsectionHeader confidence="0.999205">
6.3 Competitive baselines
</subsectionHeader>
<bodyText confidence="0.999980703703704">
We compare the results of the new method to three
fully supervised baseline systems, each of which
employs the same traditional coreference resolu-
tion approach. In particular, we use the afore-
mentioned algorithm proposed by Ng and Cardie
(2002), which combines a pairwise NP corefer-
ence classifier with single-link clustering.
For one baseline, we train the coreference reso-
lution algorithm on the MPQA src corpus — the
labeled portion of the MPQA corpus (i.e. NPs
from the source coreference chains) with unla-
beled instances removed.
The second and third baselines investigate
whether the source coreference resolution task can
benefit from NP coreference resolution training
data from a different domain. Thus, we train the
traditional coreference resolution algorithm on the
MUC6 and MUC7 coreference-annotated corpora5
that contain documents similar in style to those in
the MPQA corpus (e.g. newspaper articles), but
emanate from different domains.
For all baselines we targeted the best possi-
ble systems by trying two pairwise NP classifiers
(RIPPER and an SVM in the 5V Mli9ht imple-
mentation (Joachims, 1998)), many different pa-
rameter settings for the classifiers, two different
feature sets, two different training set sizes (the
</bodyText>
<footnote confidence="0.780062">
5We train each baseline using both the development set
and the test set from the corresponding MUC corpus.
</footnote>
<bodyText confidence="0.9996315">
full training set and a smaller training set consist-
ing of half of the documents selected at random),
and three different instance selection algorithms6.
This variety of classifier and training data settings
was motivated by reported differences in perfor-
mance of coreference resolution approaches w.r.t.
these variations (Ng and Cardie, 2002). More de-
tails on the different parameter settings and in-
stance selection algorithms as well as trends in the
performance of different settings can be found in
Stoyanov and Cardie (2006). In the experiments
below we report the best performance of each of
the two learning algorithms on the MPQA test
data.
</bodyText>
<subsectionHeader confidence="0.97114">
6.4 Evaluation
</subsectionHeader>
<bodyText confidence="0.999977181818182">
In addition to the baselines described above, we
evaluate StRip both with and without unlabeled
data. That is, we train on the MPQA corpus StRip
using either all NPs or just opinion source NPs.
We use the B3 (Bagga and Baldwin, 1998) eval-
uation measure as well as precision, recall, and
F1 measured on the (positive) pairwise decisions.
B3 is a measure widely used for evaluating coref-
erence resolution algorithms. The measure com-
putes the precision and recall for each NP mention
in a document, and then averages them to produce
combined results for the entire output. More pre-
cisely, given a mention i that has been assigned
to chain ci, the precision for mention i is defined
as the number of correctly identified mentions in
ci divided by the total number of mentions in ci.
Recall for i is defined as the number of correctly
identified mentions in ci divided by the number of
mentions in the gold standard chain for i.
Results are shown in Table 1. The first six
rows of results correspond to the fully supervised
baseline systems trained on different corpora —
MUC6, MUC7, and MPQA src. The seventh row
of results shows the performance of StRip using
only labeled data. The final row of the table shows
the results for partially supervised learning with
unlabeled data. The table lists results from the best
performing run for each algorithm.
Performance among the baselines trained on the
MUC data is comparable. However, the two base-
line runs trained on the MPQA src corpus (i.e. re-
sults rows five and six) show slightly better perfor-
mance on the B3 metric than the baselines trained
</bodyText>
<footnote confidence="0.997596666666667">
6The goal of the instance selection algorithms is to bal-
ance the data, which contains many more negative than posi-
tive instances
</footnote>
<page confidence="0.983476">
342
</page>
<table confidence="0.999745333333333">
ML Framework Training set Classifier B3 precision recall F1
Fully supervised MUC6 SVM 81.2 72.6 52.5 60.9
RIPPER 80.7 57.4 63.5 60.3
MUC7 SVM 81.7 65.6 55.9 60.4
RIPPER 79.7 71.6 48.5 57.9
MPQA src SVM 81.8 57.5 62.9 60.2
RIPPER 81.8 72.0 52.5 60.6
StRip 82.3 76.5 56.1 64.6
Partially supervised MPQA all StRip 83.2 77.1 59.4 67.1
</table>
<tableCaption confidence="0.7698055">
Table 1: Results for Source Coreference. MPQA src stands for the MPQA corpus limited to only source
NPs, while MPQA full contains the unlabeled NPs.
</tableCaption>
<bodyText confidence="0.999814">
on the MUC data, which indicates that for our
task the similarity of the documents in the train-
ing and test sets appears to be more important
than the presence of complete supervisory infor-
mation. (Improvements over the RIPPER runs
trained on the MUC corpora are statistically sig-
nificant7, while improvements over the SVM runs
are not.)
Table 1 also shows that StRip outperforms the
baselines on both performance metrics. StRip’s
performance is better than the baselines when
trained on MPQA src (improvement not statisti-
cally significant, p &gt; 0.20) and even better when
trained on the full MPQA corpus, which includes
the unlabeled NPs (improvement over the base-
lines and the former StRip run statistically signif-
icant). These results confirm our hypothesis that
StRip improves due to two factors: first, consider-
ing pairwise decisions in the context of the clus-
tering function leads to improvements in the clas-
sifier; and, second, StRip can take advantage of
the unlabeled portion of the data.
StRip’s performance is all the more impressive
considering the strength of the SVM and RIPPER
baselines, which which represent the best runs
across the 336 different parameter settings tested
for 5V Ml�9ht and 144 different settings tested for
RIPPER. In contrast, all four of the StRip runs us-
ing the full MPQA corpus (we vary the loss ratio
for false positive/false negative cost) outperform
those baselines.
</bodyText>
<sectionHeader confidence="0.999605" genericHeader="discussions">
7 Future Work
</sectionHeader>
<bodyText confidence="0.999812333333333">
Source coreference resolution is only one aspect
of opinion summarization. Additionally, an opin-
ion summarization system will need to handle
</bodyText>
<footnote confidence="0.954956">
7Statistical significance is measured using both a 2-tailed
paired t-test and the Wilcoxon matched-pairs signed-ranks
test (p &lt; 0.05). The two tests agreed on all significance
judgements, so we will not report them separately.
</footnote>
<bodyText confidence="0.999930424242424">
the closely related task of target coreference res-
olution in order to cluster targets of opinions8
and combine multiple conflicting opinions from a
source to the same targets. Furthermore, a fully
automatic opinion summarizer requires automatic
source and opinion extractors. While we antici-
pate that target coreference resolution will be sub-
ject to error rates similar to those of source coref-
erence resolution, incorporating these imperfect
opinions and sources will further impair the per-
formance of the opinion summarizer. We are not
aware of any measure that can be directly used
to assess the goodness of opinion summaries, but
plan to develop such in future work in conjunc-
tion with the development of methods for creating
opinion summaries completely automatically. The
evaluation metrics will likely have to depend on
the task for which the summaries are used.
A limitation of our approach to partially super-
vised clustering is that we do not directly optimize
for the performance measure (e.g. B3). Other ef-
forts in the area of supervised clustering (Finley
and Joachims, 2005; Li and Roth, 2005) have sug-
gested ways to learn distance measures that can
optimize directly for a desired performance mea-
sure. We plan to investigate algorithms that can di-
rectly optimize for complex measures (such as B3)
for the problem of partially supervised clustering.
Unfortunately, a measure as complex as B3 makes
extending existing approaches far from trivial due
to the difficulty of establishing the connection be-
tween individual pairwise decisions (the distance
metric) and the score of the clustering algorithm.
</bodyText>
<sectionHeader confidence="0.996309" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.998859">
The authors would like to thank Vincent Ng and
Art Munson for providing coreference resolution
</bodyText>
<footnote confidence="0.563586333333333">
8We did not tackle the task of target coreference resolu-
tion in this paper because the MPQA corpus did not contain
target annotations at the time of publication.
</footnote>
<page confidence="0.998145">
343
</page>
<bodyText confidence="0.999835">
code, members of the Cornell NLP group (es-
pecially Yejin Choi and Art Munson) for many
helpful discussions, and the anonymous review-
ers for their insightful comments. This work was
supported by the Advanced Research and Devel-
opment Activity (ARDA), by NSF Grants IIS-
0535099 and IIS-0208028, by gifts from Google
and the Xerox Foundation, and by an NSF Gradu-
ate Research Fellowship to the first author.
</bodyText>
<sectionHeader confidence="0.99886" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999883422680413">
A. Bagga and B. Baldwin. 1998. Entity-based cross-
document coreferencing using the vector space model. In
In Proceedings of COLING/ACL.
S. Basu. 2005. Semi-supervised Clustering: Probabilistic
Models, Algorithms and Experiments. Ph.D. thesis, De-
partment of Computer Sciences, UT at Austin.
S. Bethard, H. Yu, A. Thornton, V. Hativassiloglou, and
D. Jurafsky. 2004. Automatic extraction of opinion
propositions and their holders. In 2004 AAAI Spring Sym-
posium on Exploring Attitude and Affect in Text.
Y. Choi, C. Cardie, E. Riloff, and S. Patwardhan. 2005. Iden-
tifying sources of opinions with conditional random fields
and extraction patterns. In Proceedings ofEMNLP.
C. Coglianese. 2004. E-rulemaking: Information technology
and regulatory policy: New directions in digital govern-
ment research. Technical report, Harvard University, J. F.
Kennedy School of Government.
W. Cohen. 1995. Fast effective rule induction. In Proceed-
ings ofICML.
S. Das and M. Chen. 2001. Yahoo for amazon: Extracting
market sentiment from stock message boards. In Proceed-
ings ofAPFAAC.
K. Dave, S. Lawrence, and D. Pennock. 2003. Mining the
peanut gallery: Opinion extraction and semantic classifi-
cation of product reviews. In Proceedings ofIWWWC.
I. Davidson and S. Ravi. 2005. Clustering with constraints:
Feasibility issues and the k-means algorithm. In Proceed-
ings of SDM.
A. Demiriz, K. P. Bennett, and M. J. Embrechts. 1999. Semi-
supervised clustering using genetic algorithms. In Pro-
ceeding ofANNIE.
T. Finley and T. Joachims. 2005. Supervised clustering with
support vector machines. In Proceedings ofICML.
R. Iida, K. Inui, H. Takamura, and Y. Matsumoto. 2003. In-
corporating contextual cues in trainable models for coref-
erence resolution. In Proceedings of the EACL Workshop
on The Computational Treatment ofAnaphora.
T. Joachims. 1998. Making large-scale support vector
machine learning practical. In A. Smola B. Sch¨olkopf,
C. Burges, editor, Advances in Kernel Methods: Support
Vector Machines. MIT Press, Cambridge, MA.
S. Kim and E. Hovy. 2005. Identifying opinion holders for
question answering in opinion texts. In Proceedings of
AAAI Workshop on Question Answering in Restricted Do-
mains.
X. Li and D. Roth. 2005. Discriminative training of cluster-
ing functions: Theory and experiments with entity identi-
fication. In Proceedings of CoNLL.
A. McCallum and B. Wellner. 2003. Toward conditional
models of identity uncertainty with application to proper
noun coreference. In Proceedings of the IJCAI Workshop
on Information Integration on the Web.
T. Morton. 2000. Coreference for NLP applications. In Pro-
ceedings ofACL.
V. Ng and C. Cardie. 2002. Improving machine learning
approaches to coreference resolution. In In Proceedings
ofACL.
B. Pang and L. Lee. 2004. A sentimental education: Senti-
ment analysis using subjectivity summarization based on
minimum cuts. In Proceedings ofACL.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? Sentiment classification using machine learning tech-
niques. In Proceedings ofEMNLP.
E. Riloff and J. Wiebe. 2003. Learning extraction patterns
for subjective expressions. In Proceesings of EMNLP.
E. Riloff, J. Wiebe, and W. Phillips. 2005. Exploiting sub-
jectivity classification to improve information extraction.
In Proceedings ofAAAI.
V. Stoyanov and C. Cardie. 2006. Toward opinion summa-
rization: Linking the sources. In Proceedings of the ACL
Workshop on Sentiment and Subjectivity in Text.
V. Stoyanov, C. Cardie, and J. Wiebe. 2005. Multi-
Perspective question answering using the OpQA corpus.
In Proceedings ofEMNLP.
P. Turney. 2002. Thumbs up or thumbs down? Semantic ori-
entation applied to unsupervised classification of reviews.
In Proceedings ofACL.
K. Wagstaff and C. Cardie. 2000. Clustering with instance-
level constraints. In Proceedings of the 17-th National
Conference on Artificial Intelligence and 12-th Confer-
ence on Innovative Applications ofArtificial Intelligence.
J. Wiebe and E. Riloff. 2005. Creating subjective and objec-
tive sentence classifiers from unannotated texts. In Pro-
ceedings of CICLing.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating ex-
pressions of opinions and emotions in language. Lan-
guage Resources and Evaluation, 1(2).
T. Wilson, J. Wiebe, and R. Hwa. 2004. Just how mad are
you? Finding strong and weak opinion clauses. In Pro-
ceedings ofAAAI.
I.H. Witten and E. Frank. 2000. Data Mining: Practical ma-
chine learning tools with Java implementations. Morgan
Kaufmann, San Francisco.
H. Yu and V. Hatzivassiloglou. 2003. Towards answer-
ing opinion questions: Separating facts from opinions and
identifying the polarity of opinion sentences. In Proceed-
ings of EMNLP.
</reference>
<page confidence="0.998989">
344
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.872081">
<title confidence="0.99658">Partially Supervised Coreference Resolution for Opinion Summarization through Structured Rule Learning</title>
<author confidence="0.97124">Stoyanov</author>
<affiliation confidence="0.9592805">Department of Computer Cornell</affiliation>
<address confidence="0.985134">Ithaca, NY 14850,</address>
<abstract confidence="0.9998375">Combining fine-grained opinion information to produce opinion summaries is important for sentiment analysis applications. Toward that end, we tackle the problem of source coreference resolution – linking together source mentions that refer to the same entity. The partially supervised nature of the problem leads us to define and approach it as the novel problem of partially supervised clustering. We propose and evaluate a new algorithm for the task of source coreference resolution that outperforms competitive baselines.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Bagga</author>
<author>B Baldwin</author>
</authors>
<title>Entity-based crossdocument coreferencing using the vector space model.</title>
<date>1998</date>
<booktitle>In In Proceedings of COLING/ACL.</booktitle>
<contexts>
<context position="30398" citStr="Bagga and Baldwin, 1998" startWordPosition="4904" endWordPosition="4907">ce resolution approaches w.r.t. these variations (Ng and Cardie, 2002). More details on the different parameter settings and instance selection algorithms as well as trends in the performance of different settings can be found in Stoyanov and Cardie (2006). In the experiments below we report the best performance of each of the two learning algorithms on the MPQA test data. 6.4 Evaluation In addition to the baselines described above, we evaluate StRip both with and without unlabeled data. That is, we train on the MPQA corpus StRip using either all NPs or just opinion source NPs. We use the B3 (Bagga and Baldwin, 1998) evaluation measure as well as precision, recall, and F1 measured on the (positive) pairwise decisions. B3 is a measure widely used for evaluating coreference resolution algorithms. The measure computes the precision and recall for each NP mention in a document, and then averages them to produce combined results for the entire output. More precisely, given a mention i that has been assigned to chain ci, the precision for mention i is defined as the number of correctly identified mentions in ci divided by the total number of mentions in ci. Recall for i is defined as the number of correctly ide</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>A. Bagga and B. Baldwin. 1998. Entity-based crossdocument coreferencing using the vector space model. In In Proceedings of COLING/ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Basu</author>
</authors>
<title>Semi-supervised Clustering: Probabilistic Models, Algorithms and Experiments.</title>
<date>2005</date>
<booktitle>In 2004 AAAI Spring Symposium on Exploring Attitude and Affect in Text.</booktitle>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Sciences, UT</institution>
<contexts>
<context position="15643" citStr="Basu (2005)" startWordPosition="2459" endWordPosition="2460">vised clustering. 4 Partially Supervised Clustering In our desire to perform effective source coreference resolution we arrive at the following learning problem – the learning algorithm is presented with a set of partially specified examples of clusterings and acquires a function that can cluster accurately an unseen set of items, while taking advantage of the unlabeled information in the examples. This setting is to be contrasted with semisupervised clustering (or clustering with constraints), which has received much research attention (e.g. Demiriz et al. (1999), Wagstaff and Cardie (2000), Basu (2005), Davidson and Ravi (2005)). Semi-supervised clustering can be defined as the problem of clustering a set of items in the presence of limited supervisory information such as pairwise constraints (e.g. two items must/cannot be in the same cluster) or labeled points. In contrast to our setting, in the semisupervised case there is no training phase – the algorithm receives all examples (labeled and unlabeled) at the same time together with some distance or cost function and attempts to find a clustering that optimizes a given measure (usually based on the distance or cost function). Source corefe</context>
</contexts>
<marker>Basu, 2005</marker>
<rawString>S. Basu. 2005. Semi-supervised Clustering: Probabilistic Models, Algorithms and Experiments. Ph.D. thesis, Department of Computer Sciences, UT at Austin. S. Bethard, H. Yu, A. Thornton, V. Hativassiloglou, and D. Jurafsky. 2004. Automatic extraction of opinion propositions and their holders. In 2004 AAAI Spring Symposium on Exploring Attitude and Affect in Text.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Choi</author>
<author>C Cardie</author>
<author>E Riloff</author>
<author>S Patwardhan</author>
</authors>
<title>Identifying sources of opinions with conditional random fields and extraction patterns.</title>
<date>2005</date>
<booktitle>In Proceedings ofEMNLP.</booktitle>
<contexts>
<context position="1919" citStr="Choi et al. (2005)" startWordPosition="278" endWordPosition="281">al. (2005)). To date, research in the area (see Related Work section) has focused on the problem of extracting sentiment both at the document level (coarse-grained sentiment information), and at the level of sentences, clauses, or individual expressions (finegrained sentiment information). In contrast, our work concerns the summarization of fine-grained information about opinions. In particular, while recent research efforts have shown that fine-grained opinions (e.g. Riloff and Wiebe (2003), Bethard et al. (2004), Wiebe and Riloff (2005)) as well as their sources (e.g. Bethard et al. (2004), Choi et al. (2005), Kim and Hovy (2005)) can be extracted automatically, little has been done to create opinion summaries, where opinions from the same source/target are combined, statistics are computed for each source/target and multiple opinions from the same source to the same target are aggregated. A simple opinion summary is shown in figure 1.1 We expect that this type of opinion summary, based on fine-grained opinion information, will be important for information analysis applications in any domain where the analysis of opinions is critical. This paper addresses the problem of opinion summarization by co</context>
<context position="8181" citStr="Choi et al. (2005)" startWordPosition="1281" endWordPosition="1284">d phrase will be explained later in the paper. a document either positive (“thumbs up”) or negative (“thumbs down”) polarity (e.g. Das and Chen (2001), Pang et al. (2002), Turney (2002), Dave et al. (2003)). Other research has concentrated on analyzing fine-grained opinions at, or below, the sentence level. Recent work, for example, indicates that systems can be trained to recognize opinions and their polarity, strength, and sources to a reasonable degree of accuracy (e.g. Dave et al. (2003), Riloff and Wiebe (2003), Bethard et al. (2004), Wilson et al. (2004), Yu and Hatzivassiloglou (2003), Choi et al. (2005), Kim and Hovy (2005), Wiebe and Riloff (2005)). Our work extends research on fine-grained opinion extraction by augmenting the opinions with additional information that allows the creation of concise opinion summaries. In contrast to the opinion extracts produced by Pang and Lee (2004), our summaries are not text extracts, but rather explicitly identify and − Zerkin 337 characterize the relations between opinions and their sources. Coreference resolution. Coreference resolution is a relatively well studied NLP problem (e.g. Morton (2000), Ng and Cardie (2002), Iida et al. (2003), McCallum and</context>
</contexts>
<marker>Choi, Cardie, Riloff, Patwardhan, 2005</marker>
<rawString>Y. Choi, C. Cardie, E. Riloff, and S. Patwardhan. 2005. Identifying sources of opinions with conditional random fields and extraction patterns. In Proceedings ofEMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Coglianese</author>
</authors>
<title>E-rulemaking: Information technology and regulatory policy: New directions in digital government research.</title>
<date>2004</date>
<tech>Technical report,</tech>
<institution>Harvard University, J. F. Kennedy School of Government.</institution>
<contexts>
<context position="1065" citStr="Coglianese (2004)" startWordPosition="151" endWordPosition="152">ng together source mentions that refer to the same entity. The partially supervised nature of the problem leads us to define and approach it as the novel problem of partially supervised clustering. We propose and evaluate a new algorithm for the task of source coreference resolution that outperforms competitive baselines. 1 Introduction Sentiment analysis is concerned with extracting attitudes, opinions, evaluations, and sentiment from text. Work in this area has been motivated by the desire to provide information analysis applications in the arenas of government, business, and politics (e.g. Coglianese (2004)). Additionally, sentiment analysis can augment existing NLP applications such as question answering, information retrieval, summarization, and clustering by providing information about sentiment (e.g. Stoyanov et al. (2005), Riloff et al. (2005)). To date, research in the area (see Related Work section) has focused on the problem of extracting sentiment both at the document level (coarse-grained sentiment information), and at the level of sentences, clauses, or individual expressions (finegrained sentiment information). In contrast, our work concerns the summarization of fine-grained informat</context>
</contexts>
<marker>Coglianese, 2004</marker>
<rawString>C. Coglianese. 2004. E-rulemaking: Information technology and regulatory policy: New directions in digital government research. Technical report, Harvard University, J. F. Kennedy School of Government.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Cohen</author>
</authors>
<title>Fast effective rule induction.</title>
<date>1995</date>
<booktitle>In Proceedings ofICML.</booktitle>
<contexts>
<context position="21811" citStr="Cohen (1995)" startWordPosition="3491" endWordPosition="3493">future/value pair with the best infoGain to r; growData = growData - all negative instances; return r; I procedure pruneRule(r, pruneData){ for(all antecedents a in the rule){ apply all antecedents in r up to a to pruneData; compute the transitive closure of the positive instances; compute A(a) – the accuracy of the rule up to antecedent a; I Remove all antecedents after the antecedent for which A(a) is maximum. I Figure 2: The StRip algorithm. Additions to RIPPER are shown in bold. 5.1 The RIPPER Algorithm RIPPER (for Repeated Incremental Pruning to Produce Error Reduction) was introduced by Cohen (1995) as an extension of an existing rule induction algorithm. Cohen (1995) showed that RIPPER produces error rates competitive with C4.5, while exhibiting better running times. RIPPER consists of two phases – a ruleset is grown and then optimized. The ruleset creation phase begins by randomly splitting the training data into a rulegrowing set (2/3 of the training data) and a pruning set (the remaining 1/3). A rule is then grown on the former set by repeatedly adding the antecedent (the feature value test) with the largest information gain until the accuracy of the rule becomes 1.0 or there are no </context>
</contexts>
<marker>Cohen, 1995</marker>
<rawString>W. Cohen. 1995. Fast effective rule induction. In Proceedings ofICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Das</author>
<author>M Chen</author>
</authors>
<title>Yahoo for amazon: Extracting market sentiment from stock message boards.</title>
<date>2001</date>
<booktitle>In Proceedings ofAPFAAC.</booktitle>
<contexts>
<context position="7713" citStr="Das and Chen (2001)" startWordPosition="1205" endWordPosition="1208"> [+ an honest individual], his conduct shows [Target he] is [_ not stable mentally], and thus [_ undeserving] of [Target the ultimate punishment]. − Moussaoui prison for life − −/I ultimate punishment Figure 1: Example text containing opinions (above) and a summary of the opinions (below). Sources and targets of opinions are bracketed; opinion expressions are shown in italics and bracketed with associated polarity, either positive (+) or negative (-). The underlined phrase will be explained later in the paper. a document either positive (“thumbs up”) or negative (“thumbs down”) polarity (e.g. Das and Chen (2001), Pang et al. (2002), Turney (2002), Dave et al. (2003)). Other research has concentrated on analyzing fine-grained opinions at, or below, the sentence level. Recent work, for example, indicates that systems can be trained to recognize opinions and their polarity, strength, and sources to a reasonable degree of accuracy (e.g. Dave et al. (2003), Riloff and Wiebe (2003), Bethard et al. (2004), Wilson et al. (2004), Yu and Hatzivassiloglou (2003), Choi et al. (2005), Kim and Hovy (2005), Wiebe and Riloff (2005)). Our work extends research on fine-grained opinion extraction by augmenting the opin</context>
</contexts>
<marker>Das, Chen, 2001</marker>
<rawString>S. Das and M. Chen. 2001. Yahoo for amazon: Extracting market sentiment from stock message boards. In Proceedings ofAPFAAC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Dave</author>
<author>S Lawrence</author>
<author>D Pennock</author>
</authors>
<title>Mining the peanut gallery: Opinion extraction and semantic classification of product reviews.</title>
<date>2003</date>
<booktitle>In Proceedings ofIWWWC.</booktitle>
<contexts>
<context position="7768" citStr="Dave et al. (2003)" startWordPosition="1215" endWordPosition="1218"> is [_ not stable mentally], and thus [_ undeserving] of [Target the ultimate punishment]. − Moussaoui prison for life − −/I ultimate punishment Figure 1: Example text containing opinions (above) and a summary of the opinions (below). Sources and targets of opinions are bracketed; opinion expressions are shown in italics and bracketed with associated polarity, either positive (+) or negative (-). The underlined phrase will be explained later in the paper. a document either positive (“thumbs up”) or negative (“thumbs down”) polarity (e.g. Das and Chen (2001), Pang et al. (2002), Turney (2002), Dave et al. (2003)). Other research has concentrated on analyzing fine-grained opinions at, or below, the sentence level. Recent work, for example, indicates that systems can be trained to recognize opinions and their polarity, strength, and sources to a reasonable degree of accuracy (e.g. Dave et al. (2003), Riloff and Wiebe (2003), Bethard et al. (2004), Wilson et al. (2004), Yu and Hatzivassiloglou (2003), Choi et al. (2005), Kim and Hovy (2005), Wiebe and Riloff (2005)). Our work extends research on fine-grained opinion extraction by augmenting the opinions with additional information that allows the creati</context>
</contexts>
<marker>Dave, Lawrence, Pennock, 2003</marker>
<rawString>K. Dave, S. Lawrence, and D. Pennock. 2003. Mining the peanut gallery: Opinion extraction and semantic classification of product reviews. In Proceedings ofIWWWC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Davidson</author>
<author>S Ravi</author>
</authors>
<title>Clustering with constraints: Feasibility issues and the k-means algorithm.</title>
<date>2005</date>
<booktitle>In Proceedings of SDM.</booktitle>
<contexts>
<context position="15669" citStr="Davidson and Ravi (2005)" startWordPosition="2461" endWordPosition="2464">ing. 4 Partially Supervised Clustering In our desire to perform effective source coreference resolution we arrive at the following learning problem – the learning algorithm is presented with a set of partially specified examples of clusterings and acquires a function that can cluster accurately an unseen set of items, while taking advantage of the unlabeled information in the examples. This setting is to be contrasted with semisupervised clustering (or clustering with constraints), which has received much research attention (e.g. Demiriz et al. (1999), Wagstaff and Cardie (2000), Basu (2005), Davidson and Ravi (2005)). Semi-supervised clustering can be defined as the problem of clustering a set of items in the presence of limited supervisory information such as pairwise constraints (e.g. two items must/cannot be in the same cluster) or labeled points. In contrast to our setting, in the semisupervised case there is no training phase – the algorithm receives all examples (labeled and unlabeled) at the same time together with some distance or cost function and attempts to find a clustering that optimizes a given measure (usually based on the distance or cost function). Source coreference resolution might alt</context>
</contexts>
<marker>Davidson, Ravi, 2005</marker>
<rawString>I. Davidson and S. Ravi. 2005. Clustering with constraints: Feasibility issues and the k-means algorithm. In Proceedings of SDM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Demiriz</author>
<author>K P Bennett</author>
<author>M J Embrechts</author>
</authors>
<title>Semisupervised clustering using genetic algorithms.</title>
<date>1999</date>
<booktitle>In Proceeding ofANNIE.</booktitle>
<contexts>
<context position="15602" citStr="Demiriz et al. (1999)" startWordPosition="2451" endWordPosition="2454">an instance of the novel problem of partially supervised clustering. 4 Partially Supervised Clustering In our desire to perform effective source coreference resolution we arrive at the following learning problem – the learning algorithm is presented with a set of partially specified examples of clusterings and acquires a function that can cluster accurately an unseen set of items, while taking advantage of the unlabeled information in the examples. This setting is to be contrasted with semisupervised clustering (or clustering with constraints), which has received much research attention (e.g. Demiriz et al. (1999), Wagstaff and Cardie (2000), Basu (2005), Davidson and Ravi (2005)). Semi-supervised clustering can be defined as the problem of clustering a set of items in the presence of limited supervisory information such as pairwise constraints (e.g. two items must/cannot be in the same cluster) or labeled points. In contrast to our setting, in the semisupervised case there is no training phase – the algorithm receives all examples (labeled and unlabeled) at the same time together with some distance or cost function and attempts to find a clustering that optimizes a given measure (usually based on the </context>
</contexts>
<marker>Demiriz, Bennett, Embrechts, 1999</marker>
<rawString>A. Demiriz, K. P. Bennett, and M. J. Embrechts. 1999. Semisupervised clustering using genetic algorithms. In Proceeding ofANNIE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Finley</author>
<author>T Joachims</author>
</authors>
<title>Supervised clustering with support vector machines.</title>
<date>2005</date>
<booktitle>In Proceedings ofICML.</booktitle>
<contexts>
<context position="14493" citStr="Finley and Joachims, 2005" startWordPosition="2282" endWordPosition="2285">of the Moussaoui references marked as sources, but, because it is used in an objective sentence rather than as the source of an opinion, the reference would be omitted from the Moussaoui source chain. Unfortunately, this proper noun phrase might be critical in establishing the coreference of the final source reference he with the other mentions of the source Moussaoui. As mentioned previously, in order to utilize the unlabeled data, our approach differs from traditional coreference resolution, which uses NP pairs as training instances. We instead follow the framework of supervised clustering (Finley and Joachims, 2005; Li and Roth, 2005) and consider each document as a training example. As in supervised clustering, this framework has the additional advantage that the learning algorithm can consider the clustering algorithm when making decisions about pairwise classification, which could lead to improvements in the classifier. In the next section we describe our approach to classifier construction for step 3 and compare our problem to traditional weakly supervised clustering, characterizing it as an instance of the novel problem of partially supervised clustering. 4 Partially Supervised Clustering In our de</context>
<context position="16915" citStr="Finley and Joachims, 2005" startWordPosition="2657" endWordPosition="2660">hed as a supervised clustering problem. Traditionally, approaches to supervised clustering have treated the pairwise link decisions as a classification problem. These approaches first learn a distance metric that optimizes the pairwise decisions; and then follow the pairwise classification with a clustering step. However, these traditional approaches have no obvious way of utilizing the available unlabeled information. In contrast, we follow recent approaches to supervised clustering that propose ways to learn the distance measure in the context of the clustering decisions (Li and Roth, 2005; Finley and Joachims, 2005; McCallum and Wellner, 2003). This provides two advantages for the problem of source coreference resolution. First, it allows the algorithm to take advantage of the complexity of the rich structural dependencies introduced by the clustering problem. Viewed traditionally as a hurdle, the structural complexity of clustering may be beneficial in the partially supervised case. We believe that provided with a few partially specified clustering examples, an algorithm might be able to generalize from the structural dependencies to infer correctly the whole clustering of the items. In addition, consi</context>
<context position="35246" citStr="Finley and Joachims, 2005" startWordPosition="5705" endWordPosition="5708">es will further impair the performance of the opinion summarizer. We are not aware of any measure that can be directly used to assess the goodness of opinion summaries, but plan to develop such in future work in conjunction with the development of methods for creating opinion summaries completely automatically. The evaluation metrics will likely have to depend on the task for which the summaries are used. A limitation of our approach to partially supervised clustering is that we do not directly optimize for the performance measure (e.g. B3). Other efforts in the area of supervised clustering (Finley and Joachims, 2005; Li and Roth, 2005) have suggested ways to learn distance measures that can optimize directly for a desired performance measure. We plan to investigate algorithms that can directly optimize for complex measures (such as B3) for the problem of partially supervised clustering. Unfortunately, a measure as complex as B3 makes extending existing approaches far from trivial due to the difficulty of establishing the connection between individual pairwise decisions (the distance metric) and the score of the clustering algorithm. Acknowledgements The authors would like to thank Vincent Ng and Art Muns</context>
</contexts>
<marker>Finley, Joachims, 2005</marker>
<rawString>T. Finley and T. Joachims. 2005. Supervised clustering with support vector machines. In Proceedings ofICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Iida</author>
<author>K Inui</author>
<author>H Takamura</author>
<author>Y Matsumoto</author>
</authors>
<title>Incorporating contextual cues in trainable models for coreference resolution.</title>
<date>2003</date>
<booktitle>In Proceedings of the EACL Workshop on The Computational Treatment ofAnaphora.</booktitle>
<contexts>
<context position="8767" citStr="Iida et al. (2003)" startWordPosition="1371" endWordPosition="1374">oglou (2003), Choi et al. (2005), Kim and Hovy (2005), Wiebe and Riloff (2005)). Our work extends research on fine-grained opinion extraction by augmenting the opinions with additional information that allows the creation of concise opinion summaries. In contrast to the opinion extracts produced by Pang and Lee (2004), our summaries are not text extracts, but rather explicitly identify and − Zerkin 337 characterize the relations between opinions and their sources. Coreference resolution. Coreference resolution is a relatively well studied NLP problem (e.g. Morton (2000), Ng and Cardie (2002), Iida et al. (2003), McCallum and Wellner (2003)). Coreference resolution is defined as the problem of deciding which noun phrases in the text (mentions) refer to the same real world entities (are coreferent). Generally, successful approaches to coreference resolution have relied on supervised classification followed by clustering. For supervised classification these approaches learn a pairwise function to predict whether a pair of noun phrases is coreferent. Subsequently, when making coreference resolution decisions on unseen documents, the learnt pairwise NP coreference classifier is run, followed by a cluster</context>
</contexts>
<marker>Iida, Inui, Takamura, Matsumoto, 2003</marker>
<rawString>R. Iida, K. Inui, H. Takamura, and Y. Matsumoto. 2003. Incorporating contextual cues in trainable models for coreference resolution. In Proceedings of the EACL Workshop on The Computational Treatment ofAnaphora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Making large-scale support vector machine learning practical.</title>
<date>1998</date>
<booktitle>Advances in Kernel Methods: Support Vector Machines.</booktitle>
<editor>In A. Smola B. Sch¨olkopf, C. Burges, editor,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="29270" citStr="Joachims, 1998" startWordPosition="4723" endWordPosition="4724"> with unlabeled instances removed. The second and third baselines investigate whether the source coreference resolution task can benefit from NP coreference resolution training data from a different domain. Thus, we train the traditional coreference resolution algorithm on the MUC6 and MUC7 coreference-annotated corpora5 that contain documents similar in style to those in the MPQA corpus (e.g. newspaper articles), but emanate from different domains. For all baselines we targeted the best possible systems by trying two pairwise NP classifiers (RIPPER and an SVM in the 5V Mli9ht implementation (Joachims, 1998)), many different parameter settings for the classifiers, two different feature sets, two different training set sizes (the 5We train each baseline using both the development set and the test set from the corresponding MUC corpus. full training set and a smaller training set consisting of half of the documents selected at random), and three different instance selection algorithms6. This variety of classifier and training data settings was motivated by reported differences in performance of coreference resolution approaches w.r.t. these variations (Ng and Cardie, 2002). More details on the diff</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>T. Joachims. 1998. Making large-scale support vector machine learning practical. In A. Smola B. Sch¨olkopf, C. Burges, editor, Advances in Kernel Methods: Support Vector Machines. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kim</author>
<author>E Hovy</author>
</authors>
<title>Identifying opinion holders for question answering in opinion texts.</title>
<date>2005</date>
<booktitle>In Proceedings of AAAI Workshop on Question Answering in Restricted Domains.</booktitle>
<contexts>
<context position="1940" citStr="Kim and Hovy (2005)" startWordPosition="282" endWordPosition="285">, research in the area (see Related Work section) has focused on the problem of extracting sentiment both at the document level (coarse-grained sentiment information), and at the level of sentences, clauses, or individual expressions (finegrained sentiment information). In contrast, our work concerns the summarization of fine-grained information about opinions. In particular, while recent research efforts have shown that fine-grained opinions (e.g. Riloff and Wiebe (2003), Bethard et al. (2004), Wiebe and Riloff (2005)) as well as their sources (e.g. Bethard et al. (2004), Choi et al. (2005), Kim and Hovy (2005)) can be extracted automatically, little has been done to create opinion summaries, where opinions from the same source/target are combined, statistics are computed for each source/target and multiple opinions from the same source to the same target are aggregated. A simple opinion summary is shown in figure 1.1 We expect that this type of opinion summary, based on fine-grained opinion information, will be important for information analysis applications in any domain where the analysis of opinions is critical. This paper addresses the problem of opinion summarization by considering the creatio</context>
<context position="8202" citStr="Kim and Hovy (2005)" startWordPosition="1285" endWordPosition="1288">lained later in the paper. a document either positive (“thumbs up”) or negative (“thumbs down”) polarity (e.g. Das and Chen (2001), Pang et al. (2002), Turney (2002), Dave et al. (2003)). Other research has concentrated on analyzing fine-grained opinions at, or below, the sentence level. Recent work, for example, indicates that systems can be trained to recognize opinions and their polarity, strength, and sources to a reasonable degree of accuracy (e.g. Dave et al. (2003), Riloff and Wiebe (2003), Bethard et al. (2004), Wilson et al. (2004), Yu and Hatzivassiloglou (2003), Choi et al. (2005), Kim and Hovy (2005), Wiebe and Riloff (2005)). Our work extends research on fine-grained opinion extraction by augmenting the opinions with additional information that allows the creation of concise opinion summaries. In contrast to the opinion extracts produced by Pang and Lee (2004), our summaries are not text extracts, but rather explicitly identify and − Zerkin 337 characterize the relations between opinions and their sources. Coreference resolution. Coreference resolution is a relatively well studied NLP problem (e.g. Morton (2000), Ng and Cardie (2002), Iida et al. (2003), McCallum and Wellner (2003)). Cor</context>
</contexts>
<marker>Kim, Hovy, 2005</marker>
<rawString>S. Kim and E. Hovy. 2005. Identifying opinion holders for question answering in opinion texts. In Proceedings of AAAI Workshop on Question Answering in Restricted Domains.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Li</author>
<author>D Roth</author>
</authors>
<title>Discriminative training of clustering functions: Theory and experiments with entity identification.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="14513" citStr="Li and Roth, 2005" startWordPosition="2286" endWordPosition="2289"> marked as sources, but, because it is used in an objective sentence rather than as the source of an opinion, the reference would be omitted from the Moussaoui source chain. Unfortunately, this proper noun phrase might be critical in establishing the coreference of the final source reference he with the other mentions of the source Moussaoui. As mentioned previously, in order to utilize the unlabeled data, our approach differs from traditional coreference resolution, which uses NP pairs as training instances. We instead follow the framework of supervised clustering (Finley and Joachims, 2005; Li and Roth, 2005) and consider each document as a training example. As in supervised clustering, this framework has the additional advantage that the learning algorithm can consider the clustering algorithm when making decisions about pairwise classification, which could lead to improvements in the classifier. In the next section we describe our approach to classifier construction for step 3 and compare our problem to traditional weakly supervised clustering, characterizing it as an instance of the novel problem of partially supervised clustering. 4 Partially Supervised Clustering In our desire to perform effe</context>
<context position="16888" citStr="Li and Roth, 2005" startWordPosition="2653" endWordPosition="2656">natively be approached as a supervised clustering problem. Traditionally, approaches to supervised clustering have treated the pairwise link decisions as a classification problem. These approaches first learn a distance metric that optimizes the pairwise decisions; and then follow the pairwise classification with a clustering step. However, these traditional approaches have no obvious way of utilizing the available unlabeled information. In contrast, we follow recent approaches to supervised clustering that propose ways to learn the distance measure in the context of the clustering decisions (Li and Roth, 2005; Finley and Joachims, 2005; McCallum and Wellner, 2003). This provides two advantages for the problem of source coreference resolution. First, it allows the algorithm to take advantage of the complexity of the rich structural dependencies introduced by the clustering problem. Viewed traditionally as a hurdle, the structural complexity of clustering may be beneficial in the partially supervised case. We believe that provided with a few partially specified clustering examples, an algorithm might be able to generalize from the structural dependencies to infer correctly the whole clustering of th</context>
<context position="35266" citStr="Li and Roth, 2005" startWordPosition="5709" endWordPosition="5712">performance of the opinion summarizer. We are not aware of any measure that can be directly used to assess the goodness of opinion summaries, but plan to develop such in future work in conjunction with the development of methods for creating opinion summaries completely automatically. The evaluation metrics will likely have to depend on the task for which the summaries are used. A limitation of our approach to partially supervised clustering is that we do not directly optimize for the performance measure (e.g. B3). Other efforts in the area of supervised clustering (Finley and Joachims, 2005; Li and Roth, 2005) have suggested ways to learn distance measures that can optimize directly for a desired performance measure. We plan to investigate algorithms that can directly optimize for complex measures (such as B3) for the problem of partially supervised clustering. Unfortunately, a measure as complex as B3 makes extending existing approaches far from trivial due to the difficulty of establishing the connection between individual pairwise decisions (the distance metric) and the score of the clustering algorithm. Acknowledgements The authors would like to thank Vincent Ng and Art Munson for providing cor</context>
</contexts>
<marker>Li, Roth, 2005</marker>
<rawString>X. Li and D. Roth. 2005. Discriminative training of clustering functions: Theory and experiments with entity identification. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>B Wellner</author>
</authors>
<title>Toward conditional models of identity uncertainty with application to proper noun coreference.</title>
<date>2003</date>
<booktitle>In Proceedings of the IJCAI Workshop on Information Integration on the Web.</booktitle>
<contexts>
<context position="8796" citStr="McCallum and Wellner (2003)" startWordPosition="1375" endWordPosition="1378">t al. (2005), Kim and Hovy (2005), Wiebe and Riloff (2005)). Our work extends research on fine-grained opinion extraction by augmenting the opinions with additional information that allows the creation of concise opinion summaries. In contrast to the opinion extracts produced by Pang and Lee (2004), our summaries are not text extracts, but rather explicitly identify and − Zerkin 337 characterize the relations between opinions and their sources. Coreference resolution. Coreference resolution is a relatively well studied NLP problem (e.g. Morton (2000), Ng and Cardie (2002), Iida et al. (2003), McCallum and Wellner (2003)). Coreference resolution is defined as the problem of deciding which noun phrases in the text (mentions) refer to the same real world entities (are coreferent). Generally, successful approaches to coreference resolution have relied on supervised classification followed by clustering. For supervised classification these approaches learn a pairwise function to predict whether a pair of noun phrases is coreferent. Subsequently, when making coreference resolution decisions on unseen documents, the learnt pairwise NP coreference classifier is run, followed by a clustering step to produce the final</context>
<context position="16944" citStr="McCallum and Wellner, 2003" startWordPosition="2661" endWordPosition="2664">ing problem. Traditionally, approaches to supervised clustering have treated the pairwise link decisions as a classification problem. These approaches first learn a distance metric that optimizes the pairwise decisions; and then follow the pairwise classification with a clustering step. However, these traditional approaches have no obvious way of utilizing the available unlabeled information. In contrast, we follow recent approaches to supervised clustering that propose ways to learn the distance measure in the context of the clustering decisions (Li and Roth, 2005; Finley and Joachims, 2005; McCallum and Wellner, 2003). This provides two advantages for the problem of source coreference resolution. First, it allows the algorithm to take advantage of the complexity of the rich structural dependencies introduced by the clustering problem. Viewed traditionally as a hurdle, the structural complexity of clustering may be beneficial in the partially supervised case. We believe that provided with a few partially specified clustering examples, an algorithm might be able to generalize from the structural dependencies to infer correctly the whole clustering of the items. In addition, considering pairwise decisions in </context>
</contexts>
<marker>McCallum, Wellner, 2003</marker>
<rawString>A. McCallum and B. Wellner. 2003. Toward conditional models of identity uncertainty with application to proper noun coreference. In Proceedings of the IJCAI Workshop on Information Integration on the Web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Morton</author>
</authors>
<title>Coreference for NLP applications.</title>
<date>2000</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="4009" citStr="Morton (2000)" startWordPosition="605" endWordPosition="606">et statistics. 2In addition, the summary would require the closely related task of target coreference resolution and a means for aggregating the conflicting opinions from Zerkin toward Moussaoui. 336 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 336–344, Sydney, July 2006. c�2006 Association for Computational Linguistics At first glance, source coreference resolution appears equivalent to the task of noun phrase coreference resolution and therefore amenable to traditional coreference resolution techniques (e.g. Ng and Cardie (2002), Morton (2000)). We hypothesize in Section 3, however, that the task is likely to succumb to a better solution by treating it in the context of a new machine learning setting that we refer to as partially supervised clustering. In particular, due to high coreference annotation costs, data sets that are annotated with opinion information (like ours) do not typically include supervisory coreference information for all noun phrases in a document (as would be required for the application of traditional coreference resolution techniques), but only for noun phrases that act as opinion sources (or targets). As a r</context>
<context position="8725" citStr="Morton (2000)" startWordPosition="1365" endWordPosition="1366">son et al. (2004), Yu and Hatzivassiloglou (2003), Choi et al. (2005), Kim and Hovy (2005), Wiebe and Riloff (2005)). Our work extends research on fine-grained opinion extraction by augmenting the opinions with additional information that allows the creation of concise opinion summaries. In contrast to the opinion extracts produced by Pang and Lee (2004), our summaries are not text extracts, but rather explicitly identify and − Zerkin 337 characterize the relations between opinions and their sources. Coreference resolution. Coreference resolution is a relatively well studied NLP problem (e.g. Morton (2000), Ng and Cardie (2002), Iida et al. (2003), McCallum and Wellner (2003)). Coreference resolution is defined as the problem of deciding which noun phrases in the text (mentions) refer to the same real world entities (are coreferent). Generally, successful approaches to coreference resolution have relied on supervised classification followed by clustering. For supervised classification these approaches learn a pairwise function to predict whether a pair of noun phrases is coreferent. Subsequently, when making coreference resolution decisions on unseen documents, the learnt pairwise NP coreferenc</context>
</contexts>
<marker>Morton, 2000</marker>
<rawString>T. Morton. 2000. Coreference for NLP applications. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
<author>C Cardie</author>
</authors>
<title>Improving machine learning approaches to coreference resolution. In</title>
<date>2002</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="3994" citStr="Ng and Cardie (2002)" startWordPosition="601" endWordPosition="604">ontain any source/target statistics. 2In addition, the summary would require the closely related task of target coreference resolution and a means for aggregating the conflicting opinions from Zerkin toward Moussaoui. 336 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 336–344, Sydney, July 2006. c�2006 Association for Computational Linguistics At first glance, source coreference resolution appears equivalent to the task of noun phrase coreference resolution and therefore amenable to traditional coreference resolution techniques (e.g. Ng and Cardie (2002), Morton (2000)). We hypothesize in Section 3, however, that the task is likely to succumb to a better solution by treating it in the context of a new machine learning setting that we refer to as partially supervised clustering. In particular, due to high coreference annotation costs, data sets that are annotated with opinion information (like ours) do not typically include supervisory coreference information for all noun phrases in a document (as would be required for the application of traditional coreference resolution techniques), but only for noun phrases that act as opinion sources (or t</context>
<context position="8747" citStr="Ng and Cardie (2002)" startWordPosition="1367" endWordPosition="1370">4), Yu and Hatzivassiloglou (2003), Choi et al. (2005), Kim and Hovy (2005), Wiebe and Riloff (2005)). Our work extends research on fine-grained opinion extraction by augmenting the opinions with additional information that allows the creation of concise opinion summaries. In contrast to the opinion extracts produced by Pang and Lee (2004), our summaries are not text extracts, but rather explicitly identify and − Zerkin 337 characterize the relations between opinions and their sources. Coreference resolution. Coreference resolution is a relatively well studied NLP problem (e.g. Morton (2000), Ng and Cardie (2002), Iida et al. (2003), McCallum and Wellner (2003)). Coreference resolution is defined as the problem of deciding which noun phrases in the text (mentions) refer to the same real world entities (are coreferent). Generally, successful approaches to coreference resolution have relied on supervised classification followed by clustering. For supervised classification these approaches learn a pairwise function to predict whether a pair of noun phrases is coreferent. Subsequently, when making coreference resolution decisions on unseen documents, the learnt pairwise NP coreference classifier is run, f</context>
<context position="11748" citStr="Ng and Cardie (2002)" startWordPosition="1843" endWordPosition="1846">running a tokenizer, sentence splitter, POS tagger, parser, and an NP finder. Subsequently, we augment the set of NPs found by the NP finder with the help of a system for named entity detection. We then map the sources to the NPs. Since there is no one-to-one correspondence, we use a set of heuristics to create the mapping. More details about why heuristics are needed and the process used to map sources to NPs can be found in Stoyanov and Cardie (2006). 2. Feature vector creation: We extract a feature vector for every pair of NPs from the preprocessed corpus. We use the features introduced by Ng and Cardie (2002) for the task of coreference resolution. 3. Classifier construction: Using the feature vectors from step 2, we construct a training set containing one training example per document. Each training example consists of the feature vectors for all pairs of NPs in the document, including those that do not map to sources, together with the available coreference information for the source noun phrases (i.e. the noun phrases to which sources are mapped). The training instances are provided as input to a learning algorithm (see Section 5), which constructs a classifier that can take the instances assoc</context>
<context position="19739" citStr="Ng and Cardie, 2002" startWordPosition="3149" endWordPosition="3152">non-source NPs. Thus, to be able to deal with this unlabeled component of the data we arrive to the setting of partially supervised clustering, in which we relax the condition that each item is in at least one cluster (x = U yz) and replace it with the condition x _D U yz. The items with no linking information (items in x \ U yz) constitute the unlabeled (unsupervised) component of the partially supervised clustering. 5 Structured Rule Learner We develop a novel method for partially supervised clustering, which is motivated by the success of a rule learner (RIPPER) for coreference resolution (Ng and Cardie, 2002). We extend RIPPER so that it can learn rules in the context of singlelink clustering, which both suits our task (i.e. pronouns link to their single antecedent) and has exhibited good performance for coreference resolution (Ng and Cardie, 2002). We begin with a brief overview of RIPPER followed by a description of the modifications that we implemented. For ease of presentation, we assume that we are in the fully supervised case. We end this section by describing the changes for the partially supervised case. procedure StRip(TrainData){ GrowData, PruneData = Split(TrainData); //Keep instances f</context>
<context position="28399" citStr="Ng and Cardie (2002)" startWordPosition="4588" endWordPosition="4591">EKA implementation follows the original RIPPER specification. We changed the implementation to incorporate the modifications suggested by the StRip algorithm; we also modified the underlying data representations and data handling techniques for efficiency. Also due to efficiency considerations, we train StRip only on the 200-document training set. 6.3 Competitive baselines We compare the results of the new method to three fully supervised baseline systems, each of which employs the same traditional coreference resolution approach. In particular, we use the aforementioned algorithm proposed by Ng and Cardie (2002), which combines a pairwise NP coreference classifier with single-link clustering. For one baseline, we train the coreference resolution algorithm on the MPQA src corpus — the labeled portion of the MPQA corpus (i.e. NPs from the source coreference chains) with unlabeled instances removed. The second and third baselines investigate whether the source coreference resolution task can benefit from NP coreference resolution training data from a different domain. Thus, we train the traditional coreference resolution algorithm on the MUC6 and MUC7 coreference-annotated corpora5 that contain document</context>
<context position="29844" citStr="Ng and Cardie, 2002" startWordPosition="4808" endWordPosition="4811"> the 5V Mli9ht implementation (Joachims, 1998)), many different parameter settings for the classifiers, two different feature sets, two different training set sizes (the 5We train each baseline using both the development set and the test set from the corresponding MUC corpus. full training set and a smaller training set consisting of half of the documents selected at random), and three different instance selection algorithms6. This variety of classifier and training data settings was motivated by reported differences in performance of coreference resolution approaches w.r.t. these variations (Ng and Cardie, 2002). More details on the different parameter settings and instance selection algorithms as well as trends in the performance of different settings can be found in Stoyanov and Cardie (2006). In the experiments below we report the best performance of each of the two learning algorithms on the MPQA test data. 6.4 Evaluation In addition to the baselines described above, we evaluate StRip both with and without unlabeled data. That is, we train on the MPQA corpus StRip using either all NPs or just opinion source NPs. We use the B3 (Bagga and Baldwin, 1998) evaluation measure as well as precision, reca</context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>V. Ng and C. Cardie. 2002. Improving machine learning approaches to coreference resolution. In In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="8468" citStr="Pang and Lee (2004)" startWordPosition="1327" endWordPosition="1330">w, the sentence level. Recent work, for example, indicates that systems can be trained to recognize opinions and their polarity, strength, and sources to a reasonable degree of accuracy (e.g. Dave et al. (2003), Riloff and Wiebe (2003), Bethard et al. (2004), Wilson et al. (2004), Yu and Hatzivassiloglou (2003), Choi et al. (2005), Kim and Hovy (2005), Wiebe and Riloff (2005)). Our work extends research on fine-grained opinion extraction by augmenting the opinions with additional information that allows the creation of concise opinion summaries. In contrast to the opinion extracts produced by Pang and Lee (2004), our summaries are not text extracts, but rather explicitly identify and − Zerkin 337 characterize the relations between opinions and their sources. Coreference resolution. Coreference resolution is a relatively well studied NLP problem (e.g. Morton (2000), Ng and Cardie (2002), Iida et al. (2003), McCallum and Wellner (2003)). Coreference resolution is defined as the problem of deciding which noun phrases in the text (mentions) refer to the same real world entities (are coreferent). Generally, successful approaches to coreference resolution have relied on supervised classification followed b</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>B. Pang and L. Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
<author>S Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings ofEMNLP.</booktitle>
<contexts>
<context position="7733" citStr="Pang et al. (2002)" startWordPosition="1209" endWordPosition="1212">ual], his conduct shows [Target he] is [_ not stable mentally], and thus [_ undeserving] of [Target the ultimate punishment]. − Moussaoui prison for life − −/I ultimate punishment Figure 1: Example text containing opinions (above) and a summary of the opinions (below). Sources and targets of opinions are bracketed; opinion expressions are shown in italics and bracketed with associated polarity, either positive (+) or negative (-). The underlined phrase will be explained later in the paper. a document either positive (“thumbs up”) or negative (“thumbs down”) polarity (e.g. Das and Chen (2001), Pang et al. (2002), Turney (2002), Dave et al. (2003)). Other research has concentrated on analyzing fine-grained opinions at, or below, the sentence level. Recent work, for example, indicates that systems can be trained to recognize opinions and their polarity, strength, and sources to a reasonable degree of accuracy (e.g. Dave et al. (2003), Riloff and Wiebe (2003), Bethard et al. (2004), Wilson et al. (2004), Yu and Hatzivassiloglou (2003), Choi et al. (2005), Kim and Hovy (2005), Wiebe and Riloff (2005)). Our work extends research on fine-grained opinion extraction by augmenting the opinions with additional</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs up? Sentiment classification using machine learning techniques. In Proceedings ofEMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>J Wiebe</author>
</authors>
<title>Learning extraction patterns for subjective expressions.</title>
<date>2003</date>
<booktitle>In Proceesings of EMNLP.</booktitle>
<contexts>
<context position="1797" citStr="Riloff and Wiebe (2003)" startWordPosition="256" endWordPosition="259">tion retrieval, summarization, and clustering by providing information about sentiment (e.g. Stoyanov et al. (2005), Riloff et al. (2005)). To date, research in the area (see Related Work section) has focused on the problem of extracting sentiment both at the document level (coarse-grained sentiment information), and at the level of sentences, clauses, or individual expressions (finegrained sentiment information). In contrast, our work concerns the summarization of fine-grained information about opinions. In particular, while recent research efforts have shown that fine-grained opinions (e.g. Riloff and Wiebe (2003), Bethard et al. (2004), Wiebe and Riloff (2005)) as well as their sources (e.g. Bethard et al. (2004), Choi et al. (2005), Kim and Hovy (2005)) can be extracted automatically, little has been done to create opinion summaries, where opinions from the same source/target are combined, statistics are computed for each source/target and multiple opinions from the same source to the same target are aggregated. A simple opinion summary is shown in figure 1.1 We expect that this type of opinion summary, based on fine-grained opinion information, will be important for information analysis applications</context>
<context position="8084" citStr="Riloff and Wiebe (2003)" startWordPosition="1264" endWordPosition="1267"> in italics and bracketed with associated polarity, either positive (+) or negative (-). The underlined phrase will be explained later in the paper. a document either positive (“thumbs up”) or negative (“thumbs down”) polarity (e.g. Das and Chen (2001), Pang et al. (2002), Turney (2002), Dave et al. (2003)). Other research has concentrated on analyzing fine-grained opinions at, or below, the sentence level. Recent work, for example, indicates that systems can be trained to recognize opinions and their polarity, strength, and sources to a reasonable degree of accuracy (e.g. Dave et al. (2003), Riloff and Wiebe (2003), Bethard et al. (2004), Wilson et al. (2004), Yu and Hatzivassiloglou (2003), Choi et al. (2005), Kim and Hovy (2005), Wiebe and Riloff (2005)). Our work extends research on fine-grained opinion extraction by augmenting the opinions with additional information that allows the creation of concise opinion summaries. In contrast to the opinion extracts produced by Pang and Lee (2004), our summaries are not text extracts, but rather explicitly identify and − Zerkin 337 characterize the relations between opinions and their sources. Coreference resolution. Coreference resolution is a relatively wel</context>
</contexts>
<marker>Riloff, Wiebe, 2003</marker>
<rawString>E. Riloff and J. Wiebe. 2003. Learning extraction patterns for subjective expressions. In Proceesings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>J Wiebe</author>
<author>W Phillips</author>
</authors>
<title>Exploiting subjectivity classification to improve information extraction.</title>
<date>2005</date>
<booktitle>In Proceedings ofAAAI.</booktitle>
<contexts>
<context position="1311" citStr="Riloff et al. (2005)" startWordPosition="183" endWordPosition="186">he task of source coreference resolution that outperforms competitive baselines. 1 Introduction Sentiment analysis is concerned with extracting attitudes, opinions, evaluations, and sentiment from text. Work in this area has been motivated by the desire to provide information analysis applications in the arenas of government, business, and politics (e.g. Coglianese (2004)). Additionally, sentiment analysis can augment existing NLP applications such as question answering, information retrieval, summarization, and clustering by providing information about sentiment (e.g. Stoyanov et al. (2005), Riloff et al. (2005)). To date, research in the area (see Related Work section) has focused on the problem of extracting sentiment both at the document level (coarse-grained sentiment information), and at the level of sentences, clauses, or individual expressions (finegrained sentiment information). In contrast, our work concerns the summarization of fine-grained information about opinions. In particular, while recent research efforts have shown that fine-grained opinions (e.g. Riloff and Wiebe (2003), Bethard et al. (2004), Wiebe and Riloff (2005)) as well as their sources (e.g. Bethard et al. (2004), Choi et al</context>
</contexts>
<marker>Riloff, Wiebe, Phillips, 2005</marker>
<rawString>E. Riloff, J. Wiebe, and W. Phillips. 2005. Exploiting subjectivity classification to improve information extraction. In Proceedings ofAAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Stoyanov</author>
<author>C Cardie</author>
</authors>
<title>Toward opinion summarization: Linking the sources.</title>
<date>2006</date>
<booktitle>In Proceedings of the ACL Workshop on Sentiment and Subjectivity in Text.</booktitle>
<contexts>
<context position="11584" citStr="Stoyanov and Cardie (2006)" startWordPosition="1812" endWordPosition="1815">ources. More specifically, the source coreference resolution training phase proceeds through the following steps: 1. Source-to-NP mapping: We preprocess each document by running a tokenizer, sentence splitter, POS tagger, parser, and an NP finder. Subsequently, we augment the set of NPs found by the NP finder with the help of a system for named entity detection. We then map the sources to the NPs. Since there is no one-to-one correspondence, we use a set of heuristics to create the mapping. More details about why heuristics are needed and the process used to map sources to NPs can be found in Stoyanov and Cardie (2006). 2. Feature vector creation: We extract a feature vector for every pair of NPs from the preprocessed corpus. We use the features introduced by Ng and Cardie (2002) for the task of coreference resolution. 3. Classifier construction: Using the feature vectors from step 2, we construct a training set containing one training example per document. Each training example consists of the feature vectors for all pairs of NPs in the document, including those that do not map to sources, together with the available coreference information for the source noun phrases (i.e. the noun phrases to which source</context>
<context position="30030" citStr="Stoyanov and Cardie (2006)" startWordPosition="4839" endWordPosition="4842">each baseline using both the development set and the test set from the corresponding MUC corpus. full training set and a smaller training set consisting of half of the documents selected at random), and three different instance selection algorithms6. This variety of classifier and training data settings was motivated by reported differences in performance of coreference resolution approaches w.r.t. these variations (Ng and Cardie, 2002). More details on the different parameter settings and instance selection algorithms as well as trends in the performance of different settings can be found in Stoyanov and Cardie (2006). In the experiments below we report the best performance of each of the two learning algorithms on the MPQA test data. 6.4 Evaluation In addition to the baselines described above, we evaluate StRip both with and without unlabeled data. That is, we train on the MPQA corpus StRip using either all NPs or just opinion source NPs. We use the B3 (Bagga and Baldwin, 1998) evaluation measure as well as precision, recall, and F1 measured on the (positive) pairwise decisions. B3 is a measure widely used for evaluating coreference resolution algorithms. The measure computes the precision and recall for </context>
</contexts>
<marker>Stoyanov, Cardie, 2006</marker>
<rawString>V. Stoyanov and C. Cardie. 2006. Toward opinion summarization: Linking the sources. In Proceedings of the ACL Workshop on Sentiment and Subjectivity in Text.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Stoyanov</author>
<author>C Cardie</author>
<author>J Wiebe</author>
</authors>
<title>MultiPerspective question answering using the OpQA corpus.</title>
<date>2005</date>
<booktitle>In Proceedings ofEMNLP.</booktitle>
<contexts>
<context position="1289" citStr="Stoyanov et al. (2005)" startWordPosition="178" endWordPosition="182">te a new algorithm for the task of source coreference resolution that outperforms competitive baselines. 1 Introduction Sentiment analysis is concerned with extracting attitudes, opinions, evaluations, and sentiment from text. Work in this area has been motivated by the desire to provide information analysis applications in the arenas of government, business, and politics (e.g. Coglianese (2004)). Additionally, sentiment analysis can augment existing NLP applications such as question answering, information retrieval, summarization, and clustering by providing information about sentiment (e.g. Stoyanov et al. (2005), Riloff et al. (2005)). To date, research in the area (see Related Work section) has focused on the problem of extracting sentiment both at the document level (coarse-grained sentiment information), and at the level of sentences, clauses, or individual expressions (finegrained sentiment information). In contrast, our work concerns the summarization of fine-grained information about opinions. In particular, while recent research efforts have shown that fine-grained opinions (e.g. Riloff and Wiebe (2003), Bethard et al. (2004), Wiebe and Riloff (2005)) as well as their sources (e.g. Bethard et </context>
</contexts>
<marker>Stoyanov, Cardie, Wiebe, 2005</marker>
<rawString>V. Stoyanov, C. Cardie, and J. Wiebe. 2005. MultiPerspective question answering using the OpQA corpus. In Proceedings ofEMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Turney</author>
</authors>
<title>Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="7748" citStr="Turney (2002)" startWordPosition="1213" endWordPosition="1214">ows [Target he] is [_ not stable mentally], and thus [_ undeserving] of [Target the ultimate punishment]. − Moussaoui prison for life − −/I ultimate punishment Figure 1: Example text containing opinions (above) and a summary of the opinions (below). Sources and targets of opinions are bracketed; opinion expressions are shown in italics and bracketed with associated polarity, either positive (+) or negative (-). The underlined phrase will be explained later in the paper. a document either positive (“thumbs up”) or negative (“thumbs down”) polarity (e.g. Das and Chen (2001), Pang et al. (2002), Turney (2002), Dave et al. (2003)). Other research has concentrated on analyzing fine-grained opinions at, or below, the sentence level. Recent work, for example, indicates that systems can be trained to recognize opinions and their polarity, strength, and sources to a reasonable degree of accuracy (e.g. Dave et al. (2003), Riloff and Wiebe (2003), Bethard et al. (2004), Wilson et al. (2004), Yu and Hatzivassiloglou (2003), Choi et al. (2005), Kim and Hovy (2005), Wiebe and Riloff (2005)). Our work extends research on fine-grained opinion extraction by augmenting the opinions with additional information th</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>P. Turney. 2002. Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Wagstaff</author>
<author>C Cardie</author>
</authors>
<title>Clustering with instancelevel constraints.</title>
<date>2000</date>
<booktitle>In Proceedings of the 17-th National Conference on Artificial Intelligence and 12-th Conference on Innovative Applications ofArtificial Intelligence.</booktitle>
<contexts>
<context position="15630" citStr="Wagstaff and Cardie (2000)" startWordPosition="2455" endWordPosition="2458">l problem of partially supervised clustering. 4 Partially Supervised Clustering In our desire to perform effective source coreference resolution we arrive at the following learning problem – the learning algorithm is presented with a set of partially specified examples of clusterings and acquires a function that can cluster accurately an unseen set of items, while taking advantage of the unlabeled information in the examples. This setting is to be contrasted with semisupervised clustering (or clustering with constraints), which has received much research attention (e.g. Demiriz et al. (1999), Wagstaff and Cardie (2000), Basu (2005), Davidson and Ravi (2005)). Semi-supervised clustering can be defined as the problem of clustering a set of items in the presence of limited supervisory information such as pairwise constraints (e.g. two items must/cannot be in the same cluster) or labeled points. In contrast to our setting, in the semisupervised case there is no training phase – the algorithm receives all examples (labeled and unlabeled) at the same time together with some distance or cost function and attempts to find a clustering that optimizes a given measure (usually based on the distance or cost function). </context>
</contexts>
<marker>Wagstaff, Cardie, 2000</marker>
<rawString>K. Wagstaff and C. Cardie. 2000. Clustering with instancelevel constraints. In Proceedings of the 17-th National Conference on Artificial Intelligence and 12-th Conference on Innovative Applications ofArtificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
<author>E Riloff</author>
</authors>
<title>Creating subjective and objective sentence classifiers from unannotated texts.</title>
<date>2005</date>
<booktitle>In Proceedings of CICLing.</booktitle>
<contexts>
<context position="1845" citStr="Wiebe and Riloff (2005)" startWordPosition="264" endWordPosition="267"> providing information about sentiment (e.g. Stoyanov et al. (2005), Riloff et al. (2005)). To date, research in the area (see Related Work section) has focused on the problem of extracting sentiment both at the document level (coarse-grained sentiment information), and at the level of sentences, clauses, or individual expressions (finegrained sentiment information). In contrast, our work concerns the summarization of fine-grained information about opinions. In particular, while recent research efforts have shown that fine-grained opinions (e.g. Riloff and Wiebe (2003), Bethard et al. (2004), Wiebe and Riloff (2005)) as well as their sources (e.g. Bethard et al. (2004), Choi et al. (2005), Kim and Hovy (2005)) can be extracted automatically, little has been done to create opinion summaries, where opinions from the same source/target are combined, statistics are computed for each source/target and multiple opinions from the same source to the same target are aggregated. A simple opinion summary is shown in figure 1.1 We expect that this type of opinion summary, based on fine-grained opinion information, will be important for information analysis applications in any domain where the analysis of opinions is</context>
<context position="8227" citStr="Wiebe and Riloff (2005)" startWordPosition="1289" endWordPosition="1292">aper. a document either positive (“thumbs up”) or negative (“thumbs down”) polarity (e.g. Das and Chen (2001), Pang et al. (2002), Turney (2002), Dave et al. (2003)). Other research has concentrated on analyzing fine-grained opinions at, or below, the sentence level. Recent work, for example, indicates that systems can be trained to recognize opinions and their polarity, strength, and sources to a reasonable degree of accuracy (e.g. Dave et al. (2003), Riloff and Wiebe (2003), Bethard et al. (2004), Wilson et al. (2004), Yu and Hatzivassiloglou (2003), Choi et al. (2005), Kim and Hovy (2005), Wiebe and Riloff (2005)). Our work extends research on fine-grained opinion extraction by augmenting the opinions with additional information that allows the creation of concise opinion summaries. In contrast to the opinion extracts produced by Pang and Lee (2004), our summaries are not text extracts, but rather explicitly identify and − Zerkin 337 characterize the relations between opinions and their sources. Coreference resolution. Coreference resolution is a relatively well studied NLP problem (e.g. Morton (2000), Ng and Cardie (2002), Iida et al. (2003), McCallum and Wellner (2003)). Coreference resolution is de</context>
</contexts>
<marker>Wiebe, Riloff, 2005</marker>
<rawString>J. Wiebe and E. Riloff. 2005. Creating subjective and objective sentence classifiers from unannotated texts. In Proceedings of CICLing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
<author>T Wilson</author>
<author>C Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language.</title>
<date>2005</date>
<journal>Language Resources and Evaluation,</journal>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="26301" citStr="Wiebe et al., 2005" startWordPosition="4250" endWordPosition="4253">compute transitive closure by using a Union-Find structure, which runs in time O(log*n), which for practical purposes can be considered linear (O(n)) 3. However, when computing the best information gain for a nominal feature, StRip has to make a pass over the data for each value that the feature takes, while RIPPER can split the data into bags and perform the computation in one pass. 6 Evaluation and Results This section describes the source coreference data set, the baselines, our implementation of StRip, and the results of our experiments. 6.1 Data set For evaluation we use the MPQA corpus (Wiebe et al., 2005).4 The corpus consists of 535 documents from the world press. All documents in the collection are manually annotated with phraselevel opinion information following the annotation scheme of Wiebe et al. (2005). Discussion of the annotation scheme is beyond the scope of this paper; for our purposes it suffices to say that the annotations include the source of each opinion and coreference information for the sources (e.g. source coreference chains). The corpus contains no additional noun phrase coreference information. For our experiments, we randomly split the data set into a training set consis</context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, 1(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wilson</author>
<author>J Wiebe</author>
<author>R Hwa</author>
</authors>
<title>Just how mad are you? Finding strong and weak opinion clauses.</title>
<date>2004</date>
<booktitle>In Proceedings ofAAAI.</booktitle>
<contexts>
<context position="8129" citStr="Wilson et al. (2004)" startWordPosition="1272" endWordPosition="1275">ty, either positive (+) or negative (-). The underlined phrase will be explained later in the paper. a document either positive (“thumbs up”) or negative (“thumbs down”) polarity (e.g. Das and Chen (2001), Pang et al. (2002), Turney (2002), Dave et al. (2003)). Other research has concentrated on analyzing fine-grained opinions at, or below, the sentence level. Recent work, for example, indicates that systems can be trained to recognize opinions and their polarity, strength, and sources to a reasonable degree of accuracy (e.g. Dave et al. (2003), Riloff and Wiebe (2003), Bethard et al. (2004), Wilson et al. (2004), Yu and Hatzivassiloglou (2003), Choi et al. (2005), Kim and Hovy (2005), Wiebe and Riloff (2005)). Our work extends research on fine-grained opinion extraction by augmenting the opinions with additional information that allows the creation of concise opinion summaries. In contrast to the opinion extracts produced by Pang and Lee (2004), our summaries are not text extracts, but rather explicitly identify and − Zerkin 337 characterize the relations between opinions and their sources. Coreference resolution. Coreference resolution is a relatively well studied NLP problem (e.g. Morton (2000), Ng</context>
</contexts>
<marker>Wilson, Wiebe, Hwa, 2004</marker>
<rawString>T. Wilson, J. Wiebe, and R. Hwa. 2004. Just how mad are you? Finding strong and weak opinion clauses. In Proceedings ofAAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I H Witten</author>
<author>E Frank</author>
</authors>
<title>Data Mining: Practical machine learning tools with Java implementations.</title>
<date>2000</date>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco.</location>
<contexts>
<context position="27772" citStr="Witten and Frank, 2000" startWordPosition="4496" endWordPosition="4499">ransitive closure is sublinear in the number of training instances. 4The MPQA corpus is available at http://nrrc.mitre.org/NRRC/publications.htm. 341 ments, although some learning runs were trained on 200 training documents (see next Subsection). The test set contains a total of 4736 source NPs (average of 35.34 source NPs per document) split into 1710 total source NP chains (average of 12.76 chains per document) for an average of 2.77 source NPs per chain. 6.2 Implementation We implemented the StRip algorithm by modifying JRip – the java implementation of RIPPER included in the WEKA toolkit (Witten and Frank, 2000). The WEKA implementation follows the original RIPPER specification. We changed the implementation to incorporate the modifications suggested by the StRip algorithm; we also modified the underlying data representations and data handling techniques for efficiency. Also due to efficiency considerations, we train StRip only on the 200-document training set. 6.3 Competitive baselines We compare the results of the new method to three fully supervised baseline systems, each of which employs the same traditional coreference resolution approach. In particular, we use the aforementioned algorithm propo</context>
</contexts>
<marker>Witten, Frank, 2000</marker>
<rawString>I.H. Witten and E. Frank. 2000. Data Mining: Practical machine learning tools with Java implementations. Morgan Kaufmann, San Francisco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yu</author>
<author>V Hatzivassiloglou</author>
</authors>
<title>Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="8161" citStr="Yu and Hatzivassiloglou (2003)" startWordPosition="1276" endWordPosition="1280">) or negative (-). The underlined phrase will be explained later in the paper. a document either positive (“thumbs up”) or negative (“thumbs down”) polarity (e.g. Das and Chen (2001), Pang et al. (2002), Turney (2002), Dave et al. (2003)). Other research has concentrated on analyzing fine-grained opinions at, or below, the sentence level. Recent work, for example, indicates that systems can be trained to recognize opinions and their polarity, strength, and sources to a reasonable degree of accuracy (e.g. Dave et al. (2003), Riloff and Wiebe (2003), Bethard et al. (2004), Wilson et al. (2004), Yu and Hatzivassiloglou (2003), Choi et al. (2005), Kim and Hovy (2005), Wiebe and Riloff (2005)). Our work extends research on fine-grained opinion extraction by augmenting the opinions with additional information that allows the creation of concise opinion summaries. In contrast to the opinion extracts produced by Pang and Lee (2004), our summaries are not text extracts, but rather explicitly identify and − Zerkin 337 characterize the relations between opinions and their sources. Coreference resolution. Coreference resolution is a relatively well studied NLP problem (e.g. Morton (2000), Ng and Cardie (2002), Iida et al. </context>
</contexts>
<marker>Yu, Hatzivassiloglou, 2003</marker>
<rawString>H. Yu and V. Hatzivassiloglou. 2003. Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences. In Proceedings of EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>