<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000065">
<title confidence="0.99551">
Simplified Feature Set for Arabic Named Entity Recognition
</title>
<author confidence="0.999213">
Ahmed Abdul-Hamid, Kareem Darwish
</author>
<affiliation confidence="0.994193">
Cairo Microsoft Innovation Center
</affiliation>
<address confidence="0.642719">
Cairo, Egypt
</address>
<email confidence="0.995411">
{ahmedab,kareemd}@microsoft.com
</email>
<sectionHeader confidence="0.993816" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999745">
This paper introduces simplified yet effective
features that can robustly identify named enti-
ties in Arabic text without the need for mor-
phological or syntactic analysis or gazetteers.
A CRF sequence labeling model is trained on
features that primarily use character n-gram of
leading and trailing letters in words and word
n-grams. The proposed features help over-
come some of the morphological and ortho-
graphic complexities of Arabic. In comparing
to results in the literature using Arabic specific
features such POS tags on the same dataset
and same CRF implementation, the results in
this paper are lower by 2 F-measure points for
locations, but are better by 8 points for organi-
zations and 9 points for persons.
</bodyText>
<sectionHeader confidence="0.998796" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9299785625">
Named entity recognition (NER) continues to be
an important part of many NLP applications such
as information extraction, machine translation,
and question answering (Benajiba et al., 2008).
NER is concerned with identifying sequences of
words referring to named entities (NE’s) such as
persons, locations, and organizations. For exam-
ple, in the word sequence “Alan Mulally, CEO of
Detroit based Ford Motor Company,” Alan Mu-
lally, Detroit, and Ford Motor Company would
be identified as a person, a location, and an or-
ganization respectively.
Arabic is a Semitic language that present inter-
esting morphological and orthographic challeng-
es that may complicate NER. Some of these
challenges include:
</bodyText>
<listItem confidence="0.99988425">
• Coordinating conjunctions, prepositions,
possessive pronouns, and determiners are
typically attached to words as prefixes or
suffixes.
• Proper names are often common language
words. For example, the proper name
“Iman” also means faith.
• Lack capitalization of proper nouns.
</listItem>
<bodyText confidence="0.985389382352941">
The paper introduces a simplified set of features
that can robustly identify NER for Arabic with-
out the need for morphological or syntactic anal-
ysis. The proposed features include: word lead-
ing and trailing character n-gram features that
help handle prefix and suffix attachment; word
n-gram probability based features that attempt to
capture the distribution of NE’s in text; word
sequence features; and word length.
The contributions of this paper are as follows:
1. Identifying simplified features that work well
for Arabic without gazetteers and without
morphological and syntactic features, leading
to improvements over previously reported re-
sults.
2. Using leading and trailing character n-grams
in words, which help capture valuable mor-
phological and orthographic clues that would
indicate or counter-indicate the presence of
NE’s.
3. Incorporating word language modeling based
features to capture word associations and rela-
tive distribution of named entities in text.
Conditional Random Fields (CRF) sequence la-
beling was used in identifying NE’s, and the ex-
periments were performed on two standard Ara-
bic NER datasets.
The rest of the paper is organized as follows:
Section 2 surveys prior work on Arabic NER;
Section 3 introduces the proposed features and
motivates their use; Section 4 describes experi-
mental setup and evaluation sets; Section 5 re-
ports on experimental results; and Section 6 con-
cludes the paper.
</bodyText>
<sectionHeader confidence="0.981986" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.993588777777778">
Much work has been done on NER with multiple
evaluation forums dedicated to information ex-
traction in general and to NER in specific.
Nadeau and Sekine (2009) surveyed lots of work
on NER for a variety of languages and using a
myriad of techniques. Significant work has been
conducted by Benajiba and colleagues on Arabic
NER (Benajiba and Rosso, 2008; Benajiba et al.,
2008; Benajiba and Rosso, 2007; Benajiba et al.,
</bodyText>
<page confidence="0.970882">
110
</page>
<note confidence="0.637067">
Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 110–115,
Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999473609375">
2007). Benajiba et al. (2007) used a maximum
entropy based classification trained on a feature
set that include the use of gazetteers and a stop-
word list, appearance of a NE in the training set,
leading and trailing word bigrams, and the tag of
the previous word. They reported 80%, 37%,
and 47% F-measure for locations, organizations,
and persons respectively. Benajiba and Rosso
(2007) improved their system by incorporating
POS tags to improve NE boundary detection.
They reported 87%, 46%, and 52% F-measure
for locations, organizations, and persons respec-
tively. Benajiba and Rosso (2008) used CRF
sequence labeling and incorporated many lan-
guage specific features, namely POS tagging,
base-phrase chunking, Arabic tokenization, and
adjectives indicating nationality. They reported
that tokenization generally improved recall. Us-
ing POS tagging generally improved recall at the
expense of precision, leading to overall im-
provement in F-measure. Using all their sug-
gested features they reported 90%, 66%, and
73% F-measure for location, organization, and
persons respectively. In Benajiba et al. (2008),
they examined the same feature set on the Auto-
matic Content Extraction (ACE) datasets using
CRF sequence labeling and Support Vector Ma-
chine (SVM) classifier. They did not report per
category F-measure, but they reported overall
81%, 75%, and 78% macro-average F-measure
for broadcast news and newswire on the ACE
2003, 2004, and 2005 datasets respectively.
Huang (2005) used an HMM based NE recog-
nizer for Arabic and reported 77% F-measure on
the ACE 2003 dataset. Farber et al. (2008) used
POS tags obtained from an Arabic morphological
analyzer to enhance NER. They reported 70% F-
measure on the ACE 2005 dataset. Shaalan and
Raza (2007) reported on a rule-based system that
uses hand crafted grammars and regular expres-
sions in conjunction with gazetteers. They re-
ported upwards of 93% F-measure, but they con-
ducted their experiments on non-standard da-
tasets, making comparison difficult.
McNamee and Mayfield (2002) explored the
training of an SVM classifier using many lan-
guage independent binary features such as lead-
ing and trailing letters in a word, word length,
presence of digits in a word, and capitalization.
They reported promising results for Spanish and
Dutch. In follow on work, Mayfield et al. (2003)
used thousands of language independent features
such character n-grams, capitalization, word
length, and position in a sentence, along with
language dependent features such as POS tags
and BP chunking. For English, they reported
89%, 79%, and 91% F-measure for location, or-
ganization, and persons respectively.
The use of CRF sequence labeling has been
increasing over the past few years (McCallum
and Li, 2003; Nadeau and Sekine, 2009) with
good success (Benajiba and Rosso, 2008).
Though, CRF’s are not guaranteed to be better
than SVM’s (Benajiba et al., 2008).
</bodyText>
<sectionHeader confidence="0.996199" genericHeader="method">
3 NER Features
</sectionHeader>
<bodyText confidence="0.9999853">
For this work, a CRF sequence labeling was
used. The advantage of using CRF is that they
combine HMM-like generative power with clas-
sifier-like discrimination (Lafferty et al., 2001;
Sha and Pereira, 2003). When a CRF makes a
decision on the label to assign to a word, it also
accounts for the previous and succeeding words.
The CRF was trained on a large set of surface
features to minimize the use of Arabic morpho-
logical and syntactic features. Apart from stem-
ming two coordinating conjunctions, no other
Arabic specific features were used.
The features used were as follows:
 Leading and trailing character bigrams (6bi).
For a given word composed of the letter se-
quence , where and are a start and
end word markers respectively, the first three
bigrams ( , , and ) and last three bi-
grams ( , , and ) were used as
features. Using leading and trailing charac-
ter bigrams of a word was an attempt to ac-
count for morphological and orthographic
complexities of Arabic and to capture sur-
face clues that would indicate the presence of
a NE or not. For example, plural forms of
common words in Arabic are often obtained
by attaching the suffixes wn1 (نو) or yn (ني)
for masculine nouns and At (تا) for feminine
nouns. Presence of such plural form markers
would generally indicate a plural noun, but
would counter-indicate a NE. Also, verbs in
present tense start with the letters A (ا), t (ت),
y (ي), and n (ن). These would contribute to
concluding that a word may not be a NE.
Further, coordinate conjunctions, such as f
(ف) and w (و), and prepositions, such as b
(ب), k (ك), and l (ل), composed of single let-
ters are often attached as prefixes to words.
Accounting for them may help overcome
some of the problems associated with not
</bodyText>
<footnote confidence="0.918237">
1 Arabic letters are presented using the Buckwalter
transliteration scheme
</footnote>
<page confidence="0.997658">
111
</page>
<bodyText confidence="0.999799444444444">
stemming. Further, the determiner Al (Jl)
may be a good indicator for proper nouns
particularly in the case of organizations.
This would be captured by the second bi-
gram from the head of the word. If the de-
terminer is preceded by a coordinating con-
junction, the third bigram from the head of
the word would be able to capture this fea-
ture.
</bodyText>
<listItem confidence="0.6630266">
• Leading and trailing character trigrams
(6tri). For a given word composed of the
letter sequence , where and are a start
and end word markers respectively, the first
three trigrams ( , , and ) and last three
</listItem>
<bodyText confidence="0.9997846">
trigrams ( , , and ) were used as
features. The rationale for using these fea-
tures is very similar to that of using character
bigrams. The added value of using character
trigrams, is that they would allow for the
capture of combinations of prefixes and suf-
fixes. For example, a word may begin with
the prefixes w+Al (Jl+,), which are a coordi-
nating conjunction and determiner respec-
tively.
</bodyText>
<listItem confidence="0.877179">
• Leading and trailing character 4-grams
(6quad). For a given word composed of the
</listItem>
<bodyText confidence="0.897198857142857">
letter sequence , where and are a start
and end word markers respectively, the first
three 4 grams ( , , and ) and last three 4
grams ( , , and ) were used as
features. Similar to leading and trailing tri-
grams, these features can capture combina-
tions of prefixes and suffixes.
</bodyText>
<listItem confidence="0.996528333333333">
• Word position (WP). The feature captures
the relative position of a word in a sentence
as follows:
</listItem>
<bodyText confidence="0.739981333333333">
Typically, Arabic is a VSO language. Thus,
NE’s in specific and nouns in general do not
start sentences.
</bodyText>
<listItem confidence="0.99004925">
• Word length (WL). The feature captures the
length of named entities, as some NE’s, par-
ticularly transliterated NE’s, may be longer
than regular words.
• Word unigram probability (1gP). This is
simply the unigram probability of word. Ac-
counting for unigram probability would help
exclude common words. Also, named enti-
ties are often out-of-vocabulary words.
• Word with previous and word with succeed-
ing word-unigram ratio (1gPr). Given a
word wi, these two features are computed as:
</listItem>
<bodyText confidence="0.909283913043478">
( )
( )
( )
( )
This feature would potentially capture major
shifts between word probabilities. For ex-
ample, a named entity is likely to have much
lower probability compared to the word be-
fore it and the word after it.
• Features that account for dependence be-
tween words in a named entity. Popular
NE’s are likely collocations, and words that
make up named entities don’t occur next to
each other by chance. These features are as
follows:
o Word with previous and word with succeed-
ing word bigram (2gP). For a given word wi,
the two bigram probabilities are p(wi-1wi) and
p(wiwi+1). Words composing named entities
are likely conditionally dependent.
o t-test between a word and the word that pre-
cedes and succeeds it (T). Given a word se-
quence wi and wi+1:
</bodyText>
<equation confidence="0.975613666666667">
̅
√
Wher ̅ ( ), ( ) ( ) ,
</equation>
<bodyText confidence="0.8200415">
̅, and N is the number of words in the
corpus (Manning and Schutze, 1999).
o Mutual information between a word and the
word that precedes and succeeds it (MI).
Given a word sequence wi and wi+1:
[ ̅ ] , where ̅ and are identical
</bodyText>
<listItem confidence="0.770294">
to those in the t-test.
• Character n-gram probability (3gCLM).
</listItem>
<bodyText confidence="0.9518949">
Given character trigram language models for
locations, persons, organizations, and non-
NE’s, the four features are just the character
language model probabilities using the four
different language models. The motivation
for these features stem from the likelihood
that NE’s may have a different distribution
of characters particularly for person names.
This stems from the fact that many NE’s are
transliterated names.
</bodyText>
<sectionHeader confidence="0.999233" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<subsectionHeader confidence="0.765832">
4.1 Datasets
</subsectionHeader>
<bodyText confidence="0.994161333333333">
For this work, the NE’s of interest were persons,
locations, and organizations only. Two datasets
were used for the work in this paper. The first
</bodyText>
<page confidence="0.992756">
112
</page>
<bodyText confidence="0.9823818">
was a NE tagged dataset developed by Binajiba
et al. (2007). The Binajiba dataset is composed
of newswire articles totaling more than 150,000
words. The number of different NE’s in the col-
lection are:
</bodyText>
<table confidence="0.634704666666667">
Locations (LOC) 878
Organizations (ORG) 342
Persons (PER) 689
</table>
<bodyText confidence="0.948501090909091">
The second was the Arabic Automatic Content
Extraction (ACE) 2005 dataset. The ACE da-
taset is composed of newswire, broadcast news,
and weblogs. For experiments in this work, the
weblogs portion of the ACE collection was ex-
cluded, because weblogs often include colloquial
Arabic that does not conform to modern standard
Arabic. Also, ACE tags contain many sub-
categories. For example, locations are tagged as
regions, bodies of water, states, etc. All sub-tags
were ignored and were conflated to the base tags
(LOC, ORG, PER). Further, out of the 40 sub-
entity types, entities belonging to the following
13 ACE sub-entity types were excluded because
they require anaphora resolution or they refer to
non-specific NE’s: nominal, pronominal, kind of
entity (as opposed to a specific entity), negative-
ly quantified entity, underspecified entity, ad-
dress, boundary (eg. border), celestial object
(comet), entertainment venue (eg. movie theater),
sport (eg. football), indeterminate (eg. human),
vehicle, and weapon. The total number of words
in the collection is 98,530 words (66,590 from
newswire and 31,940 from broadcast news). The
number of NE’s is as follows:
Locations (LOC) 867
Organizations (ORG) 269
Persons (PER) 524
Since both collections do not follow the same
tagging conventions, training and testing were
conducted separately for each collection. Each
collection was 80/20 split for training and test-
ing.
</bodyText>
<subsectionHeader confidence="0.998719">
4.2 Data Processing and Sequence Labeling
</subsectionHeader>
<bodyText confidence="0.98779355">
Training and testing were done using CRF++
which is a CRF sequence label toolkit. The fol-
lowing processing steps of Arabic were per-
formed:
• The coordinating conjunctions w (9) and f
(,-!), which always appear as the first prefix-
es in a word, were optionally stemmed. w
and f were stemmed using an in-house Ara-
bic stemmer that is a reimplementation of the
stemmer proposed by Lee et al. (2003).
However, stemming w or f could have been
done by stemming the w or f and searching
for the stemmed word in a large Arabic cor-
pus. If the stemmed word appears more than
a certain count, then stemming was appropri-
ate.
• The different forms of alef (A (1),  |(1), &gt; (1),
and &lt; ())) were normalized to A (1), y (Ly) and
Y (Ls) were normalized to y (Ly), and p (S) was
mapped to h (-A).
</bodyText>
<subsectionHeader confidence="0.992722">
4.3 Evaluation
</subsectionHeader>
<bodyText confidence="0.99997975">
The figures of merit for evaluation were preci-
sion, recall, and F-measure (,6 = 1), with evalua-
tion being conducted at the phrase level. Report-
ing experiments with all the different combina-
tions of features would adversely affect the read-
ability of the paper. Thus, to ascertain the con-
tribution of the different features, a set of 15 ex-
periments are being reported for both datasets.
The experiments were conducted using raw Ara-
bic words (3w) and stems (3s). Using the short
names of features (bolded after feature names in
section 3), the experiments were as follows:
</bodyText>
<listItem confidence="0.9998906">
• 3w
• 3w_6bi
• 3w_6bi_6tri
• 3w_6bi_6tri_6quad
• 3w_6bi_6tri_6quad_WL
• 3w_6bi_6tri_6quad_WP
• 3s
• 3s_6bi_6tri_6quad
• 3s_6bi_6tri_6quad_1gP
• 3s_6bi_6tri_6quad_1gPr_1gP
• 3s_6bi_6tri_6quad_2gP
• 3s_6bi_6tri_6quad_3gCLM
• 3s_6bi_6tri_6quad_MI
• 3s_6bi_6tri_6quad_T
• 3s_6bi_6tri_6quad_T_MI
</listItem>
<sectionHeader confidence="0.99424" genericHeader="evaluation">
5 Experimental Results
</sectionHeader>
<bodyText confidence="0.999450857142857">
Table 1 lists the results for the Benajiba and
ACE datasets respectively. Tables 2 and 3 report
the best obtained results for both datasets. The
results include precision (P), recall (R), and F-
measure (F) for NE’s of types location (LOC),
organization (ORG), and person (PER). The best
results for P, R, and F are bolded in the tables.
In comparing the base experiments 3w and 3s in
which the only the surface forms and the stems
were used respectively, both produced the high-
est precision. However, 3s improved recall over
3w by 7, 13, and 14 points for LOC, ORG, and
PER respectively on the Benajiba dataset.
Though using 3s led to a drop in P for ORG
</bodyText>
<page confidence="0.99823">
113
</page>
<bodyText confidence="0.999513818181818">
compared to 3w, it actually led to improvement
in P for PER. Similar results were observed for
the ACE dataset, but the differences were less
pronounced with 1% to 2% improvements in re-
call. However, when including the 6bi, 6tri, and
6quad features the difference between using
words or stems dropped to about 1 point in recall
and nearly no difference in precision. This
would indicate the effectiveness of using leading
and trailing character n-grams in overcoming
morphological and orthographic complexities.
</bodyText>
<table confidence="0.999919000000001">
Benajiba ACE
Run Name Type P R F P R F
3w LOC 96 59 73 88 59 71
ORG 92 36 51 87 50 63
PER 90 32 48 94 47 63
3w_6bi LOC 92 75 82 85 72 78
ORG 83 57 67 76 54 63
PER 87 68 76 89 70 78
3w_6bi_6tri LOC 93 79 86 87 77 82
ORG 82 61 70 77 56 65
PER 89 72 80 89 73 80
3w_6bi_6tri LOC 93 83 87 87 77 81
_6quad
ORG 84 64 72 77 55 65
PER 90 73 81 92 71 80
3w_6bi_6tri LOC 93 82 87 87 78 82
_6quad_WL
ORG 83 64 73 79 56 65
PER 89 73 80 93 71 81
3w_6bi_6tri LOC 91 82 86 88 77 82
_6quad_WP
ORG 83 62 71 77 59 67
PER 89 74 81 91 70 79
3s LOC 96 66 78 89 60 72
ORG 88 49 63 86 52 65
PER 93 46 61 92 49 64
3s_6bi_6tri_ LOC 93 83 88 87 77 82
6quad
ORG 84 63 72 78 58 67
PER 90 74 81 91 70 80
3s_6bi_6tri_ LOC 93 83 88 87 77 82
6quad_1gP
ORG 84 64 73 79 57 66
PER 90 75 82 93 70 80
3s_6bi_6tri_ LOC 93 81 87 87 77 81
6quad_1gPr_
1gP
ORG 85 60 70 82 55 66
PER 91 72 81 93 69 79
3s_6bi_6tri_ LOC 93 81 87 88 77 82
6quad 2gP
ORG 85 61 71 82 56 67
PER 89 74 81 90 69 78
3s_6bi_6tri_ LOC 93 82 87 87 76 81
6quad_3gCL
M
ORG 84 65 74 78 56 66
PER 90 74 81 93 71 81
3s_6bi_6tri_ LOC 93 81 86 87 77 82
6quad MI
ORG 84 59 69 82 56 66
PER 90 72 80 93 70 80
3s_6bi_6tri_ LOC 93 81 87 87 76 81
6quad_T
ORG 85 61 71 82 55 66
PER 90 72 80 93 69 79
3s_6bi_6tri_ LOC 93 80 86 87 76 81
6quad_T_MI
ORG 85 57 68 82 54 65
PER 91 71 80 93 67 78
</table>
<tableCaption confidence="0.9494125">
Table 1: NER results for the Benajiba and
ACE datasets
</tableCaption>
<table confidence="0.974076083333334">
P R F
LOC 93 83 88
ORG 84 64 73
PERS 90 75 82
Avg. 89 74 81
Table 2: Best results on Benajiba dataset
(Run name: 3s_6bi_6tri_6quad_1gP)
P R F
LOC 87 77 82
ORG 79 56 65
PERS 93 71 81
Avg. 88 70 76
</table>
<tableCaption confidence="0.567085">
Table 3: Best results on ACE dataset
(Run name: 3w_6bi_6tri_6quad_WL)
</tableCaption>
<table confidence="0.9999568">
P R F
LOC 93 87 90
ORG 84 54 66
PERS 80 67 73
Avg. 86 69 76
</table>
<tableCaption confidence="0.974856">
Table 4: The results in (Benajiba and Rosso,
2008) on Benajiba dataset
</tableCaption>
<bodyText confidence="0.999982333333333">
The 3s_6bi_6tri_6quad run produced nearly the
best F-measure for both datasets, with extra fea-
tures improving overall F-measure by at most 1
point.
Using t-test T and mutual information NH did
not yield any improvement in either recall or
precision, and often hurt overall F-measure. As
highlighted in the results, the 1gP, 2gP, WL, WP,
and 3gCLM typically improved recall slightly,
often leading to 1 point improvement in overall
F-measure.
To compare to results in the literature, Table 4
reports the results obtained by Benajiba and Ros-
so (2008) on the Benajiba dataset using the
CRF++ implementation of CRF sequence label-
ing trained on a variety of Arabic language spe-
cific features. The comparison was not done on
their results on the ACE 2005 dataset due to po-
tential difference in tags. The averages in Tables
2, 3, and 4 are macro-averages as opposed to mi-
cro-averages reported by Benajiba and Rosso
(2008). In comparing Tables 2 and 4, the fea-
tures suggested in this paper reduced F-measure
for locations by 2 points, but improved F-
measure for organizations and persons by 8
points and 9 points respectively, due to im-
provements in both precision and recall.
</bodyText>
<page confidence="0.995944">
114
</page>
<bodyText confidence="0.999963222222222">
The notable part of this work is that using a sim-
plified feature set outperforms linguistic features.
As explained in Section 3, using leading and
trailing character n-grams implicitly capture
morphological and syntactic features that typical-
ly used for Arabic lemmatization and POS tag-
ging (Diab, 2009). The improvement over using
linguistic features could possibly be attributed to
the following reasons: not all prefixes and suf-
fixes types equally help in identifying named
entities (ex. appearance of a definite article or
not); not all prefixes and suffix surface forms
equally help (ex. appearance of the coordinating
conjunction w “و” vs. f “ف”); and mistakes in
stemming and POS tagging. The lag in recall for
locations behind the work of Benajiba and Rosso
(2008) could be due to the absence of location
gazetteers.
</bodyText>
<sectionHeader confidence="0.997488" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999993095238095">
This paper presented a set of simplified yet effec-
tive features for named entity recognition in Ar-
abic. The features helped overcome some of the
morphological and orthographic complexities of
Arabic. The features included the leading and
trailing character n-grams in words, word associ-
ation features such as t-test, mutual information,
and word n-grams, and surface features such
word length and relative word position in a sen-
tence. The most important features were leading
and trailing character n-grams in words. The
proposed feature set yielded improved results
over those in the literature with as much as 9
point F-measure improvement for recognizing
persons.
For future work, the authors would like to exam-
ine the effectiveness of the proposed feature set
on other morphologically complex languages,
particularly Semitic languages. Also, it is worth
examining the combination of the proposed fea-
tures with morphological features.
</bodyText>
<sectionHeader confidence="0.999273" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999783630769231">
Y. Benajiba, M. Diab, and P. Rosso. 2008. Arabic
Named Entity Recognition using Optimized Fea-
ture Sets. Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Pro-
cessing, pages 284–293, Honolulu, October 2008.
Y. Benajiba and P. Rosso. 2008. Arabic Named Entity
Recognition using Conditional Random Fields. In
Proc. of Workshop on HLT &amp; NLP within the Ar-
abic World, LREC’08.
Y. Benajiba, P. Rosso and J. M. Benedí. 2007. AN-
ERsys: An Arabic Named Entity Recognition sys-
tem based on Maximum Entropy. In Proc. of CI-
CLing-2007, Springer-Verlag, LNCS(4394), pp.
143-153.
Y. Benajiba and P. Rosso. 2007. ANERsys 2.0: Con-
quering the NER task for the Arabic language by
combining the Maximum Entropy with POS-tag in-
formation. In Proc. of Workshop on Natural Lan-
guage-Independent Engineering, IICAI-2007.
M. Diab. 2009. Second Generation Tools (AMIRA
2.0): Fast and Robust Tokenization, POS tagging,
and Base Phrase Chunking. Proceedings of the Se-
cond International Conference on Arabic Language
Resources and Tools, 2009.
B. Farber, D. Freitag, N. Habash, and O. Rambow.
2008. Improving NER in Arabic Using a Morpho-
logical Tagger. In Proc. of LREC’08.
F. Huang. 2005. Multilingual Named Entity Extrac-
tion and Translation from Text and Speech. Ph.D.
Thesis. Pittsburgh: Carnegie Mellon University.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for
segmenting and labeling sequence data, In Proc. of
ICML, pp.282-289, 2001.
Young-Suk Lee, Kishore Papineni, Salim Roukos,
Ossama Emam, Hany Hassan. 2003. Language
Model Based Arabic Word Segmentation. ACL
2003: 399-406
C. Manning and H. Schutze. 1999. Foundations of
Statistical Natural Language Processing. Cam-
bridge, Massachusetts: The MIT Press.
J. Mayfield, P. McNamee, and C. Piatko. 2003.
Named Entity Recognition using Hundreds of
Thousands of Features. HLT-NAACL 2003-
Volume 4, 2003.
A. McCallum and W. Li. 2003. Early Results for
Named Entity Recognition with Conditional Ran-
dom Fields, Features Induction and Web-
Enhanced Lexicons. In Proc. Conference on Com-
putational Natural Language Learning.
P. McNamee and J. Mayfield. 2002. Entity extraction
without language-specific. Proceedings of CoNLL,
2002.
D. Nadeau and S. Sekine. 2009. A survey of named
entity recognition and classification. Named enti-
ties: recognition, classification and use, ed. S.
Sekine and E. Ranchhod, John Benjamins Publish-
ing Company.
F. Sha and F. Pereira. 2003. Shallow parsing with
conditional random fields, In Proc. of
HLT/NAACL-2003.
K. Shaalan and H. Raza. 2007. Person Name Entity
Recognition for Arabic. Proceedings of the 5th
Workshop on Important Unresolved Matters, pages
17–24, Prague, Czech Republic, June 2007.
</reference>
<page confidence="0.999028">
115
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.469335">
<title confidence="0.999856">Simplified Feature Set for Arabic Named Entity Recognition</title>
<author confidence="0.995215">Ahmed Abdul-Hamid</author>
<author confidence="0.995215">Kareem</author>
<affiliation confidence="0.7528485">Cairo Microsoft Innovation Cairo,</affiliation>
<email confidence="0.99989">ahmedab@microsoft.com</email>
<email confidence="0.99989">kareemd@microsoft.com</email>
<abstract confidence="0.995119882352941">This paper introduces simplified yet effective features that can robustly identify named entities in Arabic text without the need for morphological or syntactic analysis or gazetteers. A CRF sequence labeling model is trained on features that primarily use character n-gram of leading and trailing letters in words and word n-grams. The proposed features help overcome some of the morphological and orthographic complexities of Arabic. In comparing to results in the literature using Arabic specific features such POS tags on the same dataset and same CRF implementation, the results in this paper are lower by 2 F-measure points for locations, but are better by 8 points for organizations and 9 points for persons.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Benajiba</author>
<author>M Diab</author>
<author>P Rosso</author>
</authors>
<title>Arabic Named Entity Recognition using Optimized Feature Sets.</title>
<date>2008</date>
<booktitle>Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>284--293</pages>
<location>Honolulu,</location>
<contexts>
<context position="1101" citStr="Benajiba et al., 2008" startWordPosition="162" endWordPosition="165">letters in words and word n-grams. The proposed features help overcome some of the morphological and orthographic complexities of Arabic. In comparing to results in the literature using Arabic specific features such POS tags on the same dataset and same CRF implementation, the results in this paper are lower by 2 F-measure points for locations, but are better by 8 points for organizations and 9 points for persons. 1 Introduction Named entity recognition (NER) continues to be an important part of many NLP applications such as information extraction, machine translation, and question answering (Benajiba et al., 2008). NER is concerned with identifying sequences of words referring to named entities (NE’s) such as persons, locations, and organizations. For example, in the word sequence “Alan Mulally, CEO of Detroit based Ford Motor Company,” Alan Mulally, Detroit, and Ford Motor Company would be identified as a person, a location, and an organization respectively. Arabic is a Semitic language that present interesting morphological and orthographic challenges that may complicate NER. Some of these challenges include: • Coordinating conjunctions, prepositions, possessive pronouns, and determiners are typicall</context>
<context position="3699" citStr="Benajiba et al., 2008" startWordPosition="567" endWordPosition="570">s prior work on Arabic NER; Section 3 introduces the proposed features and motivates their use; Section 4 describes experimental setup and evaluation sets; Section 5 reports on experimental results; and Section 6 concludes the paper. 2 Background Much work has been done on NER with multiple evaluation forums dedicated to information extraction in general and to NER in specific. Nadeau and Sekine (2009) surveyed lots of work on NER for a variety of languages and using a myriad of techniques. Significant work has been conducted by Benajiba and colleagues on Arabic NER (Benajiba and Rosso, 2008; Benajiba et al., 2008; Benajiba and Rosso, 2007; Benajiba et al., 110 Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 110–115, Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics 2007). Benajiba et al. (2007) used a maximum entropy based classification trained on a feature set that include the use of gazetteers and a stopword list, appearance of a NE in the training set, leading and trailing word bigrams, and the tag of the previous word. They reported 80%, 37%, and 47% F-measure for locations, organizations, and persons respectively. Benajiba and Rosso (2007) improved </context>
<context position="5008" citStr="Benajiba et al. (2008)" startWordPosition="762" endWordPosition="765">87%, 46%, and 52% F-measure for locations, organizations, and persons respectively. Benajiba and Rosso (2008) used CRF sequence labeling and incorporated many language specific features, namely POS tagging, base-phrase chunking, Arabic tokenization, and adjectives indicating nationality. They reported that tokenization generally improved recall. Using POS tagging generally improved recall at the expense of precision, leading to overall improvement in F-measure. Using all their suggested features they reported 90%, 66%, and 73% F-measure for location, organization, and persons respectively. In Benajiba et al. (2008), they examined the same feature set on the Automatic Content Extraction (ACE) datasets using CRF sequence labeling and Support Vector Machine (SVM) classifier. They did not report per category F-measure, but they reported overall 81%, 75%, and 78% macro-average F-measure for broadcast news and newswire on the ACE 2003, 2004, and 2005 datasets respectively. Huang (2005) used an HMM based NE recognizer for Arabic and reported 77% F-measure on the ACE 2003 dataset. Farber et al. (2008) used POS tags obtained from an Arabic morphological analyzer to enhance NER. They reported 70% Fmeasure on the </context>
<context position="6792" citStr="Benajiba et al., 2008" startWordPosition="1048" endWordPosition="1051">anish and Dutch. In follow on work, Mayfield et al. (2003) used thousands of language independent features such character n-grams, capitalization, word length, and position in a sentence, along with language dependent features such as POS tags and BP chunking. For English, they reported 89%, 79%, and 91% F-measure for location, organization, and persons respectively. The use of CRF sequence labeling has been increasing over the past few years (McCallum and Li, 2003; Nadeau and Sekine, 2009) with good success (Benajiba and Rosso, 2008). Though, CRF’s are not guaranteed to be better than SVM’s (Benajiba et al., 2008). 3 NER Features For this work, a CRF sequence labeling was used. The advantage of using CRF is that they combine HMM-like generative power with classifier-like discrimination (Lafferty et al., 2001; Sha and Pereira, 2003). When a CRF makes a decision on the label to assign to a word, it also accounts for the previous and succeeding words. The CRF was trained on a large set of surface features to minimize the use of Arabic morphological and syntactic features. Apart from stemming two coordinating conjunctions, no other Arabic specific features were used. The features used were as follows:  Le</context>
</contexts>
<marker>Benajiba, Diab, Rosso, 2008</marker>
<rawString>Y. Benajiba, M. Diab, and P. Rosso. 2008. Arabic Named Entity Recognition using Optimized Feature Sets. Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 284–293, Honolulu, October 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Benajiba</author>
<author>P Rosso</author>
</authors>
<title>Arabic Named Entity Recognition using Conditional Random Fields.</title>
<date>2008</date>
<booktitle>In Proc. of Workshop on HLT &amp; NLP within the Arabic World, LREC’08.</booktitle>
<contexts>
<context position="3676" citStr="Benajiba and Rosso, 2008" startWordPosition="563" endWordPosition="566"> follows: Section 2 surveys prior work on Arabic NER; Section 3 introduces the proposed features and motivates their use; Section 4 describes experimental setup and evaluation sets; Section 5 reports on experimental results; and Section 6 concludes the paper. 2 Background Much work has been done on NER with multiple evaluation forums dedicated to information extraction in general and to NER in specific. Nadeau and Sekine (2009) surveyed lots of work on NER for a variety of languages and using a myriad of techniques. Significant work has been conducted by Benajiba and colleagues on Arabic NER (Benajiba and Rosso, 2008; Benajiba et al., 2008; Benajiba and Rosso, 2007; Benajiba et al., 110 Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 110–115, Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics 2007). Benajiba et al. (2007) used a maximum entropy based classification trained on a feature set that include the use of gazetteers and a stopword list, appearance of a NE in the training set, leading and trailing word bigrams, and the tag of the previous word. They reported 80%, 37%, and 47% F-measure for locations, organizations, and persons respectively. Benajiba and</context>
<context position="6710" citStr="Benajiba and Rosso, 2008" startWordPosition="1034" endWordPosition="1037">sence of digits in a word, and capitalization. They reported promising results for Spanish and Dutch. In follow on work, Mayfield et al. (2003) used thousands of language independent features such character n-grams, capitalization, word length, and position in a sentence, along with language dependent features such as POS tags and BP chunking. For English, they reported 89%, 79%, and 91% F-measure for location, organization, and persons respectively. The use of CRF sequence labeling has been increasing over the past few years (McCallum and Li, 2003; Nadeau and Sekine, 2009) with good success (Benajiba and Rosso, 2008). Though, CRF’s are not guaranteed to be better than SVM’s (Benajiba et al., 2008). 3 NER Features For this work, a CRF sequence labeling was used. The advantage of using CRF is that they combine HMM-like generative power with classifier-like discrimination (Lafferty et al., 2001; Sha and Pereira, 2003). When a CRF makes a decision on the label to assign to a word, it also accounts for the previous and succeeding words. The CRF was trained on a large set of surface features to minimize the use of Arabic morphological and syntactic features. Apart from stemming two coordinating conjunctions, no</context>
<context position="18590" citStr="Benajiba and Rosso, 2008" startWordPosition="3225" endWordPosition="3228">0 72 80 93 70 80 3s_6bi_6tri_ LOC 93 81 87 87 76 81 6quad_T ORG 85 61 71 82 55 66 PER 90 72 80 93 69 79 3s_6bi_6tri_ LOC 93 80 86 87 76 81 6quad_T_MI ORG 85 57 68 82 54 65 PER 91 71 80 93 67 78 Table 1: NER results for the Benajiba and ACE datasets P R F LOC 93 83 88 ORG 84 64 73 PERS 90 75 82 Avg. 89 74 81 Table 2: Best results on Benajiba dataset (Run name: 3s_6bi_6tri_6quad_1gP) P R F LOC 87 77 82 ORG 79 56 65 PERS 93 71 81 Avg. 88 70 76 Table 3: Best results on ACE dataset (Run name: 3w_6bi_6tri_6quad_WL) P R F LOC 93 87 90 ORG 84 54 66 PERS 80 67 73 Avg. 86 69 76 Table 4: The results in (Benajiba and Rosso, 2008) on Benajiba dataset The 3s_6bi_6tri_6quad run produced nearly the best F-measure for both datasets, with extra features improving overall F-measure by at most 1 point. Using t-test T and mutual information NH did not yield any improvement in either recall or precision, and often hurt overall F-measure. As highlighted in the results, the 1gP, 2gP, WL, WP, and 3gCLM typically improved recall slightly, often leading to 1 point improvement in overall F-measure. To compare to results in the literature, Table 4 reports the results obtained by Benajiba and Rosso (2008) on the Benajiba dataset using </context>
<context position="20555" citStr="Benajiba and Rosso (2008)" startWordPosition="3549" endWordPosition="3552">trailing character n-grams implicitly capture morphological and syntactic features that typically used for Arabic lemmatization and POS tagging (Diab, 2009). The improvement over using linguistic features could possibly be attributed to the following reasons: not all prefixes and suffixes types equally help in identifying named entities (ex. appearance of a definite article or not); not all prefixes and suffix surface forms equally help (ex. appearance of the coordinating conjunction w “و” vs. f “ف”); and mistakes in stemming and POS tagging. The lag in recall for locations behind the work of Benajiba and Rosso (2008) could be due to the absence of location gazetteers. 6 Conclusion and Future Work This paper presented a set of simplified yet effective features for named entity recognition in Arabic. The features helped overcome some of the morphological and orthographic complexities of Arabic. The features included the leading and trailing character n-grams in words, word association features such as t-test, mutual information, and word n-grams, and surface features such word length and relative word position in a sentence. The most important features were leading and trailing character n-grams in words. T</context>
</contexts>
<marker>Benajiba, Rosso, 2008</marker>
<rawString>Y. Benajiba and P. Rosso. 2008. Arabic Named Entity Recognition using Conditional Random Fields. In Proc. of Workshop on HLT &amp; NLP within the Arabic World, LREC’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Benajiba</author>
<author>P Rosso</author>
<author>J M Benedí</author>
</authors>
<title>ANERsys: An Arabic Named Entity Recognition system based on Maximum Entropy.</title>
<date>2007</date>
<booktitle>In Proc. of CICLing-2007, Springer-Verlag, LNCS(4394),</booktitle>
<pages>143--153</pages>
<contexts>
<context position="3931" citStr="Benajiba et al. (2007)" startWordPosition="601" endWordPosition="604">r. 2 Background Much work has been done on NER with multiple evaluation forums dedicated to information extraction in general and to NER in specific. Nadeau and Sekine (2009) surveyed lots of work on NER for a variety of languages and using a myriad of techniques. Significant work has been conducted by Benajiba and colleagues on Arabic NER (Benajiba and Rosso, 2008; Benajiba et al., 2008; Benajiba and Rosso, 2007; Benajiba et al., 110 Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 110–115, Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics 2007). Benajiba et al. (2007) used a maximum entropy based classification trained on a feature set that include the use of gazetteers and a stopword list, appearance of a NE in the training set, leading and trailing word bigrams, and the tag of the previous word. They reported 80%, 37%, and 47% F-measure for locations, organizations, and persons respectively. Benajiba and Rosso (2007) improved their system by incorporating POS tags to improve NE boundary detection. They reported 87%, 46%, and 52% F-measure for locations, organizations, and persons respectively. Benajiba and Rosso (2008) used CRF sequence labeling and inco</context>
</contexts>
<marker>Benajiba, Rosso, Benedí, 2007</marker>
<rawString>Y. Benajiba, P. Rosso and J. M. Benedí. 2007. ANERsys: An Arabic Named Entity Recognition system based on Maximum Entropy. In Proc. of CICLing-2007, Springer-Verlag, LNCS(4394), pp. 143-153.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Benajiba</author>
<author>P Rosso</author>
</authors>
<title>ANERsys 2.0: Conquering the NER task for the Arabic language by combining the Maximum Entropy with POS-tag information.</title>
<date>2007</date>
<booktitle>In Proc. of Workshop on Natural Language-Independent Engineering, IICAI-2007.</booktitle>
<contexts>
<context position="3725" citStr="Benajiba and Rosso, 2007" startWordPosition="571" endWordPosition="574">NER; Section 3 introduces the proposed features and motivates their use; Section 4 describes experimental setup and evaluation sets; Section 5 reports on experimental results; and Section 6 concludes the paper. 2 Background Much work has been done on NER with multiple evaluation forums dedicated to information extraction in general and to NER in specific. Nadeau and Sekine (2009) surveyed lots of work on NER for a variety of languages and using a myriad of techniques. Significant work has been conducted by Benajiba and colleagues on Arabic NER (Benajiba and Rosso, 2008; Benajiba et al., 2008; Benajiba and Rosso, 2007; Benajiba et al., 110 Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 110–115, Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics 2007). Benajiba et al. (2007) used a maximum entropy based classification trained on a feature set that include the use of gazetteers and a stopword list, appearance of a NE in the training set, leading and trailing word bigrams, and the tag of the previous word. They reported 80%, 37%, and 47% F-measure for locations, organizations, and persons respectively. Benajiba and Rosso (2007) improved their system by incorporat</context>
</contexts>
<marker>Benajiba, Rosso, 2007</marker>
<rawString>Y. Benajiba and P. Rosso. 2007. ANERsys 2.0: Conquering the NER task for the Arabic language by combining the Maximum Entropy with POS-tag information. In Proc. of Workshop on Natural Language-Independent Engineering, IICAI-2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Diab</author>
</authors>
<title>Second Generation Tools (AMIRA 2.0): Fast and Robust Tokenization, POS tagging, and Base Phrase Chunking.</title>
<date>2009</date>
<booktitle>Proceedings of the Second International Conference on Arabic Language Resources and Tools,</booktitle>
<contexts>
<context position="20086" citStr="Diab, 2009" startWordPosition="3475" endWordPosition="3476">cro-averages reported by Benajiba and Rosso (2008). In comparing Tables 2 and 4, the features suggested in this paper reduced F-measure for locations by 2 points, but improved Fmeasure for organizations and persons by 8 points and 9 points respectively, due to improvements in both precision and recall. 114 The notable part of this work is that using a simplified feature set outperforms linguistic features. As explained in Section 3, using leading and trailing character n-grams implicitly capture morphological and syntactic features that typically used for Arabic lemmatization and POS tagging (Diab, 2009). The improvement over using linguistic features could possibly be attributed to the following reasons: not all prefixes and suffixes types equally help in identifying named entities (ex. appearance of a definite article or not); not all prefixes and suffix surface forms equally help (ex. appearance of the coordinating conjunction w “و” vs. f “ف”); and mistakes in stemming and POS tagging. The lag in recall for locations behind the work of Benajiba and Rosso (2008) could be due to the absence of location gazetteers. 6 Conclusion and Future Work This paper presented a set of simplified yet effe</context>
</contexts>
<marker>Diab, 2009</marker>
<rawString>M. Diab. 2009. Second Generation Tools (AMIRA 2.0): Fast and Robust Tokenization, POS tagging, and Base Phrase Chunking. Proceedings of the Second International Conference on Arabic Language Resources and Tools, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Farber</author>
<author>D Freitag</author>
<author>N Habash</author>
<author>O Rambow</author>
</authors>
<title>Improving NER in Arabic Using a Morphological Tagger.</title>
<date>2008</date>
<booktitle>In Proc. of LREC’08.</booktitle>
<contexts>
<context position="5496" citStr="Farber et al. (2008)" startWordPosition="842" endWordPosition="845">d features they reported 90%, 66%, and 73% F-measure for location, organization, and persons respectively. In Benajiba et al. (2008), they examined the same feature set on the Automatic Content Extraction (ACE) datasets using CRF sequence labeling and Support Vector Machine (SVM) classifier. They did not report per category F-measure, but they reported overall 81%, 75%, and 78% macro-average F-measure for broadcast news and newswire on the ACE 2003, 2004, and 2005 datasets respectively. Huang (2005) used an HMM based NE recognizer for Arabic and reported 77% F-measure on the ACE 2003 dataset. Farber et al. (2008) used POS tags obtained from an Arabic morphological analyzer to enhance NER. They reported 70% Fmeasure on the ACE 2005 dataset. Shaalan and Raza (2007) reported on a rule-based system that uses hand crafted grammars and regular expressions in conjunction with gazetteers. They reported upwards of 93% F-measure, but they conducted their experiments on non-standard datasets, making comparison difficult. McNamee and Mayfield (2002) explored the training of an SVM classifier using many language independent binary features such as leading and trailing letters in a word, word length, presence of di</context>
</contexts>
<marker>Farber, Freitag, Habash, Rambow, 2008</marker>
<rawString>B. Farber, D. Freitag, N. Habash, and O. Rambow. 2008. Improving NER in Arabic Using a Morphological Tagger. In Proc. of LREC’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Huang</author>
</authors>
<title>Multilingual Named Entity Extraction and Translation from Text and Speech.</title>
<date>2005</date>
<tech>Ph.D. Thesis.</tech>
<institution>Pittsburgh: Carnegie Mellon University.</institution>
<contexts>
<context position="5380" citStr="Huang (2005)" startWordPosition="822" endWordPosition="823">ed recall at the expense of precision, leading to overall improvement in F-measure. Using all their suggested features they reported 90%, 66%, and 73% F-measure for location, organization, and persons respectively. In Benajiba et al. (2008), they examined the same feature set on the Automatic Content Extraction (ACE) datasets using CRF sequence labeling and Support Vector Machine (SVM) classifier. They did not report per category F-measure, but they reported overall 81%, 75%, and 78% macro-average F-measure for broadcast news and newswire on the ACE 2003, 2004, and 2005 datasets respectively. Huang (2005) used an HMM based NE recognizer for Arabic and reported 77% F-measure on the ACE 2003 dataset. Farber et al. (2008) used POS tags obtained from an Arabic morphological analyzer to enhance NER. They reported 70% Fmeasure on the ACE 2005 dataset. Shaalan and Raza (2007) reported on a rule-based system that uses hand crafted grammars and regular expressions in conjunction with gazetteers. They reported upwards of 93% F-measure, but they conducted their experiments on non-standard datasets, making comparison difficult. McNamee and Mayfield (2002) explored the training of an SVM classifier using m</context>
</contexts>
<marker>Huang, 2005</marker>
<rawString>F. Huang. 2005. Multilingual Named Entity Extraction and Translation from Text and Speech. Ph.D. Thesis. Pittsburgh: Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data,</title>
<date>2001</date>
<booktitle>In Proc. of ICML,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="6990" citStr="Lafferty et al., 2001" startWordPosition="1080" endWordPosition="1083">language dependent features such as POS tags and BP chunking. For English, they reported 89%, 79%, and 91% F-measure for location, organization, and persons respectively. The use of CRF sequence labeling has been increasing over the past few years (McCallum and Li, 2003; Nadeau and Sekine, 2009) with good success (Benajiba and Rosso, 2008). Though, CRF’s are not guaranteed to be better than SVM’s (Benajiba et al., 2008). 3 NER Features For this work, a CRF sequence labeling was used. The advantage of using CRF is that they combine HMM-like generative power with classifier-like discrimination (Lafferty et al., 2001; Sha and Pereira, 2003). When a CRF makes a decision on the label to assign to a word, it also accounts for the previous and succeeding words. The CRF was trained on a large set of surface features to minimize the use of Arabic morphological and syntactic features. Apart from stemming two coordinating conjunctions, no other Arabic specific features were used. The features used were as follows:  Leading and trailing character bigrams (6bi). For a given word composed of the letter sequence , where and are a start and end word markers respectively, the first three bigrams ( , , and ) and last t</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data, In Proc. of ICML, pp.282-289, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young-Suk Lee</author>
</authors>
<title>Kishore Papineni, Salim Roukos, Ossama Emam, Hany Hassan.</title>
<date>2003</date>
<pages>399--406</pages>
<marker>Lee, 2003</marker>
<rawString>Young-Suk Lee, Kishore Papineni, Salim Roukos, Ossama Emam, Hany Hassan. 2003. Language Model Based Arabic Word Segmentation. ACL 2003: 399-406</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Manning</author>
<author>H Schutze</author>
</authors>
<date>1999</date>
<booktitle>Foundations of Statistical Natural Language Processing.</booktitle>
<publisher>The MIT Press.</publisher>
<location>Cambridge, Massachusetts:</location>
<contexts>
<context position="11468" citStr="Manning and Schutze, 1999" startWordPosition="1890" endWordPosition="1893">hat account for dependence between words in a named entity. Popular NE’s are likely collocations, and words that make up named entities don’t occur next to each other by chance. These features are as follows: o Word with previous and word with succeeding word bigram (2gP). For a given word wi, the two bigram probabilities are p(wi-1wi) and p(wiwi+1). Words composing named entities are likely conditionally dependent. o t-test between a word and the word that precedes and succeeds it (T). Given a word sequence wi and wi+1: ̅ √ Wher ̅ ( ), ( ) ( ) , ̅, and N is the number of words in the corpus (Manning and Schutze, 1999). o Mutual information between a word and the word that precedes and succeeds it (MI). Given a word sequence wi and wi+1: [ ̅ ] , where ̅ and are identical to those in the t-test. • Character n-gram probability (3gCLM). Given character trigram language models for locations, persons, organizations, and nonNE’s, the four features are just the character language model probabilities using the four different language models. The motivation for these features stem from the likelihood that NE’s may have a different distribution of characters particularly for person names. This stems from the fact tha</context>
</contexts>
<marker>Manning, Schutze, 1999</marker>
<rawString>C. Manning and H. Schutze. 1999. Foundations of Statistical Natural Language Processing. Cambridge, Massachusetts: The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Mayfield</author>
<author>P McNamee</author>
<author>C Piatko</author>
</authors>
<title>Named Entity Recognition using Hundreds of Thousands of Features.</title>
<date>2003</date>
<journal>HLT-NAACL</journal>
<volume>2003</volume>
<contexts>
<context position="6228" citStr="Mayfield et al. (2003)" startWordPosition="960" endWordPosition="963">e ACE 2005 dataset. Shaalan and Raza (2007) reported on a rule-based system that uses hand crafted grammars and regular expressions in conjunction with gazetteers. They reported upwards of 93% F-measure, but they conducted their experiments on non-standard datasets, making comparison difficult. McNamee and Mayfield (2002) explored the training of an SVM classifier using many language independent binary features such as leading and trailing letters in a word, word length, presence of digits in a word, and capitalization. They reported promising results for Spanish and Dutch. In follow on work, Mayfield et al. (2003) used thousands of language independent features such character n-grams, capitalization, word length, and position in a sentence, along with language dependent features such as POS tags and BP chunking. For English, they reported 89%, 79%, and 91% F-measure for location, organization, and persons respectively. The use of CRF sequence labeling has been increasing over the past few years (McCallum and Li, 2003; Nadeau and Sekine, 2009) with good success (Benajiba and Rosso, 2008). Though, CRF’s are not guaranteed to be better than SVM’s (Benajiba et al., 2008). 3 NER Features For this work, a CR</context>
</contexts>
<marker>Mayfield, McNamee, Piatko, 2003</marker>
<rawString>J. Mayfield, P. McNamee, and C. Piatko. 2003. Named Entity Recognition using Hundreds of Thousands of Features. HLT-NAACL 2003-Volume 4, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A McCallum</author>
<author>W Li</author>
</authors>
<title>Early Results for Named Entity Recognition with Conditional Random Fields, Features Induction and WebEnhanced Lexicons.</title>
<date>2003</date>
<booktitle>In Proc. Conference on Computational Natural Language Learning.</booktitle>
<contexts>
<context position="6639" citStr="McCallum and Li, 2003" startWordPosition="1023" endWordPosition="1026">es such as leading and trailing letters in a word, word length, presence of digits in a word, and capitalization. They reported promising results for Spanish and Dutch. In follow on work, Mayfield et al. (2003) used thousands of language independent features such character n-grams, capitalization, word length, and position in a sentence, along with language dependent features such as POS tags and BP chunking. For English, they reported 89%, 79%, and 91% F-measure for location, organization, and persons respectively. The use of CRF sequence labeling has been increasing over the past few years (McCallum and Li, 2003; Nadeau and Sekine, 2009) with good success (Benajiba and Rosso, 2008). Though, CRF’s are not guaranteed to be better than SVM’s (Benajiba et al., 2008). 3 NER Features For this work, a CRF sequence labeling was used. The advantage of using CRF is that they combine HMM-like generative power with classifier-like discrimination (Lafferty et al., 2001; Sha and Pereira, 2003). When a CRF makes a decision on the label to assign to a word, it also accounts for the previous and succeeding words. The CRF was trained on a large set of surface features to minimize the use of Arabic morphological and sy</context>
</contexts>
<marker>McCallum, Li, 2003</marker>
<rawString>A. McCallum and W. Li. 2003. Early Results for Named Entity Recognition with Conditional Random Fields, Features Induction and WebEnhanced Lexicons. In Proc. Conference on Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P McNamee</author>
<author>J Mayfield</author>
</authors>
<title>Entity extraction without language-specific.</title>
<date>2002</date>
<booktitle>Proceedings of CoNLL,</booktitle>
<contexts>
<context position="5929" citStr="McNamee and Mayfield (2002)" startWordPosition="910" endWordPosition="913">d newswire on the ACE 2003, 2004, and 2005 datasets respectively. Huang (2005) used an HMM based NE recognizer for Arabic and reported 77% F-measure on the ACE 2003 dataset. Farber et al. (2008) used POS tags obtained from an Arabic morphological analyzer to enhance NER. They reported 70% Fmeasure on the ACE 2005 dataset. Shaalan and Raza (2007) reported on a rule-based system that uses hand crafted grammars and regular expressions in conjunction with gazetteers. They reported upwards of 93% F-measure, but they conducted their experiments on non-standard datasets, making comparison difficult. McNamee and Mayfield (2002) explored the training of an SVM classifier using many language independent binary features such as leading and trailing letters in a word, word length, presence of digits in a word, and capitalization. They reported promising results for Spanish and Dutch. In follow on work, Mayfield et al. (2003) used thousands of language independent features such character n-grams, capitalization, word length, and position in a sentence, along with language dependent features such as POS tags and BP chunking. For English, they reported 89%, 79%, and 91% F-measure for location, organization, and persons res</context>
</contexts>
<marker>McNamee, Mayfield, 2002</marker>
<rawString>P. McNamee and J. Mayfield. 2002. Entity extraction without language-specific. Proceedings of CoNLL, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Nadeau</author>
<author>S Sekine</author>
</authors>
<title>A survey of named entity recognition and classification. Named entities: recognition, classification and use,</title>
<date>2009</date>
<editor>ed. S. Sekine and E. Ranchhod, John</editor>
<publisher>Benjamins Publishing Company.</publisher>
<contexts>
<context position="3483" citStr="Nadeau and Sekine (2009)" startWordPosition="530" endWordPosition="533">xt. Conditional Random Fields (CRF) sequence labeling was used in identifying NE’s, and the experiments were performed on two standard Arabic NER datasets. The rest of the paper is organized as follows: Section 2 surveys prior work on Arabic NER; Section 3 introduces the proposed features and motivates their use; Section 4 describes experimental setup and evaluation sets; Section 5 reports on experimental results; and Section 6 concludes the paper. 2 Background Much work has been done on NER with multiple evaluation forums dedicated to information extraction in general and to NER in specific. Nadeau and Sekine (2009) surveyed lots of work on NER for a variety of languages and using a myriad of techniques. Significant work has been conducted by Benajiba and colleagues on Arabic NER (Benajiba and Rosso, 2008; Benajiba et al., 2008; Benajiba and Rosso, 2007; Benajiba et al., 110 Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 110–115, Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics 2007). Benajiba et al. (2007) used a maximum entropy based classification trained on a feature set that include the use of gazetteers and a stopword list, appearance of a NE in the </context>
<context position="6665" citStr="Nadeau and Sekine, 2009" startWordPosition="1027" endWordPosition="1030">trailing letters in a word, word length, presence of digits in a word, and capitalization. They reported promising results for Spanish and Dutch. In follow on work, Mayfield et al. (2003) used thousands of language independent features such character n-grams, capitalization, word length, and position in a sentence, along with language dependent features such as POS tags and BP chunking. For English, they reported 89%, 79%, and 91% F-measure for location, organization, and persons respectively. The use of CRF sequence labeling has been increasing over the past few years (McCallum and Li, 2003; Nadeau and Sekine, 2009) with good success (Benajiba and Rosso, 2008). Though, CRF’s are not guaranteed to be better than SVM’s (Benajiba et al., 2008). 3 NER Features For this work, a CRF sequence labeling was used. The advantage of using CRF is that they combine HMM-like generative power with classifier-like discrimination (Lafferty et al., 2001; Sha and Pereira, 2003). When a CRF makes a decision on the label to assign to a word, it also accounts for the previous and succeeding words. The CRF was trained on a large set of surface features to minimize the use of Arabic morphological and syntactic features. Apart fr</context>
</contexts>
<marker>Nadeau, Sekine, 2009</marker>
<rawString>D. Nadeau and S. Sekine. 2009. A survey of named entity recognition and classification. Named entities: recognition, classification and use, ed. S. Sekine and E. Ranchhod, John Benjamins Publishing Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sha</author>
<author>F Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields,</title>
<date>2003</date>
<booktitle>In Proc. of HLT/NAACL-2003.</booktitle>
<contexts>
<context position="7014" citStr="Sha and Pereira, 2003" startWordPosition="1084" endWordPosition="1087">ures such as POS tags and BP chunking. For English, they reported 89%, 79%, and 91% F-measure for location, organization, and persons respectively. The use of CRF sequence labeling has been increasing over the past few years (McCallum and Li, 2003; Nadeau and Sekine, 2009) with good success (Benajiba and Rosso, 2008). Though, CRF’s are not guaranteed to be better than SVM’s (Benajiba et al., 2008). 3 NER Features For this work, a CRF sequence labeling was used. The advantage of using CRF is that they combine HMM-like generative power with classifier-like discrimination (Lafferty et al., 2001; Sha and Pereira, 2003). When a CRF makes a decision on the label to assign to a word, it also accounts for the previous and succeeding words. The CRF was trained on a large set of surface features to minimize the use of Arabic morphological and syntactic features. Apart from stemming two coordinating conjunctions, no other Arabic specific features were used. The features used were as follows:  Leading and trailing character bigrams (6bi). For a given word composed of the letter sequence , where and are a start and end word markers respectively, the first three bigrams ( , , and ) and last three bigrams ( , , and )</context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>F. Sha and F. Pereira. 2003. Shallow parsing with conditional random fields, In Proc. of HLT/NAACL-2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Shaalan</author>
<author>H Raza</author>
</authors>
<title>Person Name Entity Recognition for Arabic.</title>
<date>2007</date>
<booktitle>Proceedings of the 5th Workshop on Important Unresolved Matters,</booktitle>
<pages>17--24</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="5649" citStr="Shaalan and Raza (2007)" startWordPosition="868" endWordPosition="871">e same feature set on the Automatic Content Extraction (ACE) datasets using CRF sequence labeling and Support Vector Machine (SVM) classifier. They did not report per category F-measure, but they reported overall 81%, 75%, and 78% macro-average F-measure for broadcast news and newswire on the ACE 2003, 2004, and 2005 datasets respectively. Huang (2005) used an HMM based NE recognizer for Arabic and reported 77% F-measure on the ACE 2003 dataset. Farber et al. (2008) used POS tags obtained from an Arabic morphological analyzer to enhance NER. They reported 70% Fmeasure on the ACE 2005 dataset. Shaalan and Raza (2007) reported on a rule-based system that uses hand crafted grammars and regular expressions in conjunction with gazetteers. They reported upwards of 93% F-measure, but they conducted their experiments on non-standard datasets, making comparison difficult. McNamee and Mayfield (2002) explored the training of an SVM classifier using many language independent binary features such as leading and trailing letters in a word, word length, presence of digits in a word, and capitalization. They reported promising results for Spanish and Dutch. In follow on work, Mayfield et al. (2003) used thousands of la</context>
</contexts>
<marker>Shaalan, Raza, 2007</marker>
<rawString>K. Shaalan and H. Raza. 2007. Person Name Entity Recognition for Arabic. Proceedings of the 5th Workshop on Important Unresolved Matters, pages 17–24, Prague, Czech Republic, June 2007.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>