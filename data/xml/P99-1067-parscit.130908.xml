<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.017081">
<title confidence="0.99757">
Automatic Identification of Word Translations
from Unrelated English and German Corpora
</title>
<author confidence="0.991325">
Reinhard Rapp
</author>
<affiliation confidence="0.8073975">
University of Mainz, FASK
D-76711 Germersheim, Germany
</affiliation>
<email confidence="0.982477">
rapp@usun2.fask.uni-mainz.de
</email>
<sectionHeader confidence="0.997206" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99982552631579">
Algorithms for the alignment of words in
translated texts are well established. How-
ever, only recently new approaches have
been proposed to identify word translations
from non-parallel or even unrelated texts.
This task is more difficult, because most
statistical clues useful in the processing of
parallel texts cannot be applied to non-par-
allel texts. Whereas for parallel texts in
some studies up to 99% of the word align-
ments have been shown to be correct, the
accuracy for non-parallel texts has been
around 30% up to now. The current study,
which is based on the assumption that there
is a correlation between the patterns of word
co-occurrences in corpora of different lan-
guages, makes a significant improvement to
about 72% of word translations identified
correctly.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999980272727273">
Starting with the well-known paper of Brown et
al. (1990) on statistical machine translation,
there has been much scientific interest in the
alignment of sentences and words in translated
texts. Many studies show that for nicely parallel
corpora high accuracy rates of up to 99% can be
achieved for both sentence and word alignment
(Gale &amp; Church, 1993; Kay &amp; Roscheisen,
1993). Of course, in practice — due to omissions,
transpositions, insertions, and replacements in
the process of translation — with real texts there
may be all kinds of problems, and therefore ro-
bustness is still an issue (Langlais et al., 1998).
Nevertheless, the results achieved with these
algorithms have been found useful for the corn-
pilation of dictionaries, for checking the con-
sistency of terminological usage in translations,
for assisting the terminological work of trans-
lators and interpreters, and for example-based
machine translation. By now, some alignment
programs are offered commercially: Translation
memory tools for translators, such as IBM&apos;s
Translation Manager or Trados&apos; Translator&apos;s
Workbench, are bundled or can be upgraded
with programs for sentence alignment.
Most of the proposed algorithms first con-
duct an alignment of sentences, that is, they lo-
cate those pairs of sentences that are translations
of each other. In a second step a word alignment
is performed by analyzing the correspondences
of words in each pair of sentences. The algo-
rithms are usually based on one or several of the
following statistical clues:
</bodyText>
<listItem confidence="0.9482865">
1. correspondence of word and sentence order
1 correlation between word frequencies
3. cognates: similar spelling of words in related
languages
</listItem>
<bodyText confidence="0.9995964375">
All these clues usually work well for parallel
texts. However, despite serious efforts in the
compilation of parallel corpora (Armstrong et
al., 1998), the availability of a large-enough par-
allel corpus in a specific domain and for a given
pair of languages is still an exception. Since the
acquisition of monolingual corpora is much
easier, it would be desirable to have a program
that can determine the translations of words
from comparable (same domain) or possibly
unrelated monolingual texts of two languages.
This is what translators and interpreters usually
do when preparing terminology in a specific
field: They read texts corresponding to this field
in both languages and draw their conclusions on
word correspondences from the usage of the
</bodyText>
<page confidence="0.996603">
519
</page>
<bodyText confidence="0.99995461038961">
terms. Of course, the translators and interpreters
can understand the texts, whereas our programs
are only considering a few statistical clues.
For non-parallel texts the first clue, which is
usually by far the strongest of the three men-
tioned above, is not applicable at all. The second
clue is generally less powerful than the first,
since most words are ambiguous in natural lan-
guages, and many ambiguities are different
across languages. Nevertheless, this clue is ap-
plicable in the case of comparable texts, al-
though with a lower reliability than for parallel
texts. However, in the case of unrelated texts, its
usefulness may be near zero. The third clue is
generally limited to the identification of word
pairs with similar spelling. For all other pairs, it
is usually used in combination with the first
clue. Since the first clue does not work with
non-parallel texts, the third clue is useless for
the identification of the majority of pairs. For
unrelated languages, it is not applicable anyway.
In this situation, Rapp (1995) proposed using
a clue different from the three mentioned above:
His co-occurrence clue is based on the as-
sumption that there is a correlation between co-
occurrence patterns in different languages. For
example, if the words teacher and school co-
occur more often than expected by chance in a
corpus of English, then the German translations
of teacher and school, Lehrer and Schule,
should also co-occur more often than expected
in a corpus of German. In a feasibility study he
showed that this assumption actually holds for
the language pair English/German even in the
case of unrelated texts. When comparing an
English and a German co-occurrence matrix of
corresponding words, he found a high corre-
lation between the co-occurrence patterns of the
two matrices when the rows and columns of
both matrices were in corresponding word order,
and a low correlation when the rows and col-
umns were in random order.
The validity of the co-occurrence clue is ob-
vious for parallel corpora, but — as empirically
shown by Rapp — it also holds for non-parallel
corpora. It can be expected that this clue will
work best with parallel corpora, second-best
with comparable corpora, and somewhat worse
with unrelated corpora. In all three cases, the
problem of robustness — as observed when
applying the word-order clue to parallel corpo-
ra — is not severe. Transpositions of text seg-
ments have virtually no negative effect, and
omissions or insertions are not critical. How-
ever, the co-occurrence clue when applied to
comparable corpora is much weaker than the
word-order clue when applied to parallel cor-
pora, so larger corpora and well-chosen sta-
tistical methods are required.
After an attempt with a context heterogeneity
measure (Fung, 1995) for identifying word
translations, Fung based her later work also on
the co-occurrence assumption (Fung &amp; Yee,
1998; Fung &amp; McKeown, 1997). By presup-
posing a lexicon of seed words, she avoids the
prohibitively expensive computational effort en-
countered by Rapp (1995). The method des-
cribed here — although developed independently
of Fung&apos;s work — goes in the same direction.
Conceptually, it is a trivial case of Rapp&apos;s
matrix permutation method. By simply assuming
an initial lexicon the large number of permu-
tations to be considered is reduced to a much
smaller number of vector comparisons. The
main contribution of this paper is to describe a
practical implementation based on the co-occur-
rence clue that yields good results.
</bodyText>
<sectionHeader confidence="0.984028" genericHeader="introduction">
2 Approach
</sectionHeader>
<bodyText confidence="0.9993303">
As mentioned above, it is assumed that across
languages there is a correlation between the co-
occurrences of words that are translations of
each other. If — for example — in a text of one
language two words A and B co-occur more of-
ten than expected by chance, then in a text of
another language those words that are transla-
tions of A and B should also co-occur more fre-
quently than expected. This is the only statisti-
cal clue used throughout this paper.
It is further assumed that there is a small
dictionary available at the beginning, and that
our aim is to expand this base lexicon. Using a
corpus of the target language, we first compute a
co-occurrence matrix whose rows are all word
types occurring in the corpus and whose col-
umns are all target words appearing in the base
lexicon. We now select a word of the source
language whose translation is to be determined.
Using our source-language corpus, we compute
</bodyText>
<page confidence="0.990321">
520
</page>
<bodyText confidence="0.999972">
a co-occurrence vector for this word. We trans-
late all known words in this vector to the target
language. Since our base lexicon is small, only
some of the translations are known. All un-
known words are discarded from the vector and
the vector positions are sorted in order to match
the vectors of the target-language matrix. With
the resulting vector, we now perform a similar-
ity computation to all vectors in the co-occur-
rence matrix of the target language. The vector
with the highest similarity is considered to be
the translation of our source-language word.
</bodyText>
<sectionHeader confidence="0.997929" genericHeader="method">
3 Simulation
</sectionHeader>
<subsectionHeader confidence="0.998036">
3.1 Language Resources
</subsectionHeader>
<bodyText confidence="0.999912">
To conduct the simulation, a number of resour-
ces were required. These are
</bodyText>
<listItem confidence="0.9945826">
1. a German corpus
2. an English corpus
3. a number of German test words with known
English translations
4. a small base lexicon, German to English
</listItem>
<bodyText confidence="0.99978352">
As the German corpus, we used 135 million
words of the newspaper Frankfurter Allgemeine
Zeitung (1993 to 1996), and as the English
corpus 163 million words of the Guardian (1990
to 1994). Since the orientation of the two
newspapers is quite different, and since the time
spans covered are only in part overlapping, the
two corpora can be considered as more or less
unrelated.
For testing our results, we started with a list
of 100 German test words as proposed by Rus-
sell (1970), which he used for an association
experiment with German subjects. By looking
up the translations for each of these 100 words,
we obtained a test set for evaluation.
Our German/English base lexicon is derived
from the Collins Gem German Dictionary with
about 22,300 entries. From this we eliminated
all multi-word entries, so 16,380 entries re-
mained. Because we had decided on our test
word list beforehand, and since it would not
make much sense to apply our method to words
that are already in the base lexicon, we also re-
moved all entries belonging to the 100 test
words.
</bodyText>
<subsectionHeader confidence="0.999968">
3.2 Pre-processing
</subsectionHeader>
<bodyText confidence="0.998762444444445">
Since our corpora are very large, to save disk
space and processing time we decided to remove
all function words from the texts. This was done
on the basis of a list of approximately 600
German and another list of about 200 English
function words. These lists were compiled by
looking at the closed class words (mainly ar-
ticles, pronouns, and particles) in an English and
a German morphological lexicon (for details see
Lezius, Rapp, &amp; Wettler, 1998) and at word
frequency lists derived from our corpora.&apos; By
eliminating function words, we assumed we
would lose little information: Function words
are often highly ambiguous and their co-occur-
rences are mostly based on syntactic instead of
semantic patterns. Since semantic patterns are
more reliable than syntactic patterns across
language families, we hoped that eliminating the
function words would give our method more
generality.
We also decided to lemmatize our corpora.
Since we were interested in the translations of
base forms only, it was clear that lemmatization
would be useful. It not only reduces the sparse-
data problem but also takes into account that
German is a highly inflectional language,
whereas English is not. For both languages we
conducted a partial lemmatization procedure
that was based only on a morphological lexicon
and did not take the context of a word form into
account. This means that we could not lem-
matize those ambiguous word forms that can be
derived from more than one base form. How-
ever, this is a relatively rare case. (According to
Lezius, Rapp, &amp; Wettler, 1998, 93% of the to-
kens of a German text had only one lemma.) Al-
though we had a context-sensitive lemmatizer
for German available (Lezius, Rapp, &amp; Wettler,
1998), this was not the case for English, so for
reasons of symmetry we decided not to use the
context feature.
I In cases in which an ambiguous word can be both a
content and a function word (e.g., can), preference
was given to those interpretations that appeared to
occur more frequently.
</bodyText>
<page confidence="0.985589">
521
</page>
<subsectionHeader confidence="0.989214">
3.3 Co-occurrence Counting
</subsectionHeader>
<bodyText confidence="0.999826081081081">
For counting word co-occurrences, in most other
studies a fixed window size is chosen and it is
determined how often each pair of words occurs
within a text window of this size. However, this
approach does not take word order within a
window into account. Since it has been empiri-
cally observed that word order of content words
is often similar between languages (even be-
tween unrelated languages such as English and
Chinese), and since this may be a useful statisti-
cal clue, we decided to modify the common ap-
proach in the way proposed by Rapp (1996, p.
162). Instead of computing a single co-occur-
rence vector for a word A, we compute several,
one for each position within the window. For
example, if we have chosen the window size 2,
we would compute a first co-occurrence vector
for the case that word A is two words ahead of
another word B, a second vector for the case that
word A is one word ahead of word B, a third
vector for A directly following B, and a fourth
vector for A following two words after B. If we
added up these four vectors, the result would be
the co-occurrence vector as obtained when not
taking word order into account. However, this is
not what we do. Instead, we combine the four
vectors of length n into a single vector of length
4n.
Since preliminary experiments showed that a
window size of 3 with consideration of word
order seemed to give somewhat better results
than other window types, the results reported
here are based on vectors of this kind. However,
the computational methods described below are
in the same way applicable to window sizes of
any length with or without consideration of
word order.
</bodyText>
<subsectionHeader confidence="0.995947">
3.4 Association Formula
</subsectionHeader>
<bodyText confidence="0.999639148148148">
Our method is based on the assumption that
there is a correlation between the patterns of
word co-occurrences in texts of different lan-
guages. However, as Rapp (1995) proposed, this
correlation may be strengthened by not using the
co-occurrence counts directly, but association
strengths between words instead. The idea is to
eliminate word-frequency effects and to empha-
size significant word pairs by comparing their
observed co-occurrence counts with their ex-
pected co-occurrence counts. In the past, for this
purpose a number of measures have been pro-
posed. They were based on mutual information
(Church &amp; Hanks, 1989), conditional probabili-
ties (Rapp, 1996), or on some standard statisti-
cal tests, such as the chi-square test or the log-
likelihood ratio (Dunning, 1993). For the pur-
pose of this paper, we decided to use the log-
likelihood ratio, which is theoretically well
justified and more appropriate for sparse data
than chi-square. In preliminary experiments it
also led to slightly better results than the con-
ditional probability measure. Results based on
mutual information or co-occurrence counts
were significantly worse. For efficient compu-
tation of the log-likelihood ratio we used the fol-
lowing formula:2
</bodyText>
<equation confidence="0.989992625">
k N
— 2 log 2 = ku log
r, jc{1,2}
k11 N
=k11 log 77---1,1R1 k12 log cki;
+k21 log ---
°g
kC221RNI+k221 Ck222:2
</equation>
<bodyText confidence="0.663439">
where
</bodyText>
<equation confidence="0.985898333333333">
=k1 +k2 C2 1= k21 k22
R, =k11 +k21 R2 =k2 k22
N =k1, +k12 +k2 +k22
</equation>
<bodyText confidence="0.810216">
with parameters kJ expressed in terms of corpus
frequencies:
kli = frequency of common occurrence of
word A and word B
</bodyText>
<equation confidence="0.861891">
k12 = corpus frequency of word A — k11
k21 = corpus frequency of word B — k11
</equation>
<bodyText confidence="0.9589575">
k22 = size of corpus (no. of tokens) — corpus
frequency of A — corpus frequency of B
All co-occurrence vectors were transformed us-
ing this formula. Thereafter, they were nor-
malized in such a way that for each vector the
sum of its entries adds up to one. In the rest of
the paper, we refer to the transformed and nor-
malized vectors as association vectors.
</bodyText>
<footnote confidence="0.9816288">
2 This formulation of the log-likelihood ratio was pro-
posed by Ted Dunning during a discussion on the
corpora mailing list (e-mail of July 22, 1997). It is
faster and more mnemonic than the one in Dunning
(1993).
</footnote>
<page confidence="0.992852">
522
</page>
<subsectionHeader confidence="0.779451">
3.5 Vector Similarity
</subsectionHeader>
<bodyText confidence="0.999812846153846">
To determine the English translation of an un-
known German word, the association vector of
the German word is computed and compared to
all association vectors in the English association
matrix. For comparison, the correspondences
between the vector positions and the columns of
the matrix are determined by using the base
lexicon. Thus, for each vector in the English
matrix a similarity value is computed and the
English words are ranked according to these
values. It is expected that the correct translation
is ranked first in the sorted list.
For vector comparison, different similarity
measures can be considered. Salton &amp; McGill
(1983) proposed a number of measures, such as
the Cosine coefficient, the Jaccard coefficient,
and the Dice coefficient (see also Jones &amp; Fur-
nas, 1987). For the computation of related terms
and synonyms, Ruge (1995), Landauer and
Dumais (1997), and Fung and McKeown (1997)
used the cosine measure, whereas Grefenstette
(1994, p. 48) used a weighted Jaccard measure.
We propose here the city-block metric, which
computes the similarity between two vectors X
and Y as the sum of the absolute differences of
corresponding vector positions:
</bodyText>
<equation confidence="0.912496">
s= lxi—Yil
t=1
</equation>
<bodyText confidence="0.999930588235294">
In a number of experiments we compared it to
other similarity measures, such as the cosine
measure, the Jaccard measure (standard and bi-
nary), the Euclidean distance, and the scalar
product, and found that the city-block metric
yielded the best results. This may seem sur-
prising, since the formula is very simple and the
computational effort smaller than with the other
measures. It must be noted, however, that the
other authors applied their similarity measures
directly to the (log of the) co-occurrence vec-
tors, whereas we applied the measures to the as-
sociation vectors based on the log-likelihood
ratio. According to our observations, estimates
based on the log-likelihood ratio are generally
more reliable across different corpora and lan-
guages.
</bodyText>
<subsectionHeader confidence="0.995462">
3.6 Simulation Procedure
</subsectionHeader>
<bodyText confidence="0.7406095">
The results reported in the next section were
obtained using the following procedure:
1. Based on the word co-occurrences in the
German corpus, for each of the 100 German
test words its association vector was com-
puted. In these vectors, all entries belonging
to words not found in the English part of the
base lexicon were deleted.
2. Based on the word co-occurrences in the
English corpus, an association matrix was
computed whose rows were all word types of
the corpus with a frequency of 100 or higher3
and whose columns were all English words
occurring as first translations of the German
words in the base lexicon!&apos;
3. Using the similarity function, each of the
German vectors was compared to all vectors
of the English matrix. The mapping between
vector positions was based on the first trans-
lations given in the base lexicon. For each of
the German source words, the English vo-
cabulary was ranked according to the re-
sulting similarity value.
3 The limitation to words with frequencies above 99
was introduced for computational reasons to reduce
the number of vector comparisons and thus speed up
the program. (The English corpus contains 657,787
word types after lemmatization, which leads to
extremely large matrices.) The purpose of this
limitation was not to limit the number of translation
candidates considered. Experiments with lower
thresholds showed that this choice has little effect on
the results to our set of test words.
4 This means that alternative translations of a word
were not considered. Another approach, as conducted
by Fung &amp; Yee (1998), would be to consider all
possible translations listed in the lexicon and to give
them equal (or possibly descending) weight. Our
decision was motivated by the observation that many
words have a salient first translation and that this
translation is listed first in the Collins Gem Dictio-
nary German-English. We did not explore this issue
further since in a small pocket dictionary only few
ambiguities are listed.
</bodyText>
<page confidence="0.998419">
523
</page>
<sectionHeader confidence="0.998706" genericHeader="evaluation">
4 Results and Evaluation
</sectionHeader>
<bodyText confidence="0.999976266666667">
Table 1 shows the results for 20 of the 100 Ger-
man test words. For each of these test words, the
top five translations as automatically generated
are listed. In addition, for each word its ex-
pected English translation from the test set is
given together with its position in the ranked
lists of computed translations. The positions in
the ranked lists are a measure for the quality of
the predictions, with a 1 meaning that the pre-
diction is correct and a high value meaning that
the program was far from predicting the correct
word.
If we look at the table, we see that in many
cases the program predicts the expected word,
with other possible translations immediately
following. For example, for the German word
Hauschen, the correct translations bungalow,
cottage, house, and hut are listed. In other cases,
typical associates follow the correct translation.
For example, the correct translation of Mad-
chen, girl, is followed by boy, man, brother, and
lady. This behavior can be expected from our
associationist approach. Unfortunately, in some
cases the correct translation and one of its
strong associates are mixed up, as for example
with Frau, where its correct translation, woman,
is listed only second after its strong associate
man. Another example of this typical kind of
error is pfeifen, where the correct translation
whistle is listed third after linesman and referee.
Let us now look at some cases where the pro-
gram did particularly badly. For Kohl we had
expected its dictionary translation cabbage,
but — given that a substantial part of our news-
paper corpora consists of political texts — we do
not need to further explain why our program
lists Major, Kohl, Thatcher, Gorbachev, and
Bush, state leaders who were in office during
the time period the texts were written. In other
cases, such as Krankheit and Whisky, the simu-
lation program simply preferred the British us-
age of the Guardian over the American usage in
our test set: Instead of sickness, the program
predicted disease and illness, and instead of
whiskey it predicted whisky.
A much more severe problem is that our cur-
rent approach cannot properly handle ambigui-
ties: For the German word weifi it does not pre-
dict white, but instead know. The reason is that
weifi can also be third person singular of the
German verb wissen (to know), which in news-
paper texts is more frequent than the color
white. Since our lemmatizer is not context-sen-
sitive, this word was left unlemmatized, which
explains the result.
To be able to compare our results with other
work, we also did a quantitative evaluation. For
all test words we checked whether the predicted
translation (first word in the ranked list) was
identical to our expected translation. This was
true for 65 of the 100 test words. However, in
some cases the choice of the expected transla-
tion in the test set had been somewhat arbitrary.
For example, for the German word Strafie we
had expected street, but the system predicted
road, which is a translation quite as good.
Therefore, as a better measure for the accuracy
of our system we counted the number of times
where an acceptable translation of the source
word is ranked first. This was true for 72 of the
100 test words, which gives us an accuracy of
72%. In another test, we checked whether an ac-
ceptable translation appeared among the top 10
of the ranked lists. This was true in 89 cases.5
For comparison, Fung &amp; McKeown (1997)
report an accuracy of about 30% when only the
top candidate is counted. However, it must be
emphasized that their result has been achieved
under very different circumstances. On the one
hand, their task was more difficult because they
worked on a pair of unrelated languages (Eng-
lish/Japanese) using smaller corpora and a ran-
dom selection of test words, many of which
were multi-word terms. Also, they predeter-
mined a single translation as being correct. On
the other hand, when conducting their evalua-
tion, Fung &amp; McKeown limited the vocabulary
they considered as translation candidates to a
few hundred terms, which obviously facilitates
the task.
</bodyText>
<footnote confidence="0.937394">
5 We did not check for the completeness of the
translations found (recall), since this measure depends
very much on the size of the dictionary used as the
standard.
</footnote>
<page confidence="0.986456">
524
</page>
<table confidence="0.999365227272727">
German test expected trans- top five translations as automatically generated
word lation and rank
Baby baby 1 baby child mother daughter father
Brot bread 1 bread cheese meat food butter
Frau woman 2 man woman boy friend wife
gelb yellow 1 yellow blue red pink green
Hauschen cottage 2 bungalow cottage house hut village
Kind child 1 child daughter son father mother
Kohl cabbage 17074 Major Kohl Thatcher Gorbachev Bush
Krankheit sickness 86 disease illness Aids patient doctor
Madchen girl 1 girl boy man brother lady
Musik music 1 music theatre musical dance song
Ofen stove 3 heat oven stove house burn
pfeifen whistle 3 linesman referee whistle blow offside
Religion religion 1 religion culture faith religious belief
Schaf sheep 1 sheep cattle cow pig goat
Soldat soldier 1 soldier army troop force civilian
StraBe street 2 road street city town walk
siiB sweet 1 sweet smell delicious taste love
Tabak tobacco 1 tobacco cigarette consumption nicotine drink
weiB white 46 know say thought see think
Whisky whiskey 11 whisky beer Scotch bottle wine
</table>
<tableCaption confidence="0.996943">
Table 1: Results for 20 of the 100 test words (for full list see http://www.fask.uni-mainz.de/user/rapp/)
</tableCaption>
<sectionHeader confidence="0.994598" genericHeader="conclusions">
5 Discussion and Conclusion
</sectionHeader>
<bodyText confidence="0.999985863636364">
The method described can be seen as a simple
case of the gradient descent method proposed by
Rapp (1995), which does not need an initial
lexicon but is computationally prohibitively ex-
pensive. It can also be considered as an exten-
sion from the monolingual to the bilingual case
of the well-established methods for semantic or
syntactic word clustering as proposed by
Schiitze (1993), Grefenstette (1994), Ruge
(1995), Rapp (1996), Lin (1998), and others.
Some of these authors perform a shallow or full
syntactical analysis before constructing the co-
occurrence vectors. Others reduce the size of the
co-occurrence matrices by performing a singular
value decomposition. However, in yet un-
published work we found that at least for the
computation of synonyms and related words
neither syntactical analysis nor singular value
decomposition lead to significantly better results
than the approach described here when applied
to the monolingual case (see also Grefenstette,
1993), so we did not try to include these me-
thods in our system. Nevertheless, both methods
are of technical value since they lead to a re-
duction in the size of the co-occurrence matri-
ces.
Future work has to approach the difficult
problem of ambiguity resolution, which has not
been dealt with here. One possibility would be
to semantically disambiguate the words in the
corpora beforehand, another to look at co-oc-
currences between significant word sequences
instead of co-occurrences between single words.
To conclude with, let us add some specula-
tion by mentioning that the ability to identify
word translations from non-parallel texts can be
seen as an indicator in favor of the associationist
view of human language acquisition (see also
Landauer &amp; Dumais, 1997, and Wettler &amp; Rapp,
1993). It gives us an idea of how it is possible to
derive the meaning of unknown words from
texts by only presupposing a limited number of
known words and then iteratively expanding this
knowledge base. One possibility to get the pro-
</bodyText>
<page confidence="0.994082">
525
</page>
<bodyText confidence="0.999469666666667">
cess going would be to learn vocabulary lists as
in school, another to simply acquire the names
of items in the physical world.
</bodyText>
<sectionHeader confidence="0.996312" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.801393333333333">
I thank Manfred Wettler, Gisela Zunker-Rapp,
Wolfgang Lezius, and Anita Todd for their sup-
port of this work.
</bodyText>
<sectionHeader confidence="0.995714" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999899855555556">
Armstrong, S.; Kempen, M.; Petitpierre, D.; Rapp,
R.; Thompson, H. (1998). Multilingual Corpora for
Cooperation. Proceedings of the 1st International
Conference on Linguistic Resources and Evalua-
tion (LREC), Granada, Vol. 2, 975-980.
Brown, P.; Cocke, J.; Della Pietra, S. A.; Della Pietra,
V. J.; Jelinek, F.; Lafferty, J. D.; Mercer, R. L.;
Rossin, P. S. (1990). A statistical approach to ma-
chine translation. Computational Linguistics, 16(2),
79-85.
Church, K. W.; flanks, P. (1989). Word association
norms, mutual information, and lexicography. In:
Proceedings of the 27th Annual Meeting of the As-
sociation for Computational Linguistics. Vancou-
ver, British Columbia, 76-83.
Dunning, T. (1993). Accurate methods for the sta-
tistics of surprise and coincidence. Computational
Linguistics, 19(1), 61-74.
Fung, P. (1995). Compiling bilingual lexicon entries
from a non-parallel English-Chinese corpus. Pro-
ceedings of the 3rd Annual Workshop on Very
Large Corpora, Boston, Massachusetts, 173-183.
Fung, P.; McKeown, K. (1997). Finding terminology
translations from non-parallel corpora Proceedings
of the 5th Annual Workshop on Very Large Cor-
pora, Hong Kong, 192-202.
Fung, P.; Yee, L. Y. (1998). An IR approach for
translating new words from nonparallel, compa-
rable texts. In: Proceedings of COLING-ACL 1998,
Montreal, Vol. 1, 414-420.
Gale, W. A.; Church, K. W. (1993). A program for
aligning sentences in bilingual corpora. Computa-
tional Linguistics, 19(3), 75-102.
Grefenstette, G. (1993). Evaluation techniques for
automatic semantic extraction: comparing syntactic
and window based approaches. In: Proceedings of
the Workshop on Acquisition of Lexical Knowledge
from Text, Columbus, Ohio.
Grefenstette, G. (1994). Explorations in Automatic
Thesaurus Discovery. Dordrecht: Kluwer.
Jones, W. P.; Furnas, G. W. (1987). Pictures of rele-
vance: a geometric analysis of similarity measures.
Journal of the American Society for Information
Science, 38(6), 420-442.
Kay, M.; Roscheisen, M. (1993). Text-Translation
Alignment. Computational Linguistics, 19(1), 121-
142.
Landauer, T. K.; Dumais, S. T. (1997). A solution to
Plato&apos;s problem: the latent semantic analysis theory
of acquisition, induction, and representation of
knowledge. Psychological Review, 104(2), 211-
240.
Langlais, P.; Simard, M.; Veronis, J. (1998). Methods
and practical issues in evaluating alignment tech-
niques. In: Proceedings of COLING-ACL 1998,
Montreal, Vol. 1, 711-717.
Lezius, W.; Rapp, R.; Wettler, M. (1998). A freely
available morphology system, part-of-speech tag-
ger, and context-sensitive lemmatizer for German.
In: Proceedings of COLING-ACL 1998, Montreal,
Vol. 2, 743-748.
Lin, D. (1998). Automatic Retrieval and Clustering of
Similar Words. In: Proceedings of COLING-ACL
1998, Montreal, Vol. 2, 768-773.
Rapp, R. (1995). Identifying word translations in non-
parallel texts. In: Proceedings of the 33rd Meeting
of the Association for Computational Linguistics.
Cambridge, Massachusetts, 320-322.
Rapp, R. (1996). Die Berechnung von Assoziationen.
Hildesheim: Olms.
Ruge, G. (1995). Human memory models and term
association. Proceedings of the ACM SIGIR Con-
ference, Seattle, 219-227.
Russell, W. A. (1970). The complete German lan-
guage norms for responses to 100 words from the
Kent-Rosanoff word association test. In: L. Post-
man, G. Keppel (eds.): Norms of Word Association.
New York: Academic Press, 53-94.
Salton, G.; McGill, M. (1983). Introduction to Mod-
ern Information Retrieval. New York: McGraw-
Hill.
Schiitze, H. (1993). Part-of-speech induction from
scratch. In: Proceedings of the 3 I st Annual Meet-
ing of the Association for Computational Lingu-
istics, Columbus, Ohio, 251-258.
Wettler, M.; Rapp, R. (1993). Computation of word
associations based on the co-occurrences of words
in large corpora. In: Proceedings of the 1st Work-
shop on Very Large Corpora: Columbus, Ohio, 84-
93.
</reference>
<page confidence="0.998494">
526
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.584103">
<title confidence="0.8415285">Automatic Identification of Word Translations from Unrelated English and German Corpora</title>
<author confidence="0.999785">Reinhard Rapp</author>
<affiliation confidence="0.999737">University of Mainz, FASK</affiliation>
<address confidence="0.999225">D-76711 Germersheim, Germany</address>
<email confidence="0.997606">rapp@usun2.fask.uni-mainz.de</email>
<abstract confidence="0.9926879">Algorithms for the alignment of words in translated texts are well established. However, only recently new approaches have been proposed to identify word translations from non-parallel or even unrelated texts. This task is more difficult, because most statistical clues useful in the processing of parallel texts cannot be applied to non-parallel texts. Whereas for parallel texts in some studies up to 99% of the word alignments have been shown to be correct, the accuracy for non-parallel texts has been around 30% up to now. The current study, which is based on the assumption that there is a correlation between the patterns of word co-occurrences in corpora of different languages, makes a significant improvement to about 72% of word translations identified correctly.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Armstrong</author>
<author>M Kempen</author>
<author>D Petitpierre</author>
<author>R Rapp</author>
<author>H Thompson</author>
</authors>
<title>Multilingual Corpora for Cooperation.</title>
<date>1998</date>
<booktitle>Proceedings of the 1st International Conference on Linguistic Resources and Evaluation (LREC), Granada,</booktitle>
<volume>2</volume>
<pages>975--980</pages>
<contexts>
<context position="2797" citStr="Armstrong et al., 1998" startWordPosition="428" endWordPosition="431">s first conduct an alignment of sentences, that is, they locate those pairs of sentences that are translations of each other. In a second step a word alignment is performed by analyzing the correspondences of words in each pair of sentences. The algorithms are usually based on one or several of the following statistical clues: 1. correspondence of word and sentence order 1 correlation between word frequencies 3. cognates: similar spelling of words in related languages All these clues usually work well for parallel texts. However, despite serious efforts in the compilation of parallel corpora (Armstrong et al., 1998), the availability of a large-enough parallel corpus in a specific domain and for a given pair of languages is still an exception. Since the acquisition of monolingual corpora is much easier, it would be desirable to have a program that can determine the translations of words from comparable (same domain) or possibly unrelated monolingual texts of two languages. This is what translators and interpreters usually do when preparing terminology in a specific field: They read texts corresponding to this field in both languages and draw their conclusions on word correspondences from the usage of the</context>
</contexts>
<marker>Armstrong, Kempen, Petitpierre, Rapp, Thompson, 1998</marker>
<rawString>Armstrong, S.; Kempen, M.; Petitpierre, D.; Rapp, R.; Thompson, H. (1998). Multilingual Corpora for Cooperation. Proceedings of the 1st International Conference on Linguistic Resources and Evaluation (LREC), Granada, Vol. 2, 975-980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>J Cocke</author>
<author>Della Pietra</author>
<author>S A</author>
<author>Della Pietra</author>
<author>V J</author>
<author>F Jelinek</author>
<author>J D Lafferty</author>
<author>R L Mercer</author>
<author>P S Rossin</author>
</authors>
<title>A statistical approach to machine translation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>2</issue>
<pages>79--85</pages>
<contexts>
<context position="1042" citStr="Brown et al. (1990)" startWordPosition="155" endWordPosition="158">difficult, because most statistical clues useful in the processing of parallel texts cannot be applied to non-parallel texts. Whereas for parallel texts in some studies up to 99% of the word alignments have been shown to be correct, the accuracy for non-parallel texts has been around 30% up to now. The current study, which is based on the assumption that there is a correlation between the patterns of word co-occurrences in corpora of different languages, makes a significant improvement to about 72% of word translations identified correctly. 1 Introduction Starting with the well-known paper of Brown et al. (1990) on statistical machine translation, there has been much scientific interest in the alignment of sentences and words in translated texts. Many studies show that for nicely parallel corpora high accuracy rates of up to 99% can be achieved for both sentence and word alignment (Gale &amp; Church, 1993; Kay &amp; Roscheisen, 1993). Of course, in practice — due to omissions, transpositions, insertions, and replacements in the process of translation — with real texts there may be all kinds of problems, and therefore robustness is still an issue (Langlais et al., 1998). Nevertheless, the results achieved wit</context>
</contexts>
<marker>Brown, Cocke, Pietra, A, Pietra, J, Jelinek, Lafferty, Mercer, Rossin, 1990</marker>
<rawString>Brown, P.; Cocke, J.; Della Pietra, S. A.; Della Pietra, V. J.; Jelinek, F.; Lafferty, J. D.; Mercer, R. L.; Rossin, P. S. (1990). A statistical approach to machine translation. Computational Linguistics, 16(2), 79-85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
<author>P flanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography. In:</title>
<date>1989</date>
<booktitle>Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<pages>76--83</pages>
<location>Vancouver, British Columbia,</location>
<marker>Church, flanks, 1989</marker>
<rawString>Church, K. W.; flanks, P. (1989). Word association norms, mutual information, and lexicography. In: Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics. Vancouver, British Columbia, 76-83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<pages>61--74</pages>
<contexts>
<context position="14184" citStr="Dunning, 1993" startWordPosition="2355" endWordPosition="2356">p (1995) proposed, this correlation may be strengthened by not using the co-occurrence counts directly, but association strengths between words instead. The idea is to eliminate word-frequency effects and to emphasize significant word pairs by comparing their observed co-occurrence counts with their expected co-occurrence counts. In the past, for this purpose a number of measures have been proposed. They were based on mutual information (Church &amp; Hanks, 1989), conditional probabilities (Rapp, 1996), or on some standard statistical tests, such as the chi-square test or the loglikelihood ratio (Dunning, 1993). For the purpose of this paper, we decided to use the loglikelihood ratio, which is theoretically well justified and more appropriate for sparse data than chi-square. In preliminary experiments it also led to slightly better results than the conditional probability measure. Results based on mutual information or co-occurrence counts were significantly worse. For efficient computation of the log-likelihood ratio we used the following formula:2 k N — 2 log 2 = ku log r, jc{1,2} k11 N =k11 log 77---1,1R1 k12 log cki; +k21 log --- °g kC221RNI+k221 Ck222:2 where =k1 +k2 C2 1= k21 k22 R, =k11 +k21 </context>
<context position="15580" citStr="Dunning (1993)" startWordPosition="2611" endWordPosition="2612">A — k11 k21 = corpus frequency of word B — k11 k22 = size of corpus (no. of tokens) — corpus frequency of A — corpus frequency of B All co-occurrence vectors were transformed using this formula. Thereafter, they were normalized in such a way that for each vector the sum of its entries adds up to one. In the rest of the paper, we refer to the transformed and normalized vectors as association vectors. 2 This formulation of the log-likelihood ratio was proposed by Ted Dunning during a discussion on the corpora mailing list (e-mail of July 22, 1997). It is faster and more mnemonic than the one in Dunning (1993). 522 3.5 Vector Similarity To determine the English translation of an unknown German word, the association vector of the German word is computed and compared to all association vectors in the English association matrix. For comparison, the correspondences between the vector positions and the columns of the matrix are determined by using the base lexicon. Thus, for each vector in the English matrix a similarity value is computed and the English words are ranked according to these values. It is expected that the correct translation is ranked first in the sorted list. For vector comparison, diff</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>Dunning, T. (1993). Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19(1), 61-74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Fung</author>
</authors>
<title>Compiling bilingual lexicon entries from a non-parallel English-Chinese corpus.</title>
<date>1995</date>
<booktitle>Proceedings of the 3rd Annual Workshop on Very Large Corpora,</booktitle>
<pages>173--183</pages>
<location>Boston, Massachusetts,</location>
<contexts>
<context position="6164" citStr="Fung, 1995" startWordPosition="982" endWordPosition="983">el corpora, second-best with comparable corpora, and somewhat worse with unrelated corpora. In all three cases, the problem of robustness — as observed when applying the word-order clue to parallel corpora — is not severe. Transpositions of text segments have virtually no negative effect, and omissions or insertions are not critical. However, the co-occurrence clue when applied to comparable corpora is much weaker than the word-order clue when applied to parallel corpora, so larger corpora and well-chosen statistical methods are required. After an attempt with a context heterogeneity measure (Fung, 1995) for identifying word translations, Fung based her later work also on the co-occurrence assumption (Fung &amp; Yee, 1998; Fung &amp; McKeown, 1997). By presupposing a lexicon of seed words, she avoids the prohibitively expensive computational effort encountered by Rapp (1995). The method described here — although developed independently of Fung&apos;s work — goes in the same direction. Conceptually, it is a trivial case of Rapp&apos;s matrix permutation method. By simply assuming an initial lexicon the large number of permutations to be considered is reduced to a much smaller number of vector comparisons. The m</context>
</contexts>
<marker>Fung, 1995</marker>
<rawString>Fung, P. (1995). Compiling bilingual lexicon entries from a non-parallel English-Chinese corpus. Proceedings of the 3rd Annual Workshop on Very Large Corpora, Boston, Massachusetts, 173-183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Fung</author>
<author>K McKeown</author>
</authors>
<title>Finding terminology translations from non-parallel corpora</title>
<date>1997</date>
<booktitle>Proceedings of the 5th Annual Workshop on Very Large Corpora, Hong Kong,</booktitle>
<pages>192--202</pages>
<contexts>
<context position="16513" citStr="Fung and McKeown (1997)" startWordPosition="2758" endWordPosition="2761">atrix are determined by using the base lexicon. Thus, for each vector in the English matrix a similarity value is computed and the English words are ranked according to these values. It is expected that the correct translation is ranked first in the sorted list. For vector comparison, different similarity measures can be considered. Salton &amp; McGill (1983) proposed a number of measures, such as the Cosine coefficient, the Jaccard coefficient, and the Dice coefficient (see also Jones &amp; Furnas, 1987). For the computation of related terms and synonyms, Ruge (1995), Landauer and Dumais (1997), and Fung and McKeown (1997) used the cosine measure, whereas Grefenstette (1994, p. 48) used a weighted Jaccard measure. We propose here the city-block metric, which computes the similarity between two vectors X and Y as the sum of the absolute differences of corresponding vector positions: s= lxi—Yil t=1 In a number of experiments we compared it to other similarity measures, such as the cosine measure, the Jaccard measure (standard and binary), the Euclidean distance, and the scalar product, and found that the city-block metric yielded the best results. This may seem surprising, since the formula is very simple and the</context>
<context position="6303" citStr="Fung &amp; McKeown, 1997" startWordPosition="1002" endWordPosition="1005">bustness — as observed when applying the word-order clue to parallel corpora — is not severe. Transpositions of text segments have virtually no negative effect, and omissions or insertions are not critical. However, the co-occurrence clue when applied to comparable corpora is much weaker than the word-order clue when applied to parallel corpora, so larger corpora and well-chosen statistical methods are required. After an attempt with a context heterogeneity measure (Fung, 1995) for identifying word translations, Fung based her later work also on the co-occurrence assumption (Fung &amp; Yee, 1998; Fung &amp; McKeown, 1997). By presupposing a lexicon of seed words, she avoids the prohibitively expensive computational effort encountered by Rapp (1995). The method described here — although developed independently of Fung&apos;s work — goes in the same direction. Conceptually, it is a trivial case of Rapp&apos;s matrix permutation method. By simply assuming an initial lexicon the large number of permutations to be considered is reduced to a much smaller number of vector comparisons. The main contribution of this paper is to describe a practical implementation based on the co-occurrence clue that yields good results. 2 Approa</context>
<context position="22996" citStr="Fung &amp; McKeown (1997)" startWordPosition="3844" endWordPosition="3847">of the expected translation in the test set had been somewhat arbitrary. For example, for the German word Strafie we had expected street, but the system predicted road, which is a translation quite as good. Therefore, as a better measure for the accuracy of our system we counted the number of times where an acceptable translation of the source word is ranked first. This was true for 72 of the 100 test words, which gives us an accuracy of 72%. In another test, we checked whether an acceptable translation appeared among the top 10 of the ranked lists. This was true in 89 cases.5 For comparison, Fung &amp; McKeown (1997) report an accuracy of about 30% when only the top candidate is counted. However, it must be emphasized that their result has been achieved under very different circumstances. On the one hand, their task was more difficult because they worked on a pair of unrelated languages (English/Japanese) using smaller corpora and a random selection of test words, many of which were multi-word terms. Also, they predetermined a single translation as being correct. On the other hand, when conducting their evaluation, Fung &amp; McKeown limited the vocabulary they considered as translation candidates to a few hu</context>
</contexts>
<marker>Fung, McKeown, 1997</marker>
<rawString>Fung, P.; McKeown, K. (1997). Finding terminology translations from non-parallel corpora Proceedings of the 5th Annual Workshop on Very Large Corpora, Hong Kong, 192-202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Fung</author>
<author>L Y Yee</author>
</authors>
<title>An IR approach for translating new words from nonparallel, comparable texts. In:</title>
<date>1998</date>
<booktitle>Proceedings of COLING-ACL</booktitle>
<volume>1</volume>
<pages>414--420</pages>
<location>Montreal,</location>
<contexts>
<context position="6280" citStr="Fung &amp; Yee, 1998" startWordPosition="998" endWordPosition="1001"> the problem of robustness — as observed when applying the word-order clue to parallel corpora — is not severe. Transpositions of text segments have virtually no negative effect, and omissions or insertions are not critical. However, the co-occurrence clue when applied to comparable corpora is much weaker than the word-order clue when applied to parallel corpora, so larger corpora and well-chosen statistical methods are required. After an attempt with a context heterogeneity measure (Fung, 1995) for identifying word translations, Fung based her later work also on the co-occurrence assumption (Fung &amp; Yee, 1998; Fung &amp; McKeown, 1997). By presupposing a lexicon of seed words, she avoids the prohibitively expensive computational effort encountered by Rapp (1995). The method described here — although developed independently of Fung&apos;s work — goes in the same direction. Conceptually, it is a trivial case of Rapp&apos;s matrix permutation method. By simply assuming an initial lexicon the large number of permutations to be considered is reduced to a much smaller number of vector comparisons. The main contribution of this paper is to describe a practical implementation based on the co-occurrence clue that yields</context>
<context position="19137" citStr="Fung &amp; Yee (1998)" startWordPosition="3185" endWordPosition="3188">3 The limitation to words with frequencies above 99 was introduced for computational reasons to reduce the number of vector comparisons and thus speed up the program. (The English corpus contains 657,787 word types after lemmatization, which leads to extremely large matrices.) The purpose of this limitation was not to limit the number of translation candidates considered. Experiments with lower thresholds showed that this choice has little effect on the results to our set of test words. 4 This means that alternative translations of a word were not considered. Another approach, as conducted by Fung &amp; Yee (1998), would be to consider all possible translations listed in the lexicon and to give them equal (or possibly descending) weight. Our decision was motivated by the observation that many words have a salient first translation and that this translation is listed first in the Collins Gem Dictionary German-English. We did not explore this issue further since in a small pocket dictionary only few ambiguities are listed. 523 4 Results and Evaluation Table 1 shows the results for 20 of the 100 German test words. For each of these test words, the top five translations as automatically generated are liste</context>
</contexts>
<marker>Fung, Yee, 1998</marker>
<rawString>Fung, P.; Yee, L. Y. (1998). An IR approach for translating new words from nonparallel, comparable texts. In: Proceedings of COLING-ACL 1998, Montreal, Vol. 1, 414-420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Gale</author>
<author>K W Church</author>
</authors>
<title>A program for aligning sentences in bilingual corpora.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>3</issue>
<pages>75--102</pages>
<contexts>
<context position="1337" citStr="Gale &amp; Church, 1993" startWordPosition="203" endWordPosition="206">now. The current study, which is based on the assumption that there is a correlation between the patterns of word co-occurrences in corpora of different languages, makes a significant improvement to about 72% of word translations identified correctly. 1 Introduction Starting with the well-known paper of Brown et al. (1990) on statistical machine translation, there has been much scientific interest in the alignment of sentences and words in translated texts. Many studies show that for nicely parallel corpora high accuracy rates of up to 99% can be achieved for both sentence and word alignment (Gale &amp; Church, 1993; Kay &amp; Roscheisen, 1993). Of course, in practice — due to omissions, transpositions, insertions, and replacements in the process of translation — with real texts there may be all kinds of problems, and therefore robustness is still an issue (Langlais et al., 1998). Nevertheless, the results achieved with these algorithms have been found useful for the cornpilation of dictionaries, for checking the consistency of terminological usage in translations, for assisting the terminological work of translators and interpreters, and for example-based machine translation. By now, some alignment programs</context>
</contexts>
<marker>Gale, Church, 1993</marker>
<rawString>Gale, W. A.; Church, K. W. (1993). A program for aligning sentences in bilingual corpora. Computational Linguistics, 19(3), 75-102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Grefenstette</author>
</authors>
<title>Evaluation techniques for automatic semantic extraction: comparing syntactic and window based approaches. In:</title>
<date>1993</date>
<booktitle>Proceedings of the Workshop on Acquisition of Lexical Knowledge from Text,</booktitle>
<location>Columbus, Ohio.</location>
<contexts>
<context position="25977" citStr="Grefenstette, 1993" startWordPosition="4324" endWordPosition="4325"> proposed by Schiitze (1993), Grefenstette (1994), Ruge (1995), Rapp (1996), Lin (1998), and others. Some of these authors perform a shallow or full syntactical analysis before constructing the cooccurrence vectors. Others reduce the size of the co-occurrence matrices by performing a singular value decomposition. However, in yet unpublished work we found that at least for the computation of synonyms and related words neither syntactical analysis nor singular value decomposition lead to significantly better results than the approach described here when applied to the monolingual case (see also Grefenstette, 1993), so we did not try to include these methods in our system. Nevertheless, both methods are of technical value since they lead to a reduction in the size of the co-occurrence matrices. Future work has to approach the difficult problem of ambiguity resolution, which has not been dealt with here. One possibility would be to semantically disambiguate the words in the corpora beforehand, another to look at co-occurrences between significant word sequences instead of co-occurrences between single words. To conclude with, let us add some speculation by mentioning that the ability to identify word tra</context>
</contexts>
<marker>Grefenstette, 1993</marker>
<rawString>Grefenstette, G. (1993). Evaluation techniques for automatic semantic extraction: comparing syntactic and window based approaches. In: Proceedings of the Workshop on Acquisition of Lexical Knowledge from Text, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Grefenstette</author>
</authors>
<title>Explorations in Automatic Thesaurus Discovery.</title>
<date>1994</date>
<publisher>Kluwer.</publisher>
<location>Dordrecht:</location>
<contexts>
<context position="16565" citStr="Grefenstette (1994" startWordPosition="2767" endWordPosition="2768"> each vector in the English matrix a similarity value is computed and the English words are ranked according to these values. It is expected that the correct translation is ranked first in the sorted list. For vector comparison, different similarity measures can be considered. Salton &amp; McGill (1983) proposed a number of measures, such as the Cosine coefficient, the Jaccard coefficient, and the Dice coefficient (see also Jones &amp; Furnas, 1987). For the computation of related terms and synonyms, Ruge (1995), Landauer and Dumais (1997), and Fung and McKeown (1997) used the cosine measure, whereas Grefenstette (1994, p. 48) used a weighted Jaccard measure. We propose here the city-block metric, which computes the similarity between two vectors X and Y as the sum of the absolute differences of corresponding vector positions: s= lxi—Yil t=1 In a number of experiments we compared it to other similarity measures, such as the cosine measure, the Jaccard measure (standard and binary), the Euclidean distance, and the scalar product, and found that the city-block metric yielded the best results. This may seem surprising, since the formula is very simple and the computational effort smaller than with the other me</context>
<context position="25407" citStr="Grefenstette (1994)" startWordPosition="4239" endWordPosition="4240">te 46 know say thought see think Whisky whiskey 11 whisky beer Scotch bottle wine Table 1: Results for 20 of the 100 test words (for full list see http://www.fask.uni-mainz.de/user/rapp/) 5 Discussion and Conclusion The method described can be seen as a simple case of the gradient descent method proposed by Rapp (1995), which does not need an initial lexicon but is computationally prohibitively expensive. It can also be considered as an extension from the monolingual to the bilingual case of the well-established methods for semantic or syntactic word clustering as proposed by Schiitze (1993), Grefenstette (1994), Ruge (1995), Rapp (1996), Lin (1998), and others. Some of these authors perform a shallow or full syntactical analysis before constructing the cooccurrence vectors. Others reduce the size of the co-occurrence matrices by performing a singular value decomposition. However, in yet unpublished work we found that at least for the computation of synonyms and related words neither syntactical analysis nor singular value decomposition lead to significantly better results than the approach described here when applied to the monolingual case (see also Grefenstette, 1993), so we did not try to include</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>Grefenstette, G. (1994). Explorations in Automatic Thesaurus Discovery. Dordrecht: Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W P Jones</author>
<author>G W Furnas</author>
</authors>
<title>Pictures of relevance: a geometric analysis of similarity measures.</title>
<date>1987</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>38</volume>
<issue>6</issue>
<pages>420--442</pages>
<contexts>
<context position="16392" citStr="Jones &amp; Furnas, 1987" startWordPosition="2738" endWordPosition="2742">e English association matrix. For comparison, the correspondences between the vector positions and the columns of the matrix are determined by using the base lexicon. Thus, for each vector in the English matrix a similarity value is computed and the English words are ranked according to these values. It is expected that the correct translation is ranked first in the sorted list. For vector comparison, different similarity measures can be considered. Salton &amp; McGill (1983) proposed a number of measures, such as the Cosine coefficient, the Jaccard coefficient, and the Dice coefficient (see also Jones &amp; Furnas, 1987). For the computation of related terms and synonyms, Ruge (1995), Landauer and Dumais (1997), and Fung and McKeown (1997) used the cosine measure, whereas Grefenstette (1994, p. 48) used a weighted Jaccard measure. We propose here the city-block metric, which computes the similarity between two vectors X and Y as the sum of the absolute differences of corresponding vector positions: s= lxi—Yil t=1 In a number of experiments we compared it to other similarity measures, such as the cosine measure, the Jaccard measure (standard and binary), the Euclidean distance, and the scalar product, and foun</context>
</contexts>
<marker>Jones, Furnas, 1987</marker>
<rawString>Jones, W. P.; Furnas, G. W. (1987). Pictures of relevance: a geometric analysis of similarity measures. Journal of the American Society for Information Science, 38(6), 420-442.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kay</author>
<author>M Roscheisen</author>
</authors>
<title>Text-Translation Alignment.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<pages>121--142</pages>
<contexts>
<context position="1362" citStr="Kay &amp; Roscheisen, 1993" startWordPosition="207" endWordPosition="210">y, which is based on the assumption that there is a correlation between the patterns of word co-occurrences in corpora of different languages, makes a significant improvement to about 72% of word translations identified correctly. 1 Introduction Starting with the well-known paper of Brown et al. (1990) on statistical machine translation, there has been much scientific interest in the alignment of sentences and words in translated texts. Many studies show that for nicely parallel corpora high accuracy rates of up to 99% can be achieved for both sentence and word alignment (Gale &amp; Church, 1993; Kay &amp; Roscheisen, 1993). Of course, in practice — due to omissions, transpositions, insertions, and replacements in the process of translation — with real texts there may be all kinds of problems, and therefore robustness is still an issue (Langlais et al., 1998). Nevertheless, the results achieved with these algorithms have been found useful for the cornpilation of dictionaries, for checking the consistency of terminological usage in translations, for assisting the terminological work of translators and interpreters, and for example-based machine translation. By now, some alignment programs are offered commercially</context>
</contexts>
<marker>Kay, Roscheisen, 1993</marker>
<rawString>Kay, M.; Roscheisen, M. (1993). Text-Translation Alignment. Computational Linguistics, 19(1), 121-142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Landauer</author>
<author>S T Dumais</author>
</authors>
<title>A solution to Plato&apos;s problem: the latent semantic analysis theory of acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<pages>211--240</pages>
<contexts>
<context position="16484" citStr="Landauer and Dumais (1997)" startWordPosition="2753" endWordPosition="2756">sitions and the columns of the matrix are determined by using the base lexicon. Thus, for each vector in the English matrix a similarity value is computed and the English words are ranked according to these values. It is expected that the correct translation is ranked first in the sorted list. For vector comparison, different similarity measures can be considered. Salton &amp; McGill (1983) proposed a number of measures, such as the Cosine coefficient, the Jaccard coefficient, and the Dice coefficient (see also Jones &amp; Furnas, 1987). For the computation of related terms and synonyms, Ruge (1995), Landauer and Dumais (1997), and Fung and McKeown (1997) used the cosine measure, whereas Grefenstette (1994, p. 48) used a weighted Jaccard measure. We propose here the city-block metric, which computes the similarity between two vectors X and Y as the sum of the absolute differences of corresponding vector positions: s= lxi—Yil t=1 In a number of experiments we compared it to other similarity measures, such as the cosine measure, the Jaccard measure (standard and binary), the Euclidean distance, and the scalar product, and found that the city-block metric yielded the best results. This may seem surprising, since the f</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Landauer, T. K.; Dumais, S. T. (1997). A solution to Plato&apos;s problem: the latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological Review, 104(2), 211-240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Langlais</author>
<author>M Simard</author>
<author>J Veronis</author>
</authors>
<title>Methods and practical issues in evaluating alignment techniques. In:</title>
<date>1998</date>
<booktitle>Proceedings of COLING-ACL</booktitle>
<volume>1</volume>
<pages>711--717</pages>
<location>Montreal,</location>
<contexts>
<context position="1602" citStr="Langlais et al., 1998" startWordPosition="247" endWordPosition="250">tion Starting with the well-known paper of Brown et al. (1990) on statistical machine translation, there has been much scientific interest in the alignment of sentences and words in translated texts. Many studies show that for nicely parallel corpora high accuracy rates of up to 99% can be achieved for both sentence and word alignment (Gale &amp; Church, 1993; Kay &amp; Roscheisen, 1993). Of course, in practice — due to omissions, transpositions, insertions, and replacements in the process of translation — with real texts there may be all kinds of problems, and therefore robustness is still an issue (Langlais et al., 1998). Nevertheless, the results achieved with these algorithms have been found useful for the cornpilation of dictionaries, for checking the consistency of terminological usage in translations, for assisting the terminological work of translators and interpreters, and for example-based machine translation. By now, some alignment programs are offered commercially: Translation memory tools for translators, such as IBM&apos;s Translation Manager or Trados&apos; Translator&apos;s Workbench, are bundled or can be upgraded with programs for sentence alignment. Most of the proposed algorithms first conduct an alignment</context>
</contexts>
<marker>Langlais, Simard, Veronis, 1998</marker>
<rawString>Langlais, P.; Simard, M.; Veronis, J. (1998). Methods and practical issues in evaluating alignment techniques. In: Proceedings of COLING-ACL 1998, Montreal, Vol. 1, 711-717.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Lezius</author>
<author>R Rapp</author>
<author>M Wettler</author>
</authors>
<title>A freely available morphology system, part-of-speech tagger, and context-sensitive lemmatizer for German. In:</title>
<date>1998</date>
<booktitle>Proceedings of COLING-ACL</booktitle>
<volume>2</volume>
<pages>743--748</pages>
<location>Montreal,</location>
<contexts>
<context position="10169" citStr="Lezius, Rapp, &amp; Wettler, 1998" startWordPosition="1670" endWordPosition="1674">nce it would not make much sense to apply our method to words that are already in the base lexicon, we also removed all entries belonging to the 100 test words. 3.2 Pre-processing Since our corpora are very large, to save disk space and processing time we decided to remove all function words from the texts. This was done on the basis of a list of approximately 600 German and another list of about 200 English function words. These lists were compiled by looking at the closed class words (mainly articles, pronouns, and particles) in an English and a German morphological lexicon (for details see Lezius, Rapp, &amp; Wettler, 1998) and at word frequency lists derived from our corpora.&apos; By eliminating function words, we assumed we would lose little information: Function words are often highly ambiguous and their co-occurrences are mostly based on syntactic instead of semantic patterns. Since semantic patterns are more reliable than syntactic patterns across language families, we hoped that eliminating the function words would give our method more generality. We also decided to lemmatize our corpora. Since we were interested in the translations of base forms only, it was clear that lemmatization would be useful. It not o</context>
<context position="11430" citStr="Lezius, Rapp, &amp; Wettler, 1998" startWordPosition="1878" endWordPosition="1882">em but also takes into account that German is a highly inflectional language, whereas English is not. For both languages we conducted a partial lemmatization procedure that was based only on a morphological lexicon and did not take the context of a word form into account. This means that we could not lemmatize those ambiguous word forms that can be derived from more than one base form. However, this is a relatively rare case. (According to Lezius, Rapp, &amp; Wettler, 1998, 93% of the tokens of a German text had only one lemma.) Although we had a context-sensitive lemmatizer for German available (Lezius, Rapp, &amp; Wettler, 1998), this was not the case for English, so for reasons of symmetry we decided not to use the context feature. I In cases in which an ambiguous word can be both a content and a function word (e.g., can), preference was given to those interpretations that appeared to occur more frequently. 521 3.3 Co-occurrence Counting For counting word co-occurrences, in most other studies a fixed window size is chosen and it is determined how often each pair of words occurs within a text window of this size. However, this approach does not take word order within a window into account. Since it has been empirica</context>
</contexts>
<marker>Lezius, Rapp, Wettler, 1998</marker>
<rawString>Lezius, W.; Rapp, R.; Wettler, M. (1998). A freely available morphology system, part-of-speech tagger, and context-sensitive lemmatizer for German. In: Proceedings of COLING-ACL 1998, Montreal, Vol. 2, 743-748.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic Retrieval and Clustering of Similar Words. In:</title>
<date>1998</date>
<booktitle>Proceedings of COLING-ACL</booktitle>
<volume>2</volume>
<pages>768--773</pages>
<location>Montreal,</location>
<contexts>
<context position="25445" citStr="Lin (1998)" startWordPosition="4245" endWordPosition="4246"> 11 whisky beer Scotch bottle wine Table 1: Results for 20 of the 100 test words (for full list see http://www.fask.uni-mainz.de/user/rapp/) 5 Discussion and Conclusion The method described can be seen as a simple case of the gradient descent method proposed by Rapp (1995), which does not need an initial lexicon but is computationally prohibitively expensive. It can also be considered as an extension from the monolingual to the bilingual case of the well-established methods for semantic or syntactic word clustering as proposed by Schiitze (1993), Grefenstette (1994), Ruge (1995), Rapp (1996), Lin (1998), and others. Some of these authors perform a shallow or full syntactical analysis before constructing the cooccurrence vectors. Others reduce the size of the co-occurrence matrices by performing a singular value decomposition. However, in yet unpublished work we found that at least for the computation of synonyms and related words neither syntactical analysis nor singular value decomposition lead to significantly better results than the approach described here when applied to the monolingual case (see also Grefenstette, 1993), so we did not try to include these methods in our system. Neverthe</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Lin, D. (1998). Automatic Retrieval and Clustering of Similar Words. In: Proceedings of COLING-ACL 1998, Montreal, Vol. 2, 768-773.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rapp</author>
</authors>
<title>Identifying word translations in nonparallel texts. In:</title>
<date>1995</date>
<booktitle>Proceedings of the 33rd Meeting of the Association for Computational Linguistics.</booktitle>
<pages>320--322</pages>
<location>Cambridge, Massachusetts,</location>
<contexts>
<context position="4438" citStr="Rapp (1995)" startWordPosition="698" endWordPosition="699">cross languages. Nevertheless, this clue is applicable in the case of comparable texts, although with a lower reliability than for parallel texts. However, in the case of unrelated texts, its usefulness may be near zero. The third clue is generally limited to the identification of word pairs with similar spelling. For all other pairs, it is usually used in combination with the first clue. Since the first clue does not work with non-parallel texts, the third clue is useless for the identification of the majority of pairs. For unrelated languages, it is not applicable anyway. In this situation, Rapp (1995) proposed using a clue different from the three mentioned above: His co-occurrence clue is based on the assumption that there is a correlation between cooccurrence patterns in different languages. For example, if the words teacher and school cooccur more often than expected by chance in a corpus of English, then the German translations of teacher and school, Lehrer and Schule, should also co-occur more often than expected in a corpus of German. In a feasibility study he showed that this assumption actually holds for the language pair English/German even in the case of unrelated texts. When com</context>
<context position="6432" citStr="Rapp (1995)" startWordPosition="1024" endWordPosition="1025">y no negative effect, and omissions or insertions are not critical. However, the co-occurrence clue when applied to comparable corpora is much weaker than the word-order clue when applied to parallel corpora, so larger corpora and well-chosen statistical methods are required. After an attempt with a context heterogeneity measure (Fung, 1995) for identifying word translations, Fung based her later work also on the co-occurrence assumption (Fung &amp; Yee, 1998; Fung &amp; McKeown, 1997). By presupposing a lexicon of seed words, she avoids the prohibitively expensive computational effort encountered by Rapp (1995). The method described here — although developed independently of Fung&apos;s work — goes in the same direction. Conceptually, it is a trivial case of Rapp&apos;s matrix permutation method. By simply assuming an initial lexicon the large number of permutations to be considered is reduced to a much smaller number of vector comparisons. The main contribution of this paper is to describe a practical implementation based on the co-occurrence clue that yields good results. 2 Approach As mentioned above, it is assumed that across languages there is a correlation between the cooccurrences of words that are tra</context>
<context position="13578" citStr="Rapp (1995)" startWordPosition="2262" endWordPosition="2263"> length n into a single vector of length 4n. Since preliminary experiments showed that a window size of 3 with consideration of word order seemed to give somewhat better results than other window types, the results reported here are based on vectors of this kind. However, the computational methods described below are in the same way applicable to window sizes of any length with or without consideration of word order. 3.4 Association Formula Our method is based on the assumption that there is a correlation between the patterns of word co-occurrences in texts of different languages. However, as Rapp (1995) proposed, this correlation may be strengthened by not using the co-occurrence counts directly, but association strengths between words instead. The idea is to eliminate word-frequency effects and to emphasize significant word pairs by comparing their observed co-occurrence counts with their expected co-occurrence counts. In the past, for this purpose a number of measures have been proposed. They were based on mutual information (Church &amp; Hanks, 1989), conditional probabilities (Rapp, 1996), or on some standard statistical tests, such as the chi-square test or the loglikelihood ratio (Dunning,</context>
<context position="25108" citStr="Rapp (1995)" startWordPosition="4193" endWordPosition="4194">1 religion culture faith religious belief Schaf sheep 1 sheep cattle cow pig goat Soldat soldier 1 soldier army troop force civilian StraBe street 2 road street city town walk siiB sweet 1 sweet smell delicious taste love Tabak tobacco 1 tobacco cigarette consumption nicotine drink weiB white 46 know say thought see think Whisky whiskey 11 whisky beer Scotch bottle wine Table 1: Results for 20 of the 100 test words (for full list see http://www.fask.uni-mainz.de/user/rapp/) 5 Discussion and Conclusion The method described can be seen as a simple case of the gradient descent method proposed by Rapp (1995), which does not need an initial lexicon but is computationally prohibitively expensive. It can also be considered as an extension from the monolingual to the bilingual case of the well-established methods for semantic or syntactic word clustering as proposed by Schiitze (1993), Grefenstette (1994), Ruge (1995), Rapp (1996), Lin (1998), and others. Some of these authors perform a shallow or full syntactical analysis before constructing the cooccurrence vectors. Others reduce the size of the co-occurrence matrices by performing a singular value decomposition. However, in yet unpublished work we</context>
</contexts>
<marker>Rapp, 1995</marker>
<rawString>Rapp, R. (1995). Identifying word translations in nonparallel texts. In: Proceedings of the 33rd Meeting of the Association for Computational Linguistics. Cambridge, Massachusetts, 320-322.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rapp</author>
</authors>
<title>Die Berechnung von Assoziationen.</title>
<date>1996</date>
<location>Hildesheim: Olms.</location>
<contexts>
<context position="12298" citStr="Rapp (1996" startWordPosition="2034" endWordPosition="2035">ed to occur more frequently. 521 3.3 Co-occurrence Counting For counting word co-occurrences, in most other studies a fixed window size is chosen and it is determined how often each pair of words occurs within a text window of this size. However, this approach does not take word order within a window into account. Since it has been empirically observed that word order of content words is often similar between languages (even between unrelated languages such as English and Chinese), and since this may be a useful statistical clue, we decided to modify the common approach in the way proposed by Rapp (1996, p. 162). Instead of computing a single co-occurrence vector for a word A, we compute several, one for each position within the window. For example, if we have chosen the window size 2, we would compute a first co-occurrence vector for the case that word A is two words ahead of another word B, a second vector for the case that word A is one word ahead of word B, a third vector for A directly following B, and a fourth vector for A following two words after B. If we added up these four vectors, the result would be the co-occurrence vector as obtained when not taking word order into account. How</context>
<context position="14073" citStr="Rapp, 1996" startWordPosition="2336" endWordPosition="2337">s a correlation between the patterns of word co-occurrences in texts of different languages. However, as Rapp (1995) proposed, this correlation may be strengthened by not using the co-occurrence counts directly, but association strengths between words instead. The idea is to eliminate word-frequency effects and to emphasize significant word pairs by comparing their observed co-occurrence counts with their expected co-occurrence counts. In the past, for this purpose a number of measures have been proposed. They were based on mutual information (Church &amp; Hanks, 1989), conditional probabilities (Rapp, 1996), or on some standard statistical tests, such as the chi-square test or the loglikelihood ratio (Dunning, 1993). For the purpose of this paper, we decided to use the loglikelihood ratio, which is theoretically well justified and more appropriate for sparse data than chi-square. In preliminary experiments it also led to slightly better results than the conditional probability measure. Results based on mutual information or co-occurrence counts were significantly worse. For efficient computation of the log-likelihood ratio we used the following formula:2 k N — 2 log 2 = ku log r, jc{1,2} k11 N =</context>
<context position="25433" citStr="Rapp (1996)" startWordPosition="4243" endWordPosition="4244">hisky whiskey 11 whisky beer Scotch bottle wine Table 1: Results for 20 of the 100 test words (for full list see http://www.fask.uni-mainz.de/user/rapp/) 5 Discussion and Conclusion The method described can be seen as a simple case of the gradient descent method proposed by Rapp (1995), which does not need an initial lexicon but is computationally prohibitively expensive. It can also be considered as an extension from the monolingual to the bilingual case of the well-established methods for semantic or syntactic word clustering as proposed by Schiitze (1993), Grefenstette (1994), Ruge (1995), Rapp (1996), Lin (1998), and others. Some of these authors perform a shallow or full syntactical analysis before constructing the cooccurrence vectors. Others reduce the size of the co-occurrence matrices by performing a singular value decomposition. However, in yet unpublished work we found that at least for the computation of synonyms and related words neither syntactical analysis nor singular value decomposition lead to significantly better results than the approach described here when applied to the monolingual case (see also Grefenstette, 1993), so we did not try to include these methods in our syst</context>
</contexts>
<marker>Rapp, 1996</marker>
<rawString>Rapp, R. (1996). Die Berechnung von Assoziationen. Hildesheim: Olms.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Ruge</author>
</authors>
<title>Human memory models and term association.</title>
<date>1995</date>
<booktitle>Proceedings of the ACM SIGIR Conference, Seattle,</booktitle>
<pages>219--227</pages>
<contexts>
<context position="16456" citStr="Ruge (1995)" startWordPosition="2751" endWordPosition="2752">the vector positions and the columns of the matrix are determined by using the base lexicon. Thus, for each vector in the English matrix a similarity value is computed and the English words are ranked according to these values. It is expected that the correct translation is ranked first in the sorted list. For vector comparison, different similarity measures can be considered. Salton &amp; McGill (1983) proposed a number of measures, such as the Cosine coefficient, the Jaccard coefficient, and the Dice coefficient (see also Jones &amp; Furnas, 1987). For the computation of related terms and synonyms, Ruge (1995), Landauer and Dumais (1997), and Fung and McKeown (1997) used the cosine measure, whereas Grefenstette (1994, p. 48) used a weighted Jaccard measure. We propose here the city-block metric, which computes the similarity between two vectors X and Y as the sum of the absolute differences of corresponding vector positions: s= lxi—Yil t=1 In a number of experiments we compared it to other similarity measures, such as the cosine measure, the Jaccard measure (standard and binary), the Euclidean distance, and the scalar product, and found that the city-block metric yielded the best results. This may </context>
<context position="25420" citStr="Ruge (1995)" startWordPosition="4241" endWordPosition="4242">t see think Whisky whiskey 11 whisky beer Scotch bottle wine Table 1: Results for 20 of the 100 test words (for full list see http://www.fask.uni-mainz.de/user/rapp/) 5 Discussion and Conclusion The method described can be seen as a simple case of the gradient descent method proposed by Rapp (1995), which does not need an initial lexicon but is computationally prohibitively expensive. It can also be considered as an extension from the monolingual to the bilingual case of the well-established methods for semantic or syntactic word clustering as proposed by Schiitze (1993), Grefenstette (1994), Ruge (1995), Rapp (1996), Lin (1998), and others. Some of these authors perform a shallow or full syntactical analysis before constructing the cooccurrence vectors. Others reduce the size of the co-occurrence matrices by performing a singular value decomposition. However, in yet unpublished work we found that at least for the computation of synonyms and related words neither syntactical analysis nor singular value decomposition lead to significantly better results than the approach described here when applied to the monolingual case (see also Grefenstette, 1993), so we did not try to include these method</context>
</contexts>
<marker>Ruge, 1995</marker>
<rawString>Ruge, G. (1995). Human memory models and term association. Proceedings of the ACM SIGIR Conference, Seattle, 219-227.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Russell</author>
</authors>
<title>The complete German language norms for responses to 100 words from the Kent-Rosanoff word association test.</title>
<date>1970</date>
<booktitle>Norms of Word Association.</booktitle>
<pages>53--94</pages>
<editor>In: L. Postman, G. Keppel (eds.):</editor>
<publisher>Academic Press,</publisher>
<location>New York:</location>
<contexts>
<context position="9125" citStr="Russell (1970)" startWordPosition="1493" endWordPosition="1495">corpus 2. an English corpus 3. a number of German test words with known English translations 4. a small base lexicon, German to English As the German corpus, we used 135 million words of the newspaper Frankfurter Allgemeine Zeitung (1993 to 1996), and as the English corpus 163 million words of the Guardian (1990 to 1994). Since the orientation of the two newspapers is quite different, and since the time spans covered are only in part overlapping, the two corpora can be considered as more or less unrelated. For testing our results, we started with a list of 100 German test words as proposed by Russell (1970), which he used for an association experiment with German subjects. By looking up the translations for each of these 100 words, we obtained a test set for evaluation. Our German/English base lexicon is derived from the Collins Gem German Dictionary with about 22,300 entries. From this we eliminated all multi-word entries, so 16,380 entries remained. Because we had decided on our test word list beforehand, and since it would not make much sense to apply our method to words that are already in the base lexicon, we also removed all entries belonging to the 100 test words. 3.2 Pre-processing Since</context>
</contexts>
<marker>Russell, 1970</marker>
<rawString>Russell, W. A. (1970). The complete German language norms for responses to 100 words from the Kent-Rosanoff word association test. In: L. Postman, G. Keppel (eds.): Norms of Word Association. New York: Academic Press, 53-94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>M McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval.</title>
<date>1983</date>
<publisher>McGrawHill.</publisher>
<location>New York:</location>
<contexts>
<context position="16247" citStr="Salton &amp; McGill (1983)" startWordPosition="2715" endWordPosition="2718">English translation of an unknown German word, the association vector of the German word is computed and compared to all association vectors in the English association matrix. For comparison, the correspondences between the vector positions and the columns of the matrix are determined by using the base lexicon. Thus, for each vector in the English matrix a similarity value is computed and the English words are ranked according to these values. It is expected that the correct translation is ranked first in the sorted list. For vector comparison, different similarity measures can be considered. Salton &amp; McGill (1983) proposed a number of measures, such as the Cosine coefficient, the Jaccard coefficient, and the Dice coefficient (see also Jones &amp; Furnas, 1987). For the computation of related terms and synonyms, Ruge (1995), Landauer and Dumais (1997), and Fung and McKeown (1997) used the cosine measure, whereas Grefenstette (1994, p. 48) used a weighted Jaccard measure. We propose here the city-block metric, which computes the similarity between two vectors X and Y as the sum of the absolute differences of corresponding vector positions: s= lxi—Yil t=1 In a number of experiments we compared it to other sim</context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>Salton, G.; McGill, M. (1983). Introduction to Modern Information Retrieval. New York: McGrawHill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schiitze</author>
</authors>
<title>Part-of-speech induction from scratch. In:</title>
<date>1993</date>
<booktitle>Proceedings of the 3 I st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>251--258</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="25386" citStr="Schiitze (1993)" startWordPosition="4237" endWordPosition="4238">ne drink weiB white 46 know say thought see think Whisky whiskey 11 whisky beer Scotch bottle wine Table 1: Results for 20 of the 100 test words (for full list see http://www.fask.uni-mainz.de/user/rapp/) 5 Discussion and Conclusion The method described can be seen as a simple case of the gradient descent method proposed by Rapp (1995), which does not need an initial lexicon but is computationally prohibitively expensive. It can also be considered as an extension from the monolingual to the bilingual case of the well-established methods for semantic or syntactic word clustering as proposed by Schiitze (1993), Grefenstette (1994), Ruge (1995), Rapp (1996), Lin (1998), and others. Some of these authors perform a shallow or full syntactical analysis before constructing the cooccurrence vectors. Others reduce the size of the co-occurrence matrices by performing a singular value decomposition. However, in yet unpublished work we found that at least for the computation of synonyms and related words neither syntactical analysis nor singular value decomposition lead to significantly better results than the approach described here when applied to the monolingual case (see also Grefenstette, 1993), so we d</context>
</contexts>
<marker>Schiitze, 1993</marker>
<rawString>Schiitze, H. (1993). Part-of-speech induction from scratch. In: Proceedings of the 3 I st Annual Meeting of the Association for Computational Linguistics, Columbus, Ohio, 251-258.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Wettler</author>
<author>R Rapp</author>
</authors>
<title>Computation of word associations based on the co-occurrences of words in large corpora. In:</title>
<date>1993</date>
<booktitle>Proceedings of the 1st Workshop on Very Large Corpora:</booktitle>
<pages>84--93</pages>
<location>Columbus, Ohio,</location>
<marker>Wettler, Rapp, 1993</marker>
<rawString>Wettler, M.; Rapp, R. (1993). Computation of word associations based on the co-occurrences of words in large corpora. In: Proceedings of the 1st Workshop on Very Large Corpora: Columbus, Ohio, 84-93.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>