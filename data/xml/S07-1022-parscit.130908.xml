<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.056770">
<title confidence="0.9732865">
CLaC and CLaC-NB: Knowledge-based and corpus-based approaches
to sentiment tagging
</title>
<author confidence="0.992357">
Alina Andreevskaia
</author>
<affiliation confidence="0.997797">
Concordia University
</affiliation>
<address confidence="0.656522">
1455 de Maisonneuve Blvd.
Montreal, Canada
</address>
<email confidence="0.995124">
andreev@cs.concordia.ca
</email>
<author confidence="0.990887">
Sabine Bergler
</author>
<affiliation confidence="0.997937">
Concordia University
</affiliation>
<address confidence="0.658085">
1455 de Maisonneuve Blvd.
Montreal, Canada
</address>
<email confidence="0.997532">
bergler@cs.concordia.ca
</email>
<sectionHeader confidence="0.995622" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999952714285714">
For the Affective Text task at Semeval-
1/Senseval-4, the CLaC team compared a
knowledge-based, domain-independent ap-
proach and a standard, statistical machine
learning approach to ternary sentiment an-
notation of news headlines. In this paper
we describe the two systems submitted to
the competition and evaluate their results.
We show that the knowledge-based unsu-
pervised method achieves high accuracy and
precision but low recall, while supervised
statistical approach trained on small amount
of in-domain data provides relatively high
recall at the cost of low precision.
</bodyText>
<sectionHeader confidence="0.999004" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999574893617021">
Sentiment tagging of short text spans — sentences,
headlines, or clauses — poses considerable chal-
lenges for automatic systems due to the scarcity of
sentiment clues in these units: sometimes, the deci-
sion about the text span sentiment has to be based
on just a single sentiment clue and the cost of every
error is high. This is particularly true for headlines,
which are typically very short. Therefore, an ideal
system for sentiment tagging of headlines has to use
a large set of features with dependable sentiment an-
notations and to be able to reliably deduce the senti-
ment of the headline from the sentiment of its com-
ponents.
The valence labeling subtask of the Affective Text
task requires ternary — positive vs. negative vs.
neutral — classification of headlines. While such
categorization at the sentence level remains rela-
tively unexplored&apos;, the two related sentence-level,
binary classification tasks — positive vs. negative
and subjective vs. objective — have attracted con-
siderable attention in the recent years (Hu and Liu,
2004; Kim and Hovy, 2005; Riloff et al., 2006; Tur-
ney and Littman, 2003; Yu and Hatzivassiloglou,
2003). Unsupervised knowledge-based methods are
the preferred approach to classification of sentences
into positive and negative, mostly due to the lack of
adequate amounts of labeled training data (Gamon
and Aue, 2005). These approaches rely on presence
and scores of sentiment-bearing words that have
been acquired from dictionaries (Kim and Hovy,
2005) or corpora (Yu and Hatzivassiloglou, 2003).
Their accuracy on news sentences is between 65 and
68%.
Sentence-level subjectivity detection, where train-
ing data is easier to obtain than for positive vs. neg-
ative classification, has been successfully performed
using supervised statistical methods alone (Pang and
Lee, 2004) or in combination with a knowledge-
based approach (Riloff et al., 2006).
Since the extant literature does not provide clear
evidence for the choice between supervised machine
learning methods and unsupervised knowledge-
based approaches for the task of ternary sentiment
classification of sentences or headlines, we devel-
oped two systems for the Affective Text task at
SemEval-2007. The first system (CLaC) relies on
the knowledge-rich approach that takes into consid-
</bodyText>
<footnote confidence="0.691148">
&apos;To our knowledge, the only work that attempted such clas-
sification at the sentence level is (Gamon and Aue, 2005) that
classified product reviews.
</footnote>
<page confidence="0.973839">
117
</page>
<bodyText confidence="0.965450111111111">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 117–120,
Prague, June 2007. c�2007 Association for Computational Linguistics
eration multiple clues, such as a list of sentiment-
bearing unigrams and valence shifters, and makes
use of sentence structure in order to combine these
clues into an overall sentiment of the headline. The
second system (CLaC-NB) explores the potential of
a statistical method trained on a small amount of
manually labeled news headlines and sentences.
</bodyText>
<sectionHeader confidence="0.9325355" genericHeader="method">
2 CLaC System: Syntax-Aware
Dictionary-Based Approach
</sectionHeader>
<bodyText confidence="0.999955555555555">
The CLaC system relies on a knowledge-based,
domain-independent, unsupervised approach to
headline sentiment detection and scoring. The
system uses three main knowledge inputs: a list
of sentiment-bearing unigrams, a list of valence
shifters (Polanyi and Zaenen, 2006), and a set of
rules that define the scope and results of com-
bination of sentiment-bearing words with valence
shifters.
</bodyText>
<subsectionHeader confidence="0.999626">
2.1 List of sentiment-bearing words
</subsectionHeader>
<bodyText confidence="0.999993530612245">
The unigrams used for sentence/headline classifica-
tion were learned from WordNet (Fellbaum, 1998)
dictionary entries using the STEP system described
in (Andreevskaia and Bergler, 2006b). In order to
take advantage of the special properties of WordNet
glosses and relations, we developed a system that
used the human-annotated adjectives from (Hatzi-
vassiloglou and McKeown, 1997) as a seed list and
learned additional unigrams from WordNet synsets
and glosses. The STEP algorithm starts with a
small set of manually annotated seed words that
is expanded using synonymy and antonymy rela-
tions in WordNet. Then the system searches all
WordNet glosses and selects the synsets that contain
sentiment-bearing words from the expanded seed
list in their glosses. In order to eliminate errors
produced by part-of-speech ambiguity of some of
the seed words, the glosses are processed by Brill’s
part-of-speech tagger (Brill, 1995) and only the seed
words with matching part-of-speech tags are consid-
ered. Headwords with sentiment-bearing seed words
in their definitions are then added to the positive or
negative categories depending on the seed-word sen-
timent. Finally, words that were assigned contra-
dicting — positive and negative — sentiment within
the same run were eliminated. The average accu-
racy of 60 runs with non-intersecting seed lists when
compared to General Inquirer (Stone et al., 1966)
was 74%. In order to improve the list coverage,
the words annotated as “Positiv” or “Negativ” in the
General Inquirer that were not picked up by STEP
were added to the final list.
Since sentiment-bearing words in English have
different degree of centrality to the category of sen-
timent, we have constructed a measure of word cen-
trality to the category of positive or negative sen-
timent described in our earlier work (Andreevskaia
and Bergler, 2006a). The measure, termed Net Over-
lap Score (NOS), is based on the number of ties that
connect a given word to other words in the category.
The number of such ties is reflected in the num-
ber of times each word was retrieved from Word-
Net by multiple independent STEP runs with non-
intersecting seed lists. This approach allowed us
to assign NOSs to each unigram captured by mul-
tiple STEP runs. Only words with fuzzy member-
ship score not equal to zero were retained in the
list. The resulting list contained 10,809 sentiment-
bearing words of different parts of speech.
</bodyText>
<subsectionHeader confidence="0.998876">
2.2 Valence Shifters
</subsectionHeader>
<bodyText confidence="0.987507739130435">
The brevity of the headlines compared to typical
news sentences requires that the system is able to
make a correct decision based on very few sentiment
clues. Due to the scarcity of sentiment clues, the ad-
ditional factors, such as presence of valence shifters,
have a greater impact on the system performance on
headlines than on sentences or texts, where impact
of a single error can often be compensated by a num-
ber of other, correctly identified sentiment clues. For
this reason, we complemented the system based on
fuzzy score counts with the capability to discern and
take into account some relevant elements of syntac-
tic structure of sentences. We added to the system
two components in order to enable this capability:
(1) valence shifter handling rules and (2) parse tree
analysis.
Valence shifters can be defined as words that mod-
ify the sentiment expressed by a sentiment-bearing
word (Polanyi and Zaenen, 2006). The list of va-
lence shifters used in our experiments was a com-
2An average length of a sentence in a news corpus is over 20
words, while the average length of headlines in the test corpus
was only 7 words.
</bodyText>
<page confidence="0.993376">
118
</page>
<bodyText confidence="0.999933666666667">
bination of (1) a list of common English nega-
tions, (2) a subset of the list of automatically ob-
tained words with increase/decrease semantics, and
(3) words picked up in manual annotation conducted
for other research projects by two trained linguists.
The full list consists of 490 words and expressions.
Each entry in the list of valence shifters has an action
and scope associated with it. The action and scope
tags are used by special handling rules that enable
our system to identify such words and phrases in the
text and take them into account in sentence senti-
ment determination. In order to correctly determine
the scope of valence shifters in a sentence, we intro-
duced into the system the analysis of the parse trees
produced by MiniPar (Lin, 1998).
As a result of this processing, every headline re-
ceived a score according to the combined fuzzy NOS
of its constituents. We then mapped this score,
which ranged between -1.2 and 0.99, into the
[-100, 100] scale as required by the competition or-
ganizers.
</bodyText>
<sectionHeader confidence="0.99144" genericHeader="method">
3 CLaC-NB System: Naive Bayes
</sectionHeader>
<bodyText confidence="0.999905909090909">
Supervised statistical methods have been very suc-
cessful in sentiment tagging of texts and in subjec-
tivity detection at sentence level: on movie review
texts they reach an accuracy of 85-90% (Aue and
Gamon, 2005; Pang and Lee, 2004) and up to 92%
accuracy on classifying movie review snippets into
subjective and objective using both Nave Bayes and
SVM (Pang and Lee, 2004). These methods per-
form particularly well when a large volume of la-
beled data from the same domain as the test set is
available for training (Aue and Gamon, 2005). The
lack of sufficient data for training appears to be the
main reason for the virtual absence of experiments
with statistical classifiers in sentiment tagging at the
sentence level.
In order to explore the potential of statistical ap-
proaches on sentiment classification of headlines,
we implemented a basic Naive Bayes classifier with
smoothing using Lidstone’s law of succession (with
A=0.1). No feature selection was performed.
The development set for the Affective Text task
consisted of only 250 headlines, which is not suf-
ficient for training of a statistical classifier. In or-
der to increase the size of the training corpus, we
augmented it with a balanced set of 900 manually
annotated news sentences on a variety of topics ex-
tracted from the Canadian NewsStand database3 and
200 headlines from different domains collected from
Google News in January 20074.
The probabilities assigned by the classifier were
mapped to [-100, 100] as follows: all negative head-
lines received a score of -100, all positive headlines
+100, and neutral headlines 0.
</bodyText>
<sectionHeader confidence="0.998604" genericHeader="evaluation">
4 Results and Discussion
</sectionHeader>
<bodyText confidence="0.84898475">
Table 1 shows the results of the two CLaC systems
for valence labeling subtask of Affective Text task
compared to all participating systems average. The
best subtask scores are highlighted in bold.
</bodyText>
<table confidence="0.9981204">
System Pearson Acc. Prec. Rec. F1
correl.
CLaC 47.7 55.1 61.4 9.2 16
CLaC-NB 25.4 31.2 31.2 66.4 42
Task average 33.2 44.7 44.85 29.6 23.7
</table>
<tableCaption confidence="0.999156">
Table 1: System results
</tableCaption>
<bodyText confidence="0.9999556">
The comparison between the two CLaC systems
clearly demonstrates the relative advantages of the
two approaches. The knowledge-based unsuper-
vised system performed well above average on three
main measures: the Pearson correlation between
fine-grained sentiment assigned by CLaC system
and the human annotation; the accuracy for ternary
classification; and the precision of binary (positive
vs. negative) classification. These results demon-
strate that an accurately annotated list of sentiment-
bearing words combined with sophisticated valence
shifter handling produces acceptably accurate senti-
ment labels even for such difficult data as news head-
lines. This system, however, was not able to provide
good recall.
On the contrary, supervised machine learning has
very good recall, but low accuracy relative to the
results of the unsupervised knowledge-based ap-
proach. This shortcoming could be in part reduced
if more uniformly labeled headlines were available
</bodyText>
<footnote confidence="0.9943685">
3http://www.il.proquest.com/productspq/
descriptions/Canadian newsstand.shtml
4The interannotator agreement for this data, as measured by
Kappa, was 0.74.
</footnote>
<page confidence="0.998439">
119
</page>
<bodyText confidence="0.99972">
for training. However, we can hardly expect large
amounts of such manually annotated data to be
handy in real-life situations.
</bodyText>
<sectionHeader confidence="0.995079" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999968176470588">
The two CLaC systems that we submitted to the
Affective Text task have tested the applicability of
two main sentiment tagging approaches to news
headlines annotation. The results of the two sys-
tems indicate that the knowledge-based unsuper-
vised approach that relies on an automatically ac-
quired list of sentiment-bearing unigrams and takes
into account the combinatorial properties of valence
shifters, can produce high quality sentiment annota-
tions, but may miss many sentiment-laden headlines.
On the other hand, supervised machine learning has
good recall even with a relatively small training set,
but its precision and accuracy are low. In our future
work we will explore the potential of combining the
two approaches in a single system in order to im-
prove both recall and precision of sentiment annota-
tion.
</bodyText>
<sectionHeader confidence="0.97376" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.936403666666667">
Alina Andreevskaia and Sabine Bergler. 2006a. Mining
wordnet for a fuzzy sentiment: Sentiment tag extrac-
tion from wordnet glosses. In Proceedings EACL-06,
the 11rd Conference of the European Chapter of the
Association for Computational Linguistics, Trento, IT.
Alina Andreevskaia and Sabine Bergler. 2006b. Seman-
tic tag extraction from wordnet glosses. In Proceed-
ings ofLREC-06, the 5th Conference on Language Re-
sources and Evaluation, Genova, IT.
Anthony Aue and Michael Gamon. 2005. Customiz-
ing sentiment classifiers to new domains: a case study.
In RANLP-05, the International Conference on Recent
Advances in Natural Language Processing, Borovets,
Bulgaria.
Eric Brill. 1995. Transformation-Based Error-Driven
Learning and Natural Language Processing: A Case
Study in Part-of-Speech Tagging. Computational Lin-
guistics, 21(4).
</bodyText>
<reference confidence="0.999633018181818">
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge, MA.
Michael Gamon and Anthony Aue. 2005. Automatic
identification of sentiment vocabulary: exploiting low
association with known sentiment terms. In Proceed-
ings of the ACL-05 Workshop on Feature Engineering
for Machine Learning in Natural Language Process-
ing, Ann Arbor, MI.
Vasileios Hatzivassiloglou and Kathleen B. McKeown.
1997. Predicting the Semantic Orientation of Ad-
jectives. In Proceedings of ACL-97, 35nd Meeting of
the Association for Computational Linguistics, pages
174–181, Madrid, Spain. ACL.
Minqing Hu and Bing Liu. 2004. Mining and sum-
marizing customer reviews. In Tenth ACM SIGKDD
Conference on Knowledge Discovery and Data Min-
ing (KDD-04), pages 168–177.
Soo-Min Kim and Eduard Hovy. 2005. Automatic de-
tection of opinion bearing words and sentences. In
Companion Volume to the Proceedings ofIJCNLP-05,
the Second International Joint Conference on Natural
Language Processing, pages 61–66, Jeju Island, KR.
Dekang Lin. 1998. Dependency-based Evaluation
of MINIPAR. In Proceedings of the Workshop on
the Evaluation of Parsing Systems, pages 768–774,
Granada, Spain.
Bo Pang and Lilian Lee. 2004. A sentiment education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of ACL-04,
42nd Meeting of the Association for Computational
Linguistics, pages 271–278.
Livia Polanyi and Annie Zaenen. 2006. Contextual Va-
lence Shifters. In James G. Shanahan, Yan Qu, and
Janyce Wiebe, editors, Computing Attitude and Affect
in Text: Theory and Application. Springer Verlag.
Ellen Riloff, Siddharth Patwardhan, and Janyce Wiebe.
2006. Feature subsumption for opinion analysis. In
Proceedings ofEMNLP-06, the Conference on Empir-
ical Methods in Natural Language Processing, pages
440–448, Sydney, AUS.
P. J. Stone, D.C. Dumphy, M.S. Smith, and D.M. Ogilvie.
1966. The General Inquirer: a computer approach to
content analysis. M.I.T. studies in comparative poli-
tics. M.I.T. Press, Cambridge, MA.
Peter Turney and Michael Littman. 2003. Measuring
praise and criticism: inference of semantic orientation
from association. ACM Transactions on Information
Systems (TOIS), 21:315–346.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Michael Collins and Mark Steedman, ed-
itors, Proceedings of EMNLP-03, 8th Conference on
Empirical Methods in Natural Language Processing,
pages 129–136, Sapporo, Japan.
</reference>
<page confidence="0.99628">
120
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.880318">
<title confidence="0.988429">CLaC and CLaC-NB: Knowledge-based and corpus-based approaches to sentiment tagging</title>
<author confidence="0.998649">Alina Andreevskaia</author>
<affiliation confidence="0.999989">Concordia University</affiliation>
<address confidence="0.9986785">1455 de Maisonneuve Blvd. Montreal, Canada</address>
<email confidence="0.972784">andreev@cs.concordia.ca</email>
<author confidence="0.948566">Sabine Bergler</author>
<affiliation confidence="0.999963">Concordia University</affiliation>
<address confidence="0.9988495">1455 de Maisonneuve Blvd. Montreal, Canada</address>
<email confidence="0.99857">bergler@cs.concordia.ca</email>
<abstract confidence="0.998797533333333">For the Affective Text task at Semeval- 1/Senseval-4, the CLaC team compared a knowledge-based, domain-independent approach and a standard, statistical machine learning approach to ternary sentiment annotation of news headlines. In this paper we describe the two systems submitted to the competition and evaluate their results. We show that the knowledge-based unsupervised method achieves high accuracy and precision but low recall, while supervised statistical approach trained on small amount of in-domain data provides relatively high recall at the cost of low precision.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gamon</author>
<author>Anthony Aue</author>
</authors>
<title>Automatic identification of sentiment vocabulary: exploiting low association with known sentiment terms.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL-05 Workshop on Feature Engineering for Machine Learning in Natural Language Processing,</booktitle>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="2245" citStr="Gamon and Aue, 2005" startWordPosition="334" endWordPosition="337">utral — classification of headlines. While such categorization at the sentence level remains relatively unexplored&apos;, the two related sentence-level, binary classification tasks — positive vs. negative and subjective vs. objective — have attracted considerable attention in the recent years (Hu and Liu, 2004; Kim and Hovy, 2005; Riloff et al., 2006; Turney and Littman, 2003; Yu and Hatzivassiloglou, 2003). Unsupervised knowledge-based methods are the preferred approach to classification of sentences into positive and negative, mostly due to the lack of adequate amounts of labeled training data (Gamon and Aue, 2005). These approaches rely on presence and scores of sentiment-bearing words that have been acquired from dictionaries (Kim and Hovy, 2005) or corpora (Yu and Hatzivassiloglou, 2003). Their accuracy on news sentences is between 65 and 68%. Sentence-level subjectivity detection, where training data is easier to obtain than for positive vs. negative classification, has been successfully performed using supervised statistical methods alone (Pang and Lee, 2004) or in combination with a knowledgebased approach (Riloff et al., 2006). Since the extant literature does not provide clear evidence for the c</context>
</contexts>
<marker>Gamon, Aue, 2005</marker>
<rawString>Michael Gamon and Anthony Aue. 2005. Automatic identification of sentiment vocabulary: exploiting low association with known sentiment terms. In Proceedings of the ACL-05 Workshop on Feature Engineering for Machine Learning in Natural Language Processing, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Kathleen B McKeown</author>
</authors>
<title>Predicting the Semantic Orientation of Adjectives.</title>
<date>1997</date>
<booktitle>In Proceedings of ACL-97, 35nd Meeting of the Association for Computational Linguistics,</booktitle>
<pages>174--181</pages>
<publisher>ACL.</publisher>
<location>Madrid,</location>
<contexts>
<context position="4697" citStr="Hatzivassiloglou and McKeown, 1997" startWordPosition="693" endWordPosition="697">e inputs: a list of sentiment-bearing unigrams, a list of valence shifters (Polanyi and Zaenen, 2006), and a set of rules that define the scope and results of combination of sentiment-bearing words with valence shifters. 2.1 List of sentiment-bearing words The unigrams used for sentence/headline classification were learned from WordNet (Fellbaum, 1998) dictionary entries using the STEP system described in (Andreevskaia and Bergler, 2006b). In order to take advantage of the special properties of WordNet glosses and relations, we developed a system that used the human-annotated adjectives from (Hatzivassiloglou and McKeown, 1997) as a seed list and learned additional unigrams from WordNet synsets and glosses. The STEP algorithm starts with a small set of manually annotated seed words that is expanded using synonymy and antonymy relations in WordNet. Then the system searches all WordNet glosses and selects the synsets that contain sentiment-bearing words from the expanded seed list in their glosses. In order to eliminate errors produced by part-of-speech ambiguity of some of the seed words, the glosses are processed by Brill’s part-of-speech tagger (Brill, 1995) and only the seed words with matching part-of-speech tags</context>
</contexts>
<marker>Hatzivassiloglou, McKeown, 1997</marker>
<rawString>Vasileios Hatzivassiloglou and Kathleen B. McKeown. 1997. Predicting the Semantic Orientation of Adjectives. In Proceedings of ACL-97, 35nd Meeting of the Association for Computational Linguistics, pages 174–181, Madrid, Spain. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Tenth ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD-04),</booktitle>
<pages>168--177</pages>
<contexts>
<context position="1932" citStr="Hu and Liu, 2004" startWordPosition="286" endWordPosition="289">sentiment tagging of headlines has to use a large set of features with dependable sentiment annotations and to be able to reliably deduce the sentiment of the headline from the sentiment of its components. The valence labeling subtask of the Affective Text task requires ternary — positive vs. negative vs. neutral — classification of headlines. While such categorization at the sentence level remains relatively unexplored&apos;, the two related sentence-level, binary classification tasks — positive vs. negative and subjective vs. objective — have attracted considerable attention in the recent years (Hu and Liu, 2004; Kim and Hovy, 2005; Riloff et al., 2006; Turney and Littman, 2003; Yu and Hatzivassiloglou, 2003). Unsupervised knowledge-based methods are the preferred approach to classification of sentences into positive and negative, mostly due to the lack of adequate amounts of labeled training data (Gamon and Aue, 2005). These approaches rely on presence and scores of sentiment-bearing words that have been acquired from dictionaries (Kim and Hovy, 2005) or corpora (Yu and Hatzivassiloglou, 2003). Their accuracy on news sentences is between 65 and 68%. Sentence-level subjectivity detection, where train</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Tenth ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD-04), pages 168–177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Automatic detection of opinion bearing words and sentences.</title>
<date>2005</date>
<booktitle>In Companion Volume to the Proceedings ofIJCNLP-05, the Second International Joint Conference on Natural Language Processing,</booktitle>
<pages>61--66</pages>
<location>Jeju Island, KR.</location>
<contexts>
<context position="1952" citStr="Kim and Hovy, 2005" startWordPosition="290" endWordPosition="293">of headlines has to use a large set of features with dependable sentiment annotations and to be able to reliably deduce the sentiment of the headline from the sentiment of its components. The valence labeling subtask of the Affective Text task requires ternary — positive vs. negative vs. neutral — classification of headlines. While such categorization at the sentence level remains relatively unexplored&apos;, the two related sentence-level, binary classification tasks — positive vs. negative and subjective vs. objective — have attracted considerable attention in the recent years (Hu and Liu, 2004; Kim and Hovy, 2005; Riloff et al., 2006; Turney and Littman, 2003; Yu and Hatzivassiloglou, 2003). Unsupervised knowledge-based methods are the preferred approach to classification of sentences into positive and negative, mostly due to the lack of adequate amounts of labeled training data (Gamon and Aue, 2005). These approaches rely on presence and scores of sentiment-bearing words that have been acquired from dictionaries (Kim and Hovy, 2005) or corpora (Yu and Hatzivassiloglou, 2003). Their accuracy on news sentences is between 65 and 68%. Sentence-level subjectivity detection, where training data is easier t</context>
</contexts>
<marker>Kim, Hovy, 2005</marker>
<rawString>Soo-Min Kim and Eduard Hovy. 2005. Automatic detection of opinion bearing words and sentences. In Companion Volume to the Proceedings ofIJCNLP-05, the Second International Joint Conference on Natural Language Processing, pages 61–66, Jeju Island, KR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Dependency-based Evaluation of MINIPAR.</title>
<date>1998</date>
<booktitle>In Proceedings of the Workshop on the Evaluation of Parsing Systems,</booktitle>
<pages>768--774</pages>
<location>Granada,</location>
<contexts>
<context position="8631" citStr="Lin, 1998" startWordPosition="1360" endWordPosition="1361">(3) words picked up in manual annotation conducted for other research projects by two trained linguists. The full list consists of 490 words and expressions. Each entry in the list of valence shifters has an action and scope associated with it. The action and scope tags are used by special handling rules that enable our system to identify such words and phrases in the text and take them into account in sentence sentiment determination. In order to correctly determine the scope of valence shifters in a sentence, we introduced into the system the analysis of the parse trees produced by MiniPar (Lin, 1998). As a result of this processing, every headline received a score according to the combined fuzzy NOS of its constituents. We then mapped this score, which ranged between -1.2 and 0.99, into the [-100, 100] scale as required by the competition organizers. 3 CLaC-NB System: Naive Bayes Supervised statistical methods have been very successful in sentiment tagging of texts and in subjectivity detection at sentence level: on movie review texts they reach an accuracy of 85-90% (Aue and Gamon, 2005; Pang and Lee, 2004) and up to 92% accuracy on classifying movie review snippets into subjective and o</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Dependency-based Evaluation of MINIPAR. In Proceedings of the Workshop on the Evaluation of Parsing Systems, pages 768–774, Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lilian Lee</author>
</authors>
<title>A sentiment education: Sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL-04, 42nd Meeting of the Association for Computational Linguistics,</booktitle>
<pages>271--278</pages>
<contexts>
<context position="2703" citStr="Pang and Lee, 2004" startWordPosition="401" endWordPosition="404">red approach to classification of sentences into positive and negative, mostly due to the lack of adequate amounts of labeled training data (Gamon and Aue, 2005). These approaches rely on presence and scores of sentiment-bearing words that have been acquired from dictionaries (Kim and Hovy, 2005) or corpora (Yu and Hatzivassiloglou, 2003). Their accuracy on news sentences is between 65 and 68%. Sentence-level subjectivity detection, where training data is easier to obtain than for positive vs. negative classification, has been successfully performed using supervised statistical methods alone (Pang and Lee, 2004) or in combination with a knowledgebased approach (Riloff et al., 2006). Since the extant literature does not provide clear evidence for the choice between supervised machine learning methods and unsupervised knowledgebased approaches for the task of ternary sentiment classification of sentences or headlines, we developed two systems for the Affective Text task at SemEval-2007. The first system (CLaC) relies on the knowledge-rich approach that takes into consid&apos;To our knowledge, the only work that attempted such classification at the sentence level is (Gamon and Aue, 2005) that classified prod</context>
<context position="9149" citStr="Pang and Lee, 2004" startWordPosition="1446" endWordPosition="1449">ntence, we introduced into the system the analysis of the parse trees produced by MiniPar (Lin, 1998). As a result of this processing, every headline received a score according to the combined fuzzy NOS of its constituents. We then mapped this score, which ranged between -1.2 and 0.99, into the [-100, 100] scale as required by the competition organizers. 3 CLaC-NB System: Naive Bayes Supervised statistical methods have been very successful in sentiment tagging of texts and in subjectivity detection at sentence level: on movie review texts they reach an accuracy of 85-90% (Aue and Gamon, 2005; Pang and Lee, 2004) and up to 92% accuracy on classifying movie review snippets into subjective and objective using both Nave Bayes and SVM (Pang and Lee, 2004). These methods perform particularly well when a large volume of labeled data from the same domain as the test set is available for training (Aue and Gamon, 2005). The lack of sufficient data for training appears to be the main reason for the virtual absence of experiments with statistical classifiers in sentiment tagging at the sentence level. In order to explore the potential of statistical approaches on sentiment classification of headlines, we impleme</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Bo Pang and Lilian Lee. 2004. A sentiment education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of ACL-04, 42nd Meeting of the Association for Computational Linguistics, pages 271–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Livia Polanyi</author>
<author>Annie Zaenen</author>
</authors>
<title>Contextual Valence Shifters.</title>
<date>2006</date>
<booktitle>Computing Attitude and Affect in Text: Theory and Application.</booktitle>
<editor>In James G. Shanahan, Yan Qu, and Janyce Wiebe, editors,</editor>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="4163" citStr="Polanyi and Zaenen, 2006" startWordPosition="615" endWordPosition="618">ntbearing unigrams and valence shifters, and makes use of sentence structure in order to combine these clues into an overall sentiment of the headline. The second system (CLaC-NB) explores the potential of a statistical method trained on a small amount of manually labeled news headlines and sentences. 2 CLaC System: Syntax-Aware Dictionary-Based Approach The CLaC system relies on a knowledge-based, domain-independent, unsupervised approach to headline sentiment detection and scoring. The system uses three main knowledge inputs: a list of sentiment-bearing unigrams, a list of valence shifters (Polanyi and Zaenen, 2006), and a set of rules that define the scope and results of combination of sentiment-bearing words with valence shifters. 2.1 List of sentiment-bearing words The unigrams used for sentence/headline classification were learned from WordNet (Fellbaum, 1998) dictionary entries using the STEP system described in (Andreevskaia and Bergler, 2006b). In order to take advantage of the special properties of WordNet glosses and relations, we developed a system that used the human-annotated adjectives from (Hatzivassiloglou and McKeown, 1997) as a seed list and learned additional unigrams from WordNet synse</context>
<context position="7663" citStr="Polanyi and Zaenen, 2006" startWordPosition="1186" endWordPosition="1189">m performance on headlines than on sentences or texts, where impact of a single error can often be compensated by a number of other, correctly identified sentiment clues. For this reason, we complemented the system based on fuzzy score counts with the capability to discern and take into account some relevant elements of syntactic structure of sentences. We added to the system two components in order to enable this capability: (1) valence shifter handling rules and (2) parse tree analysis. Valence shifters can be defined as words that modify the sentiment expressed by a sentiment-bearing word (Polanyi and Zaenen, 2006). The list of valence shifters used in our experiments was a com2An average length of a sentence in a news corpus is over 20 words, while the average length of headlines in the test corpus was only 7 words. 118 bination of (1) a list of common English negations, (2) a subset of the list of automatically obtained words with increase/decrease semantics, and (3) words picked up in manual annotation conducted for other research projects by two trained linguists. The full list consists of 490 words and expressions. Each entry in the list of valence shifters has an action and scope associated with i</context>
</contexts>
<marker>Polanyi, Zaenen, 2006</marker>
<rawString>Livia Polanyi and Annie Zaenen. 2006. Contextual Valence Shifters. In James G. Shanahan, Yan Qu, and Janyce Wiebe, editors, Computing Attitude and Affect in Text: Theory and Application. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Siddharth Patwardhan</author>
<author>Janyce Wiebe</author>
</authors>
<title>Feature subsumption for opinion analysis.</title>
<date>2006</date>
<booktitle>In Proceedings ofEMNLP-06, the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>440--448</pages>
<location>Sydney, AUS.</location>
<contexts>
<context position="1973" citStr="Riloff et al., 2006" startWordPosition="294" endWordPosition="297">use a large set of features with dependable sentiment annotations and to be able to reliably deduce the sentiment of the headline from the sentiment of its components. The valence labeling subtask of the Affective Text task requires ternary — positive vs. negative vs. neutral — classification of headlines. While such categorization at the sentence level remains relatively unexplored&apos;, the two related sentence-level, binary classification tasks — positive vs. negative and subjective vs. objective — have attracted considerable attention in the recent years (Hu and Liu, 2004; Kim and Hovy, 2005; Riloff et al., 2006; Turney and Littman, 2003; Yu and Hatzivassiloglou, 2003). Unsupervised knowledge-based methods are the preferred approach to classification of sentences into positive and negative, mostly due to the lack of adequate amounts of labeled training data (Gamon and Aue, 2005). These approaches rely on presence and scores of sentiment-bearing words that have been acquired from dictionaries (Kim and Hovy, 2005) or corpora (Yu and Hatzivassiloglou, 2003). Their accuracy on news sentences is between 65 and 68%. Sentence-level subjectivity detection, where training data is easier to obtain than for pos</context>
</contexts>
<marker>Riloff, Patwardhan, Wiebe, 2006</marker>
<rawString>Ellen Riloff, Siddharth Patwardhan, and Janyce Wiebe. 2006. Feature subsumption for opinion analysis. In Proceedings ofEMNLP-06, the Conference on Empirical Methods in Natural Language Processing, pages 440–448, Sydney, AUS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Stone</author>
<author>D C Dumphy</author>
<author>M S Smith</author>
<author>D M Ogilvie</author>
</authors>
<title>The General Inquirer: a computer approach to content analysis. M.I.T. studies in comparative politics.</title>
<date>1966</date>
<publisher>M.I.T. Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="5711" citStr="Stone et al., 1966" startWordPosition="853" endWordPosition="856">ate errors produced by part-of-speech ambiguity of some of the seed words, the glosses are processed by Brill’s part-of-speech tagger (Brill, 1995) and only the seed words with matching part-of-speech tags are considered. Headwords with sentiment-bearing seed words in their definitions are then added to the positive or negative categories depending on the seed-word sentiment. Finally, words that were assigned contradicting — positive and negative — sentiment within the same run were eliminated. The average accuracy of 60 runs with non-intersecting seed lists when compared to General Inquirer (Stone et al., 1966) was 74%. In order to improve the list coverage, the words annotated as “Positiv” or “Negativ” in the General Inquirer that were not picked up by STEP were added to the final list. Since sentiment-bearing words in English have different degree of centrality to the category of sentiment, we have constructed a measure of word centrality to the category of positive or negative sentiment described in our earlier work (Andreevskaia and Bergler, 2006a). The measure, termed Net Overlap Score (NOS), is based on the number of ties that connect a given word to other words in the category. The number of </context>
</contexts>
<marker>Stone, Dumphy, Smith, Ogilvie, 1966</marker>
<rawString>P. J. Stone, D.C. Dumphy, M.S. Smith, and D.M. Ogilvie. 1966. The General Inquirer: a computer approach to content analysis. M.I.T. studies in comparative politics. M.I.T. Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
<author>Michael Littman</author>
</authors>
<title>Measuring praise and criticism: inference of semantic orientation from association.</title>
<date>2003</date>
<journal>ACM Transactions on Information Systems (TOIS),</journal>
<pages>21--315</pages>
<contexts>
<context position="1999" citStr="Turney and Littman, 2003" startWordPosition="298" endWordPosition="302">atures with dependable sentiment annotations and to be able to reliably deduce the sentiment of the headline from the sentiment of its components. The valence labeling subtask of the Affective Text task requires ternary — positive vs. negative vs. neutral — classification of headlines. While such categorization at the sentence level remains relatively unexplored&apos;, the two related sentence-level, binary classification tasks — positive vs. negative and subjective vs. objective — have attracted considerable attention in the recent years (Hu and Liu, 2004; Kim and Hovy, 2005; Riloff et al., 2006; Turney and Littman, 2003; Yu and Hatzivassiloglou, 2003). Unsupervised knowledge-based methods are the preferred approach to classification of sentences into positive and negative, mostly due to the lack of adequate amounts of labeled training data (Gamon and Aue, 2005). These approaches rely on presence and scores of sentiment-bearing words that have been acquired from dictionaries (Kim and Hovy, 2005) or corpora (Yu and Hatzivassiloglou, 2003). Their accuracy on news sentences is between 65 and 68%. Sentence-level subjectivity detection, where training data is easier to obtain than for positive vs. negative classif</context>
</contexts>
<marker>Turney, Littman, 2003</marker>
<rawString>Peter Turney and Michael Littman. 2003. Measuring praise and criticism: inference of semantic orientation from association. ACM Transactions on Information Systems (TOIS), 21:315–346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hong Yu</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences.</title>
<date>2003</date>
<booktitle>Proceedings of EMNLP-03, 8th Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>129--136</pages>
<editor>In Michael Collins and Mark Steedman, editors,</editor>
<location>Sapporo, Japan.</location>
<contexts>
<context position="2031" citStr="Yu and Hatzivassiloglou, 2003" startWordPosition="303" endWordPosition="306">timent annotations and to be able to reliably deduce the sentiment of the headline from the sentiment of its components. The valence labeling subtask of the Affective Text task requires ternary — positive vs. negative vs. neutral — classification of headlines. While such categorization at the sentence level remains relatively unexplored&apos;, the two related sentence-level, binary classification tasks — positive vs. negative and subjective vs. objective — have attracted considerable attention in the recent years (Hu and Liu, 2004; Kim and Hovy, 2005; Riloff et al., 2006; Turney and Littman, 2003; Yu and Hatzivassiloglou, 2003). Unsupervised knowledge-based methods are the preferred approach to classification of sentences into positive and negative, mostly due to the lack of adequate amounts of labeled training data (Gamon and Aue, 2005). These approaches rely on presence and scores of sentiment-bearing words that have been acquired from dictionaries (Kim and Hovy, 2005) or corpora (Yu and Hatzivassiloglou, 2003). Their accuracy on news sentences is between 65 and 68%. Sentence-level subjectivity detection, where training data is easier to obtain than for positive vs. negative classification, has been successfully p</context>
</contexts>
<marker>Yu, Hatzivassiloglou, 2003</marker>
<rawString>Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences. In Michael Collins and Mark Steedman, editors, Proceedings of EMNLP-03, 8th Conference on Empirical Methods in Natural Language Processing, pages 129–136, Sapporo, Japan.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>