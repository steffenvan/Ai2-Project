<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.037836">
<title confidence="0.997328">
Supervised Grammar Induction using Training Data with Limited
Constituent Information *
</title>
<author confidence="0.980537">
Rebecca Hwa
</author>
<affiliation confidence="0.978475">
Division of Engineering and Applied Sciences
Harvard University
</affiliation>
<address confidence="0.753284">
Cambridge, MA 02138 USA
</address>
<email confidence="0.998551">
rebecca@eecs.harvard.edu
</email>
<sectionHeader confidence="0.993863" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99942335">
Corpus-based grammar induction generally re-
lies on hand-parsed training data to learn the
structure of the language. Unfortunately, the
cost of building large annotated corpora is pro-
hibitively expensive. This work aims to improve
the induction strategy when there are few labels
in the training data. We show that the most in-
formative linguistic constituents are the higher
nodes in the parse trees, typically denoting com-
plex noun phrases and sentential clauses. They
account for only 20% of all constituents. For in-
ducing grammars from sparsely labeled training
data (e.g., only higher-level constituent labels),
we propose an adaptation strategy, which pro-
duces grammars that parse almost as well as
grammars induced from fully labeled corpora.
Our results suggest that for a partial parser to
replace human annotators, it must be able to
automatically extract higher-level constituents
rather than base noun phrases.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999971833333333">
The availability of large hand-parsed corpora
such as the Penn Treebank Project has made
high-quality statistical parsers possible. How-
ever, the parsers risk becoming too tailored to
these labeled training data that they cannot re-
liably process sentences from an arbitrary do-
main. Thus, while a parser trained on the
Wall Street Journal corpus can fairly accurately
parse a new Wall Street Journal article, it may
not perform as well on a New Yorker article.
To parse sentences from a new domain, one
would normally directly induce a new grammar
</bodyText>
<footnote confidence="0.810432">
* This material is based upon work supported by the Na-
tional Science Foundation under Grant No. IRI 9712068.
We thank Stuart Shieber for his guidance, and Lillian
Lee, Ric Crabbe, and the three anonymous reviewers for
their comments on the paper.
</footnote>
<bodyText confidence="0.9999686">
from that domain, in which the training pro-
cess would require hand-parsed sentences from
the new domain. Because parsing a large cor-
pus by hand is a labor-intensive task, it would
be beneficial to minimize the number of labels
needed to induce the new grammar.
We propose to adapt a grammar already
trained on an old domain to the new domain.
Adaptation can exploit the structural similar-
ity between the two domains so that fewer la-
beled data might be needed to update the gram-
mar to reflect the structure of the new domain.
This paper presents a quantitative study com-
paring direct induction and adaptation under
different training conditions. Our goal is to un-
derstand the effect of the amounts and types
of labeled data on the training process for both
induction strategies. For example, how much
training data need to be hand-labeled? Must
the parse trees for each sentence be fully spec-
ified? Are some linguistic constituents in the
parse more informative than others?
To answer these questions, we have performed
experiments that compare the parsing quali-
ties of grammars induced under different train-
ing conditions using both adaptation and di-
rect induction. We vary the number of labeled
brackets and the linguistic classes of the labeled
brackets. The study is conducted on both a sim-
ple Air Travel Information System (ATIS) cor-
pus (Hemphill et al., 1990) and the more com-
plex Wall Street Journal (WSJ) corpus (Marcus
et al., 1993).
Our results show that the training examples
do not need to be fully parsed for either strat-
egy, but adaptation produces better grammars
than direct induction under the conditions of
minimally labeled training data. For instance,
the most informative brackets, which label con-
stituents higher up in the parse trees, typically
</bodyText>
<page confidence="0.997989">
73
</page>
<bodyText confidence="0.999993368421052">
identifying complex noun phrases and senten-
tial clauses, account for only 17% of all con-
stituents in ATIS and 21% in WSJ. Trained on
this type of label, the adapted grammars parse
better than the directly induced grammars and
almost as well as those trained on fully labeled
data. Training on ATIS sentences labeled with
higher-level constituent brackets, a directly in-
duced grammar parses test sentences with 66%
accuracy, whereas an adapted grammar parses
with 91% accuracy, which is only 2% lower than
the score of a grammar induced from fully la-
beled training data. Training on WSJ sentences
labeled with higher-level constituent brackets,
a directly induced grammar parses with 70%
accuracy, whereas an adapted grammar parses
with 72% accuracy, which is 6% lower than the
score of a grammar induced from fully labeled
training data.
That the most informative brackets are
higher-level constituents and make up only one-
fifth of all the labels in the corpus has two impli-
cations. First, it shows that there is potential
reduction of labor for the human annotators.
Although the annotator still must process an
entire sentence mentally, the task of identifying
higher-level structures such as sentential clauses
and complex nouns should be less tedious than
to fully specify the complete parse tree for each
sentence. Second, one might speculate the pos-
sibilities of replacing human supervision alto-
gether with a partial parser that locates con-
stituent chunks within a sentence. However,
as our results indicate that the most informa-
tive constituents are higher-level phrases, the
parser would have to identify sentential clauses
and complex noun phrases rather than low-level
base noun phrases.
</bodyText>
<sectionHeader confidence="0.992253" genericHeader="related work">
2 Related Work on Grammar
Induction
</sectionHeader>
<bodyText confidence="0.999891409836066">
Grammar induction is the process of inferring
the structure of a language by learning from ex-
ample sentences drawn from the language. The
degree of difficulty in this task depends on three
factors. First, it depends on the amount of
supervision provided. Charniak (1996), for in-
stance, has shown that a grammar can be easily
constructed when the examples are fully labeled
parse trees. On the other hand, if the examples
consist of raw sentences with no extra struc-
tural information, grammar induction is very
difficult, even theoretically impossible (Gold,
1967). One could take a greedy approach such
as the well-known Inside-Outside re-estimation
algorithm (Baker, 1979), which induces locally
optimal grammars by iteratively improving the
parameters of the grammar so that the entropy
of the training data is minimized. In practice,
however, when trained on unmarked data, the
algorithm tends to converge on poor grammar
models. For even a moderately complex domain
such as the ATIS corpus, a grammar trained
on data with constituent bracketing information
produces much better parses than one trained
on completely unmarked raw data (Pereira and
Schabes, 1992). Part of our work explores the
in-between case, when only some constituent la-
bels are available. Section 3 defines the different
types of annotation we examine.
Second, as supervision decreases, the learning
process relies more on search. The success of
the induction depends on the initial parameters
of the grammar because a local search strategy
may converge to a local minimum. For finding
a good initial parameter set, Lan i and Young
(1990) suggested first estimating the probabili-
ties with a set of regular grammar rules. Their
experiments, however, indicated that the main
benefit from this type of pretraining is one
of run-time efficiency; the improvement in the
quality of the induced grammar was minimal.
Briscoe and Waegner (1992) argued that one
should first hand-design the grammar to en-
code some linguistic notions and then use the re-
estimation procedure to fine-tune the parame-
ters, substituting the cost of hand-labeled train-
ing data with that of hand-coded grammar. Our
idea of grammar adaptation can be seen as a
form of initialization. It attempts to seed the
grammar in a favorable search space by first
training it with data from an existing corpus.
Section 4 discusses the induction strategies in
more detail.
A third factor that affects the learning pro-
cess is the complexity of the data. In their study
of parsing the WSJ, Schabes et al. (1993) have
shown that a grammar trained on the Inside-
Outside re-estimation algorithm can perform
quite well on short simple sentences but falters
as the sentence length increases. To take this
factor into account, we perform our experiments
</bodyText>
<page confidence="0.995246">
74
</page>
<table confidence="0.999079">
Categories Labeled Sentence ATIS WSJ
HighP (I want (to take (the flight with at most one stop))) 17% 21%
BaseNP (I) want to take (the flight) with (at most one stop) 27% 29%
BaseP (I) want to take (the flight) with (at most one) stop 32% 30%
AllNP (I) want to take ((the flight) with (at most one stop)) 37% 43%
NotBaseP (I (want (to (take (the flight (with (at most one stop))))))) 68% 70%
</table>
<tableCaption confidence="0.999569">
Table 1: The second column shows how the example sentence ((I) (want (to (take ((the flight)
</tableCaption>
<bodyText confidence="0.9848756">
(with ((at most one) stop))))))) is labeled under each category. The third and fourth columns list
the percentage break-down of brackets in each category for ATIS and WSJ respectively.
on both a simple domain (ATIS) and a complex
one (WSJ). In Section 5, we describe the exper-
iments and report the results.
</bodyText>
<sectionHeader confidence="0.975941" genericHeader="method">
3 Training Data Annotation
</sectionHeader>
<bodyText confidence="0.998194977272727">
The training sets are annotated in multiple
ways, falling into two categories. First, we con-
struct training sets annotated with random sub-
sets of constituents consisting 0%, 25%, 50%,
75% and 100% of the brackets in the fully an-
notated corpus. Second, we construct sets train-
ing in which only a certain type of constituent is
annotated. We study five linguistic categories.
Table 1 summarizes the annotation differences
between the five classes and lists the percent-
age of brackets in each class with respect to
the total number of constituents&apos; for ATIS and
WSJ. In an AIINP training set, all and only
the noun phrases in the sentences are labeled.
For the BaseNP class, we label only simple
noun phrases that contain no embedded noun
phrases. Similarly for a BaseP set, all sim-
ple phrases made up of only lexical items are
labeled. Although there is a high intersection
between the set of BaseP labels and the set of
BaseNP labels, the two classes are not identical.
A BaseNP may contain a BaseP. For the exam-
ple in Table 1, the phrase &amp;quot;at most one stop&amp;quot;
is a BaseNP that contains a quantifier BaseP
&amp;quot;at most one.&amp;quot; NotBaseP is the complement
of BaseP. The majority of the constituents in
a sentence belongs to this category, in which at
least one of the constituent&apos;s sub-constituents is
not a simple lexical item. Finally, in a HighP
set, we label only complex phrases that decom-
1For computing the percentage of brackets, the outer-
most bracket around the entire sentence and the brack-
ets around singleton phrases (e.g., the pronoun &amp;quot;I&amp;quot; as a
BaseNP) are excluded because they do not contribute to
the pruning of parses.
pose into sub-phrases that may be either an-
other HighP or a BaseP. That is, a HighP con-
stituent does not directly subsume any lexical
word. A typical HighP is a sentential clause or a
complex noun phrase. The example sentence in
Table 1 contains 3 HighP constituents: a com-
plex noun phrase made up of a BaseNP and a
prepositional phrase; a sentential clause with an
omitted subject NP; and the full sentence.
</bodyText>
<sectionHeader confidence="0.996336" genericHeader="method">
4 Induction Strategies
</sectionHeader>
<bodyText confidence="0.999985586206896">
To induce a grammar from the sparsely brack-
eted training data previously described, we use
a variant of the Inside-Outside re-estimation
algorithm proposed by Pereira and Schabes
(1992). The inferred grammars are repre-
sented in the Probabilistic Lexicalized Tree In-
sertion Grammar (PLTIG) formalism (Schabes
and Waters, 1993; Hwa, 1998a), which is lexical-
ized and context-free equivalent. We favor the
PLTIG representation for two reasons. First, it
is amenable to the Inside-Outside re-estimation
algorithm (the equations calculating the inside
and outside probabilities for PLTIGs can be
found in Hwa (1998b)). Second, its lexicalized
representation makes the training process more
efficient than a traditional PCFG while main-
taining comparable parsing qualities.
Two training strategies are considered: di-
rect induction, in which a grammar is induced
from scratch, learning from only the sparsely la-
beled training data; and adaptation, a two-stage
learning process that first uses direct induction
to train the grammar on an existing fully la-
beled corpus before retraining it on the new cor-
pus. During the retraining phase, the probabil-
ities of the grammars are re-estimated based on
the new training data. We expect the adaptive
method to induce better grammars than direct
induction when the new corpus is only partially
</bodyText>
<page confidence="0.994488">
75
</page>
<bodyText confidence="0.997418666666667">
annotated because the adapted grammars have
collected better statistics from the fully labeled
data of another corpus.
</bodyText>
<sectionHeader confidence="0.991678" genericHeader="method">
5 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999968392857143">
We perform two experiments. The first uses
ATIS as the corpus from which the different
types of partially labeled training sets are gener-
ated. Both induction strategies train from these
data, but the adaptive strategy pretrains its
grammars with fully labeled data drawn from
the WSJ corpus. The trained grammars are
scored on their parsing abilities on unseen ATIS
test sets. We use the non-crossing bracket mea-
surement as the parsing metric. This experi-
ment will show whether annotations of a partic-
ular linguistic category may be more useful for
training grammars than others. It will also in-
dicate the comparative merits of the two induc-
tion strategies trained on data annotated with
these linguistic categories. However, pretrain-
ing on the much more complex WSJ corpus may
be too much of an advantage for the adaptive
strategy. Therefore, we reverse the roles of the
corpus in the second experiment. The partially
labeled data are from the WSJ corpus, and the
adaptive strategy is pretrained on fully labeled
ATIS data. In both cases, part-of-speech(POS)
tags are used as the lexical items of the sen-
tences. Backing off to POS tags is necessary
because the tags provide a considerable inter-
section in the vocabulary sets of the two cor-
pora.
</bodyText>
<subsectionHeader confidence="0.994809">
5.1 Experiment 1: Learning ATIS
</subsectionHeader>
<bodyText confidence="0.999980357142857">
The easier learning task is to induce grammars
to parse ATIS sentences. The ATIS corpus con-
sists of 577 short sentences with simple struc-
tures, and the vocabulary set is made up of 32
POS tags, a subset of the 47 tags used for the
WSJ. Due to the limited size of this corpus, ten
sets of randomly partitioned train-test-held-out
triples are generated to ensure the statistical
significance of our results. We use 80 sentences
for testing, 90 sentences for held-out data, and
the rest for training. Before proceeding with
the main discussion on training from the ATIS,
we briefly describe the pretraining stage of the
adaptive strategy.
</bodyText>
<subsectionHeader confidence="0.539081">
5.1.1 Pretraining with WSJ
</subsectionHeader>
<bodyText confidence="0.999990642857143">
The idea behind the adaptive method is simply
to make use of any existing labeled data. We
hope that pretraining the grammars on these
data might place them in a better position to
learn from the new, sparsely labeled data. In
the pretraining stage for this experiment, a
grammar is directly induced from 3600 fully
labeled WSJ sentences. Without any further
training on ATIS data, this grammar achieves a
parsing score of 87.3% on ATIS test sentences.
The relatively high parsing score suggests that
pretraining with WSJ has successfully placed
the grammar in a good position to begin train-
ing with the ATIS data.
</bodyText>
<subsubsectionHeader confidence="0.720959">
5.1.2 Partially Supervised Training on
</subsubsectionHeader>
<sectionHeader confidence="0.961704" genericHeader="method">
ATIS
</sectionHeader>
<bodyText confidence="0.999609088235294">
We now return to the main focus of this experi-
ment: learning from sparsely annotated ATIS
training data. To verify whether some con-
stituent classes are more informative than oth-
ers, we could compare the parsing scores of the
grammars trained using different constituent
class labels. But this evaluation method does
not take into account that the distribution of
the constituent classes is not uniform. To nor-
malize for this inequity, we compare the parsing
scores to a baseline that characterizes the rela-
tionship between the performance of the trained
grammar and the number of bracketed con-
stituents in the training data. To generate the
baseline, we create training data in which 0%,
25%, 50%, 75%, and 100% of the constituent
brackets are randomly chosen to be included.
One class of linguistic labels is better than an-
other if its resulting parsing improvement over
the baseline is higher than that of the other.
The test results of the grammars induced
from these different training data are summa-
rized in Figure 1. Graph (a) plots the outcome
of using the direct induction strategy, and graph
(b) plots the outcome of the adaptive strat-
egy. In each graph, the baseline of random con-
stituent brackets is shown as a solid line. Scores
of grammars trained from constituent type spe-
cific data sets are plotted as labeled dots. The
dotted horizontal line in graph (b) indicates the
ATIS parsing score of the grammar trained on
WSJ alone.
Comparing the five constituent types, we see
that the HighP class is the most informative
</bodyText>
<page confidence="0.918332">
76
</page>
<figure confidence="0.99301404">
55
Rand-75% Rand-10950_,
200 400 800 800 1000 1200 1400 1600
Number of brackets in Ihe ATIS training data
(a)
HighP g 14°&amp;quot;d-25% MOP NotBsseP
wsawy
Basel.?
200 400 600 1300 1000 1200 1400 1600
Number of brackets in the ATIS training data
(b)
90
85
5
75
70
65
a
75
3
70
z
65
60
55
</figure>
<figureCaption confidence="0.980761666666667">
Figure 1: Parsing accuracies of (a) directly induced grammars and (b) adapted grammars as a
function of the number of brackets present in the training corpus. There are 1595 brackets in the
training corpus all together.
</figureCaption>
<bodyText confidence="0.998837456521739">
for the adaptive strategy, resulting in a gram-
mar that scored better than the baseline. The
grammars trained on the AllNP annotation per-
formed as well as the baseline for both strate-
gies. Grammars trained under all the other
training conditions scored below the baseline.
Our results suggest that while an ideal train-
ing condition would include annotations of both
higher-level phrases and simple phrases, com-
plex clauses are more informative. This inter-
pretation explains the large gap between the
parsing scores of the directly induced grammar
and the adapted grammar trained on the same
HighP data. The directly induced grammar
performed poorly because it has never seen a
labeled example of simple phrases. In contrast,
the adapted grammar was already exposed to
labeled WSJ simple phrases, so that it success-
fully adapted to the new corpus from annotated
examples of higher-level phrases. On the other
hand, training the adapted grammar on anno-
tated ATIS simple phrases is not successful even
though it has seen examples of WSJ higher-
level phrases. This also explains why gram-
mars trained on the conglomerate class Not-
BaseP performed on the same level as those
trained on the AllNP class. Although the Not-
BaseP set contains the most brackets, most of
the brackets are irrelevant to the training pro-
cess, as they are neither higher-level phrases nor
simple phrases.
Our experiment also indicates that induction
strategies exhibit different learning characteris-
tics under partially supervised training condi-
tions. A side by side comparison of Figure 1
(a) and (b) shows that the adapted grammars
perform significantly better than the directly
induced grammars as the level of supervision
decreases. This supports our hypothesis that
pretraining on a different corpus can place the
grammar in a good initial search space for learn-
ing the new domain. Unfortunately, a good ini-
tial state does not obviate the need for super-
vised training. We see from Figure 1(b) that
retraining with unlabeled ATIS sentences actu-
ally lowers the grammar&apos;s parsing accuracy.
</bodyText>
<subsectionHeader confidence="0.999692">
5.2 Experiment 2: Learning WSJ
</subsectionHeader>
<bodyText confidence="0.999690866666667">
In the previous section, we have seen that anno-
tations of complex clauses are the most helpful
for inducing ATIS-style grammars. One of the
goals of this experiment is to verify whether the
result also holds for the WSJ corpus, which is
structurally very different from ATIS. The WSJ
corpus uses 47 POS tags, and its sentences are
longer and have more embedded clauses.
As in the previous experiment, we construct
training sets with annotations of different con-
stituent types and of different numbers of ran-
domly chosen labels. Each training set consists
of 3600 sentences, and 1780 sentences are used
as held-out data. The trained grammars are
tested on a set of 2245 sentences.
</bodyText>
<figureCaption confidence="0.553727">
Figure 2 (a) and (b) summarize the outcomes
</figureCaption>
<page confidence="0.994089">
77
</page>
<figureCaption confidence="0.985919666666667">
Figure 2: Parsing accuracies of (a) directly induced grammars and (b) adapted grammars as a
function of the number of brackets present in the training corpus. There is a total of 46463 brackets
in the training corpus.
</figureCaption>
<figure confidence="0.995405130434782">
5000 10003 15000 20000 25000 30000 35000 40000 45000
number of brackets in Me WSJ twining data
(b)
50D0 MOO 15000 20000 MM 30000 35000 40000 45000
Number d bracketsin dlOOM training Ma
(a)
Rma -mi-----Ronsuro4
Noteasse
60
75
70
65
80
50
45
40
Rand-75 Rand-100%
Rand-50
Nog. op
Rand-25%
ighP
BariaNFNariee
AILS only
</figure>
<bodyText confidence="0.999781896551724">
of this experiment. Many results of this section
are similar to the ATIS experiment. Higher-
level phrases still provide the most information;
the grammars trained on the HighP labels are
the only ones that scored as well as the baseline.
Labels of simple phrases still seem the least in-
formative; scores of grammars trained on BaseP
and BaseNP remained far below the baseline.
Different from the previous experiment, how-
ever, the AIINP training sets do not seem to
provide as much information for this learning
task. This may be due to the increase in the
sentence complexity of the WSJ, which further
de-emphasized the role of the simple phrases.
Thus, grammars trained on AIINP labels have
comparable parsing scores to those trained on
HighP labels. Also, we do not see as big a gap
between the scores of the two induction strate-
gies in the HighP case because the adapted
grammar&apos;s advantage of having seen annotated
ATIS base nouns is reduced. Nonetheless, the
adapted grammars still perform 2% better than
the directly induced grammars, and this im-
provement is statistically significant.2
Furthermore, grammars trained on NotBaseP
do not fall as far below the baseline and have
higher parsing scores than those trained on
HighP and AIINP. This suggests that for more
complex domains, other linguistic constituents
</bodyText>
<footnote confidence="0.876305333333333">
2A pair-wise t-test comparing the parsing scores of
the ten test sets for the two strategies shows 99% confi-
dence in the difference.
</footnote>
<bodyText confidence="0.9805857">
such as verb phrases3 become more informative.
A second goal of this experiment is to test the
adaptive strategy under more stringent condi-
tions. In the previous experiment, a WSJ-style
grammar was retrained for the simpler ATIS
corpus. Now, we reverse the roles of the cor-
pora to see whether the adaptive strategy still
offers any advantage over direct induction.
In the adaptive method&apos;s pretraining stage,
a grammar is induced from 400 fully labeled
ATIS sentences. Testing this ATIS-style gram-
mar on the WSJ test set without further train-
ing renders a parsing accuracy of 40%. The
low score suggests that fully labeled ATIS data
does not teach the grammar as much about
the structure of WSJ. Nonetheless, the adap-
tive strategy proves to be beneficial for learning
WSJ from sparsely labeled training sets. The
adapted grammars out-perform the directly in-
duced grammars when more than 50% of the
brackets are missing from the training data.
The most significant difference is when the
training data contains no label information at
all. The adapted grammar parses with 60.1%
accuracy whereas the directly induced grammar
parses with 49.8% accuracy.
3We have not experimented with training sets con-
taining only verb phrases labels (i.e., setting a pair of
bracket around the head verb and its modifiers). They
are a subset of the NotBaseP class.
</bodyText>
<page confidence="0.997218">
78
</page>
<sectionHeader confidence="0.991427" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999991536585366">
In this study, we have shown that the structure
of a grammar can be reliably learned without
having fully specified constituent information
in the training sentences and that the most in-
formative constituents of a sentence are higher-
level phrases, which make up only a small per-
centage of the total number of constituents.
Moreover, we observe that grammar adaptation
works particularly well with this type of sparse
but informative training data. An adapted
grammar consistently outperforms a directly in-
duced grammar even when adapting from a sim-
pler corpus to a more complex one.
These results point us to three future di-
rections. First, that the labels for some con-
stituents are more informative than others im-
plies that sentences containing more of these in-
formative constituents make better training ex-
amples. It may be beneficial to estimate the
informational content of potential training (un-
marked) sentences. The training set should only
include sentences that are predicted to have
high information values. Filtering out unhelpful
sentences from the training set reduces unnec-
essary work for the human annotators. Second,
although our experiments show that a sparsely
labeled training set is more of an obstacle for the
direct induction approach than for the grammar
adaptation approach, the direct induction strat-
egy might also benefit from a two stage learning
process similar to that used for grammar adap-
tation. Instead of training on a different corpus
in each stage, the grammar can be trained on
a small but fully labeled portion of the corpus
in its first stage and the sparsely labeled por-
tion in the second stage. Finally, higher-level
constituents have proved to be the most infor-
mative linguistic units. To relieve humans from
labeling any training data, we should consider
using partial parsers that can automatically de-
tect complex nouns and sentential clauses.
</bodyText>
<sectionHeader confidence="0.99941" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997667549019608">
J.K. Baker. 1979. Trainable grammars for
speech recognition. In Proceedings of the
Spring Conference of the Acoustical Society of
America, pages 547-550, Boston, MA, June.
E.J. Briscoe and N. Waegner. 1992. Robust
stochastic parsing using the inside-outside al-
gorithm. In Proceedings of the AAAI Work-
shop on Probabilistically-Based NLP Tech-
niques, pages 39-53.
E. Charniak. 1996. Tree-bank grammars. In
Proceedings of the Thirteenth National Con-
ference on Artificial Intelligence, pages 1031-
1036.
E. Mark Gold. 1967. Language identification
in the limit. Information Control, l0(5):447-
474.
C.T. Hemphill, J.J. Godfrey, and G.R. Dod-
dington. 1990. The ATIS spoken language
systems pilot corpus. In DARPA Speech and
Natural Language Workshop, Hidden Valley,
Pennsylvania, June. Morgan Kaufmann.
R. Hwa. 1998a. An empirical evaluation of
probabilistic lexicalized tree insertion gram-
mars. In Proceedings of COLING-ACL, vol-
ume 1, pages 557-563.
R. Hwa. 1998b. An empirical evaluation of
probabilistic lexicalized tree insertion gram-
mars. Technical Report 06-98, Harvard Uni-
versity. Available as cmp-lg/9808001.
K. Lan i and S.J. Young. 1990. The estima-
tion of stochastic context-free grammars us-
ing the inside-outside algorithm. Computer
Speech and Language, 4:35-56.
M. Marcus, B. Santorini, and M. Marcinkiewicz.
1993. Building a large annontated corpus of
english: the penn treebank. Computational
Linguistics, 19(2) :313-330.
F. Pereira and Y. Schabes. 1992. Inside-
Outside reestimation from partially bracketed
corpora. In Proceedings of the 30th Annual
Meeting of the ACL, pages 128-135, Newark,
Delaware.
Y. Schabes and R. Waters. 1993. Stochastic
lexicalized context-free grammar. In Proceed-
ings of the Third International Workshop on
Parsing Technologies, pages 257-266.
Y. Schabes, M. Roth, and R. Osborne. 1993.
Parsing the Wall Street Journal with the
Inside-Outside algorithm. In Proceedings of
the Sixth Conference of the European Chap-
ter of the ACL, pages 341-347.
</reference>
<page confidence="0.998991">
79
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9828595">Supervised Grammar Induction using Training Data with Limited Constituent Information *</title>
<author confidence="0.996883">Rebecca Hwa</author>
<affiliation confidence="0.9995875">Division of Engineering and Applied Sciences Harvard University</affiliation>
<address confidence="0.999936">Cambridge, MA 02138 USA</address>
<email confidence="0.999868">rebecca@eecs.harvard.edu</email>
<abstract confidence="0.984419812154696">Corpus-based grammar induction generally relies on hand-parsed training data to learn the structure of the language. Unfortunately, the cost of building large annotated corpora is prohibitively expensive. This work aims to improve the induction strategy when there are few labels in the training data. We show that the most informative linguistic constituents are the higher nodes in the parse trees, typically denoting complex noun phrases and sentential clauses. They for only all constituents. For inducing grammars from sparsely labeled training data (e.g., only higher-level constituent labels), propose an which produces grammars that parse almost as well as grammars induced from fully labeled corpora. Our results suggest that for a partial parser to replace human annotators, it must be able to automatically extract higher-level constituents rather than base noun phrases. The availability of large hand-parsed corpora such as the Penn Treebank Project has made high-quality statistical parsers possible. However, the parsers risk becoming too tailored to these labeled training data that they cannot reliably process sentences from an arbitrary domain. Thus, while a parser trained on the Wall Street Journal corpus can fairly accurately parse a new Wall Street Journal article, it may not perform as well on a New Yorker article. To parse sentences from a new domain, one normally induce new grammar * This material is based upon work supported by the National Science Foundation under Grant No. IRI 9712068. We thank Stuart Shieber for his guidance, and Lillian Lee, Ric Crabbe, and the three anonymous reviewers for their comments on the paper. from that domain, in which the training process would require hand-parsed sentences from the new domain. Because parsing a large corpus by hand is a labor-intensive task, it would be beneficial to minimize the number of labels needed to induce the new grammar. propose to grammar already trained on an old domain to the new domain. Adaptation can exploit the structural similarity between the two domains so that fewer labeled data might be needed to update the grammar to reflect the structure of the new domain. This paper presents a quantitative study comparing direct induction and adaptation under different training conditions. Our goal is to understand the effect of the amounts and types of labeled data on the training process for both induction strategies. For example, how much training data need to be hand-labeled? Must the parse trees for each sentence be fully specified? Are some linguistic constituents in the parse more informative than others? To answer these questions, we have performed experiments that compare the parsing qualities of grammars induced under different training conditions using both adaptation and direct induction. We vary the number of labeled brackets and the linguistic classes of the labeled brackets. The study is conducted on both a simple Air Travel Information System (ATIS) corpus (Hemphill et al., 1990) and the more complex Wall Street Journal (WSJ) corpus (Marcus et al., 1993). Our results show that the training examples do not need to be fully parsed for either strategy, but adaptation produces better grammars than direct induction under the conditions of minimally labeled training data. For instance, the most informative brackets, which label constituents higher up in the parse trees, typically 73 identifying complex noun phrases and sentential clauses, account for only 17% of all constituents in ATIS and 21% in WSJ. Trained on this type of label, the adapted grammars parse better than the directly induced grammars and almost as well as those trained on fully labeled data. Training on ATIS sentences labeled with higher-level constituent brackets, a directly induced grammar parses test sentences with 66% accuracy, whereas an adapted grammar parses with 91% accuracy, which is only 2% lower than the score of a grammar induced from fully labeled training data. Training on WSJ sentences labeled with higher-level constituent brackets, a directly induced grammar parses with 70% accuracy, whereas an adapted grammar parses with 72% accuracy, which is 6% lower than the score of a grammar induced from fully labeled training data. That the most informative brackets are higher-level constituents and make up only onefifth of all the labels in the corpus has two implications. First, it shows that there is potential reduction of labor for the human annotators. Although the annotator still must process an entire sentence mentally, the task of identifying higher-level structures such as sentential clauses and complex nouns should be less tedious than to fully specify the complete parse tree for each sentence. Second, one might speculate the possibilities of replacing human supervision altogether with a partial parser that locates constituent chunks within a sentence. However, as our results indicate that the most informative constituents are higher-level phrases, the parser would have to identify sentential clauses and complex noun phrases rather than low-level base noun phrases. 2 Related Work on Grammar Induction Grammar induction is the process of inferring the structure of a language by learning from example sentences drawn from the language. The degree of difficulty in this task depends on three factors. First, it depends on the amount of supervision provided. Charniak (1996), for instance, has shown that a grammar can be easily constructed when the examples are fully labeled parse trees. On the other hand, if the examples of raw sentences with no structural information, grammar induction is very difficult, even theoretically impossible (Gold, 1967). One could take a greedy approach such as the well-known Inside-Outside re-estimation algorithm (Baker, 1979), which induces locally optimal grammars by iteratively improving the parameters of the grammar so that the entropy of the training data is minimized. In practice, however, when trained on unmarked data, the algorithm tends to converge on poor grammar models. For even a moderately complex domain such as the ATIS corpus, a grammar trained on data with constituent bracketing information produces much better parses than one trained on completely unmarked raw data (Pereira and Schabes, 1992). Part of our work explores the in-between case, when only some constituent labels are available. Section 3 defines the different types of annotation we examine. Second, as supervision decreases, the learning process relies more on search. The success of the induction depends on the initial parameters of the grammar because a local search strategy may converge to a local minimum. For finding a good initial parameter set, Lan i and Young (1990) suggested first estimating the probabilities with a set of regular grammar rules. Their experiments, however, indicated that the main benefit from this type of pretraining is one of run-time efficiency; the improvement in the quality of the induced grammar was minimal. Briscoe and Waegner (1992) argued that one should first hand-design the grammar to encode some linguistic notions and then use the reestimation procedure to fine-tune the parameters, substituting the cost of hand-labeled training data with that of hand-coded grammar. Our idea of grammar adaptation can be seen as a form of initialization. It attempts to seed the grammar in a favorable search space by first training it with data from an existing corpus. Section 4 discusses the induction strategies in more detail. A third factor that affects the learning process is the complexity of the data. In their study of parsing the WSJ, Schabes et al. (1993) have shown that a grammar trained on the Inside- Outside re-estimation algorithm can perform quite well on short simple sentences but falters as the sentence length increases. To take this factor into account, we perform our experiments 74</abstract>
<keyword confidence="0.325102">Categories Labeled Sentence ATIS WSJ</keyword>
<note confidence="0.795985833333333">HighP (I want (to take (the flight with at most one stop))) 17% 21% BaseNP to take (the flight) with (at most one stop) 27% 29% BaseP to take (the flight) with (at most one) stop 32% 30% AllNP to take ((the flight) with (at most one stop)) 37% 43% NotBaseP (I (want (to (take (the flight (with (at most one stop))))))) 68% 70% 1: The second column shows how the example sentence (want (to (take ((the flight)</note>
<abstract confidence="0.994980983425415">at most one) stop))))))) labeled under each category. The third and fourth columns list the percentage break-down of brackets in each category for ATIS and WSJ respectively. on both a simple domain (ATIS) and a complex one (WSJ). In Section 5, we describe the experiments and report the results. 3 Training Data Annotation The training sets are annotated in multiple ways, falling into two categories. First, we construct training sets annotated with random subsets of constituents consisting 0%, 25%, 50%, 75% and 100% of the brackets in the fully annotated corpus. Second, we construct sets training in which only a certain type of constituent is annotated. We study five linguistic categories. Table 1 summarizes the annotation differences between the five classes and lists the percentage of brackets in each class with respect to the total number of constituents&apos; for ATIS and In an set, all and only the noun phrases in the sentences are labeled. the we label only simple noun phrases that contain no embedded noun Similarly for a all simple phrases made up of only lexical items are labeled. Although there is a high intersection between the set of BaseP labels and the set of BaseNP labels, the two classes are not identical. A BaseNP may contain a BaseP. For the example in Table 1, the phrase &amp;quot;at most one stop&amp;quot; is a BaseNP that contains a quantifier BaseP most one.&amp;quot; the complement of BaseP. The majority of the constituents in a sentence belongs to this category, in which at least one of the constituent&apos;s sub-constituents is a simple lexical item. Finally, in a we label only complex phrases that decomcomputing the percentage of brackets, the outermost bracket around the entire sentence and the brackets around singleton phrases (e.g., the pronoun &amp;quot;I&amp;quot; as a BaseNP) are excluded because they do not contribute to the pruning of parses. pose into sub-phrases that may be either another HighP or a BaseP. That is, a HighP constituent does not directly subsume any lexical word. A typical HighP is a sentential clause or a complex noun phrase. The example sentence in Table 1 contains 3 HighP constituents: a complex noun phrase made up of a BaseNP and a prepositional phrase; a sentential clause with an omitted subject NP; and the full sentence. 4 Induction Strategies To induce a grammar from the sparsely bracketed training data previously described, we use a variant of the Inside-Outside re-estimation algorithm proposed by Pereira and Schabes (1992). The inferred grammars are represented in the Probabilistic Lexicalized Tree Insertion Grammar (PLTIG) formalism (Schabes and Waters, 1993; Hwa, 1998a), which is lexicalized and context-free equivalent. We favor the PLTIG representation for two reasons. First, it is amenable to the Inside-Outside re-estimation algorithm (the equations calculating the inside and outside probabilities for PLTIGs can be found in Hwa (1998b)). Second, its lexicalized representation makes the training process more efficient than a traditional PCFG while maintaining comparable parsing qualities. Two training strategies are considered: direct induction, in which a grammar is induced from scratch, learning from only the sparsely labeled training data; and adaptation, a two-stage learning process that first uses direct induction to train the grammar on an existing fully labeled corpus before retraining it on the new corpus. During the retraining phase, the probabilities of the grammars are re-estimated based on the new training data. We expect the adaptive method to induce better grammars than direct induction when the new corpus is only partially 75 annotated because the adapted grammars have collected better statistics from the fully labeled data of another corpus. 5 Experiments and Results We perform two experiments. The first uses ATIS as the corpus from which the different types of partially labeled training sets are generated. Both induction strategies train from these data, but the adaptive strategy pretrains its grammars with fully labeled data drawn from the WSJ corpus. The trained grammars are scored on their parsing abilities on unseen ATIS test sets. We use the non-crossing bracket measurement as the parsing metric. This experiment will show whether annotations of a particular linguistic category may be more useful for training grammars than others. It will also indicate the comparative merits of the two induction strategies trained on data annotated with these linguistic categories. However, pretraining on the much more complex WSJ corpus may be too much of an advantage for the adaptive strategy. Therefore, we reverse the roles of the corpus in the second experiment. The partially labeled data are from the WSJ corpus, and the adaptive strategy is pretrained on fully labeled ATIS data. In both cases, part-of-speech(POS) tags are used as the lexical items of the sentences. Backing off to POS tags is necessary because the tags provide a considerable intersection in the vocabulary sets of the two corpora. 5.1 Experiment 1: Learning ATIS The easier learning task is to induce grammars to parse ATIS sentences. The ATIS corpus consists of 577 short sentences with simple structures, and the vocabulary set is made up of 32 POS tags, a subset of the 47 tags used for the WSJ. Due to the limited size of this corpus, ten sets of randomly partitioned train-test-held-out triples are generated to ensure the statistical significance of our results. We use 80 sentences for testing, 90 sentences for held-out data, and the rest for training. Before proceeding with the main discussion on training from the ATIS, we briefly describe the pretraining stage of the adaptive strategy. 5.1.1 Pretraining with WSJ The idea behind the adaptive method is simply to make use of any existing labeled data. We hope that pretraining the grammars on these data might place them in a better position to learn from the new, sparsely labeled data. In the pretraining stage for this experiment, a grammar is directly induced from 3600 fully labeled WSJ sentences. Without any further training on ATIS data, this grammar achieves a parsing score of 87.3% on ATIS test sentences. The relatively high parsing score suggests that pretraining with WSJ has successfully placed the grammar in a good position to begin training with the ATIS data. 5.1.2 Partially Supervised Training on ATIS We now return to the main focus of this experiment: learning from sparsely annotated ATIS training data. To verify whether some constituent classes are more informative than others, we could compare the parsing scores of the grammars trained using different constituent class labels. But this evaluation method does not take into account that the distribution of the constituent classes is not uniform. To normalize for this inequity, we compare the parsing scores to a baseline that characterizes the relationship between the performance of the trained grammar and the number of bracketed constituents in the training data. To generate the baseline, we create training data in which 0%, 25%, 50%, 75%, and 100% of the constituent brackets are randomly chosen to be included. One class of linguistic labels is better than another if its resulting parsing improvement over the baseline is higher than that of the other. The test results of the grammars induced from these different training data are summarized in Figure 1. Graph (a) plots the outcome of using the direct induction strategy, and graph (b) plots the outcome of the adaptive strategy. In each graph, the baseline of random constituent brackets is shown as a solid line. Scores of grammars trained from constituent type specific data sets are plotted as labeled dots. The dotted horizontal line in graph (b) indicates the ATIS parsing score of the grammar trained on WSJ alone. Comparing the five constituent types, we see that the HighP class is the most informative 76 55</abstract>
<address confidence="0.7284445">Rand-75% Rand-10950_, 200 400 800 800 1000 1200 1400 1600</address>
<abstract confidence="0.891558438775511">Number of brackets in Ihe ATIS training data (a) g 14°&amp;quot;d-25%MOP NotBsseP wsawy Basel.? 200 400 600 1300 1000 1200 1400 1600 Number of brackets in the ATIS training data (b) 90 85 5 75 70 65 a 75 3 70 z 65 60 55 Figure 1: Parsing accuracies of (a) directly induced grammars and (b) adapted grammars as a function of the number of brackets present in the training corpus. There are 1595 brackets in the training corpus all together. for the adaptive strategy, resulting in a grammar that scored better than the baseline. The grammars trained on the AllNP annotation performed as well as the baseline for both strategies. Grammars trained under all the other training conditions scored below the baseline. Our results suggest that while an ideal training condition would include annotations of both higher-level phrases and simple phrases, complex clauses are more informative. This interpretation explains the large gap between the parsing scores of the directly induced grammar and the adapted grammar trained on the same HighP data. The directly induced grammar performed poorly because it has never seen a labeled example of simple phrases. In contrast, the adapted grammar was already exposed to labeled WSJ simple phrases, so that it successfully adapted to the new corpus from annotated examples of higher-level phrases. On the other hand, training the adapted grammar on annotated ATIS simple phrases is not successful even though it has seen examples of WSJ higherlevel phrases. This also explains why grammars trained on the conglomerate class Not- BaseP performed on the same level as those trained on the AllNP class. Although the Not- BaseP set contains the most brackets, most of the brackets are irrelevant to the training process, as they are neither higher-level phrases nor simple phrases. Our experiment also indicates that induction strategies exhibit different learning characteristics under partially supervised training conditions. A side by side comparison of Figure 1 (a) and (b) shows that the adapted grammars perform significantly better than the directly induced grammars as the level of supervision decreases. This supports our hypothesis that pretraining on a different corpus can place the grammar in a good initial search space for learning the new domain. Unfortunately, a good initial state does not obviate the need for supervised training. We see from Figure 1(b) that retraining with unlabeled ATIS sentences actually lowers the grammar&apos;s parsing accuracy. 5.2 Experiment 2: Learning WSJ In the previous section, we have seen that annotations of complex clauses are the most helpful for inducing ATIS-style grammars. One of the goals of this experiment is to verify whether the result also holds for the WSJ corpus, which is structurally very different from ATIS. The WSJ corpus uses 47 POS tags, and its sentences are longer and have more embedded clauses. As in the previous experiment, we construct training sets with annotations of different constituent types and of different numbers of randomly chosen labels. Each training set consists of 3600 sentences, and 1780 sentences are used as held-out data. The trained grammars are tested on a set of 2245 sentences. Figure 2 (a) and (b) summarize the outcomes 77 Figure 2: Parsing accuracies of (a) directly induced grammars and (b) adapted grammars as a function of the number of brackets present in the training corpus. There is a total of 46463 brackets in the training corpus. 5000 10003 15000 20000 25000 30000 35000 40000 45000 number of brackets in Me WSJ twining data (b) 50D0 MOO 15000 20000 MM 30000 35000 40000 45000 bracketsin dlOOM Ma (a)</abstract>
<pubnum confidence="0.277235">mi-----Ronsuro4</pubnum>
<note confidence="0.961425769230769">Noteasse 60 75 70 65 80 50 45 40 Rand-75 Rand-100% Rand-50 Nog. op Rand-25%</note>
<email confidence="0.485713">ighP</email>
<abstract confidence="0.9642945">BariaNFNariee AILS only of this experiment. Many results of this section are similar to the ATIS experiment. Higherlevel phrases still provide the most information; the grammars trained on the HighP labels are the only ones that scored as well as the baseline. Labels of simple phrases still seem the least informative; scores of grammars trained on BaseP and BaseNP remained far below the baseline. Different from the previous experiment, however, the AIINP training sets do not seem to provide as much information for this learning task. This may be due to the increase in the sentence complexity of the WSJ, which further de-emphasized the role of the simple phrases. Thus, grammars trained on AIINP labels have comparable parsing scores to those trained on HighP labels. Also, we do not see as big a gap between the scores of the two induction strategies in the HighP case because the adapted grammar&apos;s advantage of having seen annotated ATIS base nouns is reduced. Nonetheless, the adapted grammars still perform 2% better than the directly induced grammars, and this imis statistically Furthermore, grammars trained on NotBaseP do not fall as far below the baseline and have higher parsing scores than those trained on HighP and AIINP. This suggests that for more complex domains, other linguistic constituents pair-wise t-test comparing the parsing scores of the ten test sets for the two strategies shows 99% confidence in the difference. as verb become more informative. A second goal of this experiment is to test the adaptive strategy under more stringent conditions. In the previous experiment, a WSJ-style grammar was retrained for the simpler ATIS corpus. Now, we reverse the roles of the corpora to see whether the adaptive strategy still offers any advantage over direct induction. In the adaptive method&apos;s pretraining stage, a grammar is induced from 400 fully labeled ATIS sentences. Testing this ATIS-style grammar on the WSJ test set without further training renders a parsing accuracy of 40%. The low score suggests that fully labeled ATIS data does not teach the grammar as much about the structure of WSJ. Nonetheless, the adaptive strategy proves to be beneficial for learning WSJ from sparsely labeled training sets. The adapted grammars out-perform the directly induced grammars when more than 50% of the brackets are missing from the training data. The most significant difference is when the training data contains no label information at The adapted grammar parses with accuracy whereas the directly induced grammar parses with 49.8% accuracy. have not experimented with training sets containing only verb phrases labels (i.e., setting a pair of bracket around the head verb and its modifiers). They are a subset of the NotBaseP class. 78 6 Conclusion and Future Work In this study, we have shown that the structure of a grammar can be reliably learned without having fully specified constituent information in the training sentences and that the most informative constituents of a sentence are higherlevel phrases, which make up only a small percentage of the total number of constituents. Moreover, we observe that grammar adaptation works particularly well with this type of sparse but informative training data. An adapted grammar consistently outperforms a directly induced grammar even when adapting from a simpler corpus to a more complex one. These results point us to three future directions. First, that the labels for some constituents are more informative than others implies that sentences containing more of these informative constituents make better training examples. It may be beneficial to estimate the informational content of potential training (unmarked) sentences. The training set should only include sentences that are predicted to have high information values. Filtering out unhelpful sentences from the training set reduces unnecessary work for the human annotators. Second, although our experiments show that a sparsely labeled training set is more of an obstacle for the direct induction approach than for the grammar adaptation approach, the direct induction strategy might also benefit from a two stage learning process similar to that used for grammar adaptation. Instead of training on a different corpus in each stage, the grammar can be trained on a small but fully labeled portion of the corpus in its first stage and the sparsely labeled portion in the second stage. Finally, higher-level constituents have proved to be the most informative linguistic units. To relieve humans from labeling any training data, we should consider using partial parsers that can automatically detect complex nouns and sentential clauses. References J.K. Baker. 1979. Trainable grammars for recognition. In of the Spring Conference of the Acoustical Society of 547-550, Boston, MA, June. E.J. Briscoe and N. Waegner. 1992. Robust stochastic parsing using the inside-outside al- In of the AAAI Workshop on Probabilistically-Based NLP Tech-</abstract>
<note confidence="0.8960058">39-53. E. Charniak. 1996. Tree-bank grammars. In Proceedings of the Thirteenth National Conon Artificial Intelligence, 1031- 1036. E. Mark Gold. 1967. Language identification the limit. Control, l0(5):447- 474. C.T. Hemphill, J.J. Godfrey, and G.R. Doddington. 1990. The ATIS spoken language</note>
<author confidence="0.439548">In Speech</author>
<affiliation confidence="0.634598">Language Workshop, Valley,</affiliation>
<address confidence="0.915166">Pennsylvania, June. Morgan Kaufmann.</address>
<abstract confidence="0.679412733333333">R. Hwa. 1998a. An empirical evaluation of probabilistic lexicalized tree insertion gram- In of volume 1, pages 557-563. R. Hwa. 1998b. An empirical evaluation of probabilistic lexicalized tree insertion grammars. Technical Report 06-98, Harvard University. Available as cmp-lg/9808001. K. Lan i and S.J. Young. 1990. The estimation of stochastic context-free grammars usthe inside-outside algorithm. and Language, M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Building a large annontated corpus of the penn treebank. :313-330. F. Pereira and Y. Schabes. 1992. Inside- Outside reestimation from partially bracketed In of the 30th Annual of the ACL, 128-135, Newark, Delaware. Y. Schabes and R. Waters. 1993. Stochastic context-free grammar. In Proceedings of the Third International Workshop on Technologies, Y. Schabes, M. Roth, and R. Osborne. 1993. Parsing the Wall Street Journal with the algorithm. In of the Sixth Conference of the European Chapof the ACL,</abstract>
<intro confidence="0.513154">79</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J K Baker</author>
</authors>
<title>Trainable grammars for speech recognition.</title>
<date>1979</date>
<booktitle>In Proceedings of the Spring Conference of the Acoustical Society of America,</booktitle>
<pages>547--550</pages>
<location>Boston, MA,</location>
<contexts>
<context position="6129" citStr="Baker, 1979" startWordPosition="978" endWordPosition="979">f a language by learning from example sentences drawn from the language. The degree of difficulty in this task depends on three factors. First, it depends on the amount of supervision provided. Charniak (1996), for instance, has shown that a grammar can be easily constructed when the examples are fully labeled parse trees. On the other hand, if the examples consist of raw sentences with no extra structural information, grammar induction is very difficult, even theoretically impossible (Gold, 1967). One could take a greedy approach such as the well-known Inside-Outside re-estimation algorithm (Baker, 1979), which induces locally optimal grammars by iteratively improving the parameters of the grammar so that the entropy of the training data is minimized. In practice, however, when trained on unmarked data, the algorithm tends to converge on poor grammar models. For even a moderately complex domain such as the ATIS corpus, a grammar trained on data with constituent bracketing information produces much better parses than one trained on completely unmarked raw data (Pereira and Schabes, 1992). Part of our work explores the in-between case, when only some constituent labels are available. Section 3 </context>
</contexts>
<marker>Baker, 1979</marker>
<rawString>J.K. Baker. 1979. Trainable grammars for speech recognition. In Proceedings of the Spring Conference of the Acoustical Society of America, pages 547-550, Boston, MA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E J Briscoe</author>
<author>N Waegner</author>
</authors>
<title>Robust stochastic parsing using the inside-outside algorithm.</title>
<date>1992</date>
<booktitle>In Proceedings of the AAAI Workshop on Probabilistically-Based NLP Techniques,</booktitle>
<pages>39--53</pages>
<contexts>
<context position="7365" citStr="Briscoe and Waegner (1992)" startWordPosition="1171" endWordPosition="1174">e different types of annotation we examine. Second, as supervision decreases, the learning process relies more on search. The success of the induction depends on the initial parameters of the grammar because a local search strategy may converge to a local minimum. For finding a good initial parameter set, Lan i and Young (1990) suggested first estimating the probabilities with a set of regular grammar rules. Their experiments, however, indicated that the main benefit from this type of pretraining is one of run-time efficiency; the improvement in the quality of the induced grammar was minimal. Briscoe and Waegner (1992) argued that one should first hand-design the grammar to encode some linguistic notions and then use the reestimation procedure to fine-tune the parameters, substituting the cost of hand-labeled training data with that of hand-coded grammar. Our idea of grammar adaptation can be seen as a form of initialization. It attempts to seed the grammar in a favorable search space by first training it with data from an existing corpus. Section 4 discusses the induction strategies in more detail. A third factor that affects the learning process is the complexity of the data. In their study of parsing the</context>
</contexts>
<marker>Briscoe, Waegner, 1992</marker>
<rawString>E.J. Briscoe and N. Waegner. 1992. Robust stochastic parsing using the inside-outside algorithm. In Proceedings of the AAAI Workshop on Probabilistically-Based NLP Techniques, pages 39-53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Tree-bank grammars.</title>
<date>1996</date>
<booktitle>In Proceedings of the Thirteenth National Conference on Artificial Intelligence,</booktitle>
<pages>1031--1036</pages>
<contexts>
<context position="5726" citStr="Charniak (1996)" startWordPosition="916" endWordPosition="917">rvision altogether with a partial parser that locates constituent chunks within a sentence. However, as our results indicate that the most informative constituents are higher-level phrases, the parser would have to identify sentential clauses and complex noun phrases rather than low-level base noun phrases. 2 Related Work on Grammar Induction Grammar induction is the process of inferring the structure of a language by learning from example sentences drawn from the language. The degree of difficulty in this task depends on three factors. First, it depends on the amount of supervision provided. Charniak (1996), for instance, has shown that a grammar can be easily constructed when the examples are fully labeled parse trees. On the other hand, if the examples consist of raw sentences with no extra structural information, grammar induction is very difficult, even theoretically impossible (Gold, 1967). One could take a greedy approach such as the well-known Inside-Outside re-estimation algorithm (Baker, 1979), which induces locally optimal grammars by iteratively improving the parameters of the grammar so that the entropy of the training data is minimized. In practice, however, when trained on unmarked</context>
</contexts>
<marker>Charniak, 1996</marker>
<rawString>E. Charniak. 1996. Tree-bank grammars. In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 1031-1036.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Mark Gold</author>
</authors>
<title>Language identification in the limit.</title>
<date>1967</date>
<journal>Information Control,</journal>
<pages>0--5</pages>
<contexts>
<context position="6019" citStr="Gold, 1967" startWordPosition="963" endWordPosition="964">un phrases. 2 Related Work on Grammar Induction Grammar induction is the process of inferring the structure of a language by learning from example sentences drawn from the language. The degree of difficulty in this task depends on three factors. First, it depends on the amount of supervision provided. Charniak (1996), for instance, has shown that a grammar can be easily constructed when the examples are fully labeled parse trees. On the other hand, if the examples consist of raw sentences with no extra structural information, grammar induction is very difficult, even theoretically impossible (Gold, 1967). One could take a greedy approach such as the well-known Inside-Outside re-estimation algorithm (Baker, 1979), which induces locally optimal grammars by iteratively improving the parameters of the grammar so that the entropy of the training data is minimized. In practice, however, when trained on unmarked data, the algorithm tends to converge on poor grammar models. For even a moderately complex domain such as the ATIS corpus, a grammar trained on data with constituent bracketing information produces much better parses than one trained on completely unmarked raw data (Pereira and Schabes, 199</context>
</contexts>
<marker>Gold, 1967</marker>
<rawString>E. Mark Gold. 1967. Language identification in the limit. Information Control, l0(5):447-474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C T Hemphill</author>
<author>J J Godfrey</author>
<author>G R Doddington</author>
</authors>
<title>The ATIS spoken language systems pilot corpus.</title>
<date>1990</date>
<booktitle>In DARPA Speech and Natural Language Workshop,</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<location>Hidden Valley, Pennsylvania,</location>
<contexts>
<context position="3316" citStr="Hemphill et al., 1990" startWordPosition="528" endWordPosition="531">cess for both induction strategies. For example, how much training data need to be hand-labeled? Must the parse trees for each sentence be fully specified? Are some linguistic constituents in the parse more informative than others? To answer these questions, we have performed experiments that compare the parsing qualities of grammars induced under different training conditions using both adaptation and direct induction. We vary the number of labeled brackets and the linguistic classes of the labeled brackets. The study is conducted on both a simple Air Travel Information System (ATIS) corpus (Hemphill et al., 1990) and the more complex Wall Street Journal (WSJ) corpus (Marcus et al., 1993). Our results show that the training examples do not need to be fully parsed for either strategy, but adaptation produces better grammars than direct induction under the conditions of minimally labeled training data. For instance, the most informative brackets, which label constituents higher up in the parse trees, typically 73 identifying complex noun phrases and sentential clauses, account for only 17% of all constituents in ATIS and 21% in WSJ. Trained on this type of label, the adapted grammars parse better than th</context>
</contexts>
<marker>Hemphill, Godfrey, Doddington, 1990</marker>
<rawString>C.T. Hemphill, J.J. Godfrey, and G.R. Doddington. 1990. The ATIS spoken language systems pilot corpus. In DARPA Speech and Natural Language Workshop, Hidden Valley, Pennsylvania, June. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Hwa</author>
</authors>
<title>An empirical evaluation of probabilistic lexicalized tree insertion grammars.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<volume>1</volume>
<pages>557--563</pages>
<contexts>
<context position="11438" citStr="Hwa, 1998" startWordPosition="1875" endWordPosition="1876"> a sentential clause or a complex noun phrase. The example sentence in Table 1 contains 3 HighP constituents: a complex noun phrase made up of a BaseNP and a prepositional phrase; a sentential clause with an omitted subject NP; and the full sentence. 4 Induction Strategies To induce a grammar from the sparsely bracketed training data previously described, we use a variant of the Inside-Outside re-estimation algorithm proposed by Pereira and Schabes (1992). The inferred grammars are represented in the Probabilistic Lexicalized Tree Insertion Grammar (PLTIG) formalism (Schabes and Waters, 1993; Hwa, 1998a), which is lexicalized and context-free equivalent. We favor the PLTIG representation for two reasons. First, it is amenable to the Inside-Outside re-estimation algorithm (the equations calculating the inside and outside probabilities for PLTIGs can be found in Hwa (1998b)). Second, its lexicalized representation makes the training process more efficient than a traditional PCFG while maintaining comparable parsing qualities. Two training strategies are considered: direct induction, in which a grammar is induced from scratch, learning from only the sparsely labeled training data; and adaptati</context>
</contexts>
<marker>Hwa, 1998</marker>
<rawString>R. Hwa. 1998a. An empirical evaluation of probabilistic lexicalized tree insertion grammars. In Proceedings of COLING-ACL, volume 1, pages 557-563.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Hwa</author>
</authors>
<title>An empirical evaluation of probabilistic lexicalized tree insertion grammars.</title>
<date>1998</date>
<tech>Technical Report 06-98,</tech>
<institution>Harvard University.</institution>
<note>Available as cmp-lg/9808001.</note>
<contexts>
<context position="11438" citStr="Hwa, 1998" startWordPosition="1875" endWordPosition="1876"> a sentential clause or a complex noun phrase. The example sentence in Table 1 contains 3 HighP constituents: a complex noun phrase made up of a BaseNP and a prepositional phrase; a sentential clause with an omitted subject NP; and the full sentence. 4 Induction Strategies To induce a grammar from the sparsely bracketed training data previously described, we use a variant of the Inside-Outside re-estimation algorithm proposed by Pereira and Schabes (1992). The inferred grammars are represented in the Probabilistic Lexicalized Tree Insertion Grammar (PLTIG) formalism (Schabes and Waters, 1993; Hwa, 1998a), which is lexicalized and context-free equivalent. We favor the PLTIG representation for two reasons. First, it is amenable to the Inside-Outside re-estimation algorithm (the equations calculating the inside and outside probabilities for PLTIGs can be found in Hwa (1998b)). Second, its lexicalized representation makes the training process more efficient than a traditional PCFG while maintaining comparable parsing qualities. Two training strategies are considered: direct induction, in which a grammar is induced from scratch, learning from only the sparsely labeled training data; and adaptati</context>
</contexts>
<marker>Hwa, 1998</marker>
<rawString>R. Hwa. 1998b. An empirical evaluation of probabilistic lexicalized tree insertion grammars. Technical Report 06-98, Harvard University. Available as cmp-lg/9808001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lan i</author>
<author>S J Young</author>
</authors>
<title>The estimation of stochastic context-free grammars using the inside-outside algorithm.</title>
<date>1990</date>
<journal>Computer Speech and Language,</journal>
<pages>4--35</pages>
<contexts>
<context position="7068" citStr="i and Young (1990)" startWordPosition="1125" endWordPosition="1128"> a grammar trained on data with constituent bracketing information produces much better parses than one trained on completely unmarked raw data (Pereira and Schabes, 1992). Part of our work explores the in-between case, when only some constituent labels are available. Section 3 defines the different types of annotation we examine. Second, as supervision decreases, the learning process relies more on search. The success of the induction depends on the initial parameters of the grammar because a local search strategy may converge to a local minimum. For finding a good initial parameter set, Lan i and Young (1990) suggested first estimating the probabilities with a set of regular grammar rules. Their experiments, however, indicated that the main benefit from this type of pretraining is one of run-time efficiency; the improvement in the quality of the induced grammar was minimal. Briscoe and Waegner (1992) argued that one should first hand-design the grammar to encode some linguistic notions and then use the reestimation procedure to fine-tune the parameters, substituting the cost of hand-labeled training data with that of hand-coded grammar. Our idea of grammar adaptation can be seen as a form of initi</context>
</contexts>
<marker>i, Young, 1990</marker>
<rawString>K. Lan i and S.J. Young. 1990. The estimation of stochastic context-free grammars using the inside-outside algorithm. Computer Speech and Language, 4:35-56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a large annontated corpus of english: the penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>313--330</pages>
<contexts>
<context position="3392" citStr="Marcus et al., 1993" startWordPosition="542" endWordPosition="545">o be hand-labeled? Must the parse trees for each sentence be fully specified? Are some linguistic constituents in the parse more informative than others? To answer these questions, we have performed experiments that compare the parsing qualities of grammars induced under different training conditions using both adaptation and direct induction. We vary the number of labeled brackets and the linguistic classes of the labeled brackets. The study is conducted on both a simple Air Travel Information System (ATIS) corpus (Hemphill et al., 1990) and the more complex Wall Street Journal (WSJ) corpus (Marcus et al., 1993). Our results show that the training examples do not need to be fully parsed for either strategy, but adaptation produces better grammars than direct induction under the conditions of minimally labeled training data. For instance, the most informative brackets, which label constituents higher up in the parse trees, typically 73 identifying complex noun phrases and sentential clauses, account for only 17% of all constituents in ATIS and 21% in WSJ. Trained on this type of label, the adapted grammars parse better than the directly induced grammars and almost as well as those trained on fully lab</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Building a large annontated corpus of english: the penn treebank. Computational Linguistics, 19(2) :313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
<author>Y Schabes</author>
</authors>
<title>InsideOutside reestimation from partially bracketed corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 30th Annual Meeting of the ACL,</booktitle>
<pages>128--135</pages>
<location>Newark, Delaware.</location>
<contexts>
<context position="6621" citStr="Pereira and Schabes, 1992" startWordPosition="1052" endWordPosition="1055"> impossible (Gold, 1967). One could take a greedy approach such as the well-known Inside-Outside re-estimation algorithm (Baker, 1979), which induces locally optimal grammars by iteratively improving the parameters of the grammar so that the entropy of the training data is minimized. In practice, however, when trained on unmarked data, the algorithm tends to converge on poor grammar models. For even a moderately complex domain such as the ATIS corpus, a grammar trained on data with constituent bracketing information produces much better parses than one trained on completely unmarked raw data (Pereira and Schabes, 1992). Part of our work explores the in-between case, when only some constituent labels are available. Section 3 defines the different types of annotation we examine. Second, as supervision decreases, the learning process relies more on search. The success of the induction depends on the initial parameters of the grammar because a local search strategy may converge to a local minimum. For finding a good initial parameter set, Lan i and Young (1990) suggested first estimating the probabilities with a set of regular grammar rules. Their experiments, however, indicated that the main benefit from this </context>
<context position="11288" citStr="Pereira and Schabes (1992)" startWordPosition="1851" endWordPosition="1854"> parses. pose into sub-phrases that may be either another HighP or a BaseP. That is, a HighP constituent does not directly subsume any lexical word. A typical HighP is a sentential clause or a complex noun phrase. The example sentence in Table 1 contains 3 HighP constituents: a complex noun phrase made up of a BaseNP and a prepositional phrase; a sentential clause with an omitted subject NP; and the full sentence. 4 Induction Strategies To induce a grammar from the sparsely bracketed training data previously described, we use a variant of the Inside-Outside re-estimation algorithm proposed by Pereira and Schabes (1992). The inferred grammars are represented in the Probabilistic Lexicalized Tree Insertion Grammar (PLTIG) formalism (Schabes and Waters, 1993; Hwa, 1998a), which is lexicalized and context-free equivalent. We favor the PLTIG representation for two reasons. First, it is amenable to the Inside-Outside re-estimation algorithm (the equations calculating the inside and outside probabilities for PLTIGs can be found in Hwa (1998b)). Second, its lexicalized representation makes the training process more efficient than a traditional PCFG while maintaining comparable parsing qualities. Two training strate</context>
</contexts>
<marker>Pereira, Schabes, 1992</marker>
<rawString>F. Pereira and Y. Schabes. 1992. InsideOutside reestimation from partially bracketed corpora. In Proceedings of the 30th Annual Meeting of the ACL, pages 128-135, Newark, Delaware.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Schabes</author>
<author>R Waters</author>
</authors>
<title>Stochastic lexicalized context-free grammar.</title>
<date>1993</date>
<booktitle>In Proceedings of the Third International Workshop on Parsing Technologies,</booktitle>
<pages>257--266</pages>
<contexts>
<context position="11427" citStr="Schabes and Waters, 1993" startWordPosition="1871" endWordPosition="1874">l word. A typical HighP is a sentential clause or a complex noun phrase. The example sentence in Table 1 contains 3 HighP constituents: a complex noun phrase made up of a BaseNP and a prepositional phrase; a sentential clause with an omitted subject NP; and the full sentence. 4 Induction Strategies To induce a grammar from the sparsely bracketed training data previously described, we use a variant of the Inside-Outside re-estimation algorithm proposed by Pereira and Schabes (1992). The inferred grammars are represented in the Probabilistic Lexicalized Tree Insertion Grammar (PLTIG) formalism (Schabes and Waters, 1993; Hwa, 1998a), which is lexicalized and context-free equivalent. We favor the PLTIG representation for two reasons. First, it is amenable to the Inside-Outside re-estimation algorithm (the equations calculating the inside and outside probabilities for PLTIGs can be found in Hwa (1998b)). Second, its lexicalized representation makes the training process more efficient than a traditional PCFG while maintaining comparable parsing qualities. Two training strategies are considered: direct induction, in which a grammar is induced from scratch, learning from only the sparsely labeled training data; a</context>
</contexts>
<marker>Schabes, Waters, 1993</marker>
<rawString>Y. Schabes and R. Waters. 1993. Stochastic lexicalized context-free grammar. In Proceedings of the Third International Workshop on Parsing Technologies, pages 257-266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Schabes</author>
<author>M Roth</author>
<author>R Osborne</author>
</authors>
<title>Parsing the Wall Street Journal with the Inside-Outside algorithm.</title>
<date>1993</date>
<booktitle>In Proceedings of the Sixth Conference of the European Chapter of the ACL,</booktitle>
<pages>341--347</pages>
<contexts>
<context position="7992" citStr="Schabes et al. (1993)" startWordPosition="1279" endWordPosition="1282">d that one should first hand-design the grammar to encode some linguistic notions and then use the reestimation procedure to fine-tune the parameters, substituting the cost of hand-labeled training data with that of hand-coded grammar. Our idea of grammar adaptation can be seen as a form of initialization. It attempts to seed the grammar in a favorable search space by first training it with data from an existing corpus. Section 4 discusses the induction strategies in more detail. A third factor that affects the learning process is the complexity of the data. In their study of parsing the WSJ, Schabes et al. (1993) have shown that a grammar trained on the InsideOutside re-estimation algorithm can perform quite well on short simple sentences but falters as the sentence length increases. To take this factor into account, we perform our experiments 74 Categories Labeled Sentence ATIS WSJ HighP (I want (to take (the flight with at most one stop))) 17% 21% BaseNP (I) want to take (the flight) with (at most one stop) 27% 29% BaseP (I) want to take (the flight) with (at most one) stop 32% 30% AllNP (I) want to take ((the flight) with (at most one stop)) 37% 43% NotBaseP (I (want (to (take (the flight (with (at</context>
</contexts>
<marker>Schabes, Roth, Osborne, 1993</marker>
<rawString>Y. Schabes, M. Roth, and R. Osborne. 1993. Parsing the Wall Street Journal with the Inside-Outside algorithm. In Proceedings of the Sixth Conference of the European Chapter of the ACL, pages 341-347.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>