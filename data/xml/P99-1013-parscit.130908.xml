<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.989135">
Compositional Semantics for Linguistic Formalisms
</title>
<author confidence="0.99809">
Shuly Wintner*
</author>
<affiliation confidence="0.998391">
Institute for Research in Cognitive Science
University of Pennsylvania
</affiliation>
<address confidence="0.7109625">
3401 Walnut St., Suite 400A
Philadelphia, PA 19018
</address>
<email confidence="0.926886">
shulyOlinc.cis.upenn.edu
</email>
<sectionHeader confidence="0.99329" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998597">
In what sense is a grammar the union of its
rules? This paper adapts the notion of com-
position, well developed in the context of pro-
gramming languages, to the domain of linguis-
tic formalisms. We study alternative definitions
for the semantics of such formalisms, suggest-
ing a denotational semantics that we show to
be compositional and fully-abstract. This fa-
cilitates a clear, mathematically sound way for
defining grammar modularity.
</bodyText>
<sectionHeader confidence="0.997891" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.994105419354839">
Developing large scale grammars for natural
languages is a complicated task, and the prob-
lems grammar engineers face when designing
broad-coverage grammars are reminiscent of
those tackled by software engineering (Erbach
and Uszkoreit, 1990). Viewing contemporary
linguistic formalisms as very high level declara-
tive programming languages, a grammar for a
natural language can be viewed as a program.
It is therefore possible to adapt methods and
techniques of software engineering to the do-
main of natural language formalisms. We be-
lieve that any advances in grammar engineering
must be preceded by a more theoretical work,
concentrating on the semantics of grammars.
This view reflects the situation in logic program-
ming, where developments in alternative defini-
tions for predicate logic semantics led to im-
plementations of various program composition
operators (Bugliesi et al., 1994).
This paper suggests a denotational seman-
tics for unification-based linguistic formalisms
and shows that it is compositional and fully-
*I am grateful to Nissim Francez for commenting on an
earlier version of this paper. This work was supported
by an IR.CS Fellowship and NSF grant SBR 8920230.
abstract. This facilitates a clear, mathemati-
cally sound way for defining grammar modu-
larity. While most of the results we report on
are probably not surprising, we believe that it
is important to derive them directly for linguis-
tic formalisms for two reasons. First, practi-
tioners of linguistic formalisms usually do not
view them as instances of a general logic pro-
gramming framework, but rather as first-class
programming environments which deserve in-
dependent study. Second, there are some cru-
cial differences between contemporary linguis-
tic formalisms and, say, Prolog: the basic ele-
ments — typed feature-structures — are more
general than first-order terms, the notion of uni-
fication is different, and computations amount
to parsing, rather than SLD-resolution. The
fact that we can derive similar results in this
new domain is encouraging, and should not be
considered trivial.
Analogously to logic programming languages,
the denotation of grammars can be defined us-
ing various techniques. We review alternative
approaches, operational and denotational, to
the semantics of linguistic formalisms in sec-
tion 2 and show that they are &amp;quot;too crude&amp;quot;
to support grammar composition. Section 3
presents an alternative semantics, shown to be
compositional (with respect to grammar union,
a simple syntactic combination operation on
grammars) However, this definition is &amp;quot;too
fine&amp;quot;: in section 4 we present an adequate,
compositional and fully-abstract semantics for
linguistic formalisms. For lack of space, some
proofs are omitted; an extended version is avail-
able as a technical report (Wintner, 1999).
</bodyText>
<sectionHeader confidence="0.943577" genericHeader="method">
2 Grammar semantics
</sectionHeader>
<bodyText confidence="0.9871425">
Viewing grammars as formal entities that share
many features with computer programs, it is
</bodyText>
<page confidence="0.98466">
96
</page>
<bodyText confidence="0.999756375">
natural to consider the notion of semantics of
unification-based formalisms. We review in this
section the operational definition of Shieber et
al. (1995) and the denotational definition of,
e.g., Pereira and Shieber (1984) or Carpenter
(1992, pp. 204-206). We show that these def-
initions are equivalent and that none of them
supports conipositionality.
</bodyText>
<subsectionHeader confidence="0.994968">
2.1 Basic notions
</subsectionHeader>
<bodyText confidence="0.999844956521739">
We assume familiarity with theories of feature
structure based unification grammars, as for-
mulated by, e.g., Carpenter (1992) or Shieber
(1992). Grammars are defined over typed fea-
ture structures (TFSs) which can be viewed as
generalizations of first-order terms (Carpenter,
1991). TFSs are partially ordered by subsump-
tion, with I the least (or most general) TFS. A
multi-rooted structure (MRS, see Sikkel (1997)
or Wintner and Francez (1999)) is a sequence
of TFSs, with possible reentrancies among dif-
ferent elements in the sequence. Meta-variables
A, B range over TFSs and a, p — over MRSs.
MRSs are partially ordered by subsumption, de-
noted &apos;C&apos;, with a least upper bound operation
of unification, denoted &apos;U&apos;, and a greatest lowest
bound denoted &apos;17&apos;. We assume the existence of
a fixed, finite set WORDS of words. A lexicon
associates with every word a set of TFSs, its cat-
egory. Meta-variable a ranges over WORDS and
-- over strings of words (elements of WORDS*).
Grammars are defined over a signature of types
and features, assumed to be fixed below.
</bodyText>
<construct confidence="0.7927467">
Definition 1. A rule is an MRS of length
greater than or equal to 1 with a designated
(first) element, the head of the rule. The rest of
the elements form the rule&apos;s body (which may
be empty, in which case the rule is depicted
(is a TFS). A lexicon is a total function from
WORDS to finite, possibly empty sets of TFSs.
A grammar G = (77., As) is a ,finite set of
rules 7Z, a lexicon G and a start symbol As
that is a TFS.
</construct>
<bodyText confidence="0.730509125">
Figure 1 depicts an example grammar,&apos; sup-
pressing the underlying type hierarchy.2
The definition of unification is lifted to MRSs:
let a, p be two MRSs of the same length; the
!Grammars are displayed using a simple description
language, where `:&apos; denotes feature values.
2Assmne that in all the example grammars, the types
s, n, v and vp are maximal and (pairwise) inconsistent.
</bodyText>
<equation confidence="0.986839">
As = (cat : s)
(cat : s)
R, = { (cat : vp)
(cat : vp)
.C(John) = L(Mary) = { (cat : n)}
G(sleeps) = L(sleep) = G(loves) = {(cat : v)}
</equation>
<figureCaption confidence="0.99788">
Figure 1: An example grammar, G
</figureCaption>
<bodyText confidence="0.59022675">
unification of a and p, denoted a U p, is the
most general MR.S that is subsumed by both a
and p, if it exists. Otherwise, the unification
fails.
</bodyText>
<construct confidence="0.6908535">
Definition 2. An MRS (it , Ak) reduces
to a TFS A with respect to a grammar G (de-
noted (ith , Ak) =-(.-; A) di there exists a
rule p E 7?, such that (B, B1, . Bk) = p
</construct>
<bodyText confidence="0.996100516129032">
(I, , Ak) and B C A. When G is under-
stood from the context it is omitted. Reduction
can be viewed as the bottom-up counterpart of
derivation.
If f, g, are functions over the same (set) do-
main, f + g is AI. f (I) U g(I). Let ITEMS =
{[w,i, A, j] I w E WORDS*, A is a TFS and
j E {0,1,2, 3, ...}}. Let I = 2ITEms• Meta-
variables x, y range over items and I — over sets
of items. When I is ordered by set inclusion
it forms a complete lattice with set union as a
least upper bound (lub) operation. A function
monotone if whenever /1 C /2, also
T(/,) C T(I2). It is continuous if for every chain
/3: C /2 C • • •, /i) = 1.jj&lt;„, T(/i). If a
function T is monotone it has a least fixpoint
(Tarski-Knaster theorem); if T is also continu-
ous, the fixpoint can be obtained by iterative
application of T to the empty set (Kleene the-
orem): llp(T) = T t co, where T t 0 = 0 and
T t n = T(T t (n — 1)) when n is a succes-
sor ordinal and Uk&lt;n(T t n) when n is a limit
ordinal.
When the semantics of programming lan-
guages are concerned, a notion of observables
is called for: Oh is a function associating a set
of objects, the observables, with every program.
The choice of semantics induces a natural equiv-
alence operator on grammars: given a semantics
G, G2 if 11Gd = 11G21 An essential re-
quirement of any semantic equivalence is that it
</bodyText>
<equation confidence="0.998698333333333">
(cat : n) (cat : vp)
—&gt; (cat : v) (cat : mi)
(cat : v)
</equation>
<page confidence="0.880915">
97
</page>
<bodyText confidence="0.885604222222222">
be correct (observables-preserving): if G1 G21
then Ob(G1) -= Ob(G2).
Let `U&apos; be a composition operation on gram-
mars and &apos;•&apos; a combination operator on deno-
tations. A (correct) semantics &apos;HI&apos; is compo-
sitional (Gaifinan and Shapiro, 1989) if when-
ever [Gi]] = [[G2]] and [[G3]] = [[G4]], also
[{6.11 U G31 = 1G2 U G41 A semantics is com-
mutative (Brogi et al., 1992) if TG1 U G2]]
</bodyText>
<listItem confidence="0.994945">
• [[G21 This is a stronger notion than
compositionality: if a semantics is commutative
with respect to some operator then it is compo-
sitional.
</listItem>
<subsectionHeader confidence="0.99976">
2.2 An operational semantics
</subsectionHeader>
<bodyText confidence="0.928216722222222">
As Van Emden and Kowalski (1976) note, &amp;quot;to
define an operational semantics for a program-
ming language is to define an implementational
independent interpreter for it. For predicate
logic the proof procedure behaves as such an in-
terpreter.&amp;quot; Shieber et al. (1995) view parsing as
a deductive process that proves claims about the
grammatical status of strings from assumptions
derived from the grammar. We follow their in-
sight and notation and list a deductive system
for parsing unification-based grammars.
Definition 3. The deductive parsing system
associated with a grammar G = (12.,L,A8) is
defined over ITEMS and is characterized by:
Axioms: [a, i, A, i + 1] if BE L(a) and B E A;
[E, i, A, i] if B is an c-rule in I?, and BE A
Goals: [tv, 0, A iw 1] where A E As
Inference rules:
</bodyText>
<equation confidence="0.9763725">
[11)1 7 ill Al&gt; il], • • • 7 [Wk 1 47 Ak7ilci
[W1&apos; • • WIcli)
</equation>
<bodyText confidence="0.952662193548387">
if &apos;Ji = i1+1 for 1 &lt; 1 &lt; k and i = i1 and
jk and (Ai, , Ak) G A
When an item [w,i,A,j] can be deduced,
applying k times the inference rules associ-
ated with a grammar G, we write 1--/[w, i, A, j].
When the number of inference steps is irrele-
vant it is omitted. Notice that the domain of
items is infinite, and in particular that the num-
ber of axioms is infinite. Also, notice that the
goal is to deduce a TFS which is subsumed by
the start symbol, and when TFSs can be cyclic,
there can be infinitely many such TFSs (and,
hence, goals) - see Wintner and Francez (1999).
Definition 4. The operational denotation
of a grammar G is nrjop = {x 1HG :1;}. Cl Op
G2 if 1101.110p-= ffG2S0p •
We use the operational semantics to de-
fine the language generated by a grammar G:
L(G) = {(w, A) I [w, 0, A, 17n1] E[[G]]Op 1. Notice
that a language is not merely a set of strings;
rather, each string is associated with a TFS
through the deduction procedure. Note also
that the start symbol A does not play a role
in this definition; this is equivalent to assuming
that the start symbol is always the most general
TFS, I.
The most natural observable for a grammar
would be its language, either as a set of strings
or augmented by TFSs. Thus we take Ob(G)
to be L(G) and by definition, the operational
semantics 1.1op &apos; preserves observables.
</bodyText>
<subsectionHeader confidence="0.998512">
2.3 Denotational semantics
</subsectionHeader>
<bodyText confidence="0.908207470588235">
In this section we consider denotational seman-
tics through a fixpoint of a transformational op-
erator associated with grammars. -This is es-
sentially similar to the definition of Pereira and
Shieber (1984) and Carpenter (1992, pp. 204-
206). We then show that the denotational se-
mantics is equivalent to the operational one.
Associate with a grammar G an operator
TG that, analogously to the immediate conse-
quence operator of logic programming, can be
thought of as a &amp;quot;parsing step&amp;quot; operator in the
context of &apos;grammatical formalisms. For the
following discussion fix a particular grammar
G = (TZ, G, As).
Definition 5. Let TG : I —&gt; I be a trans-
formation on sets of items, where for every
I c ITEMS, [W, i, A, j] E TG(I) dr either
</bodyText>
<listItem confidence="0.98616175">
• there exist Yi, ,Yk E / such that yi
ji] for 1 5. 1 &lt; k; and i1+1 ---=
for 1 &lt; I &lt; A; and i1 = 1 and jk =j and
, Ak) A and w = wi • • • wk; or
• i = j and B is an c-rule in G and B g_ A
and w e; or
• i + 1 = j and w = 1 and B E L(w) and
BEA.
</listItem>
<bodyText confidence="0.998159333333333">
For every grammar G, Tr; is monotone and
continuous, and hence its least fixpoint exists
and lfp(TG) = TG t (4). Following the paradigm
</bodyText>
<page confidence="0.995027">
98
</page>
<bodyText confidence="0.9999555">
of logic programming languages, define a fix-
point semantics for unification-based grammars
by taking the least fixpoint of the parsing step
operator as the denotation of a grammar.
</bodyText>
<equation confidence="0.718204333333333">
Definition 6. The fixpoint denotation of a
grammar G is §G. 11 fp = Up(TG)• C1 mr7, G2 if
f&apos;1)(Te1) = fP(Tc2)-
</equation>
<bodyText confidence="0.8117024">
The denotational definition is equivalent to
the operational one:
Theorem 1. For z E ITEMS, X E Up(TG) if
The proof is that [w, i, A, j] E TG -t n if
I-&apos;(!=;[w, i, A, :1], by induction on n.
</bodyText>
<figureCaption confidence="0.417719">
Corollary 2. The relation is correct:
</figureCaption>
<bodyText confidence="0.815424">
whenever G1 fp G2, also Ob(G1) = Ob(G2)•
</bodyText>
<subsectionHeader confidence="0.941696">
2.4 Compositionality
</subsectionHeader>
<bodyText confidence="0.750675590909091">
While the operational and the denotational se-
mantics defined above are standard for com-
plete grammars, they are too coarse to serve
as a model when the composition of grammars
is concerned. When the denotation of a gram-
mar is taken to be PI] Op) important character-
istics of the internal structure of the grammar
are lost. To demonstrate the problem, we intro-
duce a natural composition operator on gram-
mars, namely union of the sets of rules (and the
lexicons) in the composed grammars.
Definition 7. If G1 = Gi, Al) and G2 =
(7/2, ,C2, AD are two grammars over the same
signature, then the union of the two gram-
mars, denoted G1 U G2, is a new grammar G =
(R., G, As) such that R =- Ri U R2, L = Li -C2
and As = A ri A.
Figure 2 exemplifies grammar union. Observe
that for every G, G&apos;, G U = U G.
Proposition 3. The equivalence relation
is n,ot compositional with respect to Ob, {u}.
Proof. Consider the grammars in figure 2.
</bodyText>
<equation confidence="0.8649116">
[[G3]] op = 11G41107, {{&amp;quot;loves&amp;quot;, i, (cat : v), i + 1]
i &gt; Of but for I = {[&amp;quot;John loves John&amp;quot;, i, (cat :
s), + 3 &gt; O}, I C [[G1 U G4I1 op whereas
/ [[G1 U G310p. Thus G3 1=-.- Op G4 but
U G3) 07.) (G1 U G4), hence `z=_-op&apos; is not
</equation>
<footnote confidence="0.426408">
compositional with respect to Ob, {U}.
</footnote>
<figureCaption confidence="0.99662">
Figure 2: Grammar union
</figureCaption>
<bodyText confidence="0.999946428571429">
The implication of the above proposition is that
while grammar union might be a natural, well
defined syntactic operation on grammars, the
standard semantics of grammars is too coarse to
support it. Intuitively, this is because when a
grammar G1 includes a particular rule p that is
inapplicable for reduction, this rule contributes
nothing to the denotation of the grammar. But
when G1 is combined with some other grammar,
G2, p might be used for reduction in G1 U G2)
where it can interact with the rules of G2. We
suggest an alternative, fixpoint based semantics
for unification based grammars that naturally
supports compositionality.
</bodyText>
<sectionHeader confidence="0.894513" genericHeader="method">
3 A compositional semantics
</sectionHeader>
<bodyText confidence="0.99993425">
To overcome the problems delineated above, we
follow Mancarella and Pedreschi (1988) in con-
sidering the grammar transformation operator
itself (rather than its fixpoint) as the denota-
</bodyText>
<equation confidence="0.803918647058824">
G1: As = (cat :
(cat :s)
G(John) -=-
C2: As = (_L)
(cat: vp)
(cat : vp)
L(sleeps) =
G3: As = (_L)
,C (loves) =
C4: A&apos; = (1)
(cat : vp)
(loves) =
Ci U C2: As = (cat:
(cat : s)
(cat : vp)
(cat : vp)
(John) =
(sleeps) =
Gi Li G3 : A&apos; =- (cat :
(cat : s)
ohn) =
G(loves) =
Gi U G4 : A&apos; = (cat :
(cat : s)
(cat : vp)
L(John)
L(loves) =
s)
(cat : ii) (cat : vp)
{ (cat :
-4 (cat : v)
-4 (cat : v) (cat :
G(loves) = {(cat : v)}
{ (cat : v)}
- (cat : v) (cat : n)
{ (cat : v)}
s)
- (cat : (cat : vp)
—* (cat : v)
- (cat : v) (cat : n)
{(cat : n)}
L(loves) = {(cat : v)}
s)
—&gt; (cat : n) (cat : vp)
{ (cat : 70}
{(cat : v)}
s)
- (cat : i) (cat : vp)
-4 (cat : v) (cat : n)
{(cat : n)}
{(cat : v)}
</equation>
<page confidence="0.868011">
99
</page>
<bodyText confidence="0.732673">
tion of a grammar.
</bodyText>
<equation confidence="0.605996">
Definition 8. The algebraic denotation of
G is 11G1ai = TG. G1 -=-=at G2 if TG1= TG2 •
</equation>
<bodyText confidence="0.975361714285714">
Not only is the algebraic semantics composi-
tional, it is also commutative with respect to
grammar union. To show that, a composition
operation on denotations has to be defined, and
we follow Mancarella and Pedreschi (1988) in
its definition:
TG, • TG2 = ALTGi (I) U TG2(I)
</bodyText>
<construct confidence="0.284141333333333">
Theorem 4. The semantics is commuta-
tive with respect to grammar union and `•&apos;: for
every two grammars G1, C27 1101Sa/ • tG2S ci =
U G2Ly •
Proof. It has to be shown that for every set of
items I, TGiuG,(I) TGi(I) TG2(I) •
</construct>
<listItem confidence="0.99917">
• if :I; E TG,(I) U T2(I) then either x E
TG1(I) or x E TG2(/). From the definition
of grammar union, x E TG1uG2(/) in any
case.
• if
</listItem>
<bodyText confidence="0.971326771428571">
:1; E M1uG2(/) then x can be added by
either of the three clauses in the definition
of TG.
if x is added by the first clause then
there is a rule p E 7i U 7Z2 that li-
censes the derivation through which
x is added. Then either p E Ri or
p E 7Z2, but in any case p would have
licensed the same derivation, so either
x E TG,(I) or x E TG2(/).
if x is added by the second clause then
there is an c-rule in G1 U G2 due to
which x is added, and by the same
rationale either x E TG, (I) or x E
TG,,(I).
if x is added by the third clause then
there exists a lexical category in Li U
.G2 due to which x is added, hence this
category exists in either Li or £2, and
therefore x E TGi (/) U TG2(/).
Since is commutative, it is also compo-
sitional with respect to grammar union. In-
tuitively, since TG captures only one step of
the computation, it cannot capture interactions
among different rules in the (unioned) grammar,
and hence taking TG to he the denotation of G
yields a compositional semantics.
The TG operator reflects the structure of the
grammar better than its fixpoint. In other
words, the equivalence relation induced by TG is
finer than the relation induced by Up(TG). The
question is, how fine is the relation? To
make sure that a semantics is not too fine, one
usually checks the reverse direction.
Definition 9. A fully-abstract equivalence
</bodyText>
<equation confidence="0.980551583333333">
relation is such that Ci -1=2 G2 dr for all G,
Ob(G U G) = Ob(G2 U G).
Proposition 5. The semantic equivalence re-
lation `.=0,1&apos; is not fully abstract.
Proof. Let Ci be the grammar
A=±,
= 0,
R.1 = {(cat : s) —&gt; (cat : np) (cat : vp),
(cat: np) (cat : np)}
and G2 be the grammar
£2 0,
TZ2 {(cat : s) - (cat : np) (cat : vp)}
</equation>
<bodyText confidence="0.5318425">
-at 1
- 7
</bodyText>
<listItem confidence="0.947569">
• 0 G2: because for I = {[&amp;quot;John loves
Mary&amp;quot; ,6, (cat : np) , 9]} , T1(I) = I but
TG,(/) = 0
• for all G, Ob(G U = Ob(G U G2). The
</listItem>
<bodyText confidence="0.996182133333333">
only difference between GUGi and GUG2 is
the presence of the rule (cat : np) (cat :
np) in the former. This rule can contribute
nothing to a deduction procedure, since any
item it licenses must already be deducible.
Therefore, any item deducible with G U
is also deducible with G U G2 and hence
Ob(G U G1) = Ob(G U G2).
A better attempt would have been to con-
sider, instead of TG, the following operator as
the denotation of G: Cid = ALTG(I) U I. In
other words, the semantics is TG + Id, where
Id is the identity operator. Unfortunately, this
does not solve the problem, as &apos;Hid&apos; is still not
fully-abstract.
</bodyText>
<page confidence="0.989767">
100
</page>
<sectionHeader confidence="0.93469" genericHeader="method">
4 A fully abstract semantics
</sectionHeader>
<bodyText confidence="0.999525631578947">
We have shown so far that 11 &apos; is not corn-
positional, and that Hjd is compositional but
not fully abstract. The &amp;quot;right&amp;quot; semantics, there-
fore, lies somewhere in between: since the choice
of semantics induces a natural equivalence on
grammars, we seek an equivalence that is cruder
than &apos;Hid&apos; but finer than lib&apos;. In this section
we adapt results from Lassez and Maher (1984)
and Maher (1988) to the domain of unification-
based linguistic formalisms.
Consider the following semantics for logic
programs: rather than taking the operator asso-
ciated with the entire program, look only at the
rules (excluding the facts), and take the mean-
ing of a program to be the function that is ob-
tained by an infinite applications of the opera-
tor associated with the rules. In our framework,
this would amount to associating the following
operator with a grammar:
</bodyText>
<construct confidence="0.9512706">
Definition 10. Let RG : I I be a trans-
formation on sets of items, where for every
C ITEMS, [w,i, A, j] E RG(i) if there exist
Ill, ,yk E I such that yi = At, ji] for
1 &lt;1 &lt; k and i1+1 = j ,for 1 &lt;1&lt; k and
</construct>
<bodyText confidence="0.772792">
= 1 and jk j and (A1, , Ak) A and
</bodyText>
<equation confidence="0.53844">
&apos;011 • • • TV k.
</equation>
<bodyText confidence="0.988734764705882">
The functional denotation of a grammar G is
= (RG+ = Enc°_0(RG + Id)Th. Notice
that R* is not RG t co: the former is a function
from sets of items to set of items; the latter is
a set of items.
Observe that RG is defined similarly to TG
(definition 5), ignoring the items added (by TG)
due to c-rules and lexical items. If we define the
set of items InitG to be those items that are
added by TG independently of the argument it
operates on, then for every grammar G and ev-
ery set of items I, TG(I) = RG(I) U Init. Re-
lating the functional semantics to the fixpoint
one, we follow Lassez and Maher (1984) in prov-
ing that the fixpoint of the grammar transfor-
mation operator can be computed by applying
the functional semantics to the set InitG.
</bodyText>
<construct confidence="0.98565625">
Definition 11. For G = (R.,., AS), InitG
I B is an c-rule in G and B U
{{a, i, A, i +1] I B E G(a) for B E A}
Theorem 6. For every grammar G,
</construct>
<equation confidence="0.94625125">
(RG + Id) (Init(;) = lfp(TG)
Proof. We show that for every n, (TG + Id) t
71 (E Z 01 (RC ± I d)k)(1-ilitO) by induction on
n.
</equation>
<bodyText confidence="0.9386805">
For n = 1, (TG + Id) t 1 = (TG + Id)((TG +
Id) t 0) = (TG + /d)(0). Clearly, the only
items added by TG are due to the second and
third clauses of definition 5, which are exactly
</bodyText>
<equation confidence="0.574517">
InitG. Also, (&gt;=_0(RG +/-d)k)(Init,G) = (RG+
Id)°(InitG) = InitG.
</equation>
<bodyText confidence="0.557208666666667">
Assume that the proposition holds for n — 1,
that is, (TG + Id) t (n — 1) = (E7Aj(RG +
Id)k)(InitG). Then
</bodyText>
<equation confidence="0.937798636363636">
(TG + Id) t n =
definition of t
(TG + Id)((TG + Id) t (n — 1)) =
by the induction hypothesis
(TG + Id)((EZ,__1(RG + Id)k)(InitG)) =
since TG(/) R(I) U init,G
(RG + Id)((ErkI(RG + Id)k)(Init,G)) U initG =
(RG Id)((EL0(R0 + Id)k)(Initc)) =-
1(RG-+ Id)k)(InitG)
Hence (RG Id)w(Init,G) = (TG + Id) t w
UP(TG)•
</equation>
<bodyText confidence="0.988355428571429">
The choice of &apos;Ulf,: as the semantics calls for
a different notion of observables. The denota-
tion of a grammar is now a function which re-
flects an infinite number of applications of the
grammar&apos;s rules, but completely ignores the 6-
rules and the lexical entries. If we took the ob-
servables of a grammar G to be L(G) we could
in general have Clf = [[G21.frt but Ob(G1)
Ob(G2) (due to different lexicons), that is, the
semantics would not be correct. However, when
the lexical entries in a grammar (including the 6-
rules, which can be viewed as empty categories,
or the lexical entries of traces) are taken as in-
put, a natural notion of observables preservation
is obtained. To guarantee correctness, we define
the observables of a grammar G with respect to
a given input.
Definition 12. The observables of a gram-
mar G =-- G, As) with respect to an in-
put set of items I are 0b1(G) = (w,
[w, 0, A, HI] E lloshml.
</bodyText>
<page confidence="0.995512">
101
</page>
<note confidence="0.896938">
Corollary 7. The semantics Hfr: is correct:
C1 -= fr, G2 then for every I, Obi(Gi) =
Obi(G2).
</note>
<bodyText confidence="0.9895425">
The above definition corresponds to the pre-
vious one in a natural way: when the input is
taken to be init,G, the observables of a grammar
;-1,re its language.
</bodyText>
<figure confidence="0.931097333333334">
Theorem 8. For all G, L(G)
Proof.
L(G) =
definition of L(G)
(w, A) I [w, 0, A, IwU E IG11 Op} =
definition 4
{(w, A) I HG[w, 0, A, IwI]} =
by theorem 1
{.(w, A) I [w, 0, A, HI] G UP(TG)} =
by theorem 6
(w, A) I [w, 0, A, Iwl] E Mfri(Mit,G)} =
by definition 12
Oh/ mit, (G)
0
To show that the semantics 11.11fi.: is composi-
</figure>
<bodyText confidence="0.943702363636364">
tional we must define an operator for combining
denotations. Unfortunately, the simplest oper-
ator, `+&apos;, would not do. However, a different
operator does the job. Define [[Gdfri • P211fn to
be (rilfn + E[G21,/n)w. Then H fn&apos; is commuta-
tive (and hence compositional) with respect to
`• and V.
Theorem 9. §-GI U Gdfn = llGiifi, • ll-G2hn•
The proof is basically similar to the case of
logic programming (Lassez and Maher, 1984)
and is detailed in Wintner (1999).
</bodyText>
<construct confidence="0.99501425">
Theorem 10. The semantics &apos;Lf&apos; is fully
abstract: for every two grammars G1 and G25
if for every grammar G and set of items I,
Obi (G1 U G) = Obi (G2 U G), then G1 fn
</construct>
<bodyText confidence="0.991946166666667">
The proof is constructive: assuming that
GIi
OFn G2, we show a grammar G (which de-
pends on G1 and G2) such that Obr(Gi. U G)
Ob1(G2 U G). For the details, see Wintner
(1999).
</bodyText>
<sectionHeader confidence="0.999258" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.99997506">
This paper discusses alternative definitions for
the semantics of unification-based linguistic for-
malisms, culminating in one that is both com-
positional and fully-abstract (with respect to
grammar union, a simple syntactic combination
operations on grammars). This is mostly an
adaptation of well-known results from logic pro-
gramming to the framework of unification-based
linguistic formalisms, and it is encouraging to
see that the same choice of semantics which
is compositional and fully-abstract for Prolog
turned out to have the same desirable proper-
ties in our domain.
The functional semantics &apos;: defined here
assigns to a grammar a function which reflects
the (possibly infinite) successive application of
grammar rules, viewing the lexicon as input to
the parsing process. We believe that this is a
key to modularity in grammar design. A gram-
mar module has to define a set of items that
it &amp;quot;exports&amp;quot;, and a set of items that can be
&amp;quot;imported&amp;quot;, in a similar way to the declaration
of interfaces in programming languages. We
are currently working out the details of such
a definition. An immediate application will fa-
cilitate the implementation of grammar devel-
opment systems that support modularity in a
clear, mathematically sound way.
The results reported here can be extended
in various directions. First, we are only con-
cerned in this work with one composition oper-
ator, grammar union. But alternative operators
are possible, too. In particular, it would be in-
teresting to define an operator which combines
the information encoded in two grammar rules,
for example by unifying the rules. Such an op-
erator would facilitate a separate development
of grammars along a different axis: one module
can define the syntactic component of a gram-
mar while another module would account for the
semantics. The composition operator will unify
each rule of one module with an associated rule
in the other. It remains to be seen whether the
grammar semantics we define here is composi-
tional and fully abstract with respect to such an
operator.
A different extension of these results should
provide for a distribution of the type hierarchy
among several grammar modules. While we as-
sume in this work that all grammars are defined
</bodyText>
<page confidence="0.996137">
102
</page>
<bodyText confidence="0.998313">
over a given signature, it is more realistic to as-
sume separate, interacting signatures. We hope
to be able to explore these directions in the fu-
ture.
</bodyText>
<sectionHeader confidence="0.996515" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99572104">
Antonio Brogi, Evelina Lamma, and Paola
Mello. 1992. Compositional model-theoretic
semantics for logic programs. New Genera-
tion Computing, 11:1-21.
Michele Bugliesi, Evelina Lamma, and Paola
Mello. 1994. Modularity in logic pro-
gramming. Journal of Logic Programming,
19,20:443-502.
Bob Carpenter. 1991. Typed feature struc-
tures: A generalization of first-order terms.
In Vijai Saraswat and Ueda Kazunori, edi-
tors, Logic Programming - Proceedings of the
1991 International Symposium, pages 187-
201, Cambridge, MA. MIT Press.
Bob Carpenter. 1992. The Logic of Typed Fea-
ture Structures. Cambridge Tracts in Theo-
retical Computer Science. Cambridge Univer-
sity Press.
Gregor Erbach and Hans Uszkoreit. 1990.
Grammar engineering: Problems and
prospects. CLAUS report 1, University of
the Saarland and German research center for
Artificial Intelligence, July.
Haim Gaifman and Ehud Shapiro. 1989. Fully
abstract compositional semantics for logic
programming. In 16th Annual ACM Sym-
posium, on Principles of Logic Programming,
pages 134-142, Austin, Texas, January.
J.-L. Lassez and M. J. Maher. 1984. Closures
and fairness in the semantics of programming
logic. Theoretical computer science, 29:167-
184.
M. J. Maher. 1988. Equivalences of logic pro-
grams. In Jack Minker, editor, Foundations
of Deductive Databases and Logic Program-
ming, chapter 16, pages 627-658. Morgan
Kaufmann Publishers, Los Altos, CA.
Paolo Mancarella and Dino Pedreschi. 1988.
An algebra of logic programs In Robert A.
Kowalski and Kenneth A. Bowen, edi-
tors, Logic Programming: Proceedings of the
Fifth international conference and sympo-
sium, pages 1006-1023, Cambridge, Mass.
MIT Press.
Fernando C. N. Pereira and Stuart M. Shieber.
1984. The semantics of grammar formalisms
seen as computer languages. In Proceedings of
the 10th international conference on compu-
tational linguistics and the 22nd annual meet-
ing of the association for computational lin-
guistics, pages 123-129, Stanford, CA, July.
Stuart Shieber, Yves Schabes, and Fernando
Pereira. 1995. Principles and implementation
of deductive parsing. Journal of Logic Pro-
gramming, 24(1-2):3-36, July/August.
Stuart M. Shieber. 1992. Constraint-Based
Grammar Formalisms. MIT Press, Cam-
bridge, Mass.
Klaas Sikkel. 1997. Parsing Schemata. Texts in
Theoretical Computer Science - An EATCS
Series. Springer Verlag, Berlin.
M. H. Van Emden and Robert A. Kowalski.
1976. The semantics of predicate logic as a
programming language. Journal of the Asso-
ciation for Computing Machinery, 23(4):733-
742, October.
Shuly Wintner and Nissim Francez. 1999. Off-
line parsability and the well-foundedness of
subsumption. Journal of Logic, Language
and Information, 8(1): 1-16, January.
Shuly Wintner. 1999. Compositional semantics
for linguistic formalisms. IRCS Report 99-05,
Institute for Research in Cognitive Science,
University of Pennsylvania, 3401 Walnut St.,
Suite 400A, Philadelphia, PA 19018.
</reference>
<page confidence="0.999288">
103
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.949267">
<title confidence="0.999125">Compositional Semantics for Linguistic Formalisms</title>
<author confidence="0.978838">Shuly Wintner</author>
<affiliation confidence="0.9995755">Institute for Research in Cognitive Science University of Pennsylvania</affiliation>
<address confidence="0.997366">3401 Walnut St., Suite 400A Philadelphia, PA 19018</address>
<email confidence="0.999686">shulyOlinc.cis.upenn.edu</email>
<abstract confidence="0.997746272727273">In what sense is a grammar the union of its This paper adapts the notion of comdeveloped in the context of programming languages, to the domain of linguistic formalisms. We study alternative definitions the semantics of such formalisms, suggesting a denotational semantics that we show to be compositional and fully-abstract. This facilitates a clear, mathematically sound way for defining grammar modularity.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Antonio Brogi</author>
<author>Evelina Lamma</author>
<author>Paola Mello</author>
</authors>
<title>Compositional model-theoretic semantics for logic programs.</title>
<date>1992</date>
<journal>New Generation Computing,</journal>
<pages>11--1</pages>
<contexts>
<context position="8024" citStr="Brogi et al., 1992" startWordPosition="1384" endWordPosition="1387">ith every program. The choice of semantics induces a natural equivalence operator on grammars: given a semantics G, G2 if 11Gd = 11G21 An essential requirement of any semantic equivalence is that it (cat : n) (cat : vp) —&gt; (cat : v) (cat : mi) (cat : v) 97 be correct (observables-preserving): if G1 G21 then Ob(G1) -= Ob(G2). Let `U&apos; be a composition operation on grammars and &apos;•&apos; a combination operator on denotations. A (correct) semantics &apos;HI&apos; is compositional (Gaifinan and Shapiro, 1989) if whenever [Gi]] = [[G2]] and [[G3]] = [[G4]], also [{6.11 U G31 = 1G2 U G41 A semantics is commutative (Brogi et al., 1992) if TG1 U G2]] • [[G21 This is a stronger notion than compositionality: if a semantics is commutative with respect to some operator then it is compositional. 2.2 An operational semantics As Van Emden and Kowalski (1976) note, &amp;quot;to define an operational semantics for a programming language is to define an implementational independent interpreter for it. For predicate logic the proof procedure behaves as such an interpreter.&amp;quot; Shieber et al. (1995) view parsing as a deductive process that proves claims about the grammatical status of strings from assumptions derived from the grammar. We follow the</context>
</contexts>
<marker>Brogi, Lamma, Mello, 1992</marker>
<rawString>Antonio Brogi, Evelina Lamma, and Paola Mello. 1992. Compositional model-theoretic semantics for logic programs. New Generation Computing, 11:1-21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Bugliesi</author>
<author>Evelina Lamma</author>
<author>Paola Mello</author>
</authors>
<title>Modularity in logic programming.</title>
<date>1994</date>
<journal>Journal of Logic Programming,</journal>
<pages>19--20</pages>
<contexts>
<context position="1559" citStr="Bugliesi et al., 1994" startWordPosition="227" endWordPosition="230">inguistic formalisms as very high level declarative programming languages, a grammar for a natural language can be viewed as a program. It is therefore possible to adapt methods and techniques of software engineering to the domain of natural language formalisms. We believe that any advances in grammar engineering must be preceded by a more theoretical work, concentrating on the semantics of grammars. This view reflects the situation in logic programming, where developments in alternative definitions for predicate logic semantics led to implementations of various program composition operators (Bugliesi et al., 1994). This paper suggests a denotational semantics for unification-based linguistic formalisms and shows that it is compositional and fully*I am grateful to Nissim Francez for commenting on an earlier version of this paper. This work was supported by an IR.CS Fellowship and NSF grant SBR 8920230. abstract. This facilitates a clear, mathematically sound way for defining grammar modularity. While most of the results we report on are probably not surprising, we believe that it is important to derive them directly for linguistic formalisms for two reasons. First, practitioners of linguistic formalisms</context>
</contexts>
<marker>Bugliesi, Lamma, Mello, 1994</marker>
<rawString>Michele Bugliesi, Evelina Lamma, and Paola Mello. 1994. Modularity in logic programming. Journal of Logic Programming, 19,20:443-502.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Carpenter</author>
</authors>
<title>Typed feature structures: A generalization of first-order terms.</title>
<date>1991</date>
<booktitle>In Vijai Saraswat and Ueda Kazunori, editors, Logic Programming - Proceedings of the 1991 International Symposium,</booktitle>
<pages>187--201</pages>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="4222" citStr="Carpenter, 1991" startWordPosition="635" endWordPosition="636">mantics of unification-based formalisms. We review in this section the operational definition of Shieber et al. (1995) and the denotational definition of, e.g., Pereira and Shieber (1984) or Carpenter (1992, pp. 204-206). We show that these definitions are equivalent and that none of them supports conipositionality. 2.1 Basic notions We assume familiarity with theories of feature structure based unification grammars, as formulated by, e.g., Carpenter (1992) or Shieber (1992). Grammars are defined over typed feature structures (TFSs) which can be viewed as generalizations of first-order terms (Carpenter, 1991). TFSs are partially ordered by subsumption, with I the least (or most general) TFS. A multi-rooted structure (MRS, see Sikkel (1997) or Wintner and Francez (1999)) is a sequence of TFSs, with possible reentrancies among different elements in the sequence. Meta-variables A, B range over TFSs and a, p — over MRSs. MRSs are partially ordered by subsumption, denoted &apos;C&apos;, with a least upper bound operation of unification, denoted &apos;U&apos;, and a greatest lowest bound denoted &apos;17&apos;. We assume the existence of a fixed, finite set WORDS of words. A lexicon associates with every word a set of TFSs, its cate</context>
</contexts>
<marker>Carpenter, 1991</marker>
<rawString>Bob Carpenter. 1991. Typed feature structures: A generalization of first-order terms. In Vijai Saraswat and Ueda Kazunori, editors, Logic Programming - Proceedings of the 1991 International Symposium, pages 187-201, Cambridge, MA. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Carpenter</author>
</authors>
<title>The Logic of Typed Feature Structures. Cambridge Tracts in Theoretical Computer Science.</title>
<date>1992</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="3812" citStr="Carpenter (1992" startWordPosition="574" endWordPosition="575">ver, this definition is &amp;quot;too fine&amp;quot;: in section 4 we present an adequate, compositional and fully-abstract semantics for linguistic formalisms. For lack of space, some proofs are omitted; an extended version is available as a technical report (Wintner, 1999). 2 Grammar semantics Viewing grammars as formal entities that share many features with computer programs, it is 96 natural to consider the notion of semantics of unification-based formalisms. We review in this section the operational definition of Shieber et al. (1995) and the denotational definition of, e.g., Pereira and Shieber (1984) or Carpenter (1992, pp. 204-206). We show that these definitions are equivalent and that none of them supports conipositionality. 2.1 Basic notions We assume familiarity with theories of feature structure based unification grammars, as formulated by, e.g., Carpenter (1992) or Shieber (1992). Grammars are defined over typed feature structures (TFSs) which can be viewed as generalizations of first-order terms (Carpenter, 1991). TFSs are partially ordered by subsumption, with I the least (or most general) TFS. A multi-rooted structure (MRS, see Sikkel (1997) or Wintner and Francez (1999)) is a sequence of TFSs, wi</context>
<context position="10633" citStr="Carpenter (1992" startWordPosition="1870" endWordPosition="1871"> start symbol A does not play a role in this definition; this is equivalent to assuming that the start symbol is always the most general TFS, I. The most natural observable for a grammar would be its language, either as a set of strings or augmented by TFSs. Thus we take Ob(G) to be L(G) and by definition, the operational semantics 1.1op &apos; preserves observables. 2.3 Denotational semantics In this section we consider denotational semantics through a fixpoint of a transformational operator associated with grammars. -This is essentially similar to the definition of Pereira and Shieber (1984) and Carpenter (1992, pp. 204- 206). We then show that the denotational semantics is equivalent to the operational one. Associate with a grammar G an operator TG that, analogously to the immediate consequence operator of logic programming, can be thought of as a &amp;quot;parsing step&amp;quot; operator in the context of &apos;grammatical formalisms. For the following discussion fix a particular grammar G = (TZ, G, As). Definition 5. Let TG : I —&gt; I be a transformation on sets of items, where for every I c ITEMS, [W, i, A, j] E TG(I) dr either • there exist Yi, ,Yk E / such that yi ji] for 1 5. 1 &lt; k; and i1+1 ---= for 1 &lt; I &lt; A; and i</context>
</contexts>
<marker>Carpenter, 1992</marker>
<rawString>Bob Carpenter. 1992. The Logic of Typed Feature Structures. Cambridge Tracts in Theoretical Computer Science. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregor Erbach</author>
<author>Hans Uszkoreit</author>
</authors>
<title>Grammar engineering: Problems and prospects.</title>
<date>1990</date>
<booktitle>CLAUS report 1, University of the Saarland and German research center for Artificial Intelligence,</booktitle>
<contexts>
<context position="913" citStr="Erbach and Uszkoreit, 1990" startWordPosition="128" endWordPosition="131">pts the notion of composition, well developed in the context of programming languages, to the domain of linguistic formalisms. We study alternative definitions for the semantics of such formalisms, suggesting a denotational semantics that we show to be compositional and fully-abstract. This facilitates a clear, mathematically sound way for defining grammar modularity. 1 Introduction Developing large scale grammars for natural languages is a complicated task, and the problems grammar engineers face when designing broad-coverage grammars are reminiscent of those tackled by software engineering (Erbach and Uszkoreit, 1990). Viewing contemporary linguistic formalisms as very high level declarative programming languages, a grammar for a natural language can be viewed as a program. It is therefore possible to adapt methods and techniques of software engineering to the domain of natural language formalisms. We believe that any advances in grammar engineering must be preceded by a more theoretical work, concentrating on the semantics of grammars. This view reflects the situation in logic programming, where developments in alternative definitions for predicate logic semantics led to implementations of various program</context>
</contexts>
<marker>Erbach, Uszkoreit, 1990</marker>
<rawString>Gregor Erbach and Hans Uszkoreit. 1990. Grammar engineering: Problems and prospects. CLAUS report 1, University of the Saarland and German research center for Artificial Intelligence, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haim Gaifman</author>
<author>Ehud Shapiro</author>
</authors>
<title>Fully abstract compositional semantics for logic programming.</title>
<date>1989</date>
<booktitle>In 16th Annual ACM Symposium, on Principles of Logic Programming,</booktitle>
<pages>134--142</pages>
<location>Austin, Texas,</location>
<marker>Gaifman, Shapiro, 1989</marker>
<rawString>Haim Gaifman and Ehud Shapiro. 1989. Fully abstract compositional semantics for logic programming. In 16th Annual ACM Symposium, on Principles of Logic Programming, pages 134-142, Austin, Texas, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-L Lassez</author>
<author>M J Maher</author>
</authors>
<title>Closures and fairness in the semantics of programming logic. Theoretical computer science,</title>
<date>1984</date>
<pages>29--167</pages>
<contexts>
<context position="18423" citStr="Lassez and Maher (1984)" startWordPosition="3450" endWordPosition="3453">g operator as the denotation of G: Cid = ALTG(I) U I. In other words, the semantics is TG + Id, where Id is the identity operator. Unfortunately, this does not solve the problem, as &apos;Hid&apos; is still not fully-abstract. 100 4 A fully abstract semantics We have shown so far that 11 &apos; is not cornpositional, and that Hjd is compositional but not fully abstract. The &amp;quot;right&amp;quot; semantics, therefore, lies somewhere in between: since the choice of semantics induces a natural equivalence on grammars, we seek an equivalence that is cruder than &apos;Hid&apos; but finer than lib&apos;. In this section we adapt results from Lassez and Maher (1984) and Maher (1988) to the domain of unificationbased linguistic formalisms. Consider the following semantics for logic programs: rather than taking the operator associated with the entire program, look only at the rules (excluding the facts), and take the meaning of a program to be the function that is obtained by an infinite applications of the operator associated with the rules. In our framework, this would amount to associating the following operator with a grammar: Definition 10. Let RG : I I be a transformation on sets of items, where for every C ITEMS, [w,i, A, j] E RG(i) if there exist I</context>
<context position="19754" citStr="Lassez and Maher (1984)" startWordPosition="3718" endWordPosition="3721">and &apos;011 • • • TV k. The functional denotation of a grammar G is = (RG+ = Enc°_0(RG + Id)Th. Notice that R* is not RG t co: the former is a function from sets of items to set of items; the latter is a set of items. Observe that RG is defined similarly to TG (definition 5), ignoring the items added (by TG) due to c-rules and lexical items. If we define the set of items InitG to be those items that are added by TG independently of the argument it operates on, then for every grammar G and every set of items I, TG(I) = RG(I) U Init. Relating the functional semantics to the fixpoint one, we follow Lassez and Maher (1984) in proving that the fixpoint of the grammar transformation operator can be computed by applying the functional semantics to the set InitG. Definition 11. For G = (R.,., AS), InitG I B is an c-rule in G and B U {{a, i, A, i +1] I B E G(a) for B E A} Theorem 6. For every grammar G, (RG + Id) (Init(;) = lfp(TG) Proof. We show that for every n, (TG + Id) t 71 (E Z 01 (RC ± I d)k)(1-ilitO) by induction on n. For n = 1, (TG + Id) t 1 = (TG + Id)((TG + Id) t 0) = (TG + /d)(0). Clearly, the only items added by TG are due to the second and third clauses of definition 5, which are exactly InitG. Also, </context>
<context position="22734" citStr="Lassez and Maher, 1984" startWordPosition="4305" endWordPosition="4308">, 0, A, IwI]} = by theorem 1 {.(w, A) I [w, 0, A, HI] G UP(TG)} = by theorem 6 (w, A) I [w, 0, A, Iwl] E Mfri(Mit,G)} = by definition 12 Oh/ mit, (G) 0 To show that the semantics 11.11fi.: is compositional we must define an operator for combining denotations. Unfortunately, the simplest operator, `+&apos;, would not do. However, a different operator does the job. Define [[Gdfri • P211fn to be (rilfn + E[G21,/n)w. Then H fn&apos; is commutative (and hence compositional) with respect to `• and V. Theorem 9. §-GI U Gdfn = llGiifi, • ll-G2hn• The proof is basically similar to the case of logic programming (Lassez and Maher, 1984) and is detailed in Wintner (1999). Theorem 10. The semantics &apos;Lf&apos; is fully abstract: for every two grammars G1 and G25 if for every grammar G and set of items I, Obi (G1 U G) = Obi (G2 U G), then G1 fn The proof is constructive: assuming that GIi OFn G2, we show a grammar G (which depends on G1 and G2) such that Obr(Gi. U G) Ob1(G2 U G). For the details, see Wintner (1999). 5 Conclusions This paper discusses alternative definitions for the semantics of unification-based linguistic formalisms, culminating in one that is both compositional and fully-abstract (with respect to grammar union, a si</context>
</contexts>
<marker>Lassez, Maher, 1984</marker>
<rawString>J.-L. Lassez and M. J. Maher. 1984. Closures and fairness in the semantics of programming logic. Theoretical computer science, 29:167-184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Maher</author>
</authors>
<title>Equivalences of logic programs.</title>
<date>1988</date>
<booktitle>Foundations of Deductive Databases and Logic Programming, chapter 16,</booktitle>
<pages>627--658</pages>
<editor>In Jack Minker, editor,</editor>
<publisher>Morgan Kaufmann Publishers,</publisher>
<location>Los Altos, CA.</location>
<contexts>
<context position="18440" citStr="Maher (1988)" startWordPosition="3455" endWordPosition="3456"> of G: Cid = ALTG(I) U I. In other words, the semantics is TG + Id, where Id is the identity operator. Unfortunately, this does not solve the problem, as &apos;Hid&apos; is still not fully-abstract. 100 4 A fully abstract semantics We have shown so far that 11 &apos; is not cornpositional, and that Hjd is compositional but not fully abstract. The &amp;quot;right&amp;quot; semantics, therefore, lies somewhere in between: since the choice of semantics induces a natural equivalence on grammars, we seek an equivalence that is cruder than &apos;Hid&apos; but finer than lib&apos;. In this section we adapt results from Lassez and Maher (1984) and Maher (1988) to the domain of unificationbased linguistic formalisms. Consider the following semantics for logic programs: rather than taking the operator associated with the entire program, look only at the rules (excluding the facts), and take the meaning of a program to be the function that is obtained by an infinite applications of the operator associated with the rules. In our framework, this would amount to associating the following operator with a grammar: Definition 10. Let RG : I I be a transformation on sets of items, where for every C ITEMS, [w,i, A, j] E RG(i) if there exist Ill, ,yk E I such </context>
</contexts>
<marker>Maher, 1988</marker>
<rawString>M. J. Maher. 1988. Equivalences of logic programs. In Jack Minker, editor, Foundations of Deductive Databases and Logic Programming, chapter 16, pages 627-658. Morgan Kaufmann Publishers, Los Altos, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paolo Mancarella</author>
<author>Dino Pedreschi</author>
</authors>
<title>An algebra of logic programs</title>
<date>1988</date>
<booktitle>Logic Programming: Proceedings of the Fifth international conference and symposium,</booktitle>
<pages>1006--1023</pages>
<editor>In Robert A. Kowalski and Kenneth A. Bowen, editors,</editor>
<publisher>MIT Press.</publisher>
<location>Cambridge, Mass.</location>
<contexts>
<context position="14075" citStr="Mancarella and Pedreschi (1988)" startWordPosition="2540" endWordPosition="2543">n on grammars, the standard semantics of grammars is too coarse to support it. Intuitively, this is because when a grammar G1 includes a particular rule p that is inapplicable for reduction, this rule contributes nothing to the denotation of the grammar. But when G1 is combined with some other grammar, G2, p might be used for reduction in G1 U G2) where it can interact with the rules of G2. We suggest an alternative, fixpoint based semantics for unification based grammars that naturally supports compositionality. 3 A compositional semantics To overcome the problems delineated above, we follow Mancarella and Pedreschi (1988) in considering the grammar transformation operator itself (rather than its fixpoint) as the denotaG1: As = (cat : (cat :s) G(John) -=- C2: As = (_L) (cat: vp) (cat : vp) L(sleeps) = G3: As = (_L) ,C (loves) = C4: A&apos; = (1) (cat : vp) (loves) = Ci U C2: As = (cat: (cat : s) (cat : vp) (cat : vp) (John) = (sleeps) = Gi Li G3 : A&apos; =- (cat : (cat : s) ohn) = G(loves) = Gi U G4 : A&apos; = (cat : (cat : s) (cat : vp) L(John) L(loves) = s) (cat : ii) (cat : vp) { (cat : -4 (cat : v) -4 (cat : v) (cat : G(loves) = {(cat : v)} { (cat : v)} - (cat : v) (cat : n) { (cat : v)} s) - (cat : (cat : vp) —* (cat :</context>
</contexts>
<marker>Mancarella, Pedreschi, 1988</marker>
<rawString>Paolo Mancarella and Dino Pedreschi. 1988. An algebra of logic programs In Robert A. Kowalski and Kenneth A. Bowen, editors, Logic Programming: Proceedings of the Fifth international conference and symposium, pages 1006-1023, Cambridge, Mass. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando C N Pereira</author>
<author>Stuart M Shieber</author>
</authors>
<title>The semantics of grammar formalisms seen as computer languages.</title>
<date>1984</date>
<booktitle>In Proceedings of the 10th international conference on computational linguistics and the 22nd annual meeting of the association for computational linguistics,</booktitle>
<pages>123--129</pages>
<location>Stanford, CA,</location>
<contexts>
<context position="3793" citStr="Pereira and Shieber (1984)" startWordPosition="569" endWordPosition="572">on operation on grammars) However, this definition is &amp;quot;too fine&amp;quot;: in section 4 we present an adequate, compositional and fully-abstract semantics for linguistic formalisms. For lack of space, some proofs are omitted; an extended version is available as a technical report (Wintner, 1999). 2 Grammar semantics Viewing grammars as formal entities that share many features with computer programs, it is 96 natural to consider the notion of semantics of unification-based formalisms. We review in this section the operational definition of Shieber et al. (1995) and the denotational definition of, e.g., Pereira and Shieber (1984) or Carpenter (1992, pp. 204-206). We show that these definitions are equivalent and that none of them supports conipositionality. 2.1 Basic notions We assume familiarity with theories of feature structure based unification grammars, as formulated by, e.g., Carpenter (1992) or Shieber (1992). Grammars are defined over typed feature structures (TFSs) which can be viewed as generalizations of first-order terms (Carpenter, 1991). TFSs are partially ordered by subsumption, with I the least (or most general) TFS. A multi-rooted structure (MRS, see Sikkel (1997) or Wintner and Francez (1999)) is a s</context>
<context position="10613" citStr="Pereira and Shieber (1984)" startWordPosition="1865" endWordPosition="1868">n procedure. Note also that the start symbol A does not play a role in this definition; this is equivalent to assuming that the start symbol is always the most general TFS, I. The most natural observable for a grammar would be its language, either as a set of strings or augmented by TFSs. Thus we take Ob(G) to be L(G) and by definition, the operational semantics 1.1op &apos; preserves observables. 2.3 Denotational semantics In this section we consider denotational semantics through a fixpoint of a transformational operator associated with grammars. -This is essentially similar to the definition of Pereira and Shieber (1984) and Carpenter (1992, pp. 204- 206). We then show that the denotational semantics is equivalent to the operational one. Associate with a grammar G an operator TG that, analogously to the immediate consequence operator of logic programming, can be thought of as a &amp;quot;parsing step&amp;quot; operator in the context of &apos;grammatical formalisms. For the following discussion fix a particular grammar G = (TZ, G, As). Definition 5. Let TG : I —&gt; I be a transformation on sets of items, where for every I c ITEMS, [W, i, A, j] E TG(I) dr either • there exist Yi, ,Yk E / such that yi ji] for 1 5. 1 &lt; k; and i1+1 ---= </context>
</contexts>
<marker>Pereira, Shieber, 1984</marker>
<rawString>Fernando C. N. Pereira and Stuart M. Shieber. 1984. The semantics of grammar formalisms seen as computer languages. In Proceedings of the 10th international conference on computational linguistics and the 22nd annual meeting of the association for computational linguistics, pages 123-129, Stanford, CA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Shieber</author>
<author>Yves Schabes</author>
<author>Fernando Pereira</author>
</authors>
<title>Principles and implementation of deductive parsing.</title>
<date>1995</date>
<journal>Journal of Logic Programming,</journal>
<pages>24--1</pages>
<location>July/August.</location>
<contexts>
<context position="3724" citStr="Shieber et al. (1995)" startWordPosition="559" endWordPosition="562">nal (with respect to grammar union, a simple syntactic combination operation on grammars) However, this definition is &amp;quot;too fine&amp;quot;: in section 4 we present an adequate, compositional and fully-abstract semantics for linguistic formalisms. For lack of space, some proofs are omitted; an extended version is available as a technical report (Wintner, 1999). 2 Grammar semantics Viewing grammars as formal entities that share many features with computer programs, it is 96 natural to consider the notion of semantics of unification-based formalisms. We review in this section the operational definition of Shieber et al. (1995) and the denotational definition of, e.g., Pereira and Shieber (1984) or Carpenter (1992, pp. 204-206). We show that these definitions are equivalent and that none of them supports conipositionality. 2.1 Basic notions We assume familiarity with theories of feature structure based unification grammars, as formulated by, e.g., Carpenter (1992) or Shieber (1992). Grammars are defined over typed feature structures (TFSs) which can be viewed as generalizations of first-order terms (Carpenter, 1991). TFSs are partially ordered by subsumption, with I the least (or most general) TFS. A multi-rooted st</context>
<context position="8472" citStr="Shieber et al. (1995)" startWordPosition="1458" endWordPosition="1461">&apos; is compositional (Gaifinan and Shapiro, 1989) if whenever [Gi]] = [[G2]] and [[G3]] = [[G4]], also [{6.11 U G31 = 1G2 U G41 A semantics is commutative (Brogi et al., 1992) if TG1 U G2]] • [[G21 This is a stronger notion than compositionality: if a semantics is commutative with respect to some operator then it is compositional. 2.2 An operational semantics As Van Emden and Kowalski (1976) note, &amp;quot;to define an operational semantics for a programming language is to define an implementational independent interpreter for it. For predicate logic the proof procedure behaves as such an interpreter.&amp;quot; Shieber et al. (1995) view parsing as a deductive process that proves claims about the grammatical status of strings from assumptions derived from the grammar. We follow their insight and notation and list a deductive system for parsing unification-based grammars. Definition 3. The deductive parsing system associated with a grammar G = (12.,L,A8) is defined over ITEMS and is characterized by: Axioms: [a, i, A, i + 1] if BE L(a) and B E A; [E, i, A, i] if B is an c-rule in I?, and BE A Goals: [tv, 0, A iw 1] where A E As Inference rules: [11)1 7 ill Al&gt; il], • • • 7 [Wk 1 47 Ak7ilci [W1&apos; • • WIcli) if &apos;Ji = i1+1 fo</context>
</contexts>
<marker>Shieber, Schabes, Pereira, 1995</marker>
<rawString>Stuart Shieber, Yves Schabes, and Fernando Pereira. 1995. Principles and implementation of deductive parsing. Journal of Logic Programming, 24(1-2):3-36, July/August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
</authors>
<title>Constraint-Based Grammar Formalisms.</title>
<date>1992</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, Mass.</location>
<contexts>
<context position="4085" citStr="Shieber (1992)" startWordPosition="615" endWordPosition="616">tics Viewing grammars as formal entities that share many features with computer programs, it is 96 natural to consider the notion of semantics of unification-based formalisms. We review in this section the operational definition of Shieber et al. (1995) and the denotational definition of, e.g., Pereira and Shieber (1984) or Carpenter (1992, pp. 204-206). We show that these definitions are equivalent and that none of them supports conipositionality. 2.1 Basic notions We assume familiarity with theories of feature structure based unification grammars, as formulated by, e.g., Carpenter (1992) or Shieber (1992). Grammars are defined over typed feature structures (TFSs) which can be viewed as generalizations of first-order terms (Carpenter, 1991). TFSs are partially ordered by subsumption, with I the least (or most general) TFS. A multi-rooted structure (MRS, see Sikkel (1997) or Wintner and Francez (1999)) is a sequence of TFSs, with possible reentrancies among different elements in the sequence. Meta-variables A, B range over TFSs and a, p — over MRSs. MRSs are partially ordered by subsumption, denoted &apos;C&apos;, with a least upper bound operation of unification, denoted &apos;U&apos;, and a greatest lowest bound </context>
</contexts>
<marker>Shieber, 1992</marker>
<rawString>Stuart M. Shieber. 1992. Constraint-Based Grammar Formalisms. MIT Press, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaas Sikkel</author>
</authors>
<title>Parsing Schemata. Texts in Theoretical Computer Science - An EATCS Series.</title>
<date>1997</date>
<publisher>Springer Verlag,</publisher>
<location>Berlin.</location>
<contexts>
<context position="4355" citStr="Sikkel (1997)" startWordPosition="657" endWordPosition="658">nal definition of, e.g., Pereira and Shieber (1984) or Carpenter (1992, pp. 204-206). We show that these definitions are equivalent and that none of them supports conipositionality. 2.1 Basic notions We assume familiarity with theories of feature structure based unification grammars, as formulated by, e.g., Carpenter (1992) or Shieber (1992). Grammars are defined over typed feature structures (TFSs) which can be viewed as generalizations of first-order terms (Carpenter, 1991). TFSs are partially ordered by subsumption, with I the least (or most general) TFS. A multi-rooted structure (MRS, see Sikkel (1997) or Wintner and Francez (1999)) is a sequence of TFSs, with possible reentrancies among different elements in the sequence. Meta-variables A, B range over TFSs and a, p — over MRSs. MRSs are partially ordered by subsumption, denoted &apos;C&apos;, with a least upper bound operation of unification, denoted &apos;U&apos;, and a greatest lowest bound denoted &apos;17&apos;. We assume the existence of a fixed, finite set WORDS of words. A lexicon associates with every word a set of TFSs, its category. Meta-variable a ranges over WORDS and -- over strings of words (elements of WORDS*). Grammars are defined over a signature of t</context>
</contexts>
<marker>Sikkel, 1997</marker>
<rawString>Klaas Sikkel. 1997. Parsing Schemata. Texts in Theoretical Computer Science - An EATCS Series. Springer Verlag, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M H Van Emden</author>
<author>Robert A Kowalski</author>
</authors>
<title>The semantics of predicate logic as a programming language.</title>
<date>1976</date>
<journal>Journal of the Association for Computing Machinery,</journal>
<pages>23--4</pages>
<marker>Van Emden, Kowalski, 1976</marker>
<rawString>M. H. Van Emden and Robert A. Kowalski. 1976. The semantics of predicate logic as a programming language. Journal of the Association for Computing Machinery, 23(4):733-742, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shuly Wintner</author>
<author>Nissim Francez</author>
</authors>
<title>Offline parsability and the well-foundedness of subsumption.</title>
<date>1999</date>
<journal>Journal of Logic, Language and Information,</journal>
<volume>8</volume>
<issue>1</issue>
<pages>1--16</pages>
<contexts>
<context position="4385" citStr="Wintner and Francez (1999)" startWordPosition="660" endWordPosition="663">, e.g., Pereira and Shieber (1984) or Carpenter (1992, pp. 204-206). We show that these definitions are equivalent and that none of them supports conipositionality. 2.1 Basic notions We assume familiarity with theories of feature structure based unification grammars, as formulated by, e.g., Carpenter (1992) or Shieber (1992). Grammars are defined over typed feature structures (TFSs) which can be viewed as generalizations of first-order terms (Carpenter, 1991). TFSs are partially ordered by subsumption, with I the least (or most general) TFS. A multi-rooted structure (MRS, see Sikkel (1997) or Wintner and Francez (1999)) is a sequence of TFSs, with possible reentrancies among different elements in the sequence. Meta-variables A, B range over TFSs and a, p — over MRSs. MRSs are partially ordered by subsumption, denoted &apos;C&apos;, with a least upper bound operation of unification, denoted &apos;U&apos;, and a greatest lowest bound denoted &apos;17&apos;. We assume the existence of a fixed, finite set WORDS of words. A lexicon associates with every word a set of TFSs, its category. Meta-variable a ranges over WORDS and -- over strings of words (elements of WORDS*). Grammars are defined over a signature of types and features, assumed to </context>
<context position="9624" citStr="Wintner and Francez (1999)" startWordPosition="1690" endWordPosition="1693">1 7 ill Al&gt; il], • • • 7 [Wk 1 47 Ak7ilci [W1&apos; • • WIcli) if &apos;Ji = i1+1 for 1 &lt; 1 &lt; k and i = i1 and jk and (Ai, , Ak) G A When an item [w,i,A,j] can be deduced, applying k times the inference rules associated with a grammar G, we write 1--/[w, i, A, j]. When the number of inference steps is irrelevant it is omitted. Notice that the domain of items is infinite, and in particular that the number of axioms is infinite. Also, notice that the goal is to deduce a TFS which is subsumed by the start symbol, and when TFSs can be cyclic, there can be infinitely many such TFSs (and, hence, goals) - see Wintner and Francez (1999). Definition 4. The operational denotation of a grammar G is nrjop = {x 1HG :1;}. Cl Op G2 if 1101.110p-= ffG2S0p • We use the operational semantics to define the language generated by a grammar G: L(G) = {(w, A) I [w, 0, A, 17n1] E[[G]]Op 1. Notice that a language is not merely a set of strings; rather, each string is associated with a TFS through the deduction procedure. Note also that the start symbol A does not play a role in this definition; this is equivalent to assuming that the start symbol is always the most general TFS, I. The most natural observable for a grammar would be its langua</context>
</contexts>
<marker>Wintner, Francez, 1999</marker>
<rawString>Shuly Wintner and Nissim Francez. 1999. Offline parsability and the well-foundedness of subsumption. Journal of Logic, Language and Information, 8(1): 1-16, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shuly Wintner</author>
</authors>
<title>Compositional semantics for linguistic formalisms.</title>
<date>1999</date>
<tech>IRCS Report 99-05,</tech>
<institution>Institute for Research in Cognitive Science, University of Pennsylvania, 3401 Walnut St.,</institution>
<location>Suite 400A, Philadelphia, PA</location>
<contexts>
<context position="3454" citStr="Wintner, 1999" startWordPosition="520" endWordPosition="521">chniques. We review alternative approaches, operational and denotational, to the semantics of linguistic formalisms in section 2 and show that they are &amp;quot;too crude&amp;quot; to support grammar composition. Section 3 presents an alternative semantics, shown to be compositional (with respect to grammar union, a simple syntactic combination operation on grammars) However, this definition is &amp;quot;too fine&amp;quot;: in section 4 we present an adequate, compositional and fully-abstract semantics for linguistic formalisms. For lack of space, some proofs are omitted; an extended version is available as a technical report (Wintner, 1999). 2 Grammar semantics Viewing grammars as formal entities that share many features with computer programs, it is 96 natural to consider the notion of semantics of unification-based formalisms. We review in this section the operational definition of Shieber et al. (1995) and the denotational definition of, e.g., Pereira and Shieber (1984) or Carpenter (1992, pp. 204-206). We show that these definitions are equivalent and that none of them supports conipositionality. 2.1 Basic notions We assume familiarity with theories of feature structure based unification grammars, as formulated by, e.g., Car</context>
<context position="22768" citStr="Wintner (1999)" startWordPosition="4313" endWordPosition="4314"> 0, A, HI] G UP(TG)} = by theorem 6 (w, A) I [w, 0, A, Iwl] E Mfri(Mit,G)} = by definition 12 Oh/ mit, (G) 0 To show that the semantics 11.11fi.: is compositional we must define an operator for combining denotations. Unfortunately, the simplest operator, `+&apos;, would not do. However, a different operator does the job. Define [[Gdfri • P211fn to be (rilfn + E[G21,/n)w. Then H fn&apos; is commutative (and hence compositional) with respect to `• and V. Theorem 9. §-GI U Gdfn = llGiifi, • ll-G2hn• The proof is basically similar to the case of logic programming (Lassez and Maher, 1984) and is detailed in Wintner (1999). Theorem 10. The semantics &apos;Lf&apos; is fully abstract: for every two grammars G1 and G25 if for every grammar G and set of items I, Obi (G1 U G) = Obi (G2 U G), then G1 fn The proof is constructive: assuming that GIi OFn G2, we show a grammar G (which depends on G1 and G2) such that Obr(Gi. U G) Ob1(G2 U G). For the details, see Wintner (1999). 5 Conclusions This paper discusses alternative definitions for the semantics of unification-based linguistic formalisms, culminating in one that is both compositional and fully-abstract (with respect to grammar union, a simple syntactic combination operati</context>
</contexts>
<marker>Wintner, 1999</marker>
<rawString>Shuly Wintner. 1999. Compositional semantics for linguistic formalisms. IRCS Report 99-05, Institute for Research in Cognitive Science, University of Pennsylvania, 3401 Walnut St., Suite 400A, Philadelphia, PA 19018.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>