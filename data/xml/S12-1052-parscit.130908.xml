<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002324">
<title confidence="0.987976">
SemEval-2012 Task 7: Choice of Plausible Alternatives:
An Evaluation of Commonsense Causal Reasoning
</title>
<author confidence="0.994912">
Andrew S. Gordon Zornitsa Kozareva Melissa Roemmele
</author>
<affiliation confidence="0.9980065">
Institute for Creative Technologies Information Sciences Institute Department of Linguistics
University of Southern California University of Southern California Indiana University
</affiliation>
<address confidence="0.898307">
Los Angeles, CA Marina del Rey, CA Bloomington, IN
</address>
<email confidence="0.999138">
gordon@ict.usc.edu kozareva@isi.edu msroemme@gmail.com
</email>
<sectionHeader confidence="0.995581" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997938">
SemEval-2012 Task 7 presented a deceptively
simple challenge: given an English sentence
as a premise, select the sentence amongst two
alternatives that more plausibly has a causal
relation to the premise. In this paper, we de-
scribe the development of this task and its mo-
tivation. We describe the two systems that
competed in this task as part of SemEval-
2012, and compare their results to those
achieved in previously published research. We
discuss the characteristics that make this task
so difficult, and offer our thoughts on how
progress can be made in the future.
</bodyText>
<sectionHeader confidence="0.990018" genericHeader="keywords">
1 Motivation
</sectionHeader>
<bodyText confidence="0.999896693877551">
Open-domain commonsense reasoning is one of
the grand challenges of artificial intelligence, and
has been the subject of research since the inception
of the field. Until recently, this research history has
been dominated by formal approaches (e.g. Lenat,
1995), where logical formalizations of com-
monsense theories were hand-authored by expert
logicians and evaluated using a handful of com-
monsense challenge problems (Morgenstern,
2012). Progress via this approach has been slow,
both because of the inherent difficulties in author-
ing suitably broad-coverage formal theories of the
commonsense world and the lack of evaluation
metrics for comparing systems from different labs
and research traditions.
Radically different approaches to the com-
monsense reasoning problem have recently been
explored by natural language processing research-
ers. Speer et al. (2008) describe a novel reasoning
approach that applies dimensionality reduction to
the space of millions of English-language com-
monsense facts in a crowd-sourced knowledge
base (Liu &amp; Singh, 2004). Gordon et al., (2010)
describe a method for extracting millions of com-
monsense facts from parse trees of English sen-
tences. Jung et al. (2010) describe a novel
approach to the extraction of commonsense
knowledge about activities by mining online how-
to articles. We believe that these new NLP-based
approaches hold enormous potential for overcom-
ing the knowledge acquisition bottleneck that has
limited progress in commonsense reasoning in pre-
vious decades.
Given the growth and enthusiasm for these new
approaches, there is increasing need for a common
metric for evaluation. A common evaluation suite
would allow researchers to gauge the performance
of new versions of their own systems, and to com-
pare their approaches with those of other research
groups. Evaluations for these new NLP-based ap-
proaches should themselves be based in natural
language, and must be suitably large to truly eval-
uate the breadth of different reasoning approaches.
Still, each evaluation should be focused on one
dimension of the overall commonsense reasoning
task, so as not to create a new challenge that no
single research group could hope to succeed.
In SemEval-2012 Task 7, we presented a new
evaluation for open-domain commonsense reason-
</bodyText>
<page confidence="0.985734">
394
</page>
<note confidence="0.536858">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 394–398,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.9452425">
ing, focusing specifically on commonsense causal
reasoning about everyday events.
</bodyText>
<sectionHeader confidence="0.420185" genericHeader="introduction">
2 Choice of Plausible Alternatives
</sectionHeader>
<bodyText confidence="0.998023880952381">
Consider the following English sentence, describ-
ing a hypothetical state of the world:
The man lost his balance on the ladder.
In addition to parsing this sentence, resolving
ambiguities, and constructing a semantic interpre-
tation, human readers also imagine the causal ante-
cedents and consequents that would follow if the
statement were true. With such a brief description,
readers are left with many questions. How high up
on the ladder was this man? What was he doing on
the ladder in the first place? How much experience
does he have using ladders? Was he intoxicated?
The answers to these questions help readers formu-
late hypotheses for the two central concerns when
reasoning about events: What was the cause of
this? and What happened as a result?
As computational linguists, we imagine that our
automated natural language processing algorithms
will also, eventually, need to engage in similar rea-
soning processes in order to achieve human-like
performance on text understanding tasks. Progress
toward the goal of deep semantic interpretation of
text has been slow. However, the last decade of
natural language processing research has shown
that enormous gains can be achieved when there is
a clear evaluation metric. A shared task with an
automated scoring mechanism allows researchers
to compare different approaches, tune system pa-
rameters to maximize performance, and assess
progress toward broader research objectives. De-
veloping an evaluation metric for causal reasoning
poses a number of challenges. It is necessary to
formulate a question with answers that can be au-
tomatically graded, but can still serve as a proxy
for the complex, generative imagination of readers.
Roemmele et al. (2011) offered a solution in the
form of a simple binary-choice question. Presented
with an English sentence describing a premise,
systems must select between two alternatives (also
sentences) the one that more plausibly has a causal
relation to the premise, as in the following exam-
ple:
</bodyText>
<listItem confidence="0.66120225">
Premise: The man lost his balance on the lad-
der. What happened as a result?
Alternative 1: He fell off the ladder.
Alternative 2: He climbed up the ladder.
</listItem>
<bodyText confidence="0.958465333333333">
Both of these alternatives are conceivable, and
neither is entailed by the premise. However, hu-
man readers have no difficulty selecting the alter-
native that is the more plausible of the two. This
question asks about a causal consequent, and a
complimentary formulation asks for the causal an-
tecedent, as in the following example:
Premise: The man fell unconscious. What was
the cause of this?
</bodyText>
<figureCaption confidence="0.858462">
Alternative 1: The assailant struck the man on
the head.
Alternative 2: The assailant took the man&apos;s wal-
let.
</figureCaption>
<bodyText confidence="0.9995906">
Roemmele et al. describe their efforts to author a
collection of 1000 questions of these two types to
create a new causal reasoning evaluation tool: the
Choice of Plausible Alternatives (COPA). When
presented to humans to select the correct alterna-
tive, the inter-rater agreement was extremely high
(Cohen&apos;s kappa = 0.965). Where disagreements
between two raters were found (in 26 of 1000
items), questions were removed and replaced with
new ones with perfect agreement.
To develop an automated evaluation tool, the
1000 questions were randomly ordered and sorted
into two equally sized sets of 500 questions to
serve as development and test sets. The order of
the correct alternative was also randomized, such
that the expected accuracy of a random baseline
would be 50%. Gold-standard answers for each
split are used to automatically evaluate a given
system&apos;s performance.
The distribution of the COPA evaluation in-
cludes an automated test of statistical significance
of differences seen between two competing sys-
tems. This software tool implements a compute-
intensive randomized test of statistical significance
using stratified shuffling, as described by Noreen
(1989). By randomly sorting answers between two
systems over thousands of trials, this test computes
the likelihood that differences as great as observed
differences could be obtained by random chance.
The COPA evaluation is most similar in style to
the Recognizing Textual Entailment challenge
(Degan et al., 2006), but differs in its focus on
causal implication rather than entailment. Instead
of asking whether the interpretation of a sentence
necessitates the truth of another, COPA concerns
</bodyText>
<page confidence="0.985228">
395
</page>
<table confidence="0.859749">
the defeasible inferences that can be drawn from (Church &amp; Hanks, 1990) over all pairs of bigrams
the interpretation of a sentence. In this respect, (at the token level) between the candidate alterna-
COPA overlaps in its aims with the task of recog- tive and the premise. PMI statistics were collected
nizing causal relations in text through automated using 8.4 million documents from the LDC Giga-
discourse processing (e.g. Marcu, 1999). Some word corpus (Graff &amp; Cieri, 2003). A window of
progress in automated discourse processing has 100 terms was used for finding pairs of co-
been made using supervised machine learning occurring bigrams, and a window/slop size of 2 for
methods, where system learn the lexical-syntactic the bigram itself.
patterns that are most correlated with causal rela- UTDHLT SVM Combined: The team&apos;s second
tions from a large annotated corpus (Sagae, 2009). approach augments the first by combining it with
Lacking a dedicated training corpus, the COPA several other features and casting the task as a
evaluation encourages competitors to capture classification problem. To this end, they consider
commonsense causal knowledge from any availa- the PMI between events participating in a temporal
ble corpus or existing knowledge repository. link on a Time-ML annotated Gigaword corpus.
3 SemEval-2012 Systems and Results That is, events that occur together frequently will
The COPA evaluation was accepted as Task 7 of have a higher PMI. They also consider the differ-
the 6th International Workshop on Semantic Eval- ence between the number of positive and negative
uation (SemEval-2012). In several respects, the polarity words between an alternative and premise
COPA evaluation was different than the typical using information from the Harvard Inquisitor. In
</table>
<bodyText confidence="0.8850185">
shared task offered as part of this series of work- addition, they used the count of matching cause-
shops. First, the task materials were available and effect pairs extracted using patterns on dependency
distributed long before the evaluation period be- structures from the Gigaword corpus. Combining
gan, and there were published results of previous all of these sources of information, they trained a
systems using this evaluation.1 Second, the task support vector machine (SVM) learning algorithm
included no training data, only sets of development to classify the alternative that is most causally re-
and test questions (500 each). Participants were lated to the premise.
encouraged to use any available text corpus or These systems were assessed based on their ac-
knowledge repositories in the construction of their curacy on the 500 questions in the test split of the
systems. Success on the task would not be possible COPA evaluation, presented in Table 1. Both sys-
simply through the selection of machine learning tems significantly outperformed the random base-
algorithms and feature encodings. Instead, some line (50% accuracy), but the gains seen in the
creativity and ingenuity was needed to find a suita- second approach were not significantly different
ble source of commonsense causal information, than those of the first.
and determine an automated mechanism for apply-
ing this information to COPA questions.
Only one team successfully completed the task
and submitted results during the official two-week
</bodyText>
<table confidence="0.73948956">
SemEval-2012 evaluation period. This team was
Travis Goodwin, Bryan Rink, Kirk Roberts, and
Sanda M. Harabagiu from the University of Texas
at Dallas, Human Language Technology Research
Institute. This team submitted results from two
different systems (Goodwin et al., 2012), which
they described to us as follows:
UTDHLT Bigram PMI: The team&apos;s first ap-
proach selects the alternative with the maximum
Pointwise Mutual Information (PMI) statistic
System Accuracy
UTDHLT Bigram PMI 61.8%
UTDHLT SVM Combined 63.4%
Table 1. SemEval-2012 Task 7 system accuracy on
500 questions in the COPA test split
4 Comparison to Previous Results
In order to better evaluate the success of these two
systems, we compared these results with the pub-
lished results of other systems that have used the
COPA evaluation. Three other systems were con-
sidered.
PMI Gutenberg (W=5): Described in Roem-
mele et al. (2011), this approach calculated the
PMI between words (unigrams) in the premise and
1 http://www.ict.usc.edu/~gordon/copa.html
</table>
<page confidence="0.667637">
396
</page>
<bodyText confidence="0.99974535483871">
each alternative, and selected the alternative with
the stronger correlation. The PMI statistic was cal-
culated using every English-language document in
Project Gutenberg (16GB of text), using a window
of 5 words.
PMI Story 1M (W=25): Described in Gordon
et al. (2011), this approach was identical to that of
Roemmele et al. (2011) except that the PMI statis-
tic was calculated using a corpus of nearly one mil-
lion personal stories extracted from Internet
weblogs (Gordon &amp; Swanson, 2009), with 1.9 GB
of text. Using this corpus instead of Project Guten-
berg, the best results were obtained by using a
window of 25 words for the PMI statistic.
PMI Story 10M (W=25): Also described in
Gordon et al. (2011), this approach explores the
gains that can be achieved by calculating the PMI
statistic using a much larger corpus of weblog sto-
ries. The story extraction technology used by Gor-
don and Swanson (2009) was applied to 621
million English-language weblog entries posted to
the Internet in 2010 to create a corpus of 10.4 mil-
lion personal stories (37GB of text). Again, the
best results were obtained by using a window of 25
words for the PMI statistic.
Table 2 compares the results of these three pre-
vious systems with the two SemEval-2012 sys-
tems. Although the last two of these three previous
systems achieved higher scores than both of the
SemEval-2012 submissions, the differences are not
statistically significant.
</bodyText>
<table confidence="0.995551333333333">
System Accuracy
PMI Gutenberg (W=5) 58.8%
UTDHLT Bigram PMI 61.8%
UTDHLT SVM Combined 63.4%
PMI Story 1M (W=25) 65.2%
PMI Story 10M (W=25) 65.4%
</table>
<tableCaption confidence="0.852774">
Table 2. Comparison of SemEval-2012 Task 7 sys-
tems (in bold) with previously published results on
the 500 questions in the COPA test split
</tableCaption>
<sectionHeader confidence="0.9979" genericHeader="background">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999975464285715">
The two systems from the University of Texas at
Dallas make an important contribution to progress
on open-domain commonsense reasoning. Some
lessons are evident from the short descriptions of
their systems that they provided to us.
As in each of the previously successful systems,
this team focused their efforts on calculating corre-
lational statistics between words in COPA ques-
tions using very large text corpora. In this case, the
Gigaword corpus is used, and the calculation is
based on bigrams rather than unigrams. We believe
that the content of the news articles that comprise
the Gigaword corpus is a step further away from
the concerns of COPA questions than both the Pro-
ject Gutenberg corpus and the weblog story corpo-
ra used in previous efforts. Indeed, the gains
achieved by Gordon et al. (2011) appear to be en-
tirely due to the relationship between COPA ques-
tions and the personal stories that people write
about in their public weblogs. However, the use of
a large news corpus affords the use of more sophis-
ticated analysis techniques that have been devel-
oped for this genre. Here, the Gigaword corpus is
annotated using Time-ML relationships, which in
turn are used to modify the PMI strength between
words.
The use of bigrams is an additional enhancement
explored by this team, as is the casting of COPA
questions as a classification task using a diverse set
of lexical and discourse features. Such an approach
can facilitate the combining of diverse systems in
the future, where correlational statistics are gath-
ered from a diverse set of text corpora, each suited
for specific domains of COPA questions or yield-
ing complimentary feature sets.
Still, the modest COPA performance seen from
all existing systems is somewhat discouraging.
With the best systems performing in the 60-65%
range, we remain much closer to random perfor-
mance (50%) than human performance (99%).
These results cast some doubt that the information
necessary to answer COPA questions can be readi-
ly obtained from large text corpora. Certainly the
use of simple correlational statistics between near-
by words is not enough. In the best case, we might
wish for perfect identification of causal relation-
ships between events in an extremely large text
corpus of narratives similar in content to COPA
questions. Semantic similarity between these
events and COPA sentences could be computed to
gather evidence to select the best alternative. Even
if it were possible to achieve this ideal, it is diffi-
cult to imagine that such an approach could mirror
human performance on this task.
To move closer to human performance, systems
may need to stretch beyond corpus statistics into
</bodyText>
<page confidence="0.993915">
397
</page>
<bodyText confidence="0.999938476190476">
the realm of automated reasoning. Just as human
readers do when hearing that “the man lost his bal-
ance on the ladder,” successful systems may need
to treat COPA premises as novel world states, and
imagine a broad range of interconnected causal
antecedents and consequents. Useful knowledge
bases will be those that have adequate coverage
over commonsense concerns, but also adequate
competency to support generative inference of the
sort more commonly seen in deductive and abduc-
tive automated reasoning frameworks. This
knowledge may or may not be represented as text,
but any successful system must have the capacity
to apply this knowledge to the understanding of
COPA&apos;s textual premises and alternatives. We con-
sider the successful application of commonsense
inference to text understanding to be one of the
grand challenges of natural language processing,
and hope that the COPA evaluation continues to be
a useful tool for benchmarking progress toward
this goal.
</bodyText>
<sectionHeader confidence="0.997474" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999264">
The projects or efforts depicted were or are spon-
sored by the U. S. Army. The content or infor-
mation presented does not necessarily reflect the
position or the policy of the Government, and no
official endorsement should be inferred.
</bodyText>
<sectionHeader confidence="0.998576" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999903246153846">
Church, K. and Hanks, P. (1990) Word Association
Norms, Mutual Information, and Lexicography.
Computational Linguistics, 16(1):22-29.
Dagan, I., Glickman, O., and Magnini, B. (2006) The
PASCAL Recognising Textual Entailment Chal-
lenge. In Quifionero-Candela, J.; Dagan, I.; Magnini,
B.; d&apos;Alche-Buc, F. (Eds.), Machine Learning Chal-
lenges. Lecture Notes in Computer Science, Vol.
3944, pp. 177-190, Springer, 2006.
Goodwin, T., Rink, B., Roberts, K., and Harabagiu, S.
(2012) UTDHLT: COPACETIC System for Choos-
ing Plausible Alternatives. Proceedings of the 6th In-
ternational Workshop on Semantic Evaluation
(SemEval 2012), June 7-8, 2012, Montreal, Canada.
Gordon, A., Bejan, C., and Sagae, K. (2011) Com-
monsense Causal Reasoning Using Millions of Per-
sonal Stories. Twenty-Fifth Conference on Artificial
Intelligence (AAAI-11), August 7–11, 2011, San
Francisco, CA.
Gordon, A. and Swanson, R. (2009) Identifying Person-
al Stories in Millions of Weblog Entries. Internation-
al Conference on Weblogs and Social Media, Data
Challenge Workshop, San Jose, CA.
Gordon, J., Van Durme, B., and K. Schubert, L. (2010)
Learning from the Web: Extracting General World
Knowledge from Noisy Text. Proceedings of the
AAAI 2010 Workshop on Collaboratively-built
Knowledge Sources and Artificial Intelligence
(WikiAI 2010).
Graff, D. and Cieri, C. (2003) English Gigaword. Lin-
guistic Data Consortium, Philadelphia.
Jung, Y., Ryu, J., Kim., K. and Myaeng, S.(2010). Au-
tomatic Construction of a Large-Scale Situation On-
tology by Mining How-to Instructions from the Web.
Journal of Web Semantics 8(2-3):110-124.
Lenat, D. (1995) CYC: A Large-Scale Investment in
Knowledge Infrastructure, Communications of the
ACM 38:33-38.
Liu, H. and Singh, P. (2004) ConceptNet: A Practical
Commonsense Reasoning Toolkit. BT Technology
Journal 22(4):211-226.
Marcu, D. (1999). A decision-based approach to rhetor-
ical parsing. The 37th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL&apos;99), pages
365-372, Maryland, June 1999.
Morgenstern, L. (2012) Common Sense Problem Page.
Retrieved April 2012 at http://www-
formal.stanford.edu/leora/commonsense/
Noreen, E. (1989) Computer-Intensive Methods for
Testing Hypotheses: An Introduction. New York:
John Wiley &amp; Sons.
Roemmele, M., Bejan, C., and Gordon, A. (2011)
Choice of Plausible Alternatives: An Evaluation of
Commonsense Causal Reasoning. AAAI Spring
Symposium on Logical Formalizations of Com-
monsense Reasoning, Stanford University, March 21-
23, 2011.
Sagae, K. (2009) Analysis of discourse structure with
syntactic dependencies and data-driven shift-reduce
parsing. In Proceedings of the 11th International
Conference on Parsing Technologies (IWPT), pages
81--84. 2009.
Speer, R., Havasi, C. and Lieberman, H. (2008) Analo-
gySpace: Reducing the Dimensionality of Common
Sense Knowledge. Proceedings of AAAI 2008.
</reference>
<page confidence="0.998274">
398
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.003766">
<title confidence="0.994345">SemEval-2012 Task 7: Choice of Plausible An Evaluation of Commonsense Causal Reasoning</title>
<author confidence="0.999896">Andrew S Gordon Zornitsa Kozareva Melissa Roemmele</author>
<affiliation confidence="0.9999135">Institute for Creative Technologies Information Sciences Institute Department of Linguistics University of Southern California University of Southern California Indiana University</affiliation>
<address confidence="0.978393">Los Angeles, CA Marina del Rey, CA Bloomington, IN</address>
<email confidence="0.999626">gordon@ict.usc.edukozareva@isi.edumsroemme@gmail.com</email>
<abstract confidence="0.970639643564357">SemEval-2012 Task 7 presented a deceptively simple challenge: given an English sentence as a premise, select the sentence amongst two alternatives that more plausibly has a causal relation to the premise. In this paper, we describe the development of this task and its motivation. We describe the two systems that competed in this task as part of SemEval- 2012, and compare their results to those achieved in previously published research. We discuss the characteristics that make this task so difficult, and offer our thoughts on how progress can be made in the future. 1 Motivation Open-domain commonsense reasoning is one of the grand challenges of artificial intelligence, and has been the subject of research since the inception of the field. Until recently, this research history has been dominated by formal approaches (e.g. Lenat, 1995), where logical formalizations of commonsense theories were hand-authored by expert logicians and evaluated using a handful of commonsense challenge problems (Morgenstern, 2012). Progress via this approach has been slow, both because of the inherent difficulties in authoring suitably broad-coverage formal theories of the commonsense world and the lack of evaluation metrics for comparing systems from different labs and research traditions. Radically different approaches to the commonsense reasoning problem have recently been explored by natural language processing researchers. Speer et al. (2008) describe a novel reasoning approach that applies dimensionality reduction to the space of millions of English-language commonsense facts in a crowd-sourced knowledge base (Liu &amp; Singh, 2004). Gordon et al., (2010) describe a method for extracting millions of commonsense facts from parse trees of English sentences. Jung et al. (2010) describe a novel approach to the extraction of commonsense knowledge about activities by mining online howto articles. We believe that these new NLP-based approaches hold enormous potential for overcoming the knowledge acquisition bottleneck that has limited progress in commonsense reasoning in previous decades. Given the growth and enthusiasm for these new approaches, there is increasing need for a common metric for evaluation. A common evaluation suite would allow researchers to gauge the performance of new versions of their own systems, and to compare their approaches with those of other research groups. Evaluations for these new NLP-based approaches should themselves be based in natural language, and must be suitably large to truly evaluate the breadth of different reasoning approaches. Still, each evaluation should be focused on one dimension of the overall commonsense reasoning task, so as not to create a new challenge that no single research group could hope to succeed. In SemEval-2012 Task 7, we presented a new for open-domain commonsense reason- 394 Joint Conference on Lexical and Computational Semantics pages 394–398, Canada, June 7-8, 2012. Association for Computational Linguistics ing, focusing specifically on commonsense causal reasoning about everyday events. 2 Choice of Plausible Alternatives Consider the following English sentence, describing a hypothetical state of the world: The man lost his balance on the ladder. In addition to parsing this sentence, resolving ambiguities, and constructing a semantic interpretation, human readers also imagine the causal antecedents and consequents that would follow if the statement were true. With such a brief description, readers are left with many questions. How high up on the ladder was this man? What was he doing on the ladder in the first place? How much experience does he have using ladders? Was he intoxicated? The answers to these questions help readers formulate hypotheses for the two central concerns when about events: was the cause of happened as a result? As computational linguists, we imagine that our automated natural language processing algorithms will also, eventually, need to engage in similar reasoning processes in order to achieve human-like performance on text understanding tasks. Progress toward the goal of deep semantic interpretation of text has been slow. However, the last decade of natural language processing research has shown that enormous gains can be achieved when there is a clear evaluation metric. A shared task with an automated scoring mechanism allows researchers to compare different approaches, tune system parameters to maximize performance, and assess progress toward broader research objectives. Developing an evaluation metric for causal reasoning poses a number of challenges. It is necessary to formulate a question with answers that can be automatically graded, but can still serve as a proxy for the complex, generative imagination of readers. Roemmele et al. (2011) offered a solution in the form of a simple binary-choice question. Presented with an English sentence describing a premise, systems must select between two alternatives (also sentences) the one that more plausibly has a causal relation to the premise, as in the following example: Premise: The man lost his balance on the ladhappened as a result? Alternative 1: He fell off the ladder. Alternative 2: He climbed up the ladder. Both of these alternatives are conceivable, and neither is entailed by the premise. However, human readers have no difficulty selecting the alternative that is the more plausible of the two. This question asks about a causal consequent, and a complimentary formulation asks for the causal antecedent, as in the following example: The man fell unconscious. was the cause of this? Alternative 1: The assailant struck the man on the head. Alternative 2: The assailant took the man&apos;s wallet. Roemmele et al. describe their efforts to author a collection of 1000 questions of these two types to create a new causal reasoning evaluation tool: the Choice of Plausible Alternatives (COPA). When presented to humans to select the correct alternative, the inter-rater agreement was extremely high (Cohen&apos;s kappa = 0.965). Where disagreements between two raters were found (in 26 of 1000 items), questions were removed and replaced with new ones with perfect agreement. To develop an automated evaluation tool, the 1000 questions were randomly ordered and sorted into two equally sized sets of 500 questions to serve as development and test sets. The order of the correct alternative was also randomized, such that the expected accuracy of a random baseline would be 50%. Gold-standard answers for each split are used to automatically evaluate a given system&apos;s performance. The distribution of the COPA evaluation includes an automated test of statistical significance of differences seen between two competing systems. This software tool implements a computeintensive randomized test of statistical significance using stratified shuffling, as described by Noreen (1989). By randomly sorting answers between two systems over thousands of trials, this test computes the likelihood that differences as great as observed differences could be obtained by random chance. The COPA evaluation is most similar in style to the Recognizing Textual Entailment challenge (Degan et al., 2006), but differs in its focus on causal implication rather than entailment. Instead of asking whether the interpretation of a sentence necessitates the truth of another, COPA concerns 395 the defeasible inferences that can be drawn from the interpretation of a sentence. In this respect, COPA overlaps in its aims with the task of recog-nizing causal relations in text through automated discourse processing (e.g. Marcu, 1999). Some progress in automated discourse processing has been made using supervised machine learning methods, where system learn the lexical-syntactic patterns that are most correlated with causal rela-tions from a large annotated corpus (Sagae, 2009). Lacking a dedicated training corpus, the COPA evaluation encourages competitors to capture commonsense causal knowledge from any availa-ble corpus or existing knowledge repository. (Church &amp; Hanks, 1990) over all pairs of bigrams (at the token level) between the candidate alterna-tive and the premise. PMI statistics were collected using 8.4 million documents from the LDC Giga-word corpus (Graff &amp; Cieri, 2003). A window of 100 terms was used for finding pairs of co-occurring bigrams, and a window/slop size of 2 for the bigram itself. 3 SemEval-2012 Systems and Results SVM Combined: team&apos;s second approach augments the first by combining it with several other features and casting the task as a classification problem. To this end, they consider the PMI between events participating in a temporal link on a Time-ML annotated Gigaword corpus. That is, events that occur together frequently will have a higher PMI. They also consider the differ-ence between the number of positive and negative polarity words between an alternative and premise using information from the Harvard Inquisitor. In addition, they used the count of matching cause-effect pairs extracted using patterns on dependency structures from the Gigaword corpus. Combining all of these sources of information, they trained a support vector machine (SVM) learning algorithm to classify the alternative that is most causally re-lated to the premise. The COPA evaluation was accepted as Task 7 of the 6th International Workshop on Semantic Eval-uation (SemEval-2012). In several respects, the COPA evaluation was different than the typical shared task offered as part of this series of work-shops. First, the task materials were available and distributed long before the evaluation period be-gan, and there were published results of previous using this Second, the task included no training data, only sets of development and test questions (500 each). Participants were encouraged to use any available text corpus or knowledge repositories in the construction of their systems. Success on the task would not be possible simply through the selection of machine learning algorithms and feature encodings. Instead, some creativity and ingenuity was needed to find a suita-ble source of commonsense causal information, and determine an automated mechanism for apply-ing this information to COPA questions. These systems were assessed based on their ac-curacy on the 500 questions in the test split of the COPA evaluation, presented in Table 1. Both sys-tems significantly outperformed the random base-line (50% accuracy), but the gains seen in the second approach were not significantly different than those of the first. Only one team successfully completed the task and submitted results during the official two-week SemEval-2012 evaluation period. This team was Travis Goodwin, Bryan Rink, Kirk Roberts, and Sanda M. Harabagiu from the University of Texas at Dallas, Human Language Technology Research Institute. This team submitted results from two different systems (Goodwin et al., 2012), which they described to us as follows: Bigram PMI: team&apos;s first ap-proach selects the alternative with the maximum Pointwise Mutual Information (PMI) statistic System Accuracy UTDHLT Bigram PMI 61.8% UTDHLT SVM Combined 63.4% Table 1. SemEval-2012 Task 7 system accuracy on 500 questions in the COPA test split 4 Comparison to Previous Results In order to better evaluate the success of these two systems, we compared these results with the pub-lished results of other systems that have used the COPA evaluation. Three other systems were con-sidered. Gutenberg (W=5): in Roem-mele et al. (2011), this approach calculated the PMI between words (unigrams) in the premise and 396 each alternative, and selected the alternative with the stronger correlation. The PMI statistic was calculated using every English-language document in Project Gutenberg (16GB of text), using a window of 5 words. Story 1M (W=25): in Gordon et al. (2011), this approach was identical to that of Roemmele et al. (2011) except that the PMI statistic was calculated using a corpus of nearly one million personal stories extracted from Internet weblogs (Gordon &amp; Swanson, 2009), with 1.9 GB of text. Using this corpus instead of Project Gutenberg, the best results were obtained by using a window of 25 words for the PMI statistic. Story 10M (W=25): described in Gordon et al. (2011), this approach explores the gains that can be achieved by calculating the PMI statistic using a much larger corpus of weblog stories. The story extraction technology used by Gordon and Swanson (2009) was applied to 621 million English-language weblog entries posted to the Internet in 2010 to create a corpus of 10.4 million personal stories (37GB of text). Again, the best results were obtained by using a window of 25 words for the PMI statistic. Table 2 compares the results of these three previous systems with the two SemEval-2012 systems. Although the last two of these three previous systems achieved higher scores than both of the SemEval-2012 submissions, the differences are not statistically significant. System Accuracy PMI Gutenberg (W=5) 58.8% UTDHLT Bigram PMI 61.8% UTDHLT SVM Combined 63.4% PMI Story 1M (W=25) 65.2% PMI Story 10M (W=25) 65.4% Table 2. Comparison of SemEval-2012 Task 7 systems (in bold) with previously published results on the 500 questions in the COPA test split 5 Discussion The two systems from the University of Texas at Dallas make an important contribution to progress on open-domain commonsense reasoning. Some lessons are evident from the short descriptions of their systems that they provided to us. As in each of the previously successful systems, this team focused their efforts on calculating correlational statistics between words in COPA questions using very large text corpora. In this case, the Gigaword corpus is used, and the calculation is based on bigrams rather than unigrams. We believe that the content of the news articles that comprise the Gigaword corpus is a step further away from the concerns of COPA questions than both the Project Gutenberg corpus and the weblog story corpora used in previous efforts. Indeed, the gains achieved by Gordon et al. (2011) appear to be entirely due to the relationship between COPA questions and the personal stories that people write about in their public weblogs. However, the use of a large news corpus affords the use of more sophisticated analysis techniques that have been developed for this genre. Here, the Gigaword corpus is annotated using Time-ML relationships, which in turn are used to modify the PMI strength between words. The use of bigrams is an additional enhancement explored by this team, as is the casting of COPA questions as a classification task using a diverse set of lexical and discourse features. Such an approach can facilitate the combining of diverse systems in the future, where correlational statistics are gathered from a diverse set of text corpora, each suited for specific domains of COPA questions or yielding complimentary feature sets. Still, the modest COPA performance seen from all existing systems is somewhat discouraging. With the best systems performing in the 60-65% range, we remain much closer to random performance (50%) than human performance (99%). These results cast some doubt that the information necessary to answer COPA questions can be readily obtained from large text corpora. Certainly the use of simple correlational statistics between nearby words is not enough. In the best case, we might wish for perfect identification of causal relationships between events in an extremely large text corpus of narratives similar in content to COPA questions. Semantic similarity between these events and COPA sentences could be computed to gather evidence to select the best alternative. Even if it were possible to achieve this ideal, it is difficult to imagine that such an approach could mirror human performance on this task. To move closer to human performance, systems may need to stretch beyond corpus statistics into 397 the realm of automated reasoning. Just as human readers do when hearing that “the man lost his balance on the ladder,” successful systems may need to treat COPA premises as novel world states, and imagine a broad range of interconnected causal antecedents and consequents. Useful knowledge will be those that have adequate over commonsense concerns, but also adequate support generative inference of the sort more commonly seen in deductive and abductive automated reasoning frameworks. This knowledge may or may not be represented as text, but any successful system must have the capacity to apply this knowledge to the understanding of COPA&apos;s textual premises and alternatives. We consider the successful application of commonsense inference to text understanding to be one of the grand challenges of natural language processing, and hope that the COPA evaluation continues to be a useful tool for benchmarking progress toward this goal. Acknowledgments The projects or efforts depicted were or are sponsored by the U. S. Army. The content or information presented does not necessarily reflect the position or the policy of the Government, and no official endorsement should be inferred.</abstract>
<note confidence="0.954400083333334">References Church, K. and Hanks, P. (1990) Word Association Norms, Mutual Information, and Lexicography. Computational Linguistics, 16(1):22-29. Dagan, I., Glickman, O., and Magnini, B. (2006) The PASCAL Recognising Textual Entailment Challenge. In Quifionero-Candela, J.; Dagan, I.; Magnini, B.; d&apos;Alche-Buc, F. (Eds.), Machine Learning Challenges. Lecture Notes in Computer Science, Vol. 3944, pp. 177-190, Springer, 2006. Goodwin, T., Rink, B., Roberts, K., and Harabagiu, S. (2012) UTDHLT: COPACETIC System for Choosing Plausible Alternatives. Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), June 7-8, 2012, Montreal, Canada. Gordon, A., Bejan, C., and Sagae, K. (2011) Commonsense Causal Reasoning Using Millions of Personal Stories. Twenty-Fifth Conference on Artificial Intelligence (AAAI-11), August 7–11, 2011, San Francisco, CA. Gordon, A. and Swanson, R. (2009) Identifying Personal Stories in Millions of Weblog Entries. International Conference on Weblogs and Social Media, Data Challenge Workshop, San Jose, CA. Gordon, J., Van Durme, B., and K. Schubert, L. (2010) Learning from the Web: Extracting General World Knowledge from Noisy Text. Proceedings of the AAAI 2010 Workshop on Collaboratively-built Knowledge Sources and Artificial Intelligence (WikiAI 2010). Graff, D. and Cieri, C. (2003) English Gigaword. Linguistic Data Consortium, Philadelphia. Jung, Y., Ryu, J., Kim., K. and Myaeng, S.(2010). Automatic Construction of a Large-Scale Situation Ontology by Mining How-to Instructions from the Web. Journal of Web Semantics 8(2-3):110-124. Lenat, D. (1995) CYC: A Large-Scale Investment in Knowledge Infrastructure, Communications of the ACM 38:33-38. Liu, H. and Singh, P. (2004) ConceptNet: A Practical Commonsense Reasoning Toolkit. BT Technology Journal 22(4):211-226. Marcu, D. (1999). A decision-based approach to rhetorical parsing. The 37th Annual Meeting of the Association for Computational Linguistics (ACL&apos;99), pages 365-372, Maryland, June 1999. Morgenstern, L. (2012) Common Sense Problem Page. Retrieved April 2012 at http://www-</note>
<web confidence="0.511917">formal.stanford.edu/leora/commonsense/</web>
<author confidence="0.404887">E Computer-Intensive Methods for Noreen</author>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>K Church</author>
<author>P Hanks</author>
</authors>
<title>Word Association Norms,</title>
<date>1990</date>
<booktitle>Mutual Information, and Lexicography. Computational Linguistics,</booktitle>
<pages>16--1</pages>
<contexts>
<context position="7980" citStr="Church &amp; Hanks, 1990" startWordPosition="1225" endWordPosition="1228">sing stratified shuffling, as described by Noreen (1989). By randomly sorting answers between two systems over thousands of trials, this test computes the likelihood that differences as great as observed differences could be obtained by random chance. The COPA evaluation is most similar in style to the Recognizing Textual Entailment challenge (Degan et al., 2006), but differs in its focus on causal implication rather than entailment. Instead of asking whether the interpretation of a sentence necessitates the truth of another, COPA concerns 395 the defeasible inferences that can be drawn from (Church &amp; Hanks, 1990) over all pairs of bigrams the interpretation of a sentence. In this respect, (at the token level) between the candidate alternaCOPA overlaps in its aims with the task of recog- tive and the premise. PMI statistics were collected nizing causal relations in text through automated using 8.4 million documents from the LDC Gigadiscourse processing (e.g. Marcu, 1999). Some word corpus (Graff &amp; Cieri, 2003). A window of progress in automated discourse processing has 100 terms was used for finding pairs of cobeen made using supervised machine learning occurring bigrams, and a window/slop size of 2 fo</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Church, K. and Hanks, P. (1990) Word Association Norms, Mutual Information, and Lexicography. Computational Linguistics, 16(1):22-29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>O Glickman</author>
<author>B Magnini</author>
</authors>
<title>The PASCAL Recognising Textual Entailment Challenge. In</title>
<date>2006</date>
<journal>Lecture Notes in Computer Science,</journal>
<volume>3944</volume>
<pages>177--190</pages>
<publisher>Springer,</publisher>
<marker>Dagan, Glickman, Magnini, 2006</marker>
<rawString>Dagan, I., Glickman, O., and Magnini, B. (2006) The PASCAL Recognising Textual Entailment Challenge. In Quifionero-Candela, J.; Dagan, I.; Magnini, B.; d&apos;Alche-Buc, F. (Eds.), Machine Learning Challenges. Lecture Notes in Computer Science, Vol. 3944, pp. 177-190, Springer, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Goodwin</author>
<author>B Rink</author>
<author>K Roberts</author>
<author>S Harabagiu</author>
</authors>
<title>UTDHLT: COPACETIC System for Choosing Plausible Alternatives.</title>
<date>2012</date>
<booktitle>Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="11484" citStr="Goodwin et al., 2012" startWordPosition="1767" endWordPosition="1770">y and ingenuity was needed to find a suita- second approach were not significantly different ble source of commonsense causal information, than those of the first. and determine an automated mechanism for applying this information to COPA questions. Only one team successfully completed the task and submitted results during the official two-week SemEval-2012 evaluation period. This team was Travis Goodwin, Bryan Rink, Kirk Roberts, and Sanda M. Harabagiu from the University of Texas at Dallas, Human Language Technology Research Institute. This team submitted results from two different systems (Goodwin et al., 2012), which they described to us as follows: UTDHLT Bigram PMI: The team&apos;s first approach selects the alternative with the maximum Pointwise Mutual Information (PMI) statistic System Accuracy UTDHLT Bigram PMI 61.8% UTDHLT SVM Combined 63.4% Table 1. SemEval-2012 Task 7 system accuracy on 500 questions in the COPA test split 4 Comparison to Previous Results In order to better evaluate the success of these two systems, we compared these results with the published results of other systems that have used the COPA evaluation. Three other systems were considered. PMI Gutenberg (W=5): Described in Roemm</context>
</contexts>
<marker>Goodwin, Rink, Roberts, Harabagiu, 2012</marker>
<rawString>Goodwin, T., Rink, B., Roberts, K., and Harabagiu, S. (2012) UTDHLT: COPACETIC System for Choosing Plausible Alternatives. Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), June 7-8, 2012, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gordon</author>
<author>C Bejan</author>
<author>K Sagae</author>
</authors>
<title>Commonsense Causal Reasoning Using Millions of Personal Stories.</title>
<date>2011</date>
<booktitle>Twenty-Fifth Conference on Artificial Intelligence (AAAI-11),</booktitle>
<location>San Francisco, CA.</location>
<contexts>
<context position="12494" citStr="Gordon et al. (2011)" startWordPosition="1925" endWordPosition="1928">ccess of these two systems, we compared these results with the published results of other systems that have used the COPA evaluation. Three other systems were considered. PMI Gutenberg (W=5): Described in Roemmele et al. (2011), this approach calculated the PMI between words (unigrams) in the premise and 1 http://www.ict.usc.edu/~gordon/copa.html 396 each alternative, and selected the alternative with the stronger correlation. The PMI statistic was calculated using every English-language document in Project Gutenberg (16GB of text), using a window of 5 words. PMI Story 1M (W=25): Described in Gordon et al. (2011), this approach was identical to that of Roemmele et al. (2011) except that the PMI statistic was calculated using a corpus of nearly one million personal stories extracted from Internet weblogs (Gordon &amp; Swanson, 2009), with 1.9 GB of text. Using this corpus instead of Project Gutenberg, the best results were obtained by using a window of 25 words for the PMI statistic. PMI Story 10M (W=25): Also described in Gordon et al. (2011), this approach explores the gains that can be achieved by calculating the PMI statistic using a much larger corpus of weblog stories. The story extraction technology</context>
<context position="14749" citStr="Gordon et al. (2011)" startWordPosition="2305" endWordPosition="2308">s of their systems that they provided to us. As in each of the previously successful systems, this team focused their efforts on calculating correlational statistics between words in COPA questions using very large text corpora. In this case, the Gigaword corpus is used, and the calculation is based on bigrams rather than unigrams. We believe that the content of the news articles that comprise the Gigaword corpus is a step further away from the concerns of COPA questions than both the Project Gutenberg corpus and the weblog story corpora used in previous efforts. Indeed, the gains achieved by Gordon et al. (2011) appear to be entirely due to the relationship between COPA questions and the personal stories that people write about in their public weblogs. However, the use of a large news corpus affords the use of more sophisticated analysis techniques that have been developed for this genre. Here, the Gigaword corpus is annotated using Time-ML relationships, which in turn are used to modify the PMI strength between words. The use of bigrams is an additional enhancement explored by this team, as is the casting of COPA questions as a classification task using a diverse set of lexical and discourse feature</context>
</contexts>
<marker>Gordon, Bejan, Sagae, 2011</marker>
<rawString>Gordon, A., Bejan, C., and Sagae, K. (2011) Commonsense Causal Reasoning Using Millions of Personal Stories. Twenty-Fifth Conference on Artificial Intelligence (AAAI-11), August 7–11, 2011, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gordon</author>
<author>R Swanson</author>
</authors>
<title>Identifying Personal Stories in Millions of Weblog Entries.</title>
<date>2009</date>
<booktitle>International Conference on Weblogs and Social Media, Data Challenge Workshop,</booktitle>
<location>San Jose, CA.</location>
<contexts>
<context position="13128" citStr="Gordon and Swanson (2009)" startWordPosition="2035" endWordPosition="2039">proach was identical to that of Roemmele et al. (2011) except that the PMI statistic was calculated using a corpus of nearly one million personal stories extracted from Internet weblogs (Gordon &amp; Swanson, 2009), with 1.9 GB of text. Using this corpus instead of Project Gutenberg, the best results were obtained by using a window of 25 words for the PMI statistic. PMI Story 10M (W=25): Also described in Gordon et al. (2011), this approach explores the gains that can be achieved by calculating the PMI statistic using a much larger corpus of weblog stories. The story extraction technology used by Gordon and Swanson (2009) was applied to 621 million English-language weblog entries posted to the Internet in 2010 to create a corpus of 10.4 million personal stories (37GB of text). Again, the best results were obtained by using a window of 25 words for the PMI statistic. Table 2 compares the results of these three previous systems with the two SemEval-2012 systems. Although the last two of these three previous systems achieved higher scores than both of the SemEval-2012 submissions, the differences are not statistically significant. System Accuracy PMI Gutenberg (W=5) 58.8% UTDHLT Bigram PMI 61.8% UTDHLT SVM Combin</context>
<context position="12713" citStr="Gordon &amp; Swanson, 2009" startWordPosition="1962" endWordPosition="1965"> al. (2011), this approach calculated the PMI between words (unigrams) in the premise and 1 http://www.ict.usc.edu/~gordon/copa.html 396 each alternative, and selected the alternative with the stronger correlation. The PMI statistic was calculated using every English-language document in Project Gutenberg (16GB of text), using a window of 5 words. PMI Story 1M (W=25): Described in Gordon et al. (2011), this approach was identical to that of Roemmele et al. (2011) except that the PMI statistic was calculated using a corpus of nearly one million personal stories extracted from Internet weblogs (Gordon &amp; Swanson, 2009), with 1.9 GB of text. Using this corpus instead of Project Gutenberg, the best results were obtained by using a window of 25 words for the PMI statistic. PMI Story 10M (W=25): Also described in Gordon et al. (2011), this approach explores the gains that can be achieved by calculating the PMI statistic using a much larger corpus of weblog stories. The story extraction technology used by Gordon and Swanson (2009) was applied to 621 million English-language weblog entries posted to the Internet in 2010 to create a corpus of 10.4 million personal stories (37GB of text). Again, the best results we</context>
</contexts>
<marker>Gordon, Swanson, 2009</marker>
<rawString>Gordon, A. and Swanson, R. (2009) Identifying Personal Stories in Millions of Weblog Entries. International Conference on Weblogs and Social Media, Data Challenge Workshop, San Jose, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gordon</author>
<author>B Van Durme</author>
<author>K Schubert</author>
<author>L</author>
</authors>
<title>Learning from the Web: Extracting General World Knowledge from Noisy Text.</title>
<date>2010</date>
<booktitle>Proceedings of the AAAI 2010 Workshop on Collaboratively-built Knowledge Sources and Artificial Intelligence (WikiAI</booktitle>
<marker>Gordon, Van Durme, Schubert, L, 2010</marker>
<rawString>Gordon, J., Van Durme, B., and K. Schubert, L. (2010) Learning from the Web: Extracting General World Knowledge from Noisy Text. Proceedings of the AAAI 2010 Workshop on Collaboratively-built Knowledge Sources and Artificial Intelligence (WikiAI 2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Graff</author>
<author>C Cieri</author>
</authors>
<title>English Gigaword. Linguistic Data Consortium,</title>
<date>2003</date>
<location>Philadelphia.</location>
<contexts>
<context position="8384" citStr="Graff &amp; Cieri, 2003" startWordPosition="1291" endWordPosition="1294">lication rather than entailment. Instead of asking whether the interpretation of a sentence necessitates the truth of another, COPA concerns 395 the defeasible inferences that can be drawn from (Church &amp; Hanks, 1990) over all pairs of bigrams the interpretation of a sentence. In this respect, (at the token level) between the candidate alternaCOPA overlaps in its aims with the task of recog- tive and the premise. PMI statistics were collected nizing causal relations in text through automated using 8.4 million documents from the LDC Gigadiscourse processing (e.g. Marcu, 1999). Some word corpus (Graff &amp; Cieri, 2003). A window of progress in automated discourse processing has 100 terms was used for finding pairs of cobeen made using supervised machine learning occurring bigrams, and a window/slop size of 2 for methods, where system learn the lexical-syntactic the bigram itself. patterns that are most correlated with causal rela- UTDHLT SVM Combined: The team&apos;s second tions from a large annotated corpus (Sagae, 2009). approach augments the first by combining it with Lacking a dedicated training corpus, the COPA several other features and casting the task as a evaluation encourages competitors to capture cl</context>
</contexts>
<marker>Graff, Cieri, 2003</marker>
<rawString>Graff, D. and Cieri, C. (2003) English Gigaword. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Y Jung</author>
<author>J Ryu</author>
<author>K Kim</author>
<author>S Myaeng</author>
</authors>
<title>Automatic Construction of a Large-Scale Situation Ontology by Mining How-to Instructions from the Web.</title>
<journal>Journal of Web Semantics</journal>
<pages>8--2</pages>
<marker>Jung, Ryu, Kim, Myaeng, </marker>
<rawString>Jung, Y., Ryu, J., Kim., K. and Myaeng, S.(2010). Automatic Construction of a Large-Scale Situation Ontology by Mining How-to Instructions from the Web. Journal of Web Semantics 8(2-3):110-124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lenat</author>
</authors>
<title>CYC: A Large-Scale Investment in Knowledge Infrastructure,</title>
<date>1995</date>
<journal>Communications of the ACM</journal>
<pages>38--33</pages>
<contexts>
<context position="1290" citStr="Lenat, 1995" startWordPosition="187" endWordPosition="188">e the development of this task and its motivation. We describe the two systems that competed in this task as part of SemEval2012, and compare their results to those achieved in previously published research. We discuss the characteristics that make this task so difficult, and offer our thoughts on how progress can be made in the future. 1 Motivation Open-domain commonsense reasoning is one of the grand challenges of artificial intelligence, and has been the subject of research since the inception of the field. Until recently, this research history has been dominated by formal approaches (e.g. Lenat, 1995), where logical formalizations of commonsense theories were hand-authored by expert logicians and evaluated using a handful of commonsense challenge problems (Morgenstern, 2012). Progress via this approach has been slow, both because of the inherent difficulties in authoring suitably broad-coverage formal theories of the commonsense world and the lack of evaluation metrics for comparing systems from different labs and research traditions. Radically different approaches to the commonsense reasoning problem have recently been explored by natural language processing researchers. Speer et al. (200</context>
</contexts>
<marker>Lenat, 1995</marker>
<rawString>Lenat, D. (1995) CYC: A Large-Scale Investment in Knowledge Infrastructure, Communications of the ACM 38:33-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Liu</author>
<author>P Singh</author>
</authors>
<title>ConceptNet: A Practical Commonsense Reasoning Toolkit.</title>
<date>2004</date>
<journal>BT Technology Journal</journal>
<pages>22--4</pages>
<contexts>
<context position="2083" citStr="Liu &amp; Singh, 2004" startWordPosition="299" endWordPosition="302">2). Progress via this approach has been slow, both because of the inherent difficulties in authoring suitably broad-coverage formal theories of the commonsense world and the lack of evaluation metrics for comparing systems from different labs and research traditions. Radically different approaches to the commonsense reasoning problem have recently been explored by natural language processing researchers. Speer et al. (2008) describe a novel reasoning approach that applies dimensionality reduction to the space of millions of English-language commonsense facts in a crowd-sourced knowledge base (Liu &amp; Singh, 2004). Gordon et al., (2010) describe a method for extracting millions of commonsense facts from parse trees of English sentences. Jung et al. (2010) describe a novel approach to the extraction of commonsense knowledge about activities by mining online howto articles. We believe that these new NLP-based approaches hold enormous potential for overcoming the knowledge acquisition bottleneck that has limited progress in commonsense reasoning in previous decades. Given the growth and enthusiasm for these new approaches, there is increasing need for a common metric for evaluation. A common evaluation su</context>
</contexts>
<marker>Liu, Singh, 2004</marker>
<rawString>Liu, H. and Singh, P. (2004) ConceptNet: A Practical Commonsense Reasoning Toolkit. BT Technology Journal 22(4):211-226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
</authors>
<title>A decision-based approach to rhetorical parsing.</title>
<date>1999</date>
<booktitle>The 37th Annual Meeting of the Association for Computational Linguistics (ACL&apos;99),</booktitle>
<pages>365--372</pages>
<location>Maryland,</location>
<contexts>
<context position="8344" citStr="Marcu, 1999" startWordPosition="1286" endWordPosition="1287">ffers in its focus on causal implication rather than entailment. Instead of asking whether the interpretation of a sentence necessitates the truth of another, COPA concerns 395 the defeasible inferences that can be drawn from (Church &amp; Hanks, 1990) over all pairs of bigrams the interpretation of a sentence. In this respect, (at the token level) between the candidate alternaCOPA overlaps in its aims with the task of recog- tive and the premise. PMI statistics were collected nizing causal relations in text through automated using 8.4 million documents from the LDC Gigadiscourse processing (e.g. Marcu, 1999). Some word corpus (Graff &amp; Cieri, 2003). A window of progress in automated discourse processing has 100 terms was used for finding pairs of cobeen made using supervised machine learning occurring bigrams, and a window/slop size of 2 for methods, where system learn the lexical-syntactic the bigram itself. patterns that are most correlated with causal rela- UTDHLT SVM Combined: The team&apos;s second tions from a large annotated corpus (Sagae, 2009). approach augments the first by combining it with Lacking a dedicated training corpus, the COPA several other features and casting the task as a evaluat</context>
</contexts>
<marker>Marcu, 1999</marker>
<rawString>Marcu, D. (1999). A decision-based approach to rhetorical parsing. The 37th Annual Meeting of the Association for Computational Linguistics (ACL&apos;99), pages 365-372, Maryland, June 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Morgenstern</author>
</authors>
<title>Common Sense Problem Page. Retrieved</title>
<date>2012</date>
<note>at http://wwwformal.stanford.edu/leora/commonsense/</note>
<contexts>
<context position="1467" citStr="Morgenstern, 2012" startWordPosition="211" endWordPosition="212">ed in previously published research. We discuss the characteristics that make this task so difficult, and offer our thoughts on how progress can be made in the future. 1 Motivation Open-domain commonsense reasoning is one of the grand challenges of artificial intelligence, and has been the subject of research since the inception of the field. Until recently, this research history has been dominated by formal approaches (e.g. Lenat, 1995), where logical formalizations of commonsense theories were hand-authored by expert logicians and evaluated using a handful of commonsense challenge problems (Morgenstern, 2012). Progress via this approach has been slow, both because of the inherent difficulties in authoring suitably broad-coverage formal theories of the commonsense world and the lack of evaluation metrics for comparing systems from different labs and research traditions. Radically different approaches to the commonsense reasoning problem have recently been explored by natural language processing researchers. Speer et al. (2008) describe a novel reasoning approach that applies dimensionality reduction to the space of millions of English-language commonsense facts in a crowd-sourced knowledge base (Li</context>
</contexts>
<marker>Morgenstern, 2012</marker>
<rawString>Morgenstern, L. (2012) Common Sense Problem Page. Retrieved April 2012 at http://wwwformal.stanford.edu/leora/commonsense/</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Noreen</author>
</authors>
<title>Computer-Intensive Methods for Testing Hypotheses: An Introduction.</title>
<date>1989</date>
<publisher>John Wiley &amp; Sons.</publisher>
<location>New York:</location>
<contexts>
<context position="7415" citStr="Noreen (1989)" startWordPosition="1140" endWordPosition="1141">orted into two equally sized sets of 500 questions to serve as development and test sets. The order of the correct alternative was also randomized, such that the expected accuracy of a random baseline would be 50%. Gold-standard answers for each split are used to automatically evaluate a given system&apos;s performance. The distribution of the COPA evaluation includes an automated test of statistical significance of differences seen between two competing systems. This software tool implements a computeintensive randomized test of statistical significance using stratified shuffling, as described by Noreen (1989). By randomly sorting answers between two systems over thousands of trials, this test computes the likelihood that differences as great as observed differences could be obtained by random chance. The COPA evaluation is most similar in style to the Recognizing Textual Entailment challenge (Degan et al., 2006), but differs in its focus on causal implication rather than entailment. Instead of asking whether the interpretation of a sentence necessitates the truth of another, COPA concerns 395 the defeasible inferences that can be drawn from (Church &amp; Hanks, 1990) over all pairs of bigrams the inte</context>
</contexts>
<marker>Noreen, 1989</marker>
<rawString>Noreen, E. (1989) Computer-Intensive Methods for Testing Hypotheses: An Introduction. New York: John Wiley &amp; Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Roemmele</author>
<author>C Bejan</author>
<author>A Gordon</author>
</authors>
<title>Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning.</title>
<date>2011</date>
<booktitle>Symposium on Logical Formalizations of Commonsense Reasoning,</booktitle>
<publisher>AAAI Spring</publisher>
<institution>Stanford University,</institution>
<contexts>
<context position="5304" citStr="Roemmele et al. (2011)" startWordPosition="800" endWordPosition="803">e last decade of natural language processing research has shown that enormous gains can be achieved when there is a clear evaluation metric. A shared task with an automated scoring mechanism allows researchers to compare different approaches, tune system parameters to maximize performance, and assess progress toward broader research objectives. Developing an evaluation metric for causal reasoning poses a number of challenges. It is necessary to formulate a question with answers that can be automatically graded, but can still serve as a proxy for the complex, generative imagination of readers. Roemmele et al. (2011) offered a solution in the form of a simple binary-choice question. Presented with an English sentence describing a premise, systems must select between two alternatives (also sentences) the one that more plausibly has a causal relation to the premise, as in the following example: Premise: The man lost his balance on the ladder. What happened as a result? Alternative 1: He fell off the ladder. Alternative 2: He climbed up the ladder. Both of these alternatives are conceivable, and neither is entailed by the premise. However, human readers have no difficulty selecting the alternative that is th</context>
<context position="12101" citStr="Roemmele et al. (2011)" startWordPosition="1867" endWordPosition="1871">2012), which they described to us as follows: UTDHLT Bigram PMI: The team&apos;s first approach selects the alternative with the maximum Pointwise Mutual Information (PMI) statistic System Accuracy UTDHLT Bigram PMI 61.8% UTDHLT SVM Combined 63.4% Table 1. SemEval-2012 Task 7 system accuracy on 500 questions in the COPA test split 4 Comparison to Previous Results In order to better evaluate the success of these two systems, we compared these results with the published results of other systems that have used the COPA evaluation. Three other systems were considered. PMI Gutenberg (W=5): Described in Roemmele et al. (2011), this approach calculated the PMI between words (unigrams) in the premise and 1 http://www.ict.usc.edu/~gordon/copa.html 396 each alternative, and selected the alternative with the stronger correlation. The PMI statistic was calculated using every English-language document in Project Gutenberg (16GB of text), using a window of 5 words. PMI Story 1M (W=25): Described in Gordon et al. (2011), this approach was identical to that of Roemmele et al. (2011) except that the PMI statistic was calculated using a corpus of nearly one million personal stories extracted from Internet weblogs (Gordon &amp; Sw</context>
</contexts>
<marker>Roemmele, Bejan, Gordon, 2011</marker>
<rawString>Roemmele, M., Bejan, C., and Gordon, A. (2011) Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning. AAAI Spring Symposium on Logical Formalizations of Commonsense Reasoning, Stanford University, March 21-23, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sagae</author>
</authors>
<title>Analysis of discourse structure with syntactic dependencies and data-driven shift-reduce parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 11th International Conference on Parsing Technologies (IWPT),</booktitle>
<pages>81--84</pages>
<contexts>
<context position="8791" citStr="Sagae, 2009" startWordPosition="1357" endWordPosition="1358"> PMI statistics were collected nizing causal relations in text through automated using 8.4 million documents from the LDC Gigadiscourse processing (e.g. Marcu, 1999). Some word corpus (Graff &amp; Cieri, 2003). A window of progress in automated discourse processing has 100 terms was used for finding pairs of cobeen made using supervised machine learning occurring bigrams, and a window/slop size of 2 for methods, where system learn the lexical-syntactic the bigram itself. patterns that are most correlated with causal rela- UTDHLT SVM Combined: The team&apos;s second tions from a large annotated corpus (Sagae, 2009). approach augments the first by combining it with Lacking a dedicated training corpus, the COPA several other features and casting the task as a evaluation encourages competitors to capture classification problem. To this end, they consider commonsense causal knowledge from any availa- the PMI between events participating in a temporal ble corpus or existing knowledge repository. link on a Time-ML annotated Gigaword corpus. 3 SemEval-2012 Systems and Results That is, events that occur together frequently will The COPA evaluation was accepted as Task 7 of have a higher PMI. They also consider </context>
</contexts>
<marker>Sagae, 2009</marker>
<rawString>Sagae, K. (2009) Analysis of discourse structure with syntactic dependencies and data-driven shift-reduce parsing. In Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 81--84. 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Speer</author>
<author>C Havasi</author>
<author>H Lieberman</author>
</authors>
<title>AnalogySpace: Reducing the Dimensionality of Common Sense Knowledge.</title>
<date>2008</date>
<booktitle>Proceedings of AAAI</booktitle>
<contexts>
<context position="1892" citStr="Speer et al. (2008)" startWordPosition="271" endWordPosition="274">e.g. Lenat, 1995), where logical formalizations of commonsense theories were hand-authored by expert logicians and evaluated using a handful of commonsense challenge problems (Morgenstern, 2012). Progress via this approach has been slow, both because of the inherent difficulties in authoring suitably broad-coverage formal theories of the commonsense world and the lack of evaluation metrics for comparing systems from different labs and research traditions. Radically different approaches to the commonsense reasoning problem have recently been explored by natural language processing researchers. Speer et al. (2008) describe a novel reasoning approach that applies dimensionality reduction to the space of millions of English-language commonsense facts in a crowd-sourced knowledge base (Liu &amp; Singh, 2004). Gordon et al., (2010) describe a method for extracting millions of commonsense facts from parse trees of English sentences. Jung et al. (2010) describe a novel approach to the extraction of commonsense knowledge about activities by mining online howto articles. We believe that these new NLP-based approaches hold enormous potential for overcoming the knowledge acquisition bottleneck that has limited progr</context>
</contexts>
<marker>Speer, Havasi, Lieberman, 2008</marker>
<rawString>Speer, R., Havasi, C. and Lieberman, H. (2008) AnalogySpace: Reducing the Dimensionality of Common Sense Knowledge. Proceedings of AAAI 2008.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>