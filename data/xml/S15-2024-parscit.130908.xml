<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000165">
<note confidence="0.45849675">
SOPA: Random Forests Regression for the Semantic Textual Similarity task
Davide Buscaldi, Jorge J. Garcia Flores,
Laboratoire d’Informatique de Paris Nord, CNRS (UMR 7030)
Universit´e Paris 13, Sorbonne Paris Cit´e, Villetaneuse, France
</note>
<email confidence="0.893759">
{buscaldi,jgflores}@lipn.univ-paris13.fr
</email>
<author confidence="0.956791">
Ivan V. Meza and Isaac Rodriguez
</author>
<affiliation confidence="0.797267">
Instituto de Investigaciones en Matemticas Aplicadas y en Sistemas (IIMAS)
Universidad Nacional Autnoma de M´exico (UNAM)
Ciudad Universitaria, DF, Mexico
</affiliation>
<email confidence="0.993147">
ivanvladimir,isaac@turing.iimas.unam.mx
</email>
<sectionHeader confidence="0.996599" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999851625">
This paper describes the system used by the
LIPN-IIMAS team in the Task 2, Semantic
Textual Similarity, at SemEval 2015, in both
the English and Spanish sub-tasks. We in-
cluded some features based on alignment mea-
sures and we tested different learning models,
in particular Random Forests, which proved
the best among those used in our participation.
</bodyText>
<sectionHeader confidence="0.998425" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99989288">
Our participation in SemEval 2015 was focused
on solving the technical problems that afflicted
our previous participation (Buscaldi et al., 2014)
and including additional features based on align-
ments, such as the Sultan similarity (Sultan et al.,
2014b) and the measure available in CMU Sphinx-4
(Lamere et al., 2003) for speech recognition. We
baptised the new system SOPA from the Spanish
word for “soup”, since it uses a heterogeneous mix
of features. Well aware of the importance that the
training corpus and the regression algorithms have
for the STS task, we used language models to select
the most appropriate training corpus for a given text,
and we explored some alternatives to the v-Support
Vector Regression (v-SVR) (Sch¨olkopf et al., 1999)
used in our previous participations, specifically the
Multi-Layer Perceptron (Bishop and others, 1995)
and Random Forest (Breiman, 2001) regression al-
gorithms. The obtained results show that Random
Forests outperforms the other algorithms on every
test set. We describe all the features in Section 2; the
details on the learning algorithms and the training
corpus selection process are described in Section 3,
and the results obtained by the system are detailed
in Section 4.
</bodyText>
<sectionHeader confidence="0.989988" genericHeader="method">
2 Similarity Measures
</sectionHeader>
<bodyText confidence="0.9997493">
In this section we describe the measures used as fea-
tures in our system. The total number of features
used was 16 in English and 14 in Spanish. Since
most measures have already been used in our pre-
vious participation, we provide only basic overview,
referring the reader to the complete description in
(Buscaldi et al., 2013) for further details. When
POS tagging and NE recognition were required, we
used the Stanford CoreNLP for English and Span-
ish (Manning et al., 2014).
</bodyText>
<subsectionHeader confidence="0.984875">
2.1 WordNet-based Conceptual Similarity
</subsectionHeader>
<bodyText confidence="0.9998869375">
This measure has been introduced in order to mea-
sure similarities between concepts with respect to an
ontology. The similarity is calculated as follows:
first of all, words in sentences p and q are lemma-
tised and mapped to the related WordNet synsets.
All noun synsets are put into the set of synsets as-
sociated to the sentence, Cp and Cq, respectively. If
the synsets are in one of the other POS categories
(verb, adjective, adverb) we look for their deriva-
tionally related forms in order to find a related noun
synset: if there exists one, we put this synset in Cp
(or Cq). No disambiguation process is carried out,
so we take all possible meanings into account.
Given Cp and Cq as the sets of concepts contained
in sentences p and q, respectively, with |Cp |≥ |Cq|,
the conceptual similarity between p and q is calcu-
</bodyText>
<page confidence="0.979825">
132
</page>
<note confidence="0.402333">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 132–137,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
</note>
<equation confidence="0.821821">
lated as:
|Cp|
</equation>
<bodyText confidence="0.99988875">
where s(c1, c2) is a conceptual similarity measure.
Concept similarity can be calculated in different
ways. We used a variation of the Wu-Palmer for-
mula (Wu and Palmer, 1994) named “ProxiGenea3”,
introduced by (Dudognon et al., 2010), which is in-
spired by the analogy between a family tree and the
concept hierarchy in WordNet. The ProxiGenea3
measure is defined as:
</bodyText>
<equation confidence="0.999420333333333">
1
s(c1, c2) =
1 + d(c1) + d(c2) − 2 · d(c0)
</equation>
<bodyText confidence="0.99813">
where c0 is the most specific concept that is
present both in the synset path of c1 and c2 (that is,
the Least Common Subsumer or LCS). The function
returning the depth of a concept is noted with d.
</bodyText>
<subsectionHeader confidence="0.996051">
2.2 IC-based Similarity
</subsectionHeader>
<bodyText confidence="0.9977886">
This measure has been proposed by (Mihalcea et
al., 2006) as a corpus-based measure which uses
Resnik’s Information Content (IC) and the Jiang-
Conrath (Jiang and Conrath, 1997) similarity met-
ric. This measure is more precise than the one in-
troduced in the previous subsection because it takes
into account also the importance of concepts and not
only their relative position in the hierarchy. We re-
fer to (Buscaldi et al., 2013) and (Mihalcea et al.,
2006) for a detailed description of the measure. The
idf weights for the words were calculated using the
Google Web 1T (Brants and Franz, 2006) frequency
counts, while the IC values used are those calcu-
lated by Ted Pedersen (Pedersen et al., 2004) on the
British National Corpus1.
</bodyText>
<subsectionHeader confidence="0.999495">
2.3 Syntactic Dependencies
</subsectionHeader>
<bodyText confidence="0.999959285714286">
This measure tries to capture the syntactic similarity
between two sentences using dependencies. Previ-
ous experiments showed that converting constituents
to dependencies still achieved best results on out-of-
domain texts (Le Roux et al., 2012), so we decided
to use a 2-step architecture to obtain syntactic de-
pendencies. First we parsed pairs of sentences with
</bodyText>
<footnote confidence="0.981873">
1http://www.d.umn.edu/ tpederse/similarity.html
</footnote>
<bodyText confidence="0.999419625">
the LORG parser2. Second we converted the result-
ing parse trees to Stanford dependencies.
Given the sets of parsed dependencies Dp and Dq,
for sentence p and q, a dependency d C Dx is a
triple (l, h, t) where l is the dependency label (for in-
stance, dobj or prep), h the governor and t the depen-
dant. The similarity measure between two syntactic
dependencies d1 = (l1, h1, t1) and d2 = (l2, h2, t2)
is the levenshtein distance between the labels l1 and
l2 multiplied by the average of idfh * s(h1, h2) and
idft * s(t1, t2), where idfh and idft are the inverse
document frequencies calculated on Google Web 1T
for the governors and the dependants (we retain the
maximum for each pair), respectively, and s is the
ProxiGenea3 measure. NOTE: This measure was
used only in the English sub-task.
</bodyText>
<subsectionHeader confidence="0.993243">
2.4 Information Retrieval-based Similarity
</subsectionHeader>
<bodyText confidence="0.9467631">
Let us consider two texts p and q, an IR system S
and a document collection D indexed by S. This
measure is based on the assumption that p and q are
similar if the documents retrieved by S for the two
texts, used as input queries, are ranked similarly.
Let be Lp = {dp1,..., dpK} and Lq =
{dq1,... , dqK}, dxi C D the sets of the top K docu-
ments retrieved by S for texts p and q, respectively.
Let us define sp(d) and sq(d) the scores assigned by
S to a document d for the query p and q, respectively.
Then, the similarity score is calculated as:
simIR(p, q) = 1 |Lp n Lq|
if |Lp n Lq |=� 0, 0 otherwise.
For the participation in the English sub-task we
indexed a collection composed by the AQUAINT-
23 and the English NTCIR-84 document collections,
using the Lucene5 4.2 search engine with BM25
similarity. We indexed also DBPedia6 abstracts and
the UkWaC (Ferraresi et al., 2008), but they were
used to produce two additional features (separate
</bodyText>
<footnote confidence="0.995577166666667">
2https://github.com/CNGLdlab/LORG-Release
3http://www.nist.gov/tac/data/data desc.html#AQUAINT-2
4http://metadata.berkeley.edu/NTCIR-GeoTime/ntcir-8-
databases.php
5http://lucene.apache.org/core
6http://www.dbpedia.org/
</footnote>
<figure confidence="0.82707375">
max
c2ECq
s(c1,c2)
E
c1ECp
ss(p, q) =
E y (sp(d)−sq(d))2
dELpnLq max(sp(d),sq(d))
</figure>
<page confidence="0.993988">
133
</page>
<bodyText confidence="0.9997465">
from the basic IR one). The Spanish index was
created using the Spanish QA@CLEF 2005 (agen-
cia EFE1994-95, El Mundo 1994-95) and multiUN
(Eisele and Chen, 2010) collections. The K value
was set to 70 after a study detailed in (Buscaldi,
2013). Another IR-based feature was derived by the
rank-biased overlap measure introduced by (Webber
et al., 2010) which compares rankings without the
need of weights. In total, we had 4 IR-based mea-
sures for English and 2 for Spanish.
</bodyText>
<subsectionHeader confidence="0.998199">
2.5 N-gram Based Similarity
</subsectionHeader>
<bodyText confidence="0.999956125">
This measure tries to capture the fact that similar
sentences have similar n-grams, even if they are
not placed in the same positions. The measure is
based on the Clustered Keywords Positional Dis-
tance (CKPD) model proposed in (Buscaldi et al.,
2009) for the passage retrieval task.
The similarity between a text fragment p and an-
other text fragment q is calculated as:
</bodyText>
<equation confidence="0.992667">
�simngrams(p,q) =
dxEQ
</equation>
<bodyText confidence="0.9969415">
Where P is the set of the heaviest n-grams in p
where all terms are also contained in q; Q is the
set of all the possible n-grams in q, and n is the to-
tal number of terms in the longest sentence. The
weights for each term wi are calculated as wi =
1 − log(ni)1+log(N) where ni is the frequency of term
ti in the Google Web 1T collection, and N is the
frequency of the most frequent term in the Google
Web 1T collection. The weight for each n-gram
(h(x, P)), with |P |= j is calculated as:
</bodyText>
<equation confidence="0.99651">
h(x, P) = I Ek wk
l0
</equation>
<bodyText confidence="0.999887666666667">
The function d(x, xmax) determines the minimum
distance between a n-gram x and the heaviest one
xmax as the number of words between them.
</bodyText>
<subsectionHeader confidence="0.930398">
2.6 Geographical Context Similarity
</subsectionHeader>
<bodyText confidence="0.999976782608696">
This measure tries to measure if the two sentences
refer to events that took place in the same geograph-
ical area. It is based on the observation that the
compatibility of the geographic context between the
sentences is an important clue to determine whether
the sentences are related or not, especially in news.
We built a database of geographically-related enti-
ties, using geo-WordNet (Buscaldi and Rosso, 2008)
and expanding it with all the synsets that are related
to a geographically grounded synset. This implies
that also adjectives and verbs may be used as clues
for the identification of the geographical context of
a sentence. For instance, “Afghan” is associated to
“Afghanistan”, “Sovietize” to “Soviet Union”, etc.
The Named Entities of type PER (Person) are also
used as clues: we use Yago7 to check whether the
NE corresponds to a famous leader or not, and in the
affirmative case we include the related nation to the
geographical context of the sentence. For instance,
“Merkel” is mapped to “Germany”. Given Gp and
Gq the sets of places found in sentences p and q,
respectively, the geographical context similarity is
calculated as follows:
</bodyText>
<equation confidence="0.817620333333333">
⎛
⎜
simgeo(p, q) = 1−logK ⎝1 +
</equation>
<bodyText confidence="0.999126833333333">
Where d(x, y) is the spherical distance in Km. be-
tween x and y, and K is a normalization factor set
to 10000 Km. to obtain similarity values between
1 and 0. If no toponyms or geographically ground-
able entities are found in either sentences, then the
geographic context similarity is set to 1.
</bodyText>
<subsectionHeader confidence="0.984334">
2.7 Word Alignment Similarity
</subsectionHeader>
<bodyText confidence="0.998641866666667">
This similarity metric is based on the work of (Sul-
tan et al., 2014b; Sultan et al., 2014a). The met-
ric calculates a similarity score based on an align-
ment between two texts. It starts with an alignment
between similar words, it proceeds to align similar
name entities, to continue with words with similar
content, to finally align stop words. In the case of
content words, it proposes to use the syntactic con-
text to identify similar words. At the end, the simi-
larity is calculated as a harmonic mean between the
ratios of align words from sentence one to sentence
two, and from sentence two to sentence one.
CMU Sphinx-4 (Lamere et al., 2003) is a speech
recognition system that includes an alignment func-
tion that is used to align speech transcriptions with
</bodyText>
<footnote confidence="0.486756">
7http://www.mpi-inf.mpg.de/yago-naga/yago/
</footnote>
<figure confidence="0.6212605">
h(x, P)
Eni=1 wid(x, xmax)
if x C P
otherwise
E
xEGp
max(|Gp|, |Gq|)
⎞
⎠ ⎟
min
yEGq
d(x, y)
</figure>
<page confidence="0.994274">
134
</page>
<bodyText confidence="0.999733666666667">
text. We took one of the sentence as a reference and
the other one as a transcription and we used the out-
put of the Sphinx alignment measure as a feature.
</bodyText>
<subsectionHeader confidence="0.981838">
2.8 Other Measures
</subsectionHeader>
<bodyText confidence="0.999965666666667">
In addition to the above text similarity measures, we
used also the difference in size between sentences
and the following measures:
</bodyText>
<subsectionHeader confidence="0.798369">
Cosine
</subsectionHeader>
<bodyText confidence="0.99370825">
Cosine distance calculated between p =
(wp1,..., wp,) and q = (wql, ... , wq,,), the vec-
tors of tf.idf weights associated to sentences p and
q, with idf values calculated on Google Web 1T.
</bodyText>
<subsectionHeader confidence="0.882141">
Edit Distance
</subsectionHeader>
<bodyText confidence="0.999695333333333">
This similarity measure is calculated using the
Levenshtein distance on characters between the two
sentences.
</bodyText>
<subsectionHeader confidence="0.706963">
Named Entity Overlap
</subsectionHeader>
<bodyText confidence="0.9999874">
This is a per-class overlap measure (in this
way, “France” as an Organization does not match
“France” as a Location) calculated using the Dice
coefficient between the sets of NEs found, respec-
tively, in sentences p and q.
</bodyText>
<subsectionHeader confidence="0.972593">
Skip-gram Similarity
</subsectionHeader>
<bodyText confidence="0.999977666666667">
This measure is obtained as the dice coefficient
calculated between the set of skip-grams contained
in the two sentences.
</bodyText>
<sectionHeader confidence="0.991023" genericHeader="method">
3 Learning Models
</sectionHeader>
<bodyText confidence="0.988147951219512">
Besides the v-Support Vector Regression (v-SVR)
(Sch¨olkopf et al., 1999) used in previous participa-
tion, we used Multilayer Perceptron and Random
Forests. The Multilayer perceptron (Bishop and oth-
ers, 1995) is a neural network model which has sev-
eral interesting properties, such as robustness and
nonlinearity. Our implementation uses a simple gra-
dient descent learning algorithm with backpropaga-
tion and one hidden layer with 5 units. Random
Forests (Breiman, 2001) are an ensemble learning
method based on boosting and bagging of classifi-
cation trees. In our experiments, we used Random
Forests with 10 bootstrap samples.
In our runs, we selected a subset of the train-
ing set according to a similarity measure between
the test and the training set based on a 1- to 3-
grams language model and average sentence length.
The idea behind this selection process is that learn-
ing sentence similarities on a specific type of text
will increase the accuracy of predictions on text
with similar characteristics: image descriptions are
usually written in a very different form than word
definitions or forum answers. For each coher-
ent subset of the training set, we built a language
model Lm = (G1, G2, G3) where Gn is the dis-
tribution frequency of n-grams in the subset. We
obtained the same for the input dataset (Li) and
we calculated S(Lm, Li) = (b(Lm1, Li1) + 2 *
b(Lm2, Li2) + 3 * b(Lm3, Li3))/6 where b(F1, F2)
is the Bhatthacharyya distance between the distribu-
tions F1 and F2. We selected only those training
dataset where S(Lm, Li) &gt; 0.2. In Table 3 we
show the comparison of the results obtained with
such selection (the official ones) and those obtained
using the complete training set (not submitted). The
complete English training set was composed by the
data from SemEval STS 2012, 2013 and 2014. In
Spanish, we used our 2014 training set, which in-
cluded the automatically translated English 2012-
2013 pairs from STS and a corpus we made from
RAE8 definitions, and the 2014 Spanish test set.
</bodyText>
<sectionHeader confidence="0.999854" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999932117647059">
Table 1 and 2 presents our results our runs in Sem-
Eval 2015 (Agirre et al., 2015). Our participation
consisted on three runs for three different machine
learning approaches to regressions: Support Vec-
tor Regression (LIPN-SVM), Multi Layer Perceptron
(LIPN-MLP) and Random Forest (LIPN-RF). The
LIPN-RF configuration was our best one and was
ranked 25th run-wise and 14th system wise for the
English corpora; 5th run-wise and 3rd system-wise
for Spanish. Our English system had better overall
performance than Spanish. The best performance
was reached for the Believe dataset in English and
News dataset in Spanish.
Part of our proposal uses a language model to
select a subset of the corpus used to train the re-
gression. Table 3 shows performance with the full
dataset and the selected training corpus for the En-
</bodyText>
<footnote confidence="0.9967955">
8“Real Academia Espa˜nola de la lengua” Spanish dictio-
nary: http://www.rae.es
</footnote>
<page confidence="0.99029">
135
</page>
<table confidence="0.99972375">
Answer-forums Answer-students Headlines Believe Images Overall
LIPN-RF 0,6709 0,5914 0,7243 0,8123 0,8414 0,7356
LIPN-MLP 0,6178 0,5864 0,6886 0,8121 0,8184 0,7175
LIPN-SVM 0,5918 0,5718 0,7028 0,7985 0,8104 0,7070
</table>
<tableCaption confidence="0.98986">
Table 1: English results (Official runs).
</tableCaption>
<table confidence="0.99983575">
Wikipedia News Overall
LIPN-RF 0,5637 0,5655 0,5649
LIPN-MLP 0,25257 0,5342 0,4401
LIPN-SVM 0,4194 0,4007 0,4069
</table>
<tableCaption confidence="0.998545">
Table 2: Spanish results (Official runs).
</tableCaption>
<bodyText confidence="0.999928666666667">
glish dataset with the three regression approaches.
The overall score points that the corpus selection
was not beneficial. The most significant improve-
ment was concentrated on the Answer-students data-
set, in this case the performance felt 0,0588 points.
We checked the contribution of each feature using
the relief attribute selection measure (Kononenko,
1994) over the English training set. The best fea-
ture was the WordNet one, followed by Sultan and
IC-based similarity. The worst features were Rank-
biased Overlap followed by NE Overlap and the
Geographic context similarity (however, apart from
RBO, the other ones don’t have complete coverage).
The other features have a statistically similar contri-
bution.
</bodyText>
<sectionHeader confidence="0.988078" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.9998991">
The new learning models adopted were particularly
effective, outperforming the Support Vector Regres-
sion algorithm that we used in our previous partici-
pation. The alignment measure based on Sultan was
also very effective, as indicated by feature selection.
On the other hand, our corpus selection strategy did
not prove useful in general, although it provided
slight improvements on specific corpora. We will
need to further analyse these results to understand
how SOPA can still be improved.
</bodyText>
<sectionHeader confidence="0.97573" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9994665">
This work is supported/ partially supported by a pub-
lic grant overseen by the French National Research
Agency (ANR) as part of the progam “Investisse-
ments d’Avenir” (reference: ANR-10-LABX-0083).
</bodyText>
<sectionHeader confidence="0.998778" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995678625">
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo,
Iigo Lopez-Gazpio, Montse Maritxalar, Rada Mihal-
cea, German Rigau, Larraitz Uria, and Janyce Wiebe.
2015. SemEval-2015 Task 2: Semantic Textual Simi-
larity, English, Spanish and Pilot on Interpretability. In
Proceedings of the 9th International Workshop on Se-
mantic Evaluation (SemEval 2015), Denver, CO, June.
Christopher M Bishop et al. 1995. Neural networks for
pattern recognition.
Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram
corpus version 1.1.
Leo Breiman. 2001. Random forests. Machine learning,
45(1):5–32.
Davide Buscaldi and Paolo Rosso. 2008. Geo-WordNet:
Automatic Georeferencing of WordNet. In Proceed-
ings of the International Conference on Language Re-
sources and Evaluation, LREC 2008, Marrakech, Mo-
rocco.
Davide Buscaldi, Paolo Rosso, Jos´e Manuel G´omez, and
Emilio Sanchis. 2009. Answering questions with an
n-gram based passage retrieval engine. Journal of In-
telligent Information Systems (JIIS), 34(2):113–134.
Davide Buscaldi, Joseph Le Roux, Jorge J. Garcia Flo-
res, and Adrian Popescu. 2013. LIPN-CORE: Se-
mantic Text Similarity using n-grams, WordNet, Syn-
tactic Analysis, ESA and Information Retrieval based
Features. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 1: Pro-
ceedings of the Main Conference and the Shared Task:
Semantic Textual Similarity, pages 162–168, Atlanta,
Georgia, USA, June.
Davide Buscaldi, Jorge J Garcia Flores, Joseph Le Roux,
Nadi Tomeh, and Belem Priego-Sanchez. 2014.
LIPN: Introducing a new Geographical Context Sim-
ilarity Measure and a Statistical Similarity Measure
based on the Bhattacharyya coefficient. In SemEval
2014, pages 400–405.
Davide Buscaldi. 2013. Une mesure de similarit´e
s´emantique bas´ee sur la Recherche d’Information. In
</reference>
<page confidence="0.994979">
136
</page>
<table confidence="0.999799666666667">
Answer-forums Answer-students Headlines Believe Images Overall
Selected
LIPN-RF 0,6709 0,5914 0,7243 0,8123 0,8414 0,7244
LIPN-MLP 0,6178 0,5864 0,6886 0,8121 0,8184 0,6986
LIPN-SVM 0,5918 0,5718 0,7028 0,7985 0,8104 0,6894
Full
LIPN-RF 0,6418 0,6502 0,7320 0,8155 0,8301 0,7339
LIPN-MLP 0,6252 0,6213 0,8047 0,6856 0,8047 0,7120
LIPN-SVM 0,5701 0,6177 0,7939 0,7003 0,7939 0,7001
</table>
<tableCaption confidence="0.999869">
Table 3: Comparison of the results obtained with corpus selection and using the full corpus.
</tableCaption>
<reference confidence="0.990597328947369">
5`eme Atelier Recherche d’Information SEmantique -
RISE 2013, pages 81–91, Lille, France, July.
Damien Dudognon, Gilles Hubert, and Bachelin Jhonn
Victorino Ralalason. 2010. Proxig´en´ea : Une mesure
de similarit´e conceptuelle. In Proceedings of the Col-
loque Veille Strat´egique Scientifique et Technologique
(VSST 2010).
Andreas Eisele and Yu Chen. 2010. MultiUN: A
Multilingual Corpus from United Nation Documents.
In Daniel Tapias, Mike Rosner, Stelios Piperidis,
Jan Odjik, Joseph Mariani, Bente Maegaard, Khalid
Choukri, and Nicoletta Calzolari (Conference Chair),
editors, Proceedings of the Seventh conference on
International Language Resources and Evaluation,
pages 2868–2872, 5.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and
Silvia Bernardini. 2008. Introducing and evaluating
UkWaC, a very large web-derived corpus of English.
In Proceedings of the 4th Web as Corpus Workshop
(WAC-4) Can we beat Google, pages 47–54.
Jay J. Jiang and David W. Conrath. 1997. Semantic sim-
ilarity based on corpus statistics and lexical taxonomy.
In Proc. of the Int’l. Conf. on Research in Computa-
tional Linguistics, pages 19–33.
Igor Kononenko. 1994. Estimating attributes: analy-
sis and extensions of RELIEF. In Machine Learning:
ECML-94, pages 171–182.
Paul Lamere, Philip Kwok, Evandro Gouvea, Bhiksha
Raj, Rita Singh, William Walker, Manfred Warmuth,
and Peter Wolf. 2003. The cmu sphinx-4 speech
recognition system. In IEEE Intl. Conf. on Acoustics,
Speech and Signal Processing (ICASSP 2003), Hong
Kong, pages 2–5. Citeseer.
Joseph Le Roux, Jennifer Foster, Joachim Wagner, Rasul
Samad Zadeh Kaljahi, and Anton Bryl. 2012. DCU-
Paris13 Systems for the SANCL 2012 Shared Task. In
The NAACL 2012 First Workshop on Syntactic Analy-
sis of Non-Canonical Language (SANCL), pages 1–4,
Montr´eal, Canada.
Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David McClosky.
2014. The Stanford CoreNLP Natural Language Pro-
cessing Toolkit. In Proceedings of 52nd Annual Meet-
ing of the Association for Computational Linguistics:
System Demonstrations, pages 55–60.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and knowledge-based measures
of text semantic similarity. In Proceedings of the 21st
national conference on Artificial intelligence - Volume
1, AAAI’06, pages 775–780.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity: measuring the
relatedness of concepts. In Demonstration Papers
at HLT-NAACL 2004, HLT-NAACL–Demonstrations
’04, pages 38–41, Stroudsburg, PA, USA.
Bernhard Sch¨olkopf, Peter Bartlett, Alex Smola, and
Robert Williamson. 1999. Shrinking the tube: a new
support vector regression algorithm. In Proceedings
of the 1998 conference on Advances in neural infor-
mation processing systems II, pages 330–336, Cam-
bridge, MA, USA.
Md Arafat Sultan, Steven Bethard, and Tamara Sumner.
2014a. Back to Basics for Monolingual Alignment:
Exploiting Word Similarity and Contextual Evidence.
Md Arafat Sultan, Steven Bethard, and Tamara Sumner.
2014b. DLS@ CU: Sentence Similarity from Word
Alignment. SemEval 2014, page 241.
William Webber, Alistair Moffat, and Justin Zobel.
2010. A similarity measure for indefinite rankings.
ACM Transactions on Information Systems (TOIS),
28(4):20.
Zhibiao Wu and Martha Palmer. 1994. Verbs semantics
and lexical selection. In Proceedings of the 32nd an-
nual meeting on Association for Computational Lin-
guistics, ACL ’94, pages 133–138, Stroudsburg, PA,
USA.
</reference>
<page confidence="0.997792">
137
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.651373">
<title confidence="0.998898">SOPA: Random Forests Regression for the Semantic Textual Similarity task</title>
<author confidence="0.99999">Davide Buscaldi</author>
<author confidence="0.99999">Jorge J Garcia</author>
<affiliation confidence="0.996854">Laboratoire d’Informatique de Paris Nord, CNRS (UMR</affiliation>
<address confidence="0.800783">Universit´e Paris 13, Sorbonne Paris Cit´e, Villetaneuse,</address>
<author confidence="0.96699">Ivan V Meza</author>
<author confidence="0.96699">Isaac</author>
<affiliation confidence="0.965310666666667">Instituto de Investigaciones en Matemticas Aplicadas y en Sistemas Universidad Nacional Autnoma de M´exico Ciudad Universitaria, DF,</affiliation>
<email confidence="0.943606">ivanvladimir,isaac@turing.iimas.unam.mx</email>
<abstract confidence="0.997563">This paper describes the system used by the LIPN-IIMAS team in the Task 2, Semantic Textual Similarity, at SemEval 2015, in both the English and Spanish sub-tasks. We included some features based on alignment measures and we tested different learning models, in particular Random Forests, which proved the best among those used in our participation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Eneko Agirre</author>
<author>Carmen Banea</author>
<author>Claire Cardie</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
</authors>
<title>Aitor Gonzalez-Agirre, Weiwei Guo, Iigo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, German Rigau, Larraitz Uria, and Janyce Wiebe.</title>
<date>2015</date>
<booktitle>SemEval-2015 Task 2: Semantic Textual Similarity, English, Spanish and Pilot on Interpretability. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015),</booktitle>
<location>Denver, CO,</location>
<contexts>
<context position="14666" citStr="Agirre et al., 2015" startWordPosition="2449" endWordPosition="2452">nd F2. We selected only those training dataset where S(Lm, Li) &gt; 0.2. In Table 3 we show the comparison of the results obtained with such selection (the official ones) and those obtained using the complete training set (not submitted). The complete English training set was composed by the data from SemEval STS 2012, 2013 and 2014. In Spanish, we used our 2014 training set, which included the automatically translated English 2012- 2013 pairs from STS and a corpus we made from RAE8 definitions, and the 2014 Spanish test set. 4 Results Table 1 and 2 presents our results our runs in SemEval 2015 (Agirre et al., 2015). Our participation consisted on three runs for three different machine learning approaches to regressions: Support Vector Regression (LIPN-SVM), Multi Layer Perceptron (LIPN-MLP) and Random Forest (LIPN-RF). The LIPN-RF configuration was our best one and was ranked 25th run-wise and 14th system wise for the English corpora; 5th run-wise and 3rd system-wise for Spanish. Our English system had better overall performance than Spanish. The best performance was reached for the Believe dataset in English and News dataset in Spanish. Part of our proposal uses a language model to select a subset of t</context>
</contexts>
<marker>Agirre, Banea, Cardie, Cer, Diab, 2015</marker>
<rawString>Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Iigo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, German Rigau, Larraitz Uria, and Janyce Wiebe. 2015. SemEval-2015 Task 2: Semantic Textual Similarity, English, Spanish and Pilot on Interpretability. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), Denver, CO, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher M Bishop</author>
</authors>
<title>Neural networks for pattern recognition.</title>
<date>1995</date>
<marker>Bishop, 1995</marker>
<rawString>Christopher M Bishop et al. 1995. Neural networks for pattern recognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<title>Web 1t 5-gram corpus version 1.1.</title>
<date>2006</date>
<contexts>
<context position="4895" citStr="Brants and Franz, 2006" startWordPosition="788" endWordPosition="791">th d. 2.2 IC-based Similarity This measure has been proposed by (Mihalcea et al., 2006) as a corpus-based measure which uses Resnik’s Information Content (IC) and the JiangConrath (Jiang and Conrath, 1997) similarity metric. This measure is more precise than the one introduced in the previous subsection because it takes into account also the importance of concepts and not only their relative position in the hierarchy. We refer to (Buscaldi et al., 2013) and (Mihalcea et al., 2006) for a detailed description of the measure. The idf weights for the words were calculated using the Google Web 1T (Brants and Franz, 2006) frequency counts, while the IC values used are those calculated by Ted Pedersen (Pedersen et al., 2004) on the British National Corpus1. 2.3 Syntactic Dependencies This measure tries to capture the syntactic similarity between two sentences using dependencies. Previous experiments showed that converting constituents to dependencies still achieved best results on out-ofdomain texts (Le Roux et al., 2012), so we decided to use a 2-step architecture to obtain syntactic dependencies. First we parsed pairs of sentences with 1http://www.d.umn.edu/ tpederse/similarity.html the LORG parser2. Second w</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram corpus version 1.1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leo Breiman</author>
</authors>
<title>Random forests.</title>
<date>2001</date>
<booktitle>Machine learning,</booktitle>
<pages>45--1</pages>
<contexts>
<context position="1770" citStr="Breiman, 2001" startWordPosition="256" endWordPosition="257">ble in CMU Sphinx-4 (Lamere et al., 2003) for speech recognition. We baptised the new system SOPA from the Spanish word for “soup”, since it uses a heterogeneous mix of features. Well aware of the importance that the training corpus and the regression algorithms have for the STS task, we used language models to select the most appropriate training corpus for a given text, and we explored some alternatives to the v-Support Vector Regression (v-SVR) (Sch¨olkopf et al., 1999) used in our previous participations, specifically the Multi-Layer Perceptron (Bishop and others, 1995) and Random Forest (Breiman, 2001) regression algorithms. The obtained results show that Random Forests outperforms the other algorithms on every test set. We describe all the features in Section 2; the details on the learning algorithms and the training corpus selection process are described in Section 3, and the results obtained by the system are detailed in Section 4. 2 Similarity Measures In this section we describe the measures used as features in our system. The total number of features used was 16 in English and 14 in Spanish. Since most measures have already been used in our previous participation, we provide only basi</context>
<context position="13041" citStr="Breiman, 2001" startWordPosition="2164" endWordPosition="2165">arity This measure is obtained as the dice coefficient calculated between the set of skip-grams contained in the two sentences. 3 Learning Models Besides the v-Support Vector Regression (v-SVR) (Sch¨olkopf et al., 1999) used in previous participation, we used Multilayer Perceptron and Random Forests. The Multilayer perceptron (Bishop and others, 1995) is a neural network model which has several interesting properties, such as robustness and nonlinearity. Our implementation uses a simple gradient descent learning algorithm with backpropagation and one hidden layer with 5 units. Random Forests (Breiman, 2001) are an ensemble learning method based on boosting and bagging of classification trees. In our experiments, we used Random Forests with 10 bootstrap samples. In our runs, we selected a subset of the training set according to a similarity measure between the test and the training set based on a 1- to 3- grams language model and average sentence length. The idea behind this selection process is that learning sentence similarities on a specific type of text will increase the accuracy of predictions on text with similar characteristics: image descriptions are usually written in a very different fo</context>
</contexts>
<marker>Breiman, 2001</marker>
<rawString>Leo Breiman. 2001. Random forests. Machine learning, 45(1):5–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Davide Buscaldi</author>
<author>Paolo Rosso</author>
</authors>
<title>Geo-WordNet: Automatic Georeferencing of WordNet.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Conference on Language Resources and Evaluation, LREC</booktitle>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="9543" citStr="Buscaldi and Rosso, 2008" startWordPosition="1573" endWordPosition="1576">is calculated as: h(x, P) = I Ek wk l0 The function d(x, xmax) determines the minimum distance between a n-gram x and the heaviest one xmax as the number of words between them. 2.6 Geographical Context Similarity This measure tries to measure if the two sentences refer to events that took place in the same geographical area. It is based on the observation that the compatibility of the geographic context between the sentences is an important clue to determine whether the sentences are related or not, especially in news. We built a database of geographically-related entities, using geo-WordNet (Buscaldi and Rosso, 2008) and expanding it with all the synsets that are related to a geographically grounded synset. This implies that also adjectives and verbs may be used as clues for the identification of the geographical context of a sentence. For instance, “Afghan” is associated to “Afghanistan”, “Sovietize” to “Soviet Union”, etc. The Named Entities of type PER (Person) are also used as clues: we use Yago7 to check whether the NE corresponds to a famous leader or not, and in the affirmative case we include the related nation to the geographical context of the sentence. For instance, “Merkel” is mapped to “Germa</context>
</contexts>
<marker>Buscaldi, Rosso, 2008</marker>
<rawString>Davide Buscaldi and Paolo Rosso. 2008. Geo-WordNet: Automatic Georeferencing of WordNet. In Proceedings of the International Conference on Language Resources and Evaluation, LREC 2008, Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Davide Buscaldi</author>
<author>Paolo Rosso</author>
<author>Jos´e Manuel G´omez</author>
<author>Emilio Sanchis</author>
</authors>
<title>Answering questions with an n-gram based passage retrieval engine.</title>
<date>2009</date>
<journal>Journal of Intelligent Information Systems (JIIS),</journal>
<volume>34</volume>
<issue>2</issue>
<marker>Buscaldi, Rosso, G´omez, Sanchis, 2009</marker>
<rawString>Davide Buscaldi, Paolo Rosso, Jos´e Manuel G´omez, and Emilio Sanchis. 2009. Answering questions with an n-gram based passage retrieval engine. Journal of Intelligent Information Systems (JIIS), 34(2):113–134.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Davide Buscaldi</author>
<author>Joseph Le Roux</author>
<author>Jorge J Garcia Flores</author>
<author>Adrian Popescu</author>
</authors>
<title>LIPN-CORE: Semantic Text Similarity using n-grams, WordNet, Syntactic Analysis, ESA and Information Retrieval based Features.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity,</booktitle>
<pages>162--168</pages>
<location>Atlanta, Georgia, USA,</location>
<marker>Buscaldi, Le Roux, Flores, Popescu, 2013</marker>
<rawString>Davide Buscaldi, Joseph Le Roux, Jorge J. Garcia Flores, and Adrian Popescu. 2013. LIPN-CORE: Semantic Text Similarity using n-grams, WordNet, Syntactic Analysis, ESA and Information Retrieval based Features. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity, pages 162–168, Atlanta, Georgia, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Davide Buscaldi</author>
<author>Jorge J Garcia Flores</author>
<author>Joseph Le Roux</author>
<author>Nadi Tomeh</author>
<author>Belem Priego-Sanchez</author>
</authors>
<title>LIPN: Introducing a new Geographical Context Similarity Measure and a Statistical Similarity Measure based on the Bhattacharyya coefficient. In SemEval</title>
<date>2014</date>
<pages>400--405</pages>
<marker>Buscaldi, Flores, Le Roux, Tomeh, Priego-Sanchez, 2014</marker>
<rawString>Davide Buscaldi, Jorge J Garcia Flores, Joseph Le Roux, Nadi Tomeh, and Belem Priego-Sanchez. 2014. LIPN: Introducing a new Geographical Context Similarity Measure and a Statistical Similarity Measure based on the Bhattacharyya coefficient. In SemEval 2014, pages 400–405.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Davide Buscaldi</author>
</authors>
<title>Une mesure de similarit´e s´emantique bas´ee sur la Recherche d’Information.</title>
<date>2013</date>
<booktitle>In 5`eme Atelier Recherche d’Information SEmantique -RISE 2013,</booktitle>
<pages>81--91</pages>
<location>Lille, France,</location>
<contexts>
<context position="7795" citStr="Buscaldi, 2013" startWordPosition="1264" endWordPosition="1265">8), but they were used to produce two additional features (separate 2https://github.com/CNGLdlab/LORG-Release 3http://www.nist.gov/tac/data/data desc.html#AQUAINT-2 4http://metadata.berkeley.edu/NTCIR-GeoTime/ntcir-8- databases.php 5http://lucene.apache.org/core 6http://www.dbpedia.org/ max c2ECq s(c1,c2) E c1ECp ss(p, q) = E y (sp(d)−sq(d))2 dELpnLq max(sp(d),sq(d)) 133 from the basic IR one). The Spanish index was created using the Spanish QA@CLEF 2005 (agencia EFE1994-95, El Mundo 1994-95) and multiUN (Eisele and Chen, 2010) collections. The K value was set to 70 after a study detailed in (Buscaldi, 2013). Another IR-based feature was derived by the rank-biased overlap measure introduced by (Webber et al., 2010) which compares rankings without the need of weights. In total, we had 4 IR-based measures for English and 2 for Spanish. 2.5 N-gram Based Similarity This measure tries to capture the fact that similar sentences have similar n-grams, even if they are not placed in the same positions. The measure is based on the Clustered Keywords Positional Distance (CKPD) model proposed in (Buscaldi et al., 2009) for the passage retrieval task. The similarity between a text fragment p and another text </context>
</contexts>
<marker>Buscaldi, 2013</marker>
<rawString>Davide Buscaldi. 2013. Une mesure de similarit´e s´emantique bas´ee sur la Recherche d’Information. In 5`eme Atelier Recherche d’Information SEmantique -RISE 2013, pages 81–91, Lille, France, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Damien Dudognon</author>
<author>Gilles Hubert</author>
<author>Bachelin Jhonn Victorino Ralalason</author>
</authors>
<title>Proxig´en´ea : Une mesure de similarit´e conceptuelle.</title>
<date>2010</date>
<booktitle>In Proceedings of the Colloque Veille Strat´egique Scientifique et Technologique (VSST</booktitle>
<contexts>
<context position="3901" citStr="Dudognon et al., 2010" startWordPosition="609" endWordPosition="612"> possible meanings into account. Given Cp and Cq as the sets of concepts contained in sentences p and q, respectively, with |Cp |≥ |Cq|, the conceptual similarity between p and q is calcu132 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 132–137, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics lated as: |Cp| where s(c1, c2) is a conceptual similarity measure. Concept similarity can be calculated in different ways. We used a variation of the Wu-Palmer formula (Wu and Palmer, 1994) named “ProxiGenea3”, introduced by (Dudognon et al., 2010), which is inspired by the analogy between a family tree and the concept hierarchy in WordNet. The ProxiGenea3 measure is defined as: 1 s(c1, c2) = 1 + d(c1) + d(c2) − 2 · d(c0) where c0 is the most specific concept that is present both in the synset path of c1 and c2 (that is, the Least Common Subsumer or LCS). The function returning the depth of a concept is noted with d. 2.2 IC-based Similarity This measure has been proposed by (Mihalcea et al., 2006) as a corpus-based measure which uses Resnik’s Information Content (IC) and the JiangConrath (Jiang and Conrath, 1997) similarity metric. This</context>
</contexts>
<marker>Dudognon, Hubert, Ralalason, 2010</marker>
<rawString>Damien Dudognon, Gilles Hubert, and Bachelin Jhonn Victorino Ralalason. 2010. Proxig´en´ea : Une mesure de similarit´e conceptuelle. In Proceedings of the Colloque Veille Strat´egique Scientifique et Technologique (VSST 2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Eisele</author>
<author>Yu Chen</author>
</authors>
<title>MultiUN: A Multilingual Corpus from United Nation Documents.</title>
<date>2010</date>
<booktitle>Proceedings of the Seventh conference on International Language Resources and Evaluation,</booktitle>
<pages>2868--2872</pages>
<editor>In Daniel Tapias, Mike Rosner, Stelios Piperidis, Jan Odjik, Joseph Mariani, Bente Maegaard, Khalid Choukri, and Nicoletta Calzolari (Conference Chair), editors,</editor>
<contexts>
<context position="7713" citStr="Eisele and Chen, 2010" startWordPosition="1247" endWordPosition="1250"> BM25 similarity. We indexed also DBPedia6 abstracts and the UkWaC (Ferraresi et al., 2008), but they were used to produce two additional features (separate 2https://github.com/CNGLdlab/LORG-Release 3http://www.nist.gov/tac/data/data desc.html#AQUAINT-2 4http://metadata.berkeley.edu/NTCIR-GeoTime/ntcir-8- databases.php 5http://lucene.apache.org/core 6http://www.dbpedia.org/ max c2ECq s(c1,c2) E c1ECp ss(p, q) = E y (sp(d)−sq(d))2 dELpnLq max(sp(d),sq(d)) 133 from the basic IR one). The Spanish index was created using the Spanish QA@CLEF 2005 (agencia EFE1994-95, El Mundo 1994-95) and multiUN (Eisele and Chen, 2010) collections. The K value was set to 70 after a study detailed in (Buscaldi, 2013). Another IR-based feature was derived by the rank-biased overlap measure introduced by (Webber et al., 2010) which compares rankings without the need of weights. In total, we had 4 IR-based measures for English and 2 for Spanish. 2.5 N-gram Based Similarity This measure tries to capture the fact that similar sentences have similar n-grams, even if they are not placed in the same positions. The measure is based on the Clustered Keywords Positional Distance (CKPD) model proposed in (Buscaldi et al., 2009) for the </context>
</contexts>
<marker>Eisele, Chen, 2010</marker>
<rawString>Andreas Eisele and Yu Chen. 2010. MultiUN: A Multilingual Corpus from United Nation Documents. In Daniel Tapias, Mike Rosner, Stelios Piperidis, Jan Odjik, Joseph Mariani, Bente Maegaard, Khalid Choukri, and Nicoletta Calzolari (Conference Chair), editors, Proceedings of the Seventh conference on International Language Resources and Evaluation, pages 2868–2872, 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
</authors>
<title>Introducing and evaluating UkWaC, a very large web-derived corpus of English.</title>
<date>2008</date>
<booktitle>In Proceedings of the 4th Web as Corpus Workshop (WAC-4) Can we beat Google,</booktitle>
<pages>47--54</pages>
<contexts>
<context position="7182" citStr="Ferraresi et al., 2008" startWordPosition="1189" endWordPosition="1192"> be Lp = {dp1,..., dpK} and Lq = {dq1,... , dqK}, dxi C D the sets of the top K documents retrieved by S for texts p and q, respectively. Let us define sp(d) and sq(d) the scores assigned by S to a document d for the query p and q, respectively. Then, the similarity score is calculated as: simIR(p, q) = 1 |Lp n Lq| if |Lp n Lq |=� 0, 0 otherwise. For the participation in the English sub-task we indexed a collection composed by the AQUAINT23 and the English NTCIR-84 document collections, using the Lucene5 4.2 search engine with BM25 similarity. We indexed also DBPedia6 abstracts and the UkWaC (Ferraresi et al., 2008), but they were used to produce two additional features (separate 2https://github.com/CNGLdlab/LORG-Release 3http://www.nist.gov/tac/data/data desc.html#AQUAINT-2 4http://metadata.berkeley.edu/NTCIR-GeoTime/ntcir-8- databases.php 5http://lucene.apache.org/core 6http://www.dbpedia.org/ max c2ECq s(c1,c2) E c1ECp ss(p, q) = E y (sp(d)−sq(d))2 dELpnLq max(sp(d),sq(d)) 133 from the basic IR one). The Spanish index was created using the Spanish QA@CLEF 2005 (agencia EFE1994-95, El Mundo 1994-95) and multiUN (Eisele and Chen, 2010) collections. The K value was set to 70 after a study detailed in (Bu</context>
</contexts>
<marker>Ferraresi, Zanchetta, Baroni, Bernardini, 2008</marker>
<rawString>Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and Silvia Bernardini. 2008. Introducing and evaluating UkWaC, a very large web-derived corpus of English. In Proceedings of the 4th Web as Corpus Workshop (WAC-4) Can we beat Google, pages 47–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay J Jiang</author>
<author>David W Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In Proc. of the Int’l. Conf. on Research in Computational Linguistics,</booktitle>
<pages>pages</pages>
<contexts>
<context position="4477" citStr="Jiang and Conrath, 1997" startWordPosition="715" endWordPosition="718">xiGenea3”, introduced by (Dudognon et al., 2010), which is inspired by the analogy between a family tree and the concept hierarchy in WordNet. The ProxiGenea3 measure is defined as: 1 s(c1, c2) = 1 + d(c1) + d(c2) − 2 · d(c0) where c0 is the most specific concept that is present both in the synset path of c1 and c2 (that is, the Least Common Subsumer or LCS). The function returning the depth of a concept is noted with d. 2.2 IC-based Similarity This measure has been proposed by (Mihalcea et al., 2006) as a corpus-based measure which uses Resnik’s Information Content (IC) and the JiangConrath (Jiang and Conrath, 1997) similarity metric. This measure is more precise than the one introduced in the previous subsection because it takes into account also the importance of concepts and not only their relative position in the hierarchy. We refer to (Buscaldi et al., 2013) and (Mihalcea et al., 2006) for a detailed description of the measure. The idf weights for the words were calculated using the Google Web 1T (Brants and Franz, 2006) frequency counts, while the IC values used are those calculated by Ted Pedersen (Pedersen et al., 2004) on the British National Corpus1. 2.3 Syntactic Dependencies This measure trie</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>Jay J. Jiang and David W. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proc. of the Int’l. Conf. on Research in Computational Linguistics, pages 19–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Kononenko</author>
</authors>
<title>Estimating attributes: analysis and extensions of RELIEF.</title>
<date>1994</date>
<booktitle>In Machine Learning: ECML-94,</booktitle>
<pages>171--182</pages>
<contexts>
<context position="16251" citStr="Kononenko, 1994" startWordPosition="2685" endWordPosition="2686"> 0,7175 LIPN-SVM 0,5918 0,5718 0,7028 0,7985 0,8104 0,7070 Table 1: English results (Official runs). Wikipedia News Overall LIPN-RF 0,5637 0,5655 0,5649 LIPN-MLP 0,25257 0,5342 0,4401 LIPN-SVM 0,4194 0,4007 0,4069 Table 2: Spanish results (Official runs). glish dataset with the three regression approaches. The overall score points that the corpus selection was not beneficial. The most significant improvement was concentrated on the Answer-students dataset, in this case the performance felt 0,0588 points. We checked the contribution of each feature using the relief attribute selection measure (Kononenko, 1994) over the English training set. The best feature was the WordNet one, followed by Sultan and IC-based similarity. The worst features were Rankbiased Overlap followed by NE Overlap and the Geographic context similarity (however, apart from RBO, the other ones don’t have complete coverage). The other features have a statistically similar contribution. 5 Conclusions and Future Work The new learning models adopted were particularly effective, outperforming the Support Vector Regression algorithm that we used in our previous participation. The alignment measure based on Sultan was also very effecti</context>
</contexts>
<marker>Kononenko, 1994</marker>
<rawString>Igor Kononenko. 1994. Estimating attributes: analysis and extensions of RELIEF. In Machine Learning: ECML-94, pages 171–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Lamere</author>
<author>Philip Kwok</author>
<author>Evandro Gouvea</author>
<author>Bhiksha Raj</author>
<author>Rita Singh</author>
<author>William Walker</author>
<author>Manfred Warmuth</author>
<author>Peter Wolf</author>
</authors>
<title>The cmu sphinx-4 speech recognition system.</title>
<date>2003</date>
<booktitle>In IEEE Intl. Conf. on Acoustics, Speech and Signal Processing (ICASSP 2003), Hong Kong,</booktitle>
<pages>2--5</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="1197" citStr="Lamere et al., 2003" startWordPosition="165" endWordPosition="168"> the Task 2, Semantic Textual Similarity, at SemEval 2015, in both the English and Spanish sub-tasks. We included some features based on alignment measures and we tested different learning models, in particular Random Forests, which proved the best among those used in our participation. 1 Introduction Our participation in SemEval 2015 was focused on solving the technical problems that afflicted our previous participation (Buscaldi et al., 2014) and including additional features based on alignments, such as the Sultan similarity (Sultan et al., 2014b) and the measure available in CMU Sphinx-4 (Lamere et al., 2003) for speech recognition. We baptised the new system SOPA from the Spanish word for “soup”, since it uses a heterogeneous mix of features. Well aware of the importance that the training corpus and the regression algorithms have for the STS task, we used language models to select the most appropriate training corpus for a given text, and we explored some alternatives to the v-Support Vector Regression (v-SVR) (Sch¨olkopf et al., 1999) used in our previous participations, specifically the Multi-Layer Perceptron (Bishop and others, 1995) and Random Forest (Breiman, 2001) regression algorithms. The</context>
<context position="11286" citStr="Lamere et al., 2003" startWordPosition="1877" endWordPosition="1880">s based on the work of (Sultan et al., 2014b; Sultan et al., 2014a). The metric calculates a similarity score based on an alignment between two texts. It starts with an alignment between similar words, it proceeds to align similar name entities, to continue with words with similar content, to finally align stop words. In the case of content words, it proposes to use the syntactic context to identify similar words. At the end, the similarity is calculated as a harmonic mean between the ratios of align words from sentence one to sentence two, and from sentence two to sentence one. CMU Sphinx-4 (Lamere et al., 2003) is a speech recognition system that includes an alignment function that is used to align speech transcriptions with 7http://www.mpi-inf.mpg.de/yago-naga/yago/ h(x, P) Eni=1 wid(x, xmax) if x C P otherwise E xEGp max(|Gp|, |Gq|) ⎞ ⎠ ⎟ min yEGq d(x, y) 134 text. We took one of the sentence as a reference and the other one as a transcription and we used the output of the Sphinx alignment measure as a feature. 2.8 Other Measures In addition to the above text similarity measures, we used also the difference in size between sentences and the following measures: Cosine Cosine distance calculated bet</context>
</contexts>
<marker>Lamere, Kwok, Gouvea, Raj, Singh, Walker, Warmuth, Wolf, 2003</marker>
<rawString>Paul Lamere, Philip Kwok, Evandro Gouvea, Bhiksha Raj, Rita Singh, William Walker, Manfred Warmuth, and Peter Wolf. 2003. The cmu sphinx-4 speech recognition system. In IEEE Intl. Conf. on Acoustics, Speech and Signal Processing (ICASSP 2003), Hong Kong, pages 2–5. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Le Roux</author>
<author>Jennifer Foster</author>
<author>Joachim Wagner</author>
<author>Rasul Samad Zadeh Kaljahi</author>
<author>Anton Bryl</author>
</authors>
<date>2012</date>
<booktitle>DCUParis13 Systems for the SANCL 2012 Shared Task. In The NAACL 2012 First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL),</booktitle>
<pages>1--4</pages>
<location>Montr´eal, Canada.</location>
<marker>Le Roux, Foster, Wagner, Kaljahi, Bryl, 2012</marker>
<rawString>Joseph Le Roux, Jennifer Foster, Joachim Wagner, Rasul Samad Zadeh Kaljahi, and Anton Bryl. 2012. DCUParis13 Systems for the SANCL 2012 Shared Task. In The NAACL 2012 First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL), pages 1–4, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven J Bethard</author>
<author>David McClosky</author>
</authors>
<title>The Stanford CoreNLP Natural Language Processing Toolkit.</title>
<date>2014</date>
<booktitle>In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<pages>55--60</pages>
<contexts>
<context position="2605" citStr="Manning et al., 2014" startWordPosition="394" endWordPosition="397">aining corpus selection process are described in Section 3, and the results obtained by the system are detailed in Section 4. 2 Similarity Measures In this section we describe the measures used as features in our system. The total number of features used was 16 in English and 14 in Spanish. Since most measures have already been used in our previous participation, we provide only basic overview, referring the reader to the complete description in (Buscaldi et al., 2013) for further details. When POS tagging and NE recognition were required, we used the Stanford CoreNLP for English and Spanish (Manning et al., 2014). 2.1 WordNet-based Conceptual Similarity This measure has been introduced in order to measure similarities between concepts with respect to an ontology. The similarity is calculated as follows: first of all, words in sentences p and q are lemmatised and mapped to the related WordNet synsets. All noun synsets are put into the set of synsets associated to the sentence, Cp and Cq, respectively. If the synsets are in one of the other POS categories (verb, adjective, adverb) we look for their derivationally related forms in order to find a related noun synset: if there exists one, we put this syns</context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP Natural Language Processing Toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 55–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and knowledge-based measures of text semantic similarity.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st national conference on Artificial intelligence -</booktitle>
<volume>1</volume>
<pages>775--780</pages>
<contexts>
<context position="4359" citStr="Mihalcea et al., 2006" startWordPosition="697" endWordPosition="700">y can be calculated in different ways. We used a variation of the Wu-Palmer formula (Wu and Palmer, 1994) named “ProxiGenea3”, introduced by (Dudognon et al., 2010), which is inspired by the analogy between a family tree and the concept hierarchy in WordNet. The ProxiGenea3 measure is defined as: 1 s(c1, c2) = 1 + d(c1) + d(c2) − 2 · d(c0) where c0 is the most specific concept that is present both in the synset path of c1 and c2 (that is, the Least Common Subsumer or LCS). The function returning the depth of a concept is noted with d. 2.2 IC-based Similarity This measure has been proposed by (Mihalcea et al., 2006) as a corpus-based measure which uses Resnik’s Information Content (IC) and the JiangConrath (Jiang and Conrath, 1997) similarity metric. This measure is more precise than the one introduced in the previous subsection because it takes into account also the importance of concepts and not only their relative position in the hierarchy. We refer to (Buscaldi et al., 2013) and (Mihalcea et al., 2006) for a detailed description of the measure. The idf weights for the words were calculated using the Google Web 1T (Brants and Franz, 2006) frequency counts, while the IC values used are those calculated</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 2006. Corpus-based and knowledge-based measures of text semantic similarity. In Proceedings of the 21st national conference on Artificial intelligence - Volume 1, AAAI’06, pages 775–780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Siddharth Patwardhan</author>
<author>Jason Michelizzi</author>
</authors>
<title>Wordnet::similarity: measuring the relatedness of concepts.</title>
<date>2004</date>
<booktitle>In Demonstration Papers at HLT-NAACL 2004, HLT-NAACL–Demonstrations ’04,</booktitle>
<pages>38--41</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4999" citStr="Pedersen et al., 2004" startWordPosition="806" endWordPosition="809"> measure which uses Resnik’s Information Content (IC) and the JiangConrath (Jiang and Conrath, 1997) similarity metric. This measure is more precise than the one introduced in the previous subsection because it takes into account also the importance of concepts and not only their relative position in the hierarchy. We refer to (Buscaldi et al., 2013) and (Mihalcea et al., 2006) for a detailed description of the measure. The idf weights for the words were calculated using the Google Web 1T (Brants and Franz, 2006) frequency counts, while the IC values used are those calculated by Ted Pedersen (Pedersen et al., 2004) on the British National Corpus1. 2.3 Syntactic Dependencies This measure tries to capture the syntactic similarity between two sentences using dependencies. Previous experiments showed that converting constituents to dependencies still achieved best results on out-ofdomain texts (Le Roux et al., 2012), so we decided to use a 2-step architecture to obtain syntactic dependencies. First we parsed pairs of sentences with 1http://www.d.umn.edu/ tpederse/similarity.html the LORG parser2. Second we converted the resulting parse trees to Stanford dependencies. Given the sets of parsed dependencies Dp</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi. 2004. Wordnet::similarity: measuring the relatedness of concepts. In Demonstration Papers at HLT-NAACL 2004, HLT-NAACL–Demonstrations ’04, pages 38–41, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernhard Sch¨olkopf</author>
<author>Peter Bartlett</author>
<author>Alex Smola</author>
<author>Robert Williamson</author>
</authors>
<title>Shrinking the tube: a new support vector regression algorithm.</title>
<date>1999</date>
<booktitle>In Proceedings of the 1998 conference on Advances in neural information processing systems II,</booktitle>
<pages>330--336</pages>
<location>Cambridge, MA, USA.</location>
<marker>Sch¨olkopf, Bartlett, Smola, Williamson, 1999</marker>
<rawString>Bernhard Sch¨olkopf, Peter Bartlett, Alex Smola, and Robert Williamson. 1999. Shrinking the tube: a new support vector regression algorithm. In Proceedings of the 1998 conference on Advances in neural information processing systems II, pages 330–336, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Md Arafat Sultan</author>
<author>Steven Bethard</author>
<author>Tamara Sumner</author>
</authors>
<title>Back to Basics for Monolingual Alignment: Exploiting Word Similarity and Contextual Evidence.</title>
<date>2014</date>
<contexts>
<context position="1131" citStr="Sultan et al., 2014" startWordPosition="154" endWordPosition="157">ct This paper describes the system used by the LIPN-IIMAS team in the Task 2, Semantic Textual Similarity, at SemEval 2015, in both the English and Spanish sub-tasks. We included some features based on alignment measures and we tested different learning models, in particular Random Forests, which proved the best among those used in our participation. 1 Introduction Our participation in SemEval 2015 was focused on solving the technical problems that afflicted our previous participation (Buscaldi et al., 2014) and including additional features based on alignments, such as the Sultan similarity (Sultan et al., 2014b) and the measure available in CMU Sphinx-4 (Lamere et al., 2003) for speech recognition. We baptised the new system SOPA from the Spanish word for “soup”, since it uses a heterogeneous mix of features. Well aware of the importance that the training corpus and the regression algorithms have for the STS task, we used language models to select the most appropriate training corpus for a given text, and we explored some alternatives to the v-Support Vector Regression (v-SVR) (Sch¨olkopf et al., 1999) used in our previous participations, specifically the Multi-Layer Perceptron (Bishop and others, </context>
<context position="10709" citStr="Sultan et al., 2014" startWordPosition="1775" endWordPosition="1779"> sentence. For instance, “Merkel” is mapped to “Germany”. Given Gp and Gq the sets of places found in sentences p and q, respectively, the geographical context similarity is calculated as follows: ⎛ ⎜ simgeo(p, q) = 1−logK ⎝1 + Where d(x, y) is the spherical distance in Km. between x and y, and K is a normalization factor set to 10000 Km. to obtain similarity values between 1 and 0. If no toponyms or geographically groundable entities are found in either sentences, then the geographic context similarity is set to 1. 2.7 Word Alignment Similarity This similarity metric is based on the work of (Sultan et al., 2014b; Sultan et al., 2014a). The metric calculates a similarity score based on an alignment between two texts. It starts with an alignment between similar words, it proceeds to align similar name entities, to continue with words with similar content, to finally align stop words. In the case of content words, it proposes to use the syntactic context to identify similar words. At the end, the similarity is calculated as a harmonic mean between the ratios of align words from sentence one to sentence two, and from sentence two to sentence one. CMU Sphinx-4 (Lamere et al., 2003) is a speech recognitio</context>
</contexts>
<marker>Sultan, Bethard, Sumner, 2014</marker>
<rawString>Md Arafat Sultan, Steven Bethard, and Tamara Sumner. 2014a. Back to Basics for Monolingual Alignment: Exploiting Word Similarity and Contextual Evidence.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Md Arafat Sultan</author>
<author>Steven Bethard</author>
<author>Tamara Sumner</author>
</authors>
<booktitle>2014b. DLS@ CU: Sentence Similarity from Word Alignment. SemEval 2014,</booktitle>
<pages>241</pages>
<marker>Sultan, Bethard, Sumner, </marker>
<rawString>Md Arafat Sultan, Steven Bethard, and Tamara Sumner. 2014b. DLS@ CU: Sentence Similarity from Word Alignment. SemEval 2014, page 241.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Webber</author>
<author>Alistair Moffat</author>
<author>Justin Zobel</author>
</authors>
<title>A similarity measure for indefinite rankings.</title>
<date>2010</date>
<journal>ACM Transactions on Information Systems (TOIS),</journal>
<volume>28</volume>
<issue>4</issue>
<contexts>
<context position="7904" citStr="Webber et al., 2010" startWordPosition="1278" endWordPosition="1281">lease 3http://www.nist.gov/tac/data/data desc.html#AQUAINT-2 4http://metadata.berkeley.edu/NTCIR-GeoTime/ntcir-8- databases.php 5http://lucene.apache.org/core 6http://www.dbpedia.org/ max c2ECq s(c1,c2) E c1ECp ss(p, q) = E y (sp(d)−sq(d))2 dELpnLq max(sp(d),sq(d)) 133 from the basic IR one). The Spanish index was created using the Spanish QA@CLEF 2005 (agencia EFE1994-95, El Mundo 1994-95) and multiUN (Eisele and Chen, 2010) collections. The K value was set to 70 after a study detailed in (Buscaldi, 2013). Another IR-based feature was derived by the rank-biased overlap measure introduced by (Webber et al., 2010) which compares rankings without the need of weights. In total, we had 4 IR-based measures for English and 2 for Spanish. 2.5 N-gram Based Similarity This measure tries to capture the fact that similar sentences have similar n-grams, even if they are not placed in the same positions. The measure is based on the Clustered Keywords Positional Distance (CKPD) model proposed in (Buscaldi et al., 2009) for the passage retrieval task. The similarity between a text fragment p and another text fragment q is calculated as: �simngrams(p,q) = dxEQ Where P is the set of the heaviest n-grams in p where all</context>
</contexts>
<marker>Webber, Moffat, Zobel, 2010</marker>
<rawString>William Webber, Alistair Moffat, and Justin Zobel. 2010. A similarity measure for indefinite rankings. ACM Transactions on Information Systems (TOIS), 28(4):20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhibiao Wu</author>
<author>Martha Palmer</author>
</authors>
<title>Verbs semantics and lexical selection.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd annual meeting on Association for Computational Linguistics, ACL ’94,</booktitle>
<pages>133--138</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3842" citStr="Wu and Palmer, 1994" startWordPosition="601" endWordPosition="604"> No disambiguation process is carried out, so we take all possible meanings into account. Given Cp and Cq as the sets of concepts contained in sentences p and q, respectively, with |Cp |≥ |Cq|, the conceptual similarity between p and q is calcu132 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 132–137, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics lated as: |Cp| where s(c1, c2) is a conceptual similarity measure. Concept similarity can be calculated in different ways. We used a variation of the Wu-Palmer formula (Wu and Palmer, 1994) named “ProxiGenea3”, introduced by (Dudognon et al., 2010), which is inspired by the analogy between a family tree and the concept hierarchy in WordNet. The ProxiGenea3 measure is defined as: 1 s(c1, c2) = 1 + d(c1) + d(c2) − 2 · d(c0) where c0 is the most specific concept that is present both in the synset path of c1 and c2 (that is, the Least Common Subsumer or LCS). The function returning the depth of a concept is noted with d. 2.2 IC-based Similarity This measure has been proposed by (Mihalcea et al., 2006) as a corpus-based measure which uses Resnik’s Information Content (IC) and the Jia</context>
</contexts>
<marker>Wu, Palmer, 1994</marker>
<rawString>Zhibiao Wu and Martha Palmer. 1994. Verbs semantics and lexical selection. In Proceedings of the 32nd annual meeting on Association for Computational Linguistics, ACL ’94, pages 133–138, Stroudsburg, PA, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>