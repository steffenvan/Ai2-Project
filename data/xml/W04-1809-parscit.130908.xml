<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010173">
<note confidence="0.962904">
CompuTerm 2004 Poster Session - 3rd International Workshop on Computational Terminology 71
</note>
<title confidence="0.952289">
Term Extraction from Korean Corpora via Japanese
</title>
<author confidence="0.99415">
Atsushi Fujii, Tetsuya Ishikawa Jong-Hyeok Lee
</author>
<affiliation confidence="0.997401666666667">
Graduate School of Library, Division of Electrical and
Information and Media Studies Computer Engineering,
University of Tsukuba Pohang University of Science and Technology,
</affiliation>
<address confidence="0.806514666666667">
1-2 Kasuga, Tsukuba Advanced Information Technology Research Center
305-8550, Japan San 31 Hyoja-dong Nam-gu,
{fujii,ishikawa}@slis.tsukuba.ac.jp Pohang 790-784, Republic of Korea
</address>
<email confidence="0.996288">
jhlee@postech.ac.kr
</email>
<sectionHeader confidence="0.99559" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999909666666667">
This paper proposes a method to extract foreign
words, such as technical terms and proper nouns,
from Korean corpora and produce a Japanese-
Korean bilingual dictionary. Specific words have
been imported into multiple countries simultane-
ously, if they are influential across cultures. The
pronunciation of a source word is similar in different
languages. Our method extracts words in Korean
corpora that are phonetically similar to Katakana
words, which can easily be identified in Japanese cor-
pora. We also show the effectiveness of our method
by means of experiments.
</bodyText>
<sectionHeader confidence="0.998798" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999981396226415">
Reflecting the rapid growth in science and tech-
nology, new words have progressively been created.
However, due to the limitation of manual compila-
tion, new words are often out-of-dictionary words
and decrease the quality of human language tech-
nology, such as natural language processing, infor-
mation retrieval, machine translation, and speech
recognition. To resolve this problem, a number
of automatic methods to extract monolingual and
bilingual lexicons from corpora have been proposed
for various languages.
In this paper, we focus on extracting foreign words
(or loanwords) in Korean. Technical terms and
proper nouns are often imported from foreign lan-
guages and are spelled out (or transliterated) by the
Korean alphabet system called Hangul. The similar
trend can be observable in Japanese and Chinese. In
Japanese, foreign words are spelled out by its special
phonetic alphabet (or phonogram) called Katakana.
Thus, foreign words can be extracted from Japanese
corpora with a high accuracy, because the Katakana
characters are seldom used to describe the conven-
tional Japanese words, excepting proper nouns.
However, extracting foreign words from Korean
corpora is more difficult, because in Korean both
the conventional and foreign words are written with
Hangul characters. This problem remains a chal-
lenging issue in computational linguistic research.
It is often the case that specific words have been
imported into multiple countries simultaneously, be-
cause the source words (or concepts) are usually in-
fluential across cultures. Thus, it is feasible that a
large number of foreign words in Korean can also be
foreign words in Japanese.
In addition, the foreign words in Korean and
Japanese corresponding to the same source word are
phonetically similar. For example, the English word
“system” has been imported into both Japanese and
Korean. The romanized words are /sisutemu/ and
/siseutem/ in both countries, respectively.
Motivated by these assumptions, we propose a
method to extract foreign words in Korean corpora
by means of Japanese. In brief, our method per-
forms as follows. First, foreign words in Japanese
are collected, for which Katakana words in corpora
and existing lexicons can be used. Second, from Ko-
rean corpora the words that are phonetically similar
to Katakana words are extracted. Finally, extracted
Korean words are compiled in a lexicon with the cor-
responding Japanese words.
In summary, our method can extract foreign words
in Korean and produce a Japanese-Korean bilingual
lexicon in a single framework.
</bodyText>
<sectionHeader confidence="0.993611" genericHeader="introduction">
2 Methodology
</sectionHeader>
<subsectionHeader confidence="0.955065">
2.1 Overview
</subsectionHeader>
<bodyText confidence="0.985026195652174">
Figure 1 exemplifies our extraction method, which
produces a Japanese-Korean bilingual lexicon using
a Korean corpus and Japanese corpus and/or lexi-
con. The Japanese and Korean corpora do not have
to be parallel or comparable. However, it is desir-
able that both corpora are associated with the same
domain. For the Japanese resource, the corpus and
lexicon can alternatively be used or can be used to-
gether. Note that compiling Japanese monolingual
lexicon is less expensive than that for a bilingual lex-
icon. In addition, new Katakana words can easily be
extracted from a number of on-line resources, such
as the World Wide Web. Thus, the use of Japanese
lexicons does not decrease the utility of our method.
First, we collect Katakana words from Japanese
resources. This can systematically be performed by
means of a Japanese character code, such as EUC-
JP and SJIS.
Second, we represent the Korean corpus and
Japanese Katakana words by the Roman alphabet
(i.e., romanization), so that the phonetic similarity
can easily be computed. However, we use different
romanization methods for Japanese and Korean.
72 CompuTerm 2004 Poster Session - 3rd International Workshop on Computational Terminology
Third, we extract candidates of foreign words
from the romanized Korean corpus. An alternative
method is to first perform morphological analysis
on the corpus, extract candidate words based on
morphemes and parts-of-speech, and romanize the
extracted words. Our general model does not con-
strain as to which method should be used in the
third step. However, because the accuracy of anal-
ysis often decreases for new words to be extracted,
we experimentally adopt the former method.
Finally, we compute the phonetic similarity be-
tween each combination of the romanized Hangul
and Katakana words, and select the combinations
whose score is above a predefined threshold. As a
result, we can obtain a Japanese-Korean bilingual
lexicon consisting of foreign words.
It may be argued that English lexicons or cor-
pora can be used as source information, instead of
Japanese resources. However, because not all En-
glish words have been imported into Korean, the
extraction accuracy will decrease due to extraneous
words.
</bodyText>
<figureCaption confidence="0.997087">
Figure 1: Overview of our extraction method.
</figureCaption>
<subsectionHeader confidence="0.999839">
2.2 Romanizing Japanese
</subsectionHeader>
<bodyText confidence="0.999969066666667">
Because the number of phones consisting of Japanese
Katakana characters is limited, we manually pro-
duced the correspondence between each phone
and its Roman representation. The numbers of
Katakana characters and combined phones are 73
and 109, respectively. We also defined a symbol to
represent a long vowel. In Japanese, the Hepbern
and Kunrei systems are commonly used for roman-
ization purposes. We use the Hepburn system, be-
cause its representation is similar to that in Korean,
compared with the Kunrei system.
However, specific Japanese phones, such as /ti/,
do not exist in Korean. Thus, to adapt the Hepburn
system to Korean, /ti/ and /tu/ are converted to
/chi/ and /chu/, respectively.
</bodyText>
<subsectionHeader confidence="0.999759">
2.3 Romanizing Korean
</subsectionHeader>
<bodyText confidence="0.999983111111111">
The number of Korean Hangul characters is much
greater than that of Japanese Katakana characters.
Each Hangul character is a combination of more
than one consonant. The pronunciation of each char-
acter is determined by its component consonants.
In Korean, there are types of consonant, i.e., the
first consonant, vowel, and last consonant. The
numbers of these consonants are 19, 21, and 27, re-
spectively. The last consonant is optional. Thus, the
number of combined characters is 11,172. However,
to transliterate imported words, the official guide-
line suggests that only seven consonants be used as
the last consonant. In EUC-KR, which is a stan-
dard coding system for Korean text, 2,350 common
characters are coded independent of the pronunci-
ation. Therefore, if we target corpora represented
by EUC-KR, each of the 2,350 characters has to be
corresponded to its Roman representation.
We use Unicode, in which Hangul characters are
sorted according to the pronunciation. Figure 2 de-
picts a fragment of the Unicode table for Korean,
in which each line corresponds to a combination
of the first consonant and vowel and each column
corresponds to the last consonant. The number of
columns is 28, i.e., the number of the last consonants
and the case in which the last consonant is not used.
From this figure, the following rules can be found:
</bodyText>
<listItem confidence="0.9980796">
• the first consonant changes every 21 lines, which
corresponds to the number of vowels,
• the vowel changes every line (i.e., 28 characters)
and repeats every 21 lines,
• the last consonant changes every column.
</listItem>
<bodyText confidence="0.58048625">
Based on these rules, each character and its pro-
nunciation can be identified by the three consonant
types. Thus, we manually corresponded only the 68
consonants to Roman alphabets.
</bodyText>
<figureCaption confidence="0.960902">
Figure 2: A fragment of the Unicode table for Ko-
rean Hangul characters.
</figureCaption>
<bodyText confidence="0.999149">
We use the official romanization system for Ko-
rean, but specific Korean phones are adapted to
Japanese. For example, /j/ and /l/ are converted
to /z/ and /r/, respectively.
It should be noted that the adaptation is not in-
vertible and thus is needed for both J-to-K and K-
to-J directions.
</bodyText>
<note confidence="0.47459">
CompuTerm 2004 Poster Session - 3rd International Workshop on Computational Terminology 73
</note>
<bodyText confidence="0.999869">
For example, the English word “cheese”, which
has been imported to both Korean and Japanese as
a foreign word, is romanized as /chiseu/ in Korean
and /ti:zu/ in Japanese. Here, /:/ is the symbol
representing a Japanese long vowel. Using the adap-
tation, these expressions are converted to /chizu/
and /chi:zu/, respectively, which look more similar
to each other, compared with the original strings.
</bodyText>
<subsectionHeader confidence="0.994936">
2.4 Extracting term candidates from
Korean corpora
</subsectionHeader>
<bodyText confidence="0.999937">
To extract candidates of foreign words from a Ko-
rean corpus, we first extract phrases. This can be
performed systematically, because Korean sentences
are segmented on a phrase-by-phrase basis.
Second, because foreign words are usually nouns,
we use hand-crafted rules to remove post-position
suffixes (e.g., Josa) and extract nouns from phrases.
Third, we discard nouns including the last con-
sonants that are not recommended for translitera-
tion purposes in the official guideline. Although the
guideline suggests other rules for transliteration, ex-
isting foreign words in Korean are not necessarily
regulated by these rules.
Finally, we consult a dictionary to discard exist-
ing Korean words, because our purpose is to extract
new words. For this purpose, we experimentally
use the dictionary for SuperMorph-K morphologi-
cal analyzers, which includes approximately 50,000
Korean words.
</bodyText>
<subsectionHeader confidence="0.998716">
2.5 Computing Similarity
</subsectionHeader>
<bodyText confidence="0.999887173913043">
Given romanized Japanese and Korean words, we
compute the similarity between the two strings and
select the pairs associated with the score above a
threshold as translations. We use a DP (dynamic
programming) matching method to identify the
number of differences (i.e., insertion, deletion, and
substitution) between two strings, on a alphabet-
by-alphabet basis.
In principle, if two strings are associated with a
smaller number of differences, the similarity between
them becomes greater. For this purpose, a Dice-style
coefficient can be used.
However, while the use of consonants in translit-
eration is usually the same across languages, the
use of vowels can vary significantly depending on
the language. For example, the English word “sys-
tem” is romanized as /sisutemu/ and /siseutem/
in Japanese and Korean, respectively. Thus, the dif-
ferences in consonants between two strings should
be penalized more than the differences in vowels.
In view of the above discussion, we compute the
similarity between two romanized words by Equa-
tion (1).
</bodyText>
<equation confidence="0.995927">
2 · (α · dc + dv) (1)
α · c + v
</equation>
<bodyText confidence="0.9983665">
Here, dc and dv denote the numbers of differences
in consonants and vowels, respectively, and α is a
</bodyText>
<footnote confidence="0.925642">
1http://www.omronsoft.com/
</footnote>
<bodyText confidence="0.9988078">
parametric constant used to control the importance
of the consonants. We experimentally set α = 2. In
addition, c and v denote the numbers of all conso-
nants and vowels in the two strings. The similarity
ranges from 0 to 1.
</bodyText>
<sectionHeader confidence="0.999747" genericHeader="background">
3 Experimentation
</sectionHeader>
<subsectionHeader confidence="0.999934">
3.1 Evaluating Extraction Accuracy
</subsectionHeader>
<bodyText confidence="0.99994158">
We collected 111,166 Katakana words (word types)
from multiple Japanese lexicons, most of which were
technical term dictionaries.
We used the Korean document set in the NTCIR-3
Cross-lingual Information Retrieval test collection2.
This document set consists of 66,146 newspaper ar-
ticles of Korean Economic Daily published in 1994.
We randomly selected 50 newspaper articles and
used them for our experiment. We asked a grad-
uate student excluding the authors of this paper to
identify foreign words in the target text. As a result,
124 foreign word types (205 word tokens) were iden-
tified, which were less than we had expected. This
was partially due to the fact that newspaper articles
generally do not contain a large number of foreign
words, compared with technical publications.
We manually classified the extracted words and
used only the words that were imported to both
Japan and Korea from other languages. We dis-
carded foreign words in Korea imported from Japan,
because these words were often spelled out by non-
Katakana characters, such as Kanji (Chinese charac-
ter). A sample of these words includes “Tokyo (the
capital of Japan)”, “Heisei (the current Japanese
era name)”, and “enko (personal connection)”. In
addition, we discarded the foreign proper nouns for
which the human subject was not able to identify
the source word. As a result, we obtained 67 target
word types. Examples of original English words for
these words are as follows:
digital, group, dollar, re-engineering, line,
polyester, Asia, service, class, card, com-
puter, brand, liter, hotel.
Thus, our method can potentially be applied to
roughly a half of the foreign words in Korean text.
We used the Japanese words to extract plausi-
ble foreign words from the target Korean corpus.
We first romanized the corpus and extracted nouns
by removing post-position suffixes. As a result, we
obtained 3,106 words including all the 67 target
words. By discarding the words in the dictionary
for SuperMorph-K, 958 words including 59 target
words were remained.
For each of the remaining 958 words, we computed
the similarity between each of the 111,166 Japanese
words. For evaluation purposes, we varied a thresh-
old for the similarity and investigated the relation
between precision and recall. Recall is the ratio
of the number of target foreign words extracted by
our method and the total number of target foreign
</bodyText>
<footnote confidence="0.93882">
2http://research.nii.ac.jp/ntcir/index-en.html
1 −
</footnote>
<note confidence="0.817419">
74 CompuTerm 2004 Poster Session - 3rd International Workshop on Computational Terminology
</note>
<bodyText confidence="0.998717">
words. Precision is the ratio of the number of target
foreign words extracted by our method and the total
number of words obtained by our method.
Table 1 shows the precision and recall for differ-
ent methods. While we varied a threshold of a sim-
ilarity, we also varied the number of Korean words
corresponded to a single Katakana word (N). By
decreasing the value of the threshold and increasing
the number of words extracted, the recall can be im-
proved but the precision decreases. In Table 1, the
precision and recall are in an extreme trade-off rela-
tion. For example, when the recall was 69.5%, the
precision was only 1.2%.
We manually analyzed the words that were not ex-
tracted by our method. Out of the 59 target words,
12 compound words consisting of both conventional
and foreign words were not extracted. However,
our method extracted compound words consisting
of only foreign words. In addition, the three words
that did not have counterparts in the input Japanese
words were not extracted.
</bodyText>
<tableCaption confidence="0.979617">
Table 1: Precision/Recall for term extraction.
</tableCaption>
<table confidence="0.85342975">
Threshold for similarity
&gt;0.9 &gt;0.7 &gt;0.5
N=1 50.0/8.5 12.7/40.7 4.1/47.5
N=10 50.0/8.5 7.4/47.5 1.2/69.5
</table>
<subsectionHeader confidence="0.985289">
3.2 Application-Oriented Evaluation
</subsectionHeader>
<bodyText confidence="0.999992111111111">
During the first experiment, we determined a specific
threshold value for the similarity between Katakana
and Hangul words and selected the pairs whose sim-
ilarity was above the threshold. As a result, we ob-
tained 667 Korean words, which were used to en-
hance the dictionary for the SuperMorph-K morpho-
logical analyzer.
We performed morphological analysis on the 50
articles used in the first experiment, which included
1,213 sentences and 9,557 word tokens. We also in-
vestigated the degree to which the analytical accu-
racy is improved by means of the additional dictio-
nary. Here, accuracy is the ratio of the number of
correct word segmentations and the total segmenta-
tions generated by SuperMorph-K. The same human
subject as in the first experiment identified the cor-
rect word segmentations for the input articles.
First, we focused on the accuracy of segmenting
foreign words. The accuracy was improved from
75.8% to 79.8% by means of the additional dictio-
nary. The accuracy for all words was changed from
94.6% to 94.8% by the additional dictionary.
In summary, the additional dictionary was effec-
tive for analyzing foreign words and was not asso-
ciated with side effect for the overall accuracy. At
the same time, we concede that we need larger-scale
experiments to draw firmer conclusions.
</bodyText>
<sectionHeader confidence="0.999926" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.99998635">
A number of corpus-based methods to extract bilin-
gual lexicons have been proposed (Smadja et al.,
1996). In general, these methods use statistics ob-
tained from a parallel or comparable bilingual corpus
and extract word or phrase pairs that are strongly
associated with each other. However, our method
uses a monolingual Korean corpus and a Japanese
lexicon independent of the corpus, which can easily
be obtained, compared with parallel or comparable
bilingual corpora.
Jeong et al. (1999) and Oh and Choi (2001) in-
dependently explored a statistical approach to de-
tect foreign words in Korean text. Although the de-
tection accuracy is reasonably high, these methods
require a training corpus in which conventional and
foreign words are annotated. Our approach does not
require annotated corpora, but the detection accu-
racy is not high enough as shown in Section 3.1. A
combination of both approaches is expected to com-
pensate the drawbacks of each approach.
</bodyText>
<sectionHeader confidence="0.999113" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999993454545455">
We proposed a method to extract foreign words,
such as technical terms and proper nouns, from Ko-
rean corpora and produce a Japanese-Korean bilin-
gual dictionary. Specific words, which have been
imported into multiple countries, are usually spelled
out by special phonetic alphabets, such as Katakana
in Japanese and Hangul in Korean.
Because extracting foreign words spelled out by
Katakana in Japanese lexicons and corpora can be
performed with a high accuracy, our method ex-
tracts words in Korean corpora that are phonetically
similar to Japanese Katakana words. Our method
does not require parallel or comparable bilingual cor-
pora and human annotation for these corpora.
We also performed experiments in which we ex-
tracted foreign words from Korean newspaper arti-
cles and used the resultant dictionary for morpho-
logical analysis. We found that our method did not
correctly extract compound Korean words consist-
ing of both conventional and foreign words. Future
work includes larger-scale experiments to further in-
vestigate the effectiveness of our method.
</bodyText>
<sectionHeader confidence="0.999464" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999480692307692">
Kil Soon Jeong, Sung Hyon Myaeng, Jae Sung Lee,
and Key-Sun Choi. 1999. Automatic identification
and back-transliteration of foreign words for informa-
tion retrieval. Information Processing &amp; Management,
35:523–540.
Jong-Hoon Oh and Key sun Choi. 2001. Automatic
extraction of transliterated foreign words using hid-
den markov model. In Proceedings of ICCPOL-2001,
pages 433–438.
Frank Smadja, Kathleen R. McKeown, and Vasileios
Hatzivassiloglou. 1996. Translating collocations for
bilingual lexicons: A statistical approach. Computa-
tional Linguistics, 22(1):1–38.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.422358">
<title confidence="0.855244">CompuTerm 2004 Poster Session - 3rd International Workshop on Computational Terminology 71 Term Extraction from Korean Corpora via Japanese</title>
<author confidence="0.990986">Atsushi Fujii</author>
<author confidence="0.990986">Tetsuya Ishikawa Jong-Hyeok Lee</author>
<affiliation confidence="0.918352">Graduate School of Library, Division of Electrical and Information and Media Studies Computer Engineering, University of Tsukuba Pohang University of Science and 1-2 Kasuga, Tsukuba Advanced Information Technology Research Center</affiliation>
<address confidence="0.9194875">305-8550, Japan San 31 Hyoja-dong Nam-gu, Pohang 790-784, Republic of Korea</address>
<email confidence="0.983795">jhlee@postech.ac.kr</email>
<abstract confidence="0.999019923076923">This paper proposes a method to extract foreign words, such as technical terms and proper nouns, from Korean corpora and produce a Japanese- Korean bilingual dictionary. Specific words have been imported into multiple countries simultaneously, if they are influential across cultures. The pronunciation of a source word is similar in different languages. Our method extracts words in Korean corpora that are phonetically similar to Katakana words, which can easily be identified in Japanese corpora. We also show the effectiveness of our method by means of experiments.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kil Soon Jeong</author>
<author>Sung Hyon Myaeng</author>
<author>Jae Sung Lee</author>
<author>Key-Sun Choi</author>
</authors>
<title>Automatic identification and back-transliteration of foreign words for information retrieval.</title>
<date>1999</date>
<booktitle>Information Processing &amp; Management,</booktitle>
<pages>35--523</pages>
<contexts>
<context position="17250" citStr="Jeong et al. (1999)" startWordPosition="2722" endWordPosition="2725">e overall accuracy. At the same time, we concede that we need larger-scale experiments to draw firmer conclusions. 4 Related Work A number of corpus-based methods to extract bilingual lexicons have been proposed (Smadja et al., 1996). In general, these methods use statistics obtained from a parallel or comparable bilingual corpus and extract word or phrase pairs that are strongly associated with each other. However, our method uses a monolingual Korean corpus and a Japanese lexicon independent of the corpus, which can easily be obtained, compared with parallel or comparable bilingual corpora. Jeong et al. (1999) and Oh and Choi (2001) independently explored a statistical approach to detect foreign words in Korean text. Although the detection accuracy is reasonably high, these methods require a training corpus in which conventional and foreign words are annotated. Our approach does not require annotated corpora, but the detection accuracy is not high enough as shown in Section 3.1. A combination of both approaches is expected to compensate the drawbacks of each approach. 5 Conclusion We proposed a method to extract foreign words, such as technical terms and proper nouns, from Korean corpora and produc</context>
</contexts>
<marker>Jeong, Myaeng, Lee, Choi, 1999</marker>
<rawString>Kil Soon Jeong, Sung Hyon Myaeng, Jae Sung Lee, and Key-Sun Choi. 1999. Automatic identification and back-transliteration of foreign words for information retrieval. Information Processing &amp; Management, 35:523–540.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jong-Hoon Oh</author>
<author>Key sun Choi</author>
</authors>
<title>Automatic extraction of transliterated foreign words using hidden markov model.</title>
<date>2001</date>
<booktitle>In Proceedings of ICCPOL-2001,</booktitle>
<pages>433--438</pages>
<contexts>
<context position="17273" citStr="Oh and Choi (2001)" startWordPosition="2727" endWordPosition="2730">he same time, we concede that we need larger-scale experiments to draw firmer conclusions. 4 Related Work A number of corpus-based methods to extract bilingual lexicons have been proposed (Smadja et al., 1996). In general, these methods use statistics obtained from a parallel or comparable bilingual corpus and extract word or phrase pairs that are strongly associated with each other. However, our method uses a monolingual Korean corpus and a Japanese lexicon independent of the corpus, which can easily be obtained, compared with parallel or comparable bilingual corpora. Jeong et al. (1999) and Oh and Choi (2001) independently explored a statistical approach to detect foreign words in Korean text. Although the detection accuracy is reasonably high, these methods require a training corpus in which conventional and foreign words are annotated. Our approach does not require annotated corpora, but the detection accuracy is not high enough as shown in Section 3.1. A combination of both approaches is expected to compensate the drawbacks of each approach. 5 Conclusion We proposed a method to extract foreign words, such as technical terms and proper nouns, from Korean corpora and produce a Japanese-Korean bil</context>
</contexts>
<marker>Oh, Choi, 2001</marker>
<rawString>Jong-Hoon Oh and Key sun Choi. 2001. Automatic extraction of transliterated foreign words using hidden markov model. In Proceedings of ICCPOL-2001, pages 433–438.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Smadja</author>
<author>Kathleen R McKeown</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Translating collocations for bilingual lexicons: A statistical approach.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="16864" citStr="Smadja et al., 1996" startWordPosition="2662" endWordPosition="2665">for the input articles. First, we focused on the accuracy of segmenting foreign words. The accuracy was improved from 75.8% to 79.8% by means of the additional dictionary. The accuracy for all words was changed from 94.6% to 94.8% by the additional dictionary. In summary, the additional dictionary was effective for analyzing foreign words and was not associated with side effect for the overall accuracy. At the same time, we concede that we need larger-scale experiments to draw firmer conclusions. 4 Related Work A number of corpus-based methods to extract bilingual lexicons have been proposed (Smadja et al., 1996). In general, these methods use statistics obtained from a parallel or comparable bilingual corpus and extract word or phrase pairs that are strongly associated with each other. However, our method uses a monolingual Korean corpus and a Japanese lexicon independent of the corpus, which can easily be obtained, compared with parallel or comparable bilingual corpora. Jeong et al. (1999) and Oh and Choi (2001) independently explored a statistical approach to detect foreign words in Korean text. Although the detection accuracy is reasonably high, these methods require a training corpus in which con</context>
</contexts>
<marker>Smadja, McKeown, Hatzivassiloglou, 1996</marker>
<rawString>Frank Smadja, Kathleen R. McKeown, and Vasileios Hatzivassiloglou. 1996. Translating collocations for bilingual lexicons: A statistical approach. Computational Linguistics, 22(1):1–38.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>