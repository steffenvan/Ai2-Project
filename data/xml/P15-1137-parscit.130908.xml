<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9984695">
Learning Anaphoricity and Antecedent Ranking Features
for Coreference Resolution
</title>
<author confidence="0.998792">
Sam Wiseman1 Alexander M. Rush1,2 Stuart M. Shieber1 Jason Weston2
</author>
<affiliation confidence="0.993676">
1School of Engineering and Applied Sciences 2Facebook AI Research
Harvard University New York, NY, USA
</affiliation>
<address confidence="0.962711">
Cambridge, MA, USA jase@fb.com
</address>
<email confidence="0.999093">
{swiseman,srush,shieber}@seas.harvard.edu
</email>
<sectionHeader confidence="0.984924" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99996825">
We introduce a simple, non-linear
mention-ranking model for coreference
resolution that attempts to learn distinct
feature representations for anaphoricity
detection and antecedent ranking, which
we encourage by pre-training on a pair
of corresponding subtasks. Although we
use only simple, unconjoined features, the
model is able to learn useful representa-
tions, and we report the best overall score
on the CoNLL 2012 English test set to
date.
</bodyText>
<sectionHeader confidence="0.992546" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999944112903226">
One of the major challenges associated with re-
solving coreference is that in typical documents
the number of mentions (syntactic units capable
of referring or being referred to) that are non-
anaphoric – that is, that are not coreferent with
any previous mention – far exceeds the number
of mentions that are anaphoric (Kummerfeld and
Klein, 2013; Durrett and Klein, 2013).
This preponderance of non-anaphoric mentions
makes coreference resolution challenging, partly
because many basic coreference features, such as
those looking at head, number, or gender match
fail to distinguish between truly coreferent pairs
and the large number of matching but nonethe-
less non-coreferent pairs. Indeed, several au-
thors have noted that it is difficult to obtain good
performance on the coreference task using sim-
ple features (Lee et al., 2011; Fernandes et al.,
2012; Durrett and Klein, 2013; Kummerfeld and
Klein, 2013; Bj¨orkelund and Kuhn, 2014) and, as
a result, state-of-the-art systems tend to use lin-
ear models with complicated feature conjunction
schemes in order to capture more fine-grained in-
teractions. While this approach has shown suc-
cess, it is not obvious which additional feature
conjunctions will lead to improved performance,
which is problematic as systems attempt to scale
with new data and features.
In this work, we propose a data-driven
model for coreference that does not require pre-
specifying any feature relationships. Inspired by
recent work in learning representations for nat-
ural language tasks (Collobert et al., 2011), we
explore neural network models which take only
raw, unconjoined features as input, and attempt to
learn intermediate representations automatically.
In particular, the model we describe attempts to
create independent feature representations useful
for both detecting the anaphoricity of a mention
(that is, whether or not a mention is anaphoric) and
ranking the potential antecedents of an anaphoric
mention. Adequately capturing anaphoricity in-
formation has long been thought to be an impor-
tant aspect of the coreference task (see Ng (2004)
and Section 7), since a strong non-anaphoric sig-
nal might, for instance, discourage the erroneous
prediction of an antecedent for a non-anaphoric
mention even in the presence of a misleading head
match.
We furthermore attempt to encourage the learn-
ing of the desired feature representations by pre-
training the model’s weights on two correspond-
ing subtasks, namely, anaphoricity detection and
antecedent ranking of known anaphoric mentions.
Overall our best model has an absolute gain of
almost 2 points in CoNLL score over a similar
but linear mention-ranking model on the CoNLL
2012 English test set (Pradhan et al., 2012), and
of over 1.5 points over the state-of-the-art coref-
erence system. Moreover, unlike current state-of-
the-art systems, our model does only local infer-
ence, and is therefore significantly simpler.
</bodyText>
<subsectionHeader confidence="0.997752">
1.1 Problem Setting
</subsectionHeader>
<bodyText confidence="0.809227">
We consider here the mention-ranking (or
“mention-synchronous”) approach to coreference
1416
</bodyText>
<note confidence="0.997463666666667">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1416–1426,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.98762">
resolution (Denis and Baldridge, 2008; Bengtson
and Roth, 2008; Rahman and Ng, 2009), which
has been adopted by several recent coreference
systems (Durrett and Klein, 2013; Chang et al.,
2013). Such systems aim to identify whether a
mention is coreferent with an antecedent mention,
or whether it is instead non-anaphoric (the first
mention in the document referring to a particular
entity). This is accomplished by assigning a score
to the mention’s potential antecedents as well as
to the possibility that it is non-anaphoric, and
then predicting the greatest scoring option. We
furthermore assume the more realistic “system
mention” setting, where it is not known a priori
which mentions in a document participate in
coreference clusters, and so (all) mentions must
be automatically extracted, typically with the aid
of automatically detected parse trees.
Formally, we denote the set of automatically de-
tected mentions in a document by X. For a men-
tion x E X, let A(x) denote the set of mentions
appearing before x; we refer to this set as x’s po-
tential antecedents. Additionally let the symbol
c denote the empty antecedent, to which we will
view x as referring when x is non-anaphoric.1 De-
noting the set A(x) U {E} by Y(x), a mention-
ranking model defines a scoring function s(x, y) :
X x Y → R, and predicts the antecedent of x to
be y∗ = arg maxy∈Y(x) s(x, y).
It is common to be quite liberal when extracting
mentions, taking, essentially, every noun phrase or
pronoun to be a candidate mention, so as not to
prematurely discard those that might be coreferent
(Lee et al., 2011; Fernandes et al., 2012; Chang
et al., 2012; Durrett and Klein, 2013). For in-
stance, the Berkeley Coreference System (herein
BCS) (Durrett and Klein, 2013), which we use
for mention extraction in our experiments, recov-
ers approximately 96.4% of the truly anaphoric
mentions in the CoNLL 2012 training set, with
an almost 3.5:1 ratio of non-anaphoric mentions
to anaphoric mentions among the extracted men-
tions.
</bodyText>
<sectionHeader confidence="0.907611" genericHeader="method">
2 Mention Ranking Models
</sectionHeader>
<bodyText confidence="0.943861681818182">
The structural simplicity of the mention-ranking
framework puts much of the burden on the scor-
ing function s(x, y). We begin by consider-
ing mention-ranking systems using linear scoring
1We make this stipulation for modeling convenience; it is
not intended to reflect any linguistic fact.
functions. In the next section, we will extend these
models to operate over learned non-linear repre-
sentations.
Linear mention-ranking models generally uti-
lize the following scoring function
slin(x, y) g wTφ(x, y) ,
where φ: X x Y → Rd is a pairwise feature func-
tion defined on a mention and a potential an-
tecedent, and w is a learned parameter vector.
To add additional flexibility to the model, lin-
ear mention ranking models may duplicate indi-
vidual features in φ, with one version being used
when predicting an antecedent for x, and another
when predicting that x is non-anaphoric (Durrett
and Klein, 2013). Such a scheme effectively gives
rise to the following piecewise scoring function
</bodyText>
<equation confidence="0.998426333333333">
slin+(x, y) = u L φp(x,y) J E
� T r φ.(x) 1
vTφa(x) if y = E ,
</equation>
<bodyText confidence="0.999988916666667">
where φa : X → Rd. is a feature function defined
on a mention and its context, φp : X x Y → Rdp
is a pairwise feature function defined on a mention
and a potential antecedent, and parameters u and
v replace w. Above, we have made an explicit dis-
tinction between pairwise features (φp) and those
strictly on x and its context (φa), and moreover as-
sumed that our features need not examine potential
antecedents when predicting y = c.
We refer to the basic, unconjoined features used
for φa and φp as raw features. Figure 2 shows
two versions of these features, a base set BASIC
and an extended set BASIC+. The BASIC set are
the raw features used in BCS, and BASIC+ in-
cludes additional raw features used in other recent
coreference sytems. For instance, BASIC+ addi-
tionally includes features suggested by Recasens
et al. (2013) to be useful for anaphoricity, such
as the number of a mention, its named entity sta-
tus, and its animacy, as well as number and gen-
der information. We additionally include bilexi-
cal head features, which are used in many well-
performing systems (for instance, that of Fernan-
des et al. (2012)).
</bodyText>
<subsectionHeader confidence="0.99939">
2.1 Problems with Raw Features
</subsectionHeader>
<bodyText confidence="0.99990225">
Many authors have observed that, taken individu-
ally, raw features tend to not be particularly pre-
dictive for the coreference task. We examine
this phenomenon empirically in Figure 1. These
</bodyText>
<page confidence="0.740362">
1417
</page>
<figureCaption confidence="0.964558">
Figure 1: Two histograms illustrating the predictive ability
</figureCaption>
<bodyText confidence="0.999549321428571">
of raw (unconjoined) features per feature occurrence: (top)
mention-context features from φa as independent predictors
of anaphoricity (y =� e), and (bottom) antecedent-mention
features from φp as independent predictors of coreferent
mentions. Very few raw features are strong indicators of ei-
ther anaphoricity or an antecedent match. Data taken from
the CoNLL development set.
graphs show that the vast majority of individual
features do not give a strong positive signal either
of anaphoricity or for an antecedent match.
To address this issue, state-of-the-art mention-
ranking systems often rely on manual or otherwise
induced conjunction schemes to capture specific
feature interactions. Durrett and Klein (2013),
for instance, conjoin all raw features in Oa with
the type of the mention x, and all raw features in
Op with the types of the current mention and an-
tecedent. For these purposes, the type of a mention
is either “nominal”, “proper”, or a canonicaliza-
tion of the pronoun if it is a pronominal mention.
Fernandes et al. (2012) and Bj¨orkelund and Kuhn
(2014) use an automatic but complicated scheme
to induce conjunctions by first extracting feature
templates from a separately trained decision tree,
and then doing greedy forward selection among
the templates. These conjunctions add some non-
linearity to the scoring function while still main-
taining a tractable, though large, feature set.
</bodyText>
<sectionHeader confidence="0.869203" genericHeader="method">
3 Learning Features for Ranking
</sectionHeader>
<bodyText confidence="0.999893571428571">
As an alternative to the aforementioned feature
conjunction schemes, we consider learning feature
representations in order to better capture relevant
aspects of the task. Representation learning af-
fords the model more flexibility in exploiting fea-
ture interactions, although it can make the under-
lying training problem more difficult.
</bodyText>
<table confidence="0.995150926829268">
Mention Features (φa)
Feature Value Set
Mention Head V
Mention First Word V
Mention Last Word V
Word Preceding Mention V
Word Following Mention V
# Words in Mention {1, 2,...}
Mention Synt. Ancestry see BCS (2013)
Mention Type T
+ Mention Governor V
+ Mention Sentence Index {1, 2,...}
+ Mention Entity Type NER tags
+ Mention Number {sing.,plur.,unk}
+ Mention Animacy {an.,inan.,unk}
+ Mention Gender {m,f,neut.,unk}
+ Mention Person {1,2,3,unk}
Pairwise Features (φp)
Feature Value Set
BASIC features on Mention see above
BASIC features on Antecedent see above
Mentions between Ment., Ante. {0...10}
Sentences between Ment., Ante. {0...10}
i-within-i {T,F}
Same Speaker {T,F}
Document Type {Conv.,Art.}
Ante., Ment. String Match {T,F}
Ante. contains Ment. {T,F}
Ment. contains Ante. {T,F}
Ante. contains Ment. Head {T,F}
Mention contains Ante. Head {T,F}
Ante., Ment. Head Match {T,F}
Ante., Ment. Synt. Ancestries see above
+ BASIC+ features on Ment. see above
+ BASIC+ features on Ante. see above
+ Ante., Ment. Numbers see above
+ Ante., Ment. Genders see above
+ Ante., Ment. Persons see above
+ Ante., Ment., Entity Types see above
+ Ante., Ment. Heads see above
+ Ante., Ment. Types see above
</table>
<figureCaption confidence="0.683244">
Figure 2: Features used for φa(x) and φp(x, y). The ’+’
indicates a feature is in BASIC+ feature set. V denotes the
</figureCaption>
<bodyText confidence="0.932825375">
training vocabulary, and T denotes the set of mention types,
viz., {nominal,proper} U {canonical pronouns}, as defined
in BCS. Conv. and Art. abbreviate conversation and article
(resp.). Lexicalized features occurring fewer than 20 times
in the training set back off to part-of-speech; bilexical heads
occurring fewer than 10 times back off to an indicator feature.
Animacy information is taken from a list and rules used in the
Stanford Coreference system (Lee et al., 2013).
</bodyText>
<subsectionHeader confidence="0.999362">
3.1 Model
</subsectionHeader>
<bodyText confidence="0.9994435">
We use a neural network to define our model as
an extension to the mention-ranking model intro-
duced in Section 2. We consider in particular the
scoring function:
</bodyText>
<equation confidence="0.9448775">
� o uTg( a(x) + u0 y if E
s(x, y) = p(x,y) )
vTha(x) + v0 if y = c ,
1418
</equation>
<bodyText confidence="0.99987775">
where ha and hp are feature representations, non-
linear functions of the features φa and φp (respec-
tively), and g is a function of these representa-
tions. In particular, we define
</bodyText>
<equation confidence="0.997831">
ha(x) °= tanh(Wa φa(x) + ba)
hp(x, y) °= tanh(Wp φp(x, y) + bp) ,
</equation>
<bodyText confidence="0.999930238095238">
and we take g to either be the identity func-
tion, in which case the above model is analo-
gous to slin+ but defined over non-linear fea-
ture representations, or to be an additional hidden
layer: g(h hp a(x)) i) = tanh(W h hpa() i + b).
For ease of exposition, we will refer to these two
settings of g as g1 and g2 (respectively) in what
follows. As we will see below, both settings lead
to comparable performance, but to a different error
distribution.
In either case, by defining the functions ha and
hp, we allow the model to learn representations
of the input features φa and φp. The benefit of
the added non-linearities is that, in theory, it is no
longer necessary to explicitly specify feature con-
junctions, since the model may learn them auto-
matically if necessary. Accordingly, for this model
we use only φa and φp consisting of the raw fea-
tures in Figure 2 without conjunctions. Any inter-
action between these features must be learned by
the feature representations hp and ha.
</bodyText>
<subsectionHeader confidence="0.999016">
3.2 Training
</subsectionHeader>
<bodyText confidence="0.993540357142857">
We can directly train our model using back-
propagation. To specify the training problem, we
first define notation for the training objective.
Define the set C(x) to contain just the mentions
in A(x) that are coreferent with x. We then define
Finally, let y`n = arg maxyEC&apos;(xn) s(xn, y) be the
highest scoring correct antecedent of xn, which
may be E. (Thus, following recent work (Yu and
Joachims, 2009; Fernandes et al., 2012; Chang et
al., 2013; Durrett and Klein, 2013), we view each
mention as having a “latent antecedent”.2) We
train to minimize the regularized, slack-rescaled,
2Note that this renders the objectives of even models with
a linear scoring function non-convex.
</bodyText>
<equation confidence="0.817163">
latent-variable loss3 given by:
A(xn, ˆy)(1 + 8(xn, ˆy)−s(xn, y`n))
+ λ||θ||1,
</equation>
<bodyText confidence="0.988198571428571">
where Δ is a mistake-specific cost function,
which is 0 when yˆ E C&apos;(xn). Above, we
use θ to refer to the full set of parameters
{W, u, v, Wa, Wp, ba, bp}.
For experiments, we define Δ to take on differ-
ent costs for the three kinds of mistakes possible
in a coreference task, as follows:
</bodyText>
<equation confidence="0.98137375">
Δ(x, ˆy) = α2 if yˆ = e n e V C&apos;(x)
(α3 if yˆ=�e n yˆE�C&apos;(x)
α1 if yˆ#e n e E C&apos;(x)
.
</equation>
<bodyText confidence="0.9997952">
The αi determine the trade-off between these mis-
takes (and thus precision and recall). Adopting the
terminology of BCS, we refer to these mistakes
as “false link” (FL), “false new” (FN), and “wrong
link” (WL), respectively.
</bodyText>
<sectionHeader confidence="0.972469" genericHeader="method">
4 Representations from Subtasks
</sectionHeader>
<bodyText confidence="0.999989476190476">
While we could train our full model directly, it is
known to be difficult to train high performing non-
convex neural-network models from a random ini-
tialization (Erhan et al., 2010). In order to over-
come the problems associated with training from
this setting, and to learn feature representations
useful for the full coreference task, we pretrain
subparts of the model on the subtasks targeting
the desired feature representations. We then train
the entire model on the full coreference task (from
the pre-trained initializations). As we will see,
the pre-training scheme outlined below helps the
model achieve improved performance.
The proposed pre-training scheme involves
learning the parameters associated with ha and hp
using two natural subtasks: anaphoricity detection
and antecedent ranking. In particular, we (1) train
ha on the task of predicting whether a particular
mention is anaphoric or not, and (2) train hp on
the task of predicting the antecedent of mentions
known to be anaphoric.
</bodyText>
<subsectionHeader confidence="0.998788">
4.1 Anaphoricity Detection
</subsectionHeader>
<bodyText confidence="0.999501666666667">
For the first subtask we attempt to predict whether
a mention is anaphoric or not based only on its
3Previous work divides between log-loss and margin loss.
We use the latter because gradient updates (within backprop)
for the non-probabilistic objectives only involve terms relat-
ing to yˆ and yn, and are therefore faster.
</bodyText>
<equation confidence="0.9801693">
(
C(x) if x is anaphoric
C�(x) = {E} otherwise
.
N
E
n=1
L(θ) =
max
ˆyEY(xn)
</equation>
<table confidence="0.933944444444444">
1419
Feat. (Conj.) Model P Anaphoric Fl Ante
R Acc.
BASIC (N) Lin. 74.15 74.20 74.18 69.10
BASIC (Y) Lin. 73.98 75.04 74.51 79.76
BASIC (N) NN 75.30 75.36 75.33 81.65
BASIC+ (N) Lin. 74.14 74.71 74.43 74.02
BASIC+ (Y) Lin. 74.24 75.39 74.81 80.44
BASIC+ (N) NN 75.84 76.02 75.93 82.86
</table>
<tableCaption confidence="0.999674">
Table 1: Performance of the two subtasks on the CoNLL 2012
</tableCaption>
<bodyText confidence="0.892367333333333">
development set by feature set and model type. “Conj.” indi-
cates whether conjunctions are used. The linear anaphoric
system is an SVM (LibLinear implementation (Fan et al.,
2008)), and the linear antecedent system is a linear model
with the margin-based objective.
local context.4 Anaphoricity detection in vari-
ous forms has been used as an initial step in sev-
eral coreference systems (Ng and Cardie, 2002;
Bengtson and Roth, 2008; Rahman and Ng, 2009;
Bj¨orkelund and Farkas, 2012), and the related
question of whether a mention can be determined
to be a singleton or not has been explored recently
by Recasens et al. (2013), Ma et al. (2014), and
others.5
Formally, let tn ∈ {−1, 1} indicate whether c ∈
C&apos;(xn) or not (respectively). That is, tn =1 if and
only if xn is anaphoric. Define the subtask scoring
function sa : X → Ras
</bodyText>
<equation confidence="0.979075">
sa(x) g vaTha(x) + ν0 ,
</equation>
<bodyText confidence="0.998401">
where the vector va and the bias ν0 are specific to
this subtask and are discarded after pre-training.
We train this model to minimize the following
slack-rescaled objective
</bodyText>
<equation confidence="0.990917333333333">
N
La(0a) = Da(tn)[1 − tn sa(xn)]+ + A||0a||1,
n=1
</equation>
<bodyText confidence="0.99957175">
where Da is a class-specific cost used to help en-
courage anaphoric decisions given the imbalanced
data set, and 0a = {va, Wa, ba} are the parame-
ters of the subtask.
</bodyText>
<subsectionHeader confidence="0.997663">
4.2 Antecedent Ranking
</subsectionHeader>
<bodyText confidence="0.999703666666667">
For the second subtask, antecedent ranking, we
predict the antecedent for mentions known a pri-
ori to be anaphoric. This subtask is inspired by
</bodyText>
<footnote confidence="0.424994571428571">
4While performance on this subtask can in fact be im-
proved further by looking at previous mentions, features
learned in this way led to inferior performance on the full
task.
5Note that singleton detection is slightly different from
anaphoricity detection, since a mention can be non-anaphoric
but not a singleton if it is the first mention in a cluster.
</footnote>
<figureCaption confidence="0.768458">
Figure 3: Visualization of the representation matrix WP.
</figureCaption>
<bodyText confidence="0.984737458333333">
A subset of the raw features were manually grouped into
five classes indicating: full lexical match [F], head match
[H], mention/sentence distance [D] (near versus far), gen-
der/number match [G], and type [P] (pronoun versus other).
The heat map illustrates 10-columns of W, as a weighted
combination of these classes, roughly illustrating the com-
bination of raw features required for this dimension of the
representation.
the “gold mention” version of the coreference task.
Systems designed for this task are forced to handle
many fewer non-anaphoric mentions and can often
successfully utilize richer feature representations.
The setup for this task is similar to the full
coreference problem, except that we discard any
mention xn such that c ∈ C&apos;(xn). Thus, we define
the pairwise scoring function sp : X × Y → R as
sp(x, y) g upThp(x, y) + υ0 .
As before, up and υ0 are discarded after train-
ing for this subtask, but we keep the rest of the
parameters. For training, we use an analogous
latent-variable loss function to that used for the
full coreference task, except we replace C&apos; with
C, and the cost D(x, ˆy) is always 1 (when it is
nonzero).
</bodyText>
<subsectionHeader confidence="0.998724">
4.3 Subtask Performance
</subsectionHeader>
<bodyText confidence="0.983036851851852">
As a preliminary experiment, we train models for
these two subtasks using both the BASIC and BA-
SIC+ raw features. Table 1 shows the results. For
the first subtask, experiments look at the preci-
sion, recall, and F1 score of predicting anaphoric
mentions on the CoNLL 2012 development set.
As a baseline we use an L1-regularized SVM
implemented using LibLinear (Fan et al., 2008),
both using raw features and using features con-
joined according to the BCS scheme. For the sec-
ond subtask, experiments look at the accuracy of
the model in predicting the correct antecedent on
known anaphoric mentions. As a baseline we use
a linear mention ranking model, with and without
1420
conjunctions, trained using the same margin-based
loss.
In both subtasks, the neural network model
performs quite well, significantly better than the
unconjoined baselines and better than the model
trained with manually conjoined features. We pro-
vide a visual representation of the antecedent rank-
ing features learned in Figure 3. While the im-
proved subtask performance does not imply better
performance on the full coreference task, it shows
that model can learn useful feature representations
with only raw input features.
</bodyText>
<sectionHeader confidence="0.983671" genericHeader="method">
5 Coreference Experiments
</sectionHeader>
<bodyText confidence="0.999944666666667">
Our experiments examine performance as com-
pared with other coreference systems, as well as
the effect of features, pre-training, and model ar-
chitecture. We also perform a qualitative compar-
ison of our model with the analogous linear model
on some challenging non-anaphoric cases.
</bodyText>
<subsectionHeader confidence="0.967195">
5.1 Methods
</subsectionHeader>
<bodyText confidence="0.990147660714286">
All experiments use the CoNLL 2012 English
dataset (Pradhan et al., 2012), which is based on
the OntoNotes corpus (Hovy et al., 2006). The
data set contains 3,493 documents consisting of
1.6 million words. We use the standard experi-
mental split with the training set containing 2,802
documents and 156K annotated mentions, the de-
velopment set containing 343 documents and 19K
annotated mentions, and the test set containing
348 documents and 20K annotated mentions. For
all experiments, we use BCS (Durrett and Klein,
2013) to extract system mentions and to compute
some of the features.
For training, we minimize the loss described
above using the composite mirror descent Ada-
Grad update (Duchi et al., 2011) with docu-
ment sized mini-batches.6 We tuned the Ada-
Grad learning rate and regularization parameters
using a grid search over possible learning rates
q E {0.001, 0.002, 0.01, 0.02, 0.1, 0.2} and over
regularization parameters A E{10−6, ... ,10−1}.
For the full coreference task, we use a differ-
ent learning rate for the pre-trained weights and
for the second-layer weights, using q1 = 0.1 and
q2 = 0.001, respectively, and A =10−6. When ini-
tializing weight-matrices that were not pre-trained
6In preliminary experiments we also used Nesterov’s ac-
celerated gradient (Nesterov, 1983), but found AdaGrad to
perform better.
we used the sparse initialization technique pro-
posed by Sutskever et al. (2013). For all experi-
ments we use the cost-weights α = (0.5,1.2,1)
in defining A.
For the anaphoricity representations the ma-
trix dimensions used are Wa E R128×da, and for
the pairwise representations the matrix dimensions
used are Wp E R700×dp. In the g2 model, the
outer matrix dimensions are W ER128×(dp+da).
With the BASIC+ features, dp and da come out
to be slightly less than 106 and 104, respectively,
with bilexical head features accounting for the vast
majority of dp.7 We tuned all hyper-parameters (as
well as those of baseline systems) on the develop-
ment set.
We use the CoNLL 2012 scoring script v8.018
(Pradhan et al., 2014; Luo et al., 2014), which
scores based on 3 metrics, including MUC (Vilain
et al., 1995), CEAFe (Luo, 2005), and B3 (Bagga
and Baldwin, 1998), as well as the CoNLL score,
which is the arithmetic mean of the 3 metrics.
Code implementing our models is available
at https://github.com/swiseman/nn_
coref. The system trains in time comparable to
that of linear systems, mainly because we use only
raw features and sparse margin-based gradient up-
dates.
</bodyText>
<subsectionHeader confidence="0.886023">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.995871190476191">
Our main results are shown in Table 2. This table
compares the performance of our system with the
performance reported by several other state-of-the
art systems on the CoNLL 2012 English corefer-
ence test set. Our full models achieve the best F1
score across two of the three metrics and have the
best aggregate (CoNLL) score, with an improve-
ment of over 1.5 points over the best reported re-
sult, and of almost 2 points over the best mention-
ranking system. Our F1 improvements on all three
metrics are significant (p &lt; 0.05 under the boot-
strap resample test (Koehn, 2004)) as compared
with both Bj¨orkelund and Kuhn (2014), and Dur-
rett and Klein (2014), the two most recent, state-
of-the-art systems.
Since our full models use some additional raw
features (although an order of magnitude fewer
total features than the comparable conjunction-
7Note that the BCS conjunction scheme, for instance, ap-
plied to our raw features gives a dp and da that are over an
order of magnitude larger.
</bodyText>
<footnote confidence="0.4598475">
8http://conll.github.io/
reference-coreference-scorers/
</footnote>
<table confidence="0.978750888888889">
1421
System P MUC F1 P B3 F1 P CEAF, F1 CoNLL
R R R
BCS (2013) 74.89 67.17 70.82 64.26 53.09 58.14 58.12 52.67 55.27 61.41
Prune&amp;Score (2014) 81.03 66.16 72.84 66.9 51.10 57.94 68.75 44.34 53.91 61.56
B&amp;K (2014) 74.3 67.46 70.72 62.71 54.96 58.58 59.4 52.27 55.61 61.63
D&amp;K (2014) 72.73 69.98 71.33 61.18 56.60 58.80 56.20 54.31 55.24 61.79
This work (g2) 76.96 68.10 72.26 66.90 54.12 59.84 59.02 53.34 56.03 62.71
This work (g1) 76.23 69.31 72.60 66.07 55.83 60.52 59.41 54.88 57.05 63.39
</table>
<tableCaption confidence="0.9781935">
Table 2: Results on CoNLL 2012 English test set. We compare against recent state-of-the-art systems, including (in order)
Durrett and Klein (2013), Ma et al. (2014), Bj¨orkelund and Kuhn (2014), and Durrett and Klein (2014) (rescored with the v8.01
scorer). F1 gains are significant (p &lt; 0.05 under the bootstrap resample test (Koehn, 2004)) compared with both B&amp;K and
D&amp;K for all metrics.
</tableCaption>
<table confidence="0.9959313">
MUC B3 CEAF, CoNLL
71.80 60.93 57.51 63.41
71.77 60.84 57.05 63.22
71.92 61.06 57.59 63.52
72.74 61.77 58.63 64.38
72.31 61.79 58.06 64.05
72.68 61.70 58.32 64.23
Model
Fully Conn. 1 Layer
Fully Conn. 2 Layer
</table>
<equation confidence="0.99266325">
g1 + RI
g1 + PT
g2 + RI
g2 + PT
</equation>
<bodyText confidence="0.3198736">
Table 4: Comparison of performance (in F1 score) of vari-
ous models on CoNLL 2012 development set using BASIC+
features. “PT” and “RI” refer to pretraining and random ini-
tialization respectively. “Fully Conn.” refers to baseline fully
connected networks. See text for further model descriptions.
</bodyText>
<table confidence="0.998901571428571">
Model Features MUC B3 CEAF, CoNLL
Lin. 70.44 59.10 55.57 61.71
NN (g2) BASIC 71.59 60.56 57.45 63.20
NN (g1) 71.86 60.9 57.90 63.55
Lin. 70.92 60.05 56.39 62.45
NN (g2) BASIC+ 72.68 61.70 58.32 64.23
NN (g1) 72.74 61.77 58.63 64.38
</table>
<tableCaption confidence="0.9058395">
Table 3: F1 performance comparison between state-of-the-art
linear mention-ranking model (Durrett and Klein, 2013) and
our full models on CoNLL 2012 development set for different
feature sets.
</tableCaption>
<bodyText confidence="0.99907562962963">
based linear model), we are interested in what part
of the improvement in performance comes from
features rather than modeling power. Table 3 com-
pares the full model to BCS, a system effectively
using the slin+ scoring function together with a
manual conjunction scheme, on both BASIC and
BASIC+ features. While our models outperform
BCS in both cases, we see that as we add more
features (as in the BASIC+ set), the performance
gap between our model and the linear system be-
comes even more pronounced.
We may also wonder whether the architecture
represented by our scoring function, where the in-
termediate representations ha and hp are sepa-
rated in the first layer, is necessary for these re-
sults. We accordingly compare with the fully
connected versions of these two models (which
are equivalent to 1 and 2 layer multi-layer per-
ceptrons) using the BASIC+ features in Table 4.9
There, we also evaluate the effect of pre-training
on these models by comparing with the results of
training from a random initialization. We see that
while even randomly initialized models are capa-
ble of excellent performance, pre-training is bene-
ficial, especially for g1.
9We also experimented with bilinear models both with
and without non-linearities; these were also inferior.
</bodyText>
<sectionHeader confidence="0.998557" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999924769230769">
We attempt to gain insight into our model’s er-
rors using using two different error breakdowns.
In Table 5 we show the errors as reported by the
analysis tool of Kummerfeld and Klein (2013). In
Table 6 we show a more fine-grained breakdown
inspired by a similar analysis in Durrett and Klein
(2013). In the latter table, we categorize the er-
rors made by our system on the CoNLL 2012 de-
velopment data in terms of (1) whether or not the
mention has a head match with a previously oc-
curring mention in the document, unless it is a
pronominal mention, which we treat separately,
(2) in terms of the status of the mention in the
gold clustering, namely, singleton, first-in-cluster,
or anaphoric, and (3) in terms of the type of error
made (which, as discussed in Section 3, are one of
FL, FN, and WL).
We note that the two models have slightly dif-
ferent error profiles, with g1 being slightly better
at recall and g2 being slightly better at precision.
Indeed, we see from Table 6 that the two mod-
els make a comparable number of total errors (g1
makes only 17 fewer errors overall). The increased
precision of the g2 model is presumably due to the
second layer around ha and hp in g2 allowing for
antecedent evidence to interact with anaphoricity
</bodyText>
<table confidence="0.965177">
1422
Error Type BCS NN (g1) NN (g2)
Conflated Entities 1603 1434 1371
Extra Mention 651 568 529
Extra Entity 655 623 561
Divided Entity 1989 1837 1835
Missing Mention 1004 997 1005
Missing Entity 1070 1026 1114
</table>
<tableCaption confidence="0.99644175">
Table 5: Absolute error counts from the coreference analysis
tool of Kummerfeld and Klein (2013). The upper set roughly
corresponds to the precision and the lower to the recall of the
coreference clusters produced by the model.
</tableCaption>
<table confidence="0.999871">
NN (g1) Singleton 1st in clust. Anaphoric
FL # FL # FN + WL #
HM 817 8.2K 147 0.8K 700+318 4.7K
No HM 86 19.8K 41 2.4K 677 + 59 1.0K
Pron. 948 2.6K 257 0.5K 434+875 7.3K
NN (g2) Singleton 1st in clust. Anaphoric
FL # FL # FN + WL #
HM 770 8.2K 130 0.8K 803+306 4.7K
No HM 73 19.8K 39 2.4K 699+ 52 1.0K
Pron. 896 2.6K 249 0.5K 456+869 7.3K
</table>
<tableCaption confidence="0.9126575">
Table 6: Errors made by NN (g1) (top) and NN (g2) (bottom)
on CoNLL 2012 English development data. Rows correspond
to (1) mentions with a (previous) head match (HM), that is,
mentions x such that ,A(x) contains another mention with the
</tableCaption>
<bodyText confidence="0.998097">
same head word, (2) with no previous head match (no HM),
and (3) to pronominal mentions, respectively. The 3 column
groups correspond to singleton, first-in-cluster, and anaphoric
mentions (resp.), as determined by the gold clustering, with
the number and type of errors on the left and the total number
of mentions in the category (#) on the right.
evidence in a more complicated way. Ultimately,
however, coreference systems operating over sys-
tem mentions are already biased toward precision,
and so the increased precision of g2 is not as help-
ful as the increased recall of g1 in the final CoNLL
score.
In further analysis we found that many of the
correct predictions made by the g2 model not
made by g1 and the linear model involve predict-
ing non-anaphoric even in the presence of highly
misleading antecedent features like head-match.
Figure 4 shows some examples of mentions with
previous head matches that the linear system pre-
dicted as anaphoric and that our system correctly
identifies as non-anaphoric.
We illustrate how the features in Figure 2 might
be useful in such cases by considering the first
example in Figure 4. There, a comma follows
”the Nika TV company” in the text (and is picked
up by the “word following” feature), perhaps in-
dicating an appositive, which makes anaphoric-
ity unlikely. The model can also learn that the
</bodyText>
<table confidence="0.945770363636364">
Non-Anaphoric (x) Spurious Antecedent (y)
the Nika TV company an independent company
Lexus sales GM ’s domestic car sales
The storage area the harbor area
the Budapest location Radio Free Europe ’s new location
the synagogue the synagogue too or something
the equity market The junk market
their silver coin one silver coin
the international school The Hong Kong elementary school
the 1970s the early 1970s
the 2003 season the 2001 season
</table>
<figureCaption confidence="0.764819">
Figure 4: Example mentions x that were correctly marked
</figureCaption>
<bodyText confidence="0.979210727272727">
non-anaphoric by g2, but incorrectly marked anaphoric with
y as an antecedent by the BASIC+ linear model. These ex-
amples highlight the difficult case where there is a spurious
head-match between non-coreferent pairs. See text for fur-
ther details.
”company-company” head match is often mislead-
ing, and, in general, distance features may also
rule out head matches. Note that while these fea-
tures on their own may be more or less correlated
with a mention being non-anaphoric, the model
learns to combine them in a predictive way.
</bodyText>
<subsectionHeader confidence="0.996619">
6.1 Further Improving Coreference Systems
</subsectionHeader>
<bodyText confidence="0.991193518518519">
Table 6 also gives a sense of where coreference
systems such as ours need to improve. It is
first important to note that the case of resolving
an anaphoric mention that has no previous head
matches (e.g., identifying that “the team” and “the
New York Giants” are coreferent), which is of-
ten taken to be one of the major challenges fac-
ing coreference systems because it presumably
requires semantic information, is not the largest
source of errors. In fact, we see from Table 6
(second row, third column in both sub-tables) that
while these cases do indeed account for a substan-
tial percentage of errors, we make hundreds more
errors predicting singleton pronominal mentions
to be anaphoric (in the case of g1) and on incor-
rectly linking anaphoric pronominal mentions (in
the case of g2). Further analysis indicates that
these errors are almost entirely related to incor-
rectly linking pleonastic pronouns, such as “it” or
“you,” and that moreover the incorrectly predicted
antecedent for these pleonastic pronouns is almost
always (another instance of) the same pronoun.
That these pleonastic cases are so problematic
is interesting when considered against the back-
drop of the inference strategies typically employed
by coreference systems, which we briefly men-
tion here but discuss more fully in the next sec-
tion. Currently, coreference systems divide be-
1423
tween those using “local” models, which choose
antecedents for potentially anaphoric mentions in-
dependently of each other, and “non-local” mod-
els, which make predictions that take into ac-
count predictions made for previous mentions, and
perhaps even attempt to jointly predict all men-
tions in a document. While our model is en-
tirely local, other recent high performing sys-
tems, such as that of Bj¨orkelund and Kuhn (2014),
are not. One might suspect, then, that “non-
local” inference might allow us to capture the fact
that, for instance, a cluster of coreferent mentions
should generally not consist solely of pronouns,
and thereby avoid predicting (identical) pronomi-
nal antecedents for pleonastic pronouns.
As it turns out, however, almost 30% of the
anaphoric pronominal mentions in the CoNLL de-
velopment data participate in pronoun-only clus-
ters (primarily in the context of broadcast or tele-
phone conversations), which suggests that such a
“non-local” rule may not be particularly useful,
though further experiments are required. It is also
worth noting that a suitably modified loss function
may also be able to prevent excessive pronoun-
pronoun linking, even in a local model.
</bodyText>
<sectionHeader confidence="0.999111" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999527">
There is a voluminous literature on machine learn-
ing approaches to coreference resolution, effec-
tively beginning with Soon et al. (2001). The re-
cent introduction of the CoNLL datasets (Pradhan
et al., 2012) has spurred research that takes ad-
vantage of more fine-grained features and richer
models (Bj¨orkelund and Farkas, 2012; Chang et
al., 2012; Durrett and Klein, 2013; Chang et al.,
2013; Bj¨orkelund and Kuhn, 2014; Ma et al.,
2014). Of these approaches, our model is related
to the mention-ranking approaches (Bengtson and
Roth, 2008; Denis and Baldridge, 2008; Rahman
and Ng, 2009; Durrett and Klein, 2013; Chang
et al., 2013), as opposed to those that focus on
non-local, structured prediction (McCallum and
Wellner, 2003; Culotta et al., 2006; Haghighi and
Klein, 2010; Fernandes et al., 2012; Stoyanov and
Eisner, 2012; Bj¨orkelund and Farkas, 2012; Wick
et al., 2012; Bj¨orkelund and Kuhn, 2014; Durrett
and Klein, 2014).
In motivation, our work is most similar to that of
Ng (2004), who notes that anaphoricity informa-
tion is useful within the broader coreference task,
and who accordingly attempts to “globally” opti-
mize performance based on this information, as
well as that of Denis et al. (2007), who do joint
decoding of anaphoricity and coreference predic-
tions using ILP. Both of these works are taken to
contrast with the more popular approach of do-
ing an initial non-anaphoric pruning step (Ng and
Cardie, 2002; Rahman and Ng, 2009; Recasens et
al., 2013; Lee et al., 2013). In contrast, we jointly
learn non-linear functions of anaphoricity and an-
tecedent features, rather than tune a threshold,
or jointly decode based on independently trained
classifiers (as in Denis et al. (2007)). In a simi-
lar vein, several authors have also proposed using
the output of an anaphoricity classifier as a feature
in a downstream coreference system (Ng, 2004;
Bengtson and Roth, 2008). In our framework we
(re)learn features jointly with the full task, after
a pre-training scheme that targets anaphoricity as
well antecedent representations.
There has also been some work on automat-
ically inducing feature conjunctions for use in
coreference systems (Fernandes et al., 2012; Las-
salle and Denis, 2013), though the approach we
present here is somewhat simpler, and unlike that
of Lassalle and Denis (2013) is designed for use
on system rather than gold mentions.
There has been much interest recently in us-
ing neural networks for classic natural language
tasks such as tagging and semantic role labeling
Collobert et al. (2011), sentiment analysis (Socher
et al., 2011; Socher et al., 2012), prepositional
phrase attachment (Belinkov et al., 2014) among
others. These systems often use some form of pre-
training for initialization, often word-embeddings
learned from external tasks. However, there has
been little work of this form for coreference reso-
lution.
</bodyText>
<sectionHeader confidence="0.994652" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.99997075">
We have presented a simple, local model ca-
pable of learning feature representations useful
for coreference-related subtasks, and of thereby
achieving state-of-the-art performance. Because
our approach automatically learns intermediate
representations given raw features, directions for
further research might alternately explore includ-
ing additional (perhaps semantic) raw features,
as well as developing loss functions that further
discourage learning representations that allow for
common errors (such as those involving pleonastic
pronouns).
</bodyText>
<page confidence="0.737235">
1424
</page>
<sectionHeader confidence="0.979289" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998919962962964">
Amit Bagga and Breck Baldwin. 1998. Algorithms
for Scoring Coreference Chains. In The first in-
ternational conference on language resources and
evaluation workshop on linguistics coreference, vol-
ume 1, pages 563–566. Citeseer.
Yonatan Belinkov, Tao Lei, Regina Barzilay, and Amir
Globerson. 2014. Exploring Compositional Archi-
tectures and Word Vector Representations for Prepo-
sitional Phrase Attachment. Transactions of the As-
sociation for Computational Linguistics, 2:561–572.
Eric Bengtson and Dan Roth. 2008. Understanding
the Value of Features for Coreference Resolution.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
294–303. ACL.
Anders Bj¨orkelund and Rich´ard Farkas. 2012. Data-
driven Multilingual Coreference Resolution using
Resolver Stacking. In Joint Conference on EMNLP
and CoNLL-Shared Task, pages 49–55. ACL.
Anders Bj¨orkelund and Jonas Kuhn. 2014. Learn-
ing structured perceptrons for coreference Resolu-
tion with Latent Antecedents and Non-local Fea-
tures. ACL, Baltimore, MD, USA, June.
Kai-Wei Chang, Rajhans Samdani, Alla Rozovskaya,
Mark Sammons, and Dan Roth. 2012. Illinois-
coref: The UI System in the CoNLL-2012 Shared
Task. In Joint Conference on EMNLP and CoNLL-
Shared Task, pages 113–117. Association for Com-
putational Linguistics.
Kai-Wei Chang, Rajhans Samdani, and Dan Roth.
2013. A Constrained Latent Variable Model for
Coreference Resolution. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 601–612.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural Language Processing (almost) from
Scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.
Aron Culotta, Michael Wick, Robert Hall, and Andrew
McCallum. 2006. First-order Probabilistic Models
for Coreference Resolution. NAACL-HLT.
Pascal Denis and Jason Baldridge. 2008. Specialized
Models and Ranking for Coreference Resolution.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
660–669. ACL.
Pascal Denis, Jason Baldridge, et al. 2007. Joint De-
termination of Anaphoricity and Coreference Reso-
lution using Integer Programming. In HLT-NAACL,
pages 236–243. Citeseer.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive Subgradient Methods for Online Learning
and Stochastic Optimization. The Journal of Ma-
chine Learning Research, 12:2121–2159.
Greg Durrett and Dan Klein. 2013. Easy Victories and
Uphill Battles in Coreference Resolution. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1971–
1982.
Greg Durrett and Dan Klein. 2014. A Joint Model for
Entity Analysis: Coreference, Typing, and Linking.
Transactions of the Association for Computational
Linguistics, 2:477–490.
Dumitru Erhan, Yoshua Bengio, Aaron Courville,
Pierre-Antoine Manzagol, Pascal Vincent, and Samy
Bengio. 2010. Why does unsupervised pre-training
help deep learning? The Journal of Machine Learn-
ing Research, 11:625–660.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
Library for Large Linear Classification. The Journal
of Machine Learning Research, 9:1871–1874.
Eraldo Rezende Fernandes, C´ıcero Nogueira Dos San-
tos, and Ruy Luiz Milidi´u. 2012. Latent Structure
Perceptron with Feature Induction for Unrestricted
Coreference Resolution. In Joint Conference on
EMNLP and CoNLL-Shared Task, pages 41–48. As-
sociation for Computational Linguistics.
Aria Haghighi and Dan Klein. 2010. Coreference
Resolution in a Modular, Entity-centered Model. In
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 385–393. Association for Computa-
tional Linguistics.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
the 90% Solution. In Proceedings of the human lan-
guage technology conference of the NAACL, Com-
panion Volume: Short Papers, pages 57–60. Associ-
ation for Computational Linguistics.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proceedings of
the 2004 Conference on Empirical Methods in Natu-
ral Language Processing, pages 388–395. Citeseer.
Jonathan K. Kummerfeld and Dan Klein. 2013. Error-
driven Analysis of Challenges in Coreference Reso-
lution. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, Seattle, WA, USA, October.
Emmanuel Lassalle and Pascal Denis. 2013. Improv-
ing Pairwise Coreference Models through Feature
Space Hierarchy Learning. In ACL 2013-Annual
meeting of the Association for Computational Lin-
guistics.
Heeyoung Lee, Yves Peirsman, Angel Chang,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2011. Stanford’s Multi-pass Sieve Corefer-
ence Resolution System at the CoNLL-2011 Shared
1425
Task. In Proceedings of the Fifteenth Conference on
Computational Natural Language Learning: Shared
Task, pages 28–34. Association for Computational
Linguistics.
Heeyoung Lee, Angel Chang, Yves Peirsman,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2013. Deterministic Coreference Res-
olution based on Entity-centric, Precision-ranked
Rules. Computational Linguistics, 39(4):885–916.
Xiaoqiang Luo, Sameer Pradhan, Marta Recasens, and
Eduard Hovy. 2014. An Extension of BLANC to
System Mentions. Proceedings of ACL, Baltimore,
Maryland, June.
Xiaoqiang Luo. 2005. On Coreference Resolution
Performance Metrics. In Proceedings of the confer-
ence on Human Language Technology and Empiri-
cal Methods in Natural Language Processing, pages
25–32. Association for Computational Linguistics.
Chao Ma, Janardhan Rao Doppa, J Walker Orr,
Prashanth Mannem, Xiaoli Fern, Tom Dietterich,
and Prasad Tadepalli. 2014. Prune-and-score:
Learning for Greedy Coreference Resolution. In
Proceedings of the 2014 Conference on Empirical
Methods in Natural Language Processing.
Andrew McCallum and Ben Wellner. 2003. Toward
Conditional Models of Identity Uncertainty with
Application to Proper Noun Coreference. Advances
in Neural Information Processing Systems 17.
Yurii Nesterov. 1983. A Method of Solving a Convex
Programming Problem with Convergence Rate O
(1/k2). In Soviet Mathematics Doklady, volume 27,
pages 372–376.
Vincent Ng and Claire Cardie. 2002. Identifying
Anaphoric and Non-anaphoric Noun Phrases to Im-
prove Coreference Resolution. In Proceedings of
the 19th international conference on Computational
linguistics-Volume 1, pages 1–7. Association for
Computational Linguistics.
Vincent Ng. 2004. Learning Noun Phrase Anaphoric-
ity to Improve Coreference Resolution: Issues in
Representation and Optimization. In Proceedings of
the 42nd Annual Meeting on Association for Compu-
tational Linguistics, page 151. Association for Com-
putational Linguistics.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. Conll-
2012 Shared Task: Modeling Multilingual Unre-
stricted Coreference in OntoNotes. In Joint Con-
ference on EMNLP and CoNLL-Shared Task, pages
1–40. ACL.
Sameer Pradhan, Xiaoqiang Luo, Marta Recasens, Ed-
uard Hovy, Vincent Ng, and Michael Strube. 2014.
Scoring Coreference Partitions of Predicted Men-
tions: A Reference Implementation. In Proceedings
of the Association for Computational Linguistics.
Altaf Rahman and Vincent Ng. 2009. Supervised
Models for Coreference Resolution. In Proceed-
ings of the 2009 Conference on Empirical Methods
in Natural Language Processing: Volume 2-Volume
2, pages 968–977. ACL.
Marta Recasens, Marie-Catherine de Marneffe, and
Christopher Potts. 2013. The Life and Death of Dis-
course Entities: Identifying Singleton Mentions. In
HLT-NAACL, pages 627–633.
Richard Socher, Jeffrey Pennington, Eric H Huang,
Andrew Y Ng, and Christopher D Manning. 2011.
Semi-supervised Recursive Autoencoders for Pre-
dicting Sentiment Distributions. In Proceedings of
the 2011 Conference on Empirical Methods in Nat-
ural Language Processing, pages 151–161. ACL.
Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic Compositional-
ity through Recursive Matrix-vector Spaces. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201–1211. ACL.
Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A Machine Learning Ap-
proach to Coreference Resolution of Noun Phrases.
Computational Linguistics, 27(4):521–544.
Veselin Stoyanov and Jason Eisner. 2012. Easy-first
Coreference Resolution. In COLING, pages 2519–
2534. Citeseer.
Ilya Sutskever, James Martens, George Dahl, and Geof-
frey Hinton. 2013. On the Importance of Initializa-
tion and Momentum in Deep Learning. In Proceed-
ings of the 30th International Conference on Ma-
chine Learning, pages 1139–1147.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A Model-
theoretic Coreference Scoring Scheme. In Proceed-
ings of the 6th conference on Message Understand-
ing, pages 45–52. ACL.
Michael Wick, Sameer Singh, and Andrew McCallum.
2012. A Discriminative Hierarchical Model for Fast
Coreference at Large Scale. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics: Long Papers-Volume 1, pages
379–388. Association for Computational Linguis-
tics.
Chun-Nam John Yu and Thorsten Joachims. 2009.
Learning Structural SVMs with Latent Variables. In
Proceedings of the 26th Annual International Con-
ference on Machine Learning, pages 1169–1176.
ACM.
</reference>
<page confidence="0.78397">
1426
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.727036">
<title confidence="0.999735">Learning Anaphoricity and Antecedent Ranking Features for Coreference Resolution</title>
<author confidence="0.999995">Alexander M M Jason</author>
<affiliation confidence="0.916548">of Engineering and Applied Sciences AI Research Harvard University New York, NY, USA</affiliation>
<address confidence="0.988544">MA, USA</address>
<abstract confidence="0.990927461538462">We introduce a simple, non-linear mention-ranking model for coreference resolution that attempts to learn distinct feature representations for anaphoricity detection and antecedent ranking, which we encourage by pre-training on a pair of corresponding subtasks. Although we use only simple, unconjoined features, the model is able to learn useful representations, and we report the best overall score on the CoNLL 2012 English test set to date.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
<author>Breck Baldwin</author>
</authors>
<title>Algorithms for Scoring Coreference Chains.</title>
<date>1998</date>
<booktitle>In The first international conference on language resources and evaluation workshop on linguistics coreference,</booktitle>
<volume>1</volume>
<pages>563--566</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="23444" citStr="Bagga and Baldwin, 1998" startWordPosition="3857" endWordPosition="3860">R128×da, and for the pairwise representations the matrix dimensions used are Wp E R700×dp. In the g2 model, the outer matrix dimensions are W ER128×(dp+da). With the BASIC+ features, dp and da come out to be slightly less than 106 and 104, respectively, with bilexical head features accounting for the vast majority of dp.7 We tuned all hyper-parameters (as well as those of baseline systems) on the development set. We use the CoNLL 2012 scoring script v8.018 (Pradhan et al., 2014; Luo et al., 2014), which scores based on 3 metrics, including MUC (Vilain et al., 1995), CEAFe (Luo, 2005), and B3 (Bagga and Baldwin, 1998), as well as the CoNLL score, which is the arithmetic mean of the 3 metrics. Code implementing our models is available at https://github.com/swiseman/nn_ coref. The system trains in time comparable to that of linear systems, mainly because we use only raw features and sparse margin-based gradient updates. 5.2 Results Our main results are shown in Table 2. This table compares the performance of our system with the performance reported by several other state-of-the art systems on the CoNLL 2012 English coreference test set. Our full models achieve the best F1 score across two of the three metric</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>Amit Bagga and Breck Baldwin. 1998. Algorithms for Scoring Coreference Chains. In The first international conference on language resources and evaluation workshop on linguistics coreference, volume 1, pages 563–566. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yonatan Belinkov</author>
<author>Tao Lei</author>
<author>Regina Barzilay</author>
<author>Amir Globerson</author>
</authors>
<date>2014</date>
<booktitle>Exploring Compositional Architectures and Word Vector Representations for Prepositional Phrase Attachment. Transactions of the Association for Computational Linguistics,</booktitle>
<pages>2--561</pages>
<contexts>
<context position="37794" citStr="Belinkov et al., 2014" startWordPosition="6257" endWordPosition="6260">t representations. There has also been some work on automatically inducing feature conjunctions for use in coreference systems (Fernandes et al., 2012; Lassalle and Denis, 2013), though the approach we present here is somewhat simpler, and unlike that of Lassalle and Denis (2013) is designed for use on system rather than gold mentions. There has been much interest recently in using neural networks for classic natural language tasks such as tagging and semantic role labeling Collobert et al. (2011), sentiment analysis (Socher et al., 2011; Socher et al., 2012), prepositional phrase attachment (Belinkov et al., 2014) among others. These systems often use some form of pretraining for initialization, often word-embeddings learned from external tasks. However, there has been little work of this form for coreference resolution. 8 Conclusion We have presented a simple, local model capable of learning feature representations useful for coreference-related subtasks, and of thereby achieving state-of-the-art performance. Because our approach automatically learns intermediate representations given raw features, directions for further research might alternately explore including additional (perhaps semantic) raw fe</context>
</contexts>
<marker>Belinkov, Lei, Barzilay, Globerson, 2014</marker>
<rawString>Yonatan Belinkov, Tao Lei, Regina Barzilay, and Amir Globerson. 2014. Exploring Compositional Architectures and Word Vector Representations for Prepositional Phrase Attachment. Transactions of the Association for Computational Linguistics, 2:561–572.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Bengtson</author>
<author>Dan Roth</author>
</authors>
<title>Understanding the Value of Features for Coreference Resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>294--303</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="4134" citStr="Bengtson and Roth, 2008" startWordPosition="609" endWordPosition="612"> 1.5 points over the state-of-the-art coreference system. Moreover, unlike current state-ofthe-art systems, our model does only local inference, and is therefore significantly simpler. 1.1 Problem Setting We consider here the mention-ranking (or “mention-synchronous”) approach to coreference 1416 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1416–1426, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics resolution (Denis and Baldridge, 2008; Bengtson and Roth, 2008; Rahman and Ng, 2009), which has been adopted by several recent coreference systems (Durrett and Klein, 2013; Chang et al., 2013). Such systems aim to identify whether a mention is coreferent with an antecedent mention, or whether it is instead non-anaphoric (the first mention in the document referring to a particular entity). This is accomplished by assigning a score to the mention’s potential antecedents as well as to the possibility that it is non-anaphoric, and then predicting the greatest scoring option. We furthermore assume the more realistic “system mention” setting, where it is not k</context>
<context position="17180" citStr="Bengtson and Roth, 2008" startWordPosition="2813" endWordPosition="2816">.36 75.33 81.65 BASIC+ (N) Lin. 74.14 74.71 74.43 74.02 BASIC+ (Y) Lin. 74.24 75.39 74.81 80.44 BASIC+ (N) NN 75.84 76.02 75.93 82.86 Table 1: Performance of the two subtasks on the CoNLL 2012 development set by feature set and model type. “Conj.” indicates whether conjunctions are used. The linear anaphoric system is an SVM (LibLinear implementation (Fan et al., 2008)), and the linear antecedent system is a linear model with the margin-based objective. local context.4 Anaphoricity detection in various forms has been used as an initial step in several coreference systems (Ng and Cardie, 2002; Bengtson and Roth, 2008; Rahman and Ng, 2009; Bj¨orkelund and Farkas, 2012), and the related question of whether a mention can be determined to be a singleton or not has been explored recently by Recasens et al. (2013), Ma et al. (2014), and others.5 Formally, let tn ∈ {−1, 1} indicate whether c ∈ C&apos;(xn) or not (respectively). That is, tn =1 if and only if xn is anaphoric. Define the subtask scoring function sa : X → Ras sa(x) g vaTha(x) + ν0 , where the vector va and the bias ν0 are specific to this subtask and are discarded after pre-training. We train this model to minimize the following slack-rescaled objective </context>
<context position="35680" citStr="Bengtson and Roth, 2008" startWordPosition="5917" endWordPosition="5920"> to prevent excessive pronounpronoun linking, even in a local model. 7 Related Work There is a voluminous literature on machine learning approaches to coreference resolution, effectively beginning with Soon et al. (2001). The recent introduction of the CoNLL datasets (Pradhan et al., 2012) has spurred research that takes advantage of more fine-grained features and richer models (Bj¨orkelund and Farkas, 2012; Chang et al., 2012; Durrett and Klein, 2013; Chang et al., 2013; Bj¨orkelund and Kuhn, 2014; Ma et al., 2014). Of these approaches, our model is related to the mention-ranking approaches (Bengtson and Roth, 2008; Denis and Baldridge, 2008; Rahman and Ng, 2009; Durrett and Klein, 2013; Chang et al., 2013), as opposed to those that focus on non-local, structured prediction (McCallum and Wellner, 2003; Culotta et al., 2006; Haghighi and Klein, 2010; Fernandes et al., 2012; Stoyanov and Eisner, 2012; Bj¨orkelund and Farkas, 2012; Wick et al., 2012; Bj¨orkelund and Kuhn, 2014; Durrett and Klein, 2014). In motivation, our work is most similar to that of Ng (2004), who notes that anaphoricity information is useful within the broader coreference task, and who accordingly attempts to “globally” optimize perfo</context>
<context position="37032" citStr="Bengtson and Roth, 2008" startWordPosition="6138" endWordPosition="6141">predictions using ILP. Both of these works are taken to contrast with the more popular approach of doing an initial non-anaphoric pruning step (Ng and Cardie, 2002; Rahman and Ng, 2009; Recasens et al., 2013; Lee et al., 2013). In contrast, we jointly learn non-linear functions of anaphoricity and antecedent features, rather than tune a threshold, or jointly decode based on independently trained classifiers (as in Denis et al. (2007)). In a similar vein, several authors have also proposed using the output of an anaphoricity classifier as a feature in a downstream coreference system (Ng, 2004; Bengtson and Roth, 2008). In our framework we (re)learn features jointly with the full task, after a pre-training scheme that targets anaphoricity as well antecedent representations. There has also been some work on automatically inducing feature conjunctions for use in coreference systems (Fernandes et al., 2012; Lassalle and Denis, 2013), though the approach we present here is somewhat simpler, and unlike that of Lassalle and Denis (2013) is designed for use on system rather than gold mentions. There has been much interest recently in using neural networks for classic natural language tasks such as tagging and sema</context>
</contexts>
<marker>Bengtson, Roth, 2008</marker>
<rawString>Eric Bengtson and Dan Roth. 2008. Understanding the Value of Features for Coreference Resolution. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 294–303. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Bj¨orkelund</author>
<author>Rich´ard Farkas</author>
</authors>
<title>Datadriven Multilingual Coreference Resolution using Resolver Stacking.</title>
<date>2012</date>
<booktitle>In Joint Conference on EMNLP and CoNLL-Shared Task,</booktitle>
<pages>49--55</pages>
<publisher>ACL.</publisher>
<marker>Bj¨orkelund, Farkas, 2012</marker>
<rawString>Anders Bj¨orkelund and Rich´ard Farkas. 2012. Datadriven Multilingual Coreference Resolution using Resolver Stacking. In Joint Conference on EMNLP and CoNLL-Shared Task, pages 49–55. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Bj¨orkelund</author>
<author>Jonas Kuhn</author>
</authors>
<title>Learning structured perceptrons for coreference Resolution with Latent Antecedents and Non-local Features.</title>
<date>2014</date>
<location>ACL, Baltimore, MD, USA,</location>
<marker>Bj¨orkelund, Kuhn, 2014</marker>
<rawString>Anders Bj¨orkelund and Jonas Kuhn. 2014. Learning structured perceptrons for coreference Resolution with Latent Antecedents and Non-local Features. ACL, Baltimore, MD, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai-Wei Chang</author>
<author>Rajhans Samdani</author>
<author>Alla Rozovskaya</author>
<author>Mark Sammons</author>
<author>Dan Roth</author>
</authors>
<date>2012</date>
<booktitle>Illinoiscoref: The UI System in the CoNLL-2012 Shared Task. In Joint Conference on EMNLP and CoNLLShared Task,</booktitle>
<pages>113--117</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5700" citStr="Chang et al., 2012" startWordPosition="879" endWordPosition="882">efore x; we refer to this set as x’s potential antecedents. Additionally let the symbol c denote the empty antecedent, to which we will view x as referring when x is non-anaphoric.1 Denoting the set A(x) U {E} by Y(x), a mentionranking model defines a scoring function s(x, y) : X x Y → R, and predicts the antecedent of x to be y∗ = arg maxy∈Y(x) s(x, y). It is common to be quite liberal when extracting mentions, taking, essentially, every noun phrase or pronoun to be a candidate mention, so as not to prematurely discard those that might be coreferent (Lee et al., 2011; Fernandes et al., 2012; Chang et al., 2012; Durrett and Klein, 2013). For instance, the Berkeley Coreference System (herein BCS) (Durrett and Klein, 2013), which we use for mention extraction in our experiments, recovers approximately 96.4% of the truly anaphoric mentions in the CoNLL 2012 training set, with an almost 3.5:1 ratio of non-anaphoric mentions to anaphoric mentions among the extracted mentions. 2 Mention Ranking Models The structural simplicity of the mention-ranking framework puts much of the burden on the scoring function s(x, y). We begin by considering mention-ranking systems using linear scoring 1We make this stipulat</context>
<context position="35487" citStr="Chang et al., 2012" startWordPosition="5886" endWordPosition="5889">ggests that such a “non-local” rule may not be particularly useful, though further experiments are required. It is also worth noting that a suitably modified loss function may also be able to prevent excessive pronounpronoun linking, even in a local model. 7 Related Work There is a voluminous literature on machine learning approaches to coreference resolution, effectively beginning with Soon et al. (2001). The recent introduction of the CoNLL datasets (Pradhan et al., 2012) has spurred research that takes advantage of more fine-grained features and richer models (Bj¨orkelund and Farkas, 2012; Chang et al., 2012; Durrett and Klein, 2013; Chang et al., 2013; Bj¨orkelund and Kuhn, 2014; Ma et al., 2014). Of these approaches, our model is related to the mention-ranking approaches (Bengtson and Roth, 2008; Denis and Baldridge, 2008; Rahman and Ng, 2009; Durrett and Klein, 2013; Chang et al., 2013), as opposed to those that focus on non-local, structured prediction (McCallum and Wellner, 2003; Culotta et al., 2006; Haghighi and Klein, 2010; Fernandes et al., 2012; Stoyanov and Eisner, 2012; Bj¨orkelund and Farkas, 2012; Wick et al., 2012; Bj¨orkelund and Kuhn, 2014; Durrett and Klein, 2014). In motivation</context>
</contexts>
<marker>Chang, Samdani, Rozovskaya, Sammons, Roth, 2012</marker>
<rawString>Kai-Wei Chang, Rajhans Samdani, Alla Rozovskaya, Mark Sammons, and Dan Roth. 2012. Illinoiscoref: The UI System in the CoNLL-2012 Shared Task. In Joint Conference on EMNLP and CoNLLShared Task, pages 113–117. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai-Wei Chang</author>
<author>Rajhans Samdani</author>
<author>Dan Roth</author>
</authors>
<title>A Constrained Latent Variable Model for Coreference Resolution.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>601--612</pages>
<contexts>
<context position="4264" citStr="Chang et al., 2013" startWordPosition="630" endWordPosition="633">inference, and is therefore significantly simpler. 1.1 Problem Setting We consider here the mention-ranking (or “mention-synchronous”) approach to coreference 1416 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1416–1426, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics resolution (Denis and Baldridge, 2008; Bengtson and Roth, 2008; Rahman and Ng, 2009), which has been adopted by several recent coreference systems (Durrett and Klein, 2013; Chang et al., 2013). Such systems aim to identify whether a mention is coreferent with an antecedent mention, or whether it is instead non-anaphoric (the first mention in the document referring to a particular entity). This is accomplished by assigning a score to the mention’s potential antecedents as well as to the possibility that it is non-anaphoric, and then predicting the greatest scoring option. We furthermore assume the more realistic “system mention” setting, where it is not known a priori which mentions in a document participate in coreference clusters, and so (all) mentions must be automatically extrac</context>
<context position="14035" citStr="Chang et al., 2013" startWordPosition="2281" endWordPosition="2284">nsisting of the raw features in Figure 2 without conjunctions. Any interaction between these features must be learned by the feature representations hp and ha. 3.2 Training We can directly train our model using backpropagation. To specify the training problem, we first define notation for the training objective. Define the set C(x) to contain just the mentions in A(x) that are coreferent with x. We then define Finally, let y`n = arg maxyEC&apos;(xn) s(xn, y) be the highest scoring correct antecedent of xn, which may be E. (Thus, following recent work (Yu and Joachims, 2009; Fernandes et al., 2012; Chang et al., 2013; Durrett and Klein, 2013), we view each mention as having a “latent antecedent”.2) We train to minimize the regularized, slack-rescaled, 2Note that this renders the objectives of even models with a linear scoring function non-convex. latent-variable loss3 given by: A(xn, ˆy)(1 + 8(xn, ˆy)−s(xn, y`n)) + λ||θ||1, where Δ is a mistake-specific cost function, which is 0 when yˆ E C&apos;(xn). Above, we use θ to refer to the full set of parameters {W, u, v, Wa, Wp, ba, bp}. For experiments, we define Δ to take on different costs for the three kinds of mistakes possible in a coreference task, as follows</context>
<context position="35532" citStr="Chang et al., 2013" startWordPosition="5894" endWordPosition="5897">e particularly useful, though further experiments are required. It is also worth noting that a suitably modified loss function may also be able to prevent excessive pronounpronoun linking, even in a local model. 7 Related Work There is a voluminous literature on machine learning approaches to coreference resolution, effectively beginning with Soon et al. (2001). The recent introduction of the CoNLL datasets (Pradhan et al., 2012) has spurred research that takes advantage of more fine-grained features and richer models (Bj¨orkelund and Farkas, 2012; Chang et al., 2012; Durrett and Klein, 2013; Chang et al., 2013; Bj¨orkelund and Kuhn, 2014; Ma et al., 2014). Of these approaches, our model is related to the mention-ranking approaches (Bengtson and Roth, 2008; Denis and Baldridge, 2008; Rahman and Ng, 2009; Durrett and Klein, 2013; Chang et al., 2013), as opposed to those that focus on non-local, structured prediction (McCallum and Wellner, 2003; Culotta et al., 2006; Haghighi and Klein, 2010; Fernandes et al., 2012; Stoyanov and Eisner, 2012; Bj¨orkelund and Farkas, 2012; Wick et al., 2012; Bj¨orkelund and Kuhn, 2014; Durrett and Klein, 2014). In motivation, our work is most similar to that of Ng (200</context>
</contexts>
<marker>Chang, Samdani, Roth, 2013</marker>
<rawString>Kai-Wei Chang, Rajhans Samdani, and Dan Roth. 2013. A Constrained Latent Variable Model for Coreference Resolution. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 601–612.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural Language Processing (almost) from Scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="2330" citStr="Collobert et al., 2011" startWordPosition="343" endWordPosition="346">kelund and Kuhn, 2014) and, as a result, state-of-the-art systems tend to use linear models with complicated feature conjunction schemes in order to capture more fine-grained interactions. While this approach has shown success, it is not obvious which additional feature conjunctions will lead to improved performance, which is problematic as systems attempt to scale with new data and features. In this work, we propose a data-driven model for coreference that does not require prespecifying any feature relationships. Inspired by recent work in learning representations for natural language tasks (Collobert et al., 2011), we explore neural network models which take only raw, unconjoined features as input, and attempt to learn intermediate representations automatically. In particular, the model we describe attempts to create independent feature representations useful for both detecting the anaphoricity of a mention (that is, whether or not a mention is anaphoric) and ranking the potential antecedents of an anaphoric mention. Adequately capturing anaphoricity information has long been thought to be an important aspect of the coreference task (see Ng (2004) and Section 7), since a strong non-anaphoric signal mig</context>
<context position="37674" citStr="Collobert et al. (2011)" startWordPosition="6240" endWordPosition="6243">we (re)learn features jointly with the full task, after a pre-training scheme that targets anaphoricity as well antecedent representations. There has also been some work on automatically inducing feature conjunctions for use in coreference systems (Fernandes et al., 2012; Lassalle and Denis, 2013), though the approach we present here is somewhat simpler, and unlike that of Lassalle and Denis (2013) is designed for use on system rather than gold mentions. There has been much interest recently in using neural networks for classic natural language tasks such as tagging and semantic role labeling Collobert et al. (2011), sentiment analysis (Socher et al., 2011; Socher et al., 2012), prepositional phrase attachment (Belinkov et al., 2014) among others. These systems often use some form of pretraining for initialization, often word-embeddings learned from external tasks. However, there has been little work of this form for coreference resolution. 8 Conclusion We have presented a simple, local model capable of learning feature representations useful for coreference-related subtasks, and of thereby achieving state-of-the-art performance. Because our approach automatically learns intermediate representations give</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural Language Processing (almost) from Scratch. The Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Michael Wick</author>
<author>Robert Hall</author>
<author>Andrew McCallum</author>
</authors>
<title>First-order Probabilistic Models for Coreference Resolution.</title>
<date>2006</date>
<publisher>NAACL-HLT.</publisher>
<contexts>
<context position="35892" citStr="Culotta et al., 2006" startWordPosition="5951" endWordPosition="5954">2001). The recent introduction of the CoNLL datasets (Pradhan et al., 2012) has spurred research that takes advantage of more fine-grained features and richer models (Bj¨orkelund and Farkas, 2012; Chang et al., 2012; Durrett and Klein, 2013; Chang et al., 2013; Bj¨orkelund and Kuhn, 2014; Ma et al., 2014). Of these approaches, our model is related to the mention-ranking approaches (Bengtson and Roth, 2008; Denis and Baldridge, 2008; Rahman and Ng, 2009; Durrett and Klein, 2013; Chang et al., 2013), as opposed to those that focus on non-local, structured prediction (McCallum and Wellner, 2003; Culotta et al., 2006; Haghighi and Klein, 2010; Fernandes et al., 2012; Stoyanov and Eisner, 2012; Bj¨orkelund and Farkas, 2012; Wick et al., 2012; Bj¨orkelund and Kuhn, 2014; Durrett and Klein, 2014). In motivation, our work is most similar to that of Ng (2004), who notes that anaphoricity information is useful within the broader coreference task, and who accordingly attempts to “globally” optimize performance based on this information, as well as that of Denis et al. (2007), who do joint decoding of anaphoricity and coreference predictions using ILP. Both of these works are taken to contrast with the more popul</context>
</contexts>
<marker>Culotta, Wick, Hall, McCallum, 2006</marker>
<rawString>Aron Culotta, Michael Wick, Robert Hall, and Andrew McCallum. 2006. First-order Probabilistic Models for Coreference Resolution. NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Denis</author>
<author>Jason Baldridge</author>
</authors>
<title>Specialized Models and Ranking for Coreference Resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>660--669</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="4109" citStr="Denis and Baldridge, 2008" startWordPosition="605" endWordPosition="608"> et al., 2012), and of over 1.5 points over the state-of-the-art coreference system. Moreover, unlike current state-ofthe-art systems, our model does only local inference, and is therefore significantly simpler. 1.1 Problem Setting We consider here the mention-ranking (or “mention-synchronous”) approach to coreference 1416 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1416–1426, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics resolution (Denis and Baldridge, 2008; Bengtson and Roth, 2008; Rahman and Ng, 2009), which has been adopted by several recent coreference systems (Durrett and Klein, 2013; Chang et al., 2013). Such systems aim to identify whether a mention is coreferent with an antecedent mention, or whether it is instead non-anaphoric (the first mention in the document referring to a particular entity). This is accomplished by assigning a score to the mention’s potential antecedents as well as to the possibility that it is non-anaphoric, and then predicting the greatest scoring option. We furthermore assume the more realistic “system mention” s</context>
<context position="35707" citStr="Denis and Baldridge, 2008" startWordPosition="5921" endWordPosition="5924">nounpronoun linking, even in a local model. 7 Related Work There is a voluminous literature on machine learning approaches to coreference resolution, effectively beginning with Soon et al. (2001). The recent introduction of the CoNLL datasets (Pradhan et al., 2012) has spurred research that takes advantage of more fine-grained features and richer models (Bj¨orkelund and Farkas, 2012; Chang et al., 2012; Durrett and Klein, 2013; Chang et al., 2013; Bj¨orkelund and Kuhn, 2014; Ma et al., 2014). Of these approaches, our model is related to the mention-ranking approaches (Bengtson and Roth, 2008; Denis and Baldridge, 2008; Rahman and Ng, 2009; Durrett and Klein, 2013; Chang et al., 2013), as opposed to those that focus on non-local, structured prediction (McCallum and Wellner, 2003; Culotta et al., 2006; Haghighi and Klein, 2010; Fernandes et al., 2012; Stoyanov and Eisner, 2012; Bj¨orkelund and Farkas, 2012; Wick et al., 2012; Bj¨orkelund and Kuhn, 2014; Durrett and Klein, 2014). In motivation, our work is most similar to that of Ng (2004), who notes that anaphoricity information is useful within the broader coreference task, and who accordingly attempts to “globally” optimize performance based on this inform</context>
</contexts>
<marker>Denis, Baldridge, 2008</marker>
<rawString>Pascal Denis and Jason Baldridge. 2008. Specialized Models and Ranking for Coreference Resolution. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 660–669. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Denis</author>
<author>Jason Baldridge</author>
</authors>
<title>Joint Determination of Anaphoricity and Coreference Resolution using Integer Programming. In</title>
<date>2007</date>
<booktitle>HLT-NAACL,</booktitle>
<pages>236--243</pages>
<publisher>Citeseer.</publisher>
<marker>Denis, Baldridge, 2007</marker>
<rawString>Pascal Denis, Jason Baldridge, et al. 2007. Joint Determination of Anaphoricity and Coreference Resolution using Integer Programming. In HLT-NAACL, pages 236–243. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2121</pages>
<contexts>
<context position="21971" citStr="Duchi et al., 2011" startWordPosition="3613" endWordPosition="3616">on the OntoNotes corpus (Hovy et al., 2006). The data set contains 3,493 documents consisting of 1.6 million words. We use the standard experimental split with the training set containing 2,802 documents and 156K annotated mentions, the development set containing 343 documents and 19K annotated mentions, and the test set containing 348 documents and 20K annotated mentions. For all experiments, we use BCS (Durrett and Klein, 2013) to extract system mentions and to compute some of the features. For training, we minimize the loss described above using the composite mirror descent AdaGrad update (Duchi et al., 2011) with document sized mini-batches.6 We tuned the AdaGrad learning rate and regularization parameters using a grid search over possible learning rates q E {0.001, 0.002, 0.01, 0.02, 0.1, 0.2} and over regularization parameters A E{10−6, ... ,10−1}. For the full coreference task, we use a different learning rate for the pre-trained weights and for the second-layer weights, using q1 = 0.1 and q2 = 0.001, respectively, and A =10−6. When initializing weight-matrices that were not pre-trained 6In preliminary experiments we also used Nesterov’s accelerated gradient (Nesterov, 1983), but found AdaGrad</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. The Journal of Machine Learning Research, 12:2121–2159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Durrett</author>
<author>Dan Klein</author>
</authors>
<title>Easy Victories and Uphill Battles in Coreference Resolution.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>pages</pages>
<contexts>
<context position="1163" citStr="Durrett and Klein, 2013" startWordPosition="165" endWordPosition="168">ge by pre-training on a pair of corresponding subtasks. Although we use only simple, unconjoined features, the model is able to learn useful representations, and we report the best overall score on the CoNLL 2012 English test set to date. 1 Introduction One of the major challenges associated with resolving coreference is that in typical documents the number of mentions (syntactic units capable of referring or being referred to) that are nonanaphoric – that is, that are not coreferent with any previous mention – far exceeds the number of mentions that are anaphoric (Kummerfeld and Klein, 2013; Durrett and Klein, 2013). This preponderance of non-anaphoric mentions makes coreference resolution challenging, partly because many basic coreference features, such as those looking at head, number, or gender match fail to distinguish between truly coreferent pairs and the large number of matching but nonetheless non-coreferent pairs. Indeed, several authors have noted that it is difficult to obtain good performance on the coreference task using simple features (Lee et al., 2011; Fernandes et al., 2012; Durrett and Klein, 2013; Kummerfeld and Klein, 2013; Bj¨orkelund and Kuhn, 2014) and, as a result, state-of-the-ar</context>
<context position="4243" citStr="Durrett and Klein, 2013" startWordPosition="626" endWordPosition="629">ur model does only local inference, and is therefore significantly simpler. 1.1 Problem Setting We consider here the mention-ranking (or “mention-synchronous”) approach to coreference 1416 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1416–1426, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics resolution (Denis and Baldridge, 2008; Bengtson and Roth, 2008; Rahman and Ng, 2009), which has been adopted by several recent coreference systems (Durrett and Klein, 2013; Chang et al., 2013). Such systems aim to identify whether a mention is coreferent with an antecedent mention, or whether it is instead non-anaphoric (the first mention in the document referring to a particular entity). This is accomplished by assigning a score to the mention’s potential antecedents as well as to the possibility that it is non-anaphoric, and then predicting the greatest scoring option. We furthermore assume the more realistic “system mention” setting, where it is not known a priori which mentions in a document participate in coreference clusters, and so (all) mentions must be</context>
<context position="5726" citStr="Durrett and Klein, 2013" startWordPosition="883" endWordPosition="886"> this set as x’s potential antecedents. Additionally let the symbol c denote the empty antecedent, to which we will view x as referring when x is non-anaphoric.1 Denoting the set A(x) U {E} by Y(x), a mentionranking model defines a scoring function s(x, y) : X x Y → R, and predicts the antecedent of x to be y∗ = arg maxy∈Y(x) s(x, y). It is common to be quite liberal when extracting mentions, taking, essentially, every noun phrase or pronoun to be a candidate mention, so as not to prematurely discard those that might be coreferent (Lee et al., 2011; Fernandes et al., 2012; Chang et al., 2012; Durrett and Klein, 2013). For instance, the Berkeley Coreference System (herein BCS) (Durrett and Klein, 2013), which we use for mention extraction in our experiments, recovers approximately 96.4% of the truly anaphoric mentions in the CoNLL 2012 training set, with an almost 3.5:1 ratio of non-anaphoric mentions to anaphoric mentions among the extracted mentions. 2 Mention Ranking Models The structural simplicity of the mention-ranking framework puts much of the burden on the scoring function s(x, y). We begin by considering mention-ranking systems using linear scoring 1We make this stipulation for modeling convenien</context>
<context position="6990" citStr="Durrett and Klein, 2013" startWordPosition="1089" endWordPosition="1092">nguistic fact. functions. In the next section, we will extend these models to operate over learned non-linear representations. Linear mention-ranking models generally utilize the following scoring function slin(x, y) g wTφ(x, y) , where φ: X x Y → Rd is a pairwise feature function defined on a mention and a potential antecedent, and w is a learned parameter vector. To add additional flexibility to the model, linear mention ranking models may duplicate individual features in φ, with one version being used when predicting an antecedent for x, and another when predicting that x is non-anaphoric (Durrett and Klein, 2013). Such a scheme effectively gives rise to the following piecewise scoring function slin+(x, y) = u L φp(x,y) J E � T r φ.(x) 1 vTφa(x) if y = E , where φa : X → Rd. is a feature function defined on a mention and its context, φp : X x Y → Rdp is a pairwise feature function defined on a mention and a potential antecedent, and parameters u and v replace w. Above, we have made an explicit distinction between pairwise features (φp) and those strictly on x and its context (φa), and moreover assumed that our features need not examine potential antecedents when predicting y = c. We refer to the basic,</context>
<context position="9254" citStr="Durrett and Klein (2013)" startWordPosition="1470" endWordPosition="1473">a as independent predictors of anaphoricity (y =� e), and (bottom) antecedent-mention features from φp as independent predictors of coreferent mentions. Very few raw features are strong indicators of either anaphoricity or an antecedent match. Data taken from the CoNLL development set. graphs show that the vast majority of individual features do not give a strong positive signal either of anaphoricity or for an antecedent match. To address this issue, state-of-the-art mentionranking systems often rely on manual or otherwise induced conjunction schemes to capture specific feature interactions. Durrett and Klein (2013), for instance, conjoin all raw features in Oa with the type of the mention x, and all raw features in Op with the types of the current mention and antecedent. For these purposes, the type of a mention is either “nominal”, “proper”, or a canonicalization of the pronoun if it is a pronominal mention. Fernandes et al. (2012) and Bj¨orkelund and Kuhn (2014) use an automatic but complicated scheme to induce conjunctions by first extracting feature templates from a separately trained decision tree, and then doing greedy forward selection among the templates. These conjunctions add some nonlinearity</context>
<context position="14061" citStr="Durrett and Klein, 2013" startWordPosition="2285" endWordPosition="2288">features in Figure 2 without conjunctions. Any interaction between these features must be learned by the feature representations hp and ha. 3.2 Training We can directly train our model using backpropagation. To specify the training problem, we first define notation for the training objective. Define the set C(x) to contain just the mentions in A(x) that are coreferent with x. We then define Finally, let y`n = arg maxyEC&apos;(xn) s(xn, y) be the highest scoring correct antecedent of xn, which may be E. (Thus, following recent work (Yu and Joachims, 2009; Fernandes et al., 2012; Chang et al., 2013; Durrett and Klein, 2013), we view each mention as having a “latent antecedent”.2) We train to minimize the regularized, slack-rescaled, 2Note that this renders the objectives of even models with a linear scoring function non-convex. latent-variable loss3 given by: A(xn, ˆy)(1 + 8(xn, ˆy)−s(xn, y`n)) + λ||θ||1, where Δ is a mistake-specific cost function, which is 0 when yˆ E C&apos;(xn). Above, we use θ to refer to the full set of parameters {W, u, v, Wa, Wp, ba, bp}. For experiments, we define Δ to take on different costs for the three kinds of mistakes possible in a coreference task, as follows: Δ(x, ˆy) = α2 if yˆ = e </context>
<context position="21785" citStr="Durrett and Klein, 2013" startWordPosition="3582" endWordPosition="3585">on of our model with the analogous linear model on some challenging non-anaphoric cases. 5.1 Methods All experiments use the CoNLL 2012 English dataset (Pradhan et al., 2012), which is based on the OntoNotes corpus (Hovy et al., 2006). The data set contains 3,493 documents consisting of 1.6 million words. We use the standard experimental split with the training set containing 2,802 documents and 156K annotated mentions, the development set containing 343 documents and 19K annotated mentions, and the test set containing 348 documents and 20K annotated mentions. For all experiments, we use BCS (Durrett and Klein, 2013) to extract system mentions and to compute some of the features. For training, we minimize the loss described above using the composite mirror descent AdaGrad update (Duchi et al., 2011) with document sized mini-batches.6 We tuned the AdaGrad learning rate and regularization parameters using a grid search over possible learning rates q E {0.001, 0.002, 0.01, 0.02, 0.1, 0.2} and over regularization parameters A E{10−6, ... ,10−1}. For the full coreference task, we use a different learning rate for the pre-trained weights and for the second-layer weights, using q1 = 0.1 and q2 = 0.001, respectiv</context>
<context position="25437" citStr="Durrett and Klein (2013)" startWordPosition="4190" endWordPosition="4193"> MUC F1 P B3 F1 P CEAF, F1 CoNLL R R R BCS (2013) 74.89 67.17 70.82 64.26 53.09 58.14 58.12 52.67 55.27 61.41 Prune&amp;Score (2014) 81.03 66.16 72.84 66.9 51.10 57.94 68.75 44.34 53.91 61.56 B&amp;K (2014) 74.3 67.46 70.72 62.71 54.96 58.58 59.4 52.27 55.61 61.63 D&amp;K (2014) 72.73 69.98 71.33 61.18 56.60 58.80 56.20 54.31 55.24 61.79 This work (g2) 76.96 68.10 72.26 66.90 54.12 59.84 59.02 53.34 56.03 62.71 This work (g1) 76.23 69.31 72.60 66.07 55.83 60.52 59.41 54.88 57.05 63.39 Table 2: Results on CoNLL 2012 English test set. We compare against recent state-of-the-art systems, including (in order) Durrett and Klein (2013), Ma et al. (2014), Bj¨orkelund and Kuhn (2014), and Durrett and Klein (2014) (rescored with the v8.01 scorer). F1 gains are significant (p &lt; 0.05 under the bootstrap resample test (Koehn, 2004)) compared with both B&amp;K and D&amp;K for all metrics. MUC B3 CEAF, CoNLL 71.80 60.93 57.51 63.41 71.77 60.84 57.05 63.22 71.92 61.06 57.59 63.52 72.74 61.77 58.63 64.38 72.31 61.79 58.06 64.05 72.68 61.70 58.32 64.23 Model Fully Conn. 1 Layer Fully Conn. 2 Layer g1 + RI g1 + PT g2 + RI g2 + PT Table 4: Comparison of performance (in F1 score) of various models on CoNLL 2012 development set using BASIC+ featu</context>
<context position="28214" citStr="Durrett and Klein (2013)" startWordPosition="4656" endWordPosition="4659"> comparing with the results of training from a random initialization. We see that while even randomly initialized models are capable of excellent performance, pre-training is beneficial, especially for g1. 9We also experimented with bilinear models both with and without non-linearities; these were also inferior. 6 Discussion We attempt to gain insight into our model’s errors using using two different error breakdowns. In Table 5 we show the errors as reported by the analysis tool of Kummerfeld and Klein (2013). In Table 6 we show a more fine-grained breakdown inspired by a similar analysis in Durrett and Klein (2013). In the latter table, we categorize the errors made by our system on the CoNLL 2012 development data in terms of (1) whether or not the mention has a head match with a previously occurring mention in the document, unless it is a pronominal mention, which we treat separately, (2) in terms of the status of the mention in the gold clustering, namely, singleton, first-in-cluster, or anaphoric, and (3) in terms of the type of error made (which, as discussed in Section 3, are one of FL, FN, and WL). We note that the two models have slightly different error profiles, with g1 being slightly better at</context>
<context position="35512" citStr="Durrett and Klein, 2013" startWordPosition="5890" endWordPosition="5893">non-local” rule may not be particularly useful, though further experiments are required. It is also worth noting that a suitably modified loss function may also be able to prevent excessive pronounpronoun linking, even in a local model. 7 Related Work There is a voluminous literature on machine learning approaches to coreference resolution, effectively beginning with Soon et al. (2001). The recent introduction of the CoNLL datasets (Pradhan et al., 2012) has spurred research that takes advantage of more fine-grained features and richer models (Bj¨orkelund and Farkas, 2012; Chang et al., 2012; Durrett and Klein, 2013; Chang et al., 2013; Bj¨orkelund and Kuhn, 2014; Ma et al., 2014). Of these approaches, our model is related to the mention-ranking approaches (Bengtson and Roth, 2008; Denis and Baldridge, 2008; Rahman and Ng, 2009; Durrett and Klein, 2013; Chang et al., 2013), as opposed to those that focus on non-local, structured prediction (McCallum and Wellner, 2003; Culotta et al., 2006; Haghighi and Klein, 2010; Fernandes et al., 2012; Stoyanov and Eisner, 2012; Bj¨orkelund and Farkas, 2012; Wick et al., 2012; Bj¨orkelund and Kuhn, 2014; Durrett and Klein, 2014). In motivation, our work is most simila</context>
</contexts>
<marker>Durrett, Klein, 2013</marker>
<rawString>Greg Durrett and Dan Klein. 2013. Easy Victories and Uphill Battles in Coreference Resolution. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1971– 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Durrett</author>
<author>Dan Klein</author>
</authors>
<title>A Joint Model for Entity Analysis: Coreference, Typing, and Linking.</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>2--477</pages>
<marker>Durrett, Klein, 2014</marker>
<rawString>Greg Durrett and Dan Klein. 2014. A Joint Model for Entity Analysis: Coreference, Typing, and Linking. Transactions of the Association for Computational Linguistics, 2:477–490.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dumitru Erhan</author>
<author>Yoshua Bengio</author>
<author>Aaron Courville</author>
<author>Pierre-Antoine Manzagol</author>
<author>Pascal Vincent</author>
<author>Samy Bengio</author>
</authors>
<title>Why does unsupervised pre-training help deep learning?</title>
<date>2010</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>11--625</pages>
<contexts>
<context position="15159" citStr="Erhan et al., 2010" startWordPosition="2481" endWordPosition="2484">e on different costs for the three kinds of mistakes possible in a coreference task, as follows: Δ(x, ˆy) = α2 if yˆ = e n e V C&apos;(x) (α3 if yˆ=�e n yˆE�C&apos;(x) α1 if yˆ#e n e E C&apos;(x) . The αi determine the trade-off between these mistakes (and thus precision and recall). Adopting the terminology of BCS, we refer to these mistakes as “false link” (FL), “false new” (FN), and “wrong link” (WL), respectively. 4 Representations from Subtasks While we could train our full model directly, it is known to be difficult to train high performing nonconvex neural-network models from a random initialization (Erhan et al., 2010). In order to overcome the problems associated with training from this setting, and to learn feature representations useful for the full coreference task, we pretrain subparts of the model on the subtasks targeting the desired feature representations. We then train the entire model on the full coreference task (from the pre-trained initializations). As we will see, the pre-training scheme outlined below helps the model achieve improved performance. The proposed pre-training scheme involves learning the parameters associated with ha and hp using two natural subtasks: anaphoricity detection and </context>
</contexts>
<marker>Erhan, Bengio, Courville, Manzagol, Vincent, Bengio, 2010</marker>
<rawString>Dumitru Erhan, Yoshua Bengio, Aaron Courville, Pierre-Antoine Manzagol, Pascal Vincent, and Samy Bengio. 2010. Why does unsupervised pre-training help deep learning? The Journal of Machine Learning Research, 11:625–660.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Liblinear: A Library for Large Linear Classification.</title>
<date>2008</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="16928" citStr="Fan et al., 2008" startWordPosition="2772" endWordPosition="2775">therefore faster. ( C(x) if x is anaphoric C�(x) = {E} otherwise . N E n=1 L(θ) = max ˆyEY(xn) 1419 Feat. (Conj.) Model P Anaphoric Fl Ante R Acc. BASIC (N) Lin. 74.15 74.20 74.18 69.10 BASIC (Y) Lin. 73.98 75.04 74.51 79.76 BASIC (N) NN 75.30 75.36 75.33 81.65 BASIC+ (N) Lin. 74.14 74.71 74.43 74.02 BASIC+ (Y) Lin. 74.24 75.39 74.81 80.44 BASIC+ (N) NN 75.84 76.02 75.93 82.86 Table 1: Performance of the two subtasks on the CoNLL 2012 development set by feature set and model type. “Conj.” indicates whether conjunctions are used. The linear anaphoric system is an SVM (LibLinear implementation (Fan et al., 2008)), and the linear antecedent system is a linear model with the margin-based objective. local context.4 Anaphoricity detection in various forms has been used as an initial step in several coreference systems (Ng and Cardie, 2002; Bengtson and Roth, 2008; Rahman and Ng, 2009; Bj¨orkelund and Farkas, 2012), and the related question of whether a mention can be determined to be a singleton or not has been explored recently by Recasens et al. (2013), Ma et al. (2014), and others.5 Formally, let tn ∈ {−1, 1} indicate whether c ∈ C&apos;(xn) or not (respectively). That is, tn =1 if and only if xn is anapho</context>
<context position="20124" citStr="Fan et al., 2008" startWordPosition="3321" endWordPosition="3324">est of the parameters. For training, we use an analogous latent-variable loss function to that used for the full coreference task, except we replace C&apos; with C, and the cost D(x, ˆy) is always 1 (when it is nonzero). 4.3 Subtask Performance As a preliminary experiment, we train models for these two subtasks using both the BASIC and BASIC+ raw features. Table 1 shows the results. For the first subtask, experiments look at the precision, recall, and F1 score of predicting anaphoric mentions on the CoNLL 2012 development set. As a baseline we use an L1-regularized SVM implemented using LibLinear (Fan et al., 2008), both using raw features and using features conjoined according to the BCS scheme. For the second subtask, experiments look at the accuracy of the model in predicting the correct antecedent on known anaphoric mentions. As a baseline we use a linear mention ranking model, with and without 1420 conjunctions, trained using the same margin-based loss. In both subtasks, the neural network model performs quite well, significantly better than the unconjoined baselines and better than the model trained with manually conjoined features. We provide a visual representation of the antecedent ranking feat</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. Liblinear: A Library for Large Linear Classification. The Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<title>Eraldo Rezende Fernandes, C´ıcero Nogueira Dos Santos, and Ruy Luiz Milidi´u.</title>
<date>2012</date>
<booktitle>In Joint Conference on EMNLP and CoNLL-Shared Task,</booktitle>
<pages>41--48</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8250" citStr="(2012)" startWordPosition="1325" endWordPosition="1325">es. Figure 2 shows two versions of these features, a base set BASIC and an extended set BASIC+. The BASIC set are the raw features used in BCS, and BASIC+ includes additional raw features used in other recent coreference sytems. For instance, BASIC+ additionally includes features suggested by Recasens et al. (2013) to be useful for anaphoricity, such as the number of a mention, its named entity status, and its animacy, as well as number and gender information. We additionally include bilexical head features, which are used in many wellperforming systems (for instance, that of Fernandes et al. (2012)). 2.1 Problems with Raw Features Many authors have observed that, taken individually, raw features tend to not be particularly predictive for the coreference task. We examine this phenomenon empirically in Figure 1. These 1417 Figure 1: Two histograms illustrating the predictive ability of raw (unconjoined) features per feature occurrence: (top) mention-context features from φa as independent predictors of anaphoricity (y =� e), and (bottom) antecedent-mention features from φp as independent predictors of coreferent mentions. Very few raw features are strong indicators of either anaphoricity </context>
<context position="9578" citStr="(2012)" startWordPosition="1533" endWordPosition="1533">es do not give a strong positive signal either of anaphoricity or for an antecedent match. To address this issue, state-of-the-art mentionranking systems often rely on manual or otherwise induced conjunction schemes to capture specific feature interactions. Durrett and Klein (2013), for instance, conjoin all raw features in Oa with the type of the mention x, and all raw features in Op with the types of the current mention and antecedent. For these purposes, the type of a mention is either “nominal”, “proper”, or a canonicalization of the pronoun if it is a pronominal mention. Fernandes et al. (2012) and Bj¨orkelund and Kuhn (2014) use an automatic but complicated scheme to induce conjunctions by first extracting feature templates from a separately trained decision tree, and then doing greedy forward selection among the templates. These conjunctions add some nonlinearity to the scoring function while still maintaining a tractable, though large, feature set. 3 Learning Features for Ranking As an alternative to the aforementioned feature conjunction schemes, we consider learning feature representations in order to better capture relevant aspects of the task. Representation learning affords </context>
</contexts>
<marker>2012</marker>
<rawString>Eraldo Rezende Fernandes, C´ıcero Nogueira Dos Santos, and Ruy Luiz Milidi´u. 2012. Latent Structure Perceptron with Feature Induction for Unrestricted Coreference Resolution. In Joint Conference on EMNLP and CoNLL-Shared Task, pages 41–48. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Coreference Resolution in a Modular, Entity-centered Model.</title>
<date>2010</date>
<booktitle>In The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>385--393</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="35918" citStr="Haghighi and Klein, 2010" startWordPosition="5955" endWordPosition="5958">oduction of the CoNLL datasets (Pradhan et al., 2012) has spurred research that takes advantage of more fine-grained features and richer models (Bj¨orkelund and Farkas, 2012; Chang et al., 2012; Durrett and Klein, 2013; Chang et al., 2013; Bj¨orkelund and Kuhn, 2014; Ma et al., 2014). Of these approaches, our model is related to the mention-ranking approaches (Bengtson and Roth, 2008; Denis and Baldridge, 2008; Rahman and Ng, 2009; Durrett and Klein, 2013; Chang et al., 2013), as opposed to those that focus on non-local, structured prediction (McCallum and Wellner, 2003; Culotta et al., 2006; Haghighi and Klein, 2010; Fernandes et al., 2012; Stoyanov and Eisner, 2012; Bj¨orkelund and Farkas, 2012; Wick et al., 2012; Bj¨orkelund and Kuhn, 2014; Durrett and Klein, 2014). In motivation, our work is most similar to that of Ng (2004), who notes that anaphoricity information is useful within the broader coreference task, and who accordingly attempts to “globally” optimize performance based on this information, as well as that of Denis et al. (2007), who do joint decoding of anaphoricity and coreference predictions using ILP. Both of these works are taken to contrast with the more popular approach of doing an in</context>
</contexts>
<marker>Haghighi, Klein, 2010</marker>
<rawString>Aria Haghighi and Dan Klein. 2010. Coreference Resolution in a Modular, Entity-centered Model. In The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 385–393. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<title>Ontonotes: the 90% Solution.</title>
<date>2006</date>
<booktitle>In Proceedings of the human language technology conference of the NAACL, Companion Volume: Short Papers,</booktitle>
<pages>57--60</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="21395" citStr="Hovy et al., 2006" startWordPosition="3521" endWordPosition="3524">performance does not imply better performance on the full coreference task, it shows that model can learn useful feature representations with only raw input features. 5 Coreference Experiments Our experiments examine performance as compared with other coreference systems, as well as the effect of features, pre-training, and model architecture. We also perform a qualitative comparison of our model with the analogous linear model on some challenging non-anaphoric cases. 5.1 Methods All experiments use the CoNLL 2012 English dataset (Pradhan et al., 2012), which is based on the OntoNotes corpus (Hovy et al., 2006). The data set contains 3,493 documents consisting of 1.6 million words. We use the standard experimental split with the training set containing 2,802 documents and 156K annotated mentions, the development set containing 343 documents and 19K annotated mentions, and the test set containing 348 documents and 20K annotated mentions. For all experiments, we use BCS (Durrett and Klein, 2013) to extract system mentions and to compute some of the features. For training, we minimize the loss described above using the composite mirror descent AdaGrad update (Duchi et al., 2011) with document sized min</context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. Ontonotes: the 90% Solution. In Proceedings of the human language technology conference of the NAACL, Companion Volume: Short Papers, pages 57–60. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical Significance Tests for Machine Translation Evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>388--395</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="24333" citStr="Koehn, 2004" startWordPosition="4010" endWordPosition="4011">argin-based gradient updates. 5.2 Results Our main results are shown in Table 2. This table compares the performance of our system with the performance reported by several other state-of-the art systems on the CoNLL 2012 English coreference test set. Our full models achieve the best F1 score across two of the three metrics and have the best aggregate (CoNLL) score, with an improvement of over 1.5 points over the best reported result, and of almost 2 points over the best mentionranking system. Our F1 improvements on all three metrics are significant (p &lt; 0.05 under the bootstrap resample test (Koehn, 2004)) as compared with both Bj¨orkelund and Kuhn (2014), and Durrett and Klein (2014), the two most recent, stateof-the-art systems. Since our full models use some additional raw features (although an order of magnitude fewer total features than the comparable conjunction7Note that the BCS conjunction scheme, for instance, applied to our raw features gives a dp and da that are over an order of magnitude larger. 8http://conll.github.io/ reference-coreference-scorers/ 1421 System P MUC F1 P B3 F1 P CEAF, F1 CoNLL R R R BCS (2013) 74.89 67.17 70.82 64.26 53.09 58.14 58.12 52.67 55.27 61.41 Prune&amp;Scor</context>
<context position="25631" citStr="Koehn, 2004" startWordPosition="4224" endWordPosition="4225">.46 70.72 62.71 54.96 58.58 59.4 52.27 55.61 61.63 D&amp;K (2014) 72.73 69.98 71.33 61.18 56.60 58.80 56.20 54.31 55.24 61.79 This work (g2) 76.96 68.10 72.26 66.90 54.12 59.84 59.02 53.34 56.03 62.71 This work (g1) 76.23 69.31 72.60 66.07 55.83 60.52 59.41 54.88 57.05 63.39 Table 2: Results on CoNLL 2012 English test set. We compare against recent state-of-the-art systems, including (in order) Durrett and Klein (2013), Ma et al. (2014), Bj¨orkelund and Kuhn (2014), and Durrett and Klein (2014) (rescored with the v8.01 scorer). F1 gains are significant (p &lt; 0.05 under the bootstrap resample test (Koehn, 2004)) compared with both B&amp;K and D&amp;K for all metrics. MUC B3 CEAF, CoNLL 71.80 60.93 57.51 63.41 71.77 60.84 57.05 63.22 71.92 61.06 57.59 63.52 72.74 61.77 58.63 64.38 72.31 61.79 58.06 64.05 72.68 61.70 58.32 64.23 Model Fully Conn. 1 Layer Fully Conn. 2 Layer g1 + RI g1 + PT g2 + RI g2 + PT Table 4: Comparison of performance (in F1 score) of various models on CoNLL 2012 development set using BASIC+ features. “PT” and “RI” refer to pretraining and random initialization respectively. “Fully Conn.” refers to baseline fully connected networks. See text for further model descriptions. Model Features</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical Significance Tests for Machine Translation Evaluation. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 388–395. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan K Kummerfeld</author>
<author>Dan Klein</author>
</authors>
<title>Errordriven Analysis of Challenges in Coreference Resolution.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Seattle, WA, USA,</location>
<contexts>
<context position="1137" citStr="Kummerfeld and Klein, 2013" startWordPosition="161" endWordPosition="164">nt ranking, which we encourage by pre-training on a pair of corresponding subtasks. Although we use only simple, unconjoined features, the model is able to learn useful representations, and we report the best overall score on the CoNLL 2012 English test set to date. 1 Introduction One of the major challenges associated with resolving coreference is that in typical documents the number of mentions (syntactic units capable of referring or being referred to) that are nonanaphoric – that is, that are not coreferent with any previous mention – far exceeds the number of mentions that are anaphoric (Kummerfeld and Klein, 2013; Durrett and Klein, 2013). This preponderance of non-anaphoric mentions makes coreference resolution challenging, partly because many basic coreference features, such as those looking at head, number, or gender match fail to distinguish between truly coreferent pairs and the large number of matching but nonetheless non-coreferent pairs. Indeed, several authors have noted that it is difficult to obtain good performance on the coreference task using simple features (Lee et al., 2011; Fernandes et al., 2012; Durrett and Klein, 2013; Kummerfeld and Klein, 2013; Bj¨orkelund and Kuhn, 2014) and, as</context>
<context position="28105" citStr="Kummerfeld and Klein (2013)" startWordPosition="4637" endWordPosition="4640">ns) using the BASIC+ features in Table 4.9 There, we also evaluate the effect of pre-training on these models by comparing with the results of training from a random initialization. We see that while even randomly initialized models are capable of excellent performance, pre-training is beneficial, especially for g1. 9We also experimented with bilinear models both with and without non-linearities; these were also inferior. 6 Discussion We attempt to gain insight into our model’s errors using using two different error breakdowns. In Table 5 we show the errors as reported by the analysis tool of Kummerfeld and Klein (2013). In Table 6 we show a more fine-grained breakdown inspired by a similar analysis in Durrett and Klein (2013). In the latter table, we categorize the errors made by our system on the CoNLL 2012 development data in terms of (1) whether or not the mention has a head match with a previously occurring mention in the document, unless it is a pronominal mention, which we treat separately, (2) in terms of the status of the mention in the gold clustering, namely, singleton, first-in-cluster, or anaphoric, and (3) in terms of the type of error made (which, as discussed in Section 3, are one of FL, FN, </context>
<context position="29466" citStr="Kummerfeld and Klein (2013)" startWordPosition="4881" endWordPosition="4884">htly better at precision. Indeed, we see from Table 6 that the two models make a comparable number of total errors (g1 makes only 17 fewer errors overall). The increased precision of the g2 model is presumably due to the second layer around ha and hp in g2 allowing for antecedent evidence to interact with anaphoricity 1422 Error Type BCS NN (g1) NN (g2) Conflated Entities 1603 1434 1371 Extra Mention 651 568 529 Extra Entity 655 623 561 Divided Entity 1989 1837 1835 Missing Mention 1004 997 1005 Missing Entity 1070 1026 1114 Table 5: Absolute error counts from the coreference analysis tool of Kummerfeld and Klein (2013). The upper set roughly corresponds to the precision and the lower to the recall of the coreference clusters produced by the model. NN (g1) Singleton 1st in clust. Anaphoric FL # FL # FN + WL # HM 817 8.2K 147 0.8K 700+318 4.7K No HM 86 19.8K 41 2.4K 677 + 59 1.0K Pron. 948 2.6K 257 0.5K 434+875 7.3K NN (g2) Singleton 1st in clust. Anaphoric FL # FL # FN + WL # HM 770 8.2K 130 0.8K 803+306 4.7K No HM 73 19.8K 39 2.4K 699+ 52 1.0K Pron. 896 2.6K 249 0.5K 456+869 7.3K Table 6: Errors made by NN (g1) (top) and NN (g2) (bottom) on CoNLL 2012 English development data. Rows correspond to (1) mention</context>
</contexts>
<marker>Kummerfeld, Klein, 2013</marker>
<rawString>Jonathan K. Kummerfeld and Dan Klein. 2013. Errordriven Analysis of Challenges in Coreference Resolution. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, Seattle, WA, USA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanuel Lassalle</author>
<author>Pascal Denis</author>
</authors>
<title>Improving Pairwise Coreference Models through Feature Space Hierarchy Learning.</title>
<date>2013</date>
<booktitle>In ACL 2013-Annual meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="37349" citStr="Lassalle and Denis, 2013" startWordPosition="6185" endWordPosition="6189">es, rather than tune a threshold, or jointly decode based on independently trained classifiers (as in Denis et al. (2007)). In a similar vein, several authors have also proposed using the output of an anaphoricity classifier as a feature in a downstream coreference system (Ng, 2004; Bengtson and Roth, 2008). In our framework we (re)learn features jointly with the full task, after a pre-training scheme that targets anaphoricity as well antecedent representations. There has also been some work on automatically inducing feature conjunctions for use in coreference systems (Fernandes et al., 2012; Lassalle and Denis, 2013), though the approach we present here is somewhat simpler, and unlike that of Lassalle and Denis (2013) is designed for use on system rather than gold mentions. There has been much interest recently in using neural networks for classic natural language tasks such as tagging and semantic role labeling Collobert et al. (2011), sentiment analysis (Socher et al., 2011; Socher et al., 2012), prepositional phrase attachment (Belinkov et al., 2014) among others. These systems often use some form of pretraining for initialization, often word-embeddings learned from external tasks. However, there has b</context>
</contexts>
<marker>Lassalle, Denis, 2013</marker>
<rawString>Emmanuel Lassalle and Pascal Denis. 2013. Improving Pairwise Coreference Models through Feature Space Hierarchy Learning. In ACL 2013-Annual meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Yves Peirsman</author>
<author>Angel Chang</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<date>2011</date>
<booktitle>Stanford’s Multi-pass Sieve Coreference Resolution System at the CoNLL-2011 Shared</booktitle>
<pages>1425</pages>
<contexts>
<context position="1623" citStr="Lee et al., 2011" startWordPosition="234" endWordPosition="237">t are not coreferent with any previous mention – far exceeds the number of mentions that are anaphoric (Kummerfeld and Klein, 2013; Durrett and Klein, 2013). This preponderance of non-anaphoric mentions makes coreference resolution challenging, partly because many basic coreference features, such as those looking at head, number, or gender match fail to distinguish between truly coreferent pairs and the large number of matching but nonetheless non-coreferent pairs. Indeed, several authors have noted that it is difficult to obtain good performance on the coreference task using simple features (Lee et al., 2011; Fernandes et al., 2012; Durrett and Klein, 2013; Kummerfeld and Klein, 2013; Bj¨orkelund and Kuhn, 2014) and, as a result, state-of-the-art systems tend to use linear models with complicated feature conjunction schemes in order to capture more fine-grained interactions. While this approach has shown success, it is not obvious which additional feature conjunctions will lead to improved performance, which is problematic as systems attempt to scale with new data and features. In this work, we propose a data-driven model for coreference that does not require prespecifying any feature relationshi</context>
<context position="5656" citStr="Lee et al., 2011" startWordPosition="871" endWordPosition="874">(x) denote the set of mentions appearing before x; we refer to this set as x’s potential antecedents. Additionally let the symbol c denote the empty antecedent, to which we will view x as referring when x is non-anaphoric.1 Denoting the set A(x) U {E} by Y(x), a mentionranking model defines a scoring function s(x, y) : X x Y → R, and predicts the antecedent of x to be y∗ = arg maxy∈Y(x) s(x, y). It is common to be quite liberal when extracting mentions, taking, essentially, every noun phrase or pronoun to be a candidate mention, so as not to prematurely discard those that might be coreferent (Lee et al., 2011; Fernandes et al., 2012; Chang et al., 2012; Durrett and Klein, 2013). For instance, the Berkeley Coreference System (herein BCS) (Durrett and Klein, 2013), which we use for mention extraction in our experiments, recovers approximately 96.4% of the truly anaphoric mentions in the CoNLL 2012 training set, with an almost 3.5:1 ratio of non-anaphoric mentions to anaphoric mentions among the extracted mentions. 2 Mention Ranking Models The structural simplicity of the mention-ranking framework puts much of the burden on the scoring function s(x, y). We begin by considering mention-ranking systems</context>
</contexts>
<marker>Lee, Peirsman, Chang, Chambers, Surdeanu, Jurafsky, 2011</marker>
<rawString>Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011. Stanford’s Multi-pass Sieve Coreference Resolution System at the CoNLL-2011 Shared 1425</rawString>
</citation>
<citation valid="false">
<authors>
<author>Task</author>
</authors>
<booktitle>In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>28--34</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Task, </marker>
<rawString>Task. In Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task, pages 28–34. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Angel Chang</author>
<author>Yves Peirsman</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Deterministic Coreference Resolution based on Entity-centric, Precision-ranked Rules.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>4</issue>
<contexts>
<context position="12103" citStr="Lee et al., 2013" startWordPosition="1926" endWordPosition="1929">e., Ment. Types see above Figure 2: Features used for φa(x) and φp(x, y). The ’+’ indicates a feature is in BASIC+ feature set. V denotes the training vocabulary, and T denotes the set of mention types, viz., {nominal,proper} U {canonical pronouns}, as defined in BCS. Conv. and Art. abbreviate conversation and article (resp.). Lexicalized features occurring fewer than 20 times in the training set back off to part-of-speech; bilexical heads occurring fewer than 10 times back off to an indicator feature. Animacy information is taken from a list and rules used in the Stanford Coreference system (Lee et al., 2013). 3.1 Model We use a neural network to define our model as an extension to the mention-ranking model introduced in Section 2. We consider in particular the scoring function: � o uTg( a(x) + u0 y if E s(x, y) = p(x,y) ) vTha(x) + v0 if y = c , 1418 where ha and hp are feature representations, nonlinear functions of the features φa and φp (respectively), and g is a function of these representations. In particular, we define ha(x) °= tanh(Wa φa(x) + ba) hp(x, y) °= tanh(Wp φp(x, y) + bp) , and we take g to either be the identity function, in which case the above model is analogous to slin+ but de</context>
<context position="36634" citStr="Lee et al., 2013" startWordPosition="6075" endWordPosition="6078">2; Bj¨orkelund and Kuhn, 2014; Durrett and Klein, 2014). In motivation, our work is most similar to that of Ng (2004), who notes that anaphoricity information is useful within the broader coreference task, and who accordingly attempts to “globally” optimize performance based on this information, as well as that of Denis et al. (2007), who do joint decoding of anaphoricity and coreference predictions using ILP. Both of these works are taken to contrast with the more popular approach of doing an initial non-anaphoric pruning step (Ng and Cardie, 2002; Rahman and Ng, 2009; Recasens et al., 2013; Lee et al., 2013). In contrast, we jointly learn non-linear functions of anaphoricity and antecedent features, rather than tune a threshold, or jointly decode based on independently trained classifiers (as in Denis et al. (2007)). In a similar vein, several authors have also proposed using the output of an anaphoricity classifier as a feature in a downstream coreference system (Ng, 2004; Bengtson and Roth, 2008). In our framework we (re)learn features jointly with the full task, after a pre-training scheme that targets anaphoricity as well antecedent representations. There has also been some work on automatica</context>
</contexts>
<marker>Lee, Chang, Peirsman, Chambers, Surdeanu, Jurafsky, 2013</marker>
<rawString>Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2013. Deterministic Coreference Resolution based on Entity-centric, Precision-ranked Rules. Computational Linguistics, 39(4):885–916.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
<author>Sameer Pradhan</author>
<author>Marta Recasens</author>
<author>Eduard Hovy</author>
</authors>
<title>An Extension of BLANC to System Mentions.</title>
<date>2014</date>
<booktitle>Proceedings of ACL,</booktitle>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="23321" citStr="Luo et al., 2014" startWordPosition="3836" endWordPosition="3839">ost-weights α = (0.5,1.2,1) in defining A. For the anaphoricity representations the matrix dimensions used are Wa E R128×da, and for the pairwise representations the matrix dimensions used are Wp E R700×dp. In the g2 model, the outer matrix dimensions are W ER128×(dp+da). With the BASIC+ features, dp and da come out to be slightly less than 106 and 104, respectively, with bilexical head features accounting for the vast majority of dp.7 We tuned all hyper-parameters (as well as those of baseline systems) on the development set. We use the CoNLL 2012 scoring script v8.018 (Pradhan et al., 2014; Luo et al., 2014), which scores based on 3 metrics, including MUC (Vilain et al., 1995), CEAFe (Luo, 2005), and B3 (Bagga and Baldwin, 1998), as well as the CoNLL score, which is the arithmetic mean of the 3 metrics. Code implementing our models is available at https://github.com/swiseman/nn_ coref. The system trains in time comparable to that of linear systems, mainly because we use only raw features and sparse margin-based gradient updates. 5.2 Results Our main results are shown in Table 2. This table compares the performance of our system with the performance reported by several other state-of-the art syste</context>
</contexts>
<marker>Luo, Pradhan, Recasens, Hovy, 2014</marker>
<rawString>Xiaoqiang Luo, Sameer Pradhan, Marta Recasens, and Eduard Hovy. 2014. An Extension of BLANC to System Mentions. Proceedings of ACL, Baltimore, Maryland, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
</authors>
<title>On Coreference Resolution Performance Metrics.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>25--32</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="23410" citStr="Luo, 2005" startWordPosition="3853" endWordPosition="3854">sions used are Wa E R128×da, and for the pairwise representations the matrix dimensions used are Wp E R700×dp. In the g2 model, the outer matrix dimensions are W ER128×(dp+da). With the BASIC+ features, dp and da come out to be slightly less than 106 and 104, respectively, with bilexical head features accounting for the vast majority of dp.7 We tuned all hyper-parameters (as well as those of baseline systems) on the development set. We use the CoNLL 2012 scoring script v8.018 (Pradhan et al., 2014; Luo et al., 2014), which scores based on 3 metrics, including MUC (Vilain et al., 1995), CEAFe (Luo, 2005), and B3 (Bagga and Baldwin, 1998), as well as the CoNLL score, which is the arithmetic mean of the 3 metrics. Code implementing our models is available at https://github.com/swiseman/nn_ coref. The system trains in time comparable to that of linear systems, mainly because we use only raw features and sparse margin-based gradient updates. 5.2 Results Our main results are shown in Table 2. This table compares the performance of our system with the performance reported by several other state-of-the art systems on the CoNLL 2012 English coreference test set. Our full models achieve the best F1 sc</context>
</contexts>
<marker>Luo, 2005</marker>
<rawString>Xiaoqiang Luo. 2005. On Coreference Resolution Performance Metrics. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 25–32. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chao Ma</author>
<author>Janardhan Rao Doppa</author>
<author>J Walker Orr</author>
<author>Prashanth Mannem</author>
<author>Xiaoli Fern</author>
<author>Tom Dietterich</author>
<author>Prasad Tadepalli</author>
</authors>
<title>Prune-and-score: Learning for Greedy Coreference Resolution.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="17393" citStr="Ma et al. (2014)" startWordPosition="2851" endWordPosition="2854">set and model type. “Conj.” indicates whether conjunctions are used. The linear anaphoric system is an SVM (LibLinear implementation (Fan et al., 2008)), and the linear antecedent system is a linear model with the margin-based objective. local context.4 Anaphoricity detection in various forms has been used as an initial step in several coreference systems (Ng and Cardie, 2002; Bengtson and Roth, 2008; Rahman and Ng, 2009; Bj¨orkelund and Farkas, 2012), and the related question of whether a mention can be determined to be a singleton or not has been explored recently by Recasens et al. (2013), Ma et al. (2014), and others.5 Formally, let tn ∈ {−1, 1} indicate whether c ∈ C&apos;(xn) or not (respectively). That is, tn =1 if and only if xn is anaphoric. Define the subtask scoring function sa : X → Ras sa(x) g vaTha(x) + ν0 , where the vector va and the bias ν0 are specific to this subtask and are discarded after pre-training. We train this model to minimize the following slack-rescaled objective N La(0a) = Da(tn)[1 − tn sa(xn)]+ + A||0a||1, n=1 where Da is a class-specific cost used to help encourage anaphoric decisions given the imbalanced data set, and 0a = {va, Wa, ba} are the parameters of the subtask</context>
<context position="25455" citStr="Ma et al. (2014)" startWordPosition="4194" endWordPosition="4197"> CoNLL R R R BCS (2013) 74.89 67.17 70.82 64.26 53.09 58.14 58.12 52.67 55.27 61.41 Prune&amp;Score (2014) 81.03 66.16 72.84 66.9 51.10 57.94 68.75 44.34 53.91 61.56 B&amp;K (2014) 74.3 67.46 70.72 62.71 54.96 58.58 59.4 52.27 55.61 61.63 D&amp;K (2014) 72.73 69.98 71.33 61.18 56.60 58.80 56.20 54.31 55.24 61.79 This work (g2) 76.96 68.10 72.26 66.90 54.12 59.84 59.02 53.34 56.03 62.71 This work (g1) 76.23 69.31 72.60 66.07 55.83 60.52 59.41 54.88 57.05 63.39 Table 2: Results on CoNLL 2012 English test set. We compare against recent state-of-the-art systems, including (in order) Durrett and Klein (2013), Ma et al. (2014), Bj¨orkelund and Kuhn (2014), and Durrett and Klein (2014) (rescored with the v8.01 scorer). F1 gains are significant (p &lt; 0.05 under the bootstrap resample test (Koehn, 2004)) compared with both B&amp;K and D&amp;K for all metrics. MUC B3 CEAF, CoNLL 71.80 60.93 57.51 63.41 71.77 60.84 57.05 63.22 71.92 61.06 57.59 63.52 72.74 61.77 58.63 64.38 72.31 61.79 58.06 64.05 72.68 61.70 58.32 64.23 Model Fully Conn. 1 Layer Fully Conn. 2 Layer g1 + RI g1 + PT g2 + RI g2 + PT Table 4: Comparison of performance (in F1 score) of various models on CoNLL 2012 development set using BASIC+ features. “PT” and “RI”</context>
<context position="35578" citStr="Ma et al., 2014" startWordPosition="5902" endWordPosition="5905">s are required. It is also worth noting that a suitably modified loss function may also be able to prevent excessive pronounpronoun linking, even in a local model. 7 Related Work There is a voluminous literature on machine learning approaches to coreference resolution, effectively beginning with Soon et al. (2001). The recent introduction of the CoNLL datasets (Pradhan et al., 2012) has spurred research that takes advantage of more fine-grained features and richer models (Bj¨orkelund and Farkas, 2012; Chang et al., 2012; Durrett and Klein, 2013; Chang et al., 2013; Bj¨orkelund and Kuhn, 2014; Ma et al., 2014). Of these approaches, our model is related to the mention-ranking approaches (Bengtson and Roth, 2008; Denis and Baldridge, 2008; Rahman and Ng, 2009; Durrett and Klein, 2013; Chang et al., 2013), as opposed to those that focus on non-local, structured prediction (McCallum and Wellner, 2003; Culotta et al., 2006; Haghighi and Klein, 2010; Fernandes et al., 2012; Stoyanov and Eisner, 2012; Bj¨orkelund and Farkas, 2012; Wick et al., 2012; Bj¨orkelund and Kuhn, 2014; Durrett and Klein, 2014). In motivation, our work is most similar to that of Ng (2004), who notes that anaphoricity information is</context>
</contexts>
<marker>Ma, Doppa, Orr, Mannem, Fern, Dietterich, Tadepalli, 2014</marker>
<rawString>Chao Ma, Janardhan Rao Doppa, J Walker Orr, Prashanth Mannem, Xiaoli Fern, Tom Dietterich, and Prasad Tadepalli. 2014. Prune-and-score: Learning for Greedy Coreference Resolution. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Ben Wellner</author>
</authors>
<title>Toward Conditional Models of Identity Uncertainty with Application to Proper Noun Coreference.</title>
<date>2003</date>
<booktitle>Advances in Neural Information Processing Systems 17.</booktitle>
<contexts>
<context position="35870" citStr="McCallum and Wellner, 2003" startWordPosition="5947" endWordPosition="5950">beginning with Soon et al. (2001). The recent introduction of the CoNLL datasets (Pradhan et al., 2012) has spurred research that takes advantage of more fine-grained features and richer models (Bj¨orkelund and Farkas, 2012; Chang et al., 2012; Durrett and Klein, 2013; Chang et al., 2013; Bj¨orkelund and Kuhn, 2014; Ma et al., 2014). Of these approaches, our model is related to the mention-ranking approaches (Bengtson and Roth, 2008; Denis and Baldridge, 2008; Rahman and Ng, 2009; Durrett and Klein, 2013; Chang et al., 2013), as opposed to those that focus on non-local, structured prediction (McCallum and Wellner, 2003; Culotta et al., 2006; Haghighi and Klein, 2010; Fernandes et al., 2012; Stoyanov and Eisner, 2012; Bj¨orkelund and Farkas, 2012; Wick et al., 2012; Bj¨orkelund and Kuhn, 2014; Durrett and Klein, 2014). In motivation, our work is most similar to that of Ng (2004), who notes that anaphoricity information is useful within the broader coreference task, and who accordingly attempts to “globally” optimize performance based on this information, as well as that of Denis et al. (2007), who do joint decoding of anaphoricity and coreference predictions using ILP. Both of these works are taken to contra</context>
</contexts>
<marker>McCallum, Wellner, 2003</marker>
<rawString>Andrew McCallum and Ben Wellner. 2003. Toward Conditional Models of Identity Uncertainty with Application to Proper Noun Coreference. Advances in Neural Information Processing Systems 17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yurii Nesterov</author>
</authors>
<title>A Method of Solving a Convex Programming Problem with Convergence Rate O (1/k2).</title>
<date>1983</date>
<booktitle>In Soviet Mathematics Doklady,</booktitle>
<volume>27</volume>
<pages>372--376</pages>
<contexts>
<context position="22552" citStr="Nesterov, 1983" startWordPosition="3707" endWordPosition="3708">daGrad update (Duchi et al., 2011) with document sized mini-batches.6 We tuned the AdaGrad learning rate and regularization parameters using a grid search over possible learning rates q E {0.001, 0.002, 0.01, 0.02, 0.1, 0.2} and over regularization parameters A E{10−6, ... ,10−1}. For the full coreference task, we use a different learning rate for the pre-trained weights and for the second-layer weights, using q1 = 0.1 and q2 = 0.001, respectively, and A =10−6. When initializing weight-matrices that were not pre-trained 6In preliminary experiments we also used Nesterov’s accelerated gradient (Nesterov, 1983), but found AdaGrad to perform better. we used the sparse initialization technique proposed by Sutskever et al. (2013). For all experiments we use the cost-weights α = (0.5,1.2,1) in defining A. For the anaphoricity representations the matrix dimensions used are Wa E R128×da, and for the pairwise representations the matrix dimensions used are Wp E R700×dp. In the g2 model, the outer matrix dimensions are W ER128×(dp+da). With the BASIC+ features, dp and da come out to be slightly less than 106 and 104, respectively, with bilexical head features accounting for the vast majority of dp.7 We tuned</context>
</contexts>
<marker>Nesterov, 1983</marker>
<rawString>Yurii Nesterov. 1983. A Method of Solving a Convex Programming Problem with Convergence Rate O (1/k2). In Soviet Mathematics Doklady, volume 27, pages 372–376.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
<author>Claire Cardie</author>
</authors>
<title>Identifying Anaphoric and Non-anaphoric Noun Phrases to Improve Coreference Resolution.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th international conference on Computational linguistics-Volume 1,</booktitle>
<pages>1--7</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="17155" citStr="Ng and Cardie, 2002" startWordPosition="2809" endWordPosition="2812">BASIC (N) NN 75.30 75.36 75.33 81.65 BASIC+ (N) Lin. 74.14 74.71 74.43 74.02 BASIC+ (Y) Lin. 74.24 75.39 74.81 80.44 BASIC+ (N) NN 75.84 76.02 75.93 82.86 Table 1: Performance of the two subtasks on the CoNLL 2012 development set by feature set and model type. “Conj.” indicates whether conjunctions are used. The linear anaphoric system is an SVM (LibLinear implementation (Fan et al., 2008)), and the linear antecedent system is a linear model with the margin-based objective. local context.4 Anaphoricity detection in various forms has been used as an initial step in several coreference systems (Ng and Cardie, 2002; Bengtson and Roth, 2008; Rahman and Ng, 2009; Bj¨orkelund and Farkas, 2012), and the related question of whether a mention can be determined to be a singleton or not has been explored recently by Recasens et al. (2013), Ma et al. (2014), and others.5 Formally, let tn ∈ {−1, 1} indicate whether c ∈ C&apos;(xn) or not (respectively). That is, tn =1 if and only if xn is anaphoric. Define the subtask scoring function sa : X → Ras sa(x) g vaTha(x) + ν0 , where the vector va and the bias ν0 are specific to this subtask and are discarded after pre-training. We train this model to minimize the following </context>
<context position="36571" citStr="Ng and Cardie, 2002" startWordPosition="6063" endWordPosition="6066"> and Eisner, 2012; Bj¨orkelund and Farkas, 2012; Wick et al., 2012; Bj¨orkelund and Kuhn, 2014; Durrett and Klein, 2014). In motivation, our work is most similar to that of Ng (2004), who notes that anaphoricity information is useful within the broader coreference task, and who accordingly attempts to “globally” optimize performance based on this information, as well as that of Denis et al. (2007), who do joint decoding of anaphoricity and coreference predictions using ILP. Both of these works are taken to contrast with the more popular approach of doing an initial non-anaphoric pruning step (Ng and Cardie, 2002; Rahman and Ng, 2009; Recasens et al., 2013; Lee et al., 2013). In contrast, we jointly learn non-linear functions of anaphoricity and antecedent features, rather than tune a threshold, or jointly decode based on independently trained classifiers (as in Denis et al. (2007)). In a similar vein, several authors have also proposed using the output of an anaphoricity classifier as a feature in a downstream coreference system (Ng, 2004; Bengtson and Roth, 2008). In our framework we (re)learn features jointly with the full task, after a pre-training scheme that targets anaphoricity as well antecede</context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>Vincent Ng and Claire Cardie. 2002. Identifying Anaphoric and Non-anaphoric Noun Phrases to Improve Coreference Resolution. In Proceedings of the 19th international conference on Computational linguistics-Volume 1, pages 1–7. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
</authors>
<title>Learning Noun Phrase Anaphoricity to Improve Coreference Resolution: Issues in Representation and Optimization.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>151</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2874" citStr="Ng (2004)" startWordPosition="426" endWordPosition="427">presentations for natural language tasks (Collobert et al., 2011), we explore neural network models which take only raw, unconjoined features as input, and attempt to learn intermediate representations automatically. In particular, the model we describe attempts to create independent feature representations useful for both detecting the anaphoricity of a mention (that is, whether or not a mention is anaphoric) and ranking the potential antecedents of an anaphoric mention. Adequately capturing anaphoricity information has long been thought to be an important aspect of the coreference task (see Ng (2004) and Section 7), since a strong non-anaphoric signal might, for instance, discourage the erroneous prediction of an antecedent for a non-anaphoric mention even in the presence of a misleading head match. We furthermore attempt to encourage the learning of the desired feature representations by pretraining the model’s weights on two corresponding subtasks, namely, anaphoricity detection and antecedent ranking of known anaphoric mentions. Overall our best model has an absolute gain of almost 2 points in CoNLL score over a similar but linear mention-ranking model on the CoNLL 2012 English test se</context>
<context position="36134" citStr="Ng (2004)" startWordPosition="5993" endWordPosition="5994">., 2013; Bj¨orkelund and Kuhn, 2014; Ma et al., 2014). Of these approaches, our model is related to the mention-ranking approaches (Bengtson and Roth, 2008; Denis and Baldridge, 2008; Rahman and Ng, 2009; Durrett and Klein, 2013; Chang et al., 2013), as opposed to those that focus on non-local, structured prediction (McCallum and Wellner, 2003; Culotta et al., 2006; Haghighi and Klein, 2010; Fernandes et al., 2012; Stoyanov and Eisner, 2012; Bj¨orkelund and Farkas, 2012; Wick et al., 2012; Bj¨orkelund and Kuhn, 2014; Durrett and Klein, 2014). In motivation, our work is most similar to that of Ng (2004), who notes that anaphoricity information is useful within the broader coreference task, and who accordingly attempts to “globally” optimize performance based on this information, as well as that of Denis et al. (2007), who do joint decoding of anaphoricity and coreference predictions using ILP. Both of these works are taken to contrast with the more popular approach of doing an initial non-anaphoric pruning step (Ng and Cardie, 2002; Rahman and Ng, 2009; Recasens et al., 2013; Lee et al., 2013). In contrast, we jointly learn non-linear functions of anaphoricity and antecedent features, rather</context>
</contexts>
<marker>Ng, 2004</marker>
<rawString>Vincent Ng. 2004. Learning Noun Phrase Anaphoricity to Improve Coreference Resolution: Issues in Representation and Optimization. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 151. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Alessandro Moschitti</author>
<author>Nianwen Xue</author>
<author>Olga Uryupina</author>
<author>Yuchen Zhang</author>
</authors>
<title>Conll2012 Shared Task: Modeling Multilingual Unrestricted Coreference in OntoNotes.</title>
<date>2012</date>
<booktitle>In Joint Conference on EMNLP and CoNLL-Shared Task,</booktitle>
<pages>1--40</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="3498" citStr="Pradhan et al., 2012" startWordPosition="523" endWordPosition="526">d Section 7), since a strong non-anaphoric signal might, for instance, discourage the erroneous prediction of an antecedent for a non-anaphoric mention even in the presence of a misleading head match. We furthermore attempt to encourage the learning of the desired feature representations by pretraining the model’s weights on two corresponding subtasks, namely, anaphoricity detection and antecedent ranking of known anaphoric mentions. Overall our best model has an absolute gain of almost 2 points in CoNLL score over a similar but linear mention-ranking model on the CoNLL 2012 English test set (Pradhan et al., 2012), and of over 1.5 points over the state-of-the-art coreference system. Moreover, unlike current state-ofthe-art systems, our model does only local inference, and is therefore significantly simpler. 1.1 Problem Setting We consider here the mention-ranking (or “mention-synchronous”) approach to coreference 1416 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1416–1426, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics resolution (Denis and Bald</context>
<context position="21335" citStr="Pradhan et al., 2012" startWordPosition="3510" endWordPosition="3513">nking features learned in Figure 3. While the improved subtask performance does not imply better performance on the full coreference task, it shows that model can learn useful feature representations with only raw input features. 5 Coreference Experiments Our experiments examine performance as compared with other coreference systems, as well as the effect of features, pre-training, and model architecture. We also perform a qualitative comparison of our model with the analogous linear model on some challenging non-anaphoric cases. 5.1 Methods All experiments use the CoNLL 2012 English dataset (Pradhan et al., 2012), which is based on the OntoNotes corpus (Hovy et al., 2006). The data set contains 3,493 documents consisting of 1.6 million words. We use the standard experimental split with the training set containing 2,802 documents and 156K annotated mentions, the development set containing 343 documents and 19K annotated mentions, and the test set containing 348 documents and 20K annotated mentions. For all experiments, we use BCS (Durrett and Klein, 2013) to extract system mentions and to compute some of the features. For training, we minimize the loss described above using the composite mirror descent</context>
<context position="35347" citStr="Pradhan et al., 2012" startWordPosition="5864" endWordPosition="5867">in the CoNLL development data participate in pronoun-only clusters (primarily in the context of broadcast or telephone conversations), which suggests that such a “non-local” rule may not be particularly useful, though further experiments are required. It is also worth noting that a suitably modified loss function may also be able to prevent excessive pronounpronoun linking, even in a local model. 7 Related Work There is a voluminous literature on machine learning approaches to coreference resolution, effectively beginning with Soon et al. (2001). The recent introduction of the CoNLL datasets (Pradhan et al., 2012) has spurred research that takes advantage of more fine-grained features and richer models (Bj¨orkelund and Farkas, 2012; Chang et al., 2012; Durrett and Klein, 2013; Chang et al., 2013; Bj¨orkelund and Kuhn, 2014; Ma et al., 2014). Of these approaches, our model is related to the mention-ranking approaches (Bengtson and Roth, 2008; Denis and Baldridge, 2008; Rahman and Ng, 2009; Durrett and Klein, 2013; Chang et al., 2013), as opposed to those that focus on non-local, structured prediction (McCallum and Wellner, 2003; Culotta et al., 2006; Haghighi and Klein, 2010; Fernandes et al., 2012; Sto</context>
</contexts>
<marker>Pradhan, Moschitti, Xue, Uryupina, Zhang, 2012</marker>
<rawString>Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Olga Uryupina, and Yuchen Zhang. 2012. Conll2012 Shared Task: Modeling Multilingual Unrestricted Coreference in OntoNotes. In Joint Conference on EMNLP and CoNLL-Shared Task, pages 1–40. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Xiaoqiang Luo</author>
<author>Marta Recasens</author>
<author>Eduard Hovy</author>
<author>Vincent Ng</author>
<author>Michael Strube</author>
</authors>
<title>Scoring Coreference Partitions of Predicted Mentions: A Reference Implementation.</title>
<date>2014</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="23302" citStr="Pradhan et al., 2014" startWordPosition="3832" endWordPosition="3835">periments we use the cost-weights α = (0.5,1.2,1) in defining A. For the anaphoricity representations the matrix dimensions used are Wa E R128×da, and for the pairwise representations the matrix dimensions used are Wp E R700×dp. In the g2 model, the outer matrix dimensions are W ER128×(dp+da). With the BASIC+ features, dp and da come out to be slightly less than 106 and 104, respectively, with bilexical head features accounting for the vast majority of dp.7 We tuned all hyper-parameters (as well as those of baseline systems) on the development set. We use the CoNLL 2012 scoring script v8.018 (Pradhan et al., 2014; Luo et al., 2014), which scores based on 3 metrics, including MUC (Vilain et al., 1995), CEAFe (Luo, 2005), and B3 (Bagga and Baldwin, 1998), as well as the CoNLL score, which is the arithmetic mean of the 3 metrics. Code implementing our models is available at https://github.com/swiseman/nn_ coref. The system trains in time comparable to that of linear systems, mainly because we use only raw features and sparse margin-based gradient updates. 5.2 Results Our main results are shown in Table 2. This table compares the performance of our system with the performance reported by several other sta</context>
</contexts>
<marker>Pradhan, Luo, Recasens, Hovy, Ng, Strube, 2014</marker>
<rawString>Sameer Pradhan, Xiaoqiang Luo, Marta Recasens, Eduard Hovy, Vincent Ng, and Michael Strube. 2014. Scoring Coreference Partitions of Predicted Mentions: A Reference Implementation. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Altaf Rahman</author>
<author>Vincent Ng</author>
</authors>
<title>Supervised Models for Coreference Resolution.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>2</volume>
<pages>968--977</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="4156" citStr="Rahman and Ng, 2009" startWordPosition="613" endWordPosition="616">e-of-the-art coreference system. Moreover, unlike current state-ofthe-art systems, our model does only local inference, and is therefore significantly simpler. 1.1 Problem Setting We consider here the mention-ranking (or “mention-synchronous”) approach to coreference 1416 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1416–1426, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics resolution (Denis and Baldridge, 2008; Bengtson and Roth, 2008; Rahman and Ng, 2009), which has been adopted by several recent coreference systems (Durrett and Klein, 2013; Chang et al., 2013). Such systems aim to identify whether a mention is coreferent with an antecedent mention, or whether it is instead non-anaphoric (the first mention in the document referring to a particular entity). This is accomplished by assigning a score to the mention’s potential antecedents as well as to the possibility that it is non-anaphoric, and then predicting the greatest scoring option. We furthermore assume the more realistic “system mention” setting, where it is not known a priori which me</context>
<context position="17201" citStr="Rahman and Ng, 2009" startWordPosition="2817" endWordPosition="2820">) Lin. 74.14 74.71 74.43 74.02 BASIC+ (Y) Lin. 74.24 75.39 74.81 80.44 BASIC+ (N) NN 75.84 76.02 75.93 82.86 Table 1: Performance of the two subtasks on the CoNLL 2012 development set by feature set and model type. “Conj.” indicates whether conjunctions are used. The linear anaphoric system is an SVM (LibLinear implementation (Fan et al., 2008)), and the linear antecedent system is a linear model with the margin-based objective. local context.4 Anaphoricity detection in various forms has been used as an initial step in several coreference systems (Ng and Cardie, 2002; Bengtson and Roth, 2008; Rahman and Ng, 2009; Bj¨orkelund and Farkas, 2012), and the related question of whether a mention can be determined to be a singleton or not has been explored recently by Recasens et al. (2013), Ma et al. (2014), and others.5 Formally, let tn ∈ {−1, 1} indicate whether c ∈ C&apos;(xn) or not (respectively). That is, tn =1 if and only if xn is anaphoric. Define the subtask scoring function sa : X → Ras sa(x) g vaTha(x) + ν0 , where the vector va and the bias ν0 are specific to this subtask and are discarded after pre-training. We train this model to minimize the following slack-rescaled objective N La(0a) = Da(tn)[1 −</context>
<context position="35728" citStr="Rahman and Ng, 2009" startWordPosition="5925" endWordPosition="5928">n a local model. 7 Related Work There is a voluminous literature on machine learning approaches to coreference resolution, effectively beginning with Soon et al. (2001). The recent introduction of the CoNLL datasets (Pradhan et al., 2012) has spurred research that takes advantage of more fine-grained features and richer models (Bj¨orkelund and Farkas, 2012; Chang et al., 2012; Durrett and Klein, 2013; Chang et al., 2013; Bj¨orkelund and Kuhn, 2014; Ma et al., 2014). Of these approaches, our model is related to the mention-ranking approaches (Bengtson and Roth, 2008; Denis and Baldridge, 2008; Rahman and Ng, 2009; Durrett and Klein, 2013; Chang et al., 2013), as opposed to those that focus on non-local, structured prediction (McCallum and Wellner, 2003; Culotta et al., 2006; Haghighi and Klein, 2010; Fernandes et al., 2012; Stoyanov and Eisner, 2012; Bj¨orkelund and Farkas, 2012; Wick et al., 2012; Bj¨orkelund and Kuhn, 2014; Durrett and Klein, 2014). In motivation, our work is most similar to that of Ng (2004), who notes that anaphoricity information is useful within the broader coreference task, and who accordingly attempts to “globally” optimize performance based on this information, as well as tha</context>
</contexts>
<marker>Rahman, Ng, 2009</marker>
<rawString>Altaf Rahman and Vincent Ng. 2009. Supervised Models for Coreference Resolution. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2-Volume 2, pages 968–977. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Recasens</author>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher Potts</author>
</authors>
<title>The Life and Death of Discourse Entities: Identifying Singleton Mentions. In</title>
<date>2013</date>
<booktitle>HLT-NAACL,</booktitle>
<pages>627--633</pages>
<marker>Recasens, de Marneffe, Potts, 2013</marker>
<rawString>Marta Recasens, Marie-Catherine de Marneffe, and Christopher Potts. 2013. The Life and Death of Discourse Entities: Identifying Singleton Mentions. In HLT-NAACL, pages 627–633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Jeffrey Pennington</author>
<author>Eric H Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Semi-supervised Recursive Autoencoders for Predicting Sentiment Distributions.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>151--161</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="37715" citStr="Socher et al., 2011" startWordPosition="6246" endWordPosition="6249">ask, after a pre-training scheme that targets anaphoricity as well antecedent representations. There has also been some work on automatically inducing feature conjunctions for use in coreference systems (Fernandes et al., 2012; Lassalle and Denis, 2013), though the approach we present here is somewhat simpler, and unlike that of Lassalle and Denis (2013) is designed for use on system rather than gold mentions. There has been much interest recently in using neural networks for classic natural language tasks such as tagging and semantic role labeling Collobert et al. (2011), sentiment analysis (Socher et al., 2011; Socher et al., 2012), prepositional phrase attachment (Belinkov et al., 2014) among others. These systems often use some form of pretraining for initialization, often word-embeddings learned from external tasks. However, there has been little work of this form for coreference resolution. 8 Conclusion We have presented a simple, local model capable of learning feature representations useful for coreference-related subtasks, and of thereby achieving state-of-the-art performance. Because our approach automatically learns intermediate representations given raw features, directions for further re</context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Jeffrey Pennington, Eric H Huang, Andrew Y Ng, and Christopher D Manning. 2011. Semi-supervised Recursive Autoencoders for Predicting Sentiment Distributions. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 151–161. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic Compositionality through Recursive Matrix-vector Spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1201--1211</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="37737" citStr="Socher et al., 2012" startWordPosition="6250" endWordPosition="6253">ning scheme that targets anaphoricity as well antecedent representations. There has also been some work on automatically inducing feature conjunctions for use in coreference systems (Fernandes et al., 2012; Lassalle and Denis, 2013), though the approach we present here is somewhat simpler, and unlike that of Lassalle and Denis (2013) is designed for use on system rather than gold mentions. There has been much interest recently in using neural networks for classic natural language tasks such as tagging and semantic role labeling Collobert et al. (2011), sentiment analysis (Socher et al., 2011; Socher et al., 2012), prepositional phrase attachment (Belinkov et al., 2014) among others. These systems often use some form of pretraining for initialization, often word-embeddings learned from external tasks. However, there has been little work of this form for coreference resolution. 8 Conclusion We have presented a simple, local model capable of learning feature representations useful for coreference-related subtasks, and of thereby achieving state-of-the-art performance. Because our approach automatically learns intermediate representations given raw features, directions for further research might alternate</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D Manning, and Andrew Y Ng. 2012. Semantic Compositionality through Recursive Matrix-vector Spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1201–1211. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wee Meng Soon</author>
<author>Hwee Tou Ng</author>
<author>Daniel Chung Yong Lim</author>
</authors>
<title>A Machine Learning Approach to Coreference Resolution of Noun Phrases.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>4</issue>
<contexts>
<context position="35277" citStr="Soon et al. (2001)" startWordPosition="5852" endWordPosition="5855">urns out, however, almost 30% of the anaphoric pronominal mentions in the CoNLL development data participate in pronoun-only clusters (primarily in the context of broadcast or telephone conversations), which suggests that such a “non-local” rule may not be particularly useful, though further experiments are required. It is also worth noting that a suitably modified loss function may also be able to prevent excessive pronounpronoun linking, even in a local model. 7 Related Work There is a voluminous literature on machine learning approaches to coreference resolution, effectively beginning with Soon et al. (2001). The recent introduction of the CoNLL datasets (Pradhan et al., 2012) has spurred research that takes advantage of more fine-grained features and richer models (Bj¨orkelund and Farkas, 2012; Chang et al., 2012; Durrett and Klein, 2013; Chang et al., 2013; Bj¨orkelund and Kuhn, 2014; Ma et al., 2014). Of these approaches, our model is related to the mention-ranking approaches (Bengtson and Roth, 2008; Denis and Baldridge, 2008; Rahman and Ng, 2009; Durrett and Klein, 2013; Chang et al., 2013), as opposed to those that focus on non-local, structured prediction (McCallum and Wellner, 2003; Culot</context>
</contexts>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong Lim. 2001. A Machine Learning Approach to Coreference Resolution of Noun Phrases. Computational Linguistics, 27(4):521–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veselin Stoyanov</author>
<author>Jason Eisner</author>
</authors>
<title>Easy-first Coreference Resolution. In</title>
<date>2012</date>
<booktitle>COLING,</booktitle>
<pages>2519--2534</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="35969" citStr="Stoyanov and Eisner, 2012" startWordPosition="5963" endWordPosition="5966">12) has spurred research that takes advantage of more fine-grained features and richer models (Bj¨orkelund and Farkas, 2012; Chang et al., 2012; Durrett and Klein, 2013; Chang et al., 2013; Bj¨orkelund and Kuhn, 2014; Ma et al., 2014). Of these approaches, our model is related to the mention-ranking approaches (Bengtson and Roth, 2008; Denis and Baldridge, 2008; Rahman and Ng, 2009; Durrett and Klein, 2013; Chang et al., 2013), as opposed to those that focus on non-local, structured prediction (McCallum and Wellner, 2003; Culotta et al., 2006; Haghighi and Klein, 2010; Fernandes et al., 2012; Stoyanov and Eisner, 2012; Bj¨orkelund and Farkas, 2012; Wick et al., 2012; Bj¨orkelund and Kuhn, 2014; Durrett and Klein, 2014). In motivation, our work is most similar to that of Ng (2004), who notes that anaphoricity information is useful within the broader coreference task, and who accordingly attempts to “globally” optimize performance based on this information, as well as that of Denis et al. (2007), who do joint decoding of anaphoricity and coreference predictions using ILP. Both of these works are taken to contrast with the more popular approach of doing an initial non-anaphoric pruning step (Ng and Cardie, 20</context>
</contexts>
<marker>Stoyanov, Eisner, 2012</marker>
<rawString>Veselin Stoyanov and Jason Eisner. 2012. Easy-first Coreference Resolution. In COLING, pages 2519– 2534. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya Sutskever</author>
<author>James Martens</author>
<author>George Dahl</author>
<author>Geoffrey Hinton</author>
</authors>
<title>On the Importance of Initialization and Momentum in Deep Learning.</title>
<date>2013</date>
<booktitle>In Proceedings of the 30th International Conference on Machine Learning,</booktitle>
<pages>1139--1147</pages>
<contexts>
<context position="22670" citStr="Sutskever et al. (2013)" startWordPosition="3724" endWordPosition="3727">egularization parameters using a grid search over possible learning rates q E {0.001, 0.002, 0.01, 0.02, 0.1, 0.2} and over regularization parameters A E{10−6, ... ,10−1}. For the full coreference task, we use a different learning rate for the pre-trained weights and for the second-layer weights, using q1 = 0.1 and q2 = 0.001, respectively, and A =10−6. When initializing weight-matrices that were not pre-trained 6In preliminary experiments we also used Nesterov’s accelerated gradient (Nesterov, 1983), but found AdaGrad to perform better. we used the sparse initialization technique proposed by Sutskever et al. (2013). For all experiments we use the cost-weights α = (0.5,1.2,1) in defining A. For the anaphoricity representations the matrix dimensions used are Wa E R128×da, and for the pairwise representations the matrix dimensions used are Wp E R700×dp. In the g2 model, the outer matrix dimensions are W ER128×(dp+da). With the BASIC+ features, dp and da come out to be slightly less than 106 and 104, respectively, with bilexical head features accounting for the vast majority of dp.7 We tuned all hyper-parameters (as well as those of baseline systems) on the development set. We use the CoNLL 2012 scoring scr</context>
</contexts>
<marker>Sutskever, Martens, Dahl, Hinton, 2013</marker>
<rawString>Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. 2013. On the Importance of Initialization and Momentum in Deep Learning. In Proceedings of the 30th International Conference on Machine Learning, pages 1139–1147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Vilain</author>
<author>John Burger</author>
<author>John Aberdeen</author>
<author>Dennis Connolly</author>
<author>Lynette Hirschman</author>
</authors>
<title>A Modeltheoretic Coreference Scoring Scheme.</title>
<date>1995</date>
<booktitle>In Proceedings of the 6th conference on Message Understanding,</booktitle>
<pages>45--52</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="23391" citStr="Vilain et al., 1995" startWordPosition="3848" endWordPosition="3851">resentations the matrix dimensions used are Wa E R128×da, and for the pairwise representations the matrix dimensions used are Wp E R700×dp. In the g2 model, the outer matrix dimensions are W ER128×(dp+da). With the BASIC+ features, dp and da come out to be slightly less than 106 and 104, respectively, with bilexical head features accounting for the vast majority of dp.7 We tuned all hyper-parameters (as well as those of baseline systems) on the development set. We use the CoNLL 2012 scoring script v8.018 (Pradhan et al., 2014; Luo et al., 2014), which scores based on 3 metrics, including MUC (Vilain et al., 1995), CEAFe (Luo, 2005), and B3 (Bagga and Baldwin, 1998), as well as the CoNLL score, which is the arithmetic mean of the 3 metrics. Code implementing our models is available at https://github.com/swiseman/nn_ coref. The system trains in time comparable to that of linear systems, mainly because we use only raw features and sparse margin-based gradient updates. 5.2 Results Our main results are shown in Table 2. This table compares the performance of our system with the performance reported by several other state-of-the art systems on the CoNLL 2012 English coreference test set. Our full models ach</context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995</marker>
<rawString>Marc Vilain, John Burger, John Aberdeen, Dennis Connolly, and Lynette Hirschman. 1995. A Modeltheoretic Coreference Scoring Scheme. In Proceedings of the 6th conference on Message Understanding, pages 45–52. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Wick</author>
<author>Sameer Singh</author>
<author>Andrew McCallum</author>
</authors>
<title>A Discriminative Hierarchical Model for Fast Coreference at Large Scale.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1,</booktitle>
<pages>379--388</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="36018" citStr="Wick et al., 2012" startWordPosition="5971" endWordPosition="5974">e-grained features and richer models (Bj¨orkelund and Farkas, 2012; Chang et al., 2012; Durrett and Klein, 2013; Chang et al., 2013; Bj¨orkelund and Kuhn, 2014; Ma et al., 2014). Of these approaches, our model is related to the mention-ranking approaches (Bengtson and Roth, 2008; Denis and Baldridge, 2008; Rahman and Ng, 2009; Durrett and Klein, 2013; Chang et al., 2013), as opposed to those that focus on non-local, structured prediction (McCallum and Wellner, 2003; Culotta et al., 2006; Haghighi and Klein, 2010; Fernandes et al., 2012; Stoyanov and Eisner, 2012; Bj¨orkelund and Farkas, 2012; Wick et al., 2012; Bj¨orkelund and Kuhn, 2014; Durrett and Klein, 2014). In motivation, our work is most similar to that of Ng (2004), who notes that anaphoricity information is useful within the broader coreference task, and who accordingly attempts to “globally” optimize performance based on this information, as well as that of Denis et al. (2007), who do joint decoding of anaphoricity and coreference predictions using ILP. Both of these works are taken to contrast with the more popular approach of doing an initial non-anaphoric pruning step (Ng and Cardie, 2002; Rahman and Ng, 2009; Recasens et al., 2013; L</context>
</contexts>
<marker>Wick, Singh, McCallum, 2012</marker>
<rawString>Michael Wick, Sameer Singh, and Andrew McCallum. 2012. A Discriminative Hierarchical Model for Fast Coreference at Large Scale. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 379–388. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chun-Nam John Yu</author>
<author>Thorsten Joachims</author>
</authors>
<title>Learning Structural SVMs with Latent Variables.</title>
<date>2009</date>
<booktitle>In Proceedings of the 26th Annual International Conference on Machine Learning,</booktitle>
<pages>1169--1176</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="13991" citStr="Yu and Joachims, 2009" startWordPosition="2273" endWordPosition="2276">dingly, for this model we use only φa and φp consisting of the raw features in Figure 2 without conjunctions. Any interaction between these features must be learned by the feature representations hp and ha. 3.2 Training We can directly train our model using backpropagation. To specify the training problem, we first define notation for the training objective. Define the set C(x) to contain just the mentions in A(x) that are coreferent with x. We then define Finally, let y`n = arg maxyEC&apos;(xn) s(xn, y) be the highest scoring correct antecedent of xn, which may be E. (Thus, following recent work (Yu and Joachims, 2009; Fernandes et al., 2012; Chang et al., 2013; Durrett and Klein, 2013), we view each mention as having a “latent antecedent”.2) We train to minimize the regularized, slack-rescaled, 2Note that this renders the objectives of even models with a linear scoring function non-convex. latent-variable loss3 given by: A(xn, ˆy)(1 + 8(xn, ˆy)−s(xn, y`n)) + λ||θ||1, where Δ is a mistake-specific cost function, which is 0 when yˆ E C&apos;(xn). Above, we use θ to refer to the full set of parameters {W, u, v, Wa, Wp, ba, bp}. For experiments, we define Δ to take on different costs for the three kinds of mistake</context>
</contexts>
<marker>Yu, Joachims, 2009</marker>
<rawString>Chun-Nam John Yu and Thorsten Joachims. 2009. Learning Structural SVMs with Latent Variables. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 1169–1176. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>