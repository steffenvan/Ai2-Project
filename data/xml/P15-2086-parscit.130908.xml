<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.060046">
<title confidence="0.958656">
Learning Hidden Markov Models with Distributed
State Representations for Domain Adaptation
</title>
<author confidence="0.985592">
Min Xiao and Yuhong Guo
</author>
<affiliation confidence="0.9481895">
Department of Computer and Information Sciences
Temple University, Philadelphia, PA 19122, USA
</affiliation>
<email confidence="0.999371">
{minxiao,yuhong}@temple.edu
</email>
<sectionHeader confidence="0.994812" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999900736842105">
Recently, a variety of representation learn-
ing approaches have been developed in
the literature to induce latent generalizable
features across two domains. In this paper,
we extend the standard hidden Markov
models (HMMs) to learn distributed state
representations to improve cross-domain
prediction performance. We reformu-
late the HMMs by mapping each discrete
hidden state to a distributed representa-
tion vector and employ an expectation-
maximization algorithm to jointly learn
distributed state representations and model
parameters. We empirically investigate the
proposed model on cross-domain part-of-
speech tagging and noun-phrase chunking
tasks. The experimental results demon-
strate the effectiveness of the distributed
HMMs on facilitating domain adaptation.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999898084745763">
Domain adaptation aims to obtain an effective pre-
diction model for a particular target domain where
labeled training data is scarce by exploiting la-
beled data from a related source domain. Domain
adaptation is very important in the field of natu-
ral language processing (NLP) as it can reduce the
expensive manual annotation effort in the target
domain. Various NLP tasks have benefited from
domain adaptation techniques, including part-of-
speech tagging (Blitzer et al., 2006; Huang and
Yates, 2010a), chunking (Daum´e III, 2007; Huang
and Yates, 2009), named entity recognition (Guo
et al., 2009; Turian et al., 2010), dependency pars-
ing (Dredze et al., 2007; Sagae and Tsujii, 2007)
and semantic role labeling (Dahlmeier and Ng,
2010; Huang and Yates, 2010b).
In a typical domain adaptation scenario of NLP,
the source and target domains contain text data
of different genres (e.g., newswire vs biomedi-
cal (Blitzer et al., 2006)). Under such circum-
stances, the original lexical features may not per-
form well in cross-domain learning since differ-
ent genres of text may use very different vocab-
ularies and produce cross-domain feature distri-
bution divergence and feature sparsity issue. A
number of techniques have been developed in the
literature to tackle the problem of cross-domain
feature divergence and feature sparsity, includ-
ing clustering based word representation learn-
ing methods (Huang and Yates, 2009; Candito et
al., 2011), word embedding based representation
learning methods (Turian et al., 2010; Hovy et
al., 2015) and some other representation learning
methods (Blitzer et al., 2006).
In this paper, we extend the standard hidden
Markov models (HMMs) to perform distributed
state representation learning and induce context-
aware distributed word representations for domain
adaptation. Instead of learning a single discrete
latent state for each observation in a given sen-
tence, we learn a distributed representation vec-
tor. We define a state embedding matrix to map
each latent state value to a low-dimensional dis-
tributed vector and reformulate the three local dis-
tributions of HMMs based on the distributed state
representations. We then simultaneously learn the
state embedding matrix and the model parame-
ters using an expectation-maximization (EM) al-
gorithm. The hidden states of each word in a sen-
tence can be decoded using the standard Viterbi
decoding procedure of HMMs, and its distributed
representation can be obtained by a simple map-
ping with the state embedding matrix. We then
use the context-aware distributed representations
of the words as their augmenting features to per-
form cross-domain part-of-speech (POS) tagging
and noun-phrase (NP) chunking.
The proposed approach is closely related to
the clustering based method (Huang and Yates,
</bodyText>
<page confidence="0.90728">
524
</page>
<bodyText confidence="0.92971052631579">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 524–529,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
2009) as we both use latent state representations
as generalizable features. However, they use stan-
dard HMMs to produce discrete hidden state fea-
tures for each observation word, while we induce
distributed state representation vectors. Our dis-
tributed HMMs share similarities with the word
embedding based method (Hovy et al., 2015),
and can be more space-efficient than the stan-
dard HMMs. Moreover, our model can incor-
porate context information into observation fea-
ture vectors to perform representation learning in
a context-aware manner. The distributed state
representations induced by our model hence have
larger representing capacities and generalizing ca-
pabilities for cross-domain learning than standard
HMMs.
</bodyText>
<sectionHeader confidence="0.999785" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999484424242424">
A variety of representation learning approaches
have been developed in the literature to address
NLP domain adaptation problems. The cluster-
ing based word representation learning methods
perform word clustering within the sentence struc-
ture and use word cluster indicators as generaliz-
able features to address domain adaptation prob-
lems. For example, Huang and Yates (2009) used
the discrete hidden state of a word under HMMs
as augmenting features for cross-domain POS tag-
ging and NP chunking. Brown clusters (Brown
et al., 1992), which was used as latent features
for simple in-domain dependency parsing (Koo et
al., 2008), has recently been exploited for out-of-
domain statistical parsing (Candito et al., 2011).
The word embedding based representation
learning methods learn a dense real-valued repre-
sentation vector for each word as latent features
for domain adaptation. Turian et al. (2010) em-
pirically studied using word embeddings learned
from hierarchical log-bilinear models (Mnih and
Geoffrey, 2008) and neural language models (Col-
lobert and Weston, 2008) for cross-domain NER
tasks. Hovy et al. (2015) used the word embed-
dings learned from the Skip-gram Model (SGM)
(Mikolov et al., 2013) to develop a POS tagger for
Twitter data with labeled newswire training data.
Some other representation learning methods
have been developed to tackle NLP cross-domain
problems as well. For example, Blitzer et
al. (2006) proposed a structural correspondence
learning method for POS tagging, which first se-
lects a set of pivot features (occurring frequently in
</bodyText>
<figureCaption confidence="0.8355825">
Figure 1: Hidden Markov models with distributed
state representations (dHMM).
</figureCaption>
<bodyText confidence="0.979700125">
the two domains) and then models the correlations
between pivot features and non-pivot features to
induce generalizable features.
In terms of performing distributed representa-
tion learning for output variables, our proposed
model shares similarity with the structured out-
put representation learning approach developed
by Srikumar and Manning (2014), which extends
the structured support vector machines to simul-
taneously learn the prediction model and the dis-
tributed representations of the output labels. How-
ever, the approach in (Srikumar and Manning,
2014) assumes the training labels (i.e., output val-
ues) are given and performs learning in the stan-
dard supervised in-domain setting, while our pro-
posed distributed HMMs address cross-domain
learning problems by performing unsupervised
representation learning. There are also a few
works that extended standard HMMs in the liter-
ature, including the observable operator models
(Jaeger, 1999), and the spectral learning method
(Stratos et al., 2013). But none of them performs
representation learning to address cross-domain
adaptation problems.
</bodyText>
<sectionHeader confidence="0.997837" genericHeader="method">
3 Proposed Model
</sectionHeader>
<bodyText confidence="0.999857888888889">
In this paper, we propose a novel distributed hid-
den Markov model (dHMM) for representation
learning over sequence data. This model ex-
tends the hidden Markov models (Rabiner and
Juang, 1986) to learn distributed state representa-
tions. Similar as HMMs, a dHMM (shown in Fig-
ure 1) is a two-layer generative graphical model,
which generates a sequence of observations from
a sequence of latent state variables using Markov
</bodyText>
<page confidence="0.990248">
525
</page>
<bodyText confidence="0.999755838709677">
properties. Let O = {o1, o2, ... , oT} be the se-
quence of observations with length T, where each
observation ot E Rd is a d-dimensional feature
vector. Let S = {s1, s2, ... , sT} be the sequence
of T hidden states, where each hidden state st has
a discrete state value from a total H hidden states
H = {1, 2, ... , H}. Besides, we assume that
there is a low-dimensional distributed representa-
tion vector associated with each hidden state. Let
M E RH×m be the state embedding matrix where
the i-th row MZ: denotes the m-dimensional repre-
sentation vector for the i-th state. Previous works
have demonstrated the usefulness of discrete hid-
den states induced from a HMM on addressing
feature sparsity in domain adaptation (Huang and
Yates, 2009). However, expressing a semantic
word by a single discrete state value is too re-
strictive, as it has been shown in the literature
that words have many different features in a multi-
dimensional space where they could be separately
characterized as number, POS tag, gender, tense,
voice and other aspects (Sag and Wasow, 1999;
Huang et al., 2011). Our proposed model aims
to overcome this inherent drawback of standard
HMMs on learning word representations. Given
a set of observation sequences in two domains, the
dHMM induces a distributed representation vector
with continuous real values for each observation
word as generalizable features, which has the ca-
pacity of capturing multi-aspect latent characteris-
tics of the word clusters.
</bodyText>
<subsectionHeader confidence="0.999485">
3.1 Model Formulation
</subsectionHeader>
<bodyText confidence="0.9999963">
To build the dHMMs, we reformulate the standard
HMMs by defining three main local distributions
based on the distributed state representations, i.e.,
the initial state distribution, the state transition dis-
tribution, and the observation emission distribu-
tion. Below we introduce them by using O to de-
note the set of parameters involved and using 1 to
denote a column vector with all 1s.
First we use the following multinomial distribu-
tion as the initial state distribution,
</bodyText>
<equation confidence="0.987939">
P(s1; O) = 0(s1)&gt;A,
</equation>
<bodyText confidence="0.9996296">
where 0(st) E {0,1}H is a one-hot vector with a
single 1 value at its st-th entry, and A E RH is the
parameter vector such that A &gt; 0 and A&gt;1 = 1.
We then define a multinomial logistic regression
model for the state transition distribution,
</bodyText>
<equation confidence="0.988911">
P(st+1|st; O) = Z(st; O)
</equation>
<bodyText confidence="0.999846333333333">
where W E RH×m is the regression parameter
matrix and Z(st; O) is the normalization term.
Finally, we assume the observation vector is
generated from a multivariate Gaussian distribu-
tion, i.e., ot — N (0(st)&gt;MQ, σId), and use the
following model for the emission distribution,
</bodyText>
<equation confidence="0.9864315">
exp l−1
2σ n(st, ot)n(st, ot
,
(2π)d/2σd/2
</equation>
<bodyText confidence="0.992605969696969">
with n(st, ot) = 0(st)&gt;MQ — o&gt;t , where Q E
Rm×d and σ E R are the model parameters. Dif-
ferent from the standard HMMs which have dis-
crete hidden states and discrete observations, the
multivariate Gaussian model here generates each
observation ot as a d-dimensional continuous fea-
ture vector. This type of emission distribution pro-
vides us the flexibility to incorporate local context
information or statistical global information for in-
ducing distributed state representations. For ex-
ample, we can use the concatenation of the one-hot
word vectors within a sliding window around the
target word as the observation vector. Moreover,
we can also use the globally preprocessed continu-
ous word vectors as the observation vectors, which
we will describe later in our experiments.
The standard HMMs (Rabiner and Juang, 1986)
use conditional probability tables for the state tran-
sition distribution, which grows quadratically with
respect to the number of hidden states, and the
emission distribution, which grows linearly with
respect to the observed vocabulary size that is
usually very large in NLP tasks. Instead, the
dHMMs can significantly reduce the sizes of these
conditional probability tables by introducing the
low-dimensional state embedding vectors, and the
dHMM is much more efficient in terms of mem-
ory storage. In fact, the complexity of dHMMs
can be independent of the vocabulary size by us-
ing flexible observation features. We represent the
dHMM parameter set as O = {M E RH×m, W E
RH×m, Q E Rm×d, σ E R, A E [0,1]H}, where
m is a small constant.
</bodyText>
<subsectionHeader confidence="0.999283">
3.2 Model Training
</subsectionHeader>
<bodyText confidence="0.9825145">
Given a data set of N observed sequences
{O1, ... , On, ... , ON}, its regularized log-
</bodyText>
<equation confidence="0.919849333333333">
exp l0(st+1)&gt;WM&gt;0(st)}
)&gt;}
P (ot|st; O) =
</equation>
<page confidence="0.996502">
526
</page>
<tableCaption confidence="0.999827">
Table 1: Test performance for cross-domain POS tagging and NP chunking.
</tableCaption>
<table confidence="0.998034333333333">
Systems POS Tagging (Accuracy (%)) NP Chunking (F1-score)
All Words OOV Words All NPs OOV NPs
Baseline 88.3 67.3 0.86 0.74
SGM (Hovy et al., 2015) 89.0 71.4 0.88 0.78
HMM (Huang and Yates, 2009) 90.5 75.2 0.91 0.85
dHMM 91.1 76.0 0.93 0.88
</table>
<equation confidence="0.960416">
likelihood can be written as follows
J: log P(On;O)− η
L(O)= 2R(W,Q,M) (1)
n
</equation>
<bodyText confidence="0.9984535">
where the regularization function is defined
with Frobenius norms such as R(W, Q, M) =
</bodyText>
<equation confidence="0.917634">
kWk2F + kQk 2 F + kMk2F . Moreover, each log-
likelihood term has the following lower bound
logP(On; O) = log J: P(On, Sn; O)
Sn
≥ logP(On; O)−KL(Q(Sn)||P(Sn|On; O)) (2)
</equation>
<bodyText confidence="0.999960714285714">
where Q(Sn) is any valid distribution over the hid-
den state variables Sn and KL(.||.) denotes the
Kullback-Leibler divergence. Let F(Q, O) denote
the regularized lower bound function obtained by
plugging the lower bound (2) back into the ob-
jective function (1). We then perform training
by using an expectation-maximization (EM) algo-
rithm (Dempster et al., 1977) that iteratively max-
imizes F(Q, O) to reach a local optimal solution.
We first randomly initialize the model parame-
ters while enforcing λ to be in the feasible region
(λ ≥ 0, λT1 = 1). In the (k+1)-th iteration, given
{Q(k), O(k)}, we then sequentially update Q with
an E-step (3) and update O with a M-step (4).
</bodyText>
<equation confidence="0.9903585">
Q(k+1) = arg max F(Q, O(k)) (3)
Q
O(k+1) = arg max F(Q(k+1), O) (4)
O
</equation>
<subsectionHeader confidence="0.995958">
3.3 Domain Adaptation with Distributed
State Representations
</subsectionHeader>
<bodyText confidence="0.9996444">
We use all training data from the two domains
to train dHMMs for local optimal model pa-
rameters O* = {M*, W*, Q*, σ*, λ*}. We
then infer the latent state sequence S* =
{s*1, s*2, ... , s*T} using the standard Viterbi algo-
rithm (Rabiner and Juang, 1986) for each la-
beled source training sentence and each target
test sentence. The corresponding distributed
state representation vectors can be obtained as
{M*Tφ(s*1), M*Tφ(s*2), ... , M*Tφ(s*T )}. We
then train a supervised NLP system (e.g., POS tag-
ging or NP chunking) on the labeled source train-
ing sentences using the distributed state represen-
tations as augmenting input features and perform
prediction on the augmented test sentences.
</bodyText>
<sectionHeader confidence="0.999676" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9999031">
We conducted experiments on cross-domain part-
of-speech (POS) tagging and noun-phrase (NP)
chunking tasks. We used the same experimen-
tal datasets as in (Huang and Yates, 2009) for
cross-domain POS tagging from Wall Street Jour-
nal (WSJ) domain (Marcus et al., 1993) to MED-
LINE domain (PennBioIE, 2005) and for cross-
domain NP chunking from CoNLL shared task
dataset (Tjong et al., 2000) to Open American Na-
tional Corpus (OANC) (Reppen et al., 2005).
</bodyText>
<subsectionHeader confidence="0.997599">
4.1 Representation Learning
</subsectionHeader>
<bodyText confidence="0.999873363636363">
We first built a unified vocabulary with all the data
in the two domains. We then conducted latent
semantic analysis (LSA) over the sentence-word
frequency matrix to get a low-dimensional repre-
sentation vector for each word. We used a sliding
window with size 3 to construct the d-dimensional
feature vector (d = 1500) for each observation in
a given sentence. We used η = 0.5, set the number
of hidden states H to be 80 and the dimensionality
m = 20. We used all the labeled and unlabeled
training data in the two domains to train dHMMs.
</bodyText>
<subsectionHeader confidence="0.999701">
4.2 Test Results
</subsectionHeader>
<bodyText confidence="0.999509875">
We used the induced distributed state representa-
tions of each observation as augmenting features
to train conditional random fields (CRF) with the
CRFSuite package (Okazaki, 2007) on the labeled
source sentences and perform prediction on the
target test sentences. We compared with the fol-
lowing systems: a Baseline system without repre-
sentation learning, a SGM based word embedding
</bodyText>
<page confidence="0.992289">
527
</page>
<bodyText confidence="0.999945242424242">
system (Hovy et al., 2015), and a discrete hidden
state based clustering system (Huang and Yates,
2009). We used the word id and orthographic fea-
tures as the baseline features for POS tagging and
added POS tags for NP chunking. We reported
the POS tagging accuracy for all words and out-
of-vocabulary (OOV) words (which appear less
than three times in the labeled source training sen-
tences), and NP chunking F1 scores for all NPs
and only OOV NPs (whose beginning word is an
OOV word) in Table 1.
We can see that the Baseline method per-
forms poorly on both tasks especially on the OOV
words/NPs, which shows that the original lexical
based features are not sufficient to develop a ro-
bust POS tagger/NP chunker for the target domain
with labeled source training sentences. By us-
ing unlabeled training sentences from the two do-
mains, all representation learning approaches in-
crease the cross-domain test performance, espe-
cially on the OOV words/NPs. These improve-
ments over the Baseline method demonstrate that
the induced latent features do alleviate feature
sparsity issue across the two domains and help the
trained NLP system generalize well in the target
domain. Between these representation learning
approaches, the proposed distributed state repre-
sentation learning method outperforms both of the
word embedding based and discrete HMM hidden
state based systems. This suggests that by learn-
ing distributed representations in a context-aware
manner, dHMMs can effectively bridge domain
divergence.
</bodyText>
<subsectionHeader confidence="0.973025">
4.3 Sensitivity Analysis over the
Dimensionality of State Embeddings
</subsectionHeader>
<bodyText confidence="0.9998916">
We also conducted experiments to investigate how
does the dimensionality of the distributed state
representations, m, in our proposed approach af-
fect cross-domain test performance given a fixed
state number H = 80. We tested a number of
different m values from {10, 20, 30, 40} and used
the same experimental setting as before for each m
value. The POS tagging accuracy on all words of
the test sentences and the chunking F1 score on all
NPs with different m values are reported in Fig-
ure 2. We can see that the performance of both
POS tagging and NP chunking has notable im-
provements with m increasing from 10 to 20. The
POS tagging performance improves very slightly
from m = 20 to m = 30 and is very stable from
</bodyText>
<figureCaption confidence="0.924786">
Figure 2: Cross-domain test performance with re-
spect to different dimensionality values (m) of the
hidden state representation vectors.
</figureCaption>
<bodyText confidence="0.9998515">
m = 30 to m = 40. The NP chunking perfor-
mance is very stable from m = 20 to m = 40.
These results suggest that the distributed state rep-
resentation vectors only need to have a succinct
length to capture useful information. The pro-
posed distributed HMMs are not sensitive to the
dimensionality of the state embeddings as long as
m reaches a reasonable small value.
</bodyText>
<sectionHeader confidence="0.99383" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999806875">
In this paper, we extended the standard HMMs
to learn distributed state representations and fa-
cilitate cross-domain sequence predictions. We
mapped each state variable to a distributed rep-
resentation vector and simultaneously learned the
state embedding matrix and the model parameters
with an EM algorithm. The experimental results
on cross-domain POS tagging and NP chunking
tasks demonstrated the effectiveness of the pro-
posed approach for domain adaptation. In the
future, we plan to apply this approach to other
cross-domain prediction tasks such as named en-
tity recognition or semantic role labeling. We also
plan to extend our method to learn cross-lingual
representations with auxiliary resources such as
bilingual dictionaries or parallel sentences.
</bodyText>
<sectionHeader confidence="0.997892" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.8253255">
This research was supported in part by NSF grant
IIS-1065397.
</bodyText>
<figure confidence="0.997759066666667">
92
POS Tagging
91
90
89
10 20 30 40
Accuracy (%)
0.94
NP Chunking
0.93
0.92
0.9110 20 30 40
M
F1
M
</figure>
<page confidence="0.985508">
528
</page>
<sectionHeader confidence="0.98187" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999445875">
J. Blitzer, R. McDonald, and F. Pereira. 2006. Domain
adaptation with structural correspondence learning.
In Proc. of the Conference on Empirical Methods in
Natural Language Processing (EMNLP).
P. Brown, P. deSouza, R. Mercer, V. Pietra, and J. Lai.
1992. Class-based n-gram models of natural lan-
guage. Compututal Linguistics, 18(4):467–479.
M. Candito, E. Anguiano, and D. Seddah. 2011. A
word clustering approach to domain adaptation: Ef-
fective parsing of biomedical texts. In Proc. of the
Inter. Conference on Parsing Technologies (IWPT).
R. Collobert and J. Weston. 2008. A unified archi-
tecture for natural language processing: deep neural
networks with multitask learning. In Proc. of the In-
ter. Conference on Machine Learning (ICML).
D. Dahlmeier and H. Ng. 2010. Domain adaptation
for semantic role labeling in the biomedical domain.
Bioinformatics, 26(8):1098–1104.
H. Daum´e III. 2007. Frustratingly easy domain adap-
tation. In Proc. of the Annual Meeting of the Associ-
ation of Computational Linguistics (ACL).
A. Dempster, N. Laird, and D. Rubin. 1977. Max-
imum likelihood from incomplete data via the em
algorithm. Journal of the Royal Statistical Society,
Series B, 39(1):1–38.
M. Dredze, J. Blitzer, P. Talukdar, K. Ganchev,
J. Grac¸a, and O. Pereira. 2007. Frustratingly hard
domain adaptation for dependency parsing. In Proc.
of CoNLL Shared Task Session of EMNLP-CoNLL.
H. Guo, H. Zhu, Z. Guo, X. Zhang, X. Wu, and Z. Su.
2009. Domain adaptation with latent semantic as-
sociation for named entity recognition. In Proc. of
Human Language Technologies: The Annual Conf.
of North American Chapter of ACL (HLT-NAACL).
D. Hovy, B. Plank, H. Alonso, and A. Søgaard. 2015.
Mining for unambiguous instances to adapt pos tag-
gers to new domains. In Proc. of the Conference of
the North American Chapter of ACL (NAACL).
F. Huang and A. Yates. 2009. Distributional represen-
tations for handling sparsity in supervised sequence-
labeling. In Proc. of the Annual Meeting of the ACL
and the IJCNLP of the AFNLP (ACL-AFNLP).
F. Huang and A. Yates. 2010a. Exploring
representation-learning approaches to domain adap-
tation. In Proc. of the Workshop on Domain Adap-
tation for Natural Language Processing (DANLP).
F. Huang and A. Yates. 2010b. Open-domain semantic
role labeling by modeling word spans. In Proc. of
the Annual Meeting of ACL (ACL).
F. Huang, A. Yates, A. Ahuja, and D. Downey. 2011.
Language models as representations for weakly-
supervised nlp tasks. In Proc. of the Conference on
Comput. Natural Language Learning (CoNLL).
H. Jaeger. 1999. Observable operator models for dis-
crete stochastic time series. Neural Computation,
12:1371–1398.
T. Koo, X. Carreras, and M. Collins. 2008. Simple
semi-supervised dependency parsing. In Proc. of
the Annual Meeting of the Association for Compu-
tational Linguistics (ACL).
M. Marcus, M. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of en-
glish: The Penn treebank. Computational Linguis-
tics, 19(2):313–330.
T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and
J. Dean. 2013. Distributed representations of words
and phrases and their compositionality. In Advances
in Neural Information Processing Systems (NIPS).
A. Mnih and E. Geoffrey. 2008. A scalable hierar-
chical distributed language model. In Advances in
Neural Information Processing Systems (NIPS).
N. Okazaki. 2007. CRFsuite: a fast imple-
mentation of conditional random fields (CRFs).
http://www.chokkan.org/software/crfsuite/.
PennBioIE. 2005. Mining the bibliome project.
http://bioie.ldc.upenn.edu.
L. Rabiner and B. Juang. 1986. An introduction to hid-
den Markov models. IEEEASSP Magazine, 3(1):4–
16.
R. Reppen, N. Ide, and K. Suderman. 2005. Ameri-
can national corpus (anc) second release. Linguistic
Data Consortium.
I. Sag and T. Wasow. 1999. Syntactic theory: a formal
introduction. CSLI publications.
K. Sagae and J. Tsujii. 2007. Dependency parsing
and domain adaptation with lr models and parser en-
sembles. In Proc. of CoNLL Shared Task Session of
EMNLP-CoNLL.
V. Srikumar and C. Manning. 2014. Learning dis-
tributed representations for structured output predic-
tion. In Advances in Neural Information Processing
Systems (NIPS).
K. Stratos, A. Rush, S. Cohen, and M. Collins. 2013.
Spectral learning of refinement HMMs. In Proc.
of the Conference on Computational Natural Lan-
guage Learning (CoNLL).
K. Tjong, E. Sang, , and S. Buchholz. 2000. Introduc-
tion to the CoNLL-2000 shared task: Chunking. In
Proc. of the Conference on Computational Natural
Language Learning (CoNLL).
J. Turian, L. Ratinov, and Y. Bengio. 2010. Word rep-
resentations: a simple and general method for semi-
supervised learning. In Proc. of the Annual Meeting
of the Association for Comput. Linguistics (ACL).
</reference>
<page confidence="0.998579">
529
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.828192">
<title confidence="0.999546">Learning Hidden Markov Models with State Representations for Domain Adaptation</title>
<author confidence="0.994043">Xiao</author>
<affiliation confidence="0.998413">Department of Computer and Information</affiliation>
<address confidence="0.849923">Temple University, Philadelphia, PA 19122,</address>
<abstract confidence="0.99901775">Recently, a variety of representation learning approaches have been developed in the literature to induce latent generalizable features across two domains. In this paper, we extend the standard hidden Markov models (HMMs) to learn distributed state representations to improve cross-domain prediction performance. We reformulate the HMMs by mapping each discrete hidden state to a distributed representation vector and employ an expectationmaximization algorithm to jointly learn distributed state representations and model parameters. We empirically investigate the proposed model on cross-domain part-ofspeech tagging and noun-phrase chunking tasks. The experimental results demonstrate the effectiveness of the distributed HMMs on facilitating domain adaptation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Blitzer</author>
<author>R McDonald</author>
<author>F Pereira</author>
</authors>
<title>Domain adaptation with structural correspondence learning.</title>
<date>2006</date>
<booktitle>In Proc. of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="1501" citStr="Blitzer et al., 2006" startWordPosition="208" endWordPosition="211"> chunking tasks. The experimental results demonstrate the effectiveness of the distributed HMMs on facilitating domain adaptation. 1 Introduction Domain adaptation aims to obtain an effective prediction model for a particular target domain where labeled training data is scarce by exploiting labeled data from a related source domain. Domain adaptation is very important in the field of natural language processing (NLP) as it can reduce the expensive manual annotation effort in the target domain. Various NLP tasks have benefited from domain adaptation techniques, including part-ofspeech tagging (Blitzer et al., 2006; Huang and Yates, 2010a), chunking (Daum´e III, 2007; Huang and Yates, 2009), named entity recognition (Guo et al., 2009; Turian et al., 2010), dependency parsing (Dredze et al., 2007; Sagae and Tsujii, 2007) and semantic role labeling (Dahlmeier and Ng, 2010; Huang and Yates, 2010b). In a typical domain adaptation scenario of NLP, the source and target domains contain text data of different genres (e.g., newswire vs biomedical (Blitzer et al., 2006)). Under such circumstances, the original lexical features may not perform well in cross-domain learning since different genres of text may use v</context>
<context position="6233" citStr="Blitzer et al. (2006)" startWordPosition="925" endWordPosition="928">presentation vector for each word as latent features for domain adaptation. Turian et al. (2010) empirically studied using word embeddings learned from hierarchical log-bilinear models (Mnih and Geoffrey, 2008) and neural language models (Collobert and Weston, 2008) for cross-domain NER tasks. Hovy et al. (2015) used the word embeddings learned from the Skip-gram Model (SGM) (Mikolov et al., 2013) to develop a POS tagger for Twitter data with labeled newswire training data. Some other representation learning methods have been developed to tackle NLP cross-domain problems as well. For example, Blitzer et al. (2006) proposed a structural correspondence learning method for POS tagging, which first selects a set of pivot features (occurring frequently in Figure 1: Hidden Markov models with distributed state representations (dHMM). the two domains) and then models the correlations between pivot features and non-pivot features to induce generalizable features. In terms of performing distributed representation learning for output variables, our proposed model shares similarity with the structured output representation learning approach developed by Srikumar and Manning (2014), which extends the structured sup</context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>J. Blitzer, R. McDonald, and F. Pereira. 2006. Domain adaptation with structural correspondence learning. In Proc. of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>P deSouza</author>
<author>R Mercer</author>
<author>V Pietra</author>
<author>J Lai</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Compututal Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="5342" citStr="Brown et al., 1992" startWordPosition="789" endWordPosition="792">d generalizing capabilities for cross-domain learning than standard HMMs. 2 Related Work A variety of representation learning approaches have been developed in the literature to address NLP domain adaptation problems. The clustering based word representation learning methods perform word clustering within the sentence structure and use word cluster indicators as generalizable features to address domain adaptation problems. For example, Huang and Yates (2009) used the discrete hidden state of a word under HMMs as augmenting features for cross-domain POS tagging and NP chunking. Brown clusters (Brown et al., 1992), which was used as latent features for simple in-domain dependency parsing (Koo et al., 2008), has recently been exploited for out-ofdomain statistical parsing (Candito et al., 2011). The word embedding based representation learning methods learn a dense real-valued representation vector for each word as latent features for domain adaptation. Turian et al. (2010) empirically studied using word embeddings learned from hierarchical log-bilinear models (Mnih and Geoffrey, 2008) and neural language models (Collobert and Weston, 2008) for cross-domain NER tasks. Hovy et al. (2015) used the word em</context>
</contexts>
<marker>Brown, deSouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>P. Brown, P. deSouza, R. Mercer, V. Pietra, and J. Lai. 1992. Class-based n-gram models of natural language. Compututal Linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Candito</author>
<author>E Anguiano</author>
<author>D Seddah</author>
</authors>
<title>A word clustering approach to domain adaptation: Effective parsing of biomedical texts.</title>
<date>2011</date>
<booktitle>In Proc. of the Inter. Conference on Parsing Technologies (IWPT).</booktitle>
<contexts>
<context position="2462" citStr="Candito et al., 2011" startWordPosition="359" endWordPosition="362">ource and target domains contain text data of different genres (e.g., newswire vs biomedical (Blitzer et al., 2006)). Under such circumstances, the original lexical features may not perform well in cross-domain learning since different genres of text may use very different vocabularies and produce cross-domain feature distribution divergence and feature sparsity issue. A number of techniques have been developed in the literature to tackle the problem of cross-domain feature divergence and feature sparsity, including clustering based word representation learning methods (Huang and Yates, 2009; Candito et al., 2011), word embedding based representation learning methods (Turian et al., 2010; Hovy et al., 2015) and some other representation learning methods (Blitzer et al., 2006). In this paper, we extend the standard hidden Markov models (HMMs) to perform distributed state representation learning and induce contextaware distributed word representations for domain adaptation. Instead of learning a single discrete latent state for each observation in a given sentence, we learn a distributed representation vector. We define a state embedding matrix to map each latent state value to a low-dimensional distribu</context>
<context position="5525" citStr="Candito et al., 2011" startWordPosition="817" endWordPosition="820">ress NLP domain adaptation problems. The clustering based word representation learning methods perform word clustering within the sentence structure and use word cluster indicators as generalizable features to address domain adaptation problems. For example, Huang and Yates (2009) used the discrete hidden state of a word under HMMs as augmenting features for cross-domain POS tagging and NP chunking. Brown clusters (Brown et al., 1992), which was used as latent features for simple in-domain dependency parsing (Koo et al., 2008), has recently been exploited for out-ofdomain statistical parsing (Candito et al., 2011). The word embedding based representation learning methods learn a dense real-valued representation vector for each word as latent features for domain adaptation. Turian et al. (2010) empirically studied using word embeddings learned from hierarchical log-bilinear models (Mnih and Geoffrey, 2008) and neural language models (Collobert and Weston, 2008) for cross-domain NER tasks. Hovy et al. (2015) used the word embeddings learned from the Skip-gram Model (SGM) (Mikolov et al., 2013) to develop a POS tagger for Twitter data with labeled newswire training data. Some other representation learning</context>
</contexts>
<marker>Candito, Anguiano, Seddah, 2011</marker>
<rawString>M. Candito, E. Anguiano, and D. Seddah. 2011. A word clustering approach to domain adaptation: Effective parsing of biomedical texts. In Proc. of the Inter. Conference on Parsing Technologies (IWPT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Collobert</author>
<author>J Weston</author>
</authors>
<title>A unified architecture for natural language processing: deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proc. of the Inter. Conference on Machine Learning (ICML).</booktitle>
<contexts>
<context position="5878" citStr="Collobert and Weston, 2008" startWordPosition="867" endWordPosition="871"> features for cross-domain POS tagging and NP chunking. Brown clusters (Brown et al., 1992), which was used as latent features for simple in-domain dependency parsing (Koo et al., 2008), has recently been exploited for out-ofdomain statistical parsing (Candito et al., 2011). The word embedding based representation learning methods learn a dense real-valued representation vector for each word as latent features for domain adaptation. Turian et al. (2010) empirically studied using word embeddings learned from hierarchical log-bilinear models (Mnih and Geoffrey, 2008) and neural language models (Collobert and Weston, 2008) for cross-domain NER tasks. Hovy et al. (2015) used the word embeddings learned from the Skip-gram Model (SGM) (Mikolov et al., 2013) to develop a POS tagger for Twitter data with labeled newswire training data. Some other representation learning methods have been developed to tackle NLP cross-domain problems as well. For example, Blitzer et al. (2006) proposed a structural correspondence learning method for POS tagging, which first selects a set of pivot features (occurring frequently in Figure 1: Hidden Markov models with distributed state representations (dHMM). the two domains) and then m</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>R. Collobert and J. Weston. 2008. A unified architecture for natural language processing: deep neural networks with multitask learning. In Proc. of the Inter. Conference on Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Dahlmeier</author>
<author>H Ng</author>
</authors>
<title>Domain adaptation for semantic role labeling in the biomedical domain.</title>
<date>2010</date>
<journal>Bioinformatics,</journal>
<volume>26</volume>
<issue>8</issue>
<contexts>
<context position="1761" citStr="Dahlmeier and Ng, 2010" startWordPosition="250" endWordPosition="253">ning data is scarce by exploiting labeled data from a related source domain. Domain adaptation is very important in the field of natural language processing (NLP) as it can reduce the expensive manual annotation effort in the target domain. Various NLP tasks have benefited from domain adaptation techniques, including part-ofspeech tagging (Blitzer et al., 2006; Huang and Yates, 2010a), chunking (Daum´e III, 2007; Huang and Yates, 2009), named entity recognition (Guo et al., 2009; Turian et al., 2010), dependency parsing (Dredze et al., 2007; Sagae and Tsujii, 2007) and semantic role labeling (Dahlmeier and Ng, 2010; Huang and Yates, 2010b). In a typical domain adaptation scenario of NLP, the source and target domains contain text data of different genres (e.g., newswire vs biomedical (Blitzer et al., 2006)). Under such circumstances, the original lexical features may not perform well in cross-domain learning since different genres of text may use very different vocabularies and produce cross-domain feature distribution divergence and feature sparsity issue. A number of techniques have been developed in the literature to tackle the problem of cross-domain feature divergence and feature sparsity, includin</context>
</contexts>
<marker>Dahlmeier, Ng, 2010</marker>
<rawString>D. Dahlmeier and H. Ng. 2010. Domain adaptation for semantic role labeling in the biomedical domain. Bioinformatics, 26(8):1098–1104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Daum´e</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In Proc. of the Annual Meeting of the Association of Computational Linguistics (ACL).</booktitle>
<marker>Daum´e, 2007</marker>
<rawString>H. Daum´e III. 2007. Frustratingly easy domain adaptation. In Proc. of the Annual Meeting of the Association of Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Dempster</author>
<author>N Laird</author>
<author>D Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the em algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society, Series B,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="13284" citStr="Dempster et al., 1977" startWordPosition="2082" endWordPosition="2085">1) n where the regularization function is defined with Frobenius norms such as R(W, Q, M) = kWk2F + kQk 2 F + kMk2F . Moreover, each loglikelihood term has the following lower bound logP(On; O) = log J: P(On, Sn; O) Sn ≥ logP(On; O)−KL(Q(Sn)||P(Sn|On; O)) (2) where Q(Sn) is any valid distribution over the hidden state variables Sn and KL(.||.) denotes the Kullback-Leibler divergence. Let F(Q, O) denote the regularized lower bound function obtained by plugging the lower bound (2) back into the objective function (1). We then perform training by using an expectation-maximization (EM) algorithm (Dempster et al., 1977) that iteratively maximizes F(Q, O) to reach a local optimal solution. We first randomly initialize the model parameters while enforcing λ to be in the feasible region (λ ≥ 0, λT1 = 1). In the (k+1)-th iteration, given {Q(k), O(k)}, we then sequentially update Q with an E-step (3) and update O with a M-step (4). Q(k+1) = arg max F(Q, O(k)) (3) Q O(k+1) = arg max F(Q(k+1), O) (4) O 3.3 Domain Adaptation with Distributed State Representations We use all training data from the two domains to train dHMMs for local optimal model parameters O* = {M*, W*, Q*, σ*, λ*}. We then infer the latent state s</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. Dempster, N. Laird, and D. Rubin. 1977. Maximum likelihood from incomplete data via the em algorithm. Journal of the Royal Statistical Society, Series B, 39(1):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dredze</author>
<author>J Blitzer</author>
<author>P Talukdar</author>
<author>K Ganchev</author>
<author>J Grac¸a</author>
<author>O Pereira</author>
</authors>
<title>Frustratingly hard domain adaptation for dependency parsing.</title>
<date>2007</date>
<booktitle>In Proc. of CoNLL Shared Task Session of EMNLP-CoNLL.</booktitle>
<marker>Dredze, Blitzer, Talukdar, Ganchev, Grac¸a, Pereira, 2007</marker>
<rawString>M. Dredze, J. Blitzer, P. Talukdar, K. Ganchev, J. Grac¸a, and O. Pereira. 2007. Frustratingly hard domain adaptation for dependency parsing. In Proc. of CoNLL Shared Task Session of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Guo</author>
<author>H Zhu</author>
<author>Z Guo</author>
<author>X Zhang</author>
<author>X Wu</author>
<author>Z Su</author>
</authors>
<title>Domain adaptation with latent semantic association for named entity recognition.</title>
<date>2009</date>
<booktitle>In Proc. of Human Language Technologies: The Annual Conf. of North American Chapter of ACL (HLT-NAACL).</booktitle>
<contexts>
<context position="1622" citStr="Guo et al., 2009" startWordPosition="227" endWordPosition="230">tion. 1 Introduction Domain adaptation aims to obtain an effective prediction model for a particular target domain where labeled training data is scarce by exploiting labeled data from a related source domain. Domain adaptation is very important in the field of natural language processing (NLP) as it can reduce the expensive manual annotation effort in the target domain. Various NLP tasks have benefited from domain adaptation techniques, including part-ofspeech tagging (Blitzer et al., 2006; Huang and Yates, 2010a), chunking (Daum´e III, 2007; Huang and Yates, 2009), named entity recognition (Guo et al., 2009; Turian et al., 2010), dependency parsing (Dredze et al., 2007; Sagae and Tsujii, 2007) and semantic role labeling (Dahlmeier and Ng, 2010; Huang and Yates, 2010b). In a typical domain adaptation scenario of NLP, the source and target domains contain text data of different genres (e.g., newswire vs biomedical (Blitzer et al., 2006)). Under such circumstances, the original lexical features may not perform well in cross-domain learning since different genres of text may use very different vocabularies and produce cross-domain feature distribution divergence and feature sparsity issue. A number </context>
</contexts>
<marker>Guo, Zhu, Guo, Zhang, Wu, Su, 2009</marker>
<rawString>H. Guo, H. Zhu, Z. Guo, X. Zhang, X. Wu, and Z. Su. 2009. Domain adaptation with latent semantic association for named entity recognition. In Proc. of Human Language Technologies: The Annual Conf. of North American Chapter of ACL (HLT-NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hovy</author>
<author>B Plank</author>
<author>H Alonso</author>
<author>A Søgaard</author>
</authors>
<title>Mining for unambiguous instances to adapt pos taggers to new domains.</title>
<date>2015</date>
<booktitle>In Proc. of the Conference of the North American Chapter of ACL (NAACL).</booktitle>
<contexts>
<context position="2557" citStr="Hovy et al., 2015" startWordPosition="373" endWordPosition="376">zer et al., 2006)). Under such circumstances, the original lexical features may not perform well in cross-domain learning since different genres of text may use very different vocabularies and produce cross-domain feature distribution divergence and feature sparsity issue. A number of techniques have been developed in the literature to tackle the problem of cross-domain feature divergence and feature sparsity, including clustering based word representation learning methods (Huang and Yates, 2009; Candito et al., 2011), word embedding based representation learning methods (Turian et al., 2010; Hovy et al., 2015) and some other representation learning methods (Blitzer et al., 2006). In this paper, we extend the standard hidden Markov models (HMMs) to perform distributed state representation learning and induce contextaware distributed word representations for domain adaptation. Instead of learning a single discrete latent state for each observation in a given sentence, we learn a distributed representation vector. We define a state embedding matrix to map each latent state value to a low-dimensional distributed vector and reformulate the three local distributions of HMMs based on the distributed state</context>
<context position="4411" citStr="Hovy et al., 2015" startWordPosition="650" endWordPosition="653">ates, 524 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 524–529, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics 2009) as we both use latent state representations as generalizable features. However, they use standard HMMs to produce discrete hidden state features for each observation word, while we induce distributed state representation vectors. Our distributed HMMs share similarities with the word embedding based method (Hovy et al., 2015), and can be more space-efficient than the standard HMMs. Moreover, our model can incorporate context information into observation feature vectors to perform representation learning in a context-aware manner. The distributed state representations induced by our model hence have larger representing capacities and generalizing capabilities for cross-domain learning than standard HMMs. 2 Related Work A variety of representation learning approaches have been developed in the literature to address NLP domain adaptation problems. The clustering based word representation learning methods perform word</context>
<context position="5925" citStr="Hovy et al. (2015)" startWordPosition="876" endWordPosition="879">Brown clusters (Brown et al., 1992), which was used as latent features for simple in-domain dependency parsing (Koo et al., 2008), has recently been exploited for out-ofdomain statistical parsing (Candito et al., 2011). The word embedding based representation learning methods learn a dense real-valued representation vector for each word as latent features for domain adaptation. Turian et al. (2010) empirically studied using word embeddings learned from hierarchical log-bilinear models (Mnih and Geoffrey, 2008) and neural language models (Collobert and Weston, 2008) for cross-domain NER tasks. Hovy et al. (2015) used the word embeddings learned from the Skip-gram Model (SGM) (Mikolov et al., 2013) to develop a POS tagger for Twitter data with labeled newswire training data. Some other representation learning methods have been developed to tackle NLP cross-domain problems as well. For example, Blitzer et al. (2006) proposed a structural correspondence learning method for POS tagging, which first selects a set of pivot features (occurring frequently in Figure 1: Hidden Markov models with distributed state representations (dHMM). the two domains) and then models the correlations between pivot features a</context>
<context position="12496" citStr="Hovy et al., 2015" startWordPosition="1946" endWordPosition="1949">y storage. In fact, the complexity of dHMMs can be independent of the vocabulary size by using flexible observation features. We represent the dHMM parameter set as O = {M E RH×m, W E RH×m, Q E Rm×d, σ E R, A E [0,1]H}, where m is a small constant. 3.2 Model Training Given a data set of N observed sequences {O1, ... , On, ... , ON}, its regularized logexp l0(st+1)&gt;WM&gt;0(st)} )&gt;} P (ot|st; O) = 526 Table 1: Test performance for cross-domain POS tagging and NP chunking. Systems POS Tagging (Accuracy (%)) NP Chunking (F1-score) All Words OOV Words All NPs OOV NPs Baseline 88.3 67.3 0.86 0.74 SGM (Hovy et al., 2015) 89.0 71.4 0.88 0.78 HMM (Huang and Yates, 2009) 90.5 75.2 0.91 0.85 dHMM 91.1 76.0 0.93 0.88 likelihood can be written as follows J: log P(On;O)− η L(O)= 2R(W,Q,M) (1) n where the regularization function is defined with Frobenius norms such as R(W, Q, M) = kWk2F + kQk 2 F + kMk2F . Moreover, each loglikelihood term has the following lower bound logP(On; O) = log J: P(On, Sn; O) Sn ≥ logP(On; O)−KL(Q(Sn)||P(Sn|On; O)) (2) where Q(Sn) is any valid distribution over the hidden state variables Sn and KL(.||.) denotes the Kullback-Leibler divergence. Let F(Q, O) denote the regularized lower bound </context>
<context position="15876" citStr="Hovy et al., 2015" startWordPosition="2522" endWordPosition="2525">sentence. We used η = 0.5, set the number of hidden states H to be 80 and the dimensionality m = 20. We used all the labeled and unlabeled training data in the two domains to train dHMMs. 4.2 Test Results We used the induced distributed state representations of each observation as augmenting features to train conditional random fields (CRF) with the CRFSuite package (Okazaki, 2007) on the labeled source sentences and perform prediction on the target test sentences. We compared with the following systems: a Baseline system without representation learning, a SGM based word embedding 527 system (Hovy et al., 2015), and a discrete hidden state based clustering system (Huang and Yates, 2009). We used the word id and orthographic features as the baseline features for POS tagging and added POS tags for NP chunking. We reported the POS tagging accuracy for all words and outof-vocabulary (OOV) words (which appear less than three times in the labeled source training sentences), and NP chunking F1 scores for all NPs and only OOV NPs (whose beginning word is an OOV word) in Table 1. We can see that the Baseline method performs poorly on both tasks especially on the OOV words/NPs, which shows that the original l</context>
</contexts>
<marker>Hovy, Plank, Alonso, Søgaard, 2015</marker>
<rawString>D. Hovy, B. Plank, H. Alonso, and A. Søgaard. 2015. Mining for unambiguous instances to adapt pos taggers to new domains. In Proc. of the Conference of the North American Chapter of ACL (NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Huang</author>
<author>A Yates</author>
</authors>
<title>Distributional representations for handling sparsity in supervised sequencelabeling.</title>
<date>2009</date>
<booktitle>In Proc. of the Annual Meeting of the ACL and the IJCNLP of the AFNLP (ACL-AFNLP).</booktitle>
<contexts>
<context position="1578" citStr="Huang and Yates, 2009" startWordPosition="220" endWordPosition="223">the distributed HMMs on facilitating domain adaptation. 1 Introduction Domain adaptation aims to obtain an effective prediction model for a particular target domain where labeled training data is scarce by exploiting labeled data from a related source domain. Domain adaptation is very important in the field of natural language processing (NLP) as it can reduce the expensive manual annotation effort in the target domain. Various NLP tasks have benefited from domain adaptation techniques, including part-ofspeech tagging (Blitzer et al., 2006; Huang and Yates, 2010a), chunking (Daum´e III, 2007; Huang and Yates, 2009), named entity recognition (Guo et al., 2009; Turian et al., 2010), dependency parsing (Dredze et al., 2007; Sagae and Tsujii, 2007) and semantic role labeling (Dahlmeier and Ng, 2010; Huang and Yates, 2010b). In a typical domain adaptation scenario of NLP, the source and target domains contain text data of different genres (e.g., newswire vs biomedical (Blitzer et al., 2006)). Under such circumstances, the original lexical features may not perform well in cross-domain learning since different genres of text may use very different vocabularies and produce cross-domain feature distribution dive</context>
<context position="5185" citStr="Huang and Yates (2009)" startWordPosition="762" endWordPosition="765">rform representation learning in a context-aware manner. The distributed state representations induced by our model hence have larger representing capacities and generalizing capabilities for cross-domain learning than standard HMMs. 2 Related Work A variety of representation learning approaches have been developed in the literature to address NLP domain adaptation problems. The clustering based word representation learning methods perform word clustering within the sentence structure and use word cluster indicators as generalizable features to address domain adaptation problems. For example, Huang and Yates (2009) used the discrete hidden state of a word under HMMs as augmenting features for cross-domain POS tagging and NP chunking. Brown clusters (Brown et al., 1992), which was used as latent features for simple in-domain dependency parsing (Koo et al., 2008), has recently been exploited for out-ofdomain statistical parsing (Candito et al., 2011). The word embedding based representation learning methods learn a dense real-valued representation vector for each word as latent features for domain adaptation. Turian et al. (2010) empirically studied using word embeddings learned from hierarchical log-bili</context>
<context position="8731" citStr="Huang and Yates, 2009" startWordPosition="1319" endWordPosition="1322"> a d-dimensional feature vector. Let S = {s1, s2, ... , sT} be the sequence of T hidden states, where each hidden state st has a discrete state value from a total H hidden states H = {1, 2, ... , H}. Besides, we assume that there is a low-dimensional distributed representation vector associated with each hidden state. Let M E RH×m be the state embedding matrix where the i-th row MZ: denotes the m-dimensional representation vector for the i-th state. Previous works have demonstrated the usefulness of discrete hidden states induced from a HMM on addressing feature sparsity in domain adaptation (Huang and Yates, 2009). However, expressing a semantic word by a single discrete state value is too restrictive, as it has been shown in the literature that words have many different features in a multidimensional space where they could be separately characterized as number, POS tag, gender, tense, voice and other aspects (Sag and Wasow, 1999; Huang et al., 2011). Our proposed model aims to overcome this inherent drawback of standard HMMs on learning word representations. Given a set of observation sequences in two domains, the dHMM induces a distributed representation vector with continuous real values for each ob</context>
<context position="12544" citStr="Huang and Yates, 2009" startWordPosition="1955" endWordPosition="1958">can be independent of the vocabulary size by using flexible observation features. We represent the dHMM parameter set as O = {M E RH×m, W E RH×m, Q E Rm×d, σ E R, A E [0,1]H}, where m is a small constant. 3.2 Model Training Given a data set of N observed sequences {O1, ... , On, ... , ON}, its regularized logexp l0(st+1)&gt;WM&gt;0(st)} )&gt;} P (ot|st; O) = 526 Table 1: Test performance for cross-domain POS tagging and NP chunking. Systems POS Tagging (Accuracy (%)) NP Chunking (F1-score) All Words OOV Words All NPs OOV NPs Baseline 88.3 67.3 0.86 0.74 SGM (Hovy et al., 2015) 89.0 71.4 0.88 0.78 HMM (Huang and Yates, 2009) 90.5 75.2 0.91 0.85 dHMM 91.1 76.0 0.93 0.88 likelihood can be written as follows J: log P(On;O)− η L(O)= 2R(W,Q,M) (1) n where the regularization function is defined with Frobenius norms such as R(W, Q, M) = kWk2F + kQk 2 F + kMk2F . Moreover, each loglikelihood term has the following lower bound logP(On; O) = log J: P(On, Sn; O) Sn ≥ logP(On; O)−KL(Q(Sn)||P(Sn|On; O)) (2) where Q(Sn) is any valid distribution over the hidden state variables Sn and KL(.||.) denotes the Kullback-Leibler divergence. Let F(Q, O) denote the regularized lower bound function obtained by plugging the lower bound (2</context>
<context position="14606" citStr="Huang and Yates, 2009" startWordPosition="2308" endWordPosition="2311">r each labeled source training sentence and each target test sentence. The corresponding distributed state representation vectors can be obtained as {M*Tφ(s*1), M*Tφ(s*2), ... , M*Tφ(s*T )}. We then train a supervised NLP system (e.g., POS tagging or NP chunking) on the labeled source training sentences using the distributed state representations as augmenting input features and perform prediction on the augmented test sentences. 4 Experiments We conducted experiments on cross-domain partof-speech (POS) tagging and noun-phrase (NP) chunking tasks. We used the same experimental datasets as in (Huang and Yates, 2009) for cross-domain POS tagging from Wall Street Journal (WSJ) domain (Marcus et al., 1993) to MEDLINE domain (PennBioIE, 2005) and for crossdomain NP chunking from CoNLL shared task dataset (Tjong et al., 2000) to Open American National Corpus (OANC) (Reppen et al., 2005). 4.1 Representation Learning We first built a unified vocabulary with all the data in the two domains. We then conducted latent semantic analysis (LSA) over the sentence-word frequency matrix to get a low-dimensional representation vector for each word. We used a sliding window with size 3 to construct the d-dimensional featur</context>
<context position="15953" citStr="Huang and Yates, 2009" startWordPosition="2534" endWordPosition="2537"> the dimensionality m = 20. We used all the labeled and unlabeled training data in the two domains to train dHMMs. 4.2 Test Results We used the induced distributed state representations of each observation as augmenting features to train conditional random fields (CRF) with the CRFSuite package (Okazaki, 2007) on the labeled source sentences and perform prediction on the target test sentences. We compared with the following systems: a Baseline system without representation learning, a SGM based word embedding 527 system (Hovy et al., 2015), and a discrete hidden state based clustering system (Huang and Yates, 2009). We used the word id and orthographic features as the baseline features for POS tagging and added POS tags for NP chunking. We reported the POS tagging accuracy for all words and outof-vocabulary (OOV) words (which appear less than three times in the labeled source training sentences), and NP chunking F1 scores for all NPs and only OOV NPs (whose beginning word is an OOV word) in Table 1. We can see that the Baseline method performs poorly on both tasks especially on the OOV words/NPs, which shows that the original lexical based features are not sufficient to develop a robust POS tagger/NP ch</context>
</contexts>
<marker>Huang, Yates, 2009</marker>
<rawString>F. Huang and A. Yates. 2009. Distributional representations for handling sparsity in supervised sequencelabeling. In Proc. of the Annual Meeting of the ACL and the IJCNLP of the AFNLP (ACL-AFNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Huang</author>
<author>A Yates</author>
</authors>
<title>Exploring representation-learning approaches to domain adaptation.</title>
<date>2010</date>
<booktitle>In Proc. of the Workshop on Domain Adaptation for Natural Language Processing (DANLP).</booktitle>
<contexts>
<context position="1524" citStr="Huang and Yates, 2010" startWordPosition="212" endWordPosition="215">xperimental results demonstrate the effectiveness of the distributed HMMs on facilitating domain adaptation. 1 Introduction Domain adaptation aims to obtain an effective prediction model for a particular target domain where labeled training data is scarce by exploiting labeled data from a related source domain. Domain adaptation is very important in the field of natural language processing (NLP) as it can reduce the expensive manual annotation effort in the target domain. Various NLP tasks have benefited from domain adaptation techniques, including part-ofspeech tagging (Blitzer et al., 2006; Huang and Yates, 2010a), chunking (Daum´e III, 2007; Huang and Yates, 2009), named entity recognition (Guo et al., 2009; Turian et al., 2010), dependency parsing (Dredze et al., 2007; Sagae and Tsujii, 2007) and semantic role labeling (Dahlmeier and Ng, 2010; Huang and Yates, 2010b). In a typical domain adaptation scenario of NLP, the source and target domains contain text data of different genres (e.g., newswire vs biomedical (Blitzer et al., 2006)). Under such circumstances, the original lexical features may not perform well in cross-domain learning since different genres of text may use very different vocabular</context>
</contexts>
<marker>Huang, Yates, 2010</marker>
<rawString>F. Huang and A. Yates. 2010a. Exploring representation-learning approaches to domain adaptation. In Proc. of the Workshop on Domain Adaptation for Natural Language Processing (DANLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Huang</author>
<author>A Yates</author>
</authors>
<title>Open-domain semantic role labeling by modeling word spans.</title>
<date>2010</date>
<booktitle>In Proc. of the Annual Meeting of ACL (ACL).</booktitle>
<contexts>
<context position="1524" citStr="Huang and Yates, 2010" startWordPosition="212" endWordPosition="215">xperimental results demonstrate the effectiveness of the distributed HMMs on facilitating domain adaptation. 1 Introduction Domain adaptation aims to obtain an effective prediction model for a particular target domain where labeled training data is scarce by exploiting labeled data from a related source domain. Domain adaptation is very important in the field of natural language processing (NLP) as it can reduce the expensive manual annotation effort in the target domain. Various NLP tasks have benefited from domain adaptation techniques, including part-ofspeech tagging (Blitzer et al., 2006; Huang and Yates, 2010a), chunking (Daum´e III, 2007; Huang and Yates, 2009), named entity recognition (Guo et al., 2009; Turian et al., 2010), dependency parsing (Dredze et al., 2007; Sagae and Tsujii, 2007) and semantic role labeling (Dahlmeier and Ng, 2010; Huang and Yates, 2010b). In a typical domain adaptation scenario of NLP, the source and target domains contain text data of different genres (e.g., newswire vs biomedical (Blitzer et al., 2006)). Under such circumstances, the original lexical features may not perform well in cross-domain learning since different genres of text may use very different vocabular</context>
</contexts>
<marker>Huang, Yates, 2010</marker>
<rawString>F. Huang and A. Yates. 2010b. Open-domain semantic role labeling by modeling word spans. In Proc. of the Annual Meeting of ACL (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Huang</author>
<author>A Yates</author>
<author>A Ahuja</author>
<author>D Downey</author>
</authors>
<title>Language models as representations for weaklysupervised nlp tasks.</title>
<date>2011</date>
<booktitle>In Proc. of the Conference on Comput. Natural Language Learning (CoNLL).</booktitle>
<contexts>
<context position="9074" citStr="Huang et al., 2011" startWordPosition="1377" endWordPosition="1380">embedding matrix where the i-th row MZ: denotes the m-dimensional representation vector for the i-th state. Previous works have demonstrated the usefulness of discrete hidden states induced from a HMM on addressing feature sparsity in domain adaptation (Huang and Yates, 2009). However, expressing a semantic word by a single discrete state value is too restrictive, as it has been shown in the literature that words have many different features in a multidimensional space where they could be separately characterized as number, POS tag, gender, tense, voice and other aspects (Sag and Wasow, 1999; Huang et al., 2011). Our proposed model aims to overcome this inherent drawback of standard HMMs on learning word representations. Given a set of observation sequences in two domains, the dHMM induces a distributed representation vector with continuous real values for each observation word as generalizable features, which has the capacity of capturing multi-aspect latent characteristics of the word clusters. 3.1 Model Formulation To build the dHMMs, we reformulate the standard HMMs by defining three main local distributions based on the distributed state representations, i.e., the initial state distribution, the</context>
</contexts>
<marker>Huang, Yates, Ahuja, Downey, 2011</marker>
<rawString>F. Huang, A. Yates, A. Ahuja, and D. Downey. 2011. Language models as representations for weaklysupervised nlp tasks. In Proc. of the Conference on Comput. Natural Language Learning (CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Jaeger</author>
</authors>
<title>Observable operator models for discrete stochastic time series.</title>
<date>1999</date>
<journal>Neural Computation,</journal>
<pages>12--1371</pages>
<contexts>
<context position="7395" citStr="Jaeger, 1999" startWordPosition="1094" endWordPosition="1095">nd Manning (2014), which extends the structured support vector machines to simultaneously learn the prediction model and the distributed representations of the output labels. However, the approach in (Srikumar and Manning, 2014) assumes the training labels (i.e., output values) are given and performs learning in the standard supervised in-domain setting, while our proposed distributed HMMs address cross-domain learning problems by performing unsupervised representation learning. There are also a few works that extended standard HMMs in the literature, including the observable operator models (Jaeger, 1999), and the spectral learning method (Stratos et al., 2013). But none of them performs representation learning to address cross-domain adaptation problems. 3 Proposed Model In this paper, we propose a novel distributed hidden Markov model (dHMM) for representation learning over sequence data. This model extends the hidden Markov models (Rabiner and Juang, 1986) to learn distributed state representations. Similar as HMMs, a dHMM (shown in Figure 1) is a two-layer generative graphical model, which generates a sequence of observations from a sequence of latent state variables using Markov 525 prope</context>
</contexts>
<marker>Jaeger, 1999</marker>
<rawString>H. Jaeger. 1999. Observable operator models for discrete stochastic time series. Neural Computation, 12:1371–1398.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>X Carreras</author>
<author>M Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<booktitle>In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="5436" citStr="Koo et al., 2008" startWordPosition="804" endWordPosition="807">ty of representation learning approaches have been developed in the literature to address NLP domain adaptation problems. The clustering based word representation learning methods perform word clustering within the sentence structure and use word cluster indicators as generalizable features to address domain adaptation problems. For example, Huang and Yates (2009) used the discrete hidden state of a word under HMMs as augmenting features for cross-domain POS tagging and NP chunking. Brown clusters (Brown et al., 1992), which was used as latent features for simple in-domain dependency parsing (Koo et al., 2008), has recently been exploited for out-ofdomain statistical parsing (Candito et al., 2011). The word embedding based representation learning methods learn a dense real-valued representation vector for each word as latent features for domain adaptation. Turian et al. (2010) empirically studied using word embeddings learned from hierarchical log-bilinear models (Mnih and Geoffrey, 2008) and neural language models (Collobert and Weston, 2008) for cross-domain NER tasks. Hovy et al. (2015) used the word embeddings learned from the Skip-gram Model (SGM) (Mikolov et al., 2013) to develop a POS tagger</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>T. Koo, X. Carreras, and M. Collins. 2008. Simple semi-supervised dependency parsing. In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>M Marcinkiewicz</author>
<author>B Santorini</author>
</authors>
<title>Building a large annotated corpus of english: The Penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="14695" citStr="Marcus et al., 1993" startWordPosition="2323" endWordPosition="2326">stributed state representation vectors can be obtained as {M*Tφ(s*1), M*Tφ(s*2), ... , M*Tφ(s*T )}. We then train a supervised NLP system (e.g., POS tagging or NP chunking) on the labeled source training sentences using the distributed state representations as augmenting input features and perform prediction on the augmented test sentences. 4 Experiments We conducted experiments on cross-domain partof-speech (POS) tagging and noun-phrase (NP) chunking tasks. We used the same experimental datasets as in (Huang and Yates, 2009) for cross-domain POS tagging from Wall Street Journal (WSJ) domain (Marcus et al., 1993) to MEDLINE domain (PennBioIE, 2005) and for crossdomain NP chunking from CoNLL shared task dataset (Tjong et al., 2000) to Open American National Corpus (OANC) (Reppen et al., 2005). 4.1 Representation Learning We first built a unified vocabulary with all the data in the two domains. We then conducted latent semantic analysis (LSA) over the sentence-word frequency matrix to get a low-dimensional representation vector for each word. We used a sliding window with size 3 to construct the d-dimensional feature vector (d = 1500) for each observation in a given sentence. We used η = 0.5, set the nu</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>M. Marcus, M. Marcinkiewicz, and B. Santorini. 1993. Building a large annotated corpus of english: The Penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mikolov</author>
<author>I Sutskever</author>
<author>K Chen</author>
<author>G Corrado</author>
<author>J Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="6012" citStr="Mikolov et al., 2013" startWordPosition="891" endWordPosition="894">-domain dependency parsing (Koo et al., 2008), has recently been exploited for out-ofdomain statistical parsing (Candito et al., 2011). The word embedding based representation learning methods learn a dense real-valued representation vector for each word as latent features for domain adaptation. Turian et al. (2010) empirically studied using word embeddings learned from hierarchical log-bilinear models (Mnih and Geoffrey, 2008) and neural language models (Collobert and Weston, 2008) for cross-domain NER tasks. Hovy et al. (2015) used the word embeddings learned from the Skip-gram Model (SGM) (Mikolov et al., 2013) to develop a POS tagger for Twitter data with labeled newswire training data. Some other representation learning methods have been developed to tackle NLP cross-domain problems as well. For example, Blitzer et al. (2006) proposed a structural correspondence learning method for POS tagging, which first selects a set of pivot features (occurring frequently in Figure 1: Hidden Markov models with distributed state representations (dHMM). the two domains) and then models the correlations between pivot features and non-pivot features to induce generalizable features. In terms of performing distribu</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mnih</author>
<author>E Geoffrey</author>
</authors>
<title>A scalable hierarchical distributed language model.</title>
<date>2008</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="5822" citStr="Mnih and Geoffrey, 2008" startWordPosition="859" endWordPosition="862">crete hidden state of a word under HMMs as augmenting features for cross-domain POS tagging and NP chunking. Brown clusters (Brown et al., 1992), which was used as latent features for simple in-domain dependency parsing (Koo et al., 2008), has recently been exploited for out-ofdomain statistical parsing (Candito et al., 2011). The word embedding based representation learning methods learn a dense real-valued representation vector for each word as latent features for domain adaptation. Turian et al. (2010) empirically studied using word embeddings learned from hierarchical log-bilinear models (Mnih and Geoffrey, 2008) and neural language models (Collobert and Weston, 2008) for cross-domain NER tasks. Hovy et al. (2015) used the word embeddings learned from the Skip-gram Model (SGM) (Mikolov et al., 2013) to develop a POS tagger for Twitter data with labeled newswire training data. Some other representation learning methods have been developed to tackle NLP cross-domain problems as well. For example, Blitzer et al. (2006) proposed a structural correspondence learning method for POS tagging, which first selects a set of pivot features (occurring frequently in Figure 1: Hidden Markov models with distributed s</context>
</contexts>
<marker>Mnih, Geoffrey, 2008</marker>
<rawString>A. Mnih and E. Geoffrey. 2008. A scalable hierarchical distributed language model. In Advances in Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Okazaki</author>
</authors>
<title>CRFsuite: a fast implementation of conditional random fields</title>
<date>2007</date>
<contexts>
<context position="15642" citStr="Okazaki, 2007" startWordPosition="2486" endWordPosition="2487">) over the sentence-word frequency matrix to get a low-dimensional representation vector for each word. We used a sliding window with size 3 to construct the d-dimensional feature vector (d = 1500) for each observation in a given sentence. We used η = 0.5, set the number of hidden states H to be 80 and the dimensionality m = 20. We used all the labeled and unlabeled training data in the two domains to train dHMMs. 4.2 Test Results We used the induced distributed state representations of each observation as augmenting features to train conditional random fields (CRF) with the CRFSuite package (Okazaki, 2007) on the labeled source sentences and perform prediction on the target test sentences. We compared with the following systems: a Baseline system without representation learning, a SGM based word embedding 527 system (Hovy et al., 2015), and a discrete hidden state based clustering system (Huang and Yates, 2009). We used the word id and orthographic features as the baseline features for POS tagging and added POS tags for NP chunking. We reported the POS tagging accuracy for all words and outof-vocabulary (OOV) words (which appear less than three times in the labeled source training sentences), a</context>
</contexts>
<marker>Okazaki, 2007</marker>
<rawString>N. Okazaki. 2007. CRFsuite: a fast implementation of conditional random fields (CRFs). http://www.chokkan.org/software/crfsuite/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>PennBioIE</author>
</authors>
<title>Mining the bibliome project.</title>
<date>2005</date>
<note>http://bioie.ldc.upenn.edu.</note>
<contexts>
<context position="14731" citStr="PennBioIE, 2005" startWordPosition="2331" endWordPosition="2332">an be obtained as {M*Tφ(s*1), M*Tφ(s*2), ... , M*Tφ(s*T )}. We then train a supervised NLP system (e.g., POS tagging or NP chunking) on the labeled source training sentences using the distributed state representations as augmenting input features and perform prediction on the augmented test sentences. 4 Experiments We conducted experiments on cross-domain partof-speech (POS) tagging and noun-phrase (NP) chunking tasks. We used the same experimental datasets as in (Huang and Yates, 2009) for cross-domain POS tagging from Wall Street Journal (WSJ) domain (Marcus et al., 1993) to MEDLINE domain (PennBioIE, 2005) and for crossdomain NP chunking from CoNLL shared task dataset (Tjong et al., 2000) to Open American National Corpus (OANC) (Reppen et al., 2005). 4.1 Representation Learning We first built a unified vocabulary with all the data in the two domains. We then conducted latent semantic analysis (LSA) over the sentence-word frequency matrix to get a low-dimensional representation vector for each word. We used a sliding window with size 3 to construct the d-dimensional feature vector (d = 1500) for each observation in a given sentence. We used η = 0.5, set the number of hidden states H to be 80 and</context>
</contexts>
<marker>PennBioIE, 2005</marker>
<rawString>PennBioIE. 2005. Mining the bibliome project. http://bioie.ldc.upenn.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Rabiner</author>
<author>B Juang</author>
</authors>
<title>An introduction to hidden Markov models.</title>
<date>1986</date>
<journal>IEEEASSP Magazine,</journal>
<volume>3</volume>
<issue>1</issue>
<pages>16</pages>
<contexts>
<context position="7756" citStr="Rabiner and Juang, 1986" startWordPosition="1147" endWordPosition="1150">tting, while our proposed distributed HMMs address cross-domain learning problems by performing unsupervised representation learning. There are also a few works that extended standard HMMs in the literature, including the observable operator models (Jaeger, 1999), and the spectral learning method (Stratos et al., 2013). But none of them performs representation learning to address cross-domain adaptation problems. 3 Proposed Model In this paper, we propose a novel distributed hidden Markov model (dHMM) for representation learning over sequence data. This model extends the hidden Markov models (Rabiner and Juang, 1986) to learn distributed state representations. Similar as HMMs, a dHMM (shown in Figure 1) is a two-layer generative graphical model, which generates a sequence of observations from a sequence of latent state variables using Markov 525 properties. Let O = {o1, o2, ... , oT} be the sequence of observations with length T, where each observation ot E Rd is a d-dimensional feature vector. Let S = {s1, s2, ... , sT} be the sequence of T hidden states, where each hidden state st has a discrete state value from a total H hidden states H = {1, 2, ... , H}. Besides, we assume that there is a low-dimensio</context>
<context position="11387" citStr="Rabiner and Juang, 1986" startWordPosition="1755" endWordPosition="1758">e Gaussian model here generates each observation ot as a d-dimensional continuous feature vector. This type of emission distribution provides us the flexibility to incorporate local context information or statistical global information for inducing distributed state representations. For example, we can use the concatenation of the one-hot word vectors within a sliding window around the target word as the observation vector. Moreover, we can also use the globally preprocessed continuous word vectors as the observation vectors, which we will describe later in our experiments. The standard HMMs (Rabiner and Juang, 1986) use conditional probability tables for the state transition distribution, which grows quadratically with respect to the number of hidden states, and the emission distribution, which grows linearly with respect to the observed vocabulary size that is usually very large in NLP tasks. Instead, the dHMMs can significantly reduce the sizes of these conditional probability tables by introducing the low-dimensional state embedding vectors, and the dHMM is much more efficient in terms of memory storage. In fact, the complexity of dHMMs can be independent of the vocabulary size by using flexible obser</context>
<context position="13981" citStr="Rabiner and Juang, 1986" startWordPosition="2212" endWordPosition="2215"> first randomly initialize the model parameters while enforcing λ to be in the feasible region (λ ≥ 0, λT1 = 1). In the (k+1)-th iteration, given {Q(k), O(k)}, we then sequentially update Q with an E-step (3) and update O with a M-step (4). Q(k+1) = arg max F(Q, O(k)) (3) Q O(k+1) = arg max F(Q(k+1), O) (4) O 3.3 Domain Adaptation with Distributed State Representations We use all training data from the two domains to train dHMMs for local optimal model parameters O* = {M*, W*, Q*, σ*, λ*}. We then infer the latent state sequence S* = {s*1, s*2, ... , s*T} using the standard Viterbi algorithm (Rabiner and Juang, 1986) for each labeled source training sentence and each target test sentence. The corresponding distributed state representation vectors can be obtained as {M*Tφ(s*1), M*Tφ(s*2), ... , M*Tφ(s*T )}. We then train a supervised NLP system (e.g., POS tagging or NP chunking) on the labeled source training sentences using the distributed state representations as augmenting input features and perform prediction on the augmented test sentences. 4 Experiments We conducted experiments on cross-domain partof-speech (POS) tagging and noun-phrase (NP) chunking tasks. We used the same experimental datasets as i</context>
</contexts>
<marker>Rabiner, Juang, 1986</marker>
<rawString>L. Rabiner and B. Juang. 1986. An introduction to hidden Markov models. IEEEASSP Magazine, 3(1):4– 16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Reppen</author>
<author>N Ide</author>
<author>K Suderman</author>
</authors>
<title>American national corpus (anc) second release. Linguistic Data Consortium.</title>
<date>2005</date>
<contexts>
<context position="14877" citStr="Reppen et al., 2005" startWordPosition="2355" endWordPosition="2358">beled source training sentences using the distributed state representations as augmenting input features and perform prediction on the augmented test sentences. 4 Experiments We conducted experiments on cross-domain partof-speech (POS) tagging and noun-phrase (NP) chunking tasks. We used the same experimental datasets as in (Huang and Yates, 2009) for cross-domain POS tagging from Wall Street Journal (WSJ) domain (Marcus et al., 1993) to MEDLINE domain (PennBioIE, 2005) and for crossdomain NP chunking from CoNLL shared task dataset (Tjong et al., 2000) to Open American National Corpus (OANC) (Reppen et al., 2005). 4.1 Representation Learning We first built a unified vocabulary with all the data in the two domains. We then conducted latent semantic analysis (LSA) over the sentence-word frequency matrix to get a low-dimensional representation vector for each word. We used a sliding window with size 3 to construct the d-dimensional feature vector (d = 1500) for each observation in a given sentence. We used η = 0.5, set the number of hidden states H to be 80 and the dimensionality m = 20. We used all the labeled and unlabeled training data in the two domains to train dHMMs. 4.2 Test Results We used the in</context>
</contexts>
<marker>Reppen, Ide, Suderman, 2005</marker>
<rawString>R. Reppen, N. Ide, and K. Suderman. 2005. American national corpus (anc) second release. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Sag</author>
<author>T Wasow</author>
</authors>
<title>Syntactic theory: a formal introduction. CSLI publications.</title>
<date>1999</date>
<contexts>
<context position="9053" citStr="Sag and Wasow, 1999" startWordPosition="1373" endWordPosition="1376"> E RH×m be the state embedding matrix where the i-th row MZ: denotes the m-dimensional representation vector for the i-th state. Previous works have demonstrated the usefulness of discrete hidden states induced from a HMM on addressing feature sparsity in domain adaptation (Huang and Yates, 2009). However, expressing a semantic word by a single discrete state value is too restrictive, as it has been shown in the literature that words have many different features in a multidimensional space where they could be separately characterized as number, POS tag, gender, tense, voice and other aspects (Sag and Wasow, 1999; Huang et al., 2011). Our proposed model aims to overcome this inherent drawback of standard HMMs on learning word representations. Given a set of observation sequences in two domains, the dHMM induces a distributed representation vector with continuous real values for each observation word as generalizable features, which has the capacity of capturing multi-aspect latent characteristics of the word clusters. 3.1 Model Formulation To build the dHMMs, we reformulate the standard HMMs by defining three main local distributions based on the distributed state representations, i.e., the initial st</context>
</contexts>
<marker>Sag, Wasow, 1999</marker>
<rawString>I. Sag and T. Wasow. 1999. Syntactic theory: a formal introduction. CSLI publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sagae</author>
<author>J Tsujii</author>
</authors>
<title>Dependency parsing and domain adaptation with lr models and parser ensembles.</title>
<date>2007</date>
<booktitle>In Proc. of CoNLL Shared Task Session of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="1710" citStr="Sagae and Tsujii, 2007" startWordPosition="242" endWordPosition="245">el for a particular target domain where labeled training data is scarce by exploiting labeled data from a related source domain. Domain adaptation is very important in the field of natural language processing (NLP) as it can reduce the expensive manual annotation effort in the target domain. Various NLP tasks have benefited from domain adaptation techniques, including part-ofspeech tagging (Blitzer et al., 2006; Huang and Yates, 2010a), chunking (Daum´e III, 2007; Huang and Yates, 2009), named entity recognition (Guo et al., 2009; Turian et al., 2010), dependency parsing (Dredze et al., 2007; Sagae and Tsujii, 2007) and semantic role labeling (Dahlmeier and Ng, 2010; Huang and Yates, 2010b). In a typical domain adaptation scenario of NLP, the source and target domains contain text data of different genres (e.g., newswire vs biomedical (Blitzer et al., 2006)). Under such circumstances, the original lexical features may not perform well in cross-domain learning since different genres of text may use very different vocabularies and produce cross-domain feature distribution divergence and feature sparsity issue. A number of techniques have been developed in the literature to tackle the problem of cross-domai</context>
</contexts>
<marker>Sagae, Tsujii, 2007</marker>
<rawString>K. Sagae and J. Tsujii. 2007. Dependency parsing and domain adaptation with lr models and parser ensembles. In Proc. of CoNLL Shared Task Session of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Srikumar</author>
<author>C Manning</author>
</authors>
<title>Learning distributed representations for structured output prediction.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS).</booktitle>
<contexts>
<context position="6799" citStr="Srikumar and Manning (2014)" startWordPosition="1004" endWordPosition="1007">s-domain problems as well. For example, Blitzer et al. (2006) proposed a structural correspondence learning method for POS tagging, which first selects a set of pivot features (occurring frequently in Figure 1: Hidden Markov models with distributed state representations (dHMM). the two domains) and then models the correlations between pivot features and non-pivot features to induce generalizable features. In terms of performing distributed representation learning for output variables, our proposed model shares similarity with the structured output representation learning approach developed by Srikumar and Manning (2014), which extends the structured support vector machines to simultaneously learn the prediction model and the distributed representations of the output labels. However, the approach in (Srikumar and Manning, 2014) assumes the training labels (i.e., output values) are given and performs learning in the standard supervised in-domain setting, while our proposed distributed HMMs address cross-domain learning problems by performing unsupervised representation learning. There are also a few works that extended standard HMMs in the literature, including the observable operator models (Jaeger, 1999), an</context>
</contexts>
<marker>Srikumar, Manning, 2014</marker>
<rawString>V. Srikumar and C. Manning. 2014. Learning distributed representations for structured output prediction. In Advances in Neural Information Processing Systems (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Stratos</author>
<author>A Rush</author>
<author>S Cohen</author>
<author>M Collins</author>
</authors>
<title>Spectral learning of refinement HMMs.</title>
<date>2013</date>
<booktitle>In Proc. of the Conference on Computational Natural Language Learning (CoNLL).</booktitle>
<contexts>
<context position="7452" citStr="Stratos et al., 2013" startWordPosition="1101" endWordPosition="1104">upport vector machines to simultaneously learn the prediction model and the distributed representations of the output labels. However, the approach in (Srikumar and Manning, 2014) assumes the training labels (i.e., output values) are given and performs learning in the standard supervised in-domain setting, while our proposed distributed HMMs address cross-domain learning problems by performing unsupervised representation learning. There are also a few works that extended standard HMMs in the literature, including the observable operator models (Jaeger, 1999), and the spectral learning method (Stratos et al., 2013). But none of them performs representation learning to address cross-domain adaptation problems. 3 Proposed Model In this paper, we propose a novel distributed hidden Markov model (dHMM) for representation learning over sequence data. This model extends the hidden Markov models (Rabiner and Juang, 1986) to learn distributed state representations. Similar as HMMs, a dHMM (shown in Figure 1) is a two-layer generative graphical model, which generates a sequence of observations from a sequence of latent state variables using Markov 525 properties. Let O = {o1, o2, ... , oT} be the sequence of obse</context>
</contexts>
<marker>Stratos, Rush, Cohen, Collins, 2013</marker>
<rawString>K. Stratos, A. Rush, S. Cohen, and M. Collins. 2013. Spectral learning of refinement HMMs. In Proc. of the Conference on Computational Natural Language Learning (CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Tjong</author>
<author>E Sang</author>
</authors>
<title>Introduction to the CoNLL-2000 shared task: Chunking.</title>
<date>2000</date>
<booktitle>In Proc. of the Conference on Computational Natural Language Learning (CoNLL).</booktitle>
<marker>Tjong, Sang, 2000</marker>
<rawString>K. Tjong, E. Sang, , and S. Buchholz. 2000. Introduction to the CoNLL-2000 shared task: Chunking. In Proc. of the Conference on Computational Natural Language Learning (CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Turian</author>
<author>L Ratinov</author>
<author>Y Bengio</author>
</authors>
<title>Word representations: a simple and general method for semisupervised learning.</title>
<date>2010</date>
<booktitle>In Proc. of the Annual Meeting of the Association for Comput. Linguistics (ACL).</booktitle>
<contexts>
<context position="1644" citStr="Turian et al., 2010" startWordPosition="231" endWordPosition="234">on Domain adaptation aims to obtain an effective prediction model for a particular target domain where labeled training data is scarce by exploiting labeled data from a related source domain. Domain adaptation is very important in the field of natural language processing (NLP) as it can reduce the expensive manual annotation effort in the target domain. Various NLP tasks have benefited from domain adaptation techniques, including part-ofspeech tagging (Blitzer et al., 2006; Huang and Yates, 2010a), chunking (Daum´e III, 2007; Huang and Yates, 2009), named entity recognition (Guo et al., 2009; Turian et al., 2010), dependency parsing (Dredze et al., 2007; Sagae and Tsujii, 2007) and semantic role labeling (Dahlmeier and Ng, 2010; Huang and Yates, 2010b). In a typical domain adaptation scenario of NLP, the source and target domains contain text data of different genres (e.g., newswire vs biomedical (Blitzer et al., 2006)). Under such circumstances, the original lexical features may not perform well in cross-domain learning since different genres of text may use very different vocabularies and produce cross-domain feature distribution divergence and feature sparsity issue. A number of techniques have bee</context>
<context position="5708" citStr="Turian et al. (2010)" startWordPosition="844" endWordPosition="847">generalizable features to address domain adaptation problems. For example, Huang and Yates (2009) used the discrete hidden state of a word under HMMs as augmenting features for cross-domain POS tagging and NP chunking. Brown clusters (Brown et al., 1992), which was used as latent features for simple in-domain dependency parsing (Koo et al., 2008), has recently been exploited for out-ofdomain statistical parsing (Candito et al., 2011). The word embedding based representation learning methods learn a dense real-valued representation vector for each word as latent features for domain adaptation. Turian et al. (2010) empirically studied using word embeddings learned from hierarchical log-bilinear models (Mnih and Geoffrey, 2008) and neural language models (Collobert and Weston, 2008) for cross-domain NER tasks. Hovy et al. (2015) used the word embeddings learned from the Skip-gram Model (SGM) (Mikolov et al., 2013) to develop a POS tagger for Twitter data with labeled newswire training data. Some other representation learning methods have been developed to tackle NLP cross-domain problems as well. For example, Blitzer et al. (2006) proposed a structural correspondence learning method for POS tagging, whic</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>J. Turian, L. Ratinov, and Y. Bengio. 2010. Word representations: a simple and general method for semisupervised learning. In Proc. of the Annual Meeting of the Association for Comput. Linguistics (ACL).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>