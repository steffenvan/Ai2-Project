<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001023">
<title confidence="0.981343">
Term Distillation in Patent Retrieval
</title>
<author confidence="0.914065">
Hideo Itoh Hiroko Mano Yasushi Ogawa
</author>
<affiliation confidence="0.776608">
Software R&amp;D Group, RICOH Co., Ltd.
</affiliation>
<address confidence="0.976906">
1-1-17 Koishikawa, Bunkyo-ku, Tokyo 112-0002, JAPAN
</address>
<email confidence="0.998935">
{hideo,mano,yogawa}@src.ricoh.co.jp
</email>
<sectionHeader confidence="0.983176" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999934666666667">
In cross-database retrieval, the domain
of queries differs from that of the re-
trieval target in the distribution of
term occurrences. This causes incor-
rect term weighting in the retrieval sys-
tem which assigns to each term a re-
trieval weight based on the distribu-
tion of term occurrences. To resolve
the problem, we propose &amp;quot;term distil-
lation&amp;quot;, a framework for query term
selection in cross-database retrieval.
The experiments using the NTCIR-3
patent retrieval test collection demon-
strate that term distillation is effective
for cross-database retrieval.
</bodyText>
<sectionHeader confidence="0.997336" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999797923076923">
For the mandatory runs of NTCIR-3 patent re-
trieval task 1, participants are required to con-
struct a search query from a news article and
retrieve patents which might be relevant to the
query. This is a kind of cross-database retrieval
in that the domain of queries (news article) dif-
fers from that of the retrieval target (patent)
(Iwayama et al., 2001).
Because in the distribution of term occur-
rences the query domain differs from the tar-
get domain, some query terms are given very
large weights (importance) by the retrieval sys-
tem even if the terms are not appropriate for
</bodyText>
<footnote confidence="0.35161">
http://research.nii.ac.jp/ntcir/ntcir-ws3/patent/
</footnote>
<bodyText confidence="0.998559866666667">
retrieval. For example, the query term &amp;quot;presi-
dent&amp;quot; in a news article might not be effective for
patent retrieval. However, the retrieval system
gives the term a large weight, because the docu-
ment frequency of the term in the patent genre
is very low. We think these problematic terms
are so many that the terms cannot be eliminated
using a stop word dictionary.
In order to resolve the problem mentioned
above, we propose &amp;quot;term distillation&amp;quot; which is
a framework for query term selection in cross-
database retrieval. The experiments using the
NTCIR patent retrieval test collection demon-
strate that term distillation is effective for cross-
database retrieval.
</bodyText>
<sectionHeader confidence="0.748701" genericHeader="method">
2 System description
</sectionHeader>
<bodyText confidence="0.999531833333334">
Before describing our approach, we give a short
description on our retrieval system. For the
NTCIR-3 experiments, we revised query pro-
cessing although the framework is the same as
that of NTCIR-2 (Ogawa and Mano, 2001). The
basic features of the system are as follows :
</bodyText>
<listItem confidence="0.997634222222222">
• Effective document ranking with pseudo-
relevance feedback based on Okapi&apos;s ap-
proach (Robertson and Walker, 1997) with
some improvements.
• Scalable and efficient indexing and search
based on the inverted file system (Ogawa
and Matsuda, 1999)
• Originally developed Japanese morpholog-
ical analyzer and normalizer for document
</listItem>
<bodyText confidence="0.975791">
indexing and query processing.
The inverted file was constructed for the re-
trieval target collection which contains full texts
of two years&apos; Japanese patents. We adopted
character n-gram indexing because it might be
difficult for Japanese morphological analyzer to
correctly recognize technical terms which are
crucial for patent retrieval.
In what follows, we describe the full automatic
process of document retrieval in the NTCIR-3
patent retrieval task.
</bodyText>
<listItem confidence="0.967495">
1. Query term extraction
</listItem>
<bodyText confidence="0.999105846153846">
Input query string is transformed into a se-
quence of words using the Japanese mor-
phological analyzer. Query terms are ex-
tracted by matching patterns against the
sequence. We can easily specify term ex-
traction using some patterns which are de-
scribed in regular expression on each word
form or tag assigned by the analyzer. Stop
words are eliminated using a stop word dic-
tionary. For initial retrieval, both &amp;quot;single
terms&amp;quot; and &amp;quot;phrasal terms&amp;quot; are used. A
phrasal term consists of two adjacent words
in the query string.
</bodyText>
<sectionHeader confidence="0.772566" genericHeader="method">
2. Initial retrieval
</sectionHeader>
<bodyText confidence="0.911524">
Each query term is submitted one by one to
the ranking search module, which assigns a
weight to the term and scores documents in-
cluding it. Retrieved documents are merged
and sorted on the score in the descending
order.
</bodyText>
<listItem confidence="0.665187">
3. Seed document selection
</listItem>
<bodyText confidence="0.9974106">
As a result of the initial retrieval, top
ranked documents are assumed to be
pseudo-relevant to the query and selected
as a &amp;quot;seed&amp;quot; of query expansion. The maxi-
mum number of seed documents is ten.
</bodyText>
<listItem confidence="0.753394">
4. Query expansion
</listItem>
<bodyText confidence="0.985366529411765">
Candidates of expansion terms are ex-
tracted from the seed documents by pattern
matching as in the query term extraction
mentioned above.
Phrasal terms are not used for query ex-
pansion because phrasal terms may be less
effective to improve recall and risky in case
of pseudo-relevance feedback.
The weight of initial query term is
re-calculated with the Robertson/Spark-
Jones formula (Robertson and Sparck-
Jones, 1976) if the term is found in the can-
didate pool.
The candidates are ranked on the Robert-
son&apos;s Selection Value (Robertson, 1990) and
top-ranked terms are selected as expansion
terms.
</bodyText>
<sectionHeader confidence="0.697235" genericHeader="method">
5. Final retrieval
</sectionHeader>
<bodyText confidence="0.999365333333333">
Each query and expansion term is submit-
ted one by one to the ranking search module
as in the initial retrieval.
</bodyText>
<sectionHeader confidence="0.94717" genericHeader="method">
3 Term distillation
</sectionHeader>
<bodyText confidence="0.9981925">
In cross-database retrieval, the domain of
queries (news article) differs from that of the re-
trieval target (patent) in the distribution of term
occurrences. This causes incorrect term weight-
ing in the retrieval system which assigns to each
term a retrieval weight based on the distribution
of term occurrences. Moreover, the terms which
might be given an incorrect weight are too many
to be collected in a stop word dictionary.
For these reasons, we find it necessary to
have a query term selection stage specially de-
signed for cross-database retrieval. We define
&amp;quot;term distillation&amp;quot; as a general framework for
the query term selection.
More specifically, the term distillation consists
of the following steps :
</bodyText>
<listItem confidence="0.9982464">
1. Extraction of query term candidates
Candidates of query terms are extracted
from the query string (news articles) and
pooled.
2. Assignment of TDV (Term Distillation
Value)
Each candidate in the pool is given a TDV
which represents &amp;quot;goodness&amp;quot; of the term to
retrieve documents in the target domain.
3. Selection of query terms
</listItem>
<bodyText confidence="0.999878538461538">
The candidates are ranked on the TDV and
top-ranked n terms are selected as query
terms, where n is an unknown constant
and treated as a tuning parameter for full-
automatic retrieval.
The term distillation seems appropriate to
avoid falling foul of the &amp;quot;curse of dimensional-
ity&amp;quot; (Robertson, 1990) in case that a given query
is very lengthy.
In what follows in this section, we explain
a generic model to define the TDV. Thereafter
some instances of the model which embody the
term distillation are introduced.
</bodyText>
<subsectionHeader confidence="0.686305">
3.1 Generic Model
</subsectionHeader>
<bodyText confidence="0.9999285">
In order to define the TDV, we give a generic
model with the following formula.
</bodyText>
<equation confidence="0.986982">
TDV = QV • TV
</equation>
<bodyText confidence="0.9999885">
where QV and TV represent the importance of
the term in the query and the target domain
respectively. QV seems to be commonly used
for query term extraction in ordinary retrieval
systems, however, TV is newly introduced for
cross-database retrieval. A combination of QV
and TV embodies a term distillation method.
We instance them separately as bellow.
</bodyText>
<subsectionHeader confidence="0.996053">
3.2 Instances of TV
</subsectionHeader>
<bodyText confidence="0.999917125">
We give some instances of TV using two prob-
abilities p and q, where p is a probability that
the term occurs in the target domain and q is
a probability that the term occurs in the query
domain. Because the estimation method of p
and q is independent on the instances of TV , it
is explained later. We show each instance of TV
with the id-tag as follows:
</bodyText>
<equation confidence="0.984678454545454">
TV0 : Zero model
TV = constant = 1
TV1 : Swet model (Robertson, 1990)
TV=p — q
TV2 : Naive Bayes model
TV � p
q
TV3 : Bayesian classification model
TV
a•p
= a•p+(1-a-E)•q+E
</equation>
<bodyText confidence="0.649061333333333">
where a and e are unknown constants.
TV4 : Binary independence model (Robertson
and Sparck-Jones, 1976)
</bodyText>
<equation confidence="0.998014307692308">
TV =log p(1q)
q(1p)
TV5 : Target domain model
TV = p
TV6 : Query domain model
TV = 1 — q
TV7 : Binary model
TV = 1 (p &gt; 0) or 0 (p = 0)
TV8 : Joint probability model
TV=p•(1—q)
TV9 : Decision theoretic model (Robertson
and Sparck-Jones, 1976)
TV = log(p) — log(q)
</equation>
<subsectionHeader confidence="0.987779">
3.3 Instances of QV
</subsectionHeader>
<bodyText confidence="0.9999365">
We show each instance of QV with the id-tag as
follows:
</bodyText>
<equation confidence="0.998348166666667">
QV0 : Zero model
QV = constant = 1
QV1 : Approximated 2-poisson model
(Robertson and Walker, 1994)
QV = tf
tf+13
</equation>
<bodyText confidence="0.999741">
where tf is the within-query term frequency
and 0 is an unknown constant.
</bodyText>
<equation confidence="0.995201">
QV2 : Term frequency model
QV=tf
QV3 : Term weight model
QV = weight
</equation>
<bodyText confidence="0.998414">
where weight is the retrieval weight given
by the retrieval system.
</bodyText>
<equation confidence="0.929390666666667">
QV4 : Combination of QV1 and QV3
QV = tf
tf+� • weight
QV5 : Combination of QV2 and QV3
QV = tf • weight
4 Experiments on term distillation
</equation>
<bodyText confidence="0.99985516">
Using the NTCIR-3 patent retrieval test collec-
tion, we conducted experiments to evaluate the
effect of term distillation.
For query construction, we used only news ar-
ticle fields in the 31 topics for the formal run.
The number of query terms selected by term
distillation was just eight in each topic. As
described in the section 2, retrieval was full-
automatically executed with pseudo-relevance
feedback.
The evaluation results for some combinations
of QV and TV are summarized in Table 1, where
the documents judged to be &amp;quot;A&amp;quot; were taken as
relevant ones. The combinations were selected
on the results in our preliminary experiments.
Each of &amp;quot;t&amp;quot;, &amp;quot;i&amp;quot;, &amp;quot;a&amp;quot; and &amp;quot;w&amp;quot; in the columns
&amp;quot;p&amp;quot; or &amp;quot;q&amp;quot; represents a certain method for esti-
mation of the probability p or q as follows :
t : estimate p by the probability that the term
occurs in titles of patents. More specifically
p = Nt , where nt is the number of patent
titles including the term and Np is the num-
ber of patents in the NTCIR-3 collection.
i : estimate q by the probability that the term
occurs in news articles. More specifically
</bodyText>
<equation confidence="0.70768">
ni
q = Ni , where ni is the number of articles
N
</equation>
<bodyText confidence="0.999766882352941">
including the term and Ni is the number of
news articles in the IREX collection (&apos;98-&apos;99
MAINICHI news article).
a : estimate p by the probability that the term
occurs in abstracts of patents. More specif-
ically p = naNp , where na is the number of
patent abstracts in which the term occurs.
w : estimate q by the probability that the term
occurs in the whole patent. More specif-
ically q = nwNp , where nw is the number
of patents in which the term occurs. We
tried to approximate the difference in term
statistics between patents and news articles
using the conbination of &amp;quot;a&amp;quot; and &amp;quot;w&amp;quot; in the
term distillation.
In Table 1, the combination of QV2 and TV0
corresponds to query term extraction without
</bodyText>
<note confidence="0.884070272727273">
QV TV p q AveP P@10
QV2 TV4 t i 0.1953 0.2645
QV2 TV9 t i 0.1948 0.2677
QV5 TV3 t i 0.1844 0.2355
QV2 TV3 t i 0.1843 0.2645
QV0 TV3 t i 0.1816 0.2452
QV2 TV6 t i 0.1730 0.2258
QV2 TV2 t i 0.1701 0.2194
QV2 TV3 a w 0.1694 0.2355
QV2 TV0 - - 0.1645 0.2226
QV2 TV7 t i 0.1597 0.2065
</note>
<tableCaption confidence="0.999753">
Table 1: Results using article field
</tableCaption>
<bodyText confidence="0.999714176470588">
term distillation. Comparing with the combina-
tion, retrieval performances are improved using
instances of TV except for TV7. This means the
term distillation produces a positive effect.
The best performance in the table is pro-
duced by the combination of QV2 (raw term
frequency) and TV4 (BIM).
While the combination of &amp;quot;a&amp;quot; and &amp;quot;w&amp;quot; for es-
timation of probabilities p and q has the virtue
in that the estimation requires only target docu-
ment collection, the performance is poor in com-
parison with the combination of &amp;quot;t&amp;quot; and &amp;quot;i&amp;quot;.
Although the instances of QV can be com-
pared each other by focusing on TV3, it is un-
clear whether QV5 is superior to QV2. We think
it is necessary to proceed to the evaluation in-
cluding the other combinations of TV and QV .
</bodyText>
<sectionHeader confidence="0.997562" genericHeader="method">
5 Results in NTCIR-3 patent task
</sectionHeader>
<bodyText confidence="0.999835">
We submitted four mandatory runs. The evalu-
ation results of our submitted runs are summa-
rized in Table 2, where the documents judged to
be &amp;quot;A&amp;quot; were taken as relevant ones.
These runs were automatically produced us-
ing both article and supplement fields, where
each supplement field includes a short descrip-
tion on the content of the news article. Term dis-
tillation using TV3 (Bayes classification model)
and query expansion by pseudo-relevance feed-
back were applied to all runs.
The retrieval performances are remarkable
among all submitted runs. However, the effect
</bodyText>
<table confidence="0.5816724">
QV TV p q AveP P@10
QV2 TV3 t i 0.2794 0.3903
QV0 TV3 t i 0.2701 0.3484
QV2 TV3 a w 0.2688 0.3645
QV5 TV3 t i 0.2637 0.3613
</table>
<tableCaption confidence="0.99962">
Table 2: Results in the NTCIR-3 patent task
</tableCaption>
<bodyText confidence="0.999904666666667">
of term distillation is somewhat unclear, com-
paring with the run with only supplement fields
in Table 3 (the average precision is 0.2712). We
think supplement fields supply enough terms so
that it is difficult to evaluate the performance of
cross-database retrieval in the mandatory runs.
</bodyText>
<sectionHeader confidence="0.999248" genericHeader="evaluation">
6 Results on ad-hoc patent retrieval
</sectionHeader>
<bodyText confidence="0.900033615384616">
In Table 3, we show evaluation results corre-
sponding to various combinations of topic fields
in use. The documents judged to be &amp;quot;A&amp;quot; were
taken as relevant ones.
fields AveP P@10 Rret
t,d,c 0.3262 0.4323 1197
t,d,c,n 0.3056 0.4258 1182
d 0.3039 0.4032 1133
t,d 0.2801 0.3581 1100
t,d,n 0.2753 0.4000 1140
d,n 0.2750 0.4323 1145
s 0.2712 0.3806 991
t 0.1283 0.1968 893
</bodyText>
<tableCaption confidence="0.995417">
Table 3: Results on ad-hoc patent retrieval
</tableCaption>
<bodyText confidence="0.999947">
In the table, the fields &amp;quot;t&amp;quot;, &amp;quot;d&amp;quot;, &amp;quot;c&amp;quot;, &amp;quot;n&amp;quot; or &amp;quot;s&amp;quot;
correspond to title, description, concept, nar-
rative or supplement respectively. As a result,
the combination of &amp;quot;t,d,c&amp;quot; produced the best re-
trieval performance for a set of the formal run
topics. Pseudo-relevance feedback had a posi-
tive effect except for the case using a title field
only.
</bodyText>
<sectionHeader confidence="0.997155" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999978538461538">
We proposed term distillation for cross-database
retrieval. Using NTCIR-3 test collection, we
evaluated this technique in patent retrieval and
found a positive effect. We think cross-database
retrieval can be applied to various settings in-
cluding personalized retrieval, similar document
retrieval and so on.
For the future work, we hope to apply term
distillation to cope with vocabulary gap prob-
lems in these new settings. In addition, we think
term distillation can be used to present query
terms to users in reasonable order in interactive
retrieval systems.
</bodyText>
<sectionHeader confidence="0.999239" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999408363636364">
M. Iwayama, A. Fujii, A. Takano, and N. Kando.
2001. Patent retrieval challenge in NTCIR-3.
IPSJ SIG Notes, 2001-FI-63:49-56.
Y. Ogawa and H. Mano. 2001. RICOH at NTCIR-
2. Proc. of NTCIR Workshop 2 Meeting, pages
121{123.
Y. Ogawa and T. Matsuda. 1999. An efficient doc-
ument retrieval method using n-gram indexing.
Trans. of IEICE, J82-D-I(1):121-129.
S. E. Robertson and K. Sparck-Jones. 1976. Rele-
vance weighting of search terms. Journal of ASIS,
27:129{146.
S. E Robertson and S. Walker. 1994. Some simple
effective approximations to the 2-poisson model
for probabilistic weighted retrieval. Proc. of 17th
ACM SIGIR Conf., pages 232{241.
S. E. Robertson and S. Walker. 1997. On relevance
weights with little relevance information. Proc. of
20th ACM SIGIR Conf., pages 16{24.
S. E. Robertson. 1990. On term selection for query
expansion. Journal of Documentation, 46(4):359-
364.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.938559">
<title confidence="0.998509">Term Distillation in Patent Retrieval</title>
<author confidence="0.997067">Hideo Itoh Hiroko Mano Yasushi</author>
<affiliation confidence="0.98522">Software R&amp;D Group, RICOH Co.,</affiliation>
<address confidence="0.976888">1-1-17 Koishikawa, Bunkyo-ku, Tokyo 112-0002,</address>
<abstract confidence="0.998229125">In cross-database retrieval, the domain of queries differs from that of the retrieval target in the distribution of term occurrences. This causes incorrect term weighting in the retrieval system which assigns to each term a retrieval weight based on the distribution of term occurrences. To resolve the problem, we propose &amp;quot;term distillation&amp;quot;, a framework for query term selection in cross-database retrieval. The experiments using the NTCIR-3 patent retrieval test collection demonstrate that term distillation is effective for cross-database retrieval.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Iwayama</author>
<author>A Fujii</author>
<author>A Takano</author>
<author>N Kando</author>
</authors>
<date>2001</date>
<booktitle>Patent retrieval challenge in NTCIR-3. IPSJ SIG Notes,</booktitle>
<pages>2001--63</pages>
<contexts>
<context position="1131" citStr="Iwayama et al., 2001" startWordPosition="171" endWordPosition="174"> problem, we propose &amp;quot;term distillation&amp;quot;, a framework for query term selection in cross-database retrieval. The experiments using the NTCIR-3 patent retrieval test collection demonstrate that term distillation is effective for cross-database retrieval. 1 Introduction For the mandatory runs of NTCIR-3 patent retrieval task 1, participants are required to construct a search query from a news article and retrieve patents which might be relevant to the query. This is a kind of cross-database retrieval in that the domain of queries (news article) differs from that of the retrieval target (patent) (Iwayama et al., 2001). Because in the distribution of term occurrences the query domain differs from the target domain, some query terms are given very large weights (importance) by the retrieval system even if the terms are not appropriate for http://research.nii.ac.jp/ntcir/ntcir-ws3/patent/ retrieval. For example, the query term &amp;quot;president&amp;quot; in a news article might not be effective for patent retrieval. However, the retrieval system gives the term a large weight, because the document frequency of the term in the patent genre is very low. We think these problematic terms are so many that the terms cannot be elimi</context>
</contexts>
<marker>Iwayama, Fujii, Takano, Kando, 2001</marker>
<rawString>M. Iwayama, A. Fujii, A. Takano, and N. Kando. 2001. Patent retrieval challenge in NTCIR-3. IPSJ SIG Notes, 2001-FI-63:49-56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Ogawa</author>
<author>H Mano</author>
</authors>
<date>2001</date>
<booktitle>RICOH at NTCIR2. Proc. of NTCIR Workshop 2 Meeting,</booktitle>
<pages>121--123</pages>
<contexts>
<context position="2302" citStr="Ogawa and Mano, 2001" startWordPosition="357" endWordPosition="360">c terms are so many that the terms cannot be eliminated using a stop word dictionary. In order to resolve the problem mentioned above, we propose &amp;quot;term distillation&amp;quot; which is a framework for query term selection in crossdatabase retrieval. The experiments using the NTCIR patent retrieval test collection demonstrate that term distillation is effective for crossdatabase retrieval. 2 System description Before describing our approach, we give a short description on our retrieval system. For the NTCIR-3 experiments, we revised query processing although the framework is the same as that of NTCIR-2 (Ogawa and Mano, 2001). The basic features of the system are as follows : • Effective document ranking with pseudorelevance feedback based on Okapi&apos;s approach (Robertson and Walker, 1997) with some improvements. • Scalable and efficient indexing and search based on the inverted file system (Ogawa and Matsuda, 1999) • Originally developed Japanese morphological analyzer and normalizer for document indexing and query processing. The inverted file was constructed for the retrieval target collection which contains full texts of two years&apos; Japanese patents. We adopted character n-gram indexing because it might be diffic</context>
</contexts>
<marker>Ogawa, Mano, 2001</marker>
<rawString>Y. Ogawa and H. Mano. 2001. RICOH at NTCIR2. Proc. of NTCIR Workshop 2 Meeting, pages 121{123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Ogawa</author>
<author>T Matsuda</author>
</authors>
<title>An efficient document retrieval method using n-gram indexing.</title>
<date>1999</date>
<journal>Trans. of IEICE,</journal>
<pages>82--1</pages>
<contexts>
<context position="2596" citStr="Ogawa and Matsuda, 1999" startWordPosition="404" endWordPosition="407">st collection demonstrate that term distillation is effective for crossdatabase retrieval. 2 System description Before describing our approach, we give a short description on our retrieval system. For the NTCIR-3 experiments, we revised query processing although the framework is the same as that of NTCIR-2 (Ogawa and Mano, 2001). The basic features of the system are as follows : • Effective document ranking with pseudorelevance feedback based on Okapi&apos;s approach (Robertson and Walker, 1997) with some improvements. • Scalable and efficient indexing and search based on the inverted file system (Ogawa and Matsuda, 1999) • Originally developed Japanese morphological analyzer and normalizer for document indexing and query processing. The inverted file was constructed for the retrieval target collection which contains full texts of two years&apos; Japanese patents. We adopted character n-gram indexing because it might be difficult for Japanese morphological analyzer to correctly recognize technical terms which are crucial for patent retrieval. In what follows, we describe the full automatic process of document retrieval in the NTCIR-3 patent retrieval task. 1. Query term extraction Input query string is transformed </context>
</contexts>
<marker>Ogawa, Matsuda, 1999</marker>
<rawString>Y. Ogawa and T. Matsuda. 1999. An efficient document retrieval method using n-gram indexing. Trans. of IEICE, J82-D-I(1):121-129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Robertson</author>
<author>K Sparck-Jones</author>
</authors>
<title>Relevance weighting of search terms.</title>
<date>1976</date>
<journal>Journal of ASIS,</journal>
<pages>27--129</pages>
<contexts>
<context position="7574" citStr="Robertson and Sparck-Jones, 1976" startWordPosition="1241" endWordPosition="1244">TV We give some instances of TV using two probabilities p and q, where p is a probability that the term occurs in the target domain and q is a probability that the term occurs in the query domain. Because the estimation method of p and q is independent on the instances of TV , it is explained later. We show each instance of TV with the id-tag as follows: TV0 : Zero model TV = constant = 1 TV1 : Swet model (Robertson, 1990) TV=p — q TV2 : Naive Bayes model TV � p q TV3 : Bayesian classification model TV a•p = a•p+(1-a-E)•q+E where a and e are unknown constants. TV4 : Binary independence model (Robertson and Sparck-Jones, 1976) TV =log p(1q) q(1p) TV5 : Target domain model TV = p TV6 : Query domain model TV = 1 — q TV7 : Binary model TV = 1 (p &gt; 0) or 0 (p = 0) TV8 : Joint probability model TV=p•(1—q) TV9 : Decision theoretic model (Robertson and Sparck-Jones, 1976) TV = log(p) — log(q) 3.3 Instances of QV We show each instance of QV with the id-tag as follows: QV0 : Zero model QV = constant = 1 QV1 : Approximated 2-poisson model (Robertson and Walker, 1994) QV = tf tf+13 where tf is the within-query term frequency and 0 is an unknown constant. QV2 : Term frequency model QV=tf QV3 : Term weight model QV = weight whe</context>
</contexts>
<marker>Robertson, Sparck-Jones, 1976</marker>
<rawString>S. E. Robertson and K. Sparck-Jones. 1976. Relevance weighting of search terms. Journal of ASIS, 27:129{146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Robertson</author>
<author>S Walker</author>
</authors>
<title>Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval.</title>
<date>1994</date>
<booktitle>Proc. of 17th ACM SIGIR Conf.,</booktitle>
<pages>232--241</pages>
<contexts>
<context position="8013" citStr="Robertson and Walker, 1994" startWordPosition="1331" endWordPosition="1334">ve Bayes model TV � p q TV3 : Bayesian classification model TV a•p = a•p+(1-a-E)•q+E where a and e are unknown constants. TV4 : Binary independence model (Robertson and Sparck-Jones, 1976) TV =log p(1q) q(1p) TV5 : Target domain model TV = p TV6 : Query domain model TV = 1 — q TV7 : Binary model TV = 1 (p &gt; 0) or 0 (p = 0) TV8 : Joint probability model TV=p•(1—q) TV9 : Decision theoretic model (Robertson and Sparck-Jones, 1976) TV = log(p) — log(q) 3.3 Instances of QV We show each instance of QV with the id-tag as follows: QV0 : Zero model QV = constant = 1 QV1 : Approximated 2-poisson model (Robertson and Walker, 1994) QV = tf tf+13 where tf is the within-query term frequency and 0 is an unknown constant. QV2 : Term frequency model QV=tf QV3 : Term weight model QV = weight where weight is the retrieval weight given by the retrieval system. QV4 : Combination of QV1 and QV3 QV = tf tf+� • weight QV5 : Combination of QV2 and QV3 QV = tf • weight 4 Experiments on term distillation Using the NTCIR-3 patent retrieval test collection, we conducted experiments to evaluate the effect of term distillation. For query construction, we used only news article fields in the 31 topics for the formal run. The number of quer</context>
</contexts>
<marker>Robertson, Walker, 1994</marker>
<rawString>S. E Robertson and S. Walker. 1994. Some simple effective approximations to the 2-poisson model for probabilistic weighted retrieval. Proc. of 17th ACM SIGIR Conf., pages 232{241.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Robertson</author>
<author>S Walker</author>
</authors>
<title>On relevance weights with little relevance information.</title>
<date>1997</date>
<booktitle>Proc. of 20th ACM SIGIR Conf.,</booktitle>
<pages>16--24</pages>
<contexts>
<context position="2467" citStr="Robertson and Walker, 1997" startWordPosition="384" endWordPosition="387">ation&amp;quot; which is a framework for query term selection in crossdatabase retrieval. The experiments using the NTCIR patent retrieval test collection demonstrate that term distillation is effective for crossdatabase retrieval. 2 System description Before describing our approach, we give a short description on our retrieval system. For the NTCIR-3 experiments, we revised query processing although the framework is the same as that of NTCIR-2 (Ogawa and Mano, 2001). The basic features of the system are as follows : • Effective document ranking with pseudorelevance feedback based on Okapi&apos;s approach (Robertson and Walker, 1997) with some improvements. • Scalable and efficient indexing and search based on the inverted file system (Ogawa and Matsuda, 1999) • Originally developed Japanese morphological analyzer and normalizer for document indexing and query processing. The inverted file was constructed for the retrieval target collection which contains full texts of two years&apos; Japanese patents. We adopted character n-gram indexing because it might be difficult for Japanese morphological analyzer to correctly recognize technical terms which are crucial for patent retrieval. In what follows, we describe the full automati</context>
</contexts>
<marker>Robertson, Walker, 1997</marker>
<rawString>S. E. Robertson and S. Walker. 1997. On relevance weights with little relevance information. Proc. of 20th ACM SIGIR Conf., pages 16{24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Robertson</author>
</authors>
<title>On term selection for query expansion.</title>
<date>1990</date>
<journal>Journal of Documentation,</journal>
<pages>46--4</pages>
<contexts>
<context position="4701" citStr="Robertson, 1990" startWordPosition="748" endWordPosition="749">ry expansion. The maximum number of seed documents is ten. 4. Query expansion Candidates of expansion terms are extracted from the seed documents by pattern matching as in the query term extraction mentioned above. Phrasal terms are not used for query expansion because phrasal terms may be less effective to improve recall and risky in case of pseudo-relevance feedback. The weight of initial query term is re-calculated with the Robertson/SparkJones formula (Robertson and SparckJones, 1976) if the term is found in the candidate pool. The candidates are ranked on the Robertson&apos;s Selection Value (Robertson, 1990) and top-ranked terms are selected as expansion terms. 5. Final retrieval Each query and expansion term is submitted one by one to the ranking search module as in the initial retrieval. 3 Term distillation In cross-database retrieval, the domain of queries (news article) differs from that of the retrieval target (patent) in the distribution of term occurrences. This causes incorrect term weighting in the retrieval system which assigns to each term a retrieval weight based on the distribution of term occurrences. Moreover, the terms which might be given an incorrect weight are too many to be co</context>
<context position="6245" citStr="Robertson, 1990" startWordPosition="1000" endWordPosition="1001">tion of query term candidates Candidates of query terms are extracted from the query string (news articles) and pooled. 2. Assignment of TDV (Term Distillation Value) Each candidate in the pool is given a TDV which represents &amp;quot;goodness&amp;quot; of the term to retrieve documents in the target domain. 3. Selection of query terms The candidates are ranked on the TDV and top-ranked n terms are selected as query terms, where n is an unknown constant and treated as a tuning parameter for fullautomatic retrieval. The term distillation seems appropriate to avoid falling foul of the &amp;quot;curse of dimensionality&amp;quot; (Robertson, 1990) in case that a given query is very lengthy. In what follows in this section, we explain a generic model to define the TDV. Thereafter some instances of the model which embody the term distillation are introduced. 3.1 Generic Model In order to define the TDV, we give a generic model with the following formula. TDV = QV • TV where QV and TV represent the importance of the term in the query and the target domain respectively. QV seems to be commonly used for query term extraction in ordinary retrieval systems, however, TV is newly introduced for cross-database retrieval. A combination of QV and </context>
</contexts>
<marker>Robertson, 1990</marker>
<rawString>S. E. Robertson. 1990. On term selection for query expansion. Journal of Documentation, 46(4):359-364.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>