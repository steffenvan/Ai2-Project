<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000139">
<title confidence="0.998554">
An algorithm for efficiently generating summary paragraphs using
tree-adjoining grammar
</title>
<author confidence="0.997577">
Bruce Eddy, Diana Bental and Alison Cawsey
</author>
<affiliation confidence="0.847014333333333">
Department of Computing and Electrical Engineering
Heriot-Watt University
Edinburgh
</affiliation>
<address confidence="0.8330985">
EH14 4AS
UK
</address>
<email confidence="0.994217">
ceebde1,diana,alison @cee.hw.ac.uk
</email>
<sectionHeader confidence="0.995511" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999892533333333">
We present an algorithm which im-
proves the efficiency of a search for the
optimally aggregated paragraph which
summarises a flat structured input spec-
ification. We model the space of possi-
ble paraphrases of possible paragraphs
as the space of sequences of composi-
tions of a set of tree-adjoining grammar
(TAG) elementary trees. Our algorithm
transforms this to a set with equivalent
paraphrasing power but better compu-
tational properties. Also, it identifies an
explicit mapping between input propo-
sitions and their possible surface reali-
sations.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995608081967214">
Summarisation of simply structured data as short
natural language paragraphs has recently been
a focus of interest. Shaw (1998) and Bental
et al. (1999) looked at generating text from
database records. Robin and McKeown (1996)
summarised quantitative data. Shaw’s examples
were drawn from patient medical records; Bental
et al’s from online resource cataloging informa-
tion. A requirement common to all these studies
has been to produce aggregated (Reape and Mel-
lish, 1999) text.
Also in all these studies, the structure of the
input data used was fairly flat. In particular, in
(Shaw, 1998) and (Bental et al., 1999) each record
This work is supported by EPSRC grant GR/M23106.
is associated with a particular entity (e.g. a patient
or an online resource) and is essentially a list of
attribute-value pairs. We refer to pairs as fields,
and to attributes as field names. The relationship
between a value and the entity with which it is
associated is specified by the field name. Most
field names represent “is a” or “has a” relation-
ships and hence most values represent facts about
the entity. Slightly more complex structure may
also be coerced into this form, but we will focus
on this simple case.
For our application (summarising data about
educational resources), we additionally assume
that we are required to be able to summarise any
subset of fields from a given record, and that our
summary must include every member of that sub-
set. The challenge from this sort of summarisa-
tion is to devise a system which satisfies two po-
tentially incompatible constraints. First, it must
be flexible enough to model, for any combina-
tion of fields, the optimally aggregated paragraph
which expresses them. Second, despite the very
large search space that such flexibility probably
implies, it must be capable of finding that para-
graph in a reasonable time.
The contribution the present work makes is a
set of algorithms which prune this search space.
This space is specified in terms of compositions
of elementary trees of a tree-adjoining grammar
(TAG) (Joshi, 1986). The first transforms a TAG
into a lexicalised version of itself which has bet-
ter computational properties with respect to sum-
marising a record. The second removes those
parts of a TAG which are redundant with respect
to summarising a particular record. The third
identifies an explicit mapping from each field to
its possible surface realisations, and hence allows
a desirable surface form to be chosen for each
field. Our partial implementation of these algo-
rithms has produced some promising results.
The rest of the paper is organised as follows. In
section 2 we discuss the problem, our approach
to modelling it, and characteristics of the search
space implied by our approach. In section 3 we
present our algorithms for searching this space.
In section 4 we summarise and discuss.
</bodyText>
<sectionHeader confidence="0.994665" genericHeader="method">
2 Searching for concise, coherent
paragraphs
</sectionHeader>
<subsectionHeader confidence="0.875423">
2.1 Aggregation is a global optimisation
problem
</subsectionHeader>
<bodyText confidence="0.999394940476191">
Our aim is to generate paragraphs which are well
aggregated. This notion should be defined in
terms of conciseness and coherence, which terms
are not formally definable. However, some rea-
sonable approximation to them can be achieved
by specifying preferences for certain types of syn-
tactic constructions over others (Robin and McK-
eown, 1996), possibly by giving each generat-
able construction a score which reflects its rela-
tive preferability. We then define the best aggre-
gated paragraph to be that which achieves the best
sum of its constituent constructions’ preference
scores.
Robin and McKeown’s (1996) system,
STREAK, generates aggregated, fact-rich sen-
tences. It adds facts in order of the preferability
of their best possible realisation. It revises its
syntactic choices every time an extra fact is
aggregated. This is computationally expensive,
and makes multi-sentence generation by the same
means prohibitively slow. They do suggest how
to deal with this when many of the facts occur
in fixed positions, but this is not the case in our
corpus.
CASPER (Shaw, 1998) delays syntactic
choices until after it has decided where sentence
boundaries should fall. It thereby gains computa-
tional efficiency at the cost of its sentences being
less optimal aggregations.
Our algorithms are an attempt to avoid these
problems and to achieve greater efficiency by pre-
compiling detailed syntactic decisions, the results
of which we store in the form of explicit map-
pings from fields to surface forms. At generation
time, we search for the optimal selection of reali-
sations. This approach deviates from the pipeline
architecture of NLG systems which, it has been
observed, is not wholly suited to the generation
of aggregated texts.
The first author to discuss this was Meteer
(1992), who showed that the microplanning stage
of the pipeline is constrained by surface realisa-
tion in two ways. First, a microplan must be real-
isable in the target language; second, a realisable
microplan must make best use of the capacities
the target language for concise expression.
More generally, in order to generate aggregated
text, constraints imposed by and opportunities af-
forded by the surface form may be taken into ac-
count at any stage in the pipeline. Reape and Mel-
lish (1999) provided examples of different sys-
tems each of which takes aggregation decisions
at a different stage. It may not be easy to deter-
mine what effect a decision taken at an early stage
will have at the surface; and decisions taken at one
stage may preclude at a later stage a choice which
results in a more aggregated surface form. Simi-
larly, it may not be easy to make a decision at an
early stage which makes best use of the surface
possibilities.
Consider the examples of figures 1 and 2. Both
summarise the same set of fields; figure 2 sum-
marises additionally the field “subject = science”.
Both paragraphs summarise their fields in the
most concise and coherent manner possible (al-
though this is, of course, a subjective judgement).
Note that they treat the fields they have in com-
mon differently with respect to ordering and dis-
tribution between the sentences.
Various types of constraints cause this. Syn-
tactic constraints include: “science” may be used
as an adjective to pre-modify “lesson plan”. Se-
mantic constraints include: “Constellations” lasts
4 hours, but ProLog does not. Stylistic constraints
include: ‘Maureen Ryff wrote ...’ is preferable to
‘... was written by Maureen Ryff’. We suggest, as
do Stone and Doran (1997), that integrating these
constraints simultaneously is more efficient then
pipelining them. We additionally suggest that rep-
resenting these constraints in a unified form can
provide further efficiency gains.
“Constellations” is a 4-hour lesson plan pub-
lished by online provider ProLog. Maureen
Ryff wrote it for small group teaching.
</bodyText>
<figureCaption confidence="0.927643">
Figure 1: A paragraph which summarises the set
of fields of figure 3 in an aggregated manner.
</figureCaption>
<bodyText confidence="0.762149666666667">
“Constellations” is a science lesson plan
which lasts 4 hours. Maureen Ryff wrote
it for small group teaching and ProLog, an
online provider, is its publisher.
Figure 2: A well aggregated paragraph which
summarises the set of fields of figure 1, together
with field subject = “science”. Notice the non-
linear effect the addition of a single extra propo-
sition can have on the structure of the paragraph.
</bodyText>
<subsectionHeader confidence="0.998934">
2.2 Modeling paragraphs with TAG
</subsectionHeader>
<bodyText confidence="0.97577275">
Our approach uses, as its primary representation,
TAG formalism extended to include unification
based feature structures (Vijay-Shanker and Joshi,
1991). Joshi (1986) describes the advantages
TAG possesses as a syntactic formalism for NLG.
In generation, the TAG model has usually been
applied to generating clauses and sentences. Re-
cently, Webber et al. (1999) outlined the benefits
of modelling longer strings of text by the same
means.
The most important characteristic of TAG for
our purposes is the local definability of dependen-
cies: constraints between the nodes of elemen-
tary trees are preserved under adjoinings which
increase the distances between them. For exam-
ple, in the sentence fragment “Springer published
...”, which might be modelled by a single initial
tree, the object is constrained to be some entity
published by Springer. If an adjunction is made so
that the fragment becomes “Springer, the largest
publishing company in Europe, published ...”, this
constraint is undisturbed.
Our approach presupposes the existence of a
TAG whose string set is exactly those paragraphs
which are comprehensible summaries of subsets
of fields. We do not discuss the creation of such
a TAG here. We have made progress with design-
ing one; we believe that it is the flatness of the
input data which makes it possible.
Let us restate the problem somewhat more for-
mally. Suppose that we have a set of fields
whose values we may be required to express.
Suppose that for every there is a template
which expresses . A template is a paragraph in
which certain words are replaced by slots. A slot
is a reference to a field-name. A template ex-
presses a set of fields if the name of every ele-
ment of is referenced by a slot, and every slot
refers to an element of . We say that expresses
and that the resulting paragraph is the expres-
sion of with respect to . See figure 3.
Let denote the template which “best” ex-
presses some . Suppose also that we
have a TAG with string set such that
. The creation of is
not discussed here. Every string in is the
yield of , some derived tree of .
Each of a TAG’s derived trees is represented
by a unique derivation tree. Typically a deriva-
tion tree represents several (differently ordered)
sequences of compositions of (the same set of)
elementary trees, all of which result in the same
derived tree.
Hence, a derived tree of a TAG with el-
ementary trees is the result of some sequence
of compositions of the elements of which is
equivalent to some derivation tree . We write
or just , Hence, our problem
is, given and some , find some such
that .
There are two parts to the problem of finding
. First we must recognise , which we
may do, as described in section 2.1, by defining
to be the paragraph which achieves the best
sum of its constituent constructions’ preference
scores. Second, since the search space of deriva-
tion trees grows exponentially with the number of
trees in the grammar, we must find its derivation
in a reasonable amount of time.
For each field to be expressed, the slot which
refers to it may be expressed by means of one of
several different syntactic constructions. So each
slot will contribute one of several possible pref-
erence scores to the paragraph in which it occurs,
depending on the syntactic form in which it is re-
alised. However, the syntactic forms by which a
</bodyText>
<figure confidence="0.984872294117647">
slot . slot wrote it for slot .
slot
is a slot slot published by slot
Template Fields
Title &amp;quot;Constellations&amp;quot;
Field name Value
Type
Duration 4 hours
Pedagogy.TeachingMethods
Creator.PersonalName
Publisher.Name
Publisher.Role
Lesson plan
small group teaching
Maureen Ryff
ProLog
online provider
</figure>
<figureCaption confidence="0.9972795">
Figure 3: A template which expresses a set of fields. The curves indicate the field name to which each
slot refers. The fields’ expression with respect to the template is the paragraph in figure 1.
</figureCaption>
<bodyText confidence="0.985817">
slot may be expressed are an implicit property of
the grammar: it requires search to discover what
they are, and then further search to find their op-
timal configuration.
</bodyText>
<subsectionHeader confidence="0.990914">
2.3 The search space
</subsectionHeader>
<bodyText confidence="0.999944826086956">
We model possible paraphrases with a TAG, the
paraphrases being the elements of its string set.
The nodes in the search space are TAG derivation
trees, and an arc from to represents the com-
position into the partial derivation corresponding
to , of an elementary tree, with result the par-
tial derivation corresponding to . The size of
the search space may be reduced by collapsing
certain paths in it, and by pruning certain arcs.
These operations correspond to specific lexicali-
sation and the removal of redundant trees from
the grammar respectively.
A tree in the grammar is redundant if it cannot
contribute to a paragraph which expresses the re-
quired fields; or if it cannot contribute to a ‘best’
paragraph which expresses those fields. We will
expand on redundancy removal after describing
the specific lexicalisation algorithm. The spe-
cific lexicalisation algorithm converts a TAG into
a specific-lexicalised version of itself, in which
certain paths in the search space, which it can
be known will be used in any derivation, are col-
lapsed.
</bodyText>
<sectionHeader confidence="0.997774" genericHeader="method">
3 The algorithms
</sectionHeader>
<subsectionHeader confidence="0.7927885">
3.1 Specific lexicalisation: creating a clausal
lexicon
</subsectionHeader>
<bodyText confidence="0.99845128">
We begin by introducing some notation, and
defining some properties of TAGs and their ele-
mentary trees. Let denote , a TAG, and
, some set of elementary trees, not necessarily
in . A leaf node of an elementary tree may be
labelled as a slot, which is a special case of the
lexical anchor.
An elementary tree is specific-lexicalised if at
least one1 of its leaf nodes is a slot. An elemen-
tary tree is -lexicalised if it is specific-lexicalised
or if it has no substitution nodes or foot nodes.
A TAG is specific-lexicalised if all its elementary
trees are -lexicalised.2
Given some , let be an element of
which is not specific-lexicalised. Let
be an elementary tree of . Suppose that there is
some composition of into and that the resulting
tree is specific-lexicalised. Then we say that is
single-step-lexicalisable in . We call any
such resulting tree a single-step-lexicalisation at
of in , where is the node at which the
composition occurred.
We now present the algorithm for
our transformation Specific Lexicalisa-
tion.
</bodyText>
<listItem confidence="0.9583628">
1: is some TAG, .
2: repeat
3: Add to .
4: for all do
5: if is sinlge-step lexicalisable in
</listItem>
<footnote confidence="0.994663125">
1This is an important parameter. It specifies how many
slots each elementary tree in the transformed TAG may have
(and consequently how many times the “derivation tree” of
each of these trees branches).
2This definition is compatible with that used in the liter-
ature. A TAG is lexicalised (Joshi and Schabes, 1991) if it is
specific-lexicalised according to this definition. The impli-
cation does not necessarily hold in reverse.
</footnote>
<listItem confidence="0.932764333333333">
then
6: if then
7: Remove from .
8: Add to .
9: end if
10: For some node of , ,add all the
single-step-lexicalisations at of in
, to .3
11: end if
12: end for
13: until
14: is a specific-lexicalisation4 of .
</listItem>
<bodyText confidence="0.999910230769231">
To illustrate this procedure, we have provided
some figures. Consider the TAG , whose el-
ementary trees are shown in figure 4. We have
chosen, for reasons of space and simplicity, not to
show the feature structures attached to each node
of these trees. Their approximate form can per-
haps be deduced by examination of the templates
modelled by , shown in figure 6. A specific lex-
icalised version of the TAG, is shown in figure
5. We have named each elementary tree in by
concatenating the names of its constituents from
. The templates generated by (and hence )
are shown in figure 6.
</bodyText>
<subsectionHeader confidence="0.999162">
3.2 Redundancy removal
</subsectionHeader>
<bodyText confidence="0.869215785714286">
We can further remove redundancy in a specific-
lexicalisation, , of some TAG. Let be
a pair as in the previous section. The following
three subsets of the elementary trees of are re-
dundant. First, those trees which are not
rooted on the distinguished symbol and for which
there is no such that can be com-
posed into . Second, those which have a
substitution node into which no can be
substituted. Third, those such that for each
tree which is the result of the composition of
some into , . Our program
3Note that there is a choice at this step. Our implemen-
tation of this algorithm chooses such that the number of
single-step-lexicalisations at is maximised. But different
choices result in a transformed grammar with different prop-
erties.
4We claim that a specific-lexicalisation of a TAG is in-
deed specific-lexicalised. Note that there does not neces-
sarily exist a specific-lexicalisation of a TAG. For certain
pathological examples of TAGs, the algorithm does not ter-
minate. Note also that if a specific-lexicalisation exists, it is
not necessarily unique. Further work is required to discover
the properties of the various specific-lexicalisations in these
cases.
which implements the algorithm in fact removes
these redundancies, not only after completion, but
also after every iteration.
</bodyText>
<subsectionHeader confidence="0.784091">
3.3 Finding the (approximately) global
optimum
</subsectionHeader>
<bodyText confidence="0.997736463414634">
Specific-lexicalisation causes the (previously im-
plicit) grammatical constructions by which an el-
ement of may be expressed to become explicit
properties of the transformed grammar. Specifi-
cally, each element of occurs as the anchor of
each of a number of elementary trees. Let us refer
to the set of elementary trees in the transformed
grammar anchored by as .5 Each of
these trees corresponds to a grammatical form in
which the element may be realised. Hence, rather
than performing an exhaustive search of the space
of derivation trees , specific-lexicalisation
allows us to instead perform a bestfirst search.
That is, we choose exactly one element of
for each . Let denote the set of
all sets which contain exactly one element of
for each . Recall that we may assign to each
syntactic form in which an element of may be
realised a preference score, and that each element
of corresponds to some syntactic form. So,
for each element of we may sum the
preference scores of its elements. Hence, we may
impose an order on the elements of
according to their sum of preference scores. We
may then refer to each element of as
, where is the element’s position in
the order, with being first.
We then search, in order, the spaces of possible
compositions of the s combined with
some necessary supporting trees which are not an-
chored by an element of . Call these spaces
. In terms of redundancy removal,
is the specific-lexicalised TAG with
those trees which might be redundant with respect
to the search for removed. We begin the
search with . It is not guaranteed that
is in this space. If it is not, we repeat the
search using , and so on. At worst
(if where has
elements), this procedure exhaustively searches
the space of compositions of the elements of .
</bodyText>
<footnote confidence="0.9667415">
5This is the family (as that term is used by Yang et al.
(1991) and others) of trees anchored on .
</footnote>
<figureCaption confidence="0.964359857142857">
Figure 4: The elementary trees of a small TAG, . The trees’ names are in bold below them. Substi-
tution nodes are indicated with a ‘+’; foot nodes are indicated with a ‘*’; the distinguished symbol is
‘s’. Slots are shown as ‘@&lt;reference&gt;’, where “reference” is the field to which the slot refers. Note that
the feature structures which are associated with each node, which prohibit certain compositions, are not
shown. Note also that this is not a lexicalised TAG (LTAG). This is somewhat unusual; we intend, as
part of our ongoing work, to apply our techniques to an established LTAG, such as XTAG.
Figure 5: The elementary trees of the TAG , a specific-lexicalised version of TAG in figure 4.
</figureCaption>
<bodyText confidence="0.945694">
Each tree’s name is below it, in bold. Note that, since the feature structures are not shown, it is not
apparent why certain trees which the algorithm seems to imply do not occur in this set.
</bodyText>
<figure confidence="0.982925367647059">
predication participleclause
predication* paticipleclause+ participle_p+ item+
adj_pred_partp participleclause
item item item item item
@&lt;title&gt; @&lt;type&gt; @&lt;publisher.role&gt; @&lt;publisher.name&gt; @&lt;duration&gt;
item_title item_type item_role item_name item_duration
s
item+ verb+ predication+
s
adjective
verb
is
is
item+
adjective
participle_p
published by
publishedby
item
adjective+ item*
item_adjective_adjoin
predication
a item+
predication_a
predication
participleclause+
predication_part
item_adjective_adjoin:adjective:item_title
@&lt;publisher.name&gt;
adj_pred_partp:participleclause:item_name
@&lt;publisher.name&gt;
predication_part:participleclause:item_name
verb
is
is
item
adjective item*
item
@&lt;title&gt;
s
item verb+ predication+
@&lt;title&gt;
s:item_title
predication
predication*
participleclause
participle_p+ item
item
predication
participleclause
participle_p+
participle_p
published by
publishedby
predication
a item
item
adjective item*
item
@&lt;duration&gt;
item_adjective_adjoin:adjective:item_duration
item_adjective_adjoin:adjective:item_role
item
adjective item*
item
@&lt;publisher.role&gt;
@&lt;type&gt;
predication_a:item_type
</figure>
<equation confidence="0.927883625">
1: @&lt;title&gt; is a @&lt;type&gt;.
2: @&lt;title&gt; is a @&lt;type&gt; published by @&lt;publisher.name&gt;.
3: @&lt;title&gt; is a @&lt;type&gt; published by @&lt;publisher.role&gt; @&lt;publisher.name&gt;.
4: @&lt;title&gt; is a @&lt;duration&gt; @&lt;type&gt;.
5: @&lt;title&gt; is a @&lt;duration&gt; @&lt;type&gt; published by @&lt;publisher.name&gt;.
6: @&lt;title&gt; is a @&lt;duration&gt; @&lt;type&gt; published by @&lt;publisher.role&gt; @&lt;publisher.name&gt;.
7: @&lt;title&gt; is published by @&lt;publisher.name&gt;.
8: @&lt;title&gt; is published by @&lt;publisher.role&gt; @&lt;publisher.name&gt;.
</equation>
<figureCaption confidence="0.9679055">
Figure 6: The templates modelled by the TAGs of figures 4 and 5. Note that the expression of the fields
in figure 3 with respect to template 6 is the first sentence of the paragraph of figure 1.
</figureCaption>
<bodyText confidence="0.999615923076923">
In fact, since the s do not partition
, in the worst cases this procedure is slower
than an exhaustive search. However, is de-
fined in terms of maximal preference scores, so it
is likely to be found in for “low” .
For illustration, refer again to the specific-
lexicalisation in figure 5. Notice that @&lt;pub-
lisher.name&gt; occurs as the anchor of more
than one tree.6 These trees, predication part:-
participleclause:item name and adj pred partp:-
participleclause:item name which we will refer to
as and respectively, represent the forms in
which that slot maybe expressed. Hence, @&lt;pub-
lisher.name&gt; may be realised as a predication in
its own right using , as in templates 7 and 8
in figure 6, or as an adjunct to another predi-
cation using the second, as in templates 2, 3, 5
and 6. Suppose that our preference scores rate
more highly than , and that we must include all
four slots. Then the system would first search the
space of compositions of the trees of without
, and generate template 6. The second choice,
without leads to the generation of the con-
catenation of templates 4 and 8, which expresses
the same fields but is less aggregated. This is as
we would wish.
</bodyText>
<subsectionHeader confidence="0.802709">
3.4 Redundancies in the search space
</subsectionHeader>
<bodyText confidence="0.896893666666667">
Specific-lexicalisation is a transformation which
operates on a complete TAG and its result is
another TAG whose string set is the same as
’s. Also, the feature structures on the nodes
of the elementary trees of contain fewer un-
bound variables. Unbound variables represent
</bodyText>
<footnote confidence="0.645459">
6We are ignoring the tree item adjective adjoin:-
adjective:item title, which is not usable due to its features,
which are not shown.
</footnote>
<bodyText confidence="0.999887681159421">
dependencies between parts of the grammar. A
search of the space of compositions of elemen-
tary trees may make a long chain of compositions
before discovering that the composed structure is
forbidden by such a dependency. The forbidden
chain of compositions is redundant, and specific-
lexicalisation removes it from the search space.
Importantly, specific-lexicalisation may also
take as a parameter , the set of fields to be ex-
pressed. It then removes from all elementary
trees which are anchored on slots which do not
refer to elements of and operates on this re-
duced TAG, with result . And if
then . Then, in effect, specific-
lexicalisation, as well as removing general redun-
dant dependencies, is specifically removing some
of those parts of the grammar which are redun-
dant with respect to the search for .
Redundancy occurs in a grammar for two rea-
sons. First, it is written, by hand, with linguis-
tic rather than computational efficiency concerns
in mind. It is too complex for its writer to be
able to spot redundancies arising from long chains
of dependencies between its parts. So specific-
lexicalisation may be regarded as automatic bug
removal. Second, the grammar is written to be
able to model all the templates which express
some . So for any particular , the gram-
mar will contain information about how to ex-
press items not in that set. Specific-lexicalisation
highlights this redundancy.
We have conducted some preliminary exper-
iments using several small TAGs in which, for
each TAG and for its specific-lexicalised equiv-
alent, we measured the time our system takes
to generate the modelled sentences. The results
showed a decrease in the generation time after
lexicalisation of orders of magnitude, with the
best observed reduction being a factor of about
3000.
The specific-lexicalisation of a TAG has the
property of having the same string set (and possi-
bly the same tree set) as the original, but a smaller
space of possible compositions. We have not
proved either clause of this statement, but on the
basis of experimental evidence we believe both to
be true. Also, the following argument supports
the case for the second.
Recall that a feature structure attached to a non-
terminal symbol in some rule (tree in the case of
TAG) of a grammar is an abbreviation for several
similar rules. For example, if a node has asso-
ciated with it a feature structure containing three
features each of which may be in one of two states
and none of which are currently instantiated, then
it abbreviates nodes. So each tree in a
TAG with feature structures is an abbreviation for
trees, where is the number of possible con-
figurations of the feature structures on its nodes.
Hence, when we search the space of possible
compositions of some number of trees, we are
in fact searching the space of compositions of
trees, where is some factor related to the number
of possible configurations of the feature structures
on the trees. Specific-lexicalisation identifies ex-
actly which of the (non-featured) trees for which
a tree with feature structures is an abbreviation
are irrelevant to a search by instantiating unbound
variables in its features.
</bodyText>
<sectionHeader confidence="0.999295" genericHeader="conclusions">
4 Further work and discussion
</sectionHeader>
<bodyText confidence="0.999987785714286">
The precise circumstances under which the tech-
niques described are effective are still to be estab-
lished. In particular, it is our intention to repeat
our experiments with a standard LTAG; and with
TAGs induced automatically from our corpus.
To summarise, we claim that the generation of
an optimally aggregated summary paragraph re-
quires the ability to move facts across sentence
boundaries. A difficulty to achieving this is the
exponential relationship between the number of
possible paraphrases of a summary of a set of
facts and the number of facts in that set. Our al-
gorithm addresses this by transforming a TAG to
better model the search space.
</bodyText>
<sectionHeader confidence="0.996467" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99990708">
Diana S. Bental, Alison Cawsey, Sheila Rock, and
Patrick McAndrew. 1999. The need for natural
language generation techniques to produce resource
descriptions in mirador. In Searching for infor-
mation: artificial intelligence and information re-
trieval approaches, pages 15/1–15/3. Institution of
Electrical Engineers, London.
Aravind K. Joshi and Yves Schabes. 1991. Tree-
adjoining grammars and lexicalized grammars. In
Maurice Nivat and Andreas Podelski, editors, De-
finability and Recognizability of Sets of Trees. Else-
vier.
Aravind K. Joshi. 1986. The relevance of tree adjoin-
ing grammar to generation. In Natural Language
Generation: New results in Artificial Intelligence,
Psychology and Linguistics - NATO Advanced Re-
search Workshop, pages 233–252, Nijmegen, The
Netherlands.
Marie Meteer. 1992. Expressibility and the Problem
of Efficient Text Planning. Pinter, London.
Mike Reape and Chris Mellish. 1999. Just what is
aggregation anyway? In European Workshop on
Natural Language Generation, Toulouse, May 13–
14.
Jacques Robin and Kathleen McKeown. 1996. Em-
pirically designing and evaluating a new revision-
based model for summary generation. Artificial In-
telligence, 85(1-2), August.
James Shaw. 1998. Clause aggregation using linguis-
tic knowledge. In Proceedings of the 9th Interna-
tional Workshop on Natural Language Generation,
pages 138–147.
Mathew Stone and Christine Doran. 1997. Sentence
planning as description using tree-adjoining gram-
mar. In Proceedings od the Assosciation for Com-
putational Linguistics, pages 198–205.
K. Vijay-Shanker and Aravind K. Joshi. 1991.
Unification based tree adjoining grammars. In
J. Wedekind, editor, Unification-based Grammars.
MIT Press, Cambridge, Massachusetts.
Bonnie Webber, Aravind K. Joshi, Alistair Knott, and
Matthew Stone. 1999. What are little texts made
of? a structural presuppositional account using lex-
icalised tag. In Proceedings of International Work-
shop on Levels of Representation in Discourse, Ed-
inburgh, July. LOIRD’99.
G. Yang, K. F. McCoy, and K. Vijay-Shanker. 1991.
From functional specification to syntactic struc-
tures: functional grammar and tree-adjoining gram-
mar. Computational Intelligence, 7(4):207–219.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.231827">
<title confidence="0.9758465">An algorithm for efficiently generating summary paragraphs tree-adjoining grammar</title>
<author confidence="0.991031">Diana Bental Eddy</author>
<affiliation confidence="0.683095">Department of Computing and Electrical Heriot-Watt</affiliation>
<address confidence="0.356516">EH14</address>
<email confidence="0.457749">ceebde1,diana,alison@cee.hw.ac.uk</email>
<abstract confidence="0.9955475">We present an algorithm which improves the efficiency of a search for the optimally aggregated paragraph which summarises a flat structured input specification. We model the space of possible paraphrases of possible paragraphs as the space of sequences of compositions of a set of tree-adjoining grammar (TAG) elementary trees. Our algorithm transforms this to a set with equivalent paraphrasing power but better computational properties. Also, it identifies an explicit mapping between input propositions and their possible surface realisations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Diana S Bental</author>
<author>Alison Cawsey</author>
<author>Sheila Rock</author>
<author>Patrick McAndrew</author>
</authors>
<title>The need for natural language generation techniques to produce resource descriptions in mirador. In Searching for information: artificial intelligence and information retrieval approaches,</title>
<date>1999</date>
<booktitle>Institution of Electrical Engineers,</booktitle>
<pages>15--1</pages>
<location>London.</location>
<contexts>
<context position="985" citStr="Bental et al. (1999)" startWordPosition="139" endWordPosition="142"> paragraph which summarises a flat structured input specification. We model the space of possible paraphrases of possible paragraphs as the space of sequences of compositions of a set of tree-adjoining grammar (TAG) elementary trees. Our algorithm transforms this to a set with equivalent paraphrasing power but better computational properties. Also, it identifies an explicit mapping between input propositions and their possible surface realisations. 1 Introduction Summarisation of simply structured data as short natural language paragraphs has recently been a focus of interest. Shaw (1998) and Bental et al. (1999) looked at generating text from database records. Robin and McKeown (1996) summarised quantitative data. Shaw’s examples were drawn from patient medical records; Bental et al’s from online resource cataloging information. A requirement common to all these studies has been to produce aggregated (Reape and Mellish, 1999) text. Also in all these studies, the structure of the input data used was fairly flat. In particular, in (Shaw, 1998) and (Bental et al., 1999) each record This work is supported by EPSRC grant GR/M23106. is associated with a particular entity (e.g. a patient or an online resour</context>
</contexts>
<marker>Bental, Cawsey, Rock, McAndrew, 1999</marker>
<rawString>Diana S. Bental, Alison Cawsey, Sheila Rock, and Patrick McAndrew. 1999. The need for natural language generation techniques to produce resource descriptions in mirador. In Searching for information: artificial intelligence and information retrieval approaches, pages 15/1–15/3. Institution of Electrical Engineers, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
<author>Yves Schabes</author>
</authors>
<title>Treeadjoining grammars and lexicalized grammars.</title>
<date>1991</date>
<booktitle>In Maurice Nivat and Andreas Podelski, editors, Definability and Recognizability of Sets of Trees.</booktitle>
<publisher>Elsevier.</publisher>
<contexts>
<context position="14697" citStr="Joshi and Schabes, 1991" startWordPosition="2433" endWordPosition="2436">icalisable in . We call any such resulting tree a single-step-lexicalisation at of in , where is the node at which the composition occurred. We now present the algorithm for our transformation Specific Lexicalisation. 1: is some TAG, . 2: repeat 3: Add to . 4: for all do 5: if is sinlge-step lexicalisable in 1This is an important parameter. It specifies how many slots each elementary tree in the transformed TAG may have (and consequently how many times the “derivation tree” of each of these trees branches). 2This definition is compatible with that used in the literature. A TAG is lexicalised (Joshi and Schabes, 1991) if it is specific-lexicalised according to this definition. The implication does not necessarily hold in reverse. then 6: if then 7: Remove from . 8: Add to . 9: end if 10: For some node of , ,add all the single-step-lexicalisations at of in , to .3 11: end if 12: end for 13: until 14: is a specific-lexicalisation4 of . To illustrate this procedure, we have provided some figures. Consider the TAG , whose elementary trees are shown in figure 4. We have chosen, for reasons of space and simplicity, not to show the feature structures attached to each node of these trees. Their approximate form ca</context>
</contexts>
<marker>Joshi, Schabes, 1991</marker>
<rawString>Aravind K. Joshi and Yves Schabes. 1991. Treeadjoining grammars and lexicalized grammars. In Maurice Nivat and Andreas Podelski, editors, Definability and Recognizability of Sets of Trees. Elsevier.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
</authors>
<title>The relevance of tree adjoining grammar to generation. In Natural Language Generation: New results</title>
<date>1986</date>
<booktitle>in Artificial Intelligence, Psychology and Linguistics - NATO Advanced Research Workshop,</booktitle>
<pages>233--252</pages>
<location>Nijmegen, The Netherlands.</location>
<contexts>
<context position="2894" citStr="Joshi, 1986" startWordPosition="456" endWordPosition="457">llenge from this sort of summarisation is to devise a system which satisfies two potentially incompatible constraints. First, it must be flexible enough to model, for any combination of fields, the optimally aggregated paragraph which expresses them. Second, despite the very large search space that such flexibility probably implies, it must be capable of finding that paragraph in a reasonable time. The contribution the present work makes is a set of algorithms which prune this search space. This space is specified in terms of compositions of elementary trees of a tree-adjoining grammar (TAG) (Joshi, 1986). The first transforms a TAG into a lexicalised version of itself which has better computational properties with respect to summarising a record. The second removes those parts of a TAG which are redundant with respect to summarising a particular record. The third identifies an explicit mapping from each field to its possible surface realisations, and hence allows a desirable surface form to be chosen for each field. Our partial implementation of these algorithms has produced some promising results. The rest of the paper is organised as follows. In section 2 we discuss the problem, our approac</context>
<context position="8305" citStr="Joshi (1986)" startWordPosition="1333" endWordPosition="1334">ed manner. “Constellations” is a science lesson plan which lasts 4 hours. Maureen Ryff wrote it for small group teaching and ProLog, an online provider, is its publisher. Figure 2: A well aggregated paragraph which summarises the set of fields of figure 1, together with field subject = “science”. Notice the nonlinear effect the addition of a single extra proposition can have on the structure of the paragraph. 2.2 Modeling paragraphs with TAG Our approach uses, as its primary representation, TAG formalism extended to include unification based feature structures (Vijay-Shanker and Joshi, 1991). Joshi (1986) describes the advantages TAG possesses as a syntactic formalism for NLG. In generation, the TAG model has usually been applied to generating clauses and sentences. Recently, Webber et al. (1999) outlined the benefits of modelling longer strings of text by the same means. The most important characteristic of TAG for our purposes is the local definability of dependencies: constraints between the nodes of elementary trees are preserved under adjoinings which increase the distances between them. For example, in the sentence fragment “Springer published ...”, which might be modelled by a single in</context>
</contexts>
<marker>Joshi, 1986</marker>
<rawString>Aravind K. Joshi. 1986. The relevance of tree adjoining grammar to generation. In Natural Language Generation: New results in Artificial Intelligence, Psychology and Linguistics - NATO Advanced Research Workshop, pages 233–252, Nijmegen, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie Meteer</author>
</authors>
<title>Expressibility and the Problem of Efficient Text Planning.</title>
<date>1992</date>
<location>Pinter, London.</location>
<contexts>
<context position="5554" citStr="Meteer (1992)" startWordPosition="882" endWordPosition="883"> It thereby gains computational efficiency at the cost of its sentences being less optimal aggregations. Our algorithms are an attempt to avoid these problems and to achieve greater efficiency by precompiling detailed syntactic decisions, the results of which we store in the form of explicit mappings from fields to surface forms. At generation time, we search for the optimal selection of realisations. This approach deviates from the pipeline architecture of NLG systems which, it has been observed, is not wholly suited to the generation of aggregated texts. The first author to discuss this was Meteer (1992), who showed that the microplanning stage of the pipeline is constrained by surface realisation in two ways. First, a microplan must be realisable in the target language; second, a realisable microplan must make best use of the capacities the target language for concise expression. More generally, in order to generate aggregated text, constraints imposed by and opportunities afforded by the surface form may be taken into account at any stage in the pipeline. Reape and Mellish (1999) provided examples of different systems each of which takes aggregation decisions at a different stage. It may no</context>
</contexts>
<marker>Meteer, 1992</marker>
<rawString>Marie Meteer. 1992. Expressibility and the Problem of Efficient Text Planning. Pinter, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Reape</author>
<author>Chris Mellish</author>
</authors>
<title>Just what is aggregation anyway?</title>
<date>1999</date>
<booktitle>In European Workshop on Natural Language Generation,</booktitle>
<volume>13</volume>
<pages>14</pages>
<location>Toulouse,</location>
<contexts>
<context position="1305" citStr="Reape and Mellish, 1999" startWordPosition="186" endWordPosition="190">er computational properties. Also, it identifies an explicit mapping between input propositions and their possible surface realisations. 1 Introduction Summarisation of simply structured data as short natural language paragraphs has recently been a focus of interest. Shaw (1998) and Bental et al. (1999) looked at generating text from database records. Robin and McKeown (1996) summarised quantitative data. Shaw’s examples were drawn from patient medical records; Bental et al’s from online resource cataloging information. A requirement common to all these studies has been to produce aggregated (Reape and Mellish, 1999) text. Also in all these studies, the structure of the input data used was fairly flat. In particular, in (Shaw, 1998) and (Bental et al., 1999) each record This work is supported by EPSRC grant GR/M23106. is associated with a particular entity (e.g. a patient or an online resource) and is essentially a list of attribute-value pairs. We refer to pairs as fields, and to attributes as field names. The relationship between a value and the entity with which it is associated is specified by the field name. Most field names represent “is a” or “has a” relationships and hence most values represent fa</context>
<context position="6041" citStr="Reape and Mellish (1999)" startWordPosition="961" endWordPosition="965">ich, it has been observed, is not wholly suited to the generation of aggregated texts. The first author to discuss this was Meteer (1992), who showed that the microplanning stage of the pipeline is constrained by surface realisation in two ways. First, a microplan must be realisable in the target language; second, a realisable microplan must make best use of the capacities the target language for concise expression. More generally, in order to generate aggregated text, constraints imposed by and opportunities afforded by the surface form may be taken into account at any stage in the pipeline. Reape and Mellish (1999) provided examples of different systems each of which takes aggregation decisions at a different stage. It may not be easy to determine what effect a decision taken at an early stage will have at the surface; and decisions taken at one stage may preclude at a later stage a choice which results in a more aggregated surface form. Similarly, it may not be easy to make a decision at an early stage which makes best use of the surface possibilities. Consider the examples of figures 1 and 2. Both summarise the same set of fields; figure 2 summarises additionally the field “subject = science”. Both pa</context>
</contexts>
<marker>Reape, Mellish, 1999</marker>
<rawString>Mike Reape and Chris Mellish. 1999. Just what is aggregation anyway? In European Workshop on Natural Language Generation, Toulouse, May 13– 14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacques Robin</author>
<author>Kathleen McKeown</author>
</authors>
<title>Empirically designing and evaluating a new revisionbased model for summary generation.</title>
<date>1996</date>
<journal>Artificial Intelligence,</journal>
<pages>85--1</pages>
<contexts>
<context position="1059" citStr="Robin and McKeown (1996)" startWordPosition="150" endWordPosition="153">model the space of possible paraphrases of possible paragraphs as the space of sequences of compositions of a set of tree-adjoining grammar (TAG) elementary trees. Our algorithm transforms this to a set with equivalent paraphrasing power but better computational properties. Also, it identifies an explicit mapping between input propositions and their possible surface realisations. 1 Introduction Summarisation of simply structured data as short natural language paragraphs has recently been a focus of interest. Shaw (1998) and Bental et al. (1999) looked at generating text from database records. Robin and McKeown (1996) summarised quantitative data. Shaw’s examples were drawn from patient medical records; Bental et al’s from online resource cataloging information. A requirement common to all these studies has been to produce aggregated (Reape and Mellish, 1999) text. Also in all these studies, the structure of the input data used was fairly flat. In particular, in (Shaw, 1998) and (Bental et al., 1999) each record This work is supported by EPSRC grant GR/M23106. is associated with a particular entity (e.g. a patient or an online resource) and is essentially a list of attribute-value pairs. We refer to pairs </context>
<context position="4117" citStr="Robin and McKeown, 1996" startWordPosition="650" endWordPosition="654">ch to modelling it, and characteristics of the search space implied by our approach. In section 3 we present our algorithms for searching this space. In section 4 we summarise and discuss. 2 Searching for concise, coherent paragraphs 2.1 Aggregation is a global optimisation problem Our aim is to generate paragraphs which are well aggregated. This notion should be defined in terms of conciseness and coherence, which terms are not formally definable. However, some reasonable approximation to them can be achieved by specifying preferences for certain types of syntactic constructions over others (Robin and McKeown, 1996), possibly by giving each generatable construction a score which reflects its relative preferability. We then define the best aggregated paragraph to be that which achieves the best sum of its constituent constructions’ preference scores. Robin and McKeown’s (1996) system, STREAK, generates aggregated, fact-rich sentences. It adds facts in order of the preferability of their best possible realisation. It revises its syntactic choices every time an extra fact is aggregated. This is computationally expensive, and makes multi-sentence generation by the same means prohibitively slow. They do sugge</context>
</contexts>
<marker>Robin, McKeown, 1996</marker>
<rawString>Jacques Robin and Kathleen McKeown. 1996. Empirically designing and evaluating a new revisionbased model for summary generation. Artificial Intelligence, 85(1-2), August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Shaw</author>
</authors>
<title>Clause aggregation using linguistic knowledge.</title>
<date>1998</date>
<booktitle>In Proceedings of the 9th International Workshop on Natural Language Generation,</booktitle>
<pages>138--147</pages>
<contexts>
<context position="960" citStr="Shaw (1998)" startWordPosition="136" endWordPosition="137">mally aggregated paragraph which summarises a flat structured input specification. We model the space of possible paraphrases of possible paragraphs as the space of sequences of compositions of a set of tree-adjoining grammar (TAG) elementary trees. Our algorithm transforms this to a set with equivalent paraphrasing power but better computational properties. Also, it identifies an explicit mapping between input propositions and their possible surface realisations. 1 Introduction Summarisation of simply structured data as short natural language paragraphs has recently been a focus of interest. Shaw (1998) and Bental et al. (1999) looked at generating text from database records. Robin and McKeown (1996) summarised quantitative data. Shaw’s examples were drawn from patient medical records; Bental et al’s from online resource cataloging information. A requirement common to all these studies has been to produce aggregated (Reape and Mellish, 1999) text. Also in all these studies, the structure of the input data used was fairly flat. In particular, in (Shaw, 1998) and (Bental et al., 1999) each record This work is supported by EPSRC grant GR/M23106. is associated with a particular entity (e.g. a pa</context>
<context position="4850" citStr="Shaw, 1998" startWordPosition="769" endWordPosition="770">est aggregated paragraph to be that which achieves the best sum of its constituent constructions’ preference scores. Robin and McKeown’s (1996) system, STREAK, generates aggregated, fact-rich sentences. It adds facts in order of the preferability of their best possible realisation. It revises its syntactic choices every time an extra fact is aggregated. This is computationally expensive, and makes multi-sentence generation by the same means prohibitively slow. They do suggest how to deal with this when many of the facts occur in fixed positions, but this is not the case in our corpus. CASPER (Shaw, 1998) delays syntactic choices until after it has decided where sentence boundaries should fall. It thereby gains computational efficiency at the cost of its sentences being less optimal aggregations. Our algorithms are an attempt to avoid these problems and to achieve greater efficiency by precompiling detailed syntactic decisions, the results of which we store in the form of explicit mappings from fields to surface forms. At generation time, we search for the optimal selection of realisations. This approach deviates from the pipeline architecture of NLG systems which, it has been observed, is not</context>
</contexts>
<marker>Shaw, 1998</marker>
<rawString>James Shaw. 1998. Clause aggregation using linguistic knowledge. In Proceedings of the 9th International Workshop on Natural Language Generation, pages 138–147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathew Stone</author>
<author>Christine Doran</author>
</authors>
<title>Sentence planning as description using tree-adjoining grammar.</title>
<date>1997</date>
<booktitle>In Proceedings od the Assosciation for Computational Linguistics,</booktitle>
<pages>198--205</pages>
<contexts>
<context position="7276" citStr="Stone and Doran (1997)" startWordPosition="1171" endWordPosition="1174">arise their fields in the most concise and coherent manner possible (although this is, of course, a subjective judgement). Note that they treat the fields they have in common differently with respect to ordering and distribution between the sentences. Various types of constraints cause this. Syntactic constraints include: “science” may be used as an adjective to pre-modify “lesson plan”. Semantic constraints include: “Constellations” lasts 4 hours, but ProLog does not. Stylistic constraints include: ‘Maureen Ryff wrote ...’ is preferable to ‘... was written by Maureen Ryff’. We suggest, as do Stone and Doran (1997), that integrating these constraints simultaneously is more efficient then pipelining them. We additionally suggest that representing these constraints in a unified form can provide further efficiency gains. “Constellations” is a 4-hour lesson plan published by online provider ProLog. Maureen Ryff wrote it for small group teaching. Figure 1: A paragraph which summarises the set of fields of figure 3 in an aggregated manner. “Constellations” is a science lesson plan which lasts 4 hours. Maureen Ryff wrote it for small group teaching and ProLog, an online provider, is its publisher. Figure 2: A </context>
</contexts>
<marker>Stone, Doran, 1997</marker>
<rawString>Mathew Stone and Christine Doran. 1997. Sentence planning as description using tree-adjoining grammar. In Proceedings od the Assosciation for Computational Linguistics, pages 198–205.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>Aravind K Joshi</author>
</authors>
<title>Unification based tree adjoining grammars.</title>
<date>1991</date>
<editor>In J. Wedekind, editor, Unification-based Grammars.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="8291" citStr="Vijay-Shanker and Joshi, 1991" startWordPosition="1329" endWordPosition="1332">ields of figure 3 in an aggregated manner. “Constellations” is a science lesson plan which lasts 4 hours. Maureen Ryff wrote it for small group teaching and ProLog, an online provider, is its publisher. Figure 2: A well aggregated paragraph which summarises the set of fields of figure 1, together with field subject = “science”. Notice the nonlinear effect the addition of a single extra proposition can have on the structure of the paragraph. 2.2 Modeling paragraphs with TAG Our approach uses, as its primary representation, TAG formalism extended to include unification based feature structures (Vijay-Shanker and Joshi, 1991). Joshi (1986) describes the advantages TAG possesses as a syntactic formalism for NLG. In generation, the TAG model has usually been applied to generating clauses and sentences. Recently, Webber et al. (1999) outlined the benefits of modelling longer strings of text by the same means. The most important characteristic of TAG for our purposes is the local definability of dependencies: constraints between the nodes of elementary trees are preserved under adjoinings which increase the distances between them. For example, in the sentence fragment “Springer published ...”, which might be modelled </context>
</contexts>
<marker>Vijay-Shanker, Joshi, 1991</marker>
<rawString>K. Vijay-Shanker and Aravind K. Joshi. 1991. Unification based tree adjoining grammars. In J. Wedekind, editor, Unification-based Grammars. MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Webber</author>
<author>Aravind K Joshi</author>
<author>Alistair Knott</author>
<author>Matthew Stone</author>
</authors>
<title>What are little texts made of? a structural presuppositional account using lexicalised tag.</title>
<date>1999</date>
<booktitle>In Proceedings of International Workshop on Levels of Representation in Discourse,</booktitle>
<location>Edinburgh,</location>
<contexts>
<context position="8500" citStr="Webber et al. (1999)" startWordPosition="1362" endWordPosition="1365"> aggregated paragraph which summarises the set of fields of figure 1, together with field subject = “science”. Notice the nonlinear effect the addition of a single extra proposition can have on the structure of the paragraph. 2.2 Modeling paragraphs with TAG Our approach uses, as its primary representation, TAG formalism extended to include unification based feature structures (Vijay-Shanker and Joshi, 1991). Joshi (1986) describes the advantages TAG possesses as a syntactic formalism for NLG. In generation, the TAG model has usually been applied to generating clauses and sentences. Recently, Webber et al. (1999) outlined the benefits of modelling longer strings of text by the same means. The most important characteristic of TAG for our purposes is the local definability of dependencies: constraints between the nodes of elementary trees are preserved under adjoinings which increase the distances between them. For example, in the sentence fragment “Springer published ...”, which might be modelled by a single initial tree, the object is constrained to be some entity published by Springer. If an adjunction is made so that the fragment becomes “Springer, the largest publishing company in Europe, published</context>
</contexts>
<marker>Webber, Joshi, Knott, Stone, 1999</marker>
<rawString>Bonnie Webber, Aravind K. Joshi, Alistair Knott, and Matthew Stone. 1999. What are little texts made of? a structural presuppositional account using lexicalised tag. In Proceedings of International Workshop on Levels of Representation in Discourse, Edinburgh, July. LOIRD’99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Yang</author>
<author>K F McCoy</author>
<author>K Vijay-Shanker</author>
</authors>
<title>From functional specification to syntactic structures: functional grammar and tree-adjoining grammar.</title>
<date>1991</date>
<journal>Computational Intelligence,</journal>
<volume>7</volume>
<issue>4</issue>
<contexts>
<context position="18784" citStr="Yang et al. (1991)" startWordPosition="3145" endWordPosition="3148">h, in order, the spaces of possible compositions of the s combined with some necessary supporting trees which are not anchored by an element of . Call these spaces . In terms of redundancy removal, is the specific-lexicalised TAG with those trees which might be redundant with respect to the search for removed. We begin the search with . It is not guaranteed that is in this space. If it is not, we repeat the search using , and so on. At worst (if where has elements), this procedure exhaustively searches the space of compositions of the elements of . 5This is the family (as that term is used by Yang et al. (1991) and others) of trees anchored on . Figure 4: The elementary trees of a small TAG, . The trees’ names are in bold below them. Substitution nodes are indicated with a ‘+’; foot nodes are indicated with a ‘*’; the distinguished symbol is ‘s’. Slots are shown as ‘@&lt;reference&gt;’, where “reference” is the field to which the slot refers. Note that the feature structures which are associated with each node, which prohibit certain compositions, are not shown. Note also that this is not a lexicalised TAG (LTAG). This is somewhat unusual; we intend, as part of our ongoing work, to apply our techniques to</context>
</contexts>
<marker>Yang, McCoy, Vijay-Shanker, 1991</marker>
<rawString>G. Yang, K. F. McCoy, and K. Vijay-Shanker. 1991. From functional specification to syntactic structures: functional grammar and tree-adjoining grammar. Computational Intelligence, 7(4):207–219.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>