<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002832">
<title confidence="0.99578">
Knowledge Portability with Semantic Expansion of Ontology Labels
</title>
<author confidence="0.887903">
Mihael Arcan&apos; Marco Turchi2 Paul Buitelaar&apos;
</author>
<affiliation confidence="0.576707">
&apos; Insight Centre for Data Analytics, National University of Ireland, Galway
</affiliation>
<address confidence="0.434521">
firstname.lastname@insight-centre.org
2 FBK- Fondazione Bruno Kessler, Via Sommarive 18, 38123 Trento, Italy
</address>
<email confidence="0.994159">
turchi@fbk.eu
</email>
<sectionHeader confidence="0.993764" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99971075">
Our research focuses on the multilin-
gual enhancement of ontologies that, of-
ten represented only in English, need to
be translated in different languages to en-
able knowledge access across languages.
Ontology translation is a rather different
task then the classic document translation,
because ontologies contain highly specific
vocabulary and they lack contextual in-
formation. For these reasons, to improve
automatic ontology translations, we first
focus on identifying relevant unambigu-
ous and domain-specific sentences from a
large set of generic parallel corpora. Then,
we leverage Linked Open Data resources,
such as DBPedia, to isolate ontology-
specific bilingual lexical knowledge. In
both cases, we take advantage of the se-
mantic information of the labels to se-
lect relevant bilingual data with the aim
of building an ontology-specific statistical
machine translation system. We evaluate
our approach on the translation of a medi-
cal ontology, translating from English into
German. Our experiment shows a sig-
nificant improvement of around 3 BLEU
points compared to a generic as well as a
domain-specific translation approach.
</bodyText>
<sectionHeader confidence="0.999137" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999838428571429">
Currently, most of the semantically structured
data, i.e. ontologies or taxonomies, has labels ex-
pressed in English only.1 On the one hand, the
increasing amount of ontologies offers an excel-
lent opportunity to link this knowledge together
(G´omez-P´erez et al., 2013). On the other hand,
non-English users may encounter difficulties when
</bodyText>
<note confidence="0.624088">
1Based on (Gracia et al., 2012), around 80% of ontology
labels indexed in Watson are English.
</note>
<bodyText confidence="0.9999463">
using the ontological knowledge represented only
in English. Furthermore, applications in informa-
tion retrieval, question answering or knowledge
management, that use monolingual ontologies are
therefore limited to the language in which the on-
tology labels are stored. To make the ontologi-
cal knowledge language-independent and accessi-
ble beyond language borders, these monolingual
resources need to be transformed into multilingual
knowledge bases. This multilingual enhancement
can enable queries on documents beyond English,
e.g. for cross-lingual business intelligence in the
financial domain (O’Riain et al., 2013), provid-
ing information related to an ontology label, e.g.
other intangible assets,2 in Spanish, German or
Italian. The main challenge involved in build-
ing multilingual knowledge bases is, however, to
bridge the gap between language-specific informa-
tion and the language-independent semantic con-
tent of ontologies or taxonomies (Gracia et al.,
2012).
Since manual multilingual enhancement of on-
tologies is a very time consuming and expensive
process, we engage an ontology-specific statisti-
cal machine translation (SMT) system to automat-
ically translate the ontology labels. Due to the fact
that ontology labels are usually highly domain-
specific and stored only in knowledge represen-
tations (Chandrasekaran et al., 1999), the labels
appear infrequent in parallel corpora, which are
needed to build a domain-specific translation sys-
tem with accurate translation candidates. Addi-
tionally, ambiguous labels built out of only a few
words do often not express enough semantic or
contextual information to guide the SMT system
to translate a label into the targeted domain. This
can be observed by domain-unadapted SMT sys-
tems, e.g. Google Translate, where ambiguous
expressions, such as vessel stored in an medical
ontology, are often translated into a generic do-
</bodyText>
<footnote confidence="0.97051">
2ontology label stored in FINREP - FINancial REPorting
</footnote>
<page confidence="0.838929">
708
</page>
<note confidence="0.9714">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 708–718,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.99995914">
main as Schiff3 in German (meaning ship or boat),
but not into the targeted medical domain as Gef¨aß.
Since ontologies may change over time, keeping
up with these changes can be challenging for a hu-
man translator. Having in place an SMT system
adapted to an ontology can therefore be very ben-
eficial.
In this work, we propose an approach to select
the most relevant (parallel) sentences from a pool
of generic sentences based on the lexical and se-
mantic overlap with the ontology labels. The goal
is to identify sentences that are domain-specific in
respect of the target domain and contain as much
as possible relevant words that can allow the SMT
system to learn the translations of the monolin-
gual ontology labels. For instance, with the sen-
tence selection we aim to retain only parallel sen-
tences where the English word injection is trans-
lated into the German language as Impfung in the
medical domain, but not into Eind¨usung, belong-
ing to the technical domain. This selection process
aims to reduce the semantic noise in the translation
process, since we try to avoid learning translation
candidates that do not belong to the targeted do-
main. Nonetheless, some of the domain-specific
ontology labels may not be automatically trans-
latable with SMT, due to the fact that the bilin-
gual information is missing and cannot be learned
from the parallel sentences. Therefore we use the
information contained in the DBpedia knowledge
base (Lehmann et al., 2015) to improve the trans-
lation of expressions which are not known to the
SMT system. We tested our approach on the med-
ical domain translating from English to German,
showing improvements of around 3 BLEU points
compared to a generic as well as a domain-specific
translation model.
The remainder of this paper is organized as
follows: Section 2 gives an overview of the re-
lated work done in the field of ontology translation
within SMT. In Section 3, we present the method-
ology of parallel data selection and terminology
identification to improve ontology label transla-
tion. Furthermore we show different methods of
embedding domain-specific knowledge into SMT.
In Experimental Setting, Section 4, we describe
the ontology to be translated along the training
data needed for SMT. Moreover we introduce ex-
isting approaches and give a description of met-
rics for automatic translation evaluation. Section 5
</bodyText>
<footnote confidence="0.686739">
3Translation performed on 25.02.2015
</footnote>
<bodyText confidence="0.999911333333333">
presents the automatic and manual evaluation of
the translated labels. Finally, conclusions and fu-
ture work are shown in Section 6.
</bodyText>
<sectionHeader confidence="0.999346" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.997596717391304">
The task of ontology translation involves the find-
ing of an appropriate translation for the lexical
layer, i.e. labels, of the ontology. Most of the
previous work tackled this problem by accessing
multilingual lexical resources, e.g. EuroWordNet
or IATE (Declerck et al., 2006; Cimiano et al.,
2010). Their work focuses on the identification
of the lexical overlap between the ontology and
the multilingual resource. Since the replacement
of the source and target vocabulary guarantees a
high precision but a low recall, external transla-
tion services, e.g. BabelFish, SDL FreeTransla-
tion tool or Google Translate, were used to over-
come this issue (Fu et al., 2009; Espinoza et al.,
2009). Additionally, ontology label disambigua-
tion was performed by (Espinoza et al., 2009) and
(McCrae et al., 2011), where the structure of the
ontology along with existing multilingual ontolo-
gies was used to annotate the labels with their se-
mantic senses. Differently to the aforementioned
approaches, which rely on external knowledge or
services, we focus on how to gain adequate trans-
lations using a small, but ontology-specific SMT
system. We learned that using external SMT ser-
vices often results in wrong translations of la-
bels, because the external SMT services are not
able to adapt to the specificity of the ontology.
Avoiding existing multilingual resources, which
enables a simple replacement of source and target
labels, showed the possibility of improving label
translations without manually generated lexical re-
sources, since not every ontology may benefit of
current multilingual resources.
Due to the specificity of the labels, previous
research (Wu et al., 2008; Haddow and Koehn,
2012) showed that generic SMT systems, which
merge all accessible data together, cannot be used
to translate domain-specific vocabulary. To avoid
unsatisfactory translations of specific vocabulary
we have to provide the SMT system domain-
specific bilingual knowledge, from where it can
learn specific translation candidates. (Eck et al.,
2004) used for the language model adaptation
within SMT the information retrieval technique
tf-idf. Similarly, (Hildebrand et al., 2005) and
(L¨u et al., 2007) utilized this approach to select
</bodyText>
<page confidence="0.997391">
709
</page>
<bodyText confidence="0.999842152542373">
relevant sentences from available parallel text to
adapt translation models. The results confirmed
that large amounts of generic training data can-
not compensate for the requirement of domain-
specific training sentences. Another approach is
taken by (Moore and Lewis, 2010), where, based
on source and target language models, the authors
calculated the difference of the cross-entropy val-
ues for a given sentence. (Axelrod et al., 2011)
extend this work using the bilingual difference
of cross-entropy on in-domain and out-of-domain
language models for training sentence selection
for SMT. (Wuebker et al., 2014) reused the cross-
entropy approach and applied it to the translation
of video lectures. (Kirchhoff and Bilmes, 2014)
introduce submodular optimization using complex
features for parallel sentence selection. In their
experiments they use the source and target side
of the text to be translated, and show significant
improvements over the widely used cross-entropy
method. A different approach for sentence se-
lection is shown in (Cuong and Sima’an, 2014),
where the authors propose a latent domain transla-
tion model to distinguish between hidden in- and
out-of-domain data. (Gasc´o et al., 2012) and (Bi-
cici and Yuret, 2011) sub-sample sentence pairs
whose source has most overlap with the evaluation
dataset. Different from these approaches, we do
not embed any specific in-domain knowledge to
the generic corpus, from which sentence selection
is performed. Furthermore, none of these meth-
ods explicitly exploit the ontological hierarchy for
label disambiguation and are not specifically de-
signed to deal with the characteristics of ontology
labels.
As a lexical resource, Wikipedia with its rich
semantic knowledge was used as a resource for
bilingual term identification in the context of SMT.
(Tyers and Pieanaar, 2008) extracts bilingual dic-
tionary entries from Wikipedia to support the ma-
chine translation system. Based on exact string
matching they query Wikipedia with a list of
around 10,000 noun lemmas to generate the bilin-
gual dictionary. Besides the interwiki link system,
(Erdmann et al., 2009) enhance their bilingual dic-
tionary by using redirection page titles and anchor
text within Wikipedia. To cast the problem of
ambiguous Wikipedia titles, (Niehues and Waibel,
2011; Arcan et al., 2014a) use the information of
Wikipedia categories and the text of the articles to
provide the SMT system domain-specific bilingual
knowledge. This research showed that using the
lexical information stored in this knowledge base
improves the translation of highly domain-specific
vocabulary. However, we do not rely on cate-
gory annotations of Wikipedia articles, but per-
form domain-specific dictionary generation based
on the overlap between related words from the on-
tology label and the abstract of a Wikipedia article.
</bodyText>
<sectionHeader confidence="0.997798" genericHeader="method">
3 Methodology
</sectionHeader>
<bodyText confidence="0.999938571428571">
We propose an approach that uses the ontology
labels to be translated to select the most relevant
parallel sentences from a generic parallel corpus.
Since ontology labels tend to be short (McCrae
et al., 2011), we expand the label representation
with its semantically related words. This expan-
sion enables a larger semantic overlap between a
label and the (parallel) sentences, which gives us
more information to distinguish between related
and unrelated sentences. Our approach reduces
the ambiguity of expressions in the selected par-
allel sentences, which consequently gives more
preference to translation candidates of the targeted
domain. Furthermore, we access the DBpedia
knowledge base to identify bilingual terminology
belonging to the domain of the ontology. Once
the domain-specific parallel sentences and lexi-
cal knowledge is available, we use different tech-
niques to embed this knowledge into the SMT sys-
tem. These methods are detailed in the following
subsections.
</bodyText>
<subsectionHeader confidence="0.994651">
3.1 Domain-Specific Parallel Sentence
Selection
</subsectionHeader>
<bodyText confidence="0.999962941176471">
In order to generate the best translation system we
select only sentences from the generic parallel cor-
pus which are most relevant to the labels to be
translated. The first criteria for relevance was the
n-gram overlap between a label and a source sen-
tence coming from the generic corpus. Therefore
we calculate the cosine similarity between the n-
grams extracted from a label and the n-grams of
each source sentence in the generic corpus. The
similarity between the label and the sentence is de-
fined as the cosine of the angle between the two
vectors. The calculated similarity score allows us
to distinguish between more and less relevant sen-
tences.
Due to the specificity of ontology labels, the n-
gram overlap approach is not able to select use-
ful sentences in the presence of short labels. For
</bodyText>
<page confidence="0.969911">
710
</page>
<bodyText confidence="0.999985048780488">
this reason, we improve it by extending the se-
mantic information of labels using a technique for
computing vector representations of words. The
technique is based on a neural network that anal-
yses the textual data provided as input and pro-
vides as output a list of semantically related words
(Mikolov et al., 2013). Each input string is vector-
ized using the surrounding context and compared
to other vectorized sets of words (from the training
data) in a multi-dimensional vector space. For ob-
taining the vector representations we used a distri-
butional semantic model trained on the Wikipedia
articles,4 containing more than 3 billion words.
Word relatedness is measured through the cosine
similarity between two word vectors. A score of
1 would represent a perfect word similarity; e.g.
cholera equals cholera, while the medical expres-
sion medicine has a cosine distance of 0.678 to
cholera. Since words, which occur in similar con-
texts tend to have similar meanings (Harris, 1954),
this approach enables to group related words to-
gether. The output of this technique is the analysed
label with a vector attached to it, e.g. for the med-
ical label cholera it provides related words with
its relatedness value, e.g. typhus (0.869), smallpox
(0.849), epidemic (0.834), dysentery (0.808) ... In
our experiments, this method is implemented by
the use of Word2Vec.5
To additionally disambiguate short labels, the
related words of the current label are combined
with the related words of its direct parent in the
ontology. The usage of the ontology hierarchy al-
lows us to take advantage of the specific vocabu-
lary of the related words in the computation of the
cosine similarity. Given a label and a source sen-
tence from the generic corpus, related words and
their weights are extracted from both of them and
used as entries of the vectors passed to the cosine
similarity. The most similar source sentence and
the label should share the largest number of related
words (largest cosine similarity).
</bodyText>
<subsectionHeader confidence="0.999845">
3.2 Bilingual Terminology Identification
</subsectionHeader>
<bodyText confidence="0.999936">
The automatic translation of domain-specific vo-
cabulary can be a hard task for a generic SMT sys-
tem, if the bilingual knowledge is not present in
the parallel dataset. To complement the previous
approaches we access DBpedia6 as a multilingual
lexical resource.
</bodyText>
<footnote confidence="0.994763666666667">
4Wikipedia dump id enwiki-20141106
5https://code.google.com/p/word2vec/
6http://wiki.dbpedia.org/Downloads2014
</footnote>
<bodyText confidence="0.999993939393939">
We engage the idea of (Arcan et al., 2012)
where the authors provide to the SMT system un-
ambiguous terminology identified in Wikipedia to
improve the translations of labels in the financial
domain. To disambiguate Wikipedia entries with
translations into different domains, they query the
repository for analysing the n-gram overlap be-
tween the financial labels and the Wikipedia en-
tries and store the frequency of categories which
are associated with the matched entry. In a fi-
nal step they extract only bilingual Wikipedia en-
tries, which are associated with the most frequent
Wikipedia categories identified in the previous
step.
Since the Wikipedia entries are often associ-
ated only with a few categories, this limited vo-
cabulary may give only a small contribution for
this disambiguation of different meanings or top-
ics of the same Wikipedia entry. For this reason,
we use for each Wikipedia entry the extended ab-
stract, which contains more information about the
entry compared to the previous approach. For am-
biguous Wikipedia entries, which overlap with a
medical label, we therefore calculate the cosine
similarity between the related words associated
with the label and the lexical information of the
Wikipedia abstract. Among different ambiguous
entries, the cosine similarity gives more weight to
the Wikipedia entry, which is closer to our pre-
ferred domain. Finally, if the Wikipedia entry has
an equivalent in the target language, i.e. German,
we use the bilingual information for the lexical en-
hancement of the SMT system.
</bodyText>
<subsectionHeader confidence="0.9954455">
3.3 Integration of Domain-Specific
Knowledge into SMT
</subsectionHeader>
<bodyText confidence="0.999983333333333">
After the identification of domain-specific bilin-
gual knowledge, it has to be integrated into the
workflow of the SMT system. The injection of
new obtained knowledge can be performed by re-
training the domain-specific knowledge with the
generic parallel corpus (Langlais, 2002; Ren et al.,
2009; Haddow and Koehn, 2012) or by adding
new entries directly to the translation system (Pin-
nis et al., 2012; Bouamor et al., 2012). These
methods have the drawback that the bilingual do-
main specificity may get lost due to the usually
larger generic parallel corpora. Giving more pri-
ority to domain-specific translations than generic
ones, we focus on two techniques, i.e. the Fill-Up
model (Bisazza et al., 2011) and the Cache-Based
</bodyText>
<page confidence="0.98591">
711
</page>
<bodyText confidence="0.99859846">
Model (Bertoldi et al., 2013) approach.
The Fill-Up model has been developed to ad-
dress a common scenario where a large generic
background model exists, and only a small quan-
tity of domain-specific data can be used to build
a translation model. Its goal is to leverage the
large coverage of the background model, while
preserving the domain-specific knowledge com-
ing from the domain-specific data. For this pur-
pose the generic and the domain-specific transla-
tion models are merged. For those translation can-
didates that appear in both models, only one in-
stance is reported in the Fill-Up model with the
largest probabilities according to the translation
models. To keep track of a translation candidate’s
provenance, a binary feature is added that gives
preference to a translation candidate if it comes
from the domain-specific translation model. We
engage the idea of the Fill-Up model to combine
the domain-specific parallel knowledge from the
selected sentences with the generic (1.9M) paral-
lel corpus.
Furthermore, for embedding bilingual lexical
knowledge into the SMT system, we engage the
idea of cache-based translation and language mod-
els (Bertoldi et al., 2013). The main idea behind
these models is to combine a large static global
model with a small, but dynamic local model. This
approach has already shown its potential of in-
jecting domain-specific knowledge into a generic
SMT system (Arcan et al., 2014b). For our exper-
iments we inject the bilingual lexical knowledge
identified in DBpedia and IATE into the cache-
based models. The cache-based model relies on
a local translation model (CBTM) and language
model (CBLM). The first is implemented as an
additional table in the translation model provid-
ing one score. All entries are associated with an
’age’ (initially set to 1), corresponding to the time
when they were actually inserted. Each new in-
sertion causes an ageing of the existing translation
candidates and hence their re-scoring; in case of
re-insertion of a phrase pair, the old value is set to
the initial value. Similarly to the CBTM, the lo-
cal language model is built to give preference to
the provided target expressions. Each entry stored
in CBLM is associated with a decaying function
of the age of insertion into the model. Both mod-
els are used as additional features of the log-linear
model in the SMT system.
</bodyText>
<sectionHeader confidence="0.99732" genericHeader="method">
4 Experimental Setting
</sectionHeader>
<bodyText confidence="0.996053428571429">
In this Section, we give an overview on the dataset
and the translation toolkit used in our experiment.
Furthermore, we describe the existing approaches
and give insights into the SMT evaluation tech-
niques, considering the translation direction from
English to German.
Evaluation Dataset For our experiments we
used the International Classification of Diseases
(ICD) ontology as the gold standard,7 whereby the
considered translation direction is from English to
German. The ICD ontology, translated into 43 lan-
guages, is used to monitor diseases and to report
the general health situation of the population in a
country. This stored information also provides an
overview of the national mortality rate and appear-
ance of diseases of WHO member countries.
For our experiment we used 2000 English labels
from the ICD-10 dataset, which were aligned to
their German equivalents (Table 1). To identify the
best set of sentences we experiment with differ-
ent values of τ, which is the percentage of all the
sentences that are considered relevant (domain-
specific) by the sentence extraction approach. The
value that allows the SMT system to achieve the
best performance on the development dataset 1 is
used on the evaluation set, which is used for the
translation evaluation of ontology labels reported
in this paper. The parameters within the SMT sys-
tem are optimized on the development dataset 2.
Statistical Machine Translation and Training
Dataset For our translation task, we use the sta-
tistical translation toolkit Moses (Koehn et al.,
2007), where the word alignments were built with
the GIZA++ toolkit (Och and Ney, 2003). The
SRILM toolkit (Stolcke, 2002) was used to build
the 5-gram language model.
For a broader domain coverage of the generic
training dataset necessary for the SMT system,
we merged parts of JRC-Acquis 3.08 (Steinberger
et al., 2006), Europarl v79 (Koehn, 2005) and
OpenSubtitles201310 (Tiedemann, 2012), obtain-
ing a training corpus of 1.9M sentences, con-
</bodyText>
<footnote confidence="0.994840571428571">
7http://www.who.int/classifications/
icd/en/
8https://ec.europa.eu/jrc/en/
language-technologies/jrc-acquis
9http://www.statmt.org/europarl/
10http://opus.lingfil.uu.se/
OpenSubtitles2013.php
</footnote>
<page confidence="0.950869">
712
</page>
<table confidence="0.99986">
English German
Generic Dataset Sentences 1.9M
(out-domain) Running Words 39.8M 37.1M
Vocabulary 195,912 446,068
EMEA Dataset Sentences 1.1M
(domain-specific) Running Words 13.8M 12.7M
Vocabulary 58,935 115,754
Development Labels 500
Dataset 1 Running Words 3,025 2,908
Vocabulary 889 951
Development Labels 500
Dataset 2 Running Words 3,003 3,020
Vocabulary 938 1,027
Evaluation Labels 1,000
Dataset Running Words 5,677 5,514
Vocabulary 1,255 1,489
</table>
<tableCaption confidence="0.877108">
Table 1: Statistics for the bilingual training, de-
velopment and evaluation datasets. (’Vocabulary’
denotes the number of unique words in the dataset)
</tableCaption>
<bodyText confidence="0.999577034482759">
taining around 38M running words (Table 1).11
The generic SMT system, trained on the con-
catenated 1.9 sentences, is used as a baseline,
which we compare against the domain-specific
models generated with different sentence selection
methods. Furthermore we use the generic SMT
system in combination with the smaller domain-
specific models to evaluate different approaches
when combining generic and domain-specific data
together.
We additionally compare our results to an SMT
system built on an existing domain-specific par-
allel dataset, i.e. EMEA12 (Tiedemann, 2009),
which holds specific medical parallel data ex-
tracted from the European Medicines Agency doc-
uments and websites.
Comparison to Existing Approaches We com-
pare our approach on knowledge expansion for
sentence selection with similar methods that dis-
tinguish between more important sentences and
less important ones. First, we sort 1.9M sentences
from the generic corpus based on the perplexity
of the ontology vocabulary. The perplexity score
gives a notion of how well the probability model
based on the ontology vocabulary predicts a sam-
ple, which is in our case each sentence in the
generic corpus.
Second, we use the method shown in (Hilde-
brand et al., 2005), where the authors use a method
</bodyText>
<footnote confidence="0.872914333333333">
11For reproducibility and future evaluation we take the first
one-third part of each corpus.
12http://opus.lingfil.uu.se/EMEA.php
</footnote>
<bodyText confidence="0.999203666666666">
based on tf-idf 13 to select the most relevant sen-
tences. This widely-used method in information
retrieval tells us how important a word is to a doc-
ument, whereby each sentence from the generic
corpus is treated as a document.
Finally, we compare our approach with the in-
frequent n-gram recovery method, described in
(Gasc´o et al., 2012). Their technique consists of
selection of relevant sentences from the generic
corpus, which contain infrequent n-grams based
on their test data. They consider an n-gram as
infrequent if it appears in the generic corpus less
times than an infrequent threshold t.
Furthermore we enrich and evaluate our pro-
posed ontology-specific SMT system with the lex-
ical information coming from the terminological
database IATE14 (Inter-Active Terminology for
Europe). IATE is the institutional terminology
database of the EU and is used for the collection,
dissemination and shared management of specific
terminology and contains approximately 1.4 mil-
lion multilingual entries.
Evaluation Metrics The automatic translation
evaluation is based on the correspondence be-
tween the SMT output and reference translation
(gold standard). For the automatic evaluation
we used the BLEU (Papineni et al., 2002) and
METEOR (Denkowski and Lavie, 2014) algo-
rithms.15
BLEU (Bilingual Evaluation Understudy) is
calculated for individual translated segments (n-
grams) by comparing them with a dataset of refer-
ence translations. Considering the shortness of the
labels, we report scores based on the bi-gram over-
lap (BLEU-2) and the standard four-gram over-
lap (BLEU-4). Those scores, between 0 and 100
(perfect translation), are then averaged over the
whole evaluation dataset to reach an estimate of
the translation’s overall quality.
METEOR (Metric for Evaluation of Transla-
tion with Explicit ORdering) is based on the har-
monic mean of precision and recall, whereby re-
call is weighted higher than precision. Along with
standard exact word (or phrase) matching it has
additional features, i.e. stemming, paraphrasing
and synonymy matching. Differently to BLEU,
the metric produces good correlation with human
judgement at the sentence or segment level.
</bodyText>
<footnote confidence="0.999448333333333">
13tf-idf – term frequency-inverse document frequency
14http://iate.europa.eu/downloadTbx.do
15METEOR configuration: exact, stem, paraphrase
</footnote>
<page confidence="0.998802">
713
</page>
<bodyText confidence="0.99995925">
The approximate randomization approach in
MultEval (Clark et al., 2011) is used to test
whether differences among system performances
are statistically significant with a p-value &lt; 0.05.
</bodyText>
<sectionHeader confidence="0.850412" genericHeader="evaluation">
5 Evaluation of Ontology Labels
</sectionHeader>
<bodyText confidence="0.999459875">
In this Section, we report the translation quality
of ontology labels based on translation systems
learned from different sentence selection methods.
Additionally, we perform experiments training an
SMT system on the combination of in- and out-
domain knowledge. The final approach enhances
a domain-specific translation system with lexical
knowledge identified in IATE or DBpedia.
</bodyText>
<subsectionHeader confidence="0.975345">
5.1 Automatic Translation Evaluation
</subsectionHeader>
<bodyText confidence="0.999935441176471">
We report the automatic evaluation based on
BLEU and METEOR for the sentence selection
techniques, the combination of in- and out-domain
data and the lexical enhancement of SMT.
Sentence Selection Techniques As a first eval-
uation, we automatically compare the quality of
the ICD labels translated with different SMT sys-
tems trained on specific sentences by the afore-
mentioned selection techniques (Table 2). Due to
the in-domain bilingual knowledge, the translation
system trained using the EMEA dataset performs
slightly better compared to the large generic base-
line system. Among the different sentence selec-
tion approaches, the infrequent n-gram recovery
method (infreq. in Table 2) outperforms the base-
lines and all the other techniques. This is due to
the very strict criteria of selecting relevant sen-
tences that allows the infrequent n-gram recovery
method to identify a limited number (20,000) of
highly ontology-specific bilingual sentences. The
related words and the n-gram overlap models per-
form slightly better than the baseline, with a usage
of 81,000 and 59,000 relevant sentences, and per-
form similarly to the in-domain EMEA translation
system.
Further translation quality improvement is pos-
sible, if sentence selection methods are combined
together (last four rows in Table 2). The co-
sine similarities of the methods are combined to-
gether, whereby new thresholds τ are computed
on the development dataset 1 and applied on the
ICD evaluation dataset. Each combined method
showed improvement compared to the stand-alone
method. The best overall performance is obtained
</bodyText>
<table confidence="0.997056583333333">
Dataset Type Size BLEU-2 BLEU-4 METEOR
1.9M 17.2 6.6 24.7
1.1M 18.5 7.0 25.8
89K 17.5 6.8 24.8
21K 12,6 4.9 18,7
20K 19.1 8.1 25.3
81K 18.9 7.0 25.8
59K 17.7 7.1 23.3
22K 18.9 8.2* 25.1
24K 17.3 7.3 23.9
24K 18.4 8.4* 25.5*
30K 20.1 8.9* 27.2*
</table>
<tableCaption confidence="0.995665">
Table 2: Automatic translation evaluation on the
</tableCaption>
<bodyText confidence="0.979599594594595">
evaluation dataset of the ICD ontology (Size =
amount of selected sentences from the generic par-
allel corpus. bold results= best performance; *sta-
tistically significant compared to baseline)
when combining the n-gram overlap, the seman-
tic related words and infrequent n-gram recovery
methods. With this combination, we reduce the
amount of parallel sentences by 98% compared
to the generic corpus and significantly outperform
the baseline by 2.3 BLEU score points. These
two factors confirm the capability of the combined
approach of selecting only few ontology-specific
bilingual sentences (30,000) that allows the SMT
system to identify the correct translations in the
target ontology domain. This is due to the fact that
the three combined methods are quite complemen-
tary. In fact, the n-gram overlap method selects a
relatively large amount of bilingual sentences with
few words in common with the label, the related
words approach identifies bilingual sentences in
the ontology target domain, and the infrequent n-
gram recovery technique selects few bilingual sen-
tences with only specific n-grams in common with
the labels balancing the effect of the n-gram over-
lap method.
Combining In- and Out-Domain Data Con-
sidering the relatively small amount of parallel
data extracted with the sentence selecting meth-
ods for the SMT community, we evaluate dif-
ferent approaches that combine a large generic
translation model with domain-specific data. For
this purpose, we use the sentences selected by
the best approach ((5)∧(4)∧(3)) in the previous
experiments and combine them with the generic
parallel dataset. We evaluate the translation per-
formance when (i) concatenating the selected
domain-specific parallel dataset with the generic
</bodyText>
<figure confidence="0.999284545454545">
Generic dataset
EMEA dataset
(1) perplexity
(2) tf-idf
(3) infreq.
(4) related w.
(5) n-gram
(5) n (3)
(5) n (4)
(3) n (4)
(5) n (4) n (3)
</figure>
<page confidence="0.992714">
714
</page>
<table confidence="0.999807666666667">
Dataset Type BLEU-2 BLEU-4 METEOR
Generic dataset 17.2 6.6 24.7
(5)n(4)n(3) sent. selec. 20.1 8.9* 27.2*
Data Concatenation (i) 18.1 6.8 24.1
Log-linear Models (ii) 18.9 8.1* 25.3
Fill-Up Model (iii) 17.7 7.0 24.7
(5)n(4)n(3) + IATE 19.8 9.0* 27.8*
(5)n(4)n(3) + DBpedia(1) 20.6 9.1* 27.3*
(5)n(4)n(3) + DBpedia(2) 21.0 9.6*0 28.2*0
</table>
<tableCaption confidence="0.8348885">
Table 3: Evaluation of the ICD ontology eval-
uation dataset combining domain-specific with
</tableCaption>
<bodyText confidence="0.993552625">
generic parallel knowledge and lexical enhance-
ment of SMT using IATE and DBpedia (bold
results = best performance; *statistically signifi-
cant compared to baseline; ✸statistically signifi-
cant compared to best sentence selection model)
parallel one, (ii) combining the generated transla-
tion models from the selected domain-specific par-
allel dataset and the generic corpus and (iii) apply-
ing the Fill-Up model to emphasise the domain-
specific data in a single translation model. The
translation performance of the combination meth-
ods are shown in Table 3. It is interesting to
notice that none of them benefits from the use
of the additional generic parallel data showing
translation performance smaller than the domain-
specific model. Although all methods outperform
the generic translation model, only the log-linear
approach, keeping in- and out-domain translation
models separated, shows significant improvement.
Comparing it to the combined sentence selec-
tion technique ((5)∧(4)∧(3)) does not show any
statistical significant differences between the ap-
proaches. We conclude that the generic corpus
is too large compared to the selected in-domain
corpus, nullifying the influence of the extracted
domain-specific parallel knowledge.
Lexical enhancement for SMT Since the out-
of-vocabulary problem can be only mitigated
with sentence selection, we accessed lexical re-
sources IATE and DBpedia to further improve
the translations of the medical labels. Based on
the word overlap between labels and entries in
IATE we extracted 11,641 English lexical entries
with its equivalent in German. The DBpedia(1)
approach, which disambiguates DBpedia entries
based on the (Wikipedia article) categories (Ar-
can et al., 2012), identified 7,911 English-German
expression for the targeted domain, while the ab-
stract based disambiguation approach, marked as
DBpedia(2) in Table 3 identified 3,791 bilingual
entries. The lexical enhanced models further im-
proved the translations of the medical labels (last
three rows in Table 3) due to the additional bilin-
gual information from the lexical resources, which
is missing in the standalone sentence selection
model. Comparing the ICD evaluation dataset
and the translations generated with the DBpedia(2)
lexical enhanced model we observed that more
than 80 labels benefit from the additional lexi-
cal knowledge, e.g. correcting the mistranslated
”adrenal gland” into ”Nebenniere”. The lexical
extraction and disambiguation of bilingual knowl-
edge based on the abstract of the article compared
to the article categories further improves the lex-
ical choice, helping SMT systems to improve the
translation of ontology labels.
</bodyText>
<subsectionHeader confidence="0.998827">
5.2 Manual Evaluation of Translated Labels
</subsectionHeader>
<bodyText confidence="0.999988666666666">
Since ontologies store specific vocabulary about a
domain, this vocabulary is adapted to a concrete
language and culture community (Cimiano et al.,
2010). In order to investigate to what extent the
automatically generated translations differ from a
translator’s adapted point of view, we manually in-
spected the translations produced by the sentence
selection approaches described in Section 5.1.
While analysing the English and German part of
the ICD ontology gold standard we noticed signif-
icant differences in the translations of the medical
labels. As a result of the language and cultural
adaptation, many labels in the ICD ontology were
not always translated literally, i.e. parts of a la-
bel were semantically merged, omitted or new in-
formation was added while crossing the language
border. For example, the ICD label ”acute kid-
ney failure and chronic kidney disease” is stored
in the German part of the ontology as ”Nierenin-
suffizienz”.16 Although none of the translation
systems can generate the compounded medical
expression for German, the SMT system gener-
ated nevertheless an acceptable translation, i.e.
”akutes Nierenversagen und chronischer Nieren-
erkrankungen”.17 A more extreme example is the
English label ”slipping, tripping, stumbling and
falls”, in the German ICD ontology represented as
</bodyText>
<footnote confidence="0.99967725">
16Niereninsuffizienz+—kidney insufficiency
17akutes+—acute, Nierenversagen+—kidney failure,
und+—and, chronischer+—chronic,
Nierenerkrankungen+—kidney disease
</footnote>
<page confidence="0.997415">
715
</page>
<bodyText confidence="0.999853177777778">
”sonstige St¨urze auf gleicher Ebene”.18 The lan-
guage and cultural adaptation is very active for this
example, where the whole English label is seman-
tically merged into the word ”St¨urze”, meaning
”falls”. Additionally, the German part holds more
information within the label, i.e. ”auf gleicher
Ebene” (en. ”at the same level”), which is not
represented on the English side. Since the SMT
system will always try to translate every phrase
(word or word segments) into the target language,
an automatic translation evaluation cannot reflect
the overall SMT performance.
Further we detected a large error class caused by
compounding, a common linguistic feature of Ger-
man. Although the phrase ”heart diseases” with its
reference translation ”Herzkrankheiten” appears
frequent in the generic training dataset, the SMT
system prefers to translate it word by word into
”Herz Krankheiten”. 19 Similar observations were
made with ”upper arm” (German ”Oberarm”) with
the SMT word to word translation ”oberen Arm”.
Finally, we analysed the impact of the seman-
tically enriched sentence selection with related
words coming from Word2Vec compared to the
surface based sentence selection, e.g. preplex-
ity, infrequent n-gram recovery or n-gram overlap.
Since semantically enriched selection stored the
most relevant sentences, we observed the correct
translation of the label ”blood vessels” into ”Blut-
gef¨aße”. The generic and other surface based se-
lections translated the expression individually into
”Blut Schiffe”, where ”Schiffe” refers to the more
common English word ”ship”, but not to ’part
of the system transporting blood throughout our
body’. The last example illustrates further the se-
mantic mismatch between the training domain and
the test domain. Using the generic model, built
mainly out of European laws and parliament dis-
cussions (JRC-Acquis/Europarl) the word ”head”
inside the label ”injury of head” is wrongly trans-
lated into the word ”Leiter”, meaning ”leader” in
the legal domain. Nevertheless, the additional se-
mantic information prevents storing wrong paral-
lel sentences and guides the SMT to the correct
translation, i.e. ”Sch¨adigung des Kopfes”.20
</bodyText>
<footnote confidence="0.932299">
18sonstige+—other, St¨urze+—falls, auf+—on,
gleicher+—same, Ebene+—level
19Herz+—heart, Krankheiten+—diseases
20Sch¨adigung+—injury, des+—of, Kopfes+—head
</footnote>
<sectionHeader confidence="0.994244" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99986628125">
In this paper we presented an approach to identify
the most relevant sentences from a large generic
parallel corpus, giving the possibility to translate
highly specific ontology labels without particular
in-domain parallel data. We enhanced furthermore
the translation system build on the in-domain par-
allel knowledge with additional lexical knowledge
accessing DBpedia. With the aim to better se-
lect relevant bilingual knowledge for SMT, we ex-
tend previous sentence and lexical selection tech-
niques with additional semantic knowledge. Our
proposed ontology-specific SMT system showed a
statistical significant improvement (up to 3 BLEU
points) of ontology label translation over the com-
pared translation approaches.
In future, we plan to integrate a larger diversity
of surface, semantic and linguistic information for
relevant sentence selection. Although the SMT
system is capable of translating several words into
a compound word, the small amount of the se-
lected sentences limits this capability. To improve
the ontology label translations, we therefore see
the need to focus more on the German compound
feature. Additionally we observed that more than
25% of the identified lexical knowledge consists
of multi-word-expressions, e.g. ”fatal familial in-
somnia”. For this reason, our ongoing work fo-
cuses on the alignment of nested knowledge inside
those expressions. To move further in this direc-
tion, we plan to focus on exploiting morphological
term variations taking advantage of the alternative
terms provided by DBpedia.
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999859166666667">
This publication has emanated from research sup-
ported in part by a research grant from Science
Foundation Ireland (SFI) under Grant Number
SFI/12/RC/2289 (Insight) and the European Union
supported projects LIDER (ICT-2013.4.1-610782)
and MixedEmotions (H2020-644632).
</bodyText>
<sectionHeader confidence="0.998955" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996538333333333">
Arcan, M., Federmann, C., and Buitelaar, P. (2012).
Experiments with term translation. In Proceedings
of the 24th International Conference on Computa-
tional Linguistics, Mumbai, India.
Arcan, M., Giuliano, C., Turchi, M., and Buite-
laar, P. (2014a). Identification of Bilingual Terms
</reference>
<page confidence="0.992291">
716
</page>
<reference confidence="0.999048096153847">
from Monolingual Documents for Statistical Ma-
chine Translation. In Proceedings of the 4th Inter-
national Workshop on Computational Terminology
(Computerm), Dublin, Ireland.
Arcan, M., Turchi, M., Tonelli, S., and Buitelaar, P.
(2014b). Enhancing statistical machine translation
with bilingual terminology in a cat environment. In
Proceedings of the 11th Conference of the Associa-
tion for Machine Translation in the Americas, Van-
couver, Canada.
Axelrod, A., He, X., and Gao, J. (2011). Domain
adaptation via pseudo in-domain data selection. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ’11,
Stroudsburg, PA, USA.
Bertoldi, N., Cettolo, M., and Federico, M. (2013).
Cache-based Online Adaptation for Machine Trans-
lation Enhanced Computer Assisted Translation. In
Proceedings of Machine Translation Summit XIV,
Nice, France.
Bicici, E. and Yuret, D. (2011). Instance selection for
machine translation using feature decay algorithms.
In Proceedings of the Sixth Workshop on Statistical
Machine Translation, Edinburgh, Scotland.
Bisazza, A., Ruiz, N., and Federico, M. (2011). Fill-up
versus Interpolation Methods for Phrase-based SMT
Adaptation. In Proceedings of IWSLT.
Bouamor, D., Semmar, N., and Zweigenbaum, P.
(2012). Identifying bilingual multi-word expres-
sions for statistical machine translation. In Proceed-
ings of the Eight International Conference on Lan-
guage Resources and Evaluation, Istanbul, Turkey.
Chandrasekaran, B., Josephson, J. R., and Benjamins,
V. R. (1999). What are ontologies, and why do we
need them? IEEE Intelligent Systems, 14(1):20–26.
Cimiano, P., Montiel-Ponsoda, E., Buitelaar, P., Es-
pinoza, M., and G´omez-P´erez, A. (2010). A note on
ontology localization. Appl. Ontol., 5(2):127–137.
Clark, J., Dyer, C., Lavie, A., and Smith, N. (2011).
Better Hypothesis Testing for Statistical Machine
Translation: Controlling for Optimizer Instability .
In Proceedings of the Association for Computational
Lingustics.
Cuong, H. and Sima’an, K. (2014). Latent domain
translation models in mix-of-domains haystack. In
Proceedings of the 25th International Conference on
Computational Linguistics, Dublin, Ireland.
Declerck, T., P´erez, A. G., Vela, O., Gantner, Z., Man-
zano, D., and D-Saarbr¨ucken (2006). Multilingual
lexical semantic resources for ontology translation.
In In Proceedings of the 5th International Confer-
ence on Language Resources and Evaluation.
Denkowski, M. and Lavie, A. (2014). Meteor univer-
sal: Language specific translation evaluation for any
target language. In Proceedings of the EACL 2014
Workshop on Statistical Machine Translation.
Eck, M., Vogel, S., and Waibel, A. (2004). Language
model adaptation for statistical machine translation
based on information retrieval. In Proc. of LREC.
Erdmann, M., Nakayama, K., Hara, T., and Nishio, S.
(2009). Improving the extraction of bilingual ter-
minology from wikipedia. ACM Trans. Multimedia
Comput. Commun. Appl., 5(4).
Espinoza, M., Montiel-Ponsoda, E., and G´omez-P´erez,
A. (2009). Ontology localization. In Proceedings
of the Fifth International Conference on Knowledge
Capture, K-CAP ’09, New York, NY, USA. ACM.
Fu, B., Brennan, R., and O’Sullivan, D. (2009). Cross-
lingual ontology mapping - an investigation of the
impact of machine translation. In G´omez-P´erez, A.,
Yu, Y., and Ding, Y., editors, ASWC, volume 5926
of Lecture Notes in Computer Science. Springer.
Gasc´o, G., Rocha, M.-A., Sanchis-Trilles, G., Andr´es-
Ferrer, J., and Casacuberta, F. (2012). Does more
data always yield better translations? In Proceed-
ings of the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics,
EACL ’12, Stroudsburg, PA, USA.
G´omez-P´erez, A., Vila-Suero, D., Montiel-Ponsoda,
E., Gracia, J., and Aguado-de Cea, G. (2013).
Guidelines for multilingual linked data. In Proceed-
ings of the 3rd International Conference on Web In-
telligence, Mining and Semantics. ACM.
Gracia, J., Montiel-Ponsoda, E., Cimiano, P., G´omez-
P´erez, A., Buitelaar, P., and McCrae, J. (2012).
Challenges for the multilingual web of data. Web Se-
mantics: Science, Services and Agents on the World
Wide Web, 11.
Haddow, B. and Koehn, P. (2012). Analysing the Ef-
fect of Out-of-Domain Data on SMT Systems. In
Proceedings of the Seventh Workshop on Statistical
Machine Translation, Montr´eal, Canada.
Harris, Z. (1954). Distributional structure. Word,
10(23).
Hildebrand, A. S., Eck, M., Vogel, S., and Waibel, A.
(2005). Adaptation of the translation model for sta-
tistical machine translation based on information re-
trieval. In Proceedings of the 10th Conference of
the European Association for Machine Translation
(EAMT), Budapest.
Kirchhoff, K. and Bilmes, J. (2014). Submodularity
for data selection in machine translation. In Em-
pirical Methods in Natural Language Processing
(EMNLP).
</reference>
<page confidence="0.969092">
717
</page>
<reference confidence="0.999775036363636">
Koehn, P. (2005). Europarl: A Parallel Corpus for Sta-
tistical Machine Translation. In Conference Pro-
ceedings: the tenth Machine Translation Summit,
pages 79–86. AAMT.
Koehn, P., Hoang, H., Birch, A., Callison-Burch, C.,
Federico, M., Bertoldi, N., Cowan, B., Shen, W.,
Moran, C., Zens, R., Dyer, C., Bojar, O., Constantin,
A., and Herbst, E. (2007). Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
Stroudsburg, PA, USA.
Langlais, P. (2002). Improving a general-purpose
statistical translation engine by terminological lex-
icons. In Proceedings of the 2nd International
Workshop on Computational Terminology (COM-
PUTERM) ’2002, Taipei, Taiwan.
Lehmann, J., Isele, R., Jakob, M., Jentzsch, A., Kon-
tokostas, D., Mendes, P. N., Hellmann, S., Morsey,
M., van Kleef, P., Auer, S., and Bizer, C. (2015).
DBpedia - a large-scale, multilingual knowledge
base extracted from wikipedia. Semantic Web Jour-
nal, 6(2):167–195.
L¨u, Y., Huang, J., and Liu, Q. (2007). Improving sta-
tistical machine translation performance by training
data selection and optimization. In Proceedings of
the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL).
McCrae, J., Espinoza, M., Montiel-Ponsoda, E.,
Aguado-de Cea, G., and Cimiano, P. (2011). Com-
bining statistical and semantic approaches to the
translation of ontologies and taxonomies. In Fifth
workshop on Syntax, Structure and Semantics in Sta-
tistical Translation (SSST-5).
Mikolov, T., Chen, K., Corrado, G., and Dean, J.
(2013). Efficient estimation of word representations
in vector space. CoRR, abs/1301.3781.
Moore, R. C. and Lewis, W. (2010). Intelligent se-
lection of language model training data. In Pro-
ceedings of the ACL 2010 Conference Short Papers,
ACLShort ’10, Stroudsburg, PA, USA.
Niehues, J. and Waibel, A. (2011). Using Wikipedia
to Translate Domain-specific Terms in SMT. In In-
ternational Workshop on Spoken Language Transla-
tion, San Francisco, CA, USA.
Och, F. J. and Ney, H. (2003). A systematic compari-
son of various statistical alignment models. Compu-
tational Linguistics, 29.
O’Riain, S., Coughlan, B., Buitelaar, P., Declerck, T.,
Krieger, U., and Thomas, S. M. (2013). Cross-
lingual querying and comparison of linked financial
and business data. In Cimiano, P., Fern´andez, M.,
Lopez, V., Schlobach, S., and V¨olker, J., editors,
ESWC (Satellite Events), volume 7955 of Lecture
Notes in Computer Science. Springer.
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J.
(2002). BLEU: a method for automatic evaluation
of machine translation. In Proceedings of the 40th
Annual Meeting on Association for Computational
Linguistics, ACL ’02, pages 311–318.
Pinnis, M., Ljubeˇsi´c, N., S¸tef˘anescu, D., Skadin¸a, I.,
Tadi´c, M., and Gornostay, T. (2012). Term extrac-
tion, tagging, and mapping tools for under-resourced
languages. In Proceedings of the Terminology and
Knowledge Engineering (TKE2012) Conference.
Ren, Z., L¨u, Y., Cao, J., Liu, Q., and Huang, Y. (2009).
Improving statistical machine translation using do-
main bilingual multiword expressions. In Proceed-
ings of the Workshop on Multiword Expressions:
Identification, Interpretation, Disambiguation and
Applications, MWE ’09, Stroudsburg, PA, USA.
Steinberger, R., Pouliquen, B., Widiger, A., Ignat, C.,
Erjavec, T., Tufis, D., and Varga, D. (2006). The
JRC-Acquis: A multilingual aligned parallel corpus
with 20+ languages. In Proceedings of the 5th In-
ternational Conference on Language Resources and
Evaluation (LREC’2006).
Stolcke, A. (2002). SRILM - An extensible lan-
guage modeling toolkit. In Proceedings Interna-
tional Conference on Spoken Language Processing.
Tiedemann, J. (2009). News from OPUS - A collection
of multilingual parallel corpora with tools and inter-
faces. In Nicolov, N., Bontcheva, K., Angelova, G.,
and Mitkov, R., editors, Recent Advances in Natural
Language Processing, volume V. John Benjamins,
Amsterdam/Philadelphia, Borovets, Bulgaria.
Tiedemann, J. (2012). Parallel data, tools and inter-
faces in opus. In Chair), N. C. C., Choukri, K., De-
clerck, T., Do˘gan, M. U., Maegaard, B., Mariani,
J., Odijk, J., and Piperidis, S., editors, Proceedings
of the Eight International Conference on Language
Resources and Evaluation, Istanbul, Turkey.
Tyers, F. M. and Pieanaar, J. A. (2008). Extracting
bilingual word pairs from wikipedia. In Collabora-
tion: interoperability between people in the creation
of language resources for less-resourced languages
(A SALTMIL workshop).
Wu, H., Wang, H., and Zong, C. (2008). Domain adap-
tation for statistical machine translation with domain
dictionary and monolingual corpora. In Proceedings
of the 22nd International Conference on Computa-
tional Linguistics - Volume 1, COLING ’08.
Wuebker, J., Ney, H., Martinez-Villaronga, A.,
Gim´enez, A., , Juan, A., Servan, C., Dymetman, M.,
and Mirkin, S. (2014). Comparison of Data Selec-
tion Techniques for the Translation of Video Lec-
tures. In Proc. of the Eleventh Biennial Conf. of the
Association for Machine Translation in the Ameri-
cas (AMTA-2014), Vancouver (Canada).
</reference>
<page confidence="0.99479">
718
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.474490">
<title confidence="0.999655">Knowledge Portability with Semantic Expansion of Ontology Labels</title>
<author confidence="0.99495">Paul</author>
<affiliation confidence="0.999918">Centre for Data Analytics, National University of Ireland,</affiliation>
<email confidence="0.984735">firstname.lastname@insight-centre.org</email>
<affiliation confidence="0.490677">2FBK- Fondazione Bruno Kessler, Via Sommarive 18, 38123 Trento,</affiliation>
<email confidence="0.995409">turchi@fbk.eu</email>
<abstract confidence="0.999567413793103">Our research focuses on the multilingual enhancement of ontologies that, often represented only in English, need to be translated in different languages to enable knowledge access across languages. Ontology translation is a rather different task then the classic document translation, because ontologies contain highly specific vocabulary and they lack contextual information. For these reasons, to improve automatic ontology translations, we first focus on identifying relevant unambiguous and domain-specific sentences from a large set of generic parallel corpora. Then, we leverage Linked Open Data resources, such as DBPedia, to isolate ontologyspecific bilingual lexical knowledge. In both cases, we take advantage of the semantic information of the labels to select relevant bilingual data with the aim of building an ontology-specific statistical machine translation system. We evaluate our approach on the translation of a medical ontology, translating from English into German. Our experiment shows a significant improvement of around 3 BLEU points compared to a generic as well as a domain-specific translation approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Arcan</author>
<author>C Federmann</author>
<author>P Buitelaar</author>
</authors>
<title>Experiments with term translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 24th International Conference on Computational Linguistics,</booktitle>
<location>Mumbai, India.</location>
<contexts>
<context position="15968" citStr="Arcan et al., 2012" startWordPosition="2463" endWordPosition="2466">ctors passed to the cosine similarity. The most similar source sentence and the label should share the largest number of related words (largest cosine similarity). 3.2 Bilingual Terminology Identification The automatic translation of domain-specific vocabulary can be a hard task for a generic SMT system, if the bilingual knowledge is not present in the parallel dataset. To complement the previous approaches we access DBpedia6 as a multilingual lexical resource. 4Wikipedia dump id enwiki-20141106 5https://code.google.com/p/word2vec/ 6http://wiki.dbpedia.org/Downloads2014 We engage the idea of (Arcan et al., 2012) where the authors provide to the SMT system unambiguous terminology identified in Wikipedia to improve the translations of labels in the financial domain. To disambiguate Wikipedia entries with translations into different domains, they query the repository for analysing the n-gram overlap between the financial labels and the Wikipedia entries and store the frequency of categories which are associated with the matched entry. In a final step they extract only bilingual Wikipedia entries, which are associated with the most frequent Wikipedia categories identified in the previous step. Since the </context>
<context position="33599" citStr="Arcan et al., 2012" startWordPosition="5170" endWordPosition="5174"> corpus is too large compared to the selected in-domain corpus, nullifying the influence of the extracted domain-specific parallel knowledge. Lexical enhancement for SMT Since the outof-vocabulary problem can be only mitigated with sentence selection, we accessed lexical resources IATE and DBpedia to further improve the translations of the medical labels. Based on the word overlap between labels and entries in IATE we extracted 11,641 English lexical entries with its equivalent in German. The DBpedia(1) approach, which disambiguates DBpedia entries based on the (Wikipedia article) categories (Arcan et al., 2012), identified 7,911 English-German expression for the targeted domain, while the abstract based disambiguation approach, marked as DBpedia(2) in Table 3 identified 3,791 bilingual entries. The lexical enhanced models further improved the translations of the medical labels (last three rows in Table 3) due to the additional bilingual information from the lexical resources, which is missing in the standalone sentence selection model. Comparing the ICD evaluation dataset and the translations generated with the DBpedia(2) lexical enhanced model we observed that more than 80 labels benefit from the a</context>
</contexts>
<marker>Arcan, Federmann, Buitelaar, 2012</marker>
<rawString>Arcan, M., Federmann, C., and Buitelaar, P. (2012). Experiments with term translation. In Proceedings of the 24th International Conference on Computational Linguistics, Mumbai, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Arcan</author>
<author>C Giuliano</author>
<author>M Turchi</author>
<author>P Buitelaar</author>
</authors>
<title>Identification of Bilingual Terms from Monolingual Documents for Statistical Machine Translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 4th International Workshop on Computational Terminology (Computerm),</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="11164" citStr="Arcan et al., 2014" startWordPosition="1703" endWordPosition="1706">its rich semantic knowledge was used as a resource for bilingual term identification in the context of SMT. (Tyers and Pieanaar, 2008) extracts bilingual dictionary entries from Wikipedia to support the machine translation system. Based on exact string matching they query Wikipedia with a list of around 10,000 noun lemmas to generate the bilingual dictionary. Besides the interwiki link system, (Erdmann et al., 2009) enhance their bilingual dictionary by using redirection page titles and anchor text within Wikipedia. To cast the problem of ambiguous Wikipedia titles, (Niehues and Waibel, 2011; Arcan et al., 2014a) use the information of Wikipedia categories and the text of the articles to provide the SMT system domain-specific bilingual knowledge. This research showed that using the lexical information stored in this knowledge base improves the translation of highly domain-specific vocabulary. However, we do not rely on category annotations of Wikipedia articles, but perform domain-specific dictionary generation based on the overlap between related words from the ontology label and the abstract of a Wikipedia article. 3 Methodology We propose an approach that uses the ontology labels to be translated</context>
<context position="19664" citStr="Arcan et al., 2014" startWordPosition="3052" endWordPosition="3055">es from the domain-specific translation model. We engage the idea of the Fill-Up model to combine the domain-specific parallel knowledge from the selected sentences with the generic (1.9M) parallel corpus. Furthermore, for embedding bilingual lexical knowledge into the SMT system, we engage the idea of cache-based translation and language models (Bertoldi et al., 2013). The main idea behind these models is to combine a large static global model with a small, but dynamic local model. This approach has already shown its potential of injecting domain-specific knowledge into a generic SMT system (Arcan et al., 2014b). For our experiments we inject the bilingual lexical knowledge identified in DBpedia and IATE into the cachebased models. The cache-based model relies on a local translation model (CBTM) and language model (CBLM). The first is implemented as an additional table in the translation model providing one score. All entries are associated with an ’age’ (initially set to 1), corresponding to the time when they were actually inserted. Each new insertion causes an ageing of the existing translation candidates and hence their re-scoring; in case of re-insertion of a phrase pair, the old value is set </context>
</contexts>
<marker>Arcan, Giuliano, Turchi, Buitelaar, 2014</marker>
<rawString>Arcan, M., Giuliano, C., Turchi, M., and Buitelaar, P. (2014a). Identification of Bilingual Terms from Monolingual Documents for Statistical Machine Translation. In Proceedings of the 4th International Workshop on Computational Terminology (Computerm), Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Arcan</author>
<author>M Turchi</author>
<author>S Tonelli</author>
<author>P Buitelaar</author>
</authors>
<title>Enhancing statistical machine translation with bilingual terminology in a cat environment.</title>
<date>2014</date>
<booktitle>In Proceedings of the 11th Conference of the Association for Machine Translation in the Americas,</booktitle>
<location>Vancouver, Canada.</location>
<contexts>
<context position="11164" citStr="Arcan et al., 2014" startWordPosition="1703" endWordPosition="1706">its rich semantic knowledge was used as a resource for bilingual term identification in the context of SMT. (Tyers and Pieanaar, 2008) extracts bilingual dictionary entries from Wikipedia to support the machine translation system. Based on exact string matching they query Wikipedia with a list of around 10,000 noun lemmas to generate the bilingual dictionary. Besides the interwiki link system, (Erdmann et al., 2009) enhance their bilingual dictionary by using redirection page titles and anchor text within Wikipedia. To cast the problem of ambiguous Wikipedia titles, (Niehues and Waibel, 2011; Arcan et al., 2014a) use the information of Wikipedia categories and the text of the articles to provide the SMT system domain-specific bilingual knowledge. This research showed that using the lexical information stored in this knowledge base improves the translation of highly domain-specific vocabulary. However, we do not rely on category annotations of Wikipedia articles, but perform domain-specific dictionary generation based on the overlap between related words from the ontology label and the abstract of a Wikipedia article. 3 Methodology We propose an approach that uses the ontology labels to be translated</context>
<context position="19664" citStr="Arcan et al., 2014" startWordPosition="3052" endWordPosition="3055">es from the domain-specific translation model. We engage the idea of the Fill-Up model to combine the domain-specific parallel knowledge from the selected sentences with the generic (1.9M) parallel corpus. Furthermore, for embedding bilingual lexical knowledge into the SMT system, we engage the idea of cache-based translation and language models (Bertoldi et al., 2013). The main idea behind these models is to combine a large static global model with a small, but dynamic local model. This approach has already shown its potential of injecting domain-specific knowledge into a generic SMT system (Arcan et al., 2014b). For our experiments we inject the bilingual lexical knowledge identified in DBpedia and IATE into the cachebased models. The cache-based model relies on a local translation model (CBTM) and language model (CBLM). The first is implemented as an additional table in the translation model providing one score. All entries are associated with an ’age’ (initially set to 1), corresponding to the time when they were actually inserted. Each new insertion causes an ageing of the existing translation candidates and hence their re-scoring; in case of re-insertion of a phrase pair, the old value is set </context>
</contexts>
<marker>Arcan, Turchi, Tonelli, Buitelaar, 2014</marker>
<rawString>Arcan, M., Turchi, M., Tonelli, S., and Buitelaar, P. (2014b). Enhancing statistical machine translation with bilingual terminology in a cat environment. In Proceedings of the 11th Conference of the Association for Machine Translation in the Americas, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Axelrod</author>
<author>X He</author>
<author>J Gao</author>
</authors>
<title>Domain adaptation via pseudo in-domain data selection.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="9284" citStr="Axelrod et al., 2011" startWordPosition="1416" endWordPosition="1419"> for the language model adaptation within SMT the information retrieval technique tf-idf. Similarly, (Hildebrand et al., 2005) and (L¨u et al., 2007) utilized this approach to select 709 relevant sentences from available parallel text to adapt translation models. The results confirmed that large amounts of generic training data cannot compensate for the requirement of domainspecific training sentences. Another approach is taken by (Moore and Lewis, 2010), where, based on source and target language models, the authors calculated the difference of the cross-entropy values for a given sentence. (Axelrod et al., 2011) extend this work using the bilingual difference of cross-entropy on in-domain and out-of-domain language models for training sentence selection for SMT. (Wuebker et al., 2014) reused the crossentropy approach and applied it to the translation of video lectures. (Kirchhoff and Bilmes, 2014) introduce submodular optimization using complex features for parallel sentence selection. In their experiments they use the source and target side of the text to be translated, and show significant improvements over the widely used cross-entropy method. A different approach for sentence selection is shown i</context>
</contexts>
<marker>Axelrod, He, Gao, 2011</marker>
<rawString>Axelrod, A., He, X., and Gao, J. (2011). Domain adaptation via pseudo in-domain data selection. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Bertoldi</author>
<author>M Cettolo</author>
<author>M Federico</author>
</authors>
<title>Cache-based Online Adaptation for Machine Translation Enhanced Computer Assisted Translation.</title>
<date>2013</date>
<booktitle>In Proceedings of Machine Translation Summit XIV,</booktitle>
<location>Nice, France.</location>
<contexts>
<context position="18274" citStr="Bertoldi et al., 2013" startWordPosition="2829" endWordPosition="2832">ion of new obtained knowledge can be performed by retraining the domain-specific knowledge with the generic parallel corpus (Langlais, 2002; Ren et al., 2009; Haddow and Koehn, 2012) or by adding new entries directly to the translation system (Pinnis et al., 2012; Bouamor et al., 2012). These methods have the drawback that the bilingual domain specificity may get lost due to the usually larger generic parallel corpora. Giving more priority to domain-specific translations than generic ones, we focus on two techniques, i.e. the Fill-Up model (Bisazza et al., 2011) and the Cache-Based 711 Model (Bertoldi et al., 2013) approach. The Fill-Up model has been developed to address a common scenario where a large generic background model exists, and only a small quantity of domain-specific data can be used to build a translation model. Its goal is to leverage the large coverage of the background model, while preserving the domain-specific knowledge coming from the domain-specific data. For this purpose the generic and the domain-specific translation models are merged. For those translation candidates that appear in both models, only one instance is reported in the Fill-Up model with the largest probabilities acco</context>
</contexts>
<marker>Bertoldi, Cettolo, Federico, 2013</marker>
<rawString>Bertoldi, N., Cettolo, M., and Federico, M. (2013). Cache-based Online Adaptation for Machine Translation Enhanced Computer Assisted Translation. In Proceedings of Machine Translation Summit XIV, Nice, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Bicici</author>
<author>D Yuret</author>
</authors>
<title>Instance selection for machine translation using feature decay algorithms.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<location>Edinburgh, Scotland.</location>
<contexts>
<context position="10081" citStr="Bicici and Yuret, 2011" startWordPosition="1536" endWordPosition="1540">4) reused the crossentropy approach and applied it to the translation of video lectures. (Kirchhoff and Bilmes, 2014) introduce submodular optimization using complex features for parallel sentence selection. In their experiments they use the source and target side of the text to be translated, and show significant improvements over the widely used cross-entropy method. A different approach for sentence selection is shown in (Cuong and Sima’an, 2014), where the authors propose a latent domain translation model to distinguish between hidden in- and out-of-domain data. (Gasc´o et al., 2012) and (Bicici and Yuret, 2011) sub-sample sentence pairs whose source has most overlap with the evaluation dataset. Different from these approaches, we do not embed any specific in-domain knowledge to the generic corpus, from which sentence selection is performed. Furthermore, none of these methods explicitly exploit the ontological hierarchy for label disambiguation and are not specifically designed to deal with the characteristics of ontology labels. As a lexical resource, Wikipedia with its rich semantic knowledge was used as a resource for bilingual term identification in the context of SMT. (Tyers and Pieanaar, 2008) </context>
</contexts>
<marker>Bicici, Yuret, 2011</marker>
<rawString>Bicici, E. and Yuret, D. (2011). Instance selection for machine translation using feature decay algorithms. In Proceedings of the Sixth Workshop on Statistical Machine Translation, Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bisazza</author>
<author>N Ruiz</author>
<author>M Federico</author>
</authors>
<title>Fill-up versus Interpolation Methods for Phrase-based SMT Adaptation.</title>
<date>2011</date>
<booktitle>In Proceedings of IWSLT.</booktitle>
<contexts>
<context position="18220" citStr="Bisazza et al., 2011" startWordPosition="2820" endWordPosition="2823">rated into the workflow of the SMT system. The injection of new obtained knowledge can be performed by retraining the domain-specific knowledge with the generic parallel corpus (Langlais, 2002; Ren et al., 2009; Haddow and Koehn, 2012) or by adding new entries directly to the translation system (Pinnis et al., 2012; Bouamor et al., 2012). These methods have the drawback that the bilingual domain specificity may get lost due to the usually larger generic parallel corpora. Giving more priority to domain-specific translations than generic ones, we focus on two techniques, i.e. the Fill-Up model (Bisazza et al., 2011) and the Cache-Based 711 Model (Bertoldi et al., 2013) approach. The Fill-Up model has been developed to address a common scenario where a large generic background model exists, and only a small quantity of domain-specific data can be used to build a translation model. Its goal is to leverage the large coverage of the background model, while preserving the domain-specific knowledge coming from the domain-specific data. For this purpose the generic and the domain-specific translation models are merged. For those translation candidates that appear in both models, only one instance is reported in</context>
</contexts>
<marker>Bisazza, Ruiz, Federico, 2011</marker>
<rawString>Bisazza, A., Ruiz, N., and Federico, M. (2011). Fill-up versus Interpolation Methods for Phrase-based SMT Adaptation. In Proceedings of IWSLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bouamor</author>
<author>N Semmar</author>
<author>P Zweigenbaum</author>
</authors>
<title>Identifying bilingual multi-word expressions for statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Eight International Conference on Language Resources and Evaluation,</booktitle>
<location>Istanbul, Turkey.</location>
<contexts>
<context position="17938" citStr="Bouamor et al., 2012" startWordPosition="2775" endWordPosition="2778">dia entry has an equivalent in the target language, i.e. German, we use the bilingual information for the lexical enhancement of the SMT system. 3.3 Integration of Domain-Specific Knowledge into SMT After the identification of domain-specific bilingual knowledge, it has to be integrated into the workflow of the SMT system. The injection of new obtained knowledge can be performed by retraining the domain-specific knowledge with the generic parallel corpus (Langlais, 2002; Ren et al., 2009; Haddow and Koehn, 2012) or by adding new entries directly to the translation system (Pinnis et al., 2012; Bouamor et al., 2012). These methods have the drawback that the bilingual domain specificity may get lost due to the usually larger generic parallel corpora. Giving more priority to domain-specific translations than generic ones, we focus on two techniques, i.e. the Fill-Up model (Bisazza et al., 2011) and the Cache-Based 711 Model (Bertoldi et al., 2013) approach. The Fill-Up model has been developed to address a common scenario where a large generic background model exists, and only a small quantity of domain-specific data can be used to build a translation model. Its goal is to leverage the large coverage of th</context>
</contexts>
<marker>Bouamor, Semmar, Zweigenbaum, 2012</marker>
<rawString>Bouamor, D., Semmar, N., and Zweigenbaum, P. (2012). Identifying bilingual multi-word expressions for statistical machine translation. In Proceedings of the Eight International Conference on Language Resources and Evaluation, Istanbul, Turkey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Chandrasekaran</author>
<author>J R Josephson</author>
<author>V R Benjamins</author>
</authors>
<title>What are ontologies, and why do we need them?</title>
<date>1999</date>
<journal>IEEE Intelligent Systems,</journal>
<volume>14</volume>
<issue>1</issue>
<contexts>
<context position="3234" citStr="Chandrasekaran et al., 1999" startWordPosition="464" endWordPosition="467">ish, German or Italian. The main challenge involved in building multilingual knowledge bases is, however, to bridge the gap between language-specific information and the language-independent semantic content of ontologies or taxonomies (Gracia et al., 2012). Since manual multilingual enhancement of ontologies is a very time consuming and expensive process, we engage an ontology-specific statistical machine translation (SMT) system to automatically translate the ontology labels. Due to the fact that ontology labels are usually highly domainspecific and stored only in knowledge representations (Chandrasekaran et al., 1999), the labels appear infrequent in parallel corpora, which are needed to build a domain-specific translation system with accurate translation candidates. Additionally, ambiguous labels built out of only a few words do often not express enough semantic or contextual information to guide the SMT system to translate a label into the targeted domain. This can be observed by domain-unadapted SMT systems, e.g. Google Translate, where ambiguous expressions, such as vessel stored in an medical ontology, are often translated into a generic do2ontology label stored in FINREP - FINancial REPorting 708 Pro</context>
</contexts>
<marker>Chandrasekaran, Josephson, Benjamins, 1999</marker>
<rawString>Chandrasekaran, B., Josephson, J. R., and Benjamins, V. R. (1999). What are ontologies, and why do we need them? IEEE Intelligent Systems, 14(1):20–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Cimiano</author>
<author>E Montiel-Ponsoda</author>
<author>P Buitelaar</author>
<author>M Espinoza</author>
<author>A G´omez-P´erez</author>
</authors>
<title>A note on ontology localization.</title>
<date>2010</date>
<journal>Appl. Ontol.,</journal>
<volume>5</volume>
<issue>2</issue>
<marker>Cimiano, Montiel-Ponsoda, Buitelaar, Espinoza, G´omez-P´erez, 2010</marker>
<rawString>Cimiano, P., Montiel-Ponsoda, E., Buitelaar, P., Espinoza, M., and G´omez-P´erez, A. (2010). A note on ontology localization. Appl. Ontol., 5(2):127–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Clark</author>
<author>C Dyer</author>
<author>A Lavie</author>
<author>N Smith</author>
</authors>
<title>Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability .</title>
<date>2011</date>
<booktitle>In Proceedings of the Association for Computational Lingustics.</booktitle>
<contexts>
<context position="27149" citStr="Clark et al., 2011" startWordPosition="4180" endWordPosition="4183"> for Evaluation of Translation with Explicit ORdering) is based on the harmonic mean of precision and recall, whereby recall is weighted higher than precision. Along with standard exact word (or phrase) matching it has additional features, i.e. stemming, paraphrasing and synonymy matching. Differently to BLEU, the metric produces good correlation with human judgement at the sentence or segment level. 13tf-idf – term frequency-inverse document frequency 14http://iate.europa.eu/downloadTbx.do 15METEOR configuration: exact, stem, paraphrase 713 The approximate randomization approach in MultEval (Clark et al., 2011) is used to test whether differences among system performances are statistically significant with a p-value &lt; 0.05. 5 Evaluation of Ontology Labels In this Section, we report the translation quality of ontology labels based on translation systems learned from different sentence selection methods. Additionally, we perform experiments training an SMT system on the combination of in- and outdomain knowledge. The final approach enhances a domain-specific translation system with lexical knowledge identified in IATE or DBpedia. 5.1 Automatic Translation Evaluation We report the automatic evaluation </context>
</contexts>
<marker>Clark, Dyer, Lavie, Smith, 2011</marker>
<rawString>Clark, J., Dyer, C., Lavie, A., and Smith, N. (2011). Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability . In Proceedings of the Association for Computational Lingustics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Cuong</author>
<author>K Sima’an</author>
</authors>
<title>Latent domain translation models in mix-of-domains haystack.</title>
<date>2014</date>
<booktitle>In Proceedings of the 25th International Conference on Computational Linguistics,</booktitle>
<location>Dublin, Ireland.</location>
<marker>Cuong, Sima’an, 2014</marker>
<rawString>Cuong, H. and Sima’an, K. (2014). Latent domain translation models in mix-of-domains haystack. In Proceedings of the 25th International Conference on Computational Linguistics, Dublin, Ireland.</rawString>
</citation>
<citation valid="false">
<authors>
<author>T Declerck</author>
<author>A G P´erez</author>
<author>O Vela</author>
<author>Z Gantner</author>
<author>D Manzano</author>
</authors>
<title>and D-Saarbr¨ucken (2006). Multilingual lexical semantic resources for ontology translation. In</title>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation.</booktitle>
<marker>Declerck, P´erez, Vela, Gantner, Manzano, </marker>
<rawString>Declerck, T., P´erez, A. G., Vela, O., Gantner, Z., Manzano, D., and D-Saarbr¨ucken (2006). Multilingual lexical semantic resources for ontology translation. In In Proceedings of the 5th International Conference on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Denkowski</author>
<author>A Lavie</author>
</authors>
<title>Meteor universal: Language specific translation evaluation for any target language.</title>
<date>2014</date>
<booktitle>In Proceedings of the EACL 2014 Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="26037" citStr="Denkowski and Lavie, 2014" startWordPosition="4021" endWordPosition="4024">r proposed ontology-specific SMT system with the lexical information coming from the terminological database IATE14 (Inter-Active Terminology for Europe). IATE is the institutional terminology database of the EU and is used for the collection, dissemination and shared management of specific terminology and contains approximately 1.4 million multilingual entries. Evaluation Metrics The automatic translation evaluation is based on the correspondence between the SMT output and reference translation (gold standard). For the automatic evaluation we used the BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) algorithms.15 BLEU (Bilingual Evaluation Understudy) is calculated for individual translated segments (ngrams) by comparing them with a dataset of reference translations. Considering the shortness of the labels, we report scores based on the bi-gram overlap (BLEU-2) and the standard four-gram overlap (BLEU-4). Those scores, between 0 and 100 (perfect translation), are then averaged over the whole evaluation dataset to reach an estimate of the translation’s overall quality. METEOR (Metric for Evaluation of Translation with Explicit ORdering) is based on the harmonic mean of precision and recal</context>
</contexts>
<marker>Denkowski, Lavie, 2014</marker>
<rawString>Denkowski, M. and Lavie, A. (2014). Meteor universal: Language specific translation evaluation for any target language. In Proceedings of the EACL 2014 Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Eck</author>
<author>S Vogel</author>
<author>A Waibel</author>
</authors>
<title>Language model adaptation for statistical machine translation based on information retrieval.</title>
<date>2004</date>
<booktitle>In Proc. of LREC.</booktitle>
<contexts>
<context position="8658" citStr="Eck et al., 2004" startWordPosition="1321" endWordPosition="1324">t labels, showed the possibility of improving label translations without manually generated lexical resources, since not every ontology may benefit of current multilingual resources. Due to the specificity of the labels, previous research (Wu et al., 2008; Haddow and Koehn, 2012) showed that generic SMT systems, which merge all accessible data together, cannot be used to translate domain-specific vocabulary. To avoid unsatisfactory translations of specific vocabulary we have to provide the SMT system domainspecific bilingual knowledge, from where it can learn specific translation candidates. (Eck et al., 2004) used for the language model adaptation within SMT the information retrieval technique tf-idf. Similarly, (Hildebrand et al., 2005) and (L¨u et al., 2007) utilized this approach to select 709 relevant sentences from available parallel text to adapt translation models. The results confirmed that large amounts of generic training data cannot compensate for the requirement of domainspecific training sentences. Another approach is taken by (Moore and Lewis, 2010), where, based on source and target language models, the authors calculated the difference of the cross-entropy values for a given senten</context>
</contexts>
<marker>Eck, Vogel, Waibel, 2004</marker>
<rawString>Eck, M., Vogel, S., and Waibel, A. (2004). Language model adaptation for statistical machine translation based on information retrieval. In Proc. of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Erdmann</author>
<author>K Nakayama</author>
<author>T Hara</author>
<author>S Nishio</author>
</authors>
<title>Improving the extraction of bilingual terminology from wikipedia.</title>
<date>2009</date>
<journal>ACM Trans. Multimedia Comput. Commun. Appl.,</journal>
<volume>5</volume>
<issue>4</issue>
<contexts>
<context position="10965" citStr="Erdmann et al., 2009" startWordPosition="1672" endWordPosition="1675">ethods explicitly exploit the ontological hierarchy for label disambiguation and are not specifically designed to deal with the characteristics of ontology labels. As a lexical resource, Wikipedia with its rich semantic knowledge was used as a resource for bilingual term identification in the context of SMT. (Tyers and Pieanaar, 2008) extracts bilingual dictionary entries from Wikipedia to support the machine translation system. Based on exact string matching they query Wikipedia with a list of around 10,000 noun lemmas to generate the bilingual dictionary. Besides the interwiki link system, (Erdmann et al., 2009) enhance their bilingual dictionary by using redirection page titles and anchor text within Wikipedia. To cast the problem of ambiguous Wikipedia titles, (Niehues and Waibel, 2011; Arcan et al., 2014a) use the information of Wikipedia categories and the text of the articles to provide the SMT system domain-specific bilingual knowledge. This research showed that using the lexical information stored in this knowledge base improves the translation of highly domain-specific vocabulary. However, we do not rely on category annotations of Wikipedia articles, but perform domain-specific dictionary gen</context>
</contexts>
<marker>Erdmann, Nakayama, Hara, Nishio, 2009</marker>
<rawString>Erdmann, M., Nakayama, K., Hara, T., and Nishio, S. (2009). Improving the extraction of bilingual terminology from wikipedia. ACM Trans. Multimedia Comput. Commun. Appl., 5(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Espinoza</author>
<author>E Montiel-Ponsoda</author>
<author>A G´omez-P´erez</author>
</authors>
<title>Ontology localization.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fifth International Conference on Knowledge Capture, K-CAP ’09,</booktitle>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Espinoza, Montiel-Ponsoda, G´omez-P´erez, 2009</marker>
<rawString>Espinoza, M., Montiel-Ponsoda, E., and G´omez-P´erez, A. (2009). Ontology localization. In Proceedings of the Fifth International Conference on Knowledge Capture, K-CAP ’09, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Fu</author>
<author>R Brennan</author>
<author>D O’Sullivan</author>
</authors>
<title>Crosslingual ontology mapping - an investigation of the impact of machine translation.</title>
<date>2009</date>
<booktitle>of Lecture Notes in Computer Science.</booktitle>
<volume>5926</volume>
<editor>In G´omez-P´erez, A., Yu, Y., and Ding, Y., editors, ASWC,</editor>
<publisher>Springer.</publisher>
<marker>Fu, Brennan, O’Sullivan, 2009</marker>
<rawString>Fu, B., Brennan, R., and O’Sullivan, D. (2009). Crosslingual ontology mapping - an investigation of the impact of machine translation. In G´omez-P´erez, A., Yu, Y., and Ding, Y., editors, ASWC, volume 5926 of Lecture Notes in Computer Science. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gasc´o</author>
<author>M-A Rocha</author>
<author>G Sanchis-Trilles</author>
<author>J Andr´esFerrer</author>
<author>F Casacuberta</author>
</authors>
<title>Does more data always yield better translations?</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, EACL ’12,</booktitle>
<location>Stroudsburg, PA, USA.</location>
<marker>Gasc´o, Rocha, Sanchis-Trilles, Andr´esFerrer, Casacuberta, 2012</marker>
<rawString>Gasc´o, G., Rocha, M.-A., Sanchis-Trilles, G., Andr´esFerrer, J., and Casacuberta, F. (2012). Does more data always yield better translations? In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, EACL ’12, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A G´omez-P´erez</author>
<author>D Vila-Suero</author>
<author>E Montiel-Ponsoda</author>
<author>J Gracia</author>
<author>G Aguado-de Cea</author>
</authors>
<title>Guidelines for multilingual linked data.</title>
<date>2013</date>
<booktitle>In Proceedings of the 3rd International Conference on Web Intelligence, Mining and Semantics.</booktitle>
<publisher>ACM.</publisher>
<marker>G´omez-P´erez, Vila-Suero, Montiel-Ponsoda, Gracia, Aguado-de Cea, 2013</marker>
<rawString>G´omez-P´erez, A., Vila-Suero, D., Montiel-Ponsoda, E., Gracia, J., and Aguado-de Cea, G. (2013). Guidelines for multilingual linked data. In Proceedings of the 3rd International Conference on Web Intelligence, Mining and Semantics. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gracia</author>
<author>E Montiel-Ponsoda</author>
<author>P Cimiano</author>
<author>A G´omezP´erez</author>
<author>P Buitelaar</author>
<author>J McCrae</author>
</authors>
<title>Challenges for the multilingual web of data. Web Semantics: Science, Services and Agents on the World Wide</title>
<date>2012</date>
<journal>Web,</journal>
<volume>11</volume>
<marker>Gracia, Montiel-Ponsoda, Cimiano, G´omezP´erez, Buitelaar, McCrae, 2012</marker>
<rawString>Gracia, J., Montiel-Ponsoda, E., Cimiano, P., G´omezP´erez, A., Buitelaar, P., and McCrae, J. (2012). Challenges for the multilingual web of data. Web Semantics: Science, Services and Agents on the World Wide Web, 11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Haddow</author>
<author>P Koehn</author>
</authors>
<title>Analysing the Effect of Out-of-Domain Data on SMT Systems.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="8321" citStr="Haddow and Koehn, 2012" startWordPosition="1273" endWordPosition="1276">e translations using a small, but ontology-specific SMT system. We learned that using external SMT services often results in wrong translations of labels, because the external SMT services are not able to adapt to the specificity of the ontology. Avoiding existing multilingual resources, which enables a simple replacement of source and target labels, showed the possibility of improving label translations without manually generated lexical resources, since not every ontology may benefit of current multilingual resources. Due to the specificity of the labels, previous research (Wu et al., 2008; Haddow and Koehn, 2012) showed that generic SMT systems, which merge all accessible data together, cannot be used to translate domain-specific vocabulary. To avoid unsatisfactory translations of specific vocabulary we have to provide the SMT system domainspecific bilingual knowledge, from where it can learn specific translation candidates. (Eck et al., 2004) used for the language model adaptation within SMT the information retrieval technique tf-idf. Similarly, (Hildebrand et al., 2005) and (L¨u et al., 2007) utilized this approach to select 709 relevant sentences from available parallel text to adapt translation mo</context>
<context position="17834" citStr="Haddow and Koehn, 2012" startWordPosition="2756" endWordPosition="2759"> gives more weight to the Wikipedia entry, which is closer to our preferred domain. Finally, if the Wikipedia entry has an equivalent in the target language, i.e. German, we use the bilingual information for the lexical enhancement of the SMT system. 3.3 Integration of Domain-Specific Knowledge into SMT After the identification of domain-specific bilingual knowledge, it has to be integrated into the workflow of the SMT system. The injection of new obtained knowledge can be performed by retraining the domain-specific knowledge with the generic parallel corpus (Langlais, 2002; Ren et al., 2009; Haddow and Koehn, 2012) or by adding new entries directly to the translation system (Pinnis et al., 2012; Bouamor et al., 2012). These methods have the drawback that the bilingual domain specificity may get lost due to the usually larger generic parallel corpora. Giving more priority to domain-specific translations than generic ones, we focus on two techniques, i.e. the Fill-Up model (Bisazza et al., 2011) and the Cache-Based 711 Model (Bertoldi et al., 2013) approach. The Fill-Up model has been developed to address a common scenario where a large generic background model exists, and only a small quantity of domain-</context>
</contexts>
<marker>Haddow, Koehn, 2012</marker>
<rawString>Haddow, B. and Koehn, P. (2012). Analysing the Effect of Out-of-Domain Data on SMT Systems. In Proceedings of the Seventh Workshop on Statistical Machine Translation, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Harris</author>
</authors>
<date>1954</date>
<journal>Distributional structure. Word,</journal>
<volume>10</volume>
<issue>23</issue>
<contexts>
<context position="14504" citStr="Harris, 1954" startWordPosition="2236" endWordPosition="2237"> context and compared to other vectorized sets of words (from the training data) in a multi-dimensional vector space. For obtaining the vector representations we used a distributional semantic model trained on the Wikipedia articles,4 containing more than 3 billion words. Word relatedness is measured through the cosine similarity between two word vectors. A score of 1 would represent a perfect word similarity; e.g. cholera equals cholera, while the medical expression medicine has a cosine distance of 0.678 to cholera. Since words, which occur in similar contexts tend to have similar meanings (Harris, 1954), this approach enables to group related words together. The output of this technique is the analysed label with a vector attached to it, e.g. for the medical label cholera it provides related words with its relatedness value, e.g. typhus (0.869), smallpox (0.849), epidemic (0.834), dysentery (0.808) ... In our experiments, this method is implemented by the use of Word2Vec.5 To additionally disambiguate short labels, the related words of the current label are combined with the related words of its direct parent in the ontology. The usage of the ontology hierarchy allows us to take advantage of</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Harris, Z. (1954). Distributional structure. Word, 10(23).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A S Hildebrand</author>
<author>M Eck</author>
<author>S Vogel</author>
<author>A Waibel</author>
</authors>
<title>Adaptation of the translation model for statistical machine translation based on information retrieval.</title>
<date>2005</date>
<booktitle>In Proceedings of the 10th Conference of the European Association for Machine Translation (EAMT),</booktitle>
<location>Budapest.</location>
<contexts>
<context position="8789" citStr="Hildebrand et al., 2005" startWordPosition="1339" endWordPosition="1342">ry ontology may benefit of current multilingual resources. Due to the specificity of the labels, previous research (Wu et al., 2008; Haddow and Koehn, 2012) showed that generic SMT systems, which merge all accessible data together, cannot be used to translate domain-specific vocabulary. To avoid unsatisfactory translations of specific vocabulary we have to provide the SMT system domainspecific bilingual knowledge, from where it can learn specific translation candidates. (Eck et al., 2004) used for the language model adaptation within SMT the information retrieval technique tf-idf. Similarly, (Hildebrand et al., 2005) and (L¨u et al., 2007) utilized this approach to select 709 relevant sentences from available parallel text to adapt translation models. The results confirmed that large amounts of generic training data cannot compensate for the requirement of domainspecific training sentences. Another approach is taken by (Moore and Lewis, 2010), where, based on source and target language models, the authors calculated the difference of the cross-entropy values for a given sentence. (Axelrod et al., 2011) extend this work using the bilingual difference of cross-entropy on in-domain and out-of-domain language</context>
<context position="24610" citStr="Hildebrand et al., 2005" startWordPosition="3805" endWordPosition="3809">lel data extracted from the European Medicines Agency documents and websites. Comparison to Existing Approaches We compare our approach on knowledge expansion for sentence selection with similar methods that distinguish between more important sentences and less important ones. First, we sort 1.9M sentences from the generic corpus based on the perplexity of the ontology vocabulary. The perplexity score gives a notion of how well the probability model based on the ontology vocabulary predicts a sample, which is in our case each sentence in the generic corpus. Second, we use the method shown in (Hildebrand et al., 2005), where the authors use a method 11For reproducibility and future evaluation we take the first one-third part of each corpus. 12http://opus.lingfil.uu.se/EMEA.php based on tf-idf 13 to select the most relevant sentences. This widely-used method in information retrieval tells us how important a word is to a document, whereby each sentence from the generic corpus is treated as a document. Finally, we compare our approach with the infrequent n-gram recovery method, described in (Gasc´o et al., 2012). Their technique consists of selection of relevant sentences from the generic corpus, which contai</context>
</contexts>
<marker>Hildebrand, Eck, Vogel, Waibel, 2005</marker>
<rawString>Hildebrand, A. S., Eck, M., Vogel, S., and Waibel, A. (2005). Adaptation of the translation model for statistical machine translation based on information retrieval. In Proceedings of the 10th Conference of the European Association for Machine Translation (EAMT), Budapest.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kirchhoff</author>
<author>J Bilmes</author>
</authors>
<title>Submodularity for data selection in machine translation.</title>
<date>2014</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="9575" citStr="Kirchhoff and Bilmes, 2014" startWordPosition="1459" endWordPosition="1462"> that large amounts of generic training data cannot compensate for the requirement of domainspecific training sentences. Another approach is taken by (Moore and Lewis, 2010), where, based on source and target language models, the authors calculated the difference of the cross-entropy values for a given sentence. (Axelrod et al., 2011) extend this work using the bilingual difference of cross-entropy on in-domain and out-of-domain language models for training sentence selection for SMT. (Wuebker et al., 2014) reused the crossentropy approach and applied it to the translation of video lectures. (Kirchhoff and Bilmes, 2014) introduce submodular optimization using complex features for parallel sentence selection. In their experiments they use the source and target side of the text to be translated, and show significant improvements over the widely used cross-entropy method. A different approach for sentence selection is shown in (Cuong and Sima’an, 2014), where the authors propose a latent domain translation model to distinguish between hidden in- and out-of-domain data. (Gasc´o et al., 2012) and (Bicici and Yuret, 2011) sub-sample sentence pairs whose source has most overlap with the evaluation dataset. Differen</context>
</contexts>
<marker>Kirchhoff, Bilmes, 2014</marker>
<rawString>Kirchhoff, K. and Bilmes, J. (2014). Submodularity for data selection in machine translation. In Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Europarl: A Parallel Corpus for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Conference Proceedings: the tenth Machine Translation Summit,</booktitle>
<pages>79--86</pages>
<publisher>AAMT.</publisher>
<contexts>
<context position="22496" citStr="Koehn, 2005" startWordPosition="3514" endWordPosition="3515">ntology labels reported in this paper. The parameters within the SMT system are optimized on the development dataset 2. Statistical Machine Translation and Training Dataset For our translation task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where the word alignments were built with the GIZA++ toolkit (Och and Ney, 2003). The SRILM toolkit (Stolcke, 2002) was used to build the 5-gram language model. For a broader domain coverage of the generic training dataset necessary for the SMT system, we merged parts of JRC-Acquis 3.08 (Steinberger et al., 2006), Europarl v79 (Koehn, 2005) and OpenSubtitles201310 (Tiedemann, 2012), obtaining a training corpus of 1.9M sentences, con7http://www.who.int/classifications/ icd/en/ 8https://ec.europa.eu/jrc/en/ language-technologies/jrc-acquis 9http://www.statmt.org/europarl/ 10http://opus.lingfil.uu.se/ OpenSubtitles2013.php 712 English German Generic Dataset Sentences 1.9M (out-domain) Running Words 39.8M 37.1M Vocabulary 195,912 446,068 EMEA Dataset Sentences 1.1M (domain-specific) Running Words 13.8M 12.7M Vocabulary 58,935 115,754 Development Labels 500 Dataset 1 Running Words 3,025 2,908 Vocabulary 889 951 Development Labels 500</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Koehn, P. (2005). Europarl: A Parallel Corpus for Statistical Machine Translation. In Conference Proceedings: the tenth Machine Translation Summit, pages 79–86. AAMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions,</booktitle>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="22152" citStr="Koehn et al., 2007" startWordPosition="3456" endWordPosition="3459">es we experiment with different values of τ, which is the percentage of all the sentences that are considered relevant (domainspecific) by the sentence extraction approach. The value that allows the SMT system to achieve the best performance on the development dataset 1 is used on the evaluation set, which is used for the translation evaluation of ontology labels reported in this paper. The parameters within the SMT system are optimized on the development dataset 2. Statistical Machine Translation and Training Dataset For our translation task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where the word alignments were built with the GIZA++ toolkit (Och and Ney, 2003). The SRILM toolkit (Stolcke, 2002) was used to build the 5-gram language model. For a broader domain coverage of the generic training dataset necessary for the SMT system, we merged parts of JRC-Acquis 3.08 (Steinberger et al., 2006), Europarl v79 (Koehn, 2005) and OpenSubtitles201310 (Tiedemann, 2012), obtaining a training corpus of 1.9M sentences, con7http://www.who.int/classifications/ icd/en/ 8https://ec.europa.eu/jrc/en/ language-technologies/jrc-acquis 9http://www.statmt.org/europarl/ 10http://opus.lingfil</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico, M., Bertoldi, N., Cowan, B., Shen, W., Moran, C., Zens, R., Dyer, C., Bojar, O., Constantin, A., and Herbst, E. (2007). Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Langlais</author>
</authors>
<title>Improving a general-purpose statistical translation engine by terminological lexicons.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2nd International Workshop on Computational Terminology (COMPUTERM) ’2002,</booktitle>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="17791" citStr="Langlais, 2002" startWordPosition="2750" endWordPosition="2751">ous entries, the cosine similarity gives more weight to the Wikipedia entry, which is closer to our preferred domain. Finally, if the Wikipedia entry has an equivalent in the target language, i.e. German, we use the bilingual information for the lexical enhancement of the SMT system. 3.3 Integration of Domain-Specific Knowledge into SMT After the identification of domain-specific bilingual knowledge, it has to be integrated into the workflow of the SMT system. The injection of new obtained knowledge can be performed by retraining the domain-specific knowledge with the generic parallel corpus (Langlais, 2002; Ren et al., 2009; Haddow and Koehn, 2012) or by adding new entries directly to the translation system (Pinnis et al., 2012; Bouamor et al., 2012). These methods have the drawback that the bilingual domain specificity may get lost due to the usually larger generic parallel corpora. Giving more priority to domain-specific translations than generic ones, we focus on two techniques, i.e. the Fill-Up model (Bisazza et al., 2011) and the Cache-Based 711 Model (Bertoldi et al., 2013) approach. The Fill-Up model has been developed to address a common scenario where a large generic background model e</context>
</contexts>
<marker>Langlais, 2002</marker>
<rawString>Langlais, P. (2002). Improving a general-purpose statistical translation engine by terminological lexicons. In Proceedings of the 2nd International Workshop on Computational Terminology (COMPUTERM) ’2002, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lehmann</author>
<author>R Isele</author>
<author>M Jakob</author>
<author>A Jentzsch</author>
<author>D Kontokostas</author>
<author>P N Mendes</author>
<author>S Hellmann</author>
<author>M Morsey</author>
<author>P van Kleef</author>
<author>S Auer</author>
<author>C Bizer</author>
</authors>
<title>DBpedia - a large-scale, multilingual knowledge base extracted from wikipedia.</title>
<date>2015</date>
<journal>Semantic Web Journal,</journal>
<volume>6</volume>
<issue>2</issue>
<marker>Lehmann, Isele, Jakob, Jentzsch, Kontokostas, Mendes, Hellmann, Morsey, van Kleef, Auer, Bizer, 2015</marker>
<rawString>Lehmann, J., Isele, R., Jakob, M., Jentzsch, A., Kontokostas, D., Mendes, P. N., Hellmann, S., Morsey, M., van Kleef, P., Auer, S., and Bizer, C. (2015). DBpedia - a large-scale, multilingual knowledge base extracted from wikipedia. Semantic Web Journal, 6(2):167–195.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y L¨u</author>
<author>J Huang</author>
<author>Q Liu</author>
</authors>
<title>Improving statistical machine translation performance by training data selection and optimization.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL).</booktitle>
<marker>L¨u, Huang, Liu, 2007</marker>
<rawString>L¨u, Y., Huang, J., and Liu, Q. (2007). Improving statistical machine translation performance by training data selection and optimization. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J McCrae</author>
<author>M Espinoza</author>
<author>E Montiel-Ponsoda</author>
<author>G Aguado-de Cea</author>
<author>P Cimiano</author>
</authors>
<title>Combining statistical and semantic approaches to the translation of ontologies and taxonomies.</title>
<date>2011</date>
<booktitle>In Fifth workshop on Syntax, Structure and Semantics in Statistical Translation (SSST-5).</booktitle>
<marker>McCrae, Espinoza, Montiel-Ponsoda, Aguado-de Cea, Cimiano, 2011</marker>
<rawString>McCrae, J., Espinoza, M., Montiel-Ponsoda, E., Aguado-de Cea, G., and Cimiano, P. (2011). Combining statistical and semantic approaches to the translation of ontologies and taxonomies. In Fifth workshop on Syntax, Structure and Semantics in Statistical Translation (SSST-5).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mikolov</author>
<author>K Chen</author>
<author>G Corrado</author>
<author>J Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<location>CoRR, abs/1301.3781.</location>
<contexts>
<context position="13836" citStr="Mikolov et al., 2013" startWordPosition="2128" endWordPosition="2131">fined as the cosine of the angle between the two vectors. The calculated similarity score allows us to distinguish between more and less relevant sentences. Due to the specificity of ontology labels, the ngram overlap approach is not able to select useful sentences in the presence of short labels. For 710 this reason, we improve it by extending the semantic information of labels using a technique for computing vector representations of words. The technique is based on a neural network that analyses the textual data provided as input and provides as output a list of semantically related words (Mikolov et al., 2013). Each input string is vectorized using the surrounding context and compared to other vectorized sets of words (from the training data) in a multi-dimensional vector space. For obtaining the vector representations we used a distributional semantic model trained on the Wikipedia articles,4 containing more than 3 billion words. Word relatedness is measured through the cosine similarity between two word vectors. A score of 1 would represent a perfect word similarity; e.g. cholera equals cholera, while the medical expression medicine has a cosine distance of 0.678 to cholera. Since words, which oc</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013). Efficient estimation of word representations in vector space. CoRR, abs/1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Moore</author>
<author>W Lewis</author>
</authors>
<title>Intelligent selection of language model training data.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers, ACLShort ’10,</booktitle>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="9121" citStr="Moore and Lewis, 2010" startWordPosition="1390" endWordPosition="1393">ic vocabulary we have to provide the SMT system domainspecific bilingual knowledge, from where it can learn specific translation candidates. (Eck et al., 2004) used for the language model adaptation within SMT the information retrieval technique tf-idf. Similarly, (Hildebrand et al., 2005) and (L¨u et al., 2007) utilized this approach to select 709 relevant sentences from available parallel text to adapt translation models. The results confirmed that large amounts of generic training data cannot compensate for the requirement of domainspecific training sentences. Another approach is taken by (Moore and Lewis, 2010), where, based on source and target language models, the authors calculated the difference of the cross-entropy values for a given sentence. (Axelrod et al., 2011) extend this work using the bilingual difference of cross-entropy on in-domain and out-of-domain language models for training sentence selection for SMT. (Wuebker et al., 2014) reused the crossentropy approach and applied it to the translation of video lectures. (Kirchhoff and Bilmes, 2014) introduce submodular optimization using complex features for parallel sentence selection. In their experiments they use the source and target sid</context>
</contexts>
<marker>Moore, Lewis, 2010</marker>
<rawString>Moore, R. C. and Lewis, W. (2010). Intelligent selection of language model training data. In Proceedings of the ACL 2010 Conference Short Papers, ACLShort ’10, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Niehues</author>
<author>A Waibel</author>
</authors>
<title>Using Wikipedia to Translate Domain-specific Terms in SMT.</title>
<date>2011</date>
<booktitle>In International Workshop on Spoken Language Translation,</booktitle>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="11144" citStr="Niehues and Waibel, 2011" startWordPosition="1699" endWordPosition="1702"> resource, Wikipedia with its rich semantic knowledge was used as a resource for bilingual term identification in the context of SMT. (Tyers and Pieanaar, 2008) extracts bilingual dictionary entries from Wikipedia to support the machine translation system. Based on exact string matching they query Wikipedia with a list of around 10,000 noun lemmas to generate the bilingual dictionary. Besides the interwiki link system, (Erdmann et al., 2009) enhance their bilingual dictionary by using redirection page titles and anchor text within Wikipedia. To cast the problem of ambiguous Wikipedia titles, (Niehues and Waibel, 2011; Arcan et al., 2014a) use the information of Wikipedia categories and the text of the articles to provide the SMT system domain-specific bilingual knowledge. This research showed that using the lexical information stored in this knowledge base improves the translation of highly domain-specific vocabulary. However, we do not rely on category annotations of Wikipedia articles, but perform domain-specific dictionary generation based on the overlap between related words from the ontology label and the abstract of a Wikipedia article. 3 Methodology We propose an approach that uses the ontology lab</context>
</contexts>
<marker>Niehues, Waibel, 2011</marker>
<rawString>Niehues, J. and Waibel, A. (2011). Using Wikipedia to Translate Domain-specific Terms in SMT. In International Workshop on Spoken Language Translation, San Francisco, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<contexts>
<context position="22234" citStr="Och and Ney, 2003" startWordPosition="3470" endWordPosition="3473">tences that are considered relevant (domainspecific) by the sentence extraction approach. The value that allows the SMT system to achieve the best performance on the development dataset 1 is used on the evaluation set, which is used for the translation evaluation of ontology labels reported in this paper. The parameters within the SMT system are optimized on the development dataset 2. Statistical Machine Translation and Training Dataset For our translation task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where the word alignments were built with the GIZA++ toolkit (Och and Ney, 2003). The SRILM toolkit (Stolcke, 2002) was used to build the 5-gram language model. For a broader domain coverage of the generic training dataset necessary for the SMT system, we merged parts of JRC-Acquis 3.08 (Steinberger et al., 2006), Europarl v79 (Koehn, 2005) and OpenSubtitles201310 (Tiedemann, 2012), obtaining a training corpus of 1.9M sentences, con7http://www.who.int/classifications/ icd/en/ 8https://ec.europa.eu/jrc/en/ language-technologies/jrc-acquis 9http://www.statmt.org/europarl/ 10http://opus.lingfil.uu.se/ OpenSubtitles2013.php 712 English German Generic Dataset Sentences 1.9M (o</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Och, F. J. and Ney, H. (2003). A systematic comparison of various statistical alignment models. Computational Linguistics, 29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S O’Riain</author>
<author>B Coughlan</author>
<author>P Buitelaar</author>
<author>T Declerck</author>
<author>U Krieger</author>
<author>S M Thomas</author>
</authors>
<title>Crosslingual querying and comparison of linked financial and business data.</title>
<date>2013</date>
<booktitle>of Lecture Notes in Computer Science.</booktitle>
<volume>7955</volume>
<editor>In Cimiano, P., Fern´andez, M., Lopez, V., Schlobach, S., and V¨olker, J., editors, ESWC (Satellite Events),</editor>
<publisher>Springer.</publisher>
<marker>O’Riain, Coughlan, Buitelaar, Declerck, Krieger, Thomas, 2013</marker>
<rawString>O’Riain, S., Coughlan, B., Buitelaar, P., Declerck, T., Krieger, U., and Thomas, S. M. (2013). Crosslingual querying and comparison of linked financial and business data. In Cimiano, P., Fern´andez, M., Lopez, V., Schlobach, S., and V¨olker, J., editors, ESWC (Satellite Events), volume 7955 of Lecture Notes in Computer Science. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="25998" citStr="Papineni et al., 2002" startWordPosition="4015" endWordPosition="4018">rthermore we enrich and evaluate our proposed ontology-specific SMT system with the lexical information coming from the terminological database IATE14 (Inter-Active Terminology for Europe). IATE is the institutional terminology database of the EU and is used for the collection, dissemination and shared management of specific terminology and contains approximately 1.4 million multilingual entries. Evaluation Metrics The automatic translation evaluation is based on the correspondence between the SMT output and reference translation (gold standard). For the automatic evaluation we used the BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) algorithms.15 BLEU (Bilingual Evaluation Understudy) is calculated for individual translated segments (ngrams) by comparing them with a dataset of reference translations. Considering the shortness of the labels, we report scores based on the bi-gram overlap (BLEU-2) and the standard four-gram overlap (BLEU-4). Those scores, between 0 and 100 (perfect translation), are then averaged over the whole evaluation dataset to reach an estimate of the translation’s overall quality. METEOR (Metric for Evaluation of Translation with Explicit ORdering) is based on t</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2002). BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pinnis</author>
<author>N Ljubeˇsi´c</author>
<author>D S¸tef˘anescu</author>
<author>I Skadin¸a</author>
<author>M Tadi´c</author>
<author>T Gornostay</author>
</authors>
<title>Term extraction, tagging, and mapping tools for under-resourced languages.</title>
<date>2012</date>
<booktitle>In Proceedings of the Terminology and Knowledge Engineering (TKE2012) Conference.</booktitle>
<marker>Pinnis, Ljubeˇsi´c, S¸tef˘anescu, Skadin¸a, Tadi´c, Gornostay, 2012</marker>
<rawString>Pinnis, M., Ljubeˇsi´c, N., S¸tef˘anescu, D., Skadin¸a, I., Tadi´c, M., and Gornostay, T. (2012). Term extraction, tagging, and mapping tools for under-resourced languages. In Proceedings of the Terminology and Knowledge Engineering (TKE2012) Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Ren</author>
<author>Y L¨u</author>
<author>J Cao</author>
<author>Q Liu</author>
<author>Y Huang</author>
</authors>
<title>Improving statistical machine translation using domain bilingual multiword expressions.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Multiword Expressions: Identification, Interpretation, Disambiguation and Applications, MWE ’09,</booktitle>
<location>Stroudsburg, PA, USA.</location>
<marker>Ren, L¨u, Cao, Liu, Huang, 2009</marker>
<rawString>Ren, Z., L¨u, Y., Cao, J., Liu, Q., and Huang, Y. (2009). Improving statistical machine translation using domain bilingual multiword expressions. In Proceedings of the Workshop on Multiword Expressions: Identification, Interpretation, Disambiguation and Applications, MWE ’09, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Steinberger</author>
<author>B Pouliquen</author>
<author>A Widiger</author>
<author>C Ignat</author>
<author>T Erjavec</author>
<author>D Tufis</author>
<author>D Varga</author>
</authors>
<title>The JRC-Acquis: A multilingual aligned parallel corpus with 20+ languages.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC’2006).</booktitle>
<contexts>
<context position="22468" citStr="Steinberger et al., 2006" startWordPosition="3508" endWordPosition="3511"> used for the translation evaluation of ontology labels reported in this paper. The parameters within the SMT system are optimized on the development dataset 2. Statistical Machine Translation and Training Dataset For our translation task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where the word alignments were built with the GIZA++ toolkit (Och and Ney, 2003). The SRILM toolkit (Stolcke, 2002) was used to build the 5-gram language model. For a broader domain coverage of the generic training dataset necessary for the SMT system, we merged parts of JRC-Acquis 3.08 (Steinberger et al., 2006), Europarl v79 (Koehn, 2005) and OpenSubtitles201310 (Tiedemann, 2012), obtaining a training corpus of 1.9M sentences, con7http://www.who.int/classifications/ icd/en/ 8https://ec.europa.eu/jrc/en/ language-technologies/jrc-acquis 9http://www.statmt.org/europarl/ 10http://opus.lingfil.uu.se/ OpenSubtitles2013.php 712 English German Generic Dataset Sentences 1.9M (out-domain) Running Words 39.8M 37.1M Vocabulary 195,912 446,068 EMEA Dataset Sentences 1.1M (domain-specific) Running Words 13.8M 12.7M Vocabulary 58,935 115,754 Development Labels 500 Dataset 1 Running Words 3,025 2,908 Vocabulary 88</context>
</contexts>
<marker>Steinberger, Pouliquen, Widiger, Ignat, Erjavec, Tufis, Varga, 2006</marker>
<rawString>Steinberger, R., Pouliquen, B., Widiger, A., Ignat, C., Erjavec, T., Tufis, D., and Varga, D. (2006). The JRC-Acquis: A multilingual aligned parallel corpus with 20+ languages. In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC’2006).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM - An extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings International Conference on Spoken Language Processing.</booktitle>
<contexts>
<context position="22269" citStr="Stolcke, 2002" startWordPosition="3477" endWordPosition="3478">mainspecific) by the sentence extraction approach. The value that allows the SMT system to achieve the best performance on the development dataset 1 is used on the evaluation set, which is used for the translation evaluation of ontology labels reported in this paper. The parameters within the SMT system are optimized on the development dataset 2. Statistical Machine Translation and Training Dataset For our translation task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where the word alignments were built with the GIZA++ toolkit (Och and Ney, 2003). The SRILM toolkit (Stolcke, 2002) was used to build the 5-gram language model. For a broader domain coverage of the generic training dataset necessary for the SMT system, we merged parts of JRC-Acquis 3.08 (Steinberger et al., 2006), Europarl v79 (Koehn, 2005) and OpenSubtitles201310 (Tiedemann, 2012), obtaining a training corpus of 1.9M sentences, con7http://www.who.int/classifications/ icd/en/ 8https://ec.europa.eu/jrc/en/ language-technologies/jrc-acquis 9http://www.statmt.org/europarl/ 10http://opus.lingfil.uu.se/ OpenSubtitles2013.php 712 English German Generic Dataset Sentences 1.9M (out-domain) Running Words 39.8M 37.1</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Stolcke, A. (2002). SRILM - An extensible language modeling toolkit. In Proceedings International Conference on Spoken Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tiedemann</author>
</authors>
<title>News from OPUS - A collection of multilingual parallel corpora with tools and interfaces.</title>
<date>2009</date>
<booktitle>Recent Advances in Natural Language Processing,</booktitle>
<volume>volume</volume>
<editor>In Nicolov, N., Bontcheva, K., Angelova, G., and Mitkov, R., editors,</editor>
<location>Amsterdam/Philadelphia, Borovets, Bulgaria.</location>
<contexts>
<context position="23950" citStr="Tiedemann, 2009" startWordPosition="3701" endWordPosition="3702">denotes the number of unique words in the dataset) taining around 38M running words (Table 1).11 The generic SMT system, trained on the concatenated 1.9 sentences, is used as a baseline, which we compare against the domain-specific models generated with different sentence selection methods. Furthermore we use the generic SMT system in combination with the smaller domainspecific models to evaluate different approaches when combining generic and domain-specific data together. We additionally compare our results to an SMT system built on an existing domain-specific parallel dataset, i.e. EMEA12 (Tiedemann, 2009), which holds specific medical parallel data extracted from the European Medicines Agency documents and websites. Comparison to Existing Approaches We compare our approach on knowledge expansion for sentence selection with similar methods that distinguish between more important sentences and less important ones. First, we sort 1.9M sentences from the generic corpus based on the perplexity of the ontology vocabulary. The perplexity score gives a notion of how well the probability model based on the ontology vocabulary predicts a sample, which is in our case each sentence in the generic corpus. </context>
</contexts>
<marker>Tiedemann, 2009</marker>
<rawString>Tiedemann, J. (2009). News from OPUS - A collection of multilingual parallel corpora with tools and interfaces. In Nicolov, N., Bontcheva, K., Angelova, G., and Mitkov, R., editors, Recent Advances in Natural Language Processing, volume V. John Benjamins, Amsterdam/Philadelphia, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tiedemann</author>
</authors>
<title>Parallel data, tools and interfaces in opus.</title>
<date>2012</date>
<booktitle>Proceedings of the Eight International Conference on Language Resources and Evaluation,</booktitle>
<editor>In Chair), N. C. C., Choukri, K., Declerck, T., Do˘gan, M. U., Maegaard, B., Mariani, J., Odijk, J., and Piperidis, S., editors,</editor>
<location>Istanbul, Turkey.</location>
<contexts>
<context position="22538" citStr="Tiedemann, 2012" startWordPosition="3518" endWordPosition="3519"> The parameters within the SMT system are optimized on the development dataset 2. Statistical Machine Translation and Training Dataset For our translation task, we use the statistical translation toolkit Moses (Koehn et al., 2007), where the word alignments were built with the GIZA++ toolkit (Och and Ney, 2003). The SRILM toolkit (Stolcke, 2002) was used to build the 5-gram language model. For a broader domain coverage of the generic training dataset necessary for the SMT system, we merged parts of JRC-Acquis 3.08 (Steinberger et al., 2006), Europarl v79 (Koehn, 2005) and OpenSubtitles201310 (Tiedemann, 2012), obtaining a training corpus of 1.9M sentences, con7http://www.who.int/classifications/ icd/en/ 8https://ec.europa.eu/jrc/en/ language-technologies/jrc-acquis 9http://www.statmt.org/europarl/ 10http://opus.lingfil.uu.se/ OpenSubtitles2013.php 712 English German Generic Dataset Sentences 1.9M (out-domain) Running Words 39.8M 37.1M Vocabulary 195,912 446,068 EMEA Dataset Sentences 1.1M (domain-specific) Running Words 13.8M 12.7M Vocabulary 58,935 115,754 Development Labels 500 Dataset 1 Running Words 3,025 2,908 Vocabulary 889 951 Development Labels 500 Dataset 2 Running Words 3,003 3,020 Vocab</context>
</contexts>
<marker>Tiedemann, 2012</marker>
<rawString>Tiedemann, J. (2012). Parallel data, tools and interfaces in opus. In Chair), N. C. C., Choukri, K., Declerck, T., Do˘gan, M. U., Maegaard, B., Mariani, J., Odijk, J., and Piperidis, S., editors, Proceedings of the Eight International Conference on Language Resources and Evaluation, Istanbul, Turkey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F M Tyers</author>
<author>J A Pieanaar</author>
</authors>
<title>Extracting bilingual word pairs from wikipedia. In Collaboration: interoperability between people in the creation of language resources for less-resourced languages (A SALTMIL workshop).</title>
<date>2008</date>
<contexts>
<context position="10680" citStr="Tyers and Pieanaar, 2008" startWordPosition="1627" endWordPosition="1630">d (Bicici and Yuret, 2011) sub-sample sentence pairs whose source has most overlap with the evaluation dataset. Different from these approaches, we do not embed any specific in-domain knowledge to the generic corpus, from which sentence selection is performed. Furthermore, none of these methods explicitly exploit the ontological hierarchy for label disambiguation and are not specifically designed to deal with the characteristics of ontology labels. As a lexical resource, Wikipedia with its rich semantic knowledge was used as a resource for bilingual term identification in the context of SMT. (Tyers and Pieanaar, 2008) extracts bilingual dictionary entries from Wikipedia to support the machine translation system. Based on exact string matching they query Wikipedia with a list of around 10,000 noun lemmas to generate the bilingual dictionary. Besides the interwiki link system, (Erdmann et al., 2009) enhance their bilingual dictionary by using redirection page titles and anchor text within Wikipedia. To cast the problem of ambiguous Wikipedia titles, (Niehues and Waibel, 2011; Arcan et al., 2014a) use the information of Wikipedia categories and the text of the articles to provide the SMT system domain-specifi</context>
</contexts>
<marker>Tyers, Pieanaar, 2008</marker>
<rawString>Tyers, F. M. and Pieanaar, J. A. (2008). Extracting bilingual word pairs from wikipedia. In Collaboration: interoperability between people in the creation of language resources for less-resourced languages (A SALTMIL workshop).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Wu</author>
<author>H Wang</author>
<author>C Zong</author>
</authors>
<title>Domain adaptation for statistical machine translation with domain dictionary and monolingual corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics -</booktitle>
<volume>1</volume>
<contexts>
<context position="8296" citStr="Wu et al., 2008" startWordPosition="1269" endWordPosition="1272">w to gain adequate translations using a small, but ontology-specific SMT system. We learned that using external SMT services often results in wrong translations of labels, because the external SMT services are not able to adapt to the specificity of the ontology. Avoiding existing multilingual resources, which enables a simple replacement of source and target labels, showed the possibility of improving label translations without manually generated lexical resources, since not every ontology may benefit of current multilingual resources. Due to the specificity of the labels, previous research (Wu et al., 2008; Haddow and Koehn, 2012) showed that generic SMT systems, which merge all accessible data together, cannot be used to translate domain-specific vocabulary. To avoid unsatisfactory translations of specific vocabulary we have to provide the SMT system domainspecific bilingual knowledge, from where it can learn specific translation candidates. (Eck et al., 2004) used for the language model adaptation within SMT the information retrieval technique tf-idf. Similarly, (Hildebrand et al., 2005) and (L¨u et al., 2007) utilized this approach to select 709 relevant sentences from available parallel tex</context>
</contexts>
<marker>Wu, Wang, Zong, 2008</marker>
<rawString>Wu, H., Wang, H., and Zong, C. (2008). Domain adaptation for statistical machine translation with domain dictionary and monolingual corpora. In Proceedings of the 22nd International Conference on Computational Linguistics - Volume 1, COLING ’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Juan</author>
<author>C Servan</author>
<author>M Dymetman</author>
<author>S Mirkin</author>
</authors>
<title>Comparison of Data Selection Techniques for the Translation of Video Lectures.</title>
<date>2014</date>
<booktitle>In Proc. of the Eleventh Biennial Conf. of the Association for Machine Translation in the Americas (AMTA-2014),</booktitle>
<location>Vancouver</location>
<marker>Juan, Servan, Dymetman, Mirkin, 2014</marker>
<rawString>Wuebker, J., Ney, H., Martinez-Villaronga, A., Gim´enez, A., , Juan, A., Servan, C., Dymetman, M., and Mirkin, S. (2014). Comparison of Data Selection Techniques for the Translation of Video Lectures. In Proc. of the Eleventh Biennial Conf. of the Association for Machine Translation in the Americas (AMTA-2014), Vancouver (Canada).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>