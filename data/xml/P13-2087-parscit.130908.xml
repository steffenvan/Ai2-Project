<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.097690">
<title confidence="0.927855">
Re-embedding Words
</title>
<author confidence="0.995275">
Igor Labutov Hod Lipson
</author>
<affiliation confidence="0.999513">
Cornell University Cornell University
</affiliation>
<email confidence="0.994375">
iil4@cornell.edu hod.lipson@cornell.edu
</email>
<sectionHeader confidence="0.997315" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999979947368421">
We present a fast method for re-purposing
existing semantic word vectors to improve
performance in a supervised task. Re-
cently, with an increase in computing re-
sources, it became possible to learn rich
word embeddings from massive amounts
of unlabeled data. However, some meth-
ods take days or weeks to learn good em-
beddings, and some are notoriously dif-
ficult to train. We propose a method
that takes as input an existing embedding,
some labeled data, and produces an em-
bedding in the same space, but with a bet-
ter predictive performance in the super-
vised task. We show improvement on the
task of sentiment classification with re-
spect to several baselines, and observe that
the approach is most useful when the train-
ing set is sufficiently small.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999821316666667">
Incorporating the vector representation of a word
as a feature, has recently been shown to benefit
performance in several standard NLP tasks such
as language modeling (Bengio et al., 2003; Mnih
and Hinton, 2009), POS-tagging and NER (Col-
lobert et al., 2011), parsing (Socher et al., 2010),
as well as in sentiment and subjectivity analysis
tasks (Maas et al., 2011; Yessenalina and Cardie,
2011). Real-valued word vectors mitigate sparsity
by “smoothing” relevant semantic insight gained
during the unsupervised training over the rare and
unseen terms in the training data. To be effective,
these word-representations — and the process by
which they are assigned to the words (i.e. embed-
ding) — should capture the semantics relevant to
the task. We might, for example, consider dra-
matic (term X) and pleasant (term Y) to correlate
with a review of a good movie (task A), while find-
ing them of opposite polarity in the context of a
dating profile (task B). Consequently, good vectors
for X and Y should yield an inner product close to
1 in the context of task A, and −1 in the context
of task B. Moreover, we may already have on our
hands embeddings for X and Y obtained from yet
another (possibly unsupervised) task (C), in which
X and Y are, for example, orthogonal. If the em-
beddings for task C happen to be learned from a
much larger dataset, it would make sense to re-
use task C embeddings, but adapt them for task
A and/or task B. We will refer to task C and its
embeddings as the source task and the source em-
beddings, and task A/B, and its embeddings as the
target task and the target embeddings.
Traditionally, we would learn the embeddings
for the target task jointly with whatever unla-
beled data we may have, in an instance of semi-
supervised learning, and/or we may leverage la-
bels from multiple other related tasks in a multi-
task approach. Both methods have been applied
successfully (Collobert and Weston, 2008) to learn
task-specific embeddings. But while joint train-
ing is highly effective, a downside is that a large
amount of data (and processing time) is required
a-priori. In the case of deep neural embeddings,
for example, training time can number in days. On
the other hand, learned embeddings are becoming
more abundant, as much research and computing
effort is being invested in learning word represen-
tations using large-scale deep architectures trained
on web-scale corpora. Many of said embeddings
are published and can be harnessed in their raw
form as additional features in a number of super-
vised tasks (Turian et al., 2010). It would, thus, be
advantageous to learn a task-specific embedding
directly from another (source) embedding.
In this paper we propose a fast method for re-
embedding words from a source embedding S to a
target embedding T by performing unconstrained
optimization of a convex objective. Our objec-
tive is a linear combination of the dataset’s log-
</bodyText>
<page confidence="0.988285">
489
</page>
<bodyText confidence="0.8592086">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 489–493,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
likelihood under the target embedding and the
Frobenius norm of the distortion matrix — a ma-
trix of component-wise differences between the
target and the source embeddings. The latter acts
as a regularizer that penalizes the Euclidean dis-
tance between the source and target embeddings.
The method is much faster than joint training and
yields competitive results with several baselines.
</bodyText>
<sectionHeader confidence="0.999875" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99581545">
The most relevant to our contribution is the work
by Maas et.al (2011), where word vectors are
learned specifically for sentiment classification.
Embeddings are learned in a semi-supervised
fashion, and the components of the embedding are
given an explicit probabilistic interpretation. Their
method produces state-of-the-art results, however,
optimization is non-convex and takes approxi-
mately 10 hours on 10 machines1. Naturally, our
method is significantly faster because it operates in
the space of an existing embedding, and does not
require a large amount of training data a-priori.
Collobert and Weston (2008), in their seminal
paper on deep architectures for NLP, propose a
multilayer neural network for learning word em-
beddings. Training of the model, depending on
the task, is reported to be between an hour and
three days. While the obtained embeddings can
be “fine-tuned” using backpropogation for a su-
pervised task, like all multilayer neural network
training, optimization is non-convex, and is sensi-
tive to the dimensionality of the hidden layers.
In machine learning literature, joint semi-
supervised embedding takes form in methods such
as the LaplacianSVM (LapSVM) (Belkin et al.,
2006) and Label Propogation (Zhu and Ghahra-
mani, 2002), to which our approach is related.
These methods combine a discriminative learner
with a non-linear manifold learning technique in a
joint objective, and apply it to a combined set of
labeled and unlabeled examples to improve per-
formance in a supervised task. (Weston et al.,
2012) take it further by applying this idea to deep-
learning architectures. Our method is different in
that the (potentially) massive amount of unlabeled
data is not required a-priori, but only the resultant
embedding.
1as reported by author in private correspondence. The
runtime can be improved using recently introduced tech-
niques, see (Collobert et al., 2011)
</bodyText>
<sectionHeader confidence="0.993889" genericHeader="method">
3 Approach
</sectionHeader>
<bodyText confidence="0.999986727272727">
Let ΦS, ΦT E R|V |×K be the source and target
embedding matrices respectively, where K is the
dimension of the word vector space, identical in
the source and target embeddings, and V is the set
of embedded words, given by VS n VT. Following
this notation, φi – the ith row in Φ – is the respec-
tive vector representation of word wi E V . In what
follows, we first introduce our supervised objec-
tive, then combine it with the proposed regularizer
and learn the target embedding ΦT by optimizing
the resulting joint convex objective.
</bodyText>
<subsectionHeader confidence="0.996307">
3.1 Supervised model
</subsectionHeader>
<bodyText confidence="0.999994571428571">
We model each document dj E D (a movie re-
view, for example) as a collection of words wij
(i.i.d samples). We assign a sentiment label sj E
10, 11 to each document (converting the star rating
to a binary label), and seek to optimize the con-
ditional likelihood of the labels (sj)j∈{1,...,|D|},
given the embeddings and the documents:
</bodyText>
<equation confidence="0.9881655">
�p(s1, ..., s|D||D; ΦT) = ri p(sj|wi; ΦT)
dj∈D wi∈dj
</equation>
<bodyText confidence="0.99993975">
where p(sj = 1|wi, ΦT) is the probability of as-
signing a positive label to document j, given that
wi E dj. As in (Maas et al., 2011), we use logistic
regression to model the conditional likelihood:
</bodyText>
<equation confidence="0.990809">
1
p(sj = 1|wi; ΦT) = 1 + exp(�ψT φi)
</equation>
<bodyText confidence="0.999960545454545">
where ψ E RK+1 is a regression parameter vector
with an included bias component. Maximizing the
log-likelihood directly (for ψ and ΦT), especially
on small datasets, will result in severe overfitting,
as learning will tend to commit neutral words to
either polarity. Classical regularization will mit-
igate this effect, but can be improved further by
introducing an external embedding in the regular-
izer. In what follows, we describe re-embedding
regularization— employing existing (source) em-
beddings to bias word vector learning.
</bodyText>
<subsectionHeader confidence="0.999137">
3.2 Re-embedding regularization
</subsectionHeader>
<bodyText confidence="0.9990934">
To leverage rich semantic word representations,
we employ an external source embedding and in-
corporate it in the regularizer on the supervised
objective. We use Euclidean distance between the
source and the target embeddings as the regular-
</bodyText>
<page confidence="0.986976">
490
</page>
<bodyText confidence="0.9906255">
ization loss. Combined with the supervised objec-
tive, the resulting log-likelihood becomes:
</bodyText>
<equation confidence="0.9719795">
� argmax log p(sj|wi; 41)T) − A||Δ41)||2F (1)
ψ,ΦT dj∈Dwi∈dj
</equation>
<bodyText confidence="0.99979625">
where Δ41) = 41)T −41)S, ||·||F is a Frobenius norm,
and A is a trade-off parameter. There are almost
no restrictions on 41)S, except that it must match
the desired target vector space dimension K. The
objective is convex in ψ and 41)T, thus, yielding a
unique target re-embedding. We employ L-BFGS
algorithm (Liu and Nocedal, 1989) to find the op-
timal target embedding.
</bodyText>
<subsectionHeader confidence="0.99966">
3.3 Classification with word vectors
</subsectionHeader>
<bodyText confidence="0.999988714285714">
To classify documents, re-embedded word vectors
can now be used to construct a document-level
feature vector for a supervised learning algorithm
of choice. Perhaps the most direct approach is to
compute a weighted linear combination of the em-
beddings for words that appear in the document
to be classified, as done in (Maas et al., 2011)
and (Blacoe and Lapata, 2012). We use the docu-
ment’s binary bag-of-words vector vj, and com-
pute the document’s vector space representation
through the matrix-vector product 41)Tvj. The re-
sulting K + 1-dimensional vector is then cosine-
normalized and used as a feature vector to repre-
sent the document dj.
</bodyText>
<sectionHeader confidence="0.999847" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9996665">
Data: For our experiments, we employ a large,
recently introduced IMDB movie review dataset
(Maas et al., 2011), in place of the smaller dataset
introduced in (Pang and Lee, 2004) more com-
monly used for sentiment analysis. The dataset
(50,000 reviews) is split evenly between training
and testing sets, each containing a balanced set of
highly polar (&gt; 7 and &lt; 4 stars out of 10) reviews.
Source embeddings: We employ three external
embeddings (obtained from (Turian et al., 2010))
induced using the following models: 1) hierarchi-
cal log-bilinear model (HLBL) (Mnih and Hinton,
2009) and two neural network-based models – 2)
Collobert and Weston’s (C&amp;W) deep-learning ar-
chitecture, and 3) Huang et.al’s polysemous neural
language model (HUANG) (Huang et al., 2012).
C&amp;W and HLBL were induced using a 37M-word
newswire text (Reuters Corpus 1). We also induce
a Latent Semantic Analysis (LSA) based embed-
ding from the subset of the English project Guten-
berg collection of approximately 100M words. No
pre-processing (stemming or stopword removal),
beyond case-normalization is performed in either
the external or LSA-based embedding. For HLBL,
C&amp;W and LSA embeddings, we use two variants
of different dimensionality: 50 and 200. In total,
we obtain seven source embeddings: HLBL-50,
HLBL-200, C&amp;W-50, C&amp;W-200, HUANG-
50, LSA-50, LSA-200.
Baselines: We generate two baseline embeddings
– NULL and RANDOM. NULL is a set of zero
vectors, and RANDOM is a set of uniformly
distributed random vectors with a unit L2-norm.
NULL and RANDOM are treated as source vec-
tors and re-embedded in the same way. The
NULL baseline is equivalent to regularizing on
the target embedding without the source embed-
ding. As additional baselines, we use each of the
7 source embeddings directly as a target without
re-embedding.
Training: For each source embedding matrix 41)S,
we compute the optimal target embedding matrix
41)T by maximizing Equation 1 using the L-BFGS
algorithm. 20 % of the training set (5,000 docu-
ments) is withheld for parameter (A) tuning. We
use LIBLINEAR (Fan et al., 2008) logistic re-
gression module to classify document-level em-
beddings (computed from the 41)Tvj matrix-vector
product). Training (re-embedding and document
classification) on 20,000 documents and a 16,000
word vocabulary takes approximately 5 seconds
on a 3.0 GHz quad-core machine.
</bodyText>
<sectionHeader confidence="0.999746" genericHeader="method">
5 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999719111111111">
The main observation from the results is that our
method improves performance for smaller training
sets (&lt; 5000 examples). The reason for the perfor-
mance boost is expected – classical regularization
of the supervised objective reduces overfitting.
However, comparing to the NULL and RAN-
DOM baseline embeddings, the performance is
improved noticeably (note that a percent differ-
ence of 0.1 corresponds to 20 correctly classi-
fied reviews) for word vectors that incorporate the
source embedding in the regularizer, than those
that do not (NULL), and those that are based on
the random source embedding (RANDOM). We
hypothesize that the external embeddings, gen-
erated from a significantly larger dataset help
“smooth” the word-vectors learned from a small
labeled dataset alone. Further observations in-
clude:
</bodyText>
<page confidence="0.997288">
491
</page>
<table confidence="0.99981">
Features .5K Number of training examples
+ Bag-of-words features
5K 20K .5K 5K 20K
A. Re-embeddings (our method)
HLBL-50 74.01 79.89 80.94 78.90 84.88 85.42
HLBL-200 74.33 80.14 81.05 79.22 85.05 85.95
C&amp;W-50 74.52 79.81 80.48 78.92 84.89 85.87
C&amp;W-200 74.80 80.25 81.15 79.34 85.28 86.15
HUANG-50 74.29 79.90 79.91 79.03 84.89 85.61
LSA-50 72.83 79.67 80.67 78.71 83.44 84.73
LSA-200 73.70 80.03 80.91 79.12 84.83 85.31
B. Baselines
RANDOM-50 w/ re-embedding 72.90 79.12 80.21 78.29 84.01 84.87
RANDOM-200w/re-embedding 72.93 79.20 80.29 78.31 84.08 84.91
NULL w/ re-embedding 72.92 79.18 80.24 78.29 84.10 84.98
HLBL-200 w/o re-embedding 67.88 72.60 73.10 79.02 83.83 85.83
C&amp;W-200 w/o re-embedding 68.17 72.72 73.38 79.30 85.15 86.15
HUANG-50 w/o re-embedding 67.89 72.63 73.12 79.13 84.94 85.99
C. Related methods
Joint training (Maas, 2011) — — 84.65 — — 88.90
Bag of Words SVM — — — 79.17 84.97 86.14
</table>
<tableCaption confidence="0.884851">
Table 1: Classification accuracy for the sentiment task (IMDB
movie review dataset (Maas et al., 2011)). Subtable A compares
performance of the re-embedded vocabulary, induced from a
given source embedding. Subtable B contains a set of base-
lines: X-w/o re-embedding indicates using a source embedding
X directly without re-embedding.
</tableCaption>
<table confidence="0.993031416666667">
BORING
source: lethal, lifestyles, masterpiece ...
target: idiotic, soft-core, gimmicky
BAD
source: past, developing, lesser, .. .
target: ill, madonna, low, ...
DEPRESSING
source: versa, redemption, townsfolk ...
target: hate, pressured, unanswered,
BRILLIANT
source: high-quality, obsession, hate ...
target: all-out, bold, smiling ...
</table>
<tableCaption confidence="0.925296">
Table 2: A representative set of words from the 20 closest-
</tableCaption>
<bodyText confidence="0.996565107142857">
ranked (cosine-distance) words to (boring, bad, depressing,
brilliant) extracted from the source and target (C&amp;W-200)
embeddings. Source embeddings give higher rank to words
that are related, but not necessarily indicative of sentiment,
e.g. brilliant and obsession. Target words tend to be tuned
and ranked higher based on movie-sentiment-based rela-
tions.
Training set size: We note that with a sufficient
number of training instances for each word in the
test set, additional knowledge from an external
embedding does little to improve performance.
Source embeddings: We find C&amp;W embeddings
to perform best for the task of sentiment classi-
fication. These embeddings were found to per-
form well in other NLP tasks as well (Turian et
al., 2010).
Embedding dimensionality: We observe that for
HLBL, C&amp;W and LSA source embeddings (for all
training set sizes), 200 dimensions outperform 50.
While a smaller number of dimensions has been
shown to work better in other tasks (Turian et al.,
2010), re-embedding words may benefit from a
larger initial dimension of the word vector space.
We leave the testing of this hypothesis for future
work.
Additional features: Across all embeddings, ap-
pending the document’s binary bag-of-words rep-
resentation increases classification accuracy.
</bodyText>
<sectionHeader confidence="0.998747" genericHeader="method">
6 Future Work
</sectionHeader>
<bodyText confidence="0.999976782608696">
While “semantic smoothing” obtained from intro-
ducing an external embedding helps to improve
performance in the sentiment classification task,
the method does not help to re-embed words that
do not appear in the training set to begin with. Re-
turning to our example, if we found dramatic and
pleasant to be “far” in the original (source) em-
bedding space, but re-embed them such that they
are “near” (for the task of movie review sentiment
classification, for example), then we might ex-
pect words such as melodramatic, powerful, strik-
ing, enjoyable to be re-embedded nearby as well,
even if they did not appear in the training set.
The objective for this optimization problem can be
posed by requiring that the distance between ev-
ery pair of words in the source and target embed-
dings is preserved as much as possible, i.e. min
( ˆ0iˆ0j − 0i0j)2 ∀i, j (where, with some abuse of
notation, 0 and 0ˆ are the source and target em-
beddings respectively). However, this objective is
no longer convex in the embeddings. Global re-
embedding constitutes our ongoing work and may
pose an interesting challenge to the community.
</bodyText>
<sectionHeader confidence="0.998918" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999898066666667">
We presented a novel approach to adapting exist-
ing word vectors for improving performance in
a text classification task. While we have shown
promising results in a single task, we believe that
the method is general enough to be applied to
a range of supervised tasks and source embed-
dings. As sophistication of unsupervised methods
grows, scaling to ever-more massive datasets, so
will the representational power and coverage of in-
duced word vectors. Techniques for leveraging the
large amount of unsupervised data, but indirectly
through word vectors, can be instrumental in cases
where the data is not directly available, training
time is valuable and a set of easy low-dimensional
“plug-and-play” features is desired.
</bodyText>
<page confidence="0.997646">
492
</page>
<sectionHeader confidence="0.998636" genericHeader="acknowledgments">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999948">
This work was supported in part by the NSF CDI
Grant ECCS 0941561 and the NSF Graduate fel-
lowship. The content of this paper is solely the
responsibility of the authors and does not neces-
sarily represent the official views of the sponsor-
ing organizations. The authors would like to thank
Thorsten Joachims and Bishan Yang for helpful
and insightful discussions.
</bodyText>
<sectionHeader confidence="0.998962" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998952385542169">
Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani.
2006. Manifold regularization: A geometric frame-
work for learning from labeled and unlabeled exam-
ples. The Journal of Machine Learning Research,
7:2399–2434.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. The Journal of Machine Learning Re-
search, 3:1137–1155.
William Blacoe and Mirella Lapata. 2012. A com-
parison of vector-based representations for semantic
composition. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 546–556. Association for Compu-
tational Linguistics.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning, pages 160–167. ACM.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871–1874.
Eric H Huang, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers-Volume 1, pages 873–882. Asso-
ciation for Computational Linguistics.
Dong C Liu and Jorge Nocedal. 1989. On the limited
memory bfgs method for large scale optimization.
Mathematical programming, 45(1-3):503–528.
Andrew L Maas, Raymond E Daly, Peter T Pham, Dan
Huang, Andrew Y Ng, and Christopher Potts. 2011.
Learning word vectors for sentiment analysis. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 142–150. As-
sociation for Computational Linguistics.
Andriy Mnih and Geoffrey E Hinton. 2009. A scalable
hierarchical distributed language model. Advances
in neural information processing systems, 21:1081–
1088.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd annual meeting on Association for Compu-
tational Linguistics, page 271. Association for Com-
putational Linguistics.
Richard Socher, Christopher D Manning, and An-
drew Y Ng. 2010. Learning continuous phrase
representations and syntactic parsing with recursive
neural networks. In Proceedings of the NIPS-2010
Deep Learning and Unsupervised Feature Learning
Workshop.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384–394. Association for
Computational Linguistics.
Jason Weston, Fr´ed´eric Ratle, Hossein Mobahi, and
Ronan Collobert. 2012. Deep learning via semi-
supervised embedding. In Neural Networks: Tricks
of the Trade, pages 639–655. Springer.
Ainur Yessenalina and Claire Cardie. 2011. Com-
positional matrix-space models for sentiment anal-
ysis. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
172–182. Association for Computational Linguis-
tics.
Xiaojin Zhu and Zoubin Ghahramani. 2002. Learning
from labeled and unlabeled data with label propa-
gation. Technical report, Technical Report CMU-
CALD-02-107, Carnegie Mellon University.
</reference>
<page confidence="0.999376">
493
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.967025">
<title confidence="0.999971">Re-embedding Words</title>
<author confidence="0.980129">Igor Labutov Hod Lipson</author>
<affiliation confidence="0.999982">Cornell University Cornell University</affiliation>
<email confidence="0.995362">iil4@cornell.eduhod.lipson@cornell.edu</email>
<abstract confidence="0.9995582">We present a fast method for re-purposing existing semantic word vectors to improve performance in a supervised task. Recently, with an increase in computing resources, it became possible to learn rich word embeddings from massive amounts of unlabeled data. However, some methods take days or weeks to learn good embeddings, and some are notoriously difficult to train. We propose a method that takes as input an existing embedding, some labeled data, and produces an embedding in the same space, but with a better predictive performance in the supervised task. We show improvement on the task of sentiment classification with respect to several baselines, and observe that the approach is most useful when the training set is sufficiently small.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mikhail Belkin</author>
<author>Partha Niyogi</author>
<author>Vikas Sindhwani</author>
</authors>
<title>Manifold regularization: A geometric framework for learning from labeled and unlabeled examples.</title>
<date>2006</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>7--2399</pages>
<contexts>
<context position="5583" citStr="Belkin et al., 2006" startWordPosition="903" endWordPosition="906">iori. Collobert and Weston (2008), in their seminal paper on deep architectures for NLP, propose a multilayer neural network for learning word embeddings. Training of the model, depending on the task, is reported to be between an hour and three days. While the obtained embeddings can be “fine-tuned” using backpropogation for a supervised task, like all multilayer neural network training, optimization is non-convex, and is sensitive to the dimensionality of the hidden layers. In machine learning literature, joint semisupervised embedding takes form in methods such as the LaplacianSVM (LapSVM) (Belkin et al., 2006) and Label Propogation (Zhu and Ghahramani, 2002), to which our approach is related. These methods combine a discriminative learner with a non-linear manifold learning technique in a joint objective, and apply it to a combined set of labeled and unlabeled examples to improve performance in a supervised task. (Weston et al., 2012) take it further by applying this idea to deeplearning architectures. Our method is different in that the (potentially) massive amount of unlabeled data is not required a-priori, but only the resultant embedding. 1as reported by author in private correspondence. The ru</context>
</contexts>
<marker>Belkin, Niyogi, Sindhwani, 2006</marker>
<rawString>Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani. 2006. Manifold regularization: A geometric framework for learning from labeled and unlabeled examples. The Journal of Machine Learning Research, 7:2399–2434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="1079" citStr="Bengio et al., 2003" startWordPosition="169" endWordPosition="172">ome are notoriously difficult to train. We propose a method that takes as input an existing embedding, some labeled data, and produces an embedding in the same space, but with a better predictive performance in the supervised task. We show improvement on the task of sentiment classification with respect to several baselines, and observe that the approach is most useful when the training set is sufficiently small. 1 Introduction Incorporating the vector representation of a word as a feature, has recently been shown to benefit performance in several standard NLP tasks such as language modeling (Bengio et al., 2003; Mnih and Hinton, 2009), POS-tagging and NER (Collobert et al., 2011), parsing (Socher et al., 2010), as well as in sentiment and subjectivity analysis tasks (Maas et al., 2011; Yessenalina and Cardie, 2011). Real-valued word vectors mitigate sparsity by “smoothing” relevant semantic insight gained during the unsupervised training over the rare and unseen terms in the training data. To be effective, these word-representations — and the process by which they are assigned to the words (i.e. embedding) — should capture the semantics relevant to the task. We might, for example, consider dramatic </context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. The Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Blacoe</author>
<author>Mirella Lapata</author>
</authors>
<title>A comparison of vector-based representations for semantic composition.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>546--556</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9189" citStr="Blacoe and Lapata, 2012" startWordPosition="1503" endWordPosition="1506">h the desired target vector space dimension K. The objective is convex in ψ and 41)T, thus, yielding a unique target re-embedding. We employ L-BFGS algorithm (Liu and Nocedal, 1989) to find the optimal target embedding. 3.3 Classification with word vectors To classify documents, re-embedded word vectors can now be used to construct a document-level feature vector for a supervised learning algorithm of choice. Perhaps the most direct approach is to compute a weighted linear combination of the embeddings for words that appear in the document to be classified, as done in (Maas et al., 2011) and (Blacoe and Lapata, 2012). We use the document’s binary bag-of-words vector vj, and compute the document’s vector space representation through the matrix-vector product 41)Tvj. The resulting K + 1-dimensional vector is then cosinenormalized and used as a feature vector to represent the document dj. 4 Experiments Data: For our experiments, we employ a large, recently introduced IMDB movie review dataset (Maas et al., 2011), in place of the smaller dataset introduced in (Pang and Lee, 2004) more commonly used for sentiment analysis. The dataset (50,000 reviews) is split evenly between training and testing sets, each con</context>
</contexts>
<marker>Blacoe, Lapata, 2012</marker>
<rawString>William Blacoe and Mirella Lapata. 2012. A comparison of vector-based representations for semantic composition. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 546–556. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th international conference on Machine learning,</booktitle>
<pages>160--167</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2814" citStr="Collobert and Weston, 2008" startWordPosition="475" endWordPosition="478">C happen to be learned from a much larger dataset, it would make sense to reuse task C embeddings, but adapt them for task A and/or task B. We will refer to task C and its embeddings as the source task and the source embeddings, and task A/B, and its embeddings as the target task and the target embeddings. Traditionally, we would learn the embeddings for the target task jointly with whatever unlabeled data we may have, in an instance of semisupervised learning, and/or we may leverage labels from multiple other related tasks in a multitask approach. Both methods have been applied successfully (Collobert and Weston, 2008) to learn task-specific embeddings. But while joint training is highly effective, a downside is that a large amount of data (and processing time) is required a-priori. In the case of deep neural embeddings, for example, training time can number in days. On the other hand, learned embeddings are becoming more abundant, as much research and computing effort is being invested in learning word representations using large-scale deep architectures trained on web-scale corpora. Many of said embeddings are published and can be harnessed in their raw form as additional features in a number of supervise</context>
<context position="4996" citStr="Collobert and Weston (2008)" startWordPosition="811" endWordPosition="814">lated Work The most relevant to our contribution is the work by Maas et.al (2011), where word vectors are learned specifically for sentiment classification. Embeddings are learned in a semi-supervised fashion, and the components of the embedding are given an explicit probabilistic interpretation. Their method produces state-of-the-art results, however, optimization is non-convex and takes approximately 10 hours on 10 machines1. Naturally, our method is significantly faster because it operates in the space of an existing embedding, and does not require a large amount of training data a-priori. Collobert and Weston (2008), in their seminal paper on deep architectures for NLP, propose a multilayer neural network for learning word embeddings. Training of the model, depending on the task, is reported to be between an hour and three days. While the obtained embeddings can be “fine-tuned” using backpropogation for a supervised task, like all multilayer neural network training, optimization is non-convex, and is sensitive to the dimensionality of the hidden layers. In machine learning literature, joint semisupervised embedding takes form in methods such as the LaplacianSVM (LapSVM) (Belkin et al., 2006) and Label Pr</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160–167. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="1149" citStr="Collobert et al., 2011" startWordPosition="180" endWordPosition="184">akes as input an existing embedding, some labeled data, and produces an embedding in the same space, but with a better predictive performance in the supervised task. We show improvement on the task of sentiment classification with respect to several baselines, and observe that the approach is most useful when the training set is sufficiently small. 1 Introduction Incorporating the vector representation of a word as a feature, has recently been shown to benefit performance in several standard NLP tasks such as language modeling (Bengio et al., 2003; Mnih and Hinton, 2009), POS-tagging and NER (Collobert et al., 2011), parsing (Socher et al., 2010), as well as in sentiment and subjectivity analysis tasks (Maas et al., 2011; Yessenalina and Cardie, 2011). Real-valued word vectors mitigate sparsity by “smoothing” relevant semantic insight gained during the unsupervised training over the rare and unseen terms in the training data. To be effective, these word-representations — and the process by which they are assigned to the words (i.e. embedding) — should capture the semantics relevant to the task. We might, for example, consider dramatic (term X) and pleasant (term Y) to correlate with a review of a good mo</context>
<context position="6271" citStr="Collobert et al., 2011" startWordPosition="1012" endWordPosition="1015">pproach is related. These methods combine a discriminative learner with a non-linear manifold learning technique in a joint objective, and apply it to a combined set of labeled and unlabeled examples to improve performance in a supervised task. (Weston et al., 2012) take it further by applying this idea to deeplearning architectures. Our method is different in that the (potentially) massive amount of unlabeled data is not required a-priori, but only the resultant embedding. 1as reported by author in private correspondence. The runtime can be improved using recently introduced techniques, see (Collobert et al., 2011) 3 Approach Let ΦS, ΦT E R|V |×K be the source and target embedding matrices respectively, where K is the dimension of the word vector space, identical in the source and target embeddings, and V is the set of embedded words, given by VS n VT. Following this notation, φi – the ith row in Φ – is the respective vector representation of word wi E V . In what follows, we first introduce our supervised objective, then combine it with the proposed regularizer and learn the target embedding ΦT by optimizing the resulting joint convex objective. 3.1 Supervised model We model each document dj E D (a mov</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Liblinear: A library for large linear classification.</title>
<date>2008</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="11548" citStr="Fan et al., 2008" startWordPosition="1882" endWordPosition="1885"> uniformly distributed random vectors with a unit L2-norm. NULL and RANDOM are treated as source vectors and re-embedded in the same way. The NULL baseline is equivalent to regularizing on the target embedding without the source embedding. As additional baselines, we use each of the 7 source embeddings directly as a target without re-embedding. Training: For each source embedding matrix 41)S, we compute the optimal target embedding matrix 41)T by maximizing Equation 1 using the L-BFGS algorithm. 20 % of the training set (5,000 documents) is withheld for parameter (A) tuning. We use LIBLINEAR (Fan et al., 2008) logistic regression module to classify document-level embeddings (computed from the 41)Tvj matrix-vector product). Training (re-embedding and document classification) on 20,000 documents and a 16,000 word vocabulary takes approximately 5 seconds on a 3.0 GHz quad-core machine. 5 Results and Discussion The main observation from the results is that our method improves performance for smaller training sets (&lt; 5000 examples). The reason for the performance boost is expected – classical regularization of the supervised objective reduces overfitting. However, comparing to the NULL and RANDOM baseli</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, XiangRui Wang, and Chih-Jen Lin. 2008. Liblinear: A library for large linear classification. The Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric H Huang</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Improving word representations via global context and multiple word prototypes.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1,</booktitle>
<pages>873--882</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="10242" citStr="Huang et al., 2012" startWordPosition="1672" endWordPosition="1675">introduced in (Pang and Lee, 2004) more commonly used for sentiment analysis. The dataset (50,000 reviews) is split evenly between training and testing sets, each containing a balanced set of highly polar (&gt; 7 and &lt; 4 stars out of 10) reviews. Source embeddings: We employ three external embeddings (obtained from (Turian et al., 2010)) induced using the following models: 1) hierarchical log-bilinear model (HLBL) (Mnih and Hinton, 2009) and two neural network-based models – 2) Collobert and Weston’s (C&amp;W) deep-learning architecture, and 3) Huang et.al’s polysemous neural language model (HUANG) (Huang et al., 2012). C&amp;W and HLBL were induced using a 37M-word newswire text (Reuters Corpus 1). We also induce a Latent Semantic Analysis (LSA) based embedding from the subset of the English project Gutenberg collection of approximately 100M words. No pre-processing (stemming or stopword removal), beyond case-normalization is performed in either the external or LSA-based embedding. For HLBL, C&amp;W and LSA embeddings, we use two variants of different dimensionality: 50 and 200. In total, we obtain seven source embeddings: HLBL-50, HLBL-200, C&amp;W-50, C&amp;W-200, HUANG50, LSA-50, LSA-200. Baselines: We generate two bas</context>
</contexts>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>Eric H Huang, Richard Socher, Christopher D Manning, and Andrew Y Ng. 2012. Improving word representations via global context and multiple word prototypes. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 873–882. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong C Liu</author>
<author>Jorge Nocedal</author>
</authors>
<title>On the limited memory bfgs method for large scale optimization.</title>
<date>1989</date>
<booktitle>Mathematical programming,</booktitle>
<pages>45--1</pages>
<contexts>
<context position="8746" citStr="Liu and Nocedal, 1989" startWordPosition="1430" endWordPosition="1433">in the regularizer on the supervised objective. We use Euclidean distance between the source and the target embeddings as the regular490 ization loss. Combined with the supervised objective, the resulting log-likelihood becomes: � argmax log p(sj|wi; 41)T) − A||Δ41)||2F (1) ψ,ΦT dj∈Dwi∈dj where Δ41) = 41)T −41)S, ||·||F is a Frobenius norm, and A is a trade-off parameter. There are almost no restrictions on 41)S, except that it must match the desired target vector space dimension K. The objective is convex in ψ and 41)T, thus, yielding a unique target re-embedding. We employ L-BFGS algorithm (Liu and Nocedal, 1989) to find the optimal target embedding. 3.3 Classification with word vectors To classify documents, re-embedded word vectors can now be used to construct a document-level feature vector for a supervised learning algorithm of choice. Perhaps the most direct approach is to compute a weighted linear combination of the embeddings for words that appear in the document to be classified, as done in (Maas et al., 2011) and (Blacoe and Lapata, 2012). We use the document’s binary bag-of-words vector vj, and compute the document’s vector space representation through the matrix-vector product 41)Tvj. The r</context>
</contexts>
<marker>Liu, Nocedal, 1989</marker>
<rawString>Dong C Liu and Jorge Nocedal. 1989. On the limited memory bfgs method for large scale optimization. Mathematical programming, 45(1-3):503–528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew L Maas</author>
<author>Raymond E Daly</author>
<author>Peter T Pham</author>
<author>Dan Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Learning word vectors for sentiment analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume</booktitle>
<volume>1</volume>
<pages>142--150</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1256" citStr="Maas et al., 2011" startWordPosition="199" endWordPosition="202">etter predictive performance in the supervised task. We show improvement on the task of sentiment classification with respect to several baselines, and observe that the approach is most useful when the training set is sufficiently small. 1 Introduction Incorporating the vector representation of a word as a feature, has recently been shown to benefit performance in several standard NLP tasks such as language modeling (Bengio et al., 2003; Mnih and Hinton, 2009), POS-tagging and NER (Collobert et al., 2011), parsing (Socher et al., 2010), as well as in sentiment and subjectivity analysis tasks (Maas et al., 2011; Yessenalina and Cardie, 2011). Real-valued word vectors mitigate sparsity by “smoothing” relevant semantic insight gained during the unsupervised training over the rare and unseen terms in the training data. To be effective, these word-representations — and the process by which they are assigned to the words (i.e. embedding) — should capture the semantics relevant to the task. We might, for example, consider dramatic (term X) and pleasant (term Y) to correlate with a review of a good movie (task A), while finding them of opposite polarity in the context of a dating profile (task B). Conseque</context>
<context position="7352" citStr="Maas et al., 2011" startWordPosition="1211" endWordPosition="1214">arn the target embedding ΦT by optimizing the resulting joint convex objective. 3.1 Supervised model We model each document dj E D (a movie review, for example) as a collection of words wij (i.i.d samples). We assign a sentiment label sj E 10, 11 to each document (converting the star rating to a binary label), and seek to optimize the conditional likelihood of the labels (sj)j∈{1,...,|D|}, given the embeddings and the documents: �p(s1, ..., s|D||D; ΦT) = ri p(sj|wi; ΦT) dj∈D wi∈dj where p(sj = 1|wi, ΦT) is the probability of assigning a positive label to document j, given that wi E dj. As in (Maas et al., 2011), we use logistic regression to model the conditional likelihood: 1 p(sj = 1|wi; ΦT) = 1 + exp(�ψT φi) where ψ E RK+1 is a regression parameter vector with an included bias component. Maximizing the log-likelihood directly (for ψ and ΦT), especially on small datasets, will result in severe overfitting, as learning will tend to commit neutral words to either polarity. Classical regularization will mitigate this effect, but can be improved further by introducing an external embedding in the regularizer. In what follows, we describe re-embedding regularization— employing existing (source) embeddi</context>
<context position="9159" citStr="Maas et al., 2011" startWordPosition="1498" endWordPosition="1501">except that it must match the desired target vector space dimension K. The objective is convex in ψ and 41)T, thus, yielding a unique target re-embedding. We employ L-BFGS algorithm (Liu and Nocedal, 1989) to find the optimal target embedding. 3.3 Classification with word vectors To classify documents, re-embedded word vectors can now be used to construct a document-level feature vector for a supervised learning algorithm of choice. Perhaps the most direct approach is to compute a weighted linear combination of the embeddings for words that appear in the document to be classified, as done in (Maas et al., 2011) and (Blacoe and Lapata, 2012). We use the document’s binary bag-of-words vector vj, and compute the document’s vector space representation through the matrix-vector product 41)Tvj. The resulting K + 1-dimensional vector is then cosinenormalized and used as a feature vector to represent the document dj. 4 Experiments Data: For our experiments, we employ a large, recently introduced IMDB movie review dataset (Maas et al., 2011), in place of the smaller dataset introduced in (Pang and Lee, 2004) more commonly used for sentiment analysis. The dataset (50,000 reviews) is split evenly between train</context>
<context position="13670" citStr="Maas et al., 2011" startWordPosition="2208" endWordPosition="2211">80.03 80.91 79.12 84.83 85.31 B. Baselines RANDOM-50 w/ re-embedding 72.90 79.12 80.21 78.29 84.01 84.87 RANDOM-200w/re-embedding 72.93 79.20 80.29 78.31 84.08 84.91 NULL w/ re-embedding 72.92 79.18 80.24 78.29 84.10 84.98 HLBL-200 w/o re-embedding 67.88 72.60 73.10 79.02 83.83 85.83 C&amp;W-200 w/o re-embedding 68.17 72.72 73.38 79.30 85.15 86.15 HUANG-50 w/o re-embedding 67.89 72.63 73.12 79.13 84.94 85.99 C. Related methods Joint training (Maas, 2011) — — 84.65 — — 88.90 Bag of Words SVM — — — 79.17 84.97 86.14 Table 1: Classification accuracy for the sentiment task (IMDB movie review dataset (Maas et al., 2011)). Subtable A compares performance of the re-embedded vocabulary, induced from a given source embedding. Subtable B contains a set of baselines: X-w/o re-embedding indicates using a source embedding X directly without re-embedding. BORING source: lethal, lifestyles, masterpiece ... target: idiotic, soft-core, gimmicky BAD source: past, developing, lesser, .. . target: ill, madonna, low, ... DEPRESSING source: versa, redemption, townsfolk ... target: hate, pressured, unanswered, BRILLIANT source: high-quality, obsession, hate ... target: all-out, bold, smiling ... Table 2: A representative set </context>
</contexts>
<marker>Maas, Daly, Pham, Huang, Ng, Potts, 2011</marker>
<rawString>Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1, pages 142–150. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>A scalable hierarchical distributed language model. Advances in neural information processing systems,</title>
<date>2009</date>
<pages>21--1081</pages>
<contexts>
<context position="1103" citStr="Mnih and Hinton, 2009" startWordPosition="173" endWordPosition="176">ifficult to train. We propose a method that takes as input an existing embedding, some labeled data, and produces an embedding in the same space, but with a better predictive performance in the supervised task. We show improvement on the task of sentiment classification with respect to several baselines, and observe that the approach is most useful when the training set is sufficiently small. 1 Introduction Incorporating the vector representation of a word as a feature, has recently been shown to benefit performance in several standard NLP tasks such as language modeling (Bengio et al., 2003; Mnih and Hinton, 2009), POS-tagging and NER (Collobert et al., 2011), parsing (Socher et al., 2010), as well as in sentiment and subjectivity analysis tasks (Maas et al., 2011; Yessenalina and Cardie, 2011). Real-valued word vectors mitigate sparsity by “smoothing” relevant semantic insight gained during the unsupervised training over the rare and unseen terms in the training data. To be effective, these word-representations — and the process by which they are assigned to the words (i.e. embedding) — should capture the semantics relevant to the task. We might, for example, consider dramatic (term X) and pleasant (t</context>
<context position="10061" citStr="Mnih and Hinton, 2009" startWordPosition="1645" endWordPosition="1648">present the document dj. 4 Experiments Data: For our experiments, we employ a large, recently introduced IMDB movie review dataset (Maas et al., 2011), in place of the smaller dataset introduced in (Pang and Lee, 2004) more commonly used for sentiment analysis. The dataset (50,000 reviews) is split evenly between training and testing sets, each containing a balanced set of highly polar (&gt; 7 and &lt; 4 stars out of 10) reviews. Source embeddings: We employ three external embeddings (obtained from (Turian et al., 2010)) induced using the following models: 1) hierarchical log-bilinear model (HLBL) (Mnih and Hinton, 2009) and two neural network-based models – 2) Collobert and Weston’s (C&amp;W) deep-learning architecture, and 3) Huang et.al’s polysemous neural language model (HUANG) (Huang et al., 2012). C&amp;W and HLBL were induced using a 37M-word newswire text (Reuters Corpus 1). We also induce a Latent Semantic Analysis (LSA) based embedding from the subset of the English project Gutenberg collection of approximately 100M words. No pre-processing (stemming or stopword removal), beyond case-normalization is performed in either the external or LSA-based embedding. For HLBL, C&amp;W and LSA embeddings, we use two varian</context>
</contexts>
<marker>Mnih, Hinton, 2009</marker>
<rawString>Andriy Mnih and Geoffrey E Hinton. 2009. A scalable hierarchical distributed language model. Advances in neural information processing systems, 21:1081– 1088.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd annual meeting on Association for Computational Linguistics,</booktitle>
<pages>271</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9657" citStr="Pang and Lee, 2004" startWordPosition="1580" endWordPosition="1583">near combination of the embeddings for words that appear in the document to be classified, as done in (Maas et al., 2011) and (Blacoe and Lapata, 2012). We use the document’s binary bag-of-words vector vj, and compute the document’s vector space representation through the matrix-vector product 41)Tvj. The resulting K + 1-dimensional vector is then cosinenormalized and used as a feature vector to represent the document dj. 4 Experiments Data: For our experiments, we employ a large, recently introduced IMDB movie review dataset (Maas et al., 2011), in place of the smaller dataset introduced in (Pang and Lee, 2004) more commonly used for sentiment analysis. The dataset (50,000 reviews) is split evenly between training and testing sets, each containing a balanced set of highly polar (&gt; 7 and &lt; 4 stars out of 10) reviews. Source embeddings: We employ three external embeddings (obtained from (Turian et al., 2010)) induced using the following models: 1) hierarchical log-bilinear model (HLBL) (Mnih and Hinton, 2009) and two neural network-based models – 2) Collobert and Weston’s (C&amp;W) deep-learning architecture, and 3) Huang et.al’s polysemous neural language model (HUANG) (Huang et al., 2012). C&amp;W and HLBL </context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Bo Pang and Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd annual meeting on Association for Computational Linguistics, page 271. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning continuous phrase representations and syntactic parsing with recursive neural networks.</title>
<date>2010</date>
<booktitle>In Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop.</booktitle>
<contexts>
<context position="1180" citStr="Socher et al., 2010" startWordPosition="186" endWordPosition="189">g, some labeled data, and produces an embedding in the same space, but with a better predictive performance in the supervised task. We show improvement on the task of sentiment classification with respect to several baselines, and observe that the approach is most useful when the training set is sufficiently small. 1 Introduction Incorporating the vector representation of a word as a feature, has recently been shown to benefit performance in several standard NLP tasks such as language modeling (Bengio et al., 2003; Mnih and Hinton, 2009), POS-tagging and NER (Collobert et al., 2011), parsing (Socher et al., 2010), as well as in sentiment and subjectivity analysis tasks (Maas et al., 2011; Yessenalina and Cardie, 2011). Real-valued word vectors mitigate sparsity by “smoothing” relevant semantic insight gained during the unsupervised training over the rare and unseen terms in the training data. To be effective, these word-representations — and the process by which they are assigned to the words (i.e. embedding) — should capture the semantics relevant to the task. We might, for example, consider dramatic (term X) and pleasant (term Y) to correlate with a review of a good movie (task A), while finding the</context>
</contexts>
<marker>Socher, Manning, Ng, 2010</marker>
<rawString>Richard Socher, Christopher D Manning, and Andrew Y Ng. 2010. Learning continuous phrase representations and syntactic parsing with recursive neural networks. In Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>384--394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3443" citStr="Turian et al., 2010" startWordPosition="577" endWordPosition="580"> task-specific embeddings. But while joint training is highly effective, a downside is that a large amount of data (and processing time) is required a-priori. In the case of deep neural embeddings, for example, training time can number in days. On the other hand, learned embeddings are becoming more abundant, as much research and computing effort is being invested in learning word representations using large-scale deep architectures trained on web-scale corpora. Many of said embeddings are published and can be harnessed in their raw form as additional features in a number of supervised tasks (Turian et al., 2010). It would, thus, be advantageous to learn a task-specific embedding directly from another (source) embedding. In this paper we propose a fast method for reembedding words from a source embedding S to a target embedding T by performing unconstrained optimization of a convex objective. Our objective is a linear combination of the dataset’s log489 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 489–493, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics likelihood under the target embedding and the Frobenius norm of t</context>
<context position="9958" citStr="Turian et al., 2010" startWordPosition="1630" endWordPosition="1633">j. The resulting K + 1-dimensional vector is then cosinenormalized and used as a feature vector to represent the document dj. 4 Experiments Data: For our experiments, we employ a large, recently introduced IMDB movie review dataset (Maas et al., 2011), in place of the smaller dataset introduced in (Pang and Lee, 2004) more commonly used for sentiment analysis. The dataset (50,000 reviews) is split evenly between training and testing sets, each containing a balanced set of highly polar (&gt; 7 and &lt; 4 stars out of 10) reviews. Source embeddings: We employ three external embeddings (obtained from (Turian et al., 2010)) induced using the following models: 1) hierarchical log-bilinear model (HLBL) (Mnih and Hinton, 2009) and two neural network-based models – 2) Collobert and Weston’s (C&amp;W) deep-learning architecture, and 3) Huang et.al’s polysemous neural language model (HUANG) (Huang et al., 2012). C&amp;W and HLBL were induced using a 37M-word newswire text (Reuters Corpus 1). We also induce a Latent Semantic Analysis (LSA) based embedding from the subset of the English project Gutenberg collection of approximately 100M words. No pre-processing (stemming or stopword removal), beyond case-normalization is perfo</context>
<context position="15041" citStr="Turian et al., 2010" startWordPosition="2410" endWordPosition="2413">dings. Source embeddings give higher rank to words that are related, but not necessarily indicative of sentiment, e.g. brilliant and obsession. Target words tend to be tuned and ranked higher based on movie-sentiment-based relations. Training set size: We note that with a sufficient number of training instances for each word in the test set, additional knowledge from an external embedding does little to improve performance. Source embeddings: We find C&amp;W embeddings to perform best for the task of sentiment classification. These embeddings were found to perform well in other NLP tasks as well (Turian et al., 2010). Embedding dimensionality: We observe that for HLBL, C&amp;W and LSA source embeddings (for all training set sizes), 200 dimensions outperform 50. While a smaller number of dimensions has been shown to work better in other tasks (Turian et al., 2010), re-embedding words may benefit from a larger initial dimension of the word vector space. We leave the testing of this hypothesis for future work. Additional features: Across all embeddings, appending the document’s binary bag-of-words representation increases classification accuracy. 6 Future Work While “semantic smoothing” obtained from introducing</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 384–394. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Weston</author>
<author>Fr´ed´eric Ratle</author>
<author>Hossein Mobahi</author>
<author>Ronan Collobert</author>
</authors>
<title>Deep learning via semisupervised embedding.</title>
<date>2012</date>
<booktitle>In Neural Networks: Tricks of the Trade,</booktitle>
<pages>639--655</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="5914" citStr="Weston et al., 2012" startWordPosition="957" endWordPosition="960">pervised task, like all multilayer neural network training, optimization is non-convex, and is sensitive to the dimensionality of the hidden layers. In machine learning literature, joint semisupervised embedding takes form in methods such as the LaplacianSVM (LapSVM) (Belkin et al., 2006) and Label Propogation (Zhu and Ghahramani, 2002), to which our approach is related. These methods combine a discriminative learner with a non-linear manifold learning technique in a joint objective, and apply it to a combined set of labeled and unlabeled examples to improve performance in a supervised task. (Weston et al., 2012) take it further by applying this idea to deeplearning architectures. Our method is different in that the (potentially) massive amount of unlabeled data is not required a-priori, but only the resultant embedding. 1as reported by author in private correspondence. The runtime can be improved using recently introduced techniques, see (Collobert et al., 2011) 3 Approach Let ΦS, ΦT E R|V |×K be the source and target embedding matrices respectively, where K is the dimension of the word vector space, identical in the source and target embeddings, and V is the set of embedded words, given by VS n VT. </context>
</contexts>
<marker>Weston, Ratle, Mobahi, Collobert, 2012</marker>
<rawString>Jason Weston, Fr´ed´eric Ratle, Hossein Mobahi, and Ronan Collobert. 2012. Deep learning via semisupervised embedding. In Neural Networks: Tricks of the Trade, pages 639–655. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ainur Yessenalina</author>
<author>Claire Cardie</author>
</authors>
<title>Compositional matrix-space models for sentiment analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>172--182</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1287" citStr="Yessenalina and Cardie, 2011" startWordPosition="203" endWordPosition="206">rformance in the supervised task. We show improvement on the task of sentiment classification with respect to several baselines, and observe that the approach is most useful when the training set is sufficiently small. 1 Introduction Incorporating the vector representation of a word as a feature, has recently been shown to benefit performance in several standard NLP tasks such as language modeling (Bengio et al., 2003; Mnih and Hinton, 2009), POS-tagging and NER (Collobert et al., 2011), parsing (Socher et al., 2010), as well as in sentiment and subjectivity analysis tasks (Maas et al., 2011; Yessenalina and Cardie, 2011). Real-valued word vectors mitigate sparsity by “smoothing” relevant semantic insight gained during the unsupervised training over the rare and unseen terms in the training data. To be effective, these word-representations — and the process by which they are assigned to the words (i.e. embedding) — should capture the semantics relevant to the task. We might, for example, consider dramatic (term X) and pleasant (term Y) to correlate with a review of a good movie (task A), while finding them of opposite polarity in the context of a dating profile (task B). Consequently, good vectors for X and Y </context>
</contexts>
<marker>Yessenalina, Cardie, 2011</marker>
<rawString>Ainur Yessenalina and Claire Cardie. 2011. Compositional matrix-space models for sentiment analysis. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 172–182. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Learning from labeled and unlabeled data with label propagation.</title>
<date>2002</date>
<tech>Technical report, Technical Report CMUCALD-02-107,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="5632" citStr="Zhu and Ghahramani, 2002" startWordPosition="910" endWordPosition="914">seminal paper on deep architectures for NLP, propose a multilayer neural network for learning word embeddings. Training of the model, depending on the task, is reported to be between an hour and three days. While the obtained embeddings can be “fine-tuned” using backpropogation for a supervised task, like all multilayer neural network training, optimization is non-convex, and is sensitive to the dimensionality of the hidden layers. In machine learning literature, joint semisupervised embedding takes form in methods such as the LaplacianSVM (LapSVM) (Belkin et al., 2006) and Label Propogation (Zhu and Ghahramani, 2002), to which our approach is related. These methods combine a discriminative learner with a non-linear manifold learning technique in a joint objective, and apply it to a combined set of labeled and unlabeled examples to improve performance in a supervised task. (Weston et al., 2012) take it further by applying this idea to deeplearning architectures. Our method is different in that the (potentially) massive amount of unlabeled data is not required a-priori, but only the resultant embedding. 1as reported by author in private correspondence. The runtime can be improved using recently introduced t</context>
</contexts>
<marker>Zhu, Ghahramani, 2002</marker>
<rawString>Xiaojin Zhu and Zoubin Ghahramani. 2002. Learning from labeled and unlabeled data with label propagation. Technical report, Technical Report CMUCALD-02-107, Carnegie Mellon University.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>