<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9957055">
Learning to Adapt Credible Knowledge in Cross-lingual Sentiment
Analysis
</title>
<author confidence="0.995756">
Qiang Chen*,†, Wenjie Li†,*, Yu Lei†, Xule Liu*, Yanxiang He*,$
</author>
<affiliation confidence="0.825567">
*School of Computer Science, Wuhan University, China
†Department of Computing, The Hong Kong Polytechnic University, Hong Kong
*Hong Kong Polytechnic University Shenzhen Research Institute, China
$The State Key Lab of Software Engineering, Wuhan University, China
</affiliation>
<email confidence="0.6900505">
*{qchen, xuleliu, yxhe}@whu.edu.cn
†{csqchen, cswjli, csylei}@comp.polyu.edu.hk
</email>
<sectionHeader confidence="0.981091" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997124">
Cross-lingual sentiment analysis is a task
of identifying sentiment polarities of texts
in a low-resource language by using sen-
timent knowledge in a resource-abundant
language. While most existing approaches
are driven by transfer learning, their
performance does not reach to a promising
level due to the transferred errors. In this
paper, we propose to integrate into knowl-
edge transfer a knowledge validation mod-
el, which aims to prevent the negative
influence from the wrong knowledge by
distinguishing highly credible knowledge.
Experiment results demonstrate the neces-
sity and effectiveness of the model.
</bodyText>
<sectionHeader confidence="0.992547" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999980977272728">
With the wide range of business value, sentiment
analysis has drawn increasing attention in the past
years. The extensive research and development
efforts produce a variety of reliable sentiment
resources for English, one of the most popular
language in the world. These available rich
resources become the treasure of knowledge to
help conduct or enhance sentiment analysis in
the other languages, which is a task known as
cross-lingual sentiment analysis (CLSA). In the
literature of CSLA, the language with abundant
reliable resources is called the source language
(e.g., English), while the low-resource language is
referred to as the target language (e.g., Chinese).
However, in this paper, the situation is a low
resource language scenario, where the source
language is English, and the target language is
Chinese.
The main idea of existing CLSA researches is
to first build up the connection between the source
and target languages to overcome the language
barrier, and then develop an appropriate knowl-
edge transfer approach to leverage the annotated
data from the source language to train a sentiment
classification model in the target language, either
supervised or semi-supervised. In particular, these
approaches exploit and convert the knowledge
learned from the source language to automatically
generate and expand the pseudo-training data for
the target language.
The machine translation (MT) service is one
of the most common ways used to build the
language connection (Wan, 2008; Banea et al.,
2008; Wan, 2009; Wei and Pal, 2010; Gui et
al., 2014). Although it is claimed in Duh et al.
(2011) that the MT service is ripe for CLSA,
the imperfect MT quality hinders existing MT-
based CLSA approaches from the further advance.
In our preliminary study, we find that even the
Google translator1 (i.e., one of the most widely
used online MT service (Shankland 2013)) may
unavoidably changes the sentiment polarity of
the translated text, as illustrated below, with a
percentage of around 10%.
</bodyText>
<note confidence="0.692039285714286">
[Original English Text]: I am at home on bed
rest and desperate for something good to read.
[Sentiment Label: Negative]
[Translated Chinese Text]: At FM�IWP,
*pffflM3WRff . {Meaning: I am in bed
to rest at home and feel that desperate things are
also good to read.}[Sentiment Label: Positive]
</note>
<bodyText confidence="0.999586454545455">
The noisy data generated by MT errors for sure
will weaken the contribution of the transferred
knowledge and even worse may create conflicting
knowledge. While it is a critical step in CLSA to
localize the sentiment knowledge learned from the
source language in the target language, to the best
of our knowledge, hardly any previous research
has focused on knowledge validation to filter out
the noisy knowledge having sentiment changes
caused by wrong translations during knowledge
transfer.
</bodyText>
<footnote confidence="0.967257">
1http://translate.google.com
</footnote>
<page confidence="0.477048">
419
</page>
<note confidence="0.998665">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 419–429,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999951185185185">
To reduce the noisy sentiment knowledge intro-
duced into the target language, we are motivated to
validate the knowledge transferred from the source
language by checking its linguistic distributions
and sentiment polarity consistency with the known
knowledge in the target language. Different
from previous co-training based approaches where
two language views recommend knowledge to
each other in the same manner, we consider
the source language as the “supervisor” and the
target language as the “learner”. The “supervisor”
boosts itself with its own accumulated labeled data
(called knowledge) and meanwhile recommends
its confident knowledge to the “learner”. The
“learner” tries to select trustworthy knowledge
based on the recommendation to update and
expand its training data. Adding a process to
efficiently filter out noisy knowledge and retain the
self-adaptive and interested new knowledge makes
the subsequent boosting process more credible.
This is why our approach can outperform state-of-
the-art CLSA approaches.
The rest of this paper is organized as follows.
Section 2 summarizes the related work. Section 3
explains the proposed model. Section 4 presents
experimental results. Finally, Section 5 concludes
the paper and suggests future work.
</bodyText>
<sectionHeader confidence="0.999694" genericHeader="related work">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.999671">
2.1 Sentiment Analysis
</subsectionHeader>
<bodyText confidence="0.999586351351351">
Sentiment has been analyzed in different language
granularity, e.g., entity, aspect, sentence and
document. This paper focuses on sentiment
analysis of online product reviews in the document
level.
Existing approaches are generally categorized
into lexicon-based and machine learning based
approaches (Liu, 2012). Lexicon-based approach-
es highly depend on sentiment lexicons. Turney
(2002) derives the overall phrase and document
sentiment scores by averaging the sentiment
scores provided in a lexicon over the words
included. Similar idea is adopted in (Hiroshi et
al., 2004; Kennedy and Inkpen, 2006). Machine
learning based approaches, on the other hand,
apply classification models. The task-specific
features are designed to train sentiment polarity
classifiers. Pang et al. (2002) compare the
performance of NB, SVM and ME on movie
reviews. SVM is found more effective. Gamon
(2004) shows that SVM with deep linguistic
features can further improve the performance. A
variety of other machine learning approaches are
also proposed to sentiment classification (Mullen
and Collier, 2004; Read, 2005; Hassan and Radev,
2010; Socher et al., 2013).
Cross-domain sentiment classification (CDSC)
shares certain common characteristics with cross-
lingual sentiment classification (CLSC) (Tan et al.,
2007; Li et al., 2009; Pan and Yang, 2010; He et
al., 2011a; Glorot et al., 2011). Notice that the gap
between source domain and target domain is the
main difference between CDSC and CLSC. CLSC
copes with two different datasets in two different
languages. This difference makes CLSC a new
challenge, drawing specific attention to researcher
recently.
</bodyText>
<subsectionHeader confidence="0.998375">
2.2 Cross-lingual Sentiment Analysis
</subsectionHeader>
<bodyText confidence="0.999979969696969">
There are two alternative solutions to cross-lingual
sentiment analysis. One is ensemble learning
that combines multiple classifiers. The other is
transfer learning that develops strategies to adapt
the knowledge from one language to the other.
Wan (2008) is among the pioneers to develop
the ensemble learning solutions, where multiple
classifiers learned from different training datasets
including those in original languages and trans-
lated languages are combined by voting. Most
researches, on the other hand, explore transfer
learning and focus on knowledge adaptation. For
example, Wan (2009) applies a supervised co-
training framework to iteratively adapt knowledge
learned from the two languages by transferring
translated texts to each other. Other similar work
includes (Wei and Pal, 2010) and (He, 2011b). All
these approaches rely on MT to build language
connection.
Meanwhile, the unlabeled parallel data is also
employed to fill the gap between two languages.
To solve the feature coverage problem with the
EM algorithm, Meng et al. (2012) leverage the
unlabeled parallel data to learn unseen sentiment
words. Similarly, Popat et al. (2013) use the
unlabeled parallel data to cluster features in order
to reduce the data sparsity problem. Meng et
al. (2012) and Popat et al. (2013) also use
the unlabeled parallel data to reduce the negative
influence of the noisy and incorrect sentiment
labels introduced by machine translation and
knowledge transfer. However, the parallel data is
also a scarce resource.
</bodyText>
<page confidence="0.770457">
420
</page>
<bodyText confidence="0.99990675">
Some existing transfer learning based CLSA
methods have attempted to address the noisy
knowledge problem caused by wrong labels by
checking label consistency. For example, to
filter out the unconfident labels in Chinese, the
supervised learning method proposed by (Xu et
al., 2011) runs boosting in Chinese by checking
consistency between the labels manually annotat-
ed in English and predicted by Chinese classifiers
on translated Chinese. The work in (Gui et al.,
2014) follows the same line although it considers
knowledge transferring between two languages.
On the contrary, the main focus of our work is
to filter out the noisy knowledge having sentiment
changes by wrong translations. Actually, both
label consistency checking and linguistic distribu-
tion checking are important. Any one alone cannot
work well. In fact, both of them are considered as
the knowledge validation in our work, though the
later is our focus.
</bodyText>
<sectionHeader confidence="0.984843" genericHeader="method">
3 Credible Boosting Model
</sectionHeader>
<bodyText confidence="0.999966333333333">
In this paper, we propose a knowledge validation
approach to improve the effectiveness of knowl-
edge transfer without directly using extra parallel
data. Our target is to filter out the noisy senti-
ment labels introduced by MT and the incorrect
sentiment labels generated by imperfect classifier
in the source language. Here, the knowledge is
referred to as a collection of distributed document
presentations with sentiment labels that have been
verified to be robust in sentiment classification (Le
and Mikolov, 2014). A novel credible boosting
model, namely CredBoost is proposed to apply
transfer-supervised learning with an added self-
validation mechanism to guarantee the knowledge
transferred highly credible and self-adaptive.
</bodyText>
<subsectionHeader confidence="0.998962">
3.1 Problem Description
</subsectionHeader>
<bodyText confidence="0.9552632">
In a standard cross-lingual sentiment analysis
setting, the training data includes labeled English
reviews LEN = {(xlen
i , yi)}Mi=1 and unlabeled
Chinese reviews UCN = {xucn
</bodyText>
<equation confidence="0.9909508">
j }N j=1, where xki
(k = len or ucn) represents review i and yi ∈
{−1, 1} is the sentiment label of review xli. The
test data is Chinese reviews TCN = {xtcn
s }Ss=1.
</equation>
<bodyText confidence="0.9999188">
We now introduce the unlabeled data into
credBoost’s setting. LEN is divided into two
disjoint parts LTEN and LBEN, where LTEN for basic
training and LBEN for self-boosting. We translate
LEN into Chinese to obtain extra labeled Chinese
</bodyText>
<equation confidence="0.7588674">
pseudo-reviews LTrCN = {(xlcnTr
i , yi)}Mi=1 and
UCN into English to obtain extra unlabeled
lenTr
English pseudo-reviews UTrEN = {x }N
</equation>
<bodyText confidence="0.929862454545454">
j=1.
Thereby, we obtain a pair of pseudo-parallel data
(UCN, UTrEN).
The task is to use LEN and UCN to train a
Chinese classifier to predict sentiment polarity for
the test data TCN. It is a standard transfer learning
problem. We consider two language views, i.e.,
source language view Ds and target language
view Dτ. Ds boosts itself with the labeled English
data and recommend translated knowledge to Dτ,
while Dt selects self-adaptive ones to boost itself.
</bodyText>
<subsectionHeader confidence="0.999891">
3.2 Framework of CredBoost
</subsectionHeader>
<bodyText confidence="0.985186066666667">
The CredBoost model involves two synchronously
boosting views for two languages respectively.
During training, one view acts as a “supervisor”
that recommends and passes the knowledge to the
other view. The same knowledge is also added
into its own view for boosting by automatically
updating the weights of the labeled data. The
other view acts as a “learner” that receives the
recommended knowledge and selects the best-
suited new knowledge to learn.
As mentioned before, the knowledge trans-
ferred through MT is not reliable. The source
language view may also make wrong predictions
and thus transfer the wrong knowledge to the
target language even the translations are correct.
Whether or not the “learner” can benefit from
its “supervisor” and how much it benefits highly
depends on the credibility and adaptiveness of
the recommended knowledge accepted by the
“learner”. Knowledge validation is necessary to
ensure the quality of learning. The objective
of knowledge validation is to identify the new
and acquired knowledge from recommendations.
Both language views are iteratively trained until
learning converges or reaches the iteration upper
bound.
In the source language view, at iteration (t),
the CredBoost model first uses LT (t)
EN to train a
basic classifier C(t) ENand then uses C(t) ENto predict
</bodyText>
<equation confidence="0.978509">
LB(t)
EN and U(t)
T rEN. Top m and top n instances are
sampled from LB(t)
EN and U(t)
T rEN respectively, by
Formula (1) :
(t) _ ff// LBˆLB m
OEN - llxil , yil)}ii=1 (1)
TR(t) EN= {(xUTr,
</equation>
<bodyText confidence="0.7095245">
where O(t)
EN denotes the candidates to be added
</bodyText>
<equation confidence="0.9748818">
U
n
ˆy
)
Tr
en
�
i
}i=1
421
</equation>
<bodyText confidence="0.984003636363636">
into the training data, and TR(t) ENthe knowledge
to be recommended to the target language view.
We use the source knowledge validation function
VS(O(t)
EN) to identify the acquired knowledge
K(t) fAclearned in the previous learning process and
the new knowledge K(t) fNwfresh to the current
knowledge system from O(t)
EN. The importance of
each training instance is updated according to the
performance of prediction by Formula (2) :
</bodyText>
<equation confidence="0.9921204">
q
e�(t) · ν(t) i0· c(t)i0 =6 y0Ac
if ˆy0Ac
i0 i0
qνi(t) · c(t) otherwise;
i0
Ac
Nw — re&apos;(t) · log (1 + √e · cit)) if ˆy3Ac=6yi0
ωj0 (Jll log (1 + √e · c(t)
j0 ) otherwise.
</equation>
<bodyText confidence="0.91058737037037">
where c(t)
jf is the confidence of an instance
given by C(t)
EN, thus log (1 + √e · c(t)
jf ) &gt; 1 is to
enhance the weight of new knowledge because of
the higher significance contributing to the later
learning. v(t)
if (&lt; 1) is the adaptiveness score
given by the source knowledge validation function
VS(O(t)
EN). c(t)(&gt; 1) is the error rate of C(t)
EN,
thus e&apos;(t) &gt; 1 is to reward the wrongly predicted
data in the next iteration. ˆyif is the label
fAc
given by C(t) ENand yfAc
if is the manually annotated
label. For the incorrectly predicted instance, the
weight is boosted inversely to the performance
of the current classifier. The instance identified
as the new knowledge which contributes more
to performance improvement is given a reward
parameter to enhance its significant in the next
training iteration. Data sets update by Formula (3).
The training starts with iteration (1), the training
data is initially set as LT (1)
</bodyText>
<equation confidence="0.988511166666667">
EN = LTEN.
LT (t+1)
EN = LT (t)
EN ∪ K(t)
0Ac ∪ K(t)
0Nw
</equation>
<bodyText confidence="0.882329833333333">
the source language view. The confidence c(t)
i
is directly transferred from Ds. We reward the
validated knowledge to raise their significance in
the training data considering they are originally
Chinese.
</bodyText>
<equation confidence="0.968187">
ωAc = qci(t) · log(1 + √e · v�t))
log (1+�e·c(t)
j ) = 1 + √e · c(t)
j
</equation>
<bodyText confidence="0.998594666666667">
We update the data setting by Formula (5). The
training data is initially set as UT (1) = UTCN . The
CN CredBoost model is illustrated in Algorithm 1.
</bodyText>
<equation confidence="0.9069815">
L(t+1)
T rCN = L(t) T rCN/U KAt) ∪ KNt)
UCtN1) = UCtN − (K/A. ∪ KNE )
UT 1N = UT )EN − IK�Ac ∪ K(t)
�Nw)
Algorithm 1 CredBoost Model
</equation>
<bodyText confidence="0.968140333333333">
Input: English labeled data LTEN and LBEN, translated
English unlabeled data UTrEN, translated Chinese data
LTrCN and unlabeled Chinese data UCN;
</bodyText>
<equation confidence="0.925670666666667">
Initialize: Weights W (1)
EN = {1}M for LTEN and
W(1)
TrCN = {1}M for LTrCN;
For t = 1,··· ,T:
1. Use LT (t)
</equation>
<bodyText confidence="0.160851">
EN to learn English classifier CEN(t);
</bodyText>
<figure confidence="0.9209109375">
2. Use C(t) ENto predict LB(t)
EN and U(t)
T rEN sample top
m and top n instances from LB(t)
EN and U(t)
T rEN, O(t)
EN and
TR(t)
EN;
3. Validate O(t)
EN by knowledge validation function
VS(O(t)
EN) to identify acquired knowledge K(t)
0Ac and new
knowledge K(t)
0Nw, generate the weights for them by
Formula (2), then recommend TR(t) ENto Dτ;
4. Project TR(t) ENto O(t)
CN with pseudo-parallel data
(U(t)
CN, U(t)
T rEN), and use knowledge validation function
Vτ(O(t)
CN) to identify acquired knowledge K(t)
Ac and new
knowledge K(t)
Nw, then generate weights for them by
Formula (4);
5. Update DS by Formula (2) and Dτ by Formula (5);
End For.
Output: Chinese classifier C(T )
CN.
</figure>
<subsectionHeader confidence="0.991571">
3.3 Knowledge Validation
</subsectionHeader>
<bodyText confidence="0.999378666666667">
Knowledge is familiarity, awareness or under-
standing of someone or something, such as
facts, information or skills, which is acquired
through experience or education by perceiving,
discovering or learning2. It can be implicit or
explicit.
In machine learning, natural language knowl-
edge is a continuously improving hypothesis that
consists of both semantic and significant domain
</bodyText>
<figure confidence="0.936186025">
2Definition from Oxford Dictionary of English, avail-
ableat:http://oxforddictionaries.com/view/
entry/m_en_us126.
⎧
⎨
⎩
0Ac
ωi0 =
(2)
(3)
LB(t+1)
EN = LB(t)
EN − (K(t)
0Ac ∪ K(t)
0Nw)
In the target language view, at iteration (t),
the CredBoost model receives the recommended
knowledge TR(t) ENand projects it to O(t)
CN from
the unlabeled Chinese data U(t)
CN with the pseudo-
parallel data (U(t)
CN, U(t)
T rEN). OCN
(t) is validat-
ed by the target knowledge validation function
Vτ(O(t)
CN) to identify the acquired knowledge
K(t) Acand the new knowledge K(t) Nw. K(t) Acand
K(t) Nw are projected to K(t)
∗Ac and K(t)
∗Nw from
the unlabeled English pseudo-data U(t) TrEN. The
weight of an instance is updated by Formula (4),
and the parameter setting is similar to that in
(4)
ωj = e
Nw
(5)
422
</figure>
<bodyText confidence="0.973786255813953">
characters. While language is the expression of
semantic, semantic is the carrier of sentiment.
Using another word, two texts with more smaller
semantic distance have higher probability to share
the same sentiment polarity. Choi and Cardie
(2008) assert that the sentiment polarity of natural
language can be better inferred by compositional
semantics. They also suggest that incorporating
compositional semantics into learning can im-
prove the performance of sentiment classifiers.
Saif et al. (2012) also demonstrate that the
addition of extra semantic features can further
improve performance.
In order to filter out noisy and incorrect senti-
ment labels, we propose a knowledge validation
approach to reduce these noisy data that hinder the
improvement of learning performance. Knowl-
edge validation is a way to identify the acquired
knowledge implied in current knowledge system
and also the new knowledge fresh to current
knowledge system. The knowledge can be repre-
sented in the semantic space. (Le and Mikolov,
2014) project documents into a low-dimension
semantic space with a deep learning approach,
known as document-to-vector (Doc2Vec3). Con-
sidering that Dov2Vec has been verified to be
efficient in many NLP tasks including sentiment
analysis, we follow previous research to represent
knowledge embedded in product reviews with the
vectors generated by Doc2Vec.
Suppose distributed representations (i.e., low-
dimensional vectors) of the all reviews including
{LTEN, LBEN, UTrEN} and {LTrCN, UCN}
are {V(LTEN), V(LBEN), V(UTrEN)} and
{V(LTrCN),V(UCN)} respectively. At iteration
(t), V(LT (t)
EN ) is the current knowledge system
of the English view and V(L(t)
T rCN) is that of
the Chinese. The knowledge validation runs
separately in the source and target views.
In the target language view, at iteration (t),
suppose the prediction confidence of the candidate
</bodyText>
<equation confidence="0.753348333333333">
(xUi , ˆyUi ) ∈ O(t)
CN is c(t)
i . We define the
</equation>
<bodyText confidence="0.89451175">
adaptiveness score as the average distance of top
(+ semantic distances between the instance xLB
i
and the positive cluster of L(t)
</bodyText>
<equation confidence="0.946476666666667">
T rCN, denoted as
LTt)+N, and top (Lt) = (+ · L+ semantic distances
−
</equation>
<bodyText confidence="0.6233105">
between xUi and the negative cluster, denoted as
3Doc2Vec is one of the models implemented in the free
python library Gensim which can be freely downloaded at:
https://pypi.python.org/pypi/gensim.
</bodyText>
<equation confidence="0.990617416666667">
L(t)−
T rCN, where L(t)
+ and L(t)
− are the numbers of
the elements in L(t)+
TrCN and L(t)− Trespectively.
CN
The validation parameters are defined by Formula
(6), Wr is the weight of training instance V(r), ν(t)
i
is the adaptiveness score, and Vlabel
∗ ∈ {1, −1} is
</equation>
<bodyText confidence="0.886108625">
the validated label which denotes the knowledge
belonging to the positive cluster L(t)+
TrCN or the
negative cluster L(t)−
T rCN. The validation process
is illustrated in Algorithm 2, where the acquired
knowledge is k(t)
Ac, and the new knowledge is k(t) Nw.
</bodyText>
<equation confidence="0.995244230769231">
i )T · V(r)
D(V(xLB
i ), V(r)) = V(xLB
k V(xLB
i ) k · k V(r) k
⇒ Δ(ν(t)
i ) = ν(t)+
i − ν(t)
i
1
δ(t)
⇒ i = e1+Δ(ν(t)
i )
(
1 if δ(t)
i &gt; 0.5,
⇒ Vlabel
∗ =
−1 if δ(t)
i ≤ 0.5.
(
ν(t)+ if Vlabel
⇒ ν(t) ∗ = 1,
i
i = ν(t)− if Vlabel ∗= −1.
i
</equation>
<bodyText confidence="0.98647235">
where D(V(xLB
i ), V(r)) is the Cosine distance
between the distributed representations of the two
reviews. ν(t)+ iand ν(t)−
i are the weighted averages
of the semantic distances. S(t)
i is the Sigmoid
function which computes the probability that the
data is distributed in the positive cluster L(t)+
T rCN.
In the source language view, at iteration
(t), let’s suppose the prediction confidence of
candidate (xZB, ˆyi/ ∈ OEtN to be c(t)
i0 . The
definitions of validation parameters are similar
to those in the target language view. The
validation process is illustrated in Algorithm 3.
The validation is looser, because the training data
and candidates are both in English. This differs
from it in the target view.
</bodyText>
<sectionHeader confidence="0.999681" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.995553">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.997985888888889">
We evaluate the proposed CredBoost model on
an open cross-lingual sentiment analysis task in
NLP&amp;CC 20134. The data set provided is a
4NLP&amp;CC is an annual conference of Chinese infor-
mation technology professional committee organized by
Chinese computer Federation (CCF). It mainly focuses
on the study and application novelty of natural language
processing and Chinese computation. CLSA task is the
task 3 of NLP&amp;CC 2013. For more details and open
</bodyText>
<equation confidence="0.9501691">
P
t
1
νi = ζ(t)
− r�∈L(t)−
EN
ωr, D(V(xLB
i ), V(r0))
⎧
⎨⎪⎪
⎪⎪⎩
⇒
P
ν(t)+
i = 1 ζ+
ωr D(V(xLB
i ), V(r))
r∈L(t)+
EN
(6)
</equation>
<page confidence="0.385186">
423
</page>
<table confidence="0.370505333333333">
Algorithm 2 Knowledge Validation Vτ (Dτ)
Input: Labeled Chinese training data L(t)
TrCN, weights
of labeled data W (t) and semantics vectors of all
CN
English data for iteration (t): {V(L(t)
</table>
<equation confidence="0.8625232">
T rCN), V(U(t)
CN)};
Initialize: K(1)
0Ac = φ, K(1)
0Nw = φ;
</equation>
<figure confidence="0.909721833333333">
For xU i in O(t)
CN:
1. Use L(t)
T rCN to train a classifier C(t)
CN,
then use C(t)
CN predict xUi , giving la-
bel yCN
i ;
2. Get validated label Vlabel
∗ ,positive and
negative average distances ν(t)+
</figure>
<equation confidence="0.798672375">
i ,ν(t)−
i
of xUi by fomula (6);
3. If ν(t)+
i &lt; ψ and ν(t)−
i &lt; ψ:
If ˆyLB
i = Vlabel
∗ :
Then K(t)
Nw ← K(t)
Nw + xUi ;
Else:
If ˆyLB i = Vlabel
∗ = yCN
i :
Then K(t)
Ac ← K(t)
Ac + xUi ;
End For.
Output: K(t)
Nw, K(t)
Ac.
Algorithm 3 Knowledge Validation VS(DS)
</equation>
<bodyText confidence="0.4742955">
Input: Weights of labeled data W(1) ENand semantics
vectors of all English data for iteration (t):
</bodyText>
<equation confidence="0.832314135135135">
{V(LT (t)
EN ), V(LB(t)
EN ), V(U(t)
T rEN)};
Initialize: K(1)
0Ac = φ, K(1)
0Nw = φ;
For xLB
i0 in O(t)
EN:
1. Get validated label Vlabel
0 ,positive and
negative average distances ν(t)+
i0 , ν(t)−
i0
of xLB
i0 by fomula (6);
2. If ν(t)+
i0 &lt; ψ and ν(t)− i0 &lt; ψ:
If ˆyLB
i0 = Vlabel
0 :
Then K(t)
0Nw ← K(t)
0Nw + xLB
i0 ;
Else:
If ˆLB = Vlabel0:
yi0
Then K(t)
0Ac ← K(t)
0Ac + xLB
i0 ;
End For.
Output: K(t)
0Nw, K(t)
0Ac.
</equation>
<bodyText confidence="0.955044777777778">
collection of bilingual Amazon product reviews
in Books, DVD and Music domains. It contains
4,000 labeled English reviews, 4,000 Chinese test
reviews, and 17,814, 47,071, 29,677 unlabeled
Chinese reviews in three different domains. We
randomly select 2,000 unlabeled Chinese reviews
in each domain to train classifiers. Besides, the
pseudo-data sets described in CredBoost model
are translated with Google translator. The data set
is summarized in Table 1.
To better illustrate the significance of knowl-
edge validation during knowledge transfer, we
compare the proposed method with the following
baseline methods:
Lexicon-based (LB): The standard English
MPQA sentiment lexicons are translated into
resource, you can available at: http://tcci.ccf.org.
cn/conference/2013/index.html.
</bodyText>
<tableCaption confidence="0.848081">
Table 1: Experimental data sets. All data sets
</tableCaption>
<bodyText confidence="0.989437651162791">
are balanced, L represents labeled data and U
represents unlabeled data.
Chinese and then utilized together with a small
number of Chinese turning words, negations and
intensifiers to predict the sentiment polarities of
the Chinese test reviews.
Basic SVM (BSVM-CN): The labeled English
reviews are translated into Chinese, which are then
used as the pseudo-training data to train a Chinese
SVM classifier.
Primarily boost transfer learning (BTL-1):
The labeled English reviews are used to train
the English classifier, which is applied to label
the English translations of the unlabeled Chinese
reviews. These labeled Chinese reviews obtained
via MT together with the Chinese translations of
the labeled English reviews are then used as the
pseudo-training data to train a Chinese sentiment
classifier.
Best result in NLP&amp;CC 2013 (BR2013): This
is the best result reported in NLP&amp;CC 2013.
Unfortunately, the specification of the method is
not available.
Self-boost (SB-CN) in Chinese: The labeled
English reviews are translated into Chinese, which
are used as the pseudo-training data to train a basic
Chinese classifier. This classifier is iteratively
refined by choosing the most confidently predicted
English reviews to add into the Chinese training
data until a predefined iteration number reaches. It
can be also considered as a self-adaptive boosting
approach.
Iteratively boost transfer learning (BTL-2):
This is an enhanced transfer learning method shar-
ing the same learning framework with CredBoost
but it ignores knowledge validation. It iteratively
transfers the knowledge from English to Chinese.
The learning in both languages iteratively boosts
themselves separately. The transfer size is 16,
comparable to that in CredBoost.
Basic co-training (CoTr): The co-training
method proposed in (Wan, 2009) is implemented.
It is bidirectional transfer learning. In each
</bodyText>
<figure confidence="0.999593775">
English
Chinese
U
Domain
L
L
2,000
-
4,000
Train
Books
-
Test
4,000
-
-
2,000
DVD
-
4,000
Train
-
-
Test
4,000
-
-
-
4,000
Train
Music
-
2,000
-
-
Test
4,000
U
-
-
</figure>
<page confidence="0.73027">
424
</page>
<bodyText confidence="0.999840695652174">
iteration, 10 positive and 10 negative reviews are
transferred from one language to the other.
Doc2vec feature CredBoost (dCredB): This
method is similar to CredBoost except that
document-to-vector is used to generate features
when training basic classifiers. The vectors
are obtained from both original and translated
reviews. The dimension of doc2vec is 300, while
the other parameters are set as default.
The baseline methods described above are
categorized into three classes: the first four
which are preliminary methods, the middle three
which are several state-of-the-art models being
comparable to our proposed model, and the last
one which is a comparison to suggest that the
knowledge representation is not the answer to the
performance improvement. For all the methods
excluding LB and BR2013, we use support vector
machines (SVMs) as basic classifiers. We use
the Liblinear package (Fan et al., 2008) with the
linear kernel5. All methods use Unigram+Bigram
features to train the basic classifiers, except for
dCredB.
</bodyText>
<subsectionHeader confidence="0.993866">
4.2 Experimental Result
</subsectionHeader>
<bodyText confidence="0.999982315789474">
In this work, there are two main parameters that
may significantly influence the performance of our
proposed model. They are the new knowledge
validation boundary 0 and the validation scale
(+ in the training data. We set the values of
parameters with the grid search strategy. We
first fix initial (+ = 14 to search the best
new knowledge validation boundary 0 from an
empirical value set {0.30, 0.35, 0.40, 0.45, 0.50}.
We then fix the best 0 = 0.40 to check the
suitable validation scale (+ from the initial value
set {6, 8, 9,10,11,12,14,16} in which values are
comparable with the knowledge transfer scale
of CoTr in the training data. Besides, the
recommendation size m for English is set to 20
and the recommendation size n for Chinese is set
to 40. The final settings are listed in Table 2.
The performance is evaluated in terms of accuracy
(Ac) defined by Formula (7).
</bodyText>
<equation confidence="0.971666">
�
f
Ac(f) = pf , Avg Ac = 31 ·
f&apos; ∈F
</equation>
<bodyText confidence="0.998167666666667">
where pf is the number of correct predictions
and Pf is the total number of the test data; F ∈
{Books, DVD, Music} is the domain set.
</bodyText>
<footnote confidence="0.647052">
5The parameter setting used in this paper is ‘-s 7’.
</footnote>
<table confidence="0.99886925">
Domain ψ ζ+ m n
Books 0.45 12 20 40
DVD 0.40 12 20 40
Music 0.40 9 20 40
</table>
<tableCaption confidence="0.999906333333333">
Table 2: Parameter settings of three domains in
this paper.
Table 3: Macro performance of all approaches
</tableCaption>
<bodyText confidence="0.99848603030303">
in three domains. All values are accuracies and
Avg-Ac represents the average accuracy in three
domains.
The performances are reported in Tables 3 and
4. As shown, CredBoost outperforms all the other
comparison methods. The first four baselines
have poor performances compared to others. This
suggests that the CLSA problem cannot be well
solved by directly learning from the labeled
translated data without any knowledge adaption or
knowledge validation. SB-CN, BTL-2 and CoTr
employ iterative boosting to adapt knowledge
from the source English to the target Chinese with-
out validating the transferred knowledge. They
inevitably mis-recommend the massive noisy data
into Chinese. CredBoost, in contrast, introduces
knowledge validation into transfer learning with
iterative boosting. It better adapts knowledge from
English to Chinese and thus ensures the credibility
of the accepted knowledge. Its best result justifies
our assumption.
Specifically, SB-CN leverages both the Chinese
training data translated from the labeled English
data and the unlabeled Chinese data used for
boosting. The boosting in Chinese iteratively
selects the trustworthy data with the labels as-
signed by the Chinese classifier. Our proposed
method, however, exploits two different languages
simultaneously with an additional boosting step,
i.e., it transfers knowledge from English to
Chinese during boosting. We then use knowledge
validation model to validate the unlabeled Chinese
data whose labels are assigned by the English
</bodyText>
<figure confidence="0.997081538461539">
Music
Books
0.7595
0.7770
0.7940
0.7778
0.8010
0.7605
0.7850
0.7513
0.8012
0.8400
0.8105
0.7980
0.7812
0.8025
0.6485
0.6700
0.8465
0.8093
Avg Ac
0.7709
0.7904
0.7891
0.7712
0.8280
0.8117
0.8115
0.6646
0.8359
Approaches
LB
BSVM-CN
BTL-1
BR2013
SB-CN
BTL-2
CoTr
dCredB
CredBoost
Domain
DVD
0.7832
0.7995
0.8058
0.7773
0.8428
0.8265
0.8508
0.6753
0.8518
Ac(f&apos;) (7)
</figure>
<page confidence="0.607728">
425
</page>
<table confidence="0.999593147058824">
Model (Books) Positive Negative Ac
P R F1 P R F1
LB 0.7368 0.8400 0.7850 0.8140 0.7000 0.7527 0.7700
BSVM-CN 0.8249 0.7465 0.7837 0.7685 0.8415 0.8033 0.7940
BTL-1 0.8537 0.7265 0.7850 0.7620 0.8755 0.8148 0.8010
BR2013 - - - - - - 0.7850
SB-CN 0.8716 0.7975 0.8329 0.8134 0.8825 0.8465 0.8400
BTL-2 0.7105 0.8881 0.7894 0.9105 0.7588 0.8278 0.8105
CoTr 0.8339 0.7555 0.7928 0.7765 0.8495 0.8114 0.8025
dCredB 0.5310 0.6941 0.6017 0.7660 0.6202 0.6854 0.6485
CredBoost 0.8225 0.8640 0.8427 0.8705 0.8306 0.8501 0.8465
Model (DVD) Negative Ac
Positive
P R F1 P R F1
LB 0.7648 0.8180 0.7905 0.8044 0.7485 0.7754 0.7832
BSVM-CN 0.7745 0.8450 0.8082 0.8295 0.7540 0.7900 0.7995
BTL-1 0.8282 0.7715 0.7988 0.7861 0.8400 0.8122 0.8058
BR2013 - - - - - - 0.7773
SB-CN 0.8853 0.7875 0.8335 0.8086 0.8980 0.8510 0.8428
BTL-2 0.8525 0.8104 0.8309 0.8005 0.8444 0.8219 0.8265
CoTr 0.8374 0.8705 0.8536 0.8652 0.8310 0.8478 0.8508
dCredB 0.6070 0.7030 0.6515 0.7435 0.6542 0.6960 0.6753
CredBoost 0.8440 0.8572 0.8508 0.8595 0.8465 0.8530 0.8518
Model (Music) Positive Negative Ac
P R F1 P R F1
LB 0.7387 0.8030 0.7695 0.7842 0.7160 0.7485 0.7595
BSVM-CN 0.8492 0.6755 0.7525 0.7306 0.8800 0.7984 0.7778
BTL-1 0.8437 0.6395 0.7275 0.7097 0.8815 0.7863 0.7605
BR2013 - - - - - - 0.7513
SB-CN 0.8787 0.6990 0.7786 0.7501 0.9035 0.8197 0.8012
BTL-2 0.7285 0.8461 0.7829 0.8675 0.7616 0.8111 0.7980
CoTr 0.8536 0.6790 0.7564 0.7335 0.8835 0.8015 0.7812
dCredB 0.5860 0.7043 0.6397 0.7540 0.6455 0.6955 0.6700
CredBoost 0.7258 0.8708 0.7917 0.8928 0.7653 0.8241 0.8093
</table>
<tableCaption confidence="0.777985333333333">
Table 4: Micro performance of all approaches in three domains. P: Precision, R: Recall, F1: micro-F
measure, Ac: Accuracy, and - represents unknown. The model in BR2013 is unknown, thus its micro
performance is unavailable.
</tableCaption>
<bodyText confidence="0.999476738095238">
classifier. It is reasonable that a Chinese classifier
performs better on Chinese text than an English
classifier performs on the translated English text
due to the different language distributions and MT
errors. However, as shown in Tables 3 and 4,
the better performance of our proposed method
compared with that of the self-boosting method
further suggests the effectiveness of our proposed
knowledge validation model.
Figure 1 illustrates the continuous changes of
performances vs. the corresponding growth sizes
of the training data sets for SB-CN, BTL-2,
CoTr, and CredBoost. According to our common
sense, noisy data have negative influence on
performance improvement. Compared to the other
three methods, CredBoost accepts less number of
training instances during learning while it achieves
more improvement. This verifies the ability
of CredBoost that can filter out the noisy data
recommended by the English sentiment classifier.
In Figure 1(a), the curves of BTL-2 and CoTr
suggest that directly transferring the knowledge
recommended from English imports many noisy
data into Chinese. It is also obvious that the
performance curve of CredBoost implies a stable
improvement trend while the other three decrease
after certain iterations because of the accumulated
negative influence from the noisy data. Figure
1(b) shows CredBoost accepts decreased training
instances after certain iterations because the
number of “high-quality” instances decrease when
learning proceeds. This finding suggests that
knowledge validation would rather abandon “less-
credible” knowledge with higher probability than
easily accept it. Knowledge validation in the
proposed model guarantees highly-credible learn-
ing when transferring knowledge from English to
Chinese. The results also show that CredBoost
has great potential to achieve better performance
approaching to supervised approaches if more
unlabeled Chinese data are available.
Another interesting finding is also observed.
</bodyText>
<figure confidence="0.988128720930233">
426
0.85
0.75
0.65
0.8
0.7
0 20 40 60 80 100 120
Performance in Books domain.
Iteration Number
selfBoost
CoTr
BTL-2
CredBoost
0.86
0.85
0.84
0.83
0.82
0.81
0.79
0.78
0.77
0.76
0 20 40 60 80 100 120
0.8
Performance in DVD domain.
Iteration Number
CoTr
BTL-2
CredBoost
0.75
0.65
0.8
0.7
0.6
0 20 40 60 80 100 120
Performance in Music domain.
Iteration Number
CoTr
BTL-2
CredBoost
(a) Performances comparison in three domains
(b) Growth sizes comparison in three domains
</figure>
<figureCaption confidence="0.8836545">
Figure 1: Performances vs. Growth Sizes for SB-CN, CoTr, BTL-2, and CredBoost in three domains.
The similar performance curves of CoTr is also reported in (Gui et al., 2014).
</figureCaption>
<figure confidence="0.984033147058824">
Growth Sizes in Books domain. Growth Sizes in DVD do main. Growth Sizes in Music domain.
Iteration Number Iteration Number Iteration Number
30
25
20
15
10
5
00 20 40 60 80 100 120
selfBoost
CoTr
BTL-2
CredBoost
30
25
20
15
10
5
00 20 40 60 80 100 120
selfBoost
CoTr
BTL-2
CredBoost
25
20
15
10
5
00 20 40 60 80 100 120
selfBoost
CoTr
BTL-2
CredBoost
</figure>
<bodyText confidence="0.994979789473684">
Although document-to-vector represents content
semantic well, it cannot determine the sentiment
polarity of text well, even when the document-
to-vectors that are used to train basic classifiers
are learned on the mixture of the translated and
original reviews. The superior performance of
CredBoost to dCredB suggests that the semantic
representation is effective to identify highly-
credible acquired knowledge and new knowledge
but it alone may not be sufficient enough to model
the sentiment information.
We also conduct some other experiments to
study the sensitivity of the new knowledge valida-
tion boundary ψ and the validation scale ζ+ in the
training data. The experimental results show that
the performances with different parameter settings
fluctuate around the best result reported in Tables
3 and 4 in a small range. Our model is basically
quite stable.
</bodyText>
<sectionHeader confidence="0.997706" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.937797933333334">
In this paper, we propose a semi-supervised learn-
ing model, called CredBoost, to address cross-
lingual (English vs Chinese) sentiment analysis
without direct labeled Chinese data nor direct
parallel data. We propose to introduce knowledge
validation during transfer learning to reduce the
selfBoost
selfBoost
noisy data caused by machine translation errors or
inevitable mistakes made by the source language
sentiment classifier. The experimental result
demonstrates the effectiveness of the proposed
model. In the future, we will explore more suitable
knowledge representations and knowledge valida-
tion in the CredBoost framework.
</bodyText>
<sectionHeader confidence="0.990993" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.998935375">
We thank all the anonymous reviewers for their
detailed and insightful comments on this paper.
The work described in this paper was supported
by the Research Grants Council of Hong Kong
project (PolyU 5202/12E and PolyU 152094/14E)
and the grants from the National Natural Sci-
ence Foundation of China (61272291, 61472290,
61472291 and 61303115).
</bodyText>
<sectionHeader confidence="0.983049" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999137125">
Carmen Banea and Rada Mihalcea, Janyce Wiebe,
Samer Hassan. 2008. Multilingual Subjectivity
Analysis Using Machine Translation. In Proceed-
ings of the 2008 Conference on Empirical Methods
in Natual Language Processing, pages 127-135,
Honolulu, October.
Carmen Banea, Yoonjung Choi, Lingjia Deng, Samer
Hassan, Michael Mohler, Bishan Yang, Claire
</reference>
<page confidence="0.55136">
427
</page>
<reference confidence="0.999913122807018">
Cardie, Rada Mihalcea, Janyce Wiebe. 2013. CPN-
CORE: A Text Semantic Similarity System Infused
with Opinion Knowledge. In Proceedings of the
Main Conference and the SHared Task in *SEM
2013, pages 221-228, Atlanta, Georgia, June 13-14,
2013.
Yejin Choi and Claire Cardie. 2008. Learning with
Compositional Semantics as Structural Inference for
Subsentential Sentiment Analysis. In Proceedings
of the 2008 Conference on Empirical Methods
in Natural Language Processing, pages 792-801,
Honolulu, October 2008.
Kevin Duh and Akinori Fujino and Masaaki Nagata.
2011. Is Machine Translation Ripe for Cross-
lingual Sentiment Classification? In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics: shortpapers, pages 429-
433, Portland, Oregon, June 19-24, 2011.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Ksieh, Xiang-
Rui Wang, Chih-Jen Lin. 2008. LIBLINEAR: A
Library for Large Linear Classification. In Journal
of Machine Learning Research, 9 (2008) 1871-1874.
Micheal Gamon. 2004. Sentiment Classification
on Customer Feedback Data: Noisy Data, Large
Feature Vectors and the Role of Linguistic Analysis.
In Proceedings of the 20th International Conference
on Computational Linguistics, pages 841-847, CH.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain Adaptation for Large-scale Senti-
ment Classification: A Deep Learning Approach.
In Proceedings of the 28th International Conference
on Machine Learning, pages 513-520, Bellevue,
Washington, USA.
Lin Gui, Ruifeng Xu, Qin Lu, Jun Xu, Jian Xu,
Bin Liu, Xiaolong Wang. 2014. Cross-lingual
Opinion Analysis via Negative Transfer Detection.
In Proceedings of the 52th Annual Meeting of the
Association for Computational Linguistics (short
paper), pages 860-865, Baltimore, Maryland, USA,
June 23-25 2014.
Ahmed Hassan and Dragomir Radev. 2010. I-
dentifying Text Polarity Using Random Walks.
In Proceedings of the 48th Annual Meeting on
Association for Computational Linguistics, pages
395-403, Uppsala, Sweden, 11-16 July 2010.
Yulan He, Chenghua Lin, Harith Alani. 2011a.
Automatically Extracting Polarity-bearing Topics
for Cross Domain Sentiment Classification. In
Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Huamn
Language Technologies, pages 123-131, Portland,
Oregon, USA.
Yulan He. 2011b. Latent Sentiment Model for Weakly-
Supervised Cross-Lingual Sentiment Classification.
In Proceedings of the 33th European Conference on
Information Retrieval(ECIR 2011), 18-21 Apr 2011,
Dublin, Ireland.
KANAYAMA Hiroshi, NASUKAWAA Tetsuya,
WATANABE Hideo. 2004. Deeper Sentiment
Analysis Using Machine Translation Technology.
In Proceedings of the 20th International Conference
on Computational Linguistics, pages 494-500.
Alistair Kennedy and Diana Inkpen. 2006. Senti-
ment Classification of Movie and Product Reviews
Using Contextual Valence Shifters. Computational
Intelligence,22(2):110-125.
Quoc Le, Tomas Mikolov. 2014. Distributed
Representations of Sentences and Documents. In
Proceedings of the 31th International Conference on
Machine Learning, Beijing, China, 2014. JMLR:
W&amp;CP volume 32.
Tao Li, Vikas Sindhwani, Chris Ding, and Yi Zhang.
2009. Knowledge Transformation for Cross-
Domain Sentiment Classification. In Proceedings
of the 32th International ACM SIGIR Conference
on Research and Development in Information Re-
trieval, pages 716-717, Boston, MA, USA.
Bing Liu. May 2012. Sentiment Analysis and Opinion
Mining. Morgan &amp; Claypool Publisher.
Xinfan Meng, Furu Wei, Xiaohua Liu, Ming Zhou, Ge
Xu, Houfeng Wang. 2012. Cross-Lingual Mixture
Model for Sentiment Classification. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics, pages 572-581, Jeju,
Republic of Korea, 8-14 July 2012.
Tony Mullen and Nigel Collier. 2004. Sentiment
analysis using support vector machines with di-
verse inoformation sources. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, pages 412-418, (July 2004)
poster paper.
Sinno Jialin Pan and Qiang Yang, Fellow, IEEE. 2010.
A Survey on Transfer Learning. In Journal of IEEE
Transactions on Knowledge and Data Engineering,
Vol.22, NO.10, October 2010.
Bo Pang and Lillian Lee, Shivakumar Vaithyanathan.
2002. Thumps Up? Sentiment Classification using
Machine Learning Techniques. In Proceedings
of the 2002 Conference on Empirical Methods
in Natural Language Processing, pages 79-86,
Philadelphia, July 2002.
Kashyap Popat, Balamurali A R, Pushpak Bhat-
tacharyya and Gholamreza Haffari. 2013. The
Haves and the Have-Nots: Leverage Unlabeled
Corpora for Sentiment Analysis. In Proceedings
of the 51th Annual Meeting of the Association for
Computational Linguistics, pages 412-422, Sofia,
Bulgaria, 4-9 August 2013.
Jonathon Read. 2005. Using Emotions to reduce
Dependency in Machine Learning Techniques for
Sentiment Classification. In Proceedings of the 43th
Annual Meeting on Association for Computational
Linguistics Student Research Workshop, pages 43-
48.
</reference>
<page confidence="0.611047">
428
</page>
<reference confidence="0.998985085106383">
Hassan Saif, Yulan He and Harith Alani. 2012.
Semantic Sentiment Analysis of Twitter. In
Proceedings of the 11th International Semantics
Web Conference ISWC 2012, Boston, USA.
Stephen Shankland. 2013. Google Translate now
serves 200 millon people daily. In CNET. CBS
Interactive Inc. May 18, 2013.
Richard Socher, Alex Perelygin, Jean Y. Wu, Jason
Chuang, Chiristopher D. Manning, Andrew Y. Ng
and Christopher Potts. 2013. Recursive Deep
Models for Semantics Computationality Over a
Sentiment Treebank. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
Songbo Tan, Gaowei Wu, Huifeng Tang and Xueqi
Cheng. 2007. A Novel Scheme for Domain-transfer
Problem in the context of Sentiment Analysis. In
CIKM 2007, November 6-8, 2007, Lisboa, Portugal.
Peter D. Turney. 2002. Thumps Up or Thumps
Down? Semantic Orientation Applied to Unsuper-
vised Classification of Reviews. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 417-424, Philadelphia,
July 2002.
Xiaojun Wan. 2008. Using Bilingual Knowledge
and Ensemble Technics for Unsupervised Chi-
nese Sentiment Analysis. In Proceedings of the
2008 Conference on Empirical Methods in Natual
Language Processing, pages 553-561, Honolulu,
October 2008.
Xiaojun Wan. 2009. Co-Training for Cross-Lingual
Sentiment Classification. In Proceedings of the 47th
Annual Meeting of the ACL and the 4th IJCNLP of
the AFNLP, pages 235-243, Suntec, Singapore, 2-7
August 2009.
Bin Wei and Christopher Pal. 2010. Cross Lingual
Adaptation: An Experiment on Sentiment Classifi-
cations. In Proceedings of the 48 Annual Meeting of
the Association for Computational Linguistics (short
paper), pages 258-262, Uppsala, Sweden, 11-16
July 2010.
Ruifeng Xu, Jun Xu and Xiaolong Wang. 2011.
Instance Level Transfer Learning for Cross Lingual
Opinion Analysis. In Proceedings of the 2nd Work-
shop on Computational Approaches to Subjectivity
and Sentiment Analysis, ACL-HLT 2011, pages 182-
188, 24 June, 2011, Portland, Oregon, USA.
</reference>
<page confidence="0.964396">
429
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.717424">
<title confidence="0.9947215">Learning to Adapt Credible Knowledge in Cross-lingual Sentiment Analysis</title>
<author confidence="0.990291">Wenjie Yu Xule Yanxiang</author>
<affiliation confidence="0.97725675">of Computer Science, Wuhan University, of Computing, The Hong Kong Polytechnic University, Hong Kong Polytechnic University Shenzhen Research Institute, State Key Lab of Software Engineering, Wuhan University,</affiliation>
<email confidence="0.9165725">xuleliu,cswjli,</email>
<abstract confidence="0.997419125">Cross-lingual sentiment analysis is a task of identifying sentiment polarities of texts in a low-resource language by using sentiment knowledge in a resource-abundant language. While most existing approaches are driven by transfer learning, their performance does not reach to a promising level due to the transferred errors. In this paper, we propose to integrate into knowledge transfer a knowledge validation model, which aims to prevent the negative influence from the wrong knowledge by distinguishing highly credible knowledge. Experiment results demonstrate the necessity and effectiveness of the model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Carmen Banea</author>
<author>Rada Mihalcea</author>
<author>Janyce Wiebe</author>
<author>Samer Hassan</author>
</authors>
<title>Multilingual Subjectivity Analysis Using Machine Translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natual Language Processing,</booktitle>
<pages>127--135</pages>
<location>Honolulu,</location>
<contexts>
<context position="2631" citStr="Banea et al., 2008" startWordPosition="383" endWordPosition="386">n between the source and target languages to overcome the language barrier, and then develop an appropriate knowledge transfer approach to leverage the annotated data from the source language to train a sentiment classification model in the target language, either supervised or semi-supervised. In particular, these approaches exploit and convert the knowledge learned from the source language to automatically generate and expand the pseudo-training data for the target language. The machine translation (MT) service is one of the most common ways used to build the language connection (Wan, 2008; Banea et al., 2008; Wan, 2009; Wei and Pal, 2010; Gui et al., 2014). Although it is claimed in Duh et al. (2011) that the MT service is ripe for CLSA, the imperfect MT quality hinders existing MTbased CLSA approaches from the further advance. In our preliminary study, we find that even the Google translator1 (i.e., one of the most widely used online MT service (Shankland 2013)) may unavoidably changes the sentiment polarity of the translated text, as illustrated below, with a percentage of around 10%. [Original English Text]: I am at home on bed rest and desperate for something good to read. [Sentiment Label: N</context>
</contexts>
<marker>Banea, Mihalcea, Wiebe, Hassan, 2008</marker>
<rawString>Carmen Banea and Rada Mihalcea, Janyce Wiebe, Samer Hassan. 2008. Multilingual Subjectivity Analysis Using Machine Translation. In Proceedings of the 2008 Conference on Empirical Methods in Natual Language Processing, pages 127-135, Honolulu, October.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Carmen Banea</author>
</authors>
<title>Yoonjung Choi, Lingjia Deng, Samer</title>
<location>Hassan, Michael Mohler, Bishan Yang, Claire</location>
<marker>Banea, </marker>
<rawString>Carmen Banea, Yoonjung Choi, Lingjia Deng, Samer Hassan, Michael Mohler, Bishan Yang, Claire</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea Cardie</author>
<author>Janyce Wiebe</author>
</authors>
<title>CPNCORE: A Text Semantic Similarity System Infused with Opinion Knowledge.</title>
<date>2013</date>
<booktitle>In Proceedings of the Main Conference and the SHared Task in *SEM 2013,</booktitle>
<pages>221--228</pages>
<location>Atlanta, Georgia,</location>
<marker>Cardie, Wiebe, 2013</marker>
<rawString>Cardie, Rada Mihalcea, Janyce Wiebe. 2013. CPNCORE: A Text Semantic Similarity System Infused with Opinion Knowledge. In Proceedings of the Main Conference and the SHared Task in *SEM 2013, pages 221-228, Atlanta, Georgia, June 13-14, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
</authors>
<title>Learning with Compositional Semantics as Structural Inference for Subsentential Sentiment Analysis.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>792--801</pages>
<location>Honolulu,</location>
<contexts>
<context position="17763" citStr="Choi and Cardie (2008)" startWordPosition="2836" endWordPosition="2839">t) is validated by the target knowledge validation function Vτ(O(t) CN) to identify the acquired knowledge K(t) Acand the new knowledge K(t) Nw. K(t) Acand K(t) Nw are projected to K(t) ∗Ac and K(t) ∗Nw from the unlabeled English pseudo-data U(t) TrEN. The weight of an instance is updated by Formula (4), and the parameter setting is similar to that in (4) ωj = e Nw (5) 422 characters. While language is the expression of semantic, semantic is the carrier of sentiment. Using another word, two texts with more smaller semantic distance have higher probability to share the same sentiment polarity. Choi and Cardie (2008) assert that the sentiment polarity of natural language can be better inferred by compositional semantics. They also suggest that incorporating compositional semantics into learning can improve the performance of sentiment classifiers. Saif et al. (2012) also demonstrate that the addition of extra semantic features can further improve performance. In order to filter out noisy and incorrect sentiment labels, we propose a knowledge validation approach to reduce these noisy data that hinder the improvement of learning performance. Knowledge validation is a way to identify the acquired knowledge i</context>
</contexts>
<marker>Choi, Cardie, 2008</marker>
<rawString>Yejin Choi and Claire Cardie. 2008. Learning with Compositional Semantics as Structural Inference for Subsentential Sentiment Analysis. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 792-801, Honolulu, October 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Duh</author>
<author>Akinori Fujino</author>
<author>Masaaki Nagata</author>
</authors>
<title>Is Machine Translation Ripe for Crosslingual Sentiment Classification?</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: shortpapers,</booktitle>
<pages>429--433</pages>
<location>Portland, Oregon,</location>
<contexts>
<context position="2725" citStr="Duh et al. (2011)" startWordPosition="402" endWordPosition="405"> appropriate knowledge transfer approach to leverage the annotated data from the source language to train a sentiment classification model in the target language, either supervised or semi-supervised. In particular, these approaches exploit and convert the knowledge learned from the source language to automatically generate and expand the pseudo-training data for the target language. The machine translation (MT) service is one of the most common ways used to build the language connection (Wan, 2008; Banea et al., 2008; Wan, 2009; Wei and Pal, 2010; Gui et al., 2014). Although it is claimed in Duh et al. (2011) that the MT service is ripe for CLSA, the imperfect MT quality hinders existing MTbased CLSA approaches from the further advance. In our preliminary study, we find that even the Google translator1 (i.e., one of the most widely used online MT service (Shankland 2013)) may unavoidably changes the sentiment polarity of the translated text, as illustrated below, with a percentage of around 10%. [Original English Text]: I am at home on bed rest and desperate for something good to read. [Sentiment Label: Negative] [Translated Chinese Text]: At FM�IWP, *pffflM3WRff . {Meaning: I am in bed to rest at</context>
</contexts>
<marker>Duh, Fujino, Nagata, 2011</marker>
<rawString>Kevin Duh and Akinori Fujino and Masaaki Nagata. 2011. Is Machine Translation Ripe for Crosslingual Sentiment Classification? In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: shortpapers, pages 429-433, Portland, Oregon, June 19-24, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Ksieh</author>
<author>XiangRui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A Library for Large Linear Classification.</title>
<date>2008</date>
<journal>In Journal of Machine Learning Research,</journal>
<volume>9</volume>
<pages>1871--1874</pages>
<contexts>
<context position="26959" citStr="Fan et al., 2008" startWordPosition="4393" endWordPosition="4396">original and translated reviews. The dimension of doc2vec is 300, while the other parameters are set as default. The baseline methods described above are categorized into three classes: the first four which are preliminary methods, the middle three which are several state-of-the-art models being comparable to our proposed model, and the last one which is a comparison to suggest that the knowledge representation is not the answer to the performance improvement. For all the methods excluding LB and BR2013, we use support vector machines (SVMs) as basic classifiers. We use the Liblinear package (Fan et al., 2008) with the linear kernel5. All methods use Unigram+Bigram features to train the basic classifiers, except for dCredB. 4.2 Experimental Result In this work, there are two main parameters that may significantly influence the performance of our proposed model. They are the new knowledge validation boundary 0 and the validation scale (+ in the training data. We set the values of parameters with the grid search strategy. We first fix initial (+ = 14 to search the best new knowledge validation boundary 0 from an empirical value set {0.30, 0.35, 0.40, 0.45, 0.50}. We then fix the best 0 = 0.40 to chec</context>
</contexts>
<marker>Fan, Chang, Ksieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Ksieh, XiangRui Wang, Chih-Jen Lin. 2008. LIBLINEAR: A Library for Large Linear Classification. In Journal of Machine Learning Research, 9 (2008) 1871-1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micheal Gamon</author>
</authors>
<title>Sentiment Classification on Customer Feedback Data: Noisy Data, Large Feature Vectors and the Role of Linguistic Analysis.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics,</booktitle>
<pages>841--847</pages>
<publisher>CH.</publisher>
<contexts>
<context position="6385" citStr="Gamon (2004)" startWordPosition="949" endWordPosition="950">arning based approaches (Liu, 2012). Lexicon-based approaches highly depend on sentiment lexicons. Turney (2002) derives the overall phrase and document sentiment scores by averaging the sentiment scores provided in a lexicon over the words included. Similar idea is adopted in (Hiroshi et al., 2004; Kennedy and Inkpen, 2006). Machine learning based approaches, on the other hand, apply classification models. The task-specific features are designed to train sentiment polarity classifiers. Pang et al. (2002) compare the performance of NB, SVM and ME on movie reviews. SVM is found more effective. Gamon (2004) shows that SVM with deep linguistic features can further improve the performance. A variety of other machine learning approaches are also proposed to sentiment classification (Mullen and Collier, 2004; Read, 2005; Hassan and Radev, 2010; Socher et al., 2013). Cross-domain sentiment classification (CDSC) shares certain common characteristics with crosslingual sentiment classification (CLSC) (Tan et al., 2007; Li et al., 2009; Pan and Yang, 2010; He et al., 2011a; Glorot et al., 2011). Notice that the gap between source domain and target domain is the main difference between CDSC and CLSC. CLSC</context>
</contexts>
<marker>Gamon, 2004</marker>
<rawString>Micheal Gamon. 2004. Sentiment Classification on Customer Feedback Data: Noisy Data, Large Feature Vectors and the Role of Linguistic Analysis. In Proceedings of the 20th International Conference on Computational Linguistics, pages 841-847, CH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Glorot</author>
<author>Antoine Bordes</author>
<author>Yoshua Bengio</author>
</authors>
<title>Domain Adaptation for Large-scale Sentiment Classification: A Deep Learning Approach.</title>
<date>2011</date>
<booktitle>In Proceedings of the 28th International Conference on Machine Learning,</booktitle>
<pages>513--520</pages>
<location>Bellevue, Washington, USA.</location>
<contexts>
<context position="6873" citStr="Glorot et al., 2011" startWordPosition="1020" endWordPosition="1023">lassifiers. Pang et al. (2002) compare the performance of NB, SVM and ME on movie reviews. SVM is found more effective. Gamon (2004) shows that SVM with deep linguistic features can further improve the performance. A variety of other machine learning approaches are also proposed to sentiment classification (Mullen and Collier, 2004; Read, 2005; Hassan and Radev, 2010; Socher et al., 2013). Cross-domain sentiment classification (CDSC) shares certain common characteristics with crosslingual sentiment classification (CLSC) (Tan et al., 2007; Li et al., 2009; Pan and Yang, 2010; He et al., 2011a; Glorot et al., 2011). Notice that the gap between source domain and target domain is the main difference between CDSC and CLSC. CLSC copes with two different datasets in two different languages. This difference makes CLSC a new challenge, drawing specific attention to researcher recently. 2.2 Cross-lingual Sentiment Analysis There are two alternative solutions to cross-lingual sentiment analysis. One is ensemble learning that combines multiple classifiers. The other is transfer learning that develops strategies to adapt the knowledge from one language to the other. Wan (2008) is among the pioneers to develop the </context>
</contexts>
<marker>Glorot, Bordes, Bengio, 2011</marker>
<rawString>Xavier Glorot, Antoine Bordes, and Yoshua Bengio. 2011. Domain Adaptation for Large-scale Sentiment Classification: A Deep Learning Approach. In Proceedings of the 28th International Conference on Machine Learning, pages 513-520, Bellevue, Washington, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lin Gui</author>
<author>Ruifeng Xu</author>
<author>Qin Lu</author>
<author>Jun Xu</author>
<author>Jian Xu</author>
<author>Bin Liu</author>
<author>Xiaolong Wang</author>
</authors>
<title>Cross-lingual Opinion Analysis via Negative Transfer Detection.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52th Annual Meeting of the Association for Computational Linguistics (short paper),</booktitle>
<pages>860--865</pages>
<location>Baltimore, Maryland, USA,</location>
<contexts>
<context position="2680" citStr="Gui et al., 2014" startWordPosition="393" endWordPosition="396">ome the language barrier, and then develop an appropriate knowledge transfer approach to leverage the annotated data from the source language to train a sentiment classification model in the target language, either supervised or semi-supervised. In particular, these approaches exploit and convert the knowledge learned from the source language to automatically generate and expand the pseudo-training data for the target language. The machine translation (MT) service is one of the most common ways used to build the language connection (Wan, 2008; Banea et al., 2008; Wan, 2009; Wei and Pal, 2010; Gui et al., 2014). Although it is claimed in Duh et al. (2011) that the MT service is ripe for CLSA, the imperfect MT quality hinders existing MTbased CLSA approaches from the further advance. In our preliminary study, we find that even the Google translator1 (i.e., one of the most widely used online MT service (Shankland 2013)) may unavoidably changes the sentiment polarity of the translated text, as illustrated below, with a percentage of around 10%. [Original English Text]: I am at home on bed rest and desperate for something good to read. [Sentiment Label: Negative] [Translated Chinese Text]: At FM�IWP, *p</context>
<context position="9174" citStr="Gui et al., 2014" startWordPosition="1371" endWordPosition="1374">and incorrect sentiment labels introduced by machine translation and knowledge transfer. However, the parallel data is also a scarce resource. 420 Some existing transfer learning based CLSA methods have attempted to address the noisy knowledge problem caused by wrong labels by checking label consistency. For example, to filter out the unconfident labels in Chinese, the supervised learning method proposed by (Xu et al., 2011) runs boosting in Chinese by checking consistency between the labels manually annotated in English and predicted by Chinese classifiers on translated Chinese. The work in (Gui et al., 2014) follows the same line although it considers knowledge transferring between two languages. On the contrary, the main focus of our work is to filter out the noisy knowledge having sentiment changes by wrong translations. Actually, both label consistency checking and linguistic distribution checking are important. Any one alone cannot work well. In fact, both of them are considered as the knowledge validation in our work, though the later is our focus. 3 Credible Boosting Model In this paper, we propose a knowledge validation approach to improve the effectiveness of knowledge transfer without di</context>
<context position="34645" citStr="Gui et al., 2014" startWordPosition="5610" endWordPosition="5613">.7 0 20 40 60 80 100 120 Performance in Books domain. Iteration Number selfBoost CoTr BTL-2 CredBoost 0.86 0.85 0.84 0.83 0.82 0.81 0.79 0.78 0.77 0.76 0 20 40 60 80 100 120 0.8 Performance in DVD domain. Iteration Number CoTr BTL-2 CredBoost 0.75 0.65 0.8 0.7 0.6 0 20 40 60 80 100 120 Performance in Music domain. Iteration Number CoTr BTL-2 CredBoost (a) Performances comparison in three domains (b) Growth sizes comparison in three domains Figure 1: Performances vs. Growth Sizes for SB-CN, CoTr, BTL-2, and CredBoost in three domains. The similar performance curves of CoTr is also reported in (Gui et al., 2014). Growth Sizes in Books domain. Growth Sizes in DVD do main. Growth Sizes in Music domain. Iteration Number Iteration Number Iteration Number 30 25 20 15 10 5 00 20 40 60 80 100 120 selfBoost CoTr BTL-2 CredBoost 30 25 20 15 10 5 00 20 40 60 80 100 120 selfBoost CoTr BTL-2 CredBoost 25 20 15 10 5 00 20 40 60 80 100 120 selfBoost CoTr BTL-2 CredBoost Although document-to-vector represents content semantic well, it cannot determine the sentiment polarity of text well, even when the documentto-vectors that are used to train basic classifiers are learned on the mixture of the translated and origin</context>
</contexts>
<marker>Gui, Xu, Lu, Xu, Xu, Liu, Wang, 2014</marker>
<rawString>Lin Gui, Ruifeng Xu, Qin Lu, Jun Xu, Jian Xu, Bin Liu, Xiaolong Wang. 2014. Cross-lingual Opinion Analysis via Negative Transfer Detection. In Proceedings of the 52th Annual Meeting of the Association for Computational Linguistics (short paper), pages 860-865, Baltimore, Maryland, USA, June 23-25 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmed Hassan</author>
<author>Dragomir Radev</author>
</authors>
<title>Identifying Text Polarity Using Random Walks.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>395--403</pages>
<location>Uppsala,</location>
<contexts>
<context position="6622" citStr="Hassan and Radev, 2010" startWordPosition="982" endWordPosition="985">ver the words included. Similar idea is adopted in (Hiroshi et al., 2004; Kennedy and Inkpen, 2006). Machine learning based approaches, on the other hand, apply classification models. The task-specific features are designed to train sentiment polarity classifiers. Pang et al. (2002) compare the performance of NB, SVM and ME on movie reviews. SVM is found more effective. Gamon (2004) shows that SVM with deep linguistic features can further improve the performance. A variety of other machine learning approaches are also proposed to sentiment classification (Mullen and Collier, 2004; Read, 2005; Hassan and Radev, 2010; Socher et al., 2013). Cross-domain sentiment classification (CDSC) shares certain common characteristics with crosslingual sentiment classification (CLSC) (Tan et al., 2007; Li et al., 2009; Pan and Yang, 2010; He et al., 2011a; Glorot et al., 2011). Notice that the gap between source domain and target domain is the main difference between CDSC and CLSC. CLSC copes with two different datasets in two different languages. This difference makes CLSC a new challenge, drawing specific attention to researcher recently. 2.2 Cross-lingual Sentiment Analysis There are two alternative solutions to cro</context>
</contexts>
<marker>Hassan, Radev, 2010</marker>
<rawString>Ahmed Hassan and Dragomir Radev. 2010. Identifying Text Polarity Using Random Walks. In Proceedings of the 48th Annual Meeting on Association for Computational Linguistics, pages 395-403, Uppsala, Sweden, 11-16 July 2010.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Yulan He</author>
<author>Chenghua Lin</author>
</authors>
<title>Harith Alani. 2011a. Automatically Extracting Polarity-bearing Topics for Cross Domain Sentiment Classification.</title>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Huamn Language Technologies,</booktitle>
<pages>123--131</pages>
<location>Portland, Oregon, USA.</location>
<marker>He, Lin, </marker>
<rawString>Yulan He, Chenghua Lin, Harith Alani. 2011a. Automatically Extracting Polarity-bearing Topics for Cross Domain Sentiment Classification. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Huamn Language Technologies, pages 123-131, Portland, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yulan He</author>
</authors>
<title>Latent Sentiment Model for WeaklySupervised Cross-Lingual Sentiment Classification.</title>
<date>2011</date>
<booktitle>In Proceedings of the 33th European Conference on Information Retrieval(ECIR</booktitle>
<pages>18--21</pages>
<location>Dublin, Ireland.</location>
<contexts>
<context position="7991" citStr="He, 2011" startWordPosition="1186" endWordPosition="1187">knowledge from one language to the other. Wan (2008) is among the pioneers to develop the ensemble learning solutions, where multiple classifiers learned from different training datasets including those in original languages and translated languages are combined by voting. Most researches, on the other hand, explore transfer learning and focus on knowledge adaptation. For example, Wan (2009) applies a supervised cotraining framework to iteratively adapt knowledge learned from the two languages by transferring translated texts to each other. Other similar work includes (Wei and Pal, 2010) and (He, 2011b). All these approaches rely on MT to build language connection. Meanwhile, the unlabeled parallel data is also employed to fill the gap between two languages. To solve the feature coverage problem with the EM algorithm, Meng et al. (2012) leverage the unlabeled parallel data to learn unseen sentiment words. Similarly, Popat et al. (2013) use the unlabeled parallel data to cluster features in order to reduce the data sparsity problem. Meng et al. (2012) and Popat et al. (2013) also use the unlabeled parallel data to reduce the negative influence of the noisy and incorrect sentiment labels int</context>
</contexts>
<marker>He, 2011</marker>
<rawString>Yulan He. 2011b. Latent Sentiment Model for WeaklySupervised Cross-Lingual Sentiment Classification. In Proceedings of the 33th European Conference on Information Retrieval(ECIR 2011), 18-21 Apr 2011, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>KANAYAMA Hiroshi</author>
<author>NASUKAWAA Tetsuya</author>
<author>WATANABE Hideo</author>
</authors>
<title>Deeper Sentiment Analysis Using Machine Translation Technology.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics,</booktitle>
<pages>494--500</pages>
<contexts>
<context position="6072" citStr="Hiroshi et al., 2004" startWordPosition="900" endWordPosition="903">work. 2 Related Work 2.1 Sentiment Analysis Sentiment has been analyzed in different language granularity, e.g., entity, aspect, sentence and document. This paper focuses on sentiment analysis of online product reviews in the document level. Existing approaches are generally categorized into lexicon-based and machine learning based approaches (Liu, 2012). Lexicon-based approaches highly depend on sentiment lexicons. Turney (2002) derives the overall phrase and document sentiment scores by averaging the sentiment scores provided in a lexicon over the words included. Similar idea is adopted in (Hiroshi et al., 2004; Kennedy and Inkpen, 2006). Machine learning based approaches, on the other hand, apply classification models. The task-specific features are designed to train sentiment polarity classifiers. Pang et al. (2002) compare the performance of NB, SVM and ME on movie reviews. SVM is found more effective. Gamon (2004) shows that SVM with deep linguistic features can further improve the performance. A variety of other machine learning approaches are also proposed to sentiment classification (Mullen and Collier, 2004; Read, 2005; Hassan and Radev, 2010; Socher et al., 2013). Cross-domain sentiment cla</context>
</contexts>
<marker>Hiroshi, Tetsuya, Hideo, 2004</marker>
<rawString>KANAYAMA Hiroshi, NASUKAWAA Tetsuya, WATANABE Hideo. 2004. Deeper Sentiment Analysis Using Machine Translation Technology. In Proceedings of the 20th International Conference on Computational Linguistics, pages 494-500.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alistair Kennedy</author>
<author>Diana Inkpen</author>
</authors>
<title>Sentiment Classification of Movie and Product Reviews Using Contextual Valence Shifters.</title>
<date>2006</date>
<tech>Computational Intelligence,22(2):110-125.</tech>
<contexts>
<context position="6099" citStr="Kennedy and Inkpen, 2006" startWordPosition="904" endWordPosition="907">.1 Sentiment Analysis Sentiment has been analyzed in different language granularity, e.g., entity, aspect, sentence and document. This paper focuses on sentiment analysis of online product reviews in the document level. Existing approaches are generally categorized into lexicon-based and machine learning based approaches (Liu, 2012). Lexicon-based approaches highly depend on sentiment lexicons. Turney (2002) derives the overall phrase and document sentiment scores by averaging the sentiment scores provided in a lexicon over the words included. Similar idea is adopted in (Hiroshi et al., 2004; Kennedy and Inkpen, 2006). Machine learning based approaches, on the other hand, apply classification models. The task-specific features are designed to train sentiment polarity classifiers. Pang et al. (2002) compare the performance of NB, SVM and ME on movie reviews. SVM is found more effective. Gamon (2004) shows that SVM with deep linguistic features can further improve the performance. A variety of other machine learning approaches are also proposed to sentiment classification (Mullen and Collier, 2004; Read, 2005; Hassan and Radev, 2010; Socher et al., 2013). Cross-domain sentiment classification (CDSC) shares c</context>
</contexts>
<marker>Kennedy, Inkpen, 2006</marker>
<rawString>Alistair Kennedy and Diana Inkpen. 2006. Sentiment Classification of Movie and Product Reviews Using Contextual Valence Shifters. Computational Intelligence,22(2):110-125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quoc Le</author>
<author>Tomas Mikolov</author>
</authors>
<title>Distributed Representations of Sentences and Documents.</title>
<date>2014</date>
<journal>JMLR: W&amp;CP</journal>
<booktitle>In Proceedings of the 31th International Conference on Machine Learning,</booktitle>
<volume>32</volume>
<location>Beijing, China,</location>
<contexts>
<context position="10171" citStr="Le and Mikolov, 2014" startWordPosition="1527" endWordPosition="1530">idered as the knowledge validation in our work, though the later is our focus. 3 Credible Boosting Model In this paper, we propose a knowledge validation approach to improve the effectiveness of knowledge transfer without directly using extra parallel data. Our target is to filter out the noisy sentiment labels introduced by MT and the incorrect sentiment labels generated by imperfect classifier in the source language. Here, the knowledge is referred to as a collection of distributed document presentations with sentiment labels that have been verified to be robust in sentiment classification (Le and Mikolov, 2014). A novel credible boosting model, namely CredBoost is proposed to apply transfer-supervised learning with an added selfvalidation mechanism to guarantee the knowledge transferred highly credible and self-adaptive. 3.1 Problem Description In a standard cross-lingual sentiment analysis setting, the training data includes labeled English reviews LEN = {(xlen i , yi)}Mi=1 and unlabeled Chinese reviews UCN = {xucn j }N j=1, where xki (k = len or ucn) represents review i and yi ∈ {−1, 1} is the sentiment label of review xli. The test data is Chinese reviews TCN = {xtcn s }Ss=1. We now introduce the</context>
<context position="18538" citStr="Le and Mikolov, 2014" startWordPosition="2954" endWordPosition="2957">nal semantics into learning can improve the performance of sentiment classifiers. Saif et al. (2012) also demonstrate that the addition of extra semantic features can further improve performance. In order to filter out noisy and incorrect sentiment labels, we propose a knowledge validation approach to reduce these noisy data that hinder the improvement of learning performance. Knowledge validation is a way to identify the acquired knowledge implied in current knowledge system and also the new knowledge fresh to current knowledge system. The knowledge can be represented in the semantic space. (Le and Mikolov, 2014) project documents into a low-dimension semantic space with a deep learning approach, known as document-to-vector (Doc2Vec3). Considering that Dov2Vec has been verified to be efficient in many NLP tasks including sentiment analysis, we follow previous research to represent knowledge embedded in product reviews with the vectors generated by Doc2Vec. Suppose distributed representations (i.e., lowdimensional vectors) of the all reviews including {LTEN, LBEN, UTrEN} and {LTrCN, UCN} are {V(LTEN), V(LBEN), V(UTrEN)} and {V(LTrCN),V(UCN)} respectively. At iteration (t), V(LT (t) EN ) is the current </context>
</contexts>
<marker>Le, Mikolov, 2014</marker>
<rawString>Quoc Le, Tomas Mikolov. 2014. Distributed Representations of Sentences and Documents. In Proceedings of the 31th International Conference on Machine Learning, Beijing, China, 2014. JMLR: W&amp;CP volume 32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Li</author>
<author>Vikas Sindhwani</author>
<author>Chris Ding</author>
<author>Yi Zhang</author>
</authors>
<title>Knowledge Transformation for CrossDomain Sentiment Classification.</title>
<date>2009</date>
<booktitle>In Proceedings of the 32th International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>716--717</pages>
<location>Boston, MA, USA.</location>
<contexts>
<context position="6813" citStr="Li et al., 2009" startWordPosition="1008" endWordPosition="1011">fic features are designed to train sentiment polarity classifiers. Pang et al. (2002) compare the performance of NB, SVM and ME on movie reviews. SVM is found more effective. Gamon (2004) shows that SVM with deep linguistic features can further improve the performance. A variety of other machine learning approaches are also proposed to sentiment classification (Mullen and Collier, 2004; Read, 2005; Hassan and Radev, 2010; Socher et al., 2013). Cross-domain sentiment classification (CDSC) shares certain common characteristics with crosslingual sentiment classification (CLSC) (Tan et al., 2007; Li et al., 2009; Pan and Yang, 2010; He et al., 2011a; Glorot et al., 2011). Notice that the gap between source domain and target domain is the main difference between CDSC and CLSC. CLSC copes with two different datasets in two different languages. This difference makes CLSC a new challenge, drawing specific attention to researcher recently. 2.2 Cross-lingual Sentiment Analysis There are two alternative solutions to cross-lingual sentiment analysis. One is ensemble learning that combines multiple classifiers. The other is transfer learning that develops strategies to adapt the knowledge from one language to</context>
</contexts>
<marker>Li, Sindhwani, Ding, Zhang, 2009</marker>
<rawString>Tao Li, Vikas Sindhwani, Chris Ding, and Yi Zhang. 2009. Knowledge Transformation for CrossDomain Sentiment Classification. In Proceedings of the 32th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 716-717, Boston, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Sentiment Analysis and Opinion Mining.</title>
<date>2012</date>
<publisher>Morgan &amp; Claypool Publisher.</publisher>
<contexts>
<context position="5808" citStr="Liu, 2012" startWordPosition="862" endWordPosition="863">-ofthe-art CLSA approaches. The rest of this paper is organized as follows. Section 2 summarizes the related work. Section 3 explains the proposed model. Section 4 presents experimental results. Finally, Section 5 concludes the paper and suggests future work. 2 Related Work 2.1 Sentiment Analysis Sentiment has been analyzed in different language granularity, e.g., entity, aspect, sentence and document. This paper focuses on sentiment analysis of online product reviews in the document level. Existing approaches are generally categorized into lexicon-based and machine learning based approaches (Liu, 2012). Lexicon-based approaches highly depend on sentiment lexicons. Turney (2002) derives the overall phrase and document sentiment scores by averaging the sentiment scores provided in a lexicon over the words included. Similar idea is adopted in (Hiroshi et al., 2004; Kennedy and Inkpen, 2006). Machine learning based approaches, on the other hand, apply classification models. The task-specific features are designed to train sentiment polarity classifiers. Pang et al. (2002) compare the performance of NB, SVM and ME on movie reviews. SVM is found more effective. Gamon (2004) shows that SVM with de</context>
</contexts>
<marker>Liu, 2012</marker>
<rawString>Bing Liu. May 2012. Sentiment Analysis and Opinion Mining. Morgan &amp; Claypool Publisher.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinfan Meng</author>
<author>Furu Wei</author>
<author>Xiaohua Liu</author>
<author>Ming Zhou</author>
<author>Ge Xu</author>
<author>Houfeng Wang</author>
</authors>
<title>Cross-Lingual Mixture Model for Sentiment Classification.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>572--581</pages>
<location>Jeju, Republic of</location>
<contexts>
<context position="8231" citStr="Meng et al. (2012)" startWordPosition="1223" endWordPosition="1226">ranslated languages are combined by voting. Most researches, on the other hand, explore transfer learning and focus on knowledge adaptation. For example, Wan (2009) applies a supervised cotraining framework to iteratively adapt knowledge learned from the two languages by transferring translated texts to each other. Other similar work includes (Wei and Pal, 2010) and (He, 2011b). All these approaches rely on MT to build language connection. Meanwhile, the unlabeled parallel data is also employed to fill the gap between two languages. To solve the feature coverage problem with the EM algorithm, Meng et al. (2012) leverage the unlabeled parallel data to learn unseen sentiment words. Similarly, Popat et al. (2013) use the unlabeled parallel data to cluster features in order to reduce the data sparsity problem. Meng et al. (2012) and Popat et al. (2013) also use the unlabeled parallel data to reduce the negative influence of the noisy and incorrect sentiment labels introduced by machine translation and knowledge transfer. However, the parallel data is also a scarce resource. 420 Some existing transfer learning based CLSA methods have attempted to address the noisy knowledge problem caused by wrong labels</context>
</contexts>
<marker>Meng, Wei, Liu, Zhou, Xu, Wang, 2012</marker>
<rawString>Xinfan Meng, Furu Wei, Xiaohua Liu, Ming Zhou, Ge Xu, Houfeng Wang. 2012. Cross-Lingual Mixture Model for Sentiment Classification. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 572-581, Jeju, Republic of Korea, 8-14 July 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tony Mullen</author>
<author>Nigel Collier</author>
</authors>
<title>Sentiment analysis using support vector machines with diverse inoformation sources.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>412--418</pages>
<note>poster paper.</note>
<contexts>
<context position="6586" citStr="Mullen and Collier, 2004" startWordPosition="976" endWordPosition="979">ntiment scores provided in a lexicon over the words included. Similar idea is adopted in (Hiroshi et al., 2004; Kennedy and Inkpen, 2006). Machine learning based approaches, on the other hand, apply classification models. The task-specific features are designed to train sentiment polarity classifiers. Pang et al. (2002) compare the performance of NB, SVM and ME on movie reviews. SVM is found more effective. Gamon (2004) shows that SVM with deep linguistic features can further improve the performance. A variety of other machine learning approaches are also proposed to sentiment classification (Mullen and Collier, 2004; Read, 2005; Hassan and Radev, 2010; Socher et al., 2013). Cross-domain sentiment classification (CDSC) shares certain common characteristics with crosslingual sentiment classification (CLSC) (Tan et al., 2007; Li et al., 2009; Pan and Yang, 2010; He et al., 2011a; Glorot et al., 2011). Notice that the gap between source domain and target domain is the main difference between CDSC and CLSC. CLSC copes with two different datasets in two different languages. This difference makes CLSC a new challenge, drawing specific attention to researcher recently. 2.2 Cross-lingual Sentiment Analysis There </context>
</contexts>
<marker>Mullen, Collier, 2004</marker>
<rawString>Tony Mullen and Nigel Collier. 2004. Sentiment analysis using support vector machines with diverse inoformation sources. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 412-418, (July 2004) poster paper.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sinno Jialin Pan</author>
<author>Qiang Yang</author>
<author>IEEE Fellow</author>
</authors>
<title>A Survey on Transfer Learning.</title>
<date>2010</date>
<journal>In Journal of IEEE Transactions on Knowledge and Data Engineering,</journal>
<location>Vol.22, NO.10,</location>
<marker>Pan, Yang, Fellow, 2010</marker>
<rawString>Sinno Jialin Pan and Qiang Yang, Fellow, IEEE. 2010. A Survey on Transfer Learning. In Journal of IEEE Transactions on Knowledge and Data Engineering, Vol.22, NO.10, October 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumps Up? Sentiment Classification using Machine Learning Techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>79--86</pages>
<location>Philadelphia,</location>
<contexts>
<context position="6283" citStr="Pang et al. (2002)" startWordPosition="929" endWordPosition="932">views in the document level. Existing approaches are generally categorized into lexicon-based and machine learning based approaches (Liu, 2012). Lexicon-based approaches highly depend on sentiment lexicons. Turney (2002) derives the overall phrase and document sentiment scores by averaging the sentiment scores provided in a lexicon over the words included. Similar idea is adopted in (Hiroshi et al., 2004; Kennedy and Inkpen, 2006). Machine learning based approaches, on the other hand, apply classification models. The task-specific features are designed to train sentiment polarity classifiers. Pang et al. (2002) compare the performance of NB, SVM and ME on movie reviews. SVM is found more effective. Gamon (2004) shows that SVM with deep linguistic features can further improve the performance. A variety of other machine learning approaches are also proposed to sentiment classification (Mullen and Collier, 2004; Read, 2005; Hassan and Radev, 2010; Socher et al., 2013). Cross-domain sentiment classification (CDSC) shares certain common characteristics with crosslingual sentiment classification (CLSC) (Tan et al., 2007; Li et al., 2009; Pan and Yang, 2010; He et al., 2011a; Glorot et al., 2011). Notice t</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang and Lillian Lee, Shivakumar Vaithyanathan. 2002. Thumps Up? Sentiment Classification using Machine Learning Techniques. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, pages 79-86, Philadelphia, July 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kashyap Popat</author>
<author>A R Balamurali</author>
<author>Pushpak Bhattacharyya</author>
<author>Gholamreza Haffari</author>
</authors>
<title>The Haves and the Have-Nots: Leverage Unlabeled Corpora for Sentiment Analysis.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>412--422</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="8332" citStr="Popat et al. (2013)" startWordPosition="1238" endWordPosition="1241">rning and focus on knowledge adaptation. For example, Wan (2009) applies a supervised cotraining framework to iteratively adapt knowledge learned from the two languages by transferring translated texts to each other. Other similar work includes (Wei and Pal, 2010) and (He, 2011b). All these approaches rely on MT to build language connection. Meanwhile, the unlabeled parallel data is also employed to fill the gap between two languages. To solve the feature coverage problem with the EM algorithm, Meng et al. (2012) leverage the unlabeled parallel data to learn unseen sentiment words. Similarly, Popat et al. (2013) use the unlabeled parallel data to cluster features in order to reduce the data sparsity problem. Meng et al. (2012) and Popat et al. (2013) also use the unlabeled parallel data to reduce the negative influence of the noisy and incorrect sentiment labels introduced by machine translation and knowledge transfer. However, the parallel data is also a scarce resource. 420 Some existing transfer learning based CLSA methods have attempted to address the noisy knowledge problem caused by wrong labels by checking label consistency. For example, to filter out the unconfident labels in Chinese, the sup</context>
</contexts>
<marker>Popat, Balamurali, Bhattacharyya, Haffari, 2013</marker>
<rawString>Kashyap Popat, Balamurali A R, Pushpak Bhattacharyya and Gholamreza Haffari. 2013. The Haves and the Have-Nots: Leverage Unlabeled Corpora for Sentiment Analysis. In Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics, pages 412-422, Sofia, Bulgaria, 4-9 August 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathon Read</author>
</authors>
<title>Using Emotions to reduce Dependency in Machine Learning Techniques for Sentiment Classification.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43th Annual Meeting on Association for Computational Linguistics Student Research Workshop,</booktitle>
<pages>43--48</pages>
<contexts>
<context position="6598" citStr="Read, 2005" startWordPosition="980" endWordPosition="981"> a lexicon over the words included. Similar idea is adopted in (Hiroshi et al., 2004; Kennedy and Inkpen, 2006). Machine learning based approaches, on the other hand, apply classification models. The task-specific features are designed to train sentiment polarity classifiers. Pang et al. (2002) compare the performance of NB, SVM and ME on movie reviews. SVM is found more effective. Gamon (2004) shows that SVM with deep linguistic features can further improve the performance. A variety of other machine learning approaches are also proposed to sentiment classification (Mullen and Collier, 2004; Read, 2005; Hassan and Radev, 2010; Socher et al., 2013). Cross-domain sentiment classification (CDSC) shares certain common characteristics with crosslingual sentiment classification (CLSC) (Tan et al., 2007; Li et al., 2009; Pan and Yang, 2010; He et al., 2011a; Glorot et al., 2011). Notice that the gap between source domain and target domain is the main difference between CDSC and CLSC. CLSC copes with two different datasets in two different languages. This difference makes CLSC a new challenge, drawing specific attention to researcher recently. 2.2 Cross-lingual Sentiment Analysis There are two alte</context>
</contexts>
<marker>Read, 2005</marker>
<rawString>Jonathon Read. 2005. Using Emotions to reduce Dependency in Machine Learning Techniques for Sentiment Classification. In Proceedings of the 43th Annual Meeting on Association for Computational Linguistics Student Research Workshop, pages 43-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hassan Saif</author>
</authors>
<title>Yulan He and Harith Alani.</title>
<date>2012</date>
<booktitle>In Proceedings of the 11th International Semantics Web Conference ISWC 2012,</booktitle>
<location>Boston, USA.</location>
<marker>Saif, 2012</marker>
<rawString>Hassan Saif, Yulan He and Harith Alani. 2012. Semantic Sentiment Analysis of Twitter. In Proceedings of the 11th International Semantics Web Conference ISWC 2012, Boston, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Shankland</author>
</authors>
<title>Google Translate now serves 200 millon people daily.</title>
<date>2013</date>
<booktitle>In CNET. CBS Interactive Inc.</booktitle>
<contexts>
<context position="2992" citStr="Shankland 2013" startWordPosition="449" endWordPosition="450">arned from the source language to automatically generate and expand the pseudo-training data for the target language. The machine translation (MT) service is one of the most common ways used to build the language connection (Wan, 2008; Banea et al., 2008; Wan, 2009; Wei and Pal, 2010; Gui et al., 2014). Although it is claimed in Duh et al. (2011) that the MT service is ripe for CLSA, the imperfect MT quality hinders existing MTbased CLSA approaches from the further advance. In our preliminary study, we find that even the Google translator1 (i.e., one of the most widely used online MT service (Shankland 2013)) may unavoidably changes the sentiment polarity of the translated text, as illustrated below, with a percentage of around 10%. [Original English Text]: I am at home on bed rest and desperate for something good to read. [Sentiment Label: Negative] [Translated Chinese Text]: At FM�IWP, *pffflM3WRff . {Meaning: I am in bed to rest at home and feel that desperate things are also good to read.}[Sentiment Label: Positive] The noisy data generated by MT errors for sure will weaken the contribution of the transferred knowledge and even worse may create conflicting knowledge. While it is a critical st</context>
</contexts>
<marker>Shankland, 2013</marker>
<rawString>Stephen Shankland. 2013. Google Translate now serves 200 millon people daily. In CNET. CBS Interactive Inc. May 18, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Y Wu</author>
<author>Jason Chuang</author>
<author>Chiristopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive Deep Models for Semantics Computationality Over a Sentiment Treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="6644" citStr="Socher et al., 2013" startWordPosition="986" endWordPosition="989">Similar idea is adopted in (Hiroshi et al., 2004; Kennedy and Inkpen, 2006). Machine learning based approaches, on the other hand, apply classification models. The task-specific features are designed to train sentiment polarity classifiers. Pang et al. (2002) compare the performance of NB, SVM and ME on movie reviews. SVM is found more effective. Gamon (2004) shows that SVM with deep linguistic features can further improve the performance. A variety of other machine learning approaches are also proposed to sentiment classification (Mullen and Collier, 2004; Read, 2005; Hassan and Radev, 2010; Socher et al., 2013). Cross-domain sentiment classification (CDSC) shares certain common characteristics with crosslingual sentiment classification (CLSC) (Tan et al., 2007; Li et al., 2009; Pan and Yang, 2010; He et al., 2011a; Glorot et al., 2011). Notice that the gap between source domain and target domain is the main difference between CDSC and CLSC. CLSC copes with two different datasets in two different languages. This difference makes CLSC a new challenge, drawing specific attention to researcher recently. 2.2 Cross-lingual Sentiment Analysis There are two alternative solutions to cross-lingual sentiment a</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Y. Wu, Jason Chuang, Chiristopher D. Manning, Andrew Y. Ng and Christopher Potts. 2013. Recursive Deep Models for Semantics Computationality Over a Sentiment Treebank. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Songbo Tan</author>
<author>Gaowei Wu</author>
</authors>
<title>Huifeng Tang and Xueqi Cheng.</title>
<date>2007</date>
<booktitle>In CIKM</booktitle>
<location>Lisboa, Portugal.</location>
<marker>Tan, Wu, 2007</marker>
<rawString>Songbo Tan, Gaowei Wu, Huifeng Tang and Xueqi Cheng. 2007. A Novel Scheme for Domain-transfer Problem in the context of Sentiment Analysis. In CIKM 2007, November 6-8, 2007, Lisboa, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Thumps Up or Thumps Down? Semantic Orientation Applied to Unsupervised Classification of Reviews.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>417--424</pages>
<location>Philadelphia,</location>
<contexts>
<context position="5885" citStr="Turney (2002)" startWordPosition="872" endWordPosition="873">. Section 2 summarizes the related work. Section 3 explains the proposed model. Section 4 presents experimental results. Finally, Section 5 concludes the paper and suggests future work. 2 Related Work 2.1 Sentiment Analysis Sentiment has been analyzed in different language granularity, e.g., entity, aspect, sentence and document. This paper focuses on sentiment analysis of online product reviews in the document level. Existing approaches are generally categorized into lexicon-based and machine learning based approaches (Liu, 2012). Lexicon-based approaches highly depend on sentiment lexicons. Turney (2002) derives the overall phrase and document sentiment scores by averaging the sentiment scores provided in a lexicon over the words included. Similar idea is adopted in (Hiroshi et al., 2004; Kennedy and Inkpen, 2006). Machine learning based approaches, on the other hand, apply classification models. The task-specific features are designed to train sentiment polarity classifiers. Pang et al. (2002) compare the performance of NB, SVM and ME on movie reviews. SVM is found more effective. Gamon (2004) shows that SVM with deep linguistic features can further improve the performance. A variety of othe</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Peter D. Turney. 2002. Thumps Up or Thumps Down? Semantic Orientation Applied to Unsupervised Classification of Reviews. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 417-424, Philadelphia, July 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
</authors>
<title>Using Bilingual Knowledge and Ensemble Technics for Unsupervised Chinese Sentiment Analysis.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natual Language Processing,</booktitle>
<pages>553--561</pages>
<location>Honolulu,</location>
<contexts>
<context position="2611" citStr="Wan, 2008" startWordPosition="381" endWordPosition="382">e connection between the source and target languages to overcome the language barrier, and then develop an appropriate knowledge transfer approach to leverage the annotated data from the source language to train a sentiment classification model in the target language, either supervised or semi-supervised. In particular, these approaches exploit and convert the knowledge learned from the source language to automatically generate and expand the pseudo-training data for the target language. The machine translation (MT) service is one of the most common ways used to build the language connection (Wan, 2008; Banea et al., 2008; Wan, 2009; Wei and Pal, 2010; Gui et al., 2014). Although it is claimed in Duh et al. (2011) that the MT service is ripe for CLSA, the imperfect MT quality hinders existing MTbased CLSA approaches from the further advance. In our preliminary study, we find that even the Google translator1 (i.e., one of the most widely used online MT service (Shankland 2013)) may unavoidably changes the sentiment polarity of the translated text, as illustrated below, with a percentage of around 10%. [Original English Text]: I am at home on bed rest and desperate for something good to read.</context>
<context position="7435" citStr="Wan (2008)" startWordPosition="1104" endWordPosition="1105">ng, 2010; He et al., 2011a; Glorot et al., 2011). Notice that the gap between source domain and target domain is the main difference between CDSC and CLSC. CLSC copes with two different datasets in two different languages. This difference makes CLSC a new challenge, drawing specific attention to researcher recently. 2.2 Cross-lingual Sentiment Analysis There are two alternative solutions to cross-lingual sentiment analysis. One is ensemble learning that combines multiple classifiers. The other is transfer learning that develops strategies to adapt the knowledge from one language to the other. Wan (2008) is among the pioneers to develop the ensemble learning solutions, where multiple classifiers learned from different training datasets including those in original languages and translated languages are combined by voting. Most researches, on the other hand, explore transfer learning and focus on knowledge adaptation. For example, Wan (2009) applies a supervised cotraining framework to iteratively adapt knowledge learned from the two languages by transferring translated texts to each other. Other similar work includes (Wei and Pal, 2010) and (He, 2011b). All these approaches rely on MT to build</context>
</contexts>
<marker>Wan, 2008</marker>
<rawString>Xiaojun Wan. 2008. Using Bilingual Knowledge and Ensemble Technics for Unsupervised Chinese Sentiment Analysis. In Proceedings of the 2008 Conference on Empirical Methods in Natual Language Processing, pages 553-561, Honolulu, October 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
</authors>
<title>Co-Training for Cross-Lingual Sentiment Classification.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP,</booktitle>
<pages>235--243</pages>
<location>Suntec,</location>
<contexts>
<context position="2642" citStr="Wan, 2009" startWordPosition="387" endWordPosition="388"> and target languages to overcome the language barrier, and then develop an appropriate knowledge transfer approach to leverage the annotated data from the source language to train a sentiment classification model in the target language, either supervised or semi-supervised. In particular, these approaches exploit and convert the knowledge learned from the source language to automatically generate and expand the pseudo-training data for the target language. The machine translation (MT) service is one of the most common ways used to build the language connection (Wan, 2008; Banea et al., 2008; Wan, 2009; Wei and Pal, 2010; Gui et al., 2014). Although it is claimed in Duh et al. (2011) that the MT service is ripe for CLSA, the imperfect MT quality hinders existing MTbased CLSA approaches from the further advance. In our preliminary study, we find that even the Google translator1 (i.e., one of the most widely used online MT service (Shankland 2013)) may unavoidably changes the sentiment polarity of the translated text, as illustrated below, with a percentage of around 10%. [Original English Text]: I am at home on bed rest and desperate for something good to read. [Sentiment Label: Negative] [T</context>
<context position="7777" citStr="Wan (2009)" startWordPosition="1153" endWordPosition="1154">ent Analysis There are two alternative solutions to cross-lingual sentiment analysis. One is ensemble learning that combines multiple classifiers. The other is transfer learning that develops strategies to adapt the knowledge from one language to the other. Wan (2008) is among the pioneers to develop the ensemble learning solutions, where multiple classifiers learned from different training datasets including those in original languages and translated languages are combined by voting. Most researches, on the other hand, explore transfer learning and focus on knowledge adaptation. For example, Wan (2009) applies a supervised cotraining framework to iteratively adapt knowledge learned from the two languages by transferring translated texts to each other. Other similar work includes (Wei and Pal, 2010) and (He, 2011b). All these approaches rely on MT to build language connection. Meanwhile, the unlabeled parallel data is also employed to fill the gap between two languages. To solve the feature coverage problem with the EM algorithm, Meng et al. (2012) leverage the unlabeled parallel data to learn unseen sentiment words. Similarly, Popat et al. (2013) use the unlabeled parallel data to cluster f</context>
<context position="25815" citStr="Wan, 2009" startWordPosition="4205" endWordPosition="4206">d English reviews to add into the Chinese training data until a predefined iteration number reaches. It can be also considered as a self-adaptive boosting approach. Iteratively boost transfer learning (BTL-2): This is an enhanced transfer learning method sharing the same learning framework with CredBoost but it ignores knowledge validation. It iteratively transfers the knowledge from English to Chinese. The learning in both languages iteratively boosts themselves separately. The transfer size is 16, comparable to that in CredBoost. Basic co-training (CoTr): The co-training method proposed in (Wan, 2009) is implemented. It is bidirectional transfer learning. In each English Chinese U Domain L L 2,000 - 4,000 Train Books - Test 4,000 - - 2,000 DVD - 4,000 Train - - Test 4,000 - - - 4,000 Train Music - 2,000 - - Test 4,000 U - - 424 iteration, 10 positive and 10 negative reviews are transferred from one language to the other. Doc2vec feature CredBoost (dCredB): This method is similar to CredBoost except that document-to-vector is used to generate features when training basic classifiers. The vectors are obtained from both original and translated reviews. The dimension of doc2vec is 300, while t</context>
</contexts>
<marker>Wan, 2009</marker>
<rawString>Xiaojun Wan. 2009. Co-Training for Cross-Lingual Sentiment Classification. In Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 235-243, Suntec, Singapore, 2-7 August 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bin Wei</author>
<author>Christopher Pal</author>
</authors>
<title>Cross Lingual Adaptation: An Experiment on Sentiment Classifications.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48 Annual Meeting of the Association for Computational Linguistics (short paper),</booktitle>
<pages>258--262</pages>
<location>Uppsala,</location>
<contexts>
<context position="2661" citStr="Wei and Pal, 2010" startWordPosition="389" endWordPosition="392"> languages to overcome the language barrier, and then develop an appropriate knowledge transfer approach to leverage the annotated data from the source language to train a sentiment classification model in the target language, either supervised or semi-supervised. In particular, these approaches exploit and convert the knowledge learned from the source language to automatically generate and expand the pseudo-training data for the target language. The machine translation (MT) service is one of the most common ways used to build the language connection (Wan, 2008; Banea et al., 2008; Wan, 2009; Wei and Pal, 2010; Gui et al., 2014). Although it is claimed in Duh et al. (2011) that the MT service is ripe for CLSA, the imperfect MT quality hinders existing MTbased CLSA approaches from the further advance. In our preliminary study, we find that even the Google translator1 (i.e., one of the most widely used online MT service (Shankland 2013)) may unavoidably changes the sentiment polarity of the translated text, as illustrated below, with a percentage of around 10%. [Original English Text]: I am at home on bed rest and desperate for something good to read. [Sentiment Label: Negative] [Translated Chinese T</context>
<context position="7977" citStr="Wei and Pal, 2010" startWordPosition="1181" endWordPosition="1184">strategies to adapt the knowledge from one language to the other. Wan (2008) is among the pioneers to develop the ensemble learning solutions, where multiple classifiers learned from different training datasets including those in original languages and translated languages are combined by voting. Most researches, on the other hand, explore transfer learning and focus on knowledge adaptation. For example, Wan (2009) applies a supervised cotraining framework to iteratively adapt knowledge learned from the two languages by transferring translated texts to each other. Other similar work includes (Wei and Pal, 2010) and (He, 2011b). All these approaches rely on MT to build language connection. Meanwhile, the unlabeled parallel data is also employed to fill the gap between two languages. To solve the feature coverage problem with the EM algorithm, Meng et al. (2012) leverage the unlabeled parallel data to learn unseen sentiment words. Similarly, Popat et al. (2013) use the unlabeled parallel data to cluster features in order to reduce the data sparsity problem. Meng et al. (2012) and Popat et al. (2013) also use the unlabeled parallel data to reduce the negative influence of the noisy and incorrect sentim</context>
</contexts>
<marker>Wei, Pal, 2010</marker>
<rawString>Bin Wei and Christopher Pal. 2010. Cross Lingual Adaptation: An Experiment on Sentiment Classifications. In Proceedings of the 48 Annual Meeting of the Association for Computational Linguistics (short paper), pages 258-262, Uppsala, Sweden, 11-16 July 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruifeng Xu</author>
<author>Jun Xu</author>
<author>Xiaolong Wang</author>
</authors>
<title>Instance Level Transfer Learning for Cross Lingual Opinion Analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011,</booktitle>
<pages>182--188</pages>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="8985" citStr="Xu et al., 2011" startWordPosition="1341" endWordPosition="1344">luster features in order to reduce the data sparsity problem. Meng et al. (2012) and Popat et al. (2013) also use the unlabeled parallel data to reduce the negative influence of the noisy and incorrect sentiment labels introduced by machine translation and knowledge transfer. However, the parallel data is also a scarce resource. 420 Some existing transfer learning based CLSA methods have attempted to address the noisy knowledge problem caused by wrong labels by checking label consistency. For example, to filter out the unconfident labels in Chinese, the supervised learning method proposed by (Xu et al., 2011) runs boosting in Chinese by checking consistency between the labels manually annotated in English and predicted by Chinese classifiers on translated Chinese. The work in (Gui et al., 2014) follows the same line although it considers knowledge transferring between two languages. On the contrary, the main focus of our work is to filter out the noisy knowledge having sentiment changes by wrong translations. Actually, both label consistency checking and linguistic distribution checking are important. Any one alone cannot work well. In fact, both of them are considered as the knowledge validation </context>
</contexts>
<marker>Xu, Xu, Wang, 2011</marker>
<rawString>Ruifeng Xu, Jun Xu and Xiaolong Wang. 2011. Instance Level Transfer Learning for Cross Lingual Opinion Analysis. In Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 182-188, 24 June, 2011, Portland, Oregon, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>