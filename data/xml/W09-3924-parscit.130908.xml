<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.110085">
<title confidence="0.998013">
Leveraging POMDPs trained with User Simulations and
Rule-based Dialogue Management in a Spoken Dialogue System
</title>
<author confidence="0.999411">
Sebastian Varges, Silvia Quarteroni, Giuseppe Riccardi, Alexei V. Ivanov, Pierluigi Roberti
</author>
<affiliation confidence="0.99852">
Department of Information Engineering and Computer Science
University of Trento
</affiliation>
<address confidence="0.712884">
38050 Povo di Trento, Italy
</address>
<email confidence="0.995358">
{varges|silviaq|riccardi|ivanov|roberti}@disi.unitn.it
</email>
<sectionHeader confidence="0.993796" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999951647058824">
We have developed a complete spoken di-
alogue framework that includes rule-based
and trainable dialogue managers, speech
recognition, spoken language understand-
ing and generation modules, and a com-
prehensive web visualization interface.
We present a spoken dialogue system
based on Reinforcement Learning that
goes beyond standard rule based models
and computes on-line decisions of the best
dialogue moves. Bridging the gap between
handcrafted (e.g. rule-based) and adap-
tive (e.g. based on Partially Observable
Markov Decision Processes - POMDP) di-
alogue models, this prototype is able to
learn high rewarding policies in a number
of dialogue situations.
</bodyText>
<sectionHeader confidence="0.929355" genericHeader="keywords">
1 Reinforcement Learning in Dialogue
</sectionHeader>
<bodyText confidence="0.999142107142857">
Machine Learning techniques, and particularly
Reinforcement Learning (RL), have recently re-
ceived great interest in research on dialogue man-
agement (DM) (Levin et al., 2000; Williams and
Young, 2006). A major motivation for this choice
is to improve robustness in the face of uncertainty
due for example to speech recognition errors. A
second important motivation is to improve adap-
tivity w.r.t. different user behaviour and applica-
tion/recognition environments.
The RL approach is attractive because it offers a
statistical model representing the dynamics of the
interaction between system and user. This con-
trasts with the supervised learning approach where
system behaviour is learnt based on a fixed cor-
pus. However, exploration of the range of dialogue
management strategies requires a simulation en-
vironment that includes a simulated user (Schatz-
mann et al., 2006) in order to avoid the prohibitive
cost of using human subjects.
We demonstrate various parameters that influ-
ence the learnt dialogue management policy by
using pre-trained policies (section 5). The appli-
cation domain is a tourist information system for
accommodation and events in the local area. The
domain of the trained DMs is identical to that of a
rule-based DM that was used by human users (sec-
tion 4), allowing us to compare the two directly.
</bodyText>
<sectionHeader confidence="0.961554" genericHeader="introduction">
2 POMDP demonstration system
</sectionHeader>
<bodyText confidence="0.999956954545455">
The POMDP DM implemented in this work is
shown in figure 1: at each turn at time t, the incom-
ing N user act hypotheses a,,,,,, split the state space
5t to represent the complete set of interpretations
from the start state (N=2). A belief update is per-
formed resulting in a probability assigned to each
state. The resulting ranked state space is used as
a basis for action selection. In our current imple-
mentation, belief update is based on probabilistic
user responses that include SLU confidences. Ac-
tion selection to determine system action a„L,, is
based on the best state (m is a counter for actions
in action set A). In each turn, the system uses an
c-greedy action selection strategy to decide prob-
abilistically if to exploit the policy or explore any
other action at random. (An alternative would be
softmax, for example.) At the end of each dia-
logue/session a reward is assigned and policy en-
tries are added or updated for each state-action
pair involved. These pairs are stored in tabular
form. We perform Monte Carlo updating similar
to (Levin et al., 2000):
</bodyText>
<equation confidence="0.53732">
Qt(s, a) = R(s, a)/n + Qt_1 · (n − 1)/n (1)
</equation>
<bodyText confidence="0.999807857142857">
where n is the number of sessions, R the reward
and Q the estimate of the state-action value.
At the beginning of each dialogue, a user goal
UG (a set of concept-value pairs) is generated ran-
domly and passed to a user simulator. The user
simulator takes UG and the current dialogue con-
text to produce plausible SLU hypotheses. These
</bodyText>
<subsubsectionHeader confidence="0.791313">
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 156–159,
</subsubsectionHeader>
<affiliation confidence="0.946793">
Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics
</affiliation>
<page confidence="0.991296">
156
</page>
<figure confidence="0.999818788461538">
policy
lookup
policy
update
user goal
state space St2
U6
s4,n
s5,n
s6,n
s1,a1
s1,a2
s2,a1
s3,a4
Q(s1,a1)
Q(s1,a2)
Q(s2,a1)
Q(s3,a4)
reward
computation
Turn t1
Turn t2 Turn tn
start
state
s1,1
a1,s
a2,s
a3,s
a4,s
a1,s
s1,n
an,s
an,s
an,s
Ut1
a1,u
a2,u
s2,2
s1,2
a2,s
a3,s
a4,s
Ut2
s2,n
s3,n
an,s
state space Stn
St1
state space
POLICY:
final
state
</figure>
<figureCaption confidence="0.999989">
Figure 1: POMDP Dialogue Manager
</figureCaption>
<bodyText confidence="0.999739625">
are a subset of the concept-value pairs in UG along
with a confidence estimate bootstrapped from a
small corpus of 74 in-domain dialogs. We assume
that the user ‘runs out of patience’ after 15 turns
and ends the call.
The system visualizes POMDP-related infor-
mation live for the ongoing dialogue (figure 2).
The visualization tool shows the internal represen-
tation of the dialogue manager including the the
N best dialogue states after each user utterance
and the reranking of the action set. At the end
of each dialogue session, the reward and the pol-
icy updates are shown, i.e. new or updated state
entries and action values. Moreover, the system
generates a plot that relates the current dialogue’s
reward to the reward of previous dialogues.
</bodyText>
<sectionHeader confidence="0.993918" genericHeader="method">
3 User Simulation
</sectionHeader>
<bodyText confidence="0.999865486486486">
To conduct thousands of simulated dialogues, the
DM needs to deal with heterogeneous but plau-
sible user input. We designed a User Simulator
(US) which bootstraps likely user behaviors start-
ing from a small corpus of 74 in-domain dialogs,
acquired using a rule-based version of the system
(section 4). The role of the US is to simulate
the output of the SLU module to the DM during
the whole interaction, fully replacing the ASR and
SLU modules. This differs from other user sim-
ulation approaches where n-gram models of user
dialog acts are represented.
For each simulated dialogue, one or more user
goals are randomly selected from a list of possible
user goals stored in a database table. A goal is rep-
resented as the set of concept-value pairs defining
a task. Simulation of the user’s behaviour happens
in two stages. First, a user model, i.e. a model
of the user’s intentions at the current stage of the
dialogue, is created. This is done by mining the
previous system move to obtain the concepts re-
quired by the DM and their corresponding values
(if any) from the current user goal. Then, the out-
put of the user model is passed to an error model
that simulates the “noisy channel” recognition er-
rors based on statistics from the dialogue corpus.
Errors produce perturbations of concept values as
well as phenomena such as noInput, noMatch and
hangUp. If the latter phenomena occur, they are
directly propagated to the DM; otherwise, plau-
sible confidences (based on the dialogue corpus)
are attached to concept-value pairs. The probabil-
ity of a given concept-value observation at time
t + 1 given the system move at time t, as,t, and
the session user goal g., called P(ot+1jas,t, g.),
is obtained by combining the outputs of the error
model and the user model:
</bodyText>
<equation confidence="0.989845">
P(ot+1ja.,t+1) · P(a.,t+1jas,t, g.)
</equation>
<bodyText confidence="0.9999175">
where a.,t+1 is the true user action. Finally,
concept-value pairs are combined in an SLU hy-
pothesis and, as in the regular SLU module, a cu-
mulative utterance-level confidence is computed,
determining the rank of each of the N hypotheses
output to the DM.
</bodyText>
<sectionHeader confidence="0.993471" genericHeader="method">
4 Rule-based Dialogue Management
</sectionHeader>
<bodyText confidence="0.999834789473684">
A rule-based DM was developed as a meaning-
ful comparison to the trained DM, to obtain train-
ing data from human-system interaction for the
US, and to understand the properties of the do-
main. Rule-based dialog management works in
two stages: retrieving and preprocessing facts (tu-
ples) taken from a dialogue state database, and
inferencing over those facts to generate a system
response. We distinguish between the ‘context
model’ of the first phase – essentially allowing
more recent values for a concept to override less
recent ones – and the ‘dialog move engine’ of the
second phase. In the second stage, acceptor rules
match SLU results to dialogue context, for ex-
ample perceived user concepts to open questions.
This may result in the decision to verify the ap-
plication parameter in question, and the action is
verbalized by language generation rules. If the
parameter is accepted, application dependent task
</bodyText>
<page confidence="0.99368">
157
</page>
<figureCaption confidence="0.962092666666667">
Figure 2: A screenshot of the online visualization tool. Left: user goal (top), evolving ranked state space
(bottom). Center: per state action distribution at turn ti. Right: consequent reward computation (top) and
policy updates (bottom). See video at http://www.youtube.com/watch?v=69QR0tKKhCw.
</figureCaption>
<page confidence="0.974023">
158
</page>
<figureCaption confidence="0.944544">
Figure 3: Left Pane: overview of a selection of dialogues in our visualization tool. Right Pane: visual-
</figureCaption>
<bodyText confidence="0.890831333333333">
ization of a system opening prompt followed by the user’s activity request. All distinct SLU hypotheses
(concept-value combinations) deriving from ASR are ranked based on concept-level confidence (2 in this
turn).
rules determine the next parameter to be acquired,
resulting in the generation of an appropriate re-
quest. See (Varges et al., 2008) for more details.
</bodyText>
<sectionHeader confidence="0.991772" genericHeader="method">
5 Visualization Tool
</sectionHeader>
<bodyText confidence="0.977283625">
In addition to the POMDP-related visualization
tool (figure 2), we developed another web-based
dialogue tool for both rule-based and POMDP sys-
tem that displays ongoing and past dialogue ut-
terances, semantic interpretation confidences and
distributions of confidences for incoming user acts
(see dialogue logs in figure 3).
Users are able to talk with several systems
(via SIP phone connection to the dialogue system
server) and see their dialogues in the visualization
tool. They are able to compare the rule-based
system, a randomly exploring learner that has not
been trained yet, and several systems that use vari-
ous pre-trained policies. The web tool is available
at http://cicerone.dit.unitn.it/
DialogStatistics/.
</bodyText>
<sectionHeader confidence="0.998211" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998972">
This work was partially supported by the Euro-
pean Commission Marie Curie Excellence Grant
for the ADAMACH project (contract No. 022593)
and by LUNA STREP project (contract No.
33549).
</bodyText>
<sectionHeader confidence="0.999281" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999855294117647">
E. Levin, R. Pieraccini, and W. Eckert. 2000. A
stochastic model of human-machine interaction for
learning dialog strategies. IEEE Transactions on
Speech and Audio Processing, 8(1).
J. Schatzmann, K. Weilhammer, M. Stuttle, and
S. Young. 2006. A Survey of Statistical User Sim-
ulation Techniques for Reinforcement-Learning of
Dialogue Management Strategies. Knowledge En-
gineering Review, 21(2):97–126.
Sebastian Varges, Giuseppe Riccardi, and Silvia Quar-
teroni. 2008. Persistent information state in a data-
centric architecture. In Proc. 9th SIGdial Workhop
on Discourse and Dialogue, Columbus, Ohio.
J. D. Williams and S. Young. 2006. Partially Ob-
servable Markov Decision Processes for Spoken Di-
alog Systems. Computer Speech and Language,
21(2):393–422.
</reference>
<page confidence="0.998842">
159
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.025708">
<title confidence="0.9990165">Leveraging POMDPs trained with User Simulations Rule-based Dialogue Management in a Spoken Dialogue System</title>
<author confidence="0.999629">Sebastian Varges</author>
<author confidence="0.999629">Silvia Quarteroni</author>
<author confidence="0.999629">Giuseppe Riccardi</author>
<author confidence="0.999629">Alexei V Ivanov</author>
<author confidence="0.999629">Pierluigi</author>
<affiliation confidence="0.999318">Department of Information Engineering and Computer University of</affiliation>
<address confidence="0.996982">38050 Povo di Trento, Italy</address>
<abstract confidence="0.977294776744186">We have developed a complete spoken dialogue framework that includes rule-based and trainable dialogue managers, speech recognition, spoken language understanding and generation modules, and a comprehensive web visualization interface. We present a spoken dialogue system based on Reinforcement Learning that goes beyond standard rule based models and computes on-line decisions of the best dialogue moves. Bridging the gap between handcrafted (e.g. rule-based) and adaptive (e.g. based on Partially Observable Markov Decision Processes - POMDP) dialogue models, this prototype is able to learn high rewarding policies in a number of dialogue situations. 1 Reinforcement Learning in Dialogue Machine Learning techniques, and particularly Reinforcement Learning (RL), have recently received great interest in research on dialogue management (DM) (Levin et al., 2000; Williams and Young, 2006). A major motivation for this choice is to improve robustness in the face of uncertainty due for example to speech recognition errors. A second important motivation is to improve adaptivity w.r.t. different user behaviour and application/recognition environments. The RL approach is attractive because it offers a model representing the the interaction between system and user. This contrasts with the supervised learning approach where system behaviour is learnt based on a fixed corpus. However, exploration of the range of dialogue management strategies requires a simulation environment that includes a simulated user (Schatzmann et al., 2006) in order to avoid the prohibitive cost of using human subjects. We demonstrate various parameters that influence the learnt dialogue management policy by using pre-trained policies (section 5). The application domain is a tourist information system for accommodation and events in the local area. The domain of the trained DMs is identical to that of a rule-based DM that was used by human users (section 4), allowing us to compare the two directly. 2 POMDP demonstration system The POMDP DM implemented in this work is in figure 1: at each turn at time the incomact hypotheses the state space represent the complete set of interpretations the start state A belief update is performed resulting in a probability assigned to each state. The resulting ranked state space is used as a basis for action selection. In our current implementation, belief update is based on probabilistic user responses that include SLU confidences. Acselection to determine system action on the best state a counter for actions action set In each turn, the system uses an action selection strategy to decide probabilistically if to exploit the policy or explore any other action at random. (An alternative would be softmax, for example.) At the end of each dialogue/session a reward is assigned and policy entries are added or updated for each state-action pair involved. These pairs are stored in tabular form. We perform Monte Carlo updating similar to (Levin et al., 2000): = the number of sessions, reward estimate of the state-action value. At the beginning of each dialogue, a user goal set of concept-value pairs) is generated randomly and passed to a user simulator. The user takes the current dialogue context to produce plausible SLU hypotheses. These of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and pages 156–159, Mary University of London, September 2009. Association for Computational Linguistics 156 policy lookup policy update user goal space reward computation Turn start state space state space final state Figure 1: POMDP Dialogue Manager a subset of the concept-value pairs in with a confidence estimate bootstrapped from a small corpus of 74 in-domain dialogs. We assume that the user ‘runs out of patience’ after 15 turns and ends the call. The system visualizes POMDP-related information live for the ongoing dialogue (figure 2). The visualization tool shows the internal representation of the dialogue manager including the the dialogue states after each user utterance and the reranking of the action set. At the end of each dialogue session, the reward and the policy updates are shown, i.e. new or updated state entries and action values. Moreover, the system generates a plot that relates the current dialogue’s reward to the reward of previous dialogues. 3 User Simulation To conduct thousands of simulated dialogues, the DM needs to deal with heterogeneous but plausible user input. We designed a User Simulator (US) which bootstraps likely user behaviors starting from a small corpus of 74 in-domain dialogs, acquired using a rule-based version of the system (section 4). The role of the US is to simulate the output of the SLU module to the DM during the whole interaction, fully replacing the ASR and SLU modules. This differs from other user simapproaches where models of user dialog acts are represented. For each simulated dialogue, one or more user goals are randomly selected from a list of possible user goals stored in a database table. A goal is represented as the set of concept-value pairs defining a task. Simulation of the user’s behaviour happens two stages. First, a i.e. a model of the user’s intentions at the current stage of the dialogue, is created. This is done by mining the previous system move to obtain the concepts required by the DM and their corresponding values (if any) from the current user goal. Then, the outof the user model is passed to an model that simulates the “noisy channel” recognition errors based on statistics from the dialogue corpus. Errors produce perturbations of concept values as as phenomena such as If the latter phenomena occur, they are directly propagated to the DM; otherwise, plausible confidences (based on the dialogue corpus) are attached to concept-value pairs. The probability of a given concept-value observation at time 1 the system move at time and session user goal called is obtained by combining the outputs of the error model and the user model: the true user action. Finally, concept-value pairs are combined in an SLU hypothesis and, as in the regular SLU module, a cumulative utterance-level confidence is computed, the rank of each of the output to the DM. 4 Rule-based Dialogue Management A rule-based DM was developed as a meaningful comparison to the trained DM, to obtain training data from human-system interaction for the US, and to understand the properties of the domain. Rule-based dialog management works in two stages: retrieving and preprocessing facts (tuples) taken from a dialogue state database, and inferencing over those facts to generate a system response. We distinguish between the ‘context model’ of the first phase – essentially allowing more recent values for a concept to override less recent ones – and the ‘dialog move engine’ of the second phase. In the second stage, acceptor rules match SLU results to dialogue context, for example perceived user concepts to open questions. This may result in the decision to verify the application parameter in question, and the action is verbalized by language generation rules. If the parameter is accepted, application dependent task 157 Figure 2: A screenshot of the online visualization tool. Left: user goal (top), evolving ranked state space Center: per state action distribution at turn Right: consequent reward computation (top) and updates (bottom). See video at 158 Figure 3: Left Pane: overview of a selection of dialogues in our visualization tool. Right Pane: visualof a system opening prompt followed by the user’s activity request. All hypotheses (concept-value combinations) deriving from ASR are ranked based on concept-level confidence (2 in this turn). rules determine the next parameter to be acquired, resulting in the generation of an appropriate request. See (Varges et al., 2008) for more details. 5 Visualization Tool In addition to the POMDP-related visualization tool (figure 2), we developed another web-based dialogue tool for both rule-based and POMDP system that displays ongoing and past dialogue utterances, semantic interpretation confidences and distributions of confidences for incoming user acts (see dialogue logs in figure 3). Users are able to talk with several systems (via SIP phone connection to the dialogue system server) and see their dialogues in the visualization tool. They are able to compare the rule-based system, a randomly exploring learner that has not been trained yet, and several systems that use various pre-trained policies. The web tool is available Acknowledgments This work was partially supported by the European Commission Marie Curie Excellence Grant for the ADAMACH project (contract No. 022593) and by LUNA STREP project (contract No. 33549). References E. Levin, R. Pieraccini, and W. Eckert. 2000. A stochastic model of human-machine interaction for dialog strategies. Transactions on and Audio 8(1).</abstract>
<note confidence="0.799130928571429">J. Schatzmann, K. Weilhammer, M. Stuttle, and S. Young. 2006. A Survey of Statistical User Simulation Techniques for Reinforcement-Learning of Management Strategies. En- 21(2):97–126. Sebastian Varges, Giuseppe Riccardi, and Silvia Quarteroni. 2008. Persistent information state in a dataarchitecture. In 9th SIGdial Workhop Discourse and Columbus, Ohio. J. D. Williams and S. Young. 2006. Partially Observable Markov Decision Processes for Spoken Di- Systems. Speech and 21(2):393–422. 159</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Levin</author>
<author>R Pieraccini</author>
<author>W Eckert</author>
</authors>
<title>A stochastic model of human-machine interaction for learning dialog strategies.</title>
<date>2000</date>
<journal>IEEE Transactions on Speech and Audio Processing,</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="1239" citStr="Levin et al., 2000" startWordPosition="165" endWordPosition="168"> a spoken dialogue system based on Reinforcement Learning that goes beyond standard rule based models and computes on-line decisions of the best dialogue moves. Bridging the gap between handcrafted (e.g. rule-based) and adaptive (e.g. based on Partially Observable Markov Decision Processes - POMDP) dialogue models, this prototype is able to learn high rewarding policies in a number of dialogue situations. 1 Reinforcement Learning in Dialogue Machine Learning techniques, and particularly Reinforcement Learning (RL), have recently received great interest in research on dialogue management (DM) (Levin et al., 2000; Williams and Young, 2006). A major motivation for this choice is to improve robustness in the face of uncertainty due for example to speech recognition errors. A second important motivation is to improve adaptivity w.r.t. different user behaviour and application/recognition environments. The RL approach is attractive because it offers a statistical model representing the dynamics of the interaction between system and user. This contrasts with the supervised learning approach where system behaviour is learnt based on a fixed corpus. However, exploration of the range of dialogue management str</context>
<context position="3484" citStr="Levin et al., 2000" startWordPosition="539" endWordPosition="542">istic user responses that include SLU confidences. Action selection to determine system action a„L,, is based on the best state (m is a counter for actions in action set A). In each turn, the system uses an c-greedy action selection strategy to decide probabilistically if to exploit the policy or explore any other action at random. (An alternative would be softmax, for example.) At the end of each dialogue/session a reward is assigned and policy entries are added or updated for each state-action pair involved. These pairs are stored in tabular form. We perform Monte Carlo updating similar to (Levin et al., 2000): Qt(s, a) = R(s, a)/n + Qt_1 · (n − 1)/n (1) where n is the number of sessions, R the reward and Q the estimate of the state-action value. At the beginning of each dialogue, a user goal UG (a set of concept-value pairs) is generated randomly and passed to a user simulator. The user simulator takes UG and the current dialogue context to produce plausible SLU hypotheses. These Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 156–159, Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistic</context>
</contexts>
<marker>Levin, Pieraccini, Eckert, 2000</marker>
<rawString>E. Levin, R. Pieraccini, and W. Eckert. 2000. A stochastic model of human-machine interaction for learning dialog strategies. IEEE Transactions on Speech and Audio Processing, 8(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Schatzmann</author>
<author>K Weilhammer</author>
<author>M Stuttle</author>
<author>S Young</author>
</authors>
<title>A Survey of Statistical User Simulation Techniques for Reinforcement-Learning of Dialogue Management Strategies.</title>
<date>2006</date>
<journal>Knowledge Engineering Review,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="1937" citStr="Schatzmann et al., 2006" startWordPosition="271" endWordPosition="275">ove robustness in the face of uncertainty due for example to speech recognition errors. A second important motivation is to improve adaptivity w.r.t. different user behaviour and application/recognition environments. The RL approach is attractive because it offers a statistical model representing the dynamics of the interaction between system and user. This contrasts with the supervised learning approach where system behaviour is learnt based on a fixed corpus. However, exploration of the range of dialogue management strategies requires a simulation environment that includes a simulated user (Schatzmann et al., 2006) in order to avoid the prohibitive cost of using human subjects. We demonstrate various parameters that influence the learnt dialogue management policy by using pre-trained policies (section 5). The application domain is a tourist information system for accommodation and events in the local area. The domain of the trained DMs is identical to that of a rule-based DM that was used by human users (section 4), allowing us to compare the two directly. 2 POMDP demonstration system The POMDP DM implemented in this work is shown in figure 1: at each turn at time t, the incoming N user act hypotheses a</context>
</contexts>
<marker>Schatzmann, Weilhammer, Stuttle, Young, 2006</marker>
<rawString>J. Schatzmann, K. Weilhammer, M. Stuttle, and S. Young. 2006. A Survey of Statistical User Simulation Techniques for Reinforcement-Learning of Dialogue Management Strategies. Knowledge Engineering Review, 21(2):97–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Varges</author>
<author>Giuseppe Riccardi</author>
<author>Silvia Quarteroni</author>
</authors>
<title>Persistent information state in a datacentric architecture. In</title>
<date>2008</date>
<booktitle>Proc. 9th SIGdial Workhop on Discourse and Dialogue,</booktitle>
<location>Columbus, Ohio.</location>
<contexts>
<context position="8989" citStr="Varges et al., 2008" startWordPosition="1455" endWordPosition="1458"> state action distribution at turn ti. Right: consequent reward computation (top) and policy updates (bottom). See video at http://www.youtube.com/watch?v=69QR0tKKhCw. 158 Figure 3: Left Pane: overview of a selection of dialogues in our visualization tool. Right Pane: visualization of a system opening prompt followed by the user’s activity request. All distinct SLU hypotheses (concept-value combinations) deriving from ASR are ranked based on concept-level confidence (2 in this turn). rules determine the next parameter to be acquired, resulting in the generation of an appropriate request. See (Varges et al., 2008) for more details. 5 Visualization Tool In addition to the POMDP-related visualization tool (figure 2), we developed another web-based dialogue tool for both rule-based and POMDP system that displays ongoing and past dialogue utterances, semantic interpretation confidences and distributions of confidences for incoming user acts (see dialogue logs in figure 3). Users are able to talk with several systems (via SIP phone connection to the dialogue system server) and see their dialogues in the visualization tool. They are able to compare the rule-based system, a randomly exploring learner that has</context>
</contexts>
<marker>Varges, Riccardi, Quarteroni, 2008</marker>
<rawString>Sebastian Varges, Giuseppe Riccardi, and Silvia Quarteroni. 2008. Persistent information state in a datacentric architecture. In Proc. 9th SIGdial Workhop on Discourse and Dialogue, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D Williams</author>
<author>S Young</author>
</authors>
<title>Partially Observable Markov Decision Processes for Spoken Dialog Systems.</title>
<date>2006</date>
<journal>Computer Speech and Language,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="1266" citStr="Williams and Young, 2006" startWordPosition="169" endWordPosition="172">ystem based on Reinforcement Learning that goes beyond standard rule based models and computes on-line decisions of the best dialogue moves. Bridging the gap between handcrafted (e.g. rule-based) and adaptive (e.g. based on Partially Observable Markov Decision Processes - POMDP) dialogue models, this prototype is able to learn high rewarding policies in a number of dialogue situations. 1 Reinforcement Learning in Dialogue Machine Learning techniques, and particularly Reinforcement Learning (RL), have recently received great interest in research on dialogue management (DM) (Levin et al., 2000; Williams and Young, 2006). A major motivation for this choice is to improve robustness in the face of uncertainty due for example to speech recognition errors. A second important motivation is to improve adaptivity w.r.t. different user behaviour and application/recognition environments. The RL approach is attractive because it offers a statistical model representing the dynamics of the interaction between system and user. This contrasts with the supervised learning approach where system behaviour is learnt based on a fixed corpus. However, exploration of the range of dialogue management strategies requires a simulati</context>
</contexts>
<marker>Williams, Young, 2006</marker>
<rawString>J. D. Williams and S. Young. 2006. Partially Observable Markov Decision Processes for Spoken Dialog Systems. Computer Speech and Language, 21(2):393–422.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>