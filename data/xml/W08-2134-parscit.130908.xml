<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004749">
<title confidence="0.997665">
A Cascaded Syntactic and Semantic Dependency Parsing System
</title>
<author confidence="0.993405">
Wanxiang Che, Zhenghua Li, Yuxuan Hu, Yongqiang Li, Bing Qin, Ting Liu, Sheng Li
</author>
<affiliation confidence="0.985173333333333">
Information Retrieval Lab
School of Computer Science and Technology
Harbin Institute of Technology, China, 150001
</affiliation>
<email confidence="0.981656">
{car, lzh, yxhu, yqli, qinb, tliu, ls}@ir.hit.edu.cn
</email>
<sectionHeader confidence="0.997316" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99999665">
We describe our CoNLL 2008 Shared Task
system in this paper. The system includes
two cascaded components: a syntactic and
a semantic dependency parsers. A first-
order projective MSTParser is used as our
syntactic dependency parser. In order to
overcome the shortcoming of the MST-
Parser, that it cannot model more global in-
formation, we add a relabeling stage after
the parsing to distinguish some confusable
labels, such as ADV, TMP, and LOC. Be-
sides adding a predicate identification and
a classification stages, our semantic de-
pendency parsing simplifies the traditional
four stages semantic role labeling into two:
a maximum entropy based argument clas-
sification and an ILP-based post inference.
Finally, we gain the overall labeled macro
F1 = 82.66, which ranked the second posi-
tion in the closed challenge.
</bodyText>
<sectionHeader confidence="0.955085" genericHeader="method">
1 System Architecture
</sectionHeader>
<bodyText confidence="0.999789714285714">
Our CoNLL 2008 Shared Task (Surdeanu et al.,
2008) participating system includes two cascaded
components: a syntactic and a semantic depen-
dency parsers. They are described in Section 2
and 3 respectively. Their experimental results are
shown in Section 4. Section 5 gives our conclusion
and future work.
</bodyText>
<sectionHeader confidence="0.988843" genericHeader="method">
2 Syntactic Dependency Parsing
</sectionHeader>
<bodyText confidence="0.989348">
MSTParser (McDonald, 2006) is selected as our
basic syntactic dependency parser. It views the
</bodyText>
<footnote confidence="0.90331125">
© 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
</footnote>
<bodyText confidence="0.9975532">
syntactic dependency parsing as a problem of
finding maximum spanning trees (MST) in di-
rected graphs. MSTParser provides the state-of-
the-art performance for both projective and non-
projective tree banks.
</bodyText>
<sectionHeader confidence="0.540254" genericHeader="method">
2.1 Features
</sectionHeader>
<bodyText confidence="0.997385">
The score of each labeled arc is computed through
the Eq. (1) in MSTParser.
</bodyText>
<equation confidence="0.982315">
score(h, c, l) = w · f(h, c, l) (1)
</equation>
<bodyText confidence="0.9999926">
where node h represents the head node of the arc,
while node c is the dependent node (or child node).
l denotes the label of the arc.
There are three major differences between our
feature set and McDonald (2006)’s:
</bodyText>
<listItem confidence="0.951913363636364">
1) We use the lemma as a generalization feature
of a word, while McDonald (2006) use the word’s
prefix.
2) We add two new features: “bet-pos-h-same-
num” and “bet-pos-c-same-num”. They represent
the number of nodes which locate between node h
and node c and whose POS tags are the same with
h and c respectively.
3) We use more back-off features than McDon-
ald (2006) by completely enumerating all of the
possible combinatorial features.
</listItem>
<subsectionHeader confidence="0.997765">
2.2 Relabeling
</subsectionHeader>
<bodyText confidence="0.999939444444445">
By observing the current results of MSTParser on
the development data, we find that the performance
of some labels are far below average, such as ADV,
TMP, LOC. We think the main reason lies in that
MSTParser only uses local features restricted to a
single arc (as shown in Eq. (1)) and fails to use
more global information. Consider two sentences:
“I read books in the room.” and “I read books in
the afternoon.”. It is hard to correctly label the arc
</bodyText>
<page confidence="0.932395">
238
</page>
<reference confidence="0.214621">
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 238–242
Manchester, August 2008
</reference>
<table confidence="0.999260647058824">
Deprel Total Mislabeled as
NMOD 8,922 NAME [0.4], DEP [0.4], LOC [0.1],
AMOD [0.1]
OBJ 1,728 TMP [0.5], ADV [0.4], OPRD[0.3]
ADV 1,256 TMP [2.9], LOC [2.3], MNR [1.8],
DIR [1.5]
NAME 1,138 NMOD [2.2]
VC 953 PRD [0.9]
DEP 772 NMOD [4.0]
TMP 755 ADV [9.9], LOC [6.5]
LOC 556 ADV [12.6], NMOD [7.9], TMP [5.9]
AMOD 536 ADV [2.2]
PRD 509 VC [4.7]
APPO 444 NMOD [2.5]
OPRD 373 OBJ [4.6]
DIR 119 ADV [18.5]
MNR 109 ADV [28.4]
</table>
<tableCaption confidence="0.999905">
Table 1: Error Analysis of Each Label
</tableCaption>
<bodyText confidence="0.998668545454545">
between “read” and “in” unless we know the object
of “in”.
We count the errors of each label, and show the
top ones in Table 1. “Total” refers to the total num-
ber of the corresponding label in the development
data. The column of “Mislabeled as” lists the la-
bels that an arc may be mislabeled as. The number
in brackets shows the percentage of mislabeling.
As shown in the table, some labels are often con-
fusable with each other, such as ADV, LOC and
TMP.
</bodyText>
<subsectionHeader confidence="0.9959625">
2.3 Relabeling using Maximum Entropy
Classifier
</subsectionHeader>
<bodyText confidence="0.98852">
We constructed two confusable label set which
have a higher mutual mislabeling proportion:
(NMOD, LOC, ADV, TMP, MNR, DIR) and (OBJ,
OPRD). A maximum entropy classifier is used to
relabel them.
Features are shown in Table 2. The first column
lists local features, which contains information of
the head node h and the dependent node c of an arc.
“+ dir dist” means that conjoining existing features
with arc direction and distance composes new fea-
tures. The second column lists features using the
information of node c’s children. “word c c” rep-
resents form or lemma of one child of the node
c. “dir c” and “dist c” represents the direction and
distance of the arc which links node c to its child.
The back-off technique is also used on these fea-
tures.
Local features (+ dir dist) Global features (+ dir c dist c)
word h word c word h word c word c c
</bodyText>
<tableCaption confidence="0.982502">
Table 2: Relabeling Feature Set (+ dir dist)
</tableCaption>
<sectionHeader confidence="0.907667" genericHeader="method">
3 Semantic Dependency Parsing
</sectionHeader>
<subsectionHeader confidence="0.998685">
3.1 Architecture
</subsectionHeader>
<bodyText confidence="0.999824647058824">
The whole procedure is divided into four separate
stages: Predicate Identification, Predicate Classifi-
cation, Semantic Role Classification, and Post In-
ference.
During the Predicate Identification stage we ex-
amine each word in a sentence to discover target
predicates, including both noun predicates (from
NomBank) and verb predicates (from PropBank).
In the Predicate Classification stage, each predi-
cate is assigned a certain sense number. For each
predicate, the probabilities of a word in the sen-
tence to be each semantic role are predicted in the
Semantic Role Classification stage. Maximum en-
tropy model is selected as our classifiers in these
stages. Finally an ILP (Integer Linear Program-
ming) based method is adopted for post infer-
ence (Punyakanok et al., 2004).
</bodyText>
<subsectionHeader confidence="0.999102">
3.2 Predicate Identification
</subsectionHeader>
<bodyText confidence="0.9994125">
The predicate identification is treated as a binary
classification problem. Each word in a sentence is
predicted to be a predicate or not to be. A set of
features are extracted for each word, and an opti-
mized subset of them are adopted in our final sys-
tem. The following is a full list of the features:
DEPREL (a1): Type of relation to the parent.
WORD (a21), POS (a22), LEMMA (a23),
HEAD (a31), HEAD POS (a32), HEAD LEMMA
(a33): The forms, POS tags and lemmas of a word
and it’s headword (parent) .
FIRST WORD (a41), FIRST POS (a42),
FIRST LEMMA (a43), LAST WORD (a51),
LAST POS (a52), LAST LEMMA (a53): A
corresponding “constituent” for a word consists
of all descendants of it. The forms, POS tags and
lemmas of both the first and the last words in the
constituent are extracted.
POS PAT (a6): A “POS pattern” is produced for
the corresponding constituent as follows: a POS
bag is produced with the POS tags of the words
in the constituent except for the first and the last
ones, duplicated tags removed and the original or-
der ignored. Then we have the POS PAT feature
</bodyText>
<page confidence="0.991058">
239
</page>
<bodyText confidence="0.999914807692308">
by combining the POS tag of the first word, the
bag and the POS tag of the last word.
CHD POS (a71), CHD POS NDUP (a72),
CHD REL (a73), CHD REL NDUP (a74): The
POS tags of the child words are joined to-
gether to form feature CHD POS. With adja-
cently duplicated tags reduced to one, feature
CHD POS NDUP is produced. Similarly we can
get CHD REL and CHD REL NDUP too, with
the relation types substituted for the POS tags.
SIB REL (a81), SIB REL NDUP (a82),
SIB POS (a83), SIB POS NDUP (a84): Sibling
words (including the target word itself) and the
corresponding dependency relations (or POS tags)
are considered as well. Four features are formed
similarly to those of child words.
VERB VOICE (a9): Verbs are examined for
voices: if the headword lemma is either “be” or
“get”, or else the relation type is “APPO”, then the
verb is considered passive, otherwise active.
Also we used some “combined” features which
are combinations of single features. The final op-
timized feature set is (a1, a21, a22, a31, a32, a41,
a42, a51, a52, a6, a72, a73, a74, a81, a82, a83,
a1+a21, a21+a31, a21+a6, a21+a74, a73+a81,
a81+a83).
</bodyText>
<subsectionHeader confidence="0.995408">
3.3 Predicate Classification
</subsectionHeader>
<bodyText confidence="0.99939634375">
After predicate identification is done, the resulting
predicates are processed for sense classification. A
classifier is trained for each predicate that has mul-
tiple senses on the training data (There are totally
962 multi-sense predicates on the training corpus,
taking up 14% of all) In additional to those fea-
tures described in the predicate identification sec-
tion, some new ones relating to the predicate word
are introduced:
BAG OF WORD (b11), BAG OF WORD O
(b12): All words in a sentence joined, namely
“Bag of Words”. And an “ordered” version is in-
troduced where each word is prefixed with a letter
“L”, “R” or “T” indicating it’s to the left or right of
the predicate or is the predicate itself.
BAG OF POS O (b21), BAG OF POS N
(b22): The POS tags prefixed with “L”, “R” or
“T” indicating the word position joined together,
namely “Bag of POS (Ordered)”. With the
prefixed letter changed to a number indicating
the distance to the predicate (negative for being
left to the predicate and positive for right), an-
other feature is formed, namely “Bag of POS
(Numbered)”.
WIND5 BIGRAM (b3): 5 closest words from
both left and right plus the predicate itself, in total
11 words form a “window”, within which bigrams
are enumerated.
The final optimized feature set for the task of
predicate classification is (a1, a21, a23, a71, a72,
a73, a74, a81, a82, a83, a84, a9, b11, b12, b22, b3,
a71+a9).
</bodyText>
<subsectionHeader confidence="0.992127">
3.4 Semantic Role Classification
</subsectionHeader>
<bodyText confidence="0.99910025">
In our system, the identification and classifica-
tion of semantic roles are achieved in a single
stage (Liu et al., 2005) through one single classi-
fier (actually two, one for noun predicates, and the
other for verb predicates). Each word in a sentence
is given probabilities to be each semantic role (in-
cluding none of the these roles) for a predicate.
Features introduced in addition to those of the pre-
vious subsections are the following:
POS PATH (c11), REL PATH (c12): The “POS
Path” feature consists of POS tags of the words
along the path from a word to the predicate. Other
than “Up” and “Down”, the “Left” and “Right” di-
rection of the path is added. Similarly, the “Re-
lation Path” feature consists of the relation types
along the same path.
UP PATH (c21), UP REL PATH (c22): “Up-
stream paths” are parts of the above paths that stop
at the common ancestor of a word and the predi-
cate.
PATH LEN (c3): Length of the paths
POSITION (c4): The relative position of a word
to the predicate: Left or Right.
PRED FAMILYSHIP (c5): “Familyship rela-
tion” between a word and the predicate, being one
of “self”, “child”, “descendant”, “parent”, “ances-
tor”, “sibling”, and “not-relative”.
PRED SENSE (c6): The lemma plus sense
number of the predicate
As for the task of semantic role classification,
the features of the predicate word in addition to
those of the word under consideration can also
be used; we mark features of the predicate with
an extra ‘p’. For example, the head word of
the current word is represented as a31, and the
head word of the predicate is represented as pa31.
So, with no doubt for the representation, our fi-
nal optimized feature set for the task of seman-
tic role classification is (a1, a23, a33, a43, a53,
a6, c11, c12, c21, c3, c4, c6, pa23, pa71, pa73,
</bodyText>
<page confidence="0.984499">
240
</page>
<bodyText confidence="0.9610525">
pa83, a1+a23+a33, a21+c5, a23+c12, a33+c12,
a33+c22, a6+a33, a73+c5, c11+c12, pa71+pa73).
</bodyText>
<subsectionHeader confidence="0.942452">
3.5 ILP-based Post Inference
</subsectionHeader>
<bodyText confidence="0.963246681818182">
The final semantic role labeling result is gener-
ated through an ILP (Integer Linear Programming)
based post inference method. An ILP problem is
formulated with respect to the probability given by
the above stage. The final labeling is formed at the
same time when the problem is solved.
Let W be the set of words in the sentence, and
R be the set of semantic role labels. A virtual label
“NULL” is also added to R, representing “none of
the roles is assigned”.
For each word w E W and semantic role label
r E R we create a binary variable vwr E (0, 1),
whose value indicates whether or not the word w
is labeled as label r. pwr denotes the possibil-
ity of word w to be labeled as role r. Obviously,
when objective function f = Ew r log(pwr &apos; vwr)
is maximized, we can read the optimal labeling for
a predicate from the assignments to the variables
vwr. There are three constrains used in our system:
C1: Each relation should be and only be la-
beled with one label (including the virtual label
“NULL”), i.e.:
</bodyText>
<equation confidence="0.9947815">
E vwr = 1
r
</equation>
<bodyText confidence="0.990328928571429">
C2: Roles with a small probability should never
be labeled (except for the virtual role “NULL”).
The threshold we use in our system is 0.3, which
is optimized from the development data. i.e.:
vwr = 0, if pwr &lt; 0.3 and r =� “NULL”
C3: Statistics shows that the most roles (ex-
cept for the virtual role “NULL”) usually appear
only once for a predicate, except for some rare ex-
ception. So we impose a no-duplicated-roles con-
straint with an exception list, which is constructed
according to the times of semantic roles’ duplica-
tion for each single predicate (different senses of a
predicate are considered different) and the ratio of
duplication to non-duplication.
</bodyText>
<equation confidence="0.45501375">
Er vwr &lt;_ 1,
if &lt; p,r &gt; E�{&lt; p,r &gt; 1pEP,rER; (2)
cpr−dpr
dpr &gt; 0.3 ∧ dpr &gt; 101
</equation>
<bodyText confidence="0.9771885">
where P is the set of predicates; cpr denotes the
count of words in the training corpus, which are
</bodyText>
<table confidence="0.998682636363636">
Predicate Type Predicate Label
Noun president.01 A3
Verb match.01 A1
Verb tie.01 A1
Verb link.01 A1
Verb rate.01 A0
Verb rate.01 A2
Verb attach.01 A1
Verb connect.01 A1
Verb fit.01 A1
Noun trader.01 SU
</table>
<tableCaption confidence="0.960689">
Table 3: No-duplicated-roles constraint exception
list (obtained by Eq. (2))
</tableCaption>
<bodyText confidence="0.9997485">
labeled as r E R for predicate p E P; while dpr
denotes something similar to cpr, but what taken
into account are only those words labeled with r,
and there are more than one roles within the sen-
tence for the same predicate. Table 3 lists the com-
plete exception set, which has a size of only 10.
</bodyText>
<sectionHeader confidence="0.999879" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.99859465">
The original MSTParser1 is implemented in Java.
We were confronted with memory shortage when
trying to train a model with the entire CoNLL 2008
training data with 4GB memory. Therefore, we
rewrote it with C++ which can manage the mem-
ory more exactly. Since the time was limited, we
only rewrote the projective part without consider-
ing second-order parsing technique.
Our maximum entropy classifier is implemented
with Maximum Entropy Modeling Toolkit2. The
classifier parameters: gaussian prior and iterations,
are tuned with the development data for different
stages respectively.
lp solve 5.53 is chosen as our ILP problem
solver during the post inference stage.
The training time of the syntactic and the se-
mantic parsers are 22 and 5 hours respectively, on
all training data, with 2.0GHz Xeon CPU and 4G
memory. While the prediction can be done within
10 and 5 minutes on the development data.
</bodyText>
<subsectionHeader confidence="0.995945">
4.1 Syntactic Dependency Parsing
</subsectionHeader>
<bodyText confidence="0.9983035">
The experiments on development data show that
relabeling process is helpful, which improves the
</bodyText>
<footnote confidence="0.99975475">
1http://sourceforge.net/projects/mstparser
2http://homepages.inf.ed.ac.uk/s0450736/maxent
toolkit.html
3http://sourceforge.net/projects/lpsolve
</footnote>
<page confidence="0.987911">
241
</page>
<table confidence="0.999444">
Precision (%) Recall (%) F1
Pred Identification 91.61 91.36 91.48
Pred Classification 86.61 86.37 86.49
</table>
<tableCaption confidence="0.9776235">
Table 4: The performance of predicate identifica-
tion and classification
</tableCaption>
<table confidence="0.999556666666667">
Precision (%) Recall (%) F1
Simple 81.02 76.00 78.43
ILP-based 82.53 75.26 78.73
</table>
<tableCaption confidence="0.975313">
Table 5: Comparison between different post infer-
ence strategies
</tableCaption>
<bodyText confidence="0.996848">
LAS performance from 85.41% to 85.94%. The fi-
nal syntactic dependency parsing performances on
the WSJ and the Brown test data are 87.51% and
80.73% respectively.
</bodyText>
<subsectionHeader confidence="0.983517">
4.2 Semantic Dependency Parsing
</subsectionHeader>
<bodyText confidence="0.99995780952381">
The semantic dependency parsing component is
based on the last syntactic dependency parsing
component. All stages of the system are trained
with the closed training corpus, while predicted
against the output of the syntactic parsing.
Performance for predicate identification and
classification is given in Table 4, wherein the clas-
sification is done on top of the identification.
Semantic role classification and the post infer-
ence are done on top of the result of predicate iden-
tification and classification. The final performance
is presented in Table 5. A simple post inference
strategy is given for comparison, where the most
possible label (including the virtual label “NULL”)
is select except for those duplicated non-virtual la-
bels with lower probabilities (lower than 0.5). Our
ILP-based method produces a gain of 0.30 with re-
spect to the F1 score.
The final semantic dependency parsing perfor-
mance on the development and the test (WSJ and
Brown) data are shown in Table 6.
</bodyText>
<table confidence="0.9997165">
Precision (%) Recall (%) F1
Development 82.53 75.26 78.73
Test (WSJ) 82.67 77.50 80.00
Test (Brown) 64.38 68.50 66.37
</table>
<tableCaption confidence="0.883314">
Table 6: Semantic dependency parsing perfor-
mances
</tableCaption>
<subsectionHeader confidence="0.994076">
4.3 Overall Performance
</subsectionHeader>
<bodyText confidence="0.9999574">
The overall macro scores of our syntactic and se-
mantic dependency parsing system are 82.38%,
83.78% and 73.57% on the development and two
test (WSJ and Brown) data respectively, which is
ranked the second position in the closed challenge.
</bodyText>
<sectionHeader confidence="0.99824" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9999226">
We present our CoNLL 2008 Shared Task system
which is composed of two cascaded components:
a syntactic and a semantic dependency parsers,
which are built with some state-of-the-art methods.
Through a fine tuning features and parameters, the
final system achieves promising results. In order
to improve the performance further, we will study
how to make use of more resources and tools (open
challenge) and how to do joint learning between
syntactic and semantic parsing.
</bodyText>
<sectionHeader confidence="0.999506" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999774166666667">
The authors would like to thank the reviewers for
their helpful comments. This work was supported
by National Natural Science Foundation of China
(NSFC) via grant 60675034, 60575042, and the
“863” National High-Tech Research and Develop-
ment of China via grant 2006AA01Z145.
</bodyText>
<sectionHeader confidence="0.999546" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999302882352941">
Liu, Ting, Wanxiang Che, Sheng Li, Yuxuan Hu, and
Huaijun Liu. 2005. Semantic role labeling system
using maximum entropy classifier. In Proceedings
of CoNLL-2005, June.
McDonald, Ryan. 2006. Discriminative Learning and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
Punyakanok, Vasin, Dan Roth, Wen-tau Yih, and Dav
Zimak. 2004. Semantic role labeling via integer
linear programming inference. In Proceedings of
Coling-2004, pages 1346–1352.
Surdeanu, Mihai, Richard Johansson, Adam Meyers,
Lluis M`arquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syntac-
tic and semantic dependencies. In Proceedings of
the 12th Conference on Computational Natural Lan-
guage Learning (CoNLL-2008).
</reference>
<page confidence="0.99771">
242
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000012">
<title confidence="0.999941">A Cascaded Syntactic and Semantic Dependency Parsing System</title>
<author confidence="0.989169">Wanxiang Che</author>
<author confidence="0.989169">Zhenghua Li</author>
<author confidence="0.989169">Yuxuan Hu</author>
<author confidence="0.989169">Yongqiang Li</author>
<author confidence="0.989169">Bing Qin</author>
<author confidence="0.989169">Ting Liu</author>
<author confidence="0.989169">Sheng</author>
<affiliation confidence="0.987009">Information Retrieval School of Computer Science and</affiliation>
<address confidence="0.81799">Harbin Institute of Technology, China, 150001</address>
<email confidence="0.959839">lzh,yxhu,yqli,qinb,tliu,</email>
<abstract confidence="0.998773714285714">We describe our CoNLL 2008 Shared Task system in this paper. The system includes two cascaded components: a syntactic and a semantic dependency parsers. A firstorder projective MSTParser is used as our syntactic dependency parser. In order to overcome the shortcoming of the MST- Parser, that it cannot model more global information, we add a relabeling stage after the parsing to distinguish some confusable labels, such as ADV, TMP, and LOC. Besides adding a predicate identification and a classification stages, our semantic dependency parsing simplifies the traditional four stages semantic role labeling into two: a maximum entropy based argument classification and an ILP-based post inference. Finally, we gain the overall labeled macro F1 = 82.66, which ranked the second position in the closed challenge. 1 System Architecture Our CoNLL 2008 Shared Task (Surdeanu et al., 2008) participating system includes two cascaded components: a syntactic and a semantic dependency parsers. They are described in Section 2 and 3 respectively. Their experimental results are shown in Section 4. Section 5 gives our conclusion and future work. 2 Syntactic Dependency Parsing MSTParser (McDonald, 2006) is selected as our basic syntactic dependency parser. It views the Licensed under the Commons Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. syntactic dependency parsing as a problem of finding maximum spanning trees (MST) in directed graphs. MSTParser provides the state-ofthe-art performance for both projective and nonprojective tree banks. 2.1 Features The score of each labeled arc is computed through the Eq. (1) in MSTParser. c, = c, node the head node of the arc, node the dependent node (or child node). the label of the arc. There are three major differences between our feature set and McDonald (2006)’s: 1) We use the lemma as a generalization feature of a word, while McDonald (2006) use the word’s prefix. 2) We add two new features: “bet-pos-h-samenum” and “bet-pos-c-same-num”. They represent number of nodes which locate between node node whose POS tags are the same with 3) We use more back-off features than McDonald (2006) by completely enumerating all of the possible combinatorial features. 2.2 Relabeling By observing the current results of MSTParser on the development data, we find that the performance of some labels are far below average, such as ADV, TMP, LOC. We think the main reason lies in that MSTParser only uses local features restricted to a single arc (as shown in Eq. (1)) and fails to use more global information. Consider two sentences: “I read books in the room.” and “I read books in the afternoon.”. It is hard to correctly label the arc</abstract>
<address confidence="0.692265">238 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, Manchester, August 2008</address>
<abstract confidence="0.959467946502058">Deprel Total Mislabeled as NMOD 8,922 NAME [0.4], DEP [0.4], LOC [0.1], AMOD [0.1] OBJ 1,728 TMP [0.5], ADV [0.4], OPRD[0.3] ADV 1,256 TMP [2.9], LOC [2.3], MNR [1.8], DIR [1.5] NAME 1,138 NMOD [2.2] VC 953 PRD [0.9] DEP 772 NMOD [4.0] TMP 755 ADV [9.9], LOC [6.5] LOC 556 ADV [12.6], NMOD [7.9], TMP [5.9] AMOD 536 ADV [2.2] PRD 509 VC [4.7] APPO 444 NMOD [2.5] OPRD 373 OBJ [4.6] DIR 119 ADV [18.5] MNR 109 ADV [28.4] Table 1: Error Analysis of Each Label between “read” and “in” unless we know the object of “in”. We count the errors of each label, and show the top ones in Table 1. “Total” refers to the total number of the corresponding label in the development data. The column of “Mislabeled as” lists the labels that an arc may be mislabeled as. The number in brackets shows the percentage of mislabeling. As shown in the table, some labels are often confusable with each other, such as ADV, LOC and TMP. 2.3 Relabeling using Maximum Entropy Classifier We constructed two confusable label set which have a higher mutual mislabeling proportion: (NMOD, LOC, ADV, TMP, MNR, DIR) and (OBJ, OPRD). A maximum entropy classifier is used to relabel them. Features are shown in Table 2. The first column lists local features, which contains information of head node the dependent node an arc. “+ dir dist” means that conjoining existing features with arc direction and distance composes new features. The second column lists features using the of node children. “word c c” represents form or lemma of one child of the node “dir c” and “dist c” represents the direction and of the arc which links node its child. The back-off technique is also used on these features. Local features (+ dir dist) Global features (+ dir c dist c) word h word c word h word c word c c Table 2: Relabeling Feature Set (+ dir dist) 3 Semantic Dependency Parsing 3.1 Architecture The whole procedure is divided into four separate stages: Predicate Identification, Predicate Classification, Semantic Role Classification, and Post Inference. During the Predicate Identification stage we examine each word in a sentence to discover target predicates, including both noun predicates (from NomBank) and verb predicates (from PropBank). In the Predicate Classification stage, each predicate is assigned a certain sense number. For each predicate, the probabilities of a word in the sentence to be each semantic role are predicted in the Semantic Role Classification stage. Maximum entropy model is selected as our classifiers in these stages. Finally an ILP (Integer Linear Programming) based method is adopted for post inference (Punyakanok et al., 2004). 3.2 Predicate Identification The predicate identification is treated as a binary classification problem. Each word in a sentence is predicted to be a predicate or not to be. A set of features are extracted for each word, and an optimized subset of them are adopted in our final system. The following is a full list of the features: DEPREL (a1): Type of relation to the parent. WORD (a21), POS (a22), LEMMA (a23), HEAD (a31), HEAD POS (a32), HEAD LEMMA (a33): The forms, POS tags and lemmas of a word and it’s headword (parent) . FIRST WORD (a41), FIRST POS (a42), FIRST LEMMA (a43), LAST WORD (a51), LAST POS (a52), LAST LEMMA (a53): A corresponding “constituent” for a word consists of all descendants of it. The forms, POS tags and lemmas of both the first and the last words in the constituent are extracted. POS PAT (a6): A “POS pattern” is produced for the corresponding constituent as follows: a POS bag is produced with the POS tags of the words in the constituent except for the first and the last ones, duplicated tags removed and the original order ignored. Then we have the POS PAT feature 239 by combining the POS tag of the first word, the bag and the POS tag of the last word. CHD POS (a71), CHD POS NDUP (a72), CHD REL (a73), CHD REL NDUP (a74): The POS tags of the child words are joined together to form feature CHD POS. With adjacently duplicated tags reduced to one, feature CHD POS NDUP is produced. Similarly we can get CHD REL and CHD REL NDUP too, with the relation types substituted for the POS tags. SIB REL (a81), SIB REL NDUP (a82), SIB POS (a83), SIB POS NDUP (a84): Sibling words (including the target word itself) and the corresponding dependency relations (or POS tags) are considered as well. Four features are formed similarly to those of child words. VERB VOICE (a9): Verbs are examined for voices: if the headword lemma is either “be” or “get”, or else the relation type is “APPO”, then the verb is considered passive, otherwise active. Also we used some “combined” features which are combinations of single features. The final optimized feature set is (a1, a21, a22, a31, a32, a41, a42, a51, a52, a6, a72, a73, a74, a81, a82, a83, a1+a21, a21+a31, a21+a6, a21+a74, a73+a81, a81+a83). 3.3 Predicate Classification After predicate identification is done, the resulting predicates are processed for sense classification. A classifier is trained for each predicate that has multiple senses on the training data (There are totally 962 multi-sense predicates on the training corpus, taking up 14% of all) In additional to those features described in the predicate identification section, some new ones relating to the predicate word are introduced: BAG OF WORD (b11), BAG OF WORD O (b12): All words in a sentence joined, namely “Bag of Words”. And an “ordered” version is introduced where each word is prefixed with a letter “L”, “R” or “T” indicating it’s to the left or right of the predicate or is the predicate itself. BAG OF POS O (b21), BAG OF POS N (b22): The POS tags prefixed with “L”, “R” or “T” indicating the word position joined together, namely “Bag of POS (Ordered)”. With the prefixed letter changed to a number indicating the distance to the predicate (negative for being left to the predicate and positive for right), another feature is formed, namely “Bag of POS (Numbered)”. WIND5 BIGRAM (b3): 5 closest words from both left and right plus the predicate itself, in total 11 words form a “window”, within which bigrams are enumerated. The final optimized feature set for the task of predicate classification is (a1, a21, a23, a71, a72, a73, a74, a81, a82, a83, a84, a9, b11, b12, b22, b3, a71+a9). 3.4 Semantic Role Classification In our system, the identification and classification of semantic roles are achieved in a single stage (Liu et al., 2005) through one single classifier (actually two, one for noun predicates, and the other for verb predicates). Each word in a sentence is given probabilities to be each semantic role (including none of the these roles) for a predicate. Features introduced in addition to those of the previous subsections are the following: POS PATH (c11), REL PATH (c12): The “POS Path” feature consists of POS tags of the words along the path from a word to the predicate. Other than “Up” and “Down”, the “Left” and “Right” direction of the path is added. Similarly, the “Relation Path” feature consists of the relation types along the same path. UP PATH (c21), UP REL PATH (c22): “Upstream paths” are parts of the above paths that stop at the common ancestor of a word and the predicate. PATH LEN (c3): Length of the paths POSITION (c4): The relative position of a word to the predicate: Left or Right. PRED FAMILYSHIP (c5): “Familyship relation” between a word and the predicate, being one of “self”, “child”, “descendant”, “parent”, “ancestor”, “sibling”, and “not-relative”. PRED SENSE (c6): The lemma plus sense number of the predicate As for the task of semantic role classification, the features of the predicate word in addition to those of the word under consideration can also be used; we mark features of the predicate with an extra ‘p’. For example, the head word of the current word is represented as a31, and the head word of the predicate is represented as pa31. So, with no doubt for the representation, our final optimized feature set for the task of semantic role classification is (a1, a23, a33, a43, a53, a6, c11, c12, c21, c3, c4, c6, pa23, pa71, pa73, 240 pa83, a1+a23+a33, a21+c5, a23+c12, a33+c12, a33+c22, a6+a33, a73+c5, c11+c12, pa71+pa73). 3.5 ILP-based Post Inference The final semantic role labeling result is generated through an ILP (Integer Linear Programming) based post inference method. An ILP problem is formulated with respect to the probability given by the above stage. The final labeling is formed at the same time when the problem is solved. the set of words in the sentence, and the set of semantic role labels. A virtual label is also added to representing “none of the roles is assigned”. each word semantic role label create a binary variable value indicates whether or not the word labeled as label the possibilof word be labeled as role Obviously, objective function = maximized, we can read the labeling for a predicate from the assignments to the variables There are three constrains used in our system: C1: Each relation should be and only be labeled with one label (including the virtual label “NULL”), i.e.: 1 r C2: Roles with a small probability should never be labeled (except for the virtual role “NULL”). threshold we use in our system is which is optimized from the development data. i.e.: 0, 0.3 C3: Statistics shows that the most roles (except for the virtual role “NULL”) usually appear only once for a predicate, except for some rare exception. So we impose a no-duplicated-roles constraint with an exception list, which is constructed according to the times of semantic roles’ duplication for each single predicate (different senses of a predicate are considered different) and the ratio of duplication to non-duplication. p,r &gt; p,r &gt; 0.3 the set of predicates; the count of words in the training corpus, which are Predicate Type Predicate Label</abstract>
<note confidence="0.865471777777778">Noun president.01 A3 Verb match.01 A1 Verb tie.01 A1 Verb link.01 A1 Verb rate.01 A0 Verb rate.01 A2 Verb attach.01 A1 Verb connect.01 A1 Verb fit.01 A1</note>
<abstract confidence="0.948982885714286">Noun trader.01 SU Table 3: No-duplicated-roles constraint exception list (obtained by Eq. (2)) as predicate while something similar to but what taken account are only those words labeled with and there are more than one roles within the sentence for the same predicate. Table 3 lists the complete exception set, which has a size of only 10. 4 Experiments original implemented in Java. We were confronted with memory shortage when trying to train a model with the entire CoNLL 2008 training data with 4GB memory. Therefore, we rewrote it with C++ which can manage the memory more exactly. Since the time was limited, we only rewrote the projective part without considering second-order parsing technique. Our maximum entropy classifier is implemented Maximum Entropy Modeling The classifier parameters: gaussian prior and iterations, are tuned with the development data for different stages respectively. solve chosen as our ILP problem solver during the post inference stage. The training time of the syntactic and the semantic parsers are 22 and 5 hours respectively, on all training data, with 2.0GHz Xeon CPU and 4G memory. While the prediction can be done within 10 and 5 minutes on the development data. 4.1 Syntactic Dependency Parsing The experiments on development data show that relabeling process is helpful, which improves the toolkit.html 241 Precision (%) Recall (%) F1 Pred Identification 91.61 91.36 91.48 Pred Classification 86.61 86.37 86.49 Table 4: The performance of predicate identification and classification Precision (%) Recall (%) F1 Simple 81.02 76.00 78.43 ILP-based 82.53 75.26 78.73 Table 5: Comparison between different post inference strategies LAS performance from 85.41% to 85.94%. The final syntactic dependency parsing performances on the WSJ and the Brown test data are 87.51% and 80.73% respectively. 4.2 Semantic Dependency Parsing The semantic dependency parsing component is based on the last syntactic dependency parsing component. All stages of the system are trained with the closed training corpus, while predicted against the output of the syntactic parsing. Performance for predicate identification and classification is given in Table 4, wherein the classification is done on top of the identification. Semantic role classification and the post inference are done on top of the result of predicate identification and classification. The final performance is presented in Table 5. A simple post inference strategy is given for comparison, where the most possible label (including the virtual label “NULL”) is select except for those duplicated non-virtual labels with lower probabilities (lower than 0.5). Our ILP-based method produces a gain of 0.30 with respect to the F1 score. The final semantic dependency parsing performance on the development and the test (WSJ and</abstract>
<note confidence="0.808056">Brown) data are shown in Table 6. Precision (%) Recall (%) F1 Development 82.53 75.26 78.73 Test (WSJ) 82.67 77.50 80.00 Test (Brown) 64.38 68.50 66.37</note>
<abstract confidence="0.920567322580645">Table 6: Semantic dependency parsing performances 4.3 Overall Performance The overall macro scores of our syntactic and semantic dependency parsing system are 82.38%, 83.78% and 73.57% on the development and two test (WSJ and Brown) data respectively, which is ranked the second position in the closed challenge. 5 Conclusion and Future Work We present our CoNLL 2008 Shared Task system which is composed of two cascaded components: a syntactic and a semantic dependency parsers, which are built with some state-of-the-art methods. Through a fine tuning features and parameters, the final system achieves promising results. In order to improve the performance further, we will study how to make use of more resources and tools (open challenge) and how to do joint learning between syntactic and semantic parsing. Acknowledgments The authors would like to thank the reviewers for their helpful comments. This work was supported by National Natural Science Foundation of China (NSFC) via grant 60675034, 60575042, and the “863” National High-Tech Research and Development of China via grant 2006AA01Z145. References Liu, Ting, Wanxiang Che, Sheng Li, Yuxuan Hu, and Huaijun Liu. 2005. Semantic role labeling system maximum entropy classifier. In June.</abstract>
<note confidence="0.807018416666667">Ryan. 2006. Learning and Tree Algorithms for Dependency Ph.D. thesis, University of Pennsylvania. Punyakanok, Vasin, Dan Roth, Wen-tau Yih, and Dav Zimak. 2004. Semantic role labeling via integer programming inference. In of pages 1346–1352. Surdeanu, Mihai, Richard Johansson, Adam Meyers, Lluis M`arquez, and Joakim Nivre. 2008. The CoNLL-2008 shared task on joint parsing of syntacand semantic dependencies. In of the 12th Conference on Computational Natural Lan-</note>
<title confidence="0.532644">Learning</title>
<address confidence="0.705517">242</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>CoNLL</author>
</authors>
<date>2008</date>
<booktitle>Proceedings of the 12th Conference on Computational Natural Language Learning,</booktitle>
<pages>238--242</pages>
<location>Manchester,</location>
<contexts>
<context position="1164" citStr="CoNLL 2008" startWordPosition="179" endWordPosition="180">er to overcome the shortcoming of the MSTParser, that it cannot model more global information, we add a relabeling stage after the parsing to distinguish some confusable labels, such as ADV, TMP, and LOC. Besides adding a predicate identification and a classification stages, our semantic dependency parsing simplifies the traditional four stages semantic role labeling into two: a maximum entropy based argument classification and an ILP-based post inference. Finally, we gain the overall labeled macro F1 = 82.66, which ranked the second position in the closed challenge. 1 System Architecture Our CoNLL 2008 Shared Task (Surdeanu et al., 2008) participating system includes two cascaded components: a syntactic and a semantic dependency parsers. They are described in Section 2 and 3 respectively. Their experimental results are shown in Section 4. Section 5 gives our conclusion and future work. 2 Syntactic Dependency Parsing MSTParser (McDonald, 2006) is selected as our basic syntactic dependency parser. It views the © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. syntact</context>
</contexts>
<marker>CoNLL, 2008</marker>
<rawString>CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 238–242 Manchester, August 2008</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ting Liu</author>
<author>Wanxiang Che</author>
<author>Sheng Li</author>
<author>Yuxuan Hu</author>
<author>Huaijun Liu</author>
</authors>
<title>Semantic role labeling system using maximum entropy classifier.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL-2005,</booktitle>
<marker>Liu, Che, Li, Hu, Liu, 2005</marker>
<rawString>Liu, Ting, Wanxiang Che, Sheng Li, Yuxuan Hu, and Huaijun Liu. 2005. Semantic role labeling system using maximum entropy classifier. In Proceedings of CoNLL-2005, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
</authors>
<title>Discriminative Learning and Spanning Tree Algorithms for Dependency Parsing.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="1511" citStr="McDonald, 2006" startWordPosition="231" endWordPosition="232">ages semantic role labeling into two: a maximum entropy based argument classification and an ILP-based post inference. Finally, we gain the overall labeled macro F1 = 82.66, which ranked the second position in the closed challenge. 1 System Architecture Our CoNLL 2008 Shared Task (Surdeanu et al., 2008) participating system includes two cascaded components: a syntactic and a semantic dependency parsers. They are described in Section 2 and 3 respectively. Their experimental results are shown in Section 4. Section 5 gives our conclusion and future work. 2 Syntactic Dependency Parsing MSTParser (McDonald, 2006) is selected as our basic syntactic dependency parser. It views the © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. syntactic dependency parsing as a problem of finding maximum spanning trees (MST) in directed graphs. MSTParser provides the state-ofthe-art performance for both projective and nonprojective tree banks. 2.1 Features The score of each labeled arc is computed through the Eq. (1) in MSTParser. score(h, c, l) = w · f(h, c, l) (1) where node h represents th</context>
</contexts>
<marker>McDonald, 2006</marker>
<rawString>McDonald, Ryan. 2006. Discriminative Learning and Spanning Tree Algorithms for Dependency Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
<author>Dav Zimak</author>
</authors>
<title>Semantic role labeling via integer linear programming inference.</title>
<date>2004</date>
<booktitle>In Proceedings of Coling-2004,</booktitle>
<pages>1346--1352</pages>
<marker>Punyakanok, Roth, Yih, Zimak, 2004</marker>
<rawString>Punyakanok, Vasin, Dan Roth, Wen-tau Yih, and Dav Zimak. 2004. Semantic role labeling via integer linear programming inference. In Proceedings of Coling-2004, pages 1346–1352.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Richard Johansson</author>
<author>Adam Meyers</author>
<author>Lluis M`arquez</author>
<author>Joakim Nivre</author>
</authors>
<title>The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>In Proceedings of the 12th Conference on Computational Natural Language Learning (CoNLL-2008).</booktitle>
<marker>Surdeanu, Johansson, Meyers, M`arquez, Nivre, 2008</marker>
<rawString>Surdeanu, Mihai, Richard Johansson, Adam Meyers, Lluis M`arquez, and Joakim Nivre. 2008. The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies. In Proceedings of the 12th Conference on Computational Natural Language Learning (CoNLL-2008).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>