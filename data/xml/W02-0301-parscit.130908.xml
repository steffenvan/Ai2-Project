<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000048">
<note confidence="0.597742666666667">
Proceedings of the Workshop on Natural Language Processing in
the Biomedical Domain, Philadelphia, July 2002, pp. 1-8.
Association for Computational Linguistics.
</note>
<title confidence="0.961773">
Tuning Support Vector Machines for Biomedical Named Entity Recognition
</title>
<author confidence="0.998885">
Jun’ichi Kazama† Takaki Makino$ Yoshihiro Ohta* Jun’ichi Tsujii† I
</author>
<affiliation confidence="0.9449125">
† Department of Computer Science, Graduate School of Information Science and Technology,
University of Tokyo, Bunkyo-ku, Tokyo 113-0033, Japan
$ Department of Complexity Science and Engineering, Graduate School of Frontier Sciences,
University of Tokyo, Bunkyo-ku, Tokyo 113-0033, Japan
* Central Research Laboratory, Hitachi, Ltd., Kokubunji, Tokyo 185-8601, Japan
I CREST, JST (Japan Science and Technology Corporation)
</affiliation>
<sectionHeader confidence="0.993171" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999791833333334">
We explore the use of Support Vector Ma-
chines (SVMs) for biomedical named en-
tity recognition. To make the SVM train-
ing with the available largest corpus – the
GENIA corpus – tractable, we propose to
split the non-entity class into sub-classes,
using part-of-speech information. In ad-
dition, we explore new features such as
word cache and the states of an HMM
trained by unsupervised learning. Experi-
ments on the GENIA corpus show that our
class splitting technique not only enables
the training with the GENIA corpus but
also improves the accuracy. The proposed
new features also contribute to improve
the accuracy. We compare our SVM-
based recognition system with a system
using Maximum Entropy tagging method.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.97831996">
Application of natural language processing (NLP) is
now a key research topic in bioinformatics. Since
it is practically impossible for a researcher to grasp
all of the huge amount of knowledge provided in
the form of natural language, e.g., journal papers,
there is a strong demand for biomedical information
extraction (IE), which extracts knowledge automati-
cally from biomedical papers using NLP techniques
(Ohta et al., 1997; Proux et al., 2000; Yakushiji et
al., 2001).
The process called named entity recognition,
which finds entities that fill the information slots,
e.g., proteins, DNAs, RNAs, cells etc., in the
biomedical context, is an important building block in
such biomedical IE systems. Conceptually, named
entity recognition consists of two tasks: identifica-
tion, which finds the region of a named entity in
a text, and classification, which determines the se-
mantic class of that named entity. The following il-
lustrates biomedical named entity recognition.
“Thus, CIITAPROTEZN not only acti-
vates the expression of class II genesDNA
but recruits another B cell-specific
coactivator to increase transcriptional
activity of class II promotersDNA in
</bodyText>
<equation confidence="0.376457">
B cellsCELLTYPE.”
</equation>
<bodyText confidence="0.999227112903226">
Machine learning approach has been applied to
biomedical named entity recognition (Nobata et al.,
1999; Collier et al., 2000; Yamada et al., 2000;
Shimpuku, 2002). However, no work has achieved
sufficient recognition accuracy. One reason is the
lack of annotated corpora for training as is often
the case of a new domain. Nobata et al. (1999) and
Collier et al. (2000) trained their model with only
100 annotated paper abstracts from the MEDLINE
database (National Library of Medicine, 1999), and
Yamada et al. (2000) used only 77 annotated paper
abstracts. In addition, it is difficult to compare the
techniques used in each study because they used a
closed and different corpus.
To overcome such a situation, the GENIA cor-
pus (Ohta et al., 2002) has been developed, and at
this time it is the largest biomedical annotated cor-
pus available to public, containing 670 annotated ab-
stracts of the MEDLINE database.
Another reason for low accuracies is that biomed-
ical named entities are essentially hard to recognize
using standard feature sets compared with the named
entities in newswire articles (Nobata et al., 2000).
Thus, we need to employ powerful machine learning
techniques which can incorporate various and com-
plex features in a consistent way.
Support Vector Machines (SVMs) (Vapnik, 1995)
and Maximum Entropy (ME) method (Berger et al.,
1996) are powerful learning methods that satisfy
such requirements, and are applied successfully to
other NLP tasks (Kudo and Matsumoto, 2000; Nak-
agawa et al., 2001; Ratnaparkhi, 1996). In this pa-
per, we apply Support Vector Machines to biomed-
ical named entity recognition and train them with
the GENIA corpus. We formulate the named entity
recognition as the classification of each word with
context to one of the classes that represent region
and named entity’s semantic class. Although there
is a previous work that applied SVMs to biomedi-
cal named entity task in this formulation (Yamada et
al., 2000), their method to construct a classifier us-
ing SVMs, one-vs-rest, fails to train a classifier with
entire GENIA corpus, since the cost of SVM train-
ing is super-linear to the size of training samples.
Even with a more feasible method, pairwise (Kreßel,
1998), which is employed in (Kudo and Matsumoto,
2000), we cannot train a classifier in a reasonable
time, because we have a large number of samples
that belong to the non-entity class in this formula-
tion. To solve this problem, we propose to split the
non-entity class to several sub-classes, using part-of-
speech information. We show that this technique not
only enables the training feasible but also improves
the accuracy.
In addition, we explore new features such as word
cache and the states of an unsupervised HMM for
named entity recognition using SVMs. In the exper-
iments, we show the effect of using these features
and compare the overall performance of our SVM-
based recognition system with a system using the
Maximum Entropy method, which is an alternative
to the SVM method.
</bodyText>
<sectionHeader confidence="0.988417" genericHeader="method">
2 The GENIA Corpus
</sectionHeader>
<bodyText confidence="0.999959421052631">
The GENIA corpus is an annotated corpus of pa-
per abstracts taken from the MEDLINE database.
Currently, 670 abstracts are annotated with named
entity tags by biomedical experts and made avail-
able to public (Ver. 1.1).1 These 670 abstracts are a
subset of more than 5,000 abstracts obtained by the
query “human AND blood cell AND transcription
factor“ to the MEDLINE database. Table 1 shows
basic statistics of the GENIA corpus. Since the GE-
NIA corpus is intended to be extensive, there exist
24 distinct named entity classes in the corpus.2 Our
task is to find a named entity region in a paper ab-
stract and correctly select its class out of these 24
classes. This number of classes is relatively large
compared with other corpora used in previous stud-
ies, and compared with the named entity task for
newswire articles. This indicates that the task with
the GENIA corpus is hard, apart from the difficulty
of the biomedical domain itself.
</bodyText>
<footnote confidence="0.894226833333333">
1Available via http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/
2The GENIA corpus also has annotations for conjunc-
tive/disjunctive named entity expressions such as “human B- or
T-cell lines” (Kim et al., 2001). In this paper we ignore such
expressions and consider that constituents in such expressions
are annotated as a dummy class “temp”.
</footnote>
<tableCaption confidence="0.998846">
Table 1: Basic statistics of the GENIA corpus
</tableCaption>
<table confidence="0.999421166666667">
# of sentences 5,109
# of words 152,216
# of named entities 23,793
# of words in NEs 50,229
# of words not in NEs 101,987
Av. length ofNEs (a) 2.11 (1.40)
</table>
<sectionHeader confidence="0.958903" genericHeader="method">
3 Named Entity Recognition Using SVMs
</sectionHeader>
<subsectionHeader confidence="0.998353">
3.1 Named Entity Recognition as Classification
</subsectionHeader>
<bodyText confidence="0.998756217391304">
We formulate the named entity task as the classi-
fication of each word with context to one of the
classes that represent region information and named
entity’s semantic class. Several representations to
encode region information are proposed and exam-
ined (Ramshaw and Marcus, 1995; Uchimoto et al.,
2000; Kudo and Matsumoto, 2001). In this paper,
we employ the simplest BIO representation, which
is also used in (Yamada et al., 2000). We modify
this representation in Section 5.1 in order to acceler-
ate the SVM training.
In the BIO representation, the region information
is represented as the class prefixes “B-” and “I-”, and
a class “O”. B- means that the current word is at the
beginning of a named entity, I- means that the cur-
rent word is in a named entity (but not at the be-
ginning), and O means the word is not in a named
entity. For each named entity class C, class B-C and
I-C are produced. Therefore, if we have N named
entity classes, the BIO representation yields 2N + 1
classes, which will be the targets of a classifier. For
instance, the following corresponds to the annota-
tion “Number of glucocorticoid receptorsPROTEIN in
</bodyText>
<equation confidence="0.8788114">
lymphocytesCELLTYPE and ...”.
Number of glucocorticoid receptors
O O B-PROTEIN I-PROTEIN
in lymphocytes and ...
O B-CELLTYPE O ...
</equation>
<subsectionHeader confidence="0.999734">
3.2 Support Vector Machines
</subsectionHeader>
<bodyText confidence="0.9981336">
Support Vector Machines (SVMs) (Cortes and Vap-
nik, 1995) are powerful methods for learning a clas-
sifier, which have been applied successfully to many
NLP tasks such as base phrase chunking (Kudo and
Matsumoto, 2000) and part-of-speech tagging (Nak-
agawa et al., 2001).
The SVM constructs a binary classifier that out-
puts +1 or −1 given a sample vector x E Rn. The de-
cision is based on the separating hyperplane as fol-
lows.
</bodyText>
<equation confidence="0.9319384">
+1 if w · x + b &gt; 0, wERn,bER,
−1 otherwise
⎧ ⎨⎪
⎪⎩
c(x) =
</equation>
<bodyText confidence="0.999993166666667">
The class for an input x, c(x), is determined by see-
ing which side of the space separated by the hyper-
plane, w · x + b = 0, the input lies on.
Given a set of labeled training samples
{(y1, x1), ··· , (yL, xL)}, xi ∈ Rn, yi ∈ {+1, −1},
the SVM training tries to find the optimal hy-
perplane, i.e., the hyperplane with the maximum
margin. Margin is defined as the distance between
the hyperplane and the training samples nearest
to the hyperplane. Maximizing the margin insists
that these nearest samples (support vectors) exist
on both sides of the separating hyperplane and the
hyperplane lies exactly at the midpoint of these
support vectors. This margin maximization tightly
relates to the fine generalization power of SVMs.
Assuming that |w·xi+b |= 1 at the support vectors
without loss of generality, the SVM training can be
formulated as the following optimization problem.3
</bodyText>
<equation confidence="0.8187485">
1
minimize ||w||2
2
subject to yi(w · xi + b) ≥ 1, i = 1, · · · , L.
</equation>
<bodyText confidence="0.999837333333333">
The solution of this problem is known to be written
as follows, using only support vectors and weights
for them.
</bodyText>
<equation confidence="0.994814">
�f(x) = w · x + b= yiαix · xi + b (1)
i∈S Vs
</equation>
<bodyText confidence="0.997237809523809">
In the SVM learning, we can use a function k(xi, xj)
called a kernel function instead of the inner prod-
uct in the above equation. Introducing a kernel
function means mapping an original input x using
(D(x), s.t. (D(xi)·(D(xj) = k(xi, xj) to another, usually
a higher dimensional, feature space. We construct
the optimal hyperplane in that space. By using ker-
nel functions, we can construct a non-linear separat-
ing surface in the original feature space. Fortunately,
such non-linear training does not increase the com-
putational cost if the calculation of the kernel func-
tion is as cheap as the inner product. A polynomial
function defined as (sxi · xj + r)d is popular in ap-
plications of SVMs to NLPs (Kudo and Matsumoto,
2000; Yamada et al., 2000; Kudo and Matsumoto,
2001), because it has an intuitively sound interpre-
tation that each dimension of the mapped space is a
3For many real-world problems where the samples may be
inseparable, we allow the constraints are broken with some
penalty. In the experiments, we use so-called 1-norm soft mar-
gin formulation described as:
</bodyText>
<equation confidence="0.8313425">
minimize 2||w||2 + C
1
</equation>
<bodyText confidence="0.87794225">
subject to yi(w · xi + b) ≥ 1 − ei, i = 1, · · · , L,
ei ≥ 0, i = 1,··· , L.
(weighted) conjunction of d features in the original
sample.
</bodyText>
<subsectionHeader confidence="0.998475">
3.3 Multi-Class SVMs
</subsectionHeader>
<bodyText confidence="0.999963195652174">
As described above, the standard SVM learning con-
structs a binary classifier. To make a named entity
recognition system based on the BIO representation,
we require a multi-class classifier. Among several
methods for constructing a multi-class SVM (Hsu
and Lin, 2002), we use a pairwise method proposed
by Kre13el (1998) instead of the one-vs-rest method
used in (Yamada et al., 2000), and extend the BIO
representation to enable the training with the entire
GENIA corpus. Here we describe the one-vs-rest
method and the pairwise method to show the neces-
sity of our extension.
Both one-vs-rest and pairwise methods construct
a multi-class classifier by combining many binary
SVMs. In the following explanation, K denotes the
number of the target classes.
one-vs-rest Construct K binary SVMs, each of
which determines whether the sample should
be classified as class i or as the other classes.
The output is the class with the maximum f(x)
in Equation 1.
pairwise Construct K(K − 1)/2 binary SVMs, each
of which determines whether the sample should
be classified as class i or as class j. Each binary
SVM has one vote, and the output is the class
with the maximum votes.
Because the SVM training is a quadratic optimiza-
tion program, its cost is super-linear to the size of the
training samples even with the tailored techniques
such as SMO (Platt, 1998) and kernel evaluation
caching (Joachims, 1998). Let L be the number of
the training samples, then the one-vs-rest method
takes time in K × OSVM(L). The BIO formula-
tion produces one training sample per word, and
the training with the GENIA corpus involves over
100,000 training samples as can be seen from Ta-
ble 1. Therefore, it is apparent that the one-vs-
rest method is impractical with the GENIA corpus.
On the other hand, if target classes are equally dis-
tributed, the pairwise method will take time in K(K−
1)/2 × OS VM(2L/K). This method is worthwhile be-
cause each training is much faster, though it requires
the training of (K − 1)/2 times more classifiers. It
is also reported that the pairwise method achieves
higher accuracy than other methods in some bench-
marks (Kre13el, 1998; Hsu and Lin, 2002).
</bodyText>
<subsectionHeader confidence="0.892521">
3.4 Input Features
</subsectionHeader>
<bodyText confidence="0.999993333333333">
An input x to an SVM classifier is a feature repre-
sentation of the word to be classified and its context.
We use a bit-vector representation, each dimension
</bodyText>
<equation confidence="0.474056666666667">
L
ei
i=1
</equation>
<bodyText confidence="0.881335">
of which indicates whether the input matches with 4 Named Entity Recognition Using ME
a certain feature. The following illustrates the well- Model
used features for the named entity recognition task.
</bodyText>
<figure confidence="0.979093647058824">
wk,i = ⎧ ⎪ ⎨⎪⎪⎪ 1 if a word at k,Wk, is the ith word
⎪⎪⎪⎪⎩ in the vocabulary V
0 otherwise (word feature)
posk,i = ⎨⎪ ⎪⎪⎪⎧ 1 if Wk is assigned the ith POS tag
⎪⎪⎪⎪⎩ in the POS tag list POS
0 otherwise (part-of-speech feature)
prek,i = ⎨⎪ ⎪⎪⎪⎧ 1 if Wk starts with the ith prefix
⎪⎪⎪⎪⎩ in the prefix list P
0 otherwise (prefix feature)
sufk,i = ⎨⎪ ⎪⎪⎪⎧ 1 if Wk starts with the ith suffix
⎪⎪⎪⎪⎩ in the suffix list S
0 otherwise (suffix feature)
subk,i = ⎨⎪ ⎪⎪⎪⎧ 1 if Wk contains the ith substring
⎪⎪⎪⎪⎩ in the substring list SB
0 otherwise (substring feature)
pck,i = ⎨⎪⎧ 1 if Wk(k &lt; 0) was assigned ith class
⎪⎩ 0 otherwise (preceding class feature)
</figure>
<bodyText confidence="0.99573775">
The Maximum Entropy method, with which we
compare our SVM-based method, defines the prob-
ability that the class is c given an input vector x as
follows.
</bodyText>
<equation confidence="0.996337333333333">
P(c|x) =i,
Z1x)H
αfi(c,x)
</equation>
<bodyText confidence="0.999871">
where Z(x) is a normalization constant, and fi(c, x)
is a feature function. A feature function is defined
in the same way as the features in the SVM learn-
ing, except that it includes c in it like f(c, x) =
(c is the jth class) ∧ wi,k(x). If x contains pre-
viously assigned classes, then the most probable
</bodyText>
<equation confidence="0.8937245">
�T
class sequence, ˆcT 1 = argmaxc1,··· ,cT t=1 P(ct|xt) is
</equation>
<bodyText confidence="0.9463364">
searched by using the Viterbi-type algorithm. We
use the maximum entropy tagging method described
in (Kazama et al., 2001) for the experiments, which
is a variant of (Ratnaparkhi, 1996) modified to use
HMM state features.
</bodyText>
<sectionHeader confidence="0.704919" genericHeader="method">
5 Tuning of SVMs for Biomedical NE Task
</sectionHeader>
<bodyText confidence="0.999432136363636">
In the above definitions, k is a relative word position
from the word to be classified. A negative value rep-
resents a preceding word’s position, and a positive
value represents a following word’s position. Note
that we assume that the classification proceeds left
to right as can be seen in the definition of the pre-
ceding class feature. For the SVM classification, we
does not use a dynamic argmax-type classification
such as the Viterbi algorithm, since it is difficult to
define a good comparable value for the confidence of
a prediction such as probability. The consequences
of this limitation will be discussed with the experi-
mental results.
Features usually form a group with some vari-
ables such as the position unspecified. In this paper,
we instantiate all features, i.e., instantiate for all i,
for a group and a position. Then, it is convenient to
denote a set of features for a group g and a position
k as gk (e.g., wk and posk). Using this notation, we
write a feature set as {w−1, w0, pre−1, pre0, pc−1}.4
This feature description derives the following input
vector.5
</bodyText>
<equation confidence="0.9337955">
x = {w−1,1, w−1,2, ··· , w−1,|V|, w0,1, ··· , w0,|V|,
pre−1,1, ··· ,pre0,|P|,pc−1,1, ··· ,pc−1,K}
</equation>
<footnote confidence="0.986781333333333">
4We will further compress this as {hw,prei[−1,0],pc−1}.
5Although a huge number of features are instantiated, only
a few features have value one for a given g and k pair.
</footnote>
<subsectionHeader confidence="0.985307">
5.1 Class Splitting Technique
</subsectionHeader>
<bodyText confidence="0.999977793103448">
In Section 3.3, we described that if target classes are
equally distributed, the pairwise method will reduce
the training cost. In our case, however, we have a
very unbalanced class distribution with a large num-
ber of samples belonging to the class “O” (see Table
1). This leads to the same situation with the one-vs-
rest method, i.e., if LO is the number of the samples
belonging to the class “O”, then the most dominant
part of the training takes time in K × OSVM(LO).
One solution to this unbalanced class distribution
problem is to split the class “O” into several sub-
classes effectively. This will reduce the training cost
for the same reason that the pairwise method works.
In this paper, we propose to split the non-entity
class according to part-of-speech (POS) informa-
tion of the word. That is, given a part-of-speech
tag set POS, we produce new |POS |classes, “O-
p” p ∈ POS. Since we use a POS tagger that out-
puts 45 Penn Treebank’s POS tags in this paper, we
have new 45 sub-classes which correspond to non-
entity regions such as “O-NNS” (plural nouns), “O-
JJ” (adjectives), and “O-DT” (determiners).
Splitting by POS information seems useful for im-
proving the system accuracy as well, because in the
named entity recognition we must discriminate be-
tween nouns in named entities and nouns in ordi-
nal noun phrases. In the experiments, we show this
class splitting technique not only enables the feasi-
ble training but also improves the accuracy.
</bodyText>
<subsectionHeader confidence="0.998494">
5.2 Word Cache and HMM Features
</subsectionHeader>
<bodyText confidence="0.999989611111111">
In addition to the standard features, we explore word
cache feature and HMM state feature, mainly to
solve the data sparseness problem.
Although the GENIA corpus is the largest anno-
tated corpus for the biomedical domain, it is still
small compared with other linguistic annotated cor-
pora such as the Penn Treebank. Thus, the data
sparseness problem is severe, and must be treated
carefully. Usually, the data sparseness is prevented
by using more general features that apply to a
broader set of instances (e.g., disjunctions). While
polynomial kernels in the SVM learning can effec-
tively generate feature conjunctions, kernel func-
tions that can effectively generate feature disjunc-
tions are not known. Thus, we should explicitly add
dimensions for such general features.
The word cache feature is defined as the disjunc-
tion of several word features as:
</bodyText>
<equation confidence="0.835892">
wck{k1,···,kn},i ° Vkekwk,i
</equation>
<bodyText confidence="0.999665">
We intend that the word cache feature captures the
similarities of the patterns with a common key word
such as follows.
</bodyText>
<figure confidence="0.766077">
(a) “human W_2 W-1 W0” and “human W-1 W0”
(b) “W0 gene” and “W0 W1 gene”
</figure>
<bodyText confidence="0.986075625">
We use a left word cache defined as lwck,i �
wc{_k,···,0},i, and a right word cache defined as
rwck,i - wc{1,···,k},i for patterns like (a) and (b) in
the above example respectively.
Kazama et al. (2001) proposed to use as features
the Viterbi state sequence of a hidden Markov model
(HMM) to prevent the data sparseness problem in
the maximum entropy tagging model. An HMM is
trained with a large number of unannotated texts by
using an unsupervised learning method. Because
the number of states of the HMM is usually made
smaller than |V|, the Viterbi states give smoothed
but maximally informative representations of word
patterns tuned for the domain, from which the raw
texts are taken.
The HMM feature is defined in the same way as
the word feature as follows.
hmmk,i = { 1 if the Viterbi state for Wk is
the ith state in the HMM’s states W
0 otherwise (HMMfeature)
In the experiments, we train an HMM using raw
MEDLINE abstracts in the GENIA corpus, and
show that the HMM state feature can improve the
accuracy.
</bodyText>
<subsectionHeader confidence="0.994234">
5.3 Implementation Issues
</subsectionHeader>
<bodyText confidence="0.999915636363636">
Towards practical named entity recognition using
SVMs, we have tackled the following implementa-
tion issues. It would be impossible to carry out the
experiments in a reasonable time without such ef-
forts.
Parallel Training: The training of pairwise SVMs
has trivial parallelism, i.e., each SVM can be trained
separately. Since computers with two or more CPUs
are not expensive these days, parallelization is very
practical solution to accelerate the training of pair-
wise SVMs.
Fast Winner Finding: Although the pairwise
method reduces the cost of training, it greatly in-
creases the number of classifications needed to de-
termine the class of one sample. For example, for
our experiments using the GENIA corpus, the BIO
representation with class splitting yields more than
4,000 classification pairs. Fortunately, we can stop
classifications when a class gets K —1 votes and this
stopping greatly saves classification time (Kreßel,
1998). Moreover, we can stop classifications when
the current votes of a class is greater than the others’
possible votes.
Support Vector Caching: In the pairwise method,
though we have a large number of classifiers, each
classifier shares some support vectors with other
classifiers. By storing the bodies of all support vec-
tors together and letting each classifier have only the
weights, we can greatly reduce the size of the clas-
sifier. The sharing of support vectors also can be
exploited to accelerate the classification by caching
the value of the kernel function between a support
vector and a classifiee sample.
</bodyText>
<sectionHeader confidence="0.999674" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.9965396">
To conduct experiments, we divided 670 abstracts
of the GENIA corpus (Ver. 1.1) into the train-
ing part (590 abstracts; 4,487 sentences; 133,915
words) and the test part (80 abstracts; 622 sen-
tences; 18,211 words).6 Texts are tokenized by us-
ing Penn Treebank’s tokenizer. An HMM for the
HMM state features was trained with raw abstracts
of the GENIA corpus (39,116 sentences).7 The
number of states is 160. The vocabulary for the
word feature is constructed by taking the most fre-
quent 10,000 words from the above raw abstracts,
the prefix/suffix/prefix list by taking the most fre-
quent 10,000 prefixes/suffixes/substrings.8
The performance is measured by precision, recall,
and F-score, which are the standard measures for the
</bodyText>
<footnote confidence="0.9971234">
6Randomly selected set used in (Shimpuku, 2002). We do
not use paper titles, while he used.
7These do not include the sentences in the test part.
8These are constructed using the training part to make the
comparison with the ME method fair.
</footnote>
<tableCaption confidence="0.994601">
Table 3: Effect of each feature set assessed by
adding/subtracting (F-score). Changes in bold face
means positive effect.
Table 2: Training time and accuracy with/without
</tableCaption>
<table confidence="0.861179076923077">
the class splitting technique. The number of training
samples includes SOS and EOS (special words for
the start/end of a sentence).
no splitting splitting
training time acc. time acc.
samples (sec.) (F-score) (sec.) (F-
score)
16,000 2,809 37.04 5,581 36.82
32,000 13,614 40.65 9,175 41.36
48,000 21,174 42.44 9,709 42.49
64,000 40,869 42.52 12,502 44.34
96,000 - - 21,922 44.93
128,000 - - 36,846 45.99
</table>
<bodyText confidence="0.9969754">
named entity recognition. Systems based on the BIO
representation may produce an inconsistent class se-
quence such as “O B-DNA I-RNA O”. We interpret
such outputs as follows: once a named entity starts
with “B-C” then we interpret that the named entity
with class “C” ends only when we see another “B-”
or “O-” tag.
We have implemented SMO algorithm (Platt,
1998) and techniques described in (Joachims, 1998)
for soft margin SVMs in C++ programming lan-
guage, and implemented support codes for pairwise
classification and parallel training in Java program-
ming language. To obtain POS information required
for features and class splitting, we used an English
POS tagger described in (Kazama et al., 2001).
</bodyText>
<subsectionHeader confidence="0.999507">
6.1 Class Splitting Technique
</subsectionHeader>
<bodyText confidence="0.9999918125">
First, we show the effect of the class splitting
described in Section 5.1. Varying the size of
training data, we compared the change in the
training time and the accuracy with and with-
out the class splitting. We used a feature set
{hw, pre, suf, sub, posi[−2,···,2],pc[−2,−1]} and the in-
ner product kernel.9 The training time was mea-
sured on a machine with four 700MHz PentiumIIIs
and 16GB RAM. Table 2 shows the results of the
experiments. Figure 1 shows the results graphi-
cally. We can see that without splitting we soon suf-
fer from super-linearity of the SVM training, while
with splitting we can handle the training with over
100,000 samples in a reasonable time. It is very im-
portant that the splitting technique does not sacrifice
the accuracy for speed, rather improves the accuracy.
</bodyText>
<subsectionHeader confidence="0.999949">
6.2 Word Cache and HMM State Features
</subsectionHeader>
<bodyText confidence="0.99994">
In this experiment, we see the effect of the word
cache feature and the HMM state feature described
in Section 3.4. The effect is assessed by the
accuracy gain observed by adding each feature
set to a base feature set and the accuracy degra-
dation observed by subtracting it from a (com-
</bodyText>
<footnote confidence="0.755938">
9Soft margin constant C is 1.0 throughout the experiments.
</footnote>
<table confidence="0.9964371">
feature set (A) adding (B) sub. (k=2) (C) sub. (k=3)
Base 42.86 47.82 49.27
Left cache 43.25 (+0.39) 47.77 (-0.05) 49.02 (-0.25)
Right cache 42.34 (-0.52) 47.81 (-0.01) 49.07 (-0.20)
HMM state 44.70 (+1.84) 47.25 (-0.57) 48.03 (-1.24)
POS 44.82 (+1.96) 48.29 (+0.47) 48.75 (-0.52)
Prec. class 44.58 (+1.72) 43.32 (-4.50) 43.84 (-5.43)
Prefix 42.77 (-0.09) 48.11 (+0.29) 48.73 (-0.54)
Suffix 45.88 (+3.02) 47.07 (-0.75) 48.48 (-0.79)
Substring 42.16 (-0.70) 48.38 (+0.56) 50.23 (+0.96)
</table>
<bodyText confidence="0.999130705882353">
plete) base set. The first column (A) in Ta-
ble 3 shows an adding case where the base fea-
ture set is {w[−2,···,2]}. The columns (B) and
(C) show subtracting cases where the base feature
set is {hw, pre, suf, sub, pos, hmmi[−k,··· ,k], lwck, rwck,
pc[−2,−1]} with k = 2 and k = 3 respectively. The
kernel function is the inner product. We can see that
word cache and HMM state features surely improve
the recognition accuracy. In the table, we also in-
cluded the accuracy change for other standard fea-
tures. Preceeding classes and suffixes are definitely
helpful. On the other hand, the substring feature is
not effective in our setting. Although the effects of
part-of-speech tags and prefixes are not so definite,
it can be said that they are practically effective since
they show positive effects in the case of the maxi-
mum performance.
</bodyText>
<subsectionHeader confidence="0.999226">
6.3 Comparison with the ME Method
</subsectionHeader>
<bodyText confidence="0.975665772727273">
In this set of experiments, we compare our
SVM-based system with a named entity recog-
nition system based on the Maximum Entropy
method. For the SVM system, we used the fea-
ture set {hw, pre, suf, pos, hmmi[−3,··· ,3], lwc3, rwc3,
pc[−2,−1]}, which is shown to be the best in the pre-
vious experiment. The compared system is a max-
imum entropy tagging model described in (Kazama
et al., 2001). Though it supports several character
type features such as number and hyphen and some
conjunctive features such as word n-gram, we do not
use these features to compare the performance un-
der as close a condition as possible. The feature set
used in the maximum entropy system is expressed
as {hw,pre,suf,pos,hmmi[−2,···,2], pc[−2,−1]}.10 Both
systems use the BIO representation with splitting.
Table 4 shows the accuracies of both systems. For
the SVM system, we show the results with the inner
product kernel and several polynomial kernels. The
row “All (id)” shows the accuracy from the view-
10When the width becomes [−3,··· , 3], the accuracy de-
grades (53.72 to 51.73 in F-score).
</bodyText>
<figure confidence="0.999851677419355">
0 20000 40000 60000 80000 100000 120000 140000
Number of training samples
(a) Training size vs. time
0 5000 10000 15000 20000 25000 30000 35000 40000 45000
Training Time (seconds)
(b) Training time vs. accuracy
45000
40000
35000
30000
25000
20000
15000
10000
5000
0
No split
Split
No split
Split
0.46
0.45
0.44
0.43
0.42
0.41
0.4
0.39
0.38
0.37
0.36
</figure>
<figureCaption confidence="0.999996">
Figure 1: Effect of the class splitting technique.
</figureCaption>
<bodyText confidence="0.982430763636364">
point of the identification task, which only finds the
named entity regions. The accuracies for several ma-
jor entity classes are also shown. The SVM system
with the 2-dimensional polynomial kernel achieves
the highest accuracy. This comparison may be un-
fair since a polynomial kernel has the effect of us-
ing conjunctive features, while the ME system does
not use such conjunctive features. Nevertheless, the
facts: we can introduce the polynomial kernel very
easily; there are very few parameters to be tuned;11
we could achieve the higher accuracy; show an ad-
vantage of the SVM system.
It will be interesting to discuss why the SVM sys-
tems with the inner product kernel (and the polyno-
mial kernel with d = 1) are outperformed by the ME
system. We here discuss two possible reasons. The
first is that the SVM system does not use a dynamic
decision such as the Viterbi algorithm, while the ME
system uses it. To see this, we degrade the ME sys-
tem so that it predicts the classes deterministically
without using the Viterbi algorithm. We found that
this system only marks 51.54 in F-score. Thus, it can
be said that a dynamic decision is important for this
named entity task. However, although a method to
convert the outputs of a binary SVM to probabilistic
values is proposed (Platt, 1999), the way to obtain
meaningful probabilistic values needed in Viterbi-
type algorithms from the outputs of a multi-class
SVM is unknown. Solving this problem is certainly
a part of the future work. The second possible rea-
son is that the SVM system in this paper does not
use any cut-off or feature truncation method to re-
move data noise, while the ME system uses a sim-
ple feature cut-off method.12 We observed that the
ME system without the cut-off only marks 49.11 in
11C, s, r, and d
12Features that occur less than 10 times are removed.
F-score. Thus, such a noise reduction method is
also important. However, the cut-off method for the
ME method cannot be applied without modification
since, as described in Section 3.4, the definition of
the features are different in the two approaches. It
can be said the features in the ME method is “finer”
than those in SVMs. In this sense, the ME method
allows us more flexible feature selection. This is an
advantage of the ME method.
The accuracies achieved by both systems can be
said high compared with those of the previous meth-
ods if we consider that we have 24 named entity
classes. However, the accuracies are not sufficient
for a practical use. Though higher accuracy will be
achieved with a larger annotated corpus, we should
also explore more effective features and find effec-
tive feature combination methods to exploit such a
large corpus maximally.
</bodyText>
<sectionHeader confidence="0.998918" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999952285714286">
We have described the use of Support Vector Ma-
chines for the biomedical named entity recognition
task. To make the training of SVMs with the GE-
NIA corpus practical, we proposed to split the non-
entity class by using POS information. In addition,
we explored the new types of features, word cache
and HMM states, to avoid the data sparseness prob-
lem. In the experiments, we have shown that the
class splitting technique not only makes training fea-
sible but also improves the accuracy. We have also
shown that the proposed new features also improve
the accuracy and the SVM system with the polyno-
mial kernel function outperforms the ME-based sys-
tem.
</bodyText>
<sectionHeader confidence="0.999109" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9992545">
We would like to thank Dr. Jin-Dong Kim for pro-
viding us easy-to-use preprocessed training data.
</bodyText>
<tableCaption confidence="0.998019">
Table 4: Comparison: The SVM-based system and the ME-based system. (precision/recall/F-score)
</tableCaption>
<table confidence="0.980571727272727">
SVM ME
type # inner product polynomial (s = 0.01, r = 1.0))
d = 1 d = 2 d = 3
All (2,782) 50.7/49.8/50.2 54.6/48.8/51.5 56.2/52.8/54.4 55.1/51.5/53.2 53.4/53.0/53.2
All(id) 71.8/70.4/71.1 75.0/67.1/70.8 75.9/71.4/73.6 75.3/70.3/72.7 73.5/72.9/73.2
protein (709) 47.2/55.2/50.8 45.7/64.9/53.6 49.2/66.4/56.5 48.7/64.7/55.6 49.1/62.1/54.8
DNA (460) 39.9/37.6/38.7 48.2/31.5/38.1 49.6/37.0/42.3 47.9/37.4/42.0 47.3/39.6/43.1
cell line (121) 54.8/47.1/50.7 61.2/43.0/50.5 60.2/46.3/52.3 62.2/46.3/53.1 58.0/53.7/55.8
cell type (199) 67.6/74.4/70.8 67.4/74.9/71.0 70.0/75.4/72.6 68.6/72.4/70.4 69.9/72.4/71.1
lipid (109) 77.0/61.5/68.4 83.3/50.5/62.9 82.7/61.5/70.5 79.2/56.0/65.6 68.9/65.1/67.0
other names (590) 52.5/53.9/53.2 60.2/55.9/58.0 59.3/58.0/58.6 58.9/57.8/58.3 59.0/61.7/60.3
</table>
<sectionHeader confidence="0.998563" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999897328947368">
A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996. A
maximum entropy approach to natural language processing.
Computational Linguistics, 22(1):39–71.
N. Collier, C. Nobata, and J. Tsujii. 2000. Extracting the names
of genes and gene products with a hidden Markov model. In
Proc. of COLING 2000, pages 201–207.
C. Cortes and V. Vapnik. 1995. Support vector networks. Ma-
chine Learning, 20:273–297.
C. Hsu and C. Lin. 2002. A comparison of methods for multi-
class Support Vector Machines. In IEEE Transactions on
Neural Networks. to appear.
T. Joachims. 1998. Making large-scale support vector machine
learning practical. In Advances in Kernel Methods, pages
169–184. The MIT Press.
J. Kazama, Y. Miyao, and J. Tsujii. 2001. A maximum entropy
tagger with unsupervised hidden markov models. In Proc. of
the 6th NLPRS, pages 333–340.
J. Kim, T. Ohta, Y. Tateisi, H. Mima, and J. Tsujii. 2001. XML-
based linguistic annotation of corpus. In Proc. of the First
NLP and XML Workshop.
U. Kreliel. 1998. Pairwise classification and support vector
machines. In Advances in Kernel Methods, pages 255–268.
The MIT Press.
T. Kudo and Y. Matsumoto. 2000. Use of support vector learn-
ing for chunk identification. In Proc. of CoNLL-2000 and
LLL-2000.
T. Kudo and Y. Matsumoto. 2001. Chunking with Support
Vector Machines. In Proc. ofNAACL 2001, pages 192–199.
T. Nakagawa, T. Kudoh, and Y. Matsumoto. 2001. Unknown
word guessing and part-of-speech tagging using support vec-
tor machines. In Proc. of the 6th NLPRS, pages 325–331.
National Library of Medicine. 1999. MEDLINE. available at
http://www.ncbi.nlm.nih.gov/.
C. Nobata, N. Collier, and J. Tsujii. 1999. Automatic term
identification and classification in biology texts. In Proc. of
the 5th NLPRS, pages 369–374.
C. Nobata, N. Collier, and J. Tsujii. 2000. Comparison between
tagged corpora for the named entity task. In Proc. of the
Workshop on Comparing Corpora (at ACL’2000), pages 20–
27.
Y. Ohta, Y. Yamamoto, T. Okazaki, I. Uchiyama, and T. Tak-
agi. 1997. Automatic construction of knowledge base from
biological papers. In Proc. of the 5th ISMB, pages 218–225.
T. Ohta, Y. Tateisi, J. Kim, H. Mima, and Tsujii J. 2002. The
GENIA corpus: An annotated research abstract corpus in
molecular biology domain. In Proc. of HLT 2002.
J. C. Platt. 1998. Fast training of support vector machines us-
ing sequential minimal optimization. In Advances in Kernel
Methods, pages 185–208. The MIT Press.
J. C. Platt. 1999. Probabilistic outputs for support vector ma-
chines and comparisons to regularized likelihood methods.
Advances in Large Margin Classifiers.
D. Proux, F. Prechenmann, and L. Julliard. 2000. A pragmatic
information extraction strategy for gathering data on genetic
interactions. In Proc. of the 8th ISMB, pages 279–285.
L. A. Ramshaw and M. P. Marcus. 1995. Text chunking us-
ing transformation-based learning. In Proc. of the 3rd ACL
Workshop on Very Large Corpora.
A. Ratnaparkhi. 1996. A maximum entropy model for part-
of-speech tagging. In Proc. of the Conference on Empirical
Methods in Natural Language Processing, pages 133–142.
S. Shimpuku. 2002. A medical/biological term recognizer with
a term hidden Markov model incorporating multiple infor-
mation sources. A master thesis. University of Tokyo.
K. Uchimoto, M. Murata, Q. Ma, H. Ozaku, and H. Isahara.
2000. Named entity extraction based on a maximum entropy
model and transformation rules. In Proc. of the 38th ACL,
pages 326–335.
V. Vapnik. 1995. The Nature of Statistical Learning Theory.
Springer Verlag.
A. Yakushiji, Y. Tateisi, Y. Miyao, and J. Tsujii. 2001. Event
extraction from biomedical papers using a full parser. In
Proc. ofPSB 2001, pages 408–419.
H. Yamada, T. Kudo, and Y. Matsumoto. 2000. Using sub-
strings for technical term extraction and classification. IPSJ
SIGNotes, (NL-140):77–84. (in Japanese).
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.465996">
<note confidence="0.904828666666667">Proceedings of the Workshop on Natural Language Processing in the Biomedical Domain, Philadelphia, July 2002, pp. 1-8. Association for Computational Linguistics.</note>
<title confidence="0.995017">Tuning Support Vector Machines for Biomedical Named Entity Recognition</title>
<author confidence="0.999342">Takaki Jun’ichi I</author>
<affiliation confidence="0.935949833333333">of Computer Science, Graduate School of Information Science and University of Tokyo, Bunkyo-ku, Tokyo 113-0033, of Complexity Science and Engineering, Graduate School of Frontier University of Tokyo, Bunkyo-ku, Tokyo 113-0033, Research Laboratory, Hitachi, Ltd., Kokubunji, Tokyo 185-8601, JST (Japan Science and Technology Corporation)</affiliation>
<abstract confidence="0.999102052631579">We explore the use of Support Vector Machines (SVMs) for biomedical named entity recognition. To make the SVM training with the available largest corpus – the GENIA corpus – tractable, we propose to split the non-entity class into sub-classes, using part-of-speech information. In addition, we explore new features such as word cache and the states of an HMM trained by unsupervised learning. Experiments on the GENIA corpus show that our class splitting technique not only enables the training with the GENIA corpus but also improves the accuracy. The proposed new features also contribute to improve the accuracy. We compare our SVMbased recognition system with a system using Maximum Entropy tagging method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="3988" citStr="Berger et al., 1996" startWordPosition="608" endWordPosition="611">ta et al., 2002) has been developed, and at this time it is the largest biomedical annotated corpus available to public, containing 670 annotated abstracts of the MEDLINE database. Another reason for low accuracies is that biomedical named entities are essentially hard to recognize using standard feature sets compared with the named entities in newswire articles (Nobata et al., 2000). Thus, we need to employ powerful machine learning techniques which can incorporate various and complex features in a consistent way. Support Vector Machines (SVMs) (Vapnik, 1995) and Maximum Entropy (ME) method (Berger et al., 1996) are powerful learning methods that satisfy such requirements, and are applied successfully to other NLP tasks (Kudo and Matsumoto, 2000; Nakagawa et al., 2001; Ratnaparkhi, 1996). In this paper, we apply Support Vector Machines to biomedical named entity recognition and train them with the GENIA corpus. We formulate the named entity recognition as the classification of each word with context to one of the classes that represent region and named entity’s semantic class. Although there is a previous work that applied SVMs to biomedical named entity task in this formulation (Yamada et al., 2000)</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Collier</author>
<author>C Nobata</author>
<author>J Tsujii</author>
</authors>
<title>Extracting the names of genes and gene products with a hidden Markov model.</title>
<date>2000</date>
<booktitle>In Proc. of COLING</booktitle>
<pages>201--207</pages>
<contexts>
<context position="2761" citStr="Collier et al., 2000" startWordPosition="408" endWordPosition="411">dical IE systems. Conceptually, named entity recognition consists of two tasks: identification, which finds the region of a named entity in a text, and classification, which determines the semantic class of that named entity. The following illustrates biomedical named entity recognition. “Thus, CIITAPROTEZN not only activates the expression of class II genesDNA but recruits another B cell-specific coactivator to increase transcriptional activity of class II promotersDNA in B cellsCELLTYPE.” Machine learning approach has been applied to biomedical named entity recognition (Nobata et al., 1999; Collier et al., 2000; Yamada et al., 2000; Shimpuku, 2002). However, no work has achieved sufficient recognition accuracy. One reason is the lack of annotated corpora for training as is often the case of a new domain. Nobata et al. (1999) and Collier et al. (2000) trained their model with only 100 annotated paper abstracts from the MEDLINE database (National Library of Medicine, 1999), and Yamada et al. (2000) used only 77 annotated paper abstracts. In addition, it is difficult to compare the techniques used in each study because they used a closed and different corpus. To overcome such a situation, the GENIA cor</context>
</contexts>
<marker>Collier, Nobata, Tsujii, 2000</marker>
<rawString>N. Collier, C. Nobata, and J. Tsujii. 2000. Extracting the names of genes and gene products with a hidden Markov model. In Proc. of COLING 2000, pages 201–207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cortes</author>
<author>V Vapnik</author>
</authors>
<title>Support vector networks.</title>
<date>1995</date>
<booktitle>Machine Learning,</booktitle>
<pages>20--273</pages>
<contexts>
<context position="8557" citStr="Cortes and Vapnik, 1995" startWordPosition="1373" endWordPosition="1377">rrent word is in a named entity (but not at the beginning), and O means the word is not in a named entity. For each named entity class C, class B-C and I-C are produced. Therefore, if we have N named entity classes, the BIO representation yields 2N + 1 classes, which will be the targets of a classifier. For instance, the following corresponds to the annotation “Number of glucocorticoid receptorsPROTEIN in lymphocytesCELLTYPE and ...”. Number of glucocorticoid receptors O O B-PROTEIN I-PROTEIN in lymphocytes and ... O B-CELLTYPE O ... 3.2 Support Vector Machines Support Vector Machines (SVMs) (Cortes and Vapnik, 1995) are powerful methods for learning a classifier, which have been applied successfully to many NLP tasks such as base phrase chunking (Kudo and Matsumoto, 2000) and part-of-speech tagging (Nakagawa et al., 2001). The SVM constructs a binary classifier that outputs +1 or −1 given a sample vector x E Rn. The decision is based on the separating hyperplane as follows. +1 if w · x + b &gt; 0, wERn,bER, −1 otherwise ⎧ ⎨⎪ ⎪⎩ c(x) = The class for an input x, c(x), is determined by seeing which side of the space separated by the hyperplane, w · x + b = 0, the input lies on. Given a set of labeled training </context>
</contexts>
<marker>Cortes, Vapnik, 1995</marker>
<rawString>C. Cortes and V. Vapnik. 1995. Support vector networks. Machine Learning, 20:273–297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Hsu</author>
<author>C Lin</author>
</authors>
<title>A comparison of methods for multiclass Support Vector Machines. In</title>
<date>2002</date>
<journal>IEEE Transactions on Neural Networks.</journal>
<note>to appear.</note>
<contexts>
<context position="11612" citStr="Hsu and Lin, 2002" startWordPosition="1932" endWordPosition="1935"> the samples may be inseparable, we allow the constraints are broken with some penalty. In the experiments, we use so-called 1-norm soft margin formulation described as: minimize 2||w||2 + C 1 subject to yi(w · xi + b) ≥ 1 − ei, i = 1, · · · , L, ei ≥ 0, i = 1,··· , L. (weighted) conjunction of d features in the original sample. 3.3 Multi-Class SVMs As described above, the standard SVM learning constructs a binary classifier. To make a named entity recognition system based on the BIO representation, we require a multi-class classifier. Among several methods for constructing a multi-class SVM (Hsu and Lin, 2002), we use a pairwise method proposed by Kre13el (1998) instead of the one-vs-rest method used in (Yamada et al., 2000), and extend the BIO representation to enable the training with the entire GENIA corpus. Here we describe the one-vs-rest method and the pairwise method to show the necessity of our extension. Both one-vs-rest and pairwise methods construct a multi-class classifier by combining many binary SVMs. In the following explanation, K denotes the number of the target classes. one-vs-rest Construct K binary SVMs, each of which determines whether the sample should be classified as class i</context>
<context position="13503" citStr="Hsu and Lin, 2002" startWordPosition="2256" endWordPosition="2259">ng sample per word, and the training with the GENIA corpus involves over 100,000 training samples as can be seen from Table 1. Therefore, it is apparent that the one-vsrest method is impractical with the GENIA corpus. On the other hand, if target classes are equally distributed, the pairwise method will take time in K(K− 1)/2 × OS VM(2L/K). This method is worthwhile because each training is much faster, though it requires the training of (K − 1)/2 times more classifiers. It is also reported that the pairwise method achieves higher accuracy than other methods in some benchmarks (Kre13el, 1998; Hsu and Lin, 2002). 3.4 Input Features An input x to an SVM classifier is a feature representation of the word to be classified and its context. We use a bit-vector representation, each dimension L ei i=1 of which indicates whether the input matches with 4 Named Entity Recognition Using ME a certain feature. The following illustrates the well- Model used features for the named entity recognition task. wk,i = ⎧ ⎪ ⎨⎪⎪⎪ 1 if a word at k,Wk, is the ith word ⎪⎪⎪⎪⎩ in the vocabulary V 0 otherwise (word feature) posk,i = ⎨⎪ ⎪⎪⎪⎧ 1 if Wk is assigned the ith POS tag ⎪⎪⎪⎪⎩ in the POS tag list POS 0 otherwise (part-of-spe</context>
</contexts>
<marker>Hsu, Lin, 2002</marker>
<rawString>C. Hsu and C. Lin. 2002. A comparison of methods for multiclass Support Vector Machines. In IEEE Transactions on Neural Networks. to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Making large-scale support vector machine learning practical.</title>
<date>1998</date>
<booktitle>In Advances in Kernel Methods,</booktitle>
<pages>169--184</pages>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="12744" citStr="Joachims, 1998" startWordPosition="2124" endWordPosition="2125">y SVMs, each of which determines whether the sample should be classified as class i or as the other classes. The output is the class with the maximum f(x) in Equation 1. pairwise Construct K(K − 1)/2 binary SVMs, each of which determines whether the sample should be classified as class i or as class j. Each binary SVM has one vote, and the output is the class with the maximum votes. Because the SVM training is a quadratic optimization program, its cost is super-linear to the size of the training samples even with the tailored techniques such as SMO (Platt, 1998) and kernel evaluation caching (Joachims, 1998). Let L be the number of the training samples, then the one-vs-rest method takes time in K × OSVM(L). The BIO formulation produces one training sample per word, and the training with the GENIA corpus involves over 100,000 training samples as can be seen from Table 1. Therefore, it is apparent that the one-vsrest method is impractical with the GENIA corpus. On the other hand, if target classes are equally distributed, the pairwise method will take time in K(K− 1)/2 × OS VM(2L/K). This method is worthwhile because each training is much faster, though it requires the training of (K − 1)/2 times m</context>
<context position="23834" citStr="Joachims, 1998" startWordPosition="4012" endWordPosition="4013">ec.) (F-score) (sec.) (Fscore) 16,000 2,809 37.04 5,581 36.82 32,000 13,614 40.65 9,175 41.36 48,000 21,174 42.44 9,709 42.49 64,000 40,869 42.52 12,502 44.34 96,000 - - 21,922 44.93 128,000 - - 36,846 45.99 named entity recognition. Systems based on the BIO representation may produce an inconsistent class sequence such as “O B-DNA I-RNA O”. We interpret such outputs as follows: once a named entity starts with “B-C” then we interpret that the named entity with class “C” ends only when we see another “B-” or “O-” tag. We have implemented SMO algorithm (Platt, 1998) and techniques described in (Joachims, 1998) for soft margin SVMs in C++ programming language, and implemented support codes for pairwise classification and parallel training in Java programming language. To obtain POS information required for features and class splitting, we used an English POS tagger described in (Kazama et al., 2001). 6.1 Class Splitting Technique First, we show the effect of the class splitting described in Section 5.1. Varying the size of training data, we compared the change in the training time and the accuracy with and without the class splitting. We used a feature set {hw, pre, suf, sub, posi[−2,···,2],pc[−2,−1</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>T. Joachims. 1998. Making large-scale support vector machine learning practical. In Advances in Kernel Methods, pages 169–184. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kazama</author>
<author>Y Miyao</author>
<author>J Tsujii</author>
</authors>
<title>A maximum entropy tagger with unsupervised hidden markov models.</title>
<date>2001</date>
<booktitle>In Proc. of the 6th NLPRS,</booktitle>
<pages>333--340</pages>
<contexts>
<context position="15202" citStr="Kazama et al., 2001" startWordPosition="2572" endWordPosition="2575">mpare our SVM-based method, defines the probability that the class is c given an input vector x as follows. P(c|x) =i, Z1x)H αfi(c,x) where Z(x) is a normalization constant, and fi(c, x) is a feature function. A feature function is defined in the same way as the features in the SVM learning, except that it includes c in it like f(c, x) = (c is the jth class) ∧ wi,k(x). If x contains previously assigned classes, then the most probable �T class sequence, ˆcT 1 = argmaxc1,··· ,cT t=1 P(ct|xt) is searched by using the Viterbi-type algorithm. We use the maximum entropy tagging method described in (Kazama et al., 2001) for the experiments, which is a variant of (Ratnaparkhi, 1996) modified to use HMM state features. 5 Tuning of SVMs for Biomedical NE Task In the above definitions, k is a relative word position from the word to be classified. A negative value represents a preceding word’s position, and a positive value represents a following word’s position. Note that we assume that the classification proceeds left to right as can be seen in the definition of the preceding class feature. For the SVM classification, we does not use a dynamic argmax-type classification such as the Viterbi algorithm, since it i</context>
<context position="19482" citStr="Kazama et al. (2001)" startWordPosition="3300" endWordPosition="3303">generate feature disjunctions are not known. Thus, we should explicitly add dimensions for such general features. The word cache feature is defined as the disjunction of several word features as: wck{k1,···,kn},i ° Vkekwk,i We intend that the word cache feature captures the similarities of the patterns with a common key word such as follows. (a) “human W_2 W-1 W0” and “human W-1 W0” (b) “W0 gene” and “W0 W1 gene” We use a left word cache defined as lwck,i � wc{_k,···,0},i, and a right word cache defined as rwck,i - wc{1,···,k},i for patterns like (a) and (b) in the above example respectively. Kazama et al. (2001) proposed to use as features the Viterbi state sequence of a hidden Markov model (HMM) to prevent the data sparseness problem in the maximum entropy tagging model. An HMM is trained with a large number of unannotated texts by using an unsupervised learning method. Because the number of states of the HMM is usually made smaller than |V|, the Viterbi states give smoothed but maximally informative representations of word patterns tuned for the domain, from which the raw texts are taken. The HMM feature is defined in the same way as the word feature as follows. hmmk,i = { 1 if the Viterbi state fo</context>
<context position="24128" citStr="Kazama et al., 2001" startWordPosition="4056" endWordPosition="4059">inconsistent class sequence such as “O B-DNA I-RNA O”. We interpret such outputs as follows: once a named entity starts with “B-C” then we interpret that the named entity with class “C” ends only when we see another “B-” or “O-” tag. We have implemented SMO algorithm (Platt, 1998) and techniques described in (Joachims, 1998) for soft margin SVMs in C++ programming language, and implemented support codes for pairwise classification and parallel training in Java programming language. To obtain POS information required for features and class splitting, we used an English POS tagger described in (Kazama et al., 2001). 6.1 Class Splitting Technique First, we show the effect of the class splitting described in Section 5.1. Varying the size of training data, we compared the change in the training time and the accuracy with and without the class splitting. We used a feature set {hw, pre, suf, sub, posi[−2,···,2],pc[−2,−1]} and the inner product kernel.9 The training time was measured on a machine with four 700MHz PentiumIIIs and 16GB RAM. Table 2 shows the results of the experiments. Figure 1 shows the results graphically. We can see that without splitting we soon suffer from super-linearity of the SVM traini</context>
<context position="27077" citStr="Kazama et al., 2001" startWordPosition="4561" endWordPosition="4564">ough the effects of part-of-speech tags and prefixes are not so definite, it can be said that they are practically effective since they show positive effects in the case of the maximum performance. 6.3 Comparison with the ME Method In this set of experiments, we compare our SVM-based system with a named entity recognition system based on the Maximum Entropy method. For the SVM system, we used the feature set {hw, pre, suf, pos, hmmi[−3,··· ,3], lwc3, rwc3, pc[−2,−1]}, which is shown to be the best in the previous experiment. The compared system is a maximum entropy tagging model described in (Kazama et al., 2001). Though it supports several character type features such as number and hyphen and some conjunctive features such as word n-gram, we do not use these features to compare the performance under as close a condition as possible. The feature set used in the maximum entropy system is expressed as {hw,pre,suf,pos,hmmi[−2,···,2], pc[−2,−1]}.10 Both systems use the BIO representation with splitting. Table 4 shows the accuracies of both systems. For the SVM system, we show the results with the inner product kernel and several polynomial kernels. The row “All (id)” shows the accuracy from the view10When</context>
</contexts>
<marker>Kazama, Miyao, Tsujii, 2001</marker>
<rawString>J. Kazama, Y. Miyao, and J. Tsujii. 2001. A maximum entropy tagger with unsupervised hidden markov models. In Proc. of the 6th NLPRS, pages 333–340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kim</author>
<author>T Ohta</author>
<author>Y Tateisi</author>
<author>H Mima</author>
<author>J Tsujii</author>
</authors>
<title>XMLbased linguistic annotation of corpus.</title>
<date>2001</date>
<booktitle>In Proc. of the First NLP and XML Workshop.</booktitle>
<contexts>
<context position="6786" citStr="Kim et al., 2001" startWordPosition="1071" endWordPosition="1074">in the corpus.2 Our task is to find a named entity region in a paper abstract and correctly select its class out of these 24 classes. This number of classes is relatively large compared with other corpora used in previous studies, and compared with the named entity task for newswire articles. This indicates that the task with the GENIA corpus is hard, apart from the difficulty of the biomedical domain itself. 1Available via http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/ 2The GENIA corpus also has annotations for conjunctive/disjunctive named entity expressions such as “human B- or T-cell lines” (Kim et al., 2001). In this paper we ignore such expressions and consider that constituents in such expressions are annotated as a dummy class “temp”. Table 1: Basic statistics of the GENIA corpus # of sentences 5,109 # of words 152,216 # of named entities 23,793 # of words in NEs 50,229 # of words not in NEs 101,987 Av. length ofNEs (a) 2.11 (1.40) 3 Named Entity Recognition Using SVMs 3.1 Named Entity Recognition as Classification We formulate the named entity task as the classification of each word with context to one of the classes that represent region information and named entity’s semantic class. Several</context>
</contexts>
<marker>Kim, Ohta, Tateisi, Mima, Tsujii, 2001</marker>
<rawString>J. Kim, T. Ohta, Y. Tateisi, H. Mima, and J. Tsujii. 2001. XMLbased linguistic annotation of corpus. In Proc. of the First NLP and XML Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Kreliel</author>
</authors>
<title>Pairwise classification and support vector machines.</title>
<date>1998</date>
<booktitle>In Advances in Kernel Methods,</booktitle>
<pages>255--268</pages>
<publisher>The MIT Press.</publisher>
<marker>Kreliel, 1998</marker>
<rawString>U. Kreliel. 1998. Pairwise classification and support vector machines. In Advances in Kernel Methods, pages 255–268. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kudo</author>
<author>Y Matsumoto</author>
</authors>
<title>Use of support vector learning for chunk identification.</title>
<date>2000</date>
<booktitle>In Proc. of CoNLL-2000 and LLL-2000.</booktitle>
<contexts>
<context position="4124" citStr="Kudo and Matsumoto, 2000" startWordPosition="628" endWordPosition="631">670 annotated abstracts of the MEDLINE database. Another reason for low accuracies is that biomedical named entities are essentially hard to recognize using standard feature sets compared with the named entities in newswire articles (Nobata et al., 2000). Thus, we need to employ powerful machine learning techniques which can incorporate various and complex features in a consistent way. Support Vector Machines (SVMs) (Vapnik, 1995) and Maximum Entropy (ME) method (Berger et al., 1996) are powerful learning methods that satisfy such requirements, and are applied successfully to other NLP tasks (Kudo and Matsumoto, 2000; Nakagawa et al., 2001; Ratnaparkhi, 1996). In this paper, we apply Support Vector Machines to biomedical named entity recognition and train them with the GENIA corpus. We formulate the named entity recognition as the classification of each word with context to one of the classes that represent region and named entity’s semantic class. Although there is a previous work that applied SVMs to biomedical named entity task in this formulation (Yamada et al., 2000), their method to construct a classifier using SVMs, one-vs-rest, fails to train a classifier with entire GENIA corpus, since the cost o</context>
<context position="8716" citStr="Kudo and Matsumoto, 2000" startWordPosition="1400" endWordPosition="1403">e produced. Therefore, if we have N named entity classes, the BIO representation yields 2N + 1 classes, which will be the targets of a classifier. For instance, the following corresponds to the annotation “Number of glucocorticoid receptorsPROTEIN in lymphocytesCELLTYPE and ...”. Number of glucocorticoid receptors O O B-PROTEIN I-PROTEIN in lymphocytes and ... O B-CELLTYPE O ... 3.2 Support Vector Machines Support Vector Machines (SVMs) (Cortes and Vapnik, 1995) are powerful methods for learning a classifier, which have been applied successfully to many NLP tasks such as base phrase chunking (Kudo and Matsumoto, 2000) and part-of-speech tagging (Nakagawa et al., 2001). The SVM constructs a binary classifier that outputs +1 or −1 given a sample vector x E Rn. The decision is based on the separating hyperplane as follows. +1 if w · x + b &gt; 0, wERn,bER, −1 otherwise ⎧ ⎨⎪ ⎪⎩ c(x) = The class for an input x, c(x), is determined by seeing which side of the space separated by the hyperplane, w · x + b = 0, the input lies on. Given a set of labeled training samples {(y1, x1), ··· , (yL, xL)}, xi ∈ Rn, yi ∈ {+1, −1}, the SVM training tries to find the optimal hyperplane, i.e., the hyperplane with the maximum margin</context>
<context position="10813" citStr="Kudo and Matsumoto, 2000" startWordPosition="1790" endWordPosition="1793">oduct in the above equation. Introducing a kernel function means mapping an original input x using (D(x), s.t. (D(xi)·(D(xj) = k(xi, xj) to another, usually a higher dimensional, feature space. We construct the optimal hyperplane in that space. By using kernel functions, we can construct a non-linear separating surface in the original feature space. Fortunately, such non-linear training does not increase the computational cost if the calculation of the kernel function is as cheap as the inner product. A polynomial function defined as (sxi · xj + r)d is popular in applications of SVMs to NLPs (Kudo and Matsumoto, 2000; Yamada et al., 2000; Kudo and Matsumoto, 2001), because it has an intuitively sound interpretation that each dimension of the mapped space is a 3For many real-world problems where the samples may be inseparable, we allow the constraints are broken with some penalty. In the experiments, we use so-called 1-norm soft margin formulation described as: minimize 2||w||2 + C 1 subject to yi(w · xi + b) ≥ 1 − ei, i = 1, · · · , L, ei ≥ 0, i = 1,··· , L. (weighted) conjunction of d features in the original sample. 3.3 Multi-Class SVMs As described above, the standard SVM learning constructs a binary c</context>
</contexts>
<marker>Kudo, Matsumoto, 2000</marker>
<rawString>T. Kudo and Y. Matsumoto. 2000. Use of support vector learning for chunk identification. In Proc. of CoNLL-2000 and LLL-2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kudo</author>
<author>Y Matsumoto</author>
</authors>
<title>Chunking with Support Vector Machines.</title>
<date>2001</date>
<booktitle>In Proc. ofNAACL</booktitle>
<pages>192--199</pages>
<contexts>
<context position="7533" citStr="Kudo and Matsumoto, 2001" startWordPosition="1196" endWordPosition="1199">ss “temp”. Table 1: Basic statistics of the GENIA corpus # of sentences 5,109 # of words 152,216 # of named entities 23,793 # of words in NEs 50,229 # of words not in NEs 101,987 Av. length ofNEs (a) 2.11 (1.40) 3 Named Entity Recognition Using SVMs 3.1 Named Entity Recognition as Classification We formulate the named entity task as the classification of each word with context to one of the classes that represent region information and named entity’s semantic class. Several representations to encode region information are proposed and examined (Ramshaw and Marcus, 1995; Uchimoto et al., 2000; Kudo and Matsumoto, 2001). In this paper, we employ the simplest BIO representation, which is also used in (Yamada et al., 2000). We modify this representation in Section 5.1 in order to accelerate the SVM training. In the BIO representation, the region information is represented as the class prefixes “B-” and “I-”, and a class “O”. B- means that the current word is at the beginning of a named entity, I- means that the current word is in a named entity (but not at the beginning), and O means the word is not in a named entity. For each named entity class C, class B-C and I-C are produced. Therefore, if we have N named </context>
<context position="10861" citStr="Kudo and Matsumoto, 2001" startWordPosition="1798" endWordPosition="1801">el function means mapping an original input x using (D(x), s.t. (D(xi)·(D(xj) = k(xi, xj) to another, usually a higher dimensional, feature space. We construct the optimal hyperplane in that space. By using kernel functions, we can construct a non-linear separating surface in the original feature space. Fortunately, such non-linear training does not increase the computational cost if the calculation of the kernel function is as cheap as the inner product. A polynomial function defined as (sxi · xj + r)d is popular in applications of SVMs to NLPs (Kudo and Matsumoto, 2000; Yamada et al., 2000; Kudo and Matsumoto, 2001), because it has an intuitively sound interpretation that each dimension of the mapped space is a 3For many real-world problems where the samples may be inseparable, we allow the constraints are broken with some penalty. In the experiments, we use so-called 1-norm soft margin formulation described as: minimize 2||w||2 + C 1 subject to yi(w · xi + b) ≥ 1 − ei, i = 1, · · · , L, ei ≥ 0, i = 1,··· , L. (weighted) conjunction of d features in the original sample. 3.3 Multi-Class SVMs As described above, the standard SVM learning constructs a binary classifier. To make a named entity recognition sy</context>
</contexts>
<marker>Kudo, Matsumoto, 2001</marker>
<rawString>T. Kudo and Y. Matsumoto. 2001. Chunking with Support Vector Machines. In Proc. ofNAACL 2001, pages 192–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Nakagawa</author>
<author>T Kudoh</author>
<author>Y Matsumoto</author>
</authors>
<title>Unknown word guessing and part-of-speech tagging using support vector machines.</title>
<date>2001</date>
<booktitle>In Proc. of the 6th NLPRS,</booktitle>
<pages>325--331</pages>
<contexts>
<context position="4147" citStr="Nakagawa et al., 2001" startWordPosition="632" endWordPosition="636"> the MEDLINE database. Another reason for low accuracies is that biomedical named entities are essentially hard to recognize using standard feature sets compared with the named entities in newswire articles (Nobata et al., 2000). Thus, we need to employ powerful machine learning techniques which can incorporate various and complex features in a consistent way. Support Vector Machines (SVMs) (Vapnik, 1995) and Maximum Entropy (ME) method (Berger et al., 1996) are powerful learning methods that satisfy such requirements, and are applied successfully to other NLP tasks (Kudo and Matsumoto, 2000; Nakagawa et al., 2001; Ratnaparkhi, 1996). In this paper, we apply Support Vector Machines to biomedical named entity recognition and train them with the GENIA corpus. We formulate the named entity recognition as the classification of each word with context to one of the classes that represent region and named entity’s semantic class. Although there is a previous work that applied SVMs to biomedical named entity task in this formulation (Yamada et al., 2000), their method to construct a classifier using SVMs, one-vs-rest, fails to train a classifier with entire GENIA corpus, since the cost of SVM training is super</context>
<context position="8767" citStr="Nakagawa et al., 2001" startWordPosition="1407" endWordPosition="1411">es, the BIO representation yields 2N + 1 classes, which will be the targets of a classifier. For instance, the following corresponds to the annotation “Number of glucocorticoid receptorsPROTEIN in lymphocytesCELLTYPE and ...”. Number of glucocorticoid receptors O O B-PROTEIN I-PROTEIN in lymphocytes and ... O B-CELLTYPE O ... 3.2 Support Vector Machines Support Vector Machines (SVMs) (Cortes and Vapnik, 1995) are powerful methods for learning a classifier, which have been applied successfully to many NLP tasks such as base phrase chunking (Kudo and Matsumoto, 2000) and part-of-speech tagging (Nakagawa et al., 2001). The SVM constructs a binary classifier that outputs +1 or −1 given a sample vector x E Rn. The decision is based on the separating hyperplane as follows. +1 if w · x + b &gt; 0, wERn,bER, −1 otherwise ⎧ ⎨⎪ ⎪⎩ c(x) = The class for an input x, c(x), is determined by seeing which side of the space separated by the hyperplane, w · x + b = 0, the input lies on. Given a set of labeled training samples {(y1, x1), ··· , (yL, xL)}, xi ∈ Rn, yi ∈ {+1, −1}, the SVM training tries to find the optimal hyperplane, i.e., the hyperplane with the maximum margin. Margin is defined as the distance between the hyp</context>
</contexts>
<marker>Nakagawa, Kudoh, Matsumoto, 2001</marker>
<rawString>T. Nakagawa, T. Kudoh, and Y. Matsumoto. 2001. Unknown word guessing and part-of-speech tagging using support vector machines. In Proc. of the 6th NLPRS, pages 325–331.</rawString>
</citation>
<citation valid="false">
<date>1999</date>
<institution>National Library of Medicine.</institution>
<note>MEDLINE. available at http://www.ncbi.nlm.nih.gov/.</note>
<contexts>
<context position="2979" citStr="(1999)" startWordPosition="448" endWordPosition="448">llowing illustrates biomedical named entity recognition. “Thus, CIITAPROTEZN not only activates the expression of class II genesDNA but recruits another B cell-specific coactivator to increase transcriptional activity of class II promotersDNA in B cellsCELLTYPE.” Machine learning approach has been applied to biomedical named entity recognition (Nobata et al., 1999; Collier et al., 2000; Yamada et al., 2000; Shimpuku, 2002). However, no work has achieved sufficient recognition accuracy. One reason is the lack of annotated corpora for training as is often the case of a new domain. Nobata et al. (1999) and Collier et al. (2000) trained their model with only 100 annotated paper abstracts from the MEDLINE database (National Library of Medicine, 1999), and Yamada et al. (2000) used only 77 annotated paper abstracts. In addition, it is difficult to compare the techniques used in each study because they used a closed and different corpus. To overcome such a situation, the GENIA corpus (Ohta et al., 2002) has been developed, and at this time it is the largest biomedical annotated corpus available to public, containing 670 annotated abstracts of the MEDLINE database. Another reason for low accurac</context>
</contexts>
<marker>1999</marker>
<rawString>National Library of Medicine. 1999. MEDLINE. available at http://www.ncbi.nlm.nih.gov/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Nobata</author>
<author>N Collier</author>
<author>J Tsujii</author>
</authors>
<title>Automatic term identification and classification in biology texts.</title>
<date>1999</date>
<booktitle>In Proc. of the 5th NLPRS,</booktitle>
<pages>369--374</pages>
<contexts>
<context position="2739" citStr="Nobata et al., 1999" startWordPosition="404" endWordPosition="407">g block in such biomedical IE systems. Conceptually, named entity recognition consists of two tasks: identification, which finds the region of a named entity in a text, and classification, which determines the semantic class of that named entity. The following illustrates biomedical named entity recognition. “Thus, CIITAPROTEZN not only activates the expression of class II genesDNA but recruits another B cell-specific coactivator to increase transcriptional activity of class II promotersDNA in B cellsCELLTYPE.” Machine learning approach has been applied to biomedical named entity recognition (Nobata et al., 1999; Collier et al., 2000; Yamada et al., 2000; Shimpuku, 2002). However, no work has achieved sufficient recognition accuracy. One reason is the lack of annotated corpora for training as is often the case of a new domain. Nobata et al. (1999) and Collier et al. (2000) trained their model with only 100 annotated paper abstracts from the MEDLINE database (National Library of Medicine, 1999), and Yamada et al. (2000) used only 77 annotated paper abstracts. In addition, it is difficult to compare the techniques used in each study because they used a closed and different corpus. To overcome such a si</context>
</contexts>
<marker>Nobata, Collier, Tsujii, 1999</marker>
<rawString>C. Nobata, N. Collier, and J. Tsujii. 1999. Automatic term identification and classification in biology texts. In Proc. of the 5th NLPRS, pages 369–374.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Nobata</author>
<author>N Collier</author>
<author>J Tsujii</author>
</authors>
<title>Comparison between tagged corpora for the named entity task.</title>
<date>2000</date>
<booktitle>In Proc. of the Workshop on Comparing Corpora (at ACL’2000),</booktitle>
<pages>20--27</pages>
<contexts>
<context position="3754" citStr="Nobata et al., 2000" startWordPosition="572" endWordPosition="575">Yamada et al. (2000) used only 77 annotated paper abstracts. In addition, it is difficult to compare the techniques used in each study because they used a closed and different corpus. To overcome such a situation, the GENIA corpus (Ohta et al., 2002) has been developed, and at this time it is the largest biomedical annotated corpus available to public, containing 670 annotated abstracts of the MEDLINE database. Another reason for low accuracies is that biomedical named entities are essentially hard to recognize using standard feature sets compared with the named entities in newswire articles (Nobata et al., 2000). Thus, we need to employ powerful machine learning techniques which can incorporate various and complex features in a consistent way. Support Vector Machines (SVMs) (Vapnik, 1995) and Maximum Entropy (ME) method (Berger et al., 1996) are powerful learning methods that satisfy such requirements, and are applied successfully to other NLP tasks (Kudo and Matsumoto, 2000; Nakagawa et al., 2001; Ratnaparkhi, 1996). In this paper, we apply Support Vector Machines to biomedical named entity recognition and train them with the GENIA corpus. We formulate the named entity recognition as the classificat</context>
</contexts>
<marker>Nobata, Collier, Tsujii, 2000</marker>
<rawString>C. Nobata, N. Collier, and J. Tsujii. 2000. Comparison between tagged corpora for the named entity task. In Proc. of the Workshop on Comparing Corpora (at ACL’2000), pages 20– 27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Ohta</author>
<author>Y Yamamoto</author>
<author>T Okazaki</author>
<author>I Uchiyama</author>
<author>T Takagi</author>
</authors>
<title>Automatic construction of knowledge base from biological papers.</title>
<date>1997</date>
<booktitle>In Proc. of the 5th ISMB,</booktitle>
<pages>218--225</pages>
<contexts>
<context position="1884" citStr="Ohta et al., 1997" startWordPosition="277" endWordPosition="280">he accuracy. The proposed new features also contribute to improve the accuracy. We compare our SVMbased recognition system with a system using Maximum Entropy tagging method. 1 Introduction Application of natural language processing (NLP) is now a key research topic in bioinformatics. Since it is practically impossible for a researcher to grasp all of the huge amount of knowledge provided in the form of natural language, e.g., journal papers, there is a strong demand for biomedical information extraction (IE), which extracts knowledge automatically from biomedical papers using NLP techniques (Ohta et al., 1997; Proux et al., 2000; Yakushiji et al., 2001). The process called named entity recognition, which finds entities that fill the information slots, e.g., proteins, DNAs, RNAs, cells etc., in the biomedical context, is an important building block in such biomedical IE systems. Conceptually, named entity recognition consists of two tasks: identification, which finds the region of a named entity in a text, and classification, which determines the semantic class of that named entity. The following illustrates biomedical named entity recognition. “Thus, CIITAPROTEZN not only activates the expression </context>
</contexts>
<marker>Ohta, Yamamoto, Okazaki, Uchiyama, Takagi, 1997</marker>
<rawString>Y. Ohta, Y. Yamamoto, T. Okazaki, I. Uchiyama, and T. Takagi. 1997. Automatic construction of knowledge base from biological papers. In Proc. of the 5th ISMB, pages 218–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Ohta</author>
<author>Y Tateisi</author>
<author>J Kim</author>
<author>H Mima</author>
<author>J Tsujii</author>
</authors>
<title>The GENIA corpus: An annotated research abstract corpus in molecular biology domain.</title>
<date>2002</date>
<booktitle>In Proc. of HLT</booktitle>
<contexts>
<context position="3384" citStr="Ohta et al., 2002" startWordPosition="513" endWordPosition="516">ada et al., 2000; Shimpuku, 2002). However, no work has achieved sufficient recognition accuracy. One reason is the lack of annotated corpora for training as is often the case of a new domain. Nobata et al. (1999) and Collier et al. (2000) trained their model with only 100 annotated paper abstracts from the MEDLINE database (National Library of Medicine, 1999), and Yamada et al. (2000) used only 77 annotated paper abstracts. In addition, it is difficult to compare the techniques used in each study because they used a closed and different corpus. To overcome such a situation, the GENIA corpus (Ohta et al., 2002) has been developed, and at this time it is the largest biomedical annotated corpus available to public, containing 670 annotated abstracts of the MEDLINE database. Another reason for low accuracies is that biomedical named entities are essentially hard to recognize using standard feature sets compared with the named entities in newswire articles (Nobata et al., 2000). Thus, we need to employ powerful machine learning techniques which can incorporate various and complex features in a consistent way. Support Vector Machines (SVMs) (Vapnik, 1995) and Maximum Entropy (ME) method (Berger et al., 1</context>
</contexts>
<marker>Ohta, Tateisi, Kim, Mima, Tsujii, 2002</marker>
<rawString>T. Ohta, Y. Tateisi, J. Kim, H. Mima, and Tsujii J. 2002. The GENIA corpus: An annotated research abstract corpus in molecular biology domain. In Proc. of HLT 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Platt</author>
</authors>
<title>Fast training of support vector machines using sequential minimal optimization.</title>
<date>1998</date>
<booktitle>In Advances in Kernel Methods,</booktitle>
<pages>185--208</pages>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="12697" citStr="Platt, 1998" startWordPosition="2118" endWordPosition="2119">arget classes. one-vs-rest Construct K binary SVMs, each of which determines whether the sample should be classified as class i or as the other classes. The output is the class with the maximum f(x) in Equation 1. pairwise Construct K(K − 1)/2 binary SVMs, each of which determines whether the sample should be classified as class i or as class j. Each binary SVM has one vote, and the output is the class with the maximum votes. Because the SVM training is a quadratic optimization program, its cost is super-linear to the size of the training samples even with the tailored techniques such as SMO (Platt, 1998) and kernel evaluation caching (Joachims, 1998). Let L be the number of the training samples, then the one-vs-rest method takes time in K × OSVM(L). The BIO formulation produces one training sample per word, and the training with the GENIA corpus involves over 100,000 training samples as can be seen from Table 1. Therefore, it is apparent that the one-vsrest method is impractical with the GENIA corpus. On the other hand, if target classes are equally distributed, the pairwise method will take time in K(K− 1)/2 × OS VM(2L/K). This method is worthwhile because each training is much faster, thoug</context>
<context position="23789" citStr="Platt, 1998" startWordPosition="4006" endWordPosition="4007">ng training time acc. time acc. samples (sec.) (F-score) (sec.) (Fscore) 16,000 2,809 37.04 5,581 36.82 32,000 13,614 40.65 9,175 41.36 48,000 21,174 42.44 9,709 42.49 64,000 40,869 42.52 12,502 44.34 96,000 - - 21,922 44.93 128,000 - - 36,846 45.99 named entity recognition. Systems based on the BIO representation may produce an inconsistent class sequence such as “O B-DNA I-RNA O”. We interpret such outputs as follows: once a named entity starts with “B-C” then we interpret that the named entity with class “C” ends only when we see another “B-” or “O-” tag. We have implemented SMO algorithm (Platt, 1998) and techniques described in (Joachims, 1998) for soft margin SVMs in C++ programming language, and implemented support codes for pairwise classification and parallel training in Java programming language. To obtain POS information required for features and class splitting, we used an English POS tagger described in (Kazama et al., 2001). 6.1 Class Splitting Technique First, we show the effect of the class splitting described in Section 5.1. Varying the size of training data, we compared the change in the training time and the accuracy with and without the class splitting. We used a feature se</context>
</contexts>
<marker>Platt, 1998</marker>
<rawString>J. C. Platt. 1998. Fast training of support vector machines using sequential minimal optimization. In Advances in Kernel Methods, pages 185–208. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Platt</author>
</authors>
<title>Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. Advances in Large Margin Classifiers.</title>
<date>1999</date>
<contexts>
<context position="29451" citStr="Platt, 1999" startWordPosition="4967" endWordPosition="4968">he polynomial kernel with d = 1) are outperformed by the ME system. We here discuss two possible reasons. The first is that the SVM system does not use a dynamic decision such as the Viterbi algorithm, while the ME system uses it. To see this, we degrade the ME system so that it predicts the classes deterministically without using the Viterbi algorithm. We found that this system only marks 51.54 in F-score. Thus, it can be said that a dynamic decision is important for this named entity task. However, although a method to convert the outputs of a binary SVM to probabilistic values is proposed (Platt, 1999), the way to obtain meaningful probabilistic values needed in Viterbitype algorithms from the outputs of a multi-class SVM is unknown. Solving this problem is certainly a part of the future work. The second possible reason is that the SVM system in this paper does not use any cut-off or feature truncation method to remove data noise, while the ME system uses a simple feature cut-off method.12 We observed that the ME system without the cut-off only marks 49.11 in 11C, s, r, and d 12Features that occur less than 10 times are removed. F-score. Thus, such a noise reduction method is also important</context>
</contexts>
<marker>Platt, 1999</marker>
<rawString>J. C. Platt. 1999. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. Advances in Large Margin Classifiers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Proux</author>
<author>F Prechenmann</author>
<author>L Julliard</author>
</authors>
<title>A pragmatic information extraction strategy for gathering data on genetic interactions.</title>
<date>2000</date>
<booktitle>In Proc. of the 8th ISMB,</booktitle>
<pages>279--285</pages>
<contexts>
<context position="1904" citStr="Proux et al., 2000" startWordPosition="281" endWordPosition="284">oposed new features also contribute to improve the accuracy. We compare our SVMbased recognition system with a system using Maximum Entropy tagging method. 1 Introduction Application of natural language processing (NLP) is now a key research topic in bioinformatics. Since it is practically impossible for a researcher to grasp all of the huge amount of knowledge provided in the form of natural language, e.g., journal papers, there is a strong demand for biomedical information extraction (IE), which extracts knowledge automatically from biomedical papers using NLP techniques (Ohta et al., 1997; Proux et al., 2000; Yakushiji et al., 2001). The process called named entity recognition, which finds entities that fill the information slots, e.g., proteins, DNAs, RNAs, cells etc., in the biomedical context, is an important building block in such biomedical IE systems. Conceptually, named entity recognition consists of two tasks: identification, which finds the region of a named entity in a text, and classification, which determines the semantic class of that named entity. The following illustrates biomedical named entity recognition. “Thus, CIITAPROTEZN not only activates the expression of class II genesDNA</context>
</contexts>
<marker>Proux, Prechenmann, Julliard, 2000</marker>
<rawString>D. Proux, F. Prechenmann, and L. Julliard. 2000. A pragmatic information extraction strategy for gathering data on genetic interactions. In Proc. of the 8th ISMB, pages 279–285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L A Ramshaw</author>
<author>M P Marcus</author>
</authors>
<title>Text chunking using transformation-based learning.</title>
<date>1995</date>
<booktitle>In Proc. of the 3rd ACL Workshop on Very Large Corpora.</booktitle>
<contexts>
<context position="7483" citStr="Ramshaw and Marcus, 1995" startWordPosition="1188" endWordPosition="1191"> in such expressions are annotated as a dummy class “temp”. Table 1: Basic statistics of the GENIA corpus # of sentences 5,109 # of words 152,216 # of named entities 23,793 # of words in NEs 50,229 # of words not in NEs 101,987 Av. length ofNEs (a) 2.11 (1.40) 3 Named Entity Recognition Using SVMs 3.1 Named Entity Recognition as Classification We formulate the named entity task as the classification of each word with context to one of the classes that represent region information and named entity’s semantic class. Several representations to encode region information are proposed and examined (Ramshaw and Marcus, 1995; Uchimoto et al., 2000; Kudo and Matsumoto, 2001). In this paper, we employ the simplest BIO representation, which is also used in (Yamada et al., 2000). We modify this representation in Section 5.1 in order to accelerate the SVM training. In the BIO representation, the region information is represented as the class prefixes “B-” and “I-”, and a class “O”. B- means that the current word is at the beginning of a named entity, I- means that the current word is in a named entity (but not at the beginning), and O means the word is not in a named entity. For each named entity class C, class B-C an</context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>L. A. Ramshaw and M. P. Marcus. 1995. Text chunking using transformation-based learning. In Proc. of the 3rd ACL Workshop on Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for partof-speech tagging.</title>
<date>1996</date>
<booktitle>In Proc. of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>133--142</pages>
<contexts>
<context position="4167" citStr="Ratnaparkhi, 1996" startWordPosition="637" endWordPosition="638">Another reason for low accuracies is that biomedical named entities are essentially hard to recognize using standard feature sets compared with the named entities in newswire articles (Nobata et al., 2000). Thus, we need to employ powerful machine learning techniques which can incorporate various and complex features in a consistent way. Support Vector Machines (SVMs) (Vapnik, 1995) and Maximum Entropy (ME) method (Berger et al., 1996) are powerful learning methods that satisfy such requirements, and are applied successfully to other NLP tasks (Kudo and Matsumoto, 2000; Nakagawa et al., 2001; Ratnaparkhi, 1996). In this paper, we apply Support Vector Machines to biomedical named entity recognition and train them with the GENIA corpus. We formulate the named entity recognition as the classification of each word with context to one of the classes that represent region and named entity’s semantic class. Although there is a previous work that applied SVMs to biomedical named entity task in this formulation (Yamada et al., 2000), their method to construct a classifier using SVMs, one-vs-rest, fails to train a classifier with entire GENIA corpus, since the cost of SVM training is super-linear to the size </context>
<context position="15265" citStr="Ratnaparkhi, 1996" startWordPosition="2584" endWordPosition="2585">s is c given an input vector x as follows. P(c|x) =i, Z1x)H αfi(c,x) where Z(x) is a normalization constant, and fi(c, x) is a feature function. A feature function is defined in the same way as the features in the SVM learning, except that it includes c in it like f(c, x) = (c is the jth class) ∧ wi,k(x). If x contains previously assigned classes, then the most probable �T class sequence, ˆcT 1 = argmaxc1,··· ,cT t=1 P(ct|xt) is searched by using the Viterbi-type algorithm. We use the maximum entropy tagging method described in (Kazama et al., 2001) for the experiments, which is a variant of (Ratnaparkhi, 1996) modified to use HMM state features. 5 Tuning of SVMs for Biomedical NE Task In the above definitions, k is a relative word position from the word to be classified. A negative value represents a preceding word’s position, and a positive value represents a following word’s position. Note that we assume that the classification proceeds left to right as can be seen in the definition of the preceding class feature. For the SVM classification, we does not use a dynamic argmax-type classification such as the Viterbi algorithm, since it is difficult to define a good comparable value for the confidenc</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>A. Ratnaparkhi. 1996. A maximum entropy model for partof-speech tagging. In Proc. of the Conference on Empirical Methods in Natural Language Processing, pages 133–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Shimpuku</author>
</authors>
<title>A medical/biological term recognizer with a term hidden Markov model incorporating multiple information sources. A master thesis.</title>
<date>2002</date>
<institution>University of Tokyo.</institution>
<contexts>
<context position="2799" citStr="Shimpuku, 2002" startWordPosition="416" endWordPosition="417">y recognition consists of two tasks: identification, which finds the region of a named entity in a text, and classification, which determines the semantic class of that named entity. The following illustrates biomedical named entity recognition. “Thus, CIITAPROTEZN not only activates the expression of class II genesDNA but recruits another B cell-specific coactivator to increase transcriptional activity of class II promotersDNA in B cellsCELLTYPE.” Machine learning approach has been applied to biomedical named entity recognition (Nobata et al., 1999; Collier et al., 2000; Yamada et al., 2000; Shimpuku, 2002). However, no work has achieved sufficient recognition accuracy. One reason is the lack of annotated corpora for training as is often the case of a new domain. Nobata et al. (1999) and Collier et al. (2000) trained their model with only 100 annotated paper abstracts from the MEDLINE database (National Library of Medicine, 1999), and Yamada et al. (2000) used only 77 annotated paper abstracts. In addition, it is difficult to compare the techniques used in each study because they used a closed and different corpus. To overcome such a situation, the GENIA corpus (Ohta et al., 2002) has been devel</context>
<context position="22660" citStr="Shimpuku, 2002" startWordPosition="3819" endWordPosition="3820">test part (80 abstracts; 622 sentences; 18,211 words).6 Texts are tokenized by using Penn Treebank’s tokenizer. An HMM for the HMM state features was trained with raw abstracts of the GENIA corpus (39,116 sentences).7 The number of states is 160. The vocabulary for the word feature is constructed by taking the most frequent 10,000 words from the above raw abstracts, the prefix/suffix/prefix list by taking the most frequent 10,000 prefixes/suffixes/substrings.8 The performance is measured by precision, recall, and F-score, which are the standard measures for the 6Randomly selected set used in (Shimpuku, 2002). We do not use paper titles, while he used. 7These do not include the sentences in the test part. 8These are constructed using the training part to make the comparison with the ME method fair. Table 3: Effect of each feature set assessed by adding/subtracting (F-score). Changes in bold face means positive effect. Table 2: Training time and accuracy with/without the class splitting technique. The number of training samples includes SOS and EOS (special words for the start/end of a sentence). no splitting splitting training time acc. time acc. samples (sec.) (F-score) (sec.) (Fscore) 16,000 2,8</context>
</contexts>
<marker>Shimpuku, 2002</marker>
<rawString>S. Shimpuku. 2002. A medical/biological term recognizer with a term hidden Markov model incorporating multiple information sources. A master thesis. University of Tokyo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Uchimoto</author>
<author>M Murata</author>
<author>Q Ma</author>
<author>H Ozaku</author>
<author>H Isahara</author>
</authors>
<title>Named entity extraction based on a maximum entropy model and transformation rules.</title>
<date>2000</date>
<booktitle>In Proc. of the 38th ACL,</booktitle>
<pages>326--335</pages>
<contexts>
<context position="7506" citStr="Uchimoto et al., 2000" startWordPosition="1192" endWordPosition="1195">nnotated as a dummy class “temp”. Table 1: Basic statistics of the GENIA corpus # of sentences 5,109 # of words 152,216 # of named entities 23,793 # of words in NEs 50,229 # of words not in NEs 101,987 Av. length ofNEs (a) 2.11 (1.40) 3 Named Entity Recognition Using SVMs 3.1 Named Entity Recognition as Classification We formulate the named entity task as the classification of each word with context to one of the classes that represent region information and named entity’s semantic class. Several representations to encode region information are proposed and examined (Ramshaw and Marcus, 1995; Uchimoto et al., 2000; Kudo and Matsumoto, 2001). In this paper, we employ the simplest BIO representation, which is also used in (Yamada et al., 2000). We modify this representation in Section 5.1 in order to accelerate the SVM training. In the BIO representation, the region information is represented as the class prefixes “B-” and “I-”, and a class “O”. B- means that the current word is at the beginning of a named entity, I- means that the current word is in a named entity (but not at the beginning), and O means the word is not in a named entity. For each named entity class C, class B-C and I-C are produced. The</context>
</contexts>
<marker>Uchimoto, Murata, Ma, Ozaku, Isahara, 2000</marker>
<rawString>K. Uchimoto, M. Murata, Q. Ma, H. Ozaku, and H. Isahara. 2000. Named entity extraction based on a maximum entropy model and transformation rules. In Proc. of the 38th ACL, pages 326–335.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="3934" citStr="Vapnik, 1995" startWordPosition="601" endWordPosition="602">overcome such a situation, the GENIA corpus (Ohta et al., 2002) has been developed, and at this time it is the largest biomedical annotated corpus available to public, containing 670 annotated abstracts of the MEDLINE database. Another reason for low accuracies is that biomedical named entities are essentially hard to recognize using standard feature sets compared with the named entities in newswire articles (Nobata et al., 2000). Thus, we need to employ powerful machine learning techniques which can incorporate various and complex features in a consistent way. Support Vector Machines (SVMs) (Vapnik, 1995) and Maximum Entropy (ME) method (Berger et al., 1996) are powerful learning methods that satisfy such requirements, and are applied successfully to other NLP tasks (Kudo and Matsumoto, 2000; Nakagawa et al., 2001; Ratnaparkhi, 1996). In this paper, we apply Support Vector Machines to biomedical named entity recognition and train them with the GENIA corpus. We formulate the named entity recognition as the classification of each word with context to one of the classes that represent region and named entity’s semantic class. Although there is a previous work that applied SVMs to biomedical named</context>
<context position="8557" citStr="Vapnik, 1995" startWordPosition="1375" endWordPosition="1377">is in a named entity (but not at the beginning), and O means the word is not in a named entity. For each named entity class C, class B-C and I-C are produced. Therefore, if we have N named entity classes, the BIO representation yields 2N + 1 classes, which will be the targets of a classifier. For instance, the following corresponds to the annotation “Number of glucocorticoid receptorsPROTEIN in lymphocytesCELLTYPE and ...”. Number of glucocorticoid receptors O O B-PROTEIN I-PROTEIN in lymphocytes and ... O B-CELLTYPE O ... 3.2 Support Vector Machines Support Vector Machines (SVMs) (Cortes and Vapnik, 1995) are powerful methods for learning a classifier, which have been applied successfully to many NLP tasks such as base phrase chunking (Kudo and Matsumoto, 2000) and part-of-speech tagging (Nakagawa et al., 2001). The SVM constructs a binary classifier that outputs +1 or −1 given a sample vector x E Rn. The decision is based on the separating hyperplane as follows. +1 if w · x + b &gt; 0, wERn,bER, −1 otherwise ⎧ ⎨⎪ ⎪⎩ c(x) = The class for an input x, c(x), is determined by seeing which side of the space separated by the hyperplane, w · x + b = 0, the input lies on. Given a set of labeled training </context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>V. Vapnik. 1995. The Nature of Statistical Learning Theory. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Yakushiji</author>
<author>Y Tateisi</author>
<author>Y Miyao</author>
<author>J Tsujii</author>
</authors>
<title>Event extraction from biomedical papers using a full parser.</title>
<date>2001</date>
<booktitle>In Proc. ofPSB</booktitle>
<pages>408--419</pages>
<contexts>
<context position="1929" citStr="Yakushiji et al., 2001" startWordPosition="285" endWordPosition="288">also contribute to improve the accuracy. We compare our SVMbased recognition system with a system using Maximum Entropy tagging method. 1 Introduction Application of natural language processing (NLP) is now a key research topic in bioinformatics. Since it is practically impossible for a researcher to grasp all of the huge amount of knowledge provided in the form of natural language, e.g., journal papers, there is a strong demand for biomedical information extraction (IE), which extracts knowledge automatically from biomedical papers using NLP techniques (Ohta et al., 1997; Proux et al., 2000; Yakushiji et al., 2001). The process called named entity recognition, which finds entities that fill the information slots, e.g., proteins, DNAs, RNAs, cells etc., in the biomedical context, is an important building block in such biomedical IE systems. Conceptually, named entity recognition consists of two tasks: identification, which finds the region of a named entity in a text, and classification, which determines the semantic class of that named entity. The following illustrates biomedical named entity recognition. “Thus, CIITAPROTEZN not only activates the expression of class II genesDNA but recruits another B c</context>
</contexts>
<marker>Yakushiji, Tateisi, Miyao, Tsujii, 2001</marker>
<rawString>A. Yakushiji, Y. Tateisi, Y. Miyao, and J. Tsujii. 2001. Event extraction from biomedical papers using a full parser. In Proc. ofPSB 2001, pages 408–419.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yamada</author>
<author>T Kudo</author>
<author>Y Matsumoto</author>
</authors>
<title>Using substrings for technical term extraction and classification. IPSJ SIGNotes,</title>
<date>2000</date>
<note>(in Japanese).</note>
<contexts>
<context position="2782" citStr="Yamada et al., 2000" startWordPosition="412" endWordPosition="415">eptually, named entity recognition consists of two tasks: identification, which finds the region of a named entity in a text, and classification, which determines the semantic class of that named entity. The following illustrates biomedical named entity recognition. “Thus, CIITAPROTEZN not only activates the expression of class II genesDNA but recruits another B cell-specific coactivator to increase transcriptional activity of class II promotersDNA in B cellsCELLTYPE.” Machine learning approach has been applied to biomedical named entity recognition (Nobata et al., 1999; Collier et al., 2000; Yamada et al., 2000; Shimpuku, 2002). However, no work has achieved sufficient recognition accuracy. One reason is the lack of annotated corpora for training as is often the case of a new domain. Nobata et al. (1999) and Collier et al. (2000) trained their model with only 100 annotated paper abstracts from the MEDLINE database (National Library of Medicine, 1999), and Yamada et al. (2000) used only 77 annotated paper abstracts. In addition, it is difficult to compare the techniques used in each study because they used a closed and different corpus. To overcome such a situation, the GENIA corpus (Ohta et al., 200</context>
<context position="4588" citStr="Yamada et al., 2000" startWordPosition="706" endWordPosition="709">Berger et al., 1996) are powerful learning methods that satisfy such requirements, and are applied successfully to other NLP tasks (Kudo and Matsumoto, 2000; Nakagawa et al., 2001; Ratnaparkhi, 1996). In this paper, we apply Support Vector Machines to biomedical named entity recognition and train them with the GENIA corpus. We formulate the named entity recognition as the classification of each word with context to one of the classes that represent region and named entity’s semantic class. Although there is a previous work that applied SVMs to biomedical named entity task in this formulation (Yamada et al., 2000), their method to construct a classifier using SVMs, one-vs-rest, fails to train a classifier with entire GENIA corpus, since the cost of SVM training is super-linear to the size of training samples. Even with a more feasible method, pairwise (Kreßel, 1998), which is employed in (Kudo and Matsumoto, 2000), we cannot train a classifier in a reasonable time, because we have a large number of samples that belong to the non-entity class in this formulation. To solve this problem, we propose to split the non-entity class to several sub-classes, using part-ofspeech information. We show that this tec</context>
<context position="7636" citStr="Yamada et al., 2000" startWordPosition="1214" endWordPosition="1217">entities 23,793 # of words in NEs 50,229 # of words not in NEs 101,987 Av. length ofNEs (a) 2.11 (1.40) 3 Named Entity Recognition Using SVMs 3.1 Named Entity Recognition as Classification We formulate the named entity task as the classification of each word with context to one of the classes that represent region information and named entity’s semantic class. Several representations to encode region information are proposed and examined (Ramshaw and Marcus, 1995; Uchimoto et al., 2000; Kudo and Matsumoto, 2001). In this paper, we employ the simplest BIO representation, which is also used in (Yamada et al., 2000). We modify this representation in Section 5.1 in order to accelerate the SVM training. In the BIO representation, the region information is represented as the class prefixes “B-” and “I-”, and a class “O”. B- means that the current word is at the beginning of a named entity, I- means that the current word is in a named entity (but not at the beginning), and O means the word is not in a named entity. For each named entity class C, class B-C and I-C are produced. Therefore, if we have N named entity classes, the BIO representation yields 2N + 1 classes, which will be the targets of a classifier</context>
<context position="10834" citStr="Yamada et al., 2000" startWordPosition="1794" endWordPosition="1797">n. Introducing a kernel function means mapping an original input x using (D(x), s.t. (D(xi)·(D(xj) = k(xi, xj) to another, usually a higher dimensional, feature space. We construct the optimal hyperplane in that space. By using kernel functions, we can construct a non-linear separating surface in the original feature space. Fortunately, such non-linear training does not increase the computational cost if the calculation of the kernel function is as cheap as the inner product. A polynomial function defined as (sxi · xj + r)d is popular in applications of SVMs to NLPs (Kudo and Matsumoto, 2000; Yamada et al., 2000; Kudo and Matsumoto, 2001), because it has an intuitively sound interpretation that each dimension of the mapped space is a 3For many real-world problems where the samples may be inseparable, we allow the constraints are broken with some penalty. In the experiments, we use so-called 1-norm soft margin formulation described as: minimize 2||w||2 + C 1 subject to yi(w · xi + b) ≥ 1 − ei, i = 1, · · · , L, ei ≥ 0, i = 1,··· , L. (weighted) conjunction of d features in the original sample. 3.3 Multi-Class SVMs As described above, the standard SVM learning constructs a binary classifier. To make a </context>
</contexts>
<marker>Yamada, Kudo, Matsumoto, 2000</marker>
<rawString>H. Yamada, T. Kudo, and Y. Matsumoto. 2000. Using substrings for technical term extraction and classification. IPSJ SIGNotes, (NL-140):77–84. (in Japanese).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>