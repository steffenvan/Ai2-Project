<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000465">
<title confidence="0.963483">
Statistical Filtering and Subcategorization Frame Acquisition
</title>
<author confidence="0.986477">
Anna Korhonen and Genevieve Gorrell
</author>
<affiliation confidence="0.995315">
Computer Laboratory, University of Cambridge
</affiliation>
<address confidence="0.526744">
Pembroke Street, Cambridge CB2 3QG, UK
</address>
<email confidence="0.940241">
a1k23@c1.cam.ac.uk, genevieve.gorrell@netdecisions.co.uk
</email>
<author confidence="0.993264">
Diana McCarthy
</author>
<affiliation confidence="0.9992265">
School of Cognitive and Computing Sciences
University of Sussex, Brighton, BN1 9QH, UK
</affiliation>
<email confidence="0.992676">
dianam@cogs.susx.ac.uk
</email>
<sectionHeader confidence="0.997335" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999952125">
Research &apos; into the automatic acquisition of
subcategorization frames (scFs) from corpora
is starting to produce large-scale computa-
tional lexicons which include valuable fre-
quency information. However, the accuracy
of the resulting lexicons shows room for im-
provement. One significant source of error
lies in the statistical filtering used by some re-
searchers to remove noise from automatically
acquired subcategorization frames. In this pa-
per, we compare three different approaches to
filtering out spurious hypotheses. Two hy-
pothesis tests perform poorly, compared to
filtering frames on the basis of relative fre-
quency. We discuss reasons for this and con-
sider directions for future research.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998913597014925">
Subcategorization information is vital for suc-
cessful parsing, however, manual develop-
ment of large subcategorized lexicons has
proved difficult because predicates change be-
haviour between sublanguages, domains and
over time. Additionally, manually devel-
oped sucategorization lexicons do not provide
the relative frequency of different SCFS for a
given predicate, essential in a probabilistic ap-
proach.
Over the past years acquiring subcatego-
rization dictionaries from textual corpora has
become increasingly popular. The different
approaches (e.g. Brent, 1991, 1993; Ushioda
et ad., 1993; Briscoe and Carroll, 1997; Man-
ning, 1993; Carroll and Rooth, 1998; Gahl,
1998; Lapata, 1999; Sarkar and Zeman, 2000)
vary largely according to the methods used
and the number of SCFS being extracted. Re-
gardless of this, there is a ceiling on the perfor-
mance of these systems at around 80% token
recall 1.
&apos;Where token recall is the percentage .of SCF to-
kens in a sample of manually analysed text that were
The approaches to extracting SCF informa-
tion from corpora have frequently employed
statistical methods for filtering (e.g. Brent,
1993; Manning 1993; Briscoe and Carroll,
1997; Lapata, 1999). This has been done to
remove the noise that arises when dealing with
naturally occurring data, and from mistakes
made by the SCF acquisition system, for ex-
ample, parser errors.
Filtering is usually done with a hypothe-
sis test, and frequently with a variation of
the binomial filter introduced by Brent (1991,
1993). Hypothesis testing is performed by for-
mulating a null hypothesis, (Ho), which is as-
sumed true unless there is evidence to the con-
trary. If there is evidence to the contrary,
Ho is rejected and the alternative hypothe-
sis (H1) is accepted. In SCF acquisition, Ho is
that there is no association between a particu-
lar verb (verbj) and a SCF (SCFi), meanwhile
H1 is that there is such an association. For
SCF acquisition, the test is one-tailed since H1
states the direction of the association, a pos-
itive correlation between verbj and sch. We
compare the expected probability of sefi oc-
curring with verbj if Ho is true, to the ob-
served probability of co-occurrence obtained
from the corpus data. If the observed proba-
bility is greater than the expected probability
we reject Ho and accept H1, and if not, we
retain Ho.
Despite the popularity of this method, it
has been reported as problematic. Accord-
ing to one account (Briscoe and Carroll, 1997)
the majority of errors arise because of the sta-
tistical filtering process, which is reported to
be particularly unreliable for low frequency
scFs (Brent, 1993; Briscoe and Carroll, 1997;
Manning, 1993; Manning and Schiitze, 1999).
Lapata (1999) reported that a threshold on
the relative frequencies produced slightly bet-
ter results than those achieved with a Brent-
correctly acquired by the system.
</bodyText>
<page confidence="0.998241">
199
</page>
<bodyText confidence="0.999965666666667">
style binomial filter when establishing scFs for
diathesis alternation detection. Lapata deter-
mined thresholds for each SCF using the fre-
quency of the SCF in COMLEX Syntax dictio-
nary (Grishman et al., 1994).
Adopting the SCF acquisition system of
Briscoe and Carroll, we have experimented
with an alternative hypothesis test, the bi-
nomial log-likelihood ratio (LLR) test (Dun-
ning, 1993). Sarkar and Zeman (2000) have
also used this test when filtering scFs auto-
matically acquired for Czech. This test has
been recommended for use in NLP since it
does not assume a. normal distribution, which
invalidates many other parametric tests for
use with natural language phenomena. LLR
can be used in a form (-2log A) which is
X2 distributed. Moreover, this asymptote is
appropriate at quite low frequencies, which
makes the hypothesis test particularly useful
when dealing with natural language phenom-
ena, where low frequency events are common-
place.
A problem with using hypothesis testing for
filtering automatically acquired SCFS is ob-
taining a good estimation of the expected oc-
currence of scfi with verbi. This is often
performed using the unconditional distribu-
tion, that is the probability distribution over
all SCFS, regardless of the verb. It is as-
sumed that verbi must occur with scfi sig-
nificantly more than is expected given this
estimate. Our paper addresses the problem
that the conditional distribution, dependent
on the verb, and unconditional distribution
are rarely correlated. Therefore statistical fil-
ters which assume such correlation for Ho will
be susceptible to error,
In this paper, we compare the results of
the Brent style binomial filter of Briscoe and
Carroll and the LLR filter to a simple method
which uses a threshold on the relative frequen-
cies of the verb and SCF combinations. We
do this within the framework of the Briscoe
and Carroll SCF acquisition system, which is
described in section 2.1. The details of the
two statistical filters are described in section
2.2, along with the details of the threshold ap-
plied to the relative frequencies output from
the SCF acquisition system. The details of the
experimental evaluation are supplied in sec-
tion 3. We discuss our findings in section 3.3
and conclude with directions for future work
(section 4).
</bodyText>
<sectionHeader confidence="0.992061" genericHeader="introduction">
2 Method
</sectionHeader>
<subsectionHeader confidence="0.989661">
2.1 Framework for SCF Acquisition
</subsectionHeader>
<bodyText confidence="0.999991941176471">
Briscoe and Carroll&apos;s (1997) verbal acquisition
system distinguishes 163 SCFS and returns rel-
ative frequencies for each SCF found for a given
predicate. The scFs are a superset of classes
found in the Alvey NL Tools (ANLT) dictio-
nary, Boguraev et al. (1987) and the COMLEX
Syntax dictionary, Grishman et al. (1994).
They incorporate information about control
of predicative arguments, as well as alterna-
tions such as extraposition and particle move-
ment. The system employs a shallow parser to
obtain the subcategorization information. Po-
tential SCF entries are filtered before the final
SCF lexicon is produced. The filter is the only
component of this system which we experi-
ment with here. The three filtering methods
which we compare are described below.
</bodyText>
<subsectionHeader confidence="0.9559335">
2.2 Filtering Methods
2.2.1 Binomial Hypothesis Test
</subsectionHeader>
<bodyText confidence="0.992549">
Briscoe and Carroll (1997) used a binomial
hypothesis test (BHT) to filter the acquired
SCFS. They applied BHT as follows. The sys-
tem recorded the total number of sets of SCF
cues (n) found for a given predicate, and the
number of these sets for a given SCF (m). The
system estimated the error probability (pe)
that a cue for a SCF (Sch) occurred with a
verb which did not take scfi. pe was esti-
mated in two stages, as shown in equation 1.
Firstly, the number of verbs which are mem-
bers of the target SCF in the ANLT dictionary
were extracted. This number was converted
to a probability of class membership by divid-
ing by the total number of verbs in the dic-
tionary. The complement of this probability
provided an estimate for the probability of a
verb not taking sch. Secondly, this proba-
bility was multiplied by an estimate for the
probability of observing the cue for scfi. This
was estimated using the number of cues for i
extracted from the Susanne corpus (Sampson,
1995), divided by the total number of cues.
pe (1 Iverbs in doss il) Icues f or if
)verbsi ) &apos;cues (1)!
The probability of an event with probability p
happening exactly m times out of n attempts
is given by the following binomial distribution:
</bodyText>
<page confidence="0.825925">
200
</page>
<equation confidence="0.964644666666667">
n! (2)
P(m,n,p) = ml(n mem (1 Prn.
â€”r
</equation>
<bodyText confidence="0.9993355">
The probability of the event happening m or
more times is:
</bodyText>
<equation confidence="0.967309">
P(m+,n,p) = (3)
k=m
Finally, P(m+,n,pe) is the probability that
</equation>
<bodyText confidence="0.999972892857143">
m or more occurrences of cues for scfi will oc-
cur with a verb which is not a member of scfi,
given n occurrences of that verb. A threshold
on this probability, P(m+,n,pe), was set at
less than or equal to 0.05. This yielded a 95%
or better confidence that a high enough pro-
portion of cues for scfi have been observed for
the verb to be legitimately assigned scfi.
Other approaches which use a binomial fil-
ter differ in respect of the calculation of the
error probability. Brent (1993) estimated the
error probabilities for each SCF experimen-
tally from the behaviour of his SCF extrac-
tor, which detected simple morpho-syntactic
cues in the corpus data. Manning (1993) in-
creased the number of available cues at the ex-
pense of the reliability of these cues. To main-
tain high levels of accuracy, Manning applied
higher bounds on the error probabilities for
certain cues. These bounds were determined
experimentally. A similar approach was taken
by Briscoe, Carroll and Korhonen (1997) in a
modification to the Briscoe and Carroll sys-
tem. The overall performance was increased
by changing the estimates of pe according to
the performance of the system for the target
SCF. In the work described here, we use the
original BHT proposed by Briscoe and Carroll.
</bodyText>
<subsubsectionHeader confidence="0.663402">
2.2.2 The Binomial Log Likelihood
</subsubsectionHeader>
<bodyText confidence="0.9914015">
Ratio as a Statistical Filter
Dunning (1993) demonstrates the benefits of
the LLR statistic, compared to Pearson&apos;s chi-
squared, on the task of ranking bigram data.
The binomial log-likelihood ratio test is
simple to calculate. For each verb and SCF
combination four counts are required. These
are the number of times that:
</bodyText>
<listItem confidence="0.978446142857143">
1. the target verb occurs with the target SCF
(k1)
2. the target verb occurs with any other SCF
(ni - ki)
3. any other verb occurs with the target SCF
(k2)
4. any other verb occurs with any other SCF
</listItem>
<equation confidence="0.943195166666667">
(n2 - k2)
The statistic -21ogA is calculated as follows:-
log-likelihood = 2[1ogL(pi,k1,n1)
+logL(p2,k2, n2)
-/ogL(p, k1, n1)
-logL(p,k2,n2)] (4)
</equation>
<bodyText confidence="0.77654">
where
</bodyText>
<equation confidence="0.9532665">
logL(p,n,k) = k xlogp+ (n - k) x log(1 -p)
and
k1 k2 ki + k2
= -7 P2 = P =
ni + nz
ni n2
</equation>
<bodyText confidence="0.9968966875">
The LLR statistic provides a score that re-
flects the difference in (i) the number of bits
it takes to describe the observed data, using
pl = p(scFlverb) and p2 = p(scFl-&apos;verb),
and (ii) the number of bits it takes to de-
scribe the expected data using the probability
p = p(scFlany verb).
The LLR statistic detects differences be-
tween pl and p2. The difference could
potentially be in either direction, but we are
interested in LLRS where pl &gt; p2, i.e. where
there is a positive association between the SCF
and the verb. For these cases, we compared
the value of -21ogA to the threshold value
obtained from Pearson&apos;s Chi-Squared table,
to see if it was significant at the 95% leve12.
</bodyText>
<subsectionHeader confidence="0.845211">
2.2.3 Using a Threshold on the
</subsectionHeader>
<bodyText confidence="0.966055933333333">
Relative Frequencies as a
Baseline
In order to examine the baseline performance
of this system without employing any notion
of the significance of the observations, we
used a threshold on relative frequencies. This
was done by extracting the SCFS, and rank-
ing them in the order of the probability of
their occurrence with the verb. The probabil-
ities were estimated using a maximum likeli-
hood estimate (mLE) from the observed rela-
tive frequencies. A threshold, determined em-
pirically, was applied to these probability esti-
mates to filter out the low probability entries
for each verb.
</bodyText>
<footnote confidence="0.94672">
2See (Gorrell, 1999) for details of this method.
</footnote>
<page confidence="0.996557">
201
</page>
<sectionHeader confidence="0.996237" genericHeader="method">
3 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.964757">
3.1 Method
</subsectionHeader>
<bodyText confidence="0.987661">
To evaluate the different approaches, we took
a sample of 10 million words of the BNC cor-
pus (Leech, 1992). We extracted all sentences
containing an occurrence of one of fourteen
verbs3. The verbs were chosen at random,
subject to the constraint that they exhibited
multiple complementation patterns. After the
extraction process, we retained 3000 citations,
on average, for each verb. The sentences con-
taining these verbs were processed by the SCF
acquisition system, and then we applied the
three filtering methods described above. We
also obtained results for a baseline without
any filtering.
The results were evaluated against a man-
ual analysis of corpus data4. This was ob-
tained by analysing up to a maximum of 300
occurrences for each of the 14 test verbs in
LOB (Garside et al., 1987), Susanne and SEC
(Taylor and Knowles, 1988) corpora. Follow-
ing Briscoe and Carroll (1997), we calculated
precision (percentage of SCFs acquired which
were also exemplified in the manual analysis)
and recall (percentage of the scFs exemplified
in the manual analysis which were acquired
automatically). We also combined precision
and recall into a single measure of overall per-
formance using the F measure (Manning and
Schfitze, 1999).
F2 precision recall
=
precision + recall
</bodyText>
<subsectionHeader confidence="0.738088">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.999792222222222">
Table 1 gives the raw results for the 14 verbs
using each method. It shows the number of
true positives (TP), false positives (FP), and
false negatives (FN), as determined accord-
ing to the manual analysis. The results for
high frequency SCFs (above 0.01 relative fre-
quency), medium frequency (between 0.001
and 0.01) and low frequency (below 0.001)
SCFs are listed respectively in the second,
</bodyText>
<footnote confidence="0.978548454545454">
3These verbs were ask, begin, believe, cause, expect,
find, give, help, like, move, produce, provide, seem,
swing.
4The importance of the manual analysis is outlined
in Briscoe and Carroll (1997). We use the same man-
ual analysis as Briscoe and Carroll, i.e. one from the
Susanne, LOB, and SEC corpora. A manual analysis of
the BNC data might produce better results. However,
since the BNC is a heterogeneous corpus we felt it was
reasonable to test the data on a different corpus, which
is also heterogeneous.
</footnote>
<bodyText confidence="0.999952214285714">
third and fourth columns, and the final col-
umn includes the total results for all frequency
ranges.
Table 2 shows precision and recall for the 14
verbs and the F measure, which combines pre-
cision and recall. We also provide the baseline
results, if all SCFs were accepted.
From the results given in tables 1 and 2, the
MLE approach outperformed both hypothesis
tests. For both BHT and LLR there was an
increase in FNS at high frequencies, and an
increase in FPs at medium and low frequen-
cies, when compared to MLE. The number of
errors was typically larger for LLR than BHT.
The hypothesis tests reduced the number of
FNs at medium and low frequencies, however,
this was countered by the substantial increase
in FPs that they gave. While BHT nearly al-
ways acquired the three most frequent SCFs of
verbs correctly, LLR tended to reject these.
While the high number of FNS can be ex-
plained by reports which have shown LLR to
be over-conservative (Ribas, 1995; Pedersen,
1996), the high number of FPs is surprising.
Although theoretically, the strength of LLR
lies in its suitability for low frequency data,
the results displayed in table 1 do not suggest
that the method performs better than BHT on
low frequency frames.
MLE thresholding produced better results
than the two statistical tests used. Preci-
sion improved considerably, showing that the
classes occurring in the data with the high-
est frequency are often correct. Although MLE
thresholding clearly makes no attempt to solve
the sparse data problem, it performs better
than BUT or LLR overall. MLE is not adept at
finding low frequency SCFs, however, the other
methods are problematic in that they wrongly
accept more than they correctly reject. The
baseline, of accepting all SCFs, obtained a high
recall at the expense of precision.
</bodyText>
<subsectionHeader confidence="0.98852">
3.3 Discussion
</subsectionHeader>
<bodyText confidence="0.999460818181818">
Our results indicate that MLE outperforms
both hypothesis tests. There are two explana-
tions for this, and these are jointly responsible
for the results.
Firstly, the SCF distribution is zipfian, as
are many distributions concerned with nat-
ural language (Manning and Schiitze, 1999).
Figure 1 shows the conditional distribution
for the verb find. This unfiltered SCF prob-
ability distribution was obtained from 20 M
words of BNC data output from the SCF sys-
</bodyText>
<figure confidence="0.431168">
(5)
</figure>
<page confidence="0.991691">
202
</page>
<table confidence="0.9951838">
High Freq Medium Freq Low Freq Totals
TP FP FN TP FP FN TP FP FN TP FP FN
BHT 75 29 23 11 37 31 4 23 15 90 89 69
LLR 66 30 32 9 52 33 2 23 17 77 105 82
MLE 92 31 6 0 0 42 0 0 19 92 31 67
</table>
<tableCaption confidence="0.983911">
Table 1: Raw results for 14 test verbs
</tableCaption>
<table confidence="0.9480605">
Method Precision % Recall % F measure
BHT 50.3 56.6 53.3
LLR 42.3 48.4 45.1
MLE 74.8 57.8 65.2
baseline 24.3 83.5 37.6
0.1 -
</table>
<tableCaption confidence="0.729617">
Table 2: Precision, Recall, and F measure
</tableCaption>
<figure confidence="0.982322666666667">
01
2 0.01 â€”
0.001 7
0.0301 -
â€ž
0.01
0.001
0.0001
to&apos;
10â€¢
10 100
rank
</figure>
<figureCaption confidence="0.895364">
Figure 1: Hypothesised SCF distribution for
find
</figureCaption>
<bodyText confidence="0.994300571428571">
tem. The unconditional distribution obtained
from the observed distribution of SCFs in the
20 M words of BNC is shown in figure 2. The
figures show SCF rank on the X-axis versus
SCF frequency on the Y-axis, using logarith-
mic scales. The line indicates the closest Zipf-
like power law fit to the data.
Secondly, the hypothesis tests make the
false assumption (Ho) that the unconditional
and conditional distributions are correlated.
The fact that a significant improvement in
performance is made by correcting the prior
probabilities according to the performance of
the system (Briscoe, Carroll and Korhonen,
</bodyText>
<figureCaption confidence="0.7773765">
Figure 2: Hypothesised unconditional SCF dis-
tribution
</figureCaption>
<bodyText confidence="0.989456428571429">
1997) suggests the discrepancy between the
unconditional and the conditional distribu-
tions.
We examined the correlation between the
manual analysis for the 14 verbs, and the
unconditional distribution of verb types over
all SCFs estimated from the ANLT using the
Spearman Rank Correlation Coefficient. The
results included in table 3 show that only a
moderate correlation was found averaged over
all verb types.
Both LLR and BHT work by comparing the
observed value of p(scfilvery to that ex-
pected by chance. They both use the observed
</bodyText>
<figure confidence="0.557326">
10 100
rank
</figure>
<page confidence="0.895369">
203
</page>
<table confidence="0.9999205">
Verb Rank Correlation
ask 0.10
begin 0.83
believe 0.77
cause 0.19
expect 0.45
find 0.33
give 0.06
help 0.43
like 0.56
move 0.53
produce 0.95
provide 0.65
seem 0.16
swing 0.50
Average 0.47
</table>
<tableCaption confidence="0.739780333333333">
Table 3: Rank correlation between the condi-
tional SCF distributions of the test verbs and
the unconditional distribution
</tableCaption>
<bodyText confidence="0.983420076923077">
value for p(scfilverbj) from the system&apos;s out-
put, and they both use an estimate for the un-
conditional probability distribution (p(scf))
for estimating the expected probability. They
differ in the way that the estimate for the un-
conditional probability is obtained, and the
way that it is used in hypothesis testing.
For BHT, the null hypothesis is that the ob-
served value of p(scfi &apos;very arose by chance,
because of noise in the data. We estimate the
probability that the value observed could have
arisen by chance using p(m+,n,pe). pe is cal-
culated using:
</bodyText>
<listItem confidence="0.984845571428571">
â€¢ the SCF acquisition system&apos;s raw (unfil-
tered) estimate for the unconditional dis-
tribution, which is obtained from the Su-
sanne corpus and
â€¢ the ANLT estimate of the unconditional
distribution of a verb not taking scfi,
across all SCFs
</listItem>
<bodyText confidence="0.999655779661017">
For LLR, both the conditional (pl) and un-
conditional distributions (p2) are estimated
from the BNC data. The unconditional proba-
bility distribution uses the occurrence of scfi
with any verb other than our target.
The binomial tests look at one point in the
SCF distribution at a time, for a given verb.
The expected value is determined using the
unconditional distribution, on the assumption
that if the null hypothesis is true then this dis-
tribution will correlate with the conditional
distribution. However, this is rarely the case.
Moreover, because of the zipfian nature of
the distributions, the frequency differences at
any point can be substantial. In these exper-
iments, we used one-tailed tests because we
were looking for cases where there was a pos-
itive association between the SCF and verb,
however, in a two-tailed test the null hypoth-
esis would rarely be accepted, because of the
substantial differences in the conditional and
unconditional distributions.
A large number of false negatives occurred
for high frequency SCFs because the probabil-
ity we compared them to was too high. This
probability was estimated from the combina-
tion of many verbs genuinely occurring with
the frame in question, rather than from an es-
timate of background noise from verbs which
did not occur with the frame. We did not use
an estimate from verbs which do not take the
SCF, since this would require a priori knowl-
edge about the phenomena that we were en-
deavouring to acquire automatically. For LLR
the unconditional probability estimate (p2)
was high, simply because this SCF was a com-
mon one, rather than because the data was
particularly noisy. For BHT, pe was likewise
too high as the SCF was also common in the
Susanne data. The ANLT estimate went some-
way to compensating for this, thus we ob-
tained fewer false negatives with BHT than
LLR.
A large number of false positives occurred
for low frequency sCFs because the estimate
for p(scf) was low. This estimate was more
readily exceeded by the conditional estimate.
For BHT false positives arose because of the
low estimate of p(scf) (from Susanne) and
because the estimate of p(,scF) from ANLT
did not compensate enough for this. For LLR,
there was no means to compensate for the fact
that p2 was lower than pl.
In contrast, MLE did not compare two dis-
tributions. Simply rejecting the low frequency
data produced better results overall by avoid-
ing the false positives with the low frequency
data, and the false negatives with the high
frequency data.
</bodyText>
<sectionHeader confidence="0.999535" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.9998338">
This paper explored three possibilities for fil-
tering out the SCF entries produced by a SCF
acquisition system. These were (i) a version
of Brent&apos;s binomial filter, commonly used for
this purpose, (ii) the binomial log-likelihood
</bodyText>
<page confidence="0.994942">
204
</page>
<bodyText confidence="0.9956886">
ratio test, recommended for use with low fre-
quency data and (iii) a simple method using
a threshold on the MLES of the SCFS output
from the system. Surprisingly, the simple MLE
thresholding method worked best. The BHT
and LLR both produced an astounding num-
ber of FPS, particularly at low frequencies.
Further work on handling low frequency
data in SCF acquisition is warranted. A non-
parametric statistical test, such as Fisher&apos;s ex-
act test, recommended by Pedersen (1996),
might improve on the results obtained using
parametric tests. However, it seems from our
experiments that it would be better to avoid
hypothesis tests that make use of the uncon-
ditional distribution.
One possibility is to put more effort into the
estimation of pe, and to avoid use of the un-
conditional distribution for this. In some re-
cent experiments, we tried optimising the es-
timates for pe depending on the performance
of the system for the target SCF, using the
method proposed by Briscoe, Carroll and Ko-
rhonen (1997). The estimates of pe were ob-
tained from a training set separate to the held-
out BNC data used for testing. Results using
the new estimates for pe gave an improvement
of 10% precision and 6% recall, compared to
the BHT results reported here. Nevertheless,
the precision result was 14% worse for preci-
sion than MLE, though there was a 4% im-
provement in recall, making the overall per-
formance 3.9 worse than MLE according to the
F measure. Lapata (1999) also reported that
a simple relative frequency cut off produced
slightly better results than a Brent style BHT.
If MLE thresholding persistently achieves
better results, it would be worth investi-
gating ways of handling the low frequency
data, such as smoothing, for integration with
this method. However, more sophisticated
smoothing methods, which back-off to an un-
conditional distribution, will also suffer from
the lack of correlation between conditional
and unconditional SCF distributions. Any sta-
tistical test would work better at low frequen-
cies than the MLE, since this simply disregards
all low frequency scFs. In our experiments, if
we had used MLE only for the high frequency
data, and BHT for medium and low, then over-
all we would have had 54% precision and 67%
recall. It certainly seems worth employing hy-
pothesis tests which do not rely on the un-
conditional distribution for the low frequency
SCFS.
</bodyText>
<sectionHeader confidence="0.998147" genericHeader="acknowledgments">
5 Acknowledgements
</sectionHeader>
<bodyText confidence="0.99985875">
We thank Ted Briscoe for many helpful dis-
cussions and suggestions concerning this work.
We also acknowledge Yuval Krymolowski for
useful comments on this paper.
</bodyText>
<sectionHeader confidence="0.996102" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999486931818182">
Boguraev, B., Briscoe, E., Carroll, J., Carter,
D. and Grover, C. 1987. The derivation of a
grammatically-indexed lexicon from the Long-
man Dictionary of Contemporary English. In
Proceedings of the 25th Annual Meeting of
the Association for Computational Linguis-
tics, Stanford, CA. 193-200.
Brent, M. 1991. Automatic acquisition of
subcategorization frames from untagged text.
In Proceedings of the 29th Annual Meeting
of the Association for Computational Linguis-
tics, Berkeley, CA. 209-214.
Brent, M. 1993. From grammar to lexicon:
unsupervised learning of lexical syntax. Com-
putational Linguistics 19.3: 243-262.
Briscoe, E.J. and J. Carroll 1997. Automatic
extraction of subcategorization from corpora.
In Proceedings of the 5th ACL Conf. on Ap-
plied Nat. Lg. Proc., Washington, DC. 356-
363.
Briscoe, E., Carroll, J. and Korhonen, A.
1997. Automatic extraction of subcategoriza-
tion frames from corpora - a framework and
3 experiments. &apos;97 Sparkle WP5 Deliverable,
available in http://www.ilc.pi.cnr.it/.
Carroll, G. and Rooth, M. 1998. Valence
induction with a head-lexicalized PCFG. In
Proceedings of the 3rd Conference on Empir-
ical Methods in Natural Language Processing,
Granada, Spain.
Dunning, T. 1993. Accurate methods for the
Statistics of Surprise and Coincidence. Com-
putational Linguistics 19.1: 61-74.
Gahl, S. 1998. Automatic extraction of sub-
corpora based on subcategorization frames
from a part-of-speech tagged corpus. In Pro-
ceedings of the COLING-ACL&apos;98, Montreal,
Canada.
Garside, R., Leech, G. and Sampson, G. 1987.
The computational analysis of English: A
corpus-based approach. Longman, London.
Gorrell, G. 1999. Acquiring Subcategorisation
from Textual Corpora. MPhil dissertation,
University of Cambridge, UK.
</reference>
<page confidence="0.979637">
205
</page>
<reference confidence="0.999748458333333">
Grishman, R., Macleod, C. and Meyers, A.
1994. Comlex syntax: building a computa-
tional lexicon. In Proceedings of the Interna-
tional Conference on Computational Linguis-
tics, COLING-94, Kyoto, Japan. 268-272.
Lapata, M. 1999. Acquiring lexical gener-
alizations from corpora: A case study for
diathesis alternations. In Proceedings of the
37th Annual Meeting of the Association for
Computational Linguistics, Maryland. 397-
404.
Leech, G. 1992. 100 million words of English:
the British National Corpus. Language Re-
search 28(1): 1-13.
Manning, C. 1993. Automatic acquisition of
a large subcategorization dictionary from cor-
pora. In Proceedings of the 31st Annual Meet-
ing of the Association for Computational Lin-
guistics, Columbus, Ohio. 235-242.
Manning, C. and Schiitze, H. 1999. Founda-
tions of Statistical Natural Language Process-
ing. MIT Press, Cambridge MA.
Pedersen, T. 1996. Fishing for Exactness. In
Proceedings of the South-Central SAS Users
Group Conference SCSUG-96, Austin, Texas.
Ribas, F. 1995. On Acquiring Appropriate Se-
lectional Restrictions from Corpora Using a
Semantic Taxonomy. Ph.D thesis, University
of Catalonia.
Sampson, G. 1995. English for the computer.
Oxford University Press, Oxford UK.
Sarkar, A. and Zeman, D. 2000. Auto-
matic Extraction of Sub categorization Frames
for Czech. In Proceedings of the Inter-
national Conference on Computational Lin-
guistics, COLING-00, Saarbrucken, Germany.
691-697.
Taylor, L. and Knowles, G. 1988. Manual
of information to accompany the SEC cor-
pus: the machine-readable corpus of spoken
English. University of Lancaster, UK, Ms.
Ushioda, A., Evans, D., Gibson, T. and
Waibel, A. 1993. The automatic acquisition of
frequencies of verb subcategorization frames
from tagged corpora. In Boguraev, B. and
Pustejovsky, J. eds. SIGLEX ACL Workshop
on the Acquisition of Lexical Knowledge from
Text. Columbus, Ohio: 95-106.
</reference>
<page confidence="0.998894">
206
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.386834">
<title confidence="0.999709">Statistical Filtering and Subcategorization Frame Acquisition</title>
<author confidence="0.998775">Anna Korhonen</author>
<author confidence="0.998775">Genevieve</author>
<affiliation confidence="0.7629805">Computer Laboratory, University of Pembroke Street, Cambridge CB2 3QG,</affiliation>
<email confidence="0.851777">a1k23@c1.cam.ac.uk,genevieve.gorrell@netdecisions.co.uk</email>
<author confidence="0.936021">Diana</author>
<affiliation confidence="0.9780635">School of Cognitive and Computing University of Sussex, Brighton, BN1 9QH,</affiliation>
<email confidence="0.99783">dianam@cogs.susx.ac.uk</email>
<abstract confidence="0.997172588235294">Research &apos; into the automatic acquisition of subcategorization frames (scFs) from corpora is starting to produce large-scale computational lexicons which include valuable frequency information. However, the accuracy of the resulting lexicons shows room for improvement. One significant source of error lies in the statistical filtering used by some researchers to remove noise from automatically acquired subcategorization frames. In this paper, we compare three different approaches to filtering out spurious hypotheses. Two hypothesis tests perform poorly, compared to filtering frames on the basis of relative frequency. We discuss reasons for this and consider directions for future research.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B Boguraev</author>
<author>E Briscoe</author>
<author>J Carroll</author>
<author>D Carter</author>
<author>C Grover</author>
</authors>
<title>The derivation of a grammatically-indexed lexicon from the Longman Dictionary of Contemporary English.</title>
<date>1987</date>
<booktitle>In Proceedings of the 25th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>193--200</pages>
<location>Stanford, CA.</location>
<contexts>
<context position="6512" citStr="Boguraev et al. (1987)" startWordPosition="1022" endWordPosition="1025"> filters are described in section 2.2, along with the details of the threshold applied to the relative frequencies output from the SCF acquisition system. The details of the experimental evaluation are supplied in section 3. We discuss our findings in section 3.3 and conclude with directions for future work (section 4). 2 Method 2.1 Framework for SCF Acquisition Briscoe and Carroll&apos;s (1997) verbal acquisition system distinguishes 163 SCFS and returns relative frequencies for each SCF found for a given predicate. The scFs are a superset of classes found in the Alvey NL Tools (ANLT) dictionary, Boguraev et al. (1987) and the COMLEX Syntax dictionary, Grishman et al. (1994). They incorporate information about control of predicative arguments, as well as alternations such as extraposition and particle movement. The system employs a shallow parser to obtain the subcategorization information. Potential SCF entries are filtered before the final SCF lexicon is produced. The filter is the only component of this system which we experiment with here. The three filtering methods which we compare are described below. 2.2 Filtering Methods 2.2.1 Binomial Hypothesis Test Briscoe and Carroll (1997) used a binomial hypo</context>
</contexts>
<marker>Boguraev, Briscoe, Carroll, Carter, Grover, 1987</marker>
<rawString>Boguraev, B., Briscoe, E., Carroll, J., Carter, D. and Grover, C. 1987. The derivation of a grammatically-indexed lexicon from the Longman Dictionary of Contemporary English. In Proceedings of the 25th Annual Meeting of the Association for Computational Linguistics, Stanford, CA. 193-200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Brent</author>
</authors>
<title>Automatic acquisition of subcategorization frames from untagged text.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>209--214</pages>
<location>Berkeley, CA.</location>
<contexts>
<context position="1645" citStr="Brent, 1991" startWordPosition="221" endWordPosition="222">der directions for future research. 1 Introduction Subcategorization information is vital for successful parsing, however, manual development of large subcategorized lexicons has proved difficult because predicates change behaviour between sublanguages, domains and over time. Additionally, manually developed sucategorization lexicons do not provide the relative frequency of different SCFS for a given predicate, essential in a probabilistic approach. Over the past years acquiring subcategorization dictionaries from textual corpora has become increasingly popular. The different approaches (e.g. Brent, 1991, 1993; Ushioda et ad., 1993; Briscoe and Carroll, 1997; Manning, 1993; Carroll and Rooth, 1998; Gahl, 1998; Lapata, 1999; Sarkar and Zeman, 2000) vary largely according to the methods used and the number of SCFS being extracted. Regardless of this, there is a ceiling on the performance of these systems at around 80% token recall 1. &apos;Where token recall is the percentage .of SCF tokens in a sample of manually analysed text that were The approaches to extracting SCF information from corpora have frequently employed statistical methods for filtering (e.g. Brent, 1993; Manning 1993; Briscoe and Ca</context>
</contexts>
<marker>Brent, 1991</marker>
<rawString>Brent, M. 1991. Automatic acquisition of subcategorization frames from untagged text. In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, Berkeley, CA. 209-214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Brent</author>
</authors>
<title>From grammar to lexicon: unsupervised learning of lexical syntax.</title>
<date>1993</date>
<journal>Computational Linguistics</journal>
<volume>19</volume>
<pages>243--262</pages>
<contexts>
<context position="2215" citStr="Brent, 1993" startWordPosition="318" endWordPosition="319">he different approaches (e.g. Brent, 1991, 1993; Ushioda et ad., 1993; Briscoe and Carroll, 1997; Manning, 1993; Carroll and Rooth, 1998; Gahl, 1998; Lapata, 1999; Sarkar and Zeman, 2000) vary largely according to the methods used and the number of SCFS being extracted. Regardless of this, there is a ceiling on the performance of these systems at around 80% token recall 1. &apos;Where token recall is the percentage .of SCF tokens in a sample of manually analysed text that were The approaches to extracting SCF information from corpora have frequently employed statistical methods for filtering (e.g. Brent, 1993; Manning 1993; Briscoe and Carroll, 1997; Lapata, 1999). This has been done to remove the noise that arises when dealing with naturally occurring data, and from mistakes made by the SCF acquisition system, for example, parser errors. Filtering is usually done with a hypothesis test, and frequently with a variation of the binomial filter introduced by Brent (1991, 1993). Hypothesis testing is performed by formulating a null hypothesis, (Ho), which is assumed true unless there is evidence to the contrary. If there is evidence to the contrary, Ho is rejected and the alternative hypothesis (H1) i</context>
<context position="3691" citStr="Brent, 1993" startWordPosition="569" endWordPosition="570"> positive correlation between verbj and sch. We compare the expected probability of sefi occurring with verbj if Ho is true, to the observed probability of co-occurrence obtained from the corpus data. If the observed probability is greater than the expected probability we reject Ho and accept H1, and if not, we retain Ho. Despite the popularity of this method, it has been reported as problematic. According to one account (Briscoe and Carroll, 1997) the majority of errors arise because of the statistical filtering process, which is reported to be particularly unreliable for low frequency scFs (Brent, 1993; Briscoe and Carroll, 1997; Manning, 1993; Manning and Schiitze, 1999). Lapata (1999) reported that a threshold on the relative frequencies produced slightly better results than those achieved with a Brentcorrectly acquired by the system. 199 style binomial filter when establishing scFs for diathesis alternation detection. Lapata determined thresholds for each SCF using the frequency of the SCF in COMLEX Syntax dictionary (Grishman et al., 1994). Adopting the SCF acquisition system of Briscoe and Carroll, we have experimented with an alternative hypothesis test, the binomial log-likelihood ra</context>
<context position="8929" citStr="Brent (1993)" startWordPosition="1452" endWordPosition="1453">. â€”r The probability of the event happening m or more times is: P(m+,n,p) = (3) k=m Finally, P(m+,n,pe) is the probability that m or more occurrences of cues for scfi will occur with a verb which is not a member of scfi, given n occurrences of that verb. A threshold on this probability, P(m+,n,pe), was set at less than or equal to 0.05. This yielded a 95% or better confidence that a high enough proportion of cues for scfi have been observed for the verb to be legitimately assigned scfi. Other approaches which use a binomial filter differ in respect of the calculation of the error probability. Brent (1993) estimated the error probabilities for each SCF experimentally from the behaviour of his SCF extractor, which detected simple morpho-syntactic cues in the corpus data. Manning (1993) increased the number of available cues at the expense of the reliability of these cues. To maintain high levels of accuracy, Manning applied higher bounds on the error probabilities for certain cues. These bounds were determined experimentally. A similar approach was taken by Briscoe, Carroll and Korhonen (1997) in a modification to the Briscoe and Carroll system. The overall performance was increased by changing </context>
</contexts>
<marker>Brent, 1993</marker>
<rawString>Brent, M. 1993. From grammar to lexicon: unsupervised learning of lexical syntax. Computational Linguistics 19.3: 243-262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E J Briscoe</author>
<author>J Carroll</author>
</authors>
<title>Automatic extraction of subcategorization from corpora.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th ACL Conf. on Applied Nat. Lg. Proc.,</booktitle>
<pages>356--363</pages>
<location>Washington, DC.</location>
<contexts>
<context position="1700" citStr="Briscoe and Carroll, 1997" startWordPosition="228" endWordPosition="231">roduction Subcategorization information is vital for successful parsing, however, manual development of large subcategorized lexicons has proved difficult because predicates change behaviour between sublanguages, domains and over time. Additionally, manually developed sucategorization lexicons do not provide the relative frequency of different SCFS for a given predicate, essential in a probabilistic approach. Over the past years acquiring subcategorization dictionaries from textual corpora has become increasingly popular. The different approaches (e.g. Brent, 1991, 1993; Ushioda et ad., 1993; Briscoe and Carroll, 1997; Manning, 1993; Carroll and Rooth, 1998; Gahl, 1998; Lapata, 1999; Sarkar and Zeman, 2000) vary largely according to the methods used and the number of SCFS being extracted. Regardless of this, there is a ceiling on the performance of these systems at around 80% token recall 1. &apos;Where token recall is the percentage .of SCF tokens in a sample of manually analysed text that were The approaches to extracting SCF information from corpora have frequently employed statistical methods for filtering (e.g. Brent, 1993; Manning 1993; Briscoe and Carroll, 1997; Lapata, 1999). This has been done to remov</context>
<context position="3532" citStr="Briscoe and Carroll, 1997" startWordPosition="542" endWordPosition="545">b (verbj) and a SCF (SCFi), meanwhile H1 is that there is such an association. For SCF acquisition, the test is one-tailed since H1 states the direction of the association, a positive correlation between verbj and sch. We compare the expected probability of sefi occurring with verbj if Ho is true, to the observed probability of co-occurrence obtained from the corpus data. If the observed probability is greater than the expected probability we reject Ho and accept H1, and if not, we retain Ho. Despite the popularity of this method, it has been reported as problematic. According to one account (Briscoe and Carroll, 1997) the majority of errors arise because of the statistical filtering process, which is reported to be particularly unreliable for low frequency scFs (Brent, 1993; Briscoe and Carroll, 1997; Manning, 1993; Manning and Schiitze, 1999). Lapata (1999) reported that a threshold on the relative frequencies produced slightly better results than those achieved with a Brentcorrectly acquired by the system. 199 style binomial filter when establishing scFs for diathesis alternation detection. Lapata determined thresholds for each SCF using the frequency of the SCF in COMLEX Syntax dictionary (Grishman et a</context>
<context position="7091" citStr="Briscoe and Carroll (1997)" startWordPosition="1111" endWordPosition="1114">Tools (ANLT) dictionary, Boguraev et al. (1987) and the COMLEX Syntax dictionary, Grishman et al. (1994). They incorporate information about control of predicative arguments, as well as alternations such as extraposition and particle movement. The system employs a shallow parser to obtain the subcategorization information. Potential SCF entries are filtered before the final SCF lexicon is produced. The filter is the only component of this system which we experiment with here. The three filtering methods which we compare are described below. 2.2 Filtering Methods 2.2.1 Binomial Hypothesis Test Briscoe and Carroll (1997) used a binomial hypothesis test (BHT) to filter the acquired SCFS. They applied BHT as follows. The system recorded the total number of sets of SCF cues (n) found for a given predicate, and the number of these sets for a given SCF (m). The system estimated the error probability (pe) that a cue for a SCF (Sch) occurred with a verb which did not take scfi. pe was estimated in two stages, as shown in equation 1. Firstly, the number of verbs which are members of the target SCF in the ANLT dictionary were extracted. This number was converted to a probability of class membership by dividing by the </context>
<context position="12750" citStr="Briscoe and Carroll (1997)" startWordPosition="2104" endWordPosition="2107">exhibited multiple complementation patterns. After the extraction process, we retained 3000 citations, on average, for each verb. The sentences containing these verbs were processed by the SCF acquisition system, and then we applied the three filtering methods described above. We also obtained results for a baseline without any filtering. The results were evaluated against a manual analysis of corpus data4. This was obtained by analysing up to a maximum of 300 occurrences for each of the 14 test verbs in LOB (Garside et al., 1987), Susanne and SEC (Taylor and Knowles, 1988) corpora. Following Briscoe and Carroll (1997), we calculated precision (percentage of SCFs acquired which were also exemplified in the manual analysis) and recall (percentage of the scFs exemplified in the manual analysis which were acquired automatically). We also combined precision and recall into a single measure of overall performance using the F measure (Manning and Schfitze, 1999). F2 precision recall = precision + recall 3.2 Results Table 1 gives the raw results for the 14 verbs using each method. It shows the number of true positives (TP), false positives (FP), and false negatives (FN), as determined according to the manual analy</context>
</contexts>
<marker>Briscoe, Carroll, 1997</marker>
<rawString>Briscoe, E.J. and J. Carroll 1997. Automatic extraction of subcategorization from corpora. In Proceedings of the 5th ACL Conf. on Applied Nat. Lg. Proc., Washington, DC. 356-363.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Briscoe</author>
<author>J Carroll</author>
<author>A Korhonen</author>
</authors>
<title>Automatic extraction of subcategorization frames from corpora - a framework and 3 experiments. &apos;97 Sparkle WP5 Deliverable, available in http://www.ilc.pi.cnr.it/.</title>
<date>1997</date>
<marker>Briscoe, Carroll, Korhonen, 1997</marker>
<rawString>Briscoe, E., Carroll, J. and Korhonen, A. 1997. Automatic extraction of subcategorization frames from corpora - a framework and 3 experiments. &apos;97 Sparkle WP5 Deliverable, available in http://www.ilc.pi.cnr.it/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Carroll</author>
<author>M Rooth</author>
</authors>
<title>Valence induction with a head-lexicalized PCFG.</title>
<date>1998</date>
<booktitle>In Proceedings of the 3rd Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Granada,</location>
<contexts>
<context position="1740" citStr="Carroll and Rooth, 1998" startWordPosition="235" endWordPosition="238"> vital for successful parsing, however, manual development of large subcategorized lexicons has proved difficult because predicates change behaviour between sublanguages, domains and over time. Additionally, manually developed sucategorization lexicons do not provide the relative frequency of different SCFS for a given predicate, essential in a probabilistic approach. Over the past years acquiring subcategorization dictionaries from textual corpora has become increasingly popular. The different approaches (e.g. Brent, 1991, 1993; Ushioda et ad., 1993; Briscoe and Carroll, 1997; Manning, 1993; Carroll and Rooth, 1998; Gahl, 1998; Lapata, 1999; Sarkar and Zeman, 2000) vary largely according to the methods used and the number of SCFS being extracted. Regardless of this, there is a ceiling on the performance of these systems at around 80% token recall 1. &apos;Where token recall is the percentage .of SCF tokens in a sample of manually analysed text that were The approaches to extracting SCF information from corpora have frequently employed statistical methods for filtering (e.g. Brent, 1993; Manning 1993; Briscoe and Carroll, 1997; Lapata, 1999). This has been done to remove the noise that arises when dealing wit</context>
</contexts>
<marker>Carroll, Rooth, 1998</marker>
<rawString>Carroll, G. and Rooth, M. 1998. Valence induction with a head-lexicalized PCFG. In Proceedings of the 3rd Conference on Empirical Methods in Natural Language Processing, Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Dunning</author>
</authors>
<title>Accurate methods for the Statistics of Surprise and Coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics</journal>
<volume>19</volume>
<pages>61--74</pages>
<contexts>
<context position="4321" citStr="Dunning, 1993" startWordPosition="665" endWordPosition="667">rroll, 1997; Manning, 1993; Manning and Schiitze, 1999). Lapata (1999) reported that a threshold on the relative frequencies produced slightly better results than those achieved with a Brentcorrectly acquired by the system. 199 style binomial filter when establishing scFs for diathesis alternation detection. Lapata determined thresholds for each SCF using the frequency of the SCF in COMLEX Syntax dictionary (Grishman et al., 1994). Adopting the SCF acquisition system of Briscoe and Carroll, we have experimented with an alternative hypothesis test, the binomial log-likelihood ratio (LLR) test (Dunning, 1993). Sarkar and Zeman (2000) have also used this test when filtering scFs automatically acquired for Czech. This test has been recommended for use in NLP since it does not assume a. normal distribution, which invalidates many other parametric tests for use with natural language phenomena. LLR can be used in a form (-2log A) which is X2 distributed. Moreover, this asymptote is appropriate at quite low frequencies, which makes the hypothesis test particularly useful when dealing with natural language phenomena, where low frequency events are commonplace. A problem with using hypothesis testing for </context>
<context position="9775" citStr="Dunning (1993)" startWordPosition="1590" endWordPosition="1591">ense of the reliability of these cues. To maintain high levels of accuracy, Manning applied higher bounds on the error probabilities for certain cues. These bounds were determined experimentally. A similar approach was taken by Briscoe, Carroll and Korhonen (1997) in a modification to the Briscoe and Carroll system. The overall performance was increased by changing the estimates of pe according to the performance of the system for the target SCF. In the work described here, we use the original BHT proposed by Briscoe and Carroll. 2.2.2 The Binomial Log Likelihood Ratio as a Statistical Filter Dunning (1993) demonstrates the benefits of the LLR statistic, compared to Pearson&apos;s chisquared, on the task of ranking bigram data. The binomial log-likelihood ratio test is simple to calculate. For each verb and SCF combination four counts are required. These are the number of times that: 1. the target verb occurs with the target SCF (k1) 2. the target verb occurs with any other SCF (ni - ki) 3. any other verb occurs with the target SCF (k2) 4. any other verb occurs with any other SCF (n2 - k2) The statistic -21ogA is calculated as follows:- log-likelihood = 2[1ogL(pi,k1,n1) +logL(p2,k2, n2) -/ogL(p, k1, </context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>Dunning, T. 1993. Accurate methods for the Statistics of Surprise and Coincidence. Computational Linguistics 19.1: 61-74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Gahl</author>
</authors>
<title>Automatic extraction of subcorpora based on subcategorization frames from a part-of-speech tagged corpus.</title>
<date>1998</date>
<booktitle>In Proceedings of the COLING-ACL&apos;98,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="1752" citStr="Gahl, 1998" startWordPosition="239" endWordPosition="240">sing, however, manual development of large subcategorized lexicons has proved difficult because predicates change behaviour between sublanguages, domains and over time. Additionally, manually developed sucategorization lexicons do not provide the relative frequency of different SCFS for a given predicate, essential in a probabilistic approach. Over the past years acquiring subcategorization dictionaries from textual corpora has become increasingly popular. The different approaches (e.g. Brent, 1991, 1993; Ushioda et ad., 1993; Briscoe and Carroll, 1997; Manning, 1993; Carroll and Rooth, 1998; Gahl, 1998; Lapata, 1999; Sarkar and Zeman, 2000) vary largely according to the methods used and the number of SCFS being extracted. Regardless of this, there is a ceiling on the performance of these systems at around 80% token recall 1. &apos;Where token recall is the percentage .of SCF tokens in a sample of manually analysed text that were The approaches to extracting SCF information from corpora have frequently employed statistical methods for filtering (e.g. Brent, 1993; Manning 1993; Briscoe and Carroll, 1997; Lapata, 1999). This has been done to remove the noise that arises when dealing with naturally </context>
</contexts>
<marker>Gahl, 1998</marker>
<rawString>Gahl, S. 1998. Automatic extraction of subcorpora based on subcategorization frames from a part-of-speech tagged corpus. In Proceedings of the COLING-ACL&apos;98, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Garside</author>
<author>G Leech</author>
<author>G Sampson</author>
</authors>
<title>The computational analysis of English: A corpus-based approach.</title>
<date>1987</date>
<publisher>Longman,</publisher>
<location>London.</location>
<contexts>
<context position="12660" citStr="Garside et al., 1987" startWordPosition="2090" endWordPosition="2093">ourteen verbs3. The verbs were chosen at random, subject to the constraint that they exhibited multiple complementation patterns. After the extraction process, we retained 3000 citations, on average, for each verb. The sentences containing these verbs were processed by the SCF acquisition system, and then we applied the three filtering methods described above. We also obtained results for a baseline without any filtering. The results were evaluated against a manual analysis of corpus data4. This was obtained by analysing up to a maximum of 300 occurrences for each of the 14 test verbs in LOB (Garside et al., 1987), Susanne and SEC (Taylor and Knowles, 1988) corpora. Following Briscoe and Carroll (1997), we calculated precision (percentage of SCFs acquired which were also exemplified in the manual analysis) and recall (percentage of the scFs exemplified in the manual analysis which were acquired automatically). We also combined precision and recall into a single measure of overall performance using the F measure (Manning and Schfitze, 1999). F2 precision recall = precision + recall 3.2 Results Table 1 gives the raw results for the 14 verbs using each method. It shows the number of true positives (TP), f</context>
</contexts>
<marker>Garside, Leech, Sampson, 1987</marker>
<rawString>Garside, R., Leech, G. and Sampson, G. 1987. The computational analysis of English: A corpus-based approach. Longman, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gorrell</author>
</authors>
<title>Acquiring Subcategorisation from Textual Corpora. MPhil dissertation,</title>
<date>1999</date>
<institution>University of Cambridge, UK.</institution>
<contexts>
<context position="11811" citStr="Gorrell, 1999" startWordPosition="1950" endWordPosition="1951">ld on the Relative Frequencies as a Baseline In order to examine the baseline performance of this system without employing any notion of the significance of the observations, we used a threshold on relative frequencies. This was done by extracting the SCFS, and ranking them in the order of the probability of their occurrence with the verb. The probabilities were estimated using a maximum likelihood estimate (mLE) from the observed relative frequencies. A threshold, determined empirically, was applied to these probability estimates to filter out the low probability entries for each verb. 2See (Gorrell, 1999) for details of this method. 201 3 Evaluation 3.1 Method To evaluate the different approaches, we took a sample of 10 million words of the BNC corpus (Leech, 1992). We extracted all sentences containing an occurrence of one of fourteen verbs3. The verbs were chosen at random, subject to the constraint that they exhibited multiple complementation patterns. After the extraction process, we retained 3000 citations, on average, for each verb. The sentences containing these verbs were processed by the SCF acquisition system, and then we applied the three filtering methods described above. We also o</context>
</contexts>
<marker>Gorrell, 1999</marker>
<rawString>Gorrell, G. 1999. Acquiring Subcategorisation from Textual Corpora. MPhil dissertation, University of Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Grishman</author>
<author>C Macleod</author>
<author>A Meyers</author>
</authors>
<title>Comlex syntax: building a computational lexicon.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics, COLING-94,</booktitle>
<pages>268--272</pages>
<location>Kyoto,</location>
<contexts>
<context position="4141" citStr="Grishman et al., 1994" startWordPosition="637" endWordPosition="640">arroll, 1997) the majority of errors arise because of the statistical filtering process, which is reported to be particularly unreliable for low frequency scFs (Brent, 1993; Briscoe and Carroll, 1997; Manning, 1993; Manning and Schiitze, 1999). Lapata (1999) reported that a threshold on the relative frequencies produced slightly better results than those achieved with a Brentcorrectly acquired by the system. 199 style binomial filter when establishing scFs for diathesis alternation detection. Lapata determined thresholds for each SCF using the frequency of the SCF in COMLEX Syntax dictionary (Grishman et al., 1994). Adopting the SCF acquisition system of Briscoe and Carroll, we have experimented with an alternative hypothesis test, the binomial log-likelihood ratio (LLR) test (Dunning, 1993). Sarkar and Zeman (2000) have also used this test when filtering scFs automatically acquired for Czech. This test has been recommended for use in NLP since it does not assume a. normal distribution, which invalidates many other parametric tests for use with natural language phenomena. LLR can be used in a form (-2log A) which is X2 distributed. Moreover, this asymptote is appropriate at quite low frequencies, which </context>
<context position="6569" citStr="Grishman et al. (1994)" startWordPosition="1031" endWordPosition="1034">ails of the threshold applied to the relative frequencies output from the SCF acquisition system. The details of the experimental evaluation are supplied in section 3. We discuss our findings in section 3.3 and conclude with directions for future work (section 4). 2 Method 2.1 Framework for SCF Acquisition Briscoe and Carroll&apos;s (1997) verbal acquisition system distinguishes 163 SCFS and returns relative frequencies for each SCF found for a given predicate. The scFs are a superset of classes found in the Alvey NL Tools (ANLT) dictionary, Boguraev et al. (1987) and the COMLEX Syntax dictionary, Grishman et al. (1994). They incorporate information about control of predicative arguments, as well as alternations such as extraposition and particle movement. The system employs a shallow parser to obtain the subcategorization information. Potential SCF entries are filtered before the final SCF lexicon is produced. The filter is the only component of this system which we experiment with here. The three filtering methods which we compare are described below. 2.2 Filtering Methods 2.2.1 Binomial Hypothesis Test Briscoe and Carroll (1997) used a binomial hypothesis test (BHT) to filter the acquired SCFS. They appli</context>
</contexts>
<marker>Grishman, Macleod, Meyers, 1994</marker>
<rawString>Grishman, R., Macleod, C. and Meyers, A. 1994. Comlex syntax: building a computational lexicon. In Proceedings of the International Conference on Computational Linguistics, COLING-94, Kyoto, Japan. 268-272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lapata</author>
</authors>
<title>Acquiring lexical generalizations from corpora: A case study for diathesis alternations.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>397--404</pages>
<location>Maryland.</location>
<contexts>
<context position="1766" citStr="Lapata, 1999" startWordPosition="241" endWordPosition="242">r, manual development of large subcategorized lexicons has proved difficult because predicates change behaviour between sublanguages, domains and over time. Additionally, manually developed sucategorization lexicons do not provide the relative frequency of different SCFS for a given predicate, essential in a probabilistic approach. Over the past years acquiring subcategorization dictionaries from textual corpora has become increasingly popular. The different approaches (e.g. Brent, 1991, 1993; Ushioda et ad., 1993; Briscoe and Carroll, 1997; Manning, 1993; Carroll and Rooth, 1998; Gahl, 1998; Lapata, 1999; Sarkar and Zeman, 2000) vary largely according to the methods used and the number of SCFS being extracted. Regardless of this, there is a ceiling on the performance of these systems at around 80% token recall 1. &apos;Where token recall is the percentage .of SCF tokens in a sample of manually analysed text that were The approaches to extracting SCF information from corpora have frequently employed statistical methods for filtering (e.g. Brent, 1993; Manning 1993; Briscoe and Carroll, 1997; Lapata, 1999). This has been done to remove the noise that arises when dealing with naturally occurring data</context>
<context position="3777" citStr="Lapata (1999)" startWordPosition="581" endWordPosition="582">sefi occurring with verbj if Ho is true, to the observed probability of co-occurrence obtained from the corpus data. If the observed probability is greater than the expected probability we reject Ho and accept H1, and if not, we retain Ho. Despite the popularity of this method, it has been reported as problematic. According to one account (Briscoe and Carroll, 1997) the majority of errors arise because of the statistical filtering process, which is reported to be particularly unreliable for low frequency scFs (Brent, 1993; Briscoe and Carroll, 1997; Manning, 1993; Manning and Schiitze, 1999). Lapata (1999) reported that a threshold on the relative frequencies produced slightly better results than those achieved with a Brentcorrectly acquired by the system. 199 style binomial filter when establishing scFs for diathesis alternation detection. Lapata determined thresholds for each SCF using the frequency of the SCF in COMLEX Syntax dictionary (Grishman et al., 1994). Adopting the SCF acquisition system of Briscoe and Carroll, we have experimented with an alternative hypothesis test, the binomial log-likelihood ratio (LLR) test (Dunning, 1993). Sarkar and Zeman (2000) have also used this test when </context>
<context position="23344" citStr="Lapata (1999)" startWordPosition="3912" endWordPosition="3913">ed optimising the estimates for pe depending on the performance of the system for the target SCF, using the method proposed by Briscoe, Carroll and Korhonen (1997). The estimates of pe were obtained from a training set separate to the heldout BNC data used for testing. Results using the new estimates for pe gave an improvement of 10% precision and 6% recall, compared to the BHT results reported here. Nevertheless, the precision result was 14% worse for precision than MLE, though there was a 4% improvement in recall, making the overall performance 3.9 worse than MLE according to the F measure. Lapata (1999) also reported that a simple relative frequency cut off produced slightly better results than a Brent style BHT. If MLE thresholding persistently achieves better results, it would be worth investigating ways of handling the low frequency data, such as smoothing, for integration with this method. However, more sophisticated smoothing methods, which back-off to an unconditional distribution, will also suffer from the lack of correlation between conditional and unconditional SCF distributions. Any statistical test would work better at low frequencies than the MLE, since this simply disregards all</context>
</contexts>
<marker>Lapata, 1999</marker>
<rawString>Lapata, M. 1999. Acquiring lexical generalizations from corpora: A case study for diathesis alternations. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, Maryland. 397-404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Leech</author>
</authors>
<title>100 million words of English:</title>
<date>1992</date>
<journal>the British National Corpus. Language Research</journal>
<volume>28</volume>
<issue>1</issue>
<pages>1--13</pages>
<contexts>
<context position="11974" citStr="Leech, 1992" startWordPosition="1980" endWordPosition="1981">rvations, we used a threshold on relative frequencies. This was done by extracting the SCFS, and ranking them in the order of the probability of their occurrence with the verb. The probabilities were estimated using a maximum likelihood estimate (mLE) from the observed relative frequencies. A threshold, determined empirically, was applied to these probability estimates to filter out the low probability entries for each verb. 2See (Gorrell, 1999) for details of this method. 201 3 Evaluation 3.1 Method To evaluate the different approaches, we took a sample of 10 million words of the BNC corpus (Leech, 1992). We extracted all sentences containing an occurrence of one of fourteen verbs3. The verbs were chosen at random, subject to the constraint that they exhibited multiple complementation patterns. After the extraction process, we retained 3000 citations, on average, for each verb. The sentences containing these verbs were processed by the SCF acquisition system, and then we applied the three filtering methods described above. We also obtained results for a baseline without any filtering. The results were evaluated against a manual analysis of corpus data4. This was obtained by analysing up to a </context>
</contexts>
<marker>Leech, 1992</marker>
<rawString>Leech, G. 1992. 100 million words of English: the British National Corpus. Language Research 28(1): 1-13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Manning</author>
</authors>
<title>Automatic acquisition of a large subcategorization dictionary from corpora.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>235--242</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="1715" citStr="Manning, 1993" startWordPosition="232" endWordPosition="234"> information is vital for successful parsing, however, manual development of large subcategorized lexicons has proved difficult because predicates change behaviour between sublanguages, domains and over time. Additionally, manually developed sucategorization lexicons do not provide the relative frequency of different SCFS for a given predicate, essential in a probabilistic approach. Over the past years acquiring subcategorization dictionaries from textual corpora has become increasingly popular. The different approaches (e.g. Brent, 1991, 1993; Ushioda et ad., 1993; Briscoe and Carroll, 1997; Manning, 1993; Carroll and Rooth, 1998; Gahl, 1998; Lapata, 1999; Sarkar and Zeman, 2000) vary largely according to the methods used and the number of SCFS being extracted. Regardless of this, there is a ceiling on the performance of these systems at around 80% token recall 1. &apos;Where token recall is the percentage .of SCF tokens in a sample of manually analysed text that were The approaches to extracting SCF information from corpora have frequently employed statistical methods for filtering (e.g. Brent, 1993; Manning 1993; Briscoe and Carroll, 1997; Lapata, 1999). This has been done to remove the noise tha</context>
<context position="3733" citStr="Manning, 1993" startWordPosition="575" endWordPosition="576">sch. We compare the expected probability of sefi occurring with verbj if Ho is true, to the observed probability of co-occurrence obtained from the corpus data. If the observed probability is greater than the expected probability we reject Ho and accept H1, and if not, we retain Ho. Despite the popularity of this method, it has been reported as problematic. According to one account (Briscoe and Carroll, 1997) the majority of errors arise because of the statistical filtering process, which is reported to be particularly unreliable for low frequency scFs (Brent, 1993; Briscoe and Carroll, 1997; Manning, 1993; Manning and Schiitze, 1999). Lapata (1999) reported that a threshold on the relative frequencies produced slightly better results than those achieved with a Brentcorrectly acquired by the system. 199 style binomial filter when establishing scFs for diathesis alternation detection. Lapata determined thresholds for each SCF using the frequency of the SCF in COMLEX Syntax dictionary (Grishman et al., 1994). Adopting the SCF acquisition system of Briscoe and Carroll, we have experimented with an alternative hypothesis test, the binomial log-likelihood ratio (LLR) test (Dunning, 1993). Sarkar and</context>
<context position="9111" citStr="Manning (1993)" startWordPosition="1480" endWordPosition="1481">th a verb which is not a member of scfi, given n occurrences of that verb. A threshold on this probability, P(m+,n,pe), was set at less than or equal to 0.05. This yielded a 95% or better confidence that a high enough proportion of cues for scfi have been observed for the verb to be legitimately assigned scfi. Other approaches which use a binomial filter differ in respect of the calculation of the error probability. Brent (1993) estimated the error probabilities for each SCF experimentally from the behaviour of his SCF extractor, which detected simple morpho-syntactic cues in the corpus data. Manning (1993) increased the number of available cues at the expense of the reliability of these cues. To maintain high levels of accuracy, Manning applied higher bounds on the error probabilities for certain cues. These bounds were determined experimentally. A similar approach was taken by Briscoe, Carroll and Korhonen (1997) in a modification to the Briscoe and Carroll system. The overall performance was increased by changing the estimates of pe according to the performance of the system for the target SCF. In the work described here, we use the original BHT proposed by Briscoe and Carroll. 2.2.2 The Bino</context>
</contexts>
<marker>Manning, 1993</marker>
<rawString>Manning, C. 1993. Automatic acquisition of a large subcategorization dictionary from corpora. In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, Columbus, Ohio. 235-242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Manning</author>
<author>H Schiitze</author>
</authors>
<date>1999</date>
<booktitle>Foundations of Statistical Natural Language Processing.</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge MA.</location>
<contexts>
<context position="3762" citStr="Manning and Schiitze, 1999" startWordPosition="577" endWordPosition="580"> the expected probability of sefi occurring with verbj if Ho is true, to the observed probability of co-occurrence obtained from the corpus data. If the observed probability is greater than the expected probability we reject Ho and accept H1, and if not, we retain Ho. Despite the popularity of this method, it has been reported as problematic. According to one account (Briscoe and Carroll, 1997) the majority of errors arise because of the statistical filtering process, which is reported to be particularly unreliable for low frequency scFs (Brent, 1993; Briscoe and Carroll, 1997; Manning, 1993; Manning and Schiitze, 1999). Lapata (1999) reported that a threshold on the relative frequencies produced slightly better results than those achieved with a Brentcorrectly acquired by the system. 199 style binomial filter when establishing scFs for diathesis alternation detection. Lapata determined thresholds for each SCF using the frequency of the SCF in COMLEX Syntax dictionary (Grishman et al., 1994). Adopting the SCF acquisition system of Briscoe and Carroll, we have experimented with an alternative hypothesis test, the binomial log-likelihood ratio (LLR) test (Dunning, 1993). Sarkar and Zeman (2000) have also used </context>
<context position="16141" citStr="Manning and Schiitze, 1999" startWordPosition="2666" endWordPosition="2669"> no attempt to solve the sparse data problem, it performs better than BUT or LLR overall. MLE is not adept at finding low frequency SCFs, however, the other methods are problematic in that they wrongly accept more than they correctly reject. The baseline, of accepting all SCFs, obtained a high recall at the expense of precision. 3.3 Discussion Our results indicate that MLE outperforms both hypothesis tests. There are two explanations for this, and these are jointly responsible for the results. Firstly, the SCF distribution is zipfian, as are many distributions concerned with natural language (Manning and Schiitze, 1999). Figure 1 shows the conditional distribution for the verb find. This unfiltered SCF probability distribution was obtained from 20 M words of BNC data output from the SCF sys(5) 202 High Freq Medium Freq Low Freq Totals TP FP FN TP FP FN TP FP FN TP FP FN BHT 75 29 23 11 37 31 4 23 15 90 89 69 LLR 66 30 32 9 52 33 2 23 17 77 105 82 MLE 92 31 6 0 0 42 0 0 19 92 31 67 Table 1: Raw results for 14 test verbs Method Precision % Recall % F measure BHT 50.3 56.6 53.3 LLR 42.3 48.4 45.1 MLE 74.8 57.8 65.2 baseline 24.3 83.5 37.6 0.1 - Table 2: Precision, Recall, and F measure 01 2 0.01 â€” 0.001 7 0.030</context>
</contexts>
<marker>Manning, Schiitze, 1999</marker>
<rawString>Manning, C. and Schiitze, H. 1999. Foundations of Statistical Natural Language Processing. MIT Press, Cambridge MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
</authors>
<title>Fishing for Exactness.</title>
<date>1996</date>
<booktitle>In Proceedings of the South-Central SAS Users Group Conference SCSUG-96,</booktitle>
<location>Austin, Texas.</location>
<contexts>
<context position="15025" citStr="Pedersen, 1996" startWordPosition="2490" endWordPosition="2491">is tests. For both BHT and LLR there was an increase in FNS at high frequencies, and an increase in FPs at medium and low frequencies, when compared to MLE. The number of errors was typically larger for LLR than BHT. The hypothesis tests reduced the number of FNs at medium and low frequencies, however, this was countered by the substantial increase in FPs that they gave. While BHT nearly always acquired the three most frequent SCFs of verbs correctly, LLR tended to reject these. While the high number of FNS can be explained by reports which have shown LLR to be over-conservative (Ribas, 1995; Pedersen, 1996), the high number of FPs is surprising. Although theoretically, the strength of LLR lies in its suitability for low frequency data, the results displayed in table 1 do not suggest that the method performs better than BHT on low frequency frames. MLE thresholding produced better results than the two statistical tests used. Precision improved considerably, showing that the classes occurring in the data with the highest frequency are often correct. Although MLE thresholding clearly makes no attempt to solve the sparse data problem, it performs better than BUT or LLR overall. MLE is not adept at f</context>
<context position="22369" citStr="Pedersen (1996)" startWordPosition="3741" endWordPosition="3742"> acquisition system. These were (i) a version of Brent&apos;s binomial filter, commonly used for this purpose, (ii) the binomial log-likelihood 204 ratio test, recommended for use with low frequency data and (iii) a simple method using a threshold on the MLES of the SCFS output from the system. Surprisingly, the simple MLE thresholding method worked best. The BHT and LLR both produced an astounding number of FPS, particularly at low frequencies. Further work on handling low frequency data in SCF acquisition is warranted. A nonparametric statistical test, such as Fisher&apos;s exact test, recommended by Pedersen (1996), might improve on the results obtained using parametric tests. However, it seems from our experiments that it would be better to avoid hypothesis tests that make use of the unconditional distribution. One possibility is to put more effort into the estimation of pe, and to avoid use of the unconditional distribution for this. In some recent experiments, we tried optimising the estimates for pe depending on the performance of the system for the target SCF, using the method proposed by Briscoe, Carroll and Korhonen (1997). The estimates of pe were obtained from a training set separate to the hel</context>
</contexts>
<marker>Pedersen, 1996</marker>
<rawString>Pedersen, T. 1996. Fishing for Exactness. In Proceedings of the South-Central SAS Users Group Conference SCSUG-96, Austin, Texas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Ribas</author>
</authors>
<title>On Acquiring Appropriate Selectional Restrictions from Corpora Using a Semantic Taxonomy. Ph.D thesis,</title>
<date>1995</date>
<institution>University of Catalonia.</institution>
<contexts>
<context position="15008" citStr="Ribas, 1995" startWordPosition="2488" endWordPosition="2489">both hypothesis tests. For both BHT and LLR there was an increase in FNS at high frequencies, and an increase in FPs at medium and low frequencies, when compared to MLE. The number of errors was typically larger for LLR than BHT. The hypothesis tests reduced the number of FNs at medium and low frequencies, however, this was countered by the substantial increase in FPs that they gave. While BHT nearly always acquired the three most frequent SCFs of verbs correctly, LLR tended to reject these. While the high number of FNS can be explained by reports which have shown LLR to be over-conservative (Ribas, 1995; Pedersen, 1996), the high number of FPs is surprising. Although theoretically, the strength of LLR lies in its suitability for low frequency data, the results displayed in table 1 do not suggest that the method performs better than BHT on low frequency frames. MLE thresholding produced better results than the two statistical tests used. Precision improved considerably, showing that the classes occurring in the data with the highest frequency are often correct. Although MLE thresholding clearly makes no attempt to solve the sparse data problem, it performs better than BUT or LLR overall. MLE </context>
</contexts>
<marker>Ribas, 1995</marker>
<rawString>Ribas, F. 1995. On Acquiring Appropriate Selectional Restrictions from Corpora Using a Semantic Taxonomy. Ph.D thesis, University of Catalonia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Sampson</author>
</authors>
<title>English for the computer.</title>
<date>1995</date>
<publisher>Oxford University Press,</publisher>
<location>Oxford</location>
<contexts>
<context position="8041" citStr="Sampson, 1995" startWordPosition="1287" endWordPosition="1288">ch did not take scfi. pe was estimated in two stages, as shown in equation 1. Firstly, the number of verbs which are members of the target SCF in the ANLT dictionary were extracted. This number was converted to a probability of class membership by dividing by the total number of verbs in the dictionary. The complement of this probability provided an estimate for the probability of a verb not taking sch. Secondly, this probability was multiplied by an estimate for the probability of observing the cue for scfi. This was estimated using the number of cues for i extracted from the Susanne corpus (Sampson, 1995), divided by the total number of cues. pe (1 Iverbs in doss il) Icues f or if )verbsi ) &apos;cues (1)! The probability of an event with probability p happening exactly m times out of n attempts is given by the following binomial distribution: 200 n! (2) P(m,n,p) = ml(n mem (1 Prn. â€”r The probability of the event happening m or more times is: P(m+,n,p) = (3) k=m Finally, P(m+,n,pe) is the probability that m or more occurrences of cues for scfi will occur with a verb which is not a member of scfi, given n occurrences of that verb. A threshold on this probability, P(m+,n,pe), was set at less than or </context>
</contexts>
<marker>Sampson, 1995</marker>
<rawString>Sampson, G. 1995. English for the computer. Oxford University Press, Oxford UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Sarkar</author>
<author>D Zeman</author>
</authors>
<title>Automatic Extraction of Sub categorization Frames for Czech.</title>
<date>2000</date>
<booktitle>In Proceedings of the International Conference on Computational Linguistics, COLING-00,</booktitle>
<pages>691--697</pages>
<location>Saarbrucken,</location>
<contexts>
<context position="1791" citStr="Sarkar and Zeman, 2000" startWordPosition="243" endWordPosition="246">lopment of large subcategorized lexicons has proved difficult because predicates change behaviour between sublanguages, domains and over time. Additionally, manually developed sucategorization lexicons do not provide the relative frequency of different SCFS for a given predicate, essential in a probabilistic approach. Over the past years acquiring subcategorization dictionaries from textual corpora has become increasingly popular. The different approaches (e.g. Brent, 1991, 1993; Ushioda et ad., 1993; Briscoe and Carroll, 1997; Manning, 1993; Carroll and Rooth, 1998; Gahl, 1998; Lapata, 1999; Sarkar and Zeman, 2000) vary largely according to the methods used and the number of SCFS being extracted. Regardless of this, there is a ceiling on the performance of these systems at around 80% token recall 1. &apos;Where token recall is the percentage .of SCF tokens in a sample of manually analysed text that were The approaches to extracting SCF information from corpora have frequently employed statistical methods for filtering (e.g. Brent, 1993; Manning 1993; Briscoe and Carroll, 1997; Lapata, 1999). This has been done to remove the noise that arises when dealing with naturally occurring data, and from mistakes made </context>
<context position="4346" citStr="Sarkar and Zeman (2000)" startWordPosition="668" endWordPosition="671">ning, 1993; Manning and Schiitze, 1999). Lapata (1999) reported that a threshold on the relative frequencies produced slightly better results than those achieved with a Brentcorrectly acquired by the system. 199 style binomial filter when establishing scFs for diathesis alternation detection. Lapata determined thresholds for each SCF using the frequency of the SCF in COMLEX Syntax dictionary (Grishman et al., 1994). Adopting the SCF acquisition system of Briscoe and Carroll, we have experimented with an alternative hypothesis test, the binomial log-likelihood ratio (LLR) test (Dunning, 1993). Sarkar and Zeman (2000) have also used this test when filtering scFs automatically acquired for Czech. This test has been recommended for use in NLP since it does not assume a. normal distribution, which invalidates many other parametric tests for use with natural language phenomena. LLR can be used in a form (-2log A) which is X2 distributed. Moreover, this asymptote is appropriate at quite low frequencies, which makes the hypothesis test particularly useful when dealing with natural language phenomena, where low frequency events are commonplace. A problem with using hypothesis testing for filtering automatically a</context>
</contexts>
<marker>Sarkar, Zeman, 2000</marker>
<rawString>Sarkar, A. and Zeman, D. 2000. Automatic Extraction of Sub categorization Frames for Czech. In Proceedings of the International Conference on Computational Linguistics, COLING-00, Saarbrucken, Germany. 691-697.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Taylor</author>
<author>G Knowles</author>
</authors>
<title>Manual of information to accompany the SEC corpus: the machine-readable corpus of spoken English.</title>
<date>1988</date>
<location>University of Lancaster, UK, Ms.</location>
<contexts>
<context position="12704" citStr="Taylor and Knowles, 1988" startWordPosition="2097" endWordPosition="2100"> random, subject to the constraint that they exhibited multiple complementation patterns. After the extraction process, we retained 3000 citations, on average, for each verb. The sentences containing these verbs were processed by the SCF acquisition system, and then we applied the three filtering methods described above. We also obtained results for a baseline without any filtering. The results were evaluated against a manual analysis of corpus data4. This was obtained by analysing up to a maximum of 300 occurrences for each of the 14 test verbs in LOB (Garside et al., 1987), Susanne and SEC (Taylor and Knowles, 1988) corpora. Following Briscoe and Carroll (1997), we calculated precision (percentage of SCFs acquired which were also exemplified in the manual analysis) and recall (percentage of the scFs exemplified in the manual analysis which were acquired automatically). We also combined precision and recall into a single measure of overall performance using the F measure (Manning and Schfitze, 1999). F2 precision recall = precision + recall 3.2 Results Table 1 gives the raw results for the 14 verbs using each method. It shows the number of true positives (TP), false positives (FP), and false negatives (FN</context>
</contexts>
<marker>Taylor, Knowles, 1988</marker>
<rawString>Taylor, L. and Knowles, G. 1988. Manual of information to accompany the SEC corpus: the machine-readable corpus of spoken English. University of Lancaster, UK, Ms.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ushioda</author>
<author>D Evans</author>
<author>T Gibson</author>
<author>A Waibel</author>
</authors>
<title>The automatic acquisition of frequencies of verb subcategorization frames from tagged corpora.</title>
<date>1993</date>
<booktitle>SIGLEX ACL Workshop on the Acquisition of Lexical Knowledge from Text.</booktitle>
<pages>95--106</pages>
<editor>In Boguraev, B. and Pustejovsky, J. eds.</editor>
<location>Columbus, Ohio:</location>
<marker>Ushioda, Evans, Gibson, Waibel, 1993</marker>
<rawString>Ushioda, A., Evans, D., Gibson, T. and Waibel, A. 1993. The automatic acquisition of frequencies of verb subcategorization frames from tagged corpora. In Boguraev, B. and Pustejovsky, J. eds. SIGLEX ACL Workshop on the Acquisition of Lexical Knowledge from Text. Columbus, Ohio: 95-106.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>