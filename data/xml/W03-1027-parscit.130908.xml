<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000027">
<title confidence="0.994845">
Virtual Examples for Text Classification with Support Vector Machines
</title>
<author confidence="0.921859">
Manabu Sassano
</author>
<affiliation confidence="0.884287">
Fujitsu Laboratories Ltd.
</affiliation>
<address confidence="0.9314305">
4-1-1, Kamikodanaka, Nakahara-ku,
Kawasaki 211-8588, Japan
</address>
<email confidence="0.998175">
sassano@jp.fujitsu.com
</email>
<sectionHeader confidence="0.998595" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999902785714286">
We explore how virtual examples (artifi-
cially created examples) improve perfor-
mance of text classification with Support
Vector Machines (SVMs). We propose
techniques to create virtual examples for
text classification based on the assump-
tion that the category of a document is un-
changed even if a small number of words
are added or deleted. We evaluate the pro-
posed methods by Reuters-21758 test set
collection. Experimental results show vir-
tual examples improve the performance of
text classification with SVMs, especially
for small training sets.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999980924242425">
Corpus-based supervised learning is now a stan-
dard approach to achieve high-performance in nat-
ural language processing. However, the weakness
of supervised learning approach is to need an anno-
tated corpus, the size of which is reasonably large.
Even if we have a good supervised-learning method,
we cannot get high-performance without an anno-
tated corpus. The problem is that corpus annota-
tion is labor intensive and very expensive. In or-
der to overcome this, several methods are proposed,
including minimally-supervised learning methods
(e.g., (Yarowsky, 1995; Blum and Mitchell, 1998)),
and active learning methods (e.g., (Thompson et
al., 1999; Sassano, 2002)). The spirit behind these
methods is to utilize precious labeled examples max-
imally.
Another method following the same spirit is one
using virtual examples (artificially created exam-
ples) generated from labeled examples. This method
has been rarely discussed in natural language pro-
cessing. In terms of active learning, Lewis and Gale
(1994) mentioned the use of virtual examples in text
classification. They did not, however, take forward
this approach because it did not seem to be possi-
ble that a classifier created virtual examples of doc-
uments in natural language and then requested a hu-
man teacher to label them.
In the field of pattern recognition, some kind of
virtual examples has been studied. The first re-
port of methods using virtual examples with Sup-
port Vector Machines (SVMs) is that of Sch¨olkopf
et al. (1996), who demonstrated significant improve-
ment of the accuracy in hand-written digit recogni-
tion (Section 3). They created virtual examples from
labeled examples based on prior knowledge of the
task: slightly translated (e.g., 1 pixel shifted to the
right) images have the same label (class) of the orig-
inal image. Niyogi et al. (1998) also discussed the
use of prior knowledge by creating virtual examples
and thereby expanding the effective training set size.
The purpose of this study is to explore the effec-
tiveness of virtual examples in NLP, motivated by
the results of Sch¨olkopf et al. (1996). To our knowl-
edge, use of virtual examples in corpus-based NLP
has never been studied so far. It is, however, im-
portant to investigate this approach by which it is
expected that we can alleviate the cost of corpus an-
notation. In particular, we focus on virtual examples
with Support Vector Machines, introduced by Vap-
nik (1995). The reason for this is that SVM is one of
most successful machine learning methods in NLP.
For example, NL tasks to which SVMs have been
applied are text classification (Joachims, 1998; Du-
mais et al., 1998), chunking (Kudo and Matsumoto,
2001), dependency analysis (Kudo and Matsumoto,
2002) and so forth.
In this study, we choose text classification as a
first case of the study of virtual examples in NLP be-
cause text classification in real world requires mini-
mizing annotation cost, and it is not too complicated
to perform some non-trivial experiments. Moreover,
there are simple methods, which we propose, to gen-
erate virtual examples from labeled examples (Sec-
tion 4). We show how virtual examples can improve
the performance of a classifier with SVM in text
classification, especially for small training sets.
</bodyText>
<sectionHeader confidence="0.941082" genericHeader="method">
2 Support Vector Machines
</sectionHeader>
<bodyText confidence="0.976513">
In this section we give some theoretical definitions
of SVMs. Assume that we are given the training data
</bodyText>
<equation confidence="0.822641">
(xi; yi);::: ; (xl; yl); xi C Rn; yi C {+1; —11:
</equation>
<figureCaption confidence="0.8889995">
Figure 1: Hyperplane (solid) and Support Vectors
problem:
</figureCaption>
<figure confidence="0.673993166666667">
1
maximize Xai —
positive example
negative example
support vector
l
</figure>
<equation confidence="0.94575875">
i=1
2
Xl aiajyiyjK(xi; xj)
i&gt;j=1
</equation>
<bodyText confidence="0.5054405">
The decision function g in SVM framework is de- subject to di : 0 &lt; ai &lt; C and Xl aiyi = 0:
fined as: i=1
</bodyText>
<equation confidence="0.999701">
g(x) = sgn(f(x)) (1)
f(x) = Xl yiaiK(xi; x) + b (2)
i=1
</equation>
<bodyText confidence="0.999310333333333">
where K is a kernel function, b C R is a threshold,
and ai are weights. Besides, the weights oi satisfy
the following constraints:
</bodyText>
<equation confidence="0.9976705">
di : 0 &lt; ai &lt; C and Xl Ceiyi = 0;
i=1
</equation>
<bodyText confidence="0.97779">
where C is a misclassification cost. The vectors xi
with non-zero ai are called Support Vectors. For
linear SVMs, the kernel function K is defined as:
</bodyText>
<equation confidence="0.926395666666667">
K(xi; x) = xi • x:
In this case, Equation 2 can be written as:
f(x) = w • x + b (3)
</equation>
<bodyText confidence="0.999816666666667">
where w = Pli=1 yictiixi. To train an SVM is to
find ai and b by solving the following optimization
The solution gives an optimal hyperplane, which is a
decision boundary between the two classes. Figure 1
illustrates an optimal hyperplane and its support vec-
tors.
</bodyText>
<sectionHeader confidence="0.895318" genericHeader="method">
3 Virtual Examples and Virtual Support
Vectors
</sectionHeader>
<bodyText confidence="0.973332166666667">
Virtual examples are generated from labeled exam-
ples.1 Based on prior knowledge of a target task, the
label of a generated example is set to the same value
as that of the original example.
For example, in hand-written digit recognition,
virtual examples can be created on the assumption
that the label of an example is unchanged even if the
example is shifted by one pixel in the four princi-
pal directions (Sch¨olkopf et al., 1996; DeCoste and
Sch¨olkopf, 2002).
Virtual examples that are generated from support
vectors are called virtual support vectors (Sch¨olkopf
</bodyText>
<footnote confidence="0.982556">
1We discuss here only virtual examples which are generated
from labeled examples. We do not consider examples, the labels
of which are not known.
</footnote>
<figureCaption confidence="0.999326">
Figure 2: Hyperplane and Virtual Examples
</figureCaption>
<bodyText confidence="0.9999805">
et al., 1996). Reasonable virtual support vectors are
expected to give a better optimal hyperplane. As-
suming that virtual support vectors represent natu-
ral variations of examples of a target task, the de-
cision boundary should be more accurate. Figure 2
illustrates the idea of virtual support vectors. Note
that after virtual support vectors are given, the hy-
perplane is different from that in Figure 1.
</bodyText>
<sectionHeader confidence="0.949511" genericHeader="method">
4 Virtual Examples for Text Classification
</sectionHeader>
<bodyText confidence="0.996923157894737">
We assume on text classification the following:
Assumption 1 The category of a document is un-
changed even if a small number of words are added
or deleted.
This assumption is reasonable. In typical cases of
text classification most of the documents usually
contain two or more keywords which may indicate
the categories of the documents.
Following Assumption 1, we propose two meth-
ods to create virtual examples for text classification.
One method is to delete some portion of a document.
The label of a virtual example is given from the orig-
inal document. The other method is to add a small
number of words to a document. The words to be
added are taken from documents, the label of which
is the same as that of the document. Although one
can invent various methods to create virtual exam-
ples based on Assumption 1, we propose here very
simple ones.
</bodyText>
<equation confidence="0.9093715">
Document Id Feature Vector (x) Label (y)
(f1, f2, f3, f4, f5) +1
(f2, f4, f5, f�) +1
(f2, f3, f5, f�, f7) +1
(f1, f3, f8, f9, f10) —1
(f1, f8, f10, f11) —1
</equation>
<tableCaption confidence="0.993599">
Table 1: Example of Document Set
</tableCaption>
<bodyText confidence="0.996488333333333">
Before describing our methods, we describe text
representation which we used in this study. We to-
kenize a document to words, downcase them and
then remove stopwords, where the stopword list of
freeWAIS-sf2 is used. Stemming is not performed.
We adopt binary feature vectors where word fre-
quency is not used.
Now we describe the two proposed methods:
GenerateByDeletion and GenerateByAddition. As-
sume that we are given a feature vector (a document)
x and x&apos; is a generated vector from x. GenerateBy-
Deletion algorithm is:
</bodyText>
<listItem confidence="0.826441">
1. Copy x to x&apos;.
2. For each binary feature f of x&apos;, if rand() &lt;
t then remove the feature f, where rand() is
</listItem>
<bodyText confidence="0.9516525">
a function which generates a random number
from 0 to 1, and t is a parameter to decide how
many features are deleted.
For example, suppose that we have a set of docu-
ments as in Table 1. Some possible virtual examples
generated from Document 1 by GenerateByDeletion
algorithm are (f2, f3, f4, f5, +1), (f1, f3, f4, +1),
or (f1, f2, f4, f5, +1).
On the other hand, GenerateByAddition algo-
rithm is:
</bodyText>
<listItem confidence="0.999185">
1. Collect from a training set documents, the label
of which is the same as that of x.
2. Concatenate all the feature vectors (documents)
to create an array a of features. Each element
of a is a feature which represents a word.
3. Copy x to x&apos;.
</listItem>
<footnote confidence="0.886394">
2Available at http://ls6-www.informatik.uni-dortmund.de/ir/
projects/freeWAIS-sf/
</footnote>
<figure confidence="0.990899285714286">
Virtual
Examples
1
2
3
4
5
</figure>
<table confidence="0.978836818181818">
Category Name Training Test
earn 2877 1087
acq 1650 719
money-fx 538 179
grain 433 149
crude 389 189
trade 369 117
interest 347 131
ship 197 89
wheat 212 71
corn 181 56
</table>
<tableCaption confidence="0.999011">
Table 2: Number of Training and Test Examples
</tableCaption>
<bodyText confidence="0.934755692307692">
4. For each binary feature f of x&apos;, if rand() &lt; t
then select a feature randomly from a and put
it to x1.
For example, when we want to generate a virtual
example from Document 2 in Table 1 by Generate-
ByAddition algorithm, first we create an array a =
(f1, f2, f3, f4, f5, f2, f4, f5, f6, f2, f3, f5, f6, f7).
In this case, some possible virtual examples by
(GenerateByAddition are (/(f1, f2, f4, f5, f6, +1),
(f2, f3, f4, f5, f6, +1), or (f2, f4, f5, f6, f7, +1).
An example such as (f2, f4, f5, f6, f10, +1) is never
generated from Document 2 because there are no
positive documents which have f10.
</bodyText>
<sectionHeader confidence="0.970408" genericHeader="method">
5 Experimental Results and Discussion
</sectionHeader>
<subsectionHeader confidence="0.99853">
5.1 Test Set Collection
</subsectionHeader>
<bodyText confidence="0.9999945">
We used the Reuters-21578 dataset3 to evaluate the
proposed methods. The dataset has several splits of a
training set and a test set. We used here “ModApte”
split, which is most widely used in the literature on
text classification. This split has 9,603 training ex-
amples and 3,299 test examples. More than 100 cat-
egories are in the dataset. We use, however, only the
most frequent 10 categories. Table 2 shows the 10
categories and the number of training and test exam-
ples in each of the categories.
</bodyText>
<subsectionHeader confidence="0.978931">
5.2 Performance Measures
</subsectionHeader>
<bodyText confidence="0.9962505">
We use F-measure (van Rijsbergen, 1979; Lewis
and Gale, 1994) as a primal performance measure
</bodyText>
<footnote confidence="0.989657">
3Available from David D. Lewis’s page: http://
www.daviddlewis.com/resources/testcollections/reuters21578/
</footnote>
<bodyText confidence="0.996301111111111">
to evaluate the result. F-measure is defined as:
where p is precision and q is recall and 0 is a param-
eter which decides the relative weight of precision
and recall. The p and the q are defined as:
number of positive and correct outputs
number of positive outputs
number of positive and correct outputs
number of positive examples
In Equation 4, usually 0 = 1 is used, which means
it gives equal weight to precision and recall.
When we evaluate the performance of a classifier
to a multiple category dataset, there are two ways
to compute F-measure: macro-averaging and micro-
averaging (Yang, 1999). The former way is to first
compute F-measure for each category and then aver-
age them, while the latter way is to first compute pre-
cision and recall for all the categories and use them
to calculate the F-measure.
</bodyText>
<subsectionHeader confidence="0.993619">
5.3 SVM setting
</subsectionHeader>
<bodyText confidence="0.999769">
Through our experiments we used our original SVM
tools, the algorithm of which is based on SMO (Se-
quential Minimal Optimization) by Platt (1999). We
used linear SVMs and set a misclassification cost C
to 0:016541 which is 1/(the average of x • x) where
x is a feature vector in the 9,603 size training set.
For simplicity, we fixed C through all the experi-
ments. We built a binary classifier for each of the 10
categories shown in Table 2.
</bodyText>
<sectionHeader confidence="0.721654" genericHeader="evaluation">
5.4 Results
</sectionHeader>
<bodyText confidence="0.896171307692307">
First, we carried out experiments using GenerateBy-
Deletion and GenerateByAddition separately to cre-
ate virtual examples, where a virtual example was
created per Support Vector. We did not generate
virtual examples from non support vectors. We set
the parameter t to 0:054 for GenerateByDeletion and
GenerateByAddition for all the experiments.
To build an SVM with virtual examples we use
the following steps:
4We first tried t = 0.01, 0.05, and 0.10 with GenerateBy-
Deletion using the 9603 size training set. The value t = 0.05
yielded best micro-average F-measure for the test set. We used
the same value also for GenerateByAddition.
</bodyText>
<equation confidence="0.9462086">
F-measure = (1 + 02)pq
(4)
02p + q
p=
q=
</equation>
<listItem confidence="0.996333833333333">
1. Train an SVM.
2. Extract Support Vectors.
3. Generate virtual examples from the Support
Vectors.
4. Train another SVM using both the original la-
beled examples and the virtual examples.
</listItem>
<bodyText confidence="0.999866888888889">
We evaluated the performance of the two methods
depending on the size of a training set. We created
subsamples by selecting randomly from the 9603
size training set. We prepared seven sizes: 9603,
4802, 2401, 1200, 600, 300, and 150.5 Micro-
average F-measures of the two methods are shown
in Table 3. We see from Table 3 that both the meth-
ods give better performance than that of the origi-
nal SVM. The smaller the number of examples in
the training set is, the larger the gain is. For the
9603 size training set, the gain of GenerateByDele-
tion is 0.75 (= 90.17 — 89.42), while for the 150
size set, the gain is 6.88 (= 60.16 — 53.28). These
results suggest that in the smaller training sets there
are not enough various examples to give a accurate
decision boundary and therefore the effect of virtual
examples is larger at the smaller training sets. It
is reasonable to conclude that GenerateByDeletion
and GenerateByAddition generated good virtual ex-
amples for the task and this led to the performance
gain.
After we found that the simple two methods to
generate virtual support vectors were effective, we
examined a combined method which is to use both
GenerateByDeletion and GenerateByAddition. Two
virtual examples are generated per Support Vector.
The performance of the combined method is also
shown in Table 3. The performance gain of the com-
bined method is larger than that with either Gener-
ateByDeletion or GenerateByAddition.
Furthermore, we carried out another experiment
with a combined method to create two virtual exam-
ples with GenerateByDeletion and GenerateByAd-
dition respectively. That is, four virtual examples
were generated from a Support Vector. The perfor-
mance of that setting is shown in Table 3. The best
</bodyText>
<footnote confidence="0.885692333333333">
5Since we selected samples randomly, some smaller training
sets of low frequent categories may have had few or even zero
positive examples.
</footnote>
<figure confidence="0.891136">
100 1000 10000
Number of Examples in Training Set
</figure>
<figureCaption confidence="0.9973045">
Figure 3: Micro-Average F-Measure versus Number
of Examples in the Training Set
</figureCaption>
<figure confidence="0.98287">
100 1000 10000
Number of Examples in Training Set
</figure>
<figureCaption confidence="0.888773">
Figure 4: Macro-Average F-Measure versus Num-
ber of Examples in the Training Set. For the smaller
training sets F-measures cannot be computed be-
cause the precisions are undefined.
</figureCaption>
<bodyText confidence="0.996013785714286">
result is achieved by the combined method to create
four virtual examples per Support Vector.
For the rest of this section, we limit our discussion
to the comparison of the results of the original SVM
and SVM with four virtual examples per SV (SVM
with 4 VSVs). The learning curves of the original
SVM and SVM with 4 VSVs are shown in Figures 3
and 4. It is clear that SVM with 4 VSVs outper-
forms the original SVM considerably in terms of
both micro-average F-measure and macro-average
F-measure. SVM with 4 VSVs achieves a given
level of performance with roughly half of the labeled
examples which the original SVM requires. One
might suppose that the improvement of F-measure
</bodyText>
<figure confidence="0.97570048">
95
90
85
80
75
70
65
60
55
50
SVM + 4 Virtual SVs Per SV
SVM
Macro-average F-measure (beta = 1)
45
40
85
80
75
70
65
60
55
50
SVM + 4 Virtual SVs per SV
SVM
</figure>
<table confidence="0.93242825">
Micro-average F-measure (beta = 1)
Number of Examples in Training Set
Method 9603 4802 2401 1200 600 300 150
Original SVM 89.42 86.58 81.69 77.24 71.08 64.44 53.28
SVM + 1 VSV per SV (GenerateByDeletion) 90.17 88.62 84.45 81.11 75.32 70.11 60.16
SVM + 1 VSV per SV (GenerateByAddition) 90.00 88.51 84.48 81.14 75.33 69.59 60.04
SVM + 2 VSVs per SV (Combined) 90.27 89.33 86.27 83.59 77.44 72.81 64.22
SVM + 4 VSVs per SV (Combined) 90.45 89.69 87.12 84.97 79.16 73.25 65.05
</table>
<tableCaption confidence="0.999951">
Table 3: Comparison of Micro-Average F-measure of Different Methods. “VSV” means virtual SV.
</tableCaption>
<figure confidence="0.954899">
100 1000 10000
Number of Examples in Training Set
</figure>
<figureCaption confidence="0.994536">
Figure 5: Error Rate versus Number of Examples in
the Training Set
</figureCaption>
<bodyText confidence="0.999389235294118">
is realized simply because the recall gets highly
improved while the error rate increases. We plot
changes of the error rate for 32990 tests (3299 tests
for each of the 10 categories) in Figure 5. SVM with
4 VSVs still outperforms the original SVM signifi-
cantly.6
The performance changes for each of the 10 cat-
egories are shown in Tables 4 and 5. SVM with 4
VSVs is better than the original SVM for almost
all the categories and all the sizes except for “inter-
est” and “wheat” at the 9603 size training set. For
low frequent categories such as “ship”, “wheat” and
“corn”, the classifiers of the original SVM perform
poorly. There are many cases where they never out-
put ‘positive’, i.e. the recall is zero. It suggests that
the original SVM fails to find a good hyperplane due
to the imbalanced training sets which have very few
</bodyText>
<footnote confidence="0.9961554">
6We have done the significance test which is called “p-test”
in (Yang and Liu, 1999), requiring significance at the 0.05 level.
Although at the 9603 size training set the improvement of the
error rate is not statistically significant, in all the other cases the
improvement is significant.
</footnote>
<bodyText confidence="0.885626">
positive examples. In contrast, SVM with 4 VSVs
yields better results for such harder cases.
</bodyText>
<sectionHeader confidence="0.993109" genericHeader="conclusions">
6 Conclusion and Future Directions
</sectionHeader>
<bodyText confidence="0.9999925">
We have explored how virtual examples improve the
performance of text classification with SVMs. For
text classification, we have proposed methods to cre-
ate virtual examples on the assumption that the label
of a document is unchanged even if a small num-
ber of words are added or deleted. The experimen-
tal results have shown that our proposed methods
improve the performance of text classification with
SVMs, especially for small training sets. Although
the proposed methods are not readily applicable to
NLP tasks other than text classification, it is notable
that the use of virtual examples, which has been very
little studied in NLP, is empirically evaluated.
In the future, it would be interesting to employ
virtual examples with methods to use both labeled
and unlabeled examples (e.g., (Blum and Mitchell,
1998; Nigam et al., 1998; Joachims, 1999)). The
combined approach may yield better results with a
small number of labeled examples. Another interest-
ing direction would be to develop methods to create
virtual examples for the other tasks (e.g., named en-
tity recognition, POS tagging, and parsing) in NLP.
We believe we can use prior knowledge on these
tasks to create effective virtual examples.
</bodyText>
<sectionHeader confidence="0.988233" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.345094833333333">
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proceed-
ings of the 11th COLT, pages 92–100.
Dennis DeCoste and Bernhard Scholkopf. 2002. Train-
ing invariant support vector machines. Machine
Learning, 46:161–190.
</reference>
<figure confidence="0.972798666666667">
Error Rate (%)
4
2
6
5
3
1
SVM + 4 Virtual SVs per SV
SVM
</figure>
<table confidence="0.983172928571428">
Number of Examples in the Training Set
Category 9603 4802 2401 1200 600 300 150
earn 98.06 97.49 97.40 96.39 95.94 94.85 93.73
acq 91.94 89.87 84.43 84.01 78.17 63.10 12.03
money-fx 64.90 61.69 56.03 51.69 17.91 01.11 05.38
grain 86.96 81.68 75.20 59.63 41.27 06.49 -
crude 84.59 81.52 67.11 33.33 01.05 - -
trade 74.89 64.58 54.86 40.26 12.80 01.69 -
interest 63.89 60.29 50.27 35.15 08.57 05.88 -
ship 66.19 44.07 32.73 02.22 - - -
wheat 89.61 80.60 38.30 08.11 - - -
corn 84.62 62.79 10.17 - - - -
Macro-average 80.56 72.46 56.65 - - - -
Micro-average 89.42 86.58 81.69 77.24 71.08 64.44 53.28
</table>
<tableCaption confidence="0.808485">
Table 4: F-Measures for the Reuters Categories with the Original SVM. The hyphen ‘-’ denotes the case
</tableCaption>
<bodyText confidence="0.990828">
where F-measure cannot be computed because the classifier always says ‘negative’ and therefore its preci-
sion is undefined. The scores in bold means that the score of the original SVM is better than that of SVM
with 4 Virtual SVs per SV (shown in Table 5).
</bodyText>
<table confidence="0.997558928571429">
Number of Examples in the Training Set
Category 9603 4802 2401 1200 600 300 150
earn 98.07 98.02 97.56 97.37 97.14 96.00 95.46
acq 94.20 93.06 91.71 88.81 88.92 78.70 59.92
money-fx 70.83 73.10 62.86 65.68 47.91 32.43 33.76
grain 89.20 84.72 85.11 80.44 60.79 44.10 01.00
crude 84.93 86.33 76.92 74.36 15.53 02.00 -
trade 75.83 73.21 62.31 43.53 37.58 18.32 01.65
interest 62.73 63.16 65.77 63.35 59.11 37.50 11.92
ship 73.68 67.14 50.79 30.48 06.45 02.22 -
wheat 87.42 82.61 87.94 68.91 10.67 - -
corn 87.50 84.11 46.75 68.09 03.45 - -
Macro-average 82.44 80.55 72.77 68.10 42.76 - -
Micro-average 90.45 89.69 87.12 84.97 79.16 73.25 65.05
</table>
<tableCaption confidence="0.999940333333333">
Table 5: F-Measures for the Reuters Categories with SVM with 4 Virtual SVs per SV. The scores in bold
means that the score of SVM with 4 Virtual SVs per SV is better than that of the original SVM (shown in
Table 4).
</tableCaption>
<reference confidence="0.999697633802817">
Susan Dumais, John Platt, David Heckerman, and
Mehran Sahami. 1998. Inductive learning algorithms
and representations for text categorization. In Pro-
ceedings of the ACM CIKM International Conference
on Information and Knowledge Management, pages
148–155.
Thorsten Joachims. 1998. Text categorization with sup-
port vector machines: Learning with many relevant
features. In Proceedings of the European Conference
on Machine Learning, pages 137–142.
Thorsten Joachims. 1999. Transductive inference for
text classification using support vector machines. In
Proceedings of the 16th International Conference on
Machine Learning, pages 200–209.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with
support vector machines. In Proceedings of NAACL
2001, pages 192–199.
Taku Kudo and Yuji Matsumoto. 2002. Japanese depen-
dency analysis using cascaded chunking. In Proceed-
ings of CoNLL-2002, pages 63–69.
David D. Lewis and William A. Gale. 1994. A sequential
algorithm for training text classifiers. In Proceedings
of the Seventeenth Annual International ACM-SIGIR
Conference on Research and Development in Informa-
tion Retrieval, pages 3–12.
Kamal Nigam, Andrew McCallum, Sebastian Thrun, and
Tom Mitchell. 1998. Learning to classify text from
labeled and unlabeled documents. In Proceedings of
the Fifteenth National Conference on Artificial Intelli-
gence (AAAI-98), pages 792–799.
Partha Niyogi, Federico Girosi, and Tomaso Poggio.
1998. Incorporating prior information in machine
learning by creating virtual examples. In Proceedings
ofIEEE, volume 86, pages 2196–2207.
John C. Platt. 1999. Fast training of support vec-
tor machines using sequential minimal optimization.
In Bernhard Scholkopf, Christopher J.C. Burges, and
Alexander J. Smola, editors, Advances in Kernel Meth-
ods: Support Vector Learning, pages 185–208. MIT
Press.
Manabu Sassano. 2002. An empirical study of active
learning with support vector machines for Japanese
word segmentation. In Proceedings of ACL-2002,
pages 505–512.
Bernhard Scholkopf, Chris Burges, and Vladimir Vap-
nik. 1996. Incorporating invariances in support vector
learning machines. In C. von der Malsburg, W. von
Seelen, J.C. Vorbruggen, and B. Sendhoff, editors, Ar-
tificial Neural Networks – ICANN’96, Springer Lec-
ture Notes in Computer Science, Vol. 1112, pages 47–
52.
Cynthia A. Thompson, Mary Leaine Califf, and Ray-
mond J. Mooney. 1999. Active learning for natural
language parsing and information extraction. In Pro-
ceedings of the Sixteenth International Conference on
Machine Learning, pages 406–414.
C.J. van Rijsbergen. 1979. Information Retrieval. But-
terworths, 2nd edition.
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer-Verlag.
Yiming Yang and Xin Liu. 1999. A re-examination of
text categorization methods. In Proceedings of SIGIR-
99, 2nd ACM International Conference on Research
and Development in Information Retrieval, pages 42–
49.
Yiming Yang. 1999. An evaluation of statistical ap-
proaches to text categorization. Journal of Informa-
tion Retrieval, 1(1/2):67–88.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Proceed-
ings ofACL-1995, pages 189–196.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.794577">
<title confidence="0.999377">Virtual Examples for Text Classification with Support Vector Machines</title>
<author confidence="0.847309">Manabu</author>
<affiliation confidence="0.920793">Fujitsu Laboratories</affiliation>
<address confidence="0.9705335">4-1-1, Kamikodanaka, Kawasaki 211-8588,</address>
<email confidence="0.999885">sassano@jp.fujitsu.com</email>
<abstract confidence="0.999081266666667">We explore how virtual examples (artificially created examples) improve performance of text classification with Support Vector Machines (SVMs). We propose techniques to create virtual examples for text classification based on the assumption that the category of a document is unchanged even if a small number of words are added or deleted. We evaluate the proposed methods by Reuters-21758 test set collection. Experimental results show virtual examples improve the performance of text classification with SVMs, especially for small training sets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In Proceedings of the 11th COLT,</booktitle>
<pages>92--100</pages>
<contexts>
<context position="1351" citStr="Blum and Mitchell, 1998" startWordPosition="194" endWordPosition="197">r small training sets. 1 Introduction Corpus-based supervised learning is now a standard approach to achieve high-performance in natural language processing. However, the weakness of supervised learning approach is to need an annotated corpus, the size of which is reasonably large. Even if we have a good supervised-learning method, we cannot get high-performance without an annotated corpus. The problem is that corpus annotation is labor intensive and very expensive. In order to overcome this, several methods are proposed, including minimally-supervised learning methods (e.g., (Yarowsky, 1995; Blum and Mitchell, 1998)), and active learning methods (e.g., (Thompson et al., 1999; Sassano, 2002)). The spirit behind these methods is to utilize precious labeled examples maximally. Another method following the same spirit is one using virtual examples (artificially created examples) generated from labeled examples. This method has been rarely discussed in natural language processing. In terms of active learning, Lewis and Gale (1994) mentioned the use of virtual examples in text classification. They did not, however, take forward this approach because it did not seem to be possible that a classifier created virt</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In Proceedings of the 11th COLT, pages 92–100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dennis DeCoste</author>
<author>Bernhard Scholkopf</author>
</authors>
<title>Training invariant support vector machines.</title>
<date>2002</date>
<booktitle>Machine Learning,</booktitle>
<pages>46--161</pages>
<marker>DeCoste, Scholkopf, 2002</marker>
<rawString>Dennis DeCoste and Bernhard Scholkopf. 2002. Training invariant support vector machines. Machine Learning, 46:161–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan Dumais</author>
<author>John Platt</author>
<author>David Heckerman</author>
<author>Mehran Sahami</author>
</authors>
<title>Inductive learning algorithms and representations for text categorization.</title>
<date>1998</date>
<booktitle>In Proceedings of the ACM CIKM International Conference on Information and Knowledge Management,</booktitle>
<pages>148--155</pages>
<contexts>
<context position="3386" citStr="Dumais et al., 1998" startWordPosition="531" endWordPosition="535">ess of virtual examples in NLP, motivated by the results of Sch¨olkopf et al. (1996). To our knowledge, use of virtual examples in corpus-based NLP has never been studied so far. It is, however, important to investigate this approach by which it is expected that we can alleviate the cost of corpus annotation. In particular, we focus on virtual examples with Support Vector Machines, introduced by Vapnik (1995). The reason for this is that SVM is one of most successful machine learning methods in NLP. For example, NL tasks to which SVMs have been applied are text classification (Joachims, 1998; Dumais et al., 1998), chunking (Kudo and Matsumoto, 2001), dependency analysis (Kudo and Matsumoto, 2002) and so forth. In this study, we choose text classification as a first case of the study of virtual examples in NLP because text classification in real world requires minimizing annotation cost, and it is not too complicated to perform some non-trivial experiments. Moreover, there are simple methods, which we propose, to generate virtual examples from labeled examples (Section 4). We show how virtual examples can improve the performance of a classifier with SVM in text classification, especially for small trai</context>
</contexts>
<marker>Dumais, Platt, Heckerman, Sahami, 1998</marker>
<rawString>Susan Dumais, John Platt, David Heckerman, and Mehran Sahami. 1998. Inductive learning algorithms and representations for text categorization. In Proceedings of the ACM CIKM International Conference on Information and Knowledge Management, pages 148–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Text categorization with support vector machines: Learning with many relevant features.</title>
<date>1998</date>
<booktitle>In Proceedings of the European Conference on Machine Learning,</booktitle>
<pages>137--142</pages>
<contexts>
<context position="3364" citStr="Joachims, 1998" startWordPosition="529" endWordPosition="530">e the effectiveness of virtual examples in NLP, motivated by the results of Sch¨olkopf et al. (1996). To our knowledge, use of virtual examples in corpus-based NLP has never been studied so far. It is, however, important to investigate this approach by which it is expected that we can alleviate the cost of corpus annotation. In particular, we focus on virtual examples with Support Vector Machines, introduced by Vapnik (1995). The reason for this is that SVM is one of most successful machine learning methods in NLP. For example, NL tasks to which SVMs have been applied are text classification (Joachims, 1998; Dumais et al., 1998), chunking (Kudo and Matsumoto, 2001), dependency analysis (Kudo and Matsumoto, 2002) and so forth. In this study, we choose text classification as a first case of the study of virtual examples in NLP because text classification in real world requires minimizing annotation cost, and it is not too complicated to perform some non-trivial experiments. Moreover, there are simple methods, which we propose, to generate virtual examples from labeled examples (Section 4). We show how virtual examples can improve the performance of a classifier with SVM in text classification, esp</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>Thorsten Joachims. 1998. Text categorization with support vector machines: Learning with many relevant features. In Proceedings of the European Conference on Machine Learning, pages 137–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Transductive inference for text classification using support vector machines.</title>
<date>1999</date>
<booktitle>In Proceedings of the 16th International Conference on Machine Learning,</booktitle>
<pages>200--209</pages>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Transductive inference for text classification using support vector machines. In Proceedings of the 16th International Conference on Machine Learning, pages 200–209.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Chunking with support vector machines.</title>
<date>2001</date>
<booktitle>In Proceedings of NAACL</booktitle>
<pages>192--199</pages>
<contexts>
<context position="3423" citStr="Kudo and Matsumoto, 2001" startWordPosition="537" endWordPosition="540">motivated by the results of Sch¨olkopf et al. (1996). To our knowledge, use of virtual examples in corpus-based NLP has never been studied so far. It is, however, important to investigate this approach by which it is expected that we can alleviate the cost of corpus annotation. In particular, we focus on virtual examples with Support Vector Machines, introduced by Vapnik (1995). The reason for this is that SVM is one of most successful machine learning methods in NLP. For example, NL tasks to which SVMs have been applied are text classification (Joachims, 1998; Dumais et al., 1998), chunking (Kudo and Matsumoto, 2001), dependency analysis (Kudo and Matsumoto, 2002) and so forth. In this study, we choose text classification as a first case of the study of virtual examples in NLP because text classification in real world requires minimizing annotation cost, and it is not too complicated to perform some non-trivial experiments. Moreover, there are simple methods, which we propose, to generate virtual examples from labeled examples (Section 4). We show how virtual examples can improve the performance of a classifier with SVM in text classification, especially for small training sets. 2 Support Vector Machines </context>
</contexts>
<marker>Kudo, Matsumoto, 2001</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2001. Chunking with support vector machines. In Proceedings of NAACL 2001, pages 192–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Japanese dependency analysis using cascaded chunking.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL-2002,</booktitle>
<pages>63--69</pages>
<contexts>
<context position="3471" citStr="Kudo and Matsumoto, 2002" startWordPosition="543" endWordPosition="546">996). To our knowledge, use of virtual examples in corpus-based NLP has never been studied so far. It is, however, important to investigate this approach by which it is expected that we can alleviate the cost of corpus annotation. In particular, we focus on virtual examples with Support Vector Machines, introduced by Vapnik (1995). The reason for this is that SVM is one of most successful machine learning methods in NLP. For example, NL tasks to which SVMs have been applied are text classification (Joachims, 1998; Dumais et al., 1998), chunking (Kudo and Matsumoto, 2001), dependency analysis (Kudo and Matsumoto, 2002) and so forth. In this study, we choose text classification as a first case of the study of virtual examples in NLP because text classification in real world requires minimizing annotation cost, and it is not too complicated to perform some non-trivial experiments. Moreover, there are simple methods, which we propose, to generate virtual examples from labeled examples (Section 4). We show how virtual examples can improve the performance of a classifier with SVM in text classification, especially for small training sets. 2 Support Vector Machines In this section we give some theoretical definit</context>
</contexts>
<marker>Kudo, Matsumoto, 2002</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2002. Japanese dependency analysis using cascaded chunking. In Proceedings of CoNLL-2002, pages 63–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
<author>William A Gale</author>
</authors>
<title>A sequential algorithm for training text classifiers.</title>
<date>1994</date>
<booktitle>In Proceedings of the Seventeenth Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>3--12</pages>
<contexts>
<context position="1769" citStr="Lewis and Gale (1994)" startWordPosition="257" endWordPosition="260">annotation is labor intensive and very expensive. In order to overcome this, several methods are proposed, including minimally-supervised learning methods (e.g., (Yarowsky, 1995; Blum and Mitchell, 1998)), and active learning methods (e.g., (Thompson et al., 1999; Sassano, 2002)). The spirit behind these methods is to utilize precious labeled examples maximally. Another method following the same spirit is one using virtual examples (artificially created examples) generated from labeled examples. This method has been rarely discussed in natural language processing. In terms of active learning, Lewis and Gale (1994) mentioned the use of virtual examples in text classification. They did not, however, take forward this approach because it did not seem to be possible that a classifier created virtual examples of documents in natural language and then requested a human teacher to label them. In the field of pattern recognition, some kind of virtual examples has been studied. The first report of methods using virtual examples with Support Vector Machines (SVMs) is that of Sch¨olkopf et al. (1996), who demonstrated significant improvement of the accuracy in hand-written digit recognition (Section 3). They crea</context>
<context position="10297" citStr="Lewis and Gale, 1994" startWordPosition="1765" endWordPosition="1768">Discussion 5.1 Test Set Collection We used the Reuters-21578 dataset3 to evaluate the proposed methods. The dataset has several splits of a training set and a test set. We used here “ModApte” split, which is most widely used in the literature on text classification. This split has 9,603 training examples and 3,299 test examples. More than 100 categories are in the dataset. We use, however, only the most frequent 10 categories. Table 2 shows the 10 categories and the number of training and test examples in each of the categories. 5.2 Performance Measures We use F-measure (van Rijsbergen, 1979; Lewis and Gale, 1994) as a primal performance measure 3Available from David D. Lewis’s page: http:// www.daviddlewis.com/resources/testcollections/reuters21578/ to evaluate the result. F-measure is defined as: where p is precision and q is recall and 0 is a parameter which decides the relative weight of precision and recall. The p and the q are defined as: number of positive and correct outputs number of positive outputs number of positive and correct outputs number of positive examples In Equation 4, usually 0 = 1 is used, which means it gives equal weight to precision and recall. When we evaluate the performance</context>
</contexts>
<marker>Lewis, Gale, 1994</marker>
<rawString>David D. Lewis and William A. Gale. 1994. A sequential algorithm for training text classifiers. In Proceedings of the Seventeenth Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, pages 3–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kamal Nigam</author>
<author>Andrew McCallum</author>
<author>Sebastian Thrun</author>
<author>Tom Mitchell</author>
</authors>
<title>Learning to classify text from labeled and unlabeled documents.</title>
<date>1998</date>
<booktitle>In Proceedings of the Fifteenth National Conference on Artificial Intelligence (AAAI-98),</booktitle>
<pages>792--799</pages>
<marker>Nigam, McCallum, Thrun, Mitchell, 1998</marker>
<rawString>Kamal Nigam, Andrew McCallum, Sebastian Thrun, and Tom Mitchell. 1998. Learning to classify text from labeled and unlabeled documents. In Proceedings of the Fifteenth National Conference on Artificial Intelligence (AAAI-98), pages 792–799.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Partha Niyogi</author>
<author>Federico Girosi</author>
<author>Tomaso Poggio</author>
</authors>
<title>Incorporating prior information in machine learning by creating virtual examples.</title>
<date>1998</date>
<booktitle>In Proceedings ofIEEE,</booktitle>
<volume>86</volume>
<pages>2196--2207</pages>
<contexts>
<context position="2585" citStr="Niyogi et al. (1998)" startWordPosition="394" endWordPosition="397">f documents in natural language and then requested a human teacher to label them. In the field of pattern recognition, some kind of virtual examples has been studied. The first report of methods using virtual examples with Support Vector Machines (SVMs) is that of Sch¨olkopf et al. (1996), who demonstrated significant improvement of the accuracy in hand-written digit recognition (Section 3). They created virtual examples from labeled examples based on prior knowledge of the task: slightly translated (e.g., 1 pixel shifted to the right) images have the same label (class) of the original image. Niyogi et al. (1998) also discussed the use of prior knowledge by creating virtual examples and thereby expanding the effective training set size. The purpose of this study is to explore the effectiveness of virtual examples in NLP, motivated by the results of Sch¨olkopf et al. (1996). To our knowledge, use of virtual examples in corpus-based NLP has never been studied so far. It is, however, important to investigate this approach by which it is expected that we can alleviate the cost of corpus annotation. In particular, we focus on virtual examples with Support Vector Machines, introduced by Vapnik (1995). The r</context>
</contexts>
<marker>Niyogi, Girosi, Poggio, 1998</marker>
<rawString>Partha Niyogi, Federico Girosi, and Tomaso Poggio. 1998. Incorporating prior information in machine learning by creating virtual examples. In Proceedings ofIEEE, volume 86, pages 2196–2207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John C Platt</author>
</authors>
<title>Fast training of support vector machines using sequential minimal optimization.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods: Support Vector Learning,</booktitle>
<pages>185--208</pages>
<editor>In Bernhard Scholkopf, Christopher J.C. Burges, and Alexander J. Smola, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="11408" citStr="Platt (1999)" startWordPosition="1950" endWordPosition="1951"> is used, which means it gives equal weight to precision and recall. When we evaluate the performance of a classifier to a multiple category dataset, there are two ways to compute F-measure: macro-averaging and microaveraging (Yang, 1999). The former way is to first compute F-measure for each category and then average them, while the latter way is to first compute precision and recall for all the categories and use them to calculate the F-measure. 5.3 SVM setting Through our experiments we used our original SVM tools, the algorithm of which is based on SMO (Sequential Minimal Optimization) by Platt (1999). We used linear SVMs and set a misclassification cost C to 0:016541 which is 1/(the average of x • x) where x is a feature vector in the 9,603 size training set. For simplicity, we fixed C through all the experiments. We built a binary classifier for each of the 10 categories shown in Table 2. 5.4 Results First, we carried out experiments using GenerateByDeletion and GenerateByAddition separately to create virtual examples, where a virtual example was created per Support Vector. We did not generate virtual examples from non support vectors. We set the parameter t to 0:054 for GenerateByDeleti</context>
</contexts>
<marker>Platt, 1999</marker>
<rawString>John C. Platt. 1999. Fast training of support vector machines using sequential minimal optimization. In Bernhard Scholkopf, Christopher J.C. Burges, and Alexander J. Smola, editors, Advances in Kernel Methods: Support Vector Learning, pages 185–208. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manabu Sassano</author>
</authors>
<title>An empirical study of active learning with support vector machines for Japanese word segmentation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL-2002,</booktitle>
<pages>505--512</pages>
<contexts>
<context position="1427" citStr="Sassano, 2002" startWordPosition="207" endWordPosition="208">rd approach to achieve high-performance in natural language processing. However, the weakness of supervised learning approach is to need an annotated corpus, the size of which is reasonably large. Even if we have a good supervised-learning method, we cannot get high-performance without an annotated corpus. The problem is that corpus annotation is labor intensive and very expensive. In order to overcome this, several methods are proposed, including minimally-supervised learning methods (e.g., (Yarowsky, 1995; Blum and Mitchell, 1998)), and active learning methods (e.g., (Thompson et al., 1999; Sassano, 2002)). The spirit behind these methods is to utilize precious labeled examples maximally. Another method following the same spirit is one using virtual examples (artificially created examples) generated from labeled examples. This method has been rarely discussed in natural language processing. In terms of active learning, Lewis and Gale (1994) mentioned the use of virtual examples in text classification. They did not, however, take forward this approach because it did not seem to be possible that a classifier created virtual examples of documents in natural language and then requested a human tea</context>
</contexts>
<marker>Sassano, 2002</marker>
<rawString>Manabu Sassano. 2002. An empirical study of active learning with support vector machines for Japanese word segmentation. In Proceedings of ACL-2002, pages 505–512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernhard Scholkopf</author>
<author>Chris Burges</author>
<author>Vladimir Vapnik</author>
</authors>
<title>Incorporating invariances in support vector learning machines.</title>
<date>1996</date>
<booktitle>Artificial Neural Networks – ICANN’96, Springer Lecture Notes in Computer Science,</booktitle>
<volume>1112</volume>
<pages>47--52</pages>
<editor>In C. von der Malsburg, W. von Seelen, J.C. Vorbruggen, and B. Sendhoff, editors,</editor>
<marker>Scholkopf, Burges, Vapnik, 1996</marker>
<rawString>Bernhard Scholkopf, Chris Burges, and Vladimir Vapnik. 1996. Incorporating invariances in support vector learning machines. In C. von der Malsburg, W. von Seelen, J.C. Vorbruggen, and B. Sendhoff, editors, Artificial Neural Networks – ICANN’96, Springer Lecture Notes in Computer Science, Vol. 1112, pages 47– 52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia A Thompson</author>
<author>Mary Leaine Califf</author>
<author>Raymond J Mooney</author>
</authors>
<title>Active learning for natural language parsing and information extraction.</title>
<date>1999</date>
<booktitle>In Proceedings of the Sixteenth International Conference on Machine Learning,</booktitle>
<pages>406--414</pages>
<contexts>
<context position="1411" citStr="Thompson et al., 1999" startWordPosition="203" endWordPosition="206">earning is now a standard approach to achieve high-performance in natural language processing. However, the weakness of supervised learning approach is to need an annotated corpus, the size of which is reasonably large. Even if we have a good supervised-learning method, we cannot get high-performance without an annotated corpus. The problem is that corpus annotation is labor intensive and very expensive. In order to overcome this, several methods are proposed, including minimally-supervised learning methods (e.g., (Yarowsky, 1995; Blum and Mitchell, 1998)), and active learning methods (e.g., (Thompson et al., 1999; Sassano, 2002)). The spirit behind these methods is to utilize precious labeled examples maximally. Another method following the same spirit is one using virtual examples (artificially created examples) generated from labeled examples. This method has been rarely discussed in natural language processing. In terms of active learning, Lewis and Gale (1994) mentioned the use of virtual examples in text classification. They did not, however, take forward this approach because it did not seem to be possible that a classifier created virtual examples of documents in natural language and then reque</context>
</contexts>
<marker>Thompson, Califf, Mooney, 1999</marker>
<rawString>Cynthia A. Thompson, Mary Leaine Califf, and Raymond J. Mooney. 1999. Active learning for natural language parsing and information extraction. In Proceedings of the Sixteenth International Conference on Machine Learning, pages 406–414.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J van Rijsbergen</author>
</authors>
<date>1979</date>
<journal>Information Retrieval. Butterworths,</journal>
<note>2nd edition.</note>
<marker>van Rijsbergen, 1979</marker>
<rawString>C.J. van Rijsbergen. 1979. Information Retrieval. Butterworths, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="3178" citStr="Vapnik (1995)" startWordPosition="496" endWordPosition="498">Niyogi et al. (1998) also discussed the use of prior knowledge by creating virtual examples and thereby expanding the effective training set size. The purpose of this study is to explore the effectiveness of virtual examples in NLP, motivated by the results of Sch¨olkopf et al. (1996). To our knowledge, use of virtual examples in corpus-based NLP has never been studied so far. It is, however, important to investigate this approach by which it is expected that we can alleviate the cost of corpus annotation. In particular, we focus on virtual examples with Support Vector Machines, introduced by Vapnik (1995). The reason for this is that SVM is one of most successful machine learning methods in NLP. For example, NL tasks to which SVMs have been applied are text classification (Joachims, 1998; Dumais et al., 1998), chunking (Kudo and Matsumoto, 2001), dependency analysis (Kudo and Matsumoto, 2002) and so forth. In this study, we choose text classification as a first case of the study of virtual examples in NLP because text classification in real world requires minimizing annotation cost, and it is not too complicated to perform some non-trivial experiments. Moreover, there are simple methods, which</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>Vladimir N. Vapnik. 1995. The Nature of Statistical Learning Theory. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiming Yang</author>
<author>Xin Liu</author>
</authors>
<title>A re-examination of text categorization methods.</title>
<date>1999</date>
<booktitle>In Proceedings of SIGIR99, 2nd ACM International Conference on Research and Development in Information Retrieval,</booktitle>
<pages>42--49</pages>
<contexts>
<context position="17238" citStr="Yang and Liu, 1999" startWordPosition="2962" endWordPosition="2965">ach of the 10 categories are shown in Tables 4 and 5. SVM with 4 VSVs is better than the original SVM for almost all the categories and all the sizes except for “interest” and “wheat” at the 9603 size training set. For low frequent categories such as “ship”, “wheat” and “corn”, the classifiers of the original SVM perform poorly. There are many cases where they never output ‘positive’, i.e. the recall is zero. It suggests that the original SVM fails to find a good hyperplane due to the imbalanced training sets which have very few 6We have done the significance test which is called “p-test” in (Yang and Liu, 1999), requiring significance at the 0.05 level. Although at the 9603 size training set the improvement of the error rate is not statistically significant, in all the other cases the improvement is significant. positive examples. In contrast, SVM with 4 VSVs yields better results for such harder cases. 6 Conclusion and Future Directions We have explored how virtual examples improve the performance of text classification with SVMs. For text classification, we have proposed methods to create virtual examples on the assumption that the label of a document is unchanged even if a small number of words a</context>
</contexts>
<marker>Yang, Liu, 1999</marker>
<rawString>Yiming Yang and Xin Liu. 1999. A re-examination of text categorization methods. In Proceedings of SIGIR99, 2nd ACM International Conference on Research and Development in Information Retrieval, pages 42– 49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiming Yang</author>
</authors>
<title>An evaluation of statistical approaches to text categorization.</title>
<date>1999</date>
<journal>Journal of Information Retrieval,</journal>
<pages>1--1</pages>
<contexts>
<context position="11034" citStr="Yang, 1999" startWordPosition="1884" endWordPosition="1885">reuters21578/ to evaluate the result. F-measure is defined as: where p is precision and q is recall and 0 is a parameter which decides the relative weight of precision and recall. The p and the q are defined as: number of positive and correct outputs number of positive outputs number of positive and correct outputs number of positive examples In Equation 4, usually 0 = 1 is used, which means it gives equal weight to precision and recall. When we evaluate the performance of a classifier to a multiple category dataset, there are two ways to compute F-measure: macro-averaging and microaveraging (Yang, 1999). The former way is to first compute F-measure for each category and then average them, while the latter way is to first compute precision and recall for all the categories and use them to calculate the F-measure. 5.3 SVM setting Through our experiments we used our original SVM tools, the algorithm of which is based on SMO (Sequential Minimal Optimization) by Platt (1999). We used linear SVMs and set a misclassification cost C to 0:016541 which is 1/(the average of x • x) where x is a feature vector in the 9,603 size training set. For simplicity, we fixed C through all the experiments. We buil</context>
</contexts>
<marker>Yang, 1999</marker>
<rawString>Yiming Yang. 1999. An evaluation of statistical approaches to text categorization. Journal of Information Retrieval, 1(1/2):67–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings ofACL-1995,</booktitle>
<pages>189--196</pages>
<contexts>
<context position="1325" citStr="Yarowsky, 1995" startWordPosition="192" endWordPosition="193">s, especially for small training sets. 1 Introduction Corpus-based supervised learning is now a standard approach to achieve high-performance in natural language processing. However, the weakness of supervised learning approach is to need an annotated corpus, the size of which is reasonably large. Even if we have a good supervised-learning method, we cannot get high-performance without an annotated corpus. The problem is that corpus annotation is labor intensive and very expensive. In order to overcome this, several methods are proposed, including minimally-supervised learning methods (e.g., (Yarowsky, 1995; Blum and Mitchell, 1998)), and active learning methods (e.g., (Thompson et al., 1999; Sassano, 2002)). The spirit behind these methods is to utilize precious labeled examples maximally. Another method following the same spirit is one using virtual examples (artificially created examples) generated from labeled examples. This method has been rarely discussed in natural language processing. In terms of active learning, Lewis and Gale (1994) mentioned the use of virtual examples in text classification. They did not, however, take forward this approach because it did not seem to be possible that</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings ofACL-1995, pages 189–196.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>