<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000027">
<title confidence="0.788795">
SemEval-2015 Task 3: Answer Selection in Community Question Answering
</title>
<author confidence="0.578972">
Preslav Nakov Lluis M`arquez Walid Magdy Alessandro Moschitti
</author>
<affiliation confidence="0.47748">
ALT Research Group, Qatar Computing Research Institute
</affiliation>
<author confidence="0.375685">
James Glass
</author>
<affiliation confidence="0.221517">
MIT Computer Science and Artificial Intelligence Laboratory
</affiliation>
<author confidence="0.580207">
Bilal Randeree
</author>
<affiliation confidence="0.382464">
Qatar Living
</affiliation>
<sectionHeader confidence="0.958284" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.98828696">
Community Question Answering (cQA) pro-
vides new interesting research directions to
the traditional Question Answering (QA)
field, e.g., the exploitation of the interaction
between users and the structure of related
posts. In this context, we organized SemEval-
2015 Task 3 on Answer Selection in cQA,
which included two subtasks: (a) classifying
answers as good, bad, or potentially relevant
with respect to the question, and (b) answering
a YES/NO question with yes, no, or unsure,
based on the list of all answers. We set subtask
A for Arabic and English on two relatively
different cQA domains, i.e., the Qatar Liv-
ing website for English, and a Quran-related
website for Arabic. We used crowdsourcing
on Amazon Mechanical Turk to label a large
English training dataset, which we released to
the research community. Thirteen teams par-
ticipated in the challenge with a total of 61
submissions: 24 primary and 37 contrastive.
The best systems achieved an official score
(macro-averaged Fl) of 57.19 and 63.7 for the
English subtasks A and B, and 78.55 for the
Arabic subtask A.
</bodyText>
<sectionHeader confidence="0.999267" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99988345">
Many social activities on the Web, e.g., in forums
and social networks, are accomplished by means
of the community Question Answering (cQA)
paradigm. User interaction in this context is seldom
moderated, is rather open, and thus has little restric-
tions, if any, on who can post and who can answer a
question.
On the positive side, this means that one can
freely ask a question and expect some good, hon-
est answers. On the negative side, it takes efforts to
go through all possible answers and to make sense
of them. It is often the case that many answers are
only loosely related to the actual question, and some
even change the topic. It is also not unusual for a
question to have hundreds of answers, the vast ma-
jority of which would not satisfy a user’s informa-
tion needs; thus, finding the desired information in a
long list of answers might be very time-consuming.
In our SemEval-2015 Task 3, we proposed two
subtasks. First, subtask A asks for identifying the
posts in the answer thread that answer the question
well vs. those that can be potentially useful to the
user (e.g., because they can help educate him/her on
the subject) vs. those that are just bad or useless.
This subtask goes in the direction of automating the
answer search problem that we discussed above, and
we offered it in two languages: English and Ara-
bic. Second, for the special case of YES/NO ques-
tions, we propose an extreme summarization exer-
cise (subtask B), which aims to produce a simple
YES/NO overall answer, considering all good an-
swers to the questions (according to subtask A).
For English, the two subtasks are built on a par-
ticular application scenario of cQA, based on the
Qatar Living forum.1 However, we decoupled the
tasks from the Information Retrieval component in
order to facilitate participation, and to focus on as-
pects that are relevant for the SemEval community,
namely on learning the relationship between two
pieces of text.
</bodyText>
<footnote confidence="0.984715">
1http://www.qatarliving.com/forum/
</footnote>
<page confidence="0.936051">
269
</page>
<note confidence="0.95445">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 269–281,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999446466666667">
Subtask A goes in the direction of passage rerank-
ing, where automatic classifiers are normally applied
to pairs of questions and answer passages to derive
a relative order between passages, e.g., see (Radlin-
ski and Joachims, 2005; Jeon et al., 2005; Shen and
Lapata, 2007; Moschitti et al., 2007; Surdeanu et
al., 2008). In recent years, many advanced models
have been developed for automating answer selec-
tion, producing a large body of work.2 For instance,
Wang et al. (2007) proposed a probabilistic quasi-
synchronous grammar to learn syntactic transforma-
tions from the question to the candidate answers;
Heilman and Smith (2010) used an algorithm based
on Tree Edit Distance (TED) to learn tree transfor-
mations in pairs; Wang and Manning (2010) devel-
oped a probabilistic model to learn tree-edit oper-
ations on dependency parse trees; and Yao et al.
(2013) applied linear chain CRFs with features de-
rived from TED to automatically learn associations
between questions and candidate answers. One in-
teresting aspect of the above research is the need
for syntactic structures; this is also corroborated in
(Severyn and Moschitti, 2012; Severyn and Mos-
chitti, 2013). Note that answer selection can use
models for textual entailment, semantic similarity,
and for natural language inference in general.
For Arabic, we also made use of a real cQA por-
tal, the Fatwa website,3 where questions about Is-
lam are posed by regular users and are answered by
knowledgeable scholars. For subtask A, we used a
setup similar to that for English, but this time each
question had exactly one correct answer among the
candidate answers (see Section 3 for detail); we did
not offer subtask B for Arabic.
Overall for the task, we needed manual annota-
tions in two different languages and for two do-
mains. For English, we built the Qatar Living
datasets as a joint effort between MIT and the Qatar
Computing Research Institute, co-organizers of the
task, using Amazon’s Mechanical Turk to recruit hu-
man annotators. For Arabic, we built the dataset
automatically from the data available in the Fatwa
website, without the need for any manual annota-
tion. We made all datasets publicly available, i.e.,
also usable beyond SemEval.
</bodyText>
<footnote confidence="0.939375666666667">
2aclweb.org/aclwiki/index.php?title=
Question_Answering_(State_of_the_art)
3http://fatwa.islamweb.net/
</footnote>
<bodyText confidence="0.999709310344828">
Our SemEval task attracted 13 teams, who sub-
mitted a total of 61 runs. The participants mainly
focused on defining new features that go beyond
question-answer similarity, e.g., author- and user-
based, and spent less time on the design of com-
plex machine learning approaches. Indeed, most
systems used multi-class classifiers such as Max-
Ent and SVM, but some used regression. Overall,
almost all submissions managed to outperform the
baselines using the official Fl-based score. In par-
ticular, the best system can detect a correct answer
with an accuracy of about 73% in the English task
and 83% in the easier Arabic task. For the extreme
summarization task, the best accuracy is 72%.
An interesting outcome of this task is that the
Qatar Living company, a co-organizer of the chal-
lenge, is going to use the experience and the tech-
nology developed during the evaluation excercise to
improve their products, e.g., the automatic search of
comments useful to answer users’ questions.
The remainder of the paper is organized as fol-
lows: Section 2 gives a detailed description of the
task, Section 3 describes the datasets, Section 4 ex-
plains the scorer, Section 5 presents the participants
and the evaluation results, Section 6 provides an
overview of the various features and techniques used
by the participating systems, Section 7 offers fur-
ther discussion, and finally, Section 8 concludes and
points to possible directions for future work.
</bodyText>
<sectionHeader confidence="0.926184" genericHeader="introduction">
2 Task Definition
</sectionHeader>
<bodyText confidence="0.985972">
We have two subtasks:
</bodyText>
<listItem confidence="0.955990615384615">
• Subtask A: Given a question (short title + ex-
tended description), and several community an-
swers, classify each of the answers as
(a) definitely relevant (good),
(b) potentially useful (potential), or
(c) bad or irrelevant (bad, dialog, non-
English, other).
• Subtask B: Given a YES/NO question (short
title + extended description), and a list of com-
munity answers, decide whether the global an-
swer to the question should be yes, no, or un-
sure, based on the individual good answers.
This subtask is only available for English.
</listItem>
<page confidence="0.996611">
270
</page>
<figureCaption confidence="0.999556">
Figure 1: Annotated English question from the CQA-QL corpus.
</figureCaption>
<sectionHeader confidence="0.99791" genericHeader="method">
3 Datasets
</sectionHeader>
<bodyText confidence="0.9999634">
We offer the task in two languages, English and Ara-
bic, with some differences in the type of data pro-
vided. For English, there is a question (short title +
extended description) and a list of several commu-
nity answers to that question. For Arabic, there is
a question and a set of possible answers, which in-
clude (i) a highly accurate answer, (ii) potentially
useful answers from other questions, and (iii) an-
swers to random questions. The following subsec-
tions provide all the necessary details.
</bodyText>
<subsectionHeader confidence="0.995828">
3.1 English Data: CQA-QL corpus
</subsectionHeader>
<bodyText confidence="0.9999401">
The source of the CQA-QL corpus is the Qatar
Living forum. A sample of questions and answer
threads was selected and then manually filtered and
annotated with the categories defined in the task.
We provided a split in three datasets: training,
development, and testing. All datasets were XML-
formated and the text was encoded in UTF-8.
A dataset file is a sequence of examples (ques-
tions), where each question has a subject and a body
(text), as well as the following attributes:
</bodyText>
<listItem confidence="0.999388181818182">
• QID: question identifier;
• QCATEGORY: the question category, accord-
ing to the Qatar Living taxonomy;
• QDATE: date of posting;
• QUSERID: identifier of the user asking the
question;
• QTYPE: type of question (GENERAL or
YES/NO);
• QGOLD YN: for YES/NO questions only, an
overall Yes/No/Unsure answer based on all
comments.
</listItem>
<page confidence="0.995318">
271
</page>
<bodyText confidence="0.980077333333333">
Each question is followed by a list of comments
(or answers). A comment has a subject and a body
(text), as well as the following attributes:
</bodyText>
<listItem confidence="0.980461444444444">
• CID: comment identifier;
• CUSERID: identifier of the user posting the
comment;
• CGOLD: human assessment about whether the
comment is Good, Bad, Potential, Dialogue,
non-English, or Other.
• CGOLD YN: human assessment on whether
the comment suggests a Yes, a No, or an Un-
sure answer.
</listItem>
<bodyText confidence="0.996336529411765">
At test time, CGOLD, CGOLD YN, and
QGOLD YN are hidden, and systems are asked to
predict CGOLD for subtask A, and QGOLD YN
for subtask B; CGOLD YN is not to be predicted.
Figure 1 shows a fully annotated English
YES/NO question from the CQA-QL corpus. We
can see that it is asked and answered in a very in-
formal way and that there are many typos, incor-
rect capitalization, punctuation, slang, elongations,
etc. Four of the comments are good answers to the
question, and four are bad. The bad answers are ir-
relevant with respect to the YES/NO answer to the
question as a whole, and thus their CGOLD YN la-
bel is Not Applicable. The remaining four good an-
swers predict Yes twice, No once, and Unsure once;
as there are more Yes answers than the two alterna-
tives, the overall QGOLD YN is Yes.
</bodyText>
<subsectionHeader confidence="0.999914">
3.2 Annotating the CQA-QL corpus
</subsectionHeader>
<bodyText confidence="0.999984777777778">
The manual annotation was a joint effort between
MIT and the Qatar Computing Research Institute,
co-organizers of the task. After a first internal la-
beling of a trial dataset (50+50 questions) by several
independent annotators, we defined the annotation
procedure and prepared detailed annotation guide-
lines. We then used Amazon’s Mechanical Turk to
collect human annotations for a much larger dataset.
This involved the setup of three HITs:
</bodyText>
<listItem confidence="0.950812363636364">
• HIT 1: Select appropriate example questions
and classify them as GENERAL vs. YES/NO
(QCATEGORY);
• HIT 2: For GENERAL questions, annotate
each comment as Good, Bad, Potential, Dia-
logue, non-English, or Other (CGOLD);
• HIT 3: For YES/NO questions, annotate the
comments as in HIT 2 (CGOLD), plus a label
indicating whether the comment answers the
question with a clear Yes, a clear No, or in an
undefined way, i.e., as Unsure (CGOLD YN).
</listItem>
<bodyText confidence="0.998930095238095">
For all HITs, we collected annotations from 3-5
annotators for each decision, and we resolved dis-
crepancies using majority voting. Ties led to the
elimination of some comments and sometimes even
of entire questions.
We assigned the Yes/No/Unsure labels at the
question level (QGOLD YN) automatically, us-
ing the Yes/No/Unsure labels at the comment
level (CGOLD YN). More precisely, we labeled a
YES/NO question as Unsure, unless there was a ma-
jority of Yes or No labels among the Yes/No/Unsure
labels for the comments that are labeled as Good, in
which case we assigned the majority label.
Table 1 shows some statistics about the datasets.
We can see that the YES/NO questions are about
10% of the questions. This makes subtask B gen-
erally harder for machine learning, as there is much
less training data. We further see that on average,
there are about 6 comments per question, with the
number varying widely from 1 to 143. About half
of the comments are Good, another 10% are Po-
tential, and the rest are Bad. Note that for the
purpose of classification, Bad is in fact a hetero-
geneous class that includes about 50% Bad, 50%
Dialogue, and also a tiny fraction of non-English
and Other comments. We released the fine grained
labels to the task participants as we thought that
having information about the heterogeneous struc-
ture of Bad might be helpful for some systems.
About 40-50% of the YES/NO annotations at the
comment level (CGOLD YN) are Yes, with the rest
nearly equally split between No and Unsure, with
No slightly more frequent. However, at the question
level, the YES/NO annotations (QGOLD YN) have
more Unsure than No. Overall, the label distribution
in development and testing is similar to that in train-
ing for the CGOLD values, but there are somewhat
larger differences for QGOLD YN.
We further released the raw text of all questions
and of all comments from Qatar Living, including
more than 100 million word tokens, which are useful
for training word embeddings, topic models, etc.
</bodyText>
<page confidence="0.994216">
272
</page>
<figureCaption confidence="0.999393">
Figure 2: Annotated Arabic question from the Fatwa corpus.
</figureCaption>
<subsectionHeader confidence="0.998892">
3.3 Arabic Data: Fatwa corpus
</subsectionHeader>
<bodyText confidence="0.999988047619048">
For Arabic, we used data from the Fatwa website,
which deals with questions about Islam. This web-
site contains questions by ordinary users and an-
swers by knowledgeable scholars in Islamic studies.
The user question can be general, for example “How
to pray?”, or it can be very personal, e.g., the user
has a specific problem in his/her life and wants to
find out how to deal with it according to Islam.
Each question (Fatwa) is answered carefully by a
knowledgeable scholar. The answer is usually very
descriptive: it contains an introduction to the topic
of the question, then the general rules in Islam on
the topic, and finally an actual answer to the spe-
cific question and/or guidance on how to deal with
the problem. Typically, links to related questions are
also provided to the user to read more about similar
situations and to look at related questions.
In the Arabic version of subtask A, a question
from the website is provided with a set of exactly
five different answers. Each answer of the provided
five ones carries one of the following labels:
</bodyText>
<listItem confidence="0.9875144">
• direct: direct answer to the question;
• related: not directly answering the question,
but contains related information;
• irrelevant: answer to another question not re-
lated to the topic.
</listItem>
<bodyText confidence="0.88777975">
Similarly to the English corpus, a dataset file is a
sequence of examples (Questions), where each ques-
tion has a subject and a body (text), as well as the
following attributes:
</bodyText>
<listItem confidence="0.999988333333333">
• QID: internal question identifier;
• QCATEGORY: question category;
• QDATE: date of posting.
</listItem>
<bodyText confidence="0.984056">
Each question is followed by a list of possible an-
swers. An answer has a subject and a body (text), as
well as the following attributes:
</bodyText>
<listItem confidence="0.999755666666667">
• CID: answer identifier;
• CGOLD: label of the answer, which is one of
three: direct, related, or irrelevant.
</listItem>
<bodyText confidence="0.8429115">
Moreover, the answer body text can contain tags
such as the following:
</bodyText>
<listItem confidence="0.99997875">
• NE: named entities in the text, usually person
names;
• Quran: verse from the Quran;
• Hadeeth: saying by the Islamic prophet.
</listItem>
<bodyText confidence="0.9011605">
Figure 2 shows some fully annotated Arabic ques-
tion from the Fatwa corpus.
</bodyText>
<page confidence="0.994106">
273
</page>
<table confidence="0.999713625">
Category Train Dev Test
Questions 2,600 300 329
– GENERAL 2,376 266 304
– YES/NO 224 34 25
Comments 16,541 1,645 1,976
– min per question 1 1 1
– max per question 143 32 66
– avg per question 6.36 5.48 6.01
CGOLD values 16,541 1,645 1,976
– Good 8,069 875 997
– Potential 1,659 187 167
– Bad 6,813 583 812
– Bad 2,981 269 362
– Dialogue 3,755 312 435
– Not English 74 2 15
– Other 3 0 0
CGOLD YN values 795 115 111
– Yes 346 62 –
– No 236 32 –
– Unsure 213 21 –
QGOLD YN values 224 34 25
– Yes 87 16 15
– No 47 8 4
– Unsure 90 10 6
</table>
<tableCaption confidence="0.998676">
Table 1: Statistics about the English data.
</tableCaption>
<table confidence="0.999314333333333">
Category Train Dev Test Test30
Questions 1,300 200 200 30
Answers 6,500 1,000 1,001 151
– Direct 1,300 200 215 45
– Related 1,469 222 222 33
– Irrelevant 3,731 578 564 73
</table>
<tableCaption confidence="0.999669">
Table 2: Statistics about the Arabic data.
</tableCaption>
<subsectionHeader confidence="0.999117">
3.4 Annotating the Fatwa corpus
</subsectionHeader>
<bodyText confidence="0.999985904761905">
We selected the shortest questions and answers from
IslamWeb to create our training, development and
testing datasets. We avoided long questions and an-
swers since they are likely to be harder to parse,
analyse, and classify. For each question, we labeled
its answer as direct, the answers of linked questions
as related, and we selected some random answers
as irrelevant to make the total number of provided
answers per question equal to 5.
Table 2 shows some statistics about the resulting
datasets. We can see that the number of direct an-
swers is the same as the number of questions, since
each question has only one direct answer.
One issue with selecting random answers as ir-
relevant is that the task is too easy; thus, we manu-
ally annotated a special hard testset of 30 questions
(Test30), where we selected the irrelevant answers
using information retrieval to guarantee significant
term overlap with the questions. For the general test-
set, we used these 30 questions and 170 more where
the irrelevant answers were chosen randomly.
</bodyText>
<sectionHeader confidence="0.987901" genericHeader="method">
4 Scoring
</sectionHeader>
<bodyText confidence="0.976947">
The official score for both subtasks is Fl, macro-
averaged over the target categories:
</bodyText>
<listItem confidence="0.998389333333333">
• For English, subtask A they are Good, Poten-
tial, and Bad.
• For Arabic, subtask A these are direct, related,
and irrelevant.
• For English, subtask B they are Yes, No, and
Unsure.
</listItem>
<bodyText confidence="0.584331">
We also report classification accuracy.
</bodyText>
<subsectionHeader confidence="0.444325">
Team ID Affiliation and reference
</subsectionHeader>
<table confidence="0.9892985">
Al-Bayan Alexandria University, Egypt
(Mohamed et al., 2015)
CICBUAPnlp Instituto Polit´ecnico Nacional, Mexico
CoMiC University of T¨ubingen, Germany
(Rudzewitz and Ziai, 2015)
ECNU East China Normal University, China
(Yi et al., 2015)
FBK-HLT Fondazione Bruno Kessler, Italy
(Vo et al., 2015)
HITSZ-ICRC Harbin Institute of Technology, China
(Hou et al., 2015)
ICRC-HIT Harbin Institute of Technology, China
(Zhou et al., 2015)
JAIST Japan Advance Institute of Science
and Technology, Japan
(Tran et al., 2015)
QCRI Qatar Computing Research Institute, Qatar
(Nicosia et al., 2015)
Shiraz Shiraz University, Iran
(Heydari Alashty et al., 2015)
VectorSLU MIT Computer Science and
Artificial Intelligence Lab, USA
(Belinkov et al., 2015)
Voltron Sofia University, Bulgaria
(Zamanov et al., 2015)
Yamraj Masaryk University, Czech Republic
</table>
<tableCaption confidence="0.999718">
Table 3: The participating teams.
</tableCaption>
<page confidence="0.830058">
274
</page>
<table confidence="0.99997040625">
Submission Macro F, Acc.
JAIST-contrastive1 57.29 72.67
1 JAIST-primary 57.19 72.521
HITSZ-ICRC-contrastive1 56.44 69.43
2 HITSZ-ICRC-primary 56.41 68.675
*QCRI-contrastive1 56.40 68.27
HITSZ-ICRC-contrastive2 55.22 67.91
ICRC-HIT-contrastive1 53.82 73.18
3 *QCRI-primary 53.74 70.503
4 ECNU-primary 53.47 70.552
ECNU-contrastive1 52.55 69.48
ECNU-contrastive2 52.27 69.38
*QCRI-contrastive2 51.97 69.48
5 ICRC-HIT-primary 49.60 67.866
*VectorSLU-contrastive1 49.54 70.45
6 *VectorSLU-primary 49.10 66.457
7 Shiraz-primary 47.34 56.839
8 FBK-HLT-primary 47.32 69.134
JAIST-contrastive2 46.96 57.74
9 Voltron-primary 46.07 62.358
Voltron-contrastive2 45.16 61.74
Shiraz-contrastive1 45.03 62.55
ICRC-HIT-contrastive2 40.54 60.12
10 CICBUAPnlp-primary 40.40 53.7411
CICBUAPnlp-contrastive1 39.53 52.33
Shiraz-contrastive2 38.00 60.53
11 Yamraj-primary 37.65 45.5012
Yamraj-contrastive2 37.60 44.79
Yamraj-contrastive1 36.30 39.57
12 CoMiC-primary 30.63 54.2010
CoMiC-contrastive1 23.35 50.56
baseline: always “Good” 22.36 50.46
</table>
<tableCaption confidence="0.996844">
Table 4: Subtask A, English: results for all submissions.
</tableCaption>
<bodyText confidence="0.68522575">
The first column shows the rank for the primary submis-
sions according to macro Fl, and the subindex in the last
column shows the rank based on accuracy. Teams marked
with a * include a task co-organizer.
</bodyText>
<sectionHeader confidence="0.988873" genericHeader="method">
5 Participants and Results
</sectionHeader>
<bodyText confidence="0.999788181818182">
The list of all participating teams can be found in Ta-
ble 3. The results for subtask A, English and Arabic,
are shown in Tables 4-5 and 6-7, respectively; those
for subtask B are in Table 8. The systems are ranked
by their macro-averaged F1 scores for their primary
runs (shown in the first column); a ranking based on
accuracy is also shown as a subindex in the last col-
umn. We mark explicitly with an asterisk the teams
that had a task co-organizer as a team member. This
is for information only; these teams competed in the
same conditions as everybody else.
</bodyText>
<table confidence="0.999937384615385">
Submission Macro F, Acc.
1 HITSZ-ICRC 48.13 59.624
2 *QCRI 47.01 62.152
3 ECNU 46.57 61.343
4 FBK-HLT 42.61 62.401
5 Shiraz 40.06 48.5310
6 ICRC-HIT 39.93 59.515
7 *VectorSLU 38.69 54.357
8 CICBUAPnlp 36.13 44.8911
9 JAIST 35.09 54.616
10 Voltron 29.15 50.059
11 Yamraj 24.48 35.9312
12 CoMiC 23.35 51.778
</table>
<tableCaption confidence="0.99948">
Table 5: Subtask A, English with Dialog as a separate
</tableCaption>
<bodyText confidence="0.719099">
category: results for the primary submissions. The first
column shows the rank based on macro Fl, the subindex
in the last column shows the rank based on accuracy.
Teams marked with a * include a task co-organizer.
</bodyText>
<subsectionHeader confidence="0.963902">
5.1 Subtask A, English
</subsectionHeader>
<bodyText confidence="0.996823571428571">
Table 4 shows the results for subtask A, English,
which attracted 12 teams, which submitted 30 runs:
12 primary and 18 contrastive. We can see that all
submissions outperform, in terms of macro F1, the
majority class baseline that always predicts Good
(shown in the last line of the table); for the primary
submissions, this is so by a large margin. However,
in terms of accuracy, one of the primary submissions
falls below the baseline; this might be due to them
optimizing for macro F1 rather than for accuracy.
The best system for this subtask is JAIST, which
ranks first both in the official macro F1 score (57.19)
and in accuracy (72.52); it used a supervised feature-
rich approach, which includes topic models and
word vector representation, with an SVM classifier.
The second best system is HITSZ-ICRC, which
used an ensemble of classifiers. While it ranked sec-
ond in terms of macro F1 (56.41), it was only fifth
on accuracy (68.67); the second best in accuracy was
ECNU, with 70.55.
The third best system, in both macro F1 (53.74)
and accuracy (70.50), is QCRI. In addition to the
features they used for Arabic (see the next subsec-
tion), they further added cosine similarity based on
word embeddings, sentiment polarity lexicons, and
metadata features such as the identity of the users
asking and answering the questions or the existence
of acknowledgments.
</bodyText>
<page confidence="0.990393">
275
</page>
<bodyText confidence="0.99998328125">
Interestingly, the top two systems have contrastive
runs that scored higher than their primary runs both
in terms of macro F1 and accuracy, even though
these differences are small. This is also true for
QCRI’s contrastive run in terms of macro F1 but not
in terms of accuracy, which indicates that they op-
timized for macro F1 for that contrastive run. Note
that ECNU was very close behind QCRI in macro F1
(53.47), and it slightly outperformed it in accuracy.
Note that while most systems trained a four-way
classifier to distinguish Good/Bad/Potential/Dialog,
where Bad includes Bad, Not English and Other,
some systems targetted a three-way distinction
Good/Bad/Potential, following the grouping in Ta-
ble 1, as for the official scoring the scorer was merg-
ing Dialog with Bad anyway.
Table 5 shows the results with four classes. The
last four systems did not predict Dialog, and thus are
severely penalized by macro F1. Comparing Tables
4 and 5, we can see that the scores for the 4-way
classification are up to 10 points lower than for the
3-way case. Distinguishing Dialog from Bad turns
out to be very hard: e.g., HITSZ-ICRC achieved an
F1 of 76.52 for Good, 18.41 for Potential, 40.38 for
Bad, 57.21 for Dialog; however, merging Bad and
Dialog yielded an F1 of 74.32 for the Bad+Dialog
category. The other systems show a similar trend.
Finally, note that Potential is by far the hardest
class (with an F1 lower than 20 for all teams), and it
is also the smallest one, which amplifies its weight
with F1 macro; thus, two teams (CoMiC and FBK-
HLT) have chosen never to predict it.
</bodyText>
<subsectionHeader confidence="0.999026">
5.2 Subtask A, Arabic
</subsectionHeader>
<bodyText confidence="0.999943285714286">
Table 6 shows the results for subtask A, Arabic,
which attracted four teams, which submitted a total
of 11 runs: 4 primary and 7 contrastive. All teams
performed well above a majority class baseline that
always predicts irrelevant.
QCRI was a clear winner with a macro F1 of
78.55 and accuracy of 83.02. They used a set of
features composed of lexical similarities and word
[1, 2]-grams. Most importantly, they exploited the
fact that there is at most one good answer for a given
question: they rank the answers by means of logis-
tic regression, and label the top answer as direct, the
next one as related and the remaining as irrelevant
(a similar strategy is used by some other teams too).
</bodyText>
<table confidence="0.999888461538462">
Submission Macro F, Acc.
1 *QCRI-primary 78.55 83.021
*QCRI-contrastive2 76.97 81.92
*QCRI-contrastive1 76.60 81.82
*VectorSLU-contrastive1 73.18 78.12
2 *VectorSLU-primary 70.99 76.322
HITSZ-ICRC-contrastive1 68.36 73.93
HITSZ-ICRC-contrastive2 67.98 73.23
3 HITSZ-ICRC-primary 67.70 74.533
4 Al-Bayan-primary 67.65 74.533
Al-Bayan-contrastive2 65.70 72.53
Al-Bayan-contrastive1 61.19 71.33
baseline: always “irrelevant” 24.03 56.34
</table>
<tableCaption confidence="0.9734228">
Table 6: Subtask A, Arabic: results for all submissions.
The first column shows the rank for the primary submis-
sions according to macro Fl, and the subindex in the last
column shows the rank based on accuracy. Teams marked
with a * include a task co-organizer.
</tableCaption>
<table confidence="0.999925307692308">
Submission Macro F, Acc.
1 *QCRI-primary 46.09 48.34
*QCRI-contrastive1 43.32 46.36
*QCRI-contrastive2 43.08 49.67
Al-Bayan-contrastive1 42.04 47.02
HITSZ-ICRC-contrastive1 39.61 40.40
HITSZ-ICRC-contrastive2 39.57 40.40
2 HITSZ-ICRC-primary 38.58 39.74
*VectorSLU-contrastive1 36.43 43.05
3 *VectorSLU-primary 36.75 37.09
4 Al-Bayan-primary 34.93 38.41
Al-Bayan-contrastive2 34.42 35.76
baseline: always “irrelevant” 21.73 48.34
</table>
<tableCaption confidence="0.993715">
Table 7: Subtask A, Arabic: results for the 30 manually
annotated Arabic questions.
</tableCaption>
<bodyText confidence="0.999945">
Even though QCRI did not consider semantic
models for this subtask, and the second best team
did, the distance between them is sizeable.
The second place went to VectorSLU (F1=70.99,
Acc=76.32), whose feature vectors incorporated
text-based similarities, embedded word vectors from
both the question and answers, and features based
on normalized ranking scores. Their word embed-
dings were generated with word2vec (Mikolov et al.,
2013), and trained on the Arabic Gigaword corpus.
Their contrastive condition labeled the top scoring
response as direct, the second best as related, and
the others as irrelevant. Their primary condition did
not make use of this constraint.
</bodyText>
<page confidence="0.99534">
276
</page>
<bodyText confidence="0.999946131578947">
Then come HITSZ-ICRC and Al-Bayan, which
are tied on accuracy (74.53), and are almost tied on
macro F1: 67.70 vs. 67.65. HITSZ-ICRC trans-
lated the Arabic to English and then extracted fea-
tures from both the Arabic original and from the En-
glish translation. Al-Bayan had a knowledge-rich
approach that used MADA for morphological anal-
ysis, and then combined information retrieval scores
with explicit semantic analysis in a decision tree.
For all submitted runs, identifying the irrelevant
answers was easiest, with F1 for this class ranging
from 85% to 91%. This was expected, since most of
these answers were randomly selected and thus the
probability of finding common terms between them
and the questions was low. The F1 for detecting the
direct answers ranged from 67% to 77%, while for
the related answers, it was lowest: 47% to 67%.
Table 7 presents the results for the 30 manually
annotated Arabic questions, for which a search en-
gine was used to find possibly irrelevant answers.
We can see that the results are much lower than those
reported in Table 6, which shows that detecting di-
rect and related answers is more challenging when
the irrelevant answers contain many common terms
with the question. The decrease in performance can
be also explained by the different class distribution
in training and testing, e.g., on the average, there are
1.5 direct answers in Test30 vs. just 1 in training,
and the proportion of irrelevant also changed (see
Table 2). The team ranking changed too. QCRI re-
mained the best-performing team, but the worst per-
forming group now has one of its contrastive runs
doing quite well. VectorSLU, which relies heavily
on word overlap and similarity between the question
and the answer experienced a relatively higher drop
in performance compared to the rest. In future work,
we plan to study further the impact of selecting the
irrelevant answers in various challenging ways.
</bodyText>
<subsectionHeader confidence="0.997792">
5.3 Subtask B, English
</subsectionHeader>
<bodyText confidence="0.9997345">
Table 8 shows the results for subtask B, English,
which attracted eight teams, who submitted a total
of 20 runs: 8 primary and 12 contrastive. As for
subtask A, all submissions outperformed the major-
ity class baseline that always predicts Yes (shown in
the last line of the table). However, this is so in terms
of macro F1 only; in terms of accuracy, only half of
the systems managed to beat the baseline.
</bodyText>
<table confidence="0.999881772727273">
Submission Macro F, Acc.
1 *VectorSLU-primary 63.7 721
*VectorSLU-contrastive1 61.9 68
2 ECNU-primary 55.8 682
ECNU-contrastive2 53.9 64
3 *QCRI-primary 53.6 643
3 °HITSZ-ICRC-primary 53.6 643
ECNU-contrastive1 50.6 60
*QCRI-contrastive2 49.0 56
HITSZ-ICRC-contrastive1 42.5 60
HITSZ-ICRC-contrastive2 42.4 60
ICRC-HIT-contrastive2 40.3 60
5 CICBUAPnlp-primary 38.8 446
ICRC-HIT-contrastive1 37.6 56
6 ICRC-HIT-primary 30.9 525
7 Yamraj-primary 29.8 288
Yamraj-contrastive1 29.8 28
CICBUAPnlp-contrastive1 29.1 40
8 FBK-HLT-primary 27.8 407
*QCRI-contrastive1 25.2 56
Yamraj-contrastive2 25.1 36
baseline: always “Yes” 25.0 60
</table>
<tableCaption confidence="0.998691">
Table 8: Subtask B, English: results for all submissions.
</tableCaption>
<bodyText confidence="0.970376523809524">
The first column shows the rank for the primary submis-
sions according to macro Fl, and the subindex in the last
column shows the rank based on accuracy. Teams marked
with a * include a task co-organizer. The submission
marked with a ° was corrected after the deadline.
For most teams, the features used for subtask B
were almost the same as for subtask A, with some
teams adding extra features, e.g., that look for pos-
itive, negative and uncertainty words from small
hand-crafted dictionaries.
Most teams designed systems that make
Yes/No/Unsure decisions at the comment level,
predicting CGOLD YN labels (typically, for the
comments that were predicted to be Good by the
team’s system for subtask A), and were then as-
signed a question-level label using majority voting.4
This is a reasonable strategy as it mirrors the human
annotation process. Some teams tried to extract
features from the whole list of comments and to
predict QGOLD YN directly, but this yielded drop
in performance.
</bodyText>
<footnote confidence="0.8707432">
4In fact, the authors of the third-best system HITSZ-ICRC
submitted by mistake for their primary run predictions for
CGOLD YN instead of QGOLD YN; the results reported in
Table 8 for this team were obtained by converting these pre-
dictions using simple majority voting.
</footnote>
<page confidence="0.995332">
277
</page>
<bodyText confidence="0.999982071428571">
The top-performing system, in both macro F1
(63.7) and accuracy (72), is VectorSLU. It is fol-
lowed by ECNU with F1=55.8, Acc=68. The third
place is shared by QCRI and HITSZ-ICRC, which
have exactly the same scores (F1=53.6, Acc=64),
but different errors and different confusion matrices.
These four systems are much better than the rest; the
next system is far behind at F1=38.8, Acc=44.
Interestingly, once again there is a tie for the third
place between the participating teams, as was the
case for subtask A, Arabic and English. Note, how-
ever, that this time all top systems’ primary runs per-
formed better than their corresponding contrastive
runs, which was not the case for subtask A.
</bodyText>
<sectionHeader confidence="0.992477" genericHeader="method">
6 Features and Techniques
</sectionHeader>
<bodyText confidence="0.999147">
Most systems were supervised,5 and thus the main
efforts were focused on feature engineering. We can
group the features participants used into the follow-
ing four categories:
</bodyText>
<listItem confidence="0.957423769230769">
• question-specific features: e.g., length of the
question, words/stems/lemmata/n-grams in the
question, etc.
• comment-specific features: e.g., length of
the comment, words/stems/lemmata/n-grams
in the question, punctuation (e.g., does the
comment contain a question mark), proportion
of positive/negative sentiment words, rank of
the comment in the list of comments, named
entities (locations, organizations), formality
of the language used, surface features (e.g.,
phones, URLs), etc.
• features about the question-comment pair:
</listItem>
<bodyText confidence="0.9083162">
various kinds of similarity between the ques-
tion and the comment (e.g., lexical based on co-
sine, or based on WordNet, language modeling,
topic models such as LDA or explicit seman-
tic analysis), word/lemma/stem/n-gram/POS
overlap between the question and the com-
ment (e.g., greedy string tiling, longest com-
mon subsequences, Jaccard coefficient, con-
tainment, etc.), information gain from the com-
ment with respect to the question, etc.
</bodyText>
<footnote confidence="0.97686">
5The only two exceptions were Yamraj (unsupervised) and
CICBUAPnlp (semi-supervised).
</footnote>
<listItem confidence="0.95665075">
• metadata features: ID of the user who asked
the question, ID of the one who posted the com-
ment, whether they are the same, known num-
ber of Good/Bad/Potential comments (in the
</listItem>
<bodyText confidence="0.98281688372093">
training data) written by the user who wrote the
comment, timestamp, question category, etc.
Note that the metadata features overlap with the
other three groups as a metadata feature is about the
question, about the comment, or about the question-
comment pair. Note also that the features above
can be binary, integer, or real-valued, e.g., can be
calculated using various weighting schemes such as
TF.IDF for words/lemmata/stems.
Although most participants focused on engineer-
ing features to be used with a standard classifier such
as SVM or a decision tree, some also used more ad-
vanced techniques. For example, some teams used
sequence or partial tree kernels (Moschitti, 2006).
Another popular technique was to use word embed-
dings, e.g., modeled using convolution or recurrent
neural networks, or with latent semantic analysis,
and also vectors trained using word2vec and GloVe
(Pennington et al., 2014), as pre-trained on Google
News or Wikipedia, or trained on the provided Qatar
Living data. Less popular techniques included dia-
log modeling for the list of comments for a given
question, e.g., using conditional random fields to
model the sequence of comment labels (Good, Bad,
Potential, Dialog), mapping the question and the
comment to a graph structure and performing graph
traversal, using word alignments between the ques-
tion and the comment, time modeling, and senti-
ment analysis. Finally, for Arabic, some participants
translated the Arabic data to English, and then ex-
tracted features from both the Arabic and the En-
glish version; this is helpful, as there are many more
tools and resources for English than for Arabic.
When building their systems, participants used
a number of tools and resources for preprocessing,
feature extraction, and machine learning, includ-
ing Deeplearning4J, DKPro, GATE, GloVe, Google
translate, HeidelTime, LibLinear, LibSVM, MADA,
Mallet, Meteor, Networkx, NLTK, NRC-Canada
sentiment lexicons, PPDB, sklearn, Spam filtering
corpus, Stanford NLP toolkit, TakeLab, TiMBL,
UIMA, Weka, Wikipedia, Wiktionary, word2vec,
WordNet, and WTMF.
</bodyText>
<page confidence="0.993601">
278
</page>
<bodyText confidence="0.999951923076923">
There was also a rich variety of preprocess-
ing techniques used, including sentence splitting,
tokenization, stemming, lemmatization, morpho-
logical analysis (esp. for Arabic), dependency
parsing, part of speech tagging, temporal tag-
ging, named entity recognition, gazetteer match-
ing, word alignment between the question and the
comment, word embedding, spam filtering, remov-
ing some content (e.g., all contents enclosed in
HTML tags, emoticons, repetitive punctuation, stop-
words, the ending signature, URLs, etc.) substi-
tuting (e.g., HTML character encodings and some
common slang words), etc.
</bodyText>
<sectionHeader confidence="0.998398" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.996823375000001">
The task attracted 13 teams and 61 submissions.
Naturally, the English subtasks were more popular
(with 12 and 8 teams for subtasks A and B, respec-
tively; compared to just 4 for Arabic): there are more
tools and resources for English as well as more gen-
eral research interest. Moreover, the English data
followed the natural discussion threads in a forum,
while the Arabic data was somewhat artificial.
We have seen that all submissions managed to
outperform, on the official macro F1 metric,6 a ma-
jority class baseline for both subtasks and for both
languages; this improvement is smaller for English
and much larger for Arabic. However, if we consider
accuracy, many systems fall below the baseline for
English in both subtasks.
Overall, the results for Arabic are higher than
those for English for subtask A, e.g., there is an
absolute difference of over 21 points in macro F1
(78.55 vs. 57.19) for the top systems. This suggests
that the Arabic task was generally easier. Indeed,
it uses very formal polished language both for the
questions and the answers (as opposed to the noisy
English forum data); moreover, it is known a priori
that each question can have at most one direct an-
swer, and the teams have exploited this information.
However, looking at accuracy, the difference be-
tween the top systems for Arabic and English is just
10 points (82.02 vs. 72.52). This suggests that part
of the bigger difference for F1 macro comes from
the measure itself.
6Curiously, there was a close tie for the third place for all
three subtask-language combinations.
Indeed, having a closer look at the distribu-
tion of the F1 values for the different classes be-
fore the macro averaging, we can see that the re-
sults are much more balanced for Arabic (F1 of
77.31/67.13/91.21 for direct/related/irrelevant; with
P and R very close to F1) than for English (F1 of
78.96/14.36/78.24 for Good/Potential/Bad; with P
and R very close to F1). We can see that the Poten-
tial class is the hardest. This can hurt the accuracy
but only slightly as this class is the smallest. How-
ever, it can still have a major impact on macro-F1
due to the effect of macro-averaging.
Overall, for both Arabic and English, it was much
easier to recognize Good/direct and Bad/irrelevant
examples (P, R, F1 about 80-90), and much harder
to do so for Potential/related (P, R, F1 around 67 for
Arabic, and 14 for English). This should not be sur-
prising, as this intermediate category is easily con-
fusable with the other two: for Arabic, these are an-
swers to related questions, while for English, this is
a category that was quite hard for human annotators.
We should say that even though we had used ma-
jority voting to ensure agreement between annota-
tors, we were still worried about the the quality of
human annotations collected on Amazon’s Mechan-
ical Turk. Thus, we asked eight people to do a man-
ual re-annotation of the QGOLD YN labels for the
test data. We found a very high degree of agree-
ment between each of the human annotators and the
Turkers. Originally, there were 29 YES/NO ques-
tions, but we found that four of them were arguably
general rather than YES/NO, and thus we excluded
them. For the remaining 25 questions, we had a dis-
cussion between our annotators about any potential
disagreement, and finally, we arrived with a new an-
notation that changed the labels of three questions.
This corresponds to an agreement of 22/25=0.88 be-
tween our consolidated annotation and the Turkers,
which is very high. This new annotation was the
one we used for the final scoring. Note that using
the original Turkers’ labels yielded slightly different
scores but exactly the same ranking for the systems.
The high agreement between our re-annotations and
the Turkers and the fact that the ranking did not
change makes us optimistic about the quality of the
annotations for subtask A too (even though we are
aware of some errors and inconsistencies in the an-
notations).
</bodyText>
<page confidence="0.997053">
279
</page>
<sectionHeader confidence="0.967177" genericHeader="conclusions">
8 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999825735294118">
We have described a new task that entered SemEval-
2015: task 3 on Answer Selection in Community
Question Answering. The task has attracted a rea-
sonably high number of submissions: a total of 61
by 13 teams. The teams experimented with a large
number of features, resources and approaches, and
we believe that the lessons learned will be useful for
the overall development of the field of community
question answering. Moreover, the datasets that we
have created as part of the task, and which we have
released for use to the community,7 should be useful
beyond SemEval.
In our task description, we especially encouraged
solutions going beyond simple keyword and bag-
of-words matching, e.g., using semantic or com-
plex linguistic information in order to reason about
the relation between questions and answers. Al-
though participants experimented with a broad va-
riety of features (including semantic word-based
representations, syntactic relations, contextual fea-
tures, meta-information, and external resources), we
feel that much more can be done in this direc-
tion. Ultimately, the question of whether com-
plex linguistically-based representations and infer-
ence can be successfully applied to the very informal
and ungrammatical text from cQA forums remains
unanswered to a large extent.
Complementary to the research direction pre-
sented by this year’s task, we plan to run a follow-
up task at SemEval-2016, with a focus on answering
new questions, i.e., that were not already answered
in Qatar Living. For Arabic, we plan to use a real
community question answering dataset, similar to
Qatar Living for English.
</bodyText>
<sectionHeader confidence="0.997464" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99912325">
This research is developed by the Arabic Language
Technologies (ALT) group at Qatar Computing Re-
search Institute (QCRI) within the Qatar Foundation
in collaboration with MIT. It is part of the Interactive
sYstems for Answer Search (Iyas) project.
We would like to thank Nicole Schmidt from MIT
for her help with setting up and running the Amazon
Mechanical Turk annotation tasks.
</bodyText>
<footnote confidence="0.894436">
7http://alt.qcri.org/semeval2015/task3/
</footnote>
<sectionHeader confidence="0.899481" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999303134615384">
Yonatan Belinkov, Mitra Mohtarami, Scott Cyphers, and
James Glass. 2015. VectorSLU: A continuous word
vector approach to answer selection in community
question answering systems. In Proceedings of the 9th
International Workshop on Semantic Evaluation, Se-
mEval ’15, Denver, Colorado, USA.
Michael Heilman and Noah A. Smith. 2010. Tree
edit models for recognizing textual entailments, para-
phrases, and answers to questions. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, HLT ’10, pages 1011–
1019, Los Angeles, California, USA.
Amin Heydari Alashty, Saeed Rahmani, Meysam Roost-
aee, and Mostafa Fakhrahmad. 2015. Shiraz: A pro-
posed list wise approach to answer validation. In Pro-
ceedings of the 9th International Workshop on Seman-
tic Evaluation, SemEval ’15, Denver, Colorado, USA.
Yongshuai Hou, Cong Tan, Xiaolong Wang, Yaoyun
Zhang, Jun Xu, and Qingcai Chen. 2015. HITSZ-
ICRC: Exploiting classification approach for answer
selection in community question answering. In Pro-
ceedings of the 9th International Workshop on Seman-
tic Evaluation, SemEval ’15, Denver, Colorado, USA.
Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005.
Finding similar questions in large question and an-
swer archives. In Proceedings of the 14th ACM Inter-
national Conference on Information and Knowledge
Management, CIKM ’05, pages 84–90, Bremen, Ger-
many.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013. Distributed representations
of words and phrases and their compositionality. In
C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani,
and K.Q. Weinberger, editors, Advances in Neural In-
formation Processing Systems 26, pages 3111–3119.
Curran Associates, Inc.
Reham Mohamed, Maha Ragab, Heba Abdelnasser,
Nagwa M. El-Makky, and Marwan Torki. 2015. Al-
Bayan: A knowledge-based system for Arabic answer
selection. In Proceedings of the 9th International
Workshop on Semantic Evaluation, SemEval ’15, Den-
ver, Colorado, USA.
Alessandro Moschitti, Silvia Quarteroni, Roberto Basili,
and Suresh Manandhar. 2007. Exploiting syntactic
and shallow semantic kernels for question answer clas-
sification. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
ACL ’07, pages 776–783, Prague, Czech Republic.
Alessandro Moschitti. 2006. Efficient Convolution Ker-
nels for Dependency and Constituent Syntactic Trees.
In Johannes F¨urnkranz, Tobias Scheffer, and Myra
</reference>
<page confidence="0.951358">
280
</page>
<reference confidence="0.999796621359223">
Spiliopoulou, editors, Machine Learning: ECML
2006, volume 4212 of Lecture Notes in Computer Sci-
ence, pages 318–329. Springer Berlin Heidelberg.
Massimo Nicosia, Simone Filice, Alberto Barr´on-
Cede˜no, Iman Saleh, Hamdy Mubarak, Wei Gao,
Preslav Nakov, Giovanni Da San Martino, Alessandro
Moschitti, Kareem Darwish, Llu´ıs M`arquez, Shafiq
Joty, and Walid Magdy. 2015. QCRI: Answer selec-
tion for community question answering - experiments
for Arabic and English. In Proceedings of the 9th
International Workshop on Semantic Evaluation, Se-
mEval ’15, Denver, Colorado, USA.
Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word rep-
resentation. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing,
EMNLP ’14, pages 1532–1543, Doha, Qatar.
Filip Radlinski and Thorsten Joachims. 2005. Query
chains: Learning to rank from implicit feedback. In
Proceedings of the Eleventh ACM SIGKDD Interna-
tional Conference on Knowledge Discovery in Data
Mining, KDD ’05, pages 239–248, Chicago, Illinois,
USA.
Bj¨orn Rudzewitz and Ramon Ziai. 2015. CoMiC: Adapt-
ing a short answer assessment system for answer se-
lection. In Proceedings of the 9th International Work-
shop on Semantic Evaluation, SemEval ’15, Denver,
Colorado, USA.
Aliaksei Severyn and Alessandro Moschitti. 2012.
Structural relationships for large-scale learning of an-
swer re-ranking. In Proceedings of the 35th Interna-
tional ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, SIGIR ’12, pages
741–750, Portland, Oregon, USA.
Aliaksei Severyn and Alessandro Moschitti. 2013. Au-
tomatic feature engineering for answer selection and
extraction. In Proceedings of the 2013 Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP ’13, pages 458–467, Seattle, Washing-
ton, USA.
Dan Shen and Mirella Lapata. 2007. Using semantic
roles to improve question answering. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ’07,
pages 12–21, Prague, Czech Republic.
Mihai Surdeanu, Massimiliano Ciaramita, and Hugo
Zaragoza. 2008. Learning to rank answers on
large online QA collections. In Proceedings of the
46th Annual Meeting of the Association for Compu-
tational Linguistics and the Human Language Tech-
nology Conference, ACL-HLT ’08, pages 719–727,
Columbus, Ohio, USA.
Quan Hung Tran, Vu Tran, Tu Vu, Minh Nguyen, and
Son Bao Pham. 2015. JAIST: Combining multiple
features for answer selection in community question
answering. In Proceedings of the 9th International
Workshop on Semantic Evaluation, SemEval ’15, Den-
ver, Colorado, USA.
Ngoc Phuoc An Vo, Simone Magnolini, and Octavian
Popescu. 2015. FBK-HLT: An application of seman-
tic textual similarity for answer selection in commu-
nity question answering. In Proceedings of the 9th
International Workshop on Semantic Evaluation, Se-
mEval ’15, Denver, Colorado, USA.
Mengqiu Wang and Christopher D. Manning. 2010.
Probabilistic tree-edit models with structured latent
variables for textual entailment and question answer-
ing. In Proceedings of the 23rd International Con-
ference on Computational Linguistics, COLING ’10,
pages 1164–1172, Beijing, China.
Mengqiu Wang, Noah A. Smith, and Teruko Mitamura.
2007. What is the Jeopardy model? A quasi-
synchronous grammar for QA. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, EMNLP-CoNLL ’07, pages 22–
32, Prague, Czech Republic.
Xuchen Yao, Benjamin Van Durme, Chris Callison-
Burch, and Peter Clark. 2013. Answer extraction
as sequence tagging with tree edit distance. In Pro-
ceedings of the 2013 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, NAACL-
HLT ’13, pages 858–867.
Liang Yi, JianXiang Wang, and Man Lan. 2015. ECNU:
Using multiple sources of CQA-based information for
answers selection and YES/NO response inference. In
Proceedings of the 9th International Workshop on Se-
mantic Evaluation, SemEval ’15, Denver, Colorado,
USA.
Ivan Zamanov, Marina Kraeva, Nelly Hateva, Ivana
Yovcheva, Ivelina Nikolova, and Galia Angelova.
2015. Voltron: A hybrid system for answer validation
based on lexical and distance features. In Proceedings
of the 9th International Workshop on Semantic Evalu-
ation, SemEval ’15, Denver, Colorado, USA.
Xiaoqiang Zhou, Baotian Hu, Jiaxin Lin, Yang xiang,
and Xiaolong Wang. 2015. ICRC-HIT: A deep learn-
ing based comment sequence labeling system for an-
swer selection challenge. In Proceedings of the 9th
International Workshop on Semantic Evaluation, Se-
mEval ’15, Denver, Colorado, USA.
</reference>
<page confidence="0.997912">
281
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.129318">
<title confidence="0.995648">SemEval-2015 Task 3: Answer Selection in Community Question Answering</title>
<author confidence="0.939616">Preslav Nakov Lluis M`arquez Walid Magdy Alessandro</author>
<affiliation confidence="0.946367">ALT Research Group, Qatar Computing Research Institute</affiliation>
<author confidence="0.997085">James Glass</author>
<affiliation confidence="0.628468666666667">MIT Computer Science and Artificial Intelligence Laboratory Bilal Qatar Living</affiliation>
<abstract confidence="0.953447192307692">Community Question Answering (cQA) provides new interesting research directions to the traditional Question Answering (QA) field, e.g., the exploitation of the interaction between users and the structure of related posts. In this context, we organized SemEval- Task 3 on Selection in which included two subtasks: (a) classifying as or relevant with respect to the question, and (b) answering YES/NO question with or based on the list of all answers. We set subtask A for Arabic and English on two relatively different cQA domains, i.e., the Qatar Living website for English, and a Quran-related website for Arabic. We used crowdsourcing on Amazon Mechanical Turk to label a large English training dataset, which we released to the research community. Thirteen teams participated in the challenge with a total of 61 submissions: 24 primary and 37 contrastive. The best systems achieved an official score of 57.19 and 63.7 for the English subtasks A and B, and 78.55 for the Arabic subtask A.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yonatan Belinkov</author>
<author>Mitra Mohtarami</author>
<author>Scott Cyphers</author>
<author>James Glass</author>
</authors>
<title>VectorSLU: A continuous word vector approach to answer selection in community question answering systems.</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’15,</booktitle>
<location>Denver, Colorado, USA.</location>
<contexts>
<context position="18486" citStr="Belinkov et al., 2015" startWordPosition="3100" endWordPosition="3103">MiC University of T¨ubingen, Germany (Rudzewitz and Ziai, 2015) ECNU East China Normal University, China (Yi et al., 2015) FBK-HLT Fondazione Bruno Kessler, Italy (Vo et al., 2015) HITSZ-ICRC Harbin Institute of Technology, China (Hou et al., 2015) ICRC-HIT Harbin Institute of Technology, China (Zhou et al., 2015) JAIST Japan Advance Institute of Science and Technology, Japan (Tran et al., 2015) QCRI Qatar Computing Research Institute, Qatar (Nicosia et al., 2015) Shiraz Shiraz University, Iran (Heydari Alashty et al., 2015) VectorSLU MIT Computer Science and Artificial Intelligence Lab, USA (Belinkov et al., 2015) Voltron Sofia University, Bulgaria (Zamanov et al., 2015) Yamraj Masaryk University, Czech Republic Table 3: The participating teams. 274 Submission Macro F, Acc. JAIST-contrastive1 57.29 72.67 1 JAIST-primary 57.19 72.521 HITSZ-ICRC-contrastive1 56.44 69.43 2 HITSZ-ICRC-primary 56.41 68.675 *QCRI-contrastive1 56.40 68.27 HITSZ-ICRC-contrastive2 55.22 67.91 ICRC-HIT-contrastive1 53.82 73.18 3 *QCRI-primary 53.74 70.503 4 ECNU-primary 53.47 70.552 ECNU-contrastive1 52.55 69.48 ECNU-contrastive2 52.27 69.38 *QCRI-contrastive2 51.97 69.48 5 ICRC-HIT-primary 49.60 67.866 *VectorSLU-contrastive1 4</context>
</contexts>
<marker>Belinkov, Mohtarami, Cyphers, Glass, 2015</marker>
<rawString>Yonatan Belinkov, Mitra Mohtarami, Scott Cyphers, and James Glass. 2015. VectorSLU: A continuous word vector approach to answer selection in community question answering systems. In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’15, Denver, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Heilman</author>
<author>Noah A Smith</author>
</authors>
<title>Tree edit models for recognizing textual entailments, paraphrases, and answers to questions. In Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>1011--1019</pages>
<location>Los Angeles, California, USA.</location>
<contexts>
<context position="4170" citStr="Heilman and Smith (2010)" startWordPosition="671" endWordPosition="674"> A goes in the direction of passage reranking, where automatic classifiers are normally applied to pairs of questions and answer passages to derive a relative order between passages, e.g., see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008). In recent years, many advanced models have been developed for automating answer selection, producing a large body of work.2 For instance, Wang et al. (2007) proposed a probabilistic quasisynchronous grammar to learn syntactic transformations from the question to the candidate answers; Heilman and Smith (2010) used an algorithm based on Tree Edit Distance (TED) to learn tree transformations in pairs; Wang and Manning (2010) developed a probabilistic model to learn tree-edit operations on dependency parse trees; and Yao et al. (2013) applied linear chain CRFs with features derived from TED to automatically learn associations between questions and candidate answers. One interesting aspect of the above research is the need for syntactic structures; this is also corroborated in (Severyn and Moschitti, 2012; Severyn and Moschitti, 2013). Note that answer selection can use models for textual entailment, </context>
</contexts>
<marker>Heilman, Smith, 2010</marker>
<rawString>Michael Heilman and Noah A. Smith. 2010. Tree edit models for recognizing textual entailments, paraphrases, and answers to questions. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 1011– 1019, Los Angeles, California, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amin Heydari Alashty</author>
<author>Saeed Rahmani</author>
<author>Meysam Roostaee</author>
<author>Mostafa Fakhrahmad</author>
</authors>
<title>Shiraz: A proposed list wise approach to answer validation.</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’15,</booktitle>
<location>Denver, Colorado, USA.</location>
<contexts>
<context position="18394" citStr="Alashty et al., 2015" startWordPosition="3087" endWordPosition="3090">versity, Egypt (Mohamed et al., 2015) CICBUAPnlp Instituto Polit´ecnico Nacional, Mexico CoMiC University of T¨ubingen, Germany (Rudzewitz and Ziai, 2015) ECNU East China Normal University, China (Yi et al., 2015) FBK-HLT Fondazione Bruno Kessler, Italy (Vo et al., 2015) HITSZ-ICRC Harbin Institute of Technology, China (Hou et al., 2015) ICRC-HIT Harbin Institute of Technology, China (Zhou et al., 2015) JAIST Japan Advance Institute of Science and Technology, Japan (Tran et al., 2015) QCRI Qatar Computing Research Institute, Qatar (Nicosia et al., 2015) Shiraz Shiraz University, Iran (Heydari Alashty et al., 2015) VectorSLU MIT Computer Science and Artificial Intelligence Lab, USA (Belinkov et al., 2015) Voltron Sofia University, Bulgaria (Zamanov et al., 2015) Yamraj Masaryk University, Czech Republic Table 3: The participating teams. 274 Submission Macro F, Acc. JAIST-contrastive1 57.29 72.67 1 JAIST-primary 57.19 72.521 HITSZ-ICRC-contrastive1 56.44 69.43 2 HITSZ-ICRC-primary 56.41 68.675 *QCRI-contrastive1 56.40 68.27 HITSZ-ICRC-contrastive2 55.22 67.91 ICRC-HIT-contrastive1 53.82 73.18 3 *QCRI-primary 53.74 70.503 4 ECNU-primary 53.47 70.552 ECNU-contrastive1 52.55 69.48 ECNU-contrastive2 52.27 69</context>
</contexts>
<marker>Alashty, Rahmani, Roostaee, Fakhrahmad, 2015</marker>
<rawString>Amin Heydari Alashty, Saeed Rahmani, Meysam Roostaee, and Mostafa Fakhrahmad. 2015. Shiraz: A proposed list wise approach to answer validation. In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’15, Denver, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yongshuai Hou</author>
<author>Cong Tan</author>
<author>Xiaolong Wang</author>
<author>Yaoyun Zhang</author>
<author>Jun Xu</author>
<author>Qingcai Chen</author>
</authors>
<title>HITSZICRC: Exploiting classification approach for answer selection in community question answering.</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’15,</booktitle>
<location>Denver, Colorado, USA.</location>
<contexts>
<context position="18112" citStr="Hou et al., 2015" startWordPosition="3045" endWordPosition="3048"> For English, subtask A they are Good, Potential, and Bad. • For Arabic, subtask A these are direct, related, and irrelevant. • For English, subtask B they are Yes, No, and Unsure. We also report classification accuracy. Team ID Affiliation and reference Al-Bayan Alexandria University, Egypt (Mohamed et al., 2015) CICBUAPnlp Instituto Polit´ecnico Nacional, Mexico CoMiC University of T¨ubingen, Germany (Rudzewitz and Ziai, 2015) ECNU East China Normal University, China (Yi et al., 2015) FBK-HLT Fondazione Bruno Kessler, Italy (Vo et al., 2015) HITSZ-ICRC Harbin Institute of Technology, China (Hou et al., 2015) ICRC-HIT Harbin Institute of Technology, China (Zhou et al., 2015) JAIST Japan Advance Institute of Science and Technology, Japan (Tran et al., 2015) QCRI Qatar Computing Research Institute, Qatar (Nicosia et al., 2015) Shiraz Shiraz University, Iran (Heydari Alashty et al., 2015) VectorSLU MIT Computer Science and Artificial Intelligence Lab, USA (Belinkov et al., 2015) Voltron Sofia University, Bulgaria (Zamanov et al., 2015) Yamraj Masaryk University, Czech Republic Table 3: The participating teams. 274 Submission Macro F, Acc. JAIST-contrastive1 57.29 72.67 1 JAIST-primary 57.19 72.521 HI</context>
</contexts>
<marker>Hou, Tan, Wang, Zhang, Xu, Chen, 2015</marker>
<rawString>Yongshuai Hou, Cong Tan, Xiaolong Wang, Yaoyun Zhang, Jun Xu, and Qingcai Chen. 2015. HITSZICRC: Exploiting classification approach for answer selection in community question answering. In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’15, Denver, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiwoon Jeon</author>
<author>W Bruce Croft</author>
<author>Joon Ho Lee</author>
</authors>
<title>Finding similar questions in large question and answer archives.</title>
<date>2005</date>
<booktitle>In Proceedings of the 14th ACM International Conference on Information and Knowledge Management, CIKM ’05,</booktitle>
<pages>84--90</pages>
<location>Bremen, Germany.</location>
<contexts>
<context position="3787" citStr="Jeon et al., 2005" startWordPosition="611" endWordPosition="614">articipation, and to focus on aspects that are relevant for the SemEval community, namely on learning the relationship between two pieces of text. 1http://www.qatarliving.com/forum/ 269 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 269–281, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics Subtask A goes in the direction of passage reranking, where automatic classifiers are normally applied to pairs of questions and answer passages to derive a relative order between passages, e.g., see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008). In recent years, many advanced models have been developed for automating answer selection, producing a large body of work.2 For instance, Wang et al. (2007) proposed a probabilistic quasisynchronous grammar to learn syntactic transformations from the question to the candidate answers; Heilman and Smith (2010) used an algorithm based on Tree Edit Distance (TED) to learn tree transformations in pairs; Wang and Manning (2010) developed a probabilistic model to learn tree-edit operations on dependency parse trees; and Yao et </context>
</contexts>
<marker>Jeon, Croft, Lee, 2005</marker>
<rawString>Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005. Finding similar questions in large question and answer archives. In Proceedings of the 14th ACM International Conference on Information and Knowledge Management, CIKM ’05, pages 84–90, Bremen, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>Advances in Neural Information Processing Systems 26,</booktitle>
<pages>3111--3119</pages>
<editor>In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors,</editor>
<publisher>Curran Associates, Inc.</publisher>
<contexts>
<context position="26409" citStr="Mikolov et al., 2013" startWordPosition="4332" endWordPosition="4335">Bayan-primary 34.93 38.41 Al-Bayan-contrastive2 34.42 35.76 baseline: always “irrelevant” 21.73 48.34 Table 7: Subtask A, Arabic: results for the 30 manually annotated Arabic questions. Even though QCRI did not consider semantic models for this subtask, and the second best team did, the distance between them is sizeable. The second place went to VectorSLU (F1=70.99, Acc=76.32), whose feature vectors incorporated text-based similarities, embedded word vectors from both the question and answers, and features based on normalized ranking scores. Their word embeddings were generated with word2vec (Mikolov et al., 2013), and trained on the Arabic Gigaword corpus. Their contrastive condition labeled the top scoring response as direct, the second best as related, and the others as irrelevant. Their primary condition did not make use of this constraint. 276 Then come HITSZ-ICRC and Al-Bayan, which are tied on accuracy (74.53), and are almost tied on macro F1: 67.70 vs. 67.65. HITSZ-ICRC translated the Arabic to English and then extracted features from both the Arabic original and from the English translation. Al-Bayan had a knowledge-rich approach that used MADA for morphological analysis, and then combined inf</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 3111–3119. Curran Associates, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reham Mohamed</author>
<author>Maha Ragab</author>
<author>Heba Abdelnasser</author>
<author>Nagwa M El-Makky</author>
<author>Marwan Torki</author>
</authors>
<title>AlBayan: A knowledge-based system for Arabic answer selection.</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’15,</booktitle>
<location>Denver, Colorado, USA.</location>
<contexts>
<context position="17810" citStr="Mohamed et al., 2015" startWordPosition="3002" endWordPosition="3005">nswers using information retrieval to guarantee significant term overlap with the questions. For the general testset, we used these 30 questions and 170 more where the irrelevant answers were chosen randomly. 4 Scoring The official score for both subtasks is Fl, macroaveraged over the target categories: • For English, subtask A they are Good, Potential, and Bad. • For Arabic, subtask A these are direct, related, and irrelevant. • For English, subtask B they are Yes, No, and Unsure. We also report classification accuracy. Team ID Affiliation and reference Al-Bayan Alexandria University, Egypt (Mohamed et al., 2015) CICBUAPnlp Instituto Polit´ecnico Nacional, Mexico CoMiC University of T¨ubingen, Germany (Rudzewitz and Ziai, 2015) ECNU East China Normal University, China (Yi et al., 2015) FBK-HLT Fondazione Bruno Kessler, Italy (Vo et al., 2015) HITSZ-ICRC Harbin Institute of Technology, China (Hou et al., 2015) ICRC-HIT Harbin Institute of Technology, China (Zhou et al., 2015) JAIST Japan Advance Institute of Science and Technology, Japan (Tran et al., 2015) QCRI Qatar Computing Research Institute, Qatar (Nicosia et al., 2015) Shiraz Shiraz University, Iran (Heydari Alashty et al., 2015) VectorSLU MIT C</context>
</contexts>
<marker>Mohamed, Ragab, Abdelnasser, El-Makky, Torki, 2015</marker>
<rawString>Reham Mohamed, Maha Ragab, Heba Abdelnasser, Nagwa M. El-Makky, and Marwan Torki. 2015. AlBayan: A knowledge-based system for Arabic answer selection. In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’15, Denver, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Silvia Quarteroni</author>
<author>Roberto Basili</author>
<author>Suresh Manandhar</author>
</authors>
<title>Exploiting syntactic and shallow semantic kernels for question answer classification.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, ACL ’07,</booktitle>
<pages>776--783</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="3834" citStr="Moschitti et al., 2007" startWordPosition="619" endWordPosition="622"> are relevant for the SemEval community, namely on learning the relationship between two pieces of text. 1http://www.qatarliving.com/forum/ 269 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 269–281, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics Subtask A goes in the direction of passage reranking, where automatic classifiers are normally applied to pairs of questions and answer passages to derive a relative order between passages, e.g., see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008). In recent years, many advanced models have been developed for automating answer selection, producing a large body of work.2 For instance, Wang et al. (2007) proposed a probabilistic quasisynchronous grammar to learn syntactic transformations from the question to the candidate answers; Heilman and Smith (2010) used an algorithm based on Tree Edit Distance (TED) to learn tree transformations in pairs; Wang and Manning (2010) developed a probabilistic model to learn tree-edit operations on dependency parse trees; and Yao et al. (2013) applied linear chain CRFs with featu</context>
</contexts>
<marker>Moschitti, Quarteroni, Basili, Manandhar, 2007</marker>
<rawString>Alessandro Moschitti, Silvia Quarteroni, Roberto Basili, and Suresh Manandhar. 2007. Exploiting syntactic and shallow semantic kernels for question answer classification. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, ACL ’07, pages 776–783, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees.</title>
<date>2006</date>
<booktitle>Machine Learning: ECML 2006,</booktitle>
<volume>4212</volume>
<pages>318--329</pages>
<editor>In Johannes F¨urnkranz, Tobias Scheffer, and Myra Spiliopoulou, editors,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="33734" citStr="Moschitti, 2006" startWordPosition="5496" endWordPosition="5497">tamp, question category, etc. Note that the metadata features overlap with the other three groups as a metadata feature is about the question, about the comment, or about the questioncomment pair. Note also that the features above can be binary, integer, or real-valued, e.g., can be calculated using various weighting schemes such as TF.IDF for words/lemmata/stems. Although most participants focused on engineering features to be used with a standard classifier such as SVM or a decision tree, some also used more advanced techniques. For example, some teams used sequence or partial tree kernels (Moschitti, 2006). Another popular technique was to use word embeddings, e.g., modeled using convolution or recurrent neural networks, or with latent semantic analysis, and also vectors trained using word2vec and GloVe (Pennington et al., 2014), as pre-trained on Google News or Wikipedia, or trained on the provided Qatar Living data. Less popular techniques included dialog modeling for the list of comments for a given question, e.g., using conditional random fields to model the sequence of comment labels (Good, Bad, Potential, Dialog), mapping the question and the comment to a graph structure and performing gr</context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>Alessandro Moschitti. 2006. Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees. In Johannes F¨urnkranz, Tobias Scheffer, and Myra Spiliopoulou, editors, Machine Learning: ECML 2006, volume 4212 of Lecture Notes in Computer Science, pages 318–329. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Massimo Nicosia</author>
<author>Simone Filice</author>
<author>Alberto Barr´onCede˜no</author>
<author>Iman Saleh</author>
<author>Hamdy Mubarak</author>
<author>Wei Gao</author>
<author>Preslav Nakov</author>
<author>Giovanni Da San Martino</author>
<author>Alessandro Moschitti</author>
<author>Kareem Darwish</author>
<author>Llu´ıs M`arquez</author>
<author>Shafiq Joty</author>
<author>Walid Magdy</author>
</authors>
<title>QCRI: Answer selection for community question answering - experiments for Arabic and English.</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’15,</booktitle>
<location>Denver, Colorado, USA.</location>
<marker>Nicosia, Filice, Barr´onCede˜no, Saleh, Mubarak, Gao, Nakov, Martino, Moschitti, Darwish, M`arquez, Joty, Magdy, 2015</marker>
<rawString>Massimo Nicosia, Simone Filice, Alberto Barr´onCede˜no, Iman Saleh, Hamdy Mubarak, Wei Gao, Preslav Nakov, Giovanni Da San Martino, Alessandro Moschitti, Kareem Darwish, Llu´ıs M`arquez, Shafiq Joty, and Walid Magdy. 2015. QCRI: Answer selection for community question answering - experiments for Arabic and English. In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’15, Denver, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher Manning</author>
</authors>
<title>Glove: Global vectors for word representation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP ’14,</booktitle>
<pages>1532--1543</pages>
<location>Doha, Qatar.</location>
<contexts>
<context position="33961" citStr="Pennington et al., 2014" startWordPosition="5528" endWordPosition="5531">tures above can be binary, integer, or real-valued, e.g., can be calculated using various weighting schemes such as TF.IDF for words/lemmata/stems. Although most participants focused on engineering features to be used with a standard classifier such as SVM or a decision tree, some also used more advanced techniques. For example, some teams used sequence or partial tree kernels (Moschitti, 2006). Another popular technique was to use word embeddings, e.g., modeled using convolution or recurrent neural networks, or with latent semantic analysis, and also vectors trained using word2vec and GloVe (Pennington et al., 2014), as pre-trained on Google News or Wikipedia, or trained on the provided Qatar Living data. Less popular techniques included dialog modeling for the list of comments for a given question, e.g., using conditional random fields to model the sequence of comment labels (Good, Bad, Potential, Dialog), mapping the question and the comment to a graph structure and performing graph traversal, using word alignments between the question and the comment, time modeling, and sentiment analysis. Finally, for Arabic, some participants translated the Arabic data to English, and then extracted features from bo</context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP ’14, pages 1532–1543, Doha, Qatar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Filip Radlinski</author>
<author>Thorsten Joachims</author>
</authors>
<title>Query chains: Learning to rank from implicit feedback.</title>
<date>2005</date>
<booktitle>In Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining, KDD ’05,</booktitle>
<pages>239--248</pages>
<location>Chicago, Illinois, USA.</location>
<contexts>
<context position="3768" citStr="Radlinski and Joachims, 2005" startWordPosition="606" endWordPosition="610">onent in order to facilitate participation, and to focus on aspects that are relevant for the SemEval community, namely on learning the relationship between two pieces of text. 1http://www.qatarliving.com/forum/ 269 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 269–281, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics Subtask A goes in the direction of passage reranking, where automatic classifiers are normally applied to pairs of questions and answer passages to derive a relative order between passages, e.g., see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008). In recent years, many advanced models have been developed for automating answer selection, producing a large body of work.2 For instance, Wang et al. (2007) proposed a probabilistic quasisynchronous grammar to learn syntactic transformations from the question to the candidate answers; Heilman and Smith (2010) used an algorithm based on Tree Edit Distance (TED) to learn tree transformations in pairs; Wang and Manning (2010) developed a probabilistic model to learn tree-edit operations on dependency parse</context>
</contexts>
<marker>Radlinski, Joachims, 2005</marker>
<rawString>Filip Radlinski and Thorsten Joachims. 2005. Query chains: Learning to rank from implicit feedback. In Proceedings of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining, KDD ’05, pages 239–248, Chicago, Illinois, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bj¨orn Rudzewitz</author>
<author>Ramon Ziai</author>
</authors>
<title>CoMiC: Adapting a short answer assessment system for answer selection.</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’15,</booktitle>
<location>Denver, Colorado, USA.</location>
<contexts>
<context position="17927" citStr="Rudzewitz and Ziai, 2015" startWordPosition="3016" endWordPosition="3019">set, we used these 30 questions and 170 more where the irrelevant answers were chosen randomly. 4 Scoring The official score for both subtasks is Fl, macroaveraged over the target categories: • For English, subtask A they are Good, Potential, and Bad. • For Arabic, subtask A these are direct, related, and irrelevant. • For English, subtask B they are Yes, No, and Unsure. We also report classification accuracy. Team ID Affiliation and reference Al-Bayan Alexandria University, Egypt (Mohamed et al., 2015) CICBUAPnlp Instituto Polit´ecnico Nacional, Mexico CoMiC University of T¨ubingen, Germany (Rudzewitz and Ziai, 2015) ECNU East China Normal University, China (Yi et al., 2015) FBK-HLT Fondazione Bruno Kessler, Italy (Vo et al., 2015) HITSZ-ICRC Harbin Institute of Technology, China (Hou et al., 2015) ICRC-HIT Harbin Institute of Technology, China (Zhou et al., 2015) JAIST Japan Advance Institute of Science and Technology, Japan (Tran et al., 2015) QCRI Qatar Computing Research Institute, Qatar (Nicosia et al., 2015) Shiraz Shiraz University, Iran (Heydari Alashty et al., 2015) VectorSLU MIT Computer Science and Artificial Intelligence Lab, USA (Belinkov et al., 2015) Voltron Sofia University, Bulgaria (Zama</context>
</contexts>
<marker>Rudzewitz, Ziai, 2015</marker>
<rawString>Bj¨orn Rudzewitz and Ramon Ziai. 2015. CoMiC: Adapting a short answer assessment system for answer selection. In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’15, Denver, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aliaksei Severyn</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Structural relationships for large-scale learning of answer re-ranking.</title>
<date>2012</date>
<booktitle>In Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’12,</booktitle>
<pages>741--750</pages>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="4672" citStr="Severyn and Moschitti, 2012" startWordPosition="752" endWordPosition="755">sisynchronous grammar to learn syntactic transformations from the question to the candidate answers; Heilman and Smith (2010) used an algorithm based on Tree Edit Distance (TED) to learn tree transformations in pairs; Wang and Manning (2010) developed a probabilistic model to learn tree-edit operations on dependency parse trees; and Yao et al. (2013) applied linear chain CRFs with features derived from TED to automatically learn associations between questions and candidate answers. One interesting aspect of the above research is the need for syntactic structures; this is also corroborated in (Severyn and Moschitti, 2012; Severyn and Moschitti, 2013). Note that answer selection can use models for textual entailment, semantic similarity, and for natural language inference in general. For Arabic, we also made use of a real cQA portal, the Fatwa website,3 where questions about Islam are posed by regular users and are answered by knowledgeable scholars. For subtask A, we used a setup similar to that for English, but this time each question had exactly one correct answer among the candidate answers (see Section 3 for detail); we did not offer subtask B for Arabic. Overall for the task, we needed manual annotations</context>
</contexts>
<marker>Severyn, Moschitti, 2012</marker>
<rawString>Aliaksei Severyn and Alessandro Moschitti. 2012. Structural relationships for large-scale learning of answer re-ranking. In Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’12, pages 741–750, Portland, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aliaksei Severyn</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Automatic feature engineering for answer selection and extraction.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP ’13,</booktitle>
<pages>458--467</pages>
<location>Seattle, Washington, USA.</location>
<contexts>
<context position="4702" citStr="Severyn and Moschitti, 2013" startWordPosition="756" endWordPosition="760">n syntactic transformations from the question to the candidate answers; Heilman and Smith (2010) used an algorithm based on Tree Edit Distance (TED) to learn tree transformations in pairs; Wang and Manning (2010) developed a probabilistic model to learn tree-edit operations on dependency parse trees; and Yao et al. (2013) applied linear chain CRFs with features derived from TED to automatically learn associations between questions and candidate answers. One interesting aspect of the above research is the need for syntactic structures; this is also corroborated in (Severyn and Moschitti, 2012; Severyn and Moschitti, 2013). Note that answer selection can use models for textual entailment, semantic similarity, and for natural language inference in general. For Arabic, we also made use of a real cQA portal, the Fatwa website,3 where questions about Islam are posed by regular users and are answered by knowledgeable scholars. For subtask A, we used a setup similar to that for English, but this time each question had exactly one correct answer among the candidate answers (see Section 3 for detail); we did not offer subtask B for Arabic. Overall for the task, we needed manual annotations in two different languages an</context>
</contexts>
<marker>Severyn, Moschitti, 2013</marker>
<rawString>Aliaksei Severyn and Alessandro Moschitti. 2013. Automatic feature engineering for answer selection and extraction. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP ’13, pages 458–467, Seattle, Washington, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Shen</author>
<author>Mirella Lapata</author>
</authors>
<title>Using semantic roles to improve question answering.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’07,</booktitle>
<pages>12--21</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="3810" citStr="Shen and Lapata, 2007" startWordPosition="615" endWordPosition="618">o focus on aspects that are relevant for the SemEval community, namely on learning the relationship between two pieces of text. 1http://www.qatarliving.com/forum/ 269 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 269–281, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics Subtask A goes in the direction of passage reranking, where automatic classifiers are normally applied to pairs of questions and answer passages to derive a relative order between passages, e.g., see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008). In recent years, many advanced models have been developed for automating answer selection, producing a large body of work.2 For instance, Wang et al. (2007) proposed a probabilistic quasisynchronous grammar to learn syntactic transformations from the question to the candidate answers; Heilman and Smith (2010) used an algorithm based on Tree Edit Distance (TED) to learn tree transformations in pairs; Wang and Manning (2010) developed a probabilistic model to learn tree-edit operations on dependency parse trees; and Yao et al. (2013) applied line</context>
</contexts>
<marker>Shen, Lapata, 2007</marker>
<rawString>Dan Shen and Mirella Lapata. 2007. Using semantic roles to improve question answering. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’07, pages 12–21, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Massimiliano Ciaramita</author>
<author>Hugo Zaragoza</author>
</authors>
<title>Learning to rank answers on large online QA collections.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics and the Human Language Technology Conference, ACL-HLT ’08,</booktitle>
<pages>719--727</pages>
<location>Columbus, Ohio, USA.</location>
<contexts>
<context position="3858" citStr="Surdeanu et al., 2008" startWordPosition="623" endWordPosition="626">mEval community, namely on learning the relationship between two pieces of text. 1http://www.qatarliving.com/forum/ 269 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 269–281, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics Subtask A goes in the direction of passage reranking, where automatic classifiers are normally applied to pairs of questions and answer passages to derive a relative order between passages, e.g., see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008). In recent years, many advanced models have been developed for automating answer selection, producing a large body of work.2 For instance, Wang et al. (2007) proposed a probabilistic quasisynchronous grammar to learn syntactic transformations from the question to the candidate answers; Heilman and Smith (2010) used an algorithm based on Tree Edit Distance (TED) to learn tree transformations in pairs; Wang and Manning (2010) developed a probabilistic model to learn tree-edit operations on dependency parse trees; and Yao et al. (2013) applied linear chain CRFs with features derived from TED to </context>
</contexts>
<marker>Surdeanu, Ciaramita, Zaragoza, 2008</marker>
<rawString>Mihai Surdeanu, Massimiliano Ciaramita, and Hugo Zaragoza. 2008. Learning to rank answers on large online QA collections. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics and the Human Language Technology Conference, ACL-HLT ’08, pages 719–727, Columbus, Ohio, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quan Hung Tran</author>
<author>Vu Tran</author>
<author>Tu Vu</author>
<author>Minh Nguyen</author>
<author>Son Bao Pham</author>
</authors>
<title>JAIST: Combining multiple features for answer selection in community question answering.</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’15,</booktitle>
<location>Denver, Colorado, USA.</location>
<contexts>
<context position="18262" citStr="Tran et al., 2015" startWordPosition="3068" endWordPosition="3071">B they are Yes, No, and Unsure. We also report classification accuracy. Team ID Affiliation and reference Al-Bayan Alexandria University, Egypt (Mohamed et al., 2015) CICBUAPnlp Instituto Polit´ecnico Nacional, Mexico CoMiC University of T¨ubingen, Germany (Rudzewitz and Ziai, 2015) ECNU East China Normal University, China (Yi et al., 2015) FBK-HLT Fondazione Bruno Kessler, Italy (Vo et al., 2015) HITSZ-ICRC Harbin Institute of Technology, China (Hou et al., 2015) ICRC-HIT Harbin Institute of Technology, China (Zhou et al., 2015) JAIST Japan Advance Institute of Science and Technology, Japan (Tran et al., 2015) QCRI Qatar Computing Research Institute, Qatar (Nicosia et al., 2015) Shiraz Shiraz University, Iran (Heydari Alashty et al., 2015) VectorSLU MIT Computer Science and Artificial Intelligence Lab, USA (Belinkov et al., 2015) Voltron Sofia University, Bulgaria (Zamanov et al., 2015) Yamraj Masaryk University, Czech Republic Table 3: The participating teams. 274 Submission Macro F, Acc. JAIST-contrastive1 57.29 72.67 1 JAIST-primary 57.19 72.521 HITSZ-ICRC-contrastive1 56.44 69.43 2 HITSZ-ICRC-primary 56.41 68.675 *QCRI-contrastive1 56.40 68.27 HITSZ-ICRC-contrastive2 55.22 67.91 ICRC-HIT-contra</context>
</contexts>
<marker>Tran, Tran, Vu, Nguyen, Pham, 2015</marker>
<rawString>Quan Hung Tran, Vu Tran, Tu Vu, Minh Nguyen, and Son Bao Pham. 2015. JAIST: Combining multiple features for answer selection in community question answering. In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’15, Denver, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ngoc Phuoc An Vo</author>
<author>Simone Magnolini</author>
<author>Octavian Popescu</author>
</authors>
<title>FBK-HLT: An application of semantic textual similarity for answer selection in community question answering.</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’15,</booktitle>
<location>Denver, Colorado, USA.</location>
<contexts>
<context position="18044" citStr="Vo et al., 2015" startWordPosition="3035" endWordPosition="3038">or both subtasks is Fl, macroaveraged over the target categories: • For English, subtask A they are Good, Potential, and Bad. • For Arabic, subtask A these are direct, related, and irrelevant. • For English, subtask B they are Yes, No, and Unsure. We also report classification accuracy. Team ID Affiliation and reference Al-Bayan Alexandria University, Egypt (Mohamed et al., 2015) CICBUAPnlp Instituto Polit´ecnico Nacional, Mexico CoMiC University of T¨ubingen, Germany (Rudzewitz and Ziai, 2015) ECNU East China Normal University, China (Yi et al., 2015) FBK-HLT Fondazione Bruno Kessler, Italy (Vo et al., 2015) HITSZ-ICRC Harbin Institute of Technology, China (Hou et al., 2015) ICRC-HIT Harbin Institute of Technology, China (Zhou et al., 2015) JAIST Japan Advance Institute of Science and Technology, Japan (Tran et al., 2015) QCRI Qatar Computing Research Institute, Qatar (Nicosia et al., 2015) Shiraz Shiraz University, Iran (Heydari Alashty et al., 2015) VectorSLU MIT Computer Science and Artificial Intelligence Lab, USA (Belinkov et al., 2015) Voltron Sofia University, Bulgaria (Zamanov et al., 2015) Yamraj Masaryk University, Czech Republic Table 3: The participating teams. 274 Submission Macro F,</context>
</contexts>
<marker>Vo, Magnolini, Popescu, 2015</marker>
<rawString>Ngoc Phuoc An Vo, Simone Magnolini, and Octavian Popescu. 2015. FBK-HLT: An application of semantic textual similarity for answer selection in community question answering. In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’15, Denver, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
<author>Christopher D Manning</author>
</authors>
<title>Probabilistic tree-edit models with structured latent variables for textual entailment and question answering.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10,</booktitle>
<pages>1164--1172</pages>
<location>Beijing, China.</location>
<contexts>
<context position="4286" citStr="Wang and Manning (2010)" startWordPosition="691" endWordPosition="694"> and answer passages to derive a relative order between passages, e.g., see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008). In recent years, many advanced models have been developed for automating answer selection, producing a large body of work.2 For instance, Wang et al. (2007) proposed a probabilistic quasisynchronous grammar to learn syntactic transformations from the question to the candidate answers; Heilman and Smith (2010) used an algorithm based on Tree Edit Distance (TED) to learn tree transformations in pairs; Wang and Manning (2010) developed a probabilistic model to learn tree-edit operations on dependency parse trees; and Yao et al. (2013) applied linear chain CRFs with features derived from TED to automatically learn associations between questions and candidate answers. One interesting aspect of the above research is the need for syntactic structures; this is also corroborated in (Severyn and Moschitti, 2012; Severyn and Moschitti, 2013). Note that answer selection can use models for textual entailment, semantic similarity, and for natural language inference in general. For Arabic, we also made use of a real cQA porta</context>
</contexts>
<marker>Wang, Manning, 2010</marker>
<rawString>Mengqiu Wang and Christopher D. Manning. 2010. Probabilistic tree-edit models with structured latent variables for textual entailment and question answering. In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10, pages 1164–1172, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
<author>Noah A Smith</author>
<author>Teruko Mitamura</author>
</authors>
<title>What is the Jeopardy model? A quasisynchronous grammar for QA.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’07,</booktitle>
<pages>22--32</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="4016" citStr="Wang et al. (2007)" startWordPosition="649" endWordPosition="652">hop on Semantic Evaluation (SemEval 2015), pages 269–281, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics Subtask A goes in the direction of passage reranking, where automatic classifiers are normally applied to pairs of questions and answer passages to derive a relative order between passages, e.g., see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008). In recent years, many advanced models have been developed for automating answer selection, producing a large body of work.2 For instance, Wang et al. (2007) proposed a probabilistic quasisynchronous grammar to learn syntactic transformations from the question to the candidate answers; Heilman and Smith (2010) used an algorithm based on Tree Edit Distance (TED) to learn tree transformations in pairs; Wang and Manning (2010) developed a probabilistic model to learn tree-edit operations on dependency parse trees; and Yao et al. (2013) applied linear chain CRFs with features derived from TED to automatically learn associations between questions and candidate answers. One interesting aspect of the above research is the need for syntactic structures; t</context>
</contexts>
<marker>Wang, Smith, Mitamura, 2007</marker>
<rawString>Mengqiu Wang, Noah A. Smith, and Teruko Mitamura. 2007. What is the Jeopardy model? A quasisynchronous grammar for QA. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’07, pages 22– 32, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuchen Yao</author>
<author>Benjamin Van Durme</author>
<author>Chris CallisonBurch</author>
<author>Peter Clark</author>
</authors>
<title>Answer extraction as sequence tagging with tree edit distance.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACLHLT ’13,</booktitle>
<pages>858--867</pages>
<marker>Yao, Van Durme, CallisonBurch, Clark, 2013</marker>
<rawString>Xuchen Yao, Benjamin Van Durme, Chris CallisonBurch, and Peter Clark. 2013. Answer extraction as sequence tagging with tree edit distance. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACLHLT ’13, pages 858–867.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Yi</author>
<author>JianXiang Wang</author>
<author>Man Lan</author>
</authors>
<title>ECNU: Using multiple sources of CQA-based information for answers selection and YES/NO response inference.</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’15,</booktitle>
<location>Denver, Colorado, USA.</location>
<contexts>
<context position="17986" citStr="Yi et al., 2015" startWordPosition="3026" endWordPosition="3029">swers were chosen randomly. 4 Scoring The official score for both subtasks is Fl, macroaveraged over the target categories: • For English, subtask A they are Good, Potential, and Bad. • For Arabic, subtask A these are direct, related, and irrelevant. • For English, subtask B they are Yes, No, and Unsure. We also report classification accuracy. Team ID Affiliation and reference Al-Bayan Alexandria University, Egypt (Mohamed et al., 2015) CICBUAPnlp Instituto Polit´ecnico Nacional, Mexico CoMiC University of T¨ubingen, Germany (Rudzewitz and Ziai, 2015) ECNU East China Normal University, China (Yi et al., 2015) FBK-HLT Fondazione Bruno Kessler, Italy (Vo et al., 2015) HITSZ-ICRC Harbin Institute of Technology, China (Hou et al., 2015) ICRC-HIT Harbin Institute of Technology, China (Zhou et al., 2015) JAIST Japan Advance Institute of Science and Technology, Japan (Tran et al., 2015) QCRI Qatar Computing Research Institute, Qatar (Nicosia et al., 2015) Shiraz Shiraz University, Iran (Heydari Alashty et al., 2015) VectorSLU MIT Computer Science and Artificial Intelligence Lab, USA (Belinkov et al., 2015) Voltron Sofia University, Bulgaria (Zamanov et al., 2015) Yamraj Masaryk University, Czech Republic</context>
</contexts>
<marker>Yi, Wang, Lan, 2015</marker>
<rawString>Liang Yi, JianXiang Wang, and Man Lan. 2015. ECNU: Using multiple sources of CQA-based information for answers selection and YES/NO response inference. In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’15, Denver, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Zamanov</author>
<author>Marina Kraeva</author>
<author>Nelly Hateva</author>
<author>Ivana Yovcheva</author>
<author>Ivelina Nikolova</author>
<author>Galia Angelova</author>
</authors>
<title>Voltron: A hybrid system for answer validation based on lexical and distance features.</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’15,</booktitle>
<location>Denver, Colorado, USA.</location>
<contexts>
<context position="18544" citStr="Zamanov et al., 2015" startWordPosition="3108" endWordPosition="3111">015) ECNU East China Normal University, China (Yi et al., 2015) FBK-HLT Fondazione Bruno Kessler, Italy (Vo et al., 2015) HITSZ-ICRC Harbin Institute of Technology, China (Hou et al., 2015) ICRC-HIT Harbin Institute of Technology, China (Zhou et al., 2015) JAIST Japan Advance Institute of Science and Technology, Japan (Tran et al., 2015) QCRI Qatar Computing Research Institute, Qatar (Nicosia et al., 2015) Shiraz Shiraz University, Iran (Heydari Alashty et al., 2015) VectorSLU MIT Computer Science and Artificial Intelligence Lab, USA (Belinkov et al., 2015) Voltron Sofia University, Bulgaria (Zamanov et al., 2015) Yamraj Masaryk University, Czech Republic Table 3: The participating teams. 274 Submission Macro F, Acc. JAIST-contrastive1 57.29 72.67 1 JAIST-primary 57.19 72.521 HITSZ-ICRC-contrastive1 56.44 69.43 2 HITSZ-ICRC-primary 56.41 68.675 *QCRI-contrastive1 56.40 68.27 HITSZ-ICRC-contrastive2 55.22 67.91 ICRC-HIT-contrastive1 53.82 73.18 3 *QCRI-primary 53.74 70.503 4 ECNU-primary 53.47 70.552 ECNU-contrastive1 52.55 69.48 ECNU-contrastive2 52.27 69.38 *QCRI-contrastive2 51.97 69.48 5 ICRC-HIT-primary 49.60 67.866 *VectorSLU-contrastive1 49.54 70.45 6 *VectorSLU-primary 49.10 66.457 7 Shiraz-prim</context>
</contexts>
<marker>Zamanov, Kraeva, Hateva, Yovcheva, Nikolova, Angelova, 2015</marker>
<rawString>Ivan Zamanov, Marina Kraeva, Nelly Hateva, Ivana Yovcheva, Ivelina Nikolova, and Galia Angelova. 2015. Voltron: A hybrid system for answer validation based on lexical and distance features. In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’15, Denver, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Zhou</author>
<author>Baotian Hu</author>
<author>Jiaxin Lin</author>
<author>Yang xiang</author>
<author>Xiaolong Wang</author>
</authors>
<title>ICRC-HIT: A deep learning based comment sequence labeling system for answer selection challenge.</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’15,</booktitle>
<location>Denver, Colorado, USA.</location>
<contexts>
<context position="18179" citStr="Zhou et al., 2015" startWordPosition="3055" endWordPosition="3058">rabic, subtask A these are direct, related, and irrelevant. • For English, subtask B they are Yes, No, and Unsure. We also report classification accuracy. Team ID Affiliation and reference Al-Bayan Alexandria University, Egypt (Mohamed et al., 2015) CICBUAPnlp Instituto Polit´ecnico Nacional, Mexico CoMiC University of T¨ubingen, Germany (Rudzewitz and Ziai, 2015) ECNU East China Normal University, China (Yi et al., 2015) FBK-HLT Fondazione Bruno Kessler, Italy (Vo et al., 2015) HITSZ-ICRC Harbin Institute of Technology, China (Hou et al., 2015) ICRC-HIT Harbin Institute of Technology, China (Zhou et al., 2015) JAIST Japan Advance Institute of Science and Technology, Japan (Tran et al., 2015) QCRI Qatar Computing Research Institute, Qatar (Nicosia et al., 2015) Shiraz Shiraz University, Iran (Heydari Alashty et al., 2015) VectorSLU MIT Computer Science and Artificial Intelligence Lab, USA (Belinkov et al., 2015) Voltron Sofia University, Bulgaria (Zamanov et al., 2015) Yamraj Masaryk University, Czech Republic Table 3: The participating teams. 274 Submission Macro F, Acc. JAIST-contrastive1 57.29 72.67 1 JAIST-primary 57.19 72.521 HITSZ-ICRC-contrastive1 56.44 69.43 2 HITSZ-ICRC-primary 56.41 68.675</context>
</contexts>
<marker>Zhou, Hu, Lin, xiang, Wang, 2015</marker>
<rawString>Xiaoqiang Zhou, Baotian Hu, Jiaxin Lin, Yang xiang, and Xiaolong Wang. 2015. ICRC-HIT: A deep learning based comment sequence labeling system for answer selection challenge. In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’15, Denver, Colorado, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>