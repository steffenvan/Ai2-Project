<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.496162">
<title confidence="0.997198">
Applying Spectral Clustering for Chinese Word Sense Induction
</title>
<author confidence="0.999351">
Zhengyan He, Yang Song, Houfeng Wang
</author>
<affiliation confidence="0.9949275">
Key Laboratory of Computational Linguistics (Peking University)
Ministry of Education,China
</affiliation>
<email confidence="0.996578">
{hezhengyan, ysong, wanghf}@pku.edu.cn
</email>
<sectionHeader confidence="0.993883" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99992525">
Sense Induction is the process of identify-
ing the word sense given its context, often
treated as a clustering task. This paper ex-
plores the use of spectral cluster method
which incorporates word features and n-
gram features to determine which cluster
the word belongs to, each cluster repre-
sents one sense in the given document set.
</bodyText>
<sectionHeader confidence="0.998759" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999692693877551">
Word Sense Induction(WSI) is defined as the
process of identifying different senses of a tar-
get word in a given context in an unsupervised
method. It’s different from word sense disam-
biguation(WSD) in that senses in WSD are as-
sumed to be known. The disadvantage of WSD
is that it derives the senses of word from existing
dictionaries or other corpus and the senses cannot
be extended to other domains. WSI can overcome
this problem as it can automatically derive word
senses from the given document set, or a specific
domain.
Many different approaches based on co-
occurence have been proposed so far. Bordag
(2006) proposes an approach that uses triplets
of co-occurences. The most significant co-
occurences of target word are used to build triplets
that consist of the target word and its two co-
occurences. Then intersection built from the co-
occurence list of each word in the triplet is used
as feature vector. After merging similar triplets
that have more than 80% overlapping words, clus-
tering is performed on the triplets. Triplets with
fewer than 4 intersection words are removed in or-
der to reduce noise.
LDA model has also been applied to WSI
(Brody and Lapata, 2009). Brody proposes a
method that treats document and topics in LDA
as word context and senses respectively. The pro-
cess of generating the context words is as follows:
first generate sense from a multinomial distribu-
tion given context, then generate context words
given sense. They also derive a layered model
to incorporate different kind of features and use
Gibbs sampling method to solve the problem.
Graph-based methods become popular recently.
These methods use the co-occurence graph of
context words to obtain sense clusters based on
sub-graph density. Markov clustering(MCL) has
been used to identify dense regions of graph
(Agirre and Soroa, 2007).
Spectral clustering performs well on problems
in which points cluster based on shape. The
method is that first compute the Laplace matrix
of the affinity matrix, then reform the data points
by stacking the largest eigenvectors of the Laplace
matrix in columns, finally cluster the new data
points using a more simple clustering method like
k-means (Ng et al., 2001).
</bodyText>
<sectionHeader confidence="0.997372" genericHeader="introduction">
2 Methodology
</sectionHeader>
<bodyText confidence="0.999922021276595">
Our approach follows a common cluster model
that represents the given context as a word vec-
tor and later uses a spectral clustering method to
group each instance in its own cluster.
Different types of polysemy may arise and the
most significant distinction may be the syntactic
classes of the word and the conceptually differ-
ent senses (Bordag, 2006). Thus we must extract
the features able to distinguish these differences.
They are:
Local tokens: the word occuring in the window
-3 – +3;
Local bigram feature: bigram within -5 – +5
Chinese character range;
The above two features model the syntactic us-
age of a specific sense of a Chinese word.
Topical or conceptual feature: the content
words (pos-tagged as noun, verb, adjective) within
the given sentence. As the sentence in the training
set seems generally short, a short window may not
contains enough infomation.
We represent the words in a 0-1 vector accord-
ing to their existence in a given sentence. Then the
similarity measure between two given sentences is
derived from their cosine similarity. We find that it
is difficult to define the relative importance of dif-
ferent types of features in order to combine them
in one vector space, and find that ignoring weight
achieve better result. Brody (2009) achieves this
in LDA model through a layered model with dif-
ferent probability of feature given sense.
Later we use a spectral clustering method from
R kernlab package (Karatzoglou et al., 2004)
which implements the algorithm described in (Ng
et al., 2001). Instead of using the Gaussian kernel
matrix as the similarity matrix we use the cosine
similarity derived above.
One observation is that instances with the same
target word sense often appear in the same con-
text. However, for some verb in Chinese, it is of-
ten the case that one sense relates to a concrete
object while the other relates to a more broad and
abstract concept and the context varies consider-
ably. Simple word co-occurence cannot define a
good similarity measure to group these cases into
one cluster. We must consider semantic related-
ness measures between contexts.
</bodyText>
<sectionHeader confidence="0.996591" genericHeader="method">
3 Performance
</sectionHeader>
<bodyText confidence="0.990525333333333">
Our system performs well on the training set. Two
methods are used to evaluate the performance un-
der different features.
</bodyText>
<table confidence="0.698684333333333">
method precision recall F-score
Purity-based 81.11 83.19 81.99
B-cubed 74.41 76.51 75.33
</table>
<tableCaption confidence="0.999516">
Table 1: The performance of training set
</tableCaption>
<bodyText confidence="0.9943745">
Our system finally gets a F-score of 0.7598 on
the test set.
</bodyText>
<sectionHeader confidence="0.998642" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999973555555555">
Our experiment in the Chinese word sense induc-
tion task performs good with respect to the relative
small corpus(only the training set). But only con-
sidering token co-occurence cannot achieve better
result. Moreover, it is difficult to define a simi-
larity measure solely based on lexicon infomation
with no regard to semantic relatedness. Finally,
combining different types of features seems to be
another challenge in our model.
</bodyText>
<sectionHeader confidence="0.999264" genericHeader="acknowledgments">
5 Acknowledgments
</sectionHeader>
<bodyText confidence="0.8826505">
This research is supported by National Natural
Science Foundation of Chinese (No.9092001).
</bodyText>
<sectionHeader confidence="0.995044" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.98888">
Agirre, Eneko and Aitor Soroa. 2007. Ubc-as: a graph
based unsupervised system for induction and classi-
fication. In SemEval ’07: Proceedings of the 4th
International Workshop on Semantic Evaluations,
pages 346–349, Morristown, NJ, USA. Association
for Computational Linguistics.
Bordag, Stefan. 2006. Word sense induction: Triplet-
based clustering and automatic evaluation. In
EACL. The Association for Computer Linguistics.
Brody, Samuel and Mirella Lapata. 2009. Bayesian
word sense induction. In EACL, pages 103–111.
The Association for Computer Linguistics.
Karatzoglou, Alexandros, Alex Smola, Kurt Hornik,
and Achim Zeileis. 2004. kernlab – an S4 pack-
age for kernel methods in R. Journal of Statistical
Software, 11(9):1–20.
Ng, Andrew Y., Michael I. Jordan, and Yair Weiss.
2001. On spectral clustering: Analysis and an al-
gorithm. In Advances in Neural Information Pro-
cessing Systems 14, pages 849–856. MIT Press.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.308898">
<title confidence="0.999879">Applying Spectral Clustering for Chinese Word Sense Induction</title>
<author confidence="0.906519">Zhengyan He</author>
<author confidence="0.906519">Yang Song</author>
<author confidence="0.906519">Houfeng</author>
<affiliation confidence="0.5053725">Key Laboratory of Computational Linguistics (Peking Ministry of</affiliation>
<email confidence="0.838443">ysong,</email>
<abstract confidence="0.996201666666667">Sense Induction is the process of identifying the word sense given its context, often treated as a clustering task. This paper explores the use of spectral cluster method which incorporates word features and ngram features to determine which cluster the word belongs to, each cluster represents one sense in the given document set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Aitor Soroa</author>
</authors>
<title>Ubc-as: a graph based unsupervised system for induction and classification.</title>
<date>2007</date>
<booktitle>In SemEval ’07: Proceedings of the 4th International Workshop on Semantic Evaluations,</booktitle>
<pages>346--349</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2412" citStr="Agirre and Soroa, 2007" startWordPosition="386" endWordPosition="389"> treats document and topics in LDA as word context and senses respectively. The process of generating the context words is as follows: first generate sense from a multinomial distribution given context, then generate context words given sense. They also derive a layered model to incorporate different kind of features and use Gibbs sampling method to solve the problem. Graph-based methods become popular recently. These methods use the co-occurence graph of context words to obtain sense clusters based on sub-graph density. Markov clustering(MCL) has been used to identify dense regions of graph (Agirre and Soroa, 2007). Spectral clustering performs well on problems in which points cluster based on shape. The method is that first compute the Laplace matrix of the affinity matrix, then reform the data points by stacking the largest eigenvectors of the Laplace matrix in columns, finally cluster the new data points using a more simple clustering method like k-means (Ng et al., 2001). 2 Methodology Our approach follows a common cluster model that represents the given context as a word vector and later uses a spectral clustering method to group each instance in its own cluster. Different types of polysemy may ari</context>
</contexts>
<marker>Agirre, Soroa, 2007</marker>
<rawString>Agirre, Eneko and Aitor Soroa. 2007. Ubc-as: a graph based unsupervised system for induction and classification. In SemEval ’07: Proceedings of the 4th International Workshop on Semantic Evaluations, pages 346–349, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Bordag</author>
</authors>
<title>Word sense induction: Tripletbased clustering and automatic evaluation.</title>
<date>2006</date>
<booktitle>In EACL. The Association</booktitle>
<institution>for Computer Linguistics.</institution>
<contexts>
<context position="1199" citStr="Bordag (2006)" startWordPosition="190" endWordPosition="191">Word Sense Induction(WSI) is defined as the process of identifying different senses of a target word in a given context in an unsupervised method. It’s different from word sense disambiguation(WSD) in that senses in WSD are assumed to be known. The disadvantage of WSD is that it derives the senses of word from existing dictionaries or other corpus and the senses cannot be extended to other domains. WSI can overcome this problem as it can automatically derive word senses from the given document set, or a specific domain. Many different approaches based on cooccurence have been proposed so far. Bordag (2006) proposes an approach that uses triplets of co-occurences. The most significant cooccurences of target word are used to build triplets that consist of the target word and its two cooccurences. Then intersection built from the cooccurence list of each word in the triplet is used as feature vector. After merging similar triplets that have more than 80% overlapping words, clustering is performed on the triplets. Triplets with fewer than 4 intersection words are removed in order to reduce noise. LDA model has also been applied to WSI (Brody and Lapata, 2009). Brody proposes a method that treats do</context>
<context position="3145" citStr="Bordag, 2006" startWordPosition="509" endWordPosition="510"> the Laplace matrix of the affinity matrix, then reform the data points by stacking the largest eigenvectors of the Laplace matrix in columns, finally cluster the new data points using a more simple clustering method like k-means (Ng et al., 2001). 2 Methodology Our approach follows a common cluster model that represents the given context as a word vector and later uses a spectral clustering method to group each instance in its own cluster. Different types of polysemy may arise and the most significant distinction may be the syntactic classes of the word and the conceptually different senses (Bordag, 2006). Thus we must extract the features able to distinguish these differences. They are: Local tokens: the word occuring in the window -3 – +3; Local bigram feature: bigram within -5 – +5 Chinese character range; The above two features model the syntactic usage of a specific sense of a Chinese word. Topical or conceptual feature: the content words (pos-tagged as noun, verb, adjective) within the given sentence. As the sentence in the training set seems generally short, a short window may not contains enough infomation. We represent the words in a 0-1 vector according to their existence in a given </context>
</contexts>
<marker>Bordag, 2006</marker>
<rawString>Bordag, Stefan. 2006. Word sense induction: Tripletbased clustering and automatic evaluation. In EACL. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Brody</author>
<author>Mirella Lapata</author>
</authors>
<title>Bayesian word sense induction.</title>
<date>2009</date>
<booktitle>In EACL,</booktitle>
<pages>103--111</pages>
<institution>The Association for Computer Linguistics.</institution>
<contexts>
<context position="1759" citStr="Brody and Lapata, 2009" startWordPosition="284" endWordPosition="287">s based on cooccurence have been proposed so far. Bordag (2006) proposes an approach that uses triplets of co-occurences. The most significant cooccurences of target word are used to build triplets that consist of the target word and its two cooccurences. Then intersection built from the cooccurence list of each word in the triplet is used as feature vector. After merging similar triplets that have more than 80% overlapping words, clustering is performed on the triplets. Triplets with fewer than 4 intersection words are removed in order to reduce noise. LDA model has also been applied to WSI (Brody and Lapata, 2009). Brody proposes a method that treats document and topics in LDA as word context and senses respectively. The process of generating the context words is as follows: first generate sense from a multinomial distribution given context, then generate context words given sense. They also derive a layered model to incorporate different kind of features and use Gibbs sampling method to solve the problem. Graph-based methods become popular recently. These methods use the co-occurence graph of context words to obtain sense clusters based on sub-graph density. Markov clustering(MCL) has been used to ide</context>
</contexts>
<marker>Brody, Lapata, 2009</marker>
<rawString>Brody, Samuel and Mirella Lapata. 2009. Bayesian word sense induction. In EACL, pages 103–111. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandros Karatzoglou</author>
<author>Alex Smola</author>
<author>Kurt Hornik</author>
<author>Achim Zeileis</author>
</authors>
<title>kernlab – an S4 package for kernel methods in R.</title>
<date>2004</date>
<journal>Journal of Statistical Software,</journal>
<volume>11</volume>
<issue>9</issue>
<contexts>
<context position="4251" citStr="Karatzoglou et al., 2004" startWordPosition="692" endWordPosition="695">dow may not contains enough infomation. We represent the words in a 0-1 vector according to their existence in a given sentence. Then the similarity measure between two given sentences is derived from their cosine similarity. We find that it is difficult to define the relative importance of different types of features in order to combine them in one vector space, and find that ignoring weight achieve better result. Brody (2009) achieves this in LDA model through a layered model with different probability of feature given sense. Later we use a spectral clustering method from R kernlab package (Karatzoglou et al., 2004) which implements the algorithm described in (Ng et al., 2001). Instead of using the Gaussian kernel matrix as the similarity matrix we use the cosine similarity derived above. One observation is that instances with the same target word sense often appear in the same context. However, for some verb in Chinese, it is often the case that one sense relates to a concrete object while the other relates to a more broad and abstract concept and the context varies considerably. Simple word co-occurence cannot define a good similarity measure to group these cases into one cluster. We must consider sema</context>
</contexts>
<marker>Karatzoglou, Smola, Hornik, Zeileis, 2004</marker>
<rawString>Karatzoglou, Alexandros, Alex Smola, Kurt Hornik, and Achim Zeileis. 2004. kernlab – an S4 package for kernel methods in R. Journal of Statistical Software, 11(9):1–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
<author>Yair Weiss</author>
</authors>
<title>On spectral clustering: Analysis and an algorithm.</title>
<date>2001</date>
<booktitle>In Advances in Neural Information Processing Systems 14,</booktitle>
<pages>849--856</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="2779" citStr="Ng et al., 2001" startWordPosition="446" endWordPosition="449">ph-based methods become popular recently. These methods use the co-occurence graph of context words to obtain sense clusters based on sub-graph density. Markov clustering(MCL) has been used to identify dense regions of graph (Agirre and Soroa, 2007). Spectral clustering performs well on problems in which points cluster based on shape. The method is that first compute the Laplace matrix of the affinity matrix, then reform the data points by stacking the largest eigenvectors of the Laplace matrix in columns, finally cluster the new data points using a more simple clustering method like k-means (Ng et al., 2001). 2 Methodology Our approach follows a common cluster model that represents the given context as a word vector and later uses a spectral clustering method to group each instance in its own cluster. Different types of polysemy may arise and the most significant distinction may be the syntactic classes of the word and the conceptually different senses (Bordag, 2006). Thus we must extract the features able to distinguish these differences. They are: Local tokens: the word occuring in the window -3 – +3; Local bigram feature: bigram within -5 – +5 Chinese character range; The above two features mo</context>
<context position="4313" citStr="Ng et al., 2001" startWordPosition="702" endWordPosition="705"> vector according to their existence in a given sentence. Then the similarity measure between two given sentences is derived from their cosine similarity. We find that it is difficult to define the relative importance of different types of features in order to combine them in one vector space, and find that ignoring weight achieve better result. Brody (2009) achieves this in LDA model through a layered model with different probability of feature given sense. Later we use a spectral clustering method from R kernlab package (Karatzoglou et al., 2004) which implements the algorithm described in (Ng et al., 2001). Instead of using the Gaussian kernel matrix as the similarity matrix we use the cosine similarity derived above. One observation is that instances with the same target word sense often appear in the same context. However, for some verb in Chinese, it is often the case that one sense relates to a concrete object while the other relates to a more broad and abstract concept and the context varies considerably. Simple word co-occurence cannot define a good similarity measure to group these cases into one cluster. We must consider semantic relatedness measures between contexts. 3 Performance Our </context>
</contexts>
<marker>Ng, Jordan, Weiss, 2001</marker>
<rawString>Ng, Andrew Y., Michael I. Jordan, and Yair Weiss. 2001. On spectral clustering: Analysis and an algorithm. In Advances in Neural Information Processing Systems 14, pages 849–856. MIT Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>