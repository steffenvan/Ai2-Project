<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003870">
<title confidence="0.9926505">
Letter Level Learning for Language Independent
Diacritics Restoration
</title>
<author confidence="0.981656">
Rada Mihalcea
</author>
<affiliation confidence="0.99948">
Department of Computer Science
University of North Texas
</affiliation>
<address confidence="0.709265">
Denton, TX, 76203-1366
</address>
<email confidence="0.615158">
rada©cs.untedu
</email>
<author confidence="0.595699">
Vivi Nastase
</author>
<affiliation confidence="0.897719333333333">
School of Information Technology
University of Ottawa
Ottawa, ON, Canada
</affiliation>
<email confidence="0.613149">
vnastaseAsite.uottawa.ca
</email>
<sectionHeader confidence="0.953081" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998859">
This paper presents a method for diacritics restora-
tion based on learning mechanisms that act at let-
ter level. The method requires no additional tag-
ging tools or resources other than raw text, which
makes it independent of the language, and particu-
larly appealing for languages for which there are few
resources available. The algorithm was evaluated on
four different languages, namely Czech, Hungarian,
Polish and Romanian, and an average accuracy of
over 98% was observed.
</bodyText>
<sectionHeader confidence="0.993701" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99995745">
Diacritics restoration is the problem of inserting di-
acritics into a text where they are missing. With
the continuously increasing amount of texts avail-
able on the Web, tools for automatic insertion of di-
acritics become an essential component in many im-
portant applications such as Information Retrieval,
Machine Translation, Corpora Acquisition, and oth-
ers. Spelling correction may have a direct impact on
the processing quality in many of these applications.
For instance, in the absence of a tool for diacritics
recovery, a search for the Romanian word pete(fish)
retrieves peste(over) as well, paturi can be wrongly
translated as beds, where the intended meaning was
blankets (the translation of peituri), and so forth.
The problem as such is not very difficult, and pre-
vious work has demonstrated that a good dictionary
can lead to over 90% accuracy in accent restoration
for French and Spanish (Yarowsky, 1994), (Simard,
1998), (Galicia-Haro et al., 1999). The method de-
scribed by Michael Simard in (Simard, 1998) is an
improvement over a similar method proposed by El-
Beze (El-Beze et al., 1994). It relies on Hidden
Markov Models and learns from surrounding words
for an overall reported accuracy of 99%. Tufi§ and
Chiu (Tufi§ and Chitu, 1999) propose a similar ap-
proach for diacritics insertion in Romanian texts.
Yarowsky (Yarowsky, 1999) gives a comprehensive
overview of accent restoration techniques. Most of
the algorithms he presents rely on dictionaries and
surrounding words in deciding whether to select a
form or another for a given ambiguous word. A
different approach is proposed by Nagy et. al in
(Nagy et al., 1998), where strings extracted from
texts are used to derive statistics, with high preci-
sion reported on French texts. Their work is similar
with the approach proposed in (Angell et al., 1983),
where trigram similarity measures are employed for
automatic spelling correction.
The majority of studies performed so far in this
field have addressed well known and widely spread
languages such as French or Spanish, and very few
studies have emphasized less popular languages like
Polish, Slovene, Turkish or other languages that em-
ploy diacritics in their spelling. Table 1 lists the
diacritics encountered in European languages with
Latin-based alphabets&apos;. As seen in the table, a large
number of languages face the problem of diacritics
restoration.
From the 36 European languages considered, En-
glish is the only one that does not have diacritics
restoration problems. The few words that use spe-
cial characters are borrowed from other languages
(e.g. fiancé, cafe), and these words have no diacrit-
ics free correspondent with which they could be con-
fused. It has been observed though that English has
a higher semantic ambiguity than other languages.
The absence of diacritics may be part of the expla-
nation of this phenomenon2.
The applicability of word based methods for dia-
critics restoration is limited when:
</bodyText>
<listItem confidence="0.978607571428571">
(1) Electronic dictionaries are not available, or only
limited size dictionaries are made public. Moreover,
when the dictionary itself lacks diacritics, methods
relying on diacritics restoration from dictionaries be-
come useless.
(2) Tools for morphological and/or syntactic analy-
sis, which are considered to be helpful for the prob-
</listItem>
<footnote confidence="0.9965453">
1 The table lists only lower case letters. The information
in this table was compiled from lists of diacritics in European
languages available at http://www.tiro.com/di_intro.html
2Studies performed on bilingual parallel corpora have
shown that the vocabulary built from an English text is about
half the size of the vocabulary build for the same text writ-
ten in a different language. Senseval competition (Kilgarriff,
2001) has also reported significantly lower precision for En-
glish with respect to other languages, in a word sense disam-
biguation task.
</footnote>
<figure confidence="0.91452068627451">
Language
Italian
Lower Sorbian
Maltese
Norwegian
Polish
Portuguese
Romanian
Sami
Serbo-Croatian
Slovak
Slovene
Spanish
Swedish
Turkish
Upper Sorbian
Welsh
Diacritics
a 661116 OUU
6 eel n.1.
e A h
àø
a, é e, I ii ó
aace 666 ii
a a i t
a I e d- rI n, t-
6 e d-
aaed&apos;effri 6 of t&apos; iI3&apos;7
csz
a 6 6 n
a.a o
cgii ou
6 e el not
aelouwy
Language Diacritics
Albanian c
Basque
Breton âêñüü
Catalan àcèéIIl.ôóüu
Czech ded&apos; effiott&apos;UU3&apos;7
Danish
Dutch áàâãéèêëîíìîïóôôöüüüü
English none
Estonian äëOöü
Faroese á d- 6 011 3&apos;7
Finnish a. O
French aaa ce e 6116 ceifl
Gaelic a 6 6
German äöüf3
Hungarian áéíóööüüíí
Icelandic aaae16OU 3&apos;7 b
</figure>
<tableCaption confidence="0.99228">
Table 1: Diacritics in European languages with Latin based alphabets.
</tableCaption>
<bodyText confidence="0.985760846153846">
lem of diacritics restoration, are inexistent or are not
publicly available.
(3) Size of usable corpora containing diacritics is lim-
ited. The size of the corpora available on the Web
or in other public forms influences the size of the vo-
cabulary that can be built ad-hoc out of these texts.
Moreover, Web publishers choose in many cases to
avoid diacritics, for reasons of simplicity, uniformity
or just the lack of means for diacritics encoding.
We propose in this paper a technique for diacrit-
ics restoration based on learning performed at letter
level, rather than word level. The strongest advan-
tage of this method is that it provides the means for
generalization beyond words.
Specifically, instead of learning generalizations
that apply at word level, that would translate to
rules such as &amp;quot;anuncio should change to anuncio
when it is a verb&amp;quot;, or &amp;quot;peste should change to peste
when followed by the noun case, we are interested
in finding generalizations at letter level, which can
translate to &amp;quot;s followed by i and preceded by white
space should change to s&amp;quot;. This latest type of rules
are more general and they have higher applicabil-
ity when only small dictionaries are available, when
many unknown words are encountered in the input
text, or when there are no usable tools for morpho-
logical or syntactic analysis.
It is clear that letters constitute the smallest pos-
sible level of granularity in language analysis, and
therefore have the highest potential for generaliza-
tion. Instead of having about 150,000 units that are
potential candidates for the algorithm (the approxi-
mate size of the general purpose vocabulary of a lan-
guage), we have more or less 26 characters that will
constitute the entry to the disambiguation mecha-
nism.3
The method is particularly useful for languages
that lack publicly available text processing tools or
large electronic dictionaries with diacritics. Well
studied and widespread languages such as French
and Spanish can benefit as well from this methodol-
ogy when dealing with unknown words.
The algorithm was evaluated on four different lan-
guages, namely Czech, Hungarian, Polish and Ro-
manian, and an average precision of over 98% at let-
ter level was observed. Moreover, this method does
not require any other tools or resources other than
a corpus of raw text with diacritics. Due to the sim-
plicity of the algorithm, the processing speed is very
high, about 20 pages of text per second, measured
on an off the shelf PC. This includes both the ex-
traction of context statistics and the learning phase.
</bodyText>
<sectionHeader confidence="0.974465" genericHeader="method">
2 Experimental setup
</sectionHeader>
<bodyText confidence="0.9535525">
The goal of the experiments reported in this paper
is to see whether learning at letter level is possible
to the end of solving the problem of diacritics re-
insertion. Besides providing an additional method
for diacritics restoration, the purpose of doing learn-
ing at such a low level is to supply a methodology
for languages that have only few lexical and seman-
tic resources and for which diacritics restoration via
learning at word level is hard to perform.
3The actual numbers depend on the language considered.
It was shown, for instance, that about 85% of the French
words do not have any spelling that includes diacritics, and
hence only about 20,000 words are potentially ambiguous. On
the other hand, only 7 letters are ambiguous in French.
</bodyText>
<subsectionHeader confidence="0.90773">
2.1 Data
</subsectionHeader>
<bodyText confidence="0.999926178571428">
As mentioned earlier, the experiments were per-
formed on four different languages: Czech, Hun-
garian, Polish and Romanian. We have selected
languages that are not widely spread, and conse-
quently do not have many publicly available tools or
resources. In order to apply the algorithm, the only
requirement is a medium size corpus of raw text with
diacritics.
The data was collected over the Internet, using
mainly newspapers archives or literal texts avail-
able electronically. For the four languages, the main
sources used to construct the textual corpus are as
follows: (1) for Czech, the archive from Lidovky
newspaper, and literary texts by Kafka, HaSek and
Cape/c; (2) for Hungarian, the archive from Digitalis
Irodalmi Akadernia, and a novel by Peteifi Sandor,
(3) for Polish, the archive of Wiedza i iycie; (4)
for Romanian, the archive made available by the
Romania Literara newspaper. Additionally, other
texts with diacritics were collected from various Web
sites, to the end of building for each language a cor-
pus of at least one million words.
Next, the HTML files collected from the Web were
converted into text files. Upper case letters were
converted into lower case. After these steps, we were
left with a corpus of 1,46 million words for Czech,
1,72 million words for Hungarian, 2,50 million words
for Polish, and about 3 million words for Romanian.
</bodyText>
<subsectionHeader confidence="0.997963">
2.2 Learning algorithms
</subsectionHeader>
<bodyText confidence="0.986819708333334">
We decided to use an instance based learning algo-
rithm for our diacritics restoration task. The rea-
son for this decision is twofold. First, it has been
advocated that forgetting exceptions is harmful in
Natural Language applications, and instance based
learners are known for their property of taking into
consideration every single training example when
making a classification decision (Daelemans et al.,
1999). Secondly, this type of algorithms are efficient
in terms of training and testing time. For our ex-
periments, we used the TiMBL (Daelemans et al.,
2001) implementation4.
Since we work at the low level of letters, the target
attribute to be learned is constituted by the ambigu-
ous letters. It can be therefore any of the ambiguous
letters listed in Table 1. For the four languages con-
sidered in these experiments, the sets of ambiguous
letters are shown in Table 2. Upper case diacritics
are not considered, since they have been previously
converted to lower case.
4Similar experiments were performed with a decision tree
classifier, namely C4.5 (Quinlan, 1993). The results obtained
were similar with those obtained with the instance based
learner, at significantly higher running times.
</bodyText>
<subsectionHeader confidence="0.795997">
2.3 Features
</subsectionHeader>
<bodyText confidence="0.960197529411765">
The features used in any learning algorithm have
tremendous influence over the final accuracy. We do
not make use of part of speech taggers or any other
morphological or syntactic analyzers. Furthermore,
we do not want to rely on surrounding words, since
the data we have is limited, and therefore learning
at word level would result in many cases of unknown
words.
We have therefore decided for very simple fea-
tures, for the extraction of which no particular pro-
cessing is required. We only look at surrounding
letters, with a special notation assigned to white
spaces, commas, dots and colons (these characters
may affect the learning process, since they are con-
sidered special characters by TiMBL or C4.5). This
set of features performs surprisingly well in terms of
accuracy.
For each ambiguous pair of letters, we scan the
text and generate all possible examples encountered
in the corpus. The attributes in an example are
formed by N letters to the left and right of the am-
biguous letter, and the target attribute is the am-
biguous letter itself. Samples of feature vectors are
presented below.
I , i , n , SP , ( , u , b , SP , i , n , s.
e,CO,SP,r,o,-,g,a,r,d,§.
g,a,r,d,i,t,u,I,CO,SP,s.
e,SP,o,r,a,DO,SP,t,o,t,§.
These are the examples that were fed to the learn-
ing algorithm for the s - s ambiguous pair from Ro-
manian. CO, DO and SP are the replacement codes
we use to denote comma, dot or white characters.
Note that the surrounding letters with diacritics are
converted into their diacriticless equivalent.
</bodyText>
<sectionHeader confidence="0.999897" genericHeader="method">
3 Results
</sectionHeader>
<bodyText confidence="0.999775555555556">
From the total set of examples extracted for each
ambiguous letter set, 50,000 examples are set aside
for testing, and the rest is used to train the learner.
Table 2 shows the results obtained for the four lan-
guages. For each language, the table lists the sets
of ambiguous letters, the number of examples ex-
tracted from the corpus for each such ambiguous set,
a simple baseline computed as the precision achieved
when the most frequent element of the ambiguous
set is selected by default, the F-measures (Van Ri-
jsbergen, 1979) computed for each individual letter,
and finally the precision achieved with the instance
based learner applied at letter level (IBLLL). The
average accuracy determined for all four languages
is 98.17%.
The average accuracy per language is influenced
by the size of the corpus. The corpora collected for
Czech and Hungarian contain about 1.4-1.7 million
</bodyText>
<figure confidence="0.842661181818182">
Ambiguous
set
Total
examples
Baseline
Individual
F-measures
Overall
IBLLL
Czech
a a 649,886 75.01% 97.97%/93.92% 96.96%
</figure>
<table confidence="0.997400648648649">
c e 217,570 72.21% 98.00%/94.60% 97.08%
d d&apos; 271,070 99.05% 99.93%/84.93% 99.86%
e é 6 768,051 74.59% 98.74%/91.85%/91.81% 97.02%
i 1 504,298 60.43% 96.95%/95.27% 96.29%
n h. 439,552 98.97% 99.88%/89.06% 99.71%
o 6 566,521 99.08% 99.93%/63.44% 99.86%
r i&amp;quot;. 319,352 65.55% 98.53%/95.36% 97.60%
s S 380,805 84.44% 99.34%/96.42% 98.88%
t t&apos; 387,214 99.05% 99.92%/83.90% 99.85%
u U U 264,408 80.89% 95.83%/77.15%/95.55% 93.51%
Y Y 191,317 65.55% 96.23%/92.82% 95.06%
z 219,082 66.49% 99.02%/98.06% 98.70%
Average 80.44% 97.83%
Hungarian
aá 1,198,294 73.51% 97.90%/94.19% 96.91%
e e 1,306,944 76.34% 97.65%/92.36% 96.40%
11 647,137 89.14% 99.71%/97.65% 99.49%
o 6 o o 678,012 71.15% 97.33%/90.17%/95.27% 96.10%
u U ii ii 207,753 56.00% 97.79%/95.47%/97.33% 97.31%
Average 75.32% 97.04%
Polish
a a., 1,387,019 88.83% 98.35%/86.68% 97.07%
c e 657,669 91.50% 99.68%/96.57% 99.42%
e e., 1,305,584 89.23% 99.54%/96.18% 98.47%
1 1 506,041 59.29% 98.99%/98.53% 98.80%
nil 878,824 96.75% 99.92%/97.81% 99.85%
o 6 1,230,389 88.67% 99.93%/99.46% 99.87%
s g 688,677 88.67% 99.90%/99.26% 99.83%
z 896,909 86.26% 99.84%/99.01% 99.73%
Average 87.18% 99.02%
Romanian
a A a 2,161,556 74.70% 97.17%/91.31% 96.14%
i i 2,055,147 88.20% 99.18%/98.80% 99.69%
s s 866,964 76.53% 99.23%/98.23% 99.07%
t t 1,157,458 85.81% 99.30%/96.43% 98.75%
Average 80.92% 98.30%
Average over all languages 81.88% 98.17%
</table>
<tableCaption confidence="0.999846">
Table 2: Results obtained in solving diacritics ambiguity, in Czech, Hungarian, Polish and Romanian
</tableCaption>
<bodyText confidence="0.999978638095238">
words, and the precision achieved on these two lan-
guages is lower than for Polish and Romanian, which
have corpora of 2.5-3 million words. We expect
therefore to see an increase in these figures when
increasing the size of training data. Experiments
performed on one of the four languages have con-
firmed this hypothesis.
Interestingly, the number of diacritics in a lan-
guage do not necessarily influence the precision ob-
tained in diacritics restoration. For instance, the
precision achieved on Hungarian, which has a total
of five ambiguous letter sets, is smaller than the pre-
cision achieved on Czech, which has an impressive
number of diacritics (thirteen ambiguous sets). Yet,
the corpus used for Hungarian (1.7 million words) is
slightly larger than the corpus used for Czech (1.4
million words).
As a general observation about the results ob-
tained, the learning curve is very steep in the begin-
ning, precision growing fast with every bit of new
data available. After a certain point the learning
slows down, for each percent in precision improve-
ment a large amount of data will be necessary.
In terms of window size, the best accuracy was
observed for ten surrounding letters (i.e. N = 5).
We have therefore studied in more detail this case,
and determined learning rates associated with sev-
eral sets of ambiguous letters. We selected one of
the four languages for further analyses, including
correlation measurements between corpus size and
precision, and discussion of results. The language
of choice was Romanian, since this is the language
that has the largest corpus among the four study
languages.
Table 3 shows the results obtained for N = 5.
Tests were performed with training corpora of vari-
ous sizes, ranging from 2,000,000 examples to as few
as 10 examples, to the end of finding the learning
rate and the minimum size of a corpus required for
a satisfactory precision. All experiments are per-
formed with a test set size of 50,000 examples. The
table shows also the baseline, defined as the preci-
sion obtained when the most frequent letter in the
ambiguous set is selected by default.
The results shown in Table 3 are plotted in Figure
1. It is interesting to observe that the most impor-
tant part of the learning process is achieved with the
first 10,000 examples. We have measured that about
100,000-250,000 running characters (approx. 25-60
pages of text) are needed to generate 10,000 diacrit-
ics examples, a relatively small corpus. From there
on, a significant number of examples is required for
every single percent of improvement in accuracy. We
also show in bold the first precision figure that ex-
ceeds the baseline, as an indicative of the smallest
size of training set where a form of learning is ob-
served. Notice that as few as 1,000 examples are
enough to perform some learning.
According to (Banko and Brill, 2001) there are al-
gorithms that do not scale very well to the size of
the corpora. From the comparative results discussed
above, we observe that the letter level learning al-
gorithm that we propose is scalable to the size of
the corpus, and most importantly, it fares extremely
well for very little input data.
Using the entire set of examples extracted from
the corpus, the disambiguation of the i-i pair is
almost 100% correct. For this diacritic letter, we
now have one instance wrong out of 300 instances,
whereas the baseline implies one instance wrong
for every eight instances, therefore a significant im-
provement. The worst precision is achieved in the
case of a-a pair. From a simple error analysis, it
turns out that the main reason for this is the fact
that many Romanian nouns have their base form
ending in whereas their articulated form ends in a.
For instance, math and masa are two forms, one ar-
ticulated and one not, for the same noun table. The
learner is therefore tricked by many identical usages
for these letters. A simple solution for this would be
to avoid in the learning process those examples that
contain an a or letter at the end of a word. The
results obtained under this simplifying assumption
are reported in Table 3 under the heading a-a(2)
As shown in the table, more than four percents are
gained in precision with this simple condition.
We have also employed C4.5 on the same train-
ing data, but no improvements were observed with
respect to the results from Table 2. The disadvan-
tage of using C4.5 for this task is that the learn-
ing phase is slower than with the TiMBL imple-
mentation. On the other hand, C4.5 has the ca-
pability of generating expressive rules. &amp;quot;Li =e and
L2 =space then s&amp;quot;(99.5%), &amp;quot;Li =t and L2 =space then
s&amp;quot; (98.7%), &amp;quot;L_4 = p and L_i=v and Li =t L2 =e
then s &amp;quot;(95.5%), are examples of such rules, where
Li denotes a surrounding letter at the relative posi-
tion i with respect to the ambiguous letter. Notice
that these rules do not say anything about whether
or not the letters belong to one single word. The
learning algorithm simply relies on letters, regardless
of the word they belong to. Consequently, pseudo-
homographs words (as in peste and peste - see Sec-
tion 1) are equally addressed by this method, as the
algorithm has the capability of going across words.
</bodyText>
<subsectionHeader confidence="0.998083">
3.1 Different window sizes
</subsectionHeader>
<bodyText confidence="0.999891333333333">
We have experimented various window sizes to deter-
mine the size of the context that would best model
our problem. We considered window sizes of two,
six, ten, fourteen and eighteen surrounding letters
(i.e. N = 1, 3, 5, 7, 9). Comparative results, again
for the four Romanian ambiguous letter sets, are re-
ported in Table 4.
When no context is available, window sizes of
N=3 can be used without too much loss in preci-
sion. Nevertheless, as stated earlier, the best ac-
curacy is attained for a window of ten surrounding
letters (N=5).
</bodyText>
<sectionHeader confidence="0.966722" genericHeader="method">
4 Comparison with related work
</sectionHeader>
<bodyText confidence="0.999921857142857">
These results are best compared with the work re-
ported by Tufi§ and Chiu (Tufi§ and Chitu, 1999),
who employed one of the languages used in our ex-
periments, namely Romanian. According to Tufi§
and Chiu, the task of diacritics recovery in Roma-
nian is harder than with other languages, since Ro-
manian makes more intensive use of diacritics. As
reported in their experiments, only about 60% of the
Romanian words are diacritics free, compared to the
studies reported in (Simard, 1998) which show that
about 85% of the French words are spelled with no
accents.
The approach presented by Tufi§ and Chiu uses
dictionaries, a tokenizer and part of speech tagger,
</bodyText>
<table confidence="0.999664961538462">
Ambiguous pair
a-a a-à(2) i-i s- t-t
Data set size 2,161,556 1,369,517 2,055,147 866,964 1,157,458
Baseline 74.70% 85.90% 88.20% 76.53% 85.81%
Precision obtained with a test set
Training size of 50,000 examples
2,000,000 96.14% 99.69%
1,000,000 95.10% 99.14% 99.58% 98.75%
750,000 94.83% 98.97% 99.53% 99.07% 98.63%
500,000 94.57% 98.79% 99.46% 98.86% 98.40%
250,000 94.00% 98.37% 99.28% 98.87% 98.26%
100,000 93.03% 97.56% 98.96% 98.54% 97.81%
50,000 92.10% 96.86% 98.57% 98.13% 97.40%
25,000 90.99% 95.75% 98.11% 97.58% 96.92%
10,000 88.99% 93.75% 97.31% 96.53% 96.20%
5,000 87.56% 92.76% 96.65% 95.61% 95.10%
4,000 86.91% 91.86% 96.49% 94.99% 94.53%
3,000 86.39% 90.99% 96.19% 94.18% 94.30%
2,000 85.81% 89.93% 95.49% 93.47% 93.56%
1,000 83.49% 88.36% 93.78% 92.31% 91.85%
500 80.61% 85.66% 93.07% 90.75% 89.74%
250 77.89% 83.17% 92.75% 87.41% 87.23%
100 74.80% 84.04% 91.41% 82.13% 84.46%
50 72.79% 82.73% 88.05% 86.53% 77.54%
25 72.45% 81.34% 88.15% 78.26% 78.52%
10 73.38% 85.90% 88.20% 75.88% 85.81%
</table>
<tableCaption confidence="0.967212">
Table 3: Results obtained in solving diacritics ambiguity in Romanian, using a window size of ten surrounding
letters
</tableCaption>
<table confidence="0.999372">
Ambiguous Window size
Window
pair N=1 N=3 N=5 N=7 N=9
a-a 85.63% 95.79% 96.14% 96.10% 96.10%
i-i 94.18% 99.13% 99.69% 99.68% 99.43%
88.09% 99.06% 99.07% 99.02% 99.00%
t-1 89.45% 98.57% 98.75% 98.67% 98.25%
</table>
<tableCaption confidence="0.99964">
Table 4: Comparative results for various window sizes (Romanian)
</tableCaption>
<bodyText confidence="0.999947">
and learning is performed at word level, for an over-
all performance of 97.4%. We cannot directly com-
pare our results, as both methods and evaluations
are fundamentally different. The average precision
of 98.30% obtained with our algorithm on the Roma-
nian language (respectively 99.23% when a-a at the
end of a word is not considered during learning) is
measured at letter level, whereas the accuracy they
report is determined at word level.
The algorithm presented in this paper overcomes
previous approaches in that very high precisions and
processing speeds are obtained without any prepro-
cessing tools or dictionaries being required. This al-
gorithm is therefore applicable to any language, with
the only requirement being a medium size corpus of
texts with diacritics.
</bodyText>
<sectionHeader confidence="0.997695" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.998645933333334">
The experiments presented show that the automa-
tion of diacritics insertion based on learning at let-
ter level performs extremely well. The fact that the
resource requirements are so modest compared to
methods that use learning at word level - a medium
size corpus with diacritics versus part-of-speech tag-
gers, dictionaries, etc. - make it very appropriate
and easy to use especially for languages for which
such resources are not available. The method can
also be very useful in cases where word-based meth-
ods encounter the problem of unknown words.
Experiments were performed using four lan-
guages: Czech, Hugarian, Polish and Romanian.
Raw texts are fed to the learning mechanism, and
an average accuracy of over 98% at letter level was
</bodyText>
<figureCaption confidence="0.980522">
Figure 1: Learning rates for the four ambiguous diacritics in Romanian. The chart in the middle represents
a zoom of the 0-10,000 range area.
</figureCaption>
<figure confidence="0.999116">
100
95 -
II
90 -4
85
80 t
75
70
0 200000 400000 600000 800000 1000000
</figure>
<bodyText confidence="0.94995175">
observed. Moreover, because of the simplicity of
the algorithm, the learning is performed very fast,
at a speed of about 20 pages of text per second on
an off the shelf PC.
Acknowledgments
The authors would like to thank Jan 2iZka from
Masaryk University, Brno, Czeck Republic, and Dar-
iusz Kogut from the Warsaw University of Technol-
ogy, Warsaw, Poland, for their help in identifying
sources of electronic collections of texts with diacrit-
ics. They would also like to thank the anonymous re-
viewers for their helpful comments and suggestions.
</bodyText>
<sectionHeader confidence="0.998997" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99965088">
R.C. Angell, G.E. Freund, and P. Willett. 1983. Automatic
spelling correction using a trigram similarity measure. In-
formation Processing and Management, 19(4):255-261.
M. Banko and E. Brill. 2001. Scaling to very very large cor-
pora for natural language disambiguation. In Proceedings
of the 39th Annual Meeting of the Association for Compu-
tational Lingusitics (ACL-200I), Toulouse, France, July.
W. Daelemans, A. van den Bosch, and J. Zavrel. 1999. For-
getting exceptions is harmful in language learning. Ma-
chine Learning, 34(1-3):11-34.
W. Daelemans, J. Zavrel, K. van der Sloot, and A. van den
Bosch. 2001. Timbl: Tilburg memory based learner, ver-
sion 4.0, reference guide. Technical report, University of
Antwerp.
M. El-Beze, B. Merialdo, B. Rozeron, and A. Derouault.
1994. Accentuation automatique des textes par des
methodes probabilistes. Techniques et sciences informa-
tique, 16(6):797-815.
S.N. Galicia-Haro, I.A. Bolshakov, and A.F. Gelbukh. 1999.
A simple Spanish part of speech tagger for detection and
correction of accentuation error. In Proceedings of the Sec-
ond International Workshop on Text, Speech and Dialogue,
pages 219-222, Plzen, Czech Republic, September.
A. Kilgarriff, editor. 2001. Proceedings of SENSEVAL-
2, Association for Computational Linguistics Workshop,
Toulouse, France.
G. Nagy, Nagy N., and M. Sabourin. 1998. Signes diacri-
tiques: perdus et retrouves. In Actes du ler Collque Inter-
national Francophone sur l&apos;Ecrit et le Document CIFED
&apos;98, pages 404-412, Queebec, Canada.
J. Quinlan. 1993. C4.5: programs for machine learning.
Morgan Kaufman.
M. Simard. 1998. Automatic insertion of accents in French
text. In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing EMNLP-3, Granada.
D. Tufi. and A. Chiu. 1999. Automatic diacritics inser-
tion in Romanian texts. In Proceedings of the Interna-
tional Conference on Computational Lexicography COM-
PLEX&apos;99, Pecs, Hungary, June.
C.J. Van Rijsbergen. 1979. Information Retrieval.
London: Butterworths. available on-line at
http://www.des.gla.ac.uk/ Keith/Preface.html.
D. Yarowsky. 1994. Decision lists for lexical ambiguity res-
olution: Application to accent restoration in Spanish and
French. In Proceedings of the 32nd Annual Meeting of the
Association for Computational Linguistics, pages 88-95,
Las Cruces, NM.
D. Yarowsky, 1999. Corpus-based techniques for Restoring
accents in Spanish and French Text, pages 99-120. Kluwer
Academics Publisher.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.578267">
<title confidence="0.9985815">Letter Level Learning for Language Diacritics Restoration</title>
<author confidence="0.996485">Rada</author>
<affiliation confidence="0.902634">Department of Computer University of North Denton, TX,</affiliation>
<email confidence="0.99737">rada©cs.untedu</email>
<author confidence="0.973641">Vivi</author>
<affiliation confidence="0.999842">School of Information University of</affiliation>
<address confidence="0.886434">Ottawa, ON,</address>
<email confidence="0.989175">vnastaseAsite.uottawa.ca</email>
<abstract confidence="0.994496181818182">This paper presents a method for diacritics restoration based on learning mechanisms that act at letter level. The method requires no additional tagging tools or resources other than raw text, which makes it independent of the language, and particularly appealing for languages for which there are few resources available. The algorithm was evaluated on four different languages, namely Czech, Hungarian, Polish and Romanian, and an average accuracy of over 98% was observed.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R C Angell</author>
<author>G E Freund</author>
<author>P Willett</author>
</authors>
<title>Automatic spelling correction using a trigram similarity measure.</title>
<date>1983</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>19--4</pages>
<contexts>
<context position="2614" citStr="Angell et al., 1983" startWordPosition="405" endWordPosition="408">of 99%. Tufi§ and Chiu (Tufi§ and Chitu, 1999) propose a similar approach for diacritics insertion in Romanian texts. Yarowsky (Yarowsky, 1999) gives a comprehensive overview of accent restoration techniques. Most of the algorithms he presents rely on dictionaries and surrounding words in deciding whether to select a form or another for a given ambiguous word. A different approach is proposed by Nagy et. al in (Nagy et al., 1998), where strings extracted from texts are used to derive statistics, with high precision reported on French texts. Their work is similar with the approach proposed in (Angell et al., 1983), where trigram similarity measures are employed for automatic spelling correction. The majority of studies performed so far in this field have addressed well known and widely spread languages such as French or Spanish, and very few studies have emphasized less popular languages like Polish, Slovene, Turkish or other languages that employ diacritics in their spelling. Table 1 lists the diacritics encountered in European languages with Latin-based alphabets&apos;. As seen in the table, a large number of languages face the problem of diacritics restoration. From the 36 European languages considered, </context>
</contexts>
<marker>Angell, Freund, Willett, 1983</marker>
<rawString>R.C. Angell, G.E. Freund, and P. Willett. 1983. Automatic spelling correction using a trigram similarity measure. Information Processing and Management, 19(4):255-261.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Banko</author>
<author>E Brill</author>
</authors>
<title>Scaling to very very large corpora for natural language disambiguation.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association for Computational Lingusitics (ACL-200I),</booktitle>
<location>Toulouse, France,</location>
<contexts>
<context position="18078" citStr="Banko and Brill, 2001" startWordPosition="2931" endWordPosition="2934">learning process is achieved with the first 10,000 examples. We have measured that about 100,000-250,000 running characters (approx. 25-60 pages of text) are needed to generate 10,000 diacritics examples, a relatively small corpus. From there on, a significant number of examples is required for every single percent of improvement in accuracy. We also show in bold the first precision figure that exceeds the baseline, as an indicative of the smallest size of training set where a form of learning is observed. Notice that as few as 1,000 examples are enough to perform some learning. According to (Banko and Brill, 2001) there are algorithms that do not scale very well to the size of the corpora. From the comparative results discussed above, we observe that the letter level learning algorithm that we propose is scalable to the size of the corpus, and most importantly, it fares extremely well for very little input data. Using the entire set of examples extracted from the corpus, the disambiguation of the i-i pair is almost 100% correct. For this diacritic letter, we now have one instance wrong out of 300 instances, whereas the baseline implies one instance wrong for every eight instances, therefore a significa</context>
</contexts>
<marker>Banko, Brill, 2001</marker>
<rawString>M. Banko and E. Brill. 2001. Scaling to very very large corpora for natural language disambiguation. In Proceedings of the 39th Annual Meeting of the Association for Computational Lingusitics (ACL-200I), Toulouse, France, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>A van den Bosch</author>
<author>J Zavrel</author>
</authors>
<title>Forgetting exceptions is harmful in language learning.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<marker>Daelemans, van den Bosch, Zavrel, 1999</marker>
<rawString>W. Daelemans, A. van den Bosch, and J. Zavrel. 1999. Forgetting exceptions is harmful in language learning. Machine Learning, 34(1-3):11-34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>J Zavrel</author>
<author>K van der Sloot</author>
<author>A van den Bosch</author>
</authors>
<title>Timbl: Tilburg memory based learner, version 4.0, reference guide.</title>
<date>2001</date>
<tech>Technical report,</tech>
<institution>University of Antwerp.</institution>
<marker>Daelemans, Zavrel, van der Sloot, van den Bosch, 2001</marker>
<rawString>W. Daelemans, J. Zavrel, K. van der Sloot, and A. van den Bosch. 2001. Timbl: Tilburg memory based learner, version 4.0, reference guide. Technical report, University of Antwerp.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M El-Beze</author>
<author>B Merialdo</author>
<author>B Rozeron</author>
<author>A Derouault</author>
</authors>
<title>Accentuation automatique des textes par des methodes probabilistes. Techniques et sciences informatique,</title>
<date>1994</date>
<pages>16--6</pages>
<contexts>
<context position="1891" citStr="El-Beze et al., 1994" startWordPosition="287" endWordPosition="290">e absence of a tool for diacritics recovery, a search for the Romanian word pete(fish) retrieves peste(over) as well, paturi can be wrongly translated as beds, where the intended meaning was blankets (the translation of peituri), and so forth. The problem as such is not very difficult, and previous work has demonstrated that a good dictionary can lead to over 90% accuracy in accent restoration for French and Spanish (Yarowsky, 1994), (Simard, 1998), (Galicia-Haro et al., 1999). The method described by Michael Simard in (Simard, 1998) is an improvement over a similar method proposed by ElBeze (El-Beze et al., 1994). It relies on Hidden Markov Models and learns from surrounding words for an overall reported accuracy of 99%. Tufi§ and Chiu (Tufi§ and Chitu, 1999) propose a similar approach for diacritics insertion in Romanian texts. Yarowsky (Yarowsky, 1999) gives a comprehensive overview of accent restoration techniques. Most of the algorithms he presents rely on dictionaries and surrounding words in deciding whether to select a form or another for a given ambiguous word. A different approach is proposed by Nagy et. al in (Nagy et al., 1998), where strings extracted from texts are used to derive statisti</context>
</contexts>
<marker>El-Beze, Merialdo, Rozeron, Derouault, 1994</marker>
<rawString>M. El-Beze, B. Merialdo, B. Rozeron, and A. Derouault. 1994. Accentuation automatique des textes par des methodes probabilistes. Techniques et sciences informatique, 16(6):797-815.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S N Galicia-Haro</author>
<author>I A Bolshakov</author>
<author>A F Gelbukh</author>
</authors>
<title>A simple Spanish part of speech tagger for detection and correction of accentuation error.</title>
<date>1999</date>
<booktitle>In Proceedings of the Second International Workshop on Text, Speech and Dialogue,</booktitle>
<pages>219--222</pages>
<location>Plzen, Czech Republic,</location>
<contexts>
<context position="1751" citStr="Galicia-Haro et al., 1999" startWordPosition="262" endWordPosition="265">cquisition, and others. Spelling correction may have a direct impact on the processing quality in many of these applications. For instance, in the absence of a tool for diacritics recovery, a search for the Romanian word pete(fish) retrieves peste(over) as well, paturi can be wrongly translated as beds, where the intended meaning was blankets (the translation of peituri), and so forth. The problem as such is not very difficult, and previous work has demonstrated that a good dictionary can lead to over 90% accuracy in accent restoration for French and Spanish (Yarowsky, 1994), (Simard, 1998), (Galicia-Haro et al., 1999). The method described by Michael Simard in (Simard, 1998) is an improvement over a similar method proposed by ElBeze (El-Beze et al., 1994). It relies on Hidden Markov Models and learns from surrounding words for an overall reported accuracy of 99%. Tufi§ and Chiu (Tufi§ and Chitu, 1999) propose a similar approach for diacritics insertion in Romanian texts. Yarowsky (Yarowsky, 1999) gives a comprehensive overview of accent restoration techniques. Most of the algorithms he presents rely on dictionaries and surrounding words in deciding whether to select a form or another for a given ambiguous </context>
</contexts>
<marker>Galicia-Haro, Bolshakov, Gelbukh, 1999</marker>
<rawString>S.N. Galicia-Haro, I.A. Bolshakov, and A.F. Gelbukh. 1999. A simple Spanish part of speech tagger for detection and correction of accentuation error. In Proceedings of the Second International Workshop on Text, Speech and Dialogue, pages 219-222, Plzen, Czech Republic, September.</rawString>
</citation>
<citation valid="true">
<date>2001</date>
<booktitle>Proceedings of SENSEVAL2, Association for Computational Linguistics Workshop,</booktitle>
<editor>A. Kilgarriff, editor.</editor>
<location>Toulouse, France.</location>
<marker>2001</marker>
<rawString>A. Kilgarriff, editor. 2001. Proceedings of SENSEVAL2, Association for Computational Linguistics Workshop, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Nagy</author>
<author>N Nagy</author>
<author>M Sabourin</author>
</authors>
<title>Signes diacritiques: perdus et retrouves.</title>
<date>1998</date>
<booktitle>In Actes du ler Collque International Francophone sur l&apos;Ecrit et le Document CIFED &apos;98,</booktitle>
<pages>404--412</pages>
<location>Queebec, Canada.</location>
<contexts>
<context position="2427" citStr="Nagy et al., 1998" startWordPosition="374" endWordPosition="377"> is an improvement over a similar method proposed by ElBeze (El-Beze et al., 1994). It relies on Hidden Markov Models and learns from surrounding words for an overall reported accuracy of 99%. Tufi§ and Chiu (Tufi§ and Chitu, 1999) propose a similar approach for diacritics insertion in Romanian texts. Yarowsky (Yarowsky, 1999) gives a comprehensive overview of accent restoration techniques. Most of the algorithms he presents rely on dictionaries and surrounding words in deciding whether to select a form or another for a given ambiguous word. A different approach is proposed by Nagy et. al in (Nagy et al., 1998), where strings extracted from texts are used to derive statistics, with high precision reported on French texts. Their work is similar with the approach proposed in (Angell et al., 1983), where trigram similarity measures are employed for automatic spelling correction. The majority of studies performed so far in this field have addressed well known and widely spread languages such as French or Spanish, and very few studies have emphasized less popular languages like Polish, Slovene, Turkish or other languages that employ diacritics in their spelling. Table 1 lists the diacritics encountered i</context>
</contexts>
<marker>Nagy, Nagy, Sabourin, 1998</marker>
<rawString>G. Nagy, Nagy N., and M. Sabourin. 1998. Signes diacritiques: perdus et retrouves. In Actes du ler Collque International Francophone sur l&apos;Ecrit et le Document CIFED &apos;98, pages 404-412, Queebec, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Quinlan</author>
</authors>
<title>C4.5: programs for machine learning.</title>
<date>1993</date>
<publisher>Morgan Kaufman.</publisher>
<contexts>
<context position="11087" citStr="Quinlan, 1993" startWordPosition="1799" endWordPosition="1800">ent in terms of training and testing time. For our experiments, we used the TiMBL (Daelemans et al., 2001) implementation4. Since we work at the low level of letters, the target attribute to be learned is constituted by the ambiguous letters. It can be therefore any of the ambiguous letters listed in Table 1. For the four languages considered in these experiments, the sets of ambiguous letters are shown in Table 2. Upper case diacritics are not considered, since they have been previously converted to lower case. 4Similar experiments were performed with a decision tree classifier, namely C4.5 (Quinlan, 1993). The results obtained were similar with those obtained with the instance based learner, at significantly higher running times. 2.3 Features The features used in any learning algorithm have tremendous influence over the final accuracy. We do not make use of part of speech taggers or any other morphological or syntactic analyzers. Furthermore, we do not want to rely on surrounding words, since the data we have is limited, and therefore learning at word level would result in many cases of unknown words. We have therefore decided for very simple features, for the extraction of which no particular</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>J. Quinlan. 1993. C4.5: programs for machine learning. Morgan Kaufman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Simard</author>
</authors>
<title>Automatic insertion of accents in French text.</title>
<date>1998</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing EMNLP-3,</booktitle>
<location>Granada.</location>
<contexts>
<context position="1722" citStr="Simard, 1998" startWordPosition="260" endWordPosition="261">ation, Corpora Acquisition, and others. Spelling correction may have a direct impact on the processing quality in many of these applications. For instance, in the absence of a tool for diacritics recovery, a search for the Romanian word pete(fish) retrieves peste(over) as well, paturi can be wrongly translated as beds, where the intended meaning was blankets (the translation of peituri), and so forth. The problem as such is not very difficult, and previous work has demonstrated that a good dictionary can lead to over 90% accuracy in accent restoration for French and Spanish (Yarowsky, 1994), (Simard, 1998), (Galicia-Haro et al., 1999). The method described by Michael Simard in (Simard, 1998) is an improvement over a similar method proposed by ElBeze (El-Beze et al., 1994). It relies on Hidden Markov Models and learns from surrounding words for an overall reported accuracy of 99%. Tufi§ and Chiu (Tufi§ and Chitu, 1999) propose a similar approach for diacritics insertion in Romanian texts. Yarowsky (Yarowsky, 1999) gives a comprehensive overview of accent restoration techniques. Most of the algorithms he presents rely on dictionaries and surrounding words in deciding whether to select a form or a</context>
<context position="21463" citStr="Simard, 1998" startWordPosition="3532" endWordPosition="3533">ess, as stated earlier, the best accuracy is attained for a window of ten surrounding letters (N=5). 4 Comparison with related work These results are best compared with the work reported by Tufi§ and Chiu (Tufi§ and Chitu, 1999), who employed one of the languages used in our experiments, namely Romanian. According to Tufi§ and Chiu, the task of diacritics recovery in Romanian is harder than with other languages, since Romanian makes more intensive use of diacritics. As reported in their experiments, only about 60% of the Romanian words are diacritics free, compared to the studies reported in (Simard, 1998) which show that about 85% of the French words are spelled with no accents. The approach presented by Tufi§ and Chiu uses dictionaries, a tokenizer and part of speech tagger, Ambiguous pair a-a a-à(2) i-i s- t-t Data set size 2,161,556 1,369,517 2,055,147 866,964 1,157,458 Baseline 74.70% 85.90% 88.20% 76.53% 85.81% Precision obtained with a test set Training size of 50,000 examples 2,000,000 96.14% 99.69% 1,000,000 95.10% 99.14% 99.58% 98.75% 750,000 94.83% 98.97% 99.53% 99.07% 98.63% 500,000 94.57% 98.79% 99.46% 98.86% 98.40% 250,000 94.00% 98.37% 99.28% 98.87% 98.26% 100,000 93.03% 97.56% 9</context>
</contexts>
<marker>Simard, 1998</marker>
<rawString>M. Simard. 1998. Automatic insertion of accents in French text. In Proceedings of the Conference on Empirical Methods in Natural Language Processing EMNLP-3, Granada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Chiu</author>
</authors>
<title>Automatic diacritics insertion in Romanian texts.</title>
<date>1999</date>
<booktitle>In Proceedings of the International Conference on Computational Lexicography COMPLEX&apos;99,</booktitle>
<location>Pecs, Hungary,</location>
<marker>Chiu, 1999</marker>
<rawString>D. Tufi. and A. Chiu. 1999. Automatic diacritics insertion in Romanian texts. In Proceedings of the International Conference on Computational Lexicography COMPLEX&apos;99, Pecs, Hungary, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Van Rijsbergen</author>
</authors>
<title>Information Retrieval.</title>
<date>1979</date>
<location>London: Butterworths.</location>
<note>available on-line at http://www.des.gla.ac.uk/ Keith/Preface.html.</note>
<marker>Van Rijsbergen, 1979</marker>
<rawString>C.J. Van Rijsbergen. 1979. Information Retrieval. London: Butterworths. available on-line at http://www.des.gla.ac.uk/ Keith/Preface.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Decision lists for lexical ambiguity resolution: Application to accent restoration in Spanish and French.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>88--95</pages>
<location>Las Cruces, NM.</location>
<contexts>
<context position="1706" citStr="Yarowsky, 1994" startWordPosition="258" endWordPosition="259">al, Machine Translation, Corpora Acquisition, and others. Spelling correction may have a direct impact on the processing quality in many of these applications. For instance, in the absence of a tool for diacritics recovery, a search for the Romanian word pete(fish) retrieves peste(over) as well, paturi can be wrongly translated as beds, where the intended meaning was blankets (the translation of peituri), and so forth. The problem as such is not very difficult, and previous work has demonstrated that a good dictionary can lead to over 90% accuracy in accent restoration for French and Spanish (Yarowsky, 1994), (Simard, 1998), (Galicia-Haro et al., 1999). The method described by Michael Simard in (Simard, 1998) is an improvement over a similar method proposed by ElBeze (El-Beze et al., 1994). It relies on Hidden Markov Models and learns from surrounding words for an overall reported accuracy of 99%. Tufi§ and Chiu (Tufi§ and Chitu, 1999) propose a similar approach for diacritics insertion in Romanian texts. Yarowsky (Yarowsky, 1999) gives a comprehensive overview of accent restoration techniques. Most of the algorithms he presents rely on dictionaries and surrounding words in deciding whether to se</context>
</contexts>
<marker>Yarowsky, 1994</marker>
<rawString>D. Yarowsky. 1994. Decision lists for lexical ambiguity resolution: Application to accent restoration in Spanish and French. In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, pages 88-95, Las Cruces, NM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Corpus-based techniques for Restoring accents in Spanish and French Text,</title>
<date>1999</date>
<pages>99--120</pages>
<publisher>Kluwer Academics Publisher.</publisher>
<contexts>
<context position="2137" citStr="Yarowsky, 1999" startWordPosition="328" endWordPosition="329">lem as such is not very difficult, and previous work has demonstrated that a good dictionary can lead to over 90% accuracy in accent restoration for French and Spanish (Yarowsky, 1994), (Simard, 1998), (Galicia-Haro et al., 1999). The method described by Michael Simard in (Simard, 1998) is an improvement over a similar method proposed by ElBeze (El-Beze et al., 1994). It relies on Hidden Markov Models and learns from surrounding words for an overall reported accuracy of 99%. Tufi§ and Chiu (Tufi§ and Chitu, 1999) propose a similar approach for diacritics insertion in Romanian texts. Yarowsky (Yarowsky, 1999) gives a comprehensive overview of accent restoration techniques. Most of the algorithms he presents rely on dictionaries and surrounding words in deciding whether to select a form or another for a given ambiguous word. A different approach is proposed by Nagy et. al in (Nagy et al., 1998), where strings extracted from texts are used to derive statistics, with high precision reported on French texts. Their work is similar with the approach proposed in (Angell et al., 1983), where trigram similarity measures are employed for automatic spelling correction. The majority of studies performed so fa</context>
</contexts>
<marker>Yarowsky, 1999</marker>
<rawString>D. Yarowsky, 1999. Corpus-based techniques for Restoring accents in Spanish and French Text, pages 99-120. Kluwer Academics Publisher.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>