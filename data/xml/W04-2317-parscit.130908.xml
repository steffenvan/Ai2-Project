<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004231">
<title confidence="0.979826">
Towards automatic addressee identification in multi-party dialogues
</title>
<author confidence="0.99671">
Natasa Jovanovic
</author>
<affiliation confidence="0.9979525">
Department of Computer Science
University of Twente
</affiliation>
<address confidence="0.923706">
PO Box 217 Enschede, the Netherlands
</address>
<email confidence="0.99802">
natasa@cs.utwente.nl
</email>
<author confidence="0.877331">
Rieks op den Akker
</author>
<affiliation confidence="0.9972695">
Department of Computer Science
University of Twente
</affiliation>
<address confidence="0.92359">
PO Box 217 Enschede, the Netherlands
</address>
<email confidence="0.998011">
infrieks@cs.utwente.nl
</email>
<sectionHeader confidence="0.995616" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999925363636364">
The paper is about the issue of addressing in
multi-party dialogues. Analysis of addressing
behavior in face to face meetings results in
the identification of several addressing mech-
anisms. From these we extract several utter-
ance features and features of non-verbal com-
municative behavior of a speaker, like gaze
and gesturing, that are relevant for observers to
identify the participants the speaker is talking
to. A method for the automatic prediction of
the addressee of speech acts is discussed.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999396217391304">
Communication, between humans or between humans
and conversational computer agents, involves address-
ing. Addressing has received attention in the tradition
of conversation analysis (Clark and Carlson, 1992; Clark
and Schaefer, 1992), but not that much in the commu-
nity of computational dialogue systems. One exception is
(Traum, 2003). An explanation for this lack of attention
may be that most research in computational dialogue sys-
tems concerns systems that were designed for interaction
between one human user and one conversational agent.
In dialogues in which only two participants take part ad-
dressing goes without saying. Addressing becomes a real
issue in multi-party conversations and that is the subject
of this paper.
There are a number of application areas that could ben-
efit from studying addressing behavior in human human
interactions. It can provide valuable data for learning
more about human interaction and the way humans in-
teract with intelligent environments. The result can be
used by those who develop communicative agents in in-
teractive intelligent environments, meeting managers and
presentation assistants. If we could induce from recorded
meetings the ”who said what, when and to whom” we
can use this information for making summarizations of
meetings, and for real-time tracking.
Research on small group discussions (Carletta et al.,
2002) has shown that there is a noticeable difference in
the interaction patterns between large and small groups
(up to seven participants). A small group discussion
looks like two-way conversations but conversations oc-
cur between all pairs of members and every member can
initiate conversation. A large group discussion is more
like a series of conversations between a group leader and
various individuals with the rest participants present but
silent. We will focus our research on small group discus-
sions in meetings.
In this paper we propose research that aims at the au-
tomatic determination of the addressee of a speaker in
small meetings. Analysis of the mechanisms that people
use in identifying their addressees leads to a model of a
conversation that describes the features that play a role
in these mechanisms. These features can be of several
types: verbal, non-verbal, and features of the situation.
Our research is partly based on analysis of the IDIAP
multi-modal meeting data corpus made available through
the Media File Server 1.
</bodyText>
<sectionHeader confidence="0.732996" genericHeader="method">
2 Addressee detection - problem overview
</sectionHeader>
<bodyText confidence="0.993036">
One of the question of interest concerning a meeting is:
”Who talked to whom and about what during the meet-
ing?”. This question refers to three very important as-
pects of a conversational event: source of the message
(speaker identification), topic of the message (topic de-
tection) and addressee of the message (addressee identi-
fication).
Speaker and addressee roles are the basic conversa-
tional roles. There are different ways to categorize the
audience of a speech act. We use a taxonomy of con-
versational roles proposed in (Clark and Carlson, 1992).
People around an action are divided in those who re-
</bodyText>
<footnote confidence="0.913417">
1http://mmm.idiap.ch
</footnote>
<bodyText confidence="0.997946428571429">
ally participate in the action (active participants) and
those who do not (non-participants). The active partic-
ipants in a conversation include speaker and addressee as
well as other participants taking part in conversation but
currently not being addressed. Clark called them side-
participants. All other listeners who have no rights to
take part in conversation are called overhearers. Over-
hearers are divided in two groups: bystanders and eaves-
droppers. Bystanders are overhearers who are present and
the speaker is aware of their presence. Eavesdroppers are
those who are listening without the speakers awareness.
In determining the conversational roles in a meeting situ-
ation we will focus on the active participants. The prob-
lem of addressee identification amounts to the problem of
distinguishing the addressee from the side participants in
a conversation.
According to dialogue act theory (Bunt, 2000) an ut-
terance can consist of several segments which carry dif-
ferent dialogue acts. Each of these dialogue acts can have
it’s own addressee. The following example is an example
of multi-addressee utterances.
</bodyText>
<figure confidence="0.8915945">
A: We could use Java as a standard?
[suggestion] addressee B,C
B: yes— but what about C++ ?
[agreement]addressee A— [suggestion]addressee A,C
C: Both is OK for me
[accept] addressee A,B
</figure>
<sectionHeader confidence="0.586257" genericHeader="method">
3 Observation analysis - addressee
detection in meetings
</sectionHeader>
<bodyText confidence="0.972442357142857">
Three main questions considering addressee detections
are: 1. What are the relevant sources of information for
the addressee detection in face-to-face meetings? 2. How
does the speaker express who is the addressee of his ut-
terance? 3. How can we combine all this information in
order to determine the addressee of the speaker utterance?
In order to find answers on these questions we ob-
served meetings recorded at the IDIAP and annotated
several of them. For annotation we used the NITE Work-
bench for Windows (NWB3) annotation tool 2. We de-
fined our annotation scheme based on the initial assump-
tions about the information sources that can be used for
the addressee identification. These assumptions are the
result of our meeting observations.
</bodyText>
<subsectionHeader confidence="0.999906">
3.1 Sources of information
</subsectionHeader>
<bodyText confidence="0.9998254">
When two or more people are engaged in interaction
they communicate using verbal and/or non-verbal ele-
ments. The most natural and powerful human commu-
nication is in combined use of words, gaze, facial and
gestural movements, posture, bodily contact, etc.
</bodyText>
<footnote confidence="0.9295435">
2http://nite.nis.sdu.dk/download/. NWB The NITE Work-
bench is a general-purpose natural interactivity coding tool
</footnote>
<subsectionHeader confidence="0.733692">
3.1.1 Speech
</subsectionHeader>
<bodyText confidence="0.999157245283019">
Speech is the main communication channel used in the
meeting conversation. Therefore, it is the main source
for addressee detection. The most common heuristics that
may guide the addressee recognition process is the search
for linguistic markers in the utterance. Table 1 contains
linguistic markers that can be used as cues for addressee
detection. For instance, you is the personal pronoun that
refers to the meeting participants excluding the speaker of
the utterance. Usage of quantifying determiners, numer-
als and indefinite pronouns may help in distinguish you
as a particular person from you as a group. If an utter-
ance contains noun phrases like some ofyou, few ofyou,
most ofyou, etc., then it is addressed to all meeting par-
ticipants. The speaker doesn’t know who he is actually
addressing (He saw some ofyou yesterday).
Name detection is a powerful method for addressee
determination. The name in vocative form is used for di-
rect addressing the person with that name (What about
you, John?). Using the name of the participant the
speaker can claim something about the participant ad-
dressing the utterance to the other addressee (John was
not present et the last meeting).
Dialogue acts. There is a relation between addressees
of an utterances and the type of the dialogue act the
speaker performed. Sometimes the description of a di-
alogue act includes the possible addressees of the act.
Therefore, knowledge about the dialog act is used as a cue
for addressee detection. For dialogue act annotation we
use the Meeting Recorder Dialogue Acts (MRDA) tag set
(Dhillon et al., 2003). The MRDA is a tag set for labeling
multiparty face-to-face meetings. The tags in the MRDA
set are organized into thirteen groups according to syn-
tactic, semantic, pragmatic and functional characteristic
of the utterance they mark. For addressee detection pur-
poses we used a large subset of the MRDA tag set but we
organized them at two levels: forward looking function
(FLF) and backward looking function (BLF). FLF rep-
resents the effect that an utterance has on the subsequent
interaction. BLF indicates how an utterance relates to the
previous discourse. If an utterance has both functions the
corresponding addressee is the addressee of the BLF.
When an utterance is marked with a BLF it is related
to one of the previous utterances in the conversation. The
addressee of the associated dialogue act in most cases is
the speaker of the related utterance. However, it is pos-
sible that the speaker of the related utterance is the same
as the current speaker. For instance, a speaker can repeat
or correct himself. The addressees of these utterances are
addressees of the related utterances. Most of the BLFs are
related to the previous utterances of the other speaker (ac-
ceptance tags, answer tags, etc.). In the multiparty case
there is a number of interesting interaction patterns with
respect to addressee identification.
</bodyText>
<table confidence="0.999425875">
Word classes Example Example
Personal pronouns PP I/me, you, she/her, he/him, we/us What do you think about that?
Quantifying determiners+PP all of you, some of you, few of you He saw some of you yesterday.
Numerals+PP two of you, three of you, last of you Three of you should prepare a presentation.
Indefinite pronouns+PP anyone of you, someone of you Did anyone of you finish the job?
Possessive pronouns mine, yours, hers, his, ours, theirs Is this yours?
Personal adjectives my, your, his, her, our, their I like your style.
Indefinite pronouns everybody, somebody, anyone Does anyone have any question?
</table>
<tableCaption confidence="0.995759">
Table 1: Linguistic markers
</tableCaption>
<figure confidence="0.815106571428571">
A: We could use Java as a standard [suggestion]
B: I agree [accept]
C: No [reject]
D: For me, it is OK [accept]
A: I think that we should use Java [suggestion]
2. B: I propose C++ [suggestion]
C: I don’t agree with you [reject]
</figure>
<bodyText confidence="0.999599647058823">
In the first conversation all responses are related to A’s
proposal. Therefore, A is the addressee of the utterances
expressed by B, C and D. It means the addressee doesn’t
have to be the previous speaker. In the second example it
is not clear whether C rejects A’s or B’s proposal or both
proposals. Additional information obtained from visual
channels can help in resolving the addressee ambiguity.
Unlike BLFs, FLFs do not provide much information
about the addressee of a speaker’s utterance. Yet, some
assumptions about the utterance’s addressee are possible,
especially if we take in consideration the linguistic mark-
ers mentioned above. For instance, the speaker of an ut-
terance marked with some of the question tags directly
addresses the addressee to provide information. In com-
bination with the use of the personal pronoun we these
questions are addressed to a subgroup of participants or to
all participants rather than to a single person. Very often
questions in meeting discussions are open-ended ques-
tions. An open-ended question is a question that does
not seek a specific answer. Rather, it is asked in a broad
sense. An open-ended question is more likely addressed
to all meeting participants. If an open ended question
contains ’you’ than a single person is the most probable
addressee (What about C? questions? What about you?).
Linguistic markers and dialogue acts described above
provide us with starting assumptions about the most
likely addressee. These assumptions are mostly related
to a size of the target group i.e. whether the addressee is
a single participant, a group of participant or all partic-
ipants. Therefore, some other communication channels
are used in combination with speech for addressing the
utterance. In the following sections we will describe the
role of non-verbal communication channels in addressee
detection.
</bodyText>
<subsubsectionHeader confidence="0.548675">
3.1.2 Gaze direction
</subsubsectionHeader>
<bodyText confidence="0.9996926875">
Gaze is an important aspect of social interaction (Ar-
gyle, 1973). One of the functions of gaze is channel-
control. Mutual gaze is important when people want to
establish relationship. Unless the utterance is very short
the speaker very soon breaks the mutual gaze. When fin-
ishing the utterance, the speaker gazes back to a listener.
If the speaker decided to continue to talk at turn transition
points, or even before, he usually gazes away. Need for
feedback effects the speaker’s gaze direction. Gaze direc-
tion shows a participant’s focus of attention. In the meet-
ing scenario where all participants are around the table
the focus of attention of the current speaker are mostly the
other meeting participants. Since it is almost impossible
to record eye gazing of participants, gaze information is
obtained and induced from head movements. In (Stiefel-
hagen and Zhu, 2002) it is shown that we can predict a
participant focus of attention based on head orientation
with a reliability of 88,7 %.
The contribution of gaze information to addressee de-
tection is dependent on the current meeting action (dis-
cussion, presentation, note-taking, etc.), the participants’
location and the utterance length. During a presentation a
speaker most probably addresses utterances to all meeting
participants. Therefore, information about gaze direction
is less relevant for a presentation than for a discussion
meeting action. When the utterance is short a speaker
usually gazes only at one participant or at no one, ad-
dressing the utterance at a group ofpeople or at the whole
audience. Moreover, information about the visible areas
of the participants and hence the relative positions they
have in the meeting is relevant for interpreting the gaze
behavior in terms of focus of attention and it’s contribu-
tion to addressing. During a turn a speaker mostly looks
at the participant who are in his visible area. On the other
hand if he wants to address someone outside his visual
area he will often move his body towards the addressee.
The result of automatic or manual gaze annotation is
a list of gazed participants or objects, together with time
stamps. For the BLFs the first participant in the list is
of interest. If the participant is not in the speaker’s vis-
ible area then the gazed participant is the most likely
addressee of the speaker utterance. If the participant is
in the speaker’s visible area and he is a candidate from
the speech analysis then the likelihood that he is the ad-
dressee of the speaker utterance is greater. For the FLFs
utterance length and structure of the gaze list play a very
important role. For BLFs the last participant in the gazed
list is of interest.
</bodyText>
<figure confidence="0.416449">
1.
</figure>
<subsectionHeader confidence="0.675715">
3.1.3 Gesture
</subsectionHeader>
<bodyText confidence="0.9951849">
Pointing at a person is a way to address a speech act
to a person. It is usually accompanied with gazing at the
person. Still the addressee of a speaker’s utterance is not
necessarily the same as a person that the speaker points
at. When X talks to Y and points at Z, at the same time
X usually verbally refers to Z using a proper noun (name
of people, group name, etc.), a pronoun (he/she/they,
him/her/them, his/her/their, etc.) or using the role of par-
ticipant (boss, chairman, etc.). This means that X talks
to Y about Z. (Yesterday I met him on the street.)
</bodyText>
<subsectionHeader confidence="0.564911">
3.1.4 Context
</subsectionHeader>
<bodyText confidence="0.99998994117647">
The categories of the context that contribute to ad-
dressee detection are: interaction history, meeting action
history, user context and spatial context. Interaction his-
tory is related to the conversation history and to the non-
verbal interaction history. Conversation history contains
the temporal sequence of speakers, performed dialogue
acts and their addressees. Meeting action history is a se-
quence of previous meeting actions including the current
meeting action. For instance, if a presentation is fol-
lowed by a discussion, the presenter is the more proba-
ble addressee of the other participants’ utterances, espe-
cially those that are marked as questions. Spatial con-
text includes participants’ location, locations of the en-
vironmental objects, distance between participants, par-
ticipants’ visible area. User context includes participants
names, gender, social roles (status roles and closeness),
institutional roles etc.
</bodyText>
<subsectionHeader confidence="0.99923">
3.2 Towards an automatic addressee detection
</subsectionHeader>
<bodyText confidence="0.999995076923077">
Although participants or outsiders are most of the time
quite sure about the intended addressee of a speaker this
knowledge is essentially error-prone. Using observa-
tional features obtained from different available sources
they can only predict the most probable addressee of an
utterance. Methods for addressee detection will either be
rule based or follow a statistical approach.
A rule-based algorithm used for computing addressee
in the MRE (Mission Rehearsal Exercise) project is
shown in (Traum, 2003). The rule-based method we in-
tend to apply for addressee identification first processes
information obtained from the utterance. This returns a
list of possible addressees with corresponding probabil-
ities. The probabilities are estimations from annotated
meeting data. The idea is first to eliminate cases where
the addressee is completely determined (names in voca-
tive forms, quantifiers and numerals in combination with
’you’, etc.). According to analysis of the relation between
dialogue acts and addressee, different sets of rules are ap-
plied for FLFs and BLFs. For instance, if an utterance
is marked with a BLF that is related to an utterance of a
previous speaker, the addressee is the speaker of the re-
lated utterance with probability P. The following steps
are related to the processing of information from addi-
tional sources (gaze and gesture) adding the additional
probability values to the possible addressee. Contextual
features are used at each level of processing.
Given all available multi-modal information E about
a conversational situation a statistical addressee identifi-
cation method should classify the addressee for each dia-
logue act in the conversation. As a computational model
we will use Bayesian networks (BN). The nodes in the
Bayesian network will include all observable features as
input variable and one unobservable output variable that
represent the addressee. From some preliminary models,
we concluded that Bayesian network used for addressee
classification of FLFs is more complicated than for BLFs.
We therefore consider using separate models for BLFs
and FLFs.
</bodyText>
<sectionHeader confidence="0.998013" genericHeader="conclusions">
4 Conclusions and future directions
</sectionHeader>
<bodyText confidence="0.999954">
Addressing is an interesting aspect of communication
and the automatic identification of conversational roles in
multi-party dialogues is an open research problem. We
expect that statistical approaches can be applied at this
domain. Our future work will be based primarily on ob-
taining a huge set of data for training and testing the mod-
els. We will also define new scenario’s for new types of
meetings that will show more interesting phenomena re-
lated to addressing behavior.
</bodyText>
<sectionHeader confidence="0.999248" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998575592592593">
Michael Argyle. 1973. Social Interaction. Tavistock
Publications.
H. Bunt. 2000. Dialogue pragmatics and context specifi-
cation. In Abduction, Belief and Context in Dialogue;
studies in computational pragmatics. John Benjamins,
Amsterdam.
Jean Carletta, Anne H. Anderson, and S. Garrod. 2002.
Seeing eye to eye: an account of grounding and un-
derstanding in work groups. Bulletin of the Japanese
cognitive sciences, 9(1):1–20.
Herbert H. Clark and Thomas B. Carlson. 1992. Hear-
ers and speech acts. In Arenas of Language Use
(H.H.Clark ed.). University of Chicago Press and
CSLI.
Herbert H. Clark and Edward F. Schaefer. 1992. Deal-
ing with overhearers. In Arenas of Language Use
(H.H.Clark ed.). University of Chicago Press and
CSLI.
R. Dhillon, S Bhagat, H Carvey, and E. Shriberg. 2003.
Meeting recorder project:dialogue act labeling guide,
version 3. Technical report, ICSI.
Rainer Stiefelhagen and Jie Zhu. 2002. Head orienta-
tion and gaze direction in meetings. In Conference on
Human Factors in Computing Systems (CHI2002).
David Traum. 2003. Issues in multi-party dialogues. In
Advances in Agent Communication (F. Dignum, ed.).
Springer-Verlag LNCS.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.137049">
<title confidence="0.998912">Towards automatic addressee identification in multi-party dialogues</title>
<author confidence="0.965307">Natasa</author>
<affiliation confidence="0.9979925">Department of Computer University of</affiliation>
<author confidence="0.405977">PO Box Enschede</author>
<author confidence="0.405977">the</author>
<email confidence="0.753534">natasa@cs.utwente.nl</email>
<author confidence="0.862365">Rieks op den</author>
<affiliation confidence="0.9970025">Department of Computer University of</affiliation>
<author confidence="0.460447">PO Box Enschede</author>
<author confidence="0.460447">the</author>
<email confidence="0.807435">infrieks@cs.utwente.nl</email>
<abstract confidence="0.9970345">The paper is about the issue of addressing in multi-party dialogues. Analysis of addressing behavior in face to face meetings results in the identification of several addressing mechanisms. From these we extract several utterance features and features of non-verbal communicative behavior of a speaker, like gaze and gesturing, that are relevant for observers to identify the participants the speaker is talking to. A method for the automatic prediction of the addressee of speech acts is discussed.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael Argyle</author>
</authors>
<title>Social Interaction.</title>
<date>1973</date>
<publisher>Tavistock Publications.</publisher>
<contexts>
<context position="12158" citStr="Argyle, 1973" startWordPosition="1933" endWordPosition="1935">What about you?). Linguistic markers and dialogue acts described above provide us with starting assumptions about the most likely addressee. These assumptions are mostly related to a size of the target group i.e. whether the addressee is a single participant, a group of participant or all participants. Therefore, some other communication channels are used in combination with speech for addressing the utterance. In the following sections we will describe the role of non-verbal communication channels in addressee detection. 3.1.2 Gaze direction Gaze is an important aspect of social interaction (Argyle, 1973). One of the functions of gaze is channelcontrol. Mutual gaze is important when people want to establish relationship. Unless the utterance is very short the speaker very soon breaks the mutual gaze. When finishing the utterance, the speaker gazes back to a listener. If the speaker decided to continue to talk at turn transition points, or even before, he usually gazes away. Need for feedback effects the speaker’s gaze direction. Gaze direction shows a participant’s focus of attention. In the meeting scenario where all participants are around the table the focus of attention of the current spea</context>
</contexts>
<marker>Argyle, 1973</marker>
<rawString>Michael Argyle. 1973. Social Interaction. Tavistock Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Bunt</author>
</authors>
<title>Dialogue pragmatics and context specification.</title>
<date>2000</date>
<booktitle>In Abduction, Belief and Context in Dialogue; studies in computational pragmatics. John Benjamins,</booktitle>
<location>Amsterdam.</location>
<contexts>
<context position="4831" citStr="Bunt, 2000" startWordPosition="743" endWordPosition="744">l other listeners who have no rights to take part in conversation are called overhearers. Overhearers are divided in two groups: bystanders and eavesdroppers. Bystanders are overhearers who are present and the speaker is aware of their presence. Eavesdroppers are those who are listening without the speakers awareness. In determining the conversational roles in a meeting situation we will focus on the active participants. The problem of addressee identification amounts to the problem of distinguishing the addressee from the side participants in a conversation. According to dialogue act theory (Bunt, 2000) an utterance can consist of several segments which carry different dialogue acts. Each of these dialogue acts can have it’s own addressee. The following example is an example of multi-addressee utterances. A: We could use Java as a standard? [suggestion] addressee B,C B: yes— but what about C++ ? [agreement]addressee A— [suggestion]addressee A,C C: Both is OK for me [accept] addressee A,B 3 Observation analysis - addressee detection in meetings Three main questions considering addressee detections are: 1. What are the relevant sources of information for the addressee detection in face-to-face</context>
</contexts>
<marker>Bunt, 2000</marker>
<rawString>H. Bunt. 2000. Dialogue pragmatics and context specification. In Abduction, Belief and Context in Dialogue; studies in computational pragmatics. John Benjamins, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
<author>Anne H Anderson</author>
<author>S Garrod</author>
</authors>
<title>Seeing eye to eye: an account of grounding and understanding in work groups. Bulletin of the Japanese cognitive sciences,</title>
<date>2002</date>
<pages>9--1</pages>
<contexts>
<context position="2217" citStr="Carletta et al., 2002" startWordPosition="327" endWordPosition="330">er of application areas that could benefit from studying addressing behavior in human human interactions. It can provide valuable data for learning more about human interaction and the way humans interact with intelligent environments. The result can be used by those who develop communicative agents in interactive intelligent environments, meeting managers and presentation assistants. If we could induce from recorded meetings the ”who said what, when and to whom” we can use this information for making summarizations of meetings, and for real-time tracking. Research on small group discussions (Carletta et al., 2002) has shown that there is a noticeable difference in the interaction patterns between large and small groups (up to seven participants). A small group discussion looks like two-way conversations but conversations occur between all pairs of members and every member can initiate conversation. A large group discussion is more like a series of conversations between a group leader and various individuals with the rest participants present but silent. We will focus our research on small group discussions in meetings. In this paper we propose research that aims at the automatic determination of the ad</context>
</contexts>
<marker>Carletta, Anderson, Garrod, 2002</marker>
<rawString>Jean Carletta, Anne H. Anderson, and S. Garrod. 2002. Seeing eye to eye: an account of grounding and understanding in work groups. Bulletin of the Japanese cognitive sciences, 9(1):1–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert H Clark</author>
<author>Thomas B Carlson</author>
</authors>
<title>Hearers and speech acts.</title>
<date>1992</date>
<booktitle>In Arenas of Language Use (H.H.Clark ed.). University of Chicago Press and CSLI.</booktitle>
<contexts>
<context position="1055" citStr="Clark and Carlson, 1992" startWordPosition="146" endWordPosition="149">ior in face to face meetings results in the identification of several addressing mechanisms. From these we extract several utterance features and features of non-verbal communicative behavior of a speaker, like gaze and gesturing, that are relevant for observers to identify the participants the speaker is talking to. A method for the automatic prediction of the addressee of speech acts is discussed. 1 Introduction Communication, between humans or between humans and conversational computer agents, involves addressing. Addressing has received attention in the tradition of conversation analysis (Clark and Carlson, 1992; Clark and Schaefer, 1992), but not that much in the community of computational dialogue systems. One exception is (Traum, 2003). An explanation for this lack of attention may be that most research in computational dialogue systems concerns systems that were designed for interaction between one human user and one conversational agent. In dialogues in which only two participants take part addressing goes without saying. Addressing becomes a real issue in multi-party conversations and that is the subject of this paper. There are a number of application areas that could benefit from studying add</context>
<context position="3849" citStr="Clark and Carlson, 1992" startWordPosition="592" endWordPosition="595">through the Media File Server 1. 2 Addressee detection - problem overview One of the question of interest concerning a meeting is: ”Who talked to whom and about what during the meeting?”. This question refers to three very important aspects of a conversational event: source of the message (speaker identification), topic of the message (topic detection) and addressee of the message (addressee identification). Speaker and addressee roles are the basic conversational roles. There are different ways to categorize the audience of a speech act. We use a taxonomy of conversational roles proposed in (Clark and Carlson, 1992). People around an action are divided in those who re1http://mmm.idiap.ch ally participate in the action (active participants) and those who do not (non-participants). The active participants in a conversation include speaker and addressee as well as other participants taking part in conversation but currently not being addressed. Clark called them sideparticipants. All other listeners who have no rights to take part in conversation are called overhearers. Overhearers are divided in two groups: bystanders and eavesdroppers. Bystanders are overhearers who are present and the speaker is aware of</context>
</contexts>
<marker>Clark, Carlson, 1992</marker>
<rawString>Herbert H. Clark and Thomas B. Carlson. 1992. Hearers and speech acts. In Arenas of Language Use (H.H.Clark ed.). University of Chicago Press and CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert H Clark</author>
<author>Edward F Schaefer</author>
</authors>
<title>Dealing with overhearers.</title>
<date>1992</date>
<booktitle>In Arenas of Language Use (H.H.Clark ed.). University of Chicago Press and CSLI.</booktitle>
<contexts>
<context position="1082" citStr="Clark and Schaefer, 1992" startWordPosition="150" endWordPosition="153">ngs results in the identification of several addressing mechanisms. From these we extract several utterance features and features of non-verbal communicative behavior of a speaker, like gaze and gesturing, that are relevant for observers to identify the participants the speaker is talking to. A method for the automatic prediction of the addressee of speech acts is discussed. 1 Introduction Communication, between humans or between humans and conversational computer agents, involves addressing. Addressing has received attention in the tradition of conversation analysis (Clark and Carlson, 1992; Clark and Schaefer, 1992), but not that much in the community of computational dialogue systems. One exception is (Traum, 2003). An explanation for this lack of attention may be that most research in computational dialogue systems concerns systems that were designed for interaction between one human user and one conversational agent. In dialogues in which only two participants take part addressing goes without saying. Addressing becomes a real issue in multi-party conversations and that is the subject of this paper. There are a number of application areas that could benefit from studying addressing behavior in human h</context>
</contexts>
<marker>Clark, Schaefer, 1992</marker>
<rawString>Herbert H. Clark and Edward F. Schaefer. 1992. Dealing with overhearers. In Arenas of Language Use (H.H.Clark ed.). University of Chicago Press and CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dhillon</author>
<author>S Bhagat</author>
<author>H Carvey</author>
<author>E Shriberg</author>
</authors>
<title>Meeting recorder project:dialogue act labeling guide, version 3.</title>
<date>2003</date>
<tech>Technical report, ICSI.</tech>
<contexts>
<context position="8023" citStr="Dhillon et al., 2003" startWordPosition="1255" endWordPosition="1258">hat name (What about you, John?). Using the name of the participant the speaker can claim something about the participant addressing the utterance to the other addressee (John was not present et the last meeting). Dialogue acts. There is a relation between addressees of an utterances and the type of the dialogue act the speaker performed. Sometimes the description of a dialogue act includes the possible addressees of the act. Therefore, knowledge about the dialog act is used as a cue for addressee detection. For dialogue act annotation we use the Meeting Recorder Dialogue Acts (MRDA) tag set (Dhillon et al., 2003). The MRDA is a tag set for labeling multiparty face-to-face meetings. The tags in the MRDA set are organized into thirteen groups according to syntactic, semantic, pragmatic and functional characteristic of the utterance they mark. For addressee detection purposes we used a large subset of the MRDA tag set but we organized them at two levels: forward looking function (FLF) and backward looking function (BLF). FLF represents the effect that an utterance has on the subsequent interaction. BLF indicates how an utterance relates to the previous discourse. If an utterance has both functions the co</context>
</contexts>
<marker>Dhillon, Bhagat, Carvey, Shriberg, 2003</marker>
<rawString>R. Dhillon, S Bhagat, H Carvey, and E. Shriberg. 2003. Meeting recorder project:dialogue act labeling guide, version 3. Technical report, ICSI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rainer Stiefelhagen</author>
<author>Jie Zhu</author>
</authors>
<title>Head orientation and gaze direction in meetings.</title>
<date>2002</date>
<booktitle>In Conference on Human Factors in Computing Systems (CHI2002).</booktitle>
<contexts>
<context position="12966" citStr="Stiefelhagen and Zhu, 2002" startWordPosition="2064" endWordPosition="2068">s the mutual gaze. When finishing the utterance, the speaker gazes back to a listener. If the speaker decided to continue to talk at turn transition points, or even before, he usually gazes away. Need for feedback effects the speaker’s gaze direction. Gaze direction shows a participant’s focus of attention. In the meeting scenario where all participants are around the table the focus of attention of the current speaker are mostly the other meeting participants. Since it is almost impossible to record eye gazing of participants, gaze information is obtained and induced from head movements. In (Stiefelhagen and Zhu, 2002) it is shown that we can predict a participant focus of attention based on head orientation with a reliability of 88,7 %. The contribution of gaze information to addressee detection is dependent on the current meeting action (discussion, presentation, note-taking, etc.), the participants’ location and the utterance length. During a presentation a speaker most probably addresses utterances to all meeting participants. Therefore, information about gaze direction is less relevant for a presentation than for a discussion meeting action. When the utterance is short a speaker usually gazes only at o</context>
</contexts>
<marker>Stiefelhagen, Zhu, 2002</marker>
<rawString>Rainer Stiefelhagen and Jie Zhu. 2002. Head orientation and gaze direction in meetings. In Conference on Human Factors in Computing Systems (CHI2002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Traum</author>
</authors>
<title>Issues in multi-party dialogues.</title>
<date>2003</date>
<booktitle>In Advances in Agent Communication</booktitle>
<editor>(F. Dignum, ed.).</editor>
<publisher>Springer-Verlag LNCS.</publisher>
<contexts>
<context position="1184" citStr="Traum, 2003" startWordPosition="169" endWordPosition="170">s and features of non-verbal communicative behavior of a speaker, like gaze and gesturing, that are relevant for observers to identify the participants the speaker is talking to. A method for the automatic prediction of the addressee of speech acts is discussed. 1 Introduction Communication, between humans or between humans and conversational computer agents, involves addressing. Addressing has received attention in the tradition of conversation analysis (Clark and Carlson, 1992; Clark and Schaefer, 1992), but not that much in the community of computational dialogue systems. One exception is (Traum, 2003). An explanation for this lack of attention may be that most research in computational dialogue systems concerns systems that were designed for interaction between one human user and one conversational agent. In dialogues in which only two participants take part addressing goes without saying. Addressing becomes a real issue in multi-party conversations and that is the subject of this paper. There are a number of application areas that could benefit from studying addressing behavior in human human interactions. It can provide valuable data for learning more about human interaction and the way </context>
<context position="16842" citStr="Traum, 2003" startWordPosition="2697" endWordPosition="2698">oles (status roles and closeness), institutional roles etc. 3.2 Towards an automatic addressee detection Although participants or outsiders are most of the time quite sure about the intended addressee of a speaker this knowledge is essentially error-prone. Using observational features obtained from different available sources they can only predict the most probable addressee of an utterance. Methods for addressee detection will either be rule based or follow a statistical approach. A rule-based algorithm used for computing addressee in the MRE (Mission Rehearsal Exercise) project is shown in (Traum, 2003). The rule-based method we intend to apply for addressee identification first processes information obtained from the utterance. This returns a list of possible addressees with corresponding probabilities. The probabilities are estimations from annotated meeting data. The idea is first to eliminate cases where the addressee is completely determined (names in vocative forms, quantifiers and numerals in combination with ’you’, etc.). According to analysis of the relation between dialogue acts and addressee, different sets of rules are applied for FLFs and BLFs. For instance, if an utterance is m</context>
</contexts>
<marker>Traum, 2003</marker>
<rawString>David Traum. 2003. Issues in multi-party dialogues. In Advances in Agent Communication (F. Dignum, ed.). Springer-Verlag LNCS.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>