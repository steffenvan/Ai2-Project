<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.982638">
Answer Mining from On-Line Documents
</title>
<author confidence="0.996807">
Marius Pas¸ca and Sanda M. Harabagiu
</author>
<affiliation confidence="0.8287415">
Department of Computer Science and Engineering
Southern Methodist University
</affiliation>
<address confidence="0.825583">
Dallas, TX 75275-0122
</address>
<email confidence="0.998362">
mars,sanda @engr.smu.edu
</email>
<sectionHeader confidence="0.995624" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999901157894737">
Mining the answer of a natural lan-
guage open-domain question in a large
collection of on-line documents is made
possible by the recognition of the ex-
pected answer type in relevant text pas-
sages. If the technology of retriev-
ing texts where the answer might be
found is well developed, few studies
have been devoted to the recognition of
the answer type.
This paper presents a unified model of
answer types for open-domain Ques-
tion/Answering that enables the discov-
ery of exact answers. The evaluation
of the model, performed on real-world
questions, considers both the correct-
ness and the coverage of the answer
types as well as their contribution to an-
swer precision.
</bodyText>
<sectionHeader confidence="0.999316" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9549423">
Answer mining, a.k.a. textual Ques-
tion/Answering (Q/A), represents the task of
discovering the answer to an open-domain nat-
ural language question in large text collections.
Answer mining became a topic of significant
recent interest, partly due to the popularity of In-
ternet Q/A services like AskJeeves and partly due
to the recent evaluations of domain-independent
Q/A systems organized in the context of the
Text REtrieval Conference (TREC)1. The TREC
</bodyText>
<subsectionHeader confidence="0.703019">
&apos;The Text REtrieval Conference (TREC) is a series of
</subsectionHeader>
<bodyText confidence="0.999243010000001">
evaluations of fully automatic Q/A systems
specified two restrictions: (1) there is at least
one document in the test collection that contains
the answer to a test question; and (2) the answer
length is either 50 contiguous bytes (short an-
swers) or 250 contiguous bytes (long answers).
These two requirements intentionally simplify
the answer mining task, since the identification
of the exact answer is left to the user. However,
given that the expected information is recognized
by inspecting text snippets of relatively small
size, the TREC Q/A task took a step closer
to information retrieval rather than document
retrieval. Moreover, the techniques developed
to extract text snippets where the answers might
lie paved the way to a unified model for answer
mining.
To find the answer to a question several steps
must be taken, as reported in (Abney et al., 2000)
(Moldovan et al., 2000) (Srihari and Li, 2000):
First, the question semantics needs to be cap-
tured. This translates into identifying (i) the
expected answer type and (ii) the question
keywords that can be used to retrieve text
passages where the answer may be found.
Secondly, the index of the document collec-
tion must be used to identify the text pas-
sages of interest. The retrieval method either
employs special operators or simply modi-
fies boolean or vector retrieval. Since the
expected answer type is known at the time
workshops organized by the National Institute of Standards
and Technology (NIST), designed to advance the state-of-
the-art in information retrieval (IR)
of the retrieval, the quality of the text pas-
sages is greatly improved by filtering out
those passages where concepts of the same
category as the answer type are not present.
Thirdly, answer extraction takes place by
combining several features that take into ac-
count the expected answer type.
Since the expected answer type is the only in-
formation used in all the phases of textual Q/A,
its recognition and usage is central to the perfor-
mance of answer mining.
For an open-domain Q/A system, establishing
the possible answer types is a challenging prob-
lem. Currently, most of the systems recognize the
answer type by associating the question stem (e.g.
What, Who, Why or How) and one of the concepts
from the question to a predefined general cate-
gory, such as PERSON, ORGANIZATION, LOCA-
TION, TIME, DATE, MONEY or NUMBER. Since
many of these categories are represented in texts
as named entities, their recognition as possible
answers is enabled by state-of-the-art Named En-
tity (NE) recognizers, devised to work with high
precision in Information Extraction (IE) tasks. To
allow for NE-supported answer mining, a large
number of semantic categories corresponding to
various names must be considered, e.g. names of
cars, names of diseases, names of dishes, names
of boats, etc. Furthermore, a significant number
of entities are not unique, therefore do not bear
names, but are still potential answers to an open-
domain question. Additionally, questions do not
focus only on entities and their attributes; they
also ask about events and their related entities.
In this paper we introduce a model of answer
types that accounts for answers to questions of
various complexity. The model enables several
different formats of the exact answer to open-
domain questions and considers also the situation
when the answer is produced from a number of
different document sources. We define formally
the answer types to open-domain questions and
extend the recognition of answer types beyond the
question processing phase, thus enabling several
feed-back mechanisms derived from the process-
ing of documents and answers.
The main contribution of the paper is in provid-
ing a unified model of answer mining from large
collections of on-line documents that accounts for
the processing of open-domain natural language
questions of varied complexity. The hope is that a
coherent model of the textual answer discovery
could help developing better text mining meth-
ods, capable of acquiring and rapidly prototyping
knowledge from the vast amount of on-line texts.
Additionally, such a model enables the develop-
ment of intelligent conversational agents that op-
erate on open-domain tasks.
We first present a background of Q/A systems
and then define several classes of question com-
plexity. In Section 3 we present the formal answer
type model whereas in Section 4 we show how to
recognize the answer type of open-domain ques-
tions and use it to mine the answer. Section 5
presents the evaluation of the model and summa-
rizes the conclusions.
</bodyText>
<sectionHeader confidence="0.993571" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.99298">
Open-Domain Question/Answering
</subsectionHeader>
<bodyText confidence="0.994845407407407">
To search in a large collection of on-line docu-
ments for the answer to a natural language ques-
tion we need to know (1) what we are looking for,
i.e. the expected answer type; and (2) where the
answer might be located in the collection. Fur-
thermore, knowing the answer type and recog-
nizing a text passage where the answer might be
found is not sufficient for extracting the exact an-
swer. We also need to know the dependencies
between the answer type and the other concepts
from the question or the answer. For example, if
the answer type of the TREC question
QT: How many dogs pull a sled in the Iditarod?
is known to be a number, we also need to be aware
that this number must quantify the dogs harnessed
to a sled in the Iditarod games and not the number
of participants in the games.
Capturing question or answer dependencies
can be cast as a straightforward process of
mapping syntactic trees to sets of binary head-
modifier relationships, as first noted in (Collins,
1996). Given a parse tree, the head-child of each
syntactic constituent can be identified based on a
simple set of rules used to train syntactic parsers,
cf. (Collins, 1996). Dependency relations are es-
tablished between each leaf corresponding to the
head child and the leaves of its constituent sib-
</bodyText>
<table confidence="0.991253625">
Question ET1: What do most tourists visit in Reims?
WP Parse: VBP SBARQ NNS visit IN NP
WHNP do RBS SQ VP PP NNP
What most VB in Reims
NP
tourists
Question Dependecies (ET1):
What most tourists visit Reims
</table>
<figureCaption confidence="0.999597">
Figure 1: Example of TREC test question
</figureCaption>
<bodyText confidence="0.992489115384615">
lings that are not stop words, as illustrated by the
mapping of Figure 1(a) into Figure 1(b). Unlike in
IR systems, question stems are considered content
words. When question dependencies are known
(Harabagiu et al., 2000) proposed a technique of
identifying the answer type based on the semantic
category of the question stem and eventually of its
most connected dependent concept. For example,
in the case of question ET1, illustrated in Figure 1,
the answer type is determined by the ambiguous
question stem what and the verb visit. The answer
type is the object of the verb visit, which is a place
of attraction or entertainment, defined by the se-
mantic category LANDMARK. The answer type
replaces the question stem, generating the follow-
ing dependency graph, that can be later unified
with the answer dependency graph:
However syntactic dependencies vary across
question reformulations or equivalent answers
made possible by the productive nature of natural
language. For example, the dependency structure
of ET2, a reformulation of question ET1 differs
from the dependency structure of ET1:
Due to the fact that verbs see and visit are syn-
onyms (cf. WordNet (Miller, 1995)) and pronoun
I can be read a possible visitor, the dependency
</bodyText>
<subsectionHeader confidence="0.496969">
Question ET2: What could I see in Reims?
</subsectionHeader>
<bodyText confidence="0.998769588235294">
structures of ET1 and ET2 can be mapped one into
another. The mapping is produced by unifying the
two structures when lexical and semantic alterna-
tions are allowed. Possible lexical alternations are
synonyms or morphological alternations. Seman-
tic alternations consist of hypernyms, entailments
or paraphrases. The unifying mapping of ET1 and
ET2 shows that the two questions are equivalent
only when I refers to a visitor; other readings of
ET2 being possible when the referent is an investi-
gator or a politician. In each of the other readings,
the answer type of the question would be differ-
ent. The unifying mapping of ET1 and ET2 is:
Similarly, a pair of equivalent answers is rec-
ognized when lexical and semantic alternations of
the concepts are allowed. This observation is cru-
cial for answer mining because:
</bodyText>
<listItem confidence="0.81344475">
1. it establishes the dependency relations as the
basic processing level for Q/A; and
2. it defines the search space based on alterna-
tions of the question and answer concepts.
</listItem>
<bodyText confidence="0.999225157894737">
Consequently, lexical and semantic alternations
are incorporated as feedback loops in the architec-
ture of open-domain Q/A systems, as illustrated
in Figure 2.
To locate answers, text passages are retrieved
based on keywords assembled from the question
dependency structure. At the time of the query, it
is unknown which keywords can be unified with
answer dependencies. However, the relevance of
the query is determined by the number of result-
ing passages. If too many passages are gener-
ated, the query was too broad, thus is needs a
specialization by adding a new keyword. If too
few passages were retrieved the query was too
specific, thus one keyword needs to be dropped.
The relevance feedback based on the number of
retrieved passages ends when no more keywords
can be added or dropped. After this, the unifi-
cations of the question and answer dependencies
</bodyText>
<figure confidence="0.970533">
LANDMARK most tourists visit Reims
LANDMARK I see Reims
LANDMARK I --&gt;tourists see/visit
Reims
Question On-line Documents Answer
</figure>
<figureCaption confidence="0.896658">
Figure 2: A diagram of the feedbacks supporting Open-Domain Q/A
</figureCaption>
<figure confidence="0.997630941176471">
Question
Dependencies
Answer Type
Lexical Alternations
Keywords
Relevance Feedback (# Passages)
Text Passages
Index
Semantic Unifications Abductive Justification
Answer
Dependencies
Semantic Alternations
Answer Fusion
Lexico
Semantic
KB
KB
</figure>
<bodyText confidence="0.999844083333333">
is produced and the lexical alternations imposed
by unifications are added to the list of keywords,
making possible the retrieval of new, unseen text
passages, as illustrated in Figure 2.
The unification of dependency structures al-
lows erroneous answers when the resulting map-
ping is a sparse graph. To justify the correct-
ness of the answer an abductive proof backchain-
ing from the answer to the question must be
produced. Such abductive mechanisms are de-
tailed in (Harabagiu et al., 2000). Moreover, the
proof relies on lexico-semantic knowledge avail-
able from WordNet as well as rapidly formated
knowledge bases generated by mechanisms de-
scribed in (Chaudri et al., 2000). The justification
process brings forward semantic alternations that
are added to the list of keywords, the feedback
destination of all loops represented in Figure 2.
Mining the exact answer does not always end
after extracting the answer type from a correct
text snippet because often they result only in par-
tial answers that need to be fused together. The
fusion mechanisms are dictated by the answer
type.
</bodyText>
<subsectionHeader confidence="0.954969">
Question Complexity
</subsectionHeader>
<bodyText confidence="0.973148133333333">
Open-Domain natural language questions can
also be of different complexity levels. Gener-
ally, the test questions used in the TREC evalu-
ations were qualified as fact-based questions (cf.
(Voorhees and Tice, 2000)) as they mainly were
short inquiries about attributes or definitions of
some entity or event. Table 1 lists a sample of
TREC test questions.
The TREC test set did not include any question
Where is Romania located? Europe
Who wrote ”Dubliners”? James Joyce
What is the wingspan of a condor? 9 feet
What is the population of Japan? 120 million
What king signed the Magna Carta? King John
Name a flying mammal. bat
</bodyText>
<tableCaption confidence="0.96525">
Table 1: TREC test questions and their exact an-
swers (boldfaced)
</tableCaption>
<bodyText confidence="0.989196375">
that can be modeled as Information Extraction
(IE) task. Typically, IE templates model queries
regarding who did an event of interest, what was
produced by that event, when and where and even-
tually why. The event of interest is a complex
event, like terrorism in Latin America, joint ven-
tures or management successions. An example of
template-modeled question is:
What management successions occurred at
IBM in 1999?
In addition, questions may also ask about de-
velopments of events or trends that are usually
answered by a text summary. Since data produc-
ing these summaries can be sourced in different
documents, summary fusion techniques as pro-
posed in (Radev and McKeown, 1998) can be em-
ployed. Template-based questions and summary-
asking inquiries cover most of the classes of ques-
tion complexity proposed in (Moldovan et al.,
2000). Although the topic of natural language
open-domain question complexity needs further
study, we consider herein the following classes of
questions:
Class 1: Questions inquiring about entities,
events, entity attributes (including number),
event themes, event manners, event condi-
tions and event consequences.
Class 2: Questions modeled by templates,
including questions that focus only on one
of the template slots (e.g. “What managers
were promoted last year at Microsoft?”).
Class 3: Questions asking for a sum-
mary that is produced by fusing template-
based information from different sources
(e.g. “What happened after the Titanic
sunk?”).
Since (Radev and McKeown, 1998) describes the
summary fusion mechanisms, Class 3 of ques-
tions can be reduced in this paper to Class 2,
which deals with the processing of the template.
</bodyText>
<sectionHeader confidence="0.998378" genericHeader="method">
3 A Model of Answer Types
</sectionHeader>
<bodyText confidence="0.998738125">
This section describes a knowledge-based model
of open-domain natural language answer types
(ATs). In particular we formally define the an-
swer type through a quadruple
CATEGORY, DEPENDENCY, NUMBER,
FORMAT.
The CATEGORY is defined as one of the following
possibilities:
</bodyText>
<listItem confidence="0.9376096">
1. one of the tops of a predefined ANSWER
TAXONOMY or one of its nodes;
2. DEFINITION;
3. TEMPLATE; or
4. SUMMARY.
</listItem>
<bodyText confidence="0.98564125">
For expert Q/A systems, this list of categories can
be extended. The DEPENDENCY is defined as the
question dependency structure when the CATE-
GORY belongs to the ANSWER TAXONOMY or is
a DEFINITION. Otherwise it is a template auto-
matically generated. The NUMBER is a flag indi-
cating whether the answer should contain a single
datum or a list of elements. The FORMAT defines
the text span of the exact answer. For example, if
the CATEGORY is DIMENSION, the FORMAT is
Number Measuring Unit .
The ANSWER TAXONOMY was created in
three steps:
Step 1 We devise a set of top categories modeled
after the semantic domains encoded in the Word-
Net database, which contains 25 noun categories
and 15 verb categories. The top of each WordNet
hierarchy corresponding to every semantic cate-
gory was manually inspected to select the most
representative nodes and add them to the tops of
he ANSWER TAXONOMY. Furthermore we have
added open semantic categories corresponding to
named entities. For example Table 2 lists the
named entity categories we have considered in
our experiments. Many of the tops of the AN-
SWER TAXONOMY are further categorized, as il-
lustrated in Figure 3. In total, we have considered
33 concepts as tops of the taxonomy.
</bodyText>
<figure confidence="0.586735666666667">
NUMERICAL VALUE LOCATION
RATE DURATION
LOCATION
</figure>
<figureCaption confidence="0.662582928571429">
Figure 3: Two examples of top answer hierar-
chies.
Step 2 The additional categorization of the top
ANSWER TAXONOMY generates a many-to-
many mapping of the Named Entity categories in
the tops of the ANSWER TAXONOMY. Figure 4
illustrates some of the mappings.
date time organization city
product price country money
human disease phone number continent
percent province other location plant
mammal alphabet airport code game
bird reptile university dog breed
number quantity landmark dish
</figureCaption>
<tableCaption confidence="0.911141">
Table 2: Named Entity Categories.
</tableCaption>
<figure confidence="0.5406205">
ANSWER TYPE NAMED ENTYTY CATEGORY
human
money
price
quantity
number
</figure>
<figureCaption confidence="0.99782025">
Figure 4: Mappings of answer types in named en-
tity categories.
Step 3: Each leaf from the top of the ANSWER
TAXONOMY is connected to one or several Word-
</figureCaption>
<figure confidence="0.99731925">
DEGREE
DIMENSION PERCENTAGE TOWN
COUNT
COUNTRY
PROVINCE OTHER
PERSON
MONEY
SPEED
DURATION
AMOUNT
NATIONALITY NUMERICAL VALUE LOCATION
DEGREE TEMPERATURE DURATION COUNT SPEED DIMENSION
</figure>
<figureCaption confidence="0.999134">
Figure 5: Fragment of the ANSWER TAXONOMY.
</figureCaption>
<bodyText confidence="0.6531185">
How hot does the inside What is the duration What is the wingspan How big is our galaxy
of an active volcano get? of the trip from...? of a condor? in diameter?
</bodyText>
<figure confidence="0.99347284">
dew
point
temperature
body
temperature
absolute longness
zero
longevity
duration,
length
altitude
distance,
length
light time
wingspan,
wingspread
umf
irc
c
erence
girth
perimeter,
size
largeness
bigness
</figure>
<bodyText confidence="0.989837">
Net subherarchies. Figure 5 illustrates a fragment
of the ANSWER TAXONOMY comprising several
WordNet subhierarchies.
</bodyText>
<sectionHeader confidence="0.838087" genericHeader="method">
4 Answer Recognition and Extraction
</sectionHeader>
<bodyText confidence="0.986451571428572">
In this section we show how, given a question and
its dependency structure, we can recognize its an-
swer type and consequently extract the exact an-
swer. Here we describe four representative cases.
Case 1: The CATEGORY of the answer type is
DEFINITION when the question can be matched
by one of the following patterns:
</bodyText>
<footnote confidence="0.950725666666667">
(Q-P1):What isare phrase to define ?
(Q-P2):What is the definition of phrase to define ?
(Q-P3):Who iswasarewere person name(s) ?
</footnote>
<bodyText confidence="0.934062711111111">
The format of the DEFINITION answers is sim-
ilarly dependent on a set of patterns, determined
as the head of the Answer phrase :
(A-P1): phrase to define isare Answer phrase
(A-P2): phrase to define , athean Answer phrase
(A-P3): phrase to define –✟ Answer phrase
Case 2: The dependency structure of the ques-
tion indicates that a special instance of a concept
is sought. The cues are given either by the pres-
ence of words kind, type, name or by the ques-
tion stems what or which connected to the object
of a verb. Table 3 lists a set of such questions
and their corresponding answers. In this case the
answer type is given by the subhierarchy defined
by the node from the dependency structure whose
adjunct is either kind, type, name or the question
stem. In this situation the CATEGORY does not
belong to the top of the ANSWER TAXONOMY,
but it is rather dynamically created by the inter-
pretation of the dependency graph.
For example, the dynamic CATEGORY bridge,
generated for Q204 from Table 3, contains 14
member instances, including viaduct, rope bridge
and suspension bridge. Similarly, question Q581
generates a dynamic CATEGORY flower, with 470
member instances, comprising orchid, petunia
and sunflower. For dynamic categories all mem-
ber instances are searched in the retrieved pas-
sages during answer extraction to detect candidate
answers.
Case 3: In all other cases, the concept related
to the question stem in the question dependency
graph is searched through the ANSWER TAXON-
OMY, returning the answer type as the top of it
hierarchy. Figure 5 illustrates several questions
and their answer type CATEGORY.
Case 4: Whenever the semantic dependencies of
several correct answers can be mapped one into
another, we change the CATEGORY of the answer
type into TEMPLATE. The slots of the actual tem-
plate are determined by a three step procedure,
that we illustrate with a walk-through example
corresponding to the question What management
successions occurred at IBM in 1999?:
Step 1: For each pair of extracted candidate
</bodyText>
<note confidence="0.9393745">
Q204: What type of bridge is the Golden Gate Bridge?
Answer: the Seto Ohashi Bridge, consisting of six suspension bridges in the style of Golden Gate Bridge.
Q267: What is the name for clouds that produce rain?
Answer: Acid rain in Cheju Island and the Taean peninsula is carried by rain clouds from China.
Q503: What kind of sports team is the Buffalo Sabres?
Answer: Alexander Mogilny hopes to continue his hockey career with the NHL’s Buffalo Sabres.
Q581: Whatflower did Vincent Van Gogh paint?
Answer: In March 1987, van Gogh’s “Sunflowers” sold for $39.9 million at Christie’s in London
</note>
<tableCaption confidence="0.999874">
Table 3: TREC test questions and their answers. The exact answer is emphasized.
</tableCaption>
<table confidence="0.485287">
Position Person1 Person2 Organization
</table>
<figureCaption confidence="0.998065">
Figure 6: Dependencies that generate templates.
</figureCaption>
<bodyText confidence="0.999496736842106">
answers unify the dependency graphs and find
common generalizations whenever possible. Fig-
ure 6(a) illustrates some of the mappings.
Step 2: Identify across mappings the common
categories and the trigger-words that were used
as keywords. In Figure 6(a) the trigger words are
boldfaced.
Step 3: Collect all common categories in a tem-
plate and use their names as slots. Figure 6(b)
illustrates the resulting template.
This procedure is a reverse-engineering of the
mechanisms used generally in Information Ex-
traction (IE), where given a template, linguistic
patterns are acquired to identify the text frag-
ments having relevant information. In the case
of answer mining, the relevant text passages are
known. The dependency graphs help finding the
linguistic rules and are generalized in a template.
To be able to generate the template we also
need to have a way of extracting the text where
the answer dependencies are detected. For this
purpose we have designed a method that em-
ploys a simple machine learning mechanism: the
perceptron. For each text passage retrieved by
the keyword-based query we define the following
seven features:
the number of question words matched in
the same phrase as the answer type CATEGORY;
the number of question words matched in
the same sentence as the answer type CATEGORY;
: a flag set to 1 if the answer type CATE-
GORY is followed by a punctuation sign, and set
to 0 otherwise;
: the number of question words
matches separated from the answer type CATE-
GORY by at most three words and one comma;
: the number of question words occur-
ring in the same order in the answer text as in the
question;
: the average distance from the an-
swer type CATEGORY to any of the question word
matches;
: the number of question words
matched in the answer text.
To train the perceptron we annotated the correct
answers of 200 of the TREC test questions. Given
a pair of answers, in which one of the answers is
correct, we compute a relative comparison score
using the formula:
The perceptron learns the seven weights as well
as the value of the threshold used for future tests
on the remaining 693 TREC questions. Whenever
the relative score is larger than the threshold, a
passage is extracted as a candidate answer. In our
experiments, the performance of the perceptron
surpassed the performance of decision trees for
answer extraction.
</bodyText>
<table confidence="0.695532">
Person
Organization
resign/leave Position
Person1 replace/succeed Person2 Organization Position
Organization nominate/assign Person Position
</table>
<sectionHeader confidence="0.941909" genericHeader="evaluation">
5 Evaluations and Conclusion
</sectionHeader>
<bodyText confidence="0.983961578947368">
To evaluate our answer type model we used 693
TREC test questions on which we did not train the
perceptron. Table 4 lists the breakdown of the an-
swer type CATEGORIES recognized by our model
as well as the coverage and precision of the recog-
nition. Currently our ANSWER TAXONOMY en-
codes 8707 concepts from 129 WordNet hierar-
chies, covering only 81% of the expected answer
types. This shows that we have to continue en-
coding more top concepts in the taxonomy and
link them to more WordNet concepts.
The recognition mechanism had better preci-
sion than coverage in our experiments. Moreover
a relationship between the coverage of answer
type recognition and the overall performance of
answer mining, as illustrated in Table 4. Some of
the test questions are listed in Tables 1 and 3. The
experiments were conducted by using 736,794
on-line documents from Los Angeles Times, For-
</bodyText>
<reference confidence="0.801927">
eign Broadcast Information Service, Financial
Times AP Newswire, Wall Street Journal and San
Jose Mercury News.
</reference>
<table confidence="0.999422923076923">
CATEGORY (# Questions) Precision Coverage
DEFINITION (64) 91% 84%
Top ANSWER 79% 74%
TAXONOMY (439)
Dynamic answer 86% 79%
category (17)
TEMPLATE (14) 93% 65%
# ANSWER Answer Type Q/A Precision
Taxonomy Coverage
Tops
8 44% 42%
22 56% 55%
33 83% 78%
</table>
<tableCaption confidence="0.999827">
Table 4: Evaluation results.
</tableCaption>
<bodyText confidence="0.999949428571428">
The experiments show that open-domain natu-
ral language questions of varied degrees of com-
plexity can be answered consistently from vast
amounts of on-line texts. One of the appli-
cations of a unified model of answer mining
is the development of intelligent conversational
agents (Harabagiu et al., 2001).
</bodyText>
<sectionHeader confidence="0.992052" genericHeader="conclusions">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.6665254">
This research was supported in part by the
Advanced Research and Development Activity
(ARDA) grant 2001*H238400*000 and by the
National Science Foundation CAREER grant
CCR-9983600.
</bodyText>
<sectionHeader confidence="0.920182" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9999515">
S. Abney, M. Collins, and A. Singhal. 2000. Answer
extraction. In Proceedings of ANLP-2000, pages
296–301, Seattle, Washington.
V.K. Chaudri, M.E. Stickel, J.F. Thomere, and R.J.
Waldinger. 2000. Reusing prior knowledge: Prob-
lems and solutions. In Proceedings of AAAI-2000,
Austin, Texas.
M. Collins. 1996. A new statistical parser based on
bigram lexical dependencies. In Proceedings of the
ACL-96, pages 184–191, Copenhagen, Denmark.
S. Harabagiu, M. Pas¸ca, and S. Maiorano. 2000. Ex-
periments with open-domain textual question an-
swering. In Proceedings of COLING-2000, Saar-
brucken, Germany.
S. Harabagiu, M. Pasca, and F. Lacatusu. 2001. Dia-
logue management for interactive question answer-
ing. In Proceedings ofFLAIRS-2001. To appear.
G. Miller. 1995. WordNet: a lexical database. Com-
munications of the ACM, 38(11):39–41.
D. Moldovan, S. Harabagiu, M. Pas¸ca, R. Mihalcea,
R. Girju, R. Goodrum, and V. Rus. 2000. The
structure and performance of an open-domain ques-
tion answering system. In Proceedings of ACL-
2000, Hong Kong.
D. Radev and K. McKeown. 1998. Generating natu-
ral language summaries from multiple on-line re-
sources. Computational Linguistics, 24(3):469–
500.
R. Srihari and W. Li. 2000. A question answering sys-
tem supported by information extraction. In Pro-
ceedings ofANLP-2000, Seattle, Washington.
E.M. Voorhees and D.M. Tice. 2000. Building a
question-answering test collection. In Proceedings
of the 23rd International Conference on Research
and Development in Information Retrieval (SIGIR-
2000), Athens, Greece.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.272288">
<title confidence="0.994687">Answer Mining from On-Line Documents</title>
<author confidence="0.749606">M</author>
<affiliation confidence="0.919335">Department of Computer Science and Southern Methodist</affiliation>
<address confidence="0.392249">Dallas, TX</address>
<email confidence="0.977395">mars,sanda@engr.smu.edu</email>
<abstract confidence="0.99713915">Mining the answer of a natural language open-domain question in a large collection of on-line documents is made by the recognition of the exanswer type relevant text passages. If the technology of retrieving texts where the answer might be found is well developed, few studies have been devoted to the recognition of the answer type. This paper presents a unified model of answer types for open-domain Question/Answering that enables the discovery of exact answers. The evaluation of the model, performed on real-world questions, considers both the correctness and the coverage of the answer types as well as their contribution to answer precision.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>eign Broadcast</author>
</authors>
<title>Information Service,</title>
<journal>Financial Times AP Newswire, Wall Street Journal and San Jose Mercury News.</journal>
<marker>Broadcast, </marker>
<rawString>eign Broadcast Information Service, Financial Times AP Newswire, Wall Street Journal and San Jose Mercury News.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Abney</author>
<author>M Collins</author>
<author>A Singhal</author>
</authors>
<title>Answer extraction.</title>
<date>2000</date>
<booktitle>In Proceedings of ANLP-2000,</booktitle>
<pages>296--301</pages>
<location>Seattle, Washington.</location>
<contexts>
<context position="2263" citStr="Abney et al., 2000" startWordPosition="356" endWordPosition="359">r 250 contiguous bytes (long answers). These two requirements intentionally simplify the answer mining task, since the identification of the exact answer is left to the user. However, given that the expected information is recognized by inspecting text snippets of relatively small size, the TREC Q/A task took a step closer to information retrieval rather than document retrieval. Moreover, the techniques developed to extract text snippets where the answers might lie paved the way to a unified model for answer mining. To find the answer to a question several steps must be taken, as reported in (Abney et al., 2000) (Moldovan et al., 2000) (Srihari and Li, 2000): First, the question semantics needs to be captured. This translates into identifying (i) the expected answer type and (ii) the question keywords that can be used to retrieve text passages where the answer may be found. Secondly, the index of the document collection must be used to identify the text passages of interest. The retrieval method either employs special operators or simply modifies boolean or vector retrieval. Since the expected answer type is known at the time workshops organized by the National Institute of Standards and Technology (</context>
</contexts>
<marker>Abney, Collins, Singhal, 2000</marker>
<rawString>S. Abney, M. Collins, and A. Singhal. 2000. Answer extraction. In Proceedings of ANLP-2000, pages 296–301, Seattle, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V K Chaudri</author>
<author>M E Stickel</author>
<author>J F Thomere</author>
<author>R J Waldinger</author>
</authors>
<title>Reusing prior knowledge: Problems and solutions.</title>
<date>2000</date>
<booktitle>In Proceedings of AAAI-2000,</booktitle>
<location>Austin, Texas.</location>
<contexts>
<context position="11752" citStr="Chaudri et al., 2000" startWordPosition="1911" endWordPosition="1914">unifications are added to the list of keywords, making possible the retrieval of new, unseen text passages, as illustrated in Figure 2. The unification of dependency structures allows erroneous answers when the resulting mapping is a sparse graph. To justify the correctness of the answer an abductive proof backchaining from the answer to the question must be produced. Such abductive mechanisms are detailed in (Harabagiu et al., 2000). Moreover, the proof relies on lexico-semantic knowledge available from WordNet as well as rapidly formated knowledge bases generated by mechanisms described in (Chaudri et al., 2000). The justification process brings forward semantic alternations that are added to the list of keywords, the feedback destination of all loops represented in Figure 2. Mining the exact answer does not always end after extracting the answer type from a correct text snippet because often they result only in partial answers that need to be fused together. The fusion mechanisms are dictated by the answer type. Question Complexity Open-Domain natural language questions can also be of different complexity levels. Generally, the test questions used in the TREC evaluations were qualified as fact-based</context>
</contexts>
<marker>Chaudri, Stickel, Thomere, Waldinger, 2000</marker>
<rawString>V.K. Chaudri, M.E. Stickel, J.F. Thomere, and R.J. Waldinger. 2000. Reusing prior knowledge: Problems and solutions. In Proceedings of AAAI-2000, Austin, Texas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>A new statistical parser based on bigram lexical dependencies.</title>
<date>1996</date>
<booktitle>In Proceedings of the ACL-96,</booktitle>
<pages>184--191</pages>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="6972" citStr="Collins, 1996" startWordPosition="1139" endWordPosition="1140">racting the exact answer. We also need to know the dependencies between the answer type and the other concepts from the question or the answer. For example, if the answer type of the TREC question QT: How many dogs pull a sled in the Iditarod? is known to be a number, we also need to be aware that this number must quantify the dogs harnessed to a sled in the Iditarod games and not the number of participants in the games. Capturing question or answer dependencies can be cast as a straightforward process of mapping syntactic trees to sets of binary headmodifier relationships, as first noted in (Collins, 1996). Given a parse tree, the head-child of each syntactic constituent can be identified based on a simple set of rules used to train syntactic parsers, cf. (Collins, 1996). Dependency relations are established between each leaf corresponding to the head child and the leaves of its constituent sibQuestion ET1: What do most tourists visit in Reims? WP Parse: VBP SBARQ NNS visit IN NP WHNP do RBS SQ VP PP NNP What most VB in Reims NP tourists Question Dependecies (ET1): What most tourists visit Reims Figure 1: Example of TREC test question lings that are not stop words, as illustrated by the mapping</context>
</contexts>
<marker>Collins, 1996</marker>
<rawString>M. Collins. 1996. A new statistical parser based on bigram lexical dependencies. In Proceedings of the ACL-96, pages 184–191, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
<author>M Pas¸ca</author>
<author>S Maiorano</author>
</authors>
<title>Experiments with open-domain textual question answering.</title>
<date>2000</date>
<booktitle>In Proceedings of COLING-2000,</booktitle>
<location>Saarbrucken, Germany.</location>
<marker>Harabagiu, Pas¸ca, Maiorano, 2000</marker>
<rawString>S. Harabagiu, M. Pas¸ca, and S. Maiorano. 2000. Experiments with open-domain textual question answering. In Proceedings of COLING-2000, Saarbrucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
<author>M Pasca</author>
<author>F Lacatusu</author>
</authors>
<title>Dialogue management for interactive question answering.</title>
<date>2001</date>
<booktitle>In Proceedings ofFLAIRS-2001.</booktitle>
<note>To appear.</note>
<marker>Harabagiu, Pasca, Lacatusu, 2001</marker>
<rawString>S. Harabagiu, M. Pasca, and F. Lacatusu. 2001. Dialogue management for interactive question answering. In Proceedings ofFLAIRS-2001. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
</authors>
<title>WordNet: a lexical database.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="8690" citStr="Miller, 1995" startWordPosition="1423" endWordPosition="1424"> of the verb visit, which is a place of attraction or entertainment, defined by the semantic category LANDMARK. The answer type replaces the question stem, generating the following dependency graph, that can be later unified with the answer dependency graph: However syntactic dependencies vary across question reformulations or equivalent answers made possible by the productive nature of natural language. For example, the dependency structure of ET2, a reformulation of question ET1 differs from the dependency structure of ET1: Due to the fact that verbs see and visit are synonyms (cf. WordNet (Miller, 1995)) and pronoun I can be read a possible visitor, the dependency Question ET2: What could I see in Reims? structures of ET1 and ET2 can be mapped one into another. The mapping is produced by unifying the two structures when lexical and semantic alternations are allowed. Possible lexical alternations are synonyms or morphological alternations. Semantic alternations consist of hypernyms, entailments or paraphrases. The unifying mapping of ET1 and ET2 shows that the two questions are equivalent only when I refers to a visitor; other readings of ET2 being possible when the referent is an investigato</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>G. Miller. 1995. WordNet: a lexical database. Communications of the ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Moldovan</author>
<author>S Harabagiu</author>
<author>M Pas¸ca</author>
<author>R Mihalcea</author>
<author>R Girju</author>
<author>R Goodrum</author>
<author>V Rus</author>
</authors>
<title>The structure and performance of an open-domain question answering system.</title>
<date>2000</date>
<booktitle>In Proceedings of ACL2000,</booktitle>
<location>Hong Kong.</location>
<marker>Moldovan, Harabagiu, Pas¸ca, Mihalcea, Girju, Goodrum, Rus, 2000</marker>
<rawString>D. Moldovan, S. Harabagiu, M. Pas¸ca, R. Mihalcea, R. Girju, R. Goodrum, and V. Rus. 2000. The structure and performance of an open-domain question answering system. In Proceedings of ACL2000, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Radev</author>
<author>K McKeown</author>
</authors>
<title>Generating natural language summaries from multiple on-line resources.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>3</issue>
<pages>500</pages>
<contexts>
<context position="13550" citStr="Radev and McKeown, 1998" startWordPosition="2207" endWordPosition="2210">. Typically, IE templates model queries regarding who did an event of interest, what was produced by that event, when and where and eventually why. The event of interest is a complex event, like terrorism in Latin America, joint ventures or management successions. An example of template-modeled question is: What management successions occurred at IBM in 1999? In addition, questions may also ask about developments of events or trends that are usually answered by a text summary. Since data producing these summaries can be sourced in different documents, summary fusion techniques as proposed in (Radev and McKeown, 1998) can be employed. Template-based questions and summaryasking inquiries cover most of the classes of question complexity proposed in (Moldovan et al., 2000). Although the topic of natural language open-domain question complexity needs further study, we consider herein the following classes of questions: Class 1: Questions inquiring about entities, events, entity attributes (including number), event themes, event manners, event conditions and event consequences. Class 2: Questions modeled by templates, including questions that focus only on one of the template slots (e.g. “What managers were pro</context>
</contexts>
<marker>Radev, McKeown, 1998</marker>
<rawString>D. Radev and K. McKeown. 1998. Generating natural language summaries from multiple on-line resources. Computational Linguistics, 24(3):469– 500.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Srihari</author>
<author>W Li</author>
</authors>
<title>A question answering system supported by information extraction.</title>
<date>2000</date>
<booktitle>In Proceedings ofANLP-2000,</booktitle>
<location>Seattle, Washington.</location>
<contexts>
<context position="2310" citStr="Srihari and Li, 2000" startWordPosition="364" endWordPosition="367">two requirements intentionally simplify the answer mining task, since the identification of the exact answer is left to the user. However, given that the expected information is recognized by inspecting text snippets of relatively small size, the TREC Q/A task took a step closer to information retrieval rather than document retrieval. Moreover, the techniques developed to extract text snippets where the answers might lie paved the way to a unified model for answer mining. To find the answer to a question several steps must be taken, as reported in (Abney et al., 2000) (Moldovan et al., 2000) (Srihari and Li, 2000): First, the question semantics needs to be captured. This translates into identifying (i) the expected answer type and (ii) the question keywords that can be used to retrieve text passages where the answer may be found. Secondly, the index of the document collection must be used to identify the text passages of interest. The retrieval method either employs special operators or simply modifies boolean or vector retrieval. Since the expected answer type is known at the time workshops organized by the National Institute of Standards and Technology (NIST), designed to advance the state-ofthe-art </context>
</contexts>
<marker>Srihari, Li, 2000</marker>
<rawString>R. Srihari and W. Li. 2000. A question answering system supported by information extraction. In Proceedings ofANLP-2000, Seattle, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Voorhees</author>
<author>D M Tice</author>
</authors>
<title>Building a question-answering test collection.</title>
<date>2000</date>
<booktitle>In Proceedings of the 23rd International Conference on Research and Development in Information Retrieval (SIGIR2000),</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="12393" citStr="Voorhees and Tice, 2000" startWordPosition="2012" endWordPosition="2015">on process brings forward semantic alternations that are added to the list of keywords, the feedback destination of all loops represented in Figure 2. Mining the exact answer does not always end after extracting the answer type from a correct text snippet because often they result only in partial answers that need to be fused together. The fusion mechanisms are dictated by the answer type. Question Complexity Open-Domain natural language questions can also be of different complexity levels. Generally, the test questions used in the TREC evaluations were qualified as fact-based questions (cf. (Voorhees and Tice, 2000)) as they mainly were short inquiries about attributes or definitions of some entity or event. Table 1 lists a sample of TREC test questions. The TREC test set did not include any question Where is Romania located? Europe Who wrote ”Dubliners”? James Joyce What is the wingspan of a condor? 9 feet What is the population of Japan? 120 million What king signed the Magna Carta? King John Name a flying mammal. bat Table 1: TREC test questions and their exact answers (boldfaced) that can be modeled as Information Extraction (IE) task. Typically, IE templates model queries regarding who did an event </context>
</contexts>
<marker>Voorhees, Tice, 2000</marker>
<rawString>E.M. Voorhees and D.M. Tice. 2000. Building a question-answering test collection. In Proceedings of the 23rd International Conference on Research and Development in Information Retrieval (SIGIR2000), Athens, Greece.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>