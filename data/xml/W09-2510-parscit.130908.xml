<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.062687">
<title confidence="0.973184">
Presupposed Content and Entailments in Natural Language Inference
</title>
<author confidence="0.998922">
David Clausen Christopher D. Manning
</author>
<affiliation confidence="0.9992075">
Department of Linguistics Departments of Computer Science and Linguistics
Stanford University Stanford University
</affiliation>
<email confidence="0.997699">
clausend@stanford.edu manning@cs.stanford.edu
</email>
<sectionHeader confidence="0.993851" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999697961538461">
Previous work has presented an accurate
natural logic model for natural language in-
ference. Other work has demonstrated the ef-
fectiveness of computing presuppositions for
solving natural language inference problems.
We extend this work to create a system for
correctly computing lexical presuppositions
and their interactions within the natural logic
framework. The combination allows our sys-
tem to properly handle presupposition projec-
tion from the lexical to the sentential level
while taking advantage of the accuracy and
coverage of the natural logic system. To
solve an inference problem, our system com-
putes a sequence of edits from premise to hy-
pothesis. For each edit the system computes
an entailment relation and a presupposition
entailment relation. The relations are then
separately composed according to a syntactic
tree and the semantic properties of its nodes.
Presuppositions are projected based on the
properties of their syntactic and semantic en-
vironment. The edits are then composed and
the resulting entailment relations are com-
bined with the presupposition relation to
yield an answer to the inference problem.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999975025641026">
Various approaches to the task of Natural Lan-
guage Inference (NLI) have demonstrated dis-
tinct areas of expertise. Systems based on full
semantic interpretation in first order logic are
highly accurate but lack broad coverage, requir-
ing large amounts of background knowledge to
do open-domain NLI (Bos and Markert, 2006).
Other systems based on statistical classifiers and
machine learning achieve broad coverage but
sacrifice accuracy by using shallow semantic
representations (MacCartney et al., 2006). Natu-
ral logic was developed as a compromise be-
tween these two extremes (MacCartney and
Manning, 2009). It makes use of rich semantic
features while using syntactic representations
closely related to the natural language surface
strings to achieve broad coverage. Other work
has demonstrated the effectiveness of lexically
triggered inferences and presuppositions to the
task of natural language entailment and contra-
diction detection (Nairn et al, 2006; Hickl et al.,
2006).
The natural logic model attempted to inte-
grate these insights but recognized the difficulty
of treating presuppositions within their current
framework. Natural logic models negation,
monotonicity, lexical relations and implicatures
together as part of a sentence’s asserted content
allowing them to be treated through a single pro-
jection mechanism. Presuppositions notoriously
do not interact with these features although they
do interact with other semantic features requiring
a separate projection mechanism. We present a
model for presupposition detection and computa-
tion separate from asserted content. We extend
the natural logic model to compute lexically trig-
gered presuppositions covered by Nairn et al.
We then integrate this information to produce
improved coverage for the NLI task.
</bodyText>
<sectionHeader confidence="0.982246" genericHeader="introduction">
2 Presuppositions
</sectionHeader>
<bodyText confidence="0.999483272727273">
Presuppositions are propositions that are taken to
be true as a prerequisite for uttering a sentence.
The set of phenomena often grouped as presup-
positions are diverse, although they are fre-
quently systematically related to certain lexical
items in a sentence, in which case they are said
to be lexically triggered. Lexically triggered pre-
suppositions like (1c) from (1a) can be used by
an NLI system to expand the information avail-
able for solving a particular problem without full
semantic interpretation.
</bodyText>
<listItem confidence="0.9940824">
(1a) Bush knew that Gore won the election.
(1b) Bush did not know that Gore won the election.
(1c) Gore won the election.
(1d) If Gore won the election, Bush knew that Gore
won the election.
</listItem>
<bodyText confidence="0.57360475">
In (1a) the factive verb ‘knew’ triggers the lo-
cal factive presupposition that the sentential
complement ‘Gore won the election’ is true. (1a)
is a simple sentence so the sentence as a whole
</bodyText>
<page confidence="0.979547">
70
</page>
<note confidence="0.9848695">
Proceedings of the 2009 Workshop on Applied Textual Inference, ACL-IJCNLP 2009, pages 70–73,
Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999215210526316">
presupposes (1c) and we can make use of this
information for an NLI problem. A defining fea-
ture of presuppositions is their invariance under
negation so we have (1b) also entailing (1c). The
factive presupposition is said to project through
negation to become a presupposition of the entire
sentence. In other cases such as the consequent
of a conditional, the presupposition sometimes
does not project so sentence (1d) does not pre-
suppose (1c). Whether or not a lexically trig-
gered local presupposition becomes a presuppo-
sition of the entire sentence is known as the
problem of presupposition projection.
A complete treatment of the projection prob-
lem for all types of presupposition triggers is
outside the bounds of current NLI systems but
for most purposes we can compute presupposi-
tion projections based on a simple model first
outlined by Karttunen (1973). The model cate-
gorizes lexical items as either filters, plugs or
holes and uses these properties to determine how
local presuppositions project upwards through a
syntactic tree to become presuppositions of the
entire sentence. Lexical items are categorized
according to their effect on presuppositions they
dominate syntactically. The verb ‘realize’ is a
hole, and projects the presuppositions of its com-
plement unchanged so (2a) has a sentential pre-
supposition of (2c). The verb ‘pretend’ is a plug
and projects none of the presuppositions of its
complement so (2b) does not entail (2c). The
conditional is a filter and will sometimes project
the presuppositions of its antecedent and conse-
quent based on the entailment relation that holds
between the two. In the case of (1d) the antece-
dent entails the presupposition of the consequent
so the presupposition of the consequent is not
projected and it does not entail (1c).
</bodyText>
<listItem confidence="0.997656">
(2a) Rehnquist realized Bush knew that Gore won the
election
(2b) Rehnquist pretended Bush knew that Gore won
the election
(2c) Gore won the election
</listItem>
<bodyText confidence="0.9997475">
The verbs ‘realize’ and ‘pretend’ represent
two modest size classes of verbs and nouns
called factives and antifactives. The sentential
presuppositions for any given factive or antifac-
tive operator depend on its position in the sen-
tence’s syntactic tree and the number and type of
holes, plugs or filters that dominate it.
To implement this theory we model the local
factivity presuppositions triggered by various
sentential complement taking operators. We
then calculate the presuppositions of the entire
sentence by projecting the local presuppositions
according to Karttunen’s theory. For each opera-
tor our system traverses the sentence’s syntactic
tree from operator node to root calculating how
the local factivity presuppositions project
through the various holes, plugs and filters. The
result is a set of sentential level presuppositions
that can be used to determine inference relations
to other sentences.
</bodyText>
<sectionHeader confidence="0.981978" genericHeader="method">
3 Presupposition in NatLog
</sectionHeader>
<bodyText confidence="0.999973615384615">
The NatLog system of MacCartney and Manning
(2008; 2009) is a multi-stage NLI system that
decomposes the NLI task into 5 stages: (1) lin-
guistic analysis, (2) alignment, (3) lexical en-
tailment classification, (4) entailment projection,
and (5) entailment composition. The NatLog
architecture and the theory of presupposition pro-
jection outlined in section 2 reflect two parallel
methods for computing entailment relations be-
tween premise and hypothesis. We augment the
NatLog system at steps (1), (4) and (5) to com-
pute entailment relations and presuppositions in
parallel. The result is two separate entailment
relations which are combined to form an answer
to an NLI problem. At stage (1) we calculate the
lexically triggered factivity presuppositions for a
given sentence. At stage (4) we project the pre-
suppositions to determine the effective factivity
according to the theory outlined in section 2. In
stage (5) we compose the presuppositions across
the alignment between premise and hypothesis to
determine the presupposition entailment relation.
Finally we combine the presupposition entail-
ment relation with the entailment relation gener-
ated from the standard NatLog system to produce
a more informed inference.
</bodyText>
<subsectionHeader confidence="0.999218">
3.1 Lexical Factivity Presuppositions
</subsectionHeader>
<bodyText confidence="0.99998725">
Lexical factivity presuppositions are detected by
regular expressions over lemmatized lexical
items taken from the classes of factive and anti-
factive verbs and nouns. Figure 1 gives example
entries for two operators. A sentence is analyzed
for factivity operators by matching the regular
expressions to the tree structure and when one is
detected its terminal projection is marked as a
factive operator with the appropriate factivity.
The sentential complement of the operator is
marked as being in the scope of a factive opera-
tor of the appropriate type.
</bodyText>
<page confidence="0.983224">
71
</page>
<figure confidence="0.996057333333333">
Operator: know
Pattern: VP&lt;(/^VB/&lt;/^know$/)
Scope: /^SBAR|S$/
Factivity: FACT
Operator: pretend
Pattern:
VP&lt;(/^VB/&lt;/^pretend$/)
Scope: /^SBAR|S$/
Factivity: ANTI
</figure>
<figureCaption confidence="0.999941">
Figure 1: A factive and antifactive operator
</figureCaption>
<subsectionHeader confidence="0.993725">
3.2 Presupposition Projection
</subsectionHeader>
<bodyText confidence="0.999997863636364">
For any given constituent of a sentence we can
calculate its effective factivity presupposition by
determining the number and type of factivity op-
erators which dominate it. This is analogous to
computing the projected presuppositions for a
sentence but instead stores the information lo-
cally on the representation of the sentence. Let’s
compute the factivity of ‘Gore won the election’
in (2b). First we look for the immediately domi-
nating factivity operator and find that it is domi-
nated by the factive operator ‘know’ which as-
signs the local factivity FACT. We then traverse
up the tree and find the operator ‘pretend’, which
assigns the local factivity ANTI and dominates
the constituent and the operator ‘know’. We
then compose the two according to table 1. to
determine the effective factivity for the constitu-
ent is ANTI. If the sentence included more fac-
tive or antifactive operators we would continue
to calculate the effective factivity recursively
using the effective factivity output at each level
as the dominated input for the next level.
</bodyText>
<sectionHeader confidence="0.9661836" genericHeader="method">
Dominated Dominating Effective
ANTI ANTI ANTI
ANTI FACT ANTI
FACT ANTI ANTI
FACT FACT FACT
</sectionHeader>
<tableCaption confidence="0.891469">
Table 1: The effective factivity for any pair of
dominated and dominating factivity assignments.
</tableCaption>
<bodyText confidence="0.999192555555556">
The result tells us that the sentence in (2b) has an
antifactive presupposition that ‘Gore won the
election’. This is equivalent to the presupposi-
tion that ‘Gore did not win the election’. This
contradicts (2c) and we can conclude that (2b)
does not entail (2c). Detecting that the presup-
positions of a premise are incompatible with the
hypothesis is achieved in step (5) presupposition
composition.
</bodyText>
<subsectionHeader confidence="0.998678">
3.3 Presupposition Composition
</subsectionHeader>
<bodyText confidence="0.999954">
The NatLog model for NLI computes a sequence
of atomic edits from premise to hypothesis. The
entailment relation between each atomic edit is
computed and then composed across the se-
quence of edits to determine the entailment rela-
tion that holds between premise and hypothesis.
An atomic edit consists of an insertion (INS),
deletion (DELN) or substitution (SUB) opera-
tion. To compose the presuppositions calculated
in step (4) we compare the factivity presupposi-
tions before and after each atomic edit. In our
simplified model the only edits that can change
the factivity presuppositions are INS, DELN or
SUB of factive or antifactive operators. Using
table 2 we compute an atomic presupposition
entailment relation between each atomic edit
based on the edit type, local factivity and effec-
tive factivity. We then compose the atomic pre-
supposition entailment relations to produce the
presupposition entailment relation that holds be-
tween the premise and the conclusion. Finally
we combine the presupposition entailment rela-
tion with the entailment relation generated by the
standard NatLog architecture to yield the answer
to the NLI problem. Atomic presuppositions are
computed according to table 2.
</bodyText>
<table confidence="0.861592333333333">
Operator DEL INS
ANTI Alternation Alternation
FACT Forward Reverse
</table>
<tableCaption confidence="0.860481">
Table 2: Operator effective factivity and the re-
</tableCaption>
<bodyText confidence="0.967242642857143">
sulting atomic presupposition entailment relation
for DEL and INS edits.
The sequence of atomic edits converting the
premise (2b) to the hypothesis (2c) involves DEL
of one antifactive operator ‘pretend’ and one fac-
tive operator ‘know’. The first DEL of ‘pretend’
results in an atomic presupposition entailment
relation of Alternation. The second DEL of
‘know’ results in an atomic presupposition en-
tailment relation of Forward, together yielding a
presupposition entailment relation between the
premise and hypothesis of Alternation. This al-
lows our system to correctly predict (2c) is in-
compatible with and a contradiction of (2b).
</bodyText>
<page confidence="0.998217">
72
</page>
<sectionHeader confidence="0.996903" genericHeader="method">
4 Improvements
</sectionHeader>
<bodyText confidence="0.999902321428572">
Previous implementations of the NatLog system
were unable to handle NLI problems with (1b) as
the premise and (1c) as the hypothesis because
atomic presupposition entailment relations were
treated together with normal entailment relations.
The sequence of atomic edits from (1b) to (1c)
would involve the DEL of ‘know’ resulting in an
atomic entailment relation of Forward while
DEL of ‘not’ would result in an atomic entail-
ment relation of Negation together yielding Al-
ternation instead of Forward. Our augmented
system handles these types of inferences by sepa-
rating presupposition entailment relations from
normal entailment relations. In our augmented
system only the DEL edit of ‘know’ produces an
atomic presupposition entailment relation of
Forward. Since no other operators in (1b) pro-
duce atomic presupposition entailment relations
the resulting presupposition entailment relation
between (1b) and (1c) is the correct Forward en-
tailment.
Evaluating on a set of 3-way entailment NLI
test problems developed at PARC by the authors
of (Nairn et al. 2006) the Augmented NatLog
system achieved an accuracy of 60.53% com-
pared to the original NatLog system accuracy of
53.95% by correctly treating problems like (3)
where (3b) should be inferred form (3a).
</bodyText>
<listItem confidence="0.359220333333333">
(3a) Bush didn’t realize that Afghanistan is land-
locked.
(3b) Afghanistan is landlocked.
</listItem>
<bodyText confidence="0.9991555">
With further development we expect to extend
these results to other NLI test sets.
</bodyText>
<sectionHeader confidence="0.998953" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99998678125">
Our system extends the coverage of the NatLog
system to correctly handle factive presupposi-
tions. By computing entailments based on se-
mantic containment and exclusion separately
from those based on presupposition we avoid
unwanted interaction between the two dimen-
sions of meaning while leveraging the informa-
tion contained in presuppositions to improve NLI
performance. Although they are invariant under
negation, presuppositions do not uniformly pro-
ject. Projection is determined by a myriad of
complex factors which ultimately require logical
formalisms much more complex than predicate
logic to compute (Beaver 2001). Our treatment
does not currently take into account other types
of presuppositions including those based on as-
pectual relations, (Mary has/hasn’t stopped beat-
ing her boyfriend ⇒ Mary has been beating her
boyfriend), definitine descriptions, (The king
of France is/isn’t bald ⇒ There is a king of
France), or iteratives, (The boy cried/didn’t
cry wolf again ⇒ The boy cried wolf before).
We have, however, provided a framework that
can be extended to compute many types of
lexically triggered presupposition and their
projections. This work continues the theme of
MacCartney and Manning in asserting “open-
domain NLI is likely to require combining dis-
parate reasoners”. By augmenting NatLog
with a reasoner based on factive presupposi-
tions we take one step closer to the goal of
achieving open-domain NLI.
</bodyText>
<sectionHeader confidence="0.999435" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999885815789474">
Beaver, David I. 2001. Presupposition and assertion
in dynamic semantics. Stanford: CSLI.
Bos, Johan and Katja Markert. 2006. When logical
inference helps determining textual entailment (and
when it doesn’t). In Proceedings of the Second
PASCAL Challenges Workshop on Recognizing
Textual Entailment.
Hickl, Andrew, John Williams, Jeremy Bensley, Kirk
Roberts, Bryan Rink, and Ying Shi. 2006. Recog-
nizing textual entailment with LCC’s GROUND-
HOG system. In Proceedings of the Second PAS-
CAL Challenges Workshop on Recognizing Textual
Entailment.
Karttunen, Lauri. 1973. Presuppositions of compound
sentences. Linguistic Inquiry 4: 169-93.
MacCartney Bill, Trond Grenager, Marie-Catherine
de Marneffe, Daniel Cer, and Christopher D. Man-
ning. 2006. Learning to recognize features of valid
textual entailments. In Proceedings of the North
American Association of Computational Linguis-
tics (NAACL-06).
MacCartney, Bill and Christopher D. Manning. 2007.
Natural logic for textual inference. In ACL-07
Workshop on Textual Entailment and Paraphras-
ing, Prague.
MacCartney, Bill and Christopher D. Manning. 2008.
Modeling semantic containment and exclusion in
natural language inference. In Proceedings of Col-
ing-08.
MacCartney, Bill and Christopher D. Manning. 2009.
An extended model of natural logic. In The Eight
International Conference on Computational Se-
mantics (IWCS-8), Tilburg, Netherlands, January
2009
Nairn, Rowan, Cleo Condoravdi, and Lauri Kart-
tunen. 2006. Computing relative polarity for tex-
tual inference. In Proceedings of ICoS-5 (Inference
in Computational Semantics), Buxton, UK.
</reference>
<page confidence="0.999298">
73
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.972081">
<title confidence="0.999733">Presupposed Content and Entailments in Natural Language Inference</title>
<author confidence="0.999995">David Clausen Christopher D Manning</author>
<affiliation confidence="0.9997795">Department of Linguistics Departments of Computer Science and Linguistics Stanford University Stanford University</affiliation>
<email confidence="0.999362">clausend@stanford.edumanning@cs.stanford.edu</email>
<abstract confidence="0.998889407407407">Previous work has presented an accurate logic for natural language inference. Other work has demonstrated the effectiveness of computing presuppositions for solving natural language inference problems. We extend this work to create a system for correctly computing lexical presuppositions their interactions within the logic framework. The combination allows our system to properly handle presupposition projection from the lexical to the sentential level while taking advantage of the accuracy and of the logic To solve an inference problem, our system computes a sequence of edits from premise to hypothesis. For each edit the system computes an entailment relation and a presupposition entailment relation. The relations are then separately composed according to a syntactic tree and the semantic properties of its nodes. Presuppositions are projected based on the properties of their syntactic and semantic environment. The edits are then composed and the resulting entailment relations are combined with the presupposition relation to yield an answer to the inference problem.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David I Beaver</author>
</authors>
<title>Presupposition and assertion in dynamic semantics.</title>
<date>2001</date>
<publisher>CSLI.</publisher>
<location>Stanford:</location>
<contexts>
<context position="14887" citStr="Beaver 2001" startWordPosition="2276" endWordPosition="2277">em extends the coverage of the NatLog system to correctly handle factive presuppositions. By computing entailments based on semantic containment and exclusion separately from those based on presupposition we avoid unwanted interaction between the two dimensions of meaning while leveraging the information contained in presuppositions to improve NLI performance. Although they are invariant under negation, presuppositions do not uniformly project. Projection is determined by a myriad of complex factors which ultimately require logical formalisms much more complex than predicate logic to compute (Beaver 2001). Our treatment does not currently take into account other types of presuppositions including those based on aspectual relations, (Mary has/hasn’t stopped beating her boyfriend ⇒ Mary has been beating her boyfriend), definitine descriptions, (The king of France is/isn’t bald ⇒ There is a king of France), or iteratives, (The boy cried/didn’t cry wolf again ⇒ The boy cried wolf before). We have, however, provided a framework that can be extended to compute many types of lexically triggered presupposition and their projections. This work continues the theme of MacCartney and Manning in asserting </context>
</contexts>
<marker>Beaver, 2001</marker>
<rawString>Beaver, David I. 2001. Presupposition and assertion in dynamic semantics. Stanford: CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
<author>Katja Markert</author>
</authors>
<title>When logical inference helps determining textual entailment (and when it doesn’t).</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop on Recognizing Textual Entailment.</booktitle>
<contexts>
<context position="1739" citStr="Bos and Markert, 2006" startWordPosition="249" endWordPosition="252">antic properties of its nodes. Presuppositions are projected based on the properties of their syntactic and semantic environment. The edits are then composed and the resulting entailment relations are combined with the presupposition relation to yield an answer to the inference problem. 1 Introduction Various approaches to the task of Natural Language Inference (NLI) have demonstrated distinct areas of expertise. Systems based on full semantic interpretation in first order logic are highly accurate but lack broad coverage, requiring large amounts of background knowledge to do open-domain NLI (Bos and Markert, 2006). Other systems based on statistical classifiers and machine learning achieve broad coverage but sacrifice accuracy by using shallow semantic representations (MacCartney et al., 2006). Natural logic was developed as a compromise between these two extremes (MacCartney and Manning, 2009). It makes use of rich semantic features while using syntactic representations closely related to the natural language surface strings to achieve broad coverage. Other work has demonstrated the effectiveness of lexically triggered inferences and presuppositions to the task of natural language entailment and contr</context>
</contexts>
<marker>Bos, Markert, 2006</marker>
<rawString>Bos, Johan and Katja Markert. 2006. When logical inference helps determining textual entailment (and when it doesn’t). In Proceedings of the Second PASCAL Challenges Workshop on Recognizing Textual Entailment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Hickl</author>
<author>John Williams</author>
<author>Jeremy Bensley</author>
<author>Kirk Roberts</author>
<author>Bryan Rink</author>
<author>Ying Shi</author>
</authors>
<title>Recognizing textual entailment with LCC’s GROUNDHOG system.</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop on Recognizing Textual Entailment.</booktitle>
<contexts>
<context position="2397" citStr="Hickl et al., 2006" startWordPosition="344" endWordPosition="347">assifiers and machine learning achieve broad coverage but sacrifice accuracy by using shallow semantic representations (MacCartney et al., 2006). Natural logic was developed as a compromise between these two extremes (MacCartney and Manning, 2009). It makes use of rich semantic features while using syntactic representations closely related to the natural language surface strings to achieve broad coverage. Other work has demonstrated the effectiveness of lexically triggered inferences and presuppositions to the task of natural language entailment and contradiction detection (Nairn et al, 2006; Hickl et al., 2006). The natural logic model attempted to integrate these insights but recognized the difficulty of treating presuppositions within their current framework. Natural logic models negation, monotonicity, lexical relations and implicatures together as part of a sentence’s asserted content allowing them to be treated through a single projection mechanism. Presuppositions notoriously do not interact with these features although they do interact with other semantic features requiring a separate projection mechanism. We present a model for presupposition detection and computation separate from asserted </context>
</contexts>
<marker>Hickl, Williams, Bensley, Roberts, Rink, Shi, 2006</marker>
<rawString>Hickl, Andrew, John Williams, Jeremy Bensley, Kirk Roberts, Bryan Rink, and Ying Shi. 2006. Recognizing textual entailment with LCC’s GROUNDHOG system. In Proceedings of the Second PASCAL Challenges Workshop on Recognizing Textual Entailment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
</authors>
<title>Presuppositions of compound sentences.</title>
<date>1973</date>
<journal>Linguistic Inquiry</journal>
<volume>4</volume>
<pages>169--93</pages>
<contexts>
<context position="5113" citStr="Karttunen (1973)" startWordPosition="775" endWordPosition="776">tion to become a presupposition of the entire sentence. In other cases such as the consequent of a conditional, the presupposition sometimes does not project so sentence (1d) does not presuppose (1c). Whether or not a lexically triggered local presupposition becomes a presupposition of the entire sentence is known as the problem of presupposition projection. A complete treatment of the projection problem for all types of presupposition triggers is outside the bounds of current NLI systems but for most purposes we can compute presupposition projections based on a simple model first outlined by Karttunen (1973). The model categorizes lexical items as either filters, plugs or holes and uses these properties to determine how local presuppositions project upwards through a syntactic tree to become presuppositions of the entire sentence. Lexical items are categorized according to their effect on presuppositions they dominate syntactically. The verb ‘realize’ is a hole, and projects the presuppositions of its complement unchanged so (2a) has a sentential presupposition of (2c). The verb ‘pretend’ is a plug and projects none of the presuppositions of its complement so (2b) does not entail (2c). The condit</context>
</contexts>
<marker>Karttunen, 1973</marker>
<rawString>Karttunen, Lauri. 1973. Presuppositions of compound sentences. Linguistic Inquiry 4: 169-93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MacCartney Bill</author>
<author>Trond Grenager</author>
<author>Marie-Catherine de Marneffe</author>
<author>Daniel Cer</author>
<author>Christopher D Manning</author>
</authors>
<title>Learning to recognize features of valid textual entailments.</title>
<date>2006</date>
<booktitle>In Proceedings of the North American Association of Computational Linguistics (NAACL-06).</booktitle>
<marker>Bill, Grenager, de Marneffe, Cer, Manning, 2006</marker>
<rawString>MacCartney Bill, Trond Grenager, Marie-Catherine de Marneffe, Daniel Cer, and Christopher D. Manning. 2006. Learning to recognize features of valid textual entailments. In Proceedings of the North American Association of Computational Linguistics (NAACL-06).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Natural logic for textual inference.</title>
<date>2007</date>
<booktitle>In ACL-07 Workshop on Textual Entailment and Paraphrasing,</booktitle>
<location>Prague.</location>
<marker>MacCartney, Manning, 2007</marker>
<rawString>MacCartney, Bill and Christopher D. Manning. 2007. Natural logic for textual inference. In ACL-07 Workshop on Textual Entailment and Paraphrasing, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Modeling semantic containment and exclusion in natural language inference.</title>
<date>2008</date>
<booktitle>In Proceedings of Coling-08.</booktitle>
<contexts>
<context position="7180" citStr="MacCartney and Manning (2008" startWordPosition="1095" endWordPosition="1098">al factivity presuppositions triggered by various sentential complement taking operators. We then calculate the presuppositions of the entire sentence by projecting the local presuppositions according to Karttunen’s theory. For each operator our system traverses the sentence’s syntactic tree from operator node to root calculating how the local factivity presuppositions project through the various holes, plugs and filters. The result is a set of sentential level presuppositions that can be used to determine inference relations to other sentences. 3 Presupposition in NatLog The NatLog system of MacCartney and Manning (2008; 2009) is a multi-stage NLI system that decomposes the NLI task into 5 stages: (1) linguistic analysis, (2) alignment, (3) lexical entailment classification, (4) entailment projection, and (5) entailment composition. The NatLog architecture and the theory of presupposition projection outlined in section 2 reflect two parallel methods for computing entailment relations between premise and hypothesis. We augment the NatLog system at steps (1), (4) and (5) to compute entailment relations and presuppositions in parallel. The result is two separate entailment relations which are combined to form a</context>
</contexts>
<marker>MacCartney, Manning, 2008</marker>
<rawString>MacCartney, Bill and Christopher D. Manning. 2008. Modeling semantic containment and exclusion in natural language inference. In Proceedings of Coling-08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>An extended model of natural logic.</title>
<date>2009</date>
<booktitle>In The Eight International Conference on Computational Semantics (IWCS-8),</booktitle>
<location>Tilburg, Netherlands,</location>
<contexts>
<context position="2025" citStr="MacCartney and Manning, 2009" startWordPosition="290" endWordPosition="293">problem. 1 Introduction Various approaches to the task of Natural Language Inference (NLI) have demonstrated distinct areas of expertise. Systems based on full semantic interpretation in first order logic are highly accurate but lack broad coverage, requiring large amounts of background knowledge to do open-domain NLI (Bos and Markert, 2006). Other systems based on statistical classifiers and machine learning achieve broad coverage but sacrifice accuracy by using shallow semantic representations (MacCartney et al., 2006). Natural logic was developed as a compromise between these two extremes (MacCartney and Manning, 2009). It makes use of rich semantic features while using syntactic representations closely related to the natural language surface strings to achieve broad coverage. Other work has demonstrated the effectiveness of lexically triggered inferences and presuppositions to the task of natural language entailment and contradiction detection (Nairn et al, 2006; Hickl et al., 2006). The natural logic model attempted to integrate these insights but recognized the difficulty of treating presuppositions within their current framework. Natural logic models negation, monotonicity, lexical relations and implica</context>
</contexts>
<marker>MacCartney, Manning, 2009</marker>
<rawString>MacCartney, Bill and Christopher D. Manning. 2009. An extended model of natural logic. In The Eight International Conference on Computational Semantics (IWCS-8), Tilburg, Netherlands, January 2009</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rowan Nairn</author>
<author>Cleo Condoravdi</author>
<author>Lauri Karttunen</author>
</authors>
<title>Computing relative polarity for textual inference.</title>
<date>2006</date>
<booktitle>In Proceedings of ICoS-5 (Inference in Computational Semantics),</booktitle>
<location>Buxton, UK.</location>
<contexts>
<context position="2376" citStr="Nairn et al, 2006" startWordPosition="340" endWordPosition="343">d on statistical classifiers and machine learning achieve broad coverage but sacrifice accuracy by using shallow semantic representations (MacCartney et al., 2006). Natural logic was developed as a compromise between these two extremes (MacCartney and Manning, 2009). It makes use of rich semantic features while using syntactic representations closely related to the natural language surface strings to achieve broad coverage. Other work has demonstrated the effectiveness of lexically triggered inferences and presuppositions to the task of natural language entailment and contradiction detection (Nairn et al, 2006; Hickl et al., 2006). The natural logic model attempted to integrate these insights but recognized the difficulty of treating presuppositions within their current framework. Natural logic models negation, monotonicity, lexical relations and implicatures together as part of a sentence’s asserted content allowing them to be treated through a single projection mechanism. Presuppositions notoriously do not interact with these features although they do interact with other semantic features requiring a separate projection mechanism. We present a model for presupposition detection and computation se</context>
<context position="13883" citStr="Nairn et al. 2006" startWordPosition="2123" endWordPosition="2126">egation together yielding Alternation instead of Forward. Our augmented system handles these types of inferences by separating presupposition entailment relations from normal entailment relations. In our augmented system only the DEL edit of ‘know’ produces an atomic presupposition entailment relation of Forward. Since no other operators in (1b) produce atomic presupposition entailment relations the resulting presupposition entailment relation between (1b) and (1c) is the correct Forward entailment. Evaluating on a set of 3-way entailment NLI test problems developed at PARC by the authors of (Nairn et al. 2006) the Augmented NatLog system achieved an accuracy of 60.53% compared to the original NatLog system accuracy of 53.95% by correctly treating problems like (3) where (3b) should be inferred form (3a). (3a) Bush didn’t realize that Afghanistan is landlocked. (3b) Afghanistan is landlocked. With further development we expect to extend these results to other NLI test sets. 5 Conclusion Our system extends the coverage of the NatLog system to correctly handle factive presuppositions. By computing entailments based on semantic containment and exclusion separately from those based on presupposition we </context>
</contexts>
<marker>Nairn, Condoravdi, Karttunen, 2006</marker>
<rawString>Nairn, Rowan, Cleo Condoravdi, and Lauri Karttunen. 2006. Computing relative polarity for textual inference. In Proceedings of ICoS-5 (Inference in Computational Semantics), Buxton, UK.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>