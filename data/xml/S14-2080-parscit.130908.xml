<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009306">
<title confidence="0.98248">
Peking: Profiling Syntactic Tree Parsing Techniques for Semantic Graph
Parsing
</title>
<author confidence="0.958292">
Yantao Du, Fan Zhang, Weiwei Sun and Xiaojun Wan
</author>
<affiliation confidence="0.940729">
Institute of Computer Science and Technology, Peking University
</affiliation>
<note confidence="0.741594">
The MOE Key Laboratory of Computational Linguistics, Peking University
</note>
<email confidence="0.987276">
{duyantao,ws,wanxiaojun}@pku.edu.cn,zhangf717@gmail.com
</email>
<sectionHeader confidence="0.994571" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997905">
Using the SemEval-2014 Task 8 data, we
profile the syntactic tree parsing tech-
niques for semantic graph parsing. In par-
ticular, we implement different transition-
based and graph-based models, as well as
a parser ensembler, and evaluate their ef-
fectiveness for semantic dependency pars-
ing. Evaluation gauges how successful
data-driven dependency graph parsing can
be by applying existing techniques.
</bodyText>
<sectionHeader confidence="0.998427" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999939916666666">
Bi-lexical dependency representation is quite pow-
erful and popular to encode syntactic or semantic
information, and parsing techniques under the de-
pendency formalism have been well studied and
advanced in the last decade. The major focus is
limited to tree structures, which fortunately corre-
spond to many computationally good properties.
On the other hand, some leading linguistic theo-
ries argue that more general graphs are needed to
encode a wide variety of deep syntactic and se-
mantic phenomena, e.g. topicalization, relative
clauses, etc. However, algorithms for statistical
graph spanning have not been well explored be-
fore, and therefore it is not very clear how good
data-driven parsing techniques developed for tree
parsing can be for graph generating.
Following several well-established syntactic
theories, SemEval-2014 task 8 (Oepen et al.,
2014) proposes using graphs to represent seman-
tics. Considering that semantic dependency pars-
ing is a quite new topic and there is little previ-
ous work, we think it worth appropriately profil-
ing successful tree parsing techniques for graph
parsing. To this end, we build a hybrid system
</bodyText>
<footnote confidence="0.90758475">
This work is licenced under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
</footnote>
<bodyText confidence="0.998894">
that combines several important data-driven pars-
ing techniques and evaluate their impact with the
given data. In particular, we implement different
transition-based and graph-based models, as well
as a parser ensembler.
Our experiments highlight the following facts:
</bodyText>
<listItem confidence="0.99345675">
• Graph-based models are more effective than
transition-based models.
• Parser ensemble is very useful to boost the
parsing accuracy.
</listItem>
<sectionHeader confidence="0.955995" genericHeader="introduction">
2 Architecture
</sectionHeader>
<bodyText confidence="0.99982664">
We explore two kinds of basic models: One is
transition-based, and the other is tree approxima-
tion. Transition-based models are widely used for
dependency tree parsing, and they can be adapted
to graph parsing (Sagae and Tsujii, 2008; Titov
et al., 2009). Here we implement 5 transition-
based models for dependency graph parsing, each
of which is based on different transition system.
The motivation of developing tree approxima-
tion models is to apply existing graph-based tree
parsers to generate graphs. At the training time,
we convert the dependency graphs from the train-
ing data into dependency trees, and train second-
order arc-factored models1. At the test phase, we
parse sentences using this tree parser, and convert
the output trees back into semantic graphs. We
think tree approximation can appropriately evalu-
ate the possible effectiveness of graph-based mod-
els for graph spanning.
Finally, we integrate the outputs of different
models with a simple voter to boost the perfor-
mance. The motivation of using system combi-
nation and the choice of voting is mainly due to
the experiments presented by (Surdeanu and Man-
ning, 2010). When we obtain all the outputs of
</bodyText>
<footnote confidence="0.9990695">
1The mate parser (code.google.com/p/
mate-tools/) is used.
</footnote>
<page confidence="0.948663">
459
</page>
<note confidence="0.734458">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 459–464,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.999838">
these models, we combine them into a final result,
which is better than any of them. For combination,
we explore various systems for this task, since em-
pirically we know that variety leads to better per-
formance.
</bodyText>
<sectionHeader confidence="0.99884" genericHeader="method">
3 Transition-Based Models
</sectionHeader>
<bodyText confidence="0.99938455">
Transition-based models are usually used for de-
pendency tree parsing. For this task, we exploit it
for dependency graph parsing.
A transition system S contains a set C of con-
figurations and a set T of transitions. A configu-
ration c E C generally contains a stack σ of nodes,
a buffer β of nodes, and a set A of arcs. The ele-
ments in A is in the form (x, l, y), which denotes
a arc from x to y labeled l. A transition t E T can
be applied to a configuration and turn it into a new
one by adding new arcs or manipulating elements
of the stack or the buffer. A statistical transition-
based parser leverages a classifier to approximate
an oracle that is able to generate target graphs by
transforming the initial configuration cs(x) into a
terminal configuration ct E Ct.
An oracle of a given graph on sentence x is a
sequence of transitions which transform the initial
configuration to the terminal configuration the arc
set Act of which is the set of the arcs of the graph.
</bodyText>
<subsectionHeader confidence="0.999569">
3.1 Our Transition Systems
</subsectionHeader>
<bodyText confidence="0.999771769230769">
We implemented 5 different transition systems for
graph parsing. Here we describe two of them
in detail, one is the Titov system proposed in
(Titov et al., 2009), and the other is our Naive
system. The configurations of the two systems
each contain a stack σ, a buffer β, and a set A of
arcs, denoted by (σ, β, A). The initial configura-
tion of a sentence x = w1w2 · · · wn is cs(x) =
([0], [1, 2, · · · , n],{}), and the terminal configu-
ration set Ct is the set of all configurations with
empty buffer. These two transition systems are
shown in 1.
The transitions of the Titov system are:
</bodyText>
<listItem confidence="0.994863">
• LEFT-ARCl adds an arc from the front of the
buffer to the top of the stack, labeled l, into
A.
• RIGHT-ARCl adds an arc from the top of the
stack to the front of the buffer, labeled l, into
A.
• SHIFT removes the front of the buffer and
push it onto the stack;
• POP pops the top of the stack.
• SWAP swaps the top two elements of the
stack.
</listItem>
<bodyText confidence="0.999852">
This system uses a transition SWAP to change the
node order in the stack, thus allowing some cross-
ing arcs to be built.
The transitions of the Naive system are similar
to the Titov system’s, except that we can directly
manipulate all the nodes in the stack instead of just
the top two. In this case, the transition SWAP is not
needed.
The Titov system can cover a great proportion,
though not all, of graphs in this task. For more
discussion, see (Titov et al., 2009). The Naive
system, by comparison, covers all graphs. That
is to say, with this system, we can find an oracle
for any dependency graph on a sentence x. Other
transition systems we build are also designed for
dependency graph parsing, and they can cover de-
pendency graphs without self loop as well.
</bodyText>
<subsectionHeader confidence="0.999775">
3.2 Statistical Disambiguation
</subsectionHeader>
<bodyText confidence="0.999991277777778">
First of all, we derive oracle transition sequences
for every sentence, and train Passive-Aggressive
models (Crammer et al., 2006) to predict next tran-
sition given a configuration. When it comes to
parsing, we start with the initial configuration, pre-
dicting next transition and updating the configura-
tion with the transition iteratively. And finally we
will get a terminal configuration, we then stop and
output the arcs of the graph contained in the final
configuration.
We extracted rich feature for we utilize a set
of rich features for disambiguation, referencing to
Zhang and Nivre (2011). We examine the several
tops of the stack and the one or more fronts of the
buffer, and combine the lemmas and POS tags of
them in many ways as the features. Additionally,
we also derive features from partial parses such as
heads and dependents of these nodes.
</bodyText>
<subsectionHeader confidence="0.999886">
3.3 Sentence Reversal
</subsectionHeader>
<bodyText confidence="0.999950555555555">
Reversing the order the words of a given sentence
is a simple way to yield heterogeneous parsing
models, thus improving parsing accuracy of the
model ensemble (Sagae, 2007). In our experi-
ments, one transition system produces two mod-
els, one trained on the normal corpus, and the other
on the corpus of reversed sentences. Therefore we
can get 10 parse of a sentence based on 5 transition
systems.
</bodyText>
<page confidence="0.892728">
460
</page>
<equation confidence="0.961756363636364">
LEFT-ARCl (σ|i, j|β, A) ⇒ (σ|i, j|β, A ∪ {(j,l, i)})
RIGHT-ARCl (σ|i,j|β,A) ⇒ (σ|i,j|β,A ∪ {(i,l, j)})
SHIFT (σ, j|β, A) ⇒ (σ|j, β, A)
POP (σ|i, β, A) ⇒ (σ, β, A)
SWAP (σ|i|j, β, A) ⇒ (σ|j|i, β, A)
Titov System
LEFT-ARCkl (σ|ik |... |i2|i1,j|β,A) ⇒ (σ|ik |... |i2|i1,j|β,A ∪ {(j,l,ik)})
RIGHT-ARCkl (σ|ik |... |i2|i1,j|β,A) ⇒ (σ|ik |... |i2|i1,j|β,A ∪ {(ik,l,j)})
SHIFT (σ, j|β, A) ⇒ (σ|j, β, A)
POPk (σ|ik|ik−1 |... |i2|i1, β, A) ⇒ (σ|ik−1 |... |i2|i1, β, A)
Naive System
</equation>
<figureCaption confidence="0.998924">
Figure 1: Two of our transition systems.
</figureCaption>
<sectionHeader confidence="0.989636" genericHeader="method">
4 Tree Approximation Models
</sectionHeader>
<bodyText confidence="0.999820809523809">
Parsing based on graph spanning is quite challeng-
ing since computational properties of the seman-
tic graphs given by the shared task are less ex-
plored and thus still unknown. On the other hand,
finding the best higher-order spanning for general
graph is NP complete, and therefore it is not easy,
if not impossible, to implement arc-factored mod-
els with exact inference. In our work, we use a
practical idea to indirectly profile the graph-based
parsing techniques for dependency graph parsing.
Inspired by the PCFG approximation idea (Fowler
and Penn, 2010; Zhang and Krieger, 2011) for
deep parsing, we study tree approximation ap-
proaches for graph spanning.
This tree approximation technique can be ap-
plied to both transition-based and graph-based
parsers. However, since transition systems that
can directly handle build graphs have been devel-
oped, we only use this technique to evaluate the
possible effectiveness of graph-based models for
semantic parsing.
</bodyText>
<subsectionHeader confidence="0.96169">
4.1 Graph-to-Tree Transformation
</subsectionHeader>
<bodyText confidence="0.999596882352941">
In particular, we develop different methods to con-
vert a semantic graph into a tree, and use edge
labels to encode dependency relations as well as
structural information which helps to transform a
converted tree back to its original graph. By the
graph-to-tree transformation, we can train a tree
parser with a graph-annotated corpus, and utilize
the corresponding tree-to-graph transformation to
generate target graphs from the outputs of the tree
parser. Given that the tree-to-graph transformation
is quite trivial, we only describe the graph-to-tree
transformation approach.
We use graph traversal algorithms to convert a
directed graph to a directed tree. The transforma-
tion implies that we may lose, add or modify some
dependency relations in order to make the graph a
tree.
</bodyText>
<subsectionHeader confidence="0.984474">
4.2 Auxiliary Labels
</subsectionHeader>
<bodyText confidence="0.999240555555555">
In the transformed trees, we use auxiliary labels to
carry out information of the original graphs. To
encode multiple edges to one, we keep the origi-
nal label on the directed edge but may add other
edges’ information. On the other hand, through-
out most transformations, some edges must be re-
versed to make a tree, so we need a symbol to in-
dicate a edge on the tree is reversed during trans-
formation. The auxiliary labels are listed below:
</bodyText>
<listItem confidence="0.875761454545454">
• Label with following ∼R: The symbol ∼R
means this directed edge is reversed from the
original directed graph.
• Separator: Semicolon separates two encoded
original edges.
• [N] followed by label: The symbol [N] (N
is an integer) represents the head of the edge.
The dependent is the current one, but the head
is the dependent’s N-th ancestor where 1st
ancestor is its father and 2nd ancestor is its
father’s father.
</listItem>
<bodyText confidence="0.588397">
See Figure 2 for example.
</bodyText>
<subsectionHeader confidence="0.993952">
4.3 Traversal Strategies
</subsectionHeader>
<bodyText confidence="0.9998455">
Given directed graph (V, E), the task is to traverse
all edges on the graph and decide how to change
the labels or not contain the edge on the output.
We use 3 strategies for traversal. Here we use
x →g y to denote the edge on graph, and x →t y
the edge on tree.
</bodyText>
<page confidence="0.999033">
461
</page>
<figure confidence="0.967243">
adj ARG1
Mrs Ward was relieved
</figure>
<figureCaption confidence="0.999629">
Figure 2: One dependency graph and two possible
dependency trees after converting.
</figureCaption>
<bodyText confidence="0.99990612">
Depth-first-search We try graph traversal by
depth-first-search starting from the root on the di-
rected graph ignoring the direction of edges. Dur-
ing the traversal, we add edges to the directed tree
with (perhaps new) labels. We traverse the graph
recursively. Suppose the depth-first-search is run-
ning at the node x and the nodes set A which have
been searched. And suppose we find node y is
linked to x on the graph (x —*g y or y —*g x).
If y E/ A, we add the directed edge x —*t y to the
tree immediately. In the case of y —*g x, we add
—R to the edge label. If y E A, then y must be one
of the ancestors of x. In this case, we add this in-
formation to the label of the existing edge z —*t x.
Since the distance between two nodes x and y is
sufficient to indicate the node y, we use the dis-
tance to represent the head or dependent of this
directed edge and add the label and the distance to
the label of z —*t x. It is clear that the auxiliary
label [N] can be used for multiple edge encoding.
Under this strategy, all edges can be encoded on
the tree.
Breadth-first-search An alternative traversal
strategy is based on breadth-first-search starting
from the root. This search ignores the direction
of edge too. We regard the search tree as the de-
pendency tree. During the breadth-first-search, if
(x, l, y) exists but node y has been searched, we
just ignore the edge. Under this strategy, we may
lose some edges.
Iterative expanding This strategy is based on
depth-first-search but slightly different. The strat-
egy only searches through the forward edges on
the directed graph at first. When there is no for-
ward edge to expend, a traversed node linked to
some nodes that are not traversed must be the de-
pendent of them. Then we choose an edge and add
it (reversed) to the tree and continue to expand the
tree. Also, we ignore the edges that does not sat-
isfy the tree constraint. We call this strategy iter-
ative expanding. When we need to expand output
tree, we need to design a strategy to decide which
edge to be add. The measure to decide which node
should be expanded first is its possible location on
the tree and the number of nodes it can search dur-
ing depth-first-search. Intuitively, we want the re-
versed edges to be as few as possible. For this
purpose, this strategy is practical but not necessar-
ily the best. Like the Breadth-first-search strategy,
this strategy may also cause edge loss.
</bodyText>
<subsectionHeader confidence="0.998882">
4.4 Forest-to-Tree
</subsectionHeader>
<bodyText confidence="0.998592538461538">
After a primary searching process, if there is still
edge x —*g y that has not been searched yet, we
start a new search procedure from x or y. Even-
tually, we obtain a forest rather than a tree. To
combine disconnected trees in this forest to the fi-
nal dependency tree, we use edges with label None
to link them. Let the node set W be the set of roots
of the trees in the forest, which are not connected
to original graph root. The mission is to assign a
node v E/ W for each w E W. If we assign vi for
wi, we add the edge vi —* wi labeled by None to
the final dependency tree. We try 3 strategies in
this step:
</bodyText>
<listItem confidence="0.852798333333333">
• For each w E W we look for the first node
v E/ W on the left of w.
• For each w E W we look for the first node
v E/ W on the right of w.
• By defining the distance between two nodes
as how many words are there between the two
words, we can select the nearest node. If the
distances of more than one node are equal,
we choose v randomly.
</listItem>
<bodyText confidence="0.98809">
We also tried to link all of the nodes in W di-
rectly to the root, but it does not work well.
</bodyText>
<sectionHeader confidence="0.916137" genericHeader="method">
5 Model Ensemble
</sectionHeader>
<bodyText confidence="0.9986215">
We have 19 heterogeneous basic models (10
transition-based models, 9 tree approximation
models), and use a simple voter to combine their
outputs.
</bodyText>
<figure confidence="0.645923928571429">
noun ARG1
verb ARG1
verb ARG2
Mrs Ward was relieved
root
noun ARG1—R
root
verb ARG2
verb ARG1
adj ARG1;[2]verb ARG1
verb ARG2
Mrs Ward was relieved
root
noun ARG1—R
</figure>
<page confidence="0.800766">
462
</page>
<table confidence="0.99973125">
Algorithm DM PAS PCEDT
DFS 0 0 0
BFS 0.0117 0.0320 0.0328
FEF 0.0127 0.0380 0.0328
</table>
<tableCaption confidence="0.999941">
Table 1: Edge loss of transformation algorithms.
</tableCaption>
<bodyText confidence="0.999527692307692">
For each pair of words of a sentence, we count
the number of the models that give positive pre-
dictions. If the number is greater than a threshold,
we put this arc to the final graph, and label the arc
with the most common label of what the models
give.
Furthermore, we find that the performance of
the tree approximation models is better than the
transition based models, and therefore we take
weights of individual models too. Instead of just
counting, we sum the weights of the models that
give positive predictions. The tree approximation
models are assigned higher weights.
</bodyText>
<sectionHeader confidence="0.99972" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999943714285714">
There are 3 subtasks in the task, namely DM, PAS,
and PCEDT. For subtask DM, we finally obtained
19 models, just as stated in previous sections.
For subtask PAS and PCEDT, only 17 models are
trained due to the tight schedule.
The tree approximation algorithms may cause
some edge loss, and the statistics are shown in Ta-
ble 1. We can see that DFS does not cause edge
loss, but edge losses of other two algorithm are
not negligible. This may result in a lower recall
and higher precision, but we can tune the final re-
sults during model ensemble. Edge loss in subtask
DM is less than those in subtask PAS and PCEDT.
We present the performance of several repre-
sentative models in Table 2. We can see that the
tree approximation models performs better than
the transition-based models, which highlights the
effective of arc-factored models for semantic de-
pendency parsing. For model ensemble, besides
the accuracy of each single model, it is also im-
portant that the models to be ensembled are very
different. As shown in Table 2, the evaluation be-
tween some of our models indicates that our mod-
els do vary a lot.
Following the suggestion of the task organizers,
we use section 20 of the train data as the devel-
opment set. With the help of development set,
we tune the parameters of the models and ensem-
</bodyText>
<table confidence="0.999713">
Models DM PAS PCEDT
Titov 0.8468 0.8754 0.6978
Titov, 0.8535 0.8928 0.7063
Naive 0.8481 - -
DFS,,, 0.8692 0.9034 0.7370
DFSl 0.8692 0.9015 0.7246
BFS,,, 0.8686 0.8818 0.7247
Titov vs. Titov, 0.8607 0.8831 0.7613
Titov vs. Naive 0.9245 - -
Titov vs. DFS,,, 0.8590 0.8865 0.7650
DFS,,, vs. DFSl 0.9273 0.9579 0.8688
DFS,,, vs. BFS,,, 0.9226 0.9169 0.8367
</table>
<tableCaption confidence="0.905264">
Table 2: Evaluation between some of our models.
Labeled f-score on test set is shown. Titov, stands
</tableCaption>
<bodyText confidence="0.97073275">
for reversed Titov, DFS,,, for DFS+nearest, DFSl
for DFS+left, and BFS,,, for BFS+nearest. The up-
per part gives the performance, and the lower part
gives the agreement between systems.
</bodyText>
<table confidence="0.99885825">
Format LP LR LF LM
DM 0.9027 0.8854 0.8940 0.2982
PAS 0.9344 0.9069 0.9204 0.3872
PCEDT 0.7875 0.7396 0.7628 0.1120
</table>
<tableCaption confidence="0.999043">
Table 3: Final results of the ensembled model.
</tableCaption>
<bodyText confidence="0.9524425">
bling. We set the weight of each transition-based
model 1, and tree approximation model 2 in run
1, 3 in run 2. The threshold is set to a half of the
total weight. The final results given by the orga-
nizers are shown in Table 3. Compared to Table 2
demonstrates the effectiveness of parser ensemble.
</bodyText>
<sectionHeader confidence="0.998379" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999855727272727">
Data-driven dependency parsing techniques have
been greatly advanced during the parst decade.
Two dominant approaches, i.e. transition-based
and graph-based methods, have been well stud-
ied. In addition, parser ensemble has been shown
very effective to take advantages to combine the
strengthes of heterogeneous base parsers. In this
work, we propose different models to profile the
three techniques for semantic dependency pars-
ing. The experimental results suggest several di-
rections for future study.
</bodyText>
<sectionHeader confidence="0.9792" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.896728">
The work was supported by NSFC (61300064,
61170166 and 61331011) and National High-Tech
R&amp;D Program (2012AA011101).
</bodyText>
<page confidence="0.999619">
463
</page>
<sectionHeader confidence="0.993884" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999759234042553">
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. JOURNAL OF MA-
CHINE LEARNING RESEARCH, 7:551–585.
Timothy A. D. Fowler and Gerald Penn. 2010. Ac-
curate context-free parsing with combinatory cate-
gorial grammar. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, Uppsala, Sweden, July.
Stephan Oepen, Marco Kuhlmann, Yusuke Miyao,
Daniel Zeman, Dan Flickinger, Jan Hajiˇc, Angelina
Ivanova, and Yi Zhang. 2014. SemEval 2014 Task
8: Broad-coverage semantic dependency parsing. In
Proceedings of the 8th International Workshop on
Semantic Evaluation, Dublin, Ireland.
Kenji Sagae and Jun’ichi Tsujii. 2008. Shift-reduce
dependency DAG parsing. In Proceedings of the
22nd International Conference on Computational
Linguistics (Coling 2008), pages 753–760, Manch-
ester, UK, August. Coling 2008 Organizing Com-
mittee.
Kenji Sagae. 2007. Dependency parsing and domain
adaptation with lr models and parser ensembles. In
In Proceedings of the Eleventh Conference on Com-
putational Natural Language Learning.
Mihai Surdeanu and Christopher D. Manning. 2010.
Ensemble models for dependency parsing: Cheap
and good? In Human Language Technologies:
The 2010 Annual Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics, Los Angeles, California, June.
Ivan Titov, James Henderson, Paola Merlo, and
Gabriele Musillo. 2009. Online graph planarisa-
tion for synchronous parsing of semantic and syn-
tactic dependencies. In Proceedings of the 21st in-
ternational jont conference on Artifical intelligence,
IJCAI’09, pages 1562–1567, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
Yi Zhang and Hans-Ulrich Krieger. 2011. Large-scale
corpus-driven PCFG approximation of an hpsg. In
Proceedings of the 12th International Conference on
Parsing Technologies, Dublin, Ireland, October.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, Portland, Oregon, USA, June.
</reference>
<page confidence="0.999517">
464
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.938911">
<title confidence="0.996702">Peking: Profiling Syntactic Tree Parsing Techniques for Semantic Graph Parsing</title>
<author confidence="0.981879">Fan Zhang Du</author>
<author confidence="0.981879">Weiwei Sun</author>
<affiliation confidence="0.996237">Institute of Computer Science and Technology, Peking</affiliation>
<address confidence="0.969948">The MOE Key Laboratory of Computational Linguistics, Peking</address>
<abstract confidence="0.999231272727273">Using the SemEval-2014 Task 8 data, we profile the syntactic tree parsing techniques for semantic graph parsing. In particular, we implement different transitionbased and graph-based models, as well as a parser ensembler, and evaluate their effectiveness for semantic dependency parsing. Evaluation gauges how successful data-driven dependency graph parsing can be by applying existing techniques.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passive-aggressive algorithms.</title>
<date>2006</date>
<journal>JOURNAL OF MACHINE LEARNING RESEARCH,</journal>
<pages>7--551</pages>
<contexts>
<context position="6974" citStr="Crammer et al., 2006" startWordPosition="1150" endWordPosition="1153">ransition SWAP is not needed. The Titov system can cover a great proportion, though not all, of graphs in this task. For more discussion, see (Titov et al., 2009). The Naive system, by comparison, covers all graphs. That is to say, with this system, we can find an oracle for any dependency graph on a sentence x. Other transition systems we build are also designed for dependency graph parsing, and they can cover dependency graphs without self loop as well. 3.2 Statistical Disambiguation First of all, we derive oracle transition sequences for every sentence, and train Passive-Aggressive models (Crammer et al., 2006) to predict next transition given a configuration. When it comes to parsing, we start with the initial configuration, predicting next transition and updating the configuration with the transition iteratively. And finally we will get a terminal configuration, we then stop and output the arcs of the graph contained in the final configuration. We extracted rich feature for we utilize a set of rich features for disambiguation, referencing to Zhang and Nivre (2011). We examine the several tops of the stack and the one or more fronts of the buffer, and combine the lemmas and POS tags of them in many</context>
</contexts>
<marker>Crammer, Dekel, Keshet, Shalev-Shwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. 2006. Online passive-aggressive algorithms. JOURNAL OF MACHINE LEARNING RESEARCH, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy A D Fowler</author>
<author>Gerald Penn</author>
</authors>
<title>Accurate context-free parsing with combinatory categorial grammar.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="9221" citStr="Fowler and Penn, 2010" startWordPosition="1533" endWordPosition="1536">re 1: Two of our transition systems. 4 Tree Approximation Models Parsing based on graph spanning is quite challenging since computational properties of the semantic graphs given by the shared task are less explored and thus still unknown. On the other hand, finding the best higher-order spanning for general graph is NP complete, and therefore it is not easy, if not impossible, to implement arc-factored models with exact inference. In our work, we use a practical idea to indirectly profile the graph-based parsing techniques for dependency graph parsing. Inspired by the PCFG approximation idea (Fowler and Penn, 2010; Zhang and Krieger, 2011) for deep parsing, we study tree approximation approaches for graph spanning. This tree approximation technique can be applied to both transition-based and graph-based parsers. However, since transition systems that can directly handle build graphs have been developed, we only use this technique to evaluate the possible effectiveness of graph-based models for semantic parsing. 4.1 Graph-to-Tree Transformation In particular, we develop different methods to convert a semantic graph into a tree, and use edge labels to encode dependency relations as well as structural inf</context>
</contexts>
<marker>Fowler, Penn, 2010</marker>
<rawString>Timothy A. D. Fowler and Gerald Penn. 2010. Accurate context-free parsing with combinatory categorial grammar. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Oepen</author>
<author>Marco Kuhlmann</author>
<author>Yusuke Miyao</author>
<author>Daniel Zeman</author>
<author>Dan Flickinger</author>
<author>Jan Hajiˇc</author>
<author>Angelina Ivanova</author>
<author>Yi Zhang</author>
</authors>
<title>Task 8: Broad-coverage semantic dependency parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation,</booktitle>
<location>Dublin, Ireland.</location>
<note>SemEval</note>
<marker>Oepen, Kuhlmann, Miyao, Zeman, Flickinger, Hajiˇc, Ivanova, Zhang, 2014</marker>
<rawString>Stephan Oepen, Marco Kuhlmann, Yusuke Miyao, Daniel Zeman, Dan Flickinger, Jan Hajiˇc, Angelina Ivanova, and Yi Zhang. 2014. SemEval 2014 Task 8: Broad-coverage semantic dependency parsing. In Proceedings of the 8th International Workshop on Semantic Evaluation, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Shift-reduce dependency DAG parsing.</title>
<date>2008</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>753--760</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="2742" citStr="Sagae and Tsujii, 2008" startWordPosition="394" endWordPosition="397">tant data-driven parsing techniques and evaluate their impact with the given data. In particular, we implement different transition-based and graph-based models, as well as a parser ensembler. Our experiments highlight the following facts: • Graph-based models are more effective than transition-based models. • Parser ensemble is very useful to boost the parsing accuracy. 2 Architecture We explore two kinds of basic models: One is transition-based, and the other is tree approximation. Transition-based models are widely used for dependency tree parsing, and they can be adapted to graph parsing (Sagae and Tsujii, 2008; Titov et al., 2009). Here we implement 5 transitionbased models for dependency graph parsing, each of which is based on different transition system. The motivation of developing tree approximation models is to apply existing graph-based tree parsers to generate graphs. At the training time, we convert the dependency graphs from the training data into dependency trees, and train secondorder arc-factored models1. At the test phase, we parse sentences using this tree parser, and convert the output trees back into semantic graphs. We think tree approximation can appropriately evaluate the possib</context>
</contexts>
<marker>Sagae, Tsujii, 2008</marker>
<rawString>Kenji Sagae and Jun’ichi Tsujii. 2008. Shift-reduce dependency DAG parsing. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 753–760, Manchester, UK, August. Coling 2008 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
</authors>
<title>Dependency parsing and domain adaptation with lr models and parser ensembles. In</title>
<date>2007</date>
<booktitle>In Proceedings of the Eleventh Conference on Computational Natural Language Learning.</booktitle>
<contexts>
<context position="7894" citStr="Sagae, 2007" startWordPosition="1306" endWordPosition="1307">tained in the final configuration. We extracted rich feature for we utilize a set of rich features for disambiguation, referencing to Zhang and Nivre (2011). We examine the several tops of the stack and the one or more fronts of the buffer, and combine the lemmas and POS tags of them in many ways as the features. Additionally, we also derive features from partial parses such as heads and dependents of these nodes. 3.3 Sentence Reversal Reversing the order the words of a given sentence is a simple way to yield heterogeneous parsing models, thus improving parsing accuracy of the model ensemble (Sagae, 2007). In our experiments, one transition system produces two models, one trained on the normal corpus, and the other on the corpus of reversed sentences. Therefore we can get 10 parse of a sentence based on 5 transition systems. 460 LEFT-ARCl (σ|i, j|β, A) ⇒ (σ|i, j|β, A ∪ {(j,l, i)}) RIGHT-ARCl (σ|i,j|β,A) ⇒ (σ|i,j|β,A ∪ {(i,l, j)}) SHIFT (σ, j|β, A) ⇒ (σ|j, β, A) POP (σ|i, β, A) ⇒ (σ, β, A) SWAP (σ|i|j, β, A) ⇒ (σ|j|i, β, A) Titov System LEFT-ARCkl (σ|ik |... |i2|i1,j|β,A) ⇒ (σ|ik |... |i2|i1,j|β,A ∪ {(j,l,ik)}) RIGHT-ARCkl (σ|ik |... |i2|i1,j|β,A) ⇒ (σ|ik |... |i2|i1,j|β,A ∪ {(ik,l,j)}) SHIFT (</context>
</contexts>
<marker>Sagae, 2007</marker>
<rawString>Kenji Sagae. 2007. Dependency parsing and domain adaptation with lr models and parser ensembles. In In Proceedings of the Eleventh Conference on Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Christopher D Manning</author>
</authors>
<title>Ensemble models for dependency parsing: Cheap and good?</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<location>Los Angeles, California,</location>
<contexts>
<context position="3643" citStr="Surdeanu and Manning, 2010" startWordPosition="538" endWordPosition="542"> the training time, we convert the dependency graphs from the training data into dependency trees, and train secondorder arc-factored models1. At the test phase, we parse sentences using this tree parser, and convert the output trees back into semantic graphs. We think tree approximation can appropriately evaluate the possible effectiveness of graph-based models for graph spanning. Finally, we integrate the outputs of different models with a simple voter to boost the performance. The motivation of using system combination and the choice of voting is mainly due to the experiments presented by (Surdeanu and Manning, 2010). When we obtain all the outputs of 1The mate parser (code.google.com/p/ mate-tools/) is used. 459 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 459–464, Dublin, Ireland, August 23-24, 2014. these models, we combine them into a final result, which is better than any of them. For combination, we explore various systems for this task, since empirically we know that variety leads to better performance. 3 Transition-Based Models Transition-based models are usually used for dependency tree parsing. For this task, we exploit it for dependency graph parsin</context>
</contexts>
<marker>Surdeanu, Manning, 2010</marker>
<rawString>Mihai Surdeanu and Christopher D. Manning. 2010. Ensemble models for dependency parsing: Cheap and good? In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Los Angeles, California, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>James Henderson</author>
<author>Paola Merlo</author>
<author>Gabriele Musillo</author>
</authors>
<title>Online graph planarisation for synchronous parsing of semantic and syntactic dependencies.</title>
<date>2009</date>
<booktitle>In Proceedings of the 21st international jont conference on Artifical intelligence, IJCAI’09,</booktitle>
<pages>1562--1567</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="2763" citStr="Titov et al., 2009" startWordPosition="398" endWordPosition="401"> techniques and evaluate their impact with the given data. In particular, we implement different transition-based and graph-based models, as well as a parser ensembler. Our experiments highlight the following facts: • Graph-based models are more effective than transition-based models. • Parser ensemble is very useful to boost the parsing accuracy. 2 Architecture We explore two kinds of basic models: One is transition-based, and the other is tree approximation. Transition-based models are widely used for dependency tree parsing, and they can be adapted to graph parsing (Sagae and Tsujii, 2008; Titov et al., 2009). Here we implement 5 transitionbased models for dependency graph parsing, each of which is based on different transition system. The motivation of developing tree approximation models is to apply existing graph-based tree parsers to generate graphs. At the training time, we convert the dependency graphs from the training data into dependency trees, and train secondorder arc-factored models1. At the test phase, we parse sentences using this tree parser, and convert the output trees back into semantic graphs. We think tree approximation can appropriately evaluate the possible effectiveness of g</context>
<context position="5276" citStr="Titov et al., 2009" startWordPosition="828" endWordPosition="831"> or the buffer. A statistical transitionbased parser leverages a classifier to approximate an oracle that is able to generate target graphs by transforming the initial configuration cs(x) into a terminal configuration ct E Ct. An oracle of a given graph on sentence x is a sequence of transitions which transform the initial configuration to the terminal configuration the arc set Act of which is the set of the arcs of the graph. 3.1 Our Transition Systems We implemented 5 different transition systems for graph parsing. Here we describe two of them in detail, one is the Titov system proposed in (Titov et al., 2009), and the other is our Naive system. The configurations of the two systems each contain a stack σ, a buffer β, and a set A of arcs, denoted by (σ, β, A). The initial configuration of a sentence x = w1w2 · · · wn is cs(x) = ([0], [1, 2, · · · , n],{}), and the terminal configuration set Ct is the set of all configurations with empty buffer. These two transition systems are shown in 1. The transitions of the Titov system are: • LEFT-ARCl adds an arc from the front of the buffer to the top of the stack, labeled l, into A. • RIGHT-ARCl adds an arc from the top of the stack to the front of the buff</context>
<context position="6515" citStr="Titov et al., 2009" startWordPosition="1076" endWordPosition="1079">A. • SHIFT removes the front of the buffer and push it onto the stack; • POP pops the top of the stack. • SWAP swaps the top two elements of the stack. This system uses a transition SWAP to change the node order in the stack, thus allowing some crossing arcs to be built. The transitions of the Naive system are similar to the Titov system’s, except that we can directly manipulate all the nodes in the stack instead of just the top two. In this case, the transition SWAP is not needed. The Titov system can cover a great proportion, though not all, of graphs in this task. For more discussion, see (Titov et al., 2009). The Naive system, by comparison, covers all graphs. That is to say, with this system, we can find an oracle for any dependency graph on a sentence x. Other transition systems we build are also designed for dependency graph parsing, and they can cover dependency graphs without self loop as well. 3.2 Statistical Disambiguation First of all, we derive oracle transition sequences for every sentence, and train Passive-Aggressive models (Crammer et al., 2006) to predict next transition given a configuration. When it comes to parsing, we start with the initial configuration, predicting next transit</context>
</contexts>
<marker>Titov, Henderson, Merlo, Musillo, 2009</marker>
<rawString>Ivan Titov, James Henderson, Paola Merlo, and Gabriele Musillo. 2009. Online graph planarisation for synchronous parsing of semantic and syntactic dependencies. In Proceedings of the 21st international jont conference on Artifical intelligence, IJCAI’09, pages 1562–1567, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yi Zhang</author>
<author>Hans-Ulrich Krieger</author>
</authors>
<title>Large-scale corpus-driven PCFG approximation of an hpsg.</title>
<date>2011</date>
<booktitle>In Proceedings of the 12th International Conference on Parsing Technologies,</booktitle>
<location>Dublin, Ireland,</location>
<contexts>
<context position="9247" citStr="Zhang and Krieger, 2011" startWordPosition="1537" endWordPosition="1540">tion systems. 4 Tree Approximation Models Parsing based on graph spanning is quite challenging since computational properties of the semantic graphs given by the shared task are less explored and thus still unknown. On the other hand, finding the best higher-order spanning for general graph is NP complete, and therefore it is not easy, if not impossible, to implement arc-factored models with exact inference. In our work, we use a practical idea to indirectly profile the graph-based parsing techniques for dependency graph parsing. Inspired by the PCFG approximation idea (Fowler and Penn, 2010; Zhang and Krieger, 2011) for deep parsing, we study tree approximation approaches for graph spanning. This tree approximation technique can be applied to both transition-based and graph-based parsers. However, since transition systems that can directly handle build graphs have been developed, we only use this technique to evaluate the possible effectiveness of graph-based models for semantic parsing. 4.1 Graph-to-Tree Transformation In particular, we develop different methods to convert a semantic graph into a tree, and use edge labels to encode dependency relations as well as structural information which helps to tr</context>
</contexts>
<marker>Zhang, Krieger, 2011</marker>
<rawString>Yi Zhang and Hans-Ulrich Krieger. 2011. Large-scale corpus-driven PCFG approximation of an hpsg. In Proceedings of the 12th International Conference on Parsing Technologies, Dublin, Ireland, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Joakim Nivre</author>
</authors>
<title>Transition-based dependency parsing with rich non-local features.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="7438" citStr="Zhang and Nivre (2011)" startWordPosition="1225" endWordPosition="1228"> Statistical Disambiguation First of all, we derive oracle transition sequences for every sentence, and train Passive-Aggressive models (Crammer et al., 2006) to predict next transition given a configuration. When it comes to parsing, we start with the initial configuration, predicting next transition and updating the configuration with the transition iteratively. And finally we will get a terminal configuration, we then stop and output the arcs of the graph contained in the final configuration. We extracted rich feature for we utilize a set of rich features for disambiguation, referencing to Zhang and Nivre (2011). We examine the several tops of the stack and the one or more fronts of the buffer, and combine the lemmas and POS tags of them in many ways as the features. Additionally, we also derive features from partial parses such as heads and dependents of these nodes. 3.3 Sentence Reversal Reversing the order the words of a given sentence is a simple way to yield heterogeneous parsing models, thus improving parsing accuracy of the model ensemble (Sagae, 2007). In our experiments, one transition system produces two models, one trained on the normal corpus, and the other on the corpus of reversed sente</context>
</contexts>
<marker>Zhang, Nivre, 2011</marker>
<rawString>Yue Zhang and Joakim Nivre. 2011. Transition-based dependency parsing with rich non-local features. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Portland, Oregon, USA, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>