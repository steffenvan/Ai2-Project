<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000252">
<title confidence="0.9940815">
The Method of Improving the Specific Language Focused
Crawler
</title>
<author confidence="0.995055">
Shan-Bin Chan
</author>
<affiliation confidence="0.996749">
Graduate School of Fundamental Science and Engineering
Waseda University
</affiliation>
<email confidence="0.996332">
chrisjan@yama.info.waseda.ac.jp
</email>
<author confidence="0.989514">
Hayato Yamana
</author>
<affiliation confidence="0.9949785">
Graduate School of Fundamental Science and Engineering
Waseda University
</affiliation>
<email confidence="0.975421">
yamana@yama.info.waseda.ac.jp
</email>
<bodyText confidence="0.994377">
ciency about 10% compares to the
baseline results.
</bodyText>
<sectionHeader confidence="0.999258" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999954342857143">
The growth of web pages in the internet
becomes rapidly in recent years. How to
efficiently collect web pages and how to gather
more language or topic relative web pages
become important. Focused crawling is a kind of
method that collects topic specific web pages.
(Chakrabarti et al., 1999) Intelligent crawling
that can self-learning and predicating makes the
focus crawling more efficient. (Chara et al.,
2001) Mining for patterns and relations over
text, structures, and links is an interesting
research. (Neel et al., 2001) In the past few
years, the researches are focused on the topic
specific focused crawling and optimized the
performance of focus crawler for crawling
English web pages. Web pages are not only
described in English but also in other languages.
Our research will be emphasized on the study of
language specific focused crawler and how to
optimize the crawler for specific language (For
example: Chinese, Japanese, and Korean).
Huge amounts of hyperlinks on the CJK web
pages link to English web pages. But the hyper-
links on the English web pages almost don’t link
to the CJK web pages. If we deeply crawl the
web pages from CJK seed sets, finally we will
gather many English web pages. In this kind of
situation, the efficiency of the CJK focus crawler
is very worse.
Our research method is that the first we ex-
tract the domain name from the hyperlink URL
and then determine the top-level domain. For
example, if we try to focus crawl the Japanese
web pages and the top-level domain in the hy-
perlink URLs is .jp, this focus crawler will
</bodyText>
<sectionHeader confidence="0.648265" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.999979608695652">
In recent years, more and more CJK
(Chinese, Japanese, and Korean) web
pages appear in the Internet. The infor-
mation in the CJK web page also be-
comes more and more important. Web
crawler is a kind of tool to retrieve web
pages. Previous researches focused on
English web crawlers and the web
crawler is always optimized for English
web pages. We found that the perform-
ance of the web crawler is worse in re-
trieving CJK web pages. We tried to en-
hance the performance of the CJK
crawler by analyzing the web link struc-
ture, anchor text, and host name on the
hyperlink and changing the crawling al-
gorithm. We distinguish the top-level
domain name and the language of the
anchor text on hyperlinks. The method
that distinguishes the language of the an-
chor text on hyperlinks is not used on
CJK language specific crawler by other
researches. Control experiment is used in
this research. According to the experi-
mental results, when the target crawling
language is Japanese, the 87% of the
crawled web pages are Japanese web
pages and improves the efficiency about
0.24% compares to the baseline results.
When the target crawling language is
Chinese, the 88% of the crawled web
pages are Chinese web pages and im-
proves the efficiency about 0.07% com-
pares to the baseline results. When the
target crawling language is Korean, the
71% of the crawled web pages are Ko-
rean web pages and improves the effi-
queue these URLs for the next crawling. If the
top-level domain in the hyperlink URLs is not
.jp, we will distinguish the language of the an-
chor text of the hyperlink. If the language of the
anchor text is Japanese, we also queue these
URLs for the next crawling. Otherwise, we drop
the URLs.
This research uses the Nutch as the crawler
and uses the Hadoop as the storage. Because of
the web pages is enormous, Hadoop is a very
efficient tool that can store and process vast
amounts of data. We choose the URLs on the
DMOZ as the seeds set and extract the URLs by
the top-level domain name .cn, .tw, .jp, .kr,
.com, .net. After extracting the URLs, we sort
these URLs by language (Chinese, Japanese, and
Korean). We use these sorted URLs as the seeds
set for crawling by Nutch.
The experiment method in our research is
control experiment. We divided our experiment
to two groups. One is control group and the
other one is experimental group. The crawling
methods of the control group use the default
crawling algorithm in Nutch. The crawling
methods of the experimental group use the modi-
fied crawling rules provided by us. After crawl-
ing by both control group and experimental
group, we compare the crawled web pages be-
tween baseline results and experimental results.
Finally, the results show that we can improve the
crawling efficiency by using our modified
method.
</bodyText>
<sectionHeader confidence="0.999658" genericHeader="introduction">
2 Related Researches
</sectionHeader>
<subsectionHeader confidence="0.99986">
2.1 Web data collection of specific topic
</subsectionHeader>
<bodyText confidence="0.9998299">
The research of focus crawler is applied in the
medical information science. (Thanh et al., 2005)
Their research mentions that there are
relationships between the hyperlink URLs and
the 50 words before and after hyperlinks with
the contents in the crawling target pages. And
they also mention that if the URL filters are
applied in the breadth-first crawling, The
crawled results will be better then without URL
filters.
</bodyText>
<subsectionHeader confidence="0.898419">
2.2 Collection methods of specific language
web pages
</subsectionHeader>
<bodyText confidence="0.982199333333333">
Tamura (2006) introduces the research of spe-
cific language web pages focus crawler. Their
research is focused on the collection of Thai lan-
guage web pages. When crawling, the link selec-
tion methods of their research are descript as
below:
</bodyText>
<listItem confidence="0.968645">
1. When the authors collect web page j on
web server i, they distinguish the lan-
guage of web page j.
2. If the language of the web page j is Thai,
Nr(i) increases 1. (Nr(i) is Thai language
web page counts of web server i)
3. Na(i) increases 1. (Na(i) is collected
page counts of web server i)
4. Extract the hyperlinks in the web page j.
5. Drop the hyperlink that top-level domain
is not Thai.
6. Drop the hyperlink that has been
crawled.
7. If the language of web page j is Thai,
queue the remained hyperlinks to high
priority.
8. If the language of web page j is not Thai,
queue the remained hyperlinks to low
priority.
</listItem>
<bodyText confidence="0.999928166666667">
The research data set is collected from famous
portal web pages in Thailand from July to Au-
gust in 2004. They download 18,344,217 HTML
pages from 574,111 servers, and use this data set
for simulating their selective collection method.
The estimation methods are as below:
</bodyText>
<listItem confidence="0.997813894736842">
• Server-based filtering, aggressive: No
limits when the crawler chooses the new
web server.
• Server-based filtering, conservative: On-
ly Thai servers when the crawler chooses
the new web server.
• Directory-based filtering, conservative:
Only Thai directory when the crawler
chooses the directory on the server.
• Hard focused: Drop all the hyperlinks
when the web page is not Thai language.
• Soft focused: If the web page is Thai
language, queue the hyperlinks with high
priority. If the web page is no Thai lan-
guage, queue the hyperlinks with low
priority.
• BFS: Breath-first search.
• Perfect: First, the crawler uses the
breath-first search. When the crawler
</listItem>
<bodyText confidence="0.998244555555556">
found a hyperlink, which is Thai, web
page, the crawler start to follow this Thai
hyperlink and crawl the web pages from
this URL.
The result of the estimation method “Perfect”
is obvious. While crawling, when the total
amount of crawled web pages increase, the cu-
mulative Thai web page ratio increase smoothly
from 92% to 99%.
</bodyText>
<subsectionHeader confidence="0.8845605">
2.3 Seeds set generation method for web
crawler
</subsectionHeader>
<bodyText confidence="0.999943857142857">
HITS algorithm is used to generate crawler seeds
set. (Shervin et al., 2003) Their research consid-
ered that it’s better to crawl the most important
web pages on the resource limited internet. They
use the collected web pages to draw a web graph
and perform the HITS algorithm to generate
seeds set for crawler.
</bodyText>
<subsectionHeader confidence="0.988811">
2.4 Focus crawling for dark web forums
</subsectionHeader>
<bodyText confidence="0.977990647058823">
Dark web forum is a kind of forum that the con-
tents are associated with cybercrime, hate, and
extremism. Fu (2010) developed a focus crawler
for crawling dark web forums by using lan-
guage-independent features, including URL to-
kens, anchor text, and level features. They also
use forum software-specific traversal strategies
and wrappers to support incremental crawling.
Their system maintains up-to-date collections of
109 forums in multiple languages. Their focus
crawler gathers contents from three regions,
which are U.S. domestic supremacist, Middle
Eastern extremist, and Latin groups. Their hu-
man-assisted accessibility mechanism can access
identified forums with a success rate of over
90%.
2.5 The differences between previous stud-
ies
The previous researches almost focus on the
English web page crawlers or the topic specific
crawlers. There are few researches about the
CJK language specific focus crawlers. The
method that judges the top-level domain name of
the URLs on hyperlinks has been studied on
crawling Thai web pages. And the crawling
method of judgment the language of an anchor
text on the hyperlink has not studied by other
researchers. We consider that if we combine the
top-level domain name judgment method and
anchor text language distinguish method, we will
enhance the efficiency more on crawling CJK
web pages. So, we use this combined method to
enhance the performance of CJK language spe-
cific focus crawler.
</bodyText>
<sectionHeader confidence="0.997485" genericHeader="method">
3 Methods
</sectionHeader>
<bodyText confidence="0.999937875">
The probability is very high that hyperlinks on
the CJK web pages link to English web pages.
And the probability is very low that hyperlinks
on the English web pages link to CJK web pages.
If we crawl the web pages deeply from CJK seed
sets, finally we will gather huge amounts of Eng-
lish web pages.
We implement the control experiment to re-
solve the problems that mentioned above. We
split our research to 5 steps and show the process
in the Figure 1.
Before crawling, a seeds set is very important
for the crawler. DMOZ (http://www.dmoz.org)
is the biggest web directory service in the world.
We use the URLs in the DMOZ as the seeds set.
Our URLs extraction method shows as follow.
</bodyText>
<figure confidence="0.73056425">
Seeds Set Collection
URLs Extraction
URLs Sorting
Compare the Results
</figure>
<figureCaption confidence="0.997222">
Figure 1. Research Process
</figureCaption>
<listItem confidence="0.945642086956522">
• Download the XML formatted web di-
rectory data from DMOZ homepage on
October 26th 2009.
• Extract URLs form XML formatted web
directory data.
• Sort the URLs by top-level domain.
(.cn, .tw, .jp, .kr, .com, .net, etcÉ)
Experimental Group
Control Group
• Choose the .cn, .tw, .jp, .kr, .com, .net
top-level domains for language distin-
guish.
• Use the Perl Lingua::LanguageGuesser
which produced by Nakagawa Labora-
tory of Tokyo University to distinguish
the language (Chinese, Japanese, and
Korean) of each web pages in the sorted
URLs. Perl Lingua::LanguageGuesser is
a language distinguisher that is based on
N-Gram text categorization. (William et
al., 1994)
• Use these sorted seeds sets and perform
crawling by using Nutch.
</listItem>
<bodyText confidence="0.970790444444444">
We implement our research by separating into
two groups, control group and experimental
group. The control group follows the default
crawling rules supported by Nutch. And we
change the crawling rules in experimental group
by importing a URLs queuing replacement plug-
in into Nutch. We will explain the modified
URLs queuing rules by using the Chinese web
pages collection procedures.
</bodyText>
<listItem confidence="0.994051933333333">
• If the top-level domain in the URL is .cn,
store this URL to the queue. The reason
is that it is high probability that the web
page with .cn domain name is a Chinese
web page.
• If the top-level domain in the URL is
not .cn, distinguish the language of the
anchor text on the hyperlink. Then, if the
anchor text is Chinese, store the URL to
the queue. The reason is that it is high
probability that the web page which hy-
perlink with Chinese anchor text links to
is a Chinese a web page.
• Drop the URLs from queue when other
situations.
</listItem>
<bodyText confidence="0.998735666666667">
At the final stage, calculates the percentage of
each language and compares the results between
baseline results and experimental results.
</bodyText>
<sectionHeader confidence="0.99994" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.9943402">
URLs extracted from DMOZ that with .com
domain is 1,964,053, .net domain is 182,595, .jp
domain is 130,125, .cn domain is 14,769, .tw
domain is 10,259, .kr domain is 4,910. Figure 2
shows the percentage of each domain.
</bodyText>
<figureCaption confidence="0.999728">
Figure 2. Distribution of DMOZ top-level do-
</figureCaption>
<bodyText confidence="0.989152266666667">
mains
CJK web page counts in each top-level do-
main (.com and .net, .cn, .tw, .jp, .kr) are shown
in Table 1. We extract 43,216 Chinese URLs,
175,666 Japanese URLs, and 5,252 Korean
URLs. We randomly pick 1,000 URLs for each
language from these CJK URLs and start to
crawl by using these URLs as seeds sets.
We write two functions into URLs queuing
replacement plug-in on Nutch. One is html text
language distinguisher, and the other one is web
page counter. We use the same language distin-
guisher that supported by Nakagawa Laboratory
of Tokyo University with seeds set extraction
stage.
</bodyText>
<table confidence="0.999555714285714">
Domain CN JP KR
com&amp;net 24,851 56,256 1,975
cn 11,937 40 1
tw 6,220 573 16
jp 147 118,729 14
kr 61 68 3,246
Total 43,216 175,666 5,252
</table>
<tableCaption confidence="0.7572515">
Table 1. CJK web page counts from each top-
level domain
</tableCaption>
<bodyText confidence="0.9985635">
Figure 3 shows the crawling process of this
research. The original Nutch crawl process is
that crawl the web pages from seeds set, parse
the html text, extract the URLs from hyperlinks,
store the URLs to the queue, and implement the
breadth-first crawling. In order to recording the
language of URLs, after parsing the html text,
we add a Nutch plug-in to judges the language of
the html text. Then write the URL of this web
page to a language specific file (Chinese, Japa-
nese, Korean, Other) for counting. And then ex-
tract all the hyperlinks and store the URLs to the
queue. Control group implements the process
described above.
HTML text of this URL and extract the
hyperlinks and anchor text from HTML text. We
get a part of the hyperlink and anchor text shown
in Table 2.
</bodyText>
<figure confidence="0.813401">
HTML Text
</figure>
<figureCaption confidence="0.999747">
Figure 3. Crawling process
</figureCaption>
<bodyText confidence="0.999338">
Experimental group also implements the
process of control group. After the control group
process, we add top-level domain judgment on
hyperlinks. When we are focusing on crawling
Chinese web pages, and if the top-level domain
on hyperlinks is .cn, we store the URL of this
hyperlink to the queue. If the top-level domain
on hyperlinks is not .cn, we check the anchor
text of the hyperlink. If the language of the an-
chor is Chinese, we also store the URL of this
hyperlink to the queue. Otherwise, drop the
URLs from queue. We don’t prioritize the URLs
in queue in this research. In order to not crawl-
ing the web pages that are not Chinese, we de-
cide to drop these URLs that may not be Chinese
web pages.
We pick a Chinese web page as the crawling
example for experimental group. The crawler
chooses a URL “http://www.bsc.org.cn/” in the
sorted DOMZ seed sets. First, we parse the
</bodyText>
<figure confidence="0.9567924">
Hyperlinks
http://www.cast.org.cn
中国科学院生物物 http://www.ibp.ac.cn
理研究所
http://www.iupab.org
http://www.aba-bp.com
http://www.biophysics.org
http://www.protein-
cell.org
É
</figure>
<tableCaption confidence="0.9453065">
Table 2. A part of anchor text and hyperlinks
extracted from “http://www.bsc.org.cn/”
</tableCaption>
<bodyText confidence="0.998617">
Second, according to the URL queuing rules
mentioned above, we show a part of matched
hyperlinks and anchor text shown in Table 3.
</bodyText>
<table confidence="0.999338411764706">
Anchor Hyperlinks Matched Page
text lang
中国科学 http://www.ca Yes CHS
技术协会 st.org.cn
中国科学 http://www.ib Yes CHS
院生物物 p.ac.cn
理研究所
IUPAB http://www.iu No ENG
pab.org
Asian http://www.ab No ENG
Biophys- a-bp.com
ics Asso-
ciation
美国生物 http://www.bi No ENG
物理学会 ophysics.org
Protein http://www.pr No ENG
and Cell otein-cell.org
</table>
<tableCaption confidence="0.993631">
Table 3. A part of matched hyperlinks by using
</tableCaption>
<bodyText confidence="0.907629428571428">
experimental group queuing rules from
“http://www.bsc.org.cn/”
Finally, we use these matched hyperlinks as
the URLs for next crawling loop.
The web page crawled results of control group
and experimental group from Feb. 5th to Feb.
12th 2010 show in Table 4. Because of the longer
</bodyText>
<figure confidence="0.998605">
Language
Judgment
URL Counter
Hyperlink Judgment
(Target Language?)
Anchor Text
Judgment
(Target Language?)
URL, Anchor
Text Extraction
Drop URL
N
N
Experimental Group Only
Y
Y
URL Queue
URL Queue
URL Queue
Anchor text
中国科学技术协会
IUPAB
Asian Biophysics
Association
美国生物物理学会
Protein and Cell
É
</figure>
<bodyText confidence="0.999305666666667">
processing time of language distinguish in ex-
perimental group, the total crawl results of ex-
perimental results are fewer then baseline results.
</bodyText>
<table confidence="0.994831285714286">
Chinese Japanese Korean Other Total
KR-C∗ 12,523 1,926 80,049 36,273 130,771
KR-E 1,757 380 11,328 2,386 15,851
JP-C 6,555 66,235 108 2,838 75,736
JP-E 1,179 11,890 24 465 13,558
CN-C 112,924 2,468 1.052 11,321 127,765
CN-E 10,078 202 99 1,015 11,394
</table>
<tableCaption confidence="0.7320745">
Table 4. The crawled web pages for each lan-
guage
</tableCaption>
<bodyText confidence="0.9986782">
When the crawling target language is Korean,
the language percentage of gathered web pages
in baseline results shows as Figure 4. Total
130,771 web pages are crawled and 61.21% are
Korean web pages.
</bodyText>
<figureCaption confidence="0.644894">
Figure 4. Language distribution of Korean
baseline results
</figureCaption>
<bodyText confidence="0.936193">
15,851 web pages are crawled and 71.47% are
Korean web pages.
According to the crawled results in Figure 4
and Figure 5, our methods can improve about
10% efficiency in crawling Korean web pages.
When the crawling target language is Japa-
nese, the language percentage of gathered web
pages in baseline results shows as Figure 6. To-
tal 75,736 web pages are crawled and 87.46%
are Japanese web pages.
Figure 6. Language distribution of Japanese
baseline results
When the crawling target language is Japa-
nese, the language percentage of gathered web
pages in experimental results shows as Figure 7.
Total 13,558 web pages are crawled and 87.70%
are Japanese web pages.
Figure 5. Language distribution of Korean ex-
perimental results
When the crawling target language is Korean,
the language percentage of gathered web pages
in experimental results shows as Figure 5. Total
</bodyText>
<footnote confidence="0.73157">
∗ KR(Korean)•JP(Japanese)•CN(Chinese)• C(Baseline Re-
sults)•E(Experimental Results)
</footnote>
<figureCaption confidence="0.8878375">
Figure 7. Language distribution of Japanese ex-
perimental results
</figureCaption>
<bodyText confidence="0.999700777777778">
According to the crawled results in Figure 6
and Figure 7, our methods can improve about
0.24% efficiency in crawling Japanese web pag-
es.
When the crawling target language is Chinese,
the language percentage of gathered web pages
in baseline results shows as Figure 8. Total
127,765 web pages are crawled and 88.38% are
Japanese web pages.
</bodyText>
<figureCaption confidence="0.8953745">
Figure 8. Language distribution of Chinese
baseline results
</figureCaption>
<bodyText confidence="0.9991838">
When the crawling target language is Chinese,
the language percentage of gathered web pages
in experimental results shows as Figure 9. Total
11,394 web pages are crawled and 88.45% are
Japanese web pages.
</bodyText>
<figureCaption confidence="0.843626">
Figure 9. Language distribution of Chinese ex-
perimental results
</figureCaption>
<bodyText confidence="0.999965095238095">
According to the crawled results in Figure 8
and Figure 9, our methods can improve about
0.07% efficiency in crawling Chinese web pages.
The experimental results show that the effi-
ciency improvement of Korean crawling is very
obvious. The efficiency improvement of Chinese
and Japanese are lower then 1%, actually the
improvement is unobvious. We picked some
URLs from crawling log to analyze why the im-
provement of Korean language specific crawler
is obvious. We found that many Korean web
pages content multi-language links on the main
page, such as English, Chinese, Japanese, etcÉ.
It is high probability that the hyperlinks on the
Korean web pages link to web pages which are
other languages. The languages of anchor text on
Chinese or Japanese web pages are always Chi-
nese or Japanese. So following our modified
crawling rules, the crawled Korean web pages
increase conspicuously and the crawled Chinese
or Japanese web pages increase unobvious.
</bodyText>
<sectionHeader confidence="0.999407" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999968781818182">
The purpose of this research is to enlarge the
crawling amounts of language specific crawler.
We propose language judgment on anchor text
method to enhance the efficiency of the focus
crawler. And combine the method which is dis-
tinguish the top-level domain name of URLs on
hyperlinks to implement a control experiment in
this research.
This research also proposes language specific
focus crawler by importing a plug-in into Nutch
and crawling the web pages with a modified al-
gorithm. This method can easily program, install,
and implement modified crawling rules by using
plug-in on Nutch.
According to the comparison of control group
and experimental group, the efficiency im-
provement is obvious on Korean focus crawler.
And the efficiency improvement is unobvious on
Chinese and Japanese focus crawler. It is be-
cause many hyperlinks on the Korean web pages
link to web pages that are other languages. So
that the improvement of Korean web pages
crawling is obvious.
Perl language guesser is implemented by us-
ing JAVA external call. If we can use the JAVA
language guesser, the crawling speed will be
improved.
The append function of Hadoop is defective.
Only new file can be appended, data can’t ap-
pend to existing file in Hadoop. In order to
counting the crawled URLs, immediately append
the URLs list to a log file in Hadoop is impossi-
ble. So Perl script called by JAVA external call
is used to replace the flawed append function in
Hadoop. But this kind of method occurs a lot of
“Out of memory” and “IOExpection” errors in
JAVA. These errors slow down the gather speed
of web pages. If the append function in Hadoop
is flawless, the collection speed of web pages
will be boosted.
According to the crawled results in experi-
mental group, the percentage of crawled Chinese
web pages by Chinese focus crawler is 88%. The
percentage of crawled Japanese web pages by
Japanese focus crawler is 87%. The percentage
of crawled Korean web pages by Korean focus
crawler is 71%. We will increase the efficiency
of language specific crawler more in the future
work.
This research uses DMOZ as the seed sets.
We extract CJK URLs from DMOZ data. Actu-
ally, the percentage of CJK URLs in DMOZ data
is very low. We will try to use the famous CJK
portal web sites as the seed sets in the future
work.
</bodyText>
<sectionHeader confidence="0.997605" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.996164666666667">
This research is supported by Waseda University
Global COE Program “International Research
and Education Center for Ambient SoC” and
JST Program “Multimedia Web Analysis
Framework towards Development of Social
Analysis Software.&amp;quot;
</bodyText>
<sectionHeader confidence="0.999037" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998769522727273">
Charu C. Aggarwal, Fatima Al-Garawi, and Philip S.
Yu. 2001. Intelligent Crawling on the World Wide
Web with Arbitrary Predicates, WWW conference.
DMOZ: The Open Directory Project is the largest,
most comprehensive human-edited directory of the
Web. http://www.dmoz.org
Hadoop: A reliable, scalable, and distributed comput-
ing system. http://hadoop.apache.org
HITS: Hyperlink-Induced Topic Search.
http://en.wikipedia.org/wiki/HITS_algorithm
Lingua::LanguageGuesser. http://gensen.dl.itc.u-
tokyo.ac.jp/LanguageGuesser/LanguageGuesser_j
a.html
Neel Sundaresan, and Jeonghee Yi. 2001. Mining the
Web for Relations, Computer Networks, Volume
33, Issue 1-6: 699-711.
Nutch: A JAVA based open source web crawler de-
veloped by Apache Software Foundation.
http://nutch.apache.org
Shervin Daneshpajouh, Mojtaba Mohammadi Nasiri,
and Mohammad Ghodsi. 2003. A Fast Community
Based Algorithm For Generating Web Crawler
Seeds Set.
Soumen Chakrabarti, Martin van den Berg, and
Byron Dom. 1999. Focused Crawling: A New Ap-
proach to Topic Specific Resource Discovery,
WWW Conference.
Tamura Takayuki, Somboonviwat Kulwadee, and
Kitsuregawa Masaru. 2006. A Method for Lan-
guage Specific Web Crawling and Its Evaluation,
The IEICE transactions on information and sys-
tems, J89-D(2):199-209.
Thanh Tin Tang, David Hawking, Nick Craswell, and
Kathy Griffiths. 2005. Focused Crawling for both
Topical Relevance and Quality of Medical Infor-
mation, CIKM.
Tianjun Fu, Ahmed Abbasi, and Hsinchun Chen.
2010. A Focused Crawler for Dark Web Forums,
Journal of The American Society for Information
Science and Technology, 61(6):1213-1231
William B. Cavnar and John M. Trenkle. 1994. N-
gram-based text categorization, In Symposium On
Document Analysis and Information Re-
trieval:161–176.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.331292">
<title confidence="0.995035">The Method of Improving the Specific Language Focused Crawler</title>
<author confidence="0.996796">Shan-Bin Chan</author>
<affiliation confidence="0.999472">Graduate School of Fundamental Science and Waseda University</affiliation>
<email confidence="0.804518">chrisjan@yama.info.waseda.ac.jp</email>
<affiliation confidence="0.876872">Hayato Yamana Graduate School of Fundamental Science and Waseda University</affiliation>
<abstract confidence="0.886242666666667">yamana@yama.info.waseda.ac.jp ciency about 10% compares to the baseline results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Charu C Aggarwal</author>
<author>Fatima Al-Garawi</author>
<author>Philip S Yu</author>
</authors>
<date>2001</date>
<booktitle>Intelligent Crawling on the World Wide Web with Arbitrary Predicates, WWW conference.</booktitle>
<marker>Aggarwal, Al-Garawi, Yu, 2001</marker>
<rawString>Charu C. Aggarwal, Fatima Al-Garawi, and Philip S. Yu. 2001. Intelligent Crawling on the World Wide Web with Arbitrary Predicates, WWW conference.</rawString>
</citation>
<citation valid="false">
<title>DMOZ: The Open Directory Project is the largest, most comprehensive human-edited directory of the Web. http://www.dmoz.org</title>
<marker></marker>
<rawString>DMOZ: The Open Directory Project is the largest, most comprehensive human-edited directory of the Web. http://www.dmoz.org</rawString>
</citation>
<citation valid="false">
<title>Hadoop: A reliable, scalable, and distributed computing system. http://hadoop.apache.org HITS: Hyperlink-Induced Topic Search.</title>
<note>http://en.wikipedia.org/wiki/HITS_algorithm</note>
<marker></marker>
<rawString>Hadoop: A reliable, scalable, and distributed computing system. http://hadoop.apache.org HITS: Hyperlink-Induced Topic Search. http://en.wikipedia.org/wiki/HITS_algorithm</rawString>
</citation>
<citation valid="false">
<authors>
<author>Lingua LanguageGuesser</author>
</authors>
<note>http://gensen.dl.itc.utokyo.ac.jp/LanguageGuesser/LanguageGuesser_j a.html</note>
<marker>LanguageGuesser, </marker>
<rawString>Lingua::LanguageGuesser. http://gensen.dl.itc.utokyo.ac.jp/LanguageGuesser/LanguageGuesser_j a.html</rawString>
</citation>
<citation valid="true">
<authors>
<author>Neel Sundaresan</author>
<author>Jeonghee Yi</author>
</authors>
<title>Mining the Web for Relations,</title>
<date>2001</date>
<journal>Computer Networks,</journal>
<volume>33</volume>
<pages>699--711</pages>
<marker>Sundaresan, Yi, 2001</marker>
<rawString>Neel Sundaresan, and Jeonghee Yi. 2001. Mining the Web for Relations, Computer Networks, Volume 33, Issue 1-6: 699-711.</rawString>
</citation>
<citation valid="false">
<title>Nutch: A JAVA based open source web crawler developed by Apache Software Foundation.</title>
<note>http://nutch.apache.org</note>
<marker></marker>
<rawString>Nutch: A JAVA based open source web crawler developed by Apache Software Foundation. http://nutch.apache.org</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shervin Daneshpajouh</author>
<author>Mojtaba Mohammadi Nasiri</author>
<author>Mohammad Ghodsi</author>
</authors>
<title>A Fast Community Based Algorithm For Generating Web Crawler Seeds Set.</title>
<date>2003</date>
<marker>Daneshpajouh, Nasiri, Ghodsi, 2003</marker>
<rawString>Shervin Daneshpajouh, Mojtaba Mohammadi Nasiri, and Mohammad Ghodsi. 2003. A Fast Community Based Algorithm For Generating Web Crawler Seeds Set.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soumen Chakrabarti</author>
<author>Martin van den Berg</author>
<author>Byron Dom</author>
</authors>
<title>Focused Crawling: A New Approach to Topic Specific Resource Discovery,</title>
<date>1999</date>
<journal>WWW Conference.</journal>
<marker>Chakrabarti, van den Berg, Dom, 1999</marker>
<rawString>Soumen Chakrabarti, Martin van den Berg, and Byron Dom. 1999. Focused Crawling: A New Approach to Topic Specific Resource Discovery, WWW Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tamura Takayuki</author>
<author>Somboonviwat Kulwadee</author>
<author>Kitsuregawa Masaru</author>
</authors>
<title>A Method for Language Specific Web Crawling and Its Evaluation, The IEICE transactions on information and systems,</title>
<date>2006</date>
<pages>89--2</pages>
<marker>Takayuki, Kulwadee, Masaru, 2006</marker>
<rawString>Tamura Takayuki, Somboonviwat Kulwadee, and Kitsuregawa Masaru. 2006. A Method for Language Specific Web Crawling and Its Evaluation, The IEICE transactions on information and systems, J89-D(2):199-209.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thanh Tin Tang</author>
<author>David Hawking</author>
<author>Nick Craswell</author>
<author>Kathy Griffiths</author>
</authors>
<title>Focused Crawling for both Topical Relevance and Quality of Medical Information,</title>
<date>2005</date>
<publisher>CIKM.</publisher>
<marker>Tang, Hawking, Craswell, Griffiths, 2005</marker>
<rawString>Thanh Tin Tang, David Hawking, Nick Craswell, and Kathy Griffiths. 2005. Focused Crawling for both Topical Relevance and Quality of Medical Information, CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tianjun Fu</author>
<author>Ahmed Abbasi</author>
<author>Hsinchun Chen</author>
</authors>
<title>A Focused Crawler for Dark Web Forums,</title>
<date>2010</date>
<journal>Journal of The American Society for Information Science and Technology,</journal>
<pages>61--6</pages>
<marker>Fu, Abbasi, Chen, 2010</marker>
<rawString>Tianjun Fu, Ahmed Abbasi, and Hsinchun Chen. 2010. A Focused Crawler for Dark Web Forums, Journal of The American Society for Information Science and Technology, 61(6):1213-1231</rawString>
</citation>
<citation valid="true">
<authors>
<author>William B Cavnar</author>
<author>John M Trenkle</author>
</authors>
<title>Ngram-based text categorization,</title>
<date>1994</date>
<booktitle>In Symposium On Document Analysis and Information Retrieval:161–176.</booktitle>
<marker>Cavnar, Trenkle, 1994</marker>
<rawString>William B. Cavnar and John M. Trenkle. 1994. Ngram-based text categorization, In Symposium On Document Analysis and Information Retrieval:161–176.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>