<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003487">
<title confidence="0.997362">
Comparing Rating Scales and Preference Judgements
in Language Evaluation
</title>
<author confidence="0.994442">
Anja Belz Eric Kow
</author>
<affiliation confidence="0.997400333333333">
Natural Language Technology Group
School of Computing, Mathematical and Information Sciences
University of Brighton
</affiliation>
<address confidence="0.995873">
Brighton BN2 4GJ, UK
</address>
<email confidence="0.999637">
{asb,eykk10}@bton.ac.uk
</email>
<sectionHeader confidence="0.997404" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99967432">
Rating-scale evaluations are common in
NLP, but are problematic for a range of
reasons, e.g. they can be unintuitive for
evaluators, inter-evaluator agreement and
self-consistency tend to be low, and the
parametric statistics commonly applied to
the results are not generally considered
appropriate for ordinal data. In this pa-
per, we compare rating scales with an al-
ternative evaluation paradigm, preference-
strength judgement experiments (PJEs),
where evaluators have the simpler task of
deciding which of two texts is better in
terms of a given quality criterion. We
present three pairs of evaluation experi-
ments assessing text fluency and clarity
for different data sets, where one of each
pair of experiments is a rating-scale ex-
periment, and the other is a PJE. We find
the PJE versions of the experiments have
better evaluator self-consistency and inter-
evaluator agreement, and a larger propor-
tion of variation accounted for by system
differences, resulting in a larger number of
significant differences being found.
</bodyText>
<sectionHeader confidence="0.999624" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999684646341463">
Rating-scale evaluations, where human evaluators
assess system outputs by selecting a score on a dis-
crete scale, are the most common form of human-
assessed evaluation in NLP. Results are typically
presented in rank tables of means for each system
accompanied by means-based measures of statisti-
cal significance of the differences between system
scores.
NLP system evaluation tends to involve sets of
systems, rather than single ones (evaluations tend
to at least incorporate a baseline or, more rarely, a
topline system). The aim of system evaluation is
to gain some insight into which systems are bet-
ter than which others, in other words, the aim is
inherently relative. Yet NLP system evaluation ex-
periments have generally preferred rating scale ex-
periments where evaluators assess each system’s
quality in isolation, in absolute terms.
Such rating scales are not very intuitive to use;
deciding whether a text deserves a 5, a 4 or a 3
etc. can be difficult. Furthermore, evaluators may
ascribe different meanings to scores and the dis-
tances between them. Individual evaluators have
different tendencies in using rating scales, e.g.
what is known as ‘end-aversion’ tendency where
certain individuals tend to stay away from the ex-
treme ends of scales; other examples are positive
skew and acquiescence bias, where individuals
make disproportionately many positive or agree-
ing judgements; see e.g. Choi and Pak, (2005).
It is not surprising then that stable averages of
quality judgements, let alone high levels of agree-
ment, are hard to achieve, as has been observed for
MT (Turian et al., 2003; Lin and Och, 2004), text
summarisation (Trang Dang, 2006), and language
generation (Belz and Reiter, 2006). It has even
been demonstrated that increasing the number of
evaluators and/or data can have no stabilising ef-
fect at all on means (DUC literature).
The result of a rating scale experiment is ordi-
nal data (sets of scores selected from the discrete
rating scale). The means-based ranks and statisti-
cal significance tests that are commonly presented
with the results of RSEs are not generally consid-
ered appropriate for ordinal data in the statistics
literature (Siegel, 1957). At the least, “a test on the
means imposes the requirement that the measures
must be additive, i.e. numerical” (Siegel, 1957, p.
14). Parametric statistics are more powerful than
non-parametric alternatives, because they make a
number of strong assumptions (including that the
data is numerical). If the assumptions are violated
then the risks is that the significance of results is
overestimated.
In this paper we explore an alternative evalua-
tion paradigm, Preference-strength Judgement Ex-
periments (PJEs). Binary preference judgements
have been used in NLP system evaluation (Reiter et
al., 2005), but to our knowledge this is the first sys-
tematic investigation of preference-strength judge-
ments where evaluators express, in addition to
their preference (which system do you prefer?),
also the strength of their preference (how strongly
do you prefer the system you prefer?). It seems
intuitively convincing that it should be easier to
decide which of two texts is clearer than to de-
cide whether a text’s clarity deserves a 1, 2, 3, 4 or
5. However, it is less clear whether evaluators are
also able to express the strength of their preference
in a consistent fashion, resulting not only in good
self-consistency, but also in good agreement with
other evaluators.
We present three pairs of directly comparable
RSE and PJE evaluations, and investigate how they
compare in terms of (i) the amount of variation ac-
counted for by differences between systems (the
more the better), relative to the amount of varia-
tion accounted for by other factors such as evalu-
ator and arbitrary text properties (the less the bet-
ter); (ii) inter-evaluator agreement, (iii) evaluator
self-consistency, (iv) the number of significant dif-
ferences identified, and (v) experimental cost.
</bodyText>
<sectionHeader confidence="0.872604" genericHeader="introduction">
2 Overview of Experiments
</sectionHeader>
<bodyText confidence="0.9979055625">
In the following three sections we present the de-
sign and results of three pairs of evaluations. Each
pair consists of a rating-scale experiment (RSE)
and a preference-strength judgement experiment
(PJE) that differ only in the rating method they em-
ploy (relative ratings in the PJE and absolute rat-
ings in the RSE).1 In other words, they involve the
same set of system outputs, the same instructions
and method of presentating system outputs. Each
pair is for a different data domain and system task,
the first for generating chains of references to peo-
ple in Wikipedia articles (Section 3); the second
for weather forecast text generation (Section 4);
and the third for generating descriptions of images
of furniture and faces (Section 5).
All experiments use a Repeated Latin Squares
</bodyText>
<footnote confidence="0.9984286">
1We are currently preparing an open-source release of the
RSE/PJE toolkit we have developed for implementing the ex-
periments described in this paper which automatically gen-
erates an experiment, including webpages, given some user-
specified parameters and the data to be evaluated.
</footnote>
<figureCaption confidence="0.948692">
Figure 1: Standardised 1–5 rating scale represen-
tation for Fluency and Clarity criteria.
</figureCaption>
<bodyText confidence="0.999823210526315">
design which ensures that each subject sees the
same number of outputs from each system and
for each test set item. Following detailed instruc-
tions, subjects first do 2 or 3 practice examples,
followed by the texts to be evaluated, in an order
randomised for each subject. Subjects carry out
the evaluation over the internet, at a time and place
of their choosing. They are allowed to interrupt
and resume (but are discouraged from doing so).
There are subtle differences between the three
experiment pairs, and for ease of comparison we
provide an overview of the six experiments we in-
vestigate in this paper in Table 1. Each of the as-
pects of experimental design and execution shown
in this table is explained and described in more de-
tail in the relevant subsection below, but some of
the important differences are highlighted here.
In GREC-NEG PJE, each system is compared
with only one other comparisor system (a human-
authored topline), whereas in the other two PJE ex-
periments, each system is compared with all other
systems for each test data set item.
In the two versions of the METEO evaluation,
evaluators were not drawn from the same cohort of
people, whereas in the other two evaluation pairs
they were drawn from the same cohort. GREC-
NEG RSE and METEO RSE used radio buttons (as
shown in Figure 1) as the rating-scale evaluation
mechanism whereas in TUNA RSE it was an un-
marked slider bar. While slightly different names
were used for the evaluation criteria in two of
the evaluation pairs, Fluency/Readability were ex-
plained in very similar terms (does it read well?),
and Adequacy in TUNA was explained in terms of
clarity of reference (is it clear which entity the de-
scription refers to?), so there are in fact just two
evaluation criteria (albeit with different names).
Where we use preference-strength judgements,
</bodyText>
<table confidence="0.999277727272727">
Data set GREC-NEG METEO TUNA
Type RSE PJE RSE PJE RSE PJE
Criteria names Fluency, Clarity Readability, Clarity Fluency, Adequacy
Evaluator type linguistics students uni staff ling stud linguistics students
Num evaluators 10 10 22 22 8 28
Comparisor(s) – human topline – all systems – all systems
Test set size 30 22 112
N trials 300 300 484 1210 896 3136
Rating tool radio buttons slider radio buttons slider slider bar slider
Range 1–5 −10.0.. + 10.0 1–7 −50.0.. + 50.0 0–100 −50.0.. + 50.0
Numbers visible? yes no yes no no no
</table>
<tableCaption confidence="0.9679685">
Table 1: Overview of experiments with details of design and execution. (Comparisor(s) = the other
systems against which each system is evaluated.)
</tableCaption>
<subsectionHeader confidence="0.986385">
3.1 Data and generic design
</subsectionHeader>
<bodyText confidence="0.99876165625">
In our first pair of experiments we used system and
human outputs for the GREC-NEG task of selecting
referring expressions for people in discourse con-
text. The GREC-NEG data2 consists of introduction
sections from Wikipedia articles about people in
which all mentions of people have been annotated
by marking up the word strings that function as
referential expressions (REs) and annotating them
with coreference information as well as syntactic
and semantic features. The following is an exam-
ple of an annotated RE from the corpus:
&lt;REF ENTITY=&amp;quot;0&amp;quot; MENTION=&amp;quot;1&amp;quot; SEMCAT=&amp;quot;person&amp;quot; SYNCAT=&amp;quot;np&amp;quot;
SYNFUNC=&amp;quot;subj&amp;quot;&gt;&lt;REFEX ENTITY=&amp;quot;0&amp;quot; REG08-TYPE=&amp;quot;name&amp;quot;
the evaluation mechanism is implemented using
slider bars as shown at the bottom of Figure 2
which map to a scale −X.. + X. The evalua-
tor’s task is to express their preference in terms of
each quality criterion by moving the pointers on
the sliders. Moving the pointer to the left means
expressing a preference for the text on the left,
moving it to the right means preferring the text on
the right; the further to the left/right the slider is
moved, the stronger the preference. It was not ev-
ident to the evaluators that sliders were associated
with numerical values. Slider pointers started out
in the middle of the scale (the position correspond-
ing to no preference). If they wanted to leave the
pointer in the middle (i.e. if they had no prefer-
ence for either of the two texts), evaluators had to
check a box to confirm their rating (to avoid evalu-
ators accidentally not rating a text and leaving the
pointer in the default position).
</bodyText>
<sectionHeader confidence="0.92262" genericHeader="method">
3 GREC-NEG RSE/PJE: Named entity
reference chains
</sectionHeader>
<footnote confidence="0.734647333333333">
2The GREC-NEG data and documen-
tation is available for download from
http://www.nltg.brighton.ac.uk/home/Anja.Belz
</footnote>
<listItem confidence="0.591629333333333">
CASE=&amp;quot;plain&amp;quot;&gt;Sir Alexander Fleming&lt;/REFEX&gt; &lt;/REF&gt;
(6 August 1881 - 11 March 1955) was a Scottish biol-
ogist and pharmacologist.
</listItem>
<bodyText confidence="0.997024222222222">
This data was used in the GREC-NEG’09
shared-task competition (Belz et al., 2009), where
the task was to create systems which automatically
select suitable REs for all references to all person
entities in a text.
The evaluation experiments use Clarity and Flu-
ency as quality criteria which were explained in
the introduction as follows (the wording of the first
is from DUC):
</bodyText>
<listItem confidence="0.999187666666667">
1. Referential Clarity: It should be easy to identify who
the referring expressions are referring to. If a person
is mentioned, it should be clear what their role in the
story is. So, a reference would be unclear if a person
is referenced, but their identity or relation to the story
remains unclear.
2. Fluency: A referring expression should ‘read well’,
i.e. it should be written in good, clear English, and the
use of titles and names should seem natural. Note that
the Fluency criterion is independent of the Referential
Clarity criterion: a reference can be perfectly clear, yet
not be fluent.
</listItem>
<bodyText confidence="0.9999138">
The evaluations involved outputs for 30 randomly
selected items from the test set from 5 of the 6
systems which participated in GREC-NEG’10, the
four baseline systems developed by the organisers,
and the original corpus texts (10 systems in total).
</bodyText>
<subsectionHeader confidence="0.998468">
3.2 Preference judgement experiment
</subsectionHeader>
<bodyText confidence="0.9999409">
The human-assessed intrinsic evaluation in
GREC’09 was designed as a preference-judgement
test where subjects expressed their preference, in
terms of the two criteria, for either the original
Wikipedia text (human-authored ‘topline’) or
the version of it with system-selected referring
expressions in it. There were three 10x10 Latin
Squares, and a total of 300 trials (with two
judgements in each, one for Fluency and one for
Clarity) in this evaluation. The subjects were 10
</bodyText>
<figureCaption confidence="0.997039">
Figure 2: Example of text pair presented in human intrinsic evaluation of GREC-NEG systems.
</figureCaption>
<bodyText confidence="0.999059615384615">
native speakers of English recruited from cohorts
of students currently completing a linguistics-
related degree at Kings College London and
University College London.
Figure 2 shows what subjects saw during the
evaluation of an individual text pair. The place
(left/right) of the original Wikipedia article was
randomly determined for each individual evalua-
tion of a text pair. People references are high-
lighted in yellow/orange, those that are identical
in both texts are yellow, those that are different are
orange.3 The sliders are the standardised design
described in the preceding section.
</bodyText>
<subsectionHeader confidence="0.997792">
3.3 Rating scale experiment
</subsectionHeader>
<bodyText confidence="0.9998705">
Our new experiment used our standardised radio
button design for a 1–5 rating scale as shown in
Figure 1. We used the same Latin Squares design
as for the PJE version, and recruited 10 different
evaluators from the same student cohorts at Kings
College London and University College London.
Evaluators saw just one text in each trial, with the
people references highlighted in yellow.
</bodyText>
<sectionHeader confidence="0.613371" genericHeader="method">
3.4 Results and comparative analysis
</sectionHeader>
<bodyText confidence="0.7867175">
Measures comparing the results from the two ver-
sions of the GREC-NEG evaluation are shown in
</bodyText>
<tableCaption confidence="0.896285">
Table 2. The first row for each experiment type
</tableCaption>
<footnote confidence="0.5376505">
3When viewed in black and white, the orange highlights
are the slighly darker ones.
</footnote>
<table confidence="0.999974461538462">
Type Measure Clarity Fluency
RSE F(9,290) 10.975** 35.998**
N sig diffs 19/45 27/45
K’s W (inter) .543** .760**
avg W (intra) .5275 .7192
( Text F(29,270) 2.512** 1.825** )
( Evaluator F(9,290) 3.998** .630 )
PJE F(9,290) 29.539** 26.596**
N sig diffs 26/45 24/45
K’s W (inter) .717** .725**
avg W (intra) .6909 .7125
( Text F(29,270) .910 1.237 )
( Evaluator F(9,290) 1.237 4.145** )
</table>
<tableCaption confidence="0.9330875">
Table 2: GREC-NEG RSE/PJE: Results of analy-
ses looking at effect of System.
</tableCaption>
<bodyText confidence="0.9999344">
shows the F ratio as determined by a one-way
ANOVA with the evaluation criterion in question
as the dependent variable and System as the fac-
tor. F is the ratio of between-groups variability
over within-group (or residual) variability, i.e. the
larger the value of F, the more of the variability ob-
served in the data is accounted for by the grouping
factor, here System, relative to what variability re-
mains within the groups.
The second row shows the number of signifi-
cant differences out of the possible total, as deter-
mined by a Tukey’s HSD analysis. Kendall’s W
(interpretable as a coefficient of concordance) is
a commonly used measure of the agreement be-
tween judges and is based on mean rank. It ranges
from 0 to 1, and the closer to 1 it is the greater the
agreement. The fourth row (K’s W, inter) shows
the standard W measure, estimating the degree to
which the evaluators agreed. The 5th row (K’s W,
intra) shows the average W for repeated ratings
by the same judge, i.e. it is a measure of the av-
erage self-consistency achieved by the evaluators.
Finally, in the last two rows we give F-ratios for
Text (test data set item) and Evaluator, estimating
the effect these two have independently of System.
The F ratios and numbers of significant differ-
ences are very similar in the PJE version, but very
dissimilar in the RSE version of this experiment.
For Fluency, F is greater in the RSE version than
in the PJE version where there appear to be big-
ger differences between scores assigned by evalua-
tors. However, Kendall’s W shows that in terms of
mean score ranks, the evaluators agreed to a simi-
lar extent in both experiment versions.
Clarity in the RSE version has lower values
across the board than the rest of Table 2: it ac-
counts for less of the variation, has fewer signifi-
cant differences and lower levels of inter-evaluator
agreement and self-consistency. If the results from
the PJE version were not also available one might
be inclined to conclude that there was not as much
difference between systems in terms of Clarity as
there was in terms of Fluency. However, because
Fluency and Clarity have a similarly strong effect
in GREC-NEG PJE, it looks instead as though the
evaluators found it harder to apply the Clarity cri-
terion in GREC-NEG RSE than Fluency in GREC-
NEG RSE, and than Clarity in GREC-NEG PJE.
One way of interpreting this is that it is possible
to achieve the same good levels of inter-evaluator
and intra-evaluator variation for the Clarity crite-
rion as for Fluency (both as defined and applied
within the context of this specific experiment), and
that it is therefore worrying that the RSE version
does not achieve it.
</bodyText>
<sectionHeader confidence="0.9989" genericHeader="method">
4 METEO RSE/PJE: Weather forecasts
</sectionHeader>
<subsectionHeader confidence="0.874232">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.63742375">
Our second pair of evaluations used the Prodigy-
METEO4 version (Belz, 2009) of the SUMTIME-
METEO corpus (Sripada et al., 2002) which con-
tains system outputs and the pairs of wind forecast
</bodyText>
<footnote confidence="0.99329">
4The Prodigy-METEO corpus is freely available here:
http://www.nltg.brighton.ac.uk/home/Anja.Belz
</footnote>
<table confidence="0.900885">
texts and wind data the systems were trained on,
e.g.:
Data: 1 SSW 16 20 - - 0600 2 SSE - - -
- NOTIME 3 VAR 04 08 - - 2400
Text: SSW 16-20 GRADUALLY BACKING SSE
THEN FALLING VARIABLE 4-8 BY
LATE EVENING
</table>
<bodyText confidence="0.999977380952381">
The input vector is a sequence of 7-tuples
(i, d, smin, smax, gmin, gmax, t� where i is the tu-
ple’s ID, d is the wind direction, smin and smax are
the minimum and maximum wind speeds, gmin
and gmax are the minimum and maximum gust
speeds, and t is a time stamp (indicating for what
time of the day the data is valid). The wind fore-
cast texts were taken from comprehensive mar-
itime weather forecasts produced by the profes-
sional meteorologists employed by a commercial
weather forecasting company for clients who run
offshore oilrigs.
There were two evaluation criteria; Clarity was
explained as indicating how understandable a fore-
cast was, and Readability as indicating how fluent
and readable it was. The experiment involved 22
forecast dates and outputs from the 10 systems de-
scribed in (Belz and Kow, 2009) (also included in
the corpus release) for those dates (as well as the
corresponding forecasts in the corpus) in the eval-
uation, i.e. a total of 242 forecast texts.
</bodyText>
<subsectionHeader confidence="0.987721">
4.2 Rating scale experiment
</subsectionHeader>
<bodyText confidence="0.9997568125">
We used the results of a previous experiment (Belz
and Kow, 2009) in which participants were asked
to rate forecast texts for Clarity and Readability,
each on a scale of 1–7.
The 22 participants were all University of
Brighton staff whose first language was English
and who had no experience of NLP. While ear-
lier experiments used master mariners as well as
lay-people in a similar evaluation (Belz and Re-
iter, 2006), these experiments also demonstrated
that the correlation between the ratings by expert
evaluators and lay-people is very strong in the ME-
TEO domain (Pearson’s r = 0.845).
We used a single 22 (evaluators) by 22 (test data
items) Latin Square; there were 484 trials in this
experiment.
</bodyText>
<subsectionHeader confidence="0.975132">
4.3 Preference judgement experiment
</subsectionHeader>
<bodyText confidence="0.99968125">
Our new experiment used our standardised pref-
erence strength sliders (bottom of Figure 2). We
recruited 22 different evaluators from among stu-
dents currently completing or recently having
</bodyText>
<table confidence="0.999872363636364">
Type Measure Clarity Readability
RSE F(10,473) 23.507** 24.351**
N sig diffs 24/55 23/55
K’s W .497** .533**
( Text F(21,462) 1.467 1.961** )
( Evaluator F(21,462) 4.832** 4.824** )
PJE F(10,1865) 45.081** 41.318**
N sig diffs 34/55 32/55
K’s W .626** .542**
( Text F(21,916) 1.436 1.573 )
( Evaluator F(21,921) .794 1.057 )
</table>
<tableCaption confidence="0.927739">
Table 3: METEO RSE/PJE: Results of analyses
looking at effect of System.
</tableCaption>
<bodyText confidence="0.97839325">
completed a linguistics-related degree at Oxford,
KCL, UCL, Sussex and Brighton.
We had at our disposal 11 METEO systems, so
there were (2) = 55 system combinations to eval-
uate on the 22 test data items. We decided on a
design of ten 11 × 11 Latin Squares to accommo-
date the 55 system pairings, so there was a total of
1210 trials in this experiment.
</bodyText>
<subsectionHeader confidence="0.944409">
4.4 Results and comparative analysis
</subsectionHeader>
<bodyText confidence="0.9999923">
Table 3 shows the same types of comparative mea-
sures as in the previous section. Note that the
self-consistency measure is missing, because for
METEO-PJE we do not have multiple scores for the
same pair of systems by the same evaluator.
For the METEO task, the relative amount vari-
ation in Clarity and Radability accounted for by
System is similar in the RSE, and again similar in
the PJE. However, F ratios and numbers of signifi-
cant differences found are higher in the latter than
in the RSE. The inter-evaluator agreement measure
also has higher values for both Clarity and Read-
ability in the PJE, although the difference is much
more pronounced in the case of Clarity.
In the RSE version, Evaluator has a small but
significant effect on both Clarity and Readability,
which disappears in the PJE version. Similarly, a
small effect of Text (date of weather forecast in
this data set) on Fluency in the RSE version disap-
pears in the PJE version.
</bodyText>
<sectionHeader confidence="0.9549" genericHeader="method">
5 RSE/PJE Pair 2: Descriptions of
furniture items and faces
</sectionHeader>
<subsectionHeader confidence="0.99347">
5.1 Data and generic design
</subsectionHeader>
<bodyText confidence="0.996123444444444">
In our third pair of evaluations, we used the sys-
tem outputs from the TUNA’09 shared-task com-
petition (Gatt et al., 2009).5 The TUNA data is
a collection of images of domain entities paired
with descriptions of entities. Each pair consists of
seven entity images where one is highlighted (by a
red box surrounding it), paired with a description
of the highlighted entity, e.g.:
the small blue fan
The descriptions were collected in an online ex-
periment with anonymous participants, and then
annotated for semantic content. In TUNA’09, the
task for participating systems was to generate de-
scriptions of the highlighted entities given seman-
tic representations of all seven entities. In the eval-
uation experiments, evaluators were asked to give
two ratings in answer to the following questions
(the first for Adequacy, the second for Fluency):
</bodyText>
<listItem confidence="0.786175125">
1. How clear is this description? Try to imagine someone
who could see the same grid with the same pictures, but
didn’t know which of the pictures was the target. How
easily would they be able to find it, based on the phrase
given?
2. How fluent is this description? Here your task is to
judge how well the phrase reads. Is it good, clear En-
glish?
</listItem>
<bodyText confidence="0.999977272727273">
Participants were shown a system output, to-
gether with its corresponding domain, displayed
as the set of corresponding images on the screen.
The intended (target) referent was highlighted by
a red frame surrounding it on the screen.
Following detailed instructions, subjects did
two practice examples, followed by the 112 test
items in random order.
There were 8 ‘systems’ in the TUNA evalua-
tions: the descriptions produced by the 6 systems
and two sets of humans-authored descriptions.
</bodyText>
<subsectionHeader confidence="0.997372">
5.2 Rating scale experiment
</subsectionHeader>
<bodyText confidence="0.921522666666667">
The rating scale experiment that was part of the
TUNA’09 evaluations had a design of fourteen 8 ×
8 squares, and a total of 896 trials.
</bodyText>
<footnote confidence="0.992950333333333">
5The TUNA’09 data and documen-
tation is available for download from
http://www.nltg.brighton.ac.uk/home/Anja.Belz
</footnote>
<table confidence="0.999906692307692">
Type Measure Adequacy Fluency
RSE F(7,888) 6.371** 17.207**
N sig diffs 7/28 15/28
K’s W .471** .676**
( Text1 F(111,784) 1.519** 1.091 )
( Text2 F(14,881) 8.992** 4.694** )
( Evaluator F(7,888) 13.136** 17.479** )
PJE F(7,6264) 46.503** 89.236**
N sig diffs 19/28 22/28
K’s W .573** .654**
( Text1 F(111,3024) .746 .921 )
( Text2 F(14,3121) .856 .853 )
( Evaluator F(27,3108) 1.3 1.638* )
</table>
<tableCaption confidence="0.938353">
Table 4: TUNA RSE/PJE: Results of analyses
looking at effect of System.
</tableCaption>
<bodyText confidence="0.998908642857143">
Subjects were asked to give their judgments for
Clarity and Fluency for each item by manipulating
a slider. The slider pointer was placed in the center
at the beginning of each trial. The position of the
slider selected by the subject mapped to an integer
value between 1 and 100. However, the scale was
not visible to participants who knew only that one
end of the scale corresponded to the worst possible
score and the opposite end to the best.
Eight native speakers of English were recruited
for this experiment from among post-graduate
students currently doing a Masters degree in a
linguistics-related subject at UCL, Sussex and
Brighton universities.
</bodyText>
<subsectionHeader confidence="0.996438">
5.3 Preference judgement experiment
</subsectionHeader>
<bodyText confidence="0.9999577">
Our new experiment used our standardised pref-
erence strength sliders (bottom of Figure 2). To
accommodate all pairwise comparisons as well as
all test set items, we used a design of four 28
× 28 Latin Squares, and recruited 28 evaluators
from among students currently completing, or re-
cently having completed, a degree in a linguistics-
related subject at Oxford, KCL, UCL, Sussex and
Brighton universities. There were 3,136 trials in
this version of the experiment.
</bodyText>
<subsectionHeader confidence="0.93096">
5.4 Results and comparative analysis
</subsectionHeader>
<bodyText confidence="0.999982744680851">
Table 4 shows the same measures as we reported
for the other two experiment pairs above. The
picture is somewhat similar in that the measures
have better values for PJE version except for the
inter-evaluator agreement (Kendall’s W) for Flu-
ency which is slightly higher for the RSE version.
For the TUNA dataset, we look at two Text factors.
Text2 refers to different sets of entities used in tri-
als; there are 15 different ones. Text1 refers to sets
of entities and their specific distribution over the
visual display grid in trials (see the figure in Sec-
tion 5.1); there are 112 different combinations of
entity set and grid locations.
The most striking aspect of the results in Table 4
is the effect of Evaluator in the RSE version which
appears to account for more variability in the data
even than System (relative to other factors). In
fact, in the case of Adequacy, even Text2 causes
more variation than System. In contrast, in the PJE
version, by far the biggest cause of variability is
System (for both criteria), and the F ratios for Text
and Evaluators are not significant except for Eval-
uator on Fluency (weakly significant at .05).
On the face of it, the variation between evalua-
tors in the RSE version as evidenced by the F ra-
tio is worrying. However, Kendall’s W shows that
in terms of mean rank, evaluators actually agreed
similarly well on Fluency in both RSE and PJE.
The F measure is based on mean scores whereas W
is based on mean score ranks, so there was more
variation in the absolute scores than in the ranks.
The reason is likely to be connected to the way
ratings were expressed by evaluators in the TUNA-
RSE experiment: recall that evaluators had the task
of moving the pointer to the place on the slider
bar that they felt corresponded to the quality of
text being evaluated. As no numbers were visi-
ble, the only information evaluators had to go on
was which was the ‘worse’ end and which was the
‘better’ end of the slider. It seems that different
evaluators used this evaluation tool in very differ-
ent ways (accounting for the variation in absolute
scores), but were able to apply their way of using
the tool reasonably consistently to different texts
(so that they were able to achieve reasonably good
agreement with the other evaluators in terms of
relative scores).
</bodyText>
<sectionHeader confidence="0.999876" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999993227272728">
We have looked at a range of aspects of evalu-
ation experiments: the effect of the factors Sys-
tem, Text and Evaluator on evaluation scores; the
number of significant differences between systems
found; self-consistency; and inter-evaluator agree-
ment (as described by F ratios obtained in one-way
ANOVAs for Evaluator, as well as by Kendall’s W
measuring inter-evaluator agreement).
The results are unambiguous as far as the
Clarity criterion (called Adequacy in TUNA) is
concerned: in all three experiment pairs, the
preference-strength judgement (PSE) version had
a greater effect of System, a smaller effect of
Text and Evaluator, more significant pairwise dif-
ferences, better inter-evaluator agreement, and
(where we were able to measure it) better self-
consistency.
The same is true for Readability in METEO and
Fluency in TUNA, in the latter case except for W
which is slightly lower in TUNA-P7E than TUNA-
RSE. However, Readability in GREC-NEG bucks
the trend: here, all measures are worse in the
P7E version than in the RSE version (although for
the W measures, the differences are small). Part
of the reason for this may be that in GREC-NEG
P7E each system was only compared to one sin-
gle other ‘system’, the (human-authored) original
Wikipedia texts.
If we see less effect of Clarity than of Fluency
in an experiment (as in GREC-NEG RSE and TUNA
RSE), then we might want to conclude that sys-
tems differed less in terms of Clarity than in terms
of Fluency. However, the real explanation may
be that evaluators simply found it harder to apply
the Clarity criterion than the Fluency criterion in
a given evaluation set-up. The fact that the differ-
ence in effect between Fluency and Clarity virtu-
ally disappears in GREC-NEG P7E makes this the
more likely explanation at least for the GREC-NEG
evaluations.
Parametric statistics are more powerful than
non-parametric ones because of the strong as-
sumptions they make about the nature of the data.
Roughly speaking, they are more likely to uncover
significant differences. Where the assumptions are
violated, the risk is that significance is overesti-
mated (the likelihood that null hypotheses are in-
correctly rejected increases). One might consider
using a slider mapping to a continuous scale in-
stead of a multiple-choice rating form in order to
overcome this problem, but the evidence from the
TUNA RSE evaluation appears to be that this can
result in unacceptably large variation in how indi-
vidual evaluators apply the scale to assign absolute
scores.
What seems to make the difference in terms of
ease of application of evaluation criteria and re-
duction of undesirable effects is not the use of con-
tinuous scales (as e.g. implemented in slider bars),
but the comparative element, where pairs of sys-
tems are compared and one is selected as better in
terms of a given criterion than the other.
It makes sense intuitively that deciding which
of two texts is clearer should be an easier task than
deciding whether a system is a 5, 4, 3 or 1 in terms
of its clarity. P7Es enabled evaluators to apply the
Clarity criterion to determine ranks more consis-
tently in all three experiment pairs.
However, it was an open question whether eval-
uators would also be able to express the strength
of their preference consistently. From the results
we report here it seems clear that this is indeed the
case: the System F ratios which look at absolute
scores (in the P7Es quantifying the strength of a
preference) are higher, and the Evaluator F ratios
lower, in all but one of the experiments.
While there were the same number of trials
in the two GREC-NEG evaluations, there were
2.5 times as many trials in METEO-P7E than in
METEO-RSE, and 3.5 times as many trials in
TUNA-P7E than in TUNA-RSE. The increase in tri-
als is counter-balanced to some extent by the fact
that evaluators tend to give relative judgements
far more quickly than absolute judgements, but
clearly there is an increase in cost associated with
including all system pairings in a P7E. If this cost
grows unacceptably large, a subset of systems has
to be selected as reference systems.
</bodyText>
<sectionHeader confidence="0.991631" genericHeader="conclusions">
7 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.999962846153846">
Our aim in the research presented in this paper
was to investigate how rating-scale experiments
compare to preference-strength judgement experi-
ments in the evaluation of automatically generated
language. We find that preference-strength judge-
ment evaluations generally have a greater rela-
tive effect of System (the factor actually under in-
vestigation), a smaller relative effect of Text and
Evaluator (whose effect should be small), a larger
number of significant pairwise differences be-
tween systems, better inter-evaluator agreement,
and (where we were able to measure it) better eval-
uator self-consistency.
</bodyText>
<sectionHeader confidence="0.999619" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999822510204082">
Anja Belz and Eric Kow. 2009. System building cost
vs. output quality in data-to-text generation. In Pro-
ceedings of the 12th European Workshop on Natural
Language Generation, pages 16–24.
A. Belz and E. Reiter. 2006. Comparing automatic and
human evaluation of NLG systems. In Proceedings
ofEACL’06, pages 313–320.
Anja Belz, Eric Kow, and Jette Viethen. 2009. The
GREC named entity generation challenge 2009:
Overview and evaluation results. In Proceedings of
the ACL-IJCNLP’09 Workshop on Language Gen-
eration and Summarisation (UCNLG+Sum), pages
88–98.
A. Belz. 2009. Prodigy-METEO: Pre-alpha release
notes (Nov 2009). Technical Report NLTG-09-01,
Natural Language Technology Group, CMIS, Uni-
versity of Brighton.
Bernard Choi and Anita Pak. 2005. A catalog of bi-
ases in questionnaires. Preventing Chronic Disease,
2(1):A13.
A. Gatt, A. Belz, and E. Kow. 2009. The TUNA Chal-
lenge 2009: Overview and evaluation results. In
Proceedings of the 12th European Workshop on Nat-
ural Language Generation (ENLG’09), pages 198–
206.
C.-Y. Lin and F. J. Och. 2004. ORANGE: A method
for evaluating automatic evaluation metrics for ma-
chine translation. In Proceedings of the 20th Inter-
national Conference on Computational Linguistics
(COLING’04), pages 501–507, Geneva.
E. Reiter, S. Sripada, J. Hunter, and J. Yu. 2005.
Choosing words in computer-generated weather
forecasts. Arti�cial Intelligence, 167:137–169.
Sidney Siegel. 1957. Non-parametric statistics. The
American Statistician, 11(3):13–19.
S. Sripada, E. Reiter, J. Hunter, and J. Yu. 2002.
SUMTIME-METEO: A parallel corpus of naturally
occurring forecast texts and weather data. Technical
Report AUCS/TR0201, Computing Science Depart-
ment, University of Aberdeen.
H. Trang Dang. 2006. DUC 2005: Evaluation
of question-focused summarization systems. In
Proceedings of the COLING-ACL’06 Workshop on
Task-Focused Summarization and Question Answer-
ing, pages 48–55.
J. Turian, L. Shen, and I. D. Melamed. 2003. Evalu-
ation of machine translation and its evaluation. In
Proceedings of MT Summmit IX, pages 386–393,
New Orleans.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.498890">
<title confidence="0.9986915">Comparing Rating Scales and Preference in Language Evaluation</title>
<author confidence="0.994802">Anja Belz Eric Kow</author>
<affiliation confidence="0.975659">Natural Language Technology School of Computing, Mathematical and Information University of</affiliation>
<address confidence="0.534543">Brighton BN2 4GJ,</address>
<abstract confidence="0.999864153846154">Rating-scale evaluations are common in but are problematic for a range of reasons, e.g. they can be unintuitive for evaluators, inter-evaluator agreement and self-consistency tend to be low, and the parametric statistics commonly applied to the results are not generally considered appropriate for ordinal data. In this paper, we compare rating scales with an alternative evaluation paradigm, preferencejudgement experiments where evaluators have the simpler task of deciding which of two texts is better in terms of a given quality criterion. We present three pairs of evaluation experiments assessing text fluency and clarity for different data sets, where one of each pair of experiments is a rating-scale exand the other is a We find of the experiments have better evaluator self-consistency and interevaluator agreement, and a larger proportion of variation accounted for by system differences, resulting in a larger number of significant differences being found.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Anja Belz</author>
<author>Eric Kow</author>
</authors>
<title>System building cost vs. output quality in data-to-text generation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th European Workshop on Natural Language Generation,</booktitle>
<pages>16--24</pages>
<contexts>
<context position="18387" citStr="Belz and Kow, 2009" startWordPosition="3013" endWordPosition="3016">d gmax are the minimum and maximum gust speeds, and t is a time stamp (indicating for what time of the day the data is valid). The wind forecast texts were taken from comprehensive maritime weather forecasts produced by the professional meteorologists employed by a commercial weather forecasting company for clients who run offshore oilrigs. There were two evaluation criteria; Clarity was explained as indicating how understandable a forecast was, and Readability as indicating how fluent and readable it was. The experiment involved 22 forecast dates and outputs from the 10 systems described in (Belz and Kow, 2009) (also included in the corpus release) for those dates (as well as the corresponding forecasts in the corpus) in the evaluation, i.e. a total of 242 forecast texts. 4.2 Rating scale experiment We used the results of a previous experiment (Belz and Kow, 2009) in which participants were asked to rate forecast texts for Clarity and Readability, each on a scale of 1–7. The 22 participants were all University of Brighton staff whose first language was English and who had no experience of NLP. While earlier experiments used master mariners as well as lay-people in a similar evaluation (Belz and Reit</context>
</contexts>
<marker>Belz, Kow, 2009</marker>
<rawString>Anja Belz and Eric Kow. 2009. System building cost vs. output quality in data-to-text generation. In Proceedings of the 12th European Workshop on Natural Language Generation, pages 16–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Belz</author>
<author>E Reiter</author>
</authors>
<title>Comparing automatic and human evaluation of NLG systems.</title>
<date>2006</date>
<booktitle>In Proceedings ofEACL’06,</booktitle>
<pages>313--320</pages>
<contexts>
<context position="3002" citStr="Belz and Reiter, 2006" startWordPosition="463" endWordPosition="466">ave different tendencies in using rating scales, e.g. what is known as ‘end-aversion’ tendency where certain individuals tend to stay away from the extreme ends of scales; other examples are positive skew and acquiescence bias, where individuals make disproportionately many positive or agreeing judgements; see e.g. Choi and Pak, (2005). It is not surprising then that stable averages of quality judgements, let alone high levels of agreement, are hard to achieve, as has been observed for MT (Turian et al., 2003; Lin and Och, 2004), text summarisation (Trang Dang, 2006), and language generation (Belz and Reiter, 2006). It has even been demonstrated that increasing the number of evaluators and/or data can have no stabilising effect at all on means (DUC literature). The result of a rating scale experiment is ordinal data (sets of scores selected from the discrete rating scale). The means-based ranks and statistical significance tests that are commonly presented with the results of RSEs are not generally considered appropriate for ordinal data in the statistics literature (Siegel, 1957). At the least, “a test on the means imposes the requirement that the measures must be additive, i.e. numerical” (Siegel, 195</context>
<context position="18996" citStr="Belz and Reiter, 2006" startWordPosition="3117" endWordPosition="3121">nd Kow, 2009) (also included in the corpus release) for those dates (as well as the corresponding forecasts in the corpus) in the evaluation, i.e. a total of 242 forecast texts. 4.2 Rating scale experiment We used the results of a previous experiment (Belz and Kow, 2009) in which participants were asked to rate forecast texts for Clarity and Readability, each on a scale of 1–7. The 22 participants were all University of Brighton staff whose first language was English and who had no experience of NLP. While earlier experiments used master mariners as well as lay-people in a similar evaluation (Belz and Reiter, 2006), these experiments also demonstrated that the correlation between the ratings by expert evaluators and lay-people is very strong in the METEO domain (Pearson’s r = 0.845). We used a single 22 (evaluators) by 22 (test data items) Latin Square; there were 484 trials in this experiment. 4.3 Preference judgement experiment Our new experiment used our standardised preference strength sliders (bottom of Figure 2). We recruited 22 different evaluators from among students currently completing or recently having Type Measure Clarity Readability RSE F(10,473) 23.507** 24.351** N sig diffs 24/55 23/55 K</context>
</contexts>
<marker>Belz, Reiter, 2006</marker>
<rawString>A. Belz and E. Reiter. 2006. Comparing automatic and human evaluation of NLG systems. In Proceedings ofEACL’06, pages 313–320.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anja Belz</author>
<author>Eric Kow</author>
<author>Jette Viethen</author>
</authors>
<title>The GREC named entity generation challenge 2009: Overview and evaluation results.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP’09 Workshop on Language Generation and Summarisation (UCNLG+Sum),</booktitle>
<pages>88--98</pages>
<contexts>
<context position="10912" citStr="Belz et al., 2009" startWordPosition="1752" endWordPosition="1755">e the pointer in the middle (i.e. if they had no preference for either of the two texts), evaluators had to check a box to confirm their rating (to avoid evaluators accidentally not rating a text and leaving the pointer in the default position). 3 GREC-NEG RSE/PJE: Named entity reference chains 2The GREC-NEG data and documentation is available for download from http://www.nltg.brighton.ac.uk/home/Anja.Belz CASE=&amp;quot;plain&amp;quot;&gt;Sir Alexander Fleming&lt;/REFEX&gt; &lt;/REF&gt; (6 August 1881 - 11 March 1955) was a Scottish biologist and pharmacologist. This data was used in the GREC-NEG’09 shared-task competition (Belz et al., 2009), where the task was to create systems which automatically select suitable REs for all references to all person entities in a text. The evaluation experiments use Clarity and Fluency as quality criteria which were explained in the introduction as follows (the wording of the first is from DUC): 1. Referential Clarity: It should be easy to identify who the referring expressions are referring to. If a person is mentioned, it should be clear what their role in the story is. So, a reference would be unclear if a person is referenced, but their identity or relation to the story remains unclear. 2. F</context>
</contexts>
<marker>Belz, Kow, Viethen, 2009</marker>
<rawString>Anja Belz, Eric Kow, and Jette Viethen. 2009. The GREC named entity generation challenge 2009: Overview and evaluation results. In Proceedings of the ACL-IJCNLP’09 Workshop on Language Generation and Summarisation (UCNLG+Sum), pages 88–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Belz</author>
</authors>
<title>Prodigy-METEO: Pre-alpha release notes</title>
<date>2009</date>
<tech>Technical Report NLTG-09-01,</tech>
<institution>Natural Language Technology Group, CMIS, University of Brighton.</institution>
<contexts>
<context position="17163" citStr="Belz, 2009" startWordPosition="2801" endWordPosition="2802">looks instead as though the evaluators found it harder to apply the Clarity criterion in GREC-NEG RSE than Fluency in GRECNEG RSE, and than Clarity in GREC-NEG PJE. One way of interpreting this is that it is possible to achieve the same good levels of inter-evaluator and intra-evaluator variation for the Clarity criterion as for Fluency (both as defined and applied within the context of this specific experiment), and that it is therefore worrying that the RSE version does not achieve it. 4 METEO RSE/PJE: Weather forecasts 4.1 Data Our second pair of evaluations used the ProdigyMETEO4 version (Belz, 2009) of the SUMTIMEMETEO corpus (Sripada et al., 2002) which contains system outputs and the pairs of wind forecast 4The Prodigy-METEO corpus is freely available here: http://www.nltg.brighton.ac.uk/home/Anja.Belz texts and wind data the systems were trained on, e.g.: Data: 1 SSW 16 20 - - 0600 2 SSE - - - - NOTIME 3 VAR 04 08 - - 2400 Text: SSW 16-20 GRADUALLY BACKING SSE THEN FALLING VARIABLE 4-8 BY LATE EVENING The input vector is a sequence of 7-tuples (i, d, smin, smax, gmin, gmax, t� where i is the tuple’s ID, d is the wind direction, smin and smax are the minimum and maximum wind speeds, gm</context>
</contexts>
<marker>Belz, 2009</marker>
<rawString>A. Belz. 2009. Prodigy-METEO: Pre-alpha release notes (Nov 2009). Technical Report NLTG-09-01, Natural Language Technology Group, CMIS, University of Brighton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Choi</author>
<author>Anita Pak</author>
</authors>
<title>A catalog of biases in questionnaires. Preventing Chronic Disease,</title>
<date>2005</date>
<contexts>
<context position="2717" citStr="Choi and Pak, (2005)" startWordPosition="415" endWordPosition="418">quality in isolation, in absolute terms. Such rating scales are not very intuitive to use; deciding whether a text deserves a 5, a 4 or a 3 etc. can be difficult. Furthermore, evaluators may ascribe different meanings to scores and the distances between them. Individual evaluators have different tendencies in using rating scales, e.g. what is known as ‘end-aversion’ tendency where certain individuals tend to stay away from the extreme ends of scales; other examples are positive skew and acquiescence bias, where individuals make disproportionately many positive or agreeing judgements; see e.g. Choi and Pak, (2005). It is not surprising then that stable averages of quality judgements, let alone high levels of agreement, are hard to achieve, as has been observed for MT (Turian et al., 2003; Lin and Och, 2004), text summarisation (Trang Dang, 2006), and language generation (Belz and Reiter, 2006). It has even been demonstrated that increasing the number of evaluators and/or data can have no stabilising effect at all on means (DUC literature). The result of a rating scale experiment is ordinal data (sets of scores selected from the discrete rating scale). The means-based ranks and statistical significance </context>
</contexts>
<marker>Choi, Pak, 2005</marker>
<rawString>Bernard Choi and Anita Pak. 2005. A catalog of biases in questionnaires. Preventing Chronic Disease, 2(1):A13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gatt</author>
<author>A Belz</author>
<author>E Kow</author>
</authors>
<title>The TUNA Challenge 2009: Overview and evaluation results.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th European Workshop on Natural Language Generation (ENLG’09),</booktitle>
<pages>198--206</pages>
<contexts>
<context position="21448" citStr="Gatt et al., 2009" startWordPosition="3536" endWordPosition="3539">has higher values for both Clarity and Readability in the PJE, although the difference is much more pronounced in the case of Clarity. In the RSE version, Evaluator has a small but significant effect on both Clarity and Readability, which disappears in the PJE version. Similarly, a small effect of Text (date of weather forecast in this data set) on Fluency in the RSE version disappears in the PJE version. 5 RSE/PJE Pair 2: Descriptions of furniture items and faces 5.1 Data and generic design In our third pair of evaluations, we used the system outputs from the TUNA’09 shared-task competition (Gatt et al., 2009).5 The TUNA data is a collection of images of domain entities paired with descriptions of entities. Each pair consists of seven entity images where one is highlighted (by a red box surrounding it), paired with a description of the highlighted entity, e.g.: the small blue fan The descriptions were collected in an online experiment with anonymous participants, and then annotated for semantic content. In TUNA’09, the task for participating systems was to generate descriptions of the highlighted entities given semantic representations of all seven entities. In the evaluation experiments, evaluator</context>
</contexts>
<marker>Gatt, Belz, Kow, 2009</marker>
<rawString>A. Gatt, A. Belz, and E. Kow. 2009. The TUNA Challenge 2009: Overview and evaluation results. In Proceedings of the 12th European Workshop on Natural Language Generation (ENLG’09), pages 198– 206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C-Y Lin</author>
<author>F J Och</author>
</authors>
<title>ORANGE: A method for evaluating automatic evaluation metrics for machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics (COLING’04),</booktitle>
<pages>501--507</pages>
<location>Geneva.</location>
<contexts>
<context position="2914" citStr="Lin and Och, 2004" startWordPosition="451" endWordPosition="454">different meanings to scores and the distances between them. Individual evaluators have different tendencies in using rating scales, e.g. what is known as ‘end-aversion’ tendency where certain individuals tend to stay away from the extreme ends of scales; other examples are positive skew and acquiescence bias, where individuals make disproportionately many positive or agreeing judgements; see e.g. Choi and Pak, (2005). It is not surprising then that stable averages of quality judgements, let alone high levels of agreement, are hard to achieve, as has been observed for MT (Turian et al., 2003; Lin and Och, 2004), text summarisation (Trang Dang, 2006), and language generation (Belz and Reiter, 2006). It has even been demonstrated that increasing the number of evaluators and/or data can have no stabilising effect at all on means (DUC literature). The result of a rating scale experiment is ordinal data (sets of scores selected from the discrete rating scale). The means-based ranks and statistical significance tests that are commonly presented with the results of RSEs are not generally considered appropriate for ordinal data in the statistics literature (Siegel, 1957). At the least, “a test on the means </context>
</contexts>
<marker>Lin, Och, 2004</marker>
<rawString>C.-Y. Lin and F. J. Och. 2004. ORANGE: A method for evaluating automatic evaluation metrics for machine translation. In Proceedings of the 20th International Conference on Computational Linguistics (COLING’04), pages 501–507, Geneva.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Reiter</author>
<author>S Sripada</author>
<author>J Hunter</author>
<author>J Yu</author>
</authors>
<title>Choosing words in computer-generated weather forecasts.</title>
<date>2005</date>
<journal>Arti�cial Intelligence,</journal>
<pages>167--137</pages>
<contexts>
<context position="4078" citStr="Reiter et al., 2005" startWordPosition="630" endWordPosition="633">ture (Siegel, 1957). At the least, “a test on the means imposes the requirement that the measures must be additive, i.e. numerical” (Siegel, 1957, p. 14). Parametric statistics are more powerful than non-parametric alternatives, because they make a number of strong assumptions (including that the data is numerical). If the assumptions are violated then the risks is that the significance of results is overestimated. In this paper we explore an alternative evaluation paradigm, Preference-strength Judgement Experiments (PJEs). Binary preference judgements have been used in NLP system evaluation (Reiter et al., 2005), but to our knowledge this is the first systematic investigation of preference-strength judgements where evaluators express, in addition to their preference (which system do you prefer?), also the strength of their preference (how strongly do you prefer the system you prefer?). It seems intuitively convincing that it should be easier to decide which of two texts is clearer than to decide whether a text’s clarity deserves a 1, 2, 3, 4 or 5. However, it is less clear whether evaluators are also able to express the strength of their preference in a consistent fashion, resulting not only in good </context>
</contexts>
<marker>Reiter, Sripada, Hunter, Yu, 2005</marker>
<rawString>E. Reiter, S. Sripada, J. Hunter, and J. Yu. 2005. Choosing words in computer-generated weather forecasts. Arti�cial Intelligence, 167:137–169.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sidney Siegel</author>
</authors>
<title>Non-parametric statistics.</title>
<date>1957</date>
<journal>The American Statistician,</journal>
<volume>11</volume>
<issue>3</issue>
<contexts>
<context position="3477" citStr="Siegel, 1957" startWordPosition="542" endWordPosition="543">ed for MT (Turian et al., 2003; Lin and Och, 2004), text summarisation (Trang Dang, 2006), and language generation (Belz and Reiter, 2006). It has even been demonstrated that increasing the number of evaluators and/or data can have no stabilising effect at all on means (DUC literature). The result of a rating scale experiment is ordinal data (sets of scores selected from the discrete rating scale). The means-based ranks and statistical significance tests that are commonly presented with the results of RSEs are not generally considered appropriate for ordinal data in the statistics literature (Siegel, 1957). At the least, “a test on the means imposes the requirement that the measures must be additive, i.e. numerical” (Siegel, 1957, p. 14). Parametric statistics are more powerful than non-parametric alternatives, because they make a number of strong assumptions (including that the data is numerical). If the assumptions are violated then the risks is that the significance of results is overestimated. In this paper we explore an alternative evaluation paradigm, Preference-strength Judgement Experiments (PJEs). Binary preference judgements have been used in NLP system evaluation (Reiter et al., 2005</context>
</contexts>
<marker>Siegel, 1957</marker>
<rawString>Sidney Siegel. 1957. Non-parametric statistics. The American Statistician, 11(3):13–19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sripada</author>
<author>E Reiter</author>
<author>J Hunter</author>
<author>J Yu</author>
</authors>
<title>SUMTIME-METEO: A parallel corpus of naturally occurring forecast texts and weather data.</title>
<date>2002</date>
<tech>Technical Report AUCS/TR0201,</tech>
<institution>Computing Science Department, University of Aberdeen.</institution>
<contexts>
<context position="17213" citStr="Sripada et al., 2002" startWordPosition="2808" endWordPosition="2811">ound it harder to apply the Clarity criterion in GREC-NEG RSE than Fluency in GRECNEG RSE, and than Clarity in GREC-NEG PJE. One way of interpreting this is that it is possible to achieve the same good levels of inter-evaluator and intra-evaluator variation for the Clarity criterion as for Fluency (both as defined and applied within the context of this specific experiment), and that it is therefore worrying that the RSE version does not achieve it. 4 METEO RSE/PJE: Weather forecasts 4.1 Data Our second pair of evaluations used the ProdigyMETEO4 version (Belz, 2009) of the SUMTIMEMETEO corpus (Sripada et al., 2002) which contains system outputs and the pairs of wind forecast 4The Prodigy-METEO corpus is freely available here: http://www.nltg.brighton.ac.uk/home/Anja.Belz texts and wind data the systems were trained on, e.g.: Data: 1 SSW 16 20 - - 0600 2 SSE - - - - NOTIME 3 VAR 04 08 - - 2400 Text: SSW 16-20 GRADUALLY BACKING SSE THEN FALLING VARIABLE 4-8 BY LATE EVENING The input vector is a sequence of 7-tuples (i, d, smin, smax, gmin, gmax, t� where i is the tuple’s ID, d is the wind direction, smin and smax are the minimum and maximum wind speeds, gmin and gmax are the minimum and maximum gust speed</context>
</contexts>
<marker>Sripada, Reiter, Hunter, Yu, 2002</marker>
<rawString>S. Sripada, E. Reiter, J. Hunter, and J. Yu. 2002. SUMTIME-METEO: A parallel corpus of naturally occurring forecast texts and weather data. Technical Report AUCS/TR0201, Computing Science Department, University of Aberdeen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Trang Dang</author>
</authors>
<title>DUC 2005: Evaluation of question-focused summarization systems.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING-ACL’06 Workshop on Task-Focused Summarization and Question Answering,</booktitle>
<pages>48--55</pages>
<contexts>
<context position="2953" citStr="Dang, 2006" startWordPosition="458" endWordPosition="459"> between them. Individual evaluators have different tendencies in using rating scales, e.g. what is known as ‘end-aversion’ tendency where certain individuals tend to stay away from the extreme ends of scales; other examples are positive skew and acquiescence bias, where individuals make disproportionately many positive or agreeing judgements; see e.g. Choi and Pak, (2005). It is not surprising then that stable averages of quality judgements, let alone high levels of agreement, are hard to achieve, as has been observed for MT (Turian et al., 2003; Lin and Och, 2004), text summarisation (Trang Dang, 2006), and language generation (Belz and Reiter, 2006). It has even been demonstrated that increasing the number of evaluators and/or data can have no stabilising effect at all on means (DUC literature). The result of a rating scale experiment is ordinal data (sets of scores selected from the discrete rating scale). The means-based ranks and statistical significance tests that are commonly presented with the results of RSEs are not generally considered appropriate for ordinal data in the statistics literature (Siegel, 1957). At the least, “a test on the means imposes the requirement that the measur</context>
</contexts>
<marker>Dang, 2006</marker>
<rawString>H. Trang Dang. 2006. DUC 2005: Evaluation of question-focused summarization systems. In Proceedings of the COLING-ACL’06 Workshop on Task-Focused Summarization and Question Answering, pages 48–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Turian</author>
<author>L Shen</author>
<author>I D Melamed</author>
</authors>
<title>Evaluation of machine translation and its evaluation.</title>
<date>2003</date>
<booktitle>In Proceedings of MT Summmit IX,</booktitle>
<pages>386--393</pages>
<location>New Orleans.</location>
<contexts>
<context position="2894" citStr="Turian et al., 2003" startWordPosition="447" endWordPosition="450">aluators may ascribe different meanings to scores and the distances between them. Individual evaluators have different tendencies in using rating scales, e.g. what is known as ‘end-aversion’ tendency where certain individuals tend to stay away from the extreme ends of scales; other examples are positive skew and acquiescence bias, where individuals make disproportionately many positive or agreeing judgements; see e.g. Choi and Pak, (2005). It is not surprising then that stable averages of quality judgements, let alone high levels of agreement, are hard to achieve, as has been observed for MT (Turian et al., 2003; Lin and Och, 2004), text summarisation (Trang Dang, 2006), and language generation (Belz and Reiter, 2006). It has even been demonstrated that increasing the number of evaluators and/or data can have no stabilising effect at all on means (DUC literature). The result of a rating scale experiment is ordinal data (sets of scores selected from the discrete rating scale). The means-based ranks and statistical significance tests that are commonly presented with the results of RSEs are not generally considered appropriate for ordinal data in the statistics literature (Siegel, 1957). At the least, “</context>
</contexts>
<marker>Turian, Shen, Melamed, 2003</marker>
<rawString>J. Turian, L. Shen, and I. D. Melamed. 2003. Evaluation of machine translation and its evaluation. In Proceedings of MT Summmit IX, pages 386–393, New Orleans.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>