<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000122">
<title confidence="0.9997065">
Learning Probabilistic Paradigms for Morphology
in a Latent Class Model
</title>
<author confidence="0.998882">
Erwin Chan
</author>
<affiliation confidence="0.998696">
Dept. of Computer and Information Science
University of Pennsylvania
</affiliation>
<address confidence="0.866338">
Philadelphia, PA 19104
</address>
<email confidence="0.999319">
echan3@seas.upenn.edu
</email>
<sectionHeader confidence="0.998602" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999972631578947">
This paper introduces the probabilistic
paradigm, a probabilistic, declarative
model of morphological structure. We de-
scribe an algorithm that recursively ap-
plies Latent Dirichlet Allocation with an
orthogonality constraint to discover mor-
phological paradigms as the latent classes
within a suffix-stem matrix. We apply the
algorithm to data preprocessed in several
different ways, and show that when suf-
fixes are distinguished for part of speech
and allomorphs or gender/conjugational
variants are merged, the model is able to
correctly learn morphological paradigms
for English and Spanish. We compare our
system with Linguistica (Goldsmith
2001), and discuss the advantages of the
probabilistic paradigm over Linguistica’s
signature representation.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999957723404256">
In recent years researchers have addressed the task
of unsupervised learning of declarative representa-
tions of morphological structure. These models
include the signature of (Goldsmith 2001), the con-
flation set of (Schone and Jurafsky 2001), the
paradigm of (Brent et. al. 2002), and the inflec-
tional class of (Monson 2004). While these repre-
sentations group morphologically related words in
systematic ways, they are rather different from the
paradigm, the representation of morphology in tra-
ditional grammars. A paradigm lists the prototypi-
cal morphological properties of lexemes belonging
to a particular part of speech (POS) category; for
example, a paradigm for regular English verbs
would include the suffixes {$,ed$,ing$,s$}1.
Hand-built computational implementations of
paradigms as inheritance hierarchies include
DATR (Evans and Gazdar 1996) and Functional
Morphology (Forsberg and Ranta 2004). The two
principal ways in which learned models have dif-
fered from paradigms are that: 1) they do not have
POS types, and 2) they are not abstractions that
generalize beyond the words of the input corpus.
There are important reasons for learning a
POS-associated, paradigmatic representation of
morphology. Currently, the dominant technology
for morphological analysis involves mapping be-
tween inflected and base of forms of words with
finite-state transducers (FSTs), a procedural model
of morphological relations. Rewrite rules are hand-
crafted and compiled into FSTs, and it would be
beneficial if these rules could be learned automati-
cally. One line of research in computational mor-
phology has been directed towards learning finite-
state mapping rules from some sort of paradig-
matic structure, where all morphological forms and
POS types are presumed known for a set of lex-
emes (Clark 2001, Kazakov and Manandhar 2001,
Oflazer et. al. 2001, Zajac 2001, Albright 2002).
This can be accomplished by first deciding on a
base form, then learning rules to convert other
forms of the paradigm into this base form. If one
could develop an unsupervised algorithm for learn-
ing paradigms, it could serve as the input to rule-
learning procedures, effectively leading to an en-
tirely unsupervised system for learning FSTs from
raw data. This is our long-term goal.
</bodyText>
<equation confidence="0.505343">
1 $ is the null suffix.
</equation>
<page confidence="0.985599">
69
</page>
<note confidence="0.862911">
Proceedings of the Eighth Meeting of the ACL Special Interest Group on Computational Phonology at HLT-NAACL 2006, pages 69–78,
New York City, USA, June 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.999970432835821">
An alternative approach is to skip the paradigm
formulation step and construct a procedural model
directly from raw data. (Yarowsky and Wicen-
towski 2000) bootstrap inflected and base forms
directly from raw data and learn mappings between
them. Their results are quite successful, but the
morphological information they learn is not struc-
tured as clearly as a paradigmatic model. (Freitag
2005) constructs a morphological automaton,
where nodes are clustered word types and arcs are
suffixation rules.
This paper addresses the problem of finding an
organization of stems and suffixes as probabilistic
paradigms (section 2), a model of morphology
closer to linguistic notion of paradigm than previ-
ously proposed models. We encode the morpho-
logical structure of a language in a matrix
containing frequencies of words, and formulate the
problem of learning paradigms as one of finding
latent classes within the matrix. We present a re-
cursive LDA, a learning algorithm based on Latent
Dirichlet Allocation (section 3), and show that un-
der certain conditions (section 5), it can correctly
learn morphological paradigms for English and
Spanish. In section 6, we compare the probabilistic
paradigm to the signature model of (Goldsmith
2001). In section 7, we sketch some ideas for how
to make our system more unsupervised and more
linguistically adequate.
We assume a model of morphology where
each word is the concatenation of a stem and a sin-
gle suffix representing all of the word&apos;s morpho-
logical and POS properties. Although this is a very
simplistic view of morphology, there are many
hitherto unresolved computational issues for learn-
ing even this basic model, and we consider it nec-
essary to address these issues before developing
more sophisticated models. For a stem/suffix rep-
resentation, the task of learning a paradigm from
raw data involves proposing suffixes and stems,
proposing segmentations, and systematically orga-
nizing stems and suffixes into classes. One diffi-
culty is suffix allomorphy: a suffix has multiple
forms depending on its phonological environment
(e.g. s$/es$). Another problem is suffix cate-
gorial ambiguity (s$ is ambiguous for noun and
verb uses). Finally, lexemes appear in only a subset
of their potential forms, due to sparse data. An un-
supervised learner needs to be able to handle all of
these difficulties in order to discover abstract para-
digmatic classes.
In this paper, we are primarily interested in
how the co-occurrence of stems and suffixes in a
corpus leads them to be organized into paradigms.
We use data preprocessed with correct segmenta-
tions of words into stems and suffixes, in order to
focus on the issue of determining what additional
knowledge is needed. We demonstrate that para-
digms for English and Spanish can be successfully
learned when tokens have been assigned POS tags
and allomorphs or gender/conjugational variants
are given a common representation. Our learning
algorithm is not supervised since the target concept
of gold standard &amp;quot;input&amp;quot; POS category of stems is
not known, but rather it is an unsupervised algo-
rithm that relies on preprocessed data for optimal
performance.
</bodyText>
<sectionHeader confidence="0.985296" genericHeader="introduction">
2 The Probabilistic Paradigm
</sectionHeader>
<bodyText confidence="0.999982875">
We introduce the probabilistic paradigm, a prob-
abilistic, declarative model of regular morphology.
The probabilistic paradigm model consists of three
matrices: the data matrix D, the morphological
probabilities matrix M, and the lexical probabilities
matrix L. Let m be the number of stems, n the
number of stems, and p the number of paradigms.
The D matrix encodes the joint distribution of lexi-
cal and morphological information in a corpus. It is
of size m x n, and each cell contains the fre-
quency of the word formed by concatenating the
appropriate stem and suffix. The M matrix is of
size m x p, and each column contains the condi-
tional probabilities of each suffix given a para-
digm. The L matrix is of size p x n, and contains
the conditional probabilities of each paradigm
given a stem. Each suffix should belong to exactly
one paradigm, and the suffixes of a particular
paradigm should be conditionally independent.
Each column of the M matrix defines a canonical
paradigm, a set of suffixes that attach to stems as-
sociated with that paradigm. A lexical paradigm is
the full set of word forms for a particular stem, and
is an instantiation of the canonical paradigm for a
particular stem.
The probabilistic paradigm is not well-
developed as the usual notion of &amp;quot;paradigm&amp;quot; in
linguistics. First, the system employs no labels
such as &amp;quot;noun&amp;quot;, &amp;quot;plural&amp;quot;, &amp;quot;past&amp;quot;, etc. Second, prob-
abilistic paradigms have only a top-level categori-
zation; induced “verb” paradigms, for example, are
not substructured into different tenses or conjuga-
</bodyText>
<page confidence="0.996148">
70
</page>
<bodyText confidence="0.9999529">
tions. Third, we do not distinguish between inflec-
tional and derivational morphology; traditional
grammars place derived forms in separate lexical
paradigms. Fourth, we do not handle syncretism,
where one suffix belongs in multiple slots of the
paradigm. Fifth, we do not yet not handle irregular
and sub-regular forms. Despite these drawbacks,
our paradigms have an important advantage over
traditional paradigms, in being probabilistic and
therefore able to model language usage.
</bodyText>
<sectionHeader confidence="0.8727735" genericHeader="method">
3 Learning the probabilistic paradigm in a
latent class model
</sectionHeader>
<bodyText confidence="0.9999068125">
We learn the parameters of the probabilistic para-
digm model by applying a dimensionality reduc-
tion algorithm to the D matrix, in order to produce
the M and L matrices. This reduces the size of the
representation from m*n to m*p + p*n. The main
idea is to discover the latent classes (paradigms)
which represent the underlying structure of the in-
put matrix. This handles two important problems:
1) that words occur in a subset of their possible
morphological forms in a corpus, and 2) that the
words formed from a particular stem can belong to
multiple POS categories. The second problem can
be quantified as follows: in our English data,
14.3% of types occur with multiple open-class base
POS categories, accounting for 56.5% of tokens;
for Spanish, 13.7% of types, 37.8% of tokens.
</bodyText>
<subsectionHeader confidence="0.991422">
3.1 LDA model for morphology
</subsectionHeader>
<bodyText confidence="0.999986419354839">
The dimensionality reduction algorithm that we
employ is Latent Dirichlet Allocation (LDA) (Blei
et. al. 2003). LDA is a generative probabilistic
model for discrete data. For the application of topic
discovery within a corpus of documents, a docu-
ment consists of a mixture of underlying topics,
and each topic consists of a probability distribution
over the vocabulary. The topic proportions are
drawn from a Dirichlet distribution, and the words
are drawn from a multinomial over the topic. Prob-
ability distributions of documents and words are
conditionally independent of topics. LDA produces
two non-negative parameter matrices, Gamma and
Beta: Gamma is the matrix of Dirichlet posteriors,
encoding the distribution of documents and topics;
Beta encodes the distribution of words and topics.
The mapping of the data structures of LDA to
the probabilistic paradigm is as follows. The
document-word matrix is analogous to the suffix-
stem D matrix. For morphology, a &amp;quot;document&amp;quot; is a
multiset of tokens in a corpus, such that each of
those tokens decomposes into a stem and a speci-
fied suffix. Different underlying canonical para-
digms (&amp;quot;topics&amp;quot;) can be associated with suffixes,
and each canonical paradigm allows a set of stems
(&amp;quot;words&amp;quot;). For a suffix-stem (&amp;quot;document-word&amp;quot;)
matrix of size m x n and k latent classes, the
Gamma matrix is of size m x k, and the Beta ma-
trix is of size k x n. The Gamma matrix, normal-
ized by column, is the M matrix, and the Beta
matrix, normalized by row, is the L matrix.
</bodyText>
<subsectionHeader confidence="0.997282">
3.2 Recursive LDA
</subsectionHeader>
<bodyText confidence="0.999996257142857">
One standard issue in using these types of algo-
rithms is selecting the number of classes. To deal
with this, we have formulated a recursive wrapper
algorithm for LDA that accomplishes a divisive
clustering of suffixes. LDA is run at each stage to
find the local Gamma and Beta matrices. To split
the suffixes into two classes, we assign each suffix
to the class for which its probability is greater, by
examining the Gamma matrix. The input matrix is
then divided into two smaller matrices based on
this split, and the algorithm continues with each
submatrix. The result is a binary tree describing the
suffix splits at each node.
To construct a classification of suffixes into
paradigms, it is necessary to make a cut in the tree.
Assuming that suffix splits are optimal, we start at
the root of the tree and go down until reaching a
node where there is sufficient uncertainty about
which class a suffix should belong to. A good split
of suffixes is one where the vectors of probabilities
of suffixes given a class are orthogonal; we can
find such a split by minimizing the cosine of the
two columns of the node&apos;s Gamma matrix (we call
this the &amp;quot;Gamma cosine&amp;quot;). Thus, a node at which
suffixes should not be split has a high Gamma co-
sine, and when encountering such a node, a cut
should be made. The suffixes below this node are
grouped together as a paradigm; tree structure be-
low the cut node is ignored. In our experiments we
have selected thresholds for the Gamma cosine, but
we do not know if there is a single value that
would be successful cross-linguistically. After the
tree has been cut, the Gamma and Beta matrices
for ancestor nodes are normalized and combined to
form the M and L matrices for the language.
</bodyText>
<page confidence="0.992225">
71
</page>
<bodyText confidence="0.999877166666667">
Another issue is dealing with suboptimal solu-
tions. Random initializations of parameters lead
the EM training procedure to local maxima in the
solution space, and as a result LDA produces dif-
fering suffix splits across different runs. To get
around this, we simply run LDA multiple times (25
in our experiments) and choose the solution that
minimizes the Gamma cosine.
We also experimented with minimizing the
Beta cosine. The Beta matrix represents stem am-
biguity with respect to a suffix split. Since there
are inherently ambiguous stems, one should not
expect the Beta cosine value to be extremely low.
Minimizing the Beta cosine sometimes made the
Beta matrix &amp;quot;too disambiguated&amp;quot; and forced the
representation of ambiguity into Gamma matrix,
thereby inflating the Gamma cosine and causing
incorrect classifications of suffixes.
</bodyText>
<sectionHeader confidence="0.998444" genericHeader="method">
4 Data
</sectionHeader>
<bodyText confidence="0.999974820895523">
We conducted experiments on English and
Spanish. For English, we chose the Penn Treebank
(Marcus et. al. 1993), which is already POS-
tagged; for Spanish, we chose an equivalent-sized
portion of newswire (Graff and Galegos 1999),
POS-tagged by the FreeLing morphological ana-
lyzer (Carreras et. al. 2004). We restricted our data
to nouns, verbs, adjectives, and adverbs. Words
that did not follow canonical suffixation patterns
for their POS category (irregulars, foreign words,
incorrectly tagged words, etc.) were excluded. We
segmented each word into stem and suffix for a
specified set of suffixes. Rare suffixes were ex-
cluded, such as many English adjective-forming
suffixes and Spanish 2nd person plural forms.
Stems were not lemmatized, with the result that
there can be multiple stem variants of a particular
lemma, as with the words stemm.ing$ and
stem.s$. Tokens were not disambiguated for
word sense. Stems that occurred with only one suf-
fix were excluded.
We use several different representations of suf-
fixes in constructing the data matrices: 1) merged,
labeled suffixes; 2) merged, unlabeled suffixes; 3)
unmerged, unlabeled suffixes. For unmerged suf-
fixes, allomorphs2 are represented in their original
spelling. A merged suffix is a common representa-
tion for the multiple surface manifestations of an
underlyingly identical suffix. Suffixes also can be
unlabeled, or labeled with base POS tags. For an
example, a verb created would be segmented as
create.d$ with an unmerged, labeled suffix, or
create.d/ed$V with a merged, labeled suffix.
Labels disambiguate otherwise categorically am-
biguous suffixes.
The gold standard for each language lists the
suffixes that belong to a paradigm for stems of a
particular POS category. We call this the &amp;quot;input&amp;quot;
POS category, which is not indicated in annota-
tions and is the concept to be predicted. This
should be differentiated from the &amp;quot;output&amp;quot; POS
labels on the suffixes: for example, ly$R attaches
to stems of the input category “adjective”. Each
suffix is an atomic entity, so the system actually
has no concept of output POS categories. All that
we require is that distinct suffixes are given dis-
tinct symbols.
In the English gold standard (Table 1), each
slashed pair of suffixes denotes one merged form;
the unmerged forms are the individual suffixes.
ally$R is the suffix ly$R preceded by an epen-
thetic vowel, as in the word basically. In the
Spanish gold standard (Table 2), each slashed
group of suffixes corresponds to one merged form.
For adjectives and nouns, a$ and o$ are feminine
and masculine singular forms, and as$ and os$
are the corresponding plurals. $ and s$ do not
have gender; es$ is a plural allomorph.
mente/amente$R is a derivational suffix. The
first two groups of verbal suffixes are past partici-
ples, agreeing in number and gender. For the other
verb forms, when three are listed they correspond
to forms for the 1st, 2nd, and 3rd conjugations.
When there are two, the first is for the 1st conjuga-
tion, and the other is identical for the 2nd and 3rd.
o$V has the same form across all three conjuga-
tions.
</bodyText>
<construct confidence="0.532893">
Adjectives: $A, d/ed$A,
r/er$A, ally/ly$R
Nouns: $N, &apos;s$N, es/s$N
Verbs: $V, d/ed$V, es/s$V,
ing$V, ing$A, ing$N, r/er$N
</construct>
<tableCaption confidence="0.996952">
Table 1. Gold standard for English
</tableCaption>
<footnote confidence="0.4863725">
2 We abuse the standard usage of the term &amp;quot;allomorph&amp;quot;
to include gender and conjugational variants.
</footnote>
<page confidence="0.983365">
72
</page>
<table confidence="0.6808525">
Adjectives: a/o/$A, as/os/es/s$A,
mente/amente$R
Nouns: a/o/$N, as/os/es/s$N
Verbs: ada/ida/ado/ido$V,
</table>
<tableCaption confidence="0.960493888888889">
adas/idas/ados/idos$V, ando/iendo$V,
ar/er/ir$V, o$V, as/es$V, a/e$V,
amos/emos/imos$V, an/en$V, aba/ia$V,
ábamos/iamos$V, aban/ian$V,
aré/eré/iré$V, ará/erá/irá$V,
aremos/eremos/iremos$V, arán/erán/irán$V,
é/i$V, ó/ió$V, aron/ieron$V,
aria/eria/iria$V, arian/erian/irian$V
Table 2. Gold standard for Spanish
</tableCaption>
<sectionHeader confidence="0.998641" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.99618">
5.1 Merged, labeled suffixes
</subsectionHeader>
<bodyText confidence="0.9965609375">
Figure 1 shows the recursion tree for English data
preprocessed with merged, labeled suffixes. To
produce a classification of suffixes into paradigms,
we start at the root and go down until reaching
nodes with a Gamma cosine greater than or equal
to the threshold. The cut for a threshold of .0009
produces three paradigms exactly matching the
gold standard for verbs, adjectives, and nouns, re-
spectively. Table 3 shows the complete M matrix,
which contains suffix probabilities for each para-
digm. Table 4 shows a portion of the L matrix,
which contains the probabilities of stems belonging
to paradigms. We list the stems that are most am-
biguous with respect to paradigm membership
(note that this table does not specify the words that
belong to each category, only their stems).
</bodyText>
<table confidence="0.999824066666667">
&amp;quot;Verb&amp;quot; &amp;quot;Adj&amp;quot; &amp;quot;Noun&amp;quot;
$A 0.000 0.829 0.000
d/ed$A 0.020 0.000 0.000
r/er$A 0.000 0.033 0.000
ing$A 0.008 0.000 0.000
$N 0.000 0.000 0.706
&apos;s$N 0.000 0.000 0.036
r/er$N 0.037 0.000 0.000
ing$N 0.065 0.000 0.000
es/s$N 0.000 0.000 0.257
ally/ly$R 0.000 0.138 0.000
$V 0.342 0.000 0.000
d/ed$V 0.284 0.000 0.000
ing$V 0.133 0.000 0.000
es/s$V 0.110 0.000 0.000
</table>
<tableCaption confidence="0.94978">
Table 3. M matrix for English merged, labeled
suffixes. Columns: p(suff|paradigm).
</tableCaption>
<figureCaption confidence="0.9938922">
Figure 1. Recursion tree for English merged,
labeled suffixes. Each node shows its current
suffix set, and the Gamma cosine value for the
split. Dotted lines indicate paradigms for a
Gamma cosine threshold of .0009.
</figureCaption>
<table confidence="0.999963761904762">
&amp;quot;Verb&amp;quot; &amp;quot;Adj&amp;quot; &amp;quot;Noun&amp;quot;
reset 0.333 0.292 0.375
blunt 0.445 0.278 0.277
calm 0.417 0.375 0.209
total 0.312 0.462 0.226
clean 0.478 0.319 0.203
parallel 0.222 0.278 0.500
alert 0.500 0.222 0.277
sound 0.483 0.184 0.333
compound 0.372 0.171 0.457
pale 0.417 0.417 0.166
fine 0.254 0.230 0.516
premier 0.235 0.235 0.529
brief 0.175 0.524 0.301
polish 0.250 0.556 0.194
ski 0.378 0.108 0.513
fake 0.200 0.600 0.200
light 0.092 0.427 0.481
foster 0.226 0.161 0.613
bottom 0.107 0.304 0.589
repurchase 0.333 0.095 0.571
</table>
<tableCaption confidence="0.994991">
Table 4. Portion of L matrix for English merged,
labeled suffixes, sorted by lowest entropy.
Columns: p(paradigm|stem).
</tableCaption>
<figure confidence="0.990703363636364">
1: .0004
$A d/ed$A r/er$A ing$A $N &apos;s$N r/er$N ing$N
es/s$N ally/ly$R $V d/ed$V ing$V es/s$V
4: .0061
d/ed$A $V
d/ed$V ed/s$V
3: .0009
d/ed$A ing$A r/er$N ing$N
$V d/ed$V ing$V es/s$V
“verbs” “adjectives”
2: .0000
$A d/ed$A r/er$A ing$A
r/er$N ing$N ally/ly$R
$V d/ed$V ing$V es/s$V
5: .0000
ing$A r/er$N
ing$N ing$V
7: .0000
$A
6: .1604
$A r/er$A
ally/ly$R
8: .0000
r/er$A
ally/ly$R
10: .9271
$N &apos;s$N
9: .1413
$N &apos;s$N es/s$N
“nouns
11: .0000
es/s$N
”
</figure>
<page confidence="0.992525">
73
</page>
<bodyText confidence="0.999643363636364">
Next, we examine the morphological and lexical
conditional probabilities in the M and L matrices.
It is possible that even though the correct
classification of suffixes into paradigms was
learned, the probabilities may be off. Table 5
shows, however, that the M and L matrices are an
extremely accurate approximation of the true
morphological and lexical probabilities. We have
included statistics for the corresponding Spanish
experiment; the paradigms that were discovered for
Spanish also match the gold standard.
</bodyText>
<table confidence="0.9991334">
English Spanish
# suffixes 14 26
# stems 7315 5115
CRE M .0002 bits .0003 bits
CRE L .0006 bits .0020 bits
</table>
<tableCaption confidence="0.975039">
Table 5. Comparison of M and L matrices with
true morphological and lexical probabilities, by
conditional relative entropy (CRE).
</tableCaption>
<subsectionHeader confidence="0.99374">
5.2 Unmerged, labeled suffixes
</subsectionHeader>
<bodyText confidence="0.999227065217391">
The next experiments tested the effect of allomor-
phy on paradigm discovery, using data where suf-
fixes are labeled but not merged. There are
competing pressures at work in determining how
allomorphs are assigned to paradigms: on the one
hand, the disjointedness of stem sets for allo-
morphs would tend to place them in separate para-
digms; on the other hand, if those stem sets have
other suffixes in common that belong to the same
paradigm, the allomorphs might likewise be placed
in that paradigm. In our experiments, we found that
there was much more variability across runs than
in the merged suffix cases. In English, for exam-
ple, the suffix es$N was sometimes placed in the
&amp;quot;verb&amp;quot; paradigm, although the maximally orthogo-
nal solution placed it in the “noun” paradigm.
Figure 2 shows the recursion tree and para-
digms for Spanish. Gold standard noun and adjec-
tive categories are fragmented into multiple
paradigms in the tree. Although nouns have a
common parent node (2), the nouns of the different
genders are placed in separate paradigms -- this is
because a noun can have only one gender. The
verbs are all in a single paradigm (node 10). Node
11 contains all the first-conjugation verbs, and
node 12 contains all the second/third-conjugation
verbs. The reason that they are not in separate
paradigms is that a$V is shared by stems of all
three conjugations, which leads to a split that is not
quite orthogonal.
The case of adjectives is the most interesting.
Gendered and non-gendered adjective stems are
disjoint, so adjectives appear in two separate sub-
trees (nodes 4, 13). In node 4, the gender-
ambiguous plural es$A is in conflict with the plu-
ral s$A, but it would conflict with two plurals
as$A and os$A if it were placed in node 13.
amente$R appears together in node 14 because it
shares stems with the feminine adjectives.
amente$R also shares stems with verbs, as it is
also the derivational suffix which attaches to ver-
bal past participles in the feminine &amp;quot;a&amp;quot; form. This
is probably why the group of adjectives at node 13
is a sister to the verb nodes. The allomorph
mente$R attaches to non-gendered adjectives, and
is thus in the first adjective group.
</bodyText>
<figureCaption confidence="0.74320175">
Figure 2. Recursion tree for Spanish, unmerged,
labeled suffixes, with Gamma cosine values. Dot-
ted lines indicate paradigms for a Gamma cosine
threshold of .0021.
</figureCaption>
<subsectionHeader confidence="0.991631">
5.3 Unmerged, unlabeled suffixes
</subsectionHeader>
<bodyText confidence="0.999935333333334">
The case of unmerged, unlabeled suffixes is not as
successful. In the Gamma matrix for the root node
(Table 6), there is no orthogonal division of the
suffixes, as indicated by the high Gamma cosine
value of .1705. Despite this, the algorithm has dis-
covered useful information. There is a subpara-
</bodyText>
<figure confidence="0.994097807692308">
1: .0000
2:
.0000
“adjectives”
13: .0021
3: .0000
6: .0000
5: .0264
7: .0000
$N es$N
s$N
8: .0101
a$N as$N
4: .0008
$A es$A s$A
mente$R
14: .0000
a$A as$A
amente$R
15: .0133
o$A os$A
o$N os$N
“verbs”
10: .0098
“adjectives” “nouns” “nouns” “nouns”
11: .0107
</figure>
<equation confidence="0.9705380625">
a$V aba$V aban$V ada$V
adas$V ado$V ados$V
amos$V an$V ando$V ar$V
aremos$V aron$V ará$V
arán$V aré$V aría$V
arían$V as$V o$V
ábamos$V ó$V
12: .0056
e$V emos$V en$V er$V eremos$V
erá$V erán$V eré$V ería$V
erían$V es$V ida$V idas$V ido$V
idos$V iendo$V ieron$V imos$V
ir$V iremos$V irá$V irán$V
iré$V iría$V irían$V ió$V é$V
í$V ía$V íamos$V ían$V
9: .0009
</equation>
<page confidence="0.989753">
74
</page>
<bodyText confidence="0.815032619047619">
digm of unambiguous suffixes {&apos;s$,ally$}, and
another of {d$,ed$,ing$,r$}. The other suf-
fixes ($,er$,es$,ly$,s$) are ambiguous. The
ambiguity of ly$ seems to be a secondary effect:
since adjectives with the null suffix $ are found to
be ambiguous, ly$ is likewise ambiguous.
$ [0.9055] 0.0703
&apos;s$ [0.0351] 0.0000
ally$ [0.0007] 0.0000
d$ 0.0000 [0.1139]
ed$ 0.0000 [0.1332]
er$ [0.0087] 0.0084
es$ [0.0089] 0.0001
ing$ 0.0000 [0.1176]
ly$ 0.0033 [0.0603]
r$ 0.0000 [0.0198]
s$ 0.0378 [0.4764]
Table 6. Gamma matrix for root node, English,
unmerged, unlabeled suffixes; the categorization
is shown with brackets. Columns indicate
p(suffix|class).
</bodyText>
<sectionHeader confidence="0.868892" genericHeader="method">
6 Comparison with Linguistica
</sectionHeader>
<bodyText confidence="0.999950083333333">
In this section, we compare our system with
Linguistica3 (Goldsmith 2001), a freely available
program for unsupervised discovery of morpho-
logical structure. We focus our attention on Lin-
guistica&apos;s representation of morphology, rather
than the algorithm used to learn it. Linguistica
takes a list of word types, proposes segmentations
of words into stems and suffixes, and organizes
them into signatures. A signature is a non-
probabilistic data structure that groups together all
stems that share a common set of suffixes. Each
stem belongs to exactly one signature, and the set
of suffixes for each signature is unique. For exam-
ple, running Linguistica on our raw English text,
there is a signature {$,ful$,s$} for the stems
{resource, truth, youth}, indicating the
morphology of the words {resource$,
truth$, youth$, resourceful$, truth-
ful$, youthful$, resources$, truths$,
youths$}. There are no POS types in the system.
Thus, even for a prototypically &amp;quot;noun&amp;quot; signature
such as {$,&apos;s$}, it is quite possible that not all of
the words that the signature represents are actually
nouns. For example, the word structure$ is in
</bodyText>
<footnote confidence="0.776786">
3 http://linguistica.uchigago.edu
</footnote>
<bodyText confidence="0.999903019607843">
this signature, but occurs both as a noun (59 times)
and a verb (2 times) in our corpus.
The signature model can be derived from the
suffix-stem data matrix, by first converting all
positive counts to 1, and then placing in separate
groups all the stems that have the same 0/1 column
pattern. Another way to view the signature is as a
special case of the probabilistic paradigm where all
probabilities are restricted to being 0 or 1, for if
this were so, the only way to fit the data would be
to let there be a canonical paradigm for every dif-
ferent subset of suffixes that some stem appears
with. In theory, it is possible for the number of sig-
natures to be exponential in the number of suffixes;
in practice, Linguistica finds hundreds of signa-
tures for English and Spanish. Although there has
been work on reducing the number of signatures
(Goldwater and Johnson 2004; Hu et. al. 2005,
who report a reduction of up to 30%), the number
of remaining signatures is still two orders of mag-
nitude greater than the number of canonical para-
digms we find. The simplest explanation for this is
that a suffix can be listed many times in the differ-
ent signatures, but only has one entry in the M
matrix of the probabilistic paradigm.
It is important for a natural language system to
handle out-of-vocabulary words. A signature does
not predict the forms of potential but unseen forms
of stems. To some extent Linguistica could ac-
commodate this, as it identifies when one signa-
ture&apos;s suffixes are a proper subset of another&apos;s, but
it does not handle cases where suffixes are partially
overlapping. One principal advantage of the prob-
abilistic paradigm is that the canonical paradigm
allows the instantiation of a lexical paradigm con-
taining a complete set of predicted word forms for
a stem.
Since Linguistica is a system that starts from
raw text, it may seem that it cannot be directly
compared to our work, which assumes that seg-
mentations and suffixes are already known. How-
ever, it is possible to run Linguistica on our data by
doing further preprocessing. We rewrite the corpus
in such a way that Linguistica can detect correct
morphological and POS information for each to-
ken. Each token is replaced by the concatenation of
its stem, the dummy string 12345, and a single-
character encoding of its merged suffix. For exam-
ple, the token accelerate.d/ed$V is mapped to
accelerate12345D, where D represents d/ed$V.
The omnipresence of the dummy string enables
</bodyText>
<page confidence="0.997255">
75
</page>
<bodyText confidence="0.999691125">
Linguistica to discover all the desired stems and
suffixes, but no more. By mapping the input corpus
in this way, we can examine the type of grammar
that Linguistica would find if it knew the informa-
tion that we have assumed in the previous experi-
ments. Linguistica found 565 signatures from the
&amp;quot;cooked&amp;quot; English data (Figure 3). 50% of word
types are represented by the first 13 signatures.
</bodyText>
<figure confidence="0.934166041666667">
1. { $N, es/s$N } 1540
abortion absence accent acceptance
accident accolade accommodation
2. { $N, &apos;s$N } 1168
aba abbie abc academy achenbaum aclu
adams addington addison adobe
3. { $N, &apos;s$N, es/s$N } 224
accountant acquisition actor
administration airline airport alliance
5. { $A, ally/ly$R } 319
abrupt absolute abundant accurate
actual additional adequate adroit
6. { $A, $N, es/s$N } 173
abrasive acid activist adhesive adult
afghan african afrikaner aggregate
7. { $V, d/ed$V, es/s$V } 135
abate achieve administer afflict
aggravate alienate amass apologize
9. { $V, d/ed$V, ing$V, es/s$V } 73
abound absorb adopt applaud assert
assist attend attract avert avoid
13. { $N, $V, d/ed$V, es/s$N, es/s$V } 44
advocate amount attribute battle
bounce cause compromise decline
</figure>
<figureCaption confidence="0.8945215">
Figure 3. Selected top signatures for merged, labeled
suffix English data. Each signature shows the suffix set,
number of stems, and several example stems. Ranking is
by log(num stems)* log(num suffixes).
</figureCaption>
<bodyText confidence="0.99936028">
We have formulated two metrics to evaluate the
quality of a collection of signatures or paradigms.
Ideally, all suffixes of a particular signature would
be of the same category, and all the words of a par-
ticular category would be contained within one
signature. POS fragmentation measures to what
extent the words of an input POS category are scat-
tered across different signatures. It is the average
number of bits required to encode the probability
distribution of some category’s words over signa-
tures. Signature impurity measures the extent to
which the suffixes of a signature are of mixed in-
put POS types. It is the expected value of the num-
ber of bits required to encode the probability
distribution of some signature’s suffixes over input
POS categories. Table 7 shows that, according to
these metrics, the signature does not organize mor-
phological information as efficiently as probabilis-
tic paradigms4. Linguistica’s impurity scores are
reasonably low because many of the signatures
with the most stems are categorically homogene-
ous. Fragmentation scores show that the placement
of the majority of words within top signatures off-
sets the scattering of a POS category’s suffixes
over many signatures.
</bodyText>
<figure confidence="0.949619714285714">
(1) POS fragmentation =
(2) Signature impurity =
⎤
∑ S.numstems h ( p ( P  |S ))
× ⎥
⎦ ⎥
S
</figure>
<table confidence="0.87583325">
h: entropy
P: input POS categories
S: signatures / paradigms
Linguistica Recursive LDA
English fragmentation 5.422 bits 0 bits
English impurity .404 bits 0 bits
Spanish fragmentation 6.084 bits 0 bits
Spanish impurity .332 bits 0 bits
</table>
<tableCaption confidence="0.993388333333333">
Table 7. Comparison of Linguistica and recursive LDA
on merged, labeled suffix data. The maximum possible
impurity for 3 POS categories is log2(3) = 1.585 bits.
</tableCaption>
<bodyText confidence="0.999923071428572">
Finally, a morphological grammar should reflect
the general, abstract morphological structure of the
language from which a corpus was sampled. To
test for consistency of morphological grammars
across corpora, we split our cooked English data
into two equal parts. Linguistica found 449 total
signatures for the first half and 462 for the second.
296 signatures were common to both (in terms of
the suffixes contained by the signatures). Of the
3506 stems shared by both data sets, 1831 (52.2%)
occurred in the same signature. Of the top 50 sig-
natures for each half-corpus, 45 were in common,
and 1651 of 2403 shared stems (68.7%) occurred
in the same signature. Recursive LDA found the
</bodyText>
<footnote confidence="0.8339094">
4 Our scores would not be so good if we had chosen a
poor Gamma cosine threshold value for classification.
However, Linguistica’s scores cannot be decreased, as
there is only one signature model for a fixed set of
stems and suffixes.
</footnote>
<figure confidence="0.997796428571428">
h (p (S  |words of P ))
⎡∑Ρ
⎢
⎣ ⎢
⎤
⎥
⎦ ⎥
P
⎡
⎢
⎣ ⎢
S.numstems
∑
S
</figure>
<page confidence="0.922543">
76
</page>
<bodyText confidence="0.999737714285714">
same canonical paradigms for both data sets
(which matched the gold standard). Differences in
word counts between the corpus halves altered
stem inventories and lexical probabilities, but not
the structure of the canonical paradigms. Our sys-
tem thus displays a robustness to corpus choice
that does not hold for Linguistica.
</bodyText>
<sectionHeader confidence="0.999661" genericHeader="method">
7 Future Work
</sectionHeader>
<bodyText confidence="0.999355666666667">
This section sketches some ideas for future work to
increase the linguistic adequacy of the system, and
to make it more unsupervised.
</bodyText>
<listItem confidence="0.57394575">
1. Bootstrapping: for fully unsupervised learning,
we need to hypothesize stems and suffixes. The
output of recursive LDA indicates which suffixes
may be ambiguous. To bootstrap a disambiguator
</listItem>
<bodyText confidence="0.954725857142857">
for the different categorial uses of these suffixes,
one could use various types of distributional in-
formation, as well as knowledge of partial para-
digmatic structure for non-ambiguous suffixes.
2. Automated detection of cut nodes: currently the
system requires that the user select a Gamma co-
sine threshold for extracting paradigms from the
recursion tree. We would like to automate this
process, perhaps with different heuristics.
3. Suffix merging and formulation of generation
rules: when we decide that two suffixes should be
merged (based on some measures of distributional
similarity and word-internal context), we also need
to formulate phonological (i.e., spelling) rules to
determine which surface form to use when instan-
tiating a form from the canonical paradigm.
4. Non-regular forms: we can take advantage of
empty cells in the data matrix in order to identify
non-regularities such as suppletives, stem variants,
semi-regular subclasses, and suffix allomorphs. If
the expected frequency of a word form (as derived
from the M matrix and frequency of a stem) is rela-
tively high but the value in the D matrix is zero,
this is evidence that a non-regular form may oc-
cupy this cell. Locating irregular words could use
methods similar to those of (Yarowsky and Wicen-
towski 2000), who pair irregular inflections and
their roots from raw text. Stem variants and allo-
morphic suffixes could be detected in a similar
manner, by finding sets of stems/suffixes with mu-
tually exclusive matrix entries.
5. Multiple morphological properties per word: we
currently represent all morphological and POS in-
formation with a single suffix. The learning algo-
rithm and representation could perhaps be
modified to allow for multiple morphological
properties. One could perform recursive LDA on a
particular morphological property, then take each
of the learned paradigms and perform recursive
LDA again, but for a different morphological
property. This method might discover Spanish con-
jugational classes as subclasses within “verbs”.
</bodyText>
<sectionHeader confidence="0.999544" genericHeader="discussions">
8 Discussion
</sectionHeader>
<bodyText confidence="0.9999771">
This paper has introduced the probabilistic para-
digm model of morphology. It has some important
benefits: it is an abstract, compact representation of
a language&apos;s morphology, it accommodates lexical
ambiguity, and it predicts forms of words not seen
in the input data.
We have formulated the problem of learning
probabilistic paradigms as one of discovering la-
tent classes within a suffix-stem count matrix,
through the recursive application of LDA with an
orthogonality constraint. Under optimal data condi-
tions, it can learn the correct paradigms, and also
models morphological and lexical probabilities
extremely accurately. It is robust to corpus choice,
so we can say that it learns a morphological gram-
mar for the language. This is a new application of
matrix factorization algorithms, and an usual one:
whereas in document topic modeling, one tries to
find that a document consists of multiple topics,
we want to find orthogonal decompositions where
each suffix (document) belongs to only one input
POS category (topic).
We have demonstrated that the algorithm can
successfully learn morphological paradigms for
English and Spanish under the conditions that
segmentations are known, categorically ambiguous
suffixes have been distinguished, and allomorphs
have been merged. When suffixes have not been
merged, there is a tendency to place allomorphic
variants in different paradigms. The algorithm is
the least successful in the unmerged, unlabeled
case, as ambiguous suffixes do not allow for a
clear split of suffixes into paradigms. However, the
program output indicates which suffixes are poten-
tially ambiguous or unambiguous, and this infor-
mation could be used by bootstrapping procedures
for suffix disambiguation.
Some of the behavior of the learning algorithm
can be explained in terms of several constraints.
First, LDA assumes conditional independence of
</bodyText>
<page confidence="0.993956">
77
</page>
<bodyText confidence="0.999996290322581">
documents (suffixes) given topics (paradigms). A
stem should be able to occur with each suffix of a
canonical paradigm. But if a stem occurs with one
allomorphic variant of a suffix, we know that it
necessarily cannot occur with the other. Therefore,
allomorphy violates conditional independence of
suffixes given a paradigm, and we cope with this
by merging allomorphs. Second, LDA also as-
sumes conditional independence of words (stems)
given topics (paradigm). As our data contains stem
variants, this assumption does not hold either, but
it is a less serious violation due to the large number
of total stems. Third, we have imposed the con-
straint of orthogonality of suffixes and paradigms,
which is not required by LDA (and actually unde-
sired in document topic modeling, since documents
can contain multiple topics). Orthogonal suffix
splits are possible when categorically ambiguous
suffixes have been disambiguated.
In conclusion, we view morphology learning
as a process of manipulating the representation of
data to fit a learnable computational model. The
alternative would be to complicate the model and
learning algorithm to accommodate raw data and
all its concurrent ambiguities and dependencies.
We hypothesize that successful, fully unsupervised
learning of linguistically adequate representations
of morphology will be more easily accomplished
by first bootstrapping the sorts of information that
we have assumed, or, in other words, fitting the
data to the model.
</bodyText>
<sectionHeader confidence="0.998407" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99967825">
This work was supported by the National Science
Foundation under grant NSF IIS-0415138. The
author thanks Mitch Marcus and anonymous re-
viewers for their helpful comments.
</bodyText>
<sectionHeader confidence="0.999421" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999843107142858">
A. Albright. 2002. The identification of bases in mor-
phological paradigms. Ph.D. thesis, UCLA.
D. Blei, A. Ng, and M. Jordan. 2003. Latent dirichlet
allocation. Journal of Machine Learning Research 3,
993-1022.
X. Carreras, I. Chao, L. Padró, and M. Padró. 2004.
FreeLing: an open-source suite of language analyz-
ers. Proceedings of LREC. Lisbon, Portugal.
A. Clark. 2001. Learning morphology with pair hidden
markov models. Proceedings of the Student Work-
shop at ACL.
R. Evans and G. Gazdar. 1996. DATR: A language for
lexical knowledge representation. Computational
Linguistics 22(2), 167-216.
M. Forsberg and A. Ranta. 2004. Functional morphol-
ogy. Proceedings of the ICFP, 213-223. ACM Press.
D. Freitag. 2005. Morphology induction from term clus-
ters. Proceedings of CoNLL.
J. Goldsmith. 2001. Unsupervised learning of the mor-
phology of a natural language. Computational Lin-
guistics 27(2), 153-198.
S. Goldwater and M. Johnson. 2004. Priors in bayesian
learning of phonological rules. Proceedings of
SIGPHON.
D. Graff and G. Gallegos. 1999. Spanish newswire text,
volume 2. Linguistic Data Consortium, Philadelphia, PA.
Y. Hu, I. Matveeva, J. Goldsmith, and C. Sprague.
2005. Using morphology and syntax together in un-
supervised learning. Workshop on Psychocomputa-
tional Models of Human Language Acquisition.
D. Kazakov and S. Manandhar. 2001. Unsupervised
learning of word segmentation rules with genetic al-
gorithms and inductive logic programming. Machine
Learning 43, 121-162.
M. Marcus, B. Santorini and M.A. Marcinkiewicz.
1993. Building a large annotated corpus of English:
The Penn Treebank. Computational Linguistics
19(2), 313-330.
C. Monson, A. Lavie, J. Carbonell, and L. Levin. 2004.
Unsupervised induction of natural language mor-
phology inflection classes. Proc. of SIGPHON.
K. Oflazer, S. Nirenburg, and M. McShane. 2001. Boot-
strapping morphological analyzers by combining
human elicitation and machine learning. Computa-
tional Linguistics 27(1), 59-85.
P. Schone and D. Jurafsky. 2001. Knowledge-free in-
duction of inflectional morphologies. Proc. NAACL.
M. Snover, G. Jarosz, and M. Brent. 2002. Unsuper-
vised learning of morphology using a novel directed
search algorithm: taking the first step. Proceedings of
SIGPHON.
D. Yarowsky and R. Wicentowski. 2000. Minimally
supervised morphological analysis by multimodal
alignment. Proceedings of ACL.
R. Zajac. 2001. Morpholog: constrained and supervised
learning of morphology. Proceedings of CoNLL.
</reference>
<page confidence="0.998828">
78
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.568196">
<title confidence="0.999915">Learning Probabilistic Paradigms for Morphology in a Latent Class Model</title>
<author confidence="0.999828">Erwin Chan</author>
<affiliation confidence="0.999854">Dept. of Computer and Information University of</affiliation>
<address confidence="0.592991">Philadelphia, PA</address>
<email confidence="0.996501">echan3@seas.upenn.edu</email>
<abstract confidence="0.9980536">paper introduces the a probabilistic, declarative model of morphological structure. We describe an algorithm that recursively applies Latent Dirichlet Allocation with an orthogonality constraint to discover morphological paradigms as the latent classes within a suffix-stem matrix. We apply the algorithm to data preprocessed in several different ways, and show that when suffixes are distinguished for part of speech and allomorphs or gender/conjugational variants are merged, the model is able to correctly learn morphological paradigms for English and Spanish. We compare our system with Linguistica (Goldsmith 2001), and discuss the advantages of the probabilistic paradigm over Linguistica’s signature representation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Albright</author>
</authors>
<title>The identification of bases in morphological paradigms.</title>
<date>2002</date>
<tech>Ph.D. thesis, UCLA.</tech>
<contexts>
<context position="2846" citStr="Albright 2002" startWordPosition="413" endWordPosition="414">ical analysis involves mapping between inflected and base of forms of words with finite-state transducers (FSTs), a procedural model of morphological relations. Rewrite rules are handcrafted and compiled into FSTs, and it would be beneficial if these rules could be learned automatically. One line of research in computational morphology has been directed towards learning finitestate mapping rules from some sort of paradigmatic structure, where all morphological forms and POS types are presumed known for a set of lexemes (Clark 2001, Kazakov and Manandhar 2001, Oflazer et. al. 2001, Zajac 2001, Albright 2002). This can be accomplished by first deciding on a base form, then learning rules to convert other forms of the paradigm into this base form. If one could develop an unsupervised algorithm for learning paradigms, it could serve as the input to rulelearning procedures, effectively leading to an entirely unsupervised system for learning FSTs from raw data. This is our long-term goal. 1 $ is the null suffix. 69 Proceedings of the Eighth Meeting of the ACL Special Interest Group on Computational Phonology at HLT-NAACL 2006, pages 69–78, New York City, USA, June 2006. c�2006 Association for Computat</context>
</contexts>
<marker>Albright, 2002</marker>
<rawString>A. Albright. 2002. The identification of bases in morphological paradigms. Ph.D. thesis, UCLA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blei</author>
<author>A Ng</author>
<author>M Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research</journal>
<volume>3</volume>
<pages>993--1022</pages>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>D. Blei, A. Ng, and M. Jordan. 2003. Latent dirichlet allocation. Journal of Machine Learning Research 3, 993-1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
<author>I Chao</author>
<author>L Padró</author>
<author>M Padró</author>
</authors>
<title>FreeLing: an open-source suite of language analyzers.</title>
<date>2004</date>
<booktitle>Proceedings of LREC.</booktitle>
<location>Lisbon, Portugal.</location>
<marker>Carreras, Chao, Padró, Padró, 2004</marker>
<rawString>X. Carreras, I. Chao, L. Padró, and M. Padró. 2004. FreeLing: an open-source suite of language analyzers. Proceedings of LREC. Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Clark</author>
</authors>
<title>Learning morphology with pair hidden markov models.</title>
<date>2001</date>
<booktitle>Proceedings of the Student Workshop at ACL.</booktitle>
<contexts>
<context position="2768" citStr="Clark 2001" startWordPosition="401" endWordPosition="402">esentation of morphology. Currently, the dominant technology for morphological analysis involves mapping between inflected and base of forms of words with finite-state transducers (FSTs), a procedural model of morphological relations. Rewrite rules are handcrafted and compiled into FSTs, and it would be beneficial if these rules could be learned automatically. One line of research in computational morphology has been directed towards learning finitestate mapping rules from some sort of paradigmatic structure, where all morphological forms and POS types are presumed known for a set of lexemes (Clark 2001, Kazakov and Manandhar 2001, Oflazer et. al. 2001, Zajac 2001, Albright 2002). This can be accomplished by first deciding on a base form, then learning rules to convert other forms of the paradigm into this base form. If one could develop an unsupervised algorithm for learning paradigms, it could serve as the input to rulelearning procedures, effectively leading to an entirely unsupervised system for learning FSTs from raw data. This is our long-term goal. 1 $ is the null suffix. 69 Proceedings of the Eighth Meeting of the ACL Special Interest Group on Computational Phonology at HLT-NAACL 200</context>
</contexts>
<marker>Clark, 2001</marker>
<rawString>A. Clark. 2001. Learning morphology with pair hidden markov models. Proceedings of the Student Workshop at ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Evans</author>
<author>G Gazdar</author>
</authors>
<title>DATR: A language for lexical knowledge representation.</title>
<date>1996</date>
<journal>Computational Linguistics</journal>
<volume>22</volume>
<issue>2</issue>
<pages>167--216</pages>
<contexts>
<context position="1824" citStr="Evans and Gazdar 1996" startWordPosition="251" endWordPosition="254">afsky 2001), the paradigm of (Brent et. al. 2002), and the inflectional class of (Monson 2004). While these representations group morphologically related words in systematic ways, they are rather different from the paradigm, the representation of morphology in traditional grammars. A paradigm lists the prototypical morphological properties of lexemes belonging to a particular part of speech (POS) category; for example, a paradigm for regular English verbs would include the suffixes {$,ed$,ing$,s$}1. Hand-built computational implementations of paradigms as inheritance hierarchies include DATR (Evans and Gazdar 1996) and Functional Morphology (Forsberg and Ranta 2004). The two principal ways in which learned models have differed from paradigms are that: 1) they do not have POS types, and 2) they are not abstractions that generalize beyond the words of the input corpus. There are important reasons for learning a POS-associated, paradigmatic representation of morphology. Currently, the dominant technology for morphological analysis involves mapping between inflected and base of forms of words with finite-state transducers (FSTs), a procedural model of morphological relations. Rewrite rules are handcrafted a</context>
</contexts>
<marker>Evans, Gazdar, 1996</marker>
<rawString>R. Evans and G. Gazdar. 1996. DATR: A language for lexical knowledge representation. Computational Linguistics 22(2), 167-216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Forsberg</author>
<author>A Ranta</author>
</authors>
<title>Functional morphology.</title>
<date>2004</date>
<booktitle>Proceedings of the ICFP,</booktitle>
<pages>213--223</pages>
<publisher>ACM Press.</publisher>
<contexts>
<context position="1876" citStr="Forsberg and Ranta 2004" startWordPosition="258" endWordPosition="261"> and the inflectional class of (Monson 2004). While these representations group morphologically related words in systematic ways, they are rather different from the paradigm, the representation of morphology in traditional grammars. A paradigm lists the prototypical morphological properties of lexemes belonging to a particular part of speech (POS) category; for example, a paradigm for regular English verbs would include the suffixes {$,ed$,ing$,s$}1. Hand-built computational implementations of paradigms as inheritance hierarchies include DATR (Evans and Gazdar 1996) and Functional Morphology (Forsberg and Ranta 2004). The two principal ways in which learned models have differed from paradigms are that: 1) they do not have POS types, and 2) they are not abstractions that generalize beyond the words of the input corpus. There are important reasons for learning a POS-associated, paradigmatic representation of morphology. Currently, the dominant technology for morphological analysis involves mapping between inflected and base of forms of words with finite-state transducers (FSTs), a procedural model of morphological relations. Rewrite rules are handcrafted and compiled into FSTs, and it would be beneficial if</context>
</contexts>
<marker>Forsberg, Ranta, 2004</marker>
<rawString>M. Forsberg and A. Ranta. 2004. Functional morphology. Proceedings of the ICFP, 213-223. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Freitag</author>
</authors>
<title>Morphology induction from term clusters.</title>
<date>2005</date>
<booktitle>Proceedings of CoNLL.</booktitle>
<contexts>
<context position="3858" citStr="Freitag 2005" startWordPosition="576" endWordPosition="577"> suffix. 69 Proceedings of the Eighth Meeting of the ACL Special Interest Group on Computational Phonology at HLT-NAACL 2006, pages 69–78, New York City, USA, June 2006. c�2006 Association for Computational Linguistics An alternative approach is to skip the paradigm formulation step and construct a procedural model directly from raw data. (Yarowsky and Wicentowski 2000) bootstrap inflected and base forms directly from raw data and learn mappings between them. Their results are quite successful, but the morphological information they learn is not structured as clearly as a paradigmatic model. (Freitag 2005) constructs a morphological automaton, where nodes are clustered word types and arcs are suffixation rules. This paper addresses the problem of finding an organization of stems and suffixes as probabilistic paradigms (section 2), a model of morphology closer to linguistic notion of paradigm than previously proposed models. We encode the morphological structure of a language in a matrix containing frequencies of words, and formulate the problem of learning paradigms as one of finding latent classes within the matrix. We present a recursive LDA, a learning algorithm based on Latent Dirichlet All</context>
</contexts>
<marker>Freitag, 2005</marker>
<rawString>D. Freitag. 2005. Morphology induction from term clusters. Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goldsmith</author>
</authors>
<title>Unsupervised learning of the morphology of a natural language.</title>
<date>2001</date>
<journal>Computational Linguistics</journal>
<volume>27</volume>
<issue>2</issue>
<pages>153--198</pages>
<contexts>
<context position="854" citStr="Goldsmith 2001" startWordPosition="117" endWordPosition="118">listic paradigm, a probabilistic, declarative model of morphological structure. We describe an algorithm that recursively applies Latent Dirichlet Allocation with an orthogonality constraint to discover morphological paradigms as the latent classes within a suffix-stem matrix. We apply the algorithm to data preprocessed in several different ways, and show that when suffixes are distinguished for part of speech and allomorphs or gender/conjugational variants are merged, the model is able to correctly learn morphological paradigms for English and Spanish. We compare our system with Linguistica (Goldsmith 2001), and discuss the advantages of the probabilistic paradigm over Linguistica’s signature representation. 1 Introduction In recent years researchers have addressed the task of unsupervised learning of declarative representations of morphological structure. These models include the signature of (Goldsmith 2001), the conflation set of (Schone and Jurafsky 2001), the paradigm of (Brent et. al. 2002), and the inflectional class of (Monson 2004). While these representations group morphologically related words in systematic ways, they are rather different from the paradigm, the representation of morph</context>
<context position="4697" citStr="Goldsmith 2001" startWordPosition="706" endWordPosition="707">n 2), a model of morphology closer to linguistic notion of paradigm than previously proposed models. We encode the morphological structure of a language in a matrix containing frequencies of words, and formulate the problem of learning paradigms as one of finding latent classes within the matrix. We present a recursive LDA, a learning algorithm based on Latent Dirichlet Allocation (section 3), and show that under certain conditions (section 5), it can correctly learn morphological paradigms for English and Spanish. In section 6, we compare the probabilistic paradigm to the signature model of (Goldsmith 2001). In section 7, we sketch some ideas for how to make our system more unsupervised and more linguistically adequate. We assume a model of morphology where each word is the concatenation of a stem and a single suffix representing all of the word&apos;s morphological and POS properties. Although this is a very simplistic view of morphology, there are many hitherto unresolved computational issues for learning even this basic model, and we consider it necessary to address these issues before developing more sophisticated models. For a stem/suffix representation, the task of learning a paradigm from raw </context>
<context position="24741" citStr="Goldsmith 2001" startWordPosition="3978" endWordPosition="3979">biguity of ly$ seems to be a secondary effect: since adjectives with the null suffix $ are found to be ambiguous, ly$ is likewise ambiguous. $ [0.9055] 0.0703 &apos;s$ [0.0351] 0.0000 ally$ [0.0007] 0.0000 d$ 0.0000 [0.1139] ed$ 0.0000 [0.1332] er$ [0.0087] 0.0084 es$ [0.0089] 0.0001 ing$ 0.0000 [0.1176] ly$ 0.0033 [0.0603] r$ 0.0000 [0.0198] s$ 0.0378 [0.4764] Table 6. Gamma matrix for root node, English, unmerged, unlabeled suffixes; the categorization is shown with brackets. Columns indicate p(suffix|class). 6 Comparison with Linguistica In this section, we compare our system with Linguistica3 (Goldsmith 2001), a freely available program for unsupervised discovery of morphological structure. We focus our attention on Linguistica&apos;s representation of morphology, rather than the algorithm used to learn it. Linguistica takes a list of word types, proposes segmentations of words into stems and suffixes, and organizes them into signatures. A signature is a nonprobabilistic data structure that groups together all stems that share a common set of suffixes. Each stem belongs to exactly one signature, and the set of suffixes for each signature is unique. For example, running Linguistica on our raw English te</context>
</contexts>
<marker>Goldsmith, 2001</marker>
<rawString>J. Goldsmith. 2001. Unsupervised learning of the morphology of a natural language. Computational Linguistics 27(2), 153-198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Goldwater</author>
<author>M Johnson</author>
</authors>
<title>Priors in bayesian learning of phonological rules.</title>
<date>2004</date>
<booktitle>Proceedings of SIGPHON.</booktitle>
<contexts>
<context position="26696" citStr="Goldwater and Johnson 2004" startWordPosition="4300" endWordPosition="4303">the stems that have the same 0/1 column pattern. Another way to view the signature is as a special case of the probabilistic paradigm where all probabilities are restricted to being 0 or 1, for if this were so, the only way to fit the data would be to let there be a canonical paradigm for every different subset of suffixes that some stem appears with. In theory, it is possible for the number of signatures to be exponential in the number of suffixes; in practice, Linguistica finds hundreds of signatures for English and Spanish. Although there has been work on reducing the number of signatures (Goldwater and Johnson 2004; Hu et. al. 2005, who report a reduction of up to 30%), the number of remaining signatures is still two orders of magnitude greater than the number of canonical paradigms we find. The simplest explanation for this is that a suffix can be listed many times in the different signatures, but only has one entry in the M matrix of the probabilistic paradigm. It is important for a natural language system to handle out-of-vocabulary words. A signature does not predict the forms of potential but unseen forms of stems. To some extent Linguistica could accommodate this, as it identifies when one signatu</context>
</contexts>
<marker>Goldwater, Johnson, 2004</marker>
<rawString>S. Goldwater and M. Johnson. 2004. Priors in bayesian learning of phonological rules. Proceedings of SIGPHON.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Graff</author>
<author>G Gallegos</author>
</authors>
<title>Spanish newswire text, volume 2. Linguistic Data Consortium,</title>
<date>1999</date>
<location>Philadelphia, PA.</location>
<marker>Graff, Gallegos, 1999</marker>
<rawString>D. Graff and G. Gallegos. 1999. Spanish newswire text, volume 2. Linguistic Data Consortium, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Hu</author>
<author>I Matveeva</author>
<author>J Goldsmith</author>
<author>C Sprague</author>
</authors>
<title>Using morphology and syntax together in unsupervised learning. Workshop on Psychocomputational Models of Human Language Acquisition.</title>
<date>2005</date>
<marker>Hu, Matveeva, Goldsmith, Sprague, 2005</marker>
<rawString>Y. Hu, I. Matveeva, J. Goldsmith, and C. Sprague. 2005. Using morphology and syntax together in unsupervised learning. Workshop on Psychocomputational Models of Human Language Acquisition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Kazakov</author>
<author>S Manandhar</author>
</authors>
<title>Unsupervised learning of word segmentation rules with genetic algorithms and inductive logic programming.</title>
<date>2001</date>
<journal>Machine Learning</journal>
<volume>43</volume>
<pages>121--162</pages>
<contexts>
<context position="2796" citStr="Kazakov and Manandhar 2001" startWordPosition="403" endWordPosition="406">f morphology. Currently, the dominant technology for morphological analysis involves mapping between inflected and base of forms of words with finite-state transducers (FSTs), a procedural model of morphological relations. Rewrite rules are handcrafted and compiled into FSTs, and it would be beneficial if these rules could be learned automatically. One line of research in computational morphology has been directed towards learning finitestate mapping rules from some sort of paradigmatic structure, where all morphological forms and POS types are presumed known for a set of lexemes (Clark 2001, Kazakov and Manandhar 2001, Oflazer et. al. 2001, Zajac 2001, Albright 2002). This can be accomplished by first deciding on a base form, then learning rules to convert other forms of the paradigm into this base form. If one could develop an unsupervised algorithm for learning paradigms, it could serve as the input to rulelearning procedures, effectively leading to an entirely unsupervised system for learning FSTs from raw data. This is our long-term goal. 1 $ is the null suffix. 69 Proceedings of the Eighth Meeting of the ACL Special Interest Group on Computational Phonology at HLT-NAACL 2006, pages 69–78, New York Cit</context>
</contexts>
<marker>Kazakov, Manandhar, 2001</marker>
<rawString>D. Kazakov and S. Manandhar. 2001. Unsupervised learning of word segmentation rules with genetic algorithms and inductive logic programming. Machine Learning 43, 121-162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics</journal>
<volume>19</volume>
<issue>2</issue>
<pages>313--330</pages>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, B. Santorini and M.A. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics 19(2), 313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Monson</author>
<author>A Lavie</author>
<author>J Carbonell</author>
<author>L Levin</author>
</authors>
<title>Unsupervised induction of natural language morphology inflection classes.</title>
<date>2004</date>
<booktitle>Proc. of SIGPHON.</booktitle>
<marker>Monson, Lavie, Carbonell, Levin, 2004</marker>
<rawString>C. Monson, A. Lavie, J. Carbonell, and L. Levin. 2004. Unsupervised induction of natural language morphology inflection classes. Proc. of SIGPHON.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Oflazer</author>
<author>S Nirenburg</author>
<author>M McShane</author>
</authors>
<title>Bootstrapping morphological analyzers by combining human elicitation and machine learning.</title>
<date>2001</date>
<journal>Computational Linguistics</journal>
<volume>27</volume>
<issue>1</issue>
<pages>59--85</pages>
<marker>Oflazer, Nirenburg, McShane, 2001</marker>
<rawString>K. Oflazer, S. Nirenburg, and M. McShane. 2001. Bootstrapping morphological analyzers by combining human elicitation and machine learning. Computational Linguistics 27(1), 59-85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Schone</author>
<author>D Jurafsky</author>
</authors>
<title>Knowledge-free induction of inflectional morphologies.</title>
<date>2001</date>
<booktitle>Proc. NAACL.</booktitle>
<contexts>
<context position="1213" citStr="Schone and Jurafsky 2001" startWordPosition="164" endWordPosition="167">and show that when suffixes are distinguished for part of speech and allomorphs or gender/conjugational variants are merged, the model is able to correctly learn morphological paradigms for English and Spanish. We compare our system with Linguistica (Goldsmith 2001), and discuss the advantages of the probabilistic paradigm over Linguistica’s signature representation. 1 Introduction In recent years researchers have addressed the task of unsupervised learning of declarative representations of morphological structure. These models include the signature of (Goldsmith 2001), the conflation set of (Schone and Jurafsky 2001), the paradigm of (Brent et. al. 2002), and the inflectional class of (Monson 2004). While these representations group morphologically related words in systematic ways, they are rather different from the paradigm, the representation of morphology in traditional grammars. A paradigm lists the prototypical morphological properties of lexemes belonging to a particular part of speech (POS) category; for example, a paradigm for regular English verbs would include the suffixes {$,ed$,ing$,s$}1. Hand-built computational implementations of paradigms as inheritance hierarchies include DATR (Evans and G</context>
</contexts>
<marker>Schone, Jurafsky, 2001</marker>
<rawString>P. Schone and D. Jurafsky. 2001. Knowledge-free induction of inflectional morphologies. Proc. NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Snover</author>
<author>G Jarosz</author>
<author>M Brent</author>
</authors>
<title>Unsupervised learning of morphology using a novel directed search algorithm: taking the first step.</title>
<date>2002</date>
<booktitle>Proceedings of SIGPHON.</booktitle>
<marker>Snover, Jarosz, Brent, 2002</marker>
<rawString>M. Snover, G. Jarosz, and M. Brent. 2002. Unsupervised learning of morphology using a novel directed search algorithm: taking the first step. Proceedings of SIGPHON.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
<author>R Wicentowski</author>
</authors>
<title>Minimally supervised morphological analysis by multimodal alignment.</title>
<date>2000</date>
<booktitle>Proceedings of ACL.</booktitle>
<contexts>
<context position="3617" citStr="Yarowsky and Wicentowski 2000" startWordPosition="536" endWordPosition="540">If one could develop an unsupervised algorithm for learning paradigms, it could serve as the input to rulelearning procedures, effectively leading to an entirely unsupervised system for learning FSTs from raw data. This is our long-term goal. 1 $ is the null suffix. 69 Proceedings of the Eighth Meeting of the ACL Special Interest Group on Computational Phonology at HLT-NAACL 2006, pages 69–78, New York City, USA, June 2006. c�2006 Association for Computational Linguistics An alternative approach is to skip the paradigm formulation step and construct a procedural model directly from raw data. (Yarowsky and Wicentowski 2000) bootstrap inflected and base forms directly from raw data and learn mappings between them. Their results are quite successful, but the morphological information they learn is not structured as clearly as a paradigmatic model. (Freitag 2005) constructs a morphological automaton, where nodes are clustered word types and arcs are suffixation rules. This paper addresses the problem of finding an organization of stems and suffixes as probabilistic paradigms (section 2), a model of morphology closer to linguistic notion of paradigm than previously proposed models. We encode the morphological struct</context>
<context position="34293" citStr="Yarowsky and Wicentowski 2000" startWordPosition="5558" endWordPosition="5562">l (i.e., spelling) rules to determine which surface form to use when instantiating a form from the canonical paradigm. 4. Non-regular forms: we can take advantage of empty cells in the data matrix in order to identify non-regularities such as suppletives, stem variants, semi-regular subclasses, and suffix allomorphs. If the expected frequency of a word form (as derived from the M matrix and frequency of a stem) is relatively high but the value in the D matrix is zero, this is evidence that a non-regular form may occupy this cell. Locating irregular words could use methods similar to those of (Yarowsky and Wicentowski 2000), who pair irregular inflections and their roots from raw text. Stem variants and allomorphic suffixes could be detected in a similar manner, by finding sets of stems/suffixes with mutually exclusive matrix entries. 5. Multiple morphological properties per word: we currently represent all morphological and POS information with a single suffix. The learning algorithm and representation could perhaps be modified to allow for multiple morphological properties. One could perform recursive LDA on a particular morphological property, then take each of the learned paradigms and perform recursive LDA </context>
</contexts>
<marker>Yarowsky, Wicentowski, 2000</marker>
<rawString>D. Yarowsky and R. Wicentowski. 2000. Minimally supervised morphological analysis by multimodal alignment. Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Zajac</author>
</authors>
<title>Morpholog: constrained and supervised learning of morphology.</title>
<date>2001</date>
<booktitle>Proceedings of CoNLL.</booktitle>
<contexts>
<context position="2830" citStr="Zajac 2001" startWordPosition="411" endWordPosition="412">or morphological analysis involves mapping between inflected and base of forms of words with finite-state transducers (FSTs), a procedural model of morphological relations. Rewrite rules are handcrafted and compiled into FSTs, and it would be beneficial if these rules could be learned automatically. One line of research in computational morphology has been directed towards learning finitestate mapping rules from some sort of paradigmatic structure, where all morphological forms and POS types are presumed known for a set of lexemes (Clark 2001, Kazakov and Manandhar 2001, Oflazer et. al. 2001, Zajac 2001, Albright 2002). This can be accomplished by first deciding on a base form, then learning rules to convert other forms of the paradigm into this base form. If one could develop an unsupervised algorithm for learning paradigms, it could serve as the input to rulelearning procedures, effectively leading to an entirely unsupervised system for learning FSTs from raw data. This is our long-term goal. 1 $ is the null suffix. 69 Proceedings of the Eighth Meeting of the ACL Special Interest Group on Computational Phonology at HLT-NAACL 2006, pages 69–78, New York City, USA, June 2006. c�2006 Associat</context>
</contexts>
<marker>Zajac, 2001</marker>
<rawString>R. Zajac. 2001. Morpholog: constrained and supervised learning of morphology. Proceedings of CoNLL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>