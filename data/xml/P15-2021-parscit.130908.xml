<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.015776">
<title confidence="0.997235">
Lexicon Stratification for Translating Out-of-Vocabulary Words
</title>
<author confidence="0.991293">
Yulia Tsvetkov
</author>
<affiliation confidence="0.9839705">
Language Technologies Institute
Carnegie Mellon University
</affiliation>
<address confidence="0.536664">
Pittsburgh, PA, 15213, USA
</address>
<email confidence="0.99904">
ytsvetko@cs.cmu.edu
</email>
<sectionHeader confidence="0.993902" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999865294117647">
A language lexicon can be divided into four
main strata, depending on origin of words:
core vocabulary words, fully- and partially-
assimilated foreign words, and unassim-
ilated foreign words (or transliterations).
This paper focuses on translation of fully-
and partially-assimilated foreign words,
called “borrowed words”. Borrowed words
(or loanwords) are content words found in
nearly all languages, occupying up to 70%
of the vocabulary. We use models of lexi-
cal borrowing in machine translation as a
pivoting mechanism to obtain translations
of out-of-vocabulary loanwords in a low-
resource language. Our framework obtains
substantial improvements (up to 1.6 BLEU)
over standard baselines.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.991164">
Out-of-vocabulary (OOV) words are a ubiquitous
and difficult problem in statistical machine transla-
tion (SMT). When a translation system encounters
an OOV—a word that was not observed in the train-
ing data, and the trained system thus lacks its trans-
lation variants—it usually outputs the word just as
it is in the source language, producing erroneous
and disfluent translations.
All SMT systems, even when trained on billion-
sentence-size parallel corpora, are prone to OOVs.
These are often named entities and neologisms.
However, OOV problem is much more serious in
low-resource scenarios: there, OOVs are primarily
not lexicon-peripheral items such as names and spe-
cialized/technical terms, but regular content words.
Procuring translations for OOVs has been a sub-
ject of active research for decades. Translation of
named entities is usually generated using translit-
eration techniques (Al-Onaizan and Knight, 2002;
Hermjakob et al., 2008; Habash, 2008). Extracting
</bodyText>
<author confidence="0.536467">
Chris Dyer
</author>
<affiliation confidence="0.649307666666667">
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, 15213, USA
</affiliation>
<email confidence="0.990209">
cdyer@cs.cmu.edu
</email>
<bodyText confidence="0.999337619047619">
a translation lexicon for recovering OOV content
words and phrases is done by mining bi-lingual
and monolingual resources (Rapp, 1995; Callison-
Burch et al., 2006; Haghighi et al., 2008; Mar-
ton et al., 2009; Razmara et al., 2013; Saluja
et al., 2014; Zhao et al., 2015). In addition,
OOV content words can be recovered by exploiting
cognates, by transliterating and then pivoting via
a closely-related resource-richer language, when
such a language exists (Hajiˇc et al., 2000; Mann
and Yarowsky, 2001; Kondrak et al., 2003; De Gis-
pert and Marino, 2006; Durrani et al., 2010; Wang
et al., 2012; Nakov and Ng, 2012; Dholakia and
Sarkar, 2014). Our work is similar in spirit to the
latter line of research, but we show how to curate
translations for OOV content words by pivoting via
an unrelated, often typologically distant resource-
rich languages. To achieve this goal, we replace
transliteration by a new technique that captures
more complex morpho-phonological transforma-
tions of historically-related words.
</bodyText>
<figureCaption confidence="0.951622">
Figure 1: A language lexicon can be divided into four main
strata, depending on origin of words. This work focuses on
fully- and partially-assimilated foreign words, called borrowed
words. Borrowed words (or loanwords) are content words
found in all languages, occupying up to 70% of the vocabulary.
</figureCaption>
<bodyText confidence="0.999588875">
Our method is inspired by prior research in
constraint-based phonology, advocating “lexicon
stratification,” i.e., splitting the language lexicon
into separate strata, depending on origin of words
and degree of their assimilation in the language (Itô
and Mester, 1995). As shown in figure 1, there are
four main strata: core vocabulary, foreign words
that are fully assimilated, partially-assimilated for-
</bodyText>
<figure confidence="0.9009338">
LEXICON
Peripheral
Partially assimilated
Fully assimilated
Core
</figure>
<page confidence="0.84606">
125
</page>
<bodyText confidence="0.95400859375">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 125–131,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
eign words, and named entities which belong to
the peripheral stratum. Our work focuses on the
fully- and partially-assimilated foreign words, i.e.,
words that historically were borrowed from another
language. Borrowing is the pervasive linguistic
phenomenon of transferring and adapting linguistic
constructions (lexical, phonological, morphologi-
cal, and syntactic) from a “donor” language into
a “recipient” language (Thomason and Kaufman,
2001). In this work, we advocate a pivoting mech-
anism exploiting lexical borrowing to bridge be-
tween resource-rich and resource-poor languages.
Our method (§2) employs a model of lexical
borrowing to obtain cross-lingual links from loan-
words in a low-resource language to their donors
in a resource-rich language (§2.1). The donor
language is used as pivot to obtain translations
via triangulation of OOV loanwords (§2.2). We
conduct experiments with two resource-poor se-
tups: Swahili–English, pivoting via Arabic, and
Romanian–English,pivoting via French (§3). We
provide a systematic quantitative analysis of con-
tribution of integrated OOV translations, relative
to baselines and upper bounds, and on corpora of
varying sizes (§4). The proposed approach yields
substantial improvement (up to +1.6 BLEU) in
Swahili–Arabic–English translation, and a small
but statistically significant improvement (+0.2
BLEU) in Romanian–French–English.
</bodyText>
<sectionHeader confidence="0.994197" genericHeader="introduction">
2 Methodology
</sectionHeader>
<bodyText confidence="0.999897111111111">
Our high-level solution is depicted in figure 2.
Given an OOV word in resource-poor SMT, we
plug it into a borrowing system (§2.1) that identi-
fies the list of plausible donor words in the donor
language. Then, using the resource-rich SMT, we
translate the donor words to the same target lan-
guage as in the resource-poor SMT (here, English).
Finally, we integrate translation candidates in the
resource-poor system (§2.2).
</bodyText>
<subsectionHeader confidence="0.999826">
2.1 Models of Lexical Borrowing
</subsectionHeader>
<bodyText confidence="0.999318">
Borrowed words (also called loanwords) are found
in nearly all languages, and routinely account for
10–70% of the vocabulary (Haspelmath and Tad-
mor, 2009). Borrowing occurs across genetically
and typologically unrelated languages, for exam-
ple, about 40% of Swahili’s vocabulary is borrowed
from Arabic (Johnson, 1939). Importantly, since
resource-rich languages are (historically) geopoliti-
cally important languages, borrowed words often
</bodyText>
<figureCaption confidence="0.99179775">
Figure 2: To improve a resource-poor Swahili–English SMT
system, we extract translation candidates for OOV Swahili
words borrowed from Arabic using the Swahili-to-Arabic bor-
rowing system and Arabic–English resource-rich SMT.
</figureCaption>
<bodyText confidence="0.999903303030303">
bridge between resource-rich and resource-limited
languages; we use this observation in our work.
Transliteration and cognate discovery models
perform poorly in the task of loanword genera-
tion/identification (Tsvetkov et al., 2015). The
main reason is that the recipient language, in which
borrowed words are fully or partially assimilated,
may have very different morpho-phonological prop-
erties from the donor language (e.g., ‘orange’ and
‘sugar’ are not perceived as foreign by native speak-
ers, but these are English words borrowed from
Arabic e&apos; ,U (nArnj)1 and &lt;_J1 (Alskr), respec-
tively). Therefore, morpho-phonological loanword
adaptation is more complex than is typically cap-
tured by transliteration or cognate models.
We employ a discriminative cross-lingual model
of lexical borrowing to identify plausible donors
given a loanword (Tsvetkov et al., 2015). The
model is implemented in a cascade of finite-state
transducers that first maps orthographic word forms
in two languages into a common space of their pho-
netic representation (using IPA—the International
Phonetic Alphabet), and then performs morpholog-
ical and phonological updates to the input word
in one language to identify its (donor/loan) coun-
terpart in another language. Transduction oper-
ations include stripping donor language prefixes
and suffixes, appending recipient affixes, insertion,
deletion, and substitution of consonants and vow-
els. The output of the model, given an input loan-
word, is a n-best list of donor candidates, ranked
by linguistic constraints of the donor and recipient
languages.2
</bodyText>
<footnote confidence="0.988445">
1We use Buckwalter notation to write Arabic glosses.
2In this work, we give as input into the borrowing system
all OOV words, although, clearly, not all OOVs are loanwords,
and not all loanword OOVs are borrowed from the donor
language. However, an important property of the borrowing
model is that its operations are not general, but specific to
</footnote>
<figure confidence="0.994487142857143">
SWAHILI-ENGLISH
safari   *OOV*
ARABIC-ENGLISH
ARABIC -to- SWAHILI
BORROWING
TRANSLATION
CANDIDATES
</figure>
<page confidence="0.961828">
126
</page>
<subsectionHeader confidence="0.997708">
2.2 Pivoting via Borrowing
</subsectionHeader>
<bodyText confidence="0.997635985507247">
We now discuss integrating translation candidates
acquired via borrowing plus resource-rich transla-
tion. For each OOV, the borrowing system pro-
duces the n-best list of plausible donors; for each
donor we then extract the k-best list of its transla-
tions.3 Then, we pair the OOV with the resulting
n × k translation candidates. The translation can-
didates are noisy: some of the generated donors
may be erroneous, the errors are then propagated
in translation. To allow the low-resource system
to leverage good translations that are missing in
the default phrase inventory, while being stable to
noisy translation hypotheses, we integrate the ac-
quired translation candidates as synthetic phrases
(Tsvetkov et al., 2013; Chahuneau et al., 2013).
Synthetic phrases is a strategy of integrating trans-
lated phrases directly in the MT translation model,
rather than via pre- or post-processing MT inputs
and outputs. Synthetic phrases are phrasal trans-
lations that are not directly extractable from the
training data, generated by auxiliary translation
and postediting processes (for example, extracted
from a borrowing model). An important advantage
of synthetic phrases is that they are recall-oriented,
allowing the system to leverage good translations
that are missing in the default phrase inventory,
while being stable to noisy translation hypotheses.
To let the translation model learn whether to trust
these phrases, the translation options obtained from
the borrowing model are augmented with a boolean
translation feature indicating that the phrase was
generated externally. Additional features annotat-
ing the integrated OOV translations correspond to
properties of the donor–loan words’ relation; their
goal is to provide an indication of plausibility of
the pair (to mark possible errors in the outputs of
the borrowing system).
We employ two types of features: phonetic and
semantic. Since borrowing is primarily a phonolog-
ical phenomenon, phonetic features will provide
an indication of how typical (or atypical) pronun-
ciation of the word in a language; loanwords are
expected to be less typical than core vocabulary
the language-pair and reduced only to a small set of plausible
changes that the donor word can undergo in the process of
assimilation in the recipient language. Thus, the borrowing
system only minimally overgenerates the set of output candi-
dates given an input. If the borrowing system encounters an
input word that was not borrowed from the target donor lan-
guage, it usually (but not always) produces an empty output.
3We set n and k to 5, we did not experiment with other
values.
words. The goal of semantic features is to mea-
sure semantic similarity between donor and loan
words: erroneous candidates and borrowed words
that changed meaning over time are expected to
have different meaning from the OOV.
Phonetic features. To compute phonetic fea-
tures we first train a (5-gram) language model (LM)
of IPA pronunciations of the donor/recipient lan-
guage vocabulary (phoneLM). Then, we re-score
pronunciations of the donor and loanword can-
didates using the LMs.4 We hypothesize that in
donor–loanword pairs the donor phoneLM score
is higher but the loanword score is lower (i.e., the
loanword phonology is atypical in the recipient lan-
guage). We capture this intuition in three features:
f1=PphoneLM(donor), f2=PphoneLM(loanword),
and the harmonic mean between the two scores
</bodyText>
<equation confidence="0.907038">
f3= 2f1f2
f1+f2 .
</equation>
<bodyText confidence="0.999356470588235">
Semantic features. We compute a semantic sim-
ilarity feature between the candidate donor and
the OOV loanword as follows. We first train, us-
ing large monolingual corpora, 100-dimensional
word vector representations for donor and recip-
ient language vocabularies.5 Then, we employ
canonical correlation analysis (CCA) with small
donor–loanword dictionaries (training sets in the
borrowing models) to project the word embeddings
into 50-dimensional vectors with maximized cor-
relation between their dimensions. The semantic
feature annotating the synthetic translation candi-
dates is cosine distance between the resulting donor
and loanword vectors. We use the word2vec tool
(Mikolov et al., 2013) to train monolingual vec-
tors,6 and the CCA-based tool (Faruqui and Dyer,
2014) for projecting word vectors.7
</bodyText>
<sectionHeader confidence="0.998955" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999577166666667">
Datasets and software. The Swahili–English
parallel corpus was crawled from the Global Voices
project website8. To simulate resource-poor sce-
nario for the Romanian–English language pair, we
sample a parallel corpus of same size from the tran-
scribed TED talks (Cettolo et al., 2012). To evalu-
</bodyText>
<footnote confidence="0.9849193">
4For Arabic and French we use the GlobalPhone pro-
nunciation dictionaries (Schultz et al., 2013) (we manually
convert them to IPA). For Swahili and Romanian we automati-
cally construct pronunciation dictionaries using the Omniglot
grapheme-to-IPA conversion rules at www.omniglot.com.
5We assume that while parallel data is limited in the recip-
ient language, monolingual data is available.
6code.google.com/p/word2vec
7github.com/mfaruqui/eacl14-cca
8sw.globalvoicesonline.org
</footnote>
<page confidence="0.993031">
127
</page>
<bodyText confidence="0.999753666666667">
ate translation improvement on corpora of different
sizes we conduct experiments with sub-sampled
4K, 8K, and 14K parallel sentences from the train-
ing corpora (the smaller the training corpus, the
more OOVs it has). Corpora sizes along with statis-
tics of source-side OOV tokens and types are given
in tables 1 and 2. Statistics of the held-out dev
and test sets used in all translation experiments are
given in table 3.
</bodyText>
<table confidence="0.9627848">
SW–EN RO–EN
dev test dev test
Sentences 1,552 1,732 2,687 2,265
Tokens 33,446 35,057 24,754 19,659
Types 7,008 7,180 5,141 4,328
</table>
<tableCaption confidence="0.999717">
Table 3: Dev and test corpora sizes.
</tableCaption>
<bodyText confidence="0.999966060606061">
In all the MT experiments, we use the cdec9
toolkit (Dyer et al., 2010), and optimize parameters
with MERT (Och, 2003). English 4-gram language
models with Kneser-Ney smoothing (Kneser and
Ney, 1995) are trained using KenLM (Heafield,
2011) on the target side of the parallel training cor-
pora and on the Gigaword corpus (Parker et al.,
2009). Results are reported using case-insensitive
BLEU with a single reference (Papineni et al.,
2002). We train three systems for each MT setup;
reported BLEU scores are averaged over systems.
Upper bounds. The goal of our experiments is
not only to evaluate the contribution of the OOV
dictionaries that we extract when pivoting via bor-
rowing, but also to understand the potential con-
tribution of the lexicon stratification. What is the
overall improvement that can be achieved if we cor-
rectly translate all OOVs that were borrowed from
another language? What is the overall improve-
ment that can be achieved if we correctly translate
all OOVs? We answer this question by defining
“upper bound” experiments. In the upper bound
experiment we word-align all available parallel cor-
pora, including dev and test sets, and extract from
the alignments oracle translations of OOV words.
Then, we append the extracted OOV dictionaries
to the training corpora and re-train SMT setups
without OOVs. Translation scores of the resulting
system provide an upper bound of an improvement
from correctly translating all OOVs. When we
append oracle translations of the subset of OOV
dictionaries, in particular translations of all OOVs
for which the output of the borrowing system is
</bodyText>
<footnote confidence="0.921463">
9www.cdec-decoder.org
</footnote>
<bodyText confidence="0.999766666666667">
not empty, we obtain an upper bound that can be
achieved using our method (if the borrowing sys-
tem provided perfect outputs). Understanding the
upper bounds is relevant not only for our experi-
ments, but for any experiments that involve aug-
menting translation dictionaries; however, we are
not aware of prior work providing similar analy-
sis of upper bounds, and we recommend this as
a calibrating procedure for future work on OOV
mitigation strategies.
Borrowing-augmented setups. As described in
§2.2, we integrate translations of OOV loanwords
in the translation model. Due to data sparsity,
we conjecture that non-OOVs that occur only few
times in the training corpus can also lack appro-
priate translation candidates, i.e., these are target-
language OOVs. We therefore run the borrowing
system on OOVs and non-OOV words that occur
less than 3 times in the training corpus. We list in
table 4 sizes of translated lexicons that we integrate
in translation tables.
</bodyText>
<table confidence="0.961898666666667">
4K 8K 14K
Loan OOVs in SW–EN 5,050 4,219 3,577
Loan OOVs in RO–EN 347 271 216
</table>
<tableCaption confidence="0.9820145">
Table 4: Sizes of translated lexicons extracted using pivoting
via borrowing and integrated in translation models.
</tableCaption>
<bodyText confidence="0.9964363125">
Transliteration-augmented setups. In ad-
dition to the standard baselines, we evaluate
transliteration-augmented setups, where we
replace the borrowing model by a transliteration
model (Ammar et al., 2012). The model is a
linear-chain CRF where we label each source
character with a sequence of target characters. The
features are label unigrams and bigrams, separately
or conjoined with a moving window of source
characters. We employ the Swahili–Arabic and
Romanian–French transliteration systems that
were used as baselines in (Tsvetkov et al., 2015).
As in the borrowing system, transliteration outputs
are filtered to contain only target language lexicons.
We list in table 5 sizes of obtained translated
lexicons.
</bodyText>
<table confidence="0.968972333333333">
4K 8K 14K
Translit. OOVs in SW–EN 49 32 22
Translit. OOVs in RO–EN 906 714 578
</table>
<tableCaption confidence="0.9943555">
Table 5: Sizes of translated lexicons extracted using pivoting
via transliteration and integrated in translation models.
</tableCaption>
<page confidence="0.918954">
128
</page>
<table confidence="0.9990716">
4K 8K 14K
Tokens 84,764 170,493 300,648
Types 14,554 23,134 33,288
OOV tokens 4,465 (12.7%) 3,509 (10.0%) 2,965 (8.4%)
OOV types 3,610 (50.3%) 2,950 (41.1%) 2,523 (35.1%)
</table>
<tableCaption confidence="0.995565">
Table 1: Statistics of the Swahili–English corpora and source-side OOV for 4K, 8K, 14K parallel training sentences.
</tableCaption>
<table confidence="0.9995358">
4K 8K 14K
Tokens 35,978 71,584 121,718
Types 7,210 11,144 15,112
OOV tokens 3,268 (16.6%) 2,585 (13.1%) 2,177 (11.1%)
OOV types 2,382 (55.0%) 1,922 (44.4%) 1,649 (38.1%)
</table>
<tableCaption confidence="0.998807">
Table 2: Statistics of the Romanian–English corpora and source-side OOV for 4K, 8K, 14K parallel training sentences.
</tableCaption>
<sectionHeader confidence="0.999511" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.995304428571429">
Translation results are shown in tables 6 and 7.
We evaluate separately the contribution of the in-
tegrated OOV translations, and the same transla-
tions annotated with phonetic and semantic fea-
tures. We also provide upper bound scores for
integrated loanword dictionaries as well as for re-
covering all OOVs.
</bodyText>
<table confidence="0.995512714285714">
4K 8K 14K
Baseline 13.2 15.1 17.1
+ Translit. OOVs 13.4 15.3 17.2
+ Loan OOVs 14.3 15.7 18.2
+ Features 14.8 16.4 18.4
Upper bound loan 18.9 19.1 20.7
Upper bound all OOVs 19.2 20.4 21.1
</table>
<tableCaption confidence="0.977454">
Table 6: Swahili–English MT experiments.
</tableCaption>
<table confidence="0.999686428571429">
4K 8K 14K
Baseline 15.8 18.5 20.7
+ Translit. OOVs 15.8 18.7 20.8
+ Loan OOVs 16.0 18.7 20.7
+ Features 16.0 18.6 20.6
Upper bound loan 16.6 19.4 20.9
Upper bound all OOVs 28.0 28.8 30.4
</table>
<tableCaption confidence="0.999533">
Table 7: Romanian–English MT experiments.
</tableCaption>
<bodyText confidence="0.99986692">
Swahili–English MT performance is improved
by up to +1.6 BLEU when we augment it
with translated OOV loanwords leveraged from
the Arabic–Swahili borrowing and then Arabic–
English MT. The contribution of the borrowing
dictionaries is +0.6–1.1 BLEU, and phonetic and
semantic features contribute additional half BLEU.
More importantly, upper bound results show that
the system can be improved more substantially with
better dictionaries of OOV loanwords. This result
confirms that OOV borrowed words is an important
type of OOVs, and with proper modeling it has the
potential to improve translation by a large margin.
Romanian–English systems obtain only small (but
significant for 4K and 8K, p &lt; .01) improvement.
However, this is expected as the rate of borrow-
ing from French into Romanian is smaller, and, as
the result, the integrated loanword dictionaries are
small. Transliteration baseline, conversely, is more
effective in Romanian–French language pair, as
two languages are related typologically, and have
common cognates in addition to loanwords. Still,
even with these dictionaries the translations with
pivoting via borrowing/transliteration improve, and
even almost approach the upper bounds results.
</bodyText>
<sectionHeader confidence="0.998904" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999973">
This paper focuses on fully- and partially-
assimilated foreign words in the source lexicon—
borrowed words—and a method for obtaining their
translations. Our results substantially improve
translation and confirm that OOV loanwords are
important and merit further investigation. In addi-
tion, we propose a simple technique to calculate an
upper bound of improvements that can be obtained
from integrating OOV translations in SMT.
</bodyText>
<sectionHeader confidence="0.998295" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.997325428571428">
This work was supported by the U.S. Army Re-
search Laboratory and the U.S. Army Research
Office under contract/grant number W911NF-10-1-
0533. Computational resources were provided by
Google Cloud Computing grant. We are grateful
to Waleed Ammar for his help with transliteration,
and to the anonymous reviewers.
</bodyText>
<page confidence="0.998139">
129
</page>
<sectionHeader confidence="0.989864" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999571112068966">
Yaser Al-Onaizan and Kevin Knight. 2002. Machine
transliteration of names in Arabic text. In Proc.
the ACL workshop on Computational Approaches to
Semitic Languages, pages 1–13.
Waleed Ammar, Chris Dyer, and Noah A. Smith. 2012.
Transliteration by sequence labeling with lattice en-
codings and reranking. In Proc. NEWS workshop at
ACL.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine transla-
tion using paraphrases. In Proc. NAACL, pages 17–
24.
Mauro Cettolo, Christian Girardi, and Marcello Fed-
erico. 2012. WIT3: Web inventory of transcribed
and translated talks. In Proc. EAMT, pages 261–
268.
Victor Chahuneau, Eva Schlinger, Noah A Smith, and
Chris Dyer. 2013. Translating into morphologi-
cally rich languages with synthetic phrases. In Proc.
EMNLP, pages 1677–1687.
Adrià De Gispert and Jose B Marino. 2006. Catalan-
English statistical machine translation without par-
allel corpus: bridging through Spanish. In Proc.
LREC, pages 65–68.
Rohit Dholakia and Anoop Sarkar. 2014. Pivot-based
triangulation for low-resource languages. In Proc.
AMTA, pages 315–328.
Nadir Durrani, Hassan Sajjad, Alexander Fraser, and
Helmut Schmid. 2010. Hindi-to-Urdu machine
translation through transliteration. In Proc. ACL,
pages 465–474.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proc. ACL System Demonstrations, pages 7–12.
Manaal Faruqui and Chris Dyer. 2014. Improving
vector space word representations using multilingual
correlation. In Proc. EACL, pages 462–471.
Nizar Habash. 2008. Four techniques for online han-
dling of out-of-vocabulary words in Arabic-English
statistical machine translation. In Proc. ACL, pages
57–60.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proc. ACL, pages
771–779.
Jan Hajiˇc, Jan Hric, and Vladislav Kuboˇn. 2000. Ma-
chine translation of very close languages. In Proc.
ANLP, pages 7–12.
Martin Haspelmath and Uri Tadmor, editors. 2009.
Loanwords in the World’s Languages: A Compara-
tive Handbook. Max Planck Institute for Evolution-
ary Anthropology, Leipzig.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proc. WMT, pages 187–
197.
Ulf Hermjakob, Kevin Knight, and Hal Daumé III.
2008. Name translation in statistical machine
translation-learning when to transliterate. In Proc.
ACL, pages 389–397.
Junko Itô and Armin Mester. 1995. The core-periphery
structure of the lexicon and constraints on reranking.
Papers in Optimality Theory, 18:181–209.
Frederick Johnson. 1939. Standard Swahili-English
dictionary. Oxford University Press.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Proc.
ICASSP, volume 1, pages 181–184.
Grzegorz Kondrak, Daniel Marcu, and Kevin Knight.
2003. Cognates can improve statistical translation
models. In Proc. HLT-NAACL, pages 46–48.
Gideon S Mann and David Yarowsky. 2001. Multipath
translation lexicon induction via bridge languages.
In Proc. HLT-NAACL, pages 1–8.
Yuval Marton, Chris Callison-Burch, and Philip Resnik.
2009. Improved statistical machine translation us-
ing monolingually-derived paraphrases. In Proc.
EMNLP, pages 381–390.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-
rado, and Jeff Dean. 2013. Distributed representa-
tions of words and phrases and their compositional-
ity. In Proc. NIPS, pages 3111–3119.
Preslav Nakov and Hwee Tou Ng. 2012. Improv-
ing statistical machine translation for a resource-
poor language using related resource-rich languages.
Journal of Artificial Intelligence Research, pages
179–222.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. ACL, pages
160–167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proc. ACL, pages
311–318.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2009. English Gigaword fourth
edition.
Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proc. ACL, pages 320–322.
Majid Razmara, Maryam Siahbani, Reza Haffari, and
Anoop Sarkar. 2013. Graph propagation for para-
phrasing out-of-vocabulary words in statistical ma-
chine translation. In Proc. ACL, pages 1105–1115.
Avneesh Saluja, Hany Hassan, Kristina Toutanova, and
Chris Quirk. 2014. Graph-based semi-supervised
learning of translation models from monolingual
data. In Proc. ACL, pages 676–686.
Tanja Schultz, Ngoc Thang Vu, and Tim Schlippe.
2013. GlobalPhone: A multilingual text &amp; speech
database in 20 languages. In Proc. ICASSP, pages
8126–8130.
Sarah Grey Thomason and Terrence Kaufman. 2001.
Language contact. Edinburgh University Press Ed-
inburgh.
</reference>
<page confidence="0.971039">
130
</page>
<reference confidence="0.999567846153846">
Yulia Tsvetkov, Chris Dyer, Lori Levin, and Archna
Bhatia. 2013. Generating English determiners in
phrase-based translation with synthetic translation
options. In Proc. WMT, pages 271–280.
Yulia Tsvetkov, Waleed Ammar, and Chris Dyer. 2015.
Constraint-based models of lexical borrowing. In
Proc. NAACL, pages 598–608.
Pidong Wang, Preslav Nakov, and Hwee Tou Ng. 2012.
Source language adaptation for resource-poor ma-
chine translation. In Proc. EMNLP, pages 286–296.
Kai Zhao, Hany Hassan, and Michael Auli. 2015.
Learning translation models from monolingual con-
tinuous representations. In Proc. NAACL.
</reference>
<page confidence="0.998309">
131
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.957355">
<title confidence="0.99991">Lexicon Stratification for Translating Out-of-Vocabulary Words</title>
<author confidence="0.976907">Yulia Tsvetkov</author>
<affiliation confidence="0.994646">Language Technologies Institute Carnegie Mellon University</affiliation>
<address confidence="0.999308">Pittsburgh, PA, 15213, USA</address>
<email confidence="0.999814">ytsvetko@cs.cmu.edu</email>
<abstract confidence="0.9991745">A language lexicon can be divided into four main strata, depending on origin of words: core vocabulary words, fullyand partiallyassimilated foreign words, and unassimilated foreign words (or transliterations). This paper focuses on translation of fullyand partially-assimilated foreign words, called “borrowed words”. Borrowed words (or loanwords) are content words found in nearly all languages, occupying up to 70% of the vocabulary. We use models of lexical borrowing in machine translation as a pivoting mechanism to obtain translations of out-of-vocabulary loanwords in a lowresource language. Our framework obtains substantial improvements (up to 1.6 BLEU) over standard baselines.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>and to the anonymous reviewers.</title>
<marker></marker>
<rawString>and to the anonymous reviewers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Kevin Knight</author>
</authors>
<title>Machine transliteration of names in Arabic text.</title>
<date>2002</date>
<booktitle>In Proc. the ACL workshop on Computational Approaches to Semitic Languages,</booktitle>
<pages>1--13</pages>
<contexts>
<context position="1813" citStr="Al-Onaizan and Knight, 2002" startWordPosition="257" endWordPosition="260">word just as it is in the source language, producing erroneous and disfluent translations. All SMT systems, even when trained on billionsentence-size parallel corpora, are prone to OOVs. These are often named entities and neologisms. However, OOV problem is much more serious in low-resource scenarios: there, OOVs are primarily not lexicon-peripheral items such as names and specialized/technical terms, but regular content words. Procuring translations for OOVs has been a subject of active research for decades. Translation of named entities is usually generated using transliteration techniques (Al-Onaizan and Knight, 2002; Hermjakob et al., 2008; Habash, 2008). Extracting Chris Dyer Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, 15213, USA cdyer@cs.cmu.edu a translation lexicon for recovering OOV content words and phrases is done by mining bi-lingual and monolingual resources (Rapp, 1995; CallisonBurch et al., 2006; Haghighi et al., 2008; Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). In addition, OOV content words can be recovered by exploiting cognates, by transliterating and then pivoting via a closely-related resource-richer language, when su</context>
</contexts>
<marker>Al-Onaizan, Knight, 2002</marker>
<rawString>Yaser Al-Onaizan and Kevin Knight. 2002. Machine transliteration of names in Arabic text. In Proc. the ACL workshop on Computational Approaches to Semitic Languages, pages 1–13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Waleed Ammar</author>
<author>Chris Dyer</author>
<author>Noah A Smith</author>
</authors>
<title>Transliteration by sequence labeling with lattice encodings and reranking.</title>
<date>2012</date>
<booktitle>In Proc. NEWS workshop at ACL.</booktitle>
<contexts>
<context position="17178" citStr="Ammar et al., 2012" startWordPosition="2576" endWordPosition="2579">etlanguage OOVs. We therefore run the borrowing system on OOVs and non-OOV words that occur less than 3 times in the training corpus. We list in table 4 sizes of translated lexicons that we integrate in translation tables. 4K 8K 14K Loan OOVs in SW–EN 5,050 4,219 3,577 Loan OOVs in RO–EN 347 271 216 Table 4: Sizes of translated lexicons extracted using pivoting via borrowing and integrated in translation models. Transliteration-augmented setups. In addition to the standard baselines, we evaluate transliteration-augmented setups, where we replace the borrowing model by a transliteration model (Ammar et al., 2012). The model is a linear-chain CRF where we label each source character with a sequence of target characters. The features are label unigrams and bigrams, separately or conjoined with a moving window of source characters. We employ the Swahili–Arabic and Romanian–French transliteration systems that were used as baselines in (Tsvetkov et al., 2015). As in the borrowing system, transliteration outputs are filtered to contain only target language lexicons. We list in table 5 sizes of obtained translated lexicons. 4K 8K 14K Translit. OOVs in SW–EN 49 32 22 Translit. OOVs in RO–EN 906 714 578 Table </context>
</contexts>
<marker>Ammar, Dyer, Smith, 2012</marker>
<rawString>Waleed Ammar, Chris Dyer, and Noah A. Smith. 2012. Transliteration by sequence labeling with lattice encodings and reranking. In Proc. NEWS workshop at ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Miles Osborne</author>
</authors>
<title>Improved statistical machine translation using paraphrases.</title>
<date>2006</date>
<booktitle>In Proc. NAACL,</booktitle>
<pages>17--24</pages>
<marker>Callison-Burch, Koehn, Osborne, 2006</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, and Miles Osborne. 2006. Improved statistical machine translation using paraphrases. In Proc. NAACL, pages 17– 24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mauro Cettolo</author>
<author>Christian Girardi</author>
<author>Marcello Federico</author>
</authors>
<title>WIT3: Web inventory of transcribed and translated talks.</title>
<date>2012</date>
<booktitle>In Proc. EAMT,</booktitle>
<pages>261--268</pages>
<contexts>
<context position="13118" citStr="Cettolo et al., 2012" startWordPosition="1939" endWordPosition="1942">tween their dimensions. The semantic feature annotating the synthetic translation candidates is cosine distance between the resulting donor and loanword vectors. We use the word2vec tool (Mikolov et al., 2013) to train monolingual vectors,6 and the CCA-based tool (Faruqui and Dyer, 2014) for projecting word vectors.7 3 Experimental Setup Datasets and software. The Swahili–English parallel corpus was crawled from the Global Voices project website8. To simulate resource-poor scenario for the Romanian–English language pair, we sample a parallel corpus of same size from the transcribed TED talks (Cettolo et al., 2012). To evalu4For Arabic and French we use the GlobalPhone pronunciation dictionaries (Schultz et al., 2013) (we manually convert them to IPA). For Swahili and Romanian we automatically construct pronunciation dictionaries using the Omniglot grapheme-to-IPA conversion rules at www.omniglot.com. 5We assume that while parallel data is limited in the recipient language, monolingual data is available. 6code.google.com/p/word2vec 7github.com/mfaruqui/eacl14-cca 8sw.globalvoicesonline.org 127 ate translation improvement on corpora of different sizes we conduct experiments with sub-sampled 4K, 8K, and 1</context>
</contexts>
<marker>Cettolo, Girardi, Federico, 2012</marker>
<rawString>Mauro Cettolo, Christian Girardi, and Marcello Federico. 2012. WIT3: Web inventory of transcribed and translated talks. In Proc. EAMT, pages 261– 268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Chahuneau</author>
<author>Eva Schlinger</author>
<author>Noah A Smith</author>
<author>Chris Dyer</author>
</authors>
<title>Translating into morphologically rich languages with synthetic phrases.</title>
<date>2013</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>1677--1687</pages>
<contexts>
<context position="9338" citStr="Chahuneau et al., 2013" startWordPosition="1363" endWordPosition="1366">, the borrowing system produces the n-best list of plausible donors; for each donor we then extract the k-best list of its translations.3 Then, we pair the OOV with the resulting n × k translation candidates. The translation candidates are noisy: some of the generated donors may be erroneous, the errors are then propagated in translation. To allow the low-resource system to leverage good translations that are missing in the default phrase inventory, while being stable to noisy translation hypotheses, we integrate the acquired translation candidates as synthetic phrases (Tsvetkov et al., 2013; Chahuneau et al., 2013). Synthetic phrases is a strategy of integrating translated phrases directly in the MT translation model, rather than via pre- or post-processing MT inputs and outputs. Synthetic phrases are phrasal translations that are not directly extractable from the training data, generated by auxiliary translation and postediting processes (for example, extracted from a borrowing model). An important advantage of synthetic phrases is that they are recall-oriented, allowing the system to leverage good translations that are missing in the default phrase inventory, while being stable to noisy translation hy</context>
</contexts>
<marker>Chahuneau, Schlinger, Smith, Dyer, 2013</marker>
<rawString>Victor Chahuneau, Eva Schlinger, Noah A Smith, and Chris Dyer. 2013. Translating into morphologically rich languages with synthetic phrases. In Proc. EMNLP, pages 1677–1687.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adrià De Gispert</author>
<author>Jose B Marino</author>
</authors>
<title>CatalanEnglish statistical machine translation without parallel corpus: bridging through Spanish. In</title>
<date>2006</date>
<booktitle>Proc. LREC,</booktitle>
<pages>65--68</pages>
<marker>De Gispert, Marino, 2006</marker>
<rawString>Adrià De Gispert and Jose B Marino. 2006. CatalanEnglish statistical machine translation without parallel corpus: bridging through Spanish. In Proc. LREC, pages 65–68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohit Dholakia</author>
<author>Anoop Sarkar</author>
</authors>
<title>Pivot-based triangulation for low-resource languages. In</title>
<date>2014</date>
<booktitle>Proc. AMTA,</booktitle>
<pages>315--328</pages>
<contexts>
<context position="2619" citStr="Dholakia and Sarkar, 2014" startWordPosition="383" endWordPosition="386">lexicon for recovering OOV content words and phrases is done by mining bi-lingual and monolingual resources (Rapp, 1995; CallisonBurch et al., 2006; Haghighi et al., 2008; Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). In addition, OOV content words can be recovered by exploiting cognates, by transliterating and then pivoting via a closely-related resource-richer language, when such a language exists (Hajiˇc et al., 2000; Mann and Yarowsky, 2001; Kondrak et al., 2003; De Gispert and Marino, 2006; Durrani et al., 2010; Wang et al., 2012; Nakov and Ng, 2012; Dholakia and Sarkar, 2014). Our work is similar in spirit to the latter line of research, but we show how to curate translations for OOV content words by pivoting via an unrelated, often typologically distant resourcerich languages. To achieve this goal, we replace transliteration by a new technique that captures more complex morpho-phonological transformations of historically-related words. Figure 1: A language lexicon can be divided into four main strata, depending on origin of words. This work focuses on fully- and partially-assimilated foreign words, called borrowed words. Borrowed words (or loanwords) are content </context>
</contexts>
<marker>Dholakia, Sarkar, 2014</marker>
<rawString>Rohit Dholakia and Anoop Sarkar. 2014. Pivot-based triangulation for low-resource languages. In Proc. AMTA, pages 315–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadir Durrani</author>
<author>Hassan Sajjad</author>
<author>Alexander Fraser</author>
<author>Helmut Schmid</author>
</authors>
<title>Hindi-to-Urdu machine translation through transliteration.</title>
<date>2010</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>465--474</pages>
<contexts>
<context position="2552" citStr="Durrani et al., 2010" startWordPosition="371" endWordPosition="374">ty Pittsburgh, PA, 15213, USA cdyer@cs.cmu.edu a translation lexicon for recovering OOV content words and phrases is done by mining bi-lingual and monolingual resources (Rapp, 1995; CallisonBurch et al., 2006; Haghighi et al., 2008; Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). In addition, OOV content words can be recovered by exploiting cognates, by transliterating and then pivoting via a closely-related resource-richer language, when such a language exists (Hajiˇc et al., 2000; Mann and Yarowsky, 2001; Kondrak et al., 2003; De Gispert and Marino, 2006; Durrani et al., 2010; Wang et al., 2012; Nakov and Ng, 2012; Dholakia and Sarkar, 2014). Our work is similar in spirit to the latter line of research, but we show how to curate translations for OOV content words by pivoting via an unrelated, often typologically distant resourcerich languages. To achieve this goal, we replace transliteration by a new technique that captures more complex morpho-phonological transformations of historically-related words. Figure 1: A language lexicon can be divided into four main strata, depending on origin of words. This work focuses on fully- and partially-assimilated foreign words</context>
</contexts>
<marker>Durrani, Sajjad, Fraser, Schmid, 2010</marker>
<rawString>Nadir Durrani, Hassan Sajjad, Alexander Fraser, and Helmut Schmid. 2010. Hindi-to-Urdu machine translation through transliteration. In Proc. ACL, pages 465–474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Adam Lopez</author>
<author>Juri Ganitkevitch</author>
<author>Johnathan Weese</author>
<author>Ferhan Ture</author>
<author>Phil Blunsom</author>
<author>Hendra Setiawan</author>
<author>Vladimir Eidelman</author>
<author>Philip Resnik</author>
</authors>
<title>cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models.</title>
<date>2010</date>
<booktitle>In Proc. ACL System Demonstrations,</booktitle>
<pages>7--12</pages>
<contexts>
<context position="14264" citStr="Dyer et al., 2010" startWordPosition="2114" endWordPosition="2117">of different sizes we conduct experiments with sub-sampled 4K, 8K, and 14K parallel sentences from the training corpora (the smaller the training corpus, the more OOVs it has). Corpora sizes along with statistics of source-side OOV tokens and types are given in tables 1 and 2. Statistics of the held-out dev and test sets used in all translation experiments are given in table 3. SW–EN RO–EN dev test dev test Sentences 1,552 1,732 2,687 2,265 Tokens 33,446 35,057 24,754 19,659 Types 7,008 7,180 5,141 4,328 Table 3: Dev and test corpora sizes. In all the MT experiments, we use the cdec9 toolkit (Dyer et al., 2010), and optimize parameters with MERT (Och, 2003). English 4-gram language models with Kneser-Ney smoothing (Kneser and Ney, 1995) are trained using KenLM (Heafield, 2011) on the target side of the parallel training corpora and on the Gigaword corpus (Parker et al., 2009). Results are reported using case-insensitive BLEU with a single reference (Papineni et al., 2002). We train three systems for each MT setup; reported BLEU scores are averaged over systems. Upper bounds. The goal of our experiments is not only to evaluate the contribution of the OOV dictionaries that we extract when pivoting via</context>
</contexts>
<marker>Dyer, Lopez, Ganitkevitch, Weese, Ture, Blunsom, Setiawan, Eidelman, Resnik, 2010</marker>
<rawString>Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan, Vladimir Eidelman, and Philip Resnik. 2010. cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models. In Proc. ACL System Demonstrations, pages 7–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manaal Faruqui</author>
<author>Chris Dyer</author>
</authors>
<title>Improving vector space word representations using multilingual correlation.</title>
<date>2014</date>
<booktitle>In Proc. EACL,</booktitle>
<pages>462--471</pages>
<contexts>
<context position="12785" citStr="Faruqui and Dyer, 2014" startWordPosition="1889" endWordPosition="1892">e monolingual corpora, 100-dimensional word vector representations for donor and recipient language vocabularies.5 Then, we employ canonical correlation analysis (CCA) with small donor–loanword dictionaries (training sets in the borrowing models) to project the word embeddings into 50-dimensional vectors with maximized correlation between their dimensions. The semantic feature annotating the synthetic translation candidates is cosine distance between the resulting donor and loanword vectors. We use the word2vec tool (Mikolov et al., 2013) to train monolingual vectors,6 and the CCA-based tool (Faruqui and Dyer, 2014) for projecting word vectors.7 3 Experimental Setup Datasets and software. The Swahili–English parallel corpus was crawled from the Global Voices project website8. To simulate resource-poor scenario for the Romanian–English language pair, we sample a parallel corpus of same size from the transcribed TED talks (Cettolo et al., 2012). To evalu4For Arabic and French we use the GlobalPhone pronunciation dictionaries (Schultz et al., 2013) (we manually convert them to IPA). For Swahili and Romanian we automatically construct pronunciation dictionaries using the Omniglot grapheme-to-IPA conversion r</context>
</contexts>
<marker>Faruqui, Dyer, 2014</marker>
<rawString>Manaal Faruqui and Chris Dyer. 2014. Improving vector space word representations using multilingual correlation. In Proc. EACL, pages 462–471.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
</authors>
<title>Four techniques for online handling of out-of-vocabulary words in Arabic-English statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>57--60</pages>
<contexts>
<context position="1852" citStr="Habash, 2008" startWordPosition="265" endWordPosition="266">erroneous and disfluent translations. All SMT systems, even when trained on billionsentence-size parallel corpora, are prone to OOVs. These are often named entities and neologisms. However, OOV problem is much more serious in low-resource scenarios: there, OOVs are primarily not lexicon-peripheral items such as names and specialized/technical terms, but regular content words. Procuring translations for OOVs has been a subject of active research for decades. Translation of named entities is usually generated using transliteration techniques (Al-Onaizan and Knight, 2002; Hermjakob et al., 2008; Habash, 2008). Extracting Chris Dyer Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, 15213, USA cdyer@cs.cmu.edu a translation lexicon for recovering OOV content words and phrases is done by mining bi-lingual and monolingual resources (Rapp, 1995; CallisonBurch et al., 2006; Haghighi et al., 2008; Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). In addition, OOV content words can be recovered by exploiting cognates, by transliterating and then pivoting via a closely-related resource-richer language, when such a language exists (Hajiˇc et al., 20</context>
</contexts>
<marker>Habash, 2008</marker>
<rawString>Nizar Habash. 2008. Four techniques for online handling of out-of-vocabulary words in Arabic-English statistical machine translation. In Proc. ACL, pages 57–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Percy Liang</author>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Klein</author>
</authors>
<title>Learning bilingual lexicons from monolingual corpora.</title>
<date>2008</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>771--779</pages>
<contexts>
<context position="2163" citStr="Haghighi et al., 2008" startWordPosition="306" endWordPosition="309">s such as names and specialized/technical terms, but regular content words. Procuring translations for OOVs has been a subject of active research for decades. Translation of named entities is usually generated using transliteration techniques (Al-Onaizan and Knight, 2002; Hermjakob et al., 2008; Habash, 2008). Extracting Chris Dyer Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, 15213, USA cdyer@cs.cmu.edu a translation lexicon for recovering OOV content words and phrases is done by mining bi-lingual and monolingual resources (Rapp, 1995; CallisonBurch et al., 2006; Haghighi et al., 2008; Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). In addition, OOV content words can be recovered by exploiting cognates, by transliterating and then pivoting via a closely-related resource-richer language, when such a language exists (Hajiˇc et al., 2000; Mann and Yarowsky, 2001; Kondrak et al., 2003; De Gispert and Marino, 2006; Durrani et al., 2010; Wang et al., 2012; Nakov and Ng, 2012; Dholakia and Sarkar, 2014). Our work is similar in spirit to the latter line of research, but we show how to curate translations for OOV content words by pivoting via an </context>
</contexts>
<marker>Haghighi, Liang, Berg-Kirkpatrick, Klein, 2008</marker>
<rawString>Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick, and Dan Klein. 2008. Learning bilingual lexicons from monolingual corpora. In Proc. ACL, pages 771–779.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
<author>Jan Hric</author>
<author>Vladislav Kuboˇn</author>
</authors>
<title>Machine translation of very close languages.</title>
<date>2000</date>
<booktitle>In Proc. ANLP,</booktitle>
<pages>7--12</pages>
<marker>Hajiˇc, Hric, Kuboˇn, 2000</marker>
<rawString>Jan Hajiˇc, Jan Hric, and Vladislav Kuboˇn. 2000. Machine translation of very close languages. In Proc. ANLP, pages 7–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Haspelmath</author>
<author>Uri Tadmor</author>
<author>editors</author>
</authors>
<date>2009</date>
<booktitle>Loanwords in the World’s Languages: A Comparative Handbook. Max Planck Institute for Evolutionary Anthropology,</booktitle>
<location>Leipzig.</location>
<marker>Haspelmath, Tadmor, editors, 2009</marker>
<rawString>Martin Haspelmath and Uri Tadmor, editors. 2009. Loanwords in the World’s Languages: A Comparative Handbook. Max Planck Institute for Evolutionary Anthropology, Leipzig.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
</authors>
<title>KenLM: Faster and smaller language model queries. In</title>
<date>2011</date>
<booktitle>Proc. WMT,</booktitle>
<pages>187--197</pages>
<contexts>
<context position="14433" citStr="Heafield, 2011" startWordPosition="2140" endWordPosition="2141">as). Corpora sizes along with statistics of source-side OOV tokens and types are given in tables 1 and 2. Statistics of the held-out dev and test sets used in all translation experiments are given in table 3. SW–EN RO–EN dev test dev test Sentences 1,552 1,732 2,687 2,265 Tokens 33,446 35,057 24,754 19,659 Types 7,008 7,180 5,141 4,328 Table 3: Dev and test corpora sizes. In all the MT experiments, we use the cdec9 toolkit (Dyer et al., 2010), and optimize parameters with MERT (Och, 2003). English 4-gram language models with Kneser-Ney smoothing (Kneser and Ney, 1995) are trained using KenLM (Heafield, 2011) on the target side of the parallel training corpora and on the Gigaword corpus (Parker et al., 2009). Results are reported using case-insensitive BLEU with a single reference (Papineni et al., 2002). We train three systems for each MT setup; reported BLEU scores are averaged over systems. Upper bounds. The goal of our experiments is not only to evaluate the contribution of the OOV dictionaries that we extract when pivoting via borrowing, but also to understand the potential contribution of the lexicon stratification. What is the overall improvement that can be achieved if we correctly transla</context>
</contexts>
<marker>Heafield, 2011</marker>
<rawString>Kenneth Heafield. 2011. KenLM: Faster and smaller language model queries. In Proc. WMT, pages 187– 197.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulf Hermjakob</author>
<author>Kevin Knight</author>
<author>Hal Daumé</author>
</authors>
<title>Name translation in statistical machine translation-learning when to transliterate.</title>
<date>2008</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>389--397</pages>
<contexts>
<context position="1837" citStr="Hermjakob et al., 2008" startWordPosition="261" endWordPosition="264">rce language, producing erroneous and disfluent translations. All SMT systems, even when trained on billionsentence-size parallel corpora, are prone to OOVs. These are often named entities and neologisms. However, OOV problem is much more serious in low-resource scenarios: there, OOVs are primarily not lexicon-peripheral items such as names and specialized/technical terms, but regular content words. Procuring translations for OOVs has been a subject of active research for decades. Translation of named entities is usually generated using transliteration techniques (Al-Onaizan and Knight, 2002; Hermjakob et al., 2008; Habash, 2008). Extracting Chris Dyer Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, 15213, USA cdyer@cs.cmu.edu a translation lexicon for recovering OOV content words and phrases is done by mining bi-lingual and monolingual resources (Rapp, 1995; CallisonBurch et al., 2006; Haghighi et al., 2008; Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). In addition, OOV content words can be recovered by exploiting cognates, by transliterating and then pivoting via a closely-related resource-richer language, when such a language exists (Ha</context>
</contexts>
<marker>Hermjakob, Knight, Daumé, 2008</marker>
<rawString>Ulf Hermjakob, Kevin Knight, and Hal Daumé III. 2008. Name translation in statistical machine translation-learning when to transliterate. In Proc. ACL, pages 389–397.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junko Itô</author>
<author>Armin Mester</author>
</authors>
<title>The core-periphery structure of the lexicon and constraints on reranking.</title>
<date>1995</date>
<booktitle>Papers in Optimality Theory,</booktitle>
<pages>18--181</pages>
<contexts>
<context position="3556" citStr="Itô and Mester, 1995" startWordPosition="523" endWordPosition="526">gical transformations of historically-related words. Figure 1: A language lexicon can be divided into four main strata, depending on origin of words. This work focuses on fully- and partially-assimilated foreign words, called borrowed words. Borrowed words (or loanwords) are content words found in all languages, occupying up to 70% of the vocabulary. Our method is inspired by prior research in constraint-based phonology, advocating “lexicon stratification,” i.e., splitting the language lexicon into separate strata, depending on origin of words and degree of their assimilation in the language (Itô and Mester, 1995). As shown in figure 1, there are four main strata: core vocabulary, foreign words that are fully assimilated, partially-assimilated forLEXICON Peripheral Partially assimilated Fully assimilated Core 125 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 125–131, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics eign words, and named entities which belong to the peripheral stratum. Our work focuses on the fully- and partially-assi</context>
</contexts>
<marker>Itô, Mester, 1995</marker>
<rawString>Junko Itô and Armin Mester. 1995. The core-periphery structure of the lexicon and constraints on reranking. Papers in Optimality Theory, 18:181–209.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Johnson</author>
</authors>
<title>Standard Swahili-English dictionary.</title>
<date>1939</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="6201" citStr="Johnson, 1939" startWordPosition="902" endWordPosition="903">f plausible donor words in the donor language. Then, using the resource-rich SMT, we translate the donor words to the same target language as in the resource-poor SMT (here, English). Finally, we integrate translation candidates in the resource-poor system (§2.2). 2.1 Models of Lexical Borrowing Borrowed words (also called loanwords) are found in nearly all languages, and routinely account for 10–70% of the vocabulary (Haspelmath and Tadmor, 2009). Borrowing occurs across genetically and typologically unrelated languages, for example, about 40% of Swahili’s vocabulary is borrowed from Arabic (Johnson, 1939). Importantly, since resource-rich languages are (historically) geopolitically important languages, borrowed words often Figure 2: To improve a resource-poor Swahili–English SMT system, we extract translation candidates for OOV Swahili words borrowed from Arabic using the Swahili-to-Arabic borrowing system and Arabic–English resource-rich SMT. bridge between resource-rich and resource-limited languages; we use this observation in our work. Transliteration and cognate discovery models perform poorly in the task of loanword generation/identification (Tsvetkov et al., 2015). The main reason is th</context>
</contexts>
<marker>Johnson, 1939</marker>
<rawString>Frederick Johnson. 1939. Standard Swahili-English dictionary. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In Proc. ICASSP,</booktitle>
<volume>1</volume>
<pages>181--184</pages>
<contexts>
<context position="14392" citStr="Kneser and Ney, 1995" startWordPosition="2132" endWordPosition="2135">smaller the training corpus, the more OOVs it has). Corpora sizes along with statistics of source-side OOV tokens and types are given in tables 1 and 2. Statistics of the held-out dev and test sets used in all translation experiments are given in table 3. SW–EN RO–EN dev test dev test Sentences 1,552 1,732 2,687 2,265 Tokens 33,446 35,057 24,754 19,659 Types 7,008 7,180 5,141 4,328 Table 3: Dev and test corpora sizes. In all the MT experiments, we use the cdec9 toolkit (Dyer et al., 2010), and optimize parameters with MERT (Och, 2003). English 4-gram language models with Kneser-Ney smoothing (Kneser and Ney, 1995) are trained using KenLM (Heafield, 2011) on the target side of the parallel training corpora and on the Gigaword corpus (Parker et al., 2009). Results are reported using case-insensitive BLEU with a single reference (Papineni et al., 2002). We train three systems for each MT setup; reported BLEU scores are averaged over systems. Upper bounds. The goal of our experiments is not only to evaluate the contribution of the OOV dictionaries that we extract when pivoting via borrowing, but also to understand the potential contribution of the lexicon stratification. What is the overall improvement tha</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In Proc. ICASSP, volume 1, pages 181–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grzegorz Kondrak</author>
<author>Daniel Marcu</author>
<author>Kevin Knight</author>
</authors>
<title>Cognates can improve statistical translation models.</title>
<date>2003</date>
<booktitle>In Proc. HLT-NAACL,</booktitle>
<pages>46--48</pages>
<contexts>
<context position="2501" citStr="Kondrak et al., 2003" startWordPosition="361" endWordPosition="364">age Technologies Institute Carnegie Mellon University Pittsburgh, PA, 15213, USA cdyer@cs.cmu.edu a translation lexicon for recovering OOV content words and phrases is done by mining bi-lingual and monolingual resources (Rapp, 1995; CallisonBurch et al., 2006; Haghighi et al., 2008; Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). In addition, OOV content words can be recovered by exploiting cognates, by transliterating and then pivoting via a closely-related resource-richer language, when such a language exists (Hajiˇc et al., 2000; Mann and Yarowsky, 2001; Kondrak et al., 2003; De Gispert and Marino, 2006; Durrani et al., 2010; Wang et al., 2012; Nakov and Ng, 2012; Dholakia and Sarkar, 2014). Our work is similar in spirit to the latter line of research, but we show how to curate translations for OOV content words by pivoting via an unrelated, often typologically distant resourcerich languages. To achieve this goal, we replace transliteration by a new technique that captures more complex morpho-phonological transformations of historically-related words. Figure 1: A language lexicon can be divided into four main strata, depending on origin of words. This work focuse</context>
</contexts>
<marker>Kondrak, Marcu, Knight, 2003</marker>
<rawString>Grzegorz Kondrak, Daniel Marcu, and Kevin Knight. 2003. Cognates can improve statistical translation models. In Proc. HLT-NAACL, pages 46–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gideon S Mann</author>
<author>David Yarowsky</author>
</authors>
<title>Multipath translation lexicon induction via bridge languages. In</title>
<date>2001</date>
<booktitle>Proc. HLT-NAACL,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="2479" citStr="Mann and Yarowsky, 2001" startWordPosition="357" endWordPosition="360">tracting Chris Dyer Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, 15213, USA cdyer@cs.cmu.edu a translation lexicon for recovering OOV content words and phrases is done by mining bi-lingual and monolingual resources (Rapp, 1995; CallisonBurch et al., 2006; Haghighi et al., 2008; Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). In addition, OOV content words can be recovered by exploiting cognates, by transliterating and then pivoting via a closely-related resource-richer language, when such a language exists (Hajiˇc et al., 2000; Mann and Yarowsky, 2001; Kondrak et al., 2003; De Gispert and Marino, 2006; Durrani et al., 2010; Wang et al., 2012; Nakov and Ng, 2012; Dholakia and Sarkar, 2014). Our work is similar in spirit to the latter line of research, but we show how to curate translations for OOV content words by pivoting via an unrelated, often typologically distant resourcerich languages. To achieve this goal, we replace transliteration by a new technique that captures more complex morpho-phonological transformations of historically-related words. Figure 1: A language lexicon can be divided into four main strata, depending on origin of w</context>
</contexts>
<marker>Mann, Yarowsky, 2001</marker>
<rawString>Gideon S Mann and David Yarowsky. 2001. Multipath translation lexicon induction via bridge languages. In Proc. HLT-NAACL, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuval Marton</author>
<author>Chris Callison-Burch</author>
<author>Philip Resnik</author>
</authors>
<title>Improved statistical machine translation using monolingually-derived paraphrases.</title>
<date>2009</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>381--390</pages>
<contexts>
<context position="2184" citStr="Marton et al., 2009" startWordPosition="310" endWordPosition="314">cialized/technical terms, but regular content words. Procuring translations for OOVs has been a subject of active research for decades. Translation of named entities is usually generated using transliteration techniques (Al-Onaizan and Knight, 2002; Hermjakob et al., 2008; Habash, 2008). Extracting Chris Dyer Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, 15213, USA cdyer@cs.cmu.edu a translation lexicon for recovering OOV content words and phrases is done by mining bi-lingual and monolingual resources (Rapp, 1995; CallisonBurch et al., 2006; Haghighi et al., 2008; Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). In addition, OOV content words can be recovered by exploiting cognates, by transliterating and then pivoting via a closely-related resource-richer language, when such a language exists (Hajiˇc et al., 2000; Mann and Yarowsky, 2001; Kondrak et al., 2003; De Gispert and Marino, 2006; Durrani et al., 2010; Wang et al., 2012; Nakov and Ng, 2012; Dholakia and Sarkar, 2014). Our work is similar in spirit to the latter line of research, but we show how to curate translations for OOV content words by pivoting via an unrelated, often typo</context>
</contexts>
<marker>Marton, Callison-Burch, Resnik, 2009</marker>
<rawString>Yuval Marton, Chris Callison-Burch, and Philip Resnik. 2009. Improved statistical machine translation using monolingually-derived paraphrases. In Proc. EMNLP, pages 381–390.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Proc. NIPS,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="12706" citStr="Mikolov et al., 2013" startWordPosition="1876" endWordPosition="1879">e candidate donor and the OOV loanword as follows. We first train, using large monolingual corpora, 100-dimensional word vector representations for donor and recipient language vocabularies.5 Then, we employ canonical correlation analysis (CCA) with small donor–loanword dictionaries (training sets in the borrowing models) to project the word embeddings into 50-dimensional vectors with maximized correlation between their dimensions. The semantic feature annotating the synthetic translation candidates is cosine distance between the resulting donor and loanword vectors. We use the word2vec tool (Mikolov et al., 2013) to train monolingual vectors,6 and the CCA-based tool (Faruqui and Dyer, 2014) for projecting word vectors.7 3 Experimental Setup Datasets and software. The Swahili–English parallel corpus was crawled from the Global Voices project website8. To simulate resource-poor scenario for the Romanian–English language pair, we sample a parallel corpus of same size from the transcribed TED talks (Cettolo et al., 2012). To evalu4For Arabic and French we use the GlobalPhone pronunciation dictionaries (Schultz et al., 2013) (we manually convert them to IPA). For Swahili and Romanian we automatically const</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Proc. NIPS, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Improving statistical machine translation for a resourcepoor language using related resource-rich languages.</title>
<date>2012</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>179--222</pages>
<contexts>
<context position="2591" citStr="Nakov and Ng, 2012" startWordPosition="379" endWordPosition="382">u.edu a translation lexicon for recovering OOV content words and phrases is done by mining bi-lingual and monolingual resources (Rapp, 1995; CallisonBurch et al., 2006; Haghighi et al., 2008; Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). In addition, OOV content words can be recovered by exploiting cognates, by transliterating and then pivoting via a closely-related resource-richer language, when such a language exists (Hajiˇc et al., 2000; Mann and Yarowsky, 2001; Kondrak et al., 2003; De Gispert and Marino, 2006; Durrani et al., 2010; Wang et al., 2012; Nakov and Ng, 2012; Dholakia and Sarkar, 2014). Our work is similar in spirit to the latter line of research, but we show how to curate translations for OOV content words by pivoting via an unrelated, often typologically distant resourcerich languages. To achieve this goal, we replace transliteration by a new technique that captures more complex morpho-phonological transformations of historically-related words. Figure 1: A language lexicon can be divided into four main strata, depending on origin of words. This work focuses on fully- and partially-assimilated foreign words, called borrowed words. Borrowed words</context>
</contexts>
<marker>Nakov, Ng, 2012</marker>
<rawString>Preslav Nakov and Hwee Tou Ng. 2012. Improving statistical machine translation for a resourcepoor language using related resource-rich languages. Journal of Artificial Intelligence Research, pages 179–222.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="14311" citStr="Och, 2003" startWordPosition="2123" endWordPosition="2124">led 4K, 8K, and 14K parallel sentences from the training corpora (the smaller the training corpus, the more OOVs it has). Corpora sizes along with statistics of source-side OOV tokens and types are given in tables 1 and 2. Statistics of the held-out dev and test sets used in all translation experiments are given in table 3. SW–EN RO–EN dev test dev test Sentences 1,552 1,732 2,687 2,265 Tokens 33,446 35,057 24,754 19,659 Types 7,008 7,180 5,141 4,328 Table 3: Dev and test corpora sizes. In all the MT experiments, we use the cdec9 toolkit (Dyer et al., 2010), and optimize parameters with MERT (Och, 2003). English 4-gram language models with Kneser-Ney smoothing (Kneser and Ney, 1995) are trained using KenLM (Heafield, 2011) on the target side of the parallel training corpora and on the Gigaword corpus (Parker et al., 2009). Results are reported using case-insensitive BLEU with a single reference (Papineni et al., 2002). We train three systems for each MT setup; reported BLEU scores are averaged over systems. Upper bounds. The goal of our experiments is not only to evaluate the contribution of the OOV dictionaries that we extract when pivoting via borrowing, but also to understand the potentia</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proc. ACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="14632" citStr="Papineni et al., 2002" startWordPosition="2171" endWordPosition="2174">iven in table 3. SW–EN RO–EN dev test dev test Sentences 1,552 1,732 2,687 2,265 Tokens 33,446 35,057 24,754 19,659 Types 7,008 7,180 5,141 4,328 Table 3: Dev and test corpora sizes. In all the MT experiments, we use the cdec9 toolkit (Dyer et al., 2010), and optimize parameters with MERT (Och, 2003). English 4-gram language models with Kneser-Ney smoothing (Kneser and Ney, 1995) are trained using KenLM (Heafield, 2011) on the target side of the parallel training corpora and on the Gigaword corpus (Parker et al., 2009). Results are reported using case-insensitive BLEU with a single reference (Papineni et al., 2002). We train three systems for each MT setup; reported BLEU scores are averaged over systems. Upper bounds. The goal of our experiments is not only to evaluate the contribution of the OOV dictionaries that we extract when pivoting via borrowing, but also to understand the potential contribution of the lexicon stratification. What is the overall improvement that can be achieved if we correctly translate all OOVs that were borrowed from another language? What is the overall improvement that can be achieved if we correctly translate all OOVs? We answer this question by defining “upper bound” experi</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. ACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Parker</author>
<author>David Graff</author>
<author>Junbo Kong</author>
<author>Ke Chen</author>
<author>Kazuaki Maeda</author>
</authors>
<date>2009</date>
<note>English Gigaword fourth edition.</note>
<contexts>
<context position="14534" citStr="Parker et al., 2009" startWordPosition="2157" endWordPosition="2160"> 1 and 2. Statistics of the held-out dev and test sets used in all translation experiments are given in table 3. SW–EN RO–EN dev test dev test Sentences 1,552 1,732 2,687 2,265 Tokens 33,446 35,057 24,754 19,659 Types 7,008 7,180 5,141 4,328 Table 3: Dev and test corpora sizes. In all the MT experiments, we use the cdec9 toolkit (Dyer et al., 2010), and optimize parameters with MERT (Och, 2003). English 4-gram language models with Kneser-Ney smoothing (Kneser and Ney, 1995) are trained using KenLM (Heafield, 2011) on the target side of the parallel training corpora and on the Gigaword corpus (Parker et al., 2009). Results are reported using case-insensitive BLEU with a single reference (Papineni et al., 2002). We train three systems for each MT setup; reported BLEU scores are averaged over systems. Upper bounds. The goal of our experiments is not only to evaluate the contribution of the OOV dictionaries that we extract when pivoting via borrowing, but also to understand the potential contribution of the lexicon stratification. What is the overall improvement that can be achieved if we correctly translate all OOVs that were borrowed from another language? What is the overall improvement that can be ach</context>
</contexts>
<marker>Parker, Graff, Kong, Chen, Maeda, 2009</marker>
<rawString>Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. 2009. English Gigaword fourth edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>Identifying word translations in non-parallel texts.</title>
<date>1995</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>320--322</pages>
<contexts>
<context position="2112" citStr="Rapp, 1995" startWordPosition="299" endWordPosition="300">re primarily not lexicon-peripheral items such as names and specialized/technical terms, but regular content words. Procuring translations for OOVs has been a subject of active research for decades. Translation of named entities is usually generated using transliteration techniques (Al-Onaizan and Knight, 2002; Hermjakob et al., 2008; Habash, 2008). Extracting Chris Dyer Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, 15213, USA cdyer@cs.cmu.edu a translation lexicon for recovering OOV content words and phrases is done by mining bi-lingual and monolingual resources (Rapp, 1995; CallisonBurch et al., 2006; Haghighi et al., 2008; Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). In addition, OOV content words can be recovered by exploiting cognates, by transliterating and then pivoting via a closely-related resource-richer language, when such a language exists (Hajiˇc et al., 2000; Mann and Yarowsky, 2001; Kondrak et al., 2003; De Gispert and Marino, 2006; Durrani et al., 2010; Wang et al., 2012; Nakov and Ng, 2012; Dholakia and Sarkar, 2014). Our work is similar in spirit to the latter line of research, but we show how to curate tra</context>
</contexts>
<marker>Rapp, 1995</marker>
<rawString>Reinhard Rapp. 1995. Identifying word translations in non-parallel texts. In Proc. ACL, pages 320–322.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Majid Razmara</author>
<author>Maryam Siahbani</author>
<author>Reza Haffari</author>
<author>Anoop Sarkar</author>
</authors>
<title>Graph propagation for paraphrasing out-of-vocabulary words in statistical machine translation.</title>
<date>2013</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>1105--1115</pages>
<contexts>
<context position="2206" citStr="Razmara et al., 2013" startWordPosition="315" endWordPosition="318">rms, but regular content words. Procuring translations for OOVs has been a subject of active research for decades. Translation of named entities is usually generated using transliteration techniques (Al-Onaizan and Knight, 2002; Hermjakob et al., 2008; Habash, 2008). Extracting Chris Dyer Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, 15213, USA cdyer@cs.cmu.edu a translation lexicon for recovering OOV content words and phrases is done by mining bi-lingual and monolingual resources (Rapp, 1995; CallisonBurch et al., 2006; Haghighi et al., 2008; Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). In addition, OOV content words can be recovered by exploiting cognates, by transliterating and then pivoting via a closely-related resource-richer language, when such a language exists (Hajiˇc et al., 2000; Mann and Yarowsky, 2001; Kondrak et al., 2003; De Gispert and Marino, 2006; Durrani et al., 2010; Wang et al., 2012; Nakov and Ng, 2012; Dholakia and Sarkar, 2014). Our work is similar in spirit to the latter line of research, but we show how to curate translations for OOV content words by pivoting via an unrelated, often typologically distant reso</context>
</contexts>
<marker>Razmara, Siahbani, Haffari, Sarkar, 2013</marker>
<rawString>Majid Razmara, Maryam Siahbani, Reza Haffari, and Anoop Sarkar. 2013. Graph propagation for paraphrasing out-of-vocabulary words in statistical machine translation. In Proc. ACL, pages 1105–1115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avneesh Saluja</author>
<author>Hany Hassan</author>
<author>Kristina Toutanova</author>
<author>Chris Quirk</author>
</authors>
<title>Graph-based semi-supervised learning of translation models from monolingual data. In</title>
<date>2014</date>
<booktitle>Proc. ACL,</booktitle>
<pages>676--686</pages>
<contexts>
<context position="2227" citStr="Saluja et al., 2014" startWordPosition="319" endWordPosition="322">nt words. Procuring translations for OOVs has been a subject of active research for decades. Translation of named entities is usually generated using transliteration techniques (Al-Onaizan and Knight, 2002; Hermjakob et al., 2008; Habash, 2008). Extracting Chris Dyer Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, 15213, USA cdyer@cs.cmu.edu a translation lexicon for recovering OOV content words and phrases is done by mining bi-lingual and monolingual resources (Rapp, 1995; CallisonBurch et al., 2006; Haghighi et al., 2008; Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). In addition, OOV content words can be recovered by exploiting cognates, by transliterating and then pivoting via a closely-related resource-richer language, when such a language exists (Hajiˇc et al., 2000; Mann and Yarowsky, 2001; Kondrak et al., 2003; De Gispert and Marino, 2006; Durrani et al., 2010; Wang et al., 2012; Nakov and Ng, 2012; Dholakia and Sarkar, 2014). Our work is similar in spirit to the latter line of research, but we show how to curate translations for OOV content words by pivoting via an unrelated, often typologically distant resourcerich languages. T</context>
</contexts>
<marker>Saluja, Hassan, Toutanova, Quirk, 2014</marker>
<rawString>Avneesh Saluja, Hany Hassan, Kristina Toutanova, and Chris Quirk. 2014. Graph-based semi-supervised learning of translation models from monolingual data. In Proc. ACL, pages 676–686.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tanja Schultz</author>
<author>Ngoc Thang Vu</author>
<author>Tim Schlippe</author>
</authors>
<title>GlobalPhone: A multilingual text &amp; speech database in 20 languages.</title>
<date>2013</date>
<booktitle>In Proc. ICASSP,</booktitle>
<pages>8126--8130</pages>
<contexts>
<context position="13223" citStr="Schultz et al., 2013" startWordPosition="1956" endWordPosition="1959">stance between the resulting donor and loanword vectors. We use the word2vec tool (Mikolov et al., 2013) to train monolingual vectors,6 and the CCA-based tool (Faruqui and Dyer, 2014) for projecting word vectors.7 3 Experimental Setup Datasets and software. The Swahili–English parallel corpus was crawled from the Global Voices project website8. To simulate resource-poor scenario for the Romanian–English language pair, we sample a parallel corpus of same size from the transcribed TED talks (Cettolo et al., 2012). To evalu4For Arabic and French we use the GlobalPhone pronunciation dictionaries (Schultz et al., 2013) (we manually convert them to IPA). For Swahili and Romanian we automatically construct pronunciation dictionaries using the Omniglot grapheme-to-IPA conversion rules at www.omniglot.com. 5We assume that while parallel data is limited in the recipient language, monolingual data is available. 6code.google.com/p/word2vec 7github.com/mfaruqui/eacl14-cca 8sw.globalvoicesonline.org 127 ate translation improvement on corpora of different sizes we conduct experiments with sub-sampled 4K, 8K, and 14K parallel sentences from the training corpora (the smaller the training corpus, the more OOVs it has). </context>
</contexts>
<marker>Schultz, Vu, Schlippe, 2013</marker>
<rawString>Tanja Schultz, Ngoc Thang Vu, and Tim Schlippe. 2013. GlobalPhone: A multilingual text &amp; speech database in 20 languages. In Proc. ICASSP, pages 8126–8130.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Language contact</author>
</authors>
<publisher>Edinburgh University Press</publisher>
<location>Edinburgh.</location>
<marker>contact, </marker>
<rawString>Language contact. Edinburgh University Press Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yulia Tsvetkov</author>
<author>Chris Dyer</author>
<author>Lori Levin</author>
<author>Archna Bhatia</author>
</authors>
<title>Generating English determiners in phrase-based translation with synthetic translation options.</title>
<date>2013</date>
<booktitle>In Proc. WMT,</booktitle>
<pages>271--280</pages>
<contexts>
<context position="9313" citStr="Tsvetkov et al., 2013" startWordPosition="1359" endWordPosition="1362">anslation. For each OOV, the borrowing system produces the n-best list of plausible donors; for each donor we then extract the k-best list of its translations.3 Then, we pair the OOV with the resulting n × k translation candidates. The translation candidates are noisy: some of the generated donors may be erroneous, the errors are then propagated in translation. To allow the low-resource system to leverage good translations that are missing in the default phrase inventory, while being stable to noisy translation hypotheses, we integrate the acquired translation candidates as synthetic phrases (Tsvetkov et al., 2013; Chahuneau et al., 2013). Synthetic phrases is a strategy of integrating translated phrases directly in the MT translation model, rather than via pre- or post-processing MT inputs and outputs. Synthetic phrases are phrasal translations that are not directly extractable from the training data, generated by auxiliary translation and postediting processes (for example, extracted from a borrowing model). An important advantage of synthetic phrases is that they are recall-oriented, allowing the system to leverage good translations that are missing in the default phrase inventory, while being stabl</context>
</contexts>
<marker>Tsvetkov, Dyer, Levin, Bhatia, 2013</marker>
<rawString>Yulia Tsvetkov, Chris Dyer, Lori Levin, and Archna Bhatia. 2013. Generating English determiners in phrase-based translation with synthetic translation options. In Proc. WMT, pages 271–280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yulia Tsvetkov</author>
<author>Waleed Ammar</author>
<author>Chris Dyer</author>
</authors>
<title>Constraint-based models of lexical borrowing.</title>
<date>2015</date>
<booktitle>In Proc. NAACL,</booktitle>
<pages>598--608</pages>
<contexts>
<context position="6778" citStr="Tsvetkov et al., 2015" startWordPosition="974" endWordPosition="977">ulary is borrowed from Arabic (Johnson, 1939). Importantly, since resource-rich languages are (historically) geopolitically important languages, borrowed words often Figure 2: To improve a resource-poor Swahili–English SMT system, we extract translation candidates for OOV Swahili words borrowed from Arabic using the Swahili-to-Arabic borrowing system and Arabic–English resource-rich SMT. bridge between resource-rich and resource-limited languages; we use this observation in our work. Transliteration and cognate discovery models perform poorly in the task of loanword generation/identification (Tsvetkov et al., 2015). The main reason is that the recipient language, in which borrowed words are fully or partially assimilated, may have very different morpho-phonological properties from the donor language (e.g., ‘orange’ and ‘sugar’ are not perceived as foreign by native speakers, but these are English words borrowed from Arabic e&apos; ,U (nArnj)1 and &lt;_J1 (Alskr), respectively). Therefore, morpho-phonological loanword adaptation is more complex than is typically captured by transliteration or cognate models. We employ a discriminative cross-lingual model of lexical borrowing to identify plausible donors given a </context>
<context position="17526" citStr="Tsvetkov et al., 2015" startWordPosition="2629" endWordPosition="2632">ted using pivoting via borrowing and integrated in translation models. Transliteration-augmented setups. In addition to the standard baselines, we evaluate transliteration-augmented setups, where we replace the borrowing model by a transliteration model (Ammar et al., 2012). The model is a linear-chain CRF where we label each source character with a sequence of target characters. The features are label unigrams and bigrams, separately or conjoined with a moving window of source characters. We employ the Swahili–Arabic and Romanian–French transliteration systems that were used as baselines in (Tsvetkov et al., 2015). As in the borrowing system, transliteration outputs are filtered to contain only target language lexicons. We list in table 5 sizes of obtained translated lexicons. 4K 8K 14K Translit. OOVs in SW–EN 49 32 22 Translit. OOVs in RO–EN 906 714 578 Table 5: Sizes of translated lexicons extracted using pivoting via transliteration and integrated in translation models. 128 4K 8K 14K Tokens 84,764 170,493 300,648 Types 14,554 23,134 33,288 OOV tokens 4,465 (12.7%) 3,509 (10.0%) 2,965 (8.4%) OOV types 3,610 (50.3%) 2,950 (41.1%) 2,523 (35.1%) Table 1: Statistics of the Swahili–English corpora and sou</context>
</contexts>
<marker>Tsvetkov, Ammar, Dyer, 2015</marker>
<rawString>Yulia Tsvetkov, Waleed Ammar, and Chris Dyer. 2015. Constraint-based models of lexical borrowing. In Proc. NAACL, pages 598–608.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pidong Wang</author>
<author>Preslav Nakov</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Source language adaptation for resource-poor machine translation.</title>
<date>2012</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>286--296</pages>
<contexts>
<context position="2571" citStr="Wang et al., 2012" startWordPosition="375" endWordPosition="378">13, USA cdyer@cs.cmu.edu a translation lexicon for recovering OOV content words and phrases is done by mining bi-lingual and monolingual resources (Rapp, 1995; CallisonBurch et al., 2006; Haghighi et al., 2008; Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). In addition, OOV content words can be recovered by exploiting cognates, by transliterating and then pivoting via a closely-related resource-richer language, when such a language exists (Hajiˇc et al., 2000; Mann and Yarowsky, 2001; Kondrak et al., 2003; De Gispert and Marino, 2006; Durrani et al., 2010; Wang et al., 2012; Nakov and Ng, 2012; Dholakia and Sarkar, 2014). Our work is similar in spirit to the latter line of research, but we show how to curate translations for OOV content words by pivoting via an unrelated, often typologically distant resourcerich languages. To achieve this goal, we replace transliteration by a new technique that captures more complex morpho-phonological transformations of historically-related words. Figure 1: A language lexicon can be divided into four main strata, depending on origin of words. This work focuses on fully- and partially-assimilated foreign words, called borrowed w</context>
</contexts>
<marker>Wang, Nakov, Ng, 2012</marker>
<rawString>Pidong Wang, Preslav Nakov, and Hwee Tou Ng. 2012. Source language adaptation for resource-poor machine translation. In Proc. EMNLP, pages 286–296.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>