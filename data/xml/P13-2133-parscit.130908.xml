<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000276">
<title confidence="0.9433945">
Context Vector Disambiguation for Bilingual Lexicon Extraction from
Comparable Corpora
</title>
<author confidence="0.412935">
Content Engineering Laboratory,
</author>
<address confidence="0.535636">
91191 Gif-sur-Yvette CEDEX
France
</address>
<email confidence="0.982805">
dhouha.bouamor@cea.fr
</email>
<note confidence="0.479016">
Engineering Laboratory,
</note>
<address confidence="0.725458">
91191 Gif-sur-Yvette
CEDEX France
</address>
<email confidence="0.992691">
nasredine.semmar@cea.fr
</email>
<author confidence="0.844799">
Pierre Zweigenbaum
</author>
<affiliation confidence="0.765148">
LIMSI-CNRS,
</affiliation>
<address confidence="0.5648965">
F-91403 Orsay CEDEX
France
</address>
<email confidence="0.992036">
pz@limsi.fr
</email>
<note confidence="0.949613">
Dhouha Bouamor Nasredine Semmar
CEA, LIST, Vision and CEA, LIST, Vision and Content
</note>
<sectionHeader confidence="0.976678" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999906538461539">
This paper presents an approach that ex-
tends the standard approach used for bilin-
gual lexicon extraction from comparable
corpora. We focus on the unresolved prob-
lem of polysemous words revealed by the
bilingual dictionary and introduce a use of
a Word Sense Disambiguation process that
aims at improving the adequacy of con-
text vectors. On two specialized French-
English comparable corpora, empirical ex-
perimental results show that our method
improves the results obtained by two state-
of-the-art approaches.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999919680851064">
Over the years, bilingual lexicon extraction from
comparable corpora has attracted a wealth of re-
search works (Fung, 1998; Rapp, 1995; Chiao
and Zweigenbaum, 2003). The basic assumption
behind most studies is a distributional hypothe-
sis (Harris, 1954), which states that words with a
similar meaning are likely to appear in similar con-
texts across languages. The so-called standard ap-
proach to bilingual lexicon extraction from com-
parable corpora is based on the characterization
and comparison of context vectors of source and
target words. Each element in the context vector
of a source or target word represents its associa-
tion with a word which occurs within a window
of N words. To enable the comparison of source
and target vectors, words in the source vectors are
translated into the target language using an exist-
ing bilingual dictionary.
The core of the standard approach is the bilin-
gual dictionary. Its use is problematic when a word
has several translations, whether they are synony-
mous or polysemous. For instance, the French
word action can be translated into English as
share, stock, lawsuit or deed. In such cases, it
is difficult to identify in flat resources like bilin-
gual dictionaries which translations are most rel-
evant. The standard approach considers all avail-
able translations and gives them the same impor-
tance in the resulting translated context vectors in-
dependently of the domain of interest and word
ambiguity. Thus, in the financial domain, trans-
lating action into deed or lawsuit would introduce
noise in context vectors.
In this paper, we present a novel approach that
addresses the word polysemy problem neglected
in the standard approach. We introduce a Word
Sense Disambiguation (WSD) process that iden-
tifies the translations of polysemous words that
are more likely to give the best representation of
context vectors in the target language. For this
purpose, we employ five WordNet-based semantic
similarity and relatedness measures and use a data
fusion method that merges the results obtained by
each measure. We test our approach on two spe-
cialized French-English comparable corpora (fi-
nancial and medical) and report improved results
compared to two state-of-the-art approaches.
</bodyText>
<sectionHeader confidence="0.999766" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999962545454546">
Most previous works addressing the task of bilin-
gual lexicon extraction from comparable corpora
are based on the standard approach. In order to
improve the results of this approach, recent re-
searches based on the assumption that more the
context vectors are representative, better is the
bilingual lexicon extraction were conducted. In
these works, additional linguistic resources such
as specialized dictionaries (Chiao and Zweigen-
baum, 2002) or transliterated words (Prochasson
et al., 2009) were combined with the bilingual dic-
</bodyText>
<page confidence="0.981751">
759
</page>
<bodyText confidence="0.9442084">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 759–764,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
tionary to translate context vectors. Few works
have however focused on the ambiguity problem
revealed by the seed bilingual dictionary. (Hazem
and Morin, 2012) propose a method that filters the
entries of the bilingual dictionary on the base of
a POS-Tagging and a domain relevance measure
criteria but no improvements have been demon-
strated. Gaussier et al. (2004) attempted to solve
the problem of word ambiguities in the source and
target languages. They investigated a number of
techniques including canonical correlation analy-
sis and multilingual probabilistic latent semantic
analysis. The best results, with an improvement of
the F-Measure (+0.02 at Top20) were reported for
a mixed method. Recently, (Morin and Prochas-
son, 2011) proceed as the standard approach but
weigh the different translations according to their
frequency in the target corpus. Here, we propose a
method that differs from Gaussier et al. (2004) in
this way: If they focus on words ambiguities on
source and target languages, we thought that it
would be sufficient to disambiguate only trans-
lated source context vectors.
</bodyText>
<sectionHeader confidence="0.961551" genericHeader="method">
3 Context Vector Disambiguation
</sectionHeader>
<subsectionHeader confidence="0.999611">
3.1 Semantic similarity measures
</subsectionHeader>
<bodyText confidence="0.999954260869565">
A large number of WSD techniques were pro-
posed in the literature. The most widely used ones
are those that compute semantic similarity1 with
the help of WordNet. WordNet has been used in
many tasks relying on word-based similarity, in-
cluding document (Hwang et al., 2011) and im-
age (Cho et al., 2007; Choi et al., 2012) retrieval
systems. In this work, we use it to derive a se-
mantic similarity between lexical units within the
same context vector. To the best of our knowledge,
this is the first application of WordNet to bilingual
lexicon extraction from comparable corpora.
Among semantic similarity measures using
WordNet, we distinguish: (1) measures based on
path length which simply counts the distance be-
tween two words in the WordNet taxonomy, (2)
measures relying on information content in which
a semantically annotated corpus is needed to com-
pute frequencies of words to be compared and (3)
the ones using gloss overlap which are designed
to compute semantic relatedness. In this work,
we use five similarity measures and compare
their performances. These measures include three
</bodyText>
<footnote confidence="0.758478">
1For consiseness, we often use “semantic similarity” to
refer collectively to both similarity and relatedness.
</footnote>
<bodyText confidence="0.99980252173913">
path-based semantic similarity measures denoted
PATH,WUP (Wu and Palmer, 1994) and LEA-
COCK (Leacock and Chodorow, 1998). PATH is
a baseline that is equal to the inverse of the short-
est path between two words. WUP finds the depth
of the least common subsumer of the words, and
scales that by the sum of the depths of individual
words. The depth of a word is its distance to the
root node. LEACOCK finds the shortest path be-
tween two words, and scales that by the maximum
path length found in the is–a hierarchy in which
they occur. Path length measures have the advan-
tage of being independent of corpus statistics, and
therefor uninfluenced by sparse data.
Since semantic relatedness is considered to be
more general than semantic similarity, we also
use two relatedness measures: LESK (Banerjee
and Pedersen, 2002) and VECTOR (Patwardhan,
2003). LESK finds overlaps between the glosses
of word pairs, as well as words’ hyponyms. VEC-
TOR creates a co-occurrence matrix for each gloss
token. Each gloss is then represented as a vector
that averages token co-occurrences.
</bodyText>
<subsectionHeader confidence="0.999309">
3.2 Disambiguation process
</subsectionHeader>
<bodyText confidence="0.999961608695652">
Once translated into the target language, the con-
text vectors disambiguation process intervenes.
This process operates locally on each context vec-
tor and aims at finding the most prominent trans-
lations of polysemous words. For this purpose,
we use monosemic words as a seed set of dis-
ambiguated words to infer the polysemous word’s
translations senses. We hypothesize that a word is
monosemic if it is associated to only one entry in
the bilingual dictionary. We checked this assump-
tion by probing monosemic entries of the bilingual
dictionary against WordNet and found that 95% of
the entries are monosemic in both resources. Ac-
cording to the above-described semantic similarity
measures, a similarity value SimV alue is derived
between all the translations provided for each pol-
ysemous word by the bilingual dictionary and all
monosemic words appearing within the same con-
text vector. In practice, since a word can belong to
more than one synset2 in WordNet, the semantic
similarity between two words w1 and w2 is defined
as the maximum of SimV alue between the synset
or the synsets that include the synsets(w1) and
</bodyText>
<footnote confidence="0.600798">
2a group of a synonymous words in WordNet
</footnote>
<page confidence="0.937683">
760
</page>
<bodyText confidence="0.782083">
synsets(w2) according to the following equation:
</bodyText>
<equation confidence="0.9989535">
SemSim(w1, w2) = max{SimV alue(s1, s2);
(s1, s2) E synsets(w1) x synsets(w2)} (1)
</equation>
<bodyText confidence="0.999339333333333">
Then, to identify the most prominent transla-
tions of each polysemous unit wp, an average sim-
ilarity is computed for each translation wjp of wp:
</bodyText>
<table confidence="0.931617333333333">
1 N SemSim(wi, wjp) (2)
Ave Sim(wjp) = N
i=1
</table>
<bodyText confidence="0.999928333333333">
where N is the total number of monosemic words
in each context vector and SemSim is the simi-
larity value of wjp and the ith monosemic word.
Hence, according to average similarity values
Ave Sim(wjp), we obtain for each polysemous
word wp an ordered list of translations w1 p ... wn p.
</bodyText>
<sectionHeader confidence="0.999601" genericHeader="evaluation">
4 Experiments and Results
</sectionHeader>
<subsectionHeader confidence="0.988201">
4.1 Resources and Experimental Setup
</subsectionHeader>
<bodyText confidence="0.99993525">
We conducted our experiments on two French-
English comparable corpora specialized on the
corporate finance and the breast cancer sub-
domains. Both corpora were extracted from
Wikipedia3. We consider the domain topic in
the source language (for instance cancer du sein
[breast cancer]) as a query to Wikipedia and
extract all its sub-topics (i.e., sub-categories in
Wikipedia) to construct a domain-specific cate-
gories tree. Then we collected all articles belong-
ing to one of these categories and used inter-
language links to build the comparable corpus.
Both corpora have been normalized through the
following linguistic preprocessing steps: tokeni-
sation, part-of-speech tagging, lemmatisation and
function words removal. The resulting corpora4
sizes as well as their polysemy rate PR are given
in Table 1. The polysemy rate indicates how much
words in the comparable corpora are associated
to more than one translation in the seed bilingual
dictionary. The dictionary consists of an in-house
bilingual dictionary which contains about 120,000
entries belonging to the general language with an
average of 7 translations per entry.
In bilingual terminology extraction from com-
parable corpora, a reference list is required to
evaluate the performance of the alignment. Such
lists are often composed of about 100 single
</bodyText>
<footnote confidence="0.9990415">
3http://dumps.wikimedia.org/
4Comparable corpora will be shared publicly
</footnote>
<table confidence="0.995379">
Corpus French English PR
Corporate finance 402.486 756.840 41%
Breast cancer 396.524 524.805 47%
</table>
<tableCaption confidence="0.778752666666667">
Table 1: Comparable corpora sizes in term of
words and polysemy rates (PR) associated to each
corpus
</tableCaption>
<bodyText confidence="0.9957014">
terms (Hazem and Morin, 2012; Chiao and
Zweigenbaum, 2002). Here, we created two ref-
erence lists5 for the corporate finance and the
breast cancer sub-domains. The first list is com-
posed of 125 single terms extracted from the glos-
sary of bilingual micro-finance terms6. The second
list contains 79 terms extracted from the French-
English MESH and the UMLS thesauri7. Note
that reference terms pairs appear more than five
times in each part of both comparable corpora.
Three other parameters need to be set up,
namely the window size, the association measure
and the similarity measure. We followed (Laroche
and Langlais, 2010) to define these parame-
ters. They carried out a complete study of the
influence of these parameters on the bilingual
alignment. The context vectors were defined by
computing the Discounted Log-Odds Ratio (equa-
tion 3) between words occurring in the same con-
text window of size 7.
</bodyText>
<equation confidence="0.9424985">
(O11 + 12)(O22 + 12) (3)
Odds-Ratiodisc
=log + 1 2)(O21 + 1
(O12 2)
</equation>
<bodyText confidence="0.99999775">
where Oij are the cells of the 2 x 2 contingency
matrix of a token s co-occurring with the term S
within a given window size. As similarity mea-
sure, we chose to use the cosine measure.
</bodyText>
<subsectionHeader confidence="0.975313">
4.2 Results of bilingual lexicon extraction
</subsectionHeader>
<bodyText confidence="0.999906166666667">
To evaluate the performance of our approach, we
used both the standard approach (SA) and the ap-
proach proposed by (Morin and Prochasson, 2011)
(henceforth MP11) as baselines. The experiments
were performed with respect to the five semantic
similarity measures described in section 3.1. Each
measure provides, for each polysemous word, a
ranked list of translations. A question that arises
here is whether we should introduce only the top-
ranked translation into the context vector or con-
sider a larger number of translations, mainly when
a translation list contains synonyms. For this
</bodyText>
<footnote confidence="0.999965666666667">
5Reference lists will be shared publicly
6http://www.microfinance.lu/en/
7http://www.nlm.nih.gov/
</footnote>
<page confidence="0.987846">
761
</page>
<table confidence="0.9999578">
Method WN-T1 WN-T2 WN-T3 WN-T4 WN-T5 WN-T6 WN-T7
Standard Approach (SA) 0.172
MP11 0.336
Single WUP 0.241 0.284 0.301 0.275 0.258 0.215 0.224
measure
PATH 0.250 0.284 0.301 0.284 0.258 0.215 0.215
LEACOCK 0.250 0.293 0.301 0.275 0.275 0.241 0.232
LESK 0.272 0.293 0.293 0.275 0.258 0.250 0.215
VECTOR 0.267 0.310 0.284 0.284 0.232 0.232 0.232
CONDORCETMerge 0.362 0.379 0.353 0.362 0.336 0.275 0.267
Method WN-T1 WN-T2 WN-T3 WN-T4 WN-T5 WN-T6 WN-T7
Standard Approach (SA) 0.493
MP11 0.553
Single WUP 0.481 0.566 0.566 0.542 0.554 0.542 0.554
measure
PATH 0.542 0.542 0.554 0.566 0.578 0.554 0.554
LEACOCK 0.506 0.578 0.554 0.566 0.542 0.554 0.542
LESK 0.469 0.542 0.542 0.590 0.554 0.554 0.542
VECTOR 0.518 0.566 0.530 0.566 0.542 0.566 0.554
CONDORCETMerge 0.566 0.614 0.600 0.590 0.600 0.578 0.578
</table>
<tableCaption confidence="0.861261">
Table 2: F-Measure at Top20 for the two domains; MP11 = (Morin and Prochasson, 2011). In each
column, italics shows best single similarity measure, bold shows best result. Underline shows best result
overall.
</tableCaption>
<figure confidence="0.962876">
a) Corporate Finance
b) Breast Cancer
</figure>
<bodyText confidence="0.999964910714286">
reason, we take into account in our experiments
different numbers of translations, noted WN-Ti,
ranging from the pivot translation (i = 1) to the
seventh word in the translation list. This choice is
motivated by the fact that words in both corpora
have on average 7 translations in the bilingual dic-
tionary. Both baseline systems use all translations
associated to each entry in the bilingual dictionary.
The only difference is that in MP11 translations
are weighted according to their frequency in the
target corpus.
The results of different works focusing on bilin-
gual lexicon extraction from comparable corpora
are evaluated on the number of correct candidates
found in the first N first candidates output by the
alignment process (the TopN). Here, we use the
Top20 F-measure as evaluation metric. The results
obtained for the corporate finance corpus are pre-
sented in Table 2a. The first notable observation is
that disambiguating context vectors using seman-
tic similarity measures outperforms the SA. The
highest F-measure is reported by VECTOR. Us-
ing the top two words (WN-T2) in context vec-
tors increases the F-measure from 0.172 to 0.310.
However, compared to MP11, no improvement
is achieved. Concerning the breast cancer cor-
pus, Table 2b shows improvements in most cases
over both the SA and MP11. The maximum F-
measure was obtained by LESK when for each
polysemous word up to four translations (WN-T4)
are considered in context vectors. This method
achieves an improvement of respectively +0.097
and +0.037% over SA and MP11.
Each of the tested 5 semantic similarity mea-
sures provides a different view of how to rank
the translations of a given test word. Combining
the obtained ranked lists should reinforce the con-
fidence in consensus translations, while decreas-
ing the confidence in non-consensus translations.
We have therefore tested their combination. For
this, we used a voting method, and chose one in
the Condorcet family the Condorcet data fusion
method. This method was widely used to combine
document retrieval results from information re-
trieval systems (Montague and Aslam, 2002; Nu-
ray and Can, 2006). It is a single-winner election
method that ranks the candidates in order of pref-
erence. It is a pairwise voting, i.e. it compares ev-
ery possible pair of candidates to decide the pref-
erence of them. A matrix can be used to present
the competition process. Every candidate appears
in the matrix as a row and a column as well. If
there are m candidates, then we need m2 elements
in the matrix in total. Initially 0 is written to all the
elements. If di is preferred to dj , then we add 1 to
the element at row i and column j (aij). The pro-
</bodyText>
<page confidence="0.991028">
762
</page>
<bodyText confidence="0.9999839">
cess is repeated until all the ballots are processed.
For every element aij , if aij &gt; m/2 , then di
beats dj; if aij &lt; m/2, then dj beats di; other-
wise (aij = m/2), there is a draw between di and
dj. The total score of each candidate is quantified
by summing the raw scores it obtains in all pair-
wise competitions. Finally the ranking is achiev-
able based on the total scores calculated.
Here, we view the ranking of the extraction re-
sults from different similarity measures as a spe-
cial instance of the voting problem where the
Top20 extraction results correspond to candidates
and different semantic similarity measures are the
voters. The combination method referred to as
CONDORCETMerge outperformed all the others
(see Tables 2a and 2b): (1) individual measures,
(2) SA, and (3) MP11. Even though the two cor-
pora are fairly different (subject and polysemy
rate), the optimal results are obtained when con-
sidering up to two most similar translations in con-
text vectors. This behavior shows that the fusion
method is robust to domain change. The addition
of supplementary translations, which are probably
noisy in the given domain, degrades the overall re-
sults. The F-measure gains with respect to SA are
+0.207 for corporate finance and +0.121 for the
breast cancer corpus. More interestingly, our ap-
proach outperforms MP11, showing that the role
of disambiguation is more important than that of
feature weighting.
</bodyText>
<sectionHeader confidence="0.998904" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999979090909091">
We presented in this paper a novel method that
extends the standard approach used for bilingual
lexicon extraction. This method disambiguates
polysemous words in context vectors by selecting
only the most relevant translations. Five seman-
tic similarity and relatedness measures were used
for this purpose. Experiments conducted on two
specialized comparable corpora indicate that the
combination of similarity metrics leads to a better
performance than two state-of-the-art approaches.
This shows that the ambiguity present in special-
ized comparable corpora hampers bilingual lexi-
con extraction, and that methods such as the one
introduced here are needed. The obtained results
are very encouraging and can be improved in a
number of ways. First, we plan to mine much
larger specialized comparable corpora and focus
on their quality (Li and Gaussier, 2010). We also
plan to test our method on bilingual lexicon extrac-
tion from general-domain corpora, where ambigu-
ity is generally higher and disambiguation meth-
ods should be all the more needed.
</bodyText>
<sectionHeader confidence="0.988807" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997827208333333">
Satanjeev Banerjee and Ted Pedersen. 2002. An
adapted lesk algorithm for word sense disambigua-
tion using wordnet. In Proceedings of the Third In-
ternational Conference on Computational Linguis-
tics and Intelligent Text Processing, CICLing ’02,
pages 136–145, London, UK, UK. Springer-Verlag.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2002.
Looking for candidate translational equivalents in
specialized, comparable corpora. In Proceedings of
the 19th international conference on Computational
linguistics - Volume 2, COLING ’02, pages 1–5. As-
sociation for Computational Linguistics.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2003.
The effect of a general lexicon in corpus-based iden-
tification of french-english medical word transla-
tions. In Proceedings Medical Informatics Europe,
volume 95 of Studies in Health Technology and In-
formatics, pages 397–402, Amsterdam.
Miyoung Cho, Chang Choi, Hanil Kim, Jungpil Shin,
and PanKoo Kim. 2007. Efficient image retrieval
using conceptualization of annotated images. Lec-
ture Notes in Computer Science, pages 426–433.
Springer.
Dongjin Choi, Jungin Kim, Hayoung Kim, Myungg-
won Hwang, and Pankoo Kim. 2012. A method for
enhancing image retrieval based on annotation using
modified wup similarity in wordnet. In Proceed-
ings of the 11th WSEAS international conference
on Artificial Intelligence, Knowledge Engineering
and Data Bases, AIKED’12, pages 83–87, Stevens
Point, Wisconsin, USA. World Scientific and Engi-
neering Academy and Society (WSEAS).
Pascale Fung. 1998. A statistical view on bilingual
lexicon extraction: From parallel corpora to non-
parallel corpora. In Parallel Text Processing, pages
1–17. Springer.
´Eric Gaussier, Jean-Michel Renders, Irina Matveeva,
Cyril Goutte, and Herv´e D´ejean. 2004. A geometric
view on bilingual lexicon extraction from compara-
ble corpora. In ACL, pages 526–533.
Z.S. Harris. 1954. Distributional structure. Word.
Amir Hazem and Emmanuel Morin. 2012. Adap-
tive dictionary for bilingual lexicon extraction from
comparable corpora. In Proceedings, 8th interna-
tional conference on Language Resources and Eval-
uation (LREC), Istanbul, Turkey, May.
Myunggwon Hwang, Chang Choi, and Pankoo Kim.
2011. Automatic enrichment of semantic relation
</reference>
<page confidence="0.983626">
763
</page>
<reference confidence="0.998180442307693">
network and its application to word sense disam-
biguation. IEEE Transactions on Knowledge and
Data Engineering, 23:845–858.
Audrey Laroche and Philippe Langlais. 2010. Re-
visiting context-based projection methods for term-
translation spotting in comparable corpora. In 23rd
International Conference on Computational Lin-
guistics (Coling 2010), pages 617–625, Beijing,
China, Aug.
Claudia Leacock and Martin Chodorow, 1998. Com-
bining local context and WordNet similarity for word
sense identification, pages 305–332. In C. Fellbaum
(Ed.), MIT Press.
Bo Li and ¨Eric Gaussier. 2010. Improving corpus
comparability for bilingual lexicon extraction from
comparable corpora. In 23rd International Confer-
ence on Computational Linguistics (Coling 2010),
Beijing, China, Aug.
Mark Montague and Javed A. Aslam. 2002. Con-
dorcet fusion for improved retrieval. In Proceedings
of the eleventh international conference on Informa-
tion and knowledge management, CIKM ’02, pages
538–548, New York, NY, USA. ACM.
Emmanuel Morin and Emmanuel Prochasson. 2011.
Bilingual lexicon extraction from comparable cor-
pora enhanced with parallel corpora. In Proceed-
ings, 4th Workshop on Building and Using Compa-
rable Corpora (BUCC), page 27–34, Portland, Ore-
gon, USA.
Rabia Nuray and Fazli Can. 2006. Automatic ranking
of information retrieval systems using data fusion.
Inf. Process. Manage., 42(3):595–614, May.
Siddharth Patwardhan. 2003. Incorporating Dictio-
nary and Corpus Information into a Context Vector
Measure of Semantic Relatedness. Master’s thesis,
University of Minnesota, Duluth, August.
Emmanuel Prochasson, Emmanuel Morin, and Kyo
Kageura. 2009. Anchor points for bilingual lexi-
con extraction from small comparable corpora. In
Proceedings, 12th Conference on Machine Transla-
tion Summit (MT Summit XII), page 284–291, Ot-
tawa, Ontario, Canada.
Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proceedings of the 33rd an-
nual meeting on Association for Computational Lin-
guistics, ACL ’95, pages 320–322. Association for
Computational Linguistics.
Zhibiao Wu and Martha Palmer. 1994. Verbs seman-
tics and lexical selection. In Proceedings of the 32nd
annual meeting on Association for Computational
Linguistics, ACL ’94, pages 133–138. Association
for Computational Linguistics.
</reference>
<page confidence="0.99787">
764
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.035936">
<title confidence="0.778055666666667">Context Vector Disambiguation for Bilingual Lexicon Extraction Comparable Corpora Content Engineering</title>
<address confidence="0.8576115">91191 Gif-sur-Yvette France</address>
<email confidence="0.990708">dhouha.bouamor@cea.fr</email>
<affiliation confidence="0.785273">Engineering</affiliation>
<address confidence="0.8924605">91191 CEDEX France</address>
<email confidence="0.988958">nasredine.semmar@cea.fr</email>
<affiliation confidence="0.631373">Pierre</affiliation>
<address confidence="0.8383075">F-91403 Orsay France</address>
<email confidence="0.968469">pz@limsi.fr</email>
<note confidence="0.4663735">Dhouha Bouamor Nasredine Semmar CEA, LIST, Vision and CEA, LIST, Vision and Content</note>
<abstract confidence="0.997612571428571">This paper presents an approach that extends the standard approach used for bilingual lexicon extraction from comparable corpora. We focus on the unresolved probof words by the bilingual dictionary and introduce a use of a Word Sense Disambiguation process that aims at improving the adequacy of context vectors. On two specialized French- English comparable corpora, empirical experimental results show that our method improves the results obtained by two stateof-the-art approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>An adapted lesk algorithm for word sense disambiguation using wordnet.</title>
<date>2002</date>
<booktitle>In Proceedings of the Third International Conference on Computational Linguistics and Intelligent Text Processing, CICLing ’02,</booktitle>
<pages>136--145</pages>
<publisher>UK. Springer-Verlag.</publisher>
<location>London, UK,</location>
<contexts>
<context position="7080" citStr="Banerjee and Pedersen, 2002" startWordPosition="1097" endWordPosition="1100">ath between two words. WUP finds the depth of the least common subsumer of the words, and scales that by the sum of the depths of individual words. The depth of a word is its distance to the root node. LEACOCK finds the shortest path between two words, and scales that by the maximum path length found in the is–a hierarchy in which they occur. Path length measures have the advantage of being independent of corpus statistics, and therefor uninfluenced by sparse data. Since semantic relatedness is considered to be more general than semantic similarity, we also use two relatedness measures: LESK (Banerjee and Pedersen, 2002) and VECTOR (Patwardhan, 2003). LESK finds overlaps between the glosses of word pairs, as well as words’ hyponyms. VECTOR creates a co-occurrence matrix for each gloss token. Each gloss is then represented as a vector that averages token co-occurrences. 3.2 Disambiguation process Once translated into the target language, the context vectors disambiguation process intervenes. This process operates locally on each context vector and aims at finding the most prominent translations of polysemous words. For this purpose, we use monosemic words as a seed set of disambiguated words to infer the polys</context>
</contexts>
<marker>Banerjee, Pedersen, 2002</marker>
<rawString>Satanjeev Banerjee and Ted Pedersen. 2002. An adapted lesk algorithm for word sense disambiguation using wordnet. In Proceedings of the Third International Conference on Computational Linguistics and Intelligent Text Processing, CICLing ’02, pages 136–145, London, UK, UK. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yun-Chuang Chiao</author>
<author>Pierre Zweigenbaum</author>
</authors>
<title>Looking for candidate translational equivalents in specialized, comparable corpora.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th international conference on Computational linguistics - Volume 2, COLING ’02,</booktitle>
<pages>1--5</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3620" citStr="Chiao and Zweigenbaum, 2002" startWordPosition="545" endWordPosition="549">e test our approach on two specialized French-English comparable corpora (financial and medical) and report improved results compared to two state-of-the-art approaches. 2 Related Work Most previous works addressing the task of bilingual lexicon extraction from comparable corpora are based on the standard approach. In order to improve the results of this approach, recent researches based on the assumption that more the context vectors are representative, better is the bilingual lexicon extraction were conducted. In these works, additional linguistic resources such as specialized dictionaries (Chiao and Zweigenbaum, 2002) or transliterated words (Prochasson et al., 2009) were combined with the bilingual dic759 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 759–764, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics tionary to translate context vectors. Few works have however focused on the ambiguity problem revealed by the seed bilingual dictionary. (Hazem and Morin, 2012) propose a method that filters the entries of the bilingual dictionary on the base of a POS-Tagging and a domain relevance measure criteria but no improvements ha</context>
<context position="10836" citStr="Chiao and Zweigenbaum, 2002" startWordPosition="1695" endWordPosition="1698">ch contains about 120,000 entries belonging to the general language with an average of 7 translations per entry. In bilingual terminology extraction from comparable corpora, a reference list is required to evaluate the performance of the alignment. Such lists are often composed of about 100 single 3http://dumps.wikimedia.org/ 4Comparable corpora will be shared publicly Corpus French English PR Corporate finance 402.486 756.840 41% Breast cancer 396.524 524.805 47% Table 1: Comparable corpora sizes in term of words and polysemy rates (PR) associated to each corpus terms (Hazem and Morin, 2012; Chiao and Zweigenbaum, 2002). Here, we created two reference lists5 for the corporate finance and the breast cancer sub-domains. The first list is composed of 125 single terms extracted from the glossary of bilingual micro-finance terms6. The second list contains 79 terms extracted from the FrenchEnglish MESH and the UMLS thesauri7. Note that reference terms pairs appear more than five times in each part of both comparable corpora. Three other parameters need to be set up, namely the window size, the association measure and the similarity measure. We followed (Laroche and Langlais, 2010) to define these parameters. They </context>
</contexts>
<marker>Chiao, Zweigenbaum, 2002</marker>
<rawString>Yun-Chuang Chiao and Pierre Zweigenbaum. 2002. Looking for candidate translational equivalents in specialized, comparable corpora. In Proceedings of the 19th international conference on Computational linguistics - Volume 2, COLING ’02, pages 1–5. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yun-Chuang Chiao</author>
<author>Pierre Zweigenbaum</author>
</authors>
<title>The effect of a general lexicon in corpus-based identification of french-english medical word translations.</title>
<date>2003</date>
<booktitle>In Proceedings Medical Informatics Europe, volume 95 of Studies in Health Technology and Informatics,</booktitle>
<pages>397--402</pages>
<location>Amsterdam.</location>
<contexts>
<context position="1105" citStr="Chiao and Zweigenbaum, 2003" startWordPosition="149" endWordPosition="152"> approach used for bilingual lexicon extraction from comparable corpora. We focus on the unresolved problem of polysemous words revealed by the bilingual dictionary and introduce a use of a Word Sense Disambiguation process that aims at improving the adequacy of context vectors. On two specialized FrenchEnglish comparable corpora, empirical experimental results show that our method improves the results obtained by two stateof-the-art approaches. 1 Introduction Over the years, bilingual lexicon extraction from comparable corpora has attracted a wealth of research works (Fung, 1998; Rapp, 1995; Chiao and Zweigenbaum, 2003). The basic assumption behind most studies is a distributional hypothesis (Harris, 1954), which states that words with a similar meaning are likely to appear in similar contexts across languages. The so-called standard approach to bilingual lexicon extraction from comparable corpora is based on the characterization and comparison of context vectors of source and target words. Each element in the context vector of a source or target word represents its association with a word which occurs within a window of N words. To enable the comparison of source and target vectors, words in the source vect</context>
</contexts>
<marker>Chiao, Zweigenbaum, 2003</marker>
<rawString>Yun-Chuang Chiao and Pierre Zweigenbaum. 2003. The effect of a general lexicon in corpus-based identification of french-english medical word translations. In Proceedings Medical Informatics Europe, volume 95 of Studies in Health Technology and Informatics, pages 397–402, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miyoung Cho</author>
<author>Chang Choi</author>
<author>Hanil Kim</author>
<author>Jungpil Shin</author>
<author>PanKoo Kim</author>
</authors>
<title>Efficient image retrieval using conceptualization of annotated images.</title>
<date>2007</date>
<journal>Lecture Notes in Computer Science,</journal>
<pages>426--433</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="5363" citStr="Cho et al., 2007" startWordPosition="817" endWordPosition="820">t corpus. Here, we propose a method that differs from Gaussier et al. (2004) in this way: If they focus on words ambiguities on source and target languages, we thought that it would be sufficient to disambiguate only translated source context vectors. 3 Context Vector Disambiguation 3.1 Semantic similarity measures A large number of WSD techniques were proposed in the literature. The most widely used ones are those that compute semantic similarity1 with the help of WordNet. WordNet has been used in many tasks relying on word-based similarity, including document (Hwang et al., 2011) and image (Cho et al., 2007; Choi et al., 2012) retrieval systems. In this work, we use it to derive a semantic similarity between lexical units within the same context vector. To the best of our knowledge, this is the first application of WordNet to bilingual lexicon extraction from comparable corpora. Among semantic similarity measures using WordNet, we distinguish: (1) measures based on path length which simply counts the distance between two words in the WordNet taxonomy, (2) measures relying on information content in which a semantically annotated corpus is needed to compute frequencies of words to be compared and </context>
</contexts>
<marker>Cho, Choi, Kim, Shin, Kim, 2007</marker>
<rawString>Miyoung Cho, Chang Choi, Hanil Kim, Jungpil Shin, and PanKoo Kim. 2007. Efficient image retrieval using conceptualization of annotated images. Lecture Notes in Computer Science, pages 426–433. Springer.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Dongjin Choi</author>
<author>Jungin Kim</author>
<author>Hayoung Kim</author>
<author>Myunggwon Hwang</author>
<author>Pankoo Kim</author>
</authors>
<title>A method for enhancing image retrieval based on annotation using modified wup similarity in wordnet.</title>
<date>2012</date>
<booktitle>In Proceedings of the 11th WSEAS international conference on Artificial Intelligence, Knowledge Engineering and Data Bases, AIKED’12,</booktitle>
<pages>83--87</pages>
<location>Stevens Point, Wisconsin, USA.</location>
<contexts>
<context position="5383" citStr="Choi et al., 2012" startWordPosition="821" endWordPosition="824"> propose a method that differs from Gaussier et al. (2004) in this way: If they focus on words ambiguities on source and target languages, we thought that it would be sufficient to disambiguate only translated source context vectors. 3 Context Vector Disambiguation 3.1 Semantic similarity measures A large number of WSD techniques were proposed in the literature. The most widely used ones are those that compute semantic similarity1 with the help of WordNet. WordNet has been used in many tasks relying on word-based similarity, including document (Hwang et al., 2011) and image (Cho et al., 2007; Choi et al., 2012) retrieval systems. In this work, we use it to derive a semantic similarity between lexical units within the same context vector. To the best of our knowledge, this is the first application of WordNet to bilingual lexicon extraction from comparable corpora. Among semantic similarity measures using WordNet, we distinguish: (1) measures based on path length which simply counts the distance between two words in the WordNet taxonomy, (2) measures relying on information content in which a semantically annotated corpus is needed to compute frequencies of words to be compared and (3) the ones using g</context>
</contexts>
<marker>Choi, Kim, Kim, Hwang, Kim, 2012</marker>
<rawString>Dongjin Choi, Jungin Kim, Hayoung Kim, Myunggwon Hwang, and Pankoo Kim. 2012. A method for enhancing image retrieval based on annotation using modified wup similarity in wordnet. In Proceedings of the 11th WSEAS international conference on Artificial Intelligence, Knowledge Engineering and Data Bases, AIKED’12, pages 83–87, Stevens Point, Wisconsin, USA. World Scientific and Engineering Academy and Society (WSEAS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
</authors>
<title>A statistical view on bilingual lexicon extraction: From parallel corpora to nonparallel corpora.</title>
<date>1998</date>
<booktitle>In Parallel Text Processing,</booktitle>
<pages>1--17</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1063" citStr="Fung, 1998" startWordPosition="145" endWordPosition="146">hat extends the standard approach used for bilingual lexicon extraction from comparable corpora. We focus on the unresolved problem of polysemous words revealed by the bilingual dictionary and introduce a use of a Word Sense Disambiguation process that aims at improving the adequacy of context vectors. On two specialized FrenchEnglish comparable corpora, empirical experimental results show that our method improves the results obtained by two stateof-the-art approaches. 1 Introduction Over the years, bilingual lexicon extraction from comparable corpora has attracted a wealth of research works (Fung, 1998; Rapp, 1995; Chiao and Zweigenbaum, 2003). The basic assumption behind most studies is a distributional hypothesis (Harris, 1954), which states that words with a similar meaning are likely to appear in similar contexts across languages. The so-called standard approach to bilingual lexicon extraction from comparable corpora is based on the characterization and comparison of context vectors of source and target words. Each element in the context vector of a source or target word represents its association with a word which occurs within a window of N words. To enable the comparison of source an</context>
</contexts>
<marker>Fung, 1998</marker>
<rawString>Pascale Fung. 1998. A statistical view on bilingual lexicon extraction: From parallel corpora to nonparallel corpora. In Parallel Text Processing, pages 1–17. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>´Eric Gaussier</author>
<author>Jean-Michel Renders</author>
<author>Irina Matveeva</author>
<author>Cyril Goutte</author>
<author>Herv´e D´ejean</author>
</authors>
<title>A geometric view on bilingual lexicon extraction from comparable corpora.</title>
<date>2004</date>
<booktitle>In ACL,</booktitle>
<pages>526--533</pages>
<marker>Gaussier, Renders, Matveeva, Goutte, D´ejean, 2004</marker>
<rawString>´Eric Gaussier, Jean-Michel Renders, Irina Matveeva, Cyril Goutte, and Herv´e D´ejean. 2004. A geometric view on bilingual lexicon extraction from comparable corpora. In ACL, pages 526–533.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z S Harris</author>
</authors>
<date>1954</date>
<note>Distributional structure. Word.</note>
<contexts>
<context position="1193" citStr="Harris, 1954" startWordPosition="164" endWordPosition="165">blem of polysemous words revealed by the bilingual dictionary and introduce a use of a Word Sense Disambiguation process that aims at improving the adequacy of context vectors. On two specialized FrenchEnglish comparable corpora, empirical experimental results show that our method improves the results obtained by two stateof-the-art approaches. 1 Introduction Over the years, bilingual lexicon extraction from comparable corpora has attracted a wealth of research works (Fung, 1998; Rapp, 1995; Chiao and Zweigenbaum, 2003). The basic assumption behind most studies is a distributional hypothesis (Harris, 1954), which states that words with a similar meaning are likely to appear in similar contexts across languages. The so-called standard approach to bilingual lexicon extraction from comparable corpora is based on the characterization and comparison of context vectors of source and target words. Each element in the context vector of a source or target word represents its association with a word which occurs within a window of N words. To enable the comparison of source and target vectors, words in the source vectors are translated into the target language using an existing bilingual dictionary. The </context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Z.S. Harris. 1954. Distributional structure. Word.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amir Hazem</author>
<author>Emmanuel Morin</author>
</authors>
<title>Adaptive dictionary for bilingual lexicon extraction from comparable corpora.</title>
<date>2012</date>
<booktitle>In Proceedings, 8th international conference on Language Resources and Evaluation (LREC),</booktitle>
<location>Istanbul, Turkey,</location>
<contexts>
<context position="4058" citStr="Hazem and Morin, 2012" startWordPosition="608" endWordPosition="611">epresentative, better is the bilingual lexicon extraction were conducted. In these works, additional linguistic resources such as specialized dictionaries (Chiao and Zweigenbaum, 2002) or transliterated words (Prochasson et al., 2009) were combined with the bilingual dic759 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 759–764, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics tionary to translate context vectors. Few works have however focused on the ambiguity problem revealed by the seed bilingual dictionary. (Hazem and Morin, 2012) propose a method that filters the entries of the bilingual dictionary on the base of a POS-Tagging and a domain relevance measure criteria but no improvements have been demonstrated. Gaussier et al. (2004) attempted to solve the problem of word ambiguities in the source and target languages. They investigated a number of techniques including canonical correlation analysis and multilingual probabilistic latent semantic analysis. The best results, with an improvement of the F-Measure (+0.02 at Top20) were reported for a mixed method. Recently, (Morin and Prochasson, 2011) proceed as the standar</context>
<context position="10806" citStr="Hazem and Morin, 2012" startWordPosition="1691" endWordPosition="1694">ilingual dictionary which contains about 120,000 entries belonging to the general language with an average of 7 translations per entry. In bilingual terminology extraction from comparable corpora, a reference list is required to evaluate the performance of the alignment. Such lists are often composed of about 100 single 3http://dumps.wikimedia.org/ 4Comparable corpora will be shared publicly Corpus French English PR Corporate finance 402.486 756.840 41% Breast cancer 396.524 524.805 47% Table 1: Comparable corpora sizes in term of words and polysemy rates (PR) associated to each corpus terms (Hazem and Morin, 2012; Chiao and Zweigenbaum, 2002). Here, we created two reference lists5 for the corporate finance and the breast cancer sub-domains. The first list is composed of 125 single terms extracted from the glossary of bilingual micro-finance terms6. The second list contains 79 terms extracted from the FrenchEnglish MESH and the UMLS thesauri7. Note that reference terms pairs appear more than five times in each part of both comparable corpora. Three other parameters need to be set up, namely the window size, the association measure and the similarity measure. We followed (Laroche and Langlais, 2010) to </context>
</contexts>
<marker>Hazem, Morin, 2012</marker>
<rawString>Amir Hazem and Emmanuel Morin. 2012. Adaptive dictionary for bilingual lexicon extraction from comparable corpora. In Proceedings, 8th international conference on Language Resources and Evaluation (LREC), Istanbul, Turkey, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Myunggwon Hwang</author>
<author>Chang Choi</author>
<author>Pankoo Kim</author>
</authors>
<title>Automatic enrichment of semantic relation network and its application to word sense disambiguation.</title>
<date>2011</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<pages>23--845</pages>
<contexts>
<context position="5335" citStr="Hwang et al., 2011" startWordPosition="810" endWordPosition="813">to their frequency in the target corpus. Here, we propose a method that differs from Gaussier et al. (2004) in this way: If they focus on words ambiguities on source and target languages, we thought that it would be sufficient to disambiguate only translated source context vectors. 3 Context Vector Disambiguation 3.1 Semantic similarity measures A large number of WSD techniques were proposed in the literature. The most widely used ones are those that compute semantic similarity1 with the help of WordNet. WordNet has been used in many tasks relying on word-based similarity, including document (Hwang et al., 2011) and image (Cho et al., 2007; Choi et al., 2012) retrieval systems. In this work, we use it to derive a semantic similarity between lexical units within the same context vector. To the best of our knowledge, this is the first application of WordNet to bilingual lexicon extraction from comparable corpora. Among semantic similarity measures using WordNet, we distinguish: (1) measures based on path length which simply counts the distance between two words in the WordNet taxonomy, (2) measures relying on information content in which a semantically annotated corpus is needed to compute frequencies </context>
</contexts>
<marker>Hwang, Choi, Kim, 2011</marker>
<rawString>Myunggwon Hwang, Chang Choi, and Pankoo Kim. 2011. Automatic enrichment of semantic relation network and its application to word sense disambiguation. IEEE Transactions on Knowledge and Data Engineering, 23:845–858.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Audrey Laroche</author>
<author>Philippe Langlais</author>
</authors>
<title>Revisiting context-based projection methods for termtranslation spotting in comparable corpora.</title>
<date>2010</date>
<booktitle>In 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>617--625</pages>
<location>Beijing, China,</location>
<contexts>
<context position="11402" citStr="Laroche and Langlais, 2010" startWordPosition="1788" endWordPosition="1791">us terms (Hazem and Morin, 2012; Chiao and Zweigenbaum, 2002). Here, we created two reference lists5 for the corporate finance and the breast cancer sub-domains. The first list is composed of 125 single terms extracted from the glossary of bilingual micro-finance terms6. The second list contains 79 terms extracted from the FrenchEnglish MESH and the UMLS thesauri7. Note that reference terms pairs appear more than five times in each part of both comparable corpora. Three other parameters need to be set up, namely the window size, the association measure and the similarity measure. We followed (Laroche and Langlais, 2010) to define these parameters. They carried out a complete study of the influence of these parameters on the bilingual alignment. The context vectors were defined by computing the Discounted Log-Odds Ratio (equation 3) between words occurring in the same context window of size 7. (O11 + 12)(O22 + 12) (3) Odds-Ratiodisc =log + 1 2)(O21 + 1 (O12 2) where Oij are the cells of the 2 x 2 contingency matrix of a token s co-occurring with the term S within a given window size. As similarity measure, we chose to use the cosine measure. 4.2 Results of bilingual lexicon extraction To evaluate the performa</context>
</contexts>
<marker>Laroche, Langlais, 2010</marker>
<rawString>Audrey Laroche and Philippe Langlais. 2010. Revisiting context-based projection methods for termtranslation spotting in comparable corpora. In 23rd International Conference on Computational Linguistics (Coling 2010), pages 617–625, Beijing, China, Aug.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Martin Chodorow</author>
</authors>
<title>Combining local context and WordNet similarity for word sense identification,</title>
<date>1998</date>
<pages>305--332</pages>
<editor>In C. Fellbaum (Ed.),</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="6385" citStr="Leacock and Chodorow, 1998" startWordPosition="975" endWordPosition="978">ts the distance between two words in the WordNet taxonomy, (2) measures relying on information content in which a semantically annotated corpus is needed to compute frequencies of words to be compared and (3) the ones using gloss overlap which are designed to compute semantic relatedness. In this work, we use five similarity measures and compare their performances. These measures include three 1For consiseness, we often use “semantic similarity” to refer collectively to both similarity and relatedness. path-based semantic similarity measures denoted PATH,WUP (Wu and Palmer, 1994) and LEACOCK (Leacock and Chodorow, 1998). PATH is a baseline that is equal to the inverse of the shortest path between two words. WUP finds the depth of the least common subsumer of the words, and scales that by the sum of the depths of individual words. The depth of a word is its distance to the root node. LEACOCK finds the shortest path between two words, and scales that by the maximum path length found in the is–a hierarchy in which they occur. Path length measures have the advantage of being independent of corpus statistics, and therefor uninfluenced by sparse data. Since semantic relatedness is considered to be more general tha</context>
</contexts>
<marker>Leacock, Chodorow, 1998</marker>
<rawString>Claudia Leacock and Martin Chodorow, 1998. Combining local context and WordNet similarity for word sense identification, pages 305–332. In C. Fellbaum (Ed.), MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Li</author>
<author>¨Eric Gaussier</author>
</authors>
<title>Improving corpus comparability for bilingual lexicon extraction from comparable corpora.</title>
<date>2010</date>
<booktitle>In 23rd International Conference on Computational Linguistics (Coling</booktitle>
<location>Beijing, China,</location>
<marker>Li, Gaussier, 2010</marker>
<rawString>Bo Li and ¨Eric Gaussier. 2010. Improving corpus comparability for bilingual lexicon extraction from comparable corpora. In 23rd International Conference on Computational Linguistics (Coling 2010), Beijing, China, Aug.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Montague</author>
<author>Javed A Aslam</author>
</authors>
<title>Condorcet fusion for improved retrieval.</title>
<date>2002</date>
<booktitle>In Proceedings of the eleventh international conference on Information and knowledge management, CIKM ’02,</booktitle>
<pages>538--548</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="15812" citStr="Montague and Aslam, 2002" startWordPosition="2499" endWordPosition="2502">ement of respectively +0.097 and +0.037% over SA and MP11. Each of the tested 5 semantic similarity measures provides a different view of how to rank the translations of a given test word. Combining the obtained ranked lists should reinforce the confidence in consensus translations, while decreasing the confidence in non-consensus translations. We have therefore tested their combination. For this, we used a voting method, and chose one in the Condorcet family the Condorcet data fusion method. This method was widely used to combine document retrieval results from information retrieval systems (Montague and Aslam, 2002; Nuray and Can, 2006). It is a single-winner election method that ranks the candidates in order of preference. It is a pairwise voting, i.e. it compares every possible pair of candidates to decide the preference of them. A matrix can be used to present the competition process. Every candidate appears in the matrix as a row and a column as well. If there are m candidates, then we need m2 elements in the matrix in total. Initially 0 is written to all the elements. If di is preferred to dj , then we add 1 to the element at row i and column j (aij). The pro762 cess is repeated until all the ballo</context>
</contexts>
<marker>Montague, Aslam, 2002</marker>
<rawString>Mark Montague and Javed A. Aslam. 2002. Condorcet fusion for improved retrieval. In Proceedings of the eleventh international conference on Information and knowledge management, CIKM ’02, pages 538–548, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanuel Morin</author>
<author>Emmanuel Prochasson</author>
</authors>
<title>Bilingual lexicon extraction from comparable corpora enhanced with parallel corpora.</title>
<date>2011</date>
<booktitle>In Proceedings, 4th Workshop on Building and Using Comparable Corpora (BUCC),</booktitle>
<pages>27--34</pages>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="4635" citStr="Morin and Prochasson, 2011" startWordPosition="696" endWordPosition="700">seed bilingual dictionary. (Hazem and Morin, 2012) propose a method that filters the entries of the bilingual dictionary on the base of a POS-Tagging and a domain relevance measure criteria but no improvements have been demonstrated. Gaussier et al. (2004) attempted to solve the problem of word ambiguities in the source and target languages. They investigated a number of techniques including canonical correlation analysis and multilingual probabilistic latent semantic analysis. The best results, with an improvement of the F-Measure (+0.02 at Top20) were reported for a mixed method. Recently, (Morin and Prochasson, 2011) proceed as the standard approach but weigh the different translations according to their frequency in the target corpus. Here, we propose a method that differs from Gaussier et al. (2004) in this way: If they focus on words ambiguities on source and target languages, we thought that it would be sufficient to disambiguate only translated source context vectors. 3 Context Vector Disambiguation 3.1 Semantic similarity measures A large number of WSD techniques were proposed in the literature. The most widely used ones are those that compute semantic similarity1 with the help of WordNet. WordNet h</context>
<context position="12120" citStr="Morin and Prochasson, 2011" startWordPosition="1917" endWordPosition="1920">rameters on the bilingual alignment. The context vectors were defined by computing the Discounted Log-Odds Ratio (equation 3) between words occurring in the same context window of size 7. (O11 + 12)(O22 + 12) (3) Odds-Ratiodisc =log + 1 2)(O21 + 1 (O12 2) where Oij are the cells of the 2 x 2 contingency matrix of a token s co-occurring with the term S within a given window size. As similarity measure, we chose to use the cosine measure. 4.2 Results of bilingual lexicon extraction To evaluate the performance of our approach, we used both the standard approach (SA) and the approach proposed by (Morin and Prochasson, 2011) (henceforth MP11) as baselines. The experiments were performed with respect to the five semantic similarity measures described in section 3.1. Each measure provides, for each polysemous word, a ranked list of translations. A question that arises here is whether we should introduce only the topranked translation into the context vector or consider a larger number of translations, mainly when a translation list contains synonyms. For this 5Reference lists will be shared publicly 6http://www.microfinance.lu/en/ 7http://www.nlm.nih.gov/ 761 Method WN-T1 WN-T2 WN-T3 WN-T4 WN-T5 WN-T6 WN-T7 Standar</context>
<context position="13548" citStr="Morin and Prochasson, 2011" startWordPosition="2137" endWordPosition="2140"> 0.293 0.293 0.275 0.258 0.250 0.215 VECTOR 0.267 0.310 0.284 0.284 0.232 0.232 0.232 CONDORCETMerge 0.362 0.379 0.353 0.362 0.336 0.275 0.267 Method WN-T1 WN-T2 WN-T3 WN-T4 WN-T5 WN-T6 WN-T7 Standard Approach (SA) 0.493 MP11 0.553 Single WUP 0.481 0.566 0.566 0.542 0.554 0.542 0.554 measure PATH 0.542 0.542 0.554 0.566 0.578 0.554 0.554 LEACOCK 0.506 0.578 0.554 0.566 0.542 0.554 0.542 LESK 0.469 0.542 0.542 0.590 0.554 0.554 0.542 VECTOR 0.518 0.566 0.530 0.566 0.542 0.566 0.554 CONDORCETMerge 0.566 0.614 0.600 0.590 0.600 0.578 0.578 Table 2: F-Measure at Top20 for the two domains; MP11 = (Morin and Prochasson, 2011). In each column, italics shows best single similarity measure, bold shows best result. Underline shows best result overall. a) Corporate Finance b) Breast Cancer reason, we take into account in our experiments different numbers of translations, noted WN-Ti, ranging from the pivot translation (i = 1) to the seventh word in the translation list. This choice is motivated by the fact that words in both corpora have on average 7 translations in the bilingual dictionary. Both baseline systems use all translations associated to each entry in the bilingual dictionary. The only difference is that in M</context>
</contexts>
<marker>Morin, Prochasson, 2011</marker>
<rawString>Emmanuel Morin and Emmanuel Prochasson. 2011. Bilingual lexicon extraction from comparable corpora enhanced with parallel corpora. In Proceedings, 4th Workshop on Building and Using Comparable Corpora (BUCC), page 27–34, Portland, Oregon, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rabia Nuray</author>
<author>Fazli Can</author>
</authors>
<title>Automatic ranking of information retrieval systems using data fusion.</title>
<date>2006</date>
<journal>Inf. Process. Manage.,</journal>
<volume>42</volume>
<issue>3</issue>
<contexts>
<context position="15834" citStr="Nuray and Can, 2006" startWordPosition="2503" endWordPosition="2507">97 and +0.037% over SA and MP11. Each of the tested 5 semantic similarity measures provides a different view of how to rank the translations of a given test word. Combining the obtained ranked lists should reinforce the confidence in consensus translations, while decreasing the confidence in non-consensus translations. We have therefore tested their combination. For this, we used a voting method, and chose one in the Condorcet family the Condorcet data fusion method. This method was widely used to combine document retrieval results from information retrieval systems (Montague and Aslam, 2002; Nuray and Can, 2006). It is a single-winner election method that ranks the candidates in order of preference. It is a pairwise voting, i.e. it compares every possible pair of candidates to decide the preference of them. A matrix can be used to present the competition process. Every candidate appears in the matrix as a row and a column as well. If there are m candidates, then we need m2 elements in the matrix in total. Initially 0 is written to all the elements. If di is preferred to dj , then we add 1 to the element at row i and column j (aij). The pro762 cess is repeated until all the ballots are processed. For </context>
</contexts>
<marker>Nuray, Can, 2006</marker>
<rawString>Rabia Nuray and Fazli Can. 2006. Automatic ranking of information retrieval systems using data fusion. Inf. Process. Manage., 42(3):595–614, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
</authors>
<title>Incorporating Dictionary and Corpus Information into a Context Vector Measure of Semantic Relatedness. Master’s thesis,</title>
<date>2003</date>
<institution>University of Minnesota,</institution>
<location>Duluth,</location>
<contexts>
<context position="7110" citStr="Patwardhan, 2003" startWordPosition="1103" endWordPosition="1104">h of the least common subsumer of the words, and scales that by the sum of the depths of individual words. The depth of a word is its distance to the root node. LEACOCK finds the shortest path between two words, and scales that by the maximum path length found in the is–a hierarchy in which they occur. Path length measures have the advantage of being independent of corpus statistics, and therefor uninfluenced by sparse data. Since semantic relatedness is considered to be more general than semantic similarity, we also use two relatedness measures: LESK (Banerjee and Pedersen, 2002) and VECTOR (Patwardhan, 2003). LESK finds overlaps between the glosses of word pairs, as well as words’ hyponyms. VECTOR creates a co-occurrence matrix for each gloss token. Each gloss is then represented as a vector that averages token co-occurrences. 3.2 Disambiguation process Once translated into the target language, the context vectors disambiguation process intervenes. This process operates locally on each context vector and aims at finding the most prominent translations of polysemous words. For this purpose, we use monosemic words as a seed set of disambiguated words to infer the polysemous word’s translations sens</context>
</contexts>
<marker>Patwardhan, 2003</marker>
<rawString>Siddharth Patwardhan. 2003. Incorporating Dictionary and Corpus Information into a Context Vector Measure of Semantic Relatedness. Master’s thesis, University of Minnesota, Duluth, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanuel Prochasson</author>
<author>Emmanuel Morin</author>
<author>Kyo Kageura</author>
</authors>
<title>Anchor points for bilingual lexicon extraction from small comparable corpora.</title>
<date>2009</date>
<booktitle>In Proceedings, 12th Conference on Machine Translation Summit (MT Summit XII),</booktitle>
<pages>284--291</pages>
<location>Ottawa, Ontario, Canada.</location>
<contexts>
<context position="3670" citStr="Prochasson et al., 2009" startWordPosition="553" endWordPosition="556">comparable corpora (financial and medical) and report improved results compared to two state-of-the-art approaches. 2 Related Work Most previous works addressing the task of bilingual lexicon extraction from comparable corpora are based on the standard approach. In order to improve the results of this approach, recent researches based on the assumption that more the context vectors are representative, better is the bilingual lexicon extraction were conducted. In these works, additional linguistic resources such as specialized dictionaries (Chiao and Zweigenbaum, 2002) or transliterated words (Prochasson et al., 2009) were combined with the bilingual dic759 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 759–764, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics tionary to translate context vectors. Few works have however focused on the ambiguity problem revealed by the seed bilingual dictionary. (Hazem and Morin, 2012) propose a method that filters the entries of the bilingual dictionary on the base of a POS-Tagging and a domain relevance measure criteria but no improvements have been demonstrated. Gaussier et al. (2004) attem</context>
</contexts>
<marker>Prochasson, Morin, Kageura, 2009</marker>
<rawString>Emmanuel Prochasson, Emmanuel Morin, and Kyo Kageura. 2009. Anchor points for bilingual lexicon extraction from small comparable corpora. In Proceedings, 12th Conference on Machine Translation Summit (MT Summit XII), page 284–291, Ottawa, Ontario, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>Identifying word translations in non-parallel texts.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd annual meeting on Association for Computational Linguistics, ACL ’95,</booktitle>
<pages>320--322</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1075" citStr="Rapp, 1995" startWordPosition="147" endWordPosition="148">the standard approach used for bilingual lexicon extraction from comparable corpora. We focus on the unresolved problem of polysemous words revealed by the bilingual dictionary and introduce a use of a Word Sense Disambiguation process that aims at improving the adequacy of context vectors. On two specialized FrenchEnglish comparable corpora, empirical experimental results show that our method improves the results obtained by two stateof-the-art approaches. 1 Introduction Over the years, bilingual lexicon extraction from comparable corpora has attracted a wealth of research works (Fung, 1998; Rapp, 1995; Chiao and Zweigenbaum, 2003). The basic assumption behind most studies is a distributional hypothesis (Harris, 1954), which states that words with a similar meaning are likely to appear in similar contexts across languages. The so-called standard approach to bilingual lexicon extraction from comparable corpora is based on the characterization and comparison of context vectors of source and target words. Each element in the context vector of a source or target word represents its association with a word which occurs within a window of N words. To enable the comparison of source and target vec</context>
</contexts>
<marker>Rapp, 1995</marker>
<rawString>Reinhard Rapp. 1995. Identifying word translations in non-parallel texts. In Proceedings of the 33rd annual meeting on Association for Computational Linguistics, ACL ’95, pages 320–322. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhibiao Wu</author>
<author>Martha Palmer</author>
</authors>
<title>Verbs semantics and lexical selection.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd annual meeting on Association for Computational Linguistics, ACL ’94,</booktitle>
<pages>133--138</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6344" citStr="Wu and Palmer, 1994" startWordPosition="968" endWordPosition="971">d on path length which simply counts the distance between two words in the WordNet taxonomy, (2) measures relying on information content in which a semantically annotated corpus is needed to compute frequencies of words to be compared and (3) the ones using gloss overlap which are designed to compute semantic relatedness. In this work, we use five similarity measures and compare their performances. These measures include three 1For consiseness, we often use “semantic similarity” to refer collectively to both similarity and relatedness. path-based semantic similarity measures denoted PATH,WUP (Wu and Palmer, 1994) and LEACOCK (Leacock and Chodorow, 1998). PATH is a baseline that is equal to the inverse of the shortest path between two words. WUP finds the depth of the least common subsumer of the words, and scales that by the sum of the depths of individual words. The depth of a word is its distance to the root node. LEACOCK finds the shortest path between two words, and scales that by the maximum path length found in the is–a hierarchy in which they occur. Path length measures have the advantage of being independent of corpus statistics, and therefor uninfluenced by sparse data. Since semantic related</context>
</contexts>
<marker>Wu, Palmer, 1994</marker>
<rawString>Zhibiao Wu and Martha Palmer. 1994. Verbs semantics and lexical selection. In Proceedings of the 32nd annual meeting on Association for Computational Linguistics, ACL ’94, pages 133–138. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>