<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001277">
<title confidence="0.923947">
Rich Source-Side Context for Statistical Machine Translation
</title>
<author confidence="0.997393">
Kevin Gimpel and Noah A. Smith
</author>
<affiliation confidence="0.917156">
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.999529">
{kgimpel,nasmith}@cs.cmu.edu
</email>
<sectionHeader confidence="0.998604" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999856428571428">
We explore the augmentation of statistical ma-
chine translation models with features of the
context of each phrase to be translated. This
work extends several existing threads of re-
search in statistical MT, including the use of
context in example-based machine translation
(Carl and Way, 2003) and the incorporation of
word sense disambiguation into a translation
model (Chan et al., 2007). The context fea-
tures we consider use surrounding words and
part-of-speech tags, local syntactic structure,
and other properties of the source language
sentence to help predict each phrase’s transla-
tion. Our approach requires very little compu-
tation beyond the standard phrase extraction
algorithm and scales well to large data sce-
narios. We report significant improvements
in automatic evaluation scores for Chinese-
to-English and English-to-German translation,
and also describe our entry in the WMT-08
shared task based on this approach.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999584411764706">
Machine translation (MT) by statistical modeling of
bilingual phrases is one of the most successful ap-
proaches in the past few years. Phrase-based MT
systems are straightforward to train from parallel
corpora (Koehn et al., 2003) and, like the origi-
nal IBM models (Brown et al., 1990), benefit from
standard language models built on large monolin-
gual, target-language corpora (Brants et al., 2007).
Many of these systems perform well in competitive
evaluations and scale well to large-data situations
(NIST, 2006; Callison-Burch et al., 2007). End-to-
end phrase-based MT systems can be built entirely
from freely-available tools (Koehn et al., 2007).
We follow the approach of Koehn et al. (2003),
in which we translate a source-language sentence f
into the target-language sentence e� that maximizes a
linear combination of features and weights:1
</bodyText>
<equation confidence="0.8998638">
h�e, �ai �argmax score(e, a, f) (1)
(e,a)
M
� argmax λmhm(e, a, f) (2)
(e,a) m=1
</equation>
<bodyText confidence="0.999879857142857">
where a represents the segmentation of e and f
into phrases and a correspondence between phrases,
and each hm is a R-valued feature with learned
weight λm. The translation is typically found us-
ing beam search (Koehn et al., 2003). The weights
hλ1,..., λMi are typically learned to directly mini-
mize a standard evaluation criterion on development
data (e.g., the BLEU score; Papineni et al., (2002))
using numerical search (Och, 2003).
Many features are used in phrase-based MT, but
nearly ubiquitous are estimates of the conditional
translation probabilities p(ei  |fk) and p(fk  |ei )
for each phrase pair hez, fki in the candidate sen-
tence pair.2 In this paper, we add and evaluate fea-
</bodyText>
<footnote confidence="0.999579">
1In the statistical MT literature, this is often referred to as a
“log-linear model,” but since the score is normalized during nei-
ther parameter training nor decoding, and is never interpreted as
a log-probability, it is essentially a linear combination of feature
functions. Since many of the features are actually probabilities,
this linear combination is closer to a mixture model.
2We will use xi to denote the subsequence of x containing
the ith through jth elements of x, inclusive.
</footnote>
<page confidence="0.915556">
9
</page>
<note confidence="0.7914415">
Proceedings of the Third Workshop on Statistical Machine Translation, pages 9–17,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.5687005">
tures that condition on additional context features on
the source (f) side:
</bodyText>
<equation confidence="0.8553805">
p(eji Phrase = f`k, Context = �fk−1
1 , fF`�1, ))
</equation>
<bodyText confidence="0.99998290625">
The advantage of considering context is well-
known and exploited in the example-based MT com-
munity (Carl and Way, 2003). Recently researchers
have begun to use source phrase context informa-
tion in statistical MT systems (Stroppa et al., 2007).
Statistical NLP researchers understand that condi-
tioning a probability model on more information is
helpful only if there are sufficient training data to ac-
curately estimate the context probabilities.3 Sparse
data are often the death of elaborate models, though
this can be remedied through careful smoothing.
In this paper we leverage the existing linear
model (Equation 2) to bring source-side context into
phrase-based MT in a way that is robust to data
sparseness. We interpret the linear model as a mix-
ture of many probability estimates based on different
context features, some of which may be very sparse.
The mixture coefficients are trained in the usual way
(“minimum error-rate training,” Och, 2003), so that
the additional context is exploited when it is useful
and ignored when it isn’t.
The paper proceeds as follows. We first review re-
lated work that enriches statistical translation mod-
els using context (§2). We then propose a set
of source-side features to be incorporated into the
translation model, including the novel use of syntac-
tic context from source-side parse trees and global
position within f (§3). We explain why analogous
target-side features pose a computational challenge
(§4). Specific modifications to the standard training
and evaluation paradigm are presented in §5. Exper-
imental results are reported in §6.
</bodyText>
<sectionHeader confidence="0.99989" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999929166666667">
Stroppa et al. (2007) added souce-side context fea-
tures to a phrase-based translation system, including
conditional probabilities of the same form that we
use. They consider up to two words and/or POS tags
of context on either side. Because of the aforemen-
tioned data sparseness problem, they use a decision-
</bodyText>
<footnote confidence="0.973418333333333">
3An illustrative example is the debate over the use of bilex-
icalized grammar rules in statistical parsing (Gildea, 2001;
Bikel, 2004).
</footnote>
<bodyText confidence="0.999812710526316">
tree classifier that implicitly smooths relative fre-
quency estimates. The method improved over a stan-
dard phrase-based baseline trained on small amounts
of data (&lt; 50K sentence pairs) for Italian —* English
and Chinese —* English. We explore a significantly
larger space of context features, a smoothing method
that more naturally fits into the widely used, error-
driven linear model, and report a more comprehen-
sive experimental evaluation (including feature com-
parison and scaling up to very large datasets).
Recent research on the use of word-sense dis-
ambiguation in machine translation also points to-
ward our approach. For example, Vickrey et al.
(2005) built classifiers inspired by those used in
word sense disambiguation to fill in blanks in
a partially-completed translation. Gim´enez and
M`arquez (2007) extended the work by considering
phrases and moved to full translation instead of fill-
ing in target-side blanks. They trained an SVM for
each source language phrase using local features of
the sentences in which the phrases appear. Carpuat
and Wu (2007) and Chan et al. (2007) embedded
state-of-the-art word sense disambiguation modules
into statistical MT systems, achieving performance
improvements under several automatic measures for
Chinese —* English translation.
Our approach is also reminiscent of example-
based machine translation (Nagao, 1984; Somers,
1999; Carl and Way, 2003), which has for many
years emphasized use of the context in which source
phrases appear when translating them. Indeed, like
the example-based community, we do not begin with
any set of assumptions about which kinds of phrases
require additional disambiguation (cf. the applica-
tion of word-sense disambiguation, which is moti-
vated by lexical ambiguity). Our feature-rich ap-
proach is omnivorous and can exploit any linguistic
analysis of an input sentence.
</bodyText>
<sectionHeader confidence="0.9887" genericHeader="method">
3 Source-Side Context Features
</sectionHeader>
<bodyText confidence="0.999949857142857">
Adding features to the linear model (Equation 2)
that consider more of the source sentence requires
changing the decoder very little, if at all. The reason
is that the source sentence is fully observed, so the
information to be predicted is the same as before—
the difference is that we are using more clues to
carry out the prediction.
</bodyText>
<page confidence="0.996951">
10
</page>
<bodyText confidence="0.999987684210526">
We see this as an opportunity to include many
more features in phrase-based MT without increas-
ing the cost of decoding at runtime. This discussion
is reminiscent of an advantage gained by moving
from hidden Markov models to conditional random
fields for sequence labeling tasks. While the same
core algorithm is used for decoding with both mod-
els, a CRF allows inclusion of features that consider
the entire observed sequence—i.e., more of the ob-
servable context of each label to be predicted. Al-
though this same advantage was already obtained
in statistical MT through the transition from “noisy
channel” translation models to (log-)linear models,
the customary set of features used in most phrase-
based systems does not take full advantage of the
observed data.
The standard approach to estimating the phrase
translation conditional probability features is via rel-
ative frequencies (here e and f are phrases):
</bodyText>
<equation confidence="0.988703333333333">
count(e, f)
p(e  |f) =
Eei count(e , f)
</equation>
<bodyText confidence="0.999865296296296">
Our new features all take the form p(e |
f, fcontext), where e is the target language phrase,
f is the source language phrase, and fcontext is the
context of the source language phrase in the sentence
in which it was observed. Like the context-bare con-
ditional probabilities, we estimate probability fea-
tures using relative frequencies:
Since we expect that adding conditioning vari-
ables will lead to sparser counts and therefore more
zero estimates, we compute features for many dif-
ferent types of context. To combine the many
differently-conditioned features into a single model,
we provide them as features to the linear model
(Equation 2) and use minimum error-rate training
(Och, 2003) to obtain interpolation weights Am.
This is similar to an interpolation of backed-off es-
timates, if we imagine that all of the different con-
texts are differently-backed off estimates of the com-
plete context. The error-driven weight training ef-
fectively smooths one implicit context-rich estimate
p(e  |f,fcontext) so that all of the backed-off es-
timates are taken into account, including the orig-
inal p(e  |f). Our approach is asymmetrical; we
have not, for example, estimated features of the form
p(f,fcontext  |e).
We next discuss the specific source-side context
features used in our model.
</bodyText>
<subsectionHeader confidence="0.99913">
3.1 Lexical Context Features
</subsectionHeader>
<bodyText confidence="0.946677">
The most obvious kind of context of a source phrase
f` k is the m-length sequence before it (fk−1
k−m) and
the m-length sequence after it (f`+m
`+1 ). We include
context features for m E 11, 21, padding sentences
with m special symbols at the beginning and at the
end. For each value of m, we include three features:
</bodyText>
<listItem confidence="0.9940432">
• p(e  |f, fk−1
k−m), the left lexical context;
• p(e  |f, f`+m
`+1 ), the right lexical context;
• p(e  |f, fk−1
</listItem>
<equation confidence="0.4664545">
k−m, f`+m
`+1 ), both sides.
</equation>
<subsectionHeader confidence="0.999579">
3.2 Shallow Syntactic Features
</subsectionHeader>
<bodyText confidence="0.9967205">
Lexical context features, especially when m &gt; 1,
are expected to be sparse. Representing the context
by part-of-speech (POS) tags is one way to over-
come that sparseness. We used the same set of the
lexical context features described above, but with
POS tags replacing words in the context. We also
include a feature which conditions on the POS tag
sequence of the actual phrase being translated.
</bodyText>
<subsectionHeader confidence="0.998875">
3.3 Syntactic Features
</subsectionHeader>
<bodyText confidence="0.999929">
If a robust parser is available for the source lan-
guage, we can include context features from parse
trees. We used the following parse tree features:
</bodyText>
<listItem confidence="0.988782454545455">
• Is the phrase (exactly) a constituent?
• What is the nonterminal label of the lowest node
in the parse tree that covers the phrase?
• What is the nonterminal label or POS of the high-
est nonterminal node that ends immediately be-
fore the phrase? Begins immediately after the
phrase?
• Is the phrase strictly to the left of the root word,
does it contain the root word, or is it strictly to
the right of the root word? (Requires a parse with
head annotations.)
</listItem>
<bodyText confidence="0.8128165">
We also used a feature that conditions on both fea-
tures in the third bullet point above.
p(e  |f, fcontext) = Eek count(el, f, fcontext)
count(e, f, fcontext)
</bodyText>
<page confidence="0.951114">
11
</page>
<figure confidence="0.998164205882353">
Against this background , we support the report of the committee
In dieser Hinsicht unterstützen wir den Bericht des Ausschusses
Syntactic
Shallow
Phrase
Lexical
IN DT NN , PRP VBP DT NN IN DT NN IN NN CC NN , ...
PP
NP
NP
S[support]
VP
NP
NP
PP
NP
Syntactic Features: Positional Features:
Not a constituent Not at start
NP covers phrase Not at end
VBP to left Second fifth of sentence
PP to right Covers 18.5% of sentence
Right of root word (quantized to 20%)
NP
für Verkehr und Fremdenverkehr , in...
on transport and tourism , which...
PP
NP
SBAR
.
.
.
.
..
...
</figure>
<figureCaption confidence="0.60508875">
Figure 1: A (partial) sentence pair from the WMT-07 Europarl training corpus. Processing of the data (parsing, word
alignment) was done as discussed in §6. The phrase pair of interest is boxed and context features are shown in dotted
shapes. The context features help determine whether the phrase should be translated as “der Bericht des Ausschusses”
(nominative case) or “den Bericht des Ausschusses” (accusative case). See text for details.
</figureCaption>
<subsectionHeader confidence="0.975199">
3.4 Positional Features
</subsectionHeader>
<bodyText confidence="0.9998676">
We include features based on the position of the
phrase in the source sentence, the phrase length, and
the sentence length. These features use information
from the entire source sentence, but are not syntac-
tic. For a phrase f` k in a sentence f of length n:
</bodyText>
<listItem confidence="0.99517075">
• Is the phrase at the start of the sentence (k = 1)?
• Is the phrase at the end of the sentence (E = n)?
k+I−k+1
• A quantization of r = n 2 , the relative po-
sition in (0, 1) of the phrase’s midpoint within f.
We choose the smallest q E {0.2, 0.4, 0.6, 0.8, 1}
such that q &gt; r.
• A quantization of c = `−k+1
</listItem>
<bodyText confidence="0.99582712">
n , the fraction of the
words in f that are covered by the phrase. We
choose the smallest q E { 140, 120. 110, 1�, 1�,1} such
that q &gt; c.
An illustration of the context features is shown
in Fig. 1. Consider the phrase pair “the report
of the committee”/“den Bericht des Ausschusses”
extracted by our English —* German baseline MT
system (described in §6.3). The German word
“Bericht” is a masculine noun; therefore, it takes the
article “der” in the nominative case, “den” in the ac-
cusative case, and “dem” in the dative case. These
three translations are indeed available in the phrase
table for “the report of the committee” (see Table 1,
“no context” column), with relatively high entropy.
The choice between “den” and “der” must be made
by the language model alone.
Knowing that the phrase follows a verb, or ap-
pears to the right of the sentence’s root word, or
within the second fifth of the sentence should help.
Indeed, a probability distribution that conditions on
context features gives more peaked distributions that
give higher probability to the correct translation,
given this context, and lower probability given some
other contexts (see Table 1).
</bodyText>
<sectionHeader confidence="0.994909" genericHeader="method">
4 Why Not Target-Side Context?
</sectionHeader>
<bodyText confidence="0.999976529411765">
While source context is straightforward to exploit
in a model, including target-side context features
breaks one of the key independence assumptions
made by phrase-based translation models: the trans-
lations of the source-side phrases are conditionally
independent of each other, given f, thereby requir-
ing new algorithms for decoding (Marino et al.,
2006).
We suggest that target-side context may already
be well accounted for in current MT systems. In-
deed, language models pay attention to the local
context of phrases, as do reordering models. The re-
cent emphasis on improving these components of a
translation system (Brants et al., 2007) is likely due
in part to the widespread availability of NLP tools
for the language that is most frequently the target:
English. We will demonstrate that NLP tools (tag-
</bodyText>
<page confidence="0.976496">
12
</page>
<table confidence="0.9800926">
9 no context Shallow: 2 POS on left Syntax: of root Positional: rel. pos.
*“PRP VBP” “VBN IN” *right left *2nd fifth 1st fifth
den bericht des ausschusses 0.3125 1.0000 0.3333 0.5000 0.0000 0.6000 0.0000
der bericht des ausschusses 0.3125 0.0000 0.0000 0.1000 0.6667 0.2000 0.6667
dem bericht des ausschusses 0.2500 0.0000 0.6667 0.3000 0.1667 0.0000 0.1667
</table>
<tableCaption confidence="0.984781333333333">
Table 1: Phrase table entries for “the report of the committee” and their scores under different contexts. These are the
top three phrases in the baseline English → German system (“no context” column). Contexts from the source sentence
in Fig. 1 (starred) predict correctly; we show also alternative contexts that give very different distributions.
</tableCaption>
<bodyText confidence="0.970072333333333">
gers and parsers) for the source side can be used to
improve the translation model, exploiting analysis
tools for other languages.
</bodyText>
<sectionHeader confidence="0.998611" genericHeader="method">
5 Implementation
</sectionHeader>
<bodyText confidence="0.999997">
The additional data required to compute the context
features is extracted along with the phrase pairs dur-
ing execution of the standard phrase extraction algo-
rithm, affecting phrase extraction and scoring time
by a constant factor. We avoid the need to modify
the standard phrase-based decoder to handle context
features by appending a unique identifier to each to-
ken in the sentences to be translated. Then, we pre-
compute a phrase table for the phrases in these sen-
tences according to the phrase contexts. To avoid
extremely long lists of translations of common to-
kens, we filter the generated phrase tables, remov-
ing entries for which the estimate of p(e  |f) &lt; c,
for some small c. In our experiments, we fixed
c = 0.0002. This filtering reduced time for exper-
imentation dramatically and had no apparent effect
on the translation output. We did not perform any
filtering for the baseline system.
</bodyText>
<sectionHeader confidence="0.999776" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.992962">
In this section we present experimental results using
our context-endowed phrase translation model with
a variety of different context features, on Chinese →
English, German → English, and English → Ger-
</bodyText>
<table confidence="0.99757775">
Context features Chinese → English (UN)
BLEU NIST METEOR
None 0.3715 7.918 0.6486
Lexical 0.4030 8.367 0.6716
Shallow 0.3807 7.981 0.6523
Lexical + Shallow 0.4030 8.403 0.6703
Syntactic 0.3823 7.992 0.6531
Positional 0.3775 7.958 0.6510
</table>
<tableCaption confidence="0.990870666666667">
Table 2: Chinese → English experiments: training and
testing on UN data. Boldface marks scores significantly
higher than “None.”
</tableCaption>
<bodyText confidence="0.99783059375">
man translation tasks. Dataset details are given in
Appendices A (Chinese) and B (German).
Baseline We use the Moses MT system (Koehn et
al., 2007) as a baseline and closely follow the exam-
ple training procedure given for the WMT-07 and
WMT-08 shared tasks.4 In particular, we perform
word alignment in each direction using GIZA++
(Och and Ney, 2003), apply the “grow-diag-final-
and” heuristic for symmetrization and use a max-
imum phrase length of 7. In addition to the two
phrase translation conditionals p(e  |f) and p(f |
e), we use lexical translation probabilities in each
direction, a word penalty, a phrase penalty, a length-
based reordering model, a lexicalized reordering
model, and an n-gram language model, SRILM im-
plementation (Stolcke, 2002) with modified Kneser-
Ney smoothing (Chen and Goodman, 1998). Mini-
mum error-rate (MER) training (Och, 2003) was ap-
plied to obtain weights (am in Equation 2) for these
features. A recaser is trained on the target side of the
parallel corpus using the script provided with Moses.
All output is recased and detokenized prior to evalu-
ation.
Evaluation We evaluate translation output using
three automatic evaluation measures: BLEU (Pap-
ineni et al., 2002), NIST (Doddington, 2002), and
METEOR (Banerjee and Lavie, 2005, version 0.6).5
All measures used were the case-sensitive, corpus-
level versions. The version of BLEU used was that
provided by NIST. Significance was tested using a
paired bootstrap (Koehn, 2004) with 1000 samples
(p &lt; 0.05).6
</bodyText>
<footnote confidence="0.986191125">
4http://www.statmt.org/wmt08
5METEOR details: For English, we use exact matching,
Porter stemming, and WordNet synonym matching. For Ger-
man, we use exact matching and Porter stemming. These are the
same settings that were used to evaluate systems for the WMT-
07 shared task.
6Code implementing this test for these metrics can be freely
downloaded at http://www.ark.cs.cmu.edu/MT.
</footnote>
<page confidence="0.996365">
13
</page>
<table confidence="0.999885">
Context features BLEU Chinese → English
Testing on UN Testing on News (NIST 2005)
NIST METEOR BLEU NIST METEOR
Training on in-domain data only:
None 0.3715 7.918 0.6486 0.2700 7.986 0.5314
Training on all data:
None 0.3615 7.797 0.6414 0.2593 7.697 0.5200
Lexical 0.3898 8.231 0.6697 0.2522 7.852 0.5273
Shallow: ≤ 1 POS tag 0.3611 7.713 0.6430 0.2669 8.243 0.5526
Shallow: ≤ 2 POS tags 0.3657 7.808 0.6455 0.2591 7.843 0.5288
Lexical + Shallow 0.3886 8.245 0.6675 0.2628 7.881 0.5290
Syntactic 0.3717 7.899 0.6531 0.2653 8.123 0.5403
Lexical + Syntactic 0.3926 8.224 0.6636 0.2572 7.774 0.5234
Positional 0.3647 7.766 0.6469 0.2648 7.891 0.5275
All 0.3772 8.176 0.6582 0.2566 7.775 0.5225
Feature selection (see Sec. 6.4) 0.3843 8.079 0.6594 0.2730 8.059 0.5343
</table>
<tableCaption confidence="0.956286333333333">
Table 3: Chinese → English experiments: first row shows baseline performance when training only on in-domain
data for each task; all other rows show results when training on all data (UN and News). Left half shows results when
tuning and testing on UN test sets; right half shows results when tuning on NIST 2004 News test set and testing on
NIST 2005. For feature selection, an additional set of unseen data was used: 2000 held-out sentences from the UN
data for the left half and the NIST 2003 test set for the right half. Boldface marks scores that are significantly higher
than the first row, in-domain baseline.
</tableCaption>
<subsectionHeader confidence="0.996667">
6.1 Chinese → English
</subsectionHeader>
<bodyText confidence="0.999786904761905">
For our Chinese → English experiments, two kinds
of data were used: UN proceedings, and newswire
as used in NIST evaluations.
UN Data UN data results are reported in Ta-
ble 2. Significant improvements are obtained on all
three evaluation measures—e.g., more than 3 BLEU
points—using lexical or lexical and shallow fea-
tures. While improvements are smaller for other fea-
tures and feature combinations, performance is not
harmed by conditioning on context features. Note
that using syntactic features gave 1 BLEU point of
improvement.
News Data In News data experiments, none of our
features obtained BLEU performance statistically
distinguishable from the baseline of 0.2700 BLEU
(neither better, nor worse). The News training cor-
pus is less than half the size of the UN training cor-
pus (in words); unsurprisingly, the context features
were too sparse to be helpful. Further, newswire are
less formulaic and repetitive than UN proceedings,
so contexts do not generalize as well from training
to test data. Fortunately, our “error-minimizing mix-
ture” approach protects the BLEU score, which the
A,,, are tuned to optimize.
Combined UN + News Data Our next experi-
ment used all of the available training data (&gt; 200M
words on each side) to train the models, in-domain
A,,, tuning, and testing for each domain separately;
see Table 3. Without context features, training
on mixed-domain data consistently harms perfor-
mance. With contexts that include lexical features,
the mixed-domain model significantly outperforms
the in-domain baseline for UN data. These results
suggest that context features enable better use of out-
of-domain data, an important advantage for statis-
tical MT since parallel data often arise from very
different sources than those of “real-world” transla-
tion scenarios. On News data, context features did
not give a significant advantage on the BLEU score,
though syntactic and ≤ 1 POS contexts did give sig-
nificant NIST and METEOR improvements over the
in-domain baseline.
</bodyText>
<subsectionHeader confidence="0.999587">
6.2 German → English
</subsectionHeader>
<bodyText confidence="0.999985166666667">
We do not report full results for this task, because
the context features neither helped nor hurt perfor-
mance significantly. We believe this is due to data
sparseness resulting from the size of the training cor-
pus (26M German words), German’s relatively rich
morphology, and the challenges of German parsing.
</bodyText>
<page confidence="0.995707">
14
</page>
<table confidence="0.999686">
Context features English —* German
BLEU NIST METEOR
None 0.2069 6.020 0.2811
Lexical 0.2018 6.031 0.2772
Shallow 0.2017 5.911 0.2748
Syntactic 0.2077 6.049 0.2829
Positional 0.2045 5.930 0.2772
Lex. + Shal. + Syn. 0.2045 6.061 0.2817
All 0.2053 6.009 0.2797
Feature selection 0.2080 6.009 0.2807
</table>
<tableCaption confidence="0.772956666666667">
Table 4: English —* German experiments: training
and testing on Europarl data. WMT-07 Europarl parallel
training data was used for training, dev06 was used for
tuning, devtest06 was used for feature selection and de-
velopmental testing, and test07 was used for final testing.
Boldface marks scores significantly higher than “None.”
</tableCaption>
<subsectionHeader confidence="0.996132">
6.3 English —* German
</subsectionHeader>
<bodyText confidence="0.9999671">
English —* German results are shown in Table 4.
The baseline system here is highly competitive, hav-
ing scored higher on automatic evaluation measures
than any other system in the WMT-07 shared task
(Callison-Burch et al., 2007). Though most results
are not statistically significant, small improvements
do tend to come from syntactic context features.
Comparing with the German —* English experiment,
we attribute this effect to the high accuracy of the
English parser compared to the German parser.
</bodyText>
<subsectionHeader confidence="0.996512">
6.4 Feature Selection
</subsectionHeader>
<bodyText confidence="0.999977380952381">
Translation performance does not always increase
when features are added to the model. This mo-
tivates the use of feature selection algorithms to
choose a subset of features to optimize perfor-
mance. We experimented with several feature se-
lection algorithms based on information-theoretic
quantities computed among the source phrase, the
target phrase, and the context, but found that a sim-
ple forward variable selection algorithm (Guyon and
Elisseeff, 2003) worked best. In this procedure, we
start with no context features and, at each iteration,
add the single feature that results in the largest in-
crease in BLEU score on an unseen development
set after am tuning. The algorithm terminates if no
features are left or if none result in an increase in
BLEU. We ran this algorithm to completion for the
two Chinese —* English tune/test sets (training on all
data in each case) and the English —* German task;
see Tables 3 and 4. In all cases, the algorithm fin-
ishes after G 4 iterations.
Feature selection for Chinese —* English (UN)
first chose the lexical feature “1 word on each side,”
then the positional feature indicating which fifth of
the sentence contains the phrase, and finally the lex-
ical feature “1 word on right.” For News, the fea-
tures chosen were the shallow syntactic feature “1
POS on each side,” then the positional beginning-
of-sentence feature, then the position relative to the
root (a syntactic feature). For English —* German,
the shallow syntactic feature “2 POS on left,” then
the lexical feature “1 word on right” were selected.
In the case where context features were most
helpful (Chinese —* English UN data), we found
feature selection to be competitive at 2.28 BLEU
points above the no-context baseline, but not the best
achieved. In the other two cases (Chinese —* English
News and English —* German Europarl), our best
results were achieved using these automatically se-
lected features, and in the Chinese —* English News
case, improvements on all three scores (including
1.37 BLEU points) are significant compared to the
no-context baseline trained on the same data.
</bodyText>
<subsectionHeader confidence="0.968241">
6.5 WMT-08 Shared Task: English —* German
</subsectionHeader>
<bodyText confidence="0.999996076923077">
Since we began this research before the release
of the data for the WMT-08 shared task, we per-
formed the majority of our experiments using the
data released for the WMT-07 shared task (see Ap-
pendix B). To prepare our entry for the 2008 shared
task, we trained a baseline system on the 2008 data
using a nearly identical configuration.7 Table 5 com-
pares performance of the baseline system (with no
context features) to performance with the two con-
text features chosen automatically as described in
§6.4. In addition to the devtest06 data, we report re-
sults on the 2007 and 2008 Europarl test sets. Most
improvements were statistically significant.
</bodyText>
<sectionHeader confidence="0.999782" genericHeader="method">
7 Future Work
</sectionHeader>
<bodyText confidence="0.9980295">
In future work, we plan to apply more sophisticated
learning algorithms to rich-feature phrase table esti-
mation. Context features can also be used as condi-
tioning variables in other components of translation
</bodyText>
<footnote confidence="0.874492333333333">
7The only differences were the use of a larger max sentence
length threshold of 55 tokens instead of 50, and the use of the
better-performing “englishFactored” Stanford parser model.
</footnote>
<page confidence="0.973697">
15
</page>
<table confidence="0.99499275">
devtest06 test07 test08
System BLEU NIST METEOR BLEU NIST METEOR BLEU NIST METEOR
Baseline 0.2009 5.866 0.2719 0.2051 5.957 0.2782 0.2003 5.889 0.2720
Context 0.2039 5.941 0.2784 0.2088 6.036 0.2826 0.2016 5.956 0.2772
</table>
<tableCaption confidence="0.985042">
Table 5: English → German shared task system results using WMT-08 Europarl parallel data for training, dev06 for
tuning, and three test sets, including the final 2008 test set. The row labeled “Context” uses the top-performing feature
set 12 POS on left, 1 word on right}. Boldface marks scores that are significantly higher than the baseline.
</tableCaption>
<bodyText confidence="0.999862428571429">
models, including the lexicalized reordering model
and the lexical translation model in the Moses MT
system, or hierarchical or syntactic models (Chiang,
2005). Additional linguistic analysis (e.g., morpho-
logical disambiguation, named entity recognition,
semantic role labeling) can be used to define new
context features.
</bodyText>
<sectionHeader confidence="0.999262" genericHeader="method">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999945307692308">
We have described a straightforward, scalable
method for improving phrase translation models by
modeling features of a phrase’s source-side context.
Our method allows incorporation of features from
any kind of source-side annotation and barely affects
the decoding algorithm. Experiments show perfor-
mance rivaling or exceeding strong, state-of-the-art
baselines on standard translation tasks. Automatic
feature selection can be used to achieve performance
gains with just two or three context features. Per-
formance is strongest when large in-domain training
sets and high-accuracy NLP tools for the source lan-
guage are available.
</bodyText>
<sectionHeader confidence="0.998631" genericHeader="method">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999835375">
This research was supported in part by an ARCS
award to the first author, NSF IIS-0713265, su-
percomputing resources provided by Yahoo, and a
Google grant. We thank Abhaya Agarwal, Ashish
Venugopal, and Andreas Zollmann for helpful con-
versations and Joy Zhang for his Chinese segmenter.
We also thank the anonymous reviewers for helpful
comments.
</bodyText>
<sectionHeader confidence="0.946881" genericHeader="method">
A Dataset Details (Chinese-English)
</sectionHeader>
<bodyText confidence="0.99996132">
We trained on data from the NIST MT 2008 con-
strained Chinese-English track: Hong Kong
Hansards and news (LDC2004T08), Sino-
rama (LDC2005T10), FBIS (LDC2003E14),
Xinhua (LDC2002E18), and financial news
(LDC2006E26)—total 2.5M sents., 66M Chinese
words, 68M English. For news experiments, the
newswire portion of the NIST 2004 test set was used
for tuning, the full NIST 2003 test set was used for
developmental testing and feature selection, and the
NIST 2005 test set was used for testing (900-1000
sents. each). We also used the United Nations paral-
lel text (LDC2004E12), divided into training (4.7M
sents.; words: 136M Chinese, 144M English),
tuning (2K sents.), and test sets (2K sents.). We
removed sentence pairs where either side was longer
than 80 words, segmented all Chinese text automat-
ically,8 and parsed/tagged using the Stanford parser
with the pre-trained “xinhuaPCFG” model (Klein
and Manning, 2003). Trigram language models
were trained on the English side of the parallel
corpus along with approximately 115M words from
the Xinhua section of the English Gigaword corpus
(LDC2005T12), years 1995–2000 (total 326M
words).
</bodyText>
<sectionHeader confidence="0.567711" genericHeader="method">
B Dataset Details (German-English)
</sectionHeader>
<bodyText confidence="0.999871461538462">
For German H English experiments, we used data
provided for the WMT-07 shared task (1.1M sents.,
26M German words, 27M English). We used dev06
for tuning, devtest06 for feature selection and de-
velopmental testing, and test07 for final testing
(2K sents. each). We removed sentence pairs
where either side was longer than 50 words and
parsed/tagged the German and English data using
the Stanford parser (Klein and Manning, 2003) (with
pre-trained “germanFactored” and “englishPCFG”
models). 5-gram language models were trained on
the entire target side of the parallel corpus (37M
German words, 38M English).
</bodyText>
<footnote confidence="0.974053">
8Available at http://projectile.is.cs.cmu.
edu/research/public/tools/segmentation/
lrsegmenter/lrSegmenter.perl.
</footnote>
<page confidence="0.998864">
16
</page>
<sectionHeader confidence="0.998344" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999939172413793">
S. Banerjee and A. Lavie. 2005. METEOR: An auto-
matic metric for MT evaluation with improved corre-
lation with human judgments. In Proc. of ACL Work-
shop on Intrinsic and Extrinsic Evaluation Measures
for MT and/or Summarization.
D. M. Bikel. 2004. A distributional analysis of a lexical-
ized statistical parsing model. In Proc. of EMNLP.
T. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean.
2007. Large language models in machine translation.
In Proc. of EMNLP-CoNLL.
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della
Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and
P. S. Roossin. 1990. A statistical approach to machine
translation. Computational Linguistics, 16(2):79–85.
C. Callison-Burch, P. Koehn, C. Fordyce, and C. Monz,
editors. 2007. Proc. of the 2nd Workshop on Statisti-
cal Machine Translation.
M. Carl and A. Way. 2003. RecentAdvances in Example-
Based Machine Translation. Kluwer Academic.
M. Carpuat and D. Wu. 2007. Improving statistical ma-
chine translation using word sense disambiguation. In
Proc. of EMNLP-CoNLL.
Y. Chan, H. Ng, and D. Chiang. 2007. Word sense dis-
ambiguation improves statistical machine translation.
In Proc. of ACL.
S. Chen and J. Goodman. 1998. An empirical study of
smoothing techniques for language modeling. Techni-
cal report 10-98, Harvard University.
D. Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proc. ofACL.
G. Doddington. 2002. Automatic evaluation of machine
translation quality using n-gram co-occurrence statis-
tics. In Proc. of HLT.
D. Gildea. 2001. Corpus variation and parser perfor-
mance. In Proc. of EMNLP.
J. Gim´enez and L. M`arquez. 2007. Context-aware
discriminative phrase selection for statistical machine
translation. In Proc. of the 2nd Workshop on Statistical
Machine Translation.
I. Guyon and A. Elisseeff. 2003. An introduction to vari-
able and feature selection. Journal of Machine Learn-
ing Research, 3:1157–1182.
D. Klein and C. D. Manning. 2003. Fast exact inference
with a factored model for natural language parsing. In
Advances in NIPS 15.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statisti-
cal phrase-based translation. In Proc. of HLT-NAACL,
pages 127–133.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In ACL demonstration
session.
P. Koehn. 2004. Statistical significance tests for machine
translation evaluation. In Proc. of EMNLP.
Jos´e B. Marino, Rafael E. Banchs, Josep M. Crego, Adri`a
de Gispert, Patrik Lambert, Jos´e A. R. Fonollosa, and
Marta R. Costa-juss`a. 2006. N-gram-based machine
translation. Computational Linguistics, 32(4):527–
549.
M. Nagao. 1984. A framework of a mechanical trans-
lation between Japanese and English by analogy prin-
ciple. In Proc. of the International NATO Symposium
on Artificial and Human Intelligence. Elsevier North-
Holland, Inc.
NIST. 2006. NIST 2006 machine translation evaluation
official results.
F. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1).
F. J. Och. 2003. Minimum error rate training for sta-
tistical machine translation. In Proc. of ACL, pages
160–167.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL.
H. Somers. 1999. Review article: Example-based ma-
chine translation. Machine Translation, 14(2).
A. Stolcke. 2002. SRILM—an extensible language mod-
eling toolkit. In Proc. of ICSLP.
N. Stroppa, A. van den Bosch, and A. Way. 2007.
Exploiting source similarity for SMT using context-
informed features. In Proc. of TMI.
D. Vickrey, L. Biewald, M. Teyssier, and D. Koller. 2005.
Word-sense disambiguation for machine translation.
In Proc. of HLT-EMNLP.
</reference>
<page confidence="0.999409">
17
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.366561">
<title confidence="0.968386">Rich Source-Side Context for Statistical Machine Translation</title>
<author confidence="0.562289">A Gimpel</author>
<affiliation confidence="0.820328666666667">Language Technologies School of Computer Carnegie Mellon</affiliation>
<address confidence="0.993023">Pittsburgh, PA 15213,</address>
<abstract confidence="0.993094954545455">We explore the augmentation of statistical machine translation models with features of the each phrase to be translated. This work extends several existing threads of research in statistical MT, including the use of context in example-based machine translation (Carl and Way, 2003) and the incorporation of word sense disambiguation into a translation model (Chan et al., 2007). The context features we consider use surrounding words and part-of-speech tags, local syntactic structure, and other properties of the source language sentence to help predict each phrase’s translation. Our approach requires very little computation beyond the standard phrase extraction algorithm and scales well to large data scenarios. We report significant improvements in automatic evaluation scores for Chineseto-English and English-to-German translation, and also describe our entry in the WMT-08 shared task based on this approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Banerjee</author>
<author>A Lavie</author>
</authors>
<title>METEOR: An automatic metric for MT evaluation with improved correlation with human judgments.</title>
<date>2005</date>
<booktitle>In Proc. of ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization.</booktitle>
<contexts>
<context position="18987" citStr="Banerjee and Lavie, 2005" startWordPosition="3102" endWordPosition="3105">dering model, a lexicalized reordering model, and an n-gram language model, SRILM implementation (Stolcke, 2002) with modified KneserNey smoothing (Chen and Goodman, 1998). Minimum error-rate (MER) training (Och, 2003) was applied to obtain weights (am in Equation 2) for these features. A recaser is trained on the target side of the parallel corpus using the script provided with Moses. All output is recased and detokenized prior to evaluation. Evaluation We evaluate translation output using three automatic evaluation measures: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005, version 0.6).5 All measures used were the case-sensitive, corpuslevel versions. The version of BLEU used was that provided by NIST. Significance was tested using a paired bootstrap (Koehn, 2004) with 1000 samples (p &lt; 0.05).6 4http://www.statmt.org/wmt08 5METEOR details: For English, we use exact matching, Porter stemming, and WordNet synonym matching. For German, we use exact matching and Porter stemming. These are the same settings that were used to evaluate systems for the WMT07 shared task. 6Code implementing this test for these metrics can be freely downloaded at http://www.ark.cs.cmu.e</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>S. Banerjee and A. Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proc. of ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Bikel</author>
</authors>
<title>A distributional analysis of a lexicalized statistical parsing model.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="5621" citStr="Bikel, 2004" startWordPosition="880" endWordPosition="881">tional challenge (§4). Specific modifications to the standard training and evaluation paradigm are presented in §5. Experimental results are reported in §6. 2 Related Work Stroppa et al. (2007) added souce-side context features to a phrase-based translation system, including conditional probabilities of the same form that we use. They consider up to two words and/or POS tags of context on either side. Because of the aforementioned data sparseness problem, they use a decision3An illustrative example is the debate over the use of bilexicalized grammar rules in statistical parsing (Gildea, 2001; Bikel, 2004). tree classifier that implicitly smooths relative frequency estimates. The method improved over a standard phrase-based baseline trained on small amounts of data (&lt; 50K sentence pairs) for Italian —* English and Chinese —* English. We explore a significantly larger space of context features, a smoothing method that more naturally fits into the widely used, errordriven linear model, and report a more comprehensive experimental evaluation (including feature comparison and scaling up to very large datasets). Recent research on the use of word-sense disambiguation in machine translation also poin</context>
</contexts>
<marker>Bikel, 2004</marker>
<rawString>D. M. Bikel. 2004. A distributional analysis of a lexicalized statistical parsing model. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Brants</author>
<author>A C Popat</author>
<author>P Xu</author>
<author>F J Och</author>
<author>J Dean</author>
</authors>
<title>Large language models in machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="1583" citStr="Brants et al., 2007" startWordPosition="230" endWordPosition="233">arios. We report significant improvements in automatic evaluation scores for Chineseto-English and English-to-German translation, and also describe our entry in the WMT-08 shared task based on this approach. 1 Introduction Machine translation (MT) by statistical modeling of bilingual phrases is one of the most successful approaches in the past few years. Phrase-based MT systems are straightforward to train from parallel corpora (Koehn et al., 2003) and, like the original IBM models (Brown et al., 1990), benefit from standard language models built on large monolingual, target-language corpora (Brants et al., 2007). Many of these systems perform well in competitive evaluations and scale well to large-data situations (NIST, 2006; Callison-Burch et al., 2007). End-toend phrase-based MT systems can be built entirely from freely-available tools (Koehn et al., 2007). We follow the approach of Koehn et al. (2003), in which we translate a source-language sentence f into the target-language sentence e� that maximizes a linear combination of features and weights:1 h�e, �ai �argmax score(e, a, f) (1) (e,a) M � argmax λmhm(e, a, f) (2) (e,a) m=1 where a represents the segmentation of e and f into phrases and a cor</context>
<context position="15219" citStr="Brants et al., 2007" startWordPosition="2495" endWordPosition="2498">is straightforward to exploit in a model, including target-side context features breaks one of the key independence assumptions made by phrase-based translation models: the translations of the source-side phrases are conditionally independent of each other, given f, thereby requiring new algorithms for decoding (Marino et al., 2006). We suggest that target-side context may already be well accounted for in current MT systems. Indeed, language models pay attention to the local context of phrases, as do reordering models. The recent emphasis on improving these components of a translation system (Brants et al., 2007) is likely due in part to the widespread availability of NLP tools for the language that is most frequently the target: English. We will demonstrate that NLP tools (tag12 9 no context Shallow: 2 POS on left Syntax: of root Positional: rel. pos. *“PRP VBP” “VBN IN” *right left *2nd fifth 1st fifth den bericht des ausschusses 0.3125 1.0000 0.3333 0.5000 0.0000 0.6000 0.0000 der bericht des ausschusses 0.3125 0.0000 0.0000 0.1000 0.6667 0.2000 0.6667 dem bericht des ausschusses 0.2500 0.0000 0.6667 0.3000 0.1667 0.0000 0.1667 Table 1: Phrase table entries for “the report of the committee” and the</context>
</contexts>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>T. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean. 2007. Large language models in machine translation. In Proc. of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>J Cocke</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>F Jelinek</author>
<author>J D Lafferty</author>
<author>R L Mercer</author>
<author>P S Roossin</author>
</authors>
<title>A statistical approach to machine translation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="1470" citStr="Brown et al., 1990" startWordPosition="214" endWordPosition="217">uires very little computation beyond the standard phrase extraction algorithm and scales well to large data scenarios. We report significant improvements in automatic evaluation scores for Chineseto-English and English-to-German translation, and also describe our entry in the WMT-08 shared task based on this approach. 1 Introduction Machine translation (MT) by statistical modeling of bilingual phrases is one of the most successful approaches in the past few years. Phrase-based MT systems are straightforward to train from parallel corpora (Koehn et al., 2003) and, like the original IBM models (Brown et al., 1990), benefit from standard language models built on large monolingual, target-language corpora (Brants et al., 2007). Many of these systems perform well in competitive evaluations and scale well to large-data situations (NIST, 2006; Callison-Burch et al., 2007). End-toend phrase-based MT systems can be built entirely from freely-available tools (Koehn et al., 2007). We follow the approach of Koehn et al. (2003), in which we translate a source-language sentence f into the target-language sentence e� that maximizes a linear combination of features and weights:1 h�e, �ai �argmax score(e, a, f) (1) (</context>
</contexts>
<marker>Brown, Cocke, Pietra, Pietra, Jelinek, Lafferty, Mercer, Roossin, 1990</marker>
<rawString>P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and P. S. Roossin. 1990. A statistical approach to machine translation. Computational Linguistics, 16(2):79–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>P Koehn</author>
<author>C Fordyce</author>
</authors>
<date>2007</date>
<booktitle>Proc. of the 2nd Workshop on Statistical Machine Translation.</booktitle>
<editor>and C. Monz, editors.</editor>
<contexts>
<context position="1728" citStr="Callison-Burch et al., 2007" startWordPosition="251" endWordPosition="254">o describe our entry in the WMT-08 shared task based on this approach. 1 Introduction Machine translation (MT) by statistical modeling of bilingual phrases is one of the most successful approaches in the past few years. Phrase-based MT systems are straightforward to train from parallel corpora (Koehn et al., 2003) and, like the original IBM models (Brown et al., 1990), benefit from standard language models built on large monolingual, target-language corpora (Brants et al., 2007). Many of these systems perform well in competitive evaluations and scale well to large-data situations (NIST, 2006; Callison-Burch et al., 2007). End-toend phrase-based MT systems can be built entirely from freely-available tools (Koehn et al., 2007). We follow the approach of Koehn et al. (2003), in which we translate a source-language sentence f into the target-language sentence e� that maximizes a linear combination of features and weights:1 h�e, �ai �argmax score(e, a, f) (1) (e,a) M � argmax λmhm(e, a, f) (2) (e,a) m=1 where a represents the segmentation of e and f into phrases and a correspondence between phrases, and each hm is a R-valued feature with learned weight λm. The translation is typically found using beam search (Koeh</context>
<context position="24194" citStr="Callison-Burch et al., 2007" startWordPosition="3930" endWordPosition="3933">09 0.2797 Feature selection 0.2080 6.009 0.2807 Table 4: English —* German experiments: training and testing on Europarl data. WMT-07 Europarl parallel training data was used for training, dev06 was used for tuning, devtest06 was used for feature selection and developmental testing, and test07 was used for final testing. Boldface marks scores significantly higher than “None.” 6.3 English —* German English —* German results are shown in Table 4. The baseline system here is highly competitive, having scored higher on automatic evaluation measures than any other system in the WMT-07 shared task (Callison-Burch et al., 2007). Though most results are not statistically significant, small improvements do tend to come from syntactic context features. Comparing with the German —* English experiment, we attribute this effect to the high accuracy of the English parser compared to the German parser. 6.4 Feature Selection Translation performance does not always increase when features are added to the model. This motivates the use of feature selection algorithms to choose a subset of features to optimize performance. We experimented with several feature selection algorithms based on information-theoretic quantities compute</context>
</contexts>
<marker>Callison-Burch, Koehn, Fordyce, 2007</marker>
<rawString>C. Callison-Burch, P. Koehn, C. Fordyce, and C. Monz, editors. 2007. Proc. of the 2nd Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Carl</author>
<author>A Way</author>
</authors>
<title>RecentAdvances in ExampleBased Machine Translation.</title>
<date>2003</date>
<publisher>Kluwer Academic.</publisher>
<contexts>
<context position="3697" citStr="Carl and Way, 2003" startWordPosition="573" endWordPosition="576">ce many of the features are actually probabilities, this linear combination is closer to a mixture model. 2We will use xi to denote the subsequence of x containing the ith through jth elements of x, inclusive. 9 Proceedings of the Third Workshop on Statistical Machine Translation, pages 9–17, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics tures that condition on additional context features on the source (f) side: p(eji Phrase = f`k, Context = �fk−1 1 , fF`�1, )) The advantage of considering context is wellknown and exploited in the example-based MT community (Carl and Way, 2003). Recently researchers have begun to use source phrase context information in statistical MT systems (Stroppa et al., 2007). Statistical NLP researchers understand that conditioning a probability model on more information is helpful only if there are sufficient training data to accurately estimate the context probabilities.3 Sparse data are often the death of elaborate models, though this can be remedied through careful smoothing. In this paper we leverage the existing linear model (Equation 2) to bring source-side context into phrase-based MT in a way that is robust to data sparseness. We int</context>
<context position="7021" citStr="Carl and Way, 2003" startWordPosition="1090" endWordPosition="1093">on. Gim´enez and M`arquez (2007) extended the work by considering phrases and moved to full translation instead of filling in target-side blanks. They trained an SVM for each source language phrase using local features of the sentences in which the phrases appear. Carpuat and Wu (2007) and Chan et al. (2007) embedded state-of-the-art word sense disambiguation modules into statistical MT systems, achieving performance improvements under several automatic measures for Chinese —* English translation. Our approach is also reminiscent of examplebased machine translation (Nagao, 1984; Somers, 1999; Carl and Way, 2003), which has for many years emphasized use of the context in which source phrases appear when translating them. Indeed, like the example-based community, we do not begin with any set of assumptions about which kinds of phrases require additional disambiguation (cf. the application of word-sense disambiguation, which is motivated by lexical ambiguity). Our feature-rich approach is omnivorous and can exploit any linguistic analysis of an input sentence. 3 Source-Side Context Features Adding features to the linear model (Equation 2) that consider more of the source sentence requires changing the d</context>
</contexts>
<marker>Carl, Way, 2003</marker>
<rawString>M. Carl and A. Way. 2003. RecentAdvances in ExampleBased Machine Translation. Kluwer Academic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Carpuat</author>
<author>D Wu</author>
</authors>
<title>Improving statistical machine translation using word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proc. of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="6688" citStr="Carpuat and Wu (2007)" startWordPosition="1045" endWordPosition="1048">luding feature comparison and scaling up to very large datasets). Recent research on the use of word-sense disambiguation in machine translation also points toward our approach. For example, Vickrey et al. (2005) built classifiers inspired by those used in word sense disambiguation to fill in blanks in a partially-completed translation. Gim´enez and M`arquez (2007) extended the work by considering phrases and moved to full translation instead of filling in target-side blanks. They trained an SVM for each source language phrase using local features of the sentences in which the phrases appear. Carpuat and Wu (2007) and Chan et al. (2007) embedded state-of-the-art word sense disambiguation modules into statistical MT systems, achieving performance improvements under several automatic measures for Chinese —* English translation. Our approach is also reminiscent of examplebased machine translation (Nagao, 1984; Somers, 1999; Carl and Way, 2003), which has for many years emphasized use of the context in which source phrases appear when translating them. Indeed, like the example-based community, we do not begin with any set of assumptions about which kinds of phrases require additional disambiguation (cf. th</context>
</contexts>
<marker>Carpuat, Wu, 2007</marker>
<rawString>M. Carpuat and D. Wu. 2007. Improving statistical machine translation using word sense disambiguation. In Proc. of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Chan</author>
<author>H Ng</author>
<author>D Chiang</author>
</authors>
<title>Word sense disambiguation improves statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="630" citStr="Chan et al., 2007" startWordPosition="86" endWordPosition="89">Side Context for Statistical Machine Translation Kevin Gimpel and Noah A. Smith Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA {kgimpel,nasmith}@cs.cmu.edu Abstract We explore the augmentation of statistical machine translation models with features of the context of each phrase to be translated. This work extends several existing threads of research in statistical MT, including the use of context in example-based machine translation (Carl and Way, 2003) and the incorporation of word sense disambiguation into a translation model (Chan et al., 2007). The context features we consider use surrounding words and part-of-speech tags, local syntactic structure, and other properties of the source language sentence to help predict each phrase’s translation. Our approach requires very little computation beyond the standard phrase extraction algorithm and scales well to large data scenarios. We report significant improvements in automatic evaluation scores for Chineseto-English and English-to-German translation, and also describe our entry in the WMT-08 shared task based on this approach. 1 Introduction Machine translation (MT) by statistical mode</context>
<context position="6711" citStr="Chan et al. (2007)" startWordPosition="1050" endWordPosition="1053">and scaling up to very large datasets). Recent research on the use of word-sense disambiguation in machine translation also points toward our approach. For example, Vickrey et al. (2005) built classifiers inspired by those used in word sense disambiguation to fill in blanks in a partially-completed translation. Gim´enez and M`arquez (2007) extended the work by considering phrases and moved to full translation instead of filling in target-side blanks. They trained an SVM for each source language phrase using local features of the sentences in which the phrases appear. Carpuat and Wu (2007) and Chan et al. (2007) embedded state-of-the-art word sense disambiguation modules into statistical MT systems, achieving performance improvements under several automatic measures for Chinese —* English translation. Our approach is also reminiscent of examplebased machine translation (Nagao, 1984; Somers, 1999; Carl and Way, 2003), which has for many years emphasized use of the context in which source phrases appear when translating them. Indeed, like the example-based community, we do not begin with any set of assumptions about which kinds of phrases require additional disambiguation (cf. the application of word-s</context>
</contexts>
<marker>Chan, Ng, Chiang, 2007</marker>
<rawString>Y. Chan, H. Ng, and D. Chiang. 2007. Word sense disambiguation improves statistical machine translation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Chen</author>
<author>J Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical report 10-98,</tech>
<institution>Harvard University.</institution>
<contexts>
<context position="18534" citStr="Chen and Goodman, 1998" startWordPosition="3029" endWordPosition="3032">ample training procedure given for the WMT-07 and WMT-08 shared tasks.4 In particular, we perform word alignment in each direction using GIZA++ (Och and Ney, 2003), apply the “grow-diag-finaland” heuristic for symmetrization and use a maximum phrase length of 7. In addition to the two phrase translation conditionals p(e |f) and p(f | e), we use lexical translation probabilities in each direction, a word penalty, a phrase penalty, a lengthbased reordering model, a lexicalized reordering model, and an n-gram language model, SRILM implementation (Stolcke, 2002) with modified KneserNey smoothing (Chen and Goodman, 1998). Minimum error-rate (MER) training (Och, 2003) was applied to obtain weights (am in Equation 2) for these features. A recaser is trained on the target side of the parallel corpus using the script provided with Moses. All output is recased and detokenized prior to evaluation. Evaluation We evaluate translation output using three automatic evaluation measures: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005, version 0.6).5 All measures used were the case-sensitive, corpuslevel versions. The version of BLEU used was that provided by NIST. Significance </context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>S. Chen and J. Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical report 10-98, Harvard University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="28406" citStr="Chiang, 2005" startWordPosition="4618" endWordPosition="4619">9 0.2051 5.957 0.2782 0.2003 5.889 0.2720 Context 0.2039 5.941 0.2784 0.2088 6.036 0.2826 0.2016 5.956 0.2772 Table 5: English → German shared task system results using WMT-08 Europarl parallel data for training, dev06 for tuning, and three test sets, including the final 2008 test set. The row labeled “Context” uses the top-performing feature set 12 POS on left, 1 word on right}. Boldface marks scores that are significantly higher than the baseline. models, including the lexicalized reordering model and the lexical translation model in the Moses MT system, or hierarchical or syntactic models (Chiang, 2005). Additional linguistic analysis (e.g., morphological disambiguation, named entity recognition, semantic role labeling) can be used to define new context features. 8 Conclusion We have described a straightforward, scalable method for improving phrase translation models by modeling features of a phrase’s source-side context. Our method allows incorporation of features from any kind of source-side annotation and barely affects the decoding algorithm. Experiments show performance rivaling or exceeding strong, state-of-the-art baselines on standard translation tasks. Automatic feature selection ca</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>D. Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram co-occurrence statistics.</title>
<date>2002</date>
<booktitle>In Proc. of HLT.</booktitle>
<contexts>
<context position="18949" citStr="Doddington, 2002" startWordPosition="3098" endWordPosition="3099">ase penalty, a lengthbased reordering model, a lexicalized reordering model, and an n-gram language model, SRILM implementation (Stolcke, 2002) with modified KneserNey smoothing (Chen and Goodman, 1998). Minimum error-rate (MER) training (Och, 2003) was applied to obtain weights (am in Equation 2) for these features. A recaser is trained on the target side of the parallel corpus using the script provided with Moses. All output is recased and detokenized prior to evaluation. Evaluation We evaluate translation output using three automatic evaluation measures: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005, version 0.6).5 All measures used were the case-sensitive, corpuslevel versions. The version of BLEU used was that provided by NIST. Significance was tested using a paired bootstrap (Koehn, 2004) with 1000 samples (p &lt; 0.05).6 4http://www.statmt.org/wmt08 5METEOR details: For English, we use exact matching, Porter stemming, and WordNet synonym matching. For German, we use exact matching and Porter stemming. These are the same settings that were used to evaluate systems for the WMT07 shared task. 6Code implementing this test for these metrics can be freely</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>G. Doddington. 2002. Automatic evaluation of machine translation quality using n-gram co-occurrence statistics. In Proc. of HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
</authors>
<title>Corpus variation and parser performance.</title>
<date>2001</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="5607" citStr="Gildea, 2001" startWordPosition="878" endWordPosition="879">pose a computational challenge (§4). Specific modifications to the standard training and evaluation paradigm are presented in §5. Experimental results are reported in §6. 2 Related Work Stroppa et al. (2007) added souce-side context features to a phrase-based translation system, including conditional probabilities of the same form that we use. They consider up to two words and/or POS tags of context on either side. Because of the aforementioned data sparseness problem, they use a decision3An illustrative example is the debate over the use of bilexicalized grammar rules in statistical parsing (Gildea, 2001; Bikel, 2004). tree classifier that implicitly smooths relative frequency estimates. The method improved over a standard phrase-based baseline trained on small amounts of data (&lt; 50K sentence pairs) for Italian —* English and Chinese —* English. We explore a significantly larger space of context features, a smoothing method that more naturally fits into the widely used, errordriven linear model, and report a more comprehensive experimental evaluation (including feature comparison and scaling up to very large datasets). Recent research on the use of word-sense disambiguation in machine transla</context>
</contexts>
<marker>Gildea, 2001</marker>
<rawString>D. Gildea. 2001. Corpus variation and parser performance. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gim´enez</author>
<author>L M`arquez</author>
</authors>
<title>Context-aware discriminative phrase selection for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of the 2nd Workshop on Statistical Machine Translation.</booktitle>
<marker>Gim´enez, M`arquez, 2007</marker>
<rawString>J. Gim´enez and L. M`arquez. 2007. Context-aware discriminative phrase selection for statistical machine translation. In Proc. of the 2nd Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Guyon</author>
<author>A Elisseeff</author>
</authors>
<title>An introduction to variable and feature selection.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1157</pages>
<contexts>
<context position="24945" citStr="Guyon and Elisseeff, 2003" startWordPosition="4043" endWordPosition="4046">Comparing with the German —* English experiment, we attribute this effect to the high accuracy of the English parser compared to the German parser. 6.4 Feature Selection Translation performance does not always increase when features are added to the model. This motivates the use of feature selection algorithms to choose a subset of features to optimize performance. We experimented with several feature selection algorithms based on information-theoretic quantities computed among the source phrase, the target phrase, and the context, but found that a simple forward variable selection algorithm (Guyon and Elisseeff, 2003) worked best. In this procedure, we start with no context features and, at each iteration, add the single feature that results in the largest increase in BLEU score on an unseen development set after am tuning. The algorithm terminates if no features are left or if none result in an increase in BLEU. We ran this algorithm to completion for the two Chinese —* English tune/test sets (training on all data in each case) and the English —* German task; see Tables 3 and 4. In all cases, the algorithm finishes after G 4 iterations. Feature selection for Chinese —* English (UN) first chose the lexical</context>
</contexts>
<marker>Guyon, Elisseeff, 2003</marker>
<rawString>I. Guyon and A. Elisseeff. 2003. An introduction to variable and feature selection. Journal of Machine Learning Research, 3:1157–1182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Fast exact inference with a factored model for natural language parsing.</title>
<date>2003</date>
<booktitle>In Advances in NIPS 15.</booktitle>
<contexts>
<context position="30524" citStr="Klein and Manning, 2003" startWordPosition="4929" endWordPosition="4932"> newswire portion of the NIST 2004 test set was used for tuning, the full NIST 2003 test set was used for developmental testing and feature selection, and the NIST 2005 test set was used for testing (900-1000 sents. each). We also used the United Nations parallel text (LDC2004E12), divided into training (4.7M sents.; words: 136M Chinese, 144M English), tuning (2K sents.), and test sets (2K sents.). We removed sentence pairs where either side was longer than 80 words, segmented all Chinese text automatically,8 and parsed/tagged using the Stanford parser with the pre-trained “xinhuaPCFG” model (Klein and Manning, 2003). Trigram language models were trained on the English side of the parallel corpus along with approximately 115M words from the Xinhua section of the English Gigaword corpus (LDC2005T12), years 1995–2000 (total 326M words). B Dataset Details (German-English) For German H English experiments, we used data provided for the WMT-07 shared task (1.1M sents., 26M German words, 27M English). We used dev06 for tuning, devtest06 for feature selection and developmental testing, and test07 for final testing (2K sents. each). We removed sentence pairs where either side was longer than 50 words and parsed/t</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C. D. Manning. 2003. Fast exact inference with a factored model for natural language parsing. In Advances in NIPS 15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. of HLT-NAACL,</booktitle>
<pages>127--133</pages>
<contexts>
<context position="1415" citStr="Koehn et al., 2003" startWordPosition="203" endWordPosition="206">elp predict each phrase’s translation. Our approach requires very little computation beyond the standard phrase extraction algorithm and scales well to large data scenarios. We report significant improvements in automatic evaluation scores for Chineseto-English and English-to-German translation, and also describe our entry in the WMT-08 shared task based on this approach. 1 Introduction Machine translation (MT) by statistical modeling of bilingual phrases is one of the most successful approaches in the past few years. Phrase-based MT systems are straightforward to train from parallel corpora (Koehn et al., 2003) and, like the original IBM models (Brown et al., 1990), benefit from standard language models built on large monolingual, target-language corpora (Brants et al., 2007). Many of these systems perform well in competitive evaluations and scale well to large-data situations (NIST, 2006; Callison-Burch et al., 2007). End-toend phrase-based MT systems can be built entirely from freely-available tools (Koehn et al., 2007). We follow the approach of Koehn et al. (2003), in which we translate a source-language sentence f into the target-language sentence e� that maximizes a linear combination of featu</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-based translation. In Proc. of HLT-NAACL, pages 127–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In ACL demonstration session.</booktitle>
<contexts>
<context position="1834" citStr="Koehn et al., 2007" startWordPosition="267" endWordPosition="270">tatistical modeling of bilingual phrases is one of the most successful approaches in the past few years. Phrase-based MT systems are straightforward to train from parallel corpora (Koehn et al., 2003) and, like the original IBM models (Brown et al., 1990), benefit from standard language models built on large monolingual, target-language corpora (Brants et al., 2007). Many of these systems perform well in competitive evaluations and scale well to large-data situations (NIST, 2006; Callison-Burch et al., 2007). End-toend phrase-based MT systems can be built entirely from freely-available tools (Koehn et al., 2007). We follow the approach of Koehn et al. (2003), in which we translate a source-language sentence f into the target-language sentence e� that maximizes a linear combination of features and weights:1 h�e, �ai �argmax score(e, a, f) (1) (e,a) M � argmax λmhm(e, a, f) (2) (e,a) m=1 where a represents the segmentation of e and f into phrases and a correspondence between phrases, and each hm is a R-valued feature with learned weight λm. The translation is typically found using beam search (Koehn et al., 2003). The weights hλ1,..., λMi are typically learned to directly minimize a standard evaluation</context>
<context position="17871" citStr="Koehn et al., 2007" startWordPosition="2923" endWordPosition="2926">el with a variety of different context features, on Chinese → English, German → English, and English → GerContext features Chinese → English (UN) BLEU NIST METEOR None 0.3715 7.918 0.6486 Lexical 0.4030 8.367 0.6716 Shallow 0.3807 7.981 0.6523 Lexical + Shallow 0.4030 8.403 0.6703 Syntactic 0.3823 7.992 0.6531 Positional 0.3775 7.958 0.6510 Table 2: Chinese → English experiments: training and testing on UN data. Boldface marks scores significantly higher than “None.” man translation tasks. Dataset details are given in Appendices A (Chinese) and B (German). Baseline We use the Moses MT system (Koehn et al., 2007) as a baseline and closely follow the example training procedure given for the WMT-07 and WMT-08 shared tasks.4 In particular, we perform word alignment in each direction using GIZA++ (Och and Ney, 2003), apply the “grow-diag-finaland” heuristic for symmetrization and use a maximum phrase length of 7. In addition to the two phrase translation conditionals p(e |f) and p(f | e), we use lexical translation probabilities in each direction, a word penalty, a phrase penalty, a lengthbased reordering model, a lexicalized reordering model, and an n-gram language model, SRILM implementation (Stolcke, 2</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In ACL demonstration session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="19183" citStr="Koehn, 2004" startWordPosition="3134" endWordPosition="3135">Och, 2003) was applied to obtain weights (am in Equation 2) for these features. A recaser is trained on the target side of the parallel corpus using the script provided with Moses. All output is recased and detokenized prior to evaluation. Evaluation We evaluate translation output using three automatic evaluation measures: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005, version 0.6).5 All measures used were the case-sensitive, corpuslevel versions. The version of BLEU used was that provided by NIST. Significance was tested using a paired bootstrap (Koehn, 2004) with 1000 samples (p &lt; 0.05).6 4http://www.statmt.org/wmt08 5METEOR details: For English, we use exact matching, Porter stemming, and WordNet synonym matching. For German, we use exact matching and Porter stemming. These are the same settings that were used to evaluate systems for the WMT07 shared task. 6Code implementing this test for these metrics can be freely downloaded at http://www.ark.cs.cmu.edu/MT. 13 Context features BLEU Chinese → English Testing on UN Testing on News (NIST 2005) NIST METEOR BLEU NIST METEOR Training on in-domain data only: None 0.3715 7.918 0.6486 0.2700 7.986 0.53</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>P. Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jos´e B Marino</author>
<author>Rafael E Banchs</author>
<author>Josep M Crego</author>
<author>Adri`a de Gispert</author>
<author>Patrik Lambert</author>
<author>Jos´e A R Fonollosa</author>
<author>Marta R Costa-juss`a</author>
</authors>
<title>N-gram-based machine translation.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>4</issue>
<pages>549</pages>
<marker>Marino, Banchs, Crego, de Gispert, Lambert, Fonollosa, Costa-juss`a, 2006</marker>
<rawString>Jos´e B. Marino, Rafael E. Banchs, Josep M. Crego, Adri`a de Gispert, Patrik Lambert, Jos´e A. R. Fonollosa, and Marta R. Costa-juss`a. 2006. N-gram-based machine translation. Computational Linguistics, 32(4):527– 549.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Nagao</author>
</authors>
<title>A framework of a mechanical translation between Japanese and English by analogy principle.</title>
<date>1984</date>
<booktitle>In Proc. of the International NATO Symposium on Artificial and Human Intelligence.</booktitle>
<publisher>Elsevier NorthHolland, Inc.</publisher>
<contexts>
<context position="6986" citStr="Nagao, 1984" startWordPosition="1086" endWordPosition="1087">rtially-completed translation. Gim´enez and M`arquez (2007) extended the work by considering phrases and moved to full translation instead of filling in target-side blanks. They trained an SVM for each source language phrase using local features of the sentences in which the phrases appear. Carpuat and Wu (2007) and Chan et al. (2007) embedded state-of-the-art word sense disambiguation modules into statistical MT systems, achieving performance improvements under several automatic measures for Chinese —* English translation. Our approach is also reminiscent of examplebased machine translation (Nagao, 1984; Somers, 1999; Carl and Way, 2003), which has for many years emphasized use of the context in which source phrases appear when translating them. Indeed, like the example-based community, we do not begin with any set of assumptions about which kinds of phrases require additional disambiguation (cf. the application of word-sense disambiguation, which is motivated by lexical ambiguity). Our feature-rich approach is omnivorous and can exploit any linguistic analysis of an input sentence. 3 Source-Side Context Features Adding features to the linear model (Equation 2) that consider more of the sour</context>
</contexts>
<marker>Nagao, 1984</marker>
<rawString>M. Nagao. 1984. A framework of a mechanical translation between Japanese and English by analogy principle. In Proc. of the International NATO Symposium on Artificial and Human Intelligence. Elsevier NorthHolland, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NIST</author>
</authors>
<title>NIST</title>
<date>2006</date>
<contexts>
<context position="1698" citStr="NIST, 2006" startWordPosition="249" endWordPosition="250">ion, and also describe our entry in the WMT-08 shared task based on this approach. 1 Introduction Machine translation (MT) by statistical modeling of bilingual phrases is one of the most successful approaches in the past few years. Phrase-based MT systems are straightforward to train from parallel corpora (Koehn et al., 2003) and, like the original IBM models (Brown et al., 1990), benefit from standard language models built on large monolingual, target-language corpora (Brants et al., 2007). Many of these systems perform well in competitive evaluations and scale well to large-data situations (NIST, 2006; Callison-Burch et al., 2007). End-toend phrase-based MT systems can be built entirely from freely-available tools (Koehn et al., 2007). We follow the approach of Koehn et al. (2003), in which we translate a source-language sentence f into the target-language sentence e� that maximizes a linear combination of features and weights:1 h�e, �ai �argmax score(e, a, f) (1) (e,a) M � argmax λmhm(e, a, f) (2) (e,a) m=1 where a represents the segmentation of e and f into phrases and a correspondence between phrases, and each hm is a R-valued feature with learned weight λm. The translation is typically</context>
</contexts>
<marker>NIST, 2006</marker>
<rawString>NIST. 2006. NIST 2006 machine translation evaluation official results.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="18074" citStr="Och and Ney, 2003" startWordPosition="2957" endWordPosition="2960">.367 0.6716 Shallow 0.3807 7.981 0.6523 Lexical + Shallow 0.4030 8.403 0.6703 Syntactic 0.3823 7.992 0.6531 Positional 0.3775 7.958 0.6510 Table 2: Chinese → English experiments: training and testing on UN data. Boldface marks scores significantly higher than “None.” man translation tasks. Dataset details are given in Appendices A (Chinese) and B (German). Baseline We use the Moses MT system (Koehn et al., 2007) as a baseline and closely follow the example training procedure given for the WMT-07 and WMT-08 shared tasks.4 In particular, we perform word alignment in each direction using GIZA++ (Och and Ney, 2003), apply the “grow-diag-finaland” heuristic for symmetrization and use a maximum phrase length of 7. In addition to the two phrase translation conditionals p(e |f) and p(f | e), we use lexical translation probabilities in each direction, a word penalty, a phrase penalty, a lengthbased reordering model, a lexicalized reordering model, and an n-gram language model, SRILM implementation (Stolcke, 2002) with modified KneserNey smoothing (Chen and Goodman, 1998). Minimum error-rate (MER) training (Och, 2003) was applied to obtain weights (am in Equation 2) for these features. A recaser is trained on</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum error rate training for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="2547" citStr="Och, 2003" startWordPosition="388" endWordPosition="389">o the target-language sentence e� that maximizes a linear combination of features and weights:1 h�e, �ai �argmax score(e, a, f) (1) (e,a) M � argmax λmhm(e, a, f) (2) (e,a) m=1 where a represents the segmentation of e and f into phrases and a correspondence between phrases, and each hm is a R-valued feature with learned weight λm. The translation is typically found using beam search (Koehn et al., 2003). The weights hλ1,..., λMi are typically learned to directly minimize a standard evaluation criterion on development data (e.g., the BLEU score; Papineni et al., (2002)) using numerical search (Och, 2003). Many features are used in phrase-based MT, but nearly ubiquitous are estimates of the conditional translation probabilities p(ei |fk) and p(fk |ei ) for each phrase pair hez, fki in the candidate sentence pair.2 In this paper, we add and evaluate fea1In the statistical MT literature, this is often referred to as a “log-linear model,” but since the score is normalized during neither parameter training nor decoding, and is never interpreted as a log-probability, it is essentially a linear combination of feature functions. Since many of the features are actually probabilities, this linear combi</context>
<context position="4531" citStr="Och, 2003" startWordPosition="706" endWordPosition="707"> helpful only if there are sufficient training data to accurately estimate the context probabilities.3 Sparse data are often the death of elaborate models, though this can be remedied through careful smoothing. In this paper we leverage the existing linear model (Equation 2) to bring source-side context into phrase-based MT in a way that is robust to data sparseness. We interpret the linear model as a mixture of many probability estimates based on different context features, some of which may be very sparse. The mixture coefficients are trained in the usual way (“minimum error-rate training,” Och, 2003), so that the additional context is exploited when it is useful and ignored when it isn’t. The paper proceeds as follows. We first review related work that enriches statistical translation models using context (§2). We then propose a set of source-side features to be incorporated into the translation model, including the novel use of syntactic context from source-side parse trees and global position within f (§3). We explain why analogous target-side features pose a computational challenge (§4). Specific modifications to the standard training and evaluation paradigm are presented in §5. Experi</context>
<context position="9487" citStr="Och, 2003" startWordPosition="1493" endWordPosition="1494">guage phrase, f is the source language phrase, and fcontext is the context of the source language phrase in the sentence in which it was observed. Like the context-bare conditional probabilities, we estimate probability features using relative frequencies: Since we expect that adding conditioning variables will lead to sparser counts and therefore more zero estimates, we compute features for many different types of context. To combine the many differently-conditioned features into a single model, we provide them as features to the linear model (Equation 2) and use minimum error-rate training (Och, 2003) to obtain interpolation weights Am. This is similar to an interpolation of backed-off estimates, if we imagine that all of the different contexts are differently-backed off estimates of the complete context. The error-driven weight training effectively smooths one implicit context-rich estimate p(e |f,fcontext) so that all of the backed-off estimates are taken into account, including the original p(e |f). Our approach is asymmetrical; we have not, for example, estimated features of the form p(f,fcontext |e). We next discuss the specific source-side context features used in our model. 3.1 Lexi</context>
<context position="18581" citStr="Och, 2003" startWordPosition="3038" endWordPosition="3039">red tasks.4 In particular, we perform word alignment in each direction using GIZA++ (Och and Ney, 2003), apply the “grow-diag-finaland” heuristic for symmetrization and use a maximum phrase length of 7. In addition to the two phrase translation conditionals p(e |f) and p(f | e), we use lexical translation probabilities in each direction, a word penalty, a phrase penalty, a lengthbased reordering model, a lexicalized reordering model, and an n-gram language model, SRILM implementation (Stolcke, 2002) with modified KneserNey smoothing (Chen and Goodman, 1998). Minimum error-rate (MER) training (Och, 2003) was applied to obtain weights (am in Equation 2) for these features. A recaser is trained on the target side of the parallel corpus using the script provided with Moses. All output is recased and detokenized prior to evaluation. Evaluation We evaluate translation output using three automatic evaluation measures: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005, version 0.6).5 All measures used were the case-sensitive, corpuslevel versions. The version of BLEU used was that provided by NIST. Significance was tested using a paired bootstrap (Koehn, 200</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. J. Och. 2003. Minimum error rate training for statistical machine translation. In Proc. of ACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="2511" citStr="Papineni et al., (2002)" startWordPosition="381" endWordPosition="384">ich we translate a source-language sentence f into the target-language sentence e� that maximizes a linear combination of features and weights:1 h�e, �ai �argmax score(e, a, f) (1) (e,a) M � argmax λmhm(e, a, f) (2) (e,a) m=1 where a represents the segmentation of e and f into phrases and a correspondence between phrases, and each hm is a R-valued feature with learned weight λm. The translation is typically found using beam search (Koehn et al., 2003). The weights hλ1,..., λMi are typically learned to directly minimize a standard evaluation criterion on development data (e.g., the BLEU score; Papineni et al., (2002)) using numerical search (Och, 2003). Many features are used in phrase-based MT, but nearly ubiquitous are estimates of the conditional translation probabilities p(ei |fk) and p(fk |ei ) for each phrase pair hez, fki in the candidate sentence pair.2 In this paper, we add and evaluate fea1In the statistical MT literature, this is often referred to as a “log-linear model,” but since the score is normalized during neither parameter training nor decoding, and is never interpreted as a log-probability, it is essentially a linear combination of feature functions. Since many of the features are actua</context>
<context position="18924" citStr="Papineni et al., 2002" startWordPosition="3092" endWordPosition="3096">rection, a word penalty, a phrase penalty, a lengthbased reordering model, a lexicalized reordering model, and an n-gram language model, SRILM implementation (Stolcke, 2002) with modified KneserNey smoothing (Chen and Goodman, 1998). Minimum error-rate (MER) training (Och, 2003) was applied to obtain weights (am in Equation 2) for these features. A recaser is trained on the target side of the parallel corpus using the script provided with Moses. All output is recased and detokenized prior to evaluation. Evaluation We evaluate translation output using three automatic evaluation measures: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005, version 0.6).5 All measures used were the case-sensitive, corpuslevel versions. The version of BLEU used was that provided by NIST. Significance was tested using a paired bootstrap (Koehn, 2004) with 1000 samples (p &lt; 0.05).6 4http://www.statmt.org/wmt08 5METEOR details: For English, we use exact matching, Porter stemming, and WordNet synonym matching. For German, we use exact matching and Porter stemming. These are the same settings that were used to evaluate systems for the WMT07 shared task. 6Code implementing this test for th</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Somers</author>
</authors>
<title>Review article: Example-based machine translation.</title>
<date>1999</date>
<journal>Machine Translation,</journal>
<volume>14</volume>
<issue>2</issue>
<contexts>
<context position="7000" citStr="Somers, 1999" startWordPosition="1088" endWordPosition="1089">eted translation. Gim´enez and M`arquez (2007) extended the work by considering phrases and moved to full translation instead of filling in target-side blanks. They trained an SVM for each source language phrase using local features of the sentences in which the phrases appear. Carpuat and Wu (2007) and Chan et al. (2007) embedded state-of-the-art word sense disambiguation modules into statistical MT systems, achieving performance improvements under several automatic measures for Chinese —* English translation. Our approach is also reminiscent of examplebased machine translation (Nagao, 1984; Somers, 1999; Carl and Way, 2003), which has for many years emphasized use of the context in which source phrases appear when translating them. Indeed, like the example-based community, we do not begin with any set of assumptions about which kinds of phrases require additional disambiguation (cf. the application of word-sense disambiguation, which is motivated by lexical ambiguity). Our feature-rich approach is omnivorous and can exploit any linguistic analysis of an input sentence. 3 Source-Side Context Features Adding features to the linear model (Equation 2) that consider more of the source sentence re</context>
</contexts>
<marker>Somers, 1999</marker>
<rawString>H. Somers. 1999. Review article: Example-based machine translation. Machine Translation, 14(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM—an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. of ICSLP.</booktitle>
<contexts>
<context position="18475" citStr="Stolcke, 2002" startWordPosition="3022" endWordPosition="3023">al., 2007) as a baseline and closely follow the example training procedure given for the WMT-07 and WMT-08 shared tasks.4 In particular, we perform word alignment in each direction using GIZA++ (Och and Ney, 2003), apply the “grow-diag-finaland” heuristic for symmetrization and use a maximum phrase length of 7. In addition to the two phrase translation conditionals p(e |f) and p(f | e), we use lexical translation probabilities in each direction, a word penalty, a phrase penalty, a lengthbased reordering model, a lexicalized reordering model, and an n-gram language model, SRILM implementation (Stolcke, 2002) with modified KneserNey smoothing (Chen and Goodman, 1998). Minimum error-rate (MER) training (Och, 2003) was applied to obtain weights (am in Equation 2) for these features. A recaser is trained on the target side of the parallel corpus using the script provided with Moses. All output is recased and detokenized prior to evaluation. Evaluation We evaluate translation output using three automatic evaluation measures: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005, version 0.6).5 All measures used were the case-sensitive, corpuslevel versions. The ve</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM—an extensible language modeling toolkit. In Proc. of ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Stroppa</author>
<author>A van den Bosch</author>
<author>A Way</author>
</authors>
<title>Exploiting source similarity for SMT using contextinformed features.</title>
<date>2007</date>
<booktitle>In Proc. of TMI.</booktitle>
<marker>Stroppa, van den Bosch, Way, 2007</marker>
<rawString>N. Stroppa, A. van den Bosch, and A. Way. 2007. Exploiting source similarity for SMT using contextinformed features. In Proc. of TMI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Vickrey</author>
<author>L Biewald</author>
<author>M Teyssier</author>
<author>D Koller</author>
</authors>
<title>Word-sense disambiguation for machine translation.</title>
<date>2005</date>
<booktitle>In Proc. of HLT-EMNLP.</booktitle>
<contexts>
<context position="6279" citStr="Vickrey et al. (2005)" startWordPosition="981" endWordPosition="984">oths relative frequency estimates. The method improved over a standard phrase-based baseline trained on small amounts of data (&lt; 50K sentence pairs) for Italian —* English and Chinese —* English. We explore a significantly larger space of context features, a smoothing method that more naturally fits into the widely used, errordriven linear model, and report a more comprehensive experimental evaluation (including feature comparison and scaling up to very large datasets). Recent research on the use of word-sense disambiguation in machine translation also points toward our approach. For example, Vickrey et al. (2005) built classifiers inspired by those used in word sense disambiguation to fill in blanks in a partially-completed translation. Gim´enez and M`arquez (2007) extended the work by considering phrases and moved to full translation instead of filling in target-side blanks. They trained an SVM for each source language phrase using local features of the sentences in which the phrases appear. Carpuat and Wu (2007) and Chan et al. (2007) embedded state-of-the-art word sense disambiguation modules into statistical MT systems, achieving performance improvements under several automatic measures for Chines</context>
</contexts>
<marker>Vickrey, Biewald, Teyssier, Koller, 2005</marker>
<rawString>D. Vickrey, L. Biewald, M. Teyssier, and D. Koller. 2005. Word-sense disambiguation for machine translation. In Proc. of HLT-EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>