<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000774">
<title confidence="0.9968505">
NRC-Canada-2014: Detecting Aspects and Sentiment
in Customer Reviews
</title>
<author confidence="0.998007">
Svetlana Kiritchenko, Xiaodan Zhu, Colin Cherry, and Saif M. Mohammad
</author>
<affiliation confidence="0.743862666666667">
National Research Council Canada
1200 Montreal Rd., Ottawa, ON, Canada
{Svetlana.Kiritchenko, Xiaodan.Zhu, Colin.Cherry, Saif.Mohammad}
</affiliation>
<email confidence="0.792073">
@nrc-cnrc.gc.ca
</email>
<sectionHeader confidence="0.989808" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999866571428572">
Reviews depict sentiments of customers
towards various aspects of a product or
service. Some of these aspects can be
grouped into coarser aspect categories.
SemEval-2014 had a shared task (Task 4)
on aspect-level sentiment analysis, with
over 30 teams participated. In this pa-
per, we describe our submissions, which
stood first in detecting aspect categories,
first in detecting sentiment towards aspect
categories, third in detecting aspect terms,
and first and second in detecting senti-
ment towards aspect terms in the laptop
and restaurant domains, respectively.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997866541666666">
Automatically identifying sentiment expressed in
text has a number of applications, including track-
ing sentiment towards products, movies, politi-
cians, etc.; improving customer relation models;
and detecting happiness and well-being. In many
applications, it is important to associate sentiment
with a particular entity or an aspect of an entity.
For example, in reviews, customers might express
different sentiment towards various aspects of a
product or service they have availed. Consider:
The lasagna was great, but the service
was a bit slow.
The review is for a restaurant, and we can gather
from it that the customer has a positive sentiment
towards the lasagna they serve, but a negative sen-
timent towards the service.
The SemEval-2014 Task 4 (Aspect Based Sen-
timent Analysis) is a shared task where given a
customer review, automatic systems are to deter-
mine aspect terms, aspect categories, and senti-
ment towards these aspect terms and categories.
An aspect term is defined to be an explicit men-
tion of a feature or component of the target prod-
uct or service. The example sentence above has
</bodyText>
<subsectionHeader confidence="0.804572">
Restaurants Laptops
</subsectionHeader>
<table confidence="0.3833865">
Term T-Sent. Cat. C-Sent. Term T-Sent.
3 2 1 1 3 1
</table>
<tableCaption confidence="0.9481295">
Table 1: Rank obtained by NRC-Canada in vari-
ous subtasks of SemEval-2014 Task 4.
</tableCaption>
<bodyText confidence="0.999967727272727">
the aspect term lasagna. Similar aspect terms can
be grouped into aspect categories. For example,
lasagna and other food items can be grouped into
the aspect category of ‘food’. In Task 4, customer
reviews are provided for two domains: restaurants
and laptops. A fixed set of five aspect categories
is defined for the restaurant domain: food, ser-
vice, price, ambiance, and anecdotes. Automatic
systems are to determine if any of those aspect
categories are described in a review. The exam-
ple sentence above describes the aspect categories
of food (positive sentiment) and service (negative
sentiment). For the laptop reviews, there is no as-
pect category detection subtask. Further details of
the task and data can be found in the task descrip-
tion paper (Pontiki et al., 2014).
We present an in-house sequence tagger to de-
tect aspect terms and supervised classifiers to de-
tect aspect categories, sentiment towards aspect
terms, and sentiment towards aspect categories. A
summary of the ranks obtained by our submissions
to the shared task is provided in Table 1.
</bodyText>
<sectionHeader confidence="0.992916" genericHeader="method">
2 Lexical Resources
</sectionHeader>
<subsectionHeader confidence="0.998712">
2.1 Unlabeled Reviews Corpora
</subsectionHeader>
<bodyText confidence="0.999921285714286">
Apart from the training data provided for Task 4,
we compiled large corpora of reviews for restau-
rants and laptops that were not labeled for aspect
terms, aspect categories, or sentiment. We gen-
erated lexicons from these corpora and used them
as a source of additional features in our machine
learning systems.
</bodyText>
<page confidence="0.981405">
437
</page>
<note confidence="0.730677">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 437–442,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.99992572">
Yelp restaurant reviews corpus: The Yelp
Phoenix Academic Dataset1 contains customer re-
views posted on the Yelp website. The businesses
for which the reviews are posted are classified into
over 500 categories. Further, many of the busi-
nesses are assigned multiple business categories.
We identified all food-related business categories
(58 categories) that were grouped along with the
category ‘restaurant’ and extracted all customer
reviews for these categories. We will refer to this
corpus of 183,935 reviews as the Yelp restaurant
reviews corpus.
Amazon laptop reviews corpus: McAuley and
Leskovec (2013) collected reviews posted on
Amazon.com from June 1995 to March 2013. A
subset of this corpus is marked as reviews for elec-
tronic products. We extracted from this subset all
reviews that mention either laptop or notebook.
We will refer to this collection of 124,712 reviews
as the Amazon laptop reviews corpus.
Both the Yelp and the Amazon reviews have one
to five star ratings associated with each review. We
treated the one- and two-star reviews as negative
reviews, and the four- and five-star reviews as pos-
itive reviews.
</bodyText>
<subsectionHeader confidence="0.998622">
2.2 Lexicons
</subsectionHeader>
<bodyText confidence="0.9996075">
Sentiment Lexicons: From the Yelp restaurant
reviews corpus, we automatically created an in-
domain sentiment lexicon for restaurants. Follow-
ing Turney and Littman (2003) and Mohammad
et al. (2013), we calculated a sentiment score for
each term w in the corpus:
</bodyText>
<equation confidence="0.788052">
score (w) = PMI (w, pos)−PMI (w, neg) (1)
</equation>
<bodyText confidence="0.993768">
where pos denotes positive reviews and neg de-
notes negative reviews. PMI stands for pointwise
mutual information:
</bodyText>
<equation confidence="0.985828">
( freq (w, pos) ∗ N
PMI w, pos) = log2 freq (w) ∗ freq (pos) (2)
</equation>
<bodyText confidence="0.9995527">
where freq (w, pos) is the number of times a term
w occurs in positive reviews, freq (w) is the to-
tal frequency of term w in the corpus, freq (pos)
is the total number of tokens in positive reviews,
and N is the total number of tokens in the cor-
pus. PMI (w, neg) was calculated in a similar
way. Since PMI is known to be a poor estima-
tor of association for low-frequency events, we ig-
nored terms that occurred less than five times in
each (positive and negative) groups of reviews.
</bodyText>
<footnote confidence="0.935328">
1http://www.yelp.com/dataset_challenge
</footnote>
<bodyText confidence="0.999572580645162">
A positive sentiment score indicates a greater
overall association with positive sentiment,
whereas a negative score indicates a greater asso-
ciation with negative sentiment. The magnitude is
indicative of the degree of association.
Negation words (e.g., not, never) can signifi-
cantly affect the sentiment of an expression (Zhu
et al., 2014). Therefore, when generating the sen-
timent lexicons we distinguished terms appearing
in negated contexts (defined as text spans between
a negation word and a punctuation mark) and af-
firmative (non-negated) contexts. The sentiment
scores were then calculated separately for the two
types of contexts. For example, the term good in
affirmative contexts has a sentiment score of 1.2
whereas the same term in negated contexts has a
score of -1.4. We built two lexicons, Yelp Restau-
rant Sentiment AffLex and Yelp Restaurant Senti-
ment NegLex, as described in (Kiritchenko et al.,
2014).
Similarly, we generated in-domain sentiment
lexicons from the Amazon laptop reviews corpus.
In addition, we employed existing out-of-
domain sentiment lexicons: (1) large-coverage au-
tomatic tweet sentiment lexicons, Hashtag Sen-
timent lexicons and Sentiment140 lexicons (Kir-
itchenko et al., 2014), and (2) three manually cre-
ated sentiment lexicons, NRC Emotion Lexicon
(Mohammad and Turney, 2010), Bing Liu’s Lex-
icon (Hu and Liu, 2004), and the MPQA Subjec-
tivity Lexicon (Wilson et al., 2005).
</bodyText>
<subsectionHeader confidence="0.694794">
Yelp Restaurant Word–Aspect Association
</subsectionHeader>
<bodyText confidence="0.999743777777778">
Lexicon: The Yelp restaurant reviews corpus was
also used to generate a lexicon of terms associated
with the aspect categories of food, price, service,
ambiance, and anecdotes. Each sentence of the
corpus was labeled with zero, one, or more of the
five aspect categories by our aspect category clas-
sification system (described in Section 5). Then,
for each term w and each category c an associa-
tion score was calculated as follows:
</bodyText>
<equation confidence="0.883392">
score (w, c) = PMI (w, c) − PMI (w, ¬c) (3)
</equation>
<subsectionHeader confidence="0.998489">
2.3 Word Clusters
</subsectionHeader>
<bodyText confidence="0.9998545">
Word clusters can provide an alternative represen-
tation of text, significantly reducing the sparsity
of the token space. Using Brown clustering algo-
rithm (Brown et al., 1992), we generated 1,000
word clusters from the Yelp restaurant reviews
corpus. Additionally, we used publicly available
</bodyText>
<page confidence="0.99517">
438
</page>
<bodyText confidence="0.989056">
word clusters generated from 56 million English-
language tweets (Owoputi et al., 2013).
</bodyText>
<sectionHeader confidence="0.918953" genericHeader="method">
3 Subtask 1: Aspect Term Extraction
</sectionHeader>
<bodyText confidence="0.999155234042553">
The objective of this subtask is to detect aspect
terms in sentences. We approached this problem
using in-house entity-recognition software, very
similar to the system used by de Bruijn et al.
(2011) to detect medical concepts. First, sen-
tences were tokenized to split away punctuation,
and then the token sequence was tagged using a
semi-Markov tagger (Sarawagi and Cohen, 2004).
The tagger had two possible tags: O for outside,
and T for aspect term, where an aspect term could
tag a phrase of up to 5 consecutive tokens. The
tagger was trained using the structured Passive-
Aggressive (PA) algorithm with a maximum step-
size of C = 1 (Crammer et al., 2006).
Our features can be divided into two categories:
emission and transition features. Emission fea-
tures couple the tag sequence y to the input w.
Most of these work on the token level, and con-
join features of each token with the tag covering
that token. If a token is the first or last token cov-
ered by a tag, then we produce a second copy of
each of its features to indicate its special position.
Let wi be the token being tagged; its token fea-
ture templates are: token-identity within a win-
dow (wi−2 ... wi+2), lower-cased token-identity
within a window (lc(wi−2) ... lc(wi+2)), and pre-
fixes and suffixes of wi (up to 3 characters in
length). There are only two phrase-level emission
feature templates: the cased and uncased identity
of the entire phrase covered by a tag, which al-
low the system to memorize complete terms such
as, “getting a table” or “fish and chips.” Transi-
tion features couple tags with tags. Let the cur-
rent tag be yj. Its transition feature templates are
short n-grams of tag identities: yj; yj, yj−1; and
yj, yj−1, yj−2.
During development, we experimented with the
training algorithm, trying both PA and the simpler
structured perceptron (Collins, 2002). We also
added the lowercased back-off features. In Ta-
ble 2, we re-test these design decisions on the test
set, revealing that lower-cased back-off features
made a strong contribution, while PA training was
perhaps not as important. Our complete system
achieved an F1-score of 80.19 on the restaurant
domain and 68.57 on the laptop domain, ranking
third among 24 teams in both.
</bodyText>
<table confidence="0.9995945">
System Restaurants
P R F1
NRC-Canada (All) 84.41 76.37 80.19
All − lower-casing 83.68 75.49 79.37
All − PA + percep 83.37 76.45 79.76
Laptops
System P R F1
NRC-Canada (All) 78.77 60.70 68.57
All − lower-casing 78.11 60.55 68.22
All − PA + percep 77.76 61.47 68.66
</table>
<tableCaption confidence="0.993594">
Table 2: Test set ablation experiments for Sub-
task 1: Aspect Term Detection.
</tableCaption>
<sectionHeader confidence="0.952828" genericHeader="method">
4 Subtask 2: Aspect Term Polarity
</sectionHeader>
<bodyText confidence="0.997705636363636">
In this subtask, the goal is to detect sentiment ex-
pressed towards a given aspect term. For example,
in sentence “The asian salad is barely eatable.” the
aspect term asian salad is referred to with negative
sentiment. There were defined four categories of
sentiment: positive, negative, neutral, or conflict.
The conflict category is assigned to cases where
an aspect term is mentioned with both positive and
negative sentiment.
To address this multi-class classification prob-
lem, we trained a linear SVM classifier using
the LibSVM software (Chang and Lin, 2011).
Sentences were first tokenized and parsed with
the Stanford CoreNLP toolkits2 to obtain part-of-
speech (POS) tags and (collapsed) typed depen-
dency parse trees (de Marneffe et al., 2006). Then,
features were extracted from (1) the target term it-
self; (2) its surface context, i.e., a window of n
words surrounding the term; (3) the parse context,
i.e., the nodes in the parse tree that are connected
to the target term by at most three edges.
Surface features: (1) unigrams (single words)
and bigrams (2-word sequences) extracted from a
term and its surface context; (2) context-target bi-
grams (i.e., bigrams formed by a word from the
surface context and a word from the term itself).
Lexicon features: (1) the number of posi-
tive/negative tokens; (2) the sum of the tokens’
sentiment scores; (3) the maximal sentiment score.
The lexicon features were calculated for each
manually and automatically created sentiment lex-
icons described in Section 2.2.
Parse features: (1) word- and POS-ngrams in
</bodyText>
<footnote confidence="0.962315">
2http://nlp.stanford.edu/software/corenlp.shtml
</footnote>
<page confidence="0.986151">
439
</page>
<table confidence="0.999843">
System Laptops Rest.
Acc. Acc.
NRC-Canada (All) 70.49 80.16
All − sentiment lexicons 63.61 77.13
All − Yelp lexicons 68.65 77.85
All − Amazon lex. 68.13 80.11
All − manual lexicons 67.43 78.66
All − tweet lexicons 69.11 78.57
All − parse features 69.42 78.40
</table>
<tableCaption confidence="0.8657935">
Table 3: Test set ablation experiments for Sub-
task 2: Aspect Term Polarity.
</tableCaption>
<table confidence="0.999766857142857">
System Restaurants
P R F1
NRC-Canada (All) 91.04 86.24 88.58
All − lex. resources 86.53 78.34 82.23
All − W–A lexicon 88.47 80.10 84.08
All − word clusters 90.84 86.15 88.43
All − post-processing 91.47 84.78 88.00
</table>
<tableCaption confidence="0.961995">
Table 4: Test set ablation experiments for Sub-
task 3: Aspect Category Detection. ‘W–A lexicon’
stands for Yelp Restaurant Word–Aspect Associa-
tion Lexicon.
</tableCaption>
<bodyText confidence="0.999778735294118">
the parse context; (2) context-target bigrams, i.e.,
bigrams composed of a word from the parse con-
text and a word from the term; (3) all paths that
start or end with the root of the target terms. The
idea behind the use of the parse features is that
sometimes an aspect term is separated from its
modifying sentiment phrase and the surface con-
text is insufficient or even misleading for detect-
ing sentiment expressed towards the aspect. For
example, in sentence “The food, though different
from what we had last time, is actually great” the
word great is much closer to the word food in the
parse tree than in the surface form. Furthermore,
the features derived from the parse context can
help resolve local syntactic ambiguity (e.g., the
word bad in the phrase “a bad sushi lover” modi-
fies lover and not sushi).
Table 3 presents the results of our official sub-
mission on the test sets for the laptop and restau-
rant domains. On the laptop dataset, our system
achieved the accuracy of 70.49 and was ranked
first among 32 submissions from 29 teams. From
the ablation experiments we see that the most sig-
nificant gains come from the use of the sentiment
lexicons; without the lexicon features the perfor-
mance of the system drops by 6.88 percentage
points. Observe that the features derived from
the out-of-domain Yelp Restaurant Sentiment lex-
icon are very helpful on the laptop domain. The
parse features proved to be useful as well; they
contribute 1.07 percentage points to the final per-
formance. On the restaurant data, our system ob-
tained the accuracy of 80.16 and was ranked sec-
ond among 36 submissions from 29 teams.
</bodyText>
<sectionHeader confidence="0.980666" genericHeader="method">
5 Subtask 3: Aspect Category Detection
</sectionHeader>
<bodyText confidence="0.992513175">
The objective of this subtask is to detect aspect
categories discussed in a given sentence. There
are 5 pre-defined categories for the restaurant do-
main: food, price, service, ambience, and anec-
dotes/miscellaneous. Each sentence can be la-
beled with one or more categories from the pre-
defined set. No aspect categories were defined for
the laptop domain.
We addressed the subtask as a multi-class multi-
label text classification problem. Five binary one-
vs-all Support Vector Machine (SVM) classifiers
were built, one for each category. The parameter C
was optimized through cross-validation separately
for each classifier. Sentences were tokenized
and stemmed with Porter stemmer (Porter, 1980).
Then, the following sets of features were gener-
ated for each sentence: ngrams, stemmed ngrams,
character ngrams, non-contiguous ngrams, word
cluster ngrams, and lexicon features. For the lex-
icon features, we used the Yelp Restaurant Word–
Aspect Association Lexicon and calculated the cu-
mulative scores of all terms appeared in the sen-
tence for each aspect category. Separate scores
were calculated for unigram and bigram entries.
Sentences with no category assigned by any of the
five classifiers went through the post-processing
step. For each such sentence, a category c with the
maximal posterior probability P(c|d) was identi-
fied and the sentence was labeled with the category
c if P(c|d) &gt; 0.4.
Table 4 presents the results on the restaurant test
set. Our system obtained the F1-score of 88.58
and was ranked first among 21 submissions from
18 teams. Among the lexical resources (lexicons
and word clusters) employed in the system, the
Word–Aspect Association Lexicon provided the
most gains: an increase in F1-score of 4.5 points.
The post-processing step also proved to be benefi-
cial: the recall improved by 1.46 points increasing
the overall F1-score by 0.58 points.
</bodyText>
<page confidence="0.99727">
440
</page>
<sectionHeader confidence="0.826899" genericHeader="method">
6 Subtask 4: Aspect Category Polarity
</sectionHeader>
<bodyText confidence="0.999692">
In the Aspect Category Polarity subtask, the goal
is to detect the sentiment expressed towards a
given aspect category in a given sentence. For
each input pair (sentence, aspect category), the
output is a single sentiment label: positive, neg-
ative, neutral, or conflict.
We trained one multi-class SVM classifier
(Crammer and Singer, 2002) for all aspect cate-
gories. The feature set was extended to incorpo-
rate the information about a given aspect category
c using a domain adaptation technique (Daum´e III,
2007) as follows: each feature f had two copies,
f general (for all the aspect categories) and f c
(for the specific category of the instance). For ex-
ample, for the input pair (“The bread is top notch
as well.”, ‘food’) two copies of the unigram top
would be used: top general and top food. With
this setup the classifier can take advantage of the
whole training dataset to learn common sentiment
features (e.g., the word good is associated with
positive sentiment for all aspect categories). At the
same time, aspect-specific sentiment features can
be learned from the training instances pertaining
to a specific aspect category (e.g., the word deli-
cious is associated with positive sentiment for the
category ‘food’).
Sentences were tokenized and part-of-speech
tagged with CMU Twitter NLP tool (Gimpel et al.,
2011). Then, each sentence was represented as a
feature vector with the following groups of fea-
tures: ngrams, character ngrams, non-contiguous
ngrams, POS tags, cluster ngrams, and lexicon
features. The lexicon features were calculated as
described in Section 4.
A sentence can refer to more than one aspect
category with different sentiment. For example,
in the sentence “The pizza was delicious, but the
waiter was rude.”, food is described with posi-
tive sentiment while service with negative. If the
words delicious and rude occur in the training set,
the classifier can learn that delicious usually refers
to food (with positive sentiment) and rude to ser-
vice (with negative sentiment). If these terms do
not appear in the training set, their polarities can
still be inferred from sentiment lexicons. How-
ever, sentiment lexicons do not distinguish among
aspect categories and would treat both words, de-
licious and rude, as equally applicable to both cat-
egories, ‘food’ and ‘service’. To (partially) over-
come this problem, we applied the Yelp Restau-
</bodyText>
<table confidence="0.9992456">
System Restaurants
Accuracy
NRC-Canada (All) 82.93
All − lexical resources 74.15
All − lexicons 75.32
All − Yelp lexicons 79.22
All − manual lexicons 82.44
All − tweet lexicons 84.10
All − word clusters 82.93
All − aspect term features 82.54
</table>
<tableCaption confidence="0.987062">
Table 5: Test set ablation experiments for Sub-
task 4: Aspect Category Polarity.
</tableCaption>
<bodyText confidence="0.993563916666667">
rant Word–Aspect Association Lexicon to collect
all the terms having a high or moderate associ-
ation with the given aspect category (e.g., pizza,
delicious for the category ‘food’ and waiter, rude
for the category ‘service’). Then, the feature set
described above was augmented with the same
groups of features generated just for the terms as-
sociated with the given category. We call these
features aspect term features.
Table 5 presents the results on the test set for
the restaurant domain. Our system achieved the
accuracy of 82.93 and was ranked first among 23
submissions from 20 teams. The ablation exper-
iments demonstrate the significant impact of the
lexical resources employed in the system: 8.78
percentage point gain in accuracy. The major ad-
vantage comes from the sentiment lexicons, and
specifically from the in-domain Yelp Restaurant
Sentiment lexicons. The out-of-domain tweet sen-
timent lexicons did not prove useful on this sub-
task. Also, word clusters did not offer additional
benefits on top of those provided by the lexicons.
The use of aspect term features resulted in gains
of 0.39.
</bodyText>
<sectionHeader confidence="0.998485" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999869454545454">
The paper describes supervised machine-learning
approaches to detect aspect terms and aspect cat-
egories and to detect sentiment expressed towards
aspect terms and aspect categories in customer re-
views. Apart from common surface-form features
such as ngrams, our approaches benefit from the
use of existing and newly created lexical resources
such as word–aspect association lexicons and sen-
timent lexicons. Our submissions stood first on 3
out of 4 subtasks, and within the top 3 best results
on all 6 task-domain evaluations.
</bodyText>
<page confidence="0.99623">
441
</page>
<bodyText confidence="0.9017722">
Julian McAuley and Jure Leskovec. 2013. Hidden fac-
tors and hidden topics: understanding rating dimen-
sions with review text. In Proceedings of the 7th
ACM conference on Recommender systems, pages
165–172. ACM.
</bodyText>
<note confidence="0.4408894">
References
Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics, 18(4):467–479.
</note>
<reference confidence="0.985070195652174">
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2(3):27:1–27:27.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Koby Crammer and Yoram Singer. 2002. On the algo-
rithmic implementation of multiclass kernel-based
vector machines. The Journal of Machine Learning
Research, 2:265–292.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine
Learning Research, 7:551–585.
Hal Daum´e III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics, ACL
’07, pages 256 – 263.
Berry de Bruijn, Colin Cherry, Svetlana Kiritchenko,
Joel Martin, and Xiaodan Zhu. 2011. Machine-
learned solutions for three stages of clinical infor-
mation extraction: the state of the art at i2b2 2010.
Journal of the American Medical Informatics Asso-
ciation, 18(5):557–562.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses.
In Proceedings of the International Conference on
Language Resources and Evaluation, LREC ’06.
Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for Twitter: Annotation, features, and experiments.
In Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics, ACL ’11.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the 10th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ’04, pages
168–177, New York, NY, USA. ACM.
Svetlana Kiritchenko, Xiaodan Zhu, and Saif M. Mo-
hammad. 2014. Sentiment analysis of short infor-
mal texts. Journal ofArtificial Intelligence Research
(to appear).
Saif M. Mohammad and Peter D. Turney. 2010. Emo-
tions evoked by common words and phrases: Using
Mechanical Turk to create an emotion lexicon. In
Proceedings of the NAACL-HLT Workshop on Com-
putational Approaches to Analysis and Generation
of Emotion in Text, LA, California.
Saif M. Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. NRC-Canada: Building the state-
of-the-art in sentiment analysis of tweets. In Pro-
ceedings of the International Workshop on Semantic
Evaluation, SemEval ’13, Atlanta, Georgia, USA,
June.
Olutobi Owoputi, Brendan OConnor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL-HLT, pages 380–390.
Maria Pontiki, Dimitrios Galanis, John Pavlopou-
los, Harris Papageorgiou, Ion Androutsopoulos, and
Suresh Manandhar. 2014. SemEval-2014 Task 4:
Aspect based sentiment analysis. In Proceedings of
the International Workshop on Semantic Evaluation,
SemEval ’14, Dublin, Ireland, August.
M.F. Porter. 1980. An algorithm for suffix stripping.
Program, 3:130–137.
Sunita Sarawagi and William W Cohen. 2004. Semi-
markov conditional random fields for information
extraction. In Advances in Neural Information Pro-
cessing Systems, volume 17, pages 1185–1192.
Peter Turney and Michael L Littman. 2003. Measuring
praise and criticism: Inference of semantic orienta-
tion from association. ACM Transactions on Infor-
mation Systems, 21(4).
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
HLT ’05, pages 347–354, Stroudsburg, PA, USA.
Xiaodan Zhu, Hongyu Guo, Saif M. Mohammad, and
Svetlana Kiritchenko. 2014. An empirical study on
the effect of negation words on sentiment. In Pro-
ceedings of the Annual Meeting of the Association
for Computational Linguistics, ACL ’14.
</reference>
<page confidence="0.998444">
442
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.678309">
<title confidence="0.971874">NRC-Canada-2014: Detecting Aspects and in Customer Reviews</title>
<author confidence="0.820515">Svetlana Kiritchenko</author>
<author confidence="0.820515">Xiaodan Zhu</author>
<author confidence="0.820515">Colin Cherry</author>
<author confidence="0.820515">M Saif</author>
<affiliation confidence="0.9891">National Research Council</affiliation>
<address confidence="0.9293935">1200 Montreal Rd., Ottawa, ON, Xiaodan.Zhu, Colin.Cherry,</address>
<email confidence="0.994155">@nrc-cnrc.gc.ca</email>
<abstract confidence="0.999216">Reviews depict sentiments of customers various a product or service. Some of these aspects can be into coarser SemEval-2014 had a shared task (Task 4) on aspect-level sentiment analysis, with over 30 teams participated. In this paper, we describe our submissions, which stood first in detecting aspect categories, first in detecting sentiment towards aspect categories, third in detecting aspect terms, and first and second in detecting sentiment towards aspect terms in the laptop and restaurant domains, respectively.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<volume>2</volume>
<issue>3</issue>
<contexts>
<context position="11363" citStr="Chang and Lin, 2011" startWordPosition="1839" endWordPosition="1842">Aspect Term Detection. 4 Subtask 2: Aspect Term Polarity In this subtask, the goal is to detect sentiment expressed towards a given aspect term. For example, in sentence “The asian salad is barely eatable.” the aspect term asian salad is referred to with negative sentiment. There were defined four categories of sentiment: positive, negative, neutral, or conflict. The conflict category is assigned to cases where an aspect term is mentioned with both positive and negative sentiment. To address this multi-class classification problem, we trained a linear SVM classifier using the LibSVM software (Chang and Lin, 2011). Sentences were first tokenized and parsed with the Stanford CoreNLP toolkits2 to obtain part-ofspeech (POS) tags and (collapsed) typed dependency parse trees (de Marneffe et al., 2006). Then, features were extracted from (1) the target term itself; (2) its surface context, i.e., a window of n words surrounding the term; (3) the parse context, i.e., the nodes in the parse tree that are connected to the target term by at most three edges. Surface features: (1) unigrams (single words) and bigrams (2-word sequences) extracted from a term and its surface context; (2) context-target bigrams (i.e.,</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2(3):27:1–27:27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="10046" citStr="Collins, 2002" startWordPosition="1623" endWordPosition="1624">... lc(wi+2)), and prefixes and suffixes of wi (up to 3 characters in length). There are only two phrase-level emission feature templates: the cased and uncased identity of the entire phrase covered by a tag, which allow the system to memorize complete terms such as, “getting a table” or “fish and chips.” Transition features couple tags with tags. Let the current tag be yj. Its transition feature templates are short n-grams of tag identities: yj; yj, yj−1; and yj, yj−1, yj−2. During development, we experimented with the training algorithm, trying both PA and the simpler structured perceptron (Collins, 2002). We also added the lowercased back-off features. In Table 2, we re-test these design decisions on the test set, revealing that lower-cased back-off features made a strong contribution, while PA training was perhaps not as important. Our complete system achieved an F1-score of 80.19 on the restaurant domain and 68.57 on the laptop domain, ranking third among 24 teams in both. System Restaurants P R F1 NRC-Canada (All) 84.41 76.37 80.19 All − lower-casing 83.68 75.49 79.37 All − PA + percep 83.37 76.45 79.76 Laptops System P R F1 NRC-Canada (All) 78.77 60.70 68.57 All − lower-casing 78.11 60.55</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Yoram Singer</author>
</authors>
<title>On the algorithmic implementation of multiclass kernel-based vector machines.</title>
<date>2002</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>2--265</pages>
<contexts>
<context position="17024" citStr="Crammer and Singer, 2002" startWordPosition="2762" endWordPosition="2765">stem, the Word–Aspect Association Lexicon provided the most gains: an increase in F1-score of 4.5 points. The post-processing step also proved to be beneficial: the recall improved by 1.46 points increasing the overall F1-score by 0.58 points. 440 6 Subtask 4: Aspect Category Polarity In the Aspect Category Polarity subtask, the goal is to detect the sentiment expressed towards a given aspect category in a given sentence. For each input pair (sentence, aspect category), the output is a single sentiment label: positive, negative, neutral, or conflict. We trained one multi-class SVM classifier (Crammer and Singer, 2002) for all aspect categories. The feature set was extended to incorporate the information about a given aspect category c using a domain adaptation technique (Daum´e III, 2007) as follows: each feature f had two copies, f general (for all the aspect categories) and f c (for the specific category of the instance). For example, for the input pair (“The bread is top notch as well.”, ‘food’) two copies of the unigram top would be used: top general and top food. With this setup the classifier can take advantage of the whole training dataset to learn common sentiment features (e.g., the word good is a</context>
</contexts>
<marker>Crammer, Singer, 2002</marker>
<rawString>Koby Crammer and Yoram Singer. 2002. On the algorithmic implementation of multiclass kernel-based vector machines. The Journal of Machine Learning Research, 2:265–292.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passive-aggressive algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>7--551</pages>
<contexts>
<context position="8867" citStr="Crammer et al., 2006" startWordPosition="1416" endWordPosition="1419"> detect aspect terms in sentences. We approached this problem using in-house entity-recognition software, very similar to the system used by de Bruijn et al. (2011) to detect medical concepts. First, sentences were tokenized to split away punctuation, and then the token sequence was tagged using a semi-Markov tagger (Sarawagi and Cohen, 2004). The tagger had two possible tags: O for outside, and T for aspect term, where an aspect term could tag a phrase of up to 5 consecutive tokens. The tagger was trained using the structured PassiveAggressive (PA) algorithm with a maximum stepsize of C = 1 (Crammer et al., 2006). Our features can be divided into two categories: emission and transition features. Emission features couple the tag sequence y to the input w. Most of these work on the token level, and conjoin features of each token with the tag covering that token. If a token is the first or last token covered by a tag, then we produce a second copy of each of its features to indicate its special position. Let wi be the token being tagged; its token feature templates are: token-identity within a window (wi−2 ... wi+2), lower-cased token-identity within a window (lc(wi−2) ... lc(wi+2)), and prefixes and suf</context>
</contexts>
<marker>Crammer, Dekel, Keshet, Shalev-Shwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. 2006. Online passive-aggressive algorithms. Journal of Machine Learning Research, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics, ACL ’07,</booktitle>
<pages>256--263</pages>
<marker>Daum´e, 2007</marker>
<rawString>Hal Daum´e III. 2007. Frustratingly easy domain adaptation. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, ACL ’07, pages 256 – 263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Berry de Bruijn</author>
<author>Colin Cherry</author>
<author>Svetlana Kiritchenko</author>
<author>Joel Martin</author>
<author>Xiaodan Zhu</author>
</authors>
<title>Machinelearned solutions for three stages of clinical information extraction: the state of the art at i2b2</title>
<date>2011</date>
<journal>Journal of the American Medical Informatics Association,</journal>
<volume>18</volume>
<issue>5</issue>
<marker>de Bruijn, Cherry, Kiritchenko, Martin, Zhu, 2011</marker>
<rawString>Berry de Bruijn, Colin Cherry, Svetlana Kiritchenko, Joel Martin, and Xiaodan Zhu. 2011. Machinelearned solutions for three stages of clinical information extraction: the state of the art at i2b2 2010. Journal of the American Medical Informatics Association, 18(5):557–562.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Conference on Language Resources and Evaluation, LREC ’06.</booktitle>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of the International Conference on Language Resources and Evaluation, LREC ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Brendan O’Connor</author>
<author>Dipanjan Das</author>
<author>Daniel Mills</author>
<author>Jacob Eisenstein</author>
<author>Michael Heilman</author>
<author>Dani Yogatama</author>
<author>Jeffrey Flanigan</author>
<author>Noah A Smith</author>
</authors>
<title>Part-of-speech tagging for Twitter: Annotation, features, and experiments.</title>
<date>2011</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics, ACL ’11.</booktitle>
<marker>Gimpel, Schneider, O’Connor, Das, Mills, Eisenstein, Heilman, Yogatama, Flanigan, Smith, 2011</marker>
<rawString>Kevin Gimpel, Nathan Schneider, Brendan O’Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith. 2011. Part-of-speech tagging for Twitter: Annotation, features, and experiments. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, ACL ’11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’04,</booktitle>
<pages>168--177</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="7200" citStr="Hu and Liu, 2004" startWordPosition="1140" endWordPosition="1143"> term in negated contexts has a score of -1.4. We built two lexicons, Yelp Restaurant Sentiment AffLex and Yelp Restaurant Sentiment NegLex, as described in (Kiritchenko et al., 2014). Similarly, we generated in-domain sentiment lexicons from the Amazon laptop reviews corpus. In addition, we employed existing out-ofdomain sentiment lexicons: (1) large-coverage automatic tweet sentiment lexicons, Hashtag Sentiment lexicons and Sentiment140 lexicons (Kiritchenko et al., 2014), and (2) three manually created sentiment lexicons, NRC Emotion Lexicon (Mohammad and Turney, 2010), Bing Liu’s Lexicon (Hu and Liu, 2004), and the MPQA Subjectivity Lexicon (Wilson et al., 2005). Yelp Restaurant Word–Aspect Association Lexicon: The Yelp restaurant reviews corpus was also used to generate a lexicon of terms associated with the aspect categories of food, price, service, ambiance, and anecdotes. Each sentence of the corpus was labeled with zero, one, or more of the five aspect categories by our aspect category classification system (described in Section 5). Then, for each term w and each category c an association score was calculated as follows: score (w, c) = PMI (w, c) − PMI (w, ¬c) (3) 2.3 Word Clusters Word cl</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’04, pages 168–177, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Svetlana Kiritchenko</author>
<author>Xiaodan Zhu</author>
<author>Saif M Mohammad</author>
</authors>
<title>Sentiment analysis of short informal texts.</title>
<date>2014</date>
<journal>Journal ofArtificial Intelligence Research</journal>
<note>(to appear).</note>
<contexts>
<context position="6766" citStr="Kiritchenko et al., 2014" startWordPosition="1077" endWordPosition="1080"> sentiment of an expression (Zhu et al., 2014). Therefore, when generating the sentiment lexicons we distinguished terms appearing in negated contexts (defined as text spans between a negation word and a punctuation mark) and affirmative (non-negated) contexts. The sentiment scores were then calculated separately for the two types of contexts. For example, the term good in affirmative contexts has a sentiment score of 1.2 whereas the same term in negated contexts has a score of -1.4. We built two lexicons, Yelp Restaurant Sentiment AffLex and Yelp Restaurant Sentiment NegLex, as described in (Kiritchenko et al., 2014). Similarly, we generated in-domain sentiment lexicons from the Amazon laptop reviews corpus. In addition, we employed existing out-ofdomain sentiment lexicons: (1) large-coverage automatic tweet sentiment lexicons, Hashtag Sentiment lexicons and Sentiment140 lexicons (Kiritchenko et al., 2014), and (2) three manually created sentiment lexicons, NRC Emotion Lexicon (Mohammad and Turney, 2010), Bing Liu’s Lexicon (Hu and Liu, 2004), and the MPQA Subjectivity Lexicon (Wilson et al., 2005). Yelp Restaurant Word–Aspect Association Lexicon: The Yelp restaurant reviews corpus was also used to genera</context>
</contexts>
<marker>Kiritchenko, Zhu, Mohammad, 2014</marker>
<rawString>Svetlana Kiritchenko, Xiaodan Zhu, and Saif M. Mohammad. 2014. Sentiment analysis of short informal texts. Journal ofArtificial Intelligence Research (to appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif M Mohammad</author>
<author>Peter D Turney</author>
</authors>
<title>Emotions evoked by common words and phrases: Using Mechanical Turk to create an emotion lexicon.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL-HLT Workshop on Computational Approaches to Analysis and Generation of Emotion in Text,</booktitle>
<location>LA, California.</location>
<contexts>
<context position="7161" citStr="Mohammad and Turney, 2010" startWordPosition="1132" endWordPosition="1135">ts has a sentiment score of 1.2 whereas the same term in negated contexts has a score of -1.4. We built two lexicons, Yelp Restaurant Sentiment AffLex and Yelp Restaurant Sentiment NegLex, as described in (Kiritchenko et al., 2014). Similarly, we generated in-domain sentiment lexicons from the Amazon laptop reviews corpus. In addition, we employed existing out-ofdomain sentiment lexicons: (1) large-coverage automatic tweet sentiment lexicons, Hashtag Sentiment lexicons and Sentiment140 lexicons (Kiritchenko et al., 2014), and (2) three manually created sentiment lexicons, NRC Emotion Lexicon (Mohammad and Turney, 2010), Bing Liu’s Lexicon (Hu and Liu, 2004), and the MPQA Subjectivity Lexicon (Wilson et al., 2005). Yelp Restaurant Word–Aspect Association Lexicon: The Yelp restaurant reviews corpus was also used to generate a lexicon of terms associated with the aspect categories of food, price, service, ambiance, and anecdotes. Each sentence of the corpus was labeled with zero, one, or more of the five aspect categories by our aspect category classification system (described in Section 5). Then, for each term w and each category c an association score was calculated as follows: score (w, c) = PMI (w, c) − PM</context>
</contexts>
<marker>Mohammad, Turney, 2010</marker>
<rawString>Saif M. Mohammad and Peter D. Turney. 2010. Emotions evoked by common words and phrases: Using Mechanical Turk to create an emotion lexicon. In Proceedings of the NAACL-HLT Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, LA, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif M Mohammad</author>
<author>Svetlana Kiritchenko</author>
<author>Xiaodan Zhu</author>
</authors>
<title>NRC-Canada: Building the stateof-the-art in sentiment analysis of tweets.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation, SemEval ’13,</booktitle>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="5040" citStr="Mohammad et al. (2013)" startWordPosition="784" endWordPosition="787">ws for electronic products. We extracted from this subset all reviews that mention either laptop or notebook. We will refer to this collection of 124,712 reviews as the Amazon laptop reviews corpus. Both the Yelp and the Amazon reviews have one to five star ratings associated with each review. We treated the one- and two-star reviews as negative reviews, and the four- and five-star reviews as positive reviews. 2.2 Lexicons Sentiment Lexicons: From the Yelp restaurant reviews corpus, we automatically created an indomain sentiment lexicon for restaurants. Following Turney and Littman (2003) and Mohammad et al. (2013), we calculated a sentiment score for each term w in the corpus: score (w) = PMI (w, pos)−PMI (w, neg) (1) where pos denotes positive reviews and neg denotes negative reviews. PMI stands for pointwise mutual information: ( freq (w, pos) ∗ N PMI w, pos) = log2 freq (w) ∗ freq (pos) (2) where freq (w, pos) is the number of times a term w occurs in positive reviews, freq (w) is the total frequency of term w in the corpus, freq (pos) is the total number of tokens in positive reviews, and N is the total number of tokens in the corpus. PMI (w, neg) was calculated in a similar way. Since PMI is known</context>
</contexts>
<marker>Mohammad, Kiritchenko, Zhu, 2013</marker>
<rawString>Saif M. Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu. 2013. NRC-Canada: Building the stateof-the-art in sentiment analysis of tweets. In Proceedings of the International Workshop on Semantic Evaluation, SemEval ’13, Atlanta, Georgia, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olutobi Owoputi</author>
<author>Brendan OConnor</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Improved part-of-speech tagging for online conversational text with word clusters.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>380--390</pages>
<contexts>
<context position="8173" citStr="Owoputi et al., 2013" startWordPosition="1298" endWordPosition="1301">aspect categories by our aspect category classification system (described in Section 5). Then, for each term w and each category c an association score was calculated as follows: score (w, c) = PMI (w, c) − PMI (w, ¬c) (3) 2.3 Word Clusters Word clusters can provide an alternative representation of text, significantly reducing the sparsity of the token space. Using Brown clustering algorithm (Brown et al., 1992), we generated 1,000 word clusters from the Yelp restaurant reviews corpus. Additionally, we used publicly available 438 word clusters generated from 56 million Englishlanguage tweets (Owoputi et al., 2013). 3 Subtask 1: Aspect Term Extraction The objective of this subtask is to detect aspect terms in sentences. We approached this problem using in-house entity-recognition software, very similar to the system used by de Bruijn et al. (2011) to detect medical concepts. First, sentences were tokenized to split away punctuation, and then the token sequence was tagged using a semi-Markov tagger (Sarawagi and Cohen, 2004). The tagger had two possible tags: O for outside, and T for aspect term, where an aspect term could tag a phrase of up to 5 consecutive tokens. The tagger was trained using the struc</context>
</contexts>
<marker>Owoputi, OConnor, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>Olutobi Owoputi, Brendan OConnor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A Smith. 2013. Improved part-of-speech tagging for online conversational text with word clusters. In Proceedings of NAACL-HLT, pages 380–390.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Pontiki</author>
<author>Dimitrios Galanis</author>
<author>John Pavlopoulos</author>
<author>Harris Papageorgiou</author>
<author>Ion Androutsopoulos</author>
<author>Suresh Manandhar</author>
</authors>
<title>SemEval-2014 Task 4: Aspect based sentiment analysis.</title>
<date>2014</date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation, SemEval ’14,</booktitle>
<location>Dublin, Ireland,</location>
<contexts>
<context position="2909" citStr="Pontiki et al., 2014" startWordPosition="450" endWordPosition="453">ect category of ‘food’. In Task 4, customer reviews are provided for two domains: restaurants and laptops. A fixed set of five aspect categories is defined for the restaurant domain: food, service, price, ambiance, and anecdotes. Automatic systems are to determine if any of those aspect categories are described in a review. The example sentence above describes the aspect categories of food (positive sentiment) and service (negative sentiment). For the laptop reviews, there is no aspect category detection subtask. Further details of the task and data can be found in the task description paper (Pontiki et al., 2014). We present an in-house sequence tagger to detect aspect terms and supervised classifiers to detect aspect categories, sentiment towards aspect terms, and sentiment towards aspect categories. A summary of the ranks obtained by our submissions to the shared task is provided in Table 1. 2 Lexical Resources 2.1 Unlabeled Reviews Corpora Apart from the training data provided for Task 4, we compiled large corpora of reviews for restaurants and laptops that were not labeled for aspect terms, aspect categories, or sentiment. We generated lexicons from these corpora and used them as a source of addit</context>
</contexts>
<marker>Pontiki, Galanis, Pavlopoulos, Papageorgiou, Androutsopoulos, Manandhar, 2014</marker>
<rawString>Maria Pontiki, Dimitrios Galanis, John Pavlopoulos, Harris Papageorgiou, Ion Androutsopoulos, and Suresh Manandhar. 2014. SemEval-2014 Task 4: Aspect based sentiment analysis. In Proceedings of the International Workshop on Semantic Evaluation, SemEval ’14, Dublin, Ireland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M F Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<tech>Program,</tech>
<pages>3--130</pages>
<contexts>
<context position="15465" citStr="Porter, 1980" startWordPosition="2517" endWordPosition="2518">en sentence. There are 5 pre-defined categories for the restaurant domain: food, price, service, ambience, and anecdotes/miscellaneous. Each sentence can be labeled with one or more categories from the predefined set. No aspect categories were defined for the laptop domain. We addressed the subtask as a multi-class multilabel text classification problem. Five binary onevs-all Support Vector Machine (SVM) classifiers were built, one for each category. The parameter C was optimized through cross-validation separately for each classifier. Sentences were tokenized and stemmed with Porter stemmer (Porter, 1980). Then, the following sets of features were generated for each sentence: ngrams, stemmed ngrams, character ngrams, non-contiguous ngrams, word cluster ngrams, and lexicon features. For the lexicon features, we used the Yelp Restaurant Word– Aspect Association Lexicon and calculated the cumulative scores of all terms appeared in the sentence for each aspect category. Separate scores were calculated for unigram and bigram entries. Sentences with no category assigned by any of the five classifiers went through the post-processing step. For each such sentence, a category c with the maximal posteri</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>M.F. Porter. 1980. An algorithm for suffix stripping. Program, 3:130–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sunita Sarawagi</author>
<author>William W Cohen</author>
</authors>
<title>Semimarkov conditional random fields for information extraction.</title>
<date>2004</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<volume>17</volume>
<pages>1185--1192</pages>
<contexts>
<context position="8590" citStr="Sarawagi and Cohen, 2004" startWordPosition="1364" endWordPosition="1367">2), we generated 1,000 word clusters from the Yelp restaurant reviews corpus. Additionally, we used publicly available 438 word clusters generated from 56 million Englishlanguage tweets (Owoputi et al., 2013). 3 Subtask 1: Aspect Term Extraction The objective of this subtask is to detect aspect terms in sentences. We approached this problem using in-house entity-recognition software, very similar to the system used by de Bruijn et al. (2011) to detect medical concepts. First, sentences were tokenized to split away punctuation, and then the token sequence was tagged using a semi-Markov tagger (Sarawagi and Cohen, 2004). The tagger had two possible tags: O for outside, and T for aspect term, where an aspect term could tag a phrase of up to 5 consecutive tokens. The tagger was trained using the structured PassiveAggressive (PA) algorithm with a maximum stepsize of C = 1 (Crammer et al., 2006). Our features can be divided into two categories: emission and transition features. Emission features couple the tag sequence y to the input w. Most of these work on the token level, and conjoin features of each token with the tag covering that token. If a token is the first or last token covered by a tag, then we produc</context>
</contexts>
<marker>Sarawagi, Cohen, 2004</marker>
<rawString>Sunita Sarawagi and William W Cohen. 2004. Semimarkov conditional random fields for information extraction. In Advances in Neural Information Processing Systems, volume 17, pages 1185–1192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
<author>Michael L Littman</author>
</authors>
<title>Measuring praise and criticism: Inference of semantic orientation from association.</title>
<date>2003</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="5013" citStr="Turney and Littman (2003)" startWordPosition="779" endWordPosition="782">this corpus is marked as reviews for electronic products. We extracted from this subset all reviews that mention either laptop or notebook. We will refer to this collection of 124,712 reviews as the Amazon laptop reviews corpus. Both the Yelp and the Amazon reviews have one to five star ratings associated with each review. We treated the one- and two-star reviews as negative reviews, and the four- and five-star reviews as positive reviews. 2.2 Lexicons Sentiment Lexicons: From the Yelp restaurant reviews corpus, we automatically created an indomain sentiment lexicon for restaurants. Following Turney and Littman (2003) and Mohammad et al. (2013), we calculated a sentiment score for each term w in the corpus: score (w) = PMI (w, pos)−PMI (w, neg) (1) where pos denotes positive reviews and neg denotes negative reviews. PMI stands for pointwise mutual information: ( freq (w, pos) ∗ N PMI w, pos) = log2 freq (w) ∗ freq (pos) (2) where freq (w, pos) is the number of times a term w occurs in positive reviews, freq (w) is the total frequency of term w in the corpus, freq (pos) is the total number of tokens in positive reviews, and N is the total number of tokens in the corpus. PMI (w, neg) was calculated in a simi</context>
</contexts>
<marker>Turney, Littman, 2003</marker>
<rawString>Peter Turney and Michael L Littman. 2003. Measuring praise and criticism: Inference of semantic orientation from association. ACM Transactions on Information Systems, 21(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phraselevel sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05,</booktitle>
<pages>347--354</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7257" citStr="Wilson et al., 2005" startWordPosition="1150" endWordPosition="1153">t two lexicons, Yelp Restaurant Sentiment AffLex and Yelp Restaurant Sentiment NegLex, as described in (Kiritchenko et al., 2014). Similarly, we generated in-domain sentiment lexicons from the Amazon laptop reviews corpus. In addition, we employed existing out-ofdomain sentiment lexicons: (1) large-coverage automatic tweet sentiment lexicons, Hashtag Sentiment lexicons and Sentiment140 lexicons (Kiritchenko et al., 2014), and (2) three manually created sentiment lexicons, NRC Emotion Lexicon (Mohammad and Turney, 2010), Bing Liu’s Lexicon (Hu and Liu, 2004), and the MPQA Subjectivity Lexicon (Wilson et al., 2005). Yelp Restaurant Word–Aspect Association Lexicon: The Yelp restaurant reviews corpus was also used to generate a lexicon of terms associated with the aspect categories of food, price, service, ambiance, and anecdotes. Each sentence of the corpus was labeled with zero, one, or more of the five aspect categories by our aspect category classification system (described in Section 5). Then, for each term w and each category c an association score was calculated as follows: score (w, c) = PMI (w, c) − PMI (w, ¬c) (3) 2.3 Word Clusters Word clusters can provide an alternative representation of text,</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phraselevel sentiment analysis. In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05, pages 347–354, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodan Zhu</author>
<author>Hongyu Guo</author>
<author>Saif M Mohammad</author>
<author>Svetlana Kiritchenko</author>
</authors>
<title>An empirical study on the effect of negation words on sentiment.</title>
<date>2014</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics, ACL ’14.</booktitle>
<contexts>
<context position="6187" citStr="Zhu et al., 2014" startWordPosition="985" endWordPosition="988">rpus. PMI (w, neg) was calculated in a similar way. Since PMI is known to be a poor estimator of association for low-frequency events, we ignored terms that occurred less than five times in each (positive and negative) groups of reviews. 1http://www.yelp.com/dataset_challenge A positive sentiment score indicates a greater overall association with positive sentiment, whereas a negative score indicates a greater association with negative sentiment. The magnitude is indicative of the degree of association. Negation words (e.g., not, never) can significantly affect the sentiment of an expression (Zhu et al., 2014). Therefore, when generating the sentiment lexicons we distinguished terms appearing in negated contexts (defined as text spans between a negation word and a punctuation mark) and affirmative (non-negated) contexts. The sentiment scores were then calculated separately for the two types of contexts. For example, the term good in affirmative contexts has a sentiment score of 1.2 whereas the same term in negated contexts has a score of -1.4. We built two lexicons, Yelp Restaurant Sentiment AffLex and Yelp Restaurant Sentiment NegLex, as described in (Kiritchenko et al., 2014). Similarly, we gener</context>
</contexts>
<marker>Zhu, Guo, Mohammad, Kiritchenko, 2014</marker>
<rawString>Xiaodan Zhu, Hongyu Guo, Saif M. Mohammad, and Svetlana Kiritchenko. 2014. An empirical study on the effect of negation words on sentiment. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, ACL ’14.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>