<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000009">
<title confidence="0.9612255">
Learning Word Reorderings for Hierarchical Phrase-based Statistical
Machine Translation
</title>
<author confidence="0.967501">
Jingyi Zhang&apos;,2, Masao Utiyama&apos;, Eiichro Sumita&apos;, Hai Zhao3,4
</author>
<affiliation confidence="0.986525375">
&apos;National Institute of Information and Communications Technology,
3-5Hikaridai, Keihanna Science City, Kyoto 619-0289, Japan
2Graduate School of Information Science, Nara Institute of Science and Technology,
Takayama, Ikoma, Nara 630-0192, Japan
3Department of Computer Science and Engineering,
Shanghai Jiao Tong University, Shanghai 200240, China
4Key Laboratory of Shanghai Education Commission for Intelligent Interaction
and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai 200240, China
</affiliation>
<email confidence="0.9698765">
jingyizhang/mutiyama/eiichiro.sumita@nict.go.jp
zhaohai@cs.sjtu.edu.cn
</email>
<sectionHeader confidence="0.996531" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999877705882353">
Statistical models for reordering source
words have been used to enhance the hier-
archical phrase-based statistical machine
translation system. Existing word reorder-
ing models learn the reordering for any
two source words in a sentence or only
for two continuous words. This paper pro-
poses a series of separate sub-models to
learn reorderings for word pairs with dif-
ferent distances. Our experiments demon-
strate that reordering sub-models for word
pairs with distance less than a specific
threshold are useful to improve translation
quality. Compared with previous work,
our method may more effectively and effi-
ciently exploit helpful word reordering in-
formation.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999832185185185">
The hierarchical phrase-based model (Chiang,
2005) is capable of capturing rich translation
knowledge with the synchronous context-free
grammar. But selecting proper translation rules
during decoding is a challenge as a huge number
of hierarchical rules can be applied to one source
sentence.
Chiang (2005) used a log-linear model to com-
pute rule weights with features similar to Pharaoh
(Koehn et al., 2003). However, to select appropri-
ate rules, more effective criteria are required. A lot
of work has been done for better rule selection. He
et al. (2008) and Liu et al. (2008) used maximum
entropy approaches to integrate rich contextual in-
formation for target side rule selection. Cui et al.
(2010) proposed a joint model to select hierarchi-
cal rules for both source and target sides.
Hayashi et al. (2010) demonstrated the ef-
fectiveness of using word reordering information
within hierarchical phrase-based SMT by integrat-
ing Tromble and Eisner (2009)’s word reordering
model into decoder as a feature, which estimates
the probability of any two source words in a sen-
tence being reordered during translating. Feng
et al. (2013) proposed a word reordering model
to learn reorderings only for continuous words,
which reduced computation cost a lot compared
with Tromble and Eisner (2009)’s model and still
achieved significant reordering improvement over
the baseline system.
In this paper, we incorporate word reordering
information into hierarchical phrase-based SMT
by training a series of separate reordering sub-
models for word pairs with different distances.
We will demonstrate that the translation perfor-
mance achieves consistent improvement as more
sub-models for longer distance reorderings being
integrated, but the improvement levels off quickly.
That means sub-models for reordering distance
longer than a given threshold do not improve trans-
lation quality significantly. Compared with previ-
ous models (Tromble and Eisner, 2009; Feng et al.,
2013), our method makes full use of helpful word
reordering information and also avoids unneces-
sary computation cost for long distance reorder-
ings. Besides, our reordering model is learned
by feed-forward neural network (FNN) for better
performance and uses efficient caching strategy to
further reduce time cost.
Phrase reordering models have also been inte-
grated into hierarchical phrase-based SMT. Phrase
reordering models were originally developed for
phrase-based SMT (Koehn et al., 2005; Zens and
Ney, 2006; Ni et al., 2009; Li et al., 2014) and
</bodyText>
<page confidence="0.884722">
542
</page>
<bodyText confidence="0.903100571428571">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 542–548,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
could not be used in hierarchical phrase-based
model directly. Nguyen and Vogel (2013) and
Cao et al. (2014) proposed to integrate phrase-
based reordering features into hierarchical phrase-
based SMT. However, their work limited to learn-
ing the reordering of continuous phrases. For short
phrases, in extreme cases, when phrase length is
one, their model only learned reordering for con-
tinuous word pairs like Feng et al. (2013)’s work,
while our model can be applied to word pairs with
longer distances.
</bodyText>
<sectionHeader confidence="0.90423" genericHeader="method">
2 Our Approach
</sectionHeader>
<bodyText confidence="0.95867225">
Let em1 = e1, ... , em be a target translation of
f1l = f1, ... , fl and A be word alignments be-
tween em1 and fl1, our model estimates the reorder-
ing probability of the source sentence as follows:
</bodyText>
<equation confidence="0.99840125">
Pr �fl 1, em 1 , A~
Q Pr (fl1, em1 ,A, i, j) (1)
i,j:1≤i&lt;j≤l,j−i=n
� �
</equation>
<bodyText confidence="0.9976279">
where Pr fl 1, em 1 , A, i, j is the reordering prob-
ability of the word pair (fi, fj) during translat-
ing; N is the maximum distance for source word
reordering, which is empirically determined by
supposing that estimating reorderings longer than
N does not improve translation performance any
more.
Previous word reordering models (Tromble and
Eisner, 2009; Feng et al., 2013) consider the re-
ordering of a source word pair to be reversed or
not. When a source word is aligned to several
uncontinuous target words, it can be hard to de-
termine if a word pair is reversed or not. They
solved this problem by only using one alignment
from multiple alignments and ignoring the others.
In contrast, our model handles all alignments as
shown below.
Suppose that fi is aligned to 7ri (7ri &gt; 0) target
words. When 7ri &gt; 0, {aik|1 &lt; k &lt; 7riI stands for
the positions of target words aligned to fi. If 7ri =
</bodyText>
<equation confidence="0.9938275">
� �
0 or 7rj = 0, Pr fl 1, em 1 , A, i, j = 1, otherwise,
Pr (fl1, em1 , A, i, j)
Pr (oijuv|fi−3, ..., fj+3, eaiu, eajv)
</equation>
<bodyText confidence="0.845572666666667">
where ~ 0 (aiu &lt; ajv)
oijuv = 1 (aiu &gt; ajv) (2)
We train a series of sub-models,
</bodyText>
<equation confidence="0.64202025">
M1,M2,...,MN
Algorithm 1 Extract training instances.
Require: A pair of parallel sentence fl1 and em 1 with word
alignments.
</equation>
<bodyText confidence="0.550683571428571">
Ensure: Training examples for M1, M2, ... ,MN.
for i = 1 to l − 1 do
for j = i + 1 to l do
if j − i &lt; N then
for u = 1 to iri do
for v = 1 to irj do
if aiu &lt; ajv then
</bodyText>
<equation confidence="0.430426333333333">
(fi−3, ..., fj+3, eaiu, eajv, 0) is a neg-
ative instance for Mj−i
else(fi−3, ..., fj+3, eaiu, eajv, 1) is a posi-
</equation>
<bodyText confidence="0.904360333333333">
tive instance for Mj−i
to learn reorderings for word pairs with different
distances. That means, for the word pair (fi, fj)
</bodyText>
<equation confidence="0.908665">
with distance j − i = n, its reordering proba-
� �
</equation>
<bodyText confidence="0.976209769230769">
bility Pr oijuv|fi−3, ..., fj+3, eaiu, eajv is esti-
mated by Mn. Different sub-models are trained
and integrated into the translation system sepa-
rately.
Each sub-model Mn is implemented by an
FNN, which has the same structure with the neu-
ral language model in (Vaswani et al., 2013).
The input to Mn is a sequence of n + 9
words: fi−3, ..., fj+3, eaiu, eajv. The input layer
projects each word into a high dimensional
vector using a matrix of input word embed-
dings. Two hidden layers can combine all in-
put data1. The output layer has two neurons that
</bodyText>
<equation confidence="0.97034325">
� �
give Pr oijuv = 1|fi−3, ..., fj+3, eaiu, eajv and
Pr� �
oijuv = 0|fi−3, ..., fj+3, eaiu, eajv .
</equation>
<figureCaption confidence="0.99927">
Figure 1: A Chinese-English sentence pair.
</figureCaption>
<bodyText confidence="0.99922125">
The backpropagation algorithm is used to train
these reordering sub-models. The training in-
stances for each sub-model are extracted from the
word-aligned parallel corpus according to Algo-
rithm 1. For example, the word pair “4(wears)
93t(guy)” in Figure 1 will be extracted as a pos-
itive instance for M3. The input of this instance is
as follows: “&lt;s&gt; &lt;s&gt; a/11-M RMA 0`J 93t 14
</bodyText>
<footnote confidence="0.496206">
1If we choose the averaged perceptron algorithm to learn
reordering task as used in (Hayashi et al., 2010), we need to
artificially select n-gram features, which is not necessary for
FNN.
</footnote>
<figure confidence="0.763683307692308">
That guy who wears glasses is James
那个
戴 眼镜 的 男生 是 詹姆士
N
� Q
n=1
=
πi
Q
u=1
πj
Q
v=1
</figure>
<page confidence="0.983758">
543
</page>
<bodyText confidence="0.990282">
P :L &lt;/s&gt; wears guy”, where &lt;s&gt; and &lt;/s&gt;
represent the beginning and ending of a sentence.
If a word never occurs or only occurs once in train-
ing corpus, we replace it with a special symbol
&lt;unk&gt;.
</bodyText>
<sectionHeader confidence="0.951077" genericHeader="method">
3 Integration into the Decoder
</sectionHeader>
<bodyText confidence="0.9998625">
In the hierarchical phrase-based model, a transla-
tion rule r is like:
</bodyText>
<equation confidence="0.990214">
X — (-y, α, —)
</equation>
<bodyText confidence="0.994716875">
where X is a nonterminal, -y and α are re-
spectively source and target strings of terminals
and nonterminals, and — is the alignment between
nonterminals and terminals in -y and α.
Each rule has several features and the feature
weights are tuned by the minimum error rate train-
ing (MERT) algorithm (Och, 2003). To integrate
our model into the hierarchical phrase-based trans-
lation system, a new feature scoren (r) is added
to each rule r for each Mn. The score of this fea-
ture is calculated during decoding. Note that these
scores are correspondingly calculated for differ-
ent sub-models Mn and the sub-model weights are
tuned separately.
Suppose that r is applied to the input sentence
fl1, where
</bodyText>
<listItem confidence="0.95702875">
• r covers the source span [fϕ, fϑ]
• -y contains nonterminals {Xk|1 &lt; k &lt; K}
• Xk covers the span [fϕk, fϑk]
Then
</listItem>
<equation confidence="0.816394769230769">
scoren(r)
log Pr �fl1, em1 , A, i, j
where
S : {(i, j) |ϕ &lt; i &lt; j &lt; ϑ}
Sk : {(i, j) |ϕk &lt; i &lt; j &lt; ϑk}
For example, if a rule “X1 X2 X1 guy
X2” is applied to the input sentence in Figure 1,
then
[fw, f s] = [1, 5] ; [fw1, f s1] = [1, 1] ; [fw2, f s2] = [2, 4]
� �
(1, 2) , (1, 3) , (1, 4) , (1, 5) ,
Sk =
(2, 5) , (3, 5) , (4, 5)
</equation>
<bodyText confidence="0.9996316">
One concern in using target features is the com-
putational efficiency, because reordering probabil-
ities have to be calculated during decoding. So we
cache probabilities to reduce the expensive neural
network computation in experiments.
</bodyText>
<sectionHeader confidence="0.999229" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.983926142857143">
We evaluated the proposed approach for Chinese-
to-English (CE) and Japanese-to-English (JE)
translation tasks. The official datasets for the
patent machine translation task at NTCIR-9 (Goto
et al., 2011) were used. The detailed statistics for
training, development and test sets are given in Ta-
ble 1.
</bodyText>
<tableCaption confidence="0.998098">
Table 1: Data sets.
</tableCaption>
<bodyText confidence="0.999969068965517">
In NTCIR-9, the development and test sets were
both provided for CE task while only the test set
was provided for the JE task. Therefore, we used
the sentences from the NTCIR-8 JE test set as the
development set for JE task. The word segmenta-
tion was done by BaseSeg (Zhao et al., 2006; Zhao
and Kit, 2008; Zhao et al., 2010; Zhao and Kit,
2011; Zhao et al., 2013) for Chinese and Mecab2
for Japanese.
To learn neural reordering models, the train-
ing and development sets were put together to ob-
tain symmetric word alignments using GIZA++
(Och and Ney, 2003) and the grow-diag-final-
and heuristic (Koehn et al., 2003). The reorder-
ing instances extracted from the aligned training
and development sets were used as the training
and validation data respectively for learning neu-
ral reordering models. Neural reordering models
were trained by the toolkit NPLM (Vaswani et al.,
2013). For CE task, training instances extracted
from all the 1M sentence pairs were used to train
neural reordering models. For JE task, training
instances were from 1M sentence pairs that were
randomly selected from all the 3.14M sentence
pairs.
We also implemented Hayashi et al. (2010)’s
model for comparison. The training instances for
their model were extracted from the same sentence
pairs as ours.
</bodyText>
<footnote confidence="0.630607">
2http://sourceforge.net/projects/mecab/files/
</footnote>
<figure confidence="0.955982870967742">
SOURCE TARGET
954K
37.2M 40.4M
288K 504K
2K
2K
3.14M
118M 104M
150K 273K
2K
2K
CE
JE
TRAINING #Sents
#Words
#Vocab
DEV #Sents
TEST #Sents
TRAINING #Sents
#Words
#Vocab
DEV #Sents
TEST #Sents
�=
K
U
k=1
Skhj−i=n
(i,j)ES−
K
S _ U1
</figure>
<page confidence="0.663384">
544
</page>
<table confidence="0.94710984">
(a) BLEU scores
CE Base Hayashi M11 M21 M31
model
Hayashi » » » &gt; −
model » » » &gt;
M1 » » »
1 » »
M2 »
1
M3
1
Mi
JE Base Hayashi M11 M21 M31
model
Hayashi » » » » −
model » » » »
M1 » » »
1 » »
M2 »
1
M3
1
M4
1
(b) Significance test results using bootstrap sampling (Koehn,
</table>
<bodyText confidence="0.72014025">
2004) w.r.t. BLEU scores. The symbol » represents a sig-
nificant difference at the P &lt; 0.01 level; &gt; represents a sig-
nificant difference at the P &lt; 0.05 level; − means not signif-
icantly different at P = 0.05.
</bodyText>
<tableCaption confidence="0.977928">
Table 2: Translation results.
</tableCaption>
<bodyText confidence="0.998870769230769">
For each translation task, the recent version
of the Moses hierarchical phrase-based decoder
(Koehn et al., 2007) with the training scripts was
used as the baseline system Base. We used the
default parameters for Moses. A 5-gram language
model was trained on the target side of the training
corpus by IRST LM Toolkit3 with the improved
Kneser-Ney smoothing.
We integrated our reordering models into Base.
Table 2 gives detailed translation results. “Hayashi
model” represents the method of (Hayashi et al.,
2010). “Mj1 (j = 1, 2, 3, 4)” means that Base was
augmented with the reordering scores calcuated
from a series of sub-models M1 to Mj.
As shown in Table 2, integrating only M1,
which predicts reordering for two continuous
source words, has already given BLEU improve-
ment 1.8% and 1.2% over baseline on CE and
JE, respectively. As more sub-models for longer
distance reordering being integrated, the transla-
tion performance improved consistently, though
the improvement leveled off quickly. For CE and
JE tasks, Mn with n &gt; 3 and n &gt; 4, respectively,
cannot give further performance improvement at
any significant level.
Why did the improvement level off quickly?
</bodyText>
<footnote confidence="0.930743">
3http://hlt.fbk.eu/en/irstlm
</footnote>
<table confidence="0.995825111111111">
Sub-model M1 M2 M3 M4
CE 93.9 92.8 92.2 91.2
JE 92.9 91.3 90.1 89.3
(a) Our model
Reordering
Distance 1 2 3 4
CE 90.1 88.3 87.0 85.6
JE 85.3 81.9 80.6 78.8
(b) Hayashi model
</table>
<tableCaption confidence="0.999691">
Table 3: Classification accuracy (%).
</tableCaption>
<bodyText confidence="0.999779425">
In other words, why do long distance reordering
models have a much less leverage over translation
performance than short ones?
First, the prediction accuracy decreases as the
reordering distance increasing. Table 3a gives
classification accuracies on the validation data for
each sub-model. The reason for accuracy decreas-
ing is that the input size of sub-model grows as
reordering distance increasing. Namely, long dis-
tance reordering needs to consider more compli-
cated context.
Second, we attribute the influence decrease of
the longer reordering models to the redundancy of
the predictions among different reordering mod-
els. For example, in Figure 1, when word pairs
“7t(guy) A(is)” and “,,fits) R (James)”
are both predicted to be not reversed, the reorder-
ing for “7t(guy) R��:L(James)” can be logi-
cally determined to be not reversed without further
reordering model prediction. That means, some-
times, a long distance word reordering can be de-
termined by a series of shorter word reordering
pairs.
But still, some predictions for longer reorder-
ing are useful. For example, the reordering
of “4(wears) 7t(guy)” cannot be determined
when “4(wears) RR%glasses)” is predicted to be
not reversed and “RMA(glasses) 7t(guy)” is re-
versed. This is the reason why translation perfor-
mance improves as more sub-models being inte-
grated.
As shown in Table 2, with 4 sub-models be-
ing integrated, our model improved baseline sys-
tem significantly and also outperformed Hayashi
model clearly. It is easy to understand, since our
model was trained by feed-forward neural network
on a high dimensional space and incorporated rich
context information, while Hayashi model used
the averaged perceptron algorithm and simple fea-
tures. Table 3b shows the prediction accuracies
</bodyText>
<table confidence="0.661398285714286">
Base Hayashi M1 M2 M3 M4
1 1 1 1
model
32.95 34.25 34.78 35.75 35.97 36.05
30.13 30.70 31.35 32.07 32.40 32.60
CE
JE
</table>
<page confidence="0.997428">
545
</page>
<bodyText confidence="0.972344754716981">
of Hayashi model. Note that Hayashi model pre-
dicts reorderings for all word pairs, but only pre-
diction accuracies for word pairs with distance 4
or less are shown. Compared with Table 3a, the
prediction accuracy of our model is much higher
than Hayashi model. Actually, FNN is not suitable
for Hayashi model since the computation cost for
Hayashi model is quite expensive. Using FNN to
reorder all word pairs could cost nearly one minute
to translate one sentence according to our experi-
ments, while integrating 4 sub-models only cost
10 seconds4.
Compared with Hayashi model, our model not
only speeds up decoding time but also reduces
the training time. Training for Hayashi model
is much slower since word pairs with all differ-
ent distances are used as training data. By using
separate sub-models, we can train each sub-model
one by one and stop when translation performance
cannot be improved any more. However, despite
of efficiency, one unified model will theoretically
have better performance than separate sub-models
since separate sub-models do not share training in-
stances and the unified model will suffer less from
data sparsity. So, we did some extra experiments
and trained a neural network which had the same
structure as M4 to learn reorderings for all word
pairs with distance 4 or less, instead of using 4
separate neural networks. A specific word null
was used since word pairs with distance 1,2,3 do
not have enough inputs for M4. The significance
test results showed that translation performance
had no significant difference between one unified
model and multiple sub-models. This is because
the training corpus for our model is quite large, so
separate training sets are sufficient for each sub-
model to learn the reorderings well. Besides, us-
ing neural networks to learn these sub-models on
a continuous space can relieve the data sparsity
problem to some extent.
Note that if we only integrate M4 into Base, the
translation quality of Base can be improved in our
preliminary experiments. But M4 cannot predict
reorderings for word pairs with distance less than
4. So M31 will be still needed for predicting re-
orderings of word pairs with distance 1,2,3. But
after M31 being integrated, M4 will not be needed
due to the redundancy of the predictions among
4Note that cache was used in all our experiments to reduce
the expensive neural network computation cost and turned out
to be very useful. Without caching, integrating 4 sub-models
could cost nearly 7 minutes to translate a sentence.
different reordering models.
</bodyText>
<sectionHeader confidence="0.952007" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999968733333333">
In this paper, we propose to enhance hierarchi-
cal phrase-based SMT by training a series of sep-
arate sub-models to learn reorderings for word
pairs with distances less than a specific thresh-
old, based on the experimental fact that longer dis-
tance reordering models are not quite helpful for
translation quality. Compared with Hayashi et al.
(2010)’s work, our model is much more efficient
and keeps all helpful word reordering informa-
tion. Besides, our reordering model is learned by
feed-forward neural network and incorporates rich
context information for better performance. On
both Chinese-to-English and Japanese-to-English
translation tasks, the proposed model outperforms
the previous model significantly.
</bodyText>
<sectionHeader confidence="0.998465" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999894733333333">
Masao Utiyama and Hai Zhao are corresponding
authors. This work was done when the first au-
thor was a master’s student at Shanghai Jiao Tong
University. Hai Zhao was supported by the Na-
tional Natural Science Foundation of China un-
der Grants 60903119, 61170114 and 61272248,
the National Basic Research Program of China un-
der Grant 2013CB329401, the Science and Tech-
nology Commission of Shanghai Municipality un-
der Grant 13511500200, the European Union Sev-
enth Framework Program under Grant 247619, the
Cai Yuanpei Program (CSC fund 201304490199,
201304490171), and the art and science interdisci-
pline funds of Shanghai Jiao Tong University un-
der Grant 14X190040031(14JCRZ04).
</bodyText>
<sectionHeader confidence="0.997677" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997342461538462">
Hailong Cao, Dongdong Zhang, Mu Li, Ming Zhou,
and Tiejun Zhao. 2014. A lexicalized reorder-
ing model for hierarchical phrase-based translation.
In Proceedings of COLING 2014, the 25th Inter-
national Conference on Computational Linguistics:
Technical Papers, pages 1144–1153.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL’05), pages
263–270.
Lei Cui, Dongdong Zhang, Mu Li, Ming Zhou, and
Tiejun Zhao. 2010. A joint rule selection model
</reference>
<page confidence="0.993573">
546
</page>
<reference confidence="0.995253892857143">
for hierarchical phrase-based translation. In Pro-
ceedings of the ACL 2010 Conference Short Papers,
pages 6–11.
Minwei Feng, Jan-Thorsten Peter, and Hermann Ney.
2013. Advancements in reordering models for sta-
tistical machine translation. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
322–332.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K Tsou. 2011. Overview of the patent
machine translation task at the NTCIR-9 workshop.
In Proceedings of The 9th NII Test Collection for IR
Systems Workshop Meeting, pages 559–578.
Katsuhiko Hayashi, Hajime Tsukada, Katsuhito Su-
doh, Kevin Duh, and Seiichi Yamamoto. 2010.
Hierarchical phrase-based machine translation with
word-based reordering model. In Proceedings of the
23rd International Conference on Computational
Linguistics (Coling 2010), pages 439–446.
Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Im-
proving statistical machine translation using lexical-
ized rule selection. In Proceedings of the 22nd In-
ternational Conference on Computational Linguis-
tics (Coling 2008), pages 321–328.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 48–54.
Philipp Koehn, Amittai Axelrod, Alexandra Birch,
Chris Callison-Burch, Miles Osborne, David Tal-
bot, and Michael White. 2005. Edinburgh system
description for the 2005 IWSLT speech translation
evaluation. In The International Workshop on Spo-
ken Language Translation, pages 68–75.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177–180.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004, pages 388–395.
Peng Li, Yang Liu, Maosong Sun, Tatsuya Izuha, and
Dakun Zhang. 2014. A neural reordering model for
phrase-based translation. In Proceedings of COL-
ING 2014, the 25th International Conference on
Computational Linguistics: Technical Papers, pages
1897–1907.
Qun Liu, Zhongjun He, Yang Liu, and Shouxun Lin.
2008. Maximum entropy based rule selection model
for syntax-based statistical machine translation. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages
89–97.
ThuyLinh Nguyen and Stephan Vogel. 2013. Integrat-
ing phrase-based reordering features into a chart-
based decoder for machine translation. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 1587–1596.
Yizhao Ni, Craig Saunders, Sandor Szedmak, and Ma-
hesan Niranjan. 2009. Handling phrase reorderings
for machine translation. In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers, pages 241–
244.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19–51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics, pages 160–167.
Roy Tromble and Jason Eisner. 2009. Learning linear
ordering problems for better translation. In Proceed-
ings of the 2009 Conference on Empirical Methods
in Natural Language Processing, pages 1007–1016.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum,
and David Chiang. 2013. Decoding with large-
scale neural language models improves translation.
In Proceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1387–1392.
Richard Zens and Hermann Ney. 2006. Discriminative
reordering models for statistical machine translation.
In Proceedings on the Workshop on Statistical Ma-
chine Translation, pages 55–63.
Hai Zhao and Chunyu Kit. 2008. Exploiting unla-
beled text with different unsupervised segmentation
criteria for chinese word segmentation. Research in
Computing Science, 33:93–104.
Hai Zhao and Chunyu Kit. 2011. Integrating unsu-
pervised and supervised word segmentation: The
role of goodness measures. Information Sciences,
181(1):163–183.
Hai Zhao, Chang-Ning Huang, and Mu Li. 2006. An
improved chinese word segmentation system with
conditional random field. In Proceedings of the Fifth
SIGHAN Workshop on Chinese Language Process-
ing, pages 162–165.
Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang
Lu. 2010. A unified character-based tagging frame-
work for chinese word segmentation. ACM Trans-
actions on Asian Language Information Processing
(TALIP), 9(2):5.
</reference>
<page confidence="0.975146">
547
</page>
<reference confidence="0.9980086">
Hai Zhao, Masao Utiyama, Eiichiro Sumita, and Bao-
Liang Lu. 2013. An empirical study on word seg-
mentation for chinese machine translation. In Com-
putational Linguistics and Intelligent Text Process-
ing, pages 248–263.
</reference>
<page confidence="0.996256">
548
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.081442">
<title confidence="0.9997155">Learning Word Reorderings for Hierarchical Phrase-based Machine Translation</title>
<author confidence="0.99331">Masao Eiichro Hai</author>
<affiliation confidence="0.993693">Institute of Information and Communications</affiliation>
<address confidence="0.54101">3-5Hikaridai, Keihanna Science City, Kyoto 619-0289,</address>
<affiliation confidence="0.998866">School of Information Science, Nara Institute of Science and</affiliation>
<address confidence="0.882456">Takayama, Ikoma, Nara 630-0192,</address>
<affiliation confidence="0.71468125">of Computer Science and Shanghai Jiao Tong University, Shanghai 200240, Laboratory of Shanghai Education Commission for Intelligent and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai 200240,</affiliation>
<email confidence="0.936221">zhaohai@cs.sjtu.edu.cn</email>
<abstract confidence="0.981481722222222">Statistical models for reordering source words have been used to enhance the hierarchical phrase-based statistical machine translation system. Existing word reordering models learn the reordering for any two source words in a sentence or only for two continuous words. This paper proposes a series of separate sub-models to learn reorderings for word pairs with different distances. Our experiments demonstrate that reordering sub-models for word pairs with distance less than a specific threshold are useful to improve translation quality. Compared with previous work, our method may more effectively and efficiently exploit helpful word reordering information.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Hailong Cao</author>
<author>Dongdong Zhang</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
<author>Tiejun Zhao</author>
</authors>
<title>A lexicalized reordering model for hierarchical phrase-based translation.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,</booktitle>
<pages>1144--1153</pages>
<contexts>
<context position="4312" citStr="Cao et al. (2014)" startWordPosition="619" endWordPosition="622">ordering models have also been integrated into hierarchical phrase-based SMT. Phrase reordering models were originally developed for phrase-based SMT (Koehn et al., 2005; Zens and Ney, 2006; Ni et al., 2009; Li et al., 2014) and 542 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 542–548, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics could not be used in hierarchical phrase-based model directly. Nguyen and Vogel (2013) and Cao et al. (2014) proposed to integrate phrasebased reordering features into hierarchical phrasebased SMT. However, their work limited to learning the reordering of continuous phrases. For short phrases, in extreme cases, when phrase length is one, their model only learned reordering for continuous word pairs like Feng et al. (2013)’s work, while our model can be applied to word pairs with longer distances. 2 Our Approach Let em1 = e1, ... , em be a target translation of f1l = f1, ... , fl and A be word alignments between em1 and fl1, our model estimates the reordering probability of the source sentence as fol</context>
</contexts>
<marker>Cao, Zhang, Li, Zhou, Zhao, 2014</marker>
<rawString>Hailong Cao, Dongdong Zhang, Mu Li, Ming Zhou, and Tiejun Zhao. 2014. A lexicalized reordering model for hierarchical phrase-based translation. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1144–1153.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>263--270</pages>
<contexts>
<context position="1465" citStr="Chiang, 2005" startWordPosition="189" endWordPosition="190">machine translation system. Existing word reordering models learn the reordering for any two source words in a sentence or only for two continuous words. This paper proposes a series of separate sub-models to learn reorderings for word pairs with different distances. Our experiments demonstrate that reordering sub-models for word pairs with distance less than a specific threshold are useful to improve translation quality. Compared with previous work, our method may more effectively and efficiently exploit helpful word reordering information. 1 Introduction The hierarchical phrase-based model (Chiang, 2005) is capable of capturing rich translation knowledge with the synchronous context-free grammar. But selecting proper translation rules during decoding is a challenge as a huge number of hierarchical rules can be applied to one source sentence. Chiang (2005) used a log-linear model to compute rule weights with features similar to Pharaoh (Koehn et al., 2003). However, to select appropriate rules, more effective criteria are required. A lot of work has been done for better rule selection. He et al. (2008) and Liu et al. (2008) used maximum entropy approaches to integrate rich contextual informati</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Cui</author>
<author>Dongdong Zhang</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
<author>Tiejun Zhao</author>
</authors>
<title>A joint rule selection model for hierarchical phrase-based translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers,</booktitle>
<pages>6--11</pages>
<contexts>
<context position="2117" citStr="Cui et al. (2010)" startWordPosition="293" endWordPosition="296">lation knowledge with the synchronous context-free grammar. But selecting proper translation rules during decoding is a challenge as a huge number of hierarchical rules can be applied to one source sentence. Chiang (2005) used a log-linear model to compute rule weights with features similar to Pharaoh (Koehn et al., 2003). However, to select appropriate rules, more effective criteria are required. A lot of work has been done for better rule selection. He et al. (2008) and Liu et al. (2008) used maximum entropy approaches to integrate rich contextual information for target side rule selection. Cui et al. (2010) proposed a joint model to select hierarchical rules for both source and target sides. Hayashi et al. (2010) demonstrated the effectiveness of using word reordering information within hierarchical phrase-based SMT by integrating Tromble and Eisner (2009)’s word reordering model into decoder as a feature, which estimates the probability of any two source words in a sentence being reordered during translating. Feng et al. (2013) proposed a word reordering model to learn reorderings only for continuous words, which reduced computation cost a lot compared with Tromble and Eisner (2009)’s model and</context>
</contexts>
<marker>Cui, Zhang, Li, Zhou, Zhao, 2010</marker>
<rawString>Lei Cui, Dongdong Zhang, Mu Li, Ming Zhou, and Tiejun Zhao. 2010. A joint rule selection model for hierarchical phrase-based translation. In Proceedings of the ACL 2010 Conference Short Papers, pages 6–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minwei Feng</author>
<author>Jan-Thorsten Peter</author>
<author>Hermann Ney</author>
</authors>
<title>Advancements in reordering models for statistical machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>322--332</pages>
<contexts>
<context position="2547" citStr="Feng et al. (2013)" startWordPosition="361" endWordPosition="364">for better rule selection. He et al. (2008) and Liu et al. (2008) used maximum entropy approaches to integrate rich contextual information for target side rule selection. Cui et al. (2010) proposed a joint model to select hierarchical rules for both source and target sides. Hayashi et al. (2010) demonstrated the effectiveness of using word reordering information within hierarchical phrase-based SMT by integrating Tromble and Eisner (2009)’s word reordering model into decoder as a feature, which estimates the probability of any two source words in a sentence being reordered during translating. Feng et al. (2013) proposed a word reordering model to learn reorderings only for continuous words, which reduced computation cost a lot compared with Tromble and Eisner (2009)’s model and still achieved significant reordering improvement over the baseline system. In this paper, we incorporate word reordering information into hierarchical phrase-based SMT by training a series of separate reordering submodels for word pairs with different distances. We will demonstrate that the translation performance achieves consistent improvement as more sub-models for longer distance reorderings being integrated, but the imp</context>
<context position="4629" citStr="Feng et al. (2013)" startWordPosition="669" endWordPosition="672">tics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 542–548, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics could not be used in hierarchical phrase-based model directly. Nguyen and Vogel (2013) and Cao et al. (2014) proposed to integrate phrasebased reordering features into hierarchical phrasebased SMT. However, their work limited to learning the reordering of continuous phrases. For short phrases, in extreme cases, when phrase length is one, their model only learned reordering for continuous word pairs like Feng et al. (2013)’s work, while our model can be applied to word pairs with longer distances. 2 Our Approach Let em1 = e1, ... , em be a target translation of f1l = f1, ... , fl and A be word alignments between em1 and fl1, our model estimates the reordering probability of the source sentence as follows: Pr �fl 1, em 1 , A~ Q Pr (fl1, em1 ,A, i, j) (1) i,j:1≤i&lt;j≤l,j−i=n � � where Pr fl 1, em 1 , A, i, j is the reordering probability of the word pair (fi, fj) during translating; N is the maximum distance for source word reordering, which is empirically determined by supposing that estimating reorderings longer </context>
</contexts>
<marker>Feng, Peter, Ney, 2013</marker>
<rawString>Minwei Feng, Jan-Thorsten Peter, and Hermann Ney. 2013. Advancements in reordering models for statistical machine translation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 322–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isao Goto</author>
<author>Bin Lu</author>
<author>Ka Po Chow</author>
<author>Eiichiro Sumita</author>
<author>Benjamin K Tsou</author>
</authors>
<title>Overview of the patent machine translation task at the NTCIR-9 workshop.</title>
<date>2011</date>
<booktitle>In Proceedings of The 9th NII Test Collection for IR Systems Workshop Meeting,</booktitle>
<pages>559--578</pages>
<contexts>
<context position="9914" citStr="Goto et al., 2011" startWordPosition="1699" endWordPosition="1702">nput sentence in Figure 1, then [fw, f s] = [1, 5] ; [fw1, f s1] = [1, 1] ; [fw2, f s2] = [2, 4] � � (1, 2) , (1, 3) , (1, 4) , (1, 5) , Sk = (2, 5) , (3, 5) , (4, 5) One concern in using target features is the computational efficiency, because reordering probabilities have to be calculated during decoding. So we cache probabilities to reduce the expensive neural network computation in experiments. 4 Experiments We evaluated the proposed approach for Chineseto-English (CE) and Japanese-to-English (JE) translation tasks. The official datasets for the patent machine translation task at NTCIR-9 (Goto et al., 2011) were used. The detailed statistics for training, development and test sets are given in Table 1. Table 1: Data sets. In NTCIR-9, the development and test sets were both provided for CE task while only the test set was provided for the JE task. Therefore, we used the sentences from the NTCIR-8 JE test set as the development set for JE task. The word segmentation was done by BaseSeg (Zhao et al., 2006; Zhao and Kit, 2008; Zhao et al., 2010; Zhao and Kit, 2011; Zhao et al., 2013) for Chinese and Mecab2 for Japanese. To learn neural reordering models, the training and development sets were put to</context>
</contexts>
<marker>Goto, Lu, Chow, Sumita, Tsou, 2011</marker>
<rawString>Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and Benjamin K Tsou. 2011. Overview of the patent machine translation task at the NTCIR-9 workshop. In Proceedings of The 9th NII Test Collection for IR Systems Workshop Meeting, pages 559–578.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katsuhiko Hayashi</author>
<author>Hajime Tsukada</author>
<author>Katsuhito Sudoh</author>
<author>Kevin Duh</author>
<author>Seiichi Yamamoto</author>
</authors>
<title>Hierarchical phrase-based machine translation with word-based reordering model.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>439--446</pages>
<contexts>
<context position="2225" citStr="Hayashi et al. (2010)" startWordPosition="312" endWordPosition="315">g decoding is a challenge as a huge number of hierarchical rules can be applied to one source sentence. Chiang (2005) used a log-linear model to compute rule weights with features similar to Pharaoh (Koehn et al., 2003). However, to select appropriate rules, more effective criteria are required. A lot of work has been done for better rule selection. He et al. (2008) and Liu et al. (2008) used maximum entropy approaches to integrate rich contextual information for target side rule selection. Cui et al. (2010) proposed a joint model to select hierarchical rules for both source and target sides. Hayashi et al. (2010) demonstrated the effectiveness of using word reordering information within hierarchical phrase-based SMT by integrating Tromble and Eisner (2009)’s word reordering model into decoder as a feature, which estimates the probability of any two source words in a sentence being reordered during translating. Feng et al. (2013) proposed a word reordering model to learn reorderings only for continuous words, which reduced computation cost a lot compared with Tromble and Eisner (2009)’s model and still achieved significant reordering improvement over the baseline system. In this paper, we incorporate w</context>
<context position="7839" citStr="Hayashi et al., 2010" startWordPosition="1295" endWordPosition="1298">ive Pr oijuv = 1|fi−3, ..., fj+3, eaiu, eajv and Pr� � oijuv = 0|fi−3, ..., fj+3, eaiu, eajv . Figure 1: A Chinese-English sentence pair. The backpropagation algorithm is used to train these reordering sub-models. The training instances for each sub-model are extracted from the word-aligned parallel corpus according to Algorithm 1. For example, the word pair “4(wears) 93t(guy)” in Figure 1 will be extracted as a positive instance for M3. The input of this instance is as follows: “&lt;s&gt; &lt;s&gt; a/11-M RMA 0`J 93t 14 1If we choose the averaged perceptron algorithm to learn reordering task as used in (Hayashi et al., 2010), we need to artificially select n-gram features, which is not necessary for FNN. That guy who wears glasses is James 那个 戴 眼镜 的 男生 是 詹姆士 N � Q n=1 = πi Q u=1 πj Q v=1 543 P :L &lt;/s&gt; wears guy”, where &lt;s&gt; and &lt;/s&gt; represent the beginning and ending of a sentence. If a word never occurs or only occurs once in training corpus, we replace it with a special symbol &lt;unk&gt;. 3 Integration into the Decoder In the hierarchical phrase-based model, a translation rule r is like: X — (-y, α, —) where X is a nonterminal, -y and α are respectively source and target strings of terminals and nonterminals, and — i</context>
<context position="11193" citStr="Hayashi et al. (2010)" startWordPosition="1917" endWordPosition="1920">ch and Ney, 2003) and the grow-diag-finaland heuristic (Koehn et al., 2003). The reordering instances extracted from the aligned training and development sets were used as the training and validation data respectively for learning neural reordering models. Neural reordering models were trained by the toolkit NPLM (Vaswani et al., 2013). For CE task, training instances extracted from all the 1M sentence pairs were used to train neural reordering models. For JE task, training instances were from 1M sentence pairs that were randomly selected from all the 3.14M sentence pairs. We also implemented Hayashi et al. (2010)’s model for comparison. The training instances for their model were extracted from the same sentence pairs as ours. 2http://sourceforge.net/projects/mecab/files/ SOURCE TARGET 954K 37.2M 40.4M 288K 504K 2K 2K 3.14M 118M 104M 150K 273K 2K 2K CE JE TRAINING #Sents #Words #Vocab DEV #Sents TEST #Sents TRAINING #Sents #Words #Vocab DEV #Sents TEST #Sents �= K U k=1 Skhj−i=n (i,j)ES− K S _ U1 544 (a) BLEU scores CE Base Hayashi M11 M21 M31 model Hayashi » » » &gt; − model » » » &gt; M1 » » » 1 » » M2 » 1 M3 1 Mi JE Base Hayashi M11 M21 M31 model Hayashi » » » » − model » » » » M1 » » » 1 » » M2 » 1 M3 1</context>
<context position="12612" citStr="Hayashi et al., 2010" startWordPosition="2180" endWordPosition="2183">erence at the P &lt; 0.05 level; − means not significantly different at P = 0.05. Table 2: Translation results. For each translation task, the recent version of the Moses hierarchical phrase-based decoder (Koehn et al., 2007) with the training scripts was used as the baseline system Base. We used the default parameters for Moses. A 5-gram language model was trained on the target side of the training corpus by IRST LM Toolkit3 with the improved Kneser-Ney smoothing. We integrated our reordering models into Base. Table 2 gives detailed translation results. “Hayashi model” represents the method of (Hayashi et al., 2010). “Mj1 (j = 1, 2, 3, 4)” means that Base was augmented with the reordering scores calcuated from a series of sub-models M1 to Mj. As shown in Table 2, integrating only M1, which predicts reordering for two continuous source words, has already given BLEU improvement 1.8% and 1.2% over baseline on CE and JE, respectively. As more sub-models for longer distance reordering being integrated, the translation performance improved consistently, though the improvement leveled off quickly. For CE and JE tasks, Mn with n &gt; 3 and n &gt; 4, respectively, cannot give further performance improvement at any sign</context>
<context position="18289" citStr="Hayashi et al. (2010)" startWordPosition="3108" endWordPosition="3111">Note that cache was used in all our experiments to reduce the expensive neural network computation cost and turned out to be very useful. Without caching, integrating 4 sub-models could cost nearly 7 minutes to translate a sentence. different reordering models. 5 Conclusion In this paper, we propose to enhance hierarchical phrase-based SMT by training a series of separate sub-models to learn reorderings for word pairs with distances less than a specific threshold, based on the experimental fact that longer distance reordering models are not quite helpful for translation quality. Compared with Hayashi et al. (2010)’s work, our model is much more efficient and keeps all helpful word reordering information. Besides, our reordering model is learned by feed-forward neural network and incorporates rich context information for better performance. On both Chinese-to-English and Japanese-to-English translation tasks, the proposed model outperforms the previous model significantly. Acknowledgments Masao Utiyama and Hai Zhao are corresponding authors. This work was done when the first author was a master’s student at Shanghai Jiao Tong University. Hai Zhao was supported by the National Natural Science Foundation </context>
</contexts>
<marker>Hayashi, Tsukada, Sudoh, Duh, Yamamoto, 2010</marker>
<rawString>Katsuhiko Hayashi, Hajime Tsukada, Katsuhito Sudoh, Kevin Duh, and Seiichi Yamamoto. 2010. Hierarchical phrase-based machine translation with word-based reordering model. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 439–446.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongjun He</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Improving statistical machine translation using lexicalized rule selection.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>321--328</pages>
<contexts>
<context position="1972" citStr="He et al. (2008)" startWordPosition="269" endWordPosition="272">xploit helpful word reordering information. 1 Introduction The hierarchical phrase-based model (Chiang, 2005) is capable of capturing rich translation knowledge with the synchronous context-free grammar. But selecting proper translation rules during decoding is a challenge as a huge number of hierarchical rules can be applied to one source sentence. Chiang (2005) used a log-linear model to compute rule weights with features similar to Pharaoh (Koehn et al., 2003). However, to select appropriate rules, more effective criteria are required. A lot of work has been done for better rule selection. He et al. (2008) and Liu et al. (2008) used maximum entropy approaches to integrate rich contextual information for target side rule selection. Cui et al. (2010) proposed a joint model to select hierarchical rules for both source and target sides. Hayashi et al. (2010) demonstrated the effectiveness of using word reordering information within hierarchical phrase-based SMT by integrating Tromble and Eisner (2009)’s word reordering model into decoder as a feature, which estimates the probability of any two source words in a sentence being reordered during translating. Feng et al. (2013) proposed a word reorderi</context>
</contexts>
<marker>He, Liu, Lin, 2008</marker>
<rawString>Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Improving statistical machine translation using lexicalized rule selection. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 321–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume</booktitle>
<volume>1</volume>
<pages>48--54</pages>
<contexts>
<context position="1823" citStr="Koehn et al., 2003" startWordPosition="243" endWordPosition="246">less than a specific threshold are useful to improve translation quality. Compared with previous work, our method may more effectively and efficiently exploit helpful word reordering information. 1 Introduction The hierarchical phrase-based model (Chiang, 2005) is capable of capturing rich translation knowledge with the synchronous context-free grammar. But selecting proper translation rules during decoding is a challenge as a huge number of hierarchical rules can be applied to one source sentence. Chiang (2005) used a log-linear model to compute rule weights with features similar to Pharaoh (Koehn et al., 2003). However, to select appropriate rules, more effective criteria are required. A lot of work has been done for better rule selection. He et al. (2008) and Liu et al. (2008) used maximum entropy approaches to integrate rich contextual information for target side rule selection. Cui et al. (2010) proposed a joint model to select hierarchical rules for both source and target sides. Hayashi et al. (2010) demonstrated the effectiveness of using word reordering information within hierarchical phrase-based SMT by integrating Tromble and Eisner (2009)’s word reordering model into decoder as a feature, </context>
<context position="10647" citStr="Koehn et al., 2003" startWordPosition="1831" endWordPosition="1834">ts. In NTCIR-9, the development and test sets were both provided for CE task while only the test set was provided for the JE task. Therefore, we used the sentences from the NTCIR-8 JE test set as the development set for JE task. The word segmentation was done by BaseSeg (Zhao et al., 2006; Zhao and Kit, 2008; Zhao et al., 2010; Zhao and Kit, 2011; Zhao et al., 2013) for Chinese and Mecab2 for Japanese. To learn neural reordering models, the training and development sets were put together to obtain symmetric word alignments using GIZA++ (Och and Ney, 2003) and the grow-diag-finaland heuristic (Koehn et al., 2003). The reordering instances extracted from the aligned training and development sets were used as the training and validation data respectively for learning neural reordering models. Neural reordering models were trained by the toolkit NPLM (Vaswani et al., 2013). For CE task, training instances extracted from all the 1M sentence pairs were used to train neural reordering models. For JE task, training instances were from 1M sentence pairs that were randomly selected from all the 3.14M sentence pairs. We also implemented Hayashi et al. (2010)’s model for comparison. The training instances for th</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume 1, pages 48–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Amittai Axelrod</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>David Talbot</author>
<author>Michael White</author>
</authors>
<title>Edinburgh system description for the 2005 IWSLT speech translation evaluation.</title>
<date>2005</date>
<booktitle>In The International Workshop on Spoken Language Translation,</booktitle>
<pages>68--75</pages>
<contexts>
<context position="3864" citStr="Koehn et al., 2005" startWordPosition="551" endWordPosition="554">shold do not improve translation quality significantly. Compared with previous models (Tromble and Eisner, 2009; Feng et al., 2013), our method makes full use of helpful word reordering information and also avoids unnecessary computation cost for long distance reorderings. Besides, our reordering model is learned by feed-forward neural network (FNN) for better performance and uses efficient caching strategy to further reduce time cost. Phrase reordering models have also been integrated into hierarchical phrase-based SMT. Phrase reordering models were originally developed for phrase-based SMT (Koehn et al., 2005; Zens and Ney, 2006; Ni et al., 2009; Li et al., 2014) and 542 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 542–548, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics could not be used in hierarchical phrase-based model directly. Nguyen and Vogel (2013) and Cao et al. (2014) proposed to integrate phrasebased reordering features into hierarchical phrasebased SMT. However, their work limited to learning the reordering of cont</context>
</contexts>
<marker>Koehn, Axelrod, Birch, Callison-Burch, Osborne, Talbot, White, 2005</marker>
<rawString>Philipp Koehn, Amittai Axelrod, Alexandra Birch, Chris Callison-Burch, Miles Osborne, David Talbot, and Michael White. 2005. Edinburgh system description for the 2005 IWSLT speech translation evaluation. In The International Workshop on Spoken Language Translation, pages 68–75.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<contexts>
<context position="12213" citStr="Koehn et al., 2007" startWordPosition="2116" endWordPosition="2119">LEU scores CE Base Hayashi M11 M21 M31 model Hayashi » » » &gt; − model » » » &gt; M1 » » » 1 » » M2 » 1 M3 1 Mi JE Base Hayashi M11 M21 M31 model Hayashi » » » » − model » » » » M1 » » » 1 » » M2 » 1 M3 1 M4 1 (b) Significance test results using bootstrap sampling (Koehn, 2004) w.r.t. BLEU scores. The symbol » represents a significant difference at the P &lt; 0.01 level; &gt; represents a significant difference at the P &lt; 0.05 level; − means not significantly different at P = 0.05. Table 2: Translation results. For each translation task, the recent version of the Moses hierarchical phrase-based decoder (Koehn et al., 2007) with the training scripts was used as the baseline system Base. We used the default parameters for Moses. A 5-gram language model was trained on the target side of the training corpus by IRST LM Toolkit3 with the improved Kneser-Ney smoothing. We integrated our reordering models into Base. Table 2 gives detailed translation results. “Hayashi model” represents the method of (Hayashi et al., 2010). “Mj1 (j = 1, 2, 3, 4)” means that Base was augmented with the reordering scores calcuated from a series of sub-models M1 to Mj. As shown in Table 2, integrating only M1, which predicts reordering for</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>388--395</pages>
<contexts>
<context position="11867" citStr="Koehn, 2004" startWordPosition="2058" endWordPosition="2059">l were extracted from the same sentence pairs as ours. 2http://sourceforge.net/projects/mecab/files/ SOURCE TARGET 954K 37.2M 40.4M 288K 504K 2K 2K 3.14M 118M 104M 150K 273K 2K 2K CE JE TRAINING #Sents #Words #Vocab DEV #Sents TEST #Sents TRAINING #Sents #Words #Vocab DEV #Sents TEST #Sents �= K U k=1 Skhj−i=n (i,j)ES− K S _ U1 544 (a) BLEU scores CE Base Hayashi M11 M21 M31 model Hayashi » » » &gt; − model » » » &gt; M1 » » » 1 » » M2 » 1 M3 1 Mi JE Base Hayashi M11 M21 M31 model Hayashi » » » » − model » » » » M1 » » » 1 » » M2 » 1 M3 1 M4 1 (b) Significance test results using bootstrap sampling (Koehn, 2004) w.r.t. BLEU scores. The symbol » represents a significant difference at the P &lt; 0.01 level; &gt; represents a significant difference at the P &lt; 0.05 level; − means not significantly different at P = 0.05. Table 2: Translation results. For each translation task, the recent version of the Moses hierarchical phrase-based decoder (Koehn et al., 2007) with the training scripts was used as the baseline system Base. We used the default parameters for Moses. A 5-gram language model was trained on the target side of the training corpus by IRST LM Toolkit3 with the improved Kneser-Ney smoothing. We integr</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of EMNLP 2004, pages 388–395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Li</author>
<author>Yang Liu</author>
<author>Maosong Sun</author>
<author>Tatsuya Izuha</author>
<author>Dakun Zhang</author>
</authors>
<title>A neural reordering model for phrase-based translation.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,</booktitle>
<pages>1897--1907</pages>
<contexts>
<context position="3919" citStr="Li et al., 2014" startWordPosition="563" endWordPosition="566">ompared with previous models (Tromble and Eisner, 2009; Feng et al., 2013), our method makes full use of helpful word reordering information and also avoids unnecessary computation cost for long distance reorderings. Besides, our reordering model is learned by feed-forward neural network (FNN) for better performance and uses efficient caching strategy to further reduce time cost. Phrase reordering models have also been integrated into hierarchical phrase-based SMT. Phrase reordering models were originally developed for phrase-based SMT (Koehn et al., 2005; Zens and Ney, 2006; Ni et al., 2009; Li et al., 2014) and 542 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 542–548, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics could not be used in hierarchical phrase-based model directly. Nguyen and Vogel (2013) and Cao et al. (2014) proposed to integrate phrasebased reordering features into hierarchical phrasebased SMT. However, their work limited to learning the reordering of continuous phrases. For short phrases, in extreme cases, wh</context>
</contexts>
<marker>Li, Liu, Sun, Izuha, Zhang, 2014</marker>
<rawString>Peng Li, Yang Liu, Maosong Sun, Tatsuya Izuha, and Dakun Zhang. 2014. A neural reordering model for phrase-based translation. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1897–1907.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qun Liu</author>
<author>Zhongjun He</author>
<author>Yang Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Maximum entropy based rule selection model for syntax-based statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>89--97</pages>
<contexts>
<context position="1994" citStr="Liu et al. (2008)" startWordPosition="274" endWordPosition="277">eordering information. 1 Introduction The hierarchical phrase-based model (Chiang, 2005) is capable of capturing rich translation knowledge with the synchronous context-free grammar. But selecting proper translation rules during decoding is a challenge as a huge number of hierarchical rules can be applied to one source sentence. Chiang (2005) used a log-linear model to compute rule weights with features similar to Pharaoh (Koehn et al., 2003). However, to select appropriate rules, more effective criteria are required. A lot of work has been done for better rule selection. He et al. (2008) and Liu et al. (2008) used maximum entropy approaches to integrate rich contextual information for target side rule selection. Cui et al. (2010) proposed a joint model to select hierarchical rules for both source and target sides. Hayashi et al. (2010) demonstrated the effectiveness of using word reordering information within hierarchical phrase-based SMT by integrating Tromble and Eisner (2009)’s word reordering model into decoder as a feature, which estimates the probability of any two source words in a sentence being reordered during translating. Feng et al. (2013) proposed a word reordering model to learn reor</context>
</contexts>
<marker>Liu, He, Liu, Lin, 2008</marker>
<rawString>Qun Liu, Zhongjun He, Yang Liu, and Shouxun Lin. 2008. Maximum entropy based rule selection model for syntax-based statistical machine translation. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 89–97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ThuyLinh Nguyen</author>
<author>Stephan Vogel</author>
</authors>
<title>Integrating phrase-based reordering features into a chartbased decoder for machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1587--1596</pages>
<contexts>
<context position="4290" citStr="Nguyen and Vogel (2013)" startWordPosition="614" endWordPosition="617"> reduce time cost. Phrase reordering models have also been integrated into hierarchical phrase-based SMT. Phrase reordering models were originally developed for phrase-based SMT (Koehn et al., 2005; Zens and Ney, 2006; Ni et al., 2009; Li et al., 2014) and 542 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 542–548, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics could not be used in hierarchical phrase-based model directly. Nguyen and Vogel (2013) and Cao et al. (2014) proposed to integrate phrasebased reordering features into hierarchical phrasebased SMT. However, their work limited to learning the reordering of continuous phrases. For short phrases, in extreme cases, when phrase length is one, their model only learned reordering for continuous word pairs like Feng et al. (2013)’s work, while our model can be applied to word pairs with longer distances. 2 Our Approach Let em1 = e1, ... , em be a target translation of f1l = f1, ... , fl and A be word alignments between em1 and fl1, our model estimates the reordering probability of the </context>
</contexts>
<marker>Nguyen, Vogel, 2013</marker>
<rawString>ThuyLinh Nguyen and Stephan Vogel. 2013. Integrating phrase-based reordering features into a chartbased decoder for machine translation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1587–1596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yizhao Ni</author>
<author>Craig Saunders</author>
<author>Sandor Szedmak</author>
<author>Mahesan Niranjan</author>
</authors>
<title>Handling phrase reorderings for machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACLIJCNLP 2009 Conference Short Papers,</booktitle>
<pages>241--244</pages>
<contexts>
<context position="3901" citStr="Ni et al., 2009" startWordPosition="559" endWordPosition="562"> significantly. Compared with previous models (Tromble and Eisner, 2009; Feng et al., 2013), our method makes full use of helpful word reordering information and also avoids unnecessary computation cost for long distance reorderings. Besides, our reordering model is learned by feed-forward neural network (FNN) for better performance and uses efficient caching strategy to further reduce time cost. Phrase reordering models have also been integrated into hierarchical phrase-based SMT. Phrase reordering models were originally developed for phrase-based SMT (Koehn et al., 2005; Zens and Ney, 2006; Ni et al., 2009; Li et al., 2014) and 542 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 542–548, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics could not be used in hierarchical phrase-based model directly. Nguyen and Vogel (2013) and Cao et al. (2014) proposed to integrate phrasebased reordering features into hierarchical phrasebased SMT. However, their work limited to learning the reordering of continuous phrases. For short phrases, in</context>
</contexts>
<marker>Ni, Saunders, Szedmak, Niranjan, 2009</marker>
<rawString>Yizhao Ni, Craig Saunders, Sandor Szedmak, and Mahesan Niranjan. 2009. Handling phrase reorderings for machine translation. In Proceedings of the ACLIJCNLP 2009 Conference Short Papers, pages 241– 244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational linguistics,</journal>
<pages>29--1</pages>
<contexts>
<context position="10589" citStr="Och and Ney, 2003" startWordPosition="1822" endWordPosition="1825">ment and test sets are given in Table 1. Table 1: Data sets. In NTCIR-9, the development and test sets were both provided for CE task while only the test set was provided for the JE task. Therefore, we used the sentences from the NTCIR-8 JE test set as the development set for JE task. The word segmentation was done by BaseSeg (Zhao et al., 2006; Zhao and Kit, 2008; Zhao et al., 2010; Zhao and Kit, 2011; Zhao et al., 2013) for Chinese and Mecab2 for Japanese. To learn neural reordering models, the training and development sets were put together to obtain symmetric word alignments using GIZA++ (Och and Ney, 2003) and the grow-diag-finaland heuristic (Koehn et al., 2003). The reordering instances extracted from the aligned training and development sets were used as the training and validation data respectively for learning neural reordering models. Neural reordering models were trained by the toolkit NPLM (Vaswani et al., 2013). For CE task, training instances extracted from all the 1M sentence pairs were used to train neural reordering models. For JE task, training instances were from 1M sentence pairs that were randomly selected from all the 3.14M sentence pairs. We also implemented Hayashi et al. (2</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="8631" citStr="Och, 2003" startWordPosition="1450" endWordPosition="1451">ears guy”, where &lt;s&gt; and &lt;/s&gt; represent the beginning and ending of a sentence. If a word never occurs or only occurs once in training corpus, we replace it with a special symbol &lt;unk&gt;. 3 Integration into the Decoder In the hierarchical phrase-based model, a translation rule r is like: X — (-y, α, —) where X is a nonterminal, -y and α are respectively source and target strings of terminals and nonterminals, and — is the alignment between nonterminals and terminals in -y and α. Each rule has several features and the feature weights are tuned by the minimum error rate training (MERT) algorithm (Och, 2003). To integrate our model into the hierarchical phrase-based translation system, a new feature scoren (r) is added to each rule r for each Mn. The score of this feature is calculated during decoding. Note that these scores are correspondingly calculated for different sub-models Mn and the sub-model weights are tuned separately. Suppose that r is applied to the input sentence fl1, where • r covers the source span [fϕ, fϑ] • -y contains nonterminals {Xk|1 &lt; k &lt; K} • Xk covers the span [fϕk, fϑk] Then scoren(r) log Pr �fl1, em1 , A, i, j where S : {(i, j) |ϕ &lt; i &lt; j &lt; ϑ} Sk : {(i, j) |ϕk &lt; i &lt; j &lt;</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Tromble</author>
<author>Jason Eisner</author>
</authors>
<title>Learning linear ordering problems for better translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1007--1016</pages>
<contexts>
<context position="2371" citStr="Tromble and Eisner (2009)" startWordPosition="332" endWordPosition="335">to compute rule weights with features similar to Pharaoh (Koehn et al., 2003). However, to select appropriate rules, more effective criteria are required. A lot of work has been done for better rule selection. He et al. (2008) and Liu et al. (2008) used maximum entropy approaches to integrate rich contextual information for target side rule selection. Cui et al. (2010) proposed a joint model to select hierarchical rules for both source and target sides. Hayashi et al. (2010) demonstrated the effectiveness of using word reordering information within hierarchical phrase-based SMT by integrating Tromble and Eisner (2009)’s word reordering model into decoder as a feature, which estimates the probability of any two source words in a sentence being reordered during translating. Feng et al. (2013) proposed a word reordering model to learn reorderings only for continuous words, which reduced computation cost a lot compared with Tromble and Eisner (2009)’s model and still achieved significant reordering improvement over the baseline system. In this paper, we incorporate word reordering information into hierarchical phrase-based SMT by training a series of separate reordering submodels for word pairs with different </context>
<context position="5344" citStr="Tromble and Eisner, 2009" startWordPosition="804" endWordPosition="807"> Let em1 = e1, ... , em be a target translation of f1l = f1, ... , fl and A be word alignments between em1 and fl1, our model estimates the reordering probability of the source sentence as follows: Pr �fl 1, em 1 , A~ Q Pr (fl1, em1 ,A, i, j) (1) i,j:1≤i&lt;j≤l,j−i=n � � where Pr fl 1, em 1 , A, i, j is the reordering probability of the word pair (fi, fj) during translating; N is the maximum distance for source word reordering, which is empirically determined by supposing that estimating reorderings longer than N does not improve translation performance any more. Previous word reordering models (Tromble and Eisner, 2009; Feng et al., 2013) consider the reordering of a source word pair to be reversed or not. When a source word is aligned to several uncontinuous target words, it can be hard to determine if a word pair is reversed or not. They solved this problem by only using one alignment from multiple alignments and ignoring the others. In contrast, our model handles all alignments as shown below. Suppose that fi is aligned to 7ri (7ri &gt; 0) target words. When 7ri &gt; 0, {aik|1 &lt; k &lt; 7riI stands for the positions of target words aligned to fi. If 7ri = � � 0 or 7rj = 0, Pr fl 1, em 1 , A, i, j = 1, otherwise, P</context>
</contexts>
<marker>Tromble, Eisner, 2009</marker>
<rawString>Roy Tromble and Jason Eisner. 2009. Learning linear ordering problems for better translation. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1007–1016.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Vaswani</author>
<author>Yinggong Zhao</author>
<author>Victoria Fossum</author>
<author>David Chiang</author>
</authors>
<title>Decoding with largescale neural language models improves translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1387--1392</pages>
<contexts>
<context position="6944" citStr="Vaswani et al., 2013" startWordPosition="1133" endWordPosition="1136">r u = 1 to iri do for v = 1 to irj do if aiu &lt; ajv then (fi−3, ..., fj+3, eaiu, eajv, 0) is a negative instance for Mj−i else(fi−3, ..., fj+3, eaiu, eajv, 1) is a positive instance for Mj−i to learn reorderings for word pairs with different distances. That means, for the word pair (fi, fj) with distance j − i = n, its reordering proba� � bility Pr oijuv|fi−3, ..., fj+3, eaiu, eajv is estimated by Mn. Different sub-models are trained and integrated into the translation system separately. Each sub-model Mn is implemented by an FNN, which has the same structure with the neural language model in (Vaswani et al., 2013). The input to Mn is a sequence of n + 9 words: fi−3, ..., fj+3, eaiu, eajv. The input layer projects each word into a high dimensional vector using a matrix of input word embeddings. Two hidden layers can combine all input data1. The output layer has two neurons that � � give Pr oijuv = 1|fi−3, ..., fj+3, eaiu, eajv and Pr� � oijuv = 0|fi−3, ..., fj+3, eaiu, eajv . Figure 1: A Chinese-English sentence pair. The backpropagation algorithm is used to train these reordering sub-models. The training instances for each sub-model are extracted from the word-aligned parallel corpus according to Algor</context>
<context position="10909" citStr="Vaswani et al., 2013" startWordPosition="1871" endWordPosition="1874">by BaseSeg (Zhao et al., 2006; Zhao and Kit, 2008; Zhao et al., 2010; Zhao and Kit, 2011; Zhao et al., 2013) for Chinese and Mecab2 for Japanese. To learn neural reordering models, the training and development sets were put together to obtain symmetric word alignments using GIZA++ (Och and Ney, 2003) and the grow-diag-finaland heuristic (Koehn et al., 2003). The reordering instances extracted from the aligned training and development sets were used as the training and validation data respectively for learning neural reordering models. Neural reordering models were trained by the toolkit NPLM (Vaswani et al., 2013). For CE task, training instances extracted from all the 1M sentence pairs were used to train neural reordering models. For JE task, training instances were from 1M sentence pairs that were randomly selected from all the 3.14M sentence pairs. We also implemented Hayashi et al. (2010)’s model for comparison. The training instances for their model were extracted from the same sentence pairs as ours. 2http://sourceforge.net/projects/mecab/files/ SOURCE TARGET 954K 37.2M 40.4M 288K 504K 2K 2K 3.14M 118M 104M 150K 273K 2K 2K CE JE TRAINING #Sents #Words #Vocab DEV #Sents TEST #Sents TRAINING #Sents</context>
</contexts>
<marker>Vaswani, Zhao, Fossum, Chiang, 2013</marker>
<rawString>Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and David Chiang. 2013. Decoding with largescale neural language models improves translation. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1387–1392.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative reordering models for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings on the Workshop on Statistical Machine Translation,</booktitle>
<pages>55--63</pages>
<contexts>
<context position="3884" citStr="Zens and Ney, 2006" startWordPosition="555" endWordPosition="558"> translation quality significantly. Compared with previous models (Tromble and Eisner, 2009; Feng et al., 2013), our method makes full use of helpful word reordering information and also avoids unnecessary computation cost for long distance reorderings. Besides, our reordering model is learned by feed-forward neural network (FNN) for better performance and uses efficient caching strategy to further reduce time cost. Phrase reordering models have also been integrated into hierarchical phrase-based SMT. Phrase reordering models were originally developed for phrase-based SMT (Koehn et al., 2005; Zens and Ney, 2006; Ni et al., 2009; Li et al., 2014) and 542 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 542–548, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics could not be used in hierarchical phrase-based model directly. Nguyen and Vogel (2013) and Cao et al. (2014) proposed to integrate phrasebased reordering features into hierarchical phrasebased SMT. However, their work limited to learning the reordering of continuous phrases. For </context>
</contexts>
<marker>Zens, Ney, 2006</marker>
<rawString>Richard Zens and Hermann Ney. 2006. Discriminative reordering models for statistical machine translation. In Proceedings on the Workshop on Statistical Machine Translation, pages 55–63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chunyu Kit</author>
</authors>
<title>Exploiting unlabeled text with different unsupervised segmentation criteria for chinese word segmentation.</title>
<date>2008</date>
<journal>Research in Computing Science,</journal>
<pages>33--93</pages>
<contexts>
<context position="10337" citStr="Zhao and Kit, 2008" startWordPosition="1778" endWordPosition="1781">uated the proposed approach for Chineseto-English (CE) and Japanese-to-English (JE) translation tasks. The official datasets for the patent machine translation task at NTCIR-9 (Goto et al., 2011) were used. The detailed statistics for training, development and test sets are given in Table 1. Table 1: Data sets. In NTCIR-9, the development and test sets were both provided for CE task while only the test set was provided for the JE task. Therefore, we used the sentences from the NTCIR-8 JE test set as the development set for JE task. The word segmentation was done by BaseSeg (Zhao et al., 2006; Zhao and Kit, 2008; Zhao et al., 2010; Zhao and Kit, 2011; Zhao et al., 2013) for Chinese and Mecab2 for Japanese. To learn neural reordering models, the training and development sets were put together to obtain symmetric word alignments using GIZA++ (Och and Ney, 2003) and the grow-diag-finaland heuristic (Koehn et al., 2003). The reordering instances extracted from the aligned training and development sets were used as the training and validation data respectively for learning neural reordering models. Neural reordering models were trained by the toolkit NPLM (Vaswani et al., 2013). For CE task, training inst</context>
</contexts>
<marker>Zhao, Kit, 2008</marker>
<rawString>Hai Zhao and Chunyu Kit. 2008. Exploiting unlabeled text with different unsupervised segmentation criteria for chinese word segmentation. Research in Computing Science, 33:93–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chunyu Kit</author>
</authors>
<title>Integrating unsupervised and supervised word segmentation: The role of goodness measures.</title>
<date>2011</date>
<journal>Information Sciences,</journal>
<volume>181</volume>
<issue>1</issue>
<contexts>
<context position="10376" citStr="Zhao and Kit, 2011" startWordPosition="1786" endWordPosition="1789">to-English (CE) and Japanese-to-English (JE) translation tasks. The official datasets for the patent machine translation task at NTCIR-9 (Goto et al., 2011) were used. The detailed statistics for training, development and test sets are given in Table 1. Table 1: Data sets. In NTCIR-9, the development and test sets were both provided for CE task while only the test set was provided for the JE task. Therefore, we used the sentences from the NTCIR-8 JE test set as the development set for JE task. The word segmentation was done by BaseSeg (Zhao et al., 2006; Zhao and Kit, 2008; Zhao et al., 2010; Zhao and Kit, 2011; Zhao et al., 2013) for Chinese and Mecab2 for Japanese. To learn neural reordering models, the training and development sets were put together to obtain symmetric word alignments using GIZA++ (Och and Ney, 2003) and the grow-diag-finaland heuristic (Koehn et al., 2003). The reordering instances extracted from the aligned training and development sets were used as the training and validation data respectively for learning neural reordering models. Neural reordering models were trained by the toolkit NPLM (Vaswani et al., 2013). For CE task, training instances extracted from all the 1M sentenc</context>
</contexts>
<marker>Zhao, Kit, 2011</marker>
<rawString>Hai Zhao and Chunyu Kit. 2011. Integrating unsupervised and supervised word segmentation: The role of goodness measures. Information Sciences, 181(1):163–183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chang-Ning Huang</author>
<author>Mu Li</author>
</authors>
<title>An improved chinese word segmentation system with conditional random field.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>162--165</pages>
<contexts>
<context position="10317" citStr="Zhao et al., 2006" startWordPosition="1774" endWordPosition="1777">Experiments We evaluated the proposed approach for Chineseto-English (CE) and Japanese-to-English (JE) translation tasks. The official datasets for the patent machine translation task at NTCIR-9 (Goto et al., 2011) were used. The detailed statistics for training, development and test sets are given in Table 1. Table 1: Data sets. In NTCIR-9, the development and test sets were both provided for CE task while only the test set was provided for the JE task. Therefore, we used the sentences from the NTCIR-8 JE test set as the development set for JE task. The word segmentation was done by BaseSeg (Zhao et al., 2006; Zhao and Kit, 2008; Zhao et al., 2010; Zhao and Kit, 2011; Zhao et al., 2013) for Chinese and Mecab2 for Japanese. To learn neural reordering models, the training and development sets were put together to obtain symmetric word alignments using GIZA++ (Och and Ney, 2003) and the grow-diag-finaland heuristic (Koehn et al., 2003). The reordering instances extracted from the aligned training and development sets were used as the training and validation data respectively for learning neural reordering models. Neural reordering models were trained by the toolkit NPLM (Vaswani et al., 2013). For CE</context>
</contexts>
<marker>Zhao, Huang, Li, 2006</marker>
<rawString>Hai Zhao, Chang-Ning Huang, and Mu Li. 2006. An improved chinese word segmentation system with conditional random field. In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 162–165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chang-Ning Huang</author>
<author>Mu Li</author>
<author>Bao-Liang Lu</author>
</authors>
<title>A unified character-based tagging framework for chinese word segmentation.</title>
<date>2010</date>
<journal>ACM Transactions on Asian Language Information Processing (TALIP),</journal>
<volume>9</volume>
<issue>2</issue>
<contexts>
<context position="10356" citStr="Zhao et al., 2010" startWordPosition="1782" endWordPosition="1785">pproach for Chineseto-English (CE) and Japanese-to-English (JE) translation tasks. The official datasets for the patent machine translation task at NTCIR-9 (Goto et al., 2011) were used. The detailed statistics for training, development and test sets are given in Table 1. Table 1: Data sets. In NTCIR-9, the development and test sets were both provided for CE task while only the test set was provided for the JE task. Therefore, we used the sentences from the NTCIR-8 JE test set as the development set for JE task. The word segmentation was done by BaseSeg (Zhao et al., 2006; Zhao and Kit, 2008; Zhao et al., 2010; Zhao and Kit, 2011; Zhao et al., 2013) for Chinese and Mecab2 for Japanese. To learn neural reordering models, the training and development sets were put together to obtain symmetric word alignments using GIZA++ (Och and Ney, 2003) and the grow-diag-finaland heuristic (Koehn et al., 2003). The reordering instances extracted from the aligned training and development sets were used as the training and validation data respectively for learning neural reordering models. Neural reordering models were trained by the toolkit NPLM (Vaswani et al., 2013). For CE task, training instances extracted fro</context>
</contexts>
<marker>Zhao, Huang, Li, Lu, 2010</marker>
<rawString>Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang Lu. 2010. A unified character-based tagging framework for chinese word segmentation. ACM Transactions on Asian Language Information Processing (TALIP), 9(2):5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Masao Utiyama</author>
<author>Eiichiro Sumita</author>
<author>BaoLiang Lu</author>
</authors>
<title>An empirical study on word segmentation for chinese machine translation.</title>
<date>2013</date>
<booktitle>In Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>248--263</pages>
<contexts>
<context position="10396" citStr="Zhao et al., 2013" startWordPosition="1790" endWordPosition="1793">Japanese-to-English (JE) translation tasks. The official datasets for the patent machine translation task at NTCIR-9 (Goto et al., 2011) were used. The detailed statistics for training, development and test sets are given in Table 1. Table 1: Data sets. In NTCIR-9, the development and test sets were both provided for CE task while only the test set was provided for the JE task. Therefore, we used the sentences from the NTCIR-8 JE test set as the development set for JE task. The word segmentation was done by BaseSeg (Zhao et al., 2006; Zhao and Kit, 2008; Zhao et al., 2010; Zhao and Kit, 2011; Zhao et al., 2013) for Chinese and Mecab2 for Japanese. To learn neural reordering models, the training and development sets were put together to obtain symmetric word alignments using GIZA++ (Och and Ney, 2003) and the grow-diag-finaland heuristic (Koehn et al., 2003). The reordering instances extracted from the aligned training and development sets were used as the training and validation data respectively for learning neural reordering models. Neural reordering models were trained by the toolkit NPLM (Vaswani et al., 2013). For CE task, training instances extracted from all the 1M sentence pairs were used to</context>
</contexts>
<marker>Zhao, Utiyama, Sumita, Lu, 2013</marker>
<rawString>Hai Zhao, Masao Utiyama, Eiichiro Sumita, and BaoLiang Lu. 2013. An empirical study on word segmentation for chinese machine translation. In Computational Linguistics and Intelligent Text Processing, pages 248–263.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>