<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000123">
<title confidence="0.9655155">
Inducing Probabilistic Invertible Translation Grammars from
Aligned Texts
</title>
<author confidence="0.749235">
Michael Carl
</author>
<affiliation confidence="0.605676">
Institut fiir Angewandte Informationsforschung,
</affiliation>
<address confidence="0.90124">
Martin-Luther-StraBe 14
66111 Saarbriicken, Germany,
</address>
<email confidence="0.830085">
carlAiai.uni-sb.de
</email>
<sectionHeader confidence="0.971006" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999588307692308">
This paper presents an algorithm
for extracting invertible proba-
bilistic translation grammars from
bilingual aligned and linguistically
bracketed text. The invertibility
condition requires all translation
ambiguities to be resolved in the fi-
nal translation grammar. The pa-
per examines the complexity of in-
ducing translation grammars and
proposes a number of heuristics to
reduce the the theoretically expo-
nential computation time.
</bodyText>
<sectionHeader confidence="0.996406" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999962510204082">
In the framework of corpus-based machine
translation, a number of methods have been
proposed for automatically inducing trans-
lation correspondences from aligned texts.
Some researcher parse alignments with the
aim of linking word translations and inter-
nal nodes of derivation trees in both lan-
guage sides of the alignment. (Wu, 1995),
for instance, proposes a bilingual stochas-
tic parser. This parser analyses both lan-
guage sides of an alignment in parallel where
the leaves of the binary derivation trees are
terminal symbols and all internal nodes are
non-terminals. (Watanabe et al., 2000) and
(Meyers et al., 1996; Meyers et al., 1998)
parse each language side independently and
try to find most likely node correspondences
in the derivation trees. Other approaches, in-
cluding this present one, try to induce a set
of context-free transfer rules from the align-
ments.
This paper proposes an algorithm which gen-
erates and filters a translation grammar (i.e.
a set of context-free transfer rules) from
aligned and bracketed pieces of text. The
transfer rules in this grammar are similar
to Takeda&apos;s &amp;quot;Translation Pattern&amp;quot; (Takeda,
1996; Watanabe and Takeda, 1998) with the
difference that art accepted input is necessar-
ily also translatable. Translation grammars
are made up of lexical transfer rules which
contain only terminal symbols and transla-
tion templates - i.e. generalized transfer rules
- which also contain variables.
In the framework of Example-Based Machine
Translation (EBMT), a number of methods
have been proposed for inducing translation
grammars from aligned texts. In so-called
&amp;quot;pure&amp;quot; EBMT systems, the only available
knowledge resource is the aligned text itself
(cf. (Block, 2000; Brown, 1997)), while in
richer systems additional, linguistic knowl-
edge resources are used to a varying degree;
cf. (Somers, 1999) for a review of EBMT-
systems and resources used by these systems.
According to (Somers, 1999), EBMT sys-
tems differ in the number and the quality
of resources used, the way this knowledge
is represented, stored and used for trans-
</bodyText>
<equation confidence="0.9616805">
Cl: (a) (e&apos;) c5:(e) (e&apos;) c9: (de) (e&apos;) c13:(cde) (e&apos;)
c2:(a) (a&apos;) c6:(e) (a&apos;) cio: (de) (a&apos;) c14:(cde) (a&apos; )
c3:(a) (a&apos; b1) c7:(e) (a&apos; b1) cii: (de) (a&apos; b1) c15:(cde) (a&apos; b1)
c4:(a) (a&apos; b&apos; c&apos;) c8:(e) (a&apos; b&apos; c&apos;) c12: (de) (a&apos; b&apos; c&apos;) c16:(cde) (a&apos; b&apos; c&apos;)
</equation>
<figureCaption confidence="0.999093">
Figure 1: Set of lexical transfer rules C1 extracted from al
</figureCaption>
<bodyText confidence="0.872005326086957">
lation. EMIT systems differ also in the
way generalizations are computed. In al-
most all cases where generalizations are in-
duced from aligned texts, a set of two or more
alignments are compared and suitable sub-
sequences are replaced by variables. With re-
spect to this substitution one can distinguish
between methods which generalize similari-
ties (e.g. (Block, 2000; BostrOm, 1999) and
this present approach), methods which gen-
eralize differences (e.g. (Giivenir and Cicekli,
1998)) and methods which generalize both,
similarities and - as their complement - dif-
ferences (McTait and Trujillo, 1999). While
there is increasing research that aims at in-
ducing translation grammars from aligned
texts, a detailed analysis of the potentials
and implications of the different methods re-
mains to be undertaken in future.
The algorithm I shall present in this paper
expects as its input both language sides of
the alignments to be bracketed. We cur-
rently use the shallow parser KURD (Carl
and Schmidt-Wigger, 1998) for this bracket-
ing, although knowledge-poor methods could
similarly be used as, for instance, described
in (Zaanen, 2000). The algorithm generates
lexical transfer rules and generalizations from
the bracketed alignments and assigns prob-
abilities and weights to each of these rules.
From these rules, a translation grammar is
filtered. This grammar does not contain
translation ambiguities. Instead, each trans-
fer rule encodes a minimal context which
makes it unique in the translation grammar.
The algorithm does not require a bilingual
lexicon, but a lexicon can be provided to
bootstrap the system and enhance the out-
come. The translation grammar is expected
to re-generate the aligned text in a most com-
positional and complete manner. First, I de-
scribe how transfer rules are generated from
the bracketed alignments. Then I describe
how a set of invertible transfer rules is fil-
tered. Last I give art example of art induced
grammar.
</bodyText>
<sectionHeader confidence="0.928028" genericHeader="method">
2 Complexity of Inducing
Translation Templates
</sectionHeader>
<bodyText confidence="0.999871875">
The program assumes a bilingual text P
which consists of n aligned pieces of text
an. Each alignment a, consists of a left-
hand side (111s) e and a right-hand side (rhs)
f. By means of a (shallow) parser, both lan-
guage sides e and f are independently brack-
eted (parsed) which results in a representa-
tion similar to the followingl:
</bodyText>
<equation confidence="0.799462">
Ui : (a) b (c(d(e))) (((a1)11)cl) (e&apos;)
</equation>
<bodyText confidence="0.997643465116279">
Without no further knowledge, one cannot
know which of the 111s-brackets translates
into which bracket in the rhs or whether a
111s-bracket has a rhs-translation at all. We
therefore assume that each 111s-bracket trans-
lates with the same probability into arty rhs-
bracket. For art alignment a, we can there-
fore extract p x q lexical transfer rules C, :
{c1. .cp,,,q}, where p is the number of llts-
brackets and q is the number of rhs-brackets.
The set C1 extracted from al is shown in fig-
ure 1.
In a second step, translation templates (or
generalizations) are induced from al and
the extracted lexical transfer rules c1 ...cp,,,q.
While a lexical transfer rule consists only of
1The letters &amp;quot;abcde&amp;quot; on the left-hand side repre-
sent lemmas of e; the letters &amp;quot;a&apos;b&apos; c&apos; e&amp;quot; those of f in
the right-hand side. The brackets are also annotated
with a phrasal tag and morpho-syntactic information.
For the sake of simplicity, I will not consider this in-
formation here.
terminal symbols, a generalization contains
variables (so-called reductions) in places
where a shorter transfer rule matches a sub-
sequence in the llts and in the rhs. A gen-
eralization has thus at least one reduction in
each language side and it has art equal num-
ber of reductions in the llts and the rhs. Each
reduction in llts is linked exactly to one re-
duction in rhs.
From the transfer rule c16, for instance, can
be generated 4 different generalizations while
from transfer rule cll only one generalization
can be generated. These are shown in fig-
ure 2. From the alignment al can be gen-
erated 25 generalizations by substituting one
or more transfer rules c1 c16.
More generally, from a transfer rule c3 which
has p bracketed sequences in its llts and q
bracketed sequences in its rhs, art exponen-
tial number of generalizations #G3 can be
generated:
</bodyText>
<equation confidence="0.9999415">
#G3 = LdP i)
i=0 (q) +
</equation>
<bodyText confidence="0.99963847368421">
For instance, if we assume both, p and q to
be 10 and none of the 10 brackets in either
language side are included in another bracket
(i.e. all brackets are top-level brackets), more
than 180.000 different generalizations can be
generated. This is a number far too big to be
computed, as 10 brackets (i.e. constituents
in the parsed sentence) is not many. In fact,
50 or more brackets do appear frequently in
parsed sentences, although in the experiment
reported below, not all brackets are top-level.
Therefore, a number of heuristics is proposed
for generating from the set of possible gen-
eralizations only those achieving the highest
weight.
mar, each possible item in the translation
grammar, alignments aâ€ž lexical transfer rules
c3 and generalizations gk, is linked to two
sets.
</bodyText>
<equation confidence="0.977647666666667">
ai {Gi,Ci}
gk {-14,Ck}
c {G3, ,43}
</equation>
<bodyText confidence="0.989811736842105">
Each a, is associated with a set of lexical
transfer rules C, and a set of generalizations
G, which have been generated from a,. Each
lexical transfer rule c3 is associated with a set
A3 of alignments from which c3 has been ex-
tracted and a set of generalizations G3 which
have been generated from c3. Finally, each
generalization gk is associated with a set of
lexical transfer rules Ck which have been re-
placed in the generalization (i.e. the daugh-
ters of the generalization gk) and a set of ref-
erences Rk. Since generalizations are gener-
ated from alignments and from the extracted
lexical transfer rules, the set of references Rk
may consist of alignments a, and/or transfer
rules c3.
The probability of art alignment a, is its fre-
quency in the aligned text P divided by the
number n of alignments in P.
</bodyText>
<equation confidence="0.990463">
P(a0 f(a) (1)
</equation>
<bodyText confidence="0.9995492">
The probability of a lexical transfer rule c3 is
a function of the number of times c3 has been
extracted from alignments a, i=1...n and
the cardinality #Ci of the set Ci, normalized
by the size of P:
</bodyText>
<table confidence="0.335175">
3 Probabilistic Translation 1 . 1 (2)
Grammars p(c â€¢) = â€” N
n -V#Ci
cjEci
</table>
<bodyText confidence="0.994372">
Before introducing heuristics, I first describe
how probabilities and weights are assigned
to the lexical transfer rules and generaliza-
tions. While inducing the translation gram-
The probability of a generalization gk is the
sum of the probabilities of the reference(s)
r E Rk from which gk has been generated.
</bodyText>
<table confidence="0.9982205">
Gi Induced Generalization P(gk) tu(gk)
Gil : : (d*) (*b&apos;) 1/4 2/4
C16 g2 (cd*) (*bIcI) 1/4 2/4
g3: (cd*) (*cl) 1/4 2/4
g4 (c*) (*bIcI) 1/4 2/4
g5 : (c*) (*cl) 1/4 3/4
</table>
<figureCaption confidence="0.991926">
Figure 2: Set of generalizations Cii and C16 induced from transfer rules cll and c16
</figureCaption>
<equation confidence="0.9832675">
p(g) = P(r) (3)
rERk
</equation>
<bodyText confidence="0.999495333333333">
Based ort these probabilities, a weight is com-
puted for each aâ€ž c3, gk.
The weight w(c3) of a lexical transfer rule
c3 equals the maximum weight of the gen-
eralization that has been generated from it.
In case no generalization can be generated
</bodyText>
<equation confidence="0.9991215">
w(c3) = p(c3).
w(c3) = max{w(g E Gj)} (4)
</equation>
<bodyText confidence="0.999980833333333">
The weight of a generalization equals the sum
of the probabilities of the references r E Rk
from which it has been generated plus the
sum of the weights of the lexical transfer rules
(i.e. the daughters of gk) which have been
replaced in the generalization.
</bodyText>
<equation confidence="0.9487845">
w(g) &gt; p(&apos;ri) + &gt; w(c) (5)
rERk cECk
</equation>
<bodyText confidence="0.999807142857143">
The following properties of weights in trans-
fer rules and generalizations hold: a general-
ization has at least as high a weight as the
transfer rule from which it was generated. A
generalization has a higher weight than the
daughters which have been substituted in the
generalization.
</bodyText>
<equation confidence="0.863987">
w(r E Rk) &lt; w(gk) &gt; w(r E Ck) (6)
</equation>
<bodyText confidence="0.999482521739131">
Moreover, generalizations have higher
weights if they contain i) more reductions or
ii) if the reductions are to the highest extend
compositional. We believe that these prop-
erties are suitable when using the induced
probabilistic grammar for translation. As
art example for achieving higher weights for
more compositional generalizations, consider
the following example.
As there are 16 lexical transfer rules ex-
tracted from alignment al and assuming that
n = 1, each of the rules has probability 1/4.
A sub-sequence in cll : (de) (a111) can be
substituted by rule c6 : (e) (al). This sub-
stitution yields generalization gi, as shown
in figure 2. It is assigned the weight 2/4 (cf.
equation 5). This weight is also assigned to
the transfer rule cll (cf. equation 4). Sub-
sequently, when generalizing the longer rule
c16 (cde) (aIbIcI), a set of four gener-
alizations C16 is generated from which g,5 is
assigned the highest weight due to the com-
positional nature of the replaced daughter
</bodyText>
<listItem confidence="0.289505">
C11: (de) (Al).
</listItem>
<sectionHeader confidence="0.964665" genericHeader="method">
4 Generating Transfer Rules
</sectionHeader>
<bodyText confidence="0.998983277777778">
Evert the quadratic effort for generating the
sets C, for all alignments a, is too expensive
in a large aligned text. Therefore, alignments
aâ€ž i = 1... n are generalized in a sequential
manner. For each aâ€ž first the p x q lexical
transfer rules are extracted and sorted by the
length of the shorter string e or f. Transfer
rules c3 E C, are then generalized starting
with the shortest rule. The crucial points
in the procedure GenerateGrammar() are
lines 4 and 9. As was shown above, ex-
tracting the set of lexical transfer requires a
quadratic effort in the number of brackets,
while generating G3 from e3 is exponential.
To tackle this latter complexity, a version
of the A* algorithm considers only a limited
number of the highest weighted generaliza-
tion2.
</bodyText>
<figure confidence="0.987976615384615">
1 GenerateGrammar(P)
2 begin
3 for all a, E P:
4 extract lexical transfer rules C, from a,;
5 for all c, E C, : p(c3) = 11(-V#C, * j
6 add a, as co to C,;
7 sort C, by length of shorter e or f;
8 for each c, E C, starting with shortest c
9 generate G3 from c,
10 w(c3) = mar {te(g E G3)}
11 end
12 end
13 end;
</figure>
<bodyText confidence="0.82333275">
lit this way generalization of lexical transfer
rules is reduced to 0(k * d) where d is the
number of lexical transfer rules matching a
subsequence in e3 and k is the number of gen-
eralizations to be generated. In addition to
this, a couple of parameters can be set to re-
duce the number of extracted transfer rules
and generalizations:
</bodyText>
<listItem confidence="0.998114125">
â€¢ Only transfer rules are extracted where
the difference in the number of words in
e and f does not exceed a pre-defined
limit. This constraint reflects that gen-
erally more or less the same number of
(content) words appear in both sides of
a translation.
â€¢ By the same token, transfer rules and
</listItem>
<bodyText confidence="0.6776905">
generalizations are weighted by the dif-
ference of number of words in e and f.
This reflects the experience that transla-
tions are likely to contain approximately
the same number of words in their source
and target sides.
</bodyText>
<footnote confidence="0.639621">
21n (Meyers et al., 1996) a similar mechanism is
called &amp;quot;greedy heuristic&amp;quot;.
</footnote>
<listItem confidence="0.997308708333333">
â€¢ By means of a bilingual lexicon, trans-
fer rules and generalizations are assigned
(high) a-priori weights.
â€¢ Only a limited number of highest
weighted translation rules and general-
izations is generated.
â€¢ Each generalization can have up to
a fixed maximum number of reduc-
tions. As reductions in generalizations
are most reasonable explained to repre-
sent the arguments of the remaining un-
reduced token(s), a maximum of reduc-
tion might be set to four.
â€¢ A generalization can have maximum
number of tokens. With increasing num-
ber of tokens, the chance to match a
new sentence decreases exponentially.
Note that this constraint does not apply
to translation rules and generalizations
from a bilingual lexicon.
â€¢ Transfer rules are only asserted if their
weights are above a threshold of art al-
ready existing, ambiguous transfer rule
in the database.
</listItem>
<sectionHeader confidence="0.8205205" genericHeader="method">
5 Filtering Invertible Translation
Grammars
</sectionHeader>
<bodyText confidence="0.999826764705882">
From the set P of alignments and their as-
sociated sets of generalizations P : {al
G1, , a2 GO art invertible translation
grammar is filtered in a top down fashion.
The aim here is to find a most compositional
set of transfer rules capable to reproduce the
aligned text P in a most complete manner.
To achieve this goal, translation ambiguities
in the resulting grammar are avoided by in-
cluding the smallest possible context which
disambiguates each transfer rule. A trans-
fer rule e2 f2 is ambiguous iff the trans-
lation grammar contains a different transfer
rule el fi where either el equals e2 or
fi equals f2. A translation grammar is in-
vertible iff it contains no ambiguous trans-
fer rules. In art invertible translation gram-
</bodyText>
<table confidence="0.857958181818182">
Induced Transfer Rules p(.) w(.)
al : (dx) (an&apos;) 1/4 2/4
: (d*) (m&apos;*) 1/4 2/4
: (x) (n&apos;) 1/4 1/4
a2 : (de) (alb&apos;) 1/8 1/4
g2 : (d),) (*b9 1/8 1/4
C2 : (e) (aI) 1/8 1/8
Filtered Invertible Grammar p(.) tv(.)
: (d),) (m&apos;*) 1/4 2/4
: (x) (n&apos;) 1/4 1/4
a2 : (de) (a111) 1/8 1/4
</table>
<figureCaption confidence="0.999192">
Figure 3: Induced and Filtered Invertible Grammar
</figureCaption>
<bodyText confidence="0.999684642857143">
mar, therefore, eachllts-string e and each rhs-
string f occurs exactly once.
The procedure to filter this grammar,
FilterGrammarÂ°, starts with the most
frequent top-most generalizations - i.e. those
generalizations of a, with the highest weight
- and recursively prints their daughters. Less
frequent rules, i.e. lower weighted rules, are
likely to to come along with more context.
Only one generalization is filtered for each
alignment. The alignments a, E P are sorted
by their weights and for each alignment
starting with the highest weighted one, the
function FilterGrammar(a,) is called.
</bodyText>
<figure confidence="0.949062923076923">
1 FilterGrammar(r,)
2 begin
3 if G, not empty
4 print gk : mar {te(gk E G,)}
5 delete all generalizations g : e f \\
where e = Ihs(gk) or f = rhs(gk).
6 for all c, E Ck: FilterGrammar(c3)
7 else
8 print r,
9 delete all rules c: e f \\
where e = Ihs(r,) or f = rhs(r,).
10 end
11 end;
</figure>
<bodyText confidence="0.999889363636364">
The procedure is called recursively in line 6
to print the highest weighted daughters c3 of
generalization gk. To illustrate the resulting
translation grammar, assume the set P con-
tains two alignments, {ai, a2} as shown in
figure 3, left. Alignments al and a2 are as-
sociated with G1 and G2 which contain the
generalizations gi and g2 respectively. The
daughters of gi and g2 are Ci : {ci} and
C2 : {c2}. Note that the weights of al and
gi are 2/4 while the weight of al and g2 are
1/4.
The procedure FilterGrammar() is first
called with al and then with a2. Accord-
ingly, first generalization gi is printed and - in
the recursion - c1. Generalizations gi and g2
are ambiguous since their llts is identical but
their rhs are different. Due to the deletion of
g2, the set G2 is, thus, empty when calling
the function with a2. The resulting filtered
invertible translation grammar is shown in
figure 3, right.
</bodyText>
<sectionHeader confidence="0.991917" genericHeader="method">
6 Preliminary Experiments
</sectionHeader>
<bodyText confidence="0.999952240740741">
The algorithm has been tested on a partially
parsed text with 4997 alignments. In this
first test both languages were identical, i.e.
the words and the structure of their brack-
etings was identical in the source and target
language. The aim of this experiment was to
see to what extend the algorithm produces
reasonable translation grammars for a pair
of structurally identical languages. A rea-
sonable assumption for translating a source
language into art identical target language is
that the induced transfer rules and general-
izations are identical in their llts and rhs, too.
The quality of the translation grammar can
then easily be measured by counting the rules
which differ in their llts and rhs.
The 4997 alignments had 45.352 words and
36.687 brackets on each language side. De-
pendent on the parameter setting - i.e. how
many brackets were maximally considered
per language side - the program took be-
tween 5 and 10 minutes ort a sun work station
to generate and filter the invertible gram-
mar. The filtered invertible grammar con-
tained 814 generalizations and 3692 lexical
translation rules and alignments. 3698 of the
filtered transfer rules had art identical llts and
rhs while 808 rules (18%) were different in
llts and rhs. Roughly half of these erroneous
rules were lexical transfer rules and half gen-
eralizations. Simulating a lexicon covering
10% of the aligned text reduced erroneous
transfer rules about 50% to 9,6% of the size
of the translation grammar. Augmenting the
lexicon to 20% reduced errors to 6,5% and
with 50% of the aligned text covered by a
lexicon produced 3.4% wrong output.
These results show that i) invertible gram-
mars can be induced in reasonable time and
ii) the proposed algorithm is scalable for the
integration of further knowledge resources -
such as a bilingual lexicon - which enhance
the quality of the induced grammar.
lit further experiments we will parse a text in
different ways to see how well the algorithm
can tackle different structures in both lan-
guage sides. The goal for the future is to de-
sign (partial) parsers for different languages
which yield similar brackets ort both language
sides in order to enable the algorithm to ex-
tract and filter reasonable translation gram-
mars more easily. The induced translation
grammars are supposed to be used in art
EMIT system.
</bodyText>
<sectionHeader confidence="0.999216" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.998482363636364">
This paper presents art algorithm which gen-
erates and filters a translation grammar from
aligned texts. The produced translation
grammar consists of lexical transfer rules
and generalizations. Each rule in the gram-
mar describes art unambiguous 1-to-1 map-
ping from the source language to the target
language. A small experiment is described
to test the grammar induction performance.
The algorithm shows satisfying runtime be-
havior and promising results.
</bodyText>
<sectionHeader confidence="0.99464" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999949872340426">
Hans Ulrich Block. 2000. Example-Based Incremen-
tal Synchronous Interpretation. In (Wahlster, 2000),
pages 411-417.
Henrik BostrOm. 1999. Induction of Recursive Trans-
fer Rules. In Learning Language in Logic (LLL)
Workshop, Bled, Slovenia.
Ralf D. Brown. 1997. Automated Dictionary Extrac-
tion for &amp;quot;Knowledge-Free&amp;quot; Example-Based Transla-
tion. In TMI-97, pages 111-118.
Michael Carl and Antje Schmidt-Wigger. 1998. Shal-
low Postmorphological Processing with KURD. In
Proceedings of NeMLaP3/CoNLL98, pages 257-265,
Sydney.
Halil Altay Giivenir and Ilyas Cicekli. 1998. Learning
Translation Templates from Examples. Information
Systems, 23(6):353-363.
Kevin McTait and Arturo Trujillo. 1999. A
Language-Neutral Sparse-Data Algorithm for Ex-
tracting Translation Pattern. In TMI&apos;99.
Adam Meyers, Roman Yangarber, and Ralph Grish-
man. 1996. Alignment of shared forests for bilingual
corpora. In Coling&apos;96, Copenhagen, Denmark.
Adam Meyers, Roman Yangarber, Ralph Grish-
man, Catherine Macleod, and Antonio Moreno-
Sandoval. 1998. Deriving transfer rules from
dominance-preserving alignments. In Computerm,
First Workshop on Computational Terminology,
Montreal, Canada.
Harold Somers. 1999. Review Article: Example-
based Machine Translation. Machine Translation,
14(2):113-157.
Koichi Takeda. 1996. Pattern-Based Machine Trans-
lation. In COLING-96, pages 1155-1158.
Wolfgang (ed.) Wahlster. 2000. Verbmobil: Founda-
tions of Speech-to-Speech Translation. Springer, Hei-
delberg.
Hideo Watanabe and Koichi Takeda. 1998. A
Pattern-based Machine Translation System Extended
by Example-based Processing. In Coling 1998.
Hideo Watanabe, Sadao Kurohashi, and Eiji Ara-
maki. 2000. Finding Structural Correspondences
from Bilingual Parsed Corpus for Corpus-Based
Translation. In Coling 2000.
Dekai Wu. 1995. Grammarless extraction of phrasal
translation examples from parallel texts. In TMI-95.
Menno van Zaanen. 2000. ABL: Alignment-Based
Learning. In COLING-2000, pages 961-967.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.829244">
<title confidence="0.997268">Inducing Probabilistic Invertible Translation Grammars from Aligned Texts</title>
<author confidence="0.992679">Michael</author>
<affiliation confidence="0.938408">Institut fiir Angewandte Martin-Luther-StraBe</affiliation>
<address confidence="0.933369">66111 Saarbriicken,</address>
<email confidence="0.995096">carlAiai.uni-sb.de</email>
<abstract confidence="0.998817">This paper presents an algorithm for extracting invertible probabilistic translation grammars from bilingual aligned and linguistically bracketed text. The invertibility condition requires all translation ambiguities to be resolved in the final translation grammar. The paper examines the complexity of inducing translation grammars and proposes a number of heuristics to reduce the the theoretically exponential computation time.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Hans Ulrich Block</author>
</authors>
<title>Example-Based Incremental Synchronous Interpretation. In (Wahlster,</title>
<date>2000</date>
<pages>411--417</pages>
<contexts>
<context position="2362" citStr="Block, 2000" startWordPosition="345" endWordPosition="346">o Takeda&apos;s &amp;quot;Translation Pattern&amp;quot; (Takeda, 1996; Watanabe and Takeda, 1998) with the difference that art accepted input is necessarily also translatable. Translation grammars are made up of lexical transfer rules which contain only terminal symbols and translation templates - i.e. generalized transfer rules - which also contain variables. In the framework of Example-Based Machine Translation (EBMT), a number of methods have been proposed for inducing translation grammars from aligned texts. In so-called &amp;quot;pure&amp;quot; EBMT systems, the only available knowledge resource is the aligned text itself (cf. (Block, 2000; Brown, 1997)), while in richer systems additional, linguistic knowledge resources are used to a varying degree; cf. (Somers, 1999) for a review of EBMTsystems and resources used by these systems. According to (Somers, 1999), EBMT systems differ in the number and the quality of resources used, the way this knowledge is represented, stored and used for transCl: (a) (e&apos;) c5:(e) (e&apos;) c9: (de) (e&apos;) c13:(cde) (e&apos;) c2:(a) (a&apos;) c6:(e) (a&apos;) cio: (de) (a&apos;) c14:(cde) (a&apos; ) c3:(a) (a&apos; b1) c7:(e) (a&apos; b1) cii: (de) (a&apos; b1) c15:(cde) (a&apos; b1) c4:(a) (a&apos; b&apos; c&apos;) c8:(e) (a&apos; b&apos; c&apos;) c12: (de) (a&apos; b&apos; c&apos;) c16:(cde</context>
</contexts>
<marker>Block, 2000</marker>
<rawString>Hans Ulrich Block. 2000. Example-Based Incremental Synchronous Interpretation. In (Wahlster, 2000), pages 411-417.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henrik BostrOm</author>
</authors>
<title>Induction of Recursive Transfer Rules.</title>
<date>1999</date>
<booktitle>In Learning Language in Logic (LLL) Workshop,</booktitle>
<location>Bled, Slovenia.</location>
<contexts>
<context position="3416" citStr="BostrOm, 1999" startWordPosition="527" endWordPosition="528">o: (de) (a&apos;) c14:(cde) (a&apos; ) c3:(a) (a&apos; b1) c7:(e) (a&apos; b1) cii: (de) (a&apos; b1) c15:(cde) (a&apos; b1) c4:(a) (a&apos; b&apos; c&apos;) c8:(e) (a&apos; b&apos; c&apos;) c12: (de) (a&apos; b&apos; c&apos;) c16:(cde) (a&apos; b&apos; c&apos;) Figure 1: Set of lexical transfer rules C1 extracted from al lation. EMIT systems differ also in the way generalizations are computed. In almost all cases where generalizations are induced from aligned texts, a set of two or more alignments are compared and suitable subsequences are replaced by variables. With respect to this substitution one can distinguish between methods which generalize similarities (e.g. (Block, 2000; BostrOm, 1999) and this present approach), methods which generalize differences (e.g. (Giivenir and Cicekli, 1998)) and methods which generalize both, similarities and - as their complement - differences (McTait and Trujillo, 1999). While there is increasing research that aims at inducing translation grammars from aligned texts, a detailed analysis of the potentials and implications of the different methods remains to be undertaken in future. The algorithm I shall present in this paper expects as its input both language sides of the alignments to be bracketed. We currently use the shallow parser KURD (Carl </context>
</contexts>
<marker>BostrOm, 1999</marker>
<rawString>Henrik BostrOm. 1999. Induction of Recursive Transfer Rules. In Learning Language in Logic (LLL) Workshop, Bled, Slovenia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf D Brown</author>
</authors>
<title>Automated Dictionary Extraction for &amp;quot;Knowledge-Free&amp;quot; Example-Based Translation. In</title>
<date>1997</date>
<booktitle>TMI-97,</booktitle>
<pages>111--118</pages>
<contexts>
<context position="2376" citStr="Brown, 1997" startWordPosition="347" endWordPosition="348">ranslation Pattern&amp;quot; (Takeda, 1996; Watanabe and Takeda, 1998) with the difference that art accepted input is necessarily also translatable. Translation grammars are made up of lexical transfer rules which contain only terminal symbols and translation templates - i.e. generalized transfer rules - which also contain variables. In the framework of Example-Based Machine Translation (EBMT), a number of methods have been proposed for inducing translation grammars from aligned texts. In so-called &amp;quot;pure&amp;quot; EBMT systems, the only available knowledge resource is the aligned text itself (cf. (Block, 2000; Brown, 1997)), while in richer systems additional, linguistic knowledge resources are used to a varying degree; cf. (Somers, 1999) for a review of EBMTsystems and resources used by these systems. According to (Somers, 1999), EBMT systems differ in the number and the quality of resources used, the way this knowledge is represented, stored and used for transCl: (a) (e&apos;) c5:(e) (e&apos;) c9: (de) (e&apos;) c13:(cde) (e&apos;) c2:(a) (a&apos;) c6:(e) (a&apos;) cio: (de) (a&apos;) c14:(cde) (a&apos; ) c3:(a) (a&apos; b1) c7:(e) (a&apos; b1) cii: (de) (a&apos; b1) c15:(cde) (a&apos; b1) c4:(a) (a&apos; b&apos; c&apos;) c8:(e) (a&apos; b&apos; c&apos;) c12: (de) (a&apos; b&apos; c&apos;) c16:(cde) (a&apos; b&apos; c&apos;) F</context>
</contexts>
<marker>Brown, 1997</marker>
<rawString>Ralf D. Brown. 1997. Automated Dictionary Extraction for &amp;quot;Knowledge-Free&amp;quot; Example-Based Translation. In TMI-97, pages 111-118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Carl</author>
<author>Antje Schmidt-Wigger</author>
</authors>
<title>Shallow Postmorphological Processing with KURD.</title>
<date>1998</date>
<booktitle>In Proceedings of NeMLaP3/CoNLL98,</booktitle>
<pages>257--265</pages>
<location>Sydney.</location>
<contexts>
<context position="4041" citStr="Carl and Schmidt-Wigger, 1998" startWordPosition="624" endWordPosition="627">1999) and this present approach), methods which generalize differences (e.g. (Giivenir and Cicekli, 1998)) and methods which generalize both, similarities and - as their complement - differences (McTait and Trujillo, 1999). While there is increasing research that aims at inducing translation grammars from aligned texts, a detailed analysis of the potentials and implications of the different methods remains to be undertaken in future. The algorithm I shall present in this paper expects as its input both language sides of the alignments to be bracketed. We currently use the shallow parser KURD (Carl and Schmidt-Wigger, 1998) for this bracketing, although knowledge-poor methods could similarly be used as, for instance, described in (Zaanen, 2000). The algorithm generates lexical transfer rules and generalizations from the bracketed alignments and assigns probabilities and weights to each of these rules. From these rules, a translation grammar is filtered. This grammar does not contain translation ambiguities. Instead, each transfer rule encodes a minimal context which makes it unique in the translation grammar. The algorithm does not require a bilingual lexicon, but a lexicon can be provided to bootstrap the syste</context>
</contexts>
<marker>Carl, Schmidt-Wigger, 1998</marker>
<rawString>Michael Carl and Antje Schmidt-Wigger. 1998. Shallow Postmorphological Processing with KURD. In Proceedings of NeMLaP3/CoNLL98, pages 257-265, Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Halil Altay Giivenir</author>
<author>Ilyas Cicekli</author>
</authors>
<title>Learning Translation Templates from Examples. Information Systems,</title>
<date>1998</date>
<pages>23--6</pages>
<contexts>
<context position="3516" citStr="Giivenir and Cicekli, 1998" startWordPosition="539" endWordPosition="542">(a&apos; b1) c4:(a) (a&apos; b&apos; c&apos;) c8:(e) (a&apos; b&apos; c&apos;) c12: (de) (a&apos; b&apos; c&apos;) c16:(cde) (a&apos; b&apos; c&apos;) Figure 1: Set of lexical transfer rules C1 extracted from al lation. EMIT systems differ also in the way generalizations are computed. In almost all cases where generalizations are induced from aligned texts, a set of two or more alignments are compared and suitable subsequences are replaced by variables. With respect to this substitution one can distinguish between methods which generalize similarities (e.g. (Block, 2000; BostrOm, 1999) and this present approach), methods which generalize differences (e.g. (Giivenir and Cicekli, 1998)) and methods which generalize both, similarities and - as their complement - differences (McTait and Trujillo, 1999). While there is increasing research that aims at inducing translation grammars from aligned texts, a detailed analysis of the potentials and implications of the different methods remains to be undertaken in future. The algorithm I shall present in this paper expects as its input both language sides of the alignments to be bracketed. We currently use the shallow parser KURD (Carl and Schmidt-Wigger, 1998) for this bracketing, although knowledge-poor methods could similarly be us</context>
</contexts>
<marker>Giivenir, Cicekli, 1998</marker>
<rawString>Halil Altay Giivenir and Ilyas Cicekli. 1998. Learning Translation Templates from Examples. Information Systems, 23(6):353-363.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin McTait</author>
<author>Arturo Trujillo</author>
</authors>
<title>A Language-Neutral Sparse-Data Algorithm for Extracting Translation Pattern.</title>
<date>1999</date>
<booktitle>In TMI&apos;99.</booktitle>
<contexts>
<context position="3633" citStr="McTait and Trujillo, 1999" startWordPosition="557" endWordPosition="560">r rules C1 extracted from al lation. EMIT systems differ also in the way generalizations are computed. In almost all cases where generalizations are induced from aligned texts, a set of two or more alignments are compared and suitable subsequences are replaced by variables. With respect to this substitution one can distinguish between methods which generalize similarities (e.g. (Block, 2000; BostrOm, 1999) and this present approach), methods which generalize differences (e.g. (Giivenir and Cicekli, 1998)) and methods which generalize both, similarities and - as their complement - differences (McTait and Trujillo, 1999). While there is increasing research that aims at inducing translation grammars from aligned texts, a detailed analysis of the potentials and implications of the different methods remains to be undertaken in future. The algorithm I shall present in this paper expects as its input both language sides of the alignments to be bracketed. We currently use the shallow parser KURD (Carl and Schmidt-Wigger, 1998) for this bracketing, although knowledge-poor methods could similarly be used as, for instance, described in (Zaanen, 2000). The algorithm generates lexical transfer rules and generalizations </context>
</contexts>
<marker>McTait, Trujillo, 1999</marker>
<rawString>Kevin McTait and Arturo Trujillo. 1999. A Language-Neutral Sparse-Data Algorithm for Extracting Translation Pattern. In TMI&apos;99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Meyers</author>
<author>Roman Yangarber</author>
<author>Ralph Grishman</author>
</authors>
<title>Alignment of shared forests for bilingual corpora.</title>
<date>1996</date>
<booktitle>In Coling&apos;96,</booktitle>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="1279" citStr="Meyers et al., 1996" startWordPosition="177" endWordPosition="180">n In the framework of corpus-based machine translation, a number of methods have been proposed for automatically inducing translation correspondences from aligned texts. Some researcher parse alignments with the aim of linking word translations and internal nodes of derivation trees in both language sides of the alignment. (Wu, 1995), for instance, proposes a bilingual stochastic parser. This parser analyses both language sides of an alignment in parallel where the leaves of the binary derivation trees are terminal symbols and all internal nodes are non-terminals. (Watanabe et al., 2000) and (Meyers et al., 1996; Meyers et al., 1998) parse each language side independently and try to find most likely node correspondences in the derivation trees. Other approaches, including this present one, try to induce a set of context-free transfer rules from the alignments. This paper proposes an algorithm which generates and filters a translation grammar (i.e. a set of context-free transfer rules) from aligned and bracketed pieces of text. The transfer rules in this grammar are similar to Takeda&apos;s &amp;quot;Translation Pattern&amp;quot; (Takeda, 1996; Watanabe and Takeda, 1998) with the difference that art accepted input is necess</context>
<context position="13593" citStr="Meyers et al., 1996" startWordPosition="2313" endWordPosition="2316">an be set to reduce the number of extracted transfer rules and generalizations: â€¢ Only transfer rules are extracted where the difference in the number of words in e and f does not exceed a pre-defined limit. This constraint reflects that generally more or less the same number of (content) words appear in both sides of a translation. â€¢ By the same token, transfer rules and generalizations are weighted by the difference of number of words in e and f. This reflects the experience that translations are likely to contain approximately the same number of words in their source and target sides. 21n (Meyers et al., 1996) a similar mechanism is called &amp;quot;greedy heuristic&amp;quot;. â€¢ By means of a bilingual lexicon, transfer rules and generalizations are assigned (high) a-priori weights. â€¢ Only a limited number of highest weighted translation rules and generalizations is generated. â€¢ Each generalization can have up to a fixed maximum number of reductions. As reductions in generalizations are most reasonable explained to represent the arguments of the remaining unreduced token(s), a maximum of reduction might be set to four. â€¢ A generalization can have maximum number of tokens. With increasing number of tokens, the chance</context>
</contexts>
<marker>Meyers, Yangarber, Grishman, 1996</marker>
<rawString>Adam Meyers, Roman Yangarber, and Ralph Grishman. 1996. Alignment of shared forests for bilingual corpora. In Coling&apos;96, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Adam Meyers</author>
<author>Roman Yangarber</author>
<author>Ralph Grishman</author>
</authors>
<location>Catherine Macleod, and Antonio Moreno-</location>
<marker>Meyers, Yangarber, Grishman, </marker>
<rawString>Adam Meyers, Roman Yangarber, Ralph Grishman, Catherine Macleod, and Antonio Moreno-</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandoval</author>
</authors>
<title>Deriving transfer rules from dominance-preserving alignments.</title>
<date>1998</date>
<booktitle>In Computerm, First Workshop on Computational Terminology,</booktitle>
<location>Montreal, Canada.</location>
<marker>Sandoval, 1998</marker>
<rawString>Sandoval. 1998. Deriving transfer rules from dominance-preserving alignments. In Computerm, First Workshop on Computational Terminology, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harold Somers</author>
</authors>
<title>Review Article: Examplebased Machine Translation.</title>
<date>1999</date>
<journal>Machine Translation,</journal>
<pages>14--2</pages>
<contexts>
<context position="2494" citStr="Somers, 1999" startWordPosition="365" endWordPosition="366">ily also translatable. Translation grammars are made up of lexical transfer rules which contain only terminal symbols and translation templates - i.e. generalized transfer rules - which also contain variables. In the framework of Example-Based Machine Translation (EBMT), a number of methods have been proposed for inducing translation grammars from aligned texts. In so-called &amp;quot;pure&amp;quot; EBMT systems, the only available knowledge resource is the aligned text itself (cf. (Block, 2000; Brown, 1997)), while in richer systems additional, linguistic knowledge resources are used to a varying degree; cf. (Somers, 1999) for a review of EBMTsystems and resources used by these systems. According to (Somers, 1999), EBMT systems differ in the number and the quality of resources used, the way this knowledge is represented, stored and used for transCl: (a) (e&apos;) c5:(e) (e&apos;) c9: (de) (e&apos;) c13:(cde) (e&apos;) c2:(a) (a&apos;) c6:(e) (a&apos;) cio: (de) (a&apos;) c14:(cde) (a&apos; ) c3:(a) (a&apos; b1) c7:(e) (a&apos; b1) cii: (de) (a&apos; b1) c15:(cde) (a&apos; b1) c4:(a) (a&apos; b&apos; c&apos;) c8:(e) (a&apos; b&apos; c&apos;) c12: (de) (a&apos; b&apos; c&apos;) c16:(cde) (a&apos; b&apos; c&apos;) Figure 1: Set of lexical transfer rules C1 extracted from al lation. EMIT systems differ also in the way generalization</context>
</contexts>
<marker>Somers, 1999</marker>
<rawString>Harold Somers. 1999. Review Article: Examplebased Machine Translation. Machine Translation, 14(2):113-157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koichi Takeda</author>
</authors>
<title>Pattern-Based Machine Translation. In</title>
<date>1996</date>
<booktitle>COLING-96,</booktitle>
<pages>1155--1158</pages>
<contexts>
<context position="1797" citStr="Takeda, 1996" startWordPosition="261" endWordPosition="262">ols and all internal nodes are non-terminals. (Watanabe et al., 2000) and (Meyers et al., 1996; Meyers et al., 1998) parse each language side independently and try to find most likely node correspondences in the derivation trees. Other approaches, including this present one, try to induce a set of context-free transfer rules from the alignments. This paper proposes an algorithm which generates and filters a translation grammar (i.e. a set of context-free transfer rules) from aligned and bracketed pieces of text. The transfer rules in this grammar are similar to Takeda&apos;s &amp;quot;Translation Pattern&amp;quot; (Takeda, 1996; Watanabe and Takeda, 1998) with the difference that art accepted input is necessarily also translatable. Translation grammars are made up of lexical transfer rules which contain only terminal symbols and translation templates - i.e. generalized transfer rules - which also contain variables. In the framework of Example-Based Machine Translation (EBMT), a number of methods have been proposed for inducing translation grammars from aligned texts. In so-called &amp;quot;pure&amp;quot; EBMT systems, the only available knowledge resource is the aligned text itself (cf. (Block, 2000; Brown, 1997)), while in richer sy</context>
</contexts>
<marker>Takeda, 1996</marker>
<rawString>Koichi Takeda. 1996. Pattern-Based Machine Translation. In COLING-96, pages 1155-1158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Wahlster</author>
</authors>
<title>Verbmobil: Foundations of Speech-to-Speech Translation.</title>
<date>2000</date>
<publisher>Springer,</publisher>
<location>Heidelberg.</location>
<marker>Wahlster, 2000</marker>
<rawString>Wolfgang (ed.) Wahlster. 2000. Verbmobil: Foundations of Speech-to-Speech Translation. Springer, Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideo Watanabe</author>
<author>Koichi Takeda</author>
</authors>
<title>A Pattern-based Machine Translation System Extended by Example-based Processing. In Coling</title>
<date>1998</date>
<contexts>
<context position="1825" citStr="Watanabe and Takeda, 1998" startWordPosition="263" endWordPosition="266">ternal nodes are non-terminals. (Watanabe et al., 2000) and (Meyers et al., 1996; Meyers et al., 1998) parse each language side independently and try to find most likely node correspondences in the derivation trees. Other approaches, including this present one, try to induce a set of context-free transfer rules from the alignments. This paper proposes an algorithm which generates and filters a translation grammar (i.e. a set of context-free transfer rules) from aligned and bracketed pieces of text. The transfer rules in this grammar are similar to Takeda&apos;s &amp;quot;Translation Pattern&amp;quot; (Takeda, 1996; Watanabe and Takeda, 1998) with the difference that art accepted input is necessarily also translatable. Translation grammars are made up of lexical transfer rules which contain only terminal symbols and translation templates - i.e. generalized transfer rules - which also contain variables. In the framework of Example-Based Machine Translation (EBMT), a number of methods have been proposed for inducing translation grammars from aligned texts. In so-called &amp;quot;pure&amp;quot; EBMT systems, the only available knowledge resource is the aligned text itself (cf. (Block, 2000; Brown, 1997)), while in richer systems additional, linguistic</context>
</contexts>
<marker>Watanabe, Takeda, 1998</marker>
<rawString>Hideo Watanabe and Koichi Takeda. 1998. A Pattern-based Machine Translation System Extended by Example-based Processing. In Coling 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideo Watanabe</author>
<author>Sadao Kurohashi</author>
<author>Eiji Aramaki</author>
</authors>
<title>Finding Structural Correspondences from Bilingual Parsed Corpus for Corpus-Based Translation. In Coling</title>
<date>2000</date>
<contexts>
<context position="1254" citStr="Watanabe et al., 2000" startWordPosition="172" endWordPosition="175">putation time. 1 Introduction In the framework of corpus-based machine translation, a number of methods have been proposed for automatically inducing translation correspondences from aligned texts. Some researcher parse alignments with the aim of linking word translations and internal nodes of derivation trees in both language sides of the alignment. (Wu, 1995), for instance, proposes a bilingual stochastic parser. This parser analyses both language sides of an alignment in parallel where the leaves of the binary derivation trees are terminal symbols and all internal nodes are non-terminals. (Watanabe et al., 2000) and (Meyers et al., 1996; Meyers et al., 1998) parse each language side independently and try to find most likely node correspondences in the derivation trees. Other approaches, including this present one, try to induce a set of context-free transfer rules from the alignments. This paper proposes an algorithm which generates and filters a translation grammar (i.e. a set of context-free transfer rules) from aligned and bracketed pieces of text. The transfer rules in this grammar are similar to Takeda&apos;s &amp;quot;Translation Pattern&amp;quot; (Takeda, 1996; Watanabe and Takeda, 1998) with the difference that art</context>
</contexts>
<marker>Watanabe, Kurohashi, Aramaki, 2000</marker>
<rawString>Hideo Watanabe, Sadao Kurohashi, and Eiji Aramaki. 2000. Finding Structural Correspondences from Bilingual Parsed Corpus for Corpus-Based Translation. In Coling 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Grammarless extraction of phrasal translation examples from parallel texts.</title>
<date>1995</date>
<booktitle>In TMI-95.</booktitle>
<contexts>
<context position="995" citStr="Wu, 1995" startWordPosition="133" endWordPosition="134">condition requires all translation ambiguities to be resolved in the final translation grammar. The paper examines the complexity of inducing translation grammars and proposes a number of heuristics to reduce the the theoretically exponential computation time. 1 Introduction In the framework of corpus-based machine translation, a number of methods have been proposed for automatically inducing translation correspondences from aligned texts. Some researcher parse alignments with the aim of linking word translations and internal nodes of derivation trees in both language sides of the alignment. (Wu, 1995), for instance, proposes a bilingual stochastic parser. This parser analyses both language sides of an alignment in parallel where the leaves of the binary derivation trees are terminal symbols and all internal nodes are non-terminals. (Watanabe et al., 2000) and (Meyers et al., 1996; Meyers et al., 1998) parse each language side independently and try to find most likely node correspondences in the derivation trees. Other approaches, including this present one, try to induce a set of context-free transfer rules from the alignments. This paper proposes an algorithm which generates and filters a</context>
</contexts>
<marker>Wu, 1995</marker>
<rawString>Dekai Wu. 1995. Grammarless extraction of phrasal translation examples from parallel texts. In TMI-95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Menno van Zaanen</author>
</authors>
<title>ABL: Alignment-Based Learning. In</title>
<date>2000</date>
<booktitle>COLING-2000,</booktitle>
<pages>961--967</pages>
<marker>van Zaanen, 2000</marker>
<rawString>Menno van Zaanen. 2000. ABL: Alignment-Based Learning. In COLING-2000, pages 961-967.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>