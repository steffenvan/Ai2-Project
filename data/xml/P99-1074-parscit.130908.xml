<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007325">
<title confidence="0.996346">
Robust, Finite-State Parsing for Spoken Language Understanding
</title>
<author confidence="0.997527">
Edward C. Kaiser
</author>
<affiliation confidence="0.9840015">
Center for Spoken Language Understanding
Oregon Graduate Institute
</affiliation>
<address confidence="0.938138">
PO Box 91000 Portland OR 97291
</address>
<email confidence="0.960782">
kaiserelcse.ogi.edu
</email>
<sectionHeader confidence="0.996917" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999643111111111">
Human understanding of spoken language ap-
pears to integrate the use of contextual ex-
pectations with acoustic level perception in a
tightly-coupled, sequential fashion. Yet com-
puter speech understanding systems typically
pass the transcript produced by a speech rec-
ognizer into a natural language parser with no
integration of acoustic and grammatical con-
straints. One reason for this is the complex-
ity of implementing that integration. To ad-
dress this issue we have created a robust, se-
mantic parser as a single finite-state machine
(FSM). As such, its run-time action is less com-
plex than other robust parsers that are based
on either chart or generalized left-right (GLR)
architectures. Therefore, we believe it is ul-
timately more amenable to direct integration
with a speech decoder.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.946759135593221">
An important goal in speech processing is to ex-
tract meaningful information: in this, the task
is understanding rather than transcription. For
extracting meaning from spontaneous speech
full coverage grammars tend to be too brittle.
In the 1992 DARPA ATIS task competition,
CMU&apos;s Phoenix parser was the best scoring sys-
tem (Issar and Ward, 1993). Phoenix operates
in a loosely-coupled architecture on the 1-best
transcript produced by the recognizer. Concep-
tually it is a semantic case-frame parser (Hayes
et al., 1986). As such, it allows slots within a
particular case-frame to be filled in any order,
and allows out-of-grammar words between slots
to be skipped over. Thus it can return partial
parses — as frames in which only some of the
available slots have been filled.
Humans appear to perform robust under-
standing in a tightly-coupled fashion. They
build incremental, partial analyses of an ut-
terance as it is being spoken, in a way that
helps them to meaningfully interpret the acous-
tic evidence. To move toward machine under-
standing systems that tightly-couple acoustic
features and structural knowledge, researchers
like Pereira and Wright (1997) have argued for
the use of finite-state acceptors (FSAs) as an
efficient means of integrating structural knowl-
edge into the recognition process for limited do-
main tasks.
We have constructed a parser for spontaneous
speech that is at once both robust and finite-
state. It is called PROFER, for Predictive, RO-
bust, Finite-state parsER. Currently PROFER
accepts a transcript as input. We are modifying
it to accept a word-graph as input. Our aim is
to incorporate PROFER directly into a recog-
nizer.
For example, using a grammar that defines se-
quences of numbers (each of which is less than
ten thousand and greater than ninety-nine and
contains the word &amp;quot;hundred&amp;quot;), inputs like the
following string can be robustly parsed by PRO-
FER:
Input:
first I&apos;ve got twenty ahhh thirty yaaaaaa
thirty ohh wait no twenty twenty nine
hundred two errr three ahhh four and then
two hundred ninety uhhhhh let me be sure
here yaaaa ninety seven and last is five
oh seven uhhh I mean six
Parse-tree:
CfsType:number_type,
hundred_fs:
[decade:Etwenty,nine] ,hundred,four],
hundred_fs:
[two,hundred,decade:[ninety,seven]],
hundred_fs:
[five,hundred,six]]
</bodyText>
<page confidence="0.998049">
573
</page>
<bodyText confidence="0.9998045">
There are two characteristically &amp;quot;robust&amp;quot; ac-
tions that are illustrated by this example.
</bodyText>
<listItem confidence="0.98068525">
• For each &amp;quot;slot&amp;quot; (i.e., &amp;quot;Is&amp;quot; element) filled in
the parse-tree&apos;s case-frame structure, there
were several words both before and after
the required word, hundred, that had to be
skipped-over. This aspect of robust parsing
is akin to phrase-spotting.
• In mapping the words, &amp;quot;five oh seven uhhh
I mean six,&amp;quot; the parser had to choose a
later-in-the-input parse (i.e., &amp;quot;[five, hun-
dred, six]&amp;quot;) over a heuristically equivalent
earlier-in-the-input parse (i.e., &amp;quot;[five, hun-
dred, seven]&amp;quot;). This aspect of robust pars-
ing is akin to dynamic programming (i.e.,
finding all possible start and end points
for all possible patterns and choosing the
best).
</listItem>
<sectionHeader confidence="0.640805" genericHeader="introduction">
2 Robust Finite-state Parsing
</sectionHeader>
<bodyText confidence="0.978313466666667">
CMU&apos;s Phoenix system is implemented as a re-
cursive transition network (RTN). This is sim-
ilar to Abney&apos;s system of finite-state-cascades
(1996). Both parsers have a &amp;quot;strata!&amp;quot; system of
levels. Both are robust in the sense of skipping
over out-of-grammar areas, and building up
structural islands of certainty. And both can be
fairly described as run-time chart-parsers. How-
ever, Abney&apos;s system inserts bracketing and tag-
ging information by means of cascaded trans-
ducers, whereas Phoenix accomplishes the same
thing by storing state information in the chart
edges themselves — thus using the chart edges
like tokens. PROFER is similar to Phoenix in
this regard.
Phoenix performs a depth-first search over its
textual input, while Abney&apos;s &amp;quot;chunking&amp;quot; and
&amp;quot;attaching&amp;quot; parsers perform best-first searches
(1991). However, the demands of a tightly-
coupled, real-time system argue for a breadth-
first search-strategy, which in turn argues for
the use of a finite-state parser, as an efficient
means of supporting such a search strategy.
PROFER is a strictly sequential, breadth-first
parser.
PROFER uses a regular grammar formalism
for defining the patterns that it will parse from
the input, as illustrated in Figures 1 and 2.
Net name tags correspond to bracketed (i.e.,
&amp;quot;tagged&amp;quot;) elements in the output. Aside from
</bodyText>
<figureCaption confidence="0.997766">
Figure 1: Formalism
</figureCaption>
<bodyText confidence="0.998791357142857">
net names, a grammar definition can also con-
tain non-terminal rewrite names and terminals.
Terminals are directly matched against input.
Non-terminal rewrite names group together sev-
eral rewrite patterns (see Figure 2), just as net
names can be used to do, but rewrite names do
not appear in the output.
Each individual rewrite pattern defines a
&amp;quot;conjunction&amp;quot; of particular terms or sub-
patterns that can be mapped from the input
into the non-terminal at the head of the pattern
block, as illustrated in (Figure 1). Whereas, the
list of patterns within a block represents a &amp;quot;dis-
junction&amp;quot; (Figure 2).
</bodyText>
<figureCaption confidence="0.986309">
Figure 2: Formalism
</figureCaption>
<bodyText confidence="0.99957">
Since not all Context-Free Grammar (CFG)
expressions can be translated into regular ex-
pressions, as illustrated in Figure 3, some re-
strictions are necessary to rule out the possibil-
ity of &amp;quot;center-embedding&amp;quot; (see the right-most
block in Figure 3). The restriction is that nei-
ther a net name nor a rewrite name can appear
in one of its own descendant blocks of rewrite
patterns.
Even with this restriction it is still possible
to define regular grammars that allow for self-
</bodyText>
<figure confidence="0.984693194444445">
net name corresponds
one* te&apos;file,ntiMe stem
rewrite patterns
Cirinumar-
Deflnjtons
&amp;quot;anPi
InP1-
(EPT-IF;P%c-•-
\ ((deg DI])
A
7 - -
File
Names
ra
nAgra
junction:,both
ml must
--- --------
Grammar
Definitions
File
:Names:
Disjw*ctlofl: -------------
or (b)may.appear.
idgra
non-terminal
rewrite
(id] IGITC*-DAL,PHA )
ópdo 1)
, *-F zero-or-mere
(two) ::.:&amp;quot;one«orinpre
574
Lefl-lineer
CPC;
Case-
frame
</figure>
<figureCaption confidence="0.999941">
Figure 3: Context-Free translations to case-frame style regular expressions.
</figureCaption>
<bodyText confidence="0.999823125">
embedding to any finite depth, by copying the
net or rewrite definition and giving it a unique
name for each level of self-embedding desired.
For example, both grammars illustrated in Fig-
ure 4 can robustly parse inputs that contain
some number of a&apos;s followed by a matching
number of b&apos;s up to the level of embedding de-
fined, which in both of these cases is four deep.
</bodyText>
<figure confidence="0.995448875">
EXAMPLE: nets EXAMPLE: rewrites
[se] [ser]
(a [se_one] b) (a SE_ONE b)
(a b) (a b)
[se_one] SE_ONE
(a [se_two] b) (a SE_TWO b)
(a b) (a b)
[se_two] SE_TWO
(a [se_tbree] b) (a SE_THREE b)
(a b) (a b)
[se_three] SE_THREE
(a b) (a b)
INPUT: INPUT:
acabdeb acabdeb
PARSE: PARSE:
se:[a,se_one:[a,b],b] ser:fa,a,b,b]
</figure>
<figureCaption confidence="0.999833">
Figure 4: Finite self-embedding.
</figureCaption>
<sectionHeader confidence="0.985847" genericHeader="method">
3 The Power of Regular Grammars
</sectionHeader>
<bodyText confidence="0.997774842105263">
Tomita (1986) has argued that context-free
grammars (CFGs) are over-powered for natu-
ral language. Chart parsers are designed to
deal with the worst case of very-deep or infi-
nite self-embedding allowed by CFGs. How-
ever, in natural language this worst case does
not occur. Thus, broad coverage Generalized
Left-Right (GLR) parsers based on Tomita&apos;s al-
gorithm, which ignore the worst case scenario,
are in practice more efficient and faster than
comparable chart-parsers (Briscoe and Carroll,
1993).
PROFER explicitly disallows the worst case
of center-self-embedding that Tomita&apos;s GLR de-
sign allows — but ignores. Aside from infinite
center-self-embedding, a regular grammar for-
malism like PROFER&apos;s can be used to define
every pattern in natural language definable by
a GLR parser.
</bodyText>
<sectionHeader confidence="0.99637" genericHeader="method">
4 The Compilation Process
</sectionHeader>
<bodyText confidence="0.999554">
The following small grammar will serve as the
basis for a high-level description of the compi-
lation process.
</bodyText>
<equation confidence="0.4116394">
Cs]
(n Dr] n)
[v] p)
[v]
(v)
</equation>
<bodyText confidence="0.983534157894737">
In Kaiser et al. (1999) the relationship be-
tween PROFER&apos;s compilation process and that
of both Pereira and Wright&apos;s (1997) FSAs and
CMU&apos;s Phoenix system has been described.
Here we wish to describe what happens dur-
ing PROFER&apos;s compilation stage in terms of
the Left-Right parsing notions of item-set for-
mation and reduction.
As compilation begins the FSM always starts
at state 0:0 (i.e., net 0, start state 0) and tra-
verses an arc labeled by the top-level net name
to the 0:1 state (i.e., net 0, final state 1), as il-
lustrated in Figure 5. This initial arc is then re-
written by each of its rewrite patterns (Fig-
ure 5).
As each new net within the grammar descrip-
tion is encountered it receives a unique net-ID
number, the compilation descends recursively
into that new sub-net (Figure 5), reads in its
</bodyText>
<page confidence="0.996259">
575
</page>
<figureCaption confidence="0.9999">
Figure 5: Definition expansion.
</figureCaption>
<bodyText confidence="0.999133428571429">
grammar description file, and compiles it. Since
rewrite names are unique only within the net in
which they appear, they can be processed iter-
atively during compilation, whereas net names
must be processed recursively within the scope
of the entire grammar&apos;s definition to allow for
re-use.
As each element within a rewrite pattern
is encountered a structure describing its exact
context is filled in. All terminals that appear
in the same context are grouped together as a
&amp;quot;context-group&amp;quot; or simply &amp;quot;context.&amp;quot; So arcs in
the final FSM are traversed by &amp;quot;contexts&amp;quot; not
terminals.
When a net name itself traverses an arc it is
glued into place contextually with e arcs (i.e.,
NULL arcs) (Figure 6). Since net names, like
any other pattern element, are wrapped inside
of a context structure before being situated in
the FSM, the same net name can be re-used
inside of many different contexts, as in Figure 6.
</bodyText>
<figureCaption confidence="0.998536">
Figure 6: Contextualizing sub-nets.
</figureCaption>
<bodyText confidence="0.999642777777778">
As the end of each net definition file is
reached, all of its NULL arcs are removed. Each
initial state of a sub-net is assumed into its par-
ent state — which is equivalent to item-set for-
mation in that parent state (Figure 7 left-side).
Each final state of a sub-net is erased, and its
incoming arcs are rerouted to its terminal par-
ent&apos;s state, thus performing a reduction (Fig-
ure 7 right-side).
</bodyText>
<sectionHeader confidence="0.983212" genericHeader="method">
5 The Parsing Process
</sectionHeader>
<bodyText confidence="0.999971793103448">
At run-time, the parse proceeds in a strictly
breadth-first manner (Figure 8,(Kaiser et al.,
1999)). Each destination state within a parse
is named by a hash-table key string com-
posed of a sequence of &amp;quot;net:state&amp;quot; combina-
tions that uniquely identify the location of that
state within the FSM (see Figure 8). These
&amp;quot;net:state&amp;quot; names effectively represent a snap-
shot of the stack-configuration that would be
seen in a parallel GLR parser.
PROFER deals with ambiguity by &amp;quot;split-
ting&amp;quot; the branches of its graph-structured stack
(as is done in a Generalized Left-Right parser
(Tomita, 1986)). Each node within the graph-
structured stack holds a &amp;quot;token&amp;quot; that records
the information needed to build a bracketed
parse-tree for any given branch.
When partial-paths converge on the same
state within the FSM they are scored heuris-
tically, and all but the set of highest scoring
partial paths are pruned away. Currently the
heuristics favor interpretations that cover the
most input with the fewest slots. Command line
parameters can be used to refine the heuristics,
so that certain kinds of structures be either min-
imized or maximized over the parse.
Robustness within this scheme is achieved by
allowing multiple paths to be propagated in par-
allel across the input space. And as each such
</bodyText>
<figure confidence="0.9976213">
!;;74.ratiOfitiOtio*
fa4 ii,!!1.ffsed
• rho;i #00:40;b:serr.fmt
iatiacs,Owi
• &amp;quot; 001110:1111111*4,
i11
net
:Conloixtfarei
(sub40,1*to4iikesath#4..h.
Remert*tpormf#041
</figure>
<figureCaption confidence="0.993818">
Figure 7: Removing NULL arcs.
</figureCaption>
<page confidence="0.978257">
576
</page>
<table confidence="0.9950384">
Predictions: f1,p 1., _ _iv —1
. _
Input: 9&apos;1- 11 .
Prefix Path: 0:0J i --&gt; . 0:21:0! —.T0:3 1,-4 0:1
Parse I La :[n, ., v:Evl,
</table>
<figureCaption confidence="0.999645">
Figure 8: The parsing process.
</figureCaption>
<bodyText confidence="0.98813">
partial-path is extended, it is allowed to skip-
over terms in the input that are not licensed by
the grammar. This allows all possible start and
end times of all possible patterns to be consid-
ered.
</bodyText>
<sectionHeader confidence="0.991588" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999939405063291">
Many researchers have looked at ways to im-
prove corpus-based language modeling tech-
niques. One way is to parse the training set
with a structural parser, build statistical mod-
els of the occurrence of structural elements, and
then use these statistics to build or augment an
n-gram language model.
Gillet and Ward (1998) have reported reduc-
tions in perplexity using a stochastic context-
free grammar (SCFG) defining both simple se-
mantic &amp;quot;classes&amp;quot; like dates and times, and de-
generate classes for each individual vocabulary
word. Thus, in building up class statistics over a
corpus parsed with their grammar they are able
to capture both the traditional n-gram word se-
quences plus statistics about semantic class se-
quences.
Briscoe has pointed out that using stochas-
tic context-free grammars (SCFGs) as the ba-
sis for language modeling, &amp;quot;... means that in-
formation about the probability of a rule apply-
ing at a particular point in a parse derivation is
lost&amp;quot; (1993). For this reason Briscoe developed
a GLR parser as a more &amp;quot;natural way to obtain
a finite-state representation ...&amp;quot; on which the
statistics of individual &amp;quot;reduce&amp;quot; actions could
be determined. Since PROFER&apos;s state names
effectively represent the stack-configurations of
a parallel GLR parser it also offers the ability to
perform the full-context statistical parsing that
Briscoe has called for.
Chelba and Jelinek (1999) use a struc-
tural language model (SLM) to incorporate the
longer-range structural knowledge represented
in statistics about sequences of phrase-head-
word/non-terminal-tag elements exposed by a
tree-adjoining grammar. Unlike SCFGs their
statistics are specific to the structural context
in which head-words occur. They have shown
both reduced perplexity and improved word er-
ror rate (WER) over a conventional tri-gram
system.
One can also reduce complexity and improve
word-error rates by widening the speech recog-
nition problem to include modeling not only
the word sequence, but the word/part-of-speech
(POS) sequence. Heeman and Allen (1997) has
shown that doing so also aids in identifying
speech repairs and intonational boundaries in
spontaneous speech.
However, all of these approaches rely on
corpus-based language modeling, which is a
large and expensive task. In many practical uses
of spoken language technology, like using simple
structured dialogues for class room instruction
(as can be done with the CSLU toolkit (Sutton
et al., 1998)), corpus-based language modeling
may not be a practical possibility.
In structured dialogues one approach can
be to completely constrain recognition by the
known expectations at a given state. Indeed,
the CSLU toolkit provides a generic recognizer,
which accepts a set of vocabulary and word se-
quences defined by a regular grammar on a per-
state basis. Within this framework the task of
a recognizer is to choose the best phonetic path
through the finite-state machine defined by the
regular grammar. Out-of-vocabulary words are
accounted for by a general purpose &amp;quot;garbage&amp;quot;
phoneme model (Schalkwyk et al., 1996).
We experimented with using PROFER in the
same way; however, our initial attempts to do
so did not work well. The amount of informa-
tion carried in PROFER&apos;s token&apos;s (to allow for
bracketing and heuristic scoring of the seman-
tic hypotheses) requires structures that are an
order of magnitude larger than the tokens in
a typical acoustic recognizer. When these large
tokens are applied at the phonetic-level so many
</bodyText>
<figure confidence="0.572047">
Caseframe:
Efikv:Kni
</figure>
<page confidence="0.973112">
577
</page>
<bodyText confidence="0.999976">
are needed that a memory space explosion oc-
curs. This suggests to us that there must be two
levels of tokens: small, quickly manipulated to-
kens at the acoustic level (i.e., lexical level), and
larger, less-frequently used tokens at the struc-
tural level (i.e., syntactic, semantic, pragmatic
level).
</bodyText>
<sectionHeader confidence="0.999519" genericHeader="method">
7 Future Work
</sectionHeader>
<bodyText confidence="0.999399545454546">
In the MINDS system Young et al. (1989) re-
ported reduced word error rates and large re-
ductions in perplexity by using a dialogue struc-
ture that could track the active goals, topics
and user knowledge possible in a given dialogue
state, and use that knowledge to dynamically
create a semantic case-frame network, whose
transitions could in turn be used to constrain
the word sequences allowed by the recognizer.
Our research aim is to maximize the effective-
ness of this approach. Therefore, we hope to:
</bodyText>
<listItem confidence="0.8004378">
• expand the scope of PROFER&apos;s structural
definitions to include not only word pat-
terns, but intonation and stress patterns as
well, and
• consider how build to general language
</listItem>
<bodyText confidence="0.985853833333333">
models that complement the use of the cat-
egorial constraints PROFER can impose
(i.e., syllable-level modeling, intonational
boundary modeling, or speech repair mod-
eling).
Our immediate efforts are focused on consider-
ing how to modify PROFER to accept a word-
graph as input — at first as part of a loosely-
-coupled system, and then later as part of an
integrated system in which the elements of the
word-graph are evaluated against the structural
constraints as they are created.
</bodyText>
<sectionHeader confidence="0.998306" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.9999805">
We have presented our finite-state, robust
parser, PROFER, described some of its work-
ings, and discussed the advantages it may offer
for moving towards a tight integration of robust
natural language processing with a speech de-
coder — those advantages being: its efficiency
as an FSM and the possibility that it may pro-
vide a useful level of constraint to a recognizer
independent of a large, task-specific language
model.
</bodyText>
<sectionHeader confidence="0.995593" genericHeader="acknowledgments">
9 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999736">
The author was funded by the Intel Research
Council, the NSF (Grant No. 9354959), and
the CSLU member consortium. We also wish
to thank Peter Heeman and Michael Johnston
for valuable discussions and support.
</bodyText>
<sectionHeader confidence="0.999189" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999891384615385">
S. Abney. 1991. Parsing by chunks. In R. Berwick, S.
Abney, and C. Tenny, editors, Principle-Based Pars-
ing. Kluwer Academic Publishers.
S. Abney. 1996. Partial parsing via finite-state cas-
cades. In Proceedings of the ESSLLI &apos;96 Robust Pars-
ing Workshop.
T. Briscoe and J. Carroll, 1993. Generalized probabilis-
tic LR parsing of natural language (corpora) with
unification-based grammars. Computational Linguis-
tics, 19(1):25-59.
C. Chelba and F. Jelinek. 1999. Recognition perfor-
mance of a structured language model. In The Pro-
ceedings of Eurospeech &apos;99 (to appear), September.
J. Gillet and W. Ward. 1998. A language model combin-
ing trigrams and stochastic context-free grammars. In
Proceedings of ICSLP &apos;98, volume 6, pgs 2319-2322.
P. J. Hayes, A. G. Hauptmann, J. G. Carbonell, and M.
Tomita. 1986. Parsing spoken language: a semantic
caseframe approach. In 11th International Conference
on Computational Linguistics, Proceedings of Coling
&apos;86, pages 587-592.
P. A. Heeman and J. F. Allen. 1997. Intonational bound-
aries, speech repairs, and discourse markers: Model-
ing spoken dialog. In Proceedings of the 35th Annual
Meeting of the Association for Computational Lin-
guistics, pages 254-261.
S. Issar and W. Ward. 1993. Cmu&apos;s robust spoken lan-
guage understanding system. In Eurospeech &apos;93, pages
2147-2150.
E. Kaiser, M. Johnston, and P. Heeman. 1999. Profer:
Predictive, robust finite-state parsing for spoken lan-
guage. In Proceedings of ICASSP &apos;99.
F. C. N. Pereira and R. N. Wright. 1997. Finite-state ap-
proximations of phrase-structure grammars. In Em-
manuel Roche and Yves Schabes, editors, Finite-State
Language Processing, pages 149-173. The MIT Press.
J. Schalkwyk, L. D. Colton, and M. Fanty. 1996. The
CSLU-sh toolkit for automatic speech recognition:
Technical report no. CSLU-011-96, August.
S. Sutton, R. Cole, J. de Villiers, J. Schalkwyk, P. Ver-
meulen, M. Macon, Y. Yan, E. Kaiser, B. Rundle,
K. Shobalci, P. Hosom, A. Kain, J. Wouters, M. Mas-
saro, and M. Cohen. 1998. Universal speech tools:
the cslu toolkit&amp;quot;. In Proceedings of ICSLP &apos;98, pages
3221-3224, Nov..
M. Tomita. 1986. Efficient Parsing for Natural Lan-
guage: A Fast Algorithm for Practical Systems.
Kluwer Academic Publishers.
S. R. Young, A. G. Hauptmann, W. H. Ward, E. T.
Smith, and P. Werner. 1989. High level knowledge
sources in usable speech recognition systems. Com-
munications of the ACM, 32(2):183-194, February.
</reference>
<page confidence="0.996701">
578
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.982593">
<title confidence="0.99995">Robust, Finite-State Parsing for Spoken Language Understanding</title>
<author confidence="0.999928">Edward C Kaiser</author>
<affiliation confidence="0.9990555">Center for Spoken Language Understanding Oregon Graduate Institute</affiliation>
<address confidence="0.999917">91000 Portland OR 97291</address>
<email confidence="0.999713">kaiserelcse.ogi.edu</email>
<abstract confidence="0.999184578947368">Human understanding of spoken language appears to integrate the use of contextual expectations with acoustic level perception in a tightly-coupled, sequential fashion. Yet computer speech understanding systems typically pass the transcript produced by a speech recognizer into a natural language parser with no integration of acoustic and grammatical constraints. One reason for this is the complexity of implementing that integration. To address this issue we have created a robust, semantic parser as a single finite-state machine (FSM). As such, its run-time action is less complex than other robust parsers that are based on either chart or generalized left-right (GLR) architectures. Therefore, we believe it is ultimately more amenable to direct integration with a speech decoder.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abney</author>
</authors>
<title>Parsing by chunks. In</title>
<date>1991</date>
<editor>R. Berwick, S. Abney, and C. Tenny, editors, Principle-Based Parsing.</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<marker>Abney, 1991</marker>
<rawString>S. Abney. 1991. Parsing by chunks. In R. Berwick, S. Abney, and C. Tenny, editors, Principle-Based Parsing. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Abney</author>
</authors>
<title>Partial parsing via finite-state cascades.</title>
<date>1996</date>
<booktitle>In Proceedings of the ESSLLI &apos;96 Robust Parsing Workshop.</booktitle>
<marker>Abney, 1996</marker>
<rawString>S. Abney. 1996. Partial parsing via finite-state cascades. In Proceedings of the ESSLLI &apos;96 Robust Parsing Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Briscoe</author>
<author>J Carroll</author>
</authors>
<title>Generalized probabilistic LR parsing of natural language (corpora) with unification-based grammars.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--1</pages>
<contexts>
<context position="8191" citStr="Briscoe and Carroll, 1993" startWordPosition="1289" endWordPosition="1292">acabdeb acabdeb PARSE: PARSE: se:[a,se_one:[a,b],b] ser:fa,a,b,b] Figure 4: Finite self-embedding. 3 The Power of Regular Grammars Tomita (1986) has argued that context-free grammars (CFGs) are over-powered for natural language. Chart parsers are designed to deal with the worst case of very-deep or infinite self-embedding allowed by CFGs. However, in natural language this worst case does not occur. Thus, broad coverage Generalized Left-Right (GLR) parsers based on Tomita&apos;s algorithm, which ignore the worst case scenario, are in practice more efficient and faster than comparable chart-parsers (Briscoe and Carroll, 1993). PROFER explicitly disallows the worst case of center-self-embedding that Tomita&apos;s GLR design allows — but ignores. Aside from infinite center-self-embedding, a regular grammar formalism like PROFER&apos;s can be used to define every pattern in natural language definable by a GLR parser. 4 The Compilation Process The following small grammar will serve as the basis for a high-level description of the compilation process. Cs] (n Dr] n) [v] p) [v] (v) In Kaiser et al. (1999) the relationship between PROFER&apos;s compilation process and that of both Pereira and Wright&apos;s (1997) FSAs and CMU&apos;s Phoenix syste</context>
</contexts>
<marker>Briscoe, Carroll, 1993</marker>
<rawString>T. Briscoe and J. Carroll, 1993. Generalized probabilistic LR parsing of natural language (corpora) with unification-based grammars. Computational Linguistics, 19(1):25-59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Chelba</author>
<author>F Jelinek</author>
</authors>
<title>Recognition performance of a structured language model.</title>
<date>1999</date>
<booktitle>In The Proceedings of Eurospeech &apos;99</booktitle>
<note>(to appear),</note>
<contexts>
<context position="14068" citStr="Chelba and Jelinek (1999)" startWordPosition="2265" endWordPosition="2268">tochastic context-free grammars (SCFGs) as the basis for language modeling, &amp;quot;... means that information about the probability of a rule applying at a particular point in a parse derivation is lost&amp;quot; (1993). For this reason Briscoe developed a GLR parser as a more &amp;quot;natural way to obtain a finite-state representation ...&amp;quot; on which the statistics of individual &amp;quot;reduce&amp;quot; actions could be determined. Since PROFER&apos;s state names effectively represent the stack-configurations of a parallel GLR parser it also offers the ability to perform the full-context statistical parsing that Briscoe has called for. Chelba and Jelinek (1999) use a structural language model (SLM) to incorporate the longer-range structural knowledge represented in statistics about sequences of phrase-headword/non-terminal-tag elements exposed by a tree-adjoining grammar. Unlike SCFGs their statistics are specific to the structural context in which head-words occur. They have shown both reduced perplexity and improved word error rate (WER) over a conventional tri-gram system. One can also reduce complexity and improve word-error rates by widening the speech recognition problem to include modeling not only the word sequence, but the word/part-of-spee</context>
</contexts>
<marker>Chelba, Jelinek, 1999</marker>
<rawString>C. Chelba and F. Jelinek. 1999. Recognition performance of a structured language model. In The Proceedings of Eurospeech &apos;99 (to appear), September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gillet</author>
<author>W Ward</author>
</authors>
<title>A language model combining trigrams and stochastic context-free grammars.</title>
<date>1998</date>
<booktitle>In Proceedings of ICSLP &apos;98,</booktitle>
<volume>6</volume>
<pages>2319--2322</pages>
<contexts>
<context position="13005" citStr="Gillet and Ward (1998)" startWordPosition="2096" endWordPosition="2099">:0J i --&gt; . 0:21:0! —.T0:3 1,-4 0:1 Parse I La :[n, ., v:Evl, Figure 8: The parsing process. partial-path is extended, it is allowed to skipover terms in the input that are not licensed by the grammar. This allows all possible start and end times of all possible patterns to be considered. 6 Discussion Many researchers have looked at ways to improve corpus-based language modeling techniques. One way is to parse the training set with a structural parser, build statistical models of the occurrence of structural elements, and then use these statistics to build or augment an n-gram language model. Gillet and Ward (1998) have reported reductions in perplexity using a stochastic contextfree grammar (SCFG) defining both simple semantic &amp;quot;classes&amp;quot; like dates and times, and degenerate classes for each individual vocabulary word. Thus, in building up class statistics over a corpus parsed with their grammar they are able to capture both the traditional n-gram word sequences plus statistics about semantic class sequences. Briscoe has pointed out that using stochastic context-free grammars (SCFGs) as the basis for language modeling, &amp;quot;... means that information about the probability of a rule applying at a particular p</context>
</contexts>
<marker>Gillet, Ward, 1998</marker>
<rawString>J. Gillet and W. Ward. 1998. A language model combining trigrams and stochastic context-free grammars. In Proceedings of ICSLP &apos;98, volume 6, pgs 2319-2322.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Hayes</author>
<author>A G Hauptmann</author>
<author>J G Carbonell</author>
<author>M Tomita</author>
</authors>
<title>Parsing spoken language: a semantic caseframe approach.</title>
<date>1986</date>
<booktitle>In 11th International Conference on Computational Linguistics, Proceedings of Coling &apos;86,</booktitle>
<pages>587--592</pages>
<contexts>
<context position="1528" citStr="Hayes et al., 1986" startWordPosition="230" endWordPosition="233"> believe it is ultimately more amenable to direct integration with a speech decoder. 1 Introduction An important goal in speech processing is to extract meaningful information: in this, the task is understanding rather than transcription. For extracting meaning from spontaneous speech full coverage grammars tend to be too brittle. In the 1992 DARPA ATIS task competition, CMU&apos;s Phoenix parser was the best scoring system (Issar and Ward, 1993). Phoenix operates in a loosely-coupled architecture on the 1-best transcript produced by the recognizer. Conceptually it is a semantic case-frame parser (Hayes et al., 1986). As such, it allows slots within a particular case-frame to be filled in any order, and allows out-of-grammar words between slots to be skipped over. Thus it can return partial parses — as frames in which only some of the available slots have been filled. Humans appear to perform robust understanding in a tightly-coupled fashion. They build incremental, partial analyses of an utterance as it is being spoken, in a way that helps them to meaningfully interpret the acoustic evidence. To move toward machine understanding systems that tightly-couple acoustic features and structural knowledge, rese</context>
</contexts>
<marker>Hayes, Hauptmann, Carbonell, Tomita, 1986</marker>
<rawString>P. J. Hayes, A. G. Hauptmann, J. G. Carbonell, and M. Tomita. 1986. Parsing spoken language: a semantic caseframe approach. In 11th International Conference on Computational Linguistics, Proceedings of Coling &apos;86, pages 587-592.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P A Heeman</author>
<author>J F Allen</author>
</authors>
<title>Intonational boundaries, speech repairs, and discourse markers: Modeling spoken dialog.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>254--261</pages>
<contexts>
<context position="14710" citStr="Heeman and Allen (1997)" startWordPosition="2357" endWordPosition="2360">anguage model (SLM) to incorporate the longer-range structural knowledge represented in statistics about sequences of phrase-headword/non-terminal-tag elements exposed by a tree-adjoining grammar. Unlike SCFGs their statistics are specific to the structural context in which head-words occur. They have shown both reduced perplexity and improved word error rate (WER) over a conventional tri-gram system. One can also reduce complexity and improve word-error rates by widening the speech recognition problem to include modeling not only the word sequence, but the word/part-of-speech (POS) sequence. Heeman and Allen (1997) has shown that doing so also aids in identifying speech repairs and intonational boundaries in spontaneous speech. However, all of these approaches rely on corpus-based language modeling, which is a large and expensive task. In many practical uses of spoken language technology, like using simple structured dialogues for class room instruction (as can be done with the CSLU toolkit (Sutton et al., 1998)), corpus-based language modeling may not be a practical possibility. In structured dialogues one approach can be to completely constrain recognition by the known expectations at a given state. I</context>
</contexts>
<marker>Heeman, Allen, 1997</marker>
<rawString>P. A. Heeman and J. F. Allen. 1997. Intonational boundaries, speech repairs, and discourse markers: Modeling spoken dialog. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, pages 254-261.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Issar</author>
<author>W Ward</author>
</authors>
<title>Cmu&apos;s robust spoken language understanding system.</title>
<date>1993</date>
<booktitle>In Eurospeech &apos;93,</booktitle>
<pages>2147--2150</pages>
<contexts>
<context position="1354" citStr="Issar and Ward, 1993" startWordPosition="204" endWordPosition="207">achine (FSM). As such, its run-time action is less complex than other robust parsers that are based on either chart or generalized left-right (GLR) architectures. Therefore, we believe it is ultimately more amenable to direct integration with a speech decoder. 1 Introduction An important goal in speech processing is to extract meaningful information: in this, the task is understanding rather than transcription. For extracting meaning from spontaneous speech full coverage grammars tend to be too brittle. In the 1992 DARPA ATIS task competition, CMU&apos;s Phoenix parser was the best scoring system (Issar and Ward, 1993). Phoenix operates in a loosely-coupled architecture on the 1-best transcript produced by the recognizer. Conceptually it is a semantic case-frame parser (Hayes et al., 1986). As such, it allows slots within a particular case-frame to be filled in any order, and allows out-of-grammar words between slots to be skipped over. Thus it can return partial parses — as frames in which only some of the available slots have been filled. Humans appear to perform robust understanding in a tightly-coupled fashion. They build incremental, partial analyses of an utterance as it is being spoken, in a way that</context>
</contexts>
<marker>Issar, Ward, 1993</marker>
<rawString>S. Issar and W. Ward. 1993. Cmu&apos;s robust spoken language understanding system. In Eurospeech &apos;93, pages 2147-2150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Kaiser</author>
<author>M Johnston</author>
<author>P Heeman</author>
</authors>
<title>Profer: Predictive, robust finite-state parsing for spoken language.</title>
<date>1999</date>
<booktitle>In Proceedings of ICASSP &apos;99.</booktitle>
<contexts>
<context position="8663" citStr="Kaiser et al. (1999)" startWordPosition="1367" endWordPosition="1370">algorithm, which ignore the worst case scenario, are in practice more efficient and faster than comparable chart-parsers (Briscoe and Carroll, 1993). PROFER explicitly disallows the worst case of center-self-embedding that Tomita&apos;s GLR design allows — but ignores. Aside from infinite center-self-embedding, a regular grammar formalism like PROFER&apos;s can be used to define every pattern in natural language definable by a GLR parser. 4 The Compilation Process The following small grammar will serve as the basis for a high-level description of the compilation process. Cs] (n Dr] n) [v] p) [v] (v) In Kaiser et al. (1999) the relationship between PROFER&apos;s compilation process and that of both Pereira and Wright&apos;s (1997) FSAs and CMU&apos;s Phoenix system has been described. Here we wish to describe what happens during PROFER&apos;s compilation stage in terms of the Left-Right parsing notions of item-set formation and reduction. As compilation begins the FSM always starts at state 0:0 (i.e., net 0, start state 0) and traverses an arc labeled by the top-level net name to the 0:1 state (i.e., net 0, final state 1), as illustrated in Figure 5. This initial arc is then rewritten by each of its rewrite patterns (Figure 5). As </context>
<context position="10936" citStr="Kaiser et al., 1999" startWordPosition="1757" endWordPosition="1760">e can be re-used inside of many different contexts, as in Figure 6. Figure 6: Contextualizing sub-nets. As the end of each net definition file is reached, all of its NULL arcs are removed. Each initial state of a sub-net is assumed into its parent state — which is equivalent to item-set formation in that parent state (Figure 7 left-side). Each final state of a sub-net is erased, and its incoming arcs are rerouted to its terminal parent&apos;s state, thus performing a reduction (Figure 7 right-side). 5 The Parsing Process At run-time, the parse proceeds in a strictly breadth-first manner (Figure 8,(Kaiser et al., 1999)). Each destination state within a parse is named by a hash-table key string composed of a sequence of &amp;quot;net:state&amp;quot; combinations that uniquely identify the location of that state within the FSM (see Figure 8). These &amp;quot;net:state&amp;quot; names effectively represent a snapshot of the stack-configuration that would be seen in a parallel GLR parser. PROFER deals with ambiguity by &amp;quot;splitting&amp;quot; the branches of its graph-structured stack (as is done in a Generalized Left-Right parser (Tomita, 1986)). Each node within the graphstructured stack holds a &amp;quot;token&amp;quot; that records the information needed to build a bracke</context>
</contexts>
<marker>Kaiser, Johnston, Heeman, 1999</marker>
<rawString>E. Kaiser, M. Johnston, and P. Heeman. 1999. Profer: Predictive, robust finite-state parsing for spoken language. In Proceedings of ICASSP &apos;99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F C N Pereira</author>
<author>R N Wright</author>
</authors>
<title>Finite-state approximations of phrase-structure grammars.</title>
<date>1997</date>
<booktitle>In Emmanuel Roche and Yves Schabes, editors, Finite-State Language Processing,</booktitle>
<pages>149--173</pages>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="2166" citStr="Pereira and Wright (1997)" startWordPosition="333" endWordPosition="336"> allows slots within a particular case-frame to be filled in any order, and allows out-of-grammar words between slots to be skipped over. Thus it can return partial parses — as frames in which only some of the available slots have been filled. Humans appear to perform robust understanding in a tightly-coupled fashion. They build incremental, partial analyses of an utterance as it is being spoken, in a way that helps them to meaningfully interpret the acoustic evidence. To move toward machine understanding systems that tightly-couple acoustic features and structural knowledge, researchers like Pereira and Wright (1997) have argued for the use of finite-state acceptors (FSAs) as an efficient means of integrating structural knowledge into the recognition process for limited domain tasks. We have constructed a parser for spontaneous speech that is at once both robust and finitestate. It is called PROFER, for Predictive, RObust, Finite-state parsER. Currently PROFER accepts a transcript as input. We are modifying it to accept a word-graph as input. Our aim is to incorporate PROFER directly into a recognizer. For example, using a grammar that defines sequences of numbers (each of which is less than ten thousand </context>
</contexts>
<marker>Pereira, Wright, 1997</marker>
<rawString>F. C. N. Pereira and R. N. Wright. 1997. Finite-state approximations of phrase-structure grammars. In Emmanuel Roche and Yves Schabes, editors, Finite-State Language Processing, pages 149-173. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Schalkwyk</author>
<author>L D Colton</author>
<author>M Fanty</author>
</authors>
<title>The CSLU-sh toolkit for automatic speech recognition:</title>
<date>1996</date>
<tech>Technical report no. CSLU-011-96,</tech>
<contexts>
<context position="15727" citStr="Schalkwyk et al., 1996" startWordPosition="2516" endWordPosition="2519"> al., 1998)), corpus-based language modeling may not be a practical possibility. In structured dialogues one approach can be to completely constrain recognition by the known expectations at a given state. Indeed, the CSLU toolkit provides a generic recognizer, which accepts a set of vocabulary and word sequences defined by a regular grammar on a perstate basis. Within this framework the task of a recognizer is to choose the best phonetic path through the finite-state machine defined by the regular grammar. Out-of-vocabulary words are accounted for by a general purpose &amp;quot;garbage&amp;quot; phoneme model (Schalkwyk et al., 1996). We experimented with using PROFER in the same way; however, our initial attempts to do so did not work well. The amount of information carried in PROFER&apos;s token&apos;s (to allow for bracketing and heuristic scoring of the semantic hypotheses) requires structures that are an order of magnitude larger than the tokens in a typical acoustic recognizer. When these large tokens are applied at the phonetic-level so many Caseframe: Efikv:Kni 577 are needed that a memory space explosion occurs. This suggests to us that there must be two levels of tokens: small, quickly manipulated tokens at the acoustic l</context>
</contexts>
<marker>Schalkwyk, Colton, Fanty, 1996</marker>
<rawString>J. Schalkwyk, L. D. Colton, and M. Fanty. 1996. The CSLU-sh toolkit for automatic speech recognition: Technical report no. CSLU-011-96, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sutton</author>
<author>R Cole</author>
<author>J de Villiers</author>
<author>J Schalkwyk</author>
<author>P Vermeulen</author>
<author>M Macon</author>
<author>Y Yan</author>
<author>E Kaiser</author>
<author>B Rundle</author>
<author>K Shobalci</author>
<author>P Hosom</author>
<author>A Kain</author>
<author>J Wouters</author>
<author>M Massaro</author>
<author>M Cohen</author>
</authors>
<title>Universal speech tools: the cslu toolkit&amp;quot;.</title>
<date>1998</date>
<booktitle>In Proceedings of ICSLP &apos;98,</booktitle>
<pages>3221--3224</pages>
<marker>Sutton, Cole, de Villiers, Schalkwyk, Vermeulen, Macon, Yan, Kaiser, Rundle, Shobalci, Hosom, Kain, Wouters, Massaro, Cohen, 1998</marker>
<rawString>S. Sutton, R. Cole, J. de Villiers, J. Schalkwyk, P. Vermeulen, M. Macon, Y. Yan, E. Kaiser, B. Rundle, K. Shobalci, P. Hosom, A. Kain, J. Wouters, M. Massaro, and M. Cohen. 1998. Universal speech tools: the cslu toolkit&amp;quot;. In Proceedings of ICSLP &apos;98, pages 3221-3224, Nov..</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tomita</author>
</authors>
<title>Efficient Parsing for Natural Language: A Fast Algorithm for Practical Systems.</title>
<date>1986</date>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="7709" citStr="Tomita (1986)" startWordPosition="1217" endWordPosition="1218"> example, both grammars illustrated in Figure 4 can robustly parse inputs that contain some number of a&apos;s followed by a matching number of b&apos;s up to the level of embedding defined, which in both of these cases is four deep. EXAMPLE: nets EXAMPLE: rewrites [se] [ser] (a [se_one] b) (a SE_ONE b) (a b) (a b) [se_one] SE_ONE (a [se_two] b) (a SE_TWO b) (a b) (a b) [se_two] SE_TWO (a [se_tbree] b) (a SE_THREE b) (a b) (a b) [se_three] SE_THREE (a b) (a b) INPUT: INPUT: acabdeb acabdeb PARSE: PARSE: se:[a,se_one:[a,b],b] ser:fa,a,b,b] Figure 4: Finite self-embedding. 3 The Power of Regular Grammars Tomita (1986) has argued that context-free grammars (CFGs) are over-powered for natural language. Chart parsers are designed to deal with the worst case of very-deep or infinite self-embedding allowed by CFGs. However, in natural language this worst case does not occur. Thus, broad coverage Generalized Left-Right (GLR) parsers based on Tomita&apos;s algorithm, which ignore the worst case scenario, are in practice more efficient and faster than comparable chart-parsers (Briscoe and Carroll, 1993). PROFER explicitly disallows the worst case of center-self-embedding that Tomita&apos;s GLR design allows — but ignores. A</context>
<context position="11421" citStr="Tomita, 1986" startWordPosition="1838" endWordPosition="1839">-side). 5 The Parsing Process At run-time, the parse proceeds in a strictly breadth-first manner (Figure 8,(Kaiser et al., 1999)). Each destination state within a parse is named by a hash-table key string composed of a sequence of &amp;quot;net:state&amp;quot; combinations that uniquely identify the location of that state within the FSM (see Figure 8). These &amp;quot;net:state&amp;quot; names effectively represent a snapshot of the stack-configuration that would be seen in a parallel GLR parser. PROFER deals with ambiguity by &amp;quot;splitting&amp;quot; the branches of its graph-structured stack (as is done in a Generalized Left-Right parser (Tomita, 1986)). Each node within the graphstructured stack holds a &amp;quot;token&amp;quot; that records the information needed to build a bracketed parse-tree for any given branch. When partial-paths converge on the same state within the FSM they are scored heuristically, and all but the set of highest scoring partial paths are pruned away. Currently the heuristics favor interpretations that cover the most input with the fewest slots. Command line parameters can be used to refine the heuristics, so that certain kinds of structures be either minimized or maximized over the parse. Robustness within this scheme is achieved b</context>
</contexts>
<marker>Tomita, 1986</marker>
<rawString>M. Tomita. 1986. Efficient Parsing for Natural Language: A Fast Algorithm for Practical Systems. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S R Young</author>
<author>A G Hauptmann</author>
<author>W H Ward</author>
<author>E T Smith</author>
<author>P Werner</author>
</authors>
<title>High level knowledge sources in usable speech recognition systems.</title>
<date>1989</date>
<journal>Communications of the ACM,</journal>
<pages>32--2</pages>
<contexts>
<context position="16518" citStr="Young et al. (1989)" startWordPosition="2647" endWordPosition="2650">ow for bracketing and heuristic scoring of the semantic hypotheses) requires structures that are an order of magnitude larger than the tokens in a typical acoustic recognizer. When these large tokens are applied at the phonetic-level so many Caseframe: Efikv:Kni 577 are needed that a memory space explosion occurs. This suggests to us that there must be two levels of tokens: small, quickly manipulated tokens at the acoustic level (i.e., lexical level), and larger, less-frequently used tokens at the structural level (i.e., syntactic, semantic, pragmatic level). 7 Future Work In the MINDS system Young et al. (1989) reported reduced word error rates and large reductions in perplexity by using a dialogue structure that could track the active goals, topics and user knowledge possible in a given dialogue state, and use that knowledge to dynamically create a semantic case-frame network, whose transitions could in turn be used to constrain the word sequences allowed by the recognizer. Our research aim is to maximize the effectiveness of this approach. Therefore, we hope to: • expand the scope of PROFER&apos;s structural definitions to include not only word patterns, but intonation and stress patterns as well, and </context>
</contexts>
<marker>Young, Hauptmann, Ward, Smith, Werner, 1989</marker>
<rawString>S. R. Young, A. G. Hauptmann, W. H. Ward, E. T. Smith, and P. Werner. 1989. High level knowledge sources in usable speech recognition systems. Communications of the ACM, 32(2):183-194, February.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>