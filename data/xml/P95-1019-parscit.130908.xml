<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000258">
<title confidence="0.997075">
Response Generation in Collaborative Negotiation*
</title>
<author confidence="0.983326">
Jennifer Chu-Carroll and Sandra Carberry
</author>
<affiliation confidence="0.9812665">
Department of Computer and Information Sciences
University of Delaware
</affiliation>
<address confidence="0.580997">
Newark, DE 19716, USA
</address>
<email confidence="0.999212">
E-mail: tjchu,carberryl@cis.udel.edu
</email>
<sectionHeader confidence="0.996965" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999854952380952">
In collaborative planning activities, since the
agents are autonomous and heterogeneous, it
is inevitable that conflicts arise in their beliefs
during the planning process. In cases where
such conflicts are relevant to the task at hand,
the agents should engage in collaborative ne-
gotiation as an attempt to square away the dis-
crepancies in their beliefs. This paper presents
a computational strategy for detecting conflicts
regarding proposed beliefs and for engaging
in collaborative negotiation to resolve the con-
flicts that warrant resolution. Our model is
capable of selecting the most effective aspect
to address in its pursuit of conflict resolution in
cases where multiple conflicts arise, and of se-
lecting appropriate evidence to justify the need
for such modification. Furthermore, by cap-
turing the negotiation process in a recursive
Propose-Evaluate-Mod(y cycle of actions, our
model can successfully handle embedded ne-
gotiation subdialogues.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999947076923077">
In collaborative consultation dialogues, the consultant
and the executing agent collaborate on developing a plan
to achieve the executing agent&apos;s domain goal. Since
agents are autonomous and heterogeneous, it is inevitable
that conflicts in their beliefs arise during the planning pro-
cess. In such cases, collaborative agents should attempt
to square away (Joshi, 1982) the conflicts by engaging in
collaborative negotiation to determine what should con-
stitute their shared plan of actions and shared beliefs.
Collaborative negotiation differs from non-collaborative
negotiation and argumentation mainly in the attitude of
the participants, since collaborative agents are not self-
centered, but act in a way as to benefit the agents as
</bodyText>
<subsectionHeader confidence="0.5855245">
This material is based upon work supported by the National
Science Foundation under Grant No. IRI-9122026.
</subsectionHeader>
<bodyText confidence="0.999417">
a group. Thus, when facing a conflict, a collaborative
agent should not automatically reject a belief with which
she does not agree; instead, she should evaluate the belief
and the evidence provided to her and adopt the belief if the
evidence is convincing. On the other hand, if the evalua-
tion indicates that the agent should maintain her original
belief, she should attempt to provide sufficient justifica-
tion to convince the other agent to adopt this belief if the
belief is relevant to the task at hand.
This paper presents a model for engaging in collabo-
rative negotiation to resolve conflicts in agents&apos; beliefs
about domain knowledge. Our model 1) detects con-
flicts in beliefs and initiates a negotiation subdialogue
only when the conflict is relevant to the current task, 2)
selects the most effective aspect to address in its pursuit
of conflict resolution when multiple conflicts exist, 3)
selects appropriate evidence to justify the system&apos;s pro-
posed modification of the user&apos;s beliefs, and 4) captures
the negotiation process in a recursive Propose-Evaluate-
Modify cycle of actions, thus enabling the system to han-
dle embedded negotiation subdialogues.
</bodyText>
<sectionHeader confidence="0.999946" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999854411764706">
Researchers have studied the analysis and generation of
arguments (Birnbaum et al., 1980; Reichman, 1981; Co-
-hen, 1987; Sycara, 1989; Quilici, 1992; Maybury, 1993);
however, agents engaging in argumentative dialogues are
solely interested in winning an argument and thus ex-
hibit (Efferent behavior from collaborative agents. Sidner
(1992; 1994) formulated an artificial language for mod-
eling collaborative discourse using proposal/acceptance
and proposal/rejection sequences; however, her work
is descriptive and does not specify response generation
strategies for agents involved in collaborative interac-
tions.
Webber and Joshi (1982) have noted the importance of
a cooperative system providing support for its responses.
They identified strategies that a system can adopt in justi-
fying its beliefs; however, they did not specify the criteria
under which each of these strategies should be selected.
</bodyText>
<page confidence="0.997802">
136
</page>
<bodyText confidence="0.999970862068965">
Walker (1994) described a method of determining when
to include optional warrants to justify a claim based on
factors such as communication cost, inference cost, and
cost of memory retrieval. However, her model focuses on
determining when to include informationally redundant
utterances, whereas our model determines whether or not
justification is needed for a claim to be convincing and, if
so, selects appropriate evidence from the system&apos;s private
beliefs to support the claim.
Caswey et al. (Cawsey et al., 1993; Logan et al.,
1994) introduced the idea of utilizing a belief revision
mechanism (Galliers, 1992) to predict whether a set of
evidence is sufficient to change a user&apos;s existing belief
and to generate responses for information retrieval di-
alogues in a library domain. They argued that in the
library dialogues they analyzed, &amp;quot;in no cases does ne-
gotiation extend beyond the initial belief conflict and its
immediate resolution.&amp;quot; (Logan et al., 1994, page 141).
However, our analysis of naturally-occurring consultation
dialogues (Columbia University Transcripts, 1985; SRI
Transcripts, 1992) shows that in other domains conflict
resolution does extend beyond a single exchange of con-
flicting beliefs; therefore we employ a recursive model
for collaboration that captures extended negotiation and
represents the structure of the discourse. Furthermore,
their system deals with a single conflict, while our model
selects a focus in its pursuit of conflict resolution when
multiple conflicts arise. In addition, we provide a process
for selecting among multiple possible pieces of evidence.
</bodyText>
<sectionHeader confidence="0.985178" genericHeader="method">
3 Features of Collaborative Negotiation
</sectionHeader>
<bodyText confidence="0.99984115">
Collaborative negotiation occurs when conflicts arise
among agents developing a shared plan]. during collab-
orative planning. A collaborative agent is driven by the
goal of developing a plan that best satisfies the interests of
all the agents as a group, instead of one that maximizes his
own interest. This results in several distinctive features of
collaborative negotiation: 1) A collaborative agent does
not insist on winning an argument, and may change his
beliefs if another agent presents convincing justification
for an opposing belief. This differentiates collaborative
negotiation from argumentation (Birnbaum et al., 1980;
Reichman, 1981; Cohen, 1987; Quilici, 1992). 2) Agents
involved in collaborative negotiation are open and hon-
est with one another; they will not deliberately present
false information to other agents, present information in
such a way as to mislead the other agents, or strategi-
cally hold back information from other agents for later
use. This distinguishes collaborative negotiation from
non-collaborative negotiation such as labor negotiation
(Sycara, 1989). 3) Collaborative agents are interested in
</bodyText>
<subsectionHeader confidence="0.8550735">
&apos;The notion of shared plan has been used in (Grosz and
Sidner, 1990; Allen, 1991).
</subsectionHeader>
<bodyText confidence="0.999533566666666">
others&apos; beliefs in order to decide whether to revise their
own beliefs so as to come to agreement (Chu-Carroll and
Carberry, 1995). Although agents involved in argumenta-
tion and non-collaborative negotiation take other agents&apos;
beliefs into consideration, they do so mainly to find weak
points in their opponents&apos; beliefs and attack them to win
the argument.
In our earlier work, we built on Sidner&apos;s pro-
posal/acceptance and proposal/rejection sequences (Sid-
ner, 1994) and developed a model that captures collabo-
rative planning processes in a Propose-Evaluate-Modify
cycle of actions (Chu-Carroll and Carberry, 1994). This
model views collaborative planning as agent A propos-
ing a set of actions and beliefs lobe incorporated into the
shared plan being developed, agent B evaluating the pro-
posal to determine whether or not be accepts the proposal
and, if not, agent B proposing a set of modifications to As
original proposal. The proposed modifications will again
be evaluated by A, and if conflicts arise, she may propose
modifications to B&apos;s previously proposed modifications,
resulting in a recursive process. However, our research
did not specify, in cases where multiple conflicts arise,
how an agent should identify which part of an tmaccepted
proposal to address or how to select evidence to support
the proposed modification. This paper extends that work
by incorporating into the modification process a strategy
to determine the aspect of the proposal that the agent will
address in her pursuit of conflict resolution, as well as
a means of selecting appropriate evidence to justify the
need for such modification.
</bodyText>
<sectionHeader confidence="0.969612" genericHeader="method">
4 Response Generation in Collaborative
Negotiation
</sectionHeader>
<bodyText confidence="0.999827764705882">
In order to capture the agents&apos; intentions conveyed by
their utterances, our model of collaborative negotiation
utilizes an enhanced version of the dialogue model de-
scribed in (Lambert and Carberry, 1991) to represent
the current status of the interaction. The enhanced di-
alogue model has four levels: the domain level which
consists of the domain plan being constructed for the
user&apos;s later execution, the problem-solving level which
contains the actions being performed to construct the do-
main plan, the belief level which consists of the mutual
beliefs pursued during the planning process in order to
further the problem-solving intentions, and the discourse
level which contains the communicative actions initiated
to achieve the mutual beliefs (Chu-Carroll and Carberry,
1994). This paper focuses on the evaluation and mod-
ification of proposed beliefs, and details a strategy for
engaging in collaborative negotiations.
</bodyText>
<page confidence="0.995822">
137
</page>
<subsectionHeader confidence="0.995828">
4.1 Evaluating Proposed Beliefs
</subsectionHeader>
<bodyText confidence="0.999968918367347">
Our system maintains a set of beliefs about the domain
and about the user&apos;s beliefs. Associated with each be-
lief is a strength that represents the agent&apos;s confidence
in holding that belief. We model the strength of a belief
using endorsements, which are explicit records of factors
that affect one&apos;s certainty in a hypothesis (Cohen, 1985),
following (Gathers, 1992; Logan et al., 1994). Our en-
dorsements are based on the semantics of the utterance
used to convey a belief, the level of expertise of the agent
conveying the belief, stereotypical knowledge, etc.
The belief level of the dialogue model consists of mu-
tual beliefs proposed by the agents&apos; discourse actions.
When an agent proposes a new belief and gives (optional)
supporting evidence for it, this set of proposed beliefs is
represented as a belief tree, where the belief represented
by a child node is intended to support that represented by
its parent. The root nodes of these belief trees (top-level
beliefs) contribute to problem-solving actions and thus
affect the domain plan being developed. Given a set of
newly proposed beliefs, the system must decide whether
to accept the proposal or to initiate a negotiation dialogue
to resolve conflicts. The evaluation of proposed beliefs
starts at the leaf nodes of the proposed belief trees since
acceptance of a piece of proposed evidence may affect ac-
ceptance of the parent belief it is intended to support. The
process continues until the top-level proposed beliefs are
evaluated. Conflict resolution strategies are invoked only
if the top-level proposed beliefs are not accepted because
if collaborative agents agree on a belief relevant to the
domain plan being constructed, it is irrelevant whether
they agree on the evidence for that belief (Young et al.,
1994).
In determining whether to accept a proposed belief
or evidential relationship, the evaluator first constructs
an evidence set containing the system&apos;s evidence that
supports or attacks _bel and the evidence accepted by
the system that was proposed by the user as support for
_bel. Each piece of evidence contains a belief _heli, and
an evidential relationship supports(..elb_bel). Follow-
ing Walker&apos;s weakest link assumption (Walker, 1992) the
strength of the evidence is the weaker of the strength of
the belief and the strength of the evidential relationship.
The evaluator then employs a simplified version of Gai-
ners&apos; belief revision mechanism2 (Gainers, 1992; Logan
et al., 1994) to compare the strengths of the evidence that
supports and attacks _bel. If the strength of one set of evi-
dence strongly outweighs that of the other, the decision to
accept or reject .bel is easily made. However, if the differ-
ence in their strengths does not exceed a pre-determined
</bodyText>
<footnote confidence="0.861782">
2For details on how our model determines the acceptance of
a belief using the ranking of endorsements proposed by Galliers,
see (Chu-Carroll, 1995).
</footnote>
<figure confidence="0.5984165">
Dr. Smith is not teaching AI.
Dr. Smith is going on sabbatical next year.
</figure>
<figureCaption confidence="0.999691">
Figure 1: Belief and Discourse Levels for (2) and (3)
</figureCaption>
<bodyText confidence="0.9999012">
threshold, the evaluator has insufficient information to
determine whether to adopt _bet and therefore will ini-
tiate an information-sharing subdialogue (Chu-Carroll
and Carberry, 1995) to share information with the user
so that each of them can knowledgably re-evaluate the
user&apos;s original proposal. If, during information-sharing,
the user provides convincing support for a belief whose
negation is held by the system, the system may adopt the
belief after the re-evaluation process, thus resolving the
conflict without negotiation.
</bodyText>
<subsectionHeader confidence="0.618397">
4.1.1 Example
</subsectionHeader>
<bodyText confidence="0.999935">
To illustrate the evaluation of proposed beliefs, con-
sider the following utterances:
</bodyText>
<listItem confidence="0.97919125">
(I) S: I think Dr. Smith is teaching AI next
semester.
(2)U: Dr Smith is not teaching AL
(3) He is going on sabbatical next year.
</listItem>
<bodyText confidence="0.998162625">
Figure 1 shows the belief and discourse levels of
the dialogue model that captures utterances (2) and
(3). The belief evaluation process will start with
the belief at the leaf node of the proposed belief
tree, On-Sabbatical(Smith,next year)). The system
will first gather its evidence pertaining to the belief,
which includes 1) a warranted belief3 that Dr. Smith
has postponed his sabbatical until 1997 (Postponed-
Sabbatical(Smith, 1997)), 2) a warranted belief that
Dr. Smith postponing his sabbatical until 1997 sup-
ports the belief that he is not going on sabbatical
next year (supports(Posrponed-Sabbatical(Smith,1997),
-,On-Sabbatical(Smith,next year)), 3) a strong belief
that Dr. Smith will not be a visitor at IBM next year
(-cvisitor(Smith, IBM, next year)), and 4) a warranted
belief that Dr. Smith not being a visitor at IBM next
</bodyText>
<note confidence="0.4480225">
3The strength of a belief is classified as: warranted, strong,
or weak, based on the endorsement of the belief.
</note>
<figure confidence="0.9843761875">
Proposed Belief Level
, -71 MB(U,S,-Teaches(Smith,AD)
• suPPxt
MB(U,S,On-Sabbatical(Smith,next year))
&apos; • •
•
Inform .S.-TeachesfSmith Al)) I A
•
Inform(U,S,On-Sabbatical(Smitlynext year)) I-1/
Tell(U,S,On-Sabbatical(Smith,next year))
Discharge Level
•
_
Tell S -Teaches
Smith
Address-Acce
</figure>
<page confidence="0.985698">
138
</page>
<bodyText confidence="0.9997496">
year supports the belief that he is not going on sab-
batical next year (supportsfrivisitor(Smith, IBM, next
year), -,On-Sabbatical(Smith, next year)), perhaps be-
cause Dr. Smith has expressed his desire to spend his sab-
batical only at IBM). The belief revision mechanism will
then be invoked to determine the system&apos;s belief about
On-Sabbatical(Smith, next year) based on the system&apos;s
own evidence and the user&apos;s statement Since beliefs (1)
and (2) above constitute a warranted piece of evidence
against the proposed belief and beliefs (3) and (4) consti-
tute a strong piece of evidence against it, the system will
not accept On-Sabbatical(Smith, next year).
The system believes that being on sabbatical implies a
faculty member is not teaching any courses; thus the pro-
posed evidential relationship will be accepted. However,
the system will not accept the top-level proposed belief,
-,Teaches(Smith, Al), since the system has a prior belief
to the contrary (as expressed in utterance (1)) and the only
evidence provided by the user was an implication whose
antecedent was not accepted.
</bodyText>
<subsectionHeader confidence="0.999567">
4.2 Modifying Unaccepted Proposals
</subsectionHeader>
<bodyText confidence="0.9999615">
The collaborative planning principle in (Whittaker and
Stenton, 1988; Walker, 1992) suggests that &amp;quot;conversants
must provide evidence of a detected discrepancy in belief
as soon as possible.&amp;quot; Thus, once an agent detects a rele-
vant conflict, she must notify the other agent of the con-
flict and initiate a negotiation subdialogue to resolve it —
to do otherwise is to fail in her responsibility as a collab-
orative agent. We capture the attempt to resolve a con-
flict with the problem-solving action Modify-Proposal,
whose goal is to modify the proposal to a form that will
potentially be accepted by both agents. When applied to
belief modification, Modify-Proposal has two specializa-
tions: Correct-Node, for when a proposed belief is not
accepted, and Correct-Relation, for when a proposed ev-
idential relationship is not accepted. Figure 2 shows the
problem-solving recipes&apos; for Correct-Node and its subac-
tion, Modify-Node, that is responsible for the actual mod-
ification of the proposal. The applicability conditions5 of
Correct-Node specify that the action can only be invoked
when ..sl believes that _node is not acceptable while _s2
believes that it is (when ...sl and _s2 disagree about the
proposed belief represented by _node). However, since
this is a collaborative interaction, the actual modification
can only be performed when both ..s1 and _s2 believe that
_node is not acceptable — that is, the conflict between
...sl and _s2 must have been resolved. This is captured by
</bodyText>
<tableCaption confidence="0.7066335">
4A recipe (Pollack, 1986) is a template for performing ac-
lions. It contains the applicability conditions for performing an
action, the subactions comprising the body of an action, etc.
5Applicability conditions are conditions that must already
be satisfied in order for an action to be reasonable to pursue,
whereas an agent can try to achieve unsatisfied preconditions.
</tableCaption>
<table confidence="0.967755666666667">
Action: Coirect-Node(_s 1, ..s2, _proposed)
Decomposition
Appl Cond: believe(..s1,-,acceptable(_node))
believe(_s2, acceptable(_node))
Const: effor-in-plan(...node,_proposed)
Body: Modify-Node(...s1,_s2,_proposecl,_nocle)
Insert-Correction(-sl, _s2, .proposed)
Goal: acceptable(_proposed)
Action: Modify-Node(_s1,_s2,_proposed,_node)
Type: Specialization
App! Coed: believe(_sl, -,acceptable(_nocle))
Precond: believe(..2,-,acceptable(_node))
Body: Remove-Node(_s1,_s2,4roposed,_node)
Alter-Node(..s1,_s2,_proposed,_node)
Goal: modified(_proposed)
</table>
<figureCaption confidence="0.998546">
Figure 2: The Correct-Node and Modify-Node Recipes
</figureCaption>
<bodyText confidence="0.999736333333333">
the applicability condition and precondition of Modify-
Node. The attempt to satisfy the precondition causes the
system to post as a mutual belief to be achieved the belief
that _node is not acceptable, leading the system to adopt
discourse actions to change _s2&apos;s beliefs, thus initiating a
collaborative negotiation subdialogue.6
</bodyText>
<subsectionHeader confidence="0.800613">
4.2.1 Selecthig the Focus of Modification
</subsectionHeader>
<bodyText confidence="0.934318206896552">
When multiple conflicts arise between the system and
the user regarding the user&apos;s proposal, the system must
identify the aspect of the proposal on which it should fo-
cus in its pursuit of conflict resolution. For example, in
the case where Correct-Node is selected as the specializa-
tion of Modify-Proposal, the system must determine how
the parameter _node in Correct-Node should be instanti-
ated. The goal of the modification process is to resolve
the agents&apos; conflicts regarding the unaccepted top-level
proposed beliefs. For each such belief, the system could
provide evidence against the belief itself, address the un-
accepted evidence proposed by the user to eliminate the
user&apos;s justification for the belief, or both. Since collab-
orative agents are expected to engage in effective and
efficient dialogues, the system should address the unac-
cepted belief that it predicts will most quickly resolve
the top-level conflict. Therefore, for each unaccepted
top-level belief, our process for selecting the focus of
modification involves two steps: identifying a candidate
foci tree from the proposed belief tree, and selecting a
6This subdialogue is considered an interrupt by Whittaker,
Stenton, and Walker (Whittaker and Stenton, 1988; Walker and
Whittaker, 1990), initiated to negotiate the truth of a piece of in-
formation. However, the utterances they classify as interrupts
include not only our negotiation subdialogues, generated for
the purpose of modifying a proposal, but also clarification sub-
dialogues, and information-sharing subdialogues (Chu-Carroll
and Carberry, 1995), which we contend should be part of the
evaluation process.
</bodyText>
<page confidence="0.995804">
139
</page>
<bodyText confidence="0.999988222222223">
focus from the candidate foci tree using the heuristic &amp;quot;at-
tack the belief(s) that will most likely resolve the conflict
about the top-level belief?&apos; A candidate foci tree contains
the pieces of evidence in a proposed belief tree which, if
disbelieved by the user, might change the user&apos;s view of
the unaccepted top-level proposed belief (the root node
of that belief tree). It is identified by performing a depth-
first search on the proposed belief tree. When a node
is visited, both the belief and the evidential relationship
between it and its parent are examined. If both the be-
lief and relationship were accepted by the evaluator, the
search on the current branch will terminate, since once the
system accepts a belief, it is irrelevant whether it accepts
the user&apos;s support for that belief (Young et al., 1994).
Otherwise, this piece of evidence will be included in the
candidate foci tree and the system will continue to search
through the evidence in the belief tree proposed as support
for the unaccepted belief and/or evidential relationship.
Once a candidate foci tree is identified, the system
should select the focus of modification based on the like-
lihood of each choice changing the user&apos;s belief about
the top-level belief. Figure 3 shows our algorithm for
this selection process. Given an unaccepted belief (..bel)
and the beliefs proposed to support it, Select-Focus-
Modification will annotate _bel with 1) its focus of mod-
ification (_bel.focus), which contains a set of beliefs chef
and/or its descendents) which, if disbelieved by the user,
are predicted to cause him to disbelieve _lel, and 2) the
system&apos;s evidence against _bel itself (_bel.s-attack).
Select-Focus-Modification determines whether to at-
tack _bel&apos;s supporting evidence separately, thereby elim-
inating the user&apos;s reasons for holding _bel, to attack _bel
itself, or both. However, in evaluating the effectiveness of
attacking the proposed evidence for _bel, the system must
determine whether or not it is possible to successfully re-
fute a piece of evidence (i.e., whether or not the system
believes that sufficient evidence is available to convince
the user that a piece of proposed evidence is invalid), and
if so, whether it is more effective to attack the evidence it-
self or its support. Thus the algorithm recursively applies
itself to the evidence proposed as support for _bel which
was not accepted by the system (step 3). In this recursive
process, the algorithm annotates each unaccepted belief
or evidential relationship proposed to support _bel with
its focus of modification (Jeli.focus) and the system&apos;s
evidence against it (_beli.s-attack). ..bel. focus contains
the beliefs selected to be addressed in order to change the
user&apos;s belief about _beli, and its value will be nil if the
system predicts that insufficient evidence is available to
change the user&apos;s belief about _beli.
Based on the information obtained in step 3, Select-
Focus-Modification decides whether to attack the evi-
dence proposed to support _bel, or _bel itself (step 4).
Its preference is to address the unaccepted evidence, be-
</bodyText>
<figure confidence="0.831911972222222">
Select-Focus-1VIodilication(_bel):
I. _bel.u-evid 4- system&apos;s beliefs about the user&apos;s evidence
pertaining to _bel
_bel.s-attack +- system&apos;s own evidence against _bel
2. If _bel is a leaf node in the candidate foci tree,
2.1 If Predlet(..bel, _bel.u-evid + _bel.s-attack) =
then _bellocus 4- _bel; return
2.2 Else _bel.focus 4- nil; return
3. Select focus for each of _bel&apos;s children in the candidate
foci tree, _bell :
3.1 If supports(_belb_bel) is accepted but .beli is not,
Select-Focus-Modification(..eli).
3.2 Else if is accepted but supports(_beli,..bel) is
not, Select-Focus-Modffication(_beli,..bel).
3.3 Else Select-Focus-Modffication(_beli) and Select-
Focus-ModlficatIon(supports(_beli,_bel))
4. Choose between attacking the proposed evidence for _bel
and attacking _bel itself:
4.1 cand-set 4- {-be; I _beli E unaccepted user evidence
for _bel A locus 0 nil)
4.2 // Check if addressing _bel&apos;s unaccepted evidence is
sufficient
If Predickbel, _bel.u-evid - cand-set) = (i.e.,
the user&apos;s disbelief in all unaccepted evidence which
the system can refute will cause him to reject _bel),
min-set 4- Select-Min-Set(_bel,cand-set)
..bel. focus E_min-set imus
4.3 // Check if addressing _bel itself is sufficient
Else if PredIct(_bel, _bel.u-evid + _bel.s-attack) =
-&apos;.bel (i.e., the system&apos;s evidence against .bel will
cause the user to reject _bel),
_bel.focus 4- _bel
4.4 // Check if addressing both _bel and its unaccepted
evidence is sgOlcient
Else if Predict(_bel, _bel.s-attack + _bel.u-evid -
cand-set) =
</figure>
<figureCaption confidence="0.98479425">
min-set 4- Select-Mln-Set(_bel, cand-set + _bel)
-bel.fc&apos;cus 4- 1.i_beliE_min-set -beli.focus U _bel
4.5 Else _bellocus +- nil
Figure 3: Selecting the Focus of Modification
</figureCaption>
<bodyText confidence="0.999949235294118">
cause McKeown&apos;s focusing rules suggest that continuing
a newly introduced topic (about which there is more to be
said) is preferable to returning to a previous topic (McK-
eown, 1985). Thus the algorithm first considers whether
Of not attacking the user&apos;s support for _bel is sufficient to
convince him of --1_bel (step 4.2). It does so by gathering
(in cand-set) evidence proposed by the user as direct sup-
port for _bel but which was not accepted by the system
and which the system predicts it can successfully refute
(i.e., _beli.focus is not nil). The algorithm then hypothe-
sizes that the user has changed his mind about each belief
in cand-set and predicts how this will affect the user&apos;s
belief about _bel (step 4.2). If the user is predicted to ac-
cept -.,bel under this hypothesis, the algorithm invokes
Select-Min-Set to select a minimum subset of cand-set as
the unaccepted beliefs that it would actually pursue, and
the focus of modification (_bellocus) will be the union of
</bodyText>
<page confidence="0.993066">
140
</page>
<bodyText confidence="0.999933961538462">
the focus for each of the beliefs in this minimum subset.
If attacking the evidence for _bel does not appear to
be sufficient to convince the user of the algorithm
checks whether directly attacking _bel will accomplish
this goal. If providing evidence directly against _bel is
predicted to be successful, then the focus of modifica-
tion is _bel itself (step 4.3). If directly attacking _bel
is also predicted to fail, the algorithm considers the ef-
fect of attacking both _bel and its unaccepted proposed
evidence by combining the previous two prediction pro-
cesses (step 4.4). If the combined evidence is still pre-
dicted to fail, the system does not have sufficient evidence
to change the user&apos;s view of _bel; thus, the focus of mod-
ification for _bel is nil (step 4.5).7 Notice that steps 2 and
4 of the algorithm invoke a function, Predict, that makes
use of the belief revision mechanism (Galliers, 1992) dis-
cussed in Section 4.1 to predict the user&apos;s acceptance or
unacceptance of _bel based on the system&apos;s knowledge of
the user&apos;s beliefs and the evidence that could be presented
to him (Logan et al., 1994). The result of Select-Focus-
Modification is a set of user beliefs (in _bel.focus) that
need to be modified in order to change the user&apos;s belief
about the unaccepted top-level belief. Thus, the negations
of these beliefs will be posted by the system as mutual
beliefs to be achieved in order to perform the Modify
actions.
</bodyText>
<subsectionHeader confidence="0.552432">
4.2.2 Selecting Justification for a Claim
</subsectionHeader>
<bodyText confidence="0.978827291666667">
Studies in communication and social psychology have
shown that evidence improves the persuasiveness of a
message (Luchok and McCroskey, 1978; Reynolds and
Burgoon, 1983; Petty and Cacioppo, 1984; Hample,
1985). Research on the quantity of evidence indicates
that there is no optimal amount of evidence, but that the
use of high-quality evidence is consistent with persua-
sive effects (Reinard, 1988). On the other hand, Grice&apos;s
maxim of quantity (Grice, 1975) specifies that one should
not contribute more information than is re,quired.8 Thus,
it is important that a collaborative agent selects sufficient
and effective, but not excessive, evidence to justify an
intended mutual belief.
To convince the user of a belief, _bel, our system selects
appropriate justification by identifying beliefs that could
7In collaborative dialogues, an agent should reject a pro-
posal only if she has strong evidence against it. When an agent
does not have sufficient information to determine the accep-
tance of a proposal, she should initiate an information-sharing
subdialogue to share information with the other agent and re-
evaluate the proposal (Chu-Carroll and Carberry, 1995). Thus,
further research is needed to determine whether or not the focus
of modification for a rejected belief will ever be nil in collabo-
rative dialogues.
</bodyText>
<footnote confidence="0.820129333333333">
8Walker (1994) has shown the importance of IRU&apos;s (Infor-
mationally Redundant Utterances) in efficient discourse. We
leave including appropriate IRU&apos;s for future work.
</footnote>
<bodyText confidence="0.99954143902439">
be used to support _bel and applying filtering heuristics to
them. The system must first determine whether justifica-
tion for _bel is needed by predicting whether or not merely
informing the user of _bel will be sufficient to convince
him of _bel. If so, no justification will be presented. If
justification is predicted to be necessary, the system will
first construct the justification chains that could be used
to support _bel. For each piece of evidence that could
be used to directly support _bel, the system first predicts
whether the user will accept the evidence without justi-
fication. If the user is predicted not to accept a piece of
evidence (evidi), the system will augment the evidence to
be presented to the user by posting evidi as a mutual be-
lief to be achieved, and selecting propositions that could
serve as justification for it. This results in a recursive
process that returns a chain of belief justifications that
could be used to support _bel.
Once a set of beliefs forming justification chains is
identified, the system must then select from this set those
belief chains which, when presented to the user, are pre-
dicted to convince the user of _bel. Our system will first
construct a singleton set for each such justification chain
and select the sets containing justification which, when
presented, is predicted to convince the user of _bel. if
no single justification chain is predicted to be sufficient
to change the user&apos;s beliefs, new sets will be constructed
by combining the single justification chains, and the se-
lection process is repeated. This will produce a set of
possible candidate justification chains, and three heuris-
tics will then be applied to select from among them. The
first heuristic prefers evidence in which the system is most
confident since high-quality evidence produces more at-
titude change than any other evidence form (Luchok and
McCroskey, 1978). Furthermore, the system can better
justify a belief in which it has high confidence should the
user not accept it. The second heuristic prefers evidence
that is novel to the user, since studies have shown that ev-
idence is most persuasive if it is previously unknown to
the hearer (Wyer, 1970; Morley, 1987). The third heuris-
tic is based on Grice&apos;s maxim of quantity and prefers
justification chains that contain the fewest beliefs.
</bodyText>
<subsectionHeader confidence="0.933152">
4.2.3 Example
</subsectionHeader>
<bodyText confidence="0.999884363636364">
After the evaluation of the dialogue model in Figure 1,
Modify-Proposal is invoked because the top-level pro-
posed belief is not accepted. In selecting the focus of
modification, the system will first identify the candidate
foci tree and then invoke the Select-Focus-Modification
algorithm on the belief at the root node of the candidate
foci tree. The candidate foci tree will be identical to the
proposed belief tree in Figure 1 since both the top-level
proposed belief and its proposed evidence were rejected
during the evaluation process. This indicates that the fo-
cus of modification could be either —Teaches(Smith,AI)
</bodyText>
<page confidence="0.996599">
141
</page>
<bodyText confidence="0.999205071428572">
or On-Sabbatical(Smith, next year) (since the evidential
relationship between them was accepted). When Select-
Focus-Modification is applied to -Teaches(Smith,AI),
the algorithm will first be recursively invoked on On-
Sabbatical(Smith, next year) to determine the focus for
modifying the child belief (step 3.1 in Figure 3). Since
the system has two pieces of evidence against On-
Sabbatical(Smith, next year), 1) a warranted piece of
evidence containing Po stponed-Sabbatical(Smitha 997)
and supports(Postponed-Sabbatical(Stnith,1997),-On-
Sabbatical(Smith, next year)), and 2) a strong
piece of evidence containing -ivisitor(Smith,IBM,next
year) and supports(-wisitor(Smith,IBM,nexi year),-,On-
Sabbatical(Smith,next year)), the evidence is pre-
dicted to be sufficient to change the user&apos;s be-
lief in On-Sabbatical(Smith,next year), and hence
-,Teaches(Smith,AI); thus, the focus of modification will
be On-Sabbatical(Smith,next year). The Correct-Node
specialization of Modify-P roposal will be invoked since
the focus of modification is a belief, and in order to sat-
isfy the precondition of Modify-Node (Figure 2), MB(S,U,
-,On-Sabbatical(Smith,next year)) will be posted as a mu-
tual belief to be achieved.
Since the user has a warranted belief in On-
Sabbatical(Smith,next year) (indicated by the seman-
tic form of utterance (3)), the system will predict that
merely informing the user of the intended mutual belief
is not sufficient to change his belief; therefore it will
select justification from the two available pieces of evi-
dence supporting -On-Sabbatical(Smith,next year) pre-
sented earlier. The system will predict that either piece
of evidence combined with the proposed mutual belief
is sufficient to change the user&apos;s belief; thus, the filter-
ing heuristics are applied. The first heuristic will cause
the system to select Postponed-Sabbatical(Smith, 1997)
and supports(Postponed-Sabbatical(Smith, 1997),-,On-
Sabbatical(Smith, next year)) as support, since it is the
evidence in which the system is more confident.
The system will try to establish the mutual beliefs9 as
an attempt to satisfy the precondition of Modify-Node.
This will cause the system to invoke Inform discourse
actions to generate the following utterances:
</bodyText>
<listItem confidence="0.958309">
(4) S: Dr. Smith is not going on sabbatical next
year
(5) He postponed his sabbatical until 1997.
</listItem>
<bodyText confidence="0.9894397">
If the user accepts the system&apos;s utterances, thus satisfy-
ing the precondition that the conflict be resolved, Modify-
Node can be performed and changes made to the original
proposed beliefs. Otherwise, the user may propose mod-
90nly MB(S,U,Postponed-Sabbatical(Smith, 1997)) will be
proposed as justification because the system believes that the
evidential relationship needed to complete the inference is held
by a stereotypical user.
ifications to the system&apos;s proposed modifications, result-
ing in an embedded negotiation subdialogue.
</bodyText>
<sectionHeader confidence="0.997694" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999961076923077">
This paper has presented a computational strategy for en-
gaging in collaborative negotiation to square away con-
flicts in agents&apos; beliefs. The model captures features
specific to collaborative negotiation. It also supports ef-
fective and efficient dialogues by identifying the focus of
modification based on its predicted success in resolving
the conflict about the top-level belief and by using heuris-
tics motivated by research in social psychology to select
a set of evidence to justify the proposed modification of
beliefs. Furthermore, by capturing collaborative negoti-
ation in a cycle of Propose-Evaluate-Modify actions, the
evaluation and modification processes can be applied re-
cursively to capture embedded negotiation subdialogues.
</bodyText>
<sectionHeader confidence="0.998564" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9940414">
Discussions with Candy Sidner, Stephanie Elzer, and
Kathy McCoy have been very helpful in the development
of this work. Comments from the anonymous reviewers
have also been very useful in preparing the final version
of this paper.
</bodyText>
<sectionHeader confidence="0.998067" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.991579034482759">
James Allen. 1991. Discourse structure in the TRAINS
project. In Darpa Speech and Natural Language Work-
shop.
Lawrence Birnbaum, Margot Flowers, and Rod McGuire.
1980. Towards an AI model of argumentation. In
Proceedings of the National Conference on Artificial
Intelligence, pages 313-315.
Alison Cawsey, Julia Galliers, Brian Logan, Steven
Reece, and Karen Sparck Jones. 1993. Revising be-
liefs and intentions: A unified framework for agent
interaction. In The Ninth Biennial Conference of the
Society for the Study of Artificial Intelligence and Sim-
ulation of Behaviour, pages 130-139.
Jennifer Chu-Carroll and Sandra Carberry. 1994. A plan-
based model for response generation in collaborative
task-oriented dialogues. In Proceedings of the Twelfth
National Conference on Artificial Intelligence, pages
799-805.
Jennifer Chu-Carroll and Sandra Carberry. 1995. Gener-
ating information-sharing subdialogues in expert-user
consultation. In Proceedings of the 14th International
Joint Conference on Artificial Intelligence. To appear.
Jennifer Chu-Carroll. 1995. A Plan-Based Model for Re-
sponse Generation in Collaborative Consultation Di-
alogues. Ph.D. thesis, University of Delaware. Forth-
coming.
Paul R. Cohen. 1985. Heuristic Reasoning about Un-
certainty: An Artificial Intelligence Approach. Pitman
Publishing Company.
</reference>
<page confidence="0.990268">
142
</page>
<reference confidence="0.999681268907563">
Robin Cohen. 1987. Analyzing the structure of argu-
mentative discourse. Computational Linguistcis,13(1-
2):11-24, January-June.
Columbia University Transcripts. 1985. Transcripts de-
rived from audiotape conversations made at Columbia
University, New York, NY. Provided by Kathleen
McKeown.
Julia R. Garners. 1992. Autonomous belief revision and
communication. In Gardenfors, editor, BeliefRevision.
Cambridge University Press.
H. Paul Grice. 1975. Logic and conversation. In Peter
Cole and Jerry L. Morgan, editors, Syntax and Seman-
tics 3: Speech Acts, pages 41-58. Academic Press,
Inc., New York.
Barbara J. Grosz and Candace L. Sidner. 1990. Plans
for discourse. In Cohen, Morgan, and Pollack, editors,
Intentions in Communication, chapter 20, pages 417-
444. MIT Press.
Dale Hample. 1985. Refinements on the cognitive model
of argument: Concreteness, involvement and group
scores. The Western Journal of Speech Communica-
tion, 49:267-285.
Aravind K. Joshi. 1982. Mutual beliefs in question-
answer systems. In N.V. Smith, editor, Mutual Knowl-
edge, chapter 4, pages 181-197. Academic Press.
Lynn Lambert and Sandra Carberry. 1991. A tripartite
plan-based model of dialogue. In Proceedings of the
29th Annual Meeting of the Association for Computa-
tional Linguistics, pages 47-54.
Brian Logan, Steven Reece, Alison Cawsey, Julia Gal-
liers, and Karen Sparck Jones. 1994. Belief revision
and dialogue management in information retrieval.
Technical Report 339, University of Cambridge, Com-
puter Laboratory.
Joseph A. Luchok and James C. McCroskey. 1978. The
effect of quality of evidence on attitude change and
source credibility. The Southern Speech Communica-
tion Journal, 43:371-383.
Mark T. Maybury. 1993. Communicative acts for gen-
erating natural language arguments. In Proceedings
of the National Conference on Artificial Intelligence,
pages 357-364.
Kathleen R. McKeown. 1985. Teo Generation: Using
Discourse Strategies and Focus Constraints to Gen-
erate Natural Language Text. Cambridge University
Press.
Donald D. Morley. 1987. Subjective message constructs:
A theory of persuasion. Communication Monographs,
54:183-203.
Richard E. Petty and John T. Cacioppo. 1984. The ef-
fects of involvement on responses to argument quantity
and quality: Central and peripheral routes to persua-
sion. Journal of Personality and Social Psychology,
46(1):69-81.
Martha E. Pollack. 1986. A model of plan inference
that distinguishes between the beliefs of actors and ob-
servers. In Proceedings of the 24th Annual Meeting of
the Association for Computational Linguistics, pages
207-214.
Alex Quilici. 1992. Arguing about planning alternatives.
In Proceedings of the 14th International Conference
on Computational Linguistics, pages 906-910.
Rachel Reichman. 1981. Modeling informal debates. In
Proceedings of the 7th International Joint Conference
on Artificial Intelligence, pages 19-24.
John C. Reinard. 1988. The empirical study of the per-
suasive effects of evidence, the status after fifty years of
research. Human Communication Research, 15(1):3-
59.
Rodney A. Reynolds and Michael Burgoon. 1983. Be-
lief processing, reasoning, and evidence. In Bostrom,
editor, Communication Yearbook 7, chapter 4, pages
83-104. Sage Publications.
Candace L. Sidner. 1992. Using discourse to negotiate
in collaborative activity: An artificial language. In
AAAI-92 Workshop: Cooperation Among Heteroge-
neous Intelligent Systems, pages 121-128.
Candace L. Sidner. 1994. An artificial discourse lan-
guage for collaborative negotiation. In Proceedings of
the Twelfth National Conference on Artificial Intelli-
gence, pages 814-819.
SRI Transcripts. 1992. Transcripts derived from audio-
tape conversations made at SRI International, Menlo
Park, CA. Prepared by Jacqueline ICowtko under the
direction of Patti Price.
Katia Sycara. 1989. Argumentation: Planning other
agents&apos; plans. In Proceedings of the 11th International
Joint Conference on Artificial Intelligence, pages 517-
523.
Marilyn Walker and Steve Whittaker. 1990. Mixed ini-
tiative in dialogue: An investigation into discourse seg-
mentation. In Proceedings of the 2&amp;h Annual Meet-
ing of the Association for Computational Linguistics,
pages 70-78.
Marilyn A. Walker. 1992. Redundancy in collaborative
dialogue. In Proceedings of the 15th International
Conference on Computational Linguistics, pages 345-
351.
Marilyn A. Walker. 1994. Discourse and deliberation:
Testing a collaborative strategy. In Proceedings of
the 15th International Conference on Computational
Linguistics.
Bonnie Webber and Aravind Joshi. 1982. Taking the
initiative in natural language data base interactions:
Justifying why. In Proceedings of COLING-82, pages
413-418.
Steve Whittaker and Phil Stenton. 1988. Cues and con-
trol in expert-client dialogues. In Proceedings of the
26th Annual Meeting of the Association for Computa-
tional Linguistics, pages 123-130.
Robert S. Wyer, Jr. 1970. Information redundancy, in-
consistency, and novelty and their role in impression
formation. Journal of Experimental Social Psychol-
ogy, 6:111-127.
R. Michael Young, Johanna D. Moore, and Martha E.
Pollack. 1994. Towards a principled representation
of discourse plans. In Proceedings of the Sixteenth
Annual Meeting of the Cognitive Science Society, pages
946-951.
</reference>
<page confidence="0.999121">
143
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.947121">
<title confidence="0.998707">Response Generation in Collaborative Negotiation*</title>
<author confidence="0.999828">Jennifer Chu-Carroll</author>
<author confidence="0.999828">Sandra Carberry</author>
<affiliation confidence="0.9999075">Department of Computer and Information Sciences University of Delaware</affiliation>
<address confidence="0.999832">Newark, DE 19716, USA</address>
<email confidence="0.999987">E-mail:tjchu,carberryl@cis.udel.edu</email>
<abstract confidence="0.997669954545454">In collaborative planning activities, since the agents are autonomous and heterogeneous, it is inevitable that conflicts arise in their beliefs during the planning process. In cases where such conflicts are relevant to the task at hand, agents should engage in neas an attempt square away the discrepancies in their beliefs. This paper presents a computational strategy for detecting conflicts regarding proposed beliefs and for engaging in collaborative negotiation to resolve the conflicts that warrant resolution. Our model is capable of selecting the most effective aspect to address in its pursuit of conflict resolution in cases where multiple conflicts arise, and of selecting appropriate evidence to justify the need for such modification. Furthermore, by capturing the negotiation process in a recursive of actions, our model can successfully handle embedded negotiation subdialogues.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Allen</author>
</authors>
<title>Discourse structure in the TRAINS project.</title>
<date>1991</date>
<booktitle>In Darpa Speech and Natural Language Workshop.</booktitle>
<contexts>
<context position="6986" citStr="Allen, 1991" startWordPosition="1039" endWordPosition="1040">ion (Birnbaum et al., 1980; Reichman, 1981; Cohen, 1987; Quilici, 1992). 2) Agents involved in collaborative negotiation are open and honest with one another; they will not deliberately present false information to other agents, present information in such a way as to mislead the other agents, or strategically hold back information from other agents for later use. This distinguishes collaborative negotiation from non-collaborative negotiation such as labor negotiation (Sycara, 1989). 3) Collaborative agents are interested in &apos;The notion of shared plan has been used in (Grosz and Sidner, 1990; Allen, 1991). others&apos; beliefs in order to decide whether to revise their own beliefs so as to come to agreement (Chu-Carroll and Carberry, 1995). Although agents involved in argumentation and non-collaborative negotiation take other agents&apos; beliefs into consideration, they do so mainly to find weak points in their opponents&apos; beliefs and attack them to win the argument. In our earlier work, we built on Sidner&apos;s proposal/acceptance and proposal/rejection sequences (Sidner, 1994) and developed a model that captures collaborative planning processes in a Propose-Evaluate-Modify cycle of actions (Chu-Carroll an</context>
</contexts>
<marker>Allen, 1991</marker>
<rawString>James Allen. 1991. Discourse structure in the TRAINS project. In Darpa Speech and Natural Language Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Birnbaum</author>
<author>Margot Flowers</author>
<author>Rod McGuire</author>
</authors>
<title>Towards an AI model of argumentation.</title>
<date>1980</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence,</booktitle>
<pages>313--315</pages>
<contexts>
<context position="3308" citStr="Birnbaum et al., 1980" startWordPosition="493" endWordPosition="496">model 1) detects conflicts in beliefs and initiates a negotiation subdialogue only when the conflict is relevant to the current task, 2) selects the most effective aspect to address in its pursuit of conflict resolution when multiple conflicts exist, 3) selects appropriate evidence to justify the system&apos;s proposed modification of the user&apos;s beliefs, and 4) captures the negotiation process in a recursive Propose-EvaluateModify cycle of actions, thus enabling the system to handle embedded negotiation subdialogues. 2 Related Work Researchers have studied the analysis and generation of arguments (Birnbaum et al., 1980; Reichman, 1981; Co-hen, 1987; Sycara, 1989; Quilici, 1992; Maybury, 1993); however, agents engaging in argumentative dialogues are solely interested in winning an argument and thus exhibit (Efferent behavior from collaborative agents. Sidner (1992; 1994) formulated an artificial language for modeling collaborative discourse using proposal/acceptance and proposal/rejection sequences; however, her work is descriptive and does not specify response generation strategies for agents involved in collaborative interactions. Webber and Joshi (1982) have noted the importance of a cooperative system pr</context>
<context position="6400" citStr="Birnbaum et al., 1980" startWordPosition="949" endWordPosition="952">ive negotiation occurs when conflicts arise among agents developing a shared plan]. during collaborative planning. A collaborative agent is driven by the goal of developing a plan that best satisfies the interests of all the agents as a group, instead of one that maximizes his own interest. This results in several distinctive features of collaborative negotiation: 1) A collaborative agent does not insist on winning an argument, and may change his beliefs if another agent presents convincing justification for an opposing belief. This differentiates collaborative negotiation from argumentation (Birnbaum et al., 1980; Reichman, 1981; Cohen, 1987; Quilici, 1992). 2) Agents involved in collaborative negotiation are open and honest with one another; they will not deliberately present false information to other agents, present information in such a way as to mislead the other agents, or strategically hold back information from other agents for later use. This distinguishes collaborative negotiation from non-collaborative negotiation such as labor negotiation (Sycara, 1989). 3) Collaborative agents are interested in &apos;The notion of shared plan has been used in (Grosz and Sidner, 1990; Allen, 1991). others&apos; beli</context>
</contexts>
<marker>Birnbaum, Flowers, McGuire, 1980</marker>
<rawString>Lawrence Birnbaum, Margot Flowers, and Rod McGuire. 1980. Towards an AI model of argumentation. In Proceedings of the National Conference on Artificial Intelligence, pages 313-315.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alison Cawsey</author>
<author>Julia Galliers</author>
<author>Brian Logan</author>
<author>Steven Reece</author>
<author>Karen Sparck Jones</author>
</authors>
<title>Revising beliefs and intentions: A unified framework for agent interaction.</title>
<date>1993</date>
<booktitle>In The Ninth Biennial Conference of the Society for the Study of Artificial Intelligence and Simulation of Behaviour,</booktitle>
<pages>130--139</pages>
<contexts>
<context position="4641" citStr="Cawsey et al., 1993" startWordPosition="689" endWordPosition="692">owever, they did not specify the criteria under which each of these strategies should be selected. 136 Walker (1994) described a method of determining when to include optional warrants to justify a claim based on factors such as communication cost, inference cost, and cost of memory retrieval. However, her model focuses on determining when to include informationally redundant utterances, whereas our model determines whether or not justification is needed for a claim to be convincing and, if so, selects appropriate evidence from the system&apos;s private beliefs to support the claim. Caswey et al. (Cawsey et al., 1993; Logan et al., 1994) introduced the idea of utilizing a belief revision mechanism (Galliers, 1992) to predict whether a set of evidence is sufficient to change a user&apos;s existing belief and to generate responses for information retrieval dialogues in a library domain. They argued that in the library dialogues they analyzed, &amp;quot;in no cases does negotiation extend beyond the initial belief conflict and its immediate resolution.&amp;quot; (Logan et al., 1994, page 141). However, our analysis of naturally-occurring consultation dialogues (Columbia University Transcripts, 1985; SRI Transcripts, 1992) shows th</context>
</contexts>
<marker>Cawsey, Galliers, Logan, Reece, Jones, 1993</marker>
<rawString>Alison Cawsey, Julia Galliers, Brian Logan, Steven Reece, and Karen Sparck Jones. 1993. Revising beliefs and intentions: A unified framework for agent interaction. In The Ninth Biennial Conference of the Society for the Study of Artificial Intelligence and Simulation of Behaviour, pages 130-139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Chu-Carroll</author>
<author>Sandra Carberry</author>
</authors>
<title>A planbased model for response generation in collaborative task-oriented dialogues.</title>
<date>1994</date>
<booktitle>In Proceedings of the Twelfth National Conference on Artificial Intelligence,</booktitle>
<pages>799--805</pages>
<contexts>
<context position="7603" citStr="Chu-Carroll and Carberry, 1994" startWordPosition="1129" endWordPosition="1132">; Allen, 1991). others&apos; beliefs in order to decide whether to revise their own beliefs so as to come to agreement (Chu-Carroll and Carberry, 1995). Although agents involved in argumentation and non-collaborative negotiation take other agents&apos; beliefs into consideration, they do so mainly to find weak points in their opponents&apos; beliefs and attack them to win the argument. In our earlier work, we built on Sidner&apos;s proposal/acceptance and proposal/rejection sequences (Sidner, 1994) and developed a model that captures collaborative planning processes in a Propose-Evaluate-Modify cycle of actions (Chu-Carroll and Carberry, 1994). This model views collaborative planning as agent A proposing a set of actions and beliefs lobe incorporated into the shared plan being developed, agent B evaluating the proposal to determine whether or not be accepts the proposal and, if not, agent B proposing a set of modifications to As original proposal. The proposed modifications will again be evaluated by A, and if conflicts arise, she may propose modifications to B&apos;s previously proposed modifications, resulting in a recursive process. However, our research did not specify, in cases where multiple conflicts arise, how an agent should id</context>
<context position="9444" citStr="Chu-Carroll and Carberry, 1994" startWordPosition="1415" endWordPosition="1418">alogue model described in (Lambert and Carberry, 1991) to represent the current status of the interaction. The enhanced dialogue model has four levels: the domain level which consists of the domain plan being constructed for the user&apos;s later execution, the problem-solving level which contains the actions being performed to construct the domain plan, the belief level which consists of the mutual beliefs pursued during the planning process in order to further the problem-solving intentions, and the discourse level which contains the communicative actions initiated to achieve the mutual beliefs (Chu-Carroll and Carberry, 1994). This paper focuses on the evaluation and modification of proposed beliefs, and details a strategy for engaging in collaborative negotiations. 137 4.1 Evaluating Proposed Beliefs Our system maintains a set of beliefs about the domain and about the user&apos;s beliefs. Associated with each belief is a strength that represents the agent&apos;s confidence in holding that belief. We model the strength of a belief using endorsements, which are explicit records of factors that affect one&apos;s certainty in a hypothesis (Cohen, 1985), following (Gathers, 1992; Logan et al., 1994). Our endorsements are based on th</context>
</contexts>
<marker>Chu-Carroll, Carberry, 1994</marker>
<rawString>Jennifer Chu-Carroll and Sandra Carberry. 1994. A planbased model for response generation in collaborative task-oriented dialogues. In Proceedings of the Twelfth National Conference on Artificial Intelligence, pages 799-805.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Chu-Carroll</author>
<author>Sandra Carberry</author>
</authors>
<title>Generating information-sharing subdialogues in expert-user consultation.</title>
<date>1995</date>
<booktitle>In Proceedings of the 14th International Joint Conference on Artificial Intelligence.</booktitle>
<note>To appear.</note>
<contexts>
<context position="7118" citStr="Chu-Carroll and Carberry, 1995" startWordPosition="1059" endWordPosition="1062">tiation are open and honest with one another; they will not deliberately present false information to other agents, present information in such a way as to mislead the other agents, or strategically hold back information from other agents for later use. This distinguishes collaborative negotiation from non-collaborative negotiation such as labor negotiation (Sycara, 1989). 3) Collaborative agents are interested in &apos;The notion of shared plan has been used in (Grosz and Sidner, 1990; Allen, 1991). others&apos; beliefs in order to decide whether to revise their own beliefs so as to come to agreement (Chu-Carroll and Carberry, 1995). Although agents involved in argumentation and non-collaborative negotiation take other agents&apos; beliefs into consideration, they do so mainly to find weak points in their opponents&apos; beliefs and attack them to win the argument. In our earlier work, we built on Sidner&apos;s proposal/acceptance and proposal/rejection sequences (Sidner, 1994) and developed a model that captures collaborative planning processes in a Propose-Evaluate-Modify cycle of actions (Chu-Carroll and Carberry, 1994). This model views collaborative planning as agent A proposing a set of actions and beliefs lobe incorporated into </context>
<context position="12834" citStr="Chu-Carroll and Carberry, 1995" startWordPosition="1956" endWordPosition="1959">trongly outweighs that of the other, the decision to accept or reject .bel is easily made. However, if the difference in their strengths does not exceed a pre-determined 2For details on how our model determines the acceptance of a belief using the ranking of endorsements proposed by Galliers, see (Chu-Carroll, 1995). Dr. Smith is not teaching AI. Dr. Smith is going on sabbatical next year. Figure 1: Belief and Discourse Levels for (2) and (3) threshold, the evaluator has insufficient information to determine whether to adopt _bet and therefore will initiate an information-sharing subdialogue (Chu-Carroll and Carberry, 1995) to share information with the user so that each of them can knowledgably re-evaluate the user&apos;s original proposal. If, during information-sharing, the user provides convincing support for a belief whose negation is held by the system, the system may adopt the belief after the re-evaluation process, thus resolving the conflict without negotiation. 4.1.1 Example To illustrate the evaluation of proposed beliefs, consider the following utterances: (I) S: I think Dr. Smith is teaching AI next semester. (2)U: Dr Smith is not teaching AL (3) He is going on sabbatical next year. Figure 1 shows the be</context>
<context position="20183" citStr="Chu-Carroll and Carberry, 1995" startWordPosition="3037" endWordPosition="3040">epted top-level belief, our process for selecting the focus of modification involves two steps: identifying a candidate foci tree from the proposed belief tree, and selecting a 6This subdialogue is considered an interrupt by Whittaker, Stenton, and Walker (Whittaker and Stenton, 1988; Walker and Whittaker, 1990), initiated to negotiate the truth of a piece of information. However, the utterances they classify as interrupts include not only our negotiation subdialogues, generated for the purpose of modifying a proposal, but also clarification subdialogues, and information-sharing subdialogues (Chu-Carroll and Carberry, 1995), which we contend should be part of the evaluation process. 139 focus from the candidate foci tree using the heuristic &amp;quot;attack the belief(s) that will most likely resolve the conflict about the top-level belief?&apos; A candidate foci tree contains the pieces of evidence in a proposed belief tree which, if disbelieved by the user, might change the user&apos;s view of the unaccepted top-level proposed belief (the root node of that belief tree). It is identified by performing a depthfirst search on the proposed belief tree. When a node is visited, both the belief and the evidential relationship between i</context>
<context position="28649" citStr="Chu-Carroll and Carberry, 1995" startWordPosition="4381" endWordPosition="4384">ed.8 Thus, it is important that a collaborative agent selects sufficient and effective, but not excessive, evidence to justify an intended mutual belief. To convince the user of a belief, _bel, our system selects appropriate justification by identifying beliefs that could 7In collaborative dialogues, an agent should reject a proposal only if she has strong evidence against it. When an agent does not have sufficient information to determine the acceptance of a proposal, she should initiate an information-sharing subdialogue to share information with the other agent and reevaluate the proposal (Chu-Carroll and Carberry, 1995). Thus, further research is needed to determine whether or not the focus of modification for a rejected belief will ever be nil in collaborative dialogues. 8Walker (1994) has shown the importance of IRU&apos;s (Informationally Redundant Utterances) in efficient discourse. We leave including appropriate IRU&apos;s for future work. be used to support _bel and applying filtering heuristics to them. The system must first determine whether justification for _bel is needed by predicting whether or not merely informing the user of _bel will be sufficient to convince him of _bel. If so, no justification will be</context>
</contexts>
<marker>Chu-Carroll, Carberry, 1995</marker>
<rawString>Jennifer Chu-Carroll and Sandra Carberry. 1995. Generating information-sharing subdialogues in expert-user consultation. In Proceedings of the 14th International Joint Conference on Artificial Intelligence. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Chu-Carroll</author>
</authors>
<title>A Plan-Based Model for Response Generation in Collaborative Consultation Dialogues.</title>
<date>1995</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Delaware. Forthcoming.</institution>
<contexts>
<context position="12520" citStr="Chu-Carroll, 1995" startWordPosition="1910" endWordPosition="1911">h of the belief and the strength of the evidential relationship. The evaluator then employs a simplified version of Gainers&apos; belief revision mechanism2 (Gainers, 1992; Logan et al., 1994) to compare the strengths of the evidence that supports and attacks _bel. If the strength of one set of evidence strongly outweighs that of the other, the decision to accept or reject .bel is easily made. However, if the difference in their strengths does not exceed a pre-determined 2For details on how our model determines the acceptance of a belief using the ranking of endorsements proposed by Galliers, see (Chu-Carroll, 1995). Dr. Smith is not teaching AI. Dr. Smith is going on sabbatical next year. Figure 1: Belief and Discourse Levels for (2) and (3) threshold, the evaluator has insufficient information to determine whether to adopt _bet and therefore will initiate an information-sharing subdialogue (Chu-Carroll and Carberry, 1995) to share information with the user so that each of them can knowledgably re-evaluate the user&apos;s original proposal. If, during information-sharing, the user provides convincing support for a belief whose negation is held by the system, the system may adopt the belief after the re-evalu</context>
</contexts>
<marker>Chu-Carroll, 1995</marker>
<rawString>Jennifer Chu-Carroll. 1995. A Plan-Based Model for Response Generation in Collaborative Consultation Dialogues. Ph.D. thesis, University of Delaware. Forthcoming.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul R Cohen</author>
</authors>
<title>Heuristic Reasoning about Uncertainty: An Artificial Intelligence Approach.</title>
<date>1985</date>
<publisher>Pitman Publishing Company.</publisher>
<contexts>
<context position="9963" citStr="Cohen, 1985" startWordPosition="1499" endWordPosition="1500">ommunicative actions initiated to achieve the mutual beliefs (Chu-Carroll and Carberry, 1994). This paper focuses on the evaluation and modification of proposed beliefs, and details a strategy for engaging in collaborative negotiations. 137 4.1 Evaluating Proposed Beliefs Our system maintains a set of beliefs about the domain and about the user&apos;s beliefs. Associated with each belief is a strength that represents the agent&apos;s confidence in holding that belief. We model the strength of a belief using endorsements, which are explicit records of factors that affect one&apos;s certainty in a hypothesis (Cohen, 1985), following (Gathers, 1992; Logan et al., 1994). Our endorsements are based on the semantics of the utterance used to convey a belief, the level of expertise of the agent conveying the belief, stereotypical knowledge, etc. The belief level of the dialogue model consists of mutual beliefs proposed by the agents&apos; discourse actions. When an agent proposes a new belief and gives (optional) supporting evidence for it, this set of proposed beliefs is represented as a belief tree, where the belief represented by a child node is intended to support that represented by its parent. The root nodes of the</context>
</contexts>
<marker>Cohen, 1985</marker>
<rawString>Paul R. Cohen. 1985. Heuristic Reasoning about Uncertainty: An Artificial Intelligence Approach. Pitman Publishing Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robin Cohen</author>
</authors>
<title>Analyzing the structure of argumentative discourse.</title>
<date>1987</date>
<journal>Computational Linguistcis,13(1-2):11-24, January-June.</journal>
<contexts>
<context position="6429" citStr="Cohen, 1987" startWordPosition="955" endWordPosition="956">rise among agents developing a shared plan]. during collaborative planning. A collaborative agent is driven by the goal of developing a plan that best satisfies the interests of all the agents as a group, instead of one that maximizes his own interest. This results in several distinctive features of collaborative negotiation: 1) A collaborative agent does not insist on winning an argument, and may change his beliefs if another agent presents convincing justification for an opposing belief. This differentiates collaborative negotiation from argumentation (Birnbaum et al., 1980; Reichman, 1981; Cohen, 1987; Quilici, 1992). 2) Agents involved in collaborative negotiation are open and honest with one another; they will not deliberately present false information to other agents, present information in such a way as to mislead the other agents, or strategically hold back information from other agents for later use. This distinguishes collaborative negotiation from non-collaborative negotiation such as labor negotiation (Sycara, 1989). 3) Collaborative agents are interested in &apos;The notion of shared plan has been used in (Grosz and Sidner, 1990; Allen, 1991). others&apos; beliefs in order to decide whethe</context>
</contexts>
<marker>Cohen, 1987</marker>
<rawString>Robin Cohen. 1987. Analyzing the structure of argumentative discourse. Computational Linguistcis,13(1-2):11-24, January-June.</rawString>
</citation>
<citation valid="true">
<title>Transcripts derived from audiotape conversations made at</title>
<date>1985</date>
<institution>Columbia University Transcripts.</institution>
<location>New York, NY.</location>
<note>Provided by Kathleen McKeown.</note>
<marker>1985</marker>
<rawString>Columbia University Transcripts. 1985. Transcripts derived from audiotape conversations made at Columbia University, New York, NY. Provided by Kathleen McKeown.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia R Garners</author>
</authors>
<title>Autonomous belief revision and communication.</title>
<date>1992</date>
<editor>In Gardenfors, editor, BeliefRevision.</editor>
<publisher>Cambridge University Press.</publisher>
<marker>Garners, 1992</marker>
<rawString>Julia R. Garners. 1992. Autonomous belief revision and communication. In Gardenfors, editor, BeliefRevision. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Paul Grice</author>
</authors>
<title>Logic and conversation.</title>
<date>1975</date>
<booktitle>Syntax and Semantics 3: Speech Acts,</booktitle>
<pages>41--58</pages>
<editor>In Peter Cole and Jerry L. Morgan, editors,</editor>
<publisher>Academic Press, Inc.,</publisher>
<location>New York.</location>
<contexts>
<context position="27944" citStr="Grice, 1975" startWordPosition="4276" endWordPosition="4277">e beliefs will be posted by the system as mutual beliefs to be achieved in order to perform the Modify actions. 4.2.2 Selecting Justification for a Claim Studies in communication and social psychology have shown that evidence improves the persuasiveness of a message (Luchok and McCroskey, 1978; Reynolds and Burgoon, 1983; Petty and Cacioppo, 1984; Hample, 1985). Research on the quantity of evidence indicates that there is no optimal amount of evidence, but that the use of high-quality evidence is consistent with persuasive effects (Reinard, 1988). On the other hand, Grice&apos;s maxim of quantity (Grice, 1975) specifies that one should not contribute more information than is re,quired.8 Thus, it is important that a collaborative agent selects sufficient and effective, but not excessive, evidence to justify an intended mutual belief. To convince the user of a belief, _bel, our system selects appropriate justification by identifying beliefs that could 7In collaborative dialogues, an agent should reject a proposal only if she has strong evidence against it. When an agent does not have sufficient information to determine the acceptance of a proposal, she should initiate an information-sharing subdialog</context>
</contexts>
<marker>Grice, 1975</marker>
<rawString>H. Paul Grice. 1975. Logic and conversation. In Peter Cole and Jerry L. Morgan, editors, Syntax and Semantics 3: Speech Acts, pages 41-58. Academic Press, Inc., New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Candace L Sidner</author>
</authors>
<title>Plans for discourse.</title>
<date>1990</date>
<booktitle>Intentions in Communication, chapter 20,</booktitle>
<pages>417--444</pages>
<editor>In Cohen, Morgan, and Pollack, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="6972" citStr="Grosz and Sidner, 1990" startWordPosition="1035" endWordPosition="1038">otiation from argumentation (Birnbaum et al., 1980; Reichman, 1981; Cohen, 1987; Quilici, 1992). 2) Agents involved in collaborative negotiation are open and honest with one another; they will not deliberately present false information to other agents, present information in such a way as to mislead the other agents, or strategically hold back information from other agents for later use. This distinguishes collaborative negotiation from non-collaborative negotiation such as labor negotiation (Sycara, 1989). 3) Collaborative agents are interested in &apos;The notion of shared plan has been used in (Grosz and Sidner, 1990; Allen, 1991). others&apos; beliefs in order to decide whether to revise their own beliefs so as to come to agreement (Chu-Carroll and Carberry, 1995). Although agents involved in argumentation and non-collaborative negotiation take other agents&apos; beliefs into consideration, they do so mainly to find weak points in their opponents&apos; beliefs and attack them to win the argument. In our earlier work, we built on Sidner&apos;s proposal/acceptance and proposal/rejection sequences (Sidner, 1994) and developed a model that captures collaborative planning processes in a Propose-Evaluate-Modify cycle of actions (</context>
</contexts>
<marker>Grosz, Sidner, 1990</marker>
<rawString>Barbara J. Grosz and Candace L. Sidner. 1990. Plans for discourse. In Cohen, Morgan, and Pollack, editors, Intentions in Communication, chapter 20, pages 417-444. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dale Hample</author>
</authors>
<title>Refinements on the cognitive model of argument: Concreteness, involvement and group scores.</title>
<date>1985</date>
<journal>The Western Journal of Speech Communication,</journal>
<pages>49--267</pages>
<contexts>
<context position="27695" citStr="Hample, 1985" startWordPosition="4236" endWordPosition="4237">be presented to him (Logan et al., 1994). The result of Select-FocusModification is a set of user beliefs (in _bel.focus) that need to be modified in order to change the user&apos;s belief about the unaccepted top-level belief. Thus, the negations of these beliefs will be posted by the system as mutual beliefs to be achieved in order to perform the Modify actions. 4.2.2 Selecting Justification for a Claim Studies in communication and social psychology have shown that evidence improves the persuasiveness of a message (Luchok and McCroskey, 1978; Reynolds and Burgoon, 1983; Petty and Cacioppo, 1984; Hample, 1985). Research on the quantity of evidence indicates that there is no optimal amount of evidence, but that the use of high-quality evidence is consistent with persuasive effects (Reinard, 1988). On the other hand, Grice&apos;s maxim of quantity (Grice, 1975) specifies that one should not contribute more information than is re,quired.8 Thus, it is important that a collaborative agent selects sufficient and effective, but not excessive, evidence to justify an intended mutual belief. To convince the user of a belief, _bel, our system selects appropriate justification by identifying beliefs that could 7In </context>
</contexts>
<marker>Hample, 1985</marker>
<rawString>Dale Hample. 1985. Refinements on the cognitive model of argument: Concreteness, involvement and group scores. The Western Journal of Speech Communication, 49:267-285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
</authors>
<title>Mutual beliefs in questionanswer systems.</title>
<date>1982</date>
<booktitle>Mutual Knowledge, chapter 4,</booktitle>
<pages>181--197</pages>
<editor>In N.V. Smith, editor,</editor>
<publisher>Academic Press.</publisher>
<contexts>
<context position="1568" citStr="Joshi, 1982" startWordPosition="222" endWordPosition="223"> evidence to justify the need for such modification. Furthermore, by capturing the negotiation process in a recursive Propose-Evaluate-Mod(y cycle of actions, our model can successfully handle embedded negotiation subdialogues. 1 Introduction In collaborative consultation dialogues, the consultant and the executing agent collaborate on developing a plan to achieve the executing agent&apos;s domain goal. Since agents are autonomous and heterogeneous, it is inevitable that conflicts in their beliefs arise during the planning process. In such cases, collaborative agents should attempt to square away (Joshi, 1982) the conflicts by engaging in collaborative negotiation to determine what should constitute their shared plan of actions and shared beliefs. Collaborative negotiation differs from non-collaborative negotiation and argumentation mainly in the attitude of the participants, since collaborative agents are not selfcentered, but act in a way as to benefit the agents as This material is based upon work supported by the National Science Foundation under Grant No. IRI-9122026. a group. Thus, when facing a conflict, a collaborative agent should not automatically reject a belief with which she does not a</context>
<context position="3855" citStr="Joshi (1982)" startWordPosition="568" endWordPosition="569">d the analysis and generation of arguments (Birnbaum et al., 1980; Reichman, 1981; Co-hen, 1987; Sycara, 1989; Quilici, 1992; Maybury, 1993); however, agents engaging in argumentative dialogues are solely interested in winning an argument and thus exhibit (Efferent behavior from collaborative agents. Sidner (1992; 1994) formulated an artificial language for modeling collaborative discourse using proposal/acceptance and proposal/rejection sequences; however, her work is descriptive and does not specify response generation strategies for agents involved in collaborative interactions. Webber and Joshi (1982) have noted the importance of a cooperative system providing support for its responses. They identified strategies that a system can adopt in justifying its beliefs; however, they did not specify the criteria under which each of these strategies should be selected. 136 Walker (1994) described a method of determining when to include optional warrants to justify a claim based on factors such as communication cost, inference cost, and cost of memory retrieval. However, her model focuses on determining when to include informationally redundant utterances, whereas our model determines whether or no</context>
</contexts>
<marker>Joshi, 1982</marker>
<rawString>Aravind K. Joshi. 1982. Mutual beliefs in questionanswer systems. In N.V. Smith, editor, Mutual Knowledge, chapter 4, pages 181-197. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynn Lambert</author>
<author>Sandra Carberry</author>
</authors>
<title>A tripartite plan-based model of dialogue.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>47--54</pages>
<contexts>
<context position="8867" citStr="Lambert and Carberry, 1991" startWordPosition="1328" endWordPosition="1331">oposal to address or how to select evidence to support the proposed modification. This paper extends that work by incorporating into the modification process a strategy to determine the aspect of the proposal that the agent will address in her pursuit of conflict resolution, as well as a means of selecting appropriate evidence to justify the need for such modification. 4 Response Generation in Collaborative Negotiation In order to capture the agents&apos; intentions conveyed by their utterances, our model of collaborative negotiation utilizes an enhanced version of the dialogue model described in (Lambert and Carberry, 1991) to represent the current status of the interaction. The enhanced dialogue model has four levels: the domain level which consists of the domain plan being constructed for the user&apos;s later execution, the problem-solving level which contains the actions being performed to construct the domain plan, the belief level which consists of the mutual beliefs pursued during the planning process in order to further the problem-solving intentions, and the discourse level which contains the communicative actions initiated to achieve the mutual beliefs (Chu-Carroll and Carberry, 1994). This paper focuses on</context>
</contexts>
<marker>Lambert, Carberry, 1991</marker>
<rawString>Lynn Lambert and Sandra Carberry. 1991. A tripartite plan-based model of dialogue. In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, pages 47-54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Logan</author>
<author>Steven Reece</author>
<author>Alison Cawsey</author>
<author>Julia Galliers</author>
<author>Karen Sparck Jones</author>
</authors>
<title>Belief revision and dialogue management in information retrieval.</title>
<date>1994</date>
<tech>Technical Report 339,</tech>
<institution>University of Cambridge, Computer Laboratory.</institution>
<contexts>
<context position="4662" citStr="Logan et al., 1994" startWordPosition="693" endWordPosition="696">specify the criteria under which each of these strategies should be selected. 136 Walker (1994) described a method of determining when to include optional warrants to justify a claim based on factors such as communication cost, inference cost, and cost of memory retrieval. However, her model focuses on determining when to include informationally redundant utterances, whereas our model determines whether or not justification is needed for a claim to be convincing and, if so, selects appropriate evidence from the system&apos;s private beliefs to support the claim. Caswey et al. (Cawsey et al., 1993; Logan et al., 1994) introduced the idea of utilizing a belief revision mechanism (Galliers, 1992) to predict whether a set of evidence is sufficient to change a user&apos;s existing belief and to generate responses for information retrieval dialogues in a library domain. They argued that in the library dialogues they analyzed, &amp;quot;in no cases does negotiation extend beyond the initial belief conflict and its immediate resolution.&amp;quot; (Logan et al., 1994, page 141). However, our analysis of naturally-occurring consultation dialogues (Columbia University Transcripts, 1985; SRI Transcripts, 1992) shows that in other domains c</context>
<context position="10010" citStr="Logan et al., 1994" startWordPosition="1504" endWordPosition="1507">e the mutual beliefs (Chu-Carroll and Carberry, 1994). This paper focuses on the evaluation and modification of proposed beliefs, and details a strategy for engaging in collaborative negotiations. 137 4.1 Evaluating Proposed Beliefs Our system maintains a set of beliefs about the domain and about the user&apos;s beliefs. Associated with each belief is a strength that represents the agent&apos;s confidence in holding that belief. We model the strength of a belief using endorsements, which are explicit records of factors that affect one&apos;s certainty in a hypothesis (Cohen, 1985), following (Gathers, 1992; Logan et al., 1994). Our endorsements are based on the semantics of the utterance used to convey a belief, the level of expertise of the agent conveying the belief, stereotypical knowledge, etc. The belief level of the dialogue model consists of mutual beliefs proposed by the agents&apos; discourse actions. When an agent proposes a new belief and gives (optional) supporting evidence for it, this set of proposed beliefs is represented as a belief tree, where the belief represented by a child node is intended to support that represented by its parent. The root nodes of these belief trees (top-level beliefs) contribute </context>
<context position="12089" citStr="Logan et al., 1994" startWordPosition="1835" endWordPosition="1838">l relationship, the evaluator first constructs an evidence set containing the system&apos;s evidence that supports or attacks _bel and the evidence accepted by the system that was proposed by the user as support for _bel. Each piece of evidence contains a belief _heli, and an evidential relationship supports(..elb_bel). Following Walker&apos;s weakest link assumption (Walker, 1992) the strength of the evidence is the weaker of the strength of the belief and the strength of the evidential relationship. The evaluator then employs a simplified version of Gainers&apos; belief revision mechanism2 (Gainers, 1992; Logan et al., 1994) to compare the strengths of the evidence that supports and attacks _bel. If the strength of one set of evidence strongly outweighs that of the other, the decision to accept or reject .bel is easily made. However, if the difference in their strengths does not exceed a pre-determined 2For details on how our model determines the acceptance of a belief using the ranking of endorsements proposed by Galliers, see (Chu-Carroll, 1995). Dr. Smith is not teaching AI. Dr. Smith is going on sabbatical next year. Figure 1: Belief and Discourse Levels for (2) and (3) threshold, the evaluator has insufficie</context>
<context position="27122" citStr="Logan et al., 1994" startWordPosition="4143" endWordPosition="4146">ted proposed evidence by combining the previous two prediction processes (step 4.4). If the combined evidence is still predicted to fail, the system does not have sufficient evidence to change the user&apos;s view of _bel; thus, the focus of modification for _bel is nil (step 4.5).7 Notice that steps 2 and 4 of the algorithm invoke a function, Predict, that makes use of the belief revision mechanism (Galliers, 1992) discussed in Section 4.1 to predict the user&apos;s acceptance or unacceptance of _bel based on the system&apos;s knowledge of the user&apos;s beliefs and the evidence that could be presented to him (Logan et al., 1994). The result of Select-FocusModification is a set of user beliefs (in _bel.focus) that need to be modified in order to change the user&apos;s belief about the unaccepted top-level belief. Thus, the negations of these beliefs will be posted by the system as mutual beliefs to be achieved in order to perform the Modify actions. 4.2.2 Selecting Justification for a Claim Studies in communication and social psychology have shown that evidence improves the persuasiveness of a message (Luchok and McCroskey, 1978; Reynolds and Burgoon, 1983; Petty and Cacioppo, 1984; Hample, 1985). Research on the quantity </context>
</contexts>
<marker>Logan, Reece, Cawsey, Galliers, Jones, 1994</marker>
<rawString>Brian Logan, Steven Reece, Alison Cawsey, Julia Galliers, and Karen Sparck Jones. 1994. Belief revision and dialogue management in information retrieval. Technical Report 339, University of Cambridge, Computer Laboratory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph A Luchok</author>
<author>James C McCroskey</author>
</authors>
<title>The effect of quality of evidence on attitude change and source credibility.</title>
<date>1978</date>
<journal>The Southern Speech Communication Journal,</journal>
<pages>43--371</pages>
<contexts>
<context position="27626" citStr="Luchok and McCroskey, 1978" startWordPosition="4224" endWordPosition="4227">based on the system&apos;s knowledge of the user&apos;s beliefs and the evidence that could be presented to him (Logan et al., 1994). The result of Select-FocusModification is a set of user beliefs (in _bel.focus) that need to be modified in order to change the user&apos;s belief about the unaccepted top-level belief. Thus, the negations of these beliefs will be posted by the system as mutual beliefs to be achieved in order to perform the Modify actions. 4.2.2 Selecting Justification for a Claim Studies in communication and social psychology have shown that evidence improves the persuasiveness of a message (Luchok and McCroskey, 1978; Reynolds and Burgoon, 1983; Petty and Cacioppo, 1984; Hample, 1985). Research on the quantity of evidence indicates that there is no optimal amount of evidence, but that the use of high-quality evidence is consistent with persuasive effects (Reinard, 1988). On the other hand, Grice&apos;s maxim of quantity (Grice, 1975) specifies that one should not contribute more information than is re,quired.8 Thus, it is important that a collaborative agent selects sufficient and effective, but not excessive, evidence to justify an intended mutual belief. To convince the user of a belief, _bel, our system sel</context>
<context position="30870" citStr="Luchok and McCroskey, 1978" startWordPosition="4745" endWordPosition="4748">s containing justification which, when presented, is predicted to convince the user of _bel. if no single justification chain is predicted to be sufficient to change the user&apos;s beliefs, new sets will be constructed by combining the single justification chains, and the selection process is repeated. This will produce a set of possible candidate justification chains, and three heuristics will then be applied to select from among them. The first heuristic prefers evidence in which the system is most confident since high-quality evidence produces more attitude change than any other evidence form (Luchok and McCroskey, 1978). Furthermore, the system can better justify a belief in which it has high confidence should the user not accept it. The second heuristic prefers evidence that is novel to the user, since studies have shown that evidence is most persuasive if it is previously unknown to the hearer (Wyer, 1970; Morley, 1987). The third heuristic is based on Grice&apos;s maxim of quantity and prefers justification chains that contain the fewest beliefs. 4.2.3 Example After the evaluation of the dialogue model in Figure 1, Modify-Proposal is invoked because the top-level proposed belief is not accepted. In selecting t</context>
</contexts>
<marker>Luchok, McCroskey, 1978</marker>
<rawString>Joseph A. Luchok and James C. McCroskey. 1978. The effect of quality of evidence on attitude change and source credibility. The Southern Speech Communication Journal, 43:371-383.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark T Maybury</author>
</authors>
<title>Communicative acts for generating natural language arguments.</title>
<date>1993</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence,</booktitle>
<pages>357--364</pages>
<contexts>
<context position="3383" citStr="Maybury, 1993" startWordPosition="506" endWordPosition="507"> when the conflict is relevant to the current task, 2) selects the most effective aspect to address in its pursuit of conflict resolution when multiple conflicts exist, 3) selects appropriate evidence to justify the system&apos;s proposed modification of the user&apos;s beliefs, and 4) captures the negotiation process in a recursive Propose-EvaluateModify cycle of actions, thus enabling the system to handle embedded negotiation subdialogues. 2 Related Work Researchers have studied the analysis and generation of arguments (Birnbaum et al., 1980; Reichman, 1981; Co-hen, 1987; Sycara, 1989; Quilici, 1992; Maybury, 1993); however, agents engaging in argumentative dialogues are solely interested in winning an argument and thus exhibit (Efferent behavior from collaborative agents. Sidner (1992; 1994) formulated an artificial language for modeling collaborative discourse using proposal/acceptance and proposal/rejection sequences; however, her work is descriptive and does not specify response generation strategies for agents involved in collaborative interactions. Webber and Joshi (1982) have noted the importance of a cooperative system providing support for its responses. They identified strategies that a system</context>
</contexts>
<marker>Maybury, 1993</marker>
<rawString>Mark T. Maybury. 1993. Communicative acts for generating natural language arguments. In Proceedings of the National Conference on Artificial Intelligence, pages 357-364.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen R McKeown</author>
</authors>
<title>Teo Generation: Using Discourse Strategies and Focus Constraints to Generate Natural Language Text.</title>
<date>1985</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="25208" citStr="McKeown, 1985" startWordPosition="3817" endWordPosition="3819">.s-attack) = -&apos;.bel (i.e., the system&apos;s evidence against .bel will cause the user to reject _bel), _bel.focus 4- _bel 4.4 // Check if addressing both _bel and its unaccepted evidence is sgOlcient Else if Predict(_bel, _bel.s-attack + _bel.u-evid - cand-set) = min-set 4- Select-Mln-Set(_bel, cand-set + _bel) -bel.fc&apos;cus 4- 1.i_beliE_min-set -beli.focus U _bel 4.5 Else _bellocus +- nil Figure 3: Selecting the Focus of Modification cause McKeown&apos;s focusing rules suggest that continuing a newly introduced topic (about which there is more to be said) is preferable to returning to a previous topic (McKeown, 1985). Thus the algorithm first considers whether Of not attacking the user&apos;s support for _bel is sufficient to convince him of --1_bel (step 4.2). It does so by gathering (in cand-set) evidence proposed by the user as direct support for _bel but which was not accepted by the system and which the system predicts it can successfully refute (i.e., _beli.focus is not nil). The algorithm then hypothesizes that the user has changed his mind about each belief in cand-set and predicts how this will affect the user&apos;s belief about _bel (step 4.2). If the user is predicted to accept -.,bel under this hypothe</context>
</contexts>
<marker>McKeown, 1985</marker>
<rawString>Kathleen R. McKeown. 1985. Teo Generation: Using Discourse Strategies and Focus Constraints to Generate Natural Language Text. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald D Morley</author>
</authors>
<title>Subjective message constructs: A theory of persuasion.</title>
<date>1987</date>
<journal>Communication Monographs,</journal>
<pages>54--183</pages>
<contexts>
<context position="31178" citStr="Morley, 1987" startWordPosition="4800" endWordPosition="4801">t of possible candidate justification chains, and three heuristics will then be applied to select from among them. The first heuristic prefers evidence in which the system is most confident since high-quality evidence produces more attitude change than any other evidence form (Luchok and McCroskey, 1978). Furthermore, the system can better justify a belief in which it has high confidence should the user not accept it. The second heuristic prefers evidence that is novel to the user, since studies have shown that evidence is most persuasive if it is previously unknown to the hearer (Wyer, 1970; Morley, 1987). The third heuristic is based on Grice&apos;s maxim of quantity and prefers justification chains that contain the fewest beliefs. 4.2.3 Example After the evaluation of the dialogue model in Figure 1, Modify-Proposal is invoked because the top-level proposed belief is not accepted. In selecting the focus of modification, the system will first identify the candidate foci tree and then invoke the Select-Focus-Modification algorithm on the belief at the root node of the candidate foci tree. The candidate foci tree will be identical to the proposed belief tree in Figure 1 since both the top-level propo</context>
</contexts>
<marker>Morley, 1987</marker>
<rawString>Donald D. Morley. 1987. Subjective message constructs: A theory of persuasion. Communication Monographs, 54:183-203.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard E Petty</author>
<author>John T Cacioppo</author>
</authors>
<title>The effects of involvement on responses to argument quantity and quality: Central and peripheral routes to persuasion.</title>
<date>1984</date>
<journal>Journal of Personality and Social Psychology,</journal>
<pages>46--1</pages>
<contexts>
<context position="27680" citStr="Petty and Cacioppo, 1984" startWordPosition="4232" endWordPosition="4235">d the evidence that could be presented to him (Logan et al., 1994). The result of Select-FocusModification is a set of user beliefs (in _bel.focus) that need to be modified in order to change the user&apos;s belief about the unaccepted top-level belief. Thus, the negations of these beliefs will be posted by the system as mutual beliefs to be achieved in order to perform the Modify actions. 4.2.2 Selecting Justification for a Claim Studies in communication and social psychology have shown that evidence improves the persuasiveness of a message (Luchok and McCroskey, 1978; Reynolds and Burgoon, 1983; Petty and Cacioppo, 1984; Hample, 1985). Research on the quantity of evidence indicates that there is no optimal amount of evidence, but that the use of high-quality evidence is consistent with persuasive effects (Reinard, 1988). On the other hand, Grice&apos;s maxim of quantity (Grice, 1975) specifies that one should not contribute more information than is re,quired.8 Thus, it is important that a collaborative agent selects sufficient and effective, but not excessive, evidence to justify an intended mutual belief. To convince the user of a belief, _bel, our system selects appropriate justification by identifying beliefs </context>
</contexts>
<marker>Petty, Cacioppo, 1984</marker>
<rawString>Richard E. Petty and John T. Cacioppo. 1984. The effects of involvement on responses to argument quantity and quality: Central and peripheral routes to persuasion. Journal of Personality and Social Psychology, 46(1):69-81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha E Pollack</author>
</authors>
<title>A model of plan inference that distinguishes between the beliefs of actors and observers.</title>
<date>1986</date>
<booktitle>In Proceedings of the 24th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>207--214</pages>
<contexts>
<context position="17285" citStr="Pollack, 1986" startWordPosition="2650" endWordPosition="2651"> its subaction, Modify-Node, that is responsible for the actual modification of the proposal. The applicability conditions5 of Correct-Node specify that the action can only be invoked when ..sl believes that _node is not acceptable while _s2 believes that it is (when ...sl and _s2 disagree about the proposed belief represented by _node). However, since this is a collaborative interaction, the actual modification can only be performed when both ..s1 and _s2 believe that _node is not acceptable — that is, the conflict between ...sl and _s2 must have been resolved. This is captured by 4A recipe (Pollack, 1986) is a template for performing aclions. It contains the applicability conditions for performing an action, the subactions comprising the body of an action, etc. 5Applicability conditions are conditions that must already be satisfied in order for an action to be reasonable to pursue, whereas an agent can try to achieve unsatisfied preconditions. Action: Coirect-Node(_s 1, ..s2, _proposed) Decomposition Appl Cond: believe(..s1,-,acceptable(_node)) believe(_s2, acceptable(_node)) Const: effor-in-plan(...node,_proposed) Body: Modify-Node(...s1,_s2,_proposecl,_nocle) Insert-Correction(-sl, _s2, .pro</context>
</contexts>
<marker>Pollack, 1986</marker>
<rawString>Martha E. Pollack. 1986. A model of plan inference that distinguishes between the beliefs of actors and observers. In Proceedings of the 24th Annual Meeting of the Association for Computational Linguistics, pages 207-214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Quilici</author>
</authors>
<title>Arguing about planning alternatives.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th International Conference on Computational Linguistics,</booktitle>
<pages>906--910</pages>
<contexts>
<context position="3367" citStr="Quilici, 1992" startWordPosition="504" endWordPosition="505">ubdialogue only when the conflict is relevant to the current task, 2) selects the most effective aspect to address in its pursuit of conflict resolution when multiple conflicts exist, 3) selects appropriate evidence to justify the system&apos;s proposed modification of the user&apos;s beliefs, and 4) captures the negotiation process in a recursive Propose-EvaluateModify cycle of actions, thus enabling the system to handle embedded negotiation subdialogues. 2 Related Work Researchers have studied the analysis and generation of arguments (Birnbaum et al., 1980; Reichman, 1981; Co-hen, 1987; Sycara, 1989; Quilici, 1992; Maybury, 1993); however, agents engaging in argumentative dialogues are solely interested in winning an argument and thus exhibit (Efferent behavior from collaborative agents. Sidner (1992; 1994) formulated an artificial language for modeling collaborative discourse using proposal/acceptance and proposal/rejection sequences; however, her work is descriptive and does not specify response generation strategies for agents involved in collaborative interactions. Webber and Joshi (1982) have noted the importance of a cooperative system providing support for its responses. They identified strategi</context>
<context position="6445" citStr="Quilici, 1992" startWordPosition="957" endWordPosition="958">ents developing a shared plan]. during collaborative planning. A collaborative agent is driven by the goal of developing a plan that best satisfies the interests of all the agents as a group, instead of one that maximizes his own interest. This results in several distinctive features of collaborative negotiation: 1) A collaborative agent does not insist on winning an argument, and may change his beliefs if another agent presents convincing justification for an opposing belief. This differentiates collaborative negotiation from argumentation (Birnbaum et al., 1980; Reichman, 1981; Cohen, 1987; Quilici, 1992). 2) Agents involved in collaborative negotiation are open and honest with one another; they will not deliberately present false information to other agents, present information in such a way as to mislead the other agents, or strategically hold back information from other agents for later use. This distinguishes collaborative negotiation from non-collaborative negotiation such as labor negotiation (Sycara, 1989). 3) Collaborative agents are interested in &apos;The notion of shared plan has been used in (Grosz and Sidner, 1990; Allen, 1991). others&apos; beliefs in order to decide whether to revise thei</context>
</contexts>
<marker>Quilici, 1992</marker>
<rawString>Alex Quilici. 1992. Arguing about planning alternatives. In Proceedings of the 14th International Conference on Computational Linguistics, pages 906-910.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rachel Reichman</author>
</authors>
<title>Modeling informal debates.</title>
<date>1981</date>
<booktitle>In Proceedings of the 7th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>19--24</pages>
<contexts>
<context position="3324" citStr="Reichman, 1981" startWordPosition="497" endWordPosition="498">cts in beliefs and initiates a negotiation subdialogue only when the conflict is relevant to the current task, 2) selects the most effective aspect to address in its pursuit of conflict resolution when multiple conflicts exist, 3) selects appropriate evidence to justify the system&apos;s proposed modification of the user&apos;s beliefs, and 4) captures the negotiation process in a recursive Propose-EvaluateModify cycle of actions, thus enabling the system to handle embedded negotiation subdialogues. 2 Related Work Researchers have studied the analysis and generation of arguments (Birnbaum et al., 1980; Reichman, 1981; Co-hen, 1987; Sycara, 1989; Quilici, 1992; Maybury, 1993); however, agents engaging in argumentative dialogues are solely interested in winning an argument and thus exhibit (Efferent behavior from collaborative agents. Sidner (1992; 1994) formulated an artificial language for modeling collaborative discourse using proposal/acceptance and proposal/rejection sequences; however, her work is descriptive and does not specify response generation strategies for agents involved in collaborative interactions. Webber and Joshi (1982) have noted the importance of a cooperative system providing support </context>
<context position="6416" citStr="Reichman, 1981" startWordPosition="953" endWordPosition="954">when conflicts arise among agents developing a shared plan]. during collaborative planning. A collaborative agent is driven by the goal of developing a plan that best satisfies the interests of all the agents as a group, instead of one that maximizes his own interest. This results in several distinctive features of collaborative negotiation: 1) A collaborative agent does not insist on winning an argument, and may change his beliefs if another agent presents convincing justification for an opposing belief. This differentiates collaborative negotiation from argumentation (Birnbaum et al., 1980; Reichman, 1981; Cohen, 1987; Quilici, 1992). 2) Agents involved in collaborative negotiation are open and honest with one another; they will not deliberately present false information to other agents, present information in such a way as to mislead the other agents, or strategically hold back information from other agents for later use. This distinguishes collaborative negotiation from non-collaborative negotiation such as labor negotiation (Sycara, 1989). 3) Collaborative agents are interested in &apos;The notion of shared plan has been used in (Grosz and Sidner, 1990; Allen, 1991). others&apos; beliefs in order to </context>
</contexts>
<marker>Reichman, 1981</marker>
<rawString>Rachel Reichman. 1981. Modeling informal debates. In Proceedings of the 7th International Joint Conference on Artificial Intelligence, pages 19-24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John C Reinard</author>
</authors>
<title>The empirical study of the persuasive effects of evidence, the status after fifty years of research.</title>
<date>1988</date>
<journal>Human Communication Research,</journal>
<pages>15--1</pages>
<contexts>
<context position="27884" citStr="Reinard, 1988" startWordPosition="4266" endWordPosition="4267">t the unaccepted top-level belief. Thus, the negations of these beliefs will be posted by the system as mutual beliefs to be achieved in order to perform the Modify actions. 4.2.2 Selecting Justification for a Claim Studies in communication and social psychology have shown that evidence improves the persuasiveness of a message (Luchok and McCroskey, 1978; Reynolds and Burgoon, 1983; Petty and Cacioppo, 1984; Hample, 1985). Research on the quantity of evidence indicates that there is no optimal amount of evidence, but that the use of high-quality evidence is consistent with persuasive effects (Reinard, 1988). On the other hand, Grice&apos;s maxim of quantity (Grice, 1975) specifies that one should not contribute more information than is re,quired.8 Thus, it is important that a collaborative agent selects sufficient and effective, but not excessive, evidence to justify an intended mutual belief. To convince the user of a belief, _bel, our system selects appropriate justification by identifying beliefs that could 7In collaborative dialogues, an agent should reject a proposal only if she has strong evidence against it. When an agent does not have sufficient information to determine the acceptance of a pr</context>
</contexts>
<marker>Reinard, 1988</marker>
<rawString>John C. Reinard. 1988. The empirical study of the persuasive effects of evidence, the status after fifty years of research. Human Communication Research, 15(1):3-59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rodney A Reynolds</author>
<author>Michael Burgoon</author>
</authors>
<title>Belief processing, reasoning, and evidence.</title>
<date>1983</date>
<booktitle>Communication Yearbook 7, chapter 4,</booktitle>
<pages>83--104</pages>
<editor>In Bostrom, editor,</editor>
<publisher>Sage Publications.</publisher>
<contexts>
<context position="27654" citStr="Reynolds and Burgoon, 1983" startWordPosition="4228" endWordPosition="4231">dge of the user&apos;s beliefs and the evidence that could be presented to him (Logan et al., 1994). The result of Select-FocusModification is a set of user beliefs (in _bel.focus) that need to be modified in order to change the user&apos;s belief about the unaccepted top-level belief. Thus, the negations of these beliefs will be posted by the system as mutual beliefs to be achieved in order to perform the Modify actions. 4.2.2 Selecting Justification for a Claim Studies in communication and social psychology have shown that evidence improves the persuasiveness of a message (Luchok and McCroskey, 1978; Reynolds and Burgoon, 1983; Petty and Cacioppo, 1984; Hample, 1985). Research on the quantity of evidence indicates that there is no optimal amount of evidence, but that the use of high-quality evidence is consistent with persuasive effects (Reinard, 1988). On the other hand, Grice&apos;s maxim of quantity (Grice, 1975) specifies that one should not contribute more information than is re,quired.8 Thus, it is important that a collaborative agent selects sufficient and effective, but not excessive, evidence to justify an intended mutual belief. To convince the user of a belief, _bel, our system selects appropriate justificati</context>
</contexts>
<marker>Reynolds, Burgoon, 1983</marker>
<rawString>Rodney A. Reynolds and Michael Burgoon. 1983. Belief processing, reasoning, and evidence. In Bostrom, editor, Communication Yearbook 7, chapter 4, pages 83-104. Sage Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Candace L Sidner</author>
</authors>
<title>Using discourse to negotiate in collaborative activity: An artificial language.</title>
<date>1992</date>
<booktitle>In AAAI-92 Workshop: Cooperation Among Heterogeneous Intelligent Systems,</booktitle>
<pages>121--128</pages>
<contexts>
<context position="3557" citStr="Sidner (1992" startWordPosition="530" endWordPosition="531">cts appropriate evidence to justify the system&apos;s proposed modification of the user&apos;s beliefs, and 4) captures the negotiation process in a recursive Propose-EvaluateModify cycle of actions, thus enabling the system to handle embedded negotiation subdialogues. 2 Related Work Researchers have studied the analysis and generation of arguments (Birnbaum et al., 1980; Reichman, 1981; Co-hen, 1987; Sycara, 1989; Quilici, 1992; Maybury, 1993); however, agents engaging in argumentative dialogues are solely interested in winning an argument and thus exhibit (Efferent behavior from collaborative agents. Sidner (1992; 1994) formulated an artificial language for modeling collaborative discourse using proposal/acceptance and proposal/rejection sequences; however, her work is descriptive and does not specify response generation strategies for agents involved in collaborative interactions. Webber and Joshi (1982) have noted the importance of a cooperative system providing support for its responses. They identified strategies that a system can adopt in justifying its beliefs; however, they did not specify the criteria under which each of these strategies should be selected. 136 Walker (1994) described a method</context>
</contexts>
<marker>Sidner, 1992</marker>
<rawString>Candace L. Sidner. 1992. Using discourse to negotiate in collaborative activity: An artificial language. In AAAI-92 Workshop: Cooperation Among Heterogeneous Intelligent Systems, pages 121-128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Candace L Sidner</author>
</authors>
<title>An artificial discourse language for collaborative negotiation.</title>
<date>1994</date>
<booktitle>In Proceedings of the Twelfth National Conference on Artificial Intelligence,</booktitle>
<pages>814--819</pages>
<contexts>
<context position="7455" citStr="Sidner, 1994" startWordPosition="1110" endWordPosition="1112">tion (Sycara, 1989). 3) Collaborative agents are interested in &apos;The notion of shared plan has been used in (Grosz and Sidner, 1990; Allen, 1991). others&apos; beliefs in order to decide whether to revise their own beliefs so as to come to agreement (Chu-Carroll and Carberry, 1995). Although agents involved in argumentation and non-collaborative negotiation take other agents&apos; beliefs into consideration, they do so mainly to find weak points in their opponents&apos; beliefs and attack them to win the argument. In our earlier work, we built on Sidner&apos;s proposal/acceptance and proposal/rejection sequences (Sidner, 1994) and developed a model that captures collaborative planning processes in a Propose-Evaluate-Modify cycle of actions (Chu-Carroll and Carberry, 1994). This model views collaborative planning as agent A proposing a set of actions and beliefs lobe incorporated into the shared plan being developed, agent B evaluating the proposal to determine whether or not be accepts the proposal and, if not, agent B proposing a set of modifications to As original proposal. The proposed modifications will again be evaluated by A, and if conflicts arise, she may propose modifications to B&apos;s previously proposed mod</context>
</contexts>
<marker>Sidner, 1994</marker>
<rawString>Candace L. Sidner. 1994. An artificial discourse language for collaborative negotiation. In Proceedings of the Twelfth National Conference on Artificial Intelligence, pages 814-819.</rawString>
</citation>
<citation valid="true">
<authors>
<author>SRI Transcripts</author>
</authors>
<title>Transcripts derived from audiotape conversations made at SRI International, Menlo Park, CA. Prepared by Jacqueline ICowtko under the direction of Patti Price.</title>
<date>1992</date>
<contexts>
<context position="5232" citStr="Transcripts, 1992" startWordPosition="779" endWordPosition="780">et al. (Cawsey et al., 1993; Logan et al., 1994) introduced the idea of utilizing a belief revision mechanism (Galliers, 1992) to predict whether a set of evidence is sufficient to change a user&apos;s existing belief and to generate responses for information retrieval dialogues in a library domain. They argued that in the library dialogues they analyzed, &amp;quot;in no cases does negotiation extend beyond the initial belief conflict and its immediate resolution.&amp;quot; (Logan et al., 1994, page 141). However, our analysis of naturally-occurring consultation dialogues (Columbia University Transcripts, 1985; SRI Transcripts, 1992) shows that in other domains conflict resolution does extend beyond a single exchange of conflicting beliefs; therefore we employ a recursive model for collaboration that captures extended negotiation and represents the structure of the discourse. Furthermore, their system deals with a single conflict, while our model selects a focus in its pursuit of conflict resolution when multiple conflicts arise. In addition, we provide a process for selecting among multiple possible pieces of evidence. 3 Features of Collaborative Negotiation Collaborative negotiation occurs when conflicts arise among age</context>
</contexts>
<marker>Transcripts, 1992</marker>
<rawString>SRI Transcripts. 1992. Transcripts derived from audiotape conversations made at SRI International, Menlo Park, CA. Prepared by Jacqueline ICowtko under the direction of Patti Price.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katia Sycara</author>
</authors>
<title>Argumentation: Planning other agents&apos; plans.</title>
<date>1989</date>
<booktitle>In Proceedings of the 11th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>517--523</pages>
<contexts>
<context position="3352" citStr="Sycara, 1989" startWordPosition="502" endWordPosition="503"> negotiation subdialogue only when the conflict is relevant to the current task, 2) selects the most effective aspect to address in its pursuit of conflict resolution when multiple conflicts exist, 3) selects appropriate evidence to justify the system&apos;s proposed modification of the user&apos;s beliefs, and 4) captures the negotiation process in a recursive Propose-EvaluateModify cycle of actions, thus enabling the system to handle embedded negotiation subdialogues. 2 Related Work Researchers have studied the analysis and generation of arguments (Birnbaum et al., 1980; Reichman, 1981; Co-hen, 1987; Sycara, 1989; Quilici, 1992; Maybury, 1993); however, agents engaging in argumentative dialogues are solely interested in winning an argument and thus exhibit (Efferent behavior from collaborative agents. Sidner (1992; 1994) formulated an artificial language for modeling collaborative discourse using proposal/acceptance and proposal/rejection sequences; however, her work is descriptive and does not specify response generation strategies for agents involved in collaborative interactions. Webber and Joshi (1982) have noted the importance of a cooperative system providing support for its responses. They iden</context>
<context position="6861" citStr="Sycara, 1989" startWordPosition="1018" endWordPosition="1019"> agent presents convincing justification for an opposing belief. This differentiates collaborative negotiation from argumentation (Birnbaum et al., 1980; Reichman, 1981; Cohen, 1987; Quilici, 1992). 2) Agents involved in collaborative negotiation are open and honest with one another; they will not deliberately present false information to other agents, present information in such a way as to mislead the other agents, or strategically hold back information from other agents for later use. This distinguishes collaborative negotiation from non-collaborative negotiation such as labor negotiation (Sycara, 1989). 3) Collaborative agents are interested in &apos;The notion of shared plan has been used in (Grosz and Sidner, 1990; Allen, 1991). others&apos; beliefs in order to decide whether to revise their own beliefs so as to come to agreement (Chu-Carroll and Carberry, 1995). Although agents involved in argumentation and non-collaborative negotiation take other agents&apos; beliefs into consideration, they do so mainly to find weak points in their opponents&apos; beliefs and attack them to win the argument. In our earlier work, we built on Sidner&apos;s proposal/acceptance and proposal/rejection sequences (Sidner, 1994) and d</context>
</contexts>
<marker>Sycara, 1989</marker>
<rawString>Katia Sycara. 1989. Argumentation: Planning other agents&apos; plans. In Proceedings of the 11th International Joint Conference on Artificial Intelligence, pages 517-523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn Walker</author>
<author>Steve Whittaker</author>
</authors>
<title>Mixed initiative in dialogue: An investigation into discourse segmentation.</title>
<date>1990</date>
<booktitle>In Proceedings of the 2&amp;h Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>70--78</pages>
<contexts>
<context position="19865" citStr="Walker and Whittaker, 1990" startWordPosition="2993" endWordPosition="2996">e proposed by the user to eliminate the user&apos;s justification for the belief, or both. Since collaborative agents are expected to engage in effective and efficient dialogues, the system should address the unaccepted belief that it predicts will most quickly resolve the top-level conflict. Therefore, for each unaccepted top-level belief, our process for selecting the focus of modification involves two steps: identifying a candidate foci tree from the proposed belief tree, and selecting a 6This subdialogue is considered an interrupt by Whittaker, Stenton, and Walker (Whittaker and Stenton, 1988; Walker and Whittaker, 1990), initiated to negotiate the truth of a piece of information. However, the utterances they classify as interrupts include not only our negotiation subdialogues, generated for the purpose of modifying a proposal, but also clarification subdialogues, and information-sharing subdialogues (Chu-Carroll and Carberry, 1995), which we contend should be part of the evaluation process. 139 focus from the candidate foci tree using the heuristic &amp;quot;attack the belief(s) that will most likely resolve the conflict about the top-level belief?&apos; A candidate foci tree contains the pieces of evidence in a proposed </context>
</contexts>
<marker>Walker, Whittaker, 1990</marker>
<rawString>Marilyn Walker and Steve Whittaker. 1990. Mixed initiative in dialogue: An investigation into discourse segmentation. In Proceedings of the 2&amp;h Annual Meeting of the Association for Computational Linguistics, pages 70-78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
</authors>
<title>Redundancy in collaborative dialogue.</title>
<date>1992</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics,</booktitle>
<pages>345--351</pages>
<contexts>
<context position="11844" citStr="Walker, 1992" startWordPosition="1797" endWordPosition="1798">collaborative agents agree on a belief relevant to the domain plan being constructed, it is irrelevant whether they agree on the evidence for that belief (Young et al., 1994). In determining whether to accept a proposed belief or evidential relationship, the evaluator first constructs an evidence set containing the system&apos;s evidence that supports or attacks _bel and the evidence accepted by the system that was proposed by the user as support for _bel. Each piece of evidence contains a belief _heli, and an evidential relationship supports(..elb_bel). Following Walker&apos;s weakest link assumption (Walker, 1992) the strength of the evidence is the weaker of the strength of the belief and the strength of the evidential relationship. The evaluator then employs a simplified version of Gainers&apos; belief revision mechanism2 (Gainers, 1992; Logan et al., 1994) to compare the strengths of the evidence that supports and attacks _bel. If the strength of one set of evidence strongly outweighs that of the other, the decision to accept or reject .bel is easily made. However, if the difference in their strengths does not exceed a pre-determined 2For details on how our model determines the acceptance of a belief usi</context>
<context position="15862" citStr="Walker, 1992" startWordPosition="2420" endWordPosition="2421">t it, the system will not accept On-Sabbatical(Smith, next year). The system believes that being on sabbatical implies a faculty member is not teaching any courses; thus the proposed evidential relationship will be accepted. However, the system will not accept the top-level proposed belief, -,Teaches(Smith, Al), since the system has a prior belief to the contrary (as expressed in utterance (1)) and the only evidence provided by the user was an implication whose antecedent was not accepted. 4.2 Modifying Unaccepted Proposals The collaborative planning principle in (Whittaker and Stenton, 1988; Walker, 1992) suggests that &amp;quot;conversants must provide evidence of a detected discrepancy in belief as soon as possible.&amp;quot; Thus, once an agent detects a relevant conflict, she must notify the other agent of the conflict and initiate a negotiation subdialogue to resolve it — to do otherwise is to fail in her responsibility as a collaborative agent. We capture the attempt to resolve a conflict with the problem-solving action Modify-Proposal, whose goal is to modify the proposal to a form that will potentially be accepted by both agents. When applied to belief modification, Modify-Proposal has two specializatio</context>
</contexts>
<marker>Walker, 1992</marker>
<rawString>Marilyn A. Walker. 1992. Redundancy in collaborative dialogue. In Proceedings of the 15th International Conference on Computational Linguistics, pages 345-351.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
</authors>
<title>Discourse and deliberation: Testing a collaborative strategy.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="4138" citStr="Walker (1994)" startWordPosition="613" endWordPosition="614">llaborative agents. Sidner (1992; 1994) formulated an artificial language for modeling collaborative discourse using proposal/acceptance and proposal/rejection sequences; however, her work is descriptive and does not specify response generation strategies for agents involved in collaborative interactions. Webber and Joshi (1982) have noted the importance of a cooperative system providing support for its responses. They identified strategies that a system can adopt in justifying its beliefs; however, they did not specify the criteria under which each of these strategies should be selected. 136 Walker (1994) described a method of determining when to include optional warrants to justify a claim based on factors such as communication cost, inference cost, and cost of memory retrieval. However, her model focuses on determining when to include informationally redundant utterances, whereas our model determines whether or not justification is needed for a claim to be convincing and, if so, selects appropriate evidence from the system&apos;s private beliefs to support the claim. Caswey et al. (Cawsey et al., 1993; Logan et al., 1994) introduced the idea of utilizing a belief revision mechanism (Galliers, 199</context>
<context position="28819" citStr="Walker (1994)" startWordPosition="4411" endWordPosition="4412">_bel, our system selects appropriate justification by identifying beliefs that could 7In collaborative dialogues, an agent should reject a proposal only if she has strong evidence against it. When an agent does not have sufficient information to determine the acceptance of a proposal, she should initiate an information-sharing subdialogue to share information with the other agent and reevaluate the proposal (Chu-Carroll and Carberry, 1995). Thus, further research is needed to determine whether or not the focus of modification for a rejected belief will ever be nil in collaborative dialogues. 8Walker (1994) has shown the importance of IRU&apos;s (Informationally Redundant Utterances) in efficient discourse. We leave including appropriate IRU&apos;s for future work. be used to support _bel and applying filtering heuristics to them. The system must first determine whether justification for _bel is needed by predicting whether or not merely informing the user of _bel will be sufficient to convince him of _bel. If so, no justification will be presented. If justification is predicted to be necessary, the system will first construct the justification chains that could be used to support _bel. For each piece of </context>
</contexts>
<marker>Walker, 1994</marker>
<rawString>Marilyn A. Walker. 1994. Discourse and deliberation: Testing a collaborative strategy. In Proceedings of the 15th International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Webber</author>
<author>Aravind Joshi</author>
</authors>
<title>Taking the initiative in natural language data base interactions: Justifying why.</title>
<date>1982</date>
<booktitle>In Proceedings of COLING-82,</booktitle>
<pages>413--418</pages>
<contexts>
<context position="3855" citStr="Webber and Joshi (1982)" startWordPosition="566" endWordPosition="569">have studied the analysis and generation of arguments (Birnbaum et al., 1980; Reichman, 1981; Co-hen, 1987; Sycara, 1989; Quilici, 1992; Maybury, 1993); however, agents engaging in argumentative dialogues are solely interested in winning an argument and thus exhibit (Efferent behavior from collaborative agents. Sidner (1992; 1994) formulated an artificial language for modeling collaborative discourse using proposal/acceptance and proposal/rejection sequences; however, her work is descriptive and does not specify response generation strategies for agents involved in collaborative interactions. Webber and Joshi (1982) have noted the importance of a cooperative system providing support for its responses. They identified strategies that a system can adopt in justifying its beliefs; however, they did not specify the criteria under which each of these strategies should be selected. 136 Walker (1994) described a method of determining when to include optional warrants to justify a claim based on factors such as communication cost, inference cost, and cost of memory retrieval. However, her model focuses on determining when to include informationally redundant utterances, whereas our model determines whether or no</context>
</contexts>
<marker>Webber, Joshi, 1982</marker>
<rawString>Bonnie Webber and Aravind Joshi. 1982. Taking the initiative in natural language data base interactions: Justifying why. In Proceedings of COLING-82, pages 413-418.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve Whittaker</author>
<author>Phil Stenton</author>
</authors>
<title>Cues and control in expert-client dialogues.</title>
<date>1988</date>
<booktitle>In Proceedings of the 26th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>123--130</pages>
<contexts>
<context position="15847" citStr="Whittaker and Stenton, 1988" startWordPosition="2416" endWordPosition="2419">rong piece of evidence against it, the system will not accept On-Sabbatical(Smith, next year). The system believes that being on sabbatical implies a faculty member is not teaching any courses; thus the proposed evidential relationship will be accepted. However, the system will not accept the top-level proposed belief, -,Teaches(Smith, Al), since the system has a prior belief to the contrary (as expressed in utterance (1)) and the only evidence provided by the user was an implication whose antecedent was not accepted. 4.2 Modifying Unaccepted Proposals The collaborative planning principle in (Whittaker and Stenton, 1988; Walker, 1992) suggests that &amp;quot;conversants must provide evidence of a detected discrepancy in belief as soon as possible.&amp;quot; Thus, once an agent detects a relevant conflict, she must notify the other agent of the conflict and initiate a negotiation subdialogue to resolve it — to do otherwise is to fail in her responsibility as a collaborative agent. We capture the attempt to resolve a conflict with the problem-solving action Modify-Proposal, whose goal is to modify the proposal to a form that will potentially be accepted by both agents. When applied to belief modification, Modify-Proposal has tw</context>
<context position="19836" citStr="Whittaker and Stenton, 1988" startWordPosition="2989" endWordPosition="2992">ddress the unaccepted evidence proposed by the user to eliminate the user&apos;s justification for the belief, or both. Since collaborative agents are expected to engage in effective and efficient dialogues, the system should address the unaccepted belief that it predicts will most quickly resolve the top-level conflict. Therefore, for each unaccepted top-level belief, our process for selecting the focus of modification involves two steps: identifying a candidate foci tree from the proposed belief tree, and selecting a 6This subdialogue is considered an interrupt by Whittaker, Stenton, and Walker (Whittaker and Stenton, 1988; Walker and Whittaker, 1990), initiated to negotiate the truth of a piece of information. However, the utterances they classify as interrupts include not only our negotiation subdialogues, generated for the purpose of modifying a proposal, but also clarification subdialogues, and information-sharing subdialogues (Chu-Carroll and Carberry, 1995), which we contend should be part of the evaluation process. 139 focus from the candidate foci tree using the heuristic &amp;quot;attack the belief(s) that will most likely resolve the conflict about the top-level belief?&apos; A candidate foci tree contains the piec</context>
</contexts>
<marker>Whittaker, Stenton, 1988</marker>
<rawString>Steve Whittaker and Phil Stenton. 1988. Cues and control in expert-client dialogues. In Proceedings of the 26th Annual Meeting of the Association for Computational Linguistics, pages 123-130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert S Wyer</author>
</authors>
<title>Information redundancy, inconsistency, and novelty and their role in impression formation.</title>
<date>1970</date>
<journal>Journal of Experimental Social Psychology,</journal>
<pages>6--111</pages>
<contexts>
<context position="31163" citStr="Wyer, 1970" startWordPosition="4798" endWordPosition="4799">produce a set of possible candidate justification chains, and three heuristics will then be applied to select from among them. The first heuristic prefers evidence in which the system is most confident since high-quality evidence produces more attitude change than any other evidence form (Luchok and McCroskey, 1978). Furthermore, the system can better justify a belief in which it has high confidence should the user not accept it. The second heuristic prefers evidence that is novel to the user, since studies have shown that evidence is most persuasive if it is previously unknown to the hearer (Wyer, 1970; Morley, 1987). The third heuristic is based on Grice&apos;s maxim of quantity and prefers justification chains that contain the fewest beliefs. 4.2.3 Example After the evaluation of the dialogue model in Figure 1, Modify-Proposal is invoked because the top-level proposed belief is not accepted. In selecting the focus of modification, the system will first identify the candidate foci tree and then invoke the Select-Focus-Modification algorithm on the belief at the root node of the candidate foci tree. The candidate foci tree will be identical to the proposed belief tree in Figure 1 since both the </context>
</contexts>
<marker>Wyer, 1970</marker>
<rawString>Robert S. Wyer, Jr. 1970. Information redundancy, inconsistency, and novelty and their role in impression formation. Journal of Experimental Social Psychology, 6:111-127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Michael Young</author>
<author>Johanna D Moore</author>
<author>Martha E Pollack</author>
</authors>
<title>Towards a principled representation of discourse plans.</title>
<date>1994</date>
<booktitle>In Proceedings of the Sixteenth Annual Meeting of the Cognitive Science Society,</booktitle>
<pages>946--951</pages>
<contexts>
<context position="11405" citStr="Young et al., 1994" startWordPosition="1730" endWordPosition="1733">nitiate a negotiation dialogue to resolve conflicts. The evaluation of proposed beliefs starts at the leaf nodes of the proposed belief trees since acceptance of a piece of proposed evidence may affect acceptance of the parent belief it is intended to support. The process continues until the top-level proposed beliefs are evaluated. Conflict resolution strategies are invoked only if the top-level proposed beliefs are not accepted because if collaborative agents agree on a belief relevant to the domain plan being constructed, it is irrelevant whether they agree on the evidence for that belief (Young et al., 1994). In determining whether to accept a proposed belief or evidential relationship, the evaluator first constructs an evidence set containing the system&apos;s evidence that supports or attacks _bel and the evidence accepted by the system that was proposed by the user as support for _bel. Each piece of evidence contains a belief _heli, and an evidential relationship supports(..elb_bel). Following Walker&apos;s weakest link assumption (Walker, 1992) the strength of the evidence is the weaker of the strength of the belief and the strength of the evidential relationship. The evaluator then employs a simplifie</context>
<context position="21062" citStr="Young et al., 1994" startWordPosition="3186" endWordPosition="3189">vidence in a proposed belief tree which, if disbelieved by the user, might change the user&apos;s view of the unaccepted top-level proposed belief (the root node of that belief tree). It is identified by performing a depthfirst search on the proposed belief tree. When a node is visited, both the belief and the evidential relationship between it and its parent are examined. If both the belief and relationship were accepted by the evaluator, the search on the current branch will terminate, since once the system accepts a belief, it is irrelevant whether it accepts the user&apos;s support for that belief (Young et al., 1994). Otherwise, this piece of evidence will be included in the candidate foci tree and the system will continue to search through the evidence in the belief tree proposed as support for the unaccepted belief and/or evidential relationship. Once a candidate foci tree is identified, the system should select the focus of modification based on the likelihood of each choice changing the user&apos;s belief about the top-level belief. Figure 3 shows our algorithm for this selection process. Given an unaccepted belief (..bel) and the beliefs proposed to support it, Select-FocusModification will annotate _bel </context>
</contexts>
<marker>Young, Moore, Pollack, 1994</marker>
<rawString>R. Michael Young, Johanna D. Moore, and Martha E. Pollack. 1994. Towards a principled representation of discourse plans. In Proceedings of the Sixteenth Annual Meeting of the Cognitive Science Society, pages 946-951.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>