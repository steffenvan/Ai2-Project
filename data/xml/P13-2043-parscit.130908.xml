<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000218">
<title confidence="0.9947995">
Discriminative Approach to Fill-in-the-Blank Quiz Generation for
Language Learners
</title>
<author confidence="0.973597">
Keisuke Sakaguchi&apos;* Yuki Arase2 Mamoru Komachi&apos;†
</author>
<affiliation confidence="0.904176333333333">
&apos;Nara Institute of Science and Technology
8916-5 Takayama, Ikoma, Nara, 630-0192, Japan
2Microsoft Research Asia
</affiliation>
<address confidence="0.969435">
Bldg.2, No. 5 Danling St., Haidian Dist., Beijing, P. R. China
</address>
<email confidence="0.998188">
{keisuke-sa, komachil@is.naist.jp, yukiar@microsoft.com
</email>
<sectionHeader confidence="0.993907" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999880933333333">
We propose discriminative methods to
generate semantic distractors of fill-in-the-
blank quiz for language learners using a
large-scale language learners’ corpus. Un-
like previous studies, the proposed meth-
ods aim at satisfying both reliability and
validity of generated distractors; distrac-
tors should be exclusive against answers
to avoid multiple answers in one quiz,
and distractors should discriminate learn-
ers’ proficiency. Detailed user evaluation
with 3 native and 23 non-native speakers
of English shows that our methods achieve
better reliability and validity than previous
methods.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998406875">
Fill-in-the-blank is a popular style used for eval-
uating proficiency of language learners, from
homework to official tests, such as TOEIC1 and
TOEFL2. As shown in Figure 1, a quiz is com-
posed of 4 parts; (1) sentence, (2) blank to fill in,
(3) correct answer, and (4) distractors (incorrect
options). However, it is not easy to come up with
appropriate distractors without rich experience in
language education. There are two major require-
ments that distractors should satisfy: reliability
and validity (Alderson et al., 1995). First, distrac-
tors should be reliable; they are exclusive against
the answer and none of distractors can replace the
answer to avoid allowing multiple correct answers
in one quiz. Second, distractors should be valid;
they discriminate learners’ proficiency adequately.
</bodyText>
<footnote confidence="0.855168875">
* This work has been done when the author was visiting
Microsoft Research Asia.
† Now at Tokyo Metropolitan University (Email: ko-
machi@tmu.ac.jp).
lhttp://www.ets.org/toeic
zhttp://www.ets.org/toefl
Each side, government and opposition, is _____
the other for the political crisis, and for the violence.
</footnote>
<figure confidence="0.975308">
(a) blaming (b) accusing (c) BOTH
</figure>
<figureCaption confidence="0.999629">
Figure 1: Example of a fill-in-the-blank quiz,
</figureCaption>
<bodyText confidence="0.987810176470588">
where (a) blaming is the answer and (b) accusing
is a distractor.
There are previous studies on distractor gener-
ation for automatic fill-in-the-blank quiz genera-
tion (Mitkov et al., 2006). Hoshino and Nakagawa
(2005) randomly selected distractors from words
in the same document. Sumita et al. (2005) used
an English thesaurus to generate distractors. Liu et
al. (2005) collected distractor candidates that are
close to the answer in terms of word-frequency,
and ranked them by an association/collocation
measure between the candidate and surrounding
words in a given context. Dahlmeier and Ng
(2011) generated candidates for collocation er-
ror correction for English as a Second Language
(ESL) writing using paraphrasing with native lan-
guage (L1) pivoting technique. This method takes
an sentence containing a collocation error as in-
put and translates it into L1, and then translate it
back to English to generate correction candidates.
Although the purpose is different, the technique is
also applicable for distractor generation. To our
best knowledge, there have not been studies that
fully employed actual errors made by ESL learn-
ers for distractor generation.
In this paper, we propose automated distrac-
tor generation methods using a large-scale ESL
corpus with a discriminative model. We focus
on semantically confusing distractors that measure
learners’ competence to distinguish word-sense
and select an appropriate word. We especially tar-
get verbs, because verbs are difficult for language
learners to use correctly (Leacock et al., 2010).
Our proposed methods use discriminative models
</bodyText>
<page confidence="0.960704">
238
</page>
<note confidence="0.3737895">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 238–242,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<figure confidence="0.705648">
Orig. I stop company on today .
Corr. I quit a company today .
Type NA #REP# #DEL# NA #INS# NA NA
</figure>
<figureCaption confidence="0.888753">
Figure 2: Example of a sentence correction pair
</figureCaption>
<bodyText confidence="0.978972421052632">
and error tags (Replacement, Deletion and Inser-
tion).
trained on error patterns extracted from an ESL
corpus, and can generate exclusive distractors with
taking context of a given sentence into considera-
tion.
We conduct human evaluation using 3 native
and 23 non-native speakers of English. The result
shows that 98.3% of distractors generated by our
methods are reliable. Furthermore, the non-native
speakers’ performance on quiz generated by our
method has about 0.76 of correlation coefficient
with their TOEIC scores, which shows that dis-
tractors generated by our methods satisfy validity.
Contributions of this paper are twofold; (1) we
present methods for generating reliable and valid
distractors, (2) we also demonstrate the effective-
ness of ESL corpus and discriminative models on
distractor generation.
</bodyText>
<sectionHeader confidence="0.970696" genericHeader="method">
2 Proposed Method
</sectionHeader>
<bodyText confidence="0.99994175">
To generate distractors, we first need to decide
which word to be blanked. We then generate can-
didates of distractors and rank them based on a
certain criterion to select distractors to output.
In this section, we propose our methods for ex-
tracting target words from ESL corpus and select-
ing distractors by a discriminative model that con-
siders long-distance context of a given sentence.
</bodyText>
<subsectionHeader confidence="0.992217">
2.1 Error-Correction Pair Extraction
</subsectionHeader>
<bodyText confidence="0.999971">
We use the Lang-8 Corpus of Learner English3 as
a large-scale ESL corpus, which consists of 1.2M
sentence correction pairs. For generating semantic
distractors, we regard a correction as a target and
the misused word as one of the distractor candi-
dates.
In the Lang-8 corpus, there is no clue to align
the original and corrected words. In addition,
words may be deleted and inserted in the corrected
sentence, which makes the alignment difficult.
Therefore, we detect word deletion, insertion, and
replacement by dynamic programming4. We com-
</bodyText>
<footnote confidence="0.960573666666667">
3http://cl.naist.jp/nldata/lang-8/
4The implementation is available at https:
//github.com/tkyf/epair
</footnote>
<table confidence="0.450955857142857">
Feature Example
Word[i-2] ,
Word[i-1] is
Word[i+1] the
Word[i+2] other
Dep[i] child nsubj side, aux is, dobj other, prep for
Class accuse
</table>
<tableCaption confidence="0.7930425">
Table 1: Example of features and class label ex-
tracted from a sentence: Each side, government
</tableCaption>
<bodyText confidence="0.97818240625">
and opposition, is *accusing/blaming the other for
the political crisis, and for the violence.
pare a corrected sentence against its original sen-
tence, and when word insertion and deletion er-
rors are identified, we put a spaceholder (Figure
2). We then extract error-correction (i.e. replace-
ment) pairs by comparing trigrams around the re-
placement in the original and corrected sentences,
for considering surrounding context of the target.
These error-correction pairs are a mixture of gram-
matical mistakes, spelling errors, and semantic
confusions. Therefore, we identify pairs due to se-
mantic confusion; we exclude grammatical error
corrections by eliminating pairs whose error and
correction have different part-of-speech (POS)5,
and exclude spelling error corrections based on
edit-distance. As a result, we extract 689 unique
verbs (lemma) and 3,885 correction pairs in total.
Using the error-correction pairs, we calculate
conditional probabilities P(welw,), which repre-
sent how probable that ESL learners misuse the
word w, as we. Based on the probabilities, we
compute a confusion matrix. The confusion ma-
trix can generate distractors reflecting error pat-
terns of ESL learners. Given a sentence, we iden-
tify verbs appearing in the confusion matrix and
make them blank, then outputs distractor candi-
dates that have high confusion probability. We
rank the candidates by a generative model to
consider the surrounding context (e.g. N-gram).
We refer to this generative method as Confusion-
matrix Method (CFM).
</bodyText>
<subsectionHeader confidence="0.533425">
2.2 Discriminative Model for Distractor
Generation and Selection
</subsectionHeader>
<bodyText confidence="0.9995434">
To generate distractors that considers long-
distance context and reflects detailed syntactic in-
formation of the sentence, we train multiple clas-
sifiers for each target word using error-correction
pairs extracted from ESL corpus. A classifier for
</bodyText>
<footnote confidence="0.9953155">
5Because the Lang-8 corpus does not have POS tags, we
assign POS by the NLTK (http://nltk.org/) toolkit.
</footnote>
<page confidence="0.991474">
239
</page>
<bodyText confidence="0.999972361111111">
a target word takes a sentence (in which the tar-
get word appears) as an input and outputs a verb
as the best distractor given the context using fol-
lowing features: 5-gram (f1 and f2 words of the
target) lemmas and dependency type with the tar-
get child (lemma). The dependent is normalized
when it is a pronoun, date, time, or number (e.g. he
-+ #PRP#) to avoid making feature space sparse.
Table 1 shows an example of features and a class
label for the classifier of a target verb (blame).
These classifiers are based on a discriminative
model: Support Vector Machine (SVM)6 (Vapnik,
1995). We propose two methods for training the
classifiers.
First, we directly use the corrected sentences in
the Lang-8 corpus. As shown in Table 1, we use
the 5-gram and dependency features7, and use the
original word (misused word by ESL learners) as
a class. We refer to this method as DiscESL.
Second, we train classifiers with an ESL-
simulated native corpus, because (1) the number
of sentences containing a certain error-correction
pair is still limited in the ESL corpus and (2)
corrected sentences are still difficult to parse cor-
rectly due to inherent noise in the Lang-8 corpus.
Specifically, we use articles collected from Voice
of America (VOA) Learning English8, which con-
sist of 270k sentences. For each target in a given
sentence, we artificially change the target into an
incorrect word according to the error probabilities
obtained from the learners confusion matrix ex-
plained in Section 2.2. In order to collect a suf-
ficient amount of training data, we generate 100
samples for each training sentence in which the
target word is replaced into an erroneous word.
We refer to this method as DiscSimESL9.
</bodyText>
<sectionHeader confidence="0.92325" genericHeader="method">
3 Evaluation with Native-Speakers
</sectionHeader>
<bodyText confidence="0.999897714285714">
In this experiment, we evaluate the reliability of
generated distractors. The authors asked the help
of 3 native speakers of English (1 male and 2 fe-
males, majoring computer science) from an au-
thor’s graduate school. We provide each partici-
pant a gift card of $30 as a compensation when
completing the task.
</bodyText>
<footnote confidence="0.922239">
6We use Linear SVM with default settings in the scikit-
learn toolkit 0.13.1. http://scikit-learn.org
7We use the Stanford CoreNLP 1.3.4 http://nlp.
stanford.edu/software/corenlp.shtml
8http://learningenglish.voanews.com/
9The implementation is available at https:
//github.com/keisks/disc-sim-esl
</footnote>
<table confidence="0.9983475">
Method Corpus Model
Proposed ESL Generative
CFM ESL Discriminative
DiscESL Pseudo-ESL Discriminative
DiscSimESL
Baseline Native Generative
THM Native Generative
RTM
</table>
<tableCaption confidence="0.836193">
Table 2: Summary of proposed methods (CFM:
Confusion Matrix Method, DiscESL: Discrimina-
</tableCaption>
<bodyText confidence="0.986771416666667">
tive model with ESL corpus, DiscSimESL: Dis-
criminative model with simulated ESL corpus)
and baseline (THM: Thesaurus Method, RTM:
Roundtrip Method).
In order to compare distractors generated by dif-
ferent methods, we ask participants to solve the
generated fill-in-the-blank quiz presented in Fig-
ure 1. Each quiz has 3 options: (a) only word A
is correct, (b) only word B is correct, (c) both are
correct. The source sentences to generate a quiz
are collected from VOA, which are not included in
the training dataset of the DiscSimESL. We gen-
erate 50 quizzes using different sentences per each
method to avoid showing the same sentence mul-
tiple times to participants. We randomly ordered
the quizzes generated by different methods for fair
comparison.
We compare the proposed methods to two base-
lines implementing previous studies: Thesaurus-
based Method (THM) and Roundtrip Translation
Method (RTM). Table 2 shows a summary of each
method. The THM is based on (Sumita et al.,
2005) and extract distractor candidates from syn-
onyms of the target extracted from WordNet10.
The RTM is based on (Dahlmeier and Ng, 2011)
and extracts distractor candidates from roundtrip
(pivoting) translation lexicon constructed from the
WITS corpus (Cettolo et al., 2012)11, which cov-
ers a wide variety of topics. We build English-
Japanese and Japanese-English word-based trans-
lation tables using GIZA++ (IBM Model4). In
this dictionary, the target word is translated into
Japanese words and they are translated back to En-
glish as distractor candidates. To consider (local)
context, the candidates generated by the THM,
RTM, and CFM are re-ranked by 5-gram language
</bodyText>
<footnote confidence="0.998541333333333">
10WordNet 3.0 http://wordnet.princeton.
edu/wordnet/
11Available at http://wit3.fbk.eu
</footnote>
<page confidence="0.975196">
240
</page>
<table confidence="0.999336777777778">
Method RAD r,
(%)
Proposed
CFM 94.5 (93.1 - 96.0) 0.55
DiscESL 95.0 (93.6 - 96.3) 0.73
DiscSimESL 98.3 (97.5 - 99.1) 0.69
Baseline
THM 89.3 (87.4 - 91.3) 0.57
RTM 93.6 (92.1 - 95.1) 0.53
</table>
<tableCaption confidence="0.996879">
Table 3: Ratio of appropriate distractors (RAD)
</tableCaption>
<bodyText confidence="0.982811305555555">
with a 95% confidence interval and inter-rater
agreement statistics K.
model score trained on Google 1T Web Corpus
(Brants and Franz, 2006) with IRSTLM toolkit12.
As an evaluation metric, we compute the ratio
of appropriate distractors (RAD) by the following
equation: RAD = NAD/NALL, where NALL is
the total number of quizzes and NAD is the num-
ber of quizzes on which more than or equal to 2
participants agree by selecting the correct answer.
When at least 2 participants select the option (c)
(both options are correct), we determine the dis-
tractor as inappropriate. We also compute the av-
erage of inter-rater agreement K among all partici-
pants for each method.
Table 3 shows the results of the first experiment;
RAD with a 95% confidence interval and inter-
rater agreement K. All of our proposed methods
outperform baselines regarding RAD with high
inter-rater agreement. In particular, DiscSimESL
achieves 9.0% and 4.7% higher RAD than THM
and RTM, respectively. These results show that
the effectiveness of using ESL corpus to gener-
ate reliable distractors. With respect to K, our
discriminative models achieve from 0.12 to 0.2
higher agreement than baselines, indicating that
the discriminative models can generate sound dis-
tractors more effectively than generative models.
The lower K on generative models may be because
the distractors are semantically too close to the tar-
get (correct answer) as following examples:
The coalition has *published/issued a
report saying that... .
As a result, the quiz from generative models is not
reliable since both published and issued are cor-
rect.
</bodyText>
<sectionHeader confidence="0.944885" genericHeader="method">
4 Evaluation with ESL Learners
</sectionHeader>
<bodyText confidence="0.927747">
In this experiment, we evaluate the validity of gen-
erated distractors regarding ESL learners’ profi-
</bodyText>
<table confidence="0.997845125">
Method r Corr Dist Both Std
Proposed
CFM 0.71 56.7 29.6 13.5 11.5
DiscESL 0.48 62.4 27.9 10.4 12.8
DiscSimESL 0.76 64.0 20.7 15.1 13.4
Baseline
THM 0.68 57.2 28.1 14.6 10.7
RTM 0.67 63.4 26.9 9.5 13.2
</table>
<tableCaption confidence="0.793266666666667">
Table 4: (1) Correlation coefficient r against par-
ticipants’ TOEIC scores, (2) the average percent-
age of correct answer (Corr), incorrect answer of
distractor (Dist), and incorrect answer that both
are correct (Both) chosen by participants, and (3)
standard deviation (Std) of Corr.
</tableCaption>
<figure confidence="0.99586075">
100
DiscSimESL
Thesaurus (THM)
80
70
60
50
40
30
20
300 400 500 600 700 800 900 1000
TOEIC Score
</figure>
<figureCaption confidence="0.840652333333333">
Figure 3: Correlation between the participants’
TOEIC scores and accuracy on THM and Disc-
SimESL.
</figureCaption>
<bodyText confidence="0.9831165">
ciency. Twenty-three Japanese native speakers (15
males and 8 females) are participated. All the par-
ticipants, who have taken at least 8 years of En-
glish education, self-report proficiency levels as
the TOEIC scores from 380 to 99013. All the par-
ticipants are graduate students majoring in science
related courses. We call for participants by e-
mailing to a graduate school. We provide each
participant a gift card of $10 as a compensation
when completing the task. We ask participants
to solve 20 quizzes per each method in the same
manner as Section 3. To evaluate validity of dis-
tractors, we use only reliable quizzes accepted in
Section 3. Namely, we exclude quizzes whose op-
tions are both correct. We evaluate correlation be-
tween learners’ accuracy for the generated quizzes
and the TOEIC score.
Table 4 represents the results; the highest corre-
</bodyText>
<figure confidence="0.858499">
Accuracy (%)
90
</figure>
<footnote confidence="0.971502">
12The irstlm toolkit 5.80 http://sourceforge.
net/projects/irstlm/files/irstlm/
13The official score range of the TOEIC is from 10 to 990.
</footnote>
<page confidence="0.996721">
241
</page>
<bodyText confidence="0.999688833333333">
lation coefficient r and standard deviation on Disc-
SimESL shows that its distractors achieve best va-
lidity. Figure 3 depicts the correlations between
the participants’ TOEIC scores and accuracy (i.e.
Corr.) on THM and DiscSimESL. It illustrates that
DiscSimESL achieves higher level of positive cor-
relation than THM. Table 4 also shows high per-
centage of choosing “(c) both are correct” on Disc-
SimESL, which indicates that distractors gener-
ated from DiscSimESL are difficult to distinguish
for ESL learners but not for native speakers as a
following example:
..., she found herself on stage ...
*playing/performing a number one hit.
A relatively lower correlation coefficient on
DiscESL may be caused by inherent noise on pars-
ing the Lang-8 corpus and domain difference from
quiz sentences (VOA).
</bodyText>
<sectionHeader confidence="0.998834" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999990882352941">
We have presented methods that automatically
generate semantic distractors of a fill-in-the-blank
quiz for ESL learners. The proposed methods em-
ploy discriminative models trained using error pat-
terns extracted from ESL corpus and can gener-
ate reliable distractors by taking context of a given
sentence into consideration. The human evalua-
tion shows that 98.3% of distractors are reliable
when generated by our method (DiscSimESL).
The results also demonstrate 0.76 of correlation
coefficient to their TOEIC scores, indicating that
the distractors have better validity than previous
methods. As future work, we plan to extend
our methods for other POS, such as adjective and
noun. Moreover, we will take ESL learners’ pro-
ficiency into account for generating distractors of
appropriate levels for different learners.
</bodyText>
<sectionHeader confidence="0.998361" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999924285714286">
This work was supported by the Microsoft Re-
search Collaborative Research (CORE) Projects.
We are grateful to Yangyang Xi for granting per-
mission to use text from Lang-8 and Takuya Fu-
jino for his error pair extraction algorithm. We
would also thank anonymous reviewers for valu-
able comments and suggestions.
</bodyText>
<sectionHeader confidence="0.996323" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999890395833333">
Charles Alderson, Caroline Clapham, and Dianne Wall.
1995. Language Test Construction and Evaluation.
Cambridge University Press.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-
gram Corpus version 1.1. Technical report, Google
Research.
Mauro Cettolo, Christian Girardi, and Marcello Fed-
erico. 2012. WIT3 : Web Inventory of Transcribed
and Translated Talks. In Proceedings of the 16th
Conference of the European Associattion for Ma-
chine Translation (EAMT), pages 261–268, Trent,
Italy, May.
Daniel Dahlmeier and Hwee Tou Ng. 2011. Cor-
recting semantic collocation errors with l1-induced
paraphrases. In Proceedings of the 2011 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 107–117, Edinburgh, Scotland, UK.,
July.
Ayako Hoshino and Hiroshi Nakagawa. 2005. A Real-
Time Multiple-Choice Question Generation for Lan-
guage Testing ― A Preliminary Study ―. In Pro-
ceedings of the 2nd Workshop on Building Educa-
tional Applications Using NLP, pages 17–20, Ann
Arbor, June.
Claudia Leacock, Martin Chodorow, Michael Gamon,
and Joel R. Tetreault. 2010. Automated Grammat-
ical Error Detection for Language Learners. Syn-
thesis Lectures on Human Language Technologies.
Morgan &amp; Claypool Publishers.
Chao-Lin Liu, Chun-Hung Wang, Zhao-Ming Gao,
and Shang-Ming Huang. 2005. Applications of
Lexical Information for Algorithmically Composing
Multiple-Choice Cloze Items. In Proceedings of the
2nd Workshop on Building Educational Applications
Using NLP, pages 1–8, Ann Arbor, June.
Ruslan Mitkov, Le An Ha, and Nikiforos Karamanis.
2006. A Computer-Aided Environment for Generat-
ing Multiple-Choice Test Items. Natural Language
Engineering, 12:177–194, 5.
Eiichiro Sumita, Fumiaki Sugaya, and Seiichi Ya-
mamoto. 2005. Measuring Non-native Speak-
ers’ Proficiency of English by Using a Test with
Automatically-Generated Fill-in-the-Blank Ques-
tions. In Proceedings of the 2nd Workshop on Build-
ing Educational Applications Using NLP, pages 61–
68, Ann Arbor, June.
Vladimir Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer.
</reference>
<page confidence="0.997717">
242
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.651317">
<title confidence="0.9783865">Discriminative Approach to Fill-in-the-Blank Quiz Generation Language Learners</title>
<affiliation confidence="0.997769">Institute of Science and</affiliation>
<address confidence="0.968684">8916-5 Takayama, Ikoma, Nara, 630-0192,</address>
<affiliation confidence="0.734248">Research</affiliation>
<address confidence="0.890559">Bldg.2, No. 5 Danling St., Haidian Dist., Beijing, P. R. China</address>
<email confidence="0.999865">yukiar@microsoft.com</email>
<abstract confidence="0.99781725">We propose discriminative methods to generate semantic distractors of fill-in-theblank quiz for language learners using a large-scale language learners’ corpus. Unlike previous studies, the proposed methaim at satisfying both generated distractors; distractors should be exclusive against answers to avoid multiple answers in one quiz, and distractors should discriminate learners’ proficiency. Detailed user evaluation with 3 native and 23 non-native speakers of English shows that our methods achieve better reliability and validity than previous methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Charles Alderson</author>
<author>Caroline Clapham</author>
<author>Dianne Wall</author>
</authors>
<title>Language Test Construction and Evaluation.</title>
<date>1995</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="1504" citStr="Alderson et al., 1995" startWordPosition="212" endWordPosition="215"> that our methods achieve better reliability and validity than previous methods. 1 Introduction Fill-in-the-blank is a popular style used for evaluating proficiency of language learners, from homework to official tests, such as TOEIC1 and TOEFL2. As shown in Figure 1, a quiz is composed of 4 parts; (1) sentence, (2) blank to fill in, (3) correct answer, and (4) distractors (incorrect options). However, it is not easy to come up with appropriate distractors without rich experience in language education. There are two major requirements that distractors should satisfy: reliability and validity (Alderson et al., 1995). First, distractors should be reliable; they are exclusive against the answer and none of distractors can replace the answer to avoid allowing multiple correct answers in one quiz. Second, distractors should be valid; they discriminate learners’ proficiency adequately. * This work has been done when the author was visiting Microsoft Research Asia. † Now at Tokyo Metropolitan University (Email: komachi@tmu.ac.jp). lhttp://www.ets.org/toeic zhttp://www.ets.org/toefl Each side, government and opposition, is _____ the other for the political crisis, and for the violence. (a) blaming (b) accusing </context>
</contexts>
<marker>Alderson, Clapham, Wall, 1995</marker>
<rawString>Charles Alderson, Caroline Clapham, and Dianne Wall. 1995. Language Test Construction and Evaluation. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<title>Web 1T 5-gram Corpus version 1.1.</title>
<date>2006</date>
<tech>Technical report, Google Research.</tech>
<contexts>
<context position="12860" citStr="Brants and Franz, 2006" startWordPosition="1963" endWordPosition="1966">ted back to English as distractor candidates. To consider (local) context, the candidates generated by the THM, RTM, and CFM are re-ranked by 5-gram language 10WordNet 3.0 http://wordnet.princeton. edu/wordnet/ 11Available at http://wit3.fbk.eu 240 Method RAD r, (%) Proposed CFM 94.5 (93.1 - 96.0) 0.55 DiscESL 95.0 (93.6 - 96.3) 0.73 DiscSimESL 98.3 (97.5 - 99.1) 0.69 Baseline THM 89.3 (87.4 - 91.3) 0.57 RTM 93.6 (92.1 - 95.1) 0.53 Table 3: Ratio of appropriate distractors (RAD) with a 95% confidence interval and inter-rater agreement statistics K. model score trained on Google 1T Web Corpus (Brants and Franz, 2006) with IRSTLM toolkit12. As an evaluation metric, we compute the ratio of appropriate distractors (RAD) by the following equation: RAD = NAD/NALL, where NALL is the total number of quizzes and NAD is the number of quizzes on which more than or equal to 2 participants agree by selecting the correct answer. When at least 2 participants select the option (c) (both options are correct), we determine the distractor as inappropriate. We also compute the average of inter-rater agreement K among all participants for each method. Table 3 shows the results of the first experiment; RAD with a 95% confiden</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram Corpus version 1.1. Technical report, Google Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mauro Cettolo</author>
<author>Christian Girardi</author>
<author>Marcello Federico</author>
</authors>
<title>WIT3 : Web Inventory of Transcribed and Translated Talks.</title>
<date>2012</date>
<booktitle>In Proceedings of the 16th Conference of the European Associattion for Machine Translation (EAMT),</booktitle>
<pages>261--268</pages>
<location>Trent, Italy,</location>
<contexts>
<context position="12001" citStr="Cettolo et al., 2012" startWordPosition="1828" endWordPosition="1831">tence multiple times to participants. We randomly ordered the quizzes generated by different methods for fair comparison. We compare the proposed methods to two baselines implementing previous studies: Thesaurusbased Method (THM) and Roundtrip Translation Method (RTM). Table 2 shows a summary of each method. The THM is based on (Sumita et al., 2005) and extract distractor candidates from synonyms of the target extracted from WordNet10. The RTM is based on (Dahlmeier and Ng, 2011) and extracts distractor candidates from roundtrip (pivoting) translation lexicon constructed from the WITS corpus (Cettolo et al., 2012)11, which covers a wide variety of topics. We build EnglishJapanese and Japanese-English word-based translation tables using GIZA++ (IBM Model4). In this dictionary, the target word is translated into Japanese words and they are translated back to English as distractor candidates. To consider (local) context, the candidates generated by the THM, RTM, and CFM are re-ranked by 5-gram language 10WordNet 3.0 http://wordnet.princeton. edu/wordnet/ 11Available at http://wit3.fbk.eu 240 Method RAD r, (%) Proposed CFM 94.5 (93.1 - 96.0) 0.55 DiscESL 95.0 (93.6 - 96.3) 0.73 DiscSimESL 98.3 (97.5 - 99.1</context>
</contexts>
<marker>Cettolo, Girardi, Federico, 2012</marker>
<rawString>Mauro Cettolo, Christian Girardi, and Marcello Federico. 2012. WIT3 : Web Inventory of Transcribed and Translated Talks. In Proceedings of the 16th Conference of the European Associattion for Machine Translation (EAMT), pages 261–268, Trent, Italy, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Dahlmeier</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Correcting semantic collocation errors with l1-induced paraphrases.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>107--117</pages>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="2760" citStr="Dahlmeier and Ng (2011)" startWordPosition="398" endWordPosition="401">ill-in-the-blank quiz, where (a) blaming is the answer and (b) accusing is a distractor. There are previous studies on distractor generation for automatic fill-in-the-blank quiz generation (Mitkov et al., 2006). Hoshino and Nakagawa (2005) randomly selected distractors from words in the same document. Sumita et al. (2005) used an English thesaurus to generate distractors. Liu et al. (2005) collected distractor candidates that are close to the answer in terms of word-frequency, and ranked them by an association/collocation measure between the candidate and surrounding words in a given context. Dahlmeier and Ng (2011) generated candidates for collocation error correction for English as a Second Language (ESL) writing using paraphrasing with native language (L1) pivoting technique. This method takes an sentence containing a collocation error as input and translates it into L1, and then translate it back to English to generate correction candidates. Although the purpose is different, the technique is also applicable for distractor generation. To our best knowledge, there have not been studies that fully employed actual errors made by ESL learners for distractor generation. In this paper, we propose automated</context>
<context position="11864" citStr="Dahlmeier and Ng, 2011" startWordPosition="1810" endWordPosition="1813">d in the training dataset of the DiscSimESL. We generate 50 quizzes using different sentences per each method to avoid showing the same sentence multiple times to participants. We randomly ordered the quizzes generated by different methods for fair comparison. We compare the proposed methods to two baselines implementing previous studies: Thesaurusbased Method (THM) and Roundtrip Translation Method (RTM). Table 2 shows a summary of each method. The THM is based on (Sumita et al., 2005) and extract distractor candidates from synonyms of the target extracted from WordNet10. The RTM is based on (Dahlmeier and Ng, 2011) and extracts distractor candidates from roundtrip (pivoting) translation lexicon constructed from the WITS corpus (Cettolo et al., 2012)11, which covers a wide variety of topics. We build EnglishJapanese and Japanese-English word-based translation tables using GIZA++ (IBM Model4). In this dictionary, the target word is translated into Japanese words and they are translated back to English as distractor candidates. To consider (local) context, the candidates generated by the THM, RTM, and CFM are re-ranked by 5-gram language 10WordNet 3.0 http://wordnet.princeton. edu/wordnet/ 11Available at h</context>
</contexts>
<marker>Dahlmeier, Ng, 2011</marker>
<rawString>Daniel Dahlmeier and Hwee Tou Ng. 2011. Correcting semantic collocation errors with l1-induced paraphrases. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 107–117, Edinburgh, Scotland, UK., July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ayako Hoshino</author>
<author>Hiroshi Nakagawa</author>
</authors>
<title>A RealTime Multiple-Choice Question Generation for Language Testing ― A Preliminary Study ―.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2nd Workshop on Building Educational Applications Using NLP,</booktitle>
<pages>17--20</pages>
<location>Ann Arbor,</location>
<contexts>
<context position="2376" citStr="Hoshino and Nakagawa (2005)" startWordPosition="340" endWordPosition="343">’ proficiency adequately. * This work has been done when the author was visiting Microsoft Research Asia. † Now at Tokyo Metropolitan University (Email: komachi@tmu.ac.jp). lhttp://www.ets.org/toeic zhttp://www.ets.org/toefl Each side, government and opposition, is _____ the other for the political crisis, and for the violence. (a) blaming (b) accusing (c) BOTH Figure 1: Example of a fill-in-the-blank quiz, where (a) blaming is the answer and (b) accusing is a distractor. There are previous studies on distractor generation for automatic fill-in-the-blank quiz generation (Mitkov et al., 2006). Hoshino and Nakagawa (2005) randomly selected distractors from words in the same document. Sumita et al. (2005) used an English thesaurus to generate distractors. Liu et al. (2005) collected distractor candidates that are close to the answer in terms of word-frequency, and ranked them by an association/collocation measure between the candidate and surrounding words in a given context. Dahlmeier and Ng (2011) generated candidates for collocation error correction for English as a Second Language (ESL) writing using paraphrasing with native language (L1) pivoting technique. This method takes an sentence containing a colloc</context>
</contexts>
<marker>Hoshino, Nakagawa, 2005</marker>
<rawString>Ayako Hoshino and Hiroshi Nakagawa. 2005. A RealTime Multiple-Choice Question Generation for Language Testing ― A Preliminary Study ―. In Proceedings of the 2nd Workshop on Building Educational Applications Using NLP, pages 17–20, Ann Arbor, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Martin Chodorow</author>
<author>Michael Gamon</author>
<author>Joel R Tetreault</author>
</authors>
<title>Automated Grammatical Error Detection for Language Learners. Synthesis Lectures on Human Language Technologies.</title>
<date>2010</date>
<publisher>Morgan &amp; Claypool Publishers.</publisher>
<contexts>
<context position="3707" citStr="Leacock et al., 2010" startWordPosition="541" endWordPosition="544">andidates. Although the purpose is different, the technique is also applicable for distractor generation. To our best knowledge, there have not been studies that fully employed actual errors made by ESL learners for distractor generation. In this paper, we propose automated distractor generation methods using a large-scale ESL corpus with a discriminative model. We focus on semantically confusing distractors that measure learners’ competence to distinguish word-sense and select an appropriate word. We especially target verbs, because verbs are difficult for language learners to use correctly (Leacock et al., 2010). Our proposed methods use discriminative models 238 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 238–242, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Orig. I stop company on today . Corr. I quit a company today . Type NA #REP# #DEL# NA #INS# NA NA Figure 2: Example of a sentence correction pair and error tags (Replacement, Deletion and Insertion). trained on error patterns extracted from an ESL corpus, and can generate exclusive distractors with taking context of a given sentence into consideration. We co</context>
</contexts>
<marker>Leacock, Chodorow, Gamon, Tetreault, 2010</marker>
<rawString>Claudia Leacock, Martin Chodorow, Michael Gamon, and Joel R. Tetreault. 2010. Automated Grammatical Error Detection for Language Learners. Synthesis Lectures on Human Language Technologies. Morgan &amp; Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chao-Lin Liu</author>
<author>Chun-Hung Wang</author>
<author>Zhao-Ming Gao</author>
<author>Shang-Ming Huang</author>
</authors>
<title>Applications of Lexical Information for Algorithmically Composing Multiple-Choice Cloze Items.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2nd Workshop on Building Educational Applications Using NLP,</booktitle>
<pages>1--8</pages>
<location>Ann Arbor,</location>
<contexts>
<context position="2529" citStr="Liu et al. (2005)" startWordPosition="364" endWordPosition="367">u.ac.jp). lhttp://www.ets.org/toeic zhttp://www.ets.org/toefl Each side, government and opposition, is _____ the other for the political crisis, and for the violence. (a) blaming (b) accusing (c) BOTH Figure 1: Example of a fill-in-the-blank quiz, where (a) blaming is the answer and (b) accusing is a distractor. There are previous studies on distractor generation for automatic fill-in-the-blank quiz generation (Mitkov et al., 2006). Hoshino and Nakagawa (2005) randomly selected distractors from words in the same document. Sumita et al. (2005) used an English thesaurus to generate distractors. Liu et al. (2005) collected distractor candidates that are close to the answer in terms of word-frequency, and ranked them by an association/collocation measure between the candidate and surrounding words in a given context. Dahlmeier and Ng (2011) generated candidates for collocation error correction for English as a Second Language (ESL) writing using paraphrasing with native language (L1) pivoting technique. This method takes an sentence containing a collocation error as input and translates it into L1, and then translate it back to English to generate correction candidates. Although the purpose is differen</context>
</contexts>
<marker>Liu, Wang, Gao, Huang, 2005</marker>
<rawString>Chao-Lin Liu, Chun-Hung Wang, Zhao-Ming Gao, and Shang-Ming Huang. 2005. Applications of Lexical Information for Algorithmically Composing Multiple-Choice Cloze Items. In Proceedings of the 2nd Workshop on Building Educational Applications Using NLP, pages 1–8, Ann Arbor, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruslan Mitkov</author>
</authors>
<title>Le An Ha, and Nikiforos Karamanis.</title>
<date>2006</date>
<journal>Natural Language Engineering,</journal>
<volume>12</volume>
<pages>5</pages>
<marker>Mitkov, 2006</marker>
<rawString>Ruslan Mitkov, Le An Ha, and Nikiforos Karamanis. 2006. A Computer-Aided Environment for Generating Multiple-Choice Test Items. Natural Language Engineering, 12:177–194, 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eiichiro Sumita</author>
<author>Fumiaki Sugaya</author>
<author>Seiichi Yamamoto</author>
</authors>
<title>Measuring Non-native Speakers’ Proficiency of English by Using a Test with Automatically-Generated Fill-in-the-Blank Questions.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2nd Workshop on Building Educational Applications Using NLP,</booktitle>
<pages>61--68</pages>
<location>Ann Arbor,</location>
<contexts>
<context position="2460" citStr="Sumita et al. (2005)" startWordPosition="353" endWordPosition="356">Research Asia. † Now at Tokyo Metropolitan University (Email: komachi@tmu.ac.jp). lhttp://www.ets.org/toeic zhttp://www.ets.org/toefl Each side, government and opposition, is _____ the other for the political crisis, and for the violence. (a) blaming (b) accusing (c) BOTH Figure 1: Example of a fill-in-the-blank quiz, where (a) blaming is the answer and (b) accusing is a distractor. There are previous studies on distractor generation for automatic fill-in-the-blank quiz generation (Mitkov et al., 2006). Hoshino and Nakagawa (2005) randomly selected distractors from words in the same document. Sumita et al. (2005) used an English thesaurus to generate distractors. Liu et al. (2005) collected distractor candidates that are close to the answer in terms of word-frequency, and ranked them by an association/collocation measure between the candidate and surrounding words in a given context. Dahlmeier and Ng (2011) generated candidates for collocation error correction for English as a Second Language (ESL) writing using paraphrasing with native language (L1) pivoting technique. This method takes an sentence containing a collocation error as input and translates it into L1, and then translate it back to Englis</context>
<context position="11731" citStr="Sumita et al., 2005" startWordPosition="1788" endWordPosition="1791">nly word B is correct, (c) both are correct. The source sentences to generate a quiz are collected from VOA, which are not included in the training dataset of the DiscSimESL. We generate 50 quizzes using different sentences per each method to avoid showing the same sentence multiple times to participants. We randomly ordered the quizzes generated by different methods for fair comparison. We compare the proposed methods to two baselines implementing previous studies: Thesaurusbased Method (THM) and Roundtrip Translation Method (RTM). Table 2 shows a summary of each method. The THM is based on (Sumita et al., 2005) and extract distractor candidates from synonyms of the target extracted from WordNet10. The RTM is based on (Dahlmeier and Ng, 2011) and extracts distractor candidates from roundtrip (pivoting) translation lexicon constructed from the WITS corpus (Cettolo et al., 2012)11, which covers a wide variety of topics. We build EnglishJapanese and Japanese-English word-based translation tables using GIZA++ (IBM Model4). In this dictionary, the target word is translated into Japanese words and they are translated back to English as distractor candidates. To consider (local) context, the candidates gene</context>
</contexts>
<marker>Sumita, Sugaya, Yamamoto, 2005</marker>
<rawString>Eiichiro Sumita, Fumiaki Sugaya, and Seiichi Yamamoto. 2005. Measuring Non-native Speakers’ Proficiency of English by Using a Test with Automatically-Generated Fill-in-the-Blank Questions. In Proceedings of the 2nd Workshop on Building Educational Applications Using NLP, pages 61– 68, Ann Arbor, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer.</publisher>
<contexts>
<context position="8746" citStr="Vapnik, 1995" startWordPosition="1326" endWordPosition="1327">nltk.org/) toolkit. 239 a target word takes a sentence (in which the target word appears) as an input and outputs a verb as the best distractor given the context using following features: 5-gram (f1 and f2 words of the target) lemmas and dependency type with the target child (lemma). The dependent is normalized when it is a pronoun, date, time, or number (e.g. he -+ #PRP#) to avoid making feature space sparse. Table 1 shows an example of features and a class label for the classifier of a target verb (blame). These classifiers are based on a discriminative model: Support Vector Machine (SVM)6 (Vapnik, 1995). We propose two methods for training the classifiers. First, we directly use the corrected sentences in the Lang-8 corpus. As shown in Table 1, we use the 5-gram and dependency features7, and use the original word (misused word by ESL learners) as a class. We refer to this method as DiscESL. Second, we train classifiers with an ESLsimulated native corpus, because (1) the number of sentences containing a certain error-correction pair is still limited in the ESL corpus and (2) corrected sentences are still difficult to parse correctly due to inherent noise in the Lang-8 corpus. Specifically, we</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>Vladimir Vapnik. 1995. The Nature of Statistical Learning Theory. Springer.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>