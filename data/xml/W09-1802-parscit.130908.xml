<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000009">
<title confidence="0.996525">
A Scalable Global Model for Summarization
</title>
<author confidence="0.977189">
Dan Gillick1 2, Benoit Favre2
</author>
<affiliation confidence="0.956844">
1 Computer Science Division, University of California Berkeley, USA
2 International Computer Science Institute, Berkeley, USA
</affiliation>
<email confidence="0.99761">
{dgillick,favre}@icsi.berkeley.edu
</email>
<sectionHeader confidence="0.996653" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999753">
We present an Integer Linear Program for
exact inference under a maximum coverage
model for automatic summarization. We com-
pare our model, which operates at the sub-
sentence or “concept”-level, to a sentence-
level model, previously solved with an ILP.
Our model scales more efficiently to larger
problems because it does not require a
quadratic number of variables to address re-
dundancy in pairs of selected sentences. We
also show how to include sentence compres-
sion in the ILP formulation, which has the
desirable property of performing compression
and sentence selection simultaneously. The
resulting system performs at least as well as
the best systems participating in the recent
Text Analysis Conference, as judged by a va-
riety of automatic and manual content-based
metrics.
</bodyText>
<sectionHeader confidence="0.998885" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999238909090909">
Automatic summarization systems are typically ex-
tractive or abstractive. Since abstraction is quite
hard, the most successful systems tested at the Text
Analysis Conference (TAC) and Document Under-
standing Conference (DUC)1, for example, are ex-
tractive. In particular, sentence selection represents
a reasonable trade-off between linguistic quality,
guaranteed by longer textual units, and summary
content, often improved with shorter units.
Whereas the majority of approaches employ a
greedy search to find a set of sentences that is
</bodyText>
<footnote confidence="0.939691">
1TAC is a continuation of DUC, which ran from 2001-2007.
</footnote>
<bodyText confidence="0.999807970588235">
both relevant and non-redundant (Goldstein et al.,
2000; Nenkova and Vanderwende, 2005), some re-
cent work focuses on improved search (McDonald,
2007; Yih et al., 2007). Among them, McDonald is
the first to consider a non-approximated maximiza-
tion of an objective function through Integer Linear
Programming (ILP), which improves on a greedy
search by 4-12%. His formulation assumes that the
quality of a summary is proportional to the sum of
the relevance scores of the selected sentences, penal-
ized by the sum of the redundancy scores of all pairs
of selected sentences. Under a maximum summary
length constraint, this problem can be expressed as
a quadratic knapsack (Gallo et al., 1980) and many
methods are available to solve it (Pisinger et al.,
2005). However, McDonald reports that the method
is not scalable above 100 input sentences and dis-
cusses more practical approximations. Still, an ILP
formulation is appealing because it gives exact so-
lutions and lends itself well to extensions through
additional constraints.
Methods like McDonald’s, including the well-
known Maximal Marginal Relevance (MMR) algo-
rithm (Goldstein et al., 2000), are subject to an-
other problem: Summary-level redundancy is not
always well modeled by pairwise sentence-level re-
dundancy. Figure 1 shows an example where the
combination of sentences (1) and (2) overlaps com-
pletely with sentence (3), a fact not captured by pair-
wise redundancy measures. Redundancy, like con-
tent selection, is a global problem.
Here, we discuss a model for sentence selection
with a globally optimal solution that also addresses
redundancy globally. We choose to represent infor-
</bodyText>
<page confidence="0.987175">
10
</page>
<note confidence="0.9895075">
Proceedings of the NAACL HLT Workshop on Integer Linear Programming for Natural Language Processing, pages 10–18,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<listItem confidence="0.646660333333333">
(1) The cat is in the kitchen.
(2) The cat drinks the milk.
(3) The cat drinks the milk in the kitchen.
</listItem>
<figureCaption confidence="0.987942">
Figure 1: Example of sentences redundant as a group.
</figureCaption>
<bodyText confidence="0.974020681818182">
Their redundancy is only partially captured by sentence-
level pairwise measurement.
mation at a finer granularity than sentences, with
concepts, and assume that the value of a summary is
the sum of the values of the unique concepts it con-
tains. While the concepts we use in experiments are
word n-grams, we use the generic term to emphasize
that this is just one possible definition. Only credit-
ing each concept once serves as an implicit global
constraint on redundancy. We show how the result-
ing optimization problem can be mapped to an ILP
that can be solved efficiently with standard software.
We begin by comparing our model to McDonald’s
(section 2) and detail the differences between the re-
sulting ILP formulations (section 3), showing that
ours can give competitive results (section 4) and of-
fer better scalability2 (section 5). Next we demon-
strate how our ILP formulation can be extended to
include efficient parse-tree-based sentence compres-
sion (section 6). We review related work (section 7)
and conclude with a discussion of potential improve-
ments to the model (section 8).
</bodyText>
<sectionHeader confidence="0.978846" genericHeader="introduction">
2 Models
</sectionHeader>
<bodyText confidence="0.999779888888889">
The model proposed by McDonald (2007) considers
information and redundancy at the sentence level.
The score of a summary is defined as the sum of
the relevance scores of the sentences it contains mi-
nus the sum of the redundancy scores of each pair of
these sentences. If si is an indicator for the presence
of sentence i in the summary, Reli is its relevance,
and Redij is its redundancy with sentence j, then a
summary is scored according to:
</bodyText>
<equation confidence="0.8720315">
� �Relisi − Redijsisj
i ij
</equation>
<bodyText confidence="0.97917">
Generating a summary under this model involves
maximizing this objective function, subject to a
</bodyText>
<footnote confidence="0.985005333333333">
2Strictly speaking, exact inference for the models discussed
in this paper is NP-hard. Thus we use the term “scalable” in a
purely practical sense.
</footnote>
<bodyText confidence="0.996547214285714">
length constraint. A variety of choices for Reli and
Redij are possible, from simple word overlap met-
rics to the output of feature-based classifiers trained
to perform information retrieval and textual entail-
ment.
As an alternative, we consider information and re-
dundancy at a sub-sentence, “concept” level, model-
ing the value of a summary as a function of the con-
cepts it covers. While McDonald uses an explicit
redundancy term, we model redundancy implicitly:
a summary only benefits from including each con-
cept once. With ci an indicator for the presence of
concept i in the summary, and its weight wi, the ob-
jective function is:
</bodyText>
<equation confidence="0.9068595">
�
i
</equation>
<bodyText confidence="0.9999745">
We generate a summary by choosing a set of sen-
tences that maximizes this objective function, sub-
ject to the usual length constraint.
In summing over concept weights, we assume that
the value of including a concept is not effected by the
presence of any other concept in the summary. That
is, concepts are assumed to be independent. Choos-
ing a suitable definition for concepts, and a map-
ping from the input documents to concept weights,
is both important and difficult. Concepts could be
words, named entities, syntactic subtrees or seman-
tic relations, for example. While deeper semantics
make more appealing concepts, their extraction and
weighting are much more error-prone. Any error in
concept extraction can result in a biased objective
function, leading to poor sentence selection.
</bodyText>
<sectionHeader confidence="0.993246" genericHeader="method">
3 Inference by ILP
</sectionHeader>
<bodyText confidence="0.9987535">
Each model presented above can be formalized as an
Integer Linear Program, with a solution represent-
ing an optimal selection of sentences under the ob-
jective function, subject to a length constraint. Mc-
Donald observes that the redundancy term makes for
a quadratic objective function, which he coerces to
a linear function by introducing additional variables
sij that represent the presence of both sentence i and
sentence j in the summary. Additional constraints
ensure the consistency between the sentence vari-
ables (si, sj) and the quadratic term (sij). With li
the length of sentence i and L the length limit for
</bodyText>
<equation confidence="0.852777">
wici
</equation>
<page confidence="0.966935">
11
</page>
<bodyText confidence="0.93757">
the whole summary, the resulting ILP is:
</bodyText>
<equation confidence="0.87105375">
�Maximize: �Relisi − Redijsij
i ij
�Subject to: ljsj &lt; L
j
sij &lt; si sij &lt; sj Vi, j
si + sj − sij &lt; 1 Vi, j
si E {0, 11 Vi
sij E {0, 11 Vi, j
</equation>
<bodyText confidence="0.998924428571429">
To express our concept-based model as an ILP, we
maintain our notation from section 2, with ci an in-
dicator for the presence of concept i in the summary
and sj an indicator for the presence of sentence j in
the summary. We add Occij to indicate the occur-
rence of concept i in sentence j, resulting in a new
ILP:
</bodyText>
<equation confidence="0.963112222222222">
�Maximize: wici
i
�Subject to: ljsj &lt; L
j
sjOccij &lt; ci, Vi,j (1)
� sjOccij &gt; ci Vi (2)
j
ci E {0, 11 Vi
sj E {0, 11 Vj
</equation>
<bodyText confidence="0.999876">
Note that Occ, like Rel and Red, is a constant pa-
rameter. The constraints formalized in equations (1)
and (2) ensure the logical consistency of the solu-
tion: selecting a sentence necessitates selecting all
the concepts it contains and selecting a concept is
only possible if it is present in at least one selected
sentence. Constraint (1) also prevents the inclusion
of concept-less sentences.
</bodyText>
<sectionHeader confidence="0.997248" genericHeader="method">
4 Performance
</sectionHeader>
<bodyText confidence="0.9980808">
Here we compare both models on a common sum-
marization task. The data is part of the Text Analy-
sis Conference (TAC) multi-document summariza-
tion evaluation and involves generating 100-word
summaries from 10 newswire documents, each on
a given topic. While the 2008 edition of TAC also
includes an update task—additional summaries as-
suming some prior knowledge—we focus only on
the standard task. This includes 48 topics, averag-
ing 235 input sentences (ranging from 47 to 652).
Since the mean sentence length is around 25 words,
a typical summary consists of 4 sentences.
In order to facilitate comparison, we generate
summaries from both models using a common
pipeline:
</bodyText>
<listItem confidence="0.9924263">
1. Clean input documents. A simple set of rules
removes headers and formatting markup.
2. Split text into sentences. We use the unsuper-
vised Punkt system (Kiss and Strunk, 2006).
3. Prune sentences shorter than 5 words.
4. Compute parameters needed by the models.
5. Map to ILP format and solve. We use an open
source solver3.
6. Order sentences picked by the ILP for inclusion
in the summary.
</listItem>
<bodyText confidence="0.99762075">
The specifics of step 4 are described in detail in
(McDonald, 2007) and (Gillick et al., 2008). Mc-
Donald’s sentence relevance combines word-level
cosine similarity with the source document and the
inverse of its position (early sentences tend to be
more important). Redundancy between a pair of sen-
tences is their cosine similarity. For sentence i in
document D,
</bodyText>
<equation confidence="0.99321">
Reli = cosine(i, D) + 1/pos(i, D)
Redij = cosine(i, j)
</equation>
<bodyText confidence="0.946056538461539">
In our concept-based model, we use word bi-
grams, weighted by the number of input documents
in which they appear. While word bigrams stretch
the notion of a concept a bit thin, they are eas-
ily extracted and matched (we use stemming to al-
low slightly more robust matching). Table 1 pro-
vides some justification for document frequency as a
weighting function. Note that bigrams gave consis-
tently better performance than unigrams or trigrams
for a variety of ROUGE measures. Normalizing
by document frequency measured over a generic set
(TFIDF weighting) degraded ROUGE performance.
3gnu.org/software/glpk
</bodyText>
<page confidence="0.991164">
12
</page>
<bodyText confidence="0.999426">
Bigrams consisting of two stopwords are pruned, as
are those appearing in fewer than three documents.
We largely ignore the sentence ordering problem,
sorting the resulting sentences first by source docu-
ment date, and then by position, so that the order of
two originally adjacent sentences is preserved, for
example.
</bodyText>
<table confidence="0.9991955">
Doc. Freq. (D) 1 2 3 4 5 6
In Gold Set 156 48 25 15 10 7
Not in Gold Set 5270 448 114 42 21 11
Relevant (P) 0.03 0.10 0.18 0.26 0.33 0.39
</table>
<tableCaption confidence="0.5984495">
Table 1: There is a strong relationship between the docu-
ment frequency of input bigrams and the fraction of those
bigrams that appear in the human generated “gold” set:
Let di be document frequency i and pi be the percent of
input bigrams with di that are actually in the gold set.
Then the correlation ρ(D, P) = 0.95 for DUC 2007 and
0.97 for DUC 2006. Data here averaged over all prob-
lems in DUC 2007.
</tableCaption>
<bodyText confidence="0.999987464285714">
The summaries produced by the two systems
have been evaluated automatically with ROUGE and
manually with the Pyramid metric. In particular,
ROUGE-2 is the recall in bigrams with a set of
human-written abstractive summaries (Lin, 2004).
The Pyramid score arises from a manual alignment
of basic facts from the reference summaries, called
Summary Content Units (SCUs), in a hypothesis
summary (Nenkova and Passonneau, 2004). We
used the SCUs provided by the TAC evaluation.
Table 2 compares these results, alongside a base-
line that uses the first 100 words of the most re-
cent document. All the scores are significantly
different, showing that according to both human
and automatic content evaluation, the concept-
based model outperforms McDonald’s sentence-
based model, which in turn outperforms the base-
line. Of course, the relevance and redundancy func-
tions used for McDonald’s formulation in this exper-
iment are rather primitive, and results would likely
improve with better relevance features as used in
many TAC systems. Nonetheless, our system based
on word bigram concepts, similarly primitive, per-
formed at least as well as any in the TAC evaluation,
according to two-tailed t-tests comparing ROUGE,
Pyramid, and manually evaluated “content respon-
siveness” (Dang and Owczarzak, 2008) of our sys-
tem and the highest scoring system in each category.
</bodyText>
<table confidence="0.88119125">
System ROUGE-2 Pyramid
Baseline 0.058 0.186
McDonald 0.072 0.295
Concepts 0.110 0.345
</table>
<tableCaption confidence="0.793842333333333">
Table 2: Scores for both systems and a baseline on TAC
2008 data (Set A) for ROUGE-2 and Pyramid evalua-
tions.
</tableCaption>
<sectionHeader confidence="0.988979" genericHeader="method">
5 Scalability
</sectionHeader>
<bodyText confidence="0.999687777777778">
McDonald’s sentence-level formulation corresponds
to a quadratic knapsack, and he shows his particu-
lar variant is NP-hard by reduction to 3-D matching.
The concept-level formulation is similar in spirit to
the classical maximum coverage problem: Given a
set of items X, a set of subsets 5 of X, and an in-
teger k, the goal is to pick at most k subsets from
5 that maximizes the size of their union. Maximum
coverage is known to be NP-hard by reduction to the
set cover problem (Hochbaum, 1996).
Perhaps the simplest way to show that our formu-
lation is NP-hard is by reduction to the knapsack
problem (Karp, 1972). Consider the special case
where sentences do not share any overlapping con-
cepts. Then, the value of each sentence to the sum-
mary is independent of every other sentence. This is
a knapsack problem: trying to maximize the value
in a container of limited size. Given a solver for our
problem, we could solve all knapsack problem in-
stances, so our problem must also be NP-hard.
With n input sentences and m concepts, both
formulations generate a quadratic number of con-
straints. However, McDonald’s has O(n2) variables
while ours has O(n + m). In practice, scalability
is largely determined by the sparsity of the redun-
dancy matrix Red and the sentence-concept matrix
Occ. Efficient solutions thus depend heavily on the
choice of redundancy measure in McDonald’s for-
mulation and the choice of concepts in ours. Prun-
ing to reduce complexity involves removing low-
relevance sentences or ignoring low redundancy val-
ues in the former, and corresponds to removing low-
weight concepts in the latter. Note that pruning con-
cepts may be more desirable: Pruned sentences are
irretrievable, but pruned concepts may well appear
in the selected sentences through co-occurrence.
</bodyText>
<figureCaption confidence="0.529511">
Figure 2 compares ILP run-times for the two
</figureCaption>
<page confidence="0.99496">
13
</page>
<bodyText confidence="0.9999565">
formulations, using a set of 25 topics from DUC
2007, each of which have at least 500 input sen-
tences. These are very similar to the TAC 2008
topics, but more input documents are provided for
each topic, which allowed us to extend the analysis
to larger problems. While the ILP solver finds opti-
mal solutions efficiently for our concept-based for-
mulation, run-time for McDonald’s approach grows
very rapidly. The plot includes timing results for
250-word summaries as well, showing that our ap-
proach is fast even for much more complex prob-
lems: A rough estimate for the number of possible
summaries has(540) = 2.6 × 109 for 100-word sum-
maries and (500) = 2.5 × 1020 for 250 words sum-
maries.
While exact solutions are theoretically appealing,
they are only useful in practice if fast approxima-
tions are inferior. A greedy approximation of our
objective function gives 10% lower ROUGE scores
than the exact solution, a gap that separates the high-
est scoring systems from the middle of the pack in
the TAC evaluation. The greedy solution (linear in
the number of sentences, assuming a constant sum-
mary length) marks an upper bound on speed and
a lower bound on performance; The ILP solution
marks an upper bound on performance but is subject
to the perils of exponential scaling. While we have
not experimented with much larger documents, ap-
proximate methods will likely be valuable in bridg-
ing the performance gap for complex problems. Pre-
liminary experiments with local search methods are
promising in this regard.
</bodyText>
<sectionHeader confidence="0.998407" genericHeader="method">
6 Extensions
</sectionHeader>
<bodyText confidence="0.999974538461538">
Here we describe how our ILP formulation can
be extended with additional constraints to incor-
porate sentence compression. In particular, we
are interested in creating compressed alternatives
for the original sentence by manipulating its parse
tree (Knight and Marcu, 2000). This idea has been
applied with some success to summarization (Turner
and Charniak, 2005; Hovy et al., 2005; Nenkova,
2008) with the goal of removing irrelevant or redun-
dant details, thus freeing space for more relevant in-
formation. One way to achieve this end is to gen-
erate compressed candidates for each sentence, cre-
ating an expanded pool of input sentences, and em-
</bodyText>
<figure confidence="0.560988">
Number of Sentences
</figure>
<figureCaption confidence="0.94305625">
Figure 2: A comparison of ILP run-times (on an AMD
1.8Ghz desktop machine) of McDonald’s sentence-based
formulation and our concept-based formulation with an
increasing number of input sentences.
</figureCaption>
<bodyText confidence="0.995762074074074">
ploy some redundancy removal on the final selec-
tion (Madnani et al., 2007).
We adapt this approach to fit the ILP formulations
so that the optimization procedure decides which
compressed alternatives to pick. Formally, each
compression candidate belongs to a group gk corre-
sponding to its original sentence. We can then craft
a constraint to ensure that at most one sentence can
be selected from group gk, which also includes the
original:
� si G 1, bgk
i∈gk
Assuming that all the compressed candidates are
themselves well-formed, meaningful sentences, we
would expect this approach to generate higher qual-
ity summaries. In general, however, compression
algorithms can generate an exponential number of
candidates. Within McDonald’s framework, this
can increase the number of variables and constraints
tremendously. Thus, we seek a compact representa-
tion for compression in our concept framework.
Specifically, we assume that compression in-
volves some combination of three basic operations
on sentences: extraction, removal, and substitution.
In extraction, a sub-sentence (perhaps the content of
a quotation) may be used independently, and the rest
of the sentence is dropped. In removal, a substring
</bodyText>
<figure confidence="0.995034214285714">
50 100 150 200 250 300 350 400 450 500
Average Time per Problem (seconds)
8
6
4
2
7
5
3
0
1
100 word summaries
250 word summaries
100 word summaries (McDonald)
</figure>
<page confidence="0.993873">
14
</page>
<bodyText confidence="0.999608">
is dropped (a temporal clause, for example) that pre-
serves the grammaticality of the sentence. In sub-
stitution, one substring is replaced by another (US
replaces United States, for example).
Arbitrary combinations of these operations are
too general to be represented efficiently in an ILP.
In particular, we need to compute the length of a
sentence and the concepts it covers for all compres-
sion candidates. Thus, we insist that the operations
can only affect non-overlapping spans of text, and
end up with a tree representation of each sentence:
Nodes correspond to compression operations and
leaves map to the words. Each node holds the length
it contributes to the sentence recursively, as the sum
of the lengths of its children. Similarly, the concepts
covered by a node are the union of the concepts cov-
ered by its children. When a node is activated in the
ILP, we consider that the text attached to it is present
in the summary and update the length constraint and
concept selection accordingly. Figure 3 gives an ex-
ample of this tree representation for a sentence from
the TAC data, showing the derivations of some com-
pressed candidates.
For a given sentence j, let Nj be the set of nodes
in its compression tree, Ej ⊆ Nj be the set of
nodes that can be extracted (used as independent
sentences), Rj ⊆ Nj be the set of nodes that can
be removed, and 5j ⊆ Nj be the set of substitu-
tion group nodes. Let x and y be nodes from Nj; we
create binary variables nx and ny to represent the in-
clusion of x or y in the summary. Let x &gt;- y denote
the fact that x ∈ Nj is a direct parent of y ∈ Nj.
The constraints corresponding to the compression
tree are:
</bodyText>
<equation confidence="0.996854166666667">
E nx ≤ 1 ∀j (3)
x∈Ej
E ny = nx ∀x ∈ 5j ∀j (4)
xry
nx ≥ ny ∀(y &gt;- x ∧ x ∈/ {Rj ∪ 5j}) ∀j (5)
nx ≤ ny ∀(y &gt;- x ∧ x ∈/ {Ej ∪ 5j}) ∀j (6)
</equation>
<bodyText confidence="0.994118611111111">
Eq. (3) enforces that only one sub-sentence is ex-
tracted from the original sentence; eq. (4) enforces
that one child of a substitution group is selected if
and only if the substitution node is selected; eq. (5)
ensures that a child node is selected when its parent
is selected unless the child is removable (or a substi-
tution group); eq. (6) ensures that if a child node is
selected, its parent is also selected unless the child is
an extraction node (that can be used as a root).
Each node is associated with the words and the
concepts it contains directly (which are not con-
tained by a child node) in order to compute the new
length constraints and activate concepts in the ob-
jective function. We set Occix to represent the oc-
currence of concept i in node x as a direct child.
Let lx be the length contributed to node x as direct
children. The resulting ILP for performing sentence
compression jointly with sentence selection is:
</bodyText>
<equation confidence="0.9790703">
EMaximize: wici
i
ESubject to: lxnx ≤ L
j
nxOccix ≤ ci, ∀i, x
E nxOccix ≥ ci ∀i
x
idem constraints (3) to (6)
ci ∈ {0, 1} ∀i
nx ∈ {0, 1} ∀x
</equation>
<bodyText confidence="0.9977848">
While this framework can be used to imple-
ment a wide range of compression techniques, we
choose to derive the compression tree from the
sentence’s parse tree, extracted with the Berkeley
parser (Petrov and Klein, 2007), and use a set of
rules to label parse tree nodes with compression op-
erations. For example, declarative clauses contain-
ing a subject and a verb are labeled with the extract
(E) operation; adverbial clauses and non-mandatory
prepositional clauses are labeled with the remove
(R) operation; Acronyms can be replaced by their
full form by using substitution (S) operations and a
primitive form of co-reference resolution is used to
allow the substitution of noun phrases by their refer-
ent.
</bodyText>
<table confidence="0.964543333333333">
System R-2 Pyr. LQ
No comp. 0.110 0.345 2.479
Comp. 0.111 0.323 2.021
</table>
<tableCaption confidence="0.986284">
Table 3: Scores of the system with and without sentence
compression included in the ILP (TAC’08 Set A data).
</tableCaption>
<bodyText confidence="0.711145">
When implemented in the system presented in
section 4, this approach gives a slight improvement
</bodyText>
<page confidence="0.988988">
15
</page>
<bodyText confidence="0.240962">
foreign currency
</bodyText>
<subsectionHeader confidence="0.520681">
Node Len. Concepts
</subsectionHeader>
<footnote confidence="0.492713">
(1):E 6 {the magazine, magazine quoted, chief Wilm, Wilm Disenberg}
</footnote>
<equation confidence="0.961309">
(2):E 7 {countries are, planning to, to hold, hold the, the euro}
(3):S 0 {}
(3a) 1 {ECB}
(3b) 3 {European Central, Central Bank}
(4):R 2 {as saying}
(5):R 3 {a number, number of}
(6):R 1 {}
(7):R 5 {as part, part of, reserves}
(8):R 2 {foreign currency}
</equation>
<listItem confidence="0.990644">
• Original: A number of Countries
are already planning to hold the
euro as part of their foreign cur-
rency reserves, the magazine quoted
European Central Bank chief Wim
Duisenberg as saying.
• [1,2,5,3a]: A number of countries are
planning to hold the euro, the maga-
zine quoted ECB chief Wim Duisen-
berg.
• [2,5,6,7,8]: A number of countries
are already planning to hold the euro
as part of their foreign currency re-
serves.
• [2,7,8]: Countries are planning to
hold the euro as part of their foreign
currency reserves.
• [2]: Countries are planning to hold
the euro.
</listItem>
<figure confidence="0.998626571428571">
(2):E
(3):S
(4):R
(ECB  |European Central Bank)
as saying
A number of
already
(1):E
the magazine quoted chief Wilm Disenberg
(6):R (7):R
countries are planning to hold the euro
(8):R
as part of their reserves
(5):R
</figure>
<figureCaption confidence="0.962288">
Figure 3: A compression tree for an example sentence. E-nodes (diamonds) can be extracted and used as an indepen-
dent sentences, R-nodes (circles) can be removed, and S-nodes (squares) contain substitution alternatives. The table
shows the word bigram concepts covered by each node and the length it contributes to the summary. Examples of
resulting compression candidates are given on the right side, with the list of nodes activated in their derivations.
</figureCaption>
<bodyText confidence="0.999952470588235">
in ROUGE-2 score (see Table 3), but a reduction in
Pyramid score. An analysis of the resulting sum-
maries showed that the rules used for implementing
sentence compression fail to ensure that all com-
pression candidates are valid sentences, and about
60% of the summaries contain ungrammatical sen-
tences. This is confirmed by the linguistic qual-
ity4 score drop for this system. The poor quality
of the compressed sentences explains the reduction
in Pyramid scores: Human judges tend to not give
credit to ungrammatical sentences because they ob-
scure the SCUs.
We have shown in this section how sentence com-
pression can be implemented in a more scalable way
under the concept-based model, but it remains to be
shown that such a technique can improve summary
quality.
</bodyText>
<sectionHeader confidence="0.999958" genericHeader="method">
7 Related work
</sectionHeader>
<bodyText confidence="0.999848333333333">
In addition to proposing an ILP for the sentence-
level model, McDonald (2007) discusses a kind of
summary-level model: The score of a summary is
</bodyText>
<footnote confidence="0.552883">
4As measured according to the TAC’08 guidelines.
</footnote>
<bodyText confidence="0.999756458333333">
determined by its cosine similarity to the collection
of input documents. Though this idea is only imple-
mented with approximate methods, it is similar in
spirit to our concept-based model since it relies on
weights for individual summary words rather than
sentences.
Using a maximum coverage model for summa-
rization is not new. Filatova (2004) formalizes the
idea, discussing its similarity to the classical NP-
hard problem, but in the end uses a greedy approxi-
mation to generate summaries. More recently, Yih et
al. (2007) employ a similar model and uses a stack
decoder to improve on a greedy search. Globally
optimal summaries are also discussed by Liu (2006)
and Jaoua Kallel (2004) who apply genetic algo-
rithms for finding selections of sentences that maxi-
mize summary-level metrics. Hassel (2006) uses hill
climbing to build summaries that maximize a global
information criterion based on random indexing.
The general idea of concept-level scoring for
summarization is employed in the SumBasic sys-
tem (Nenkova and Vanderwende, 2005), which
chooses sentences greedily according to the sum
of their word values (values are derived from fre-
</bodyText>
<page confidence="0.990153">
16
</page>
<bodyText confidence="0.99971675">
quency). Conroy (2006) describes a bag-of-words
model, with the goal of approximating the distribu-
tion of words from the input documents in the sum-
mary. Others, like (Yih et al., 2007) train a model to
learn the value of each word from a set of features
including frequency and position. Filatova’s model
is most theoretically similar to ours, though the con-
cepts she chooses are “events”.
</bodyText>
<sectionHeader confidence="0.985755" genericHeader="conclusions">
8 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999988176470588">
We have synthesized a number of ideas from
the field of automatic summarization, including
concept-level weighting, a maximum coverage
model to minimize redundancy globally, and sen-
tence compression derived from parse trees. While
an ILP formulation for summarization is not novel,
ours provides reasonably scalable, efficient solutions
for practical problems, including those in recent
TAC and DUC evaluations. We have also shown
how it can be extended to perform sentence com-
pression and sentence selection jointly.
In ROUGE and Pyramid evaluation, our system
significantly outperformed McDonald’s ILP sys-
tem. However, we would note that better design of
sentence-level scoring would likely yield better re-
sults as suggested by the success of greedy sentence-
based methods at the DUC and TAC conferences
(see for instance (Toutanova et al., 2007)). Still, the
performance of our system, on par with the current
state-of-the-art, is encouraging.
There are three principal directions for future
work. First, word bigram concepts are convenient,
but semantically unappealing. We plan to explore
concepts derived from parse trees, where weights
may be a function of frequency as well as hierar-
chical relationships.
Second, our current approach relies entirely on
word frequency, a reasonable proxy for relevance,
but likely inferior to learning weights from train-
ing data. A number of systems have shown im-
provements by learning word values, though prelim-
inary attempts to improve on our frequency heuristic
by learning bigram values have not produced sig-
nificant gains. Better features may be necessary.
However, since the ILP gives optimal solutions so
quickly, we are more interested in discriminative
training where we learn weights for features that
push the resulting summaries in the right direction,
as opposed to the individual concept values.
Third, our rule-based sentence compression is
more of a proof of concept, showing that joint com-
pression and optimal selection is feasible. Better
statistical methods have been developed for produc-
ing high quality compression candidates (McDon-
ald, 2006), that maintain linguistic quality, some re-
cent work even uses ILPs for exact inference (Clarke
and Lapata, 2008). The addition of compressed sen-
tences tends to yield less coherent summaries, mak-
ing sentence ordering more important. We would
like to add constraints on sentence ordering to the
ILP formulation to address this issue.
</bodyText>
<sectionHeader confidence="0.998465" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9991765">
This work is supported by the Defense Advanced
Research Projects Agency (DARPA) GALE project,
under Contract No. HR0011-06-C-0023. Any opin-
ions, findings, conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of DARPA.
</bodyText>
<sectionHeader confidence="0.996939" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998578583333333">
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression: An integer linear pro-
gramming approach. Journal ofArtificial Intelligence
Research, 31:273–381.
John M. Conroy, Judith D. Schlesinger, and Dianne P.
O’Leary. 2006. Topic-focused multi-document sum-
marization using an approximate oracle score. In Pro-
ceedings of COLING/ACL.
Hoa Trang Dang and Karolina Owczarzak. 2008.
Overview of the TAC 2008 Update Summarization
Task. In Proceedings of Text Analysis Conference.
E. Filatova and V. Hatzivassiloglou. 2004. Event-based
extractive summarization. In Proceedings of ACL
Workshop on Summarization, volume 111.
G. Gallo, PL Hammer, and B. Simeone. 1980. Quadratic
knapsack problems. Mathematical Programming
Study, 12:132–149.
D. Gillick, B. Favre, and D. Hakkani-Tur. 2008. The
ICSI Summarization System at TAC 2008. In Pro-
ceedings of the Text Understanding Conference.
Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and Mark
Kantrowitz. 2000. Multi-document summarization by
sentence extraction. Proceedings of the ANLP/NAACL
Workshop on Automatic Summarization, pages 40–48.
</reference>
<page confidence="0.987756">
17
</page>
<reference confidence="0.999931739726027">
Martin Hassel and Jonas Sj¨obergh. 2006. Towards
holistic summarization: Selecting summaries, not sen-
tences. In Proceedings of Language Resources and
Evaluation.
D.S. Hochbaum. 1996. Approximating covering and
packing problems: set cover, vertex cover, indepen-
dent set, and related problems. PWS Publishing Co.
Boston, MA, USA, pages 94–143.
E. Hovy, C.Y. Lin, and L. Zhou. 2005. A BE-based
multi-document summarizer with sentence compres-
sion. In Proceedings of Multilingual Summarization
Evaluation.
Fatma Jaoua Kallel, Maher Jaoua, Lamia Bel-
guith Hadrich, and Abdelmajid Ben Hamadou. 2004.
Summarization at LARIS Laboratory. In Proceedings
of the Document Understanding Conference.
Richard Manning Karp. 1972. Reducibility among com-
binatorial problems. Complexity of Computer Compu-
tations, 43:85–103.
Tibor Kiss and Jan Strunk. 2006. Unsupervised multi-
lingual sentence boundary detection. Computational
Linguistics, 32.
K. Knight and D. Marcu. 2000. Statistics-Based
Summarization-Step One: Sentence Compression. In
Proceedings of the National Conference on Artificial
Intelligence, pages 703–710. Menlo Park, CA; Cam-
bridge, MA; London; AAAI Press; MIT Press; 1999.
Chin-Yew Lin. 2004. ROUGE: A Package for Auto-
matic Evaluation of Summaries. In Proceedings of the
Workshop on Text Summarization Branches Out (WAS
2004), pages 25–26.
D. Liu, Y. Wang, C. Liu, and Z. Wang. 2006. Multiple
Documents Summarization Based on Genetic Algo-
rithm. Lecture Notes in Computer Science, 4223:355.
N. Madnani, D. Zajic, B. Dorr, N.F. Ayan, and J. Lin.
2007. Multiple Alternative Sentence Compressions
for Automatic Text Summarization. In Proceed-
ings of the Document Understanding Conference at
NLT/NAACL.
R. McDonald. 2006. Discriminative sentence compres-
sion with soft syntactic constraints. In Proceedings of
the 11th EACL, pages 297–304.
R. McDonald. 2007. A Study of Global Inference Al-
gorithms in Multi-document Summarization. Lecture
Notes in Computer Science, 4425:557.
Ani Nenkova and Rebecca Passonneau. 2004. Evaluat-
ing content selection in summarization: The pyramid
method. In Proceedings ofHLT-NAACL.
A. Nenkova and L. Vanderwende. 2005. The impact of
frequency on summarization. Technical Report MSR-
TR-2005-101, Microsoft Research, Redmond, Wash-
ington.
A. Nenkova. 2008. Entity-driven rewrite for multidocu-
ment summarization. Proceedings ofIJCNLP.
Slav Petrov and Dan Klein. 2007. Learning and infer-
ence for hierarchically split PCFGs. In AAAI 2007
(Nectar Track).
D. Pisinger, A.B. Rasmussen, and R. Sandvik. 2005.
Solution of large-sized quadratic knapsack problems
through aggressive reduction. INFORMS Journal on
Computing.
K. Toutanova, C. Brockett, M. Gamon, J. Jagarlamudi,
H. Suzuki, and L. Vanderwende. 2007. The Pythy
Summarization System: Microsoft Research at DUC
2007. In Proceedings of the Document Understanding
Conference.
J. Turner and E. Charniak. 2005. Supervised and Unsu-
pervised Learning for Sentence Compression. In Pro-
ceedings ofACL.
W. Yih, J. Goodman, L. Vanderwende, and H. Suzuki.
2007. Multi-document summarization by maximiz-
ing informative content-words. In International Joint
Conference on Artificial Intelligence.
</reference>
<page confidence="0.999291">
18
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.902539">
<title confidence="0.999989">A Scalable Global Model for Summarization</title>
<author confidence="0.99551">Benoit</author>
<affiliation confidence="0.989009">Science Division, University of California Berkeley, Computer Science Institute, Berkeley,</affiliation>
<abstract confidence="0.99625045">We present an Integer Linear Program for exact inference under a maximum coverage model for automatic summarization. We compare our model, which operates at the subsentence or “concept”-level, to a sentencelevel model, previously solved with an ILP. Our model scales more efficiently to larger problems because it does not require a quadratic number of variables to address redundancy in pairs of selected sentences. We also show how to include sentence compression in the ILP formulation, which has the desirable property of performing compression and sentence selection simultaneously. The resulting system performs at least as well as the best systems participating in the recent Text Analysis Conference, as judged by a variety of automatic and manual content-based metrics.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Mirella Lapata</author>
</authors>
<title>Global inference for sentence compression: An integer linear programming approach.</title>
<date>2008</date>
<journal>Journal ofArtificial Intelligence Research,</journal>
<pages>31--273</pages>
<marker>Clarke, Lapata, 2008</marker>
<rawString>James Clarke and Mirella Lapata. 2008. Global inference for sentence compression: An integer linear programming approach. Journal ofArtificial Intelligence Research, 31:273–381.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Conroy</author>
<author>Judith D Schlesinger</author>
<author>Dianne P O’Leary</author>
</authors>
<title>Topic-focused multi-document summarization using an approximate oracle score.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING/ACL.</booktitle>
<marker>Conroy, Schlesinger, O’Leary, 2006</marker>
<rawString>John M. Conroy, Judith D. Schlesinger, and Dianne P. O’Leary. 2006. Topic-focused multi-document summarization using an approximate oracle score. In Proceedings of COLING/ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoa Trang Dang</author>
<author>Karolina Owczarzak</author>
</authors>
<title>Update Summarization Task.</title>
<date>2008</date>
<journal>Overview of the TAC</journal>
<booktitle>In Proceedings of Text Analysis Conference.</booktitle>
<contexts>
<context position="12752" citStr="Dang and Owczarzak, 2008" startWordPosition="2109" endWordPosition="2112">an and automatic content evaluation, the conceptbased model outperforms McDonald’s sentencebased model, which in turn outperforms the baseline. Of course, the relevance and redundancy functions used for McDonald’s formulation in this experiment are rather primitive, and results would likely improve with better relevance features as used in many TAC systems. Nonetheless, our system based on word bigram concepts, similarly primitive, performed at least as well as any in the TAC evaluation, according to two-tailed t-tests comparing ROUGE, Pyramid, and manually evaluated “content responsiveness” (Dang and Owczarzak, 2008) of our system and the highest scoring system in each category. System ROUGE-2 Pyramid Baseline 0.058 0.186 McDonald 0.072 0.295 Concepts 0.110 0.345 Table 2: Scores for both systems and a baseline on TAC 2008 data (Set A) for ROUGE-2 and Pyramid evaluations. 5 Scalability McDonald’s sentence-level formulation corresponds to a quadratic knapsack, and he shows his particular variant is NP-hard by reduction to 3-D matching. The concept-level formulation is similar in spirit to the classical maximum coverage problem: Given a set of items X, a set of subsets 5 of X, and an integer k, the goal is t</context>
</contexts>
<marker>Dang, Owczarzak, 2008</marker>
<rawString>Hoa Trang Dang and Karolina Owczarzak. 2008. Overview of the TAC 2008 Update Summarization Task. In Proceedings of Text Analysis Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Filatova</author>
<author>V Hatzivassiloglou</author>
</authors>
<title>Event-based extractive summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL Workshop on Summarization,</booktitle>
<volume>111</volume>
<marker>Filatova, Hatzivassiloglou, 2004</marker>
<rawString>E. Filatova and V. Hatzivassiloglou. 2004. Event-based extractive summarization. In Proceedings of ACL Workshop on Summarization, volume 111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gallo</author>
<author>PL Hammer</author>
<author>B Simeone</author>
</authors>
<title>Quadratic knapsack problems.</title>
<date>1980</date>
<booktitle>Mathematical Programming Study,</booktitle>
<pages>12--132</pages>
<contexts>
<context position="2317" citStr="Gallo et al., 1980" startWordPosition="348" endWordPosition="351">anderwende, 2005), some recent work focuses on improved search (McDonald, 2007; Yih et al., 2007). Among them, McDonald is the first to consider a non-approximated maximization of an objective function through Integer Linear Programming (ILP), which improves on a greedy search by 4-12%. His formulation assumes that the quality of a summary is proportional to the sum of the relevance scores of the selected sentences, penalized by the sum of the redundancy scores of all pairs of selected sentences. Under a maximum summary length constraint, this problem can be expressed as a quadratic knapsack (Gallo et al., 1980) and many methods are available to solve it (Pisinger et al., 2005). However, McDonald reports that the method is not scalable above 100 input sentences and discusses more practical approximations. Still, an ILP formulation is appealing because it gives exact solutions and lends itself well to extensions through additional constraints. Methods like McDonald’s, including the wellknown Maximal Marginal Relevance (MMR) algorithm (Goldstein et al., 2000), are subject to another problem: Summary-level redundancy is not always well modeled by pairwise sentence-level redundancy. Figure 1 shows an exa</context>
</contexts>
<marker>Gallo, Hammer, Simeone, 1980</marker>
<rawString>G. Gallo, PL Hammer, and B. Simeone. 1980. Quadratic knapsack problems. Mathematical Programming Study, 12:132–149.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gillick</author>
<author>B Favre</author>
<author>D Hakkani-Tur</author>
</authors>
<title>The ICSI Summarization System at TAC</title>
<date>2008</date>
<booktitle>In Proceedings of the Text Understanding Conference.</booktitle>
<contexts>
<context position="9677" citStr="Gillick et al., 2008" startWordPosition="1598" endWordPosition="1601"> summary consists of 4 sentences. In order to facilitate comparison, we generate summaries from both models using a common pipeline: 1. Clean input documents. A simple set of rules removes headers and formatting markup. 2. Split text into sentences. We use the unsupervised Punkt system (Kiss and Strunk, 2006). 3. Prune sentences shorter than 5 words. 4. Compute parameters needed by the models. 5. Map to ILP format and solve. We use an open source solver3. 6. Order sentences picked by the ILP for inclusion in the summary. The specifics of step 4 are described in detail in (McDonald, 2007) and (Gillick et al., 2008). McDonald’s sentence relevance combines word-level cosine similarity with the source document and the inverse of its position (early sentences tend to be more important). Redundancy between a pair of sentences is their cosine similarity. For sentence i in document D, Reli = cosine(i, D) + 1/pos(i, D) Redij = cosine(i, j) In our concept-based model, we use word bigrams, weighted by the number of input documents in which they appear. While word bigrams stretch the notion of a concept a bit thin, they are easily extracted and matched (we use stemming to allow slightly more robust matching). Tabl</context>
</contexts>
<marker>Gillick, Favre, Hakkani-Tur, 2008</marker>
<rawString>D. Gillick, B. Favre, and D. Hakkani-Tur. 2008. The ICSI Summarization System at TAC 2008. In Proceedings of the Text Understanding Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jade Goldstein</author>
<author>Vibhu Mittal</author>
<author>Jaime Carbonell</author>
<author>Mark Kantrowitz</author>
</authors>
<title>Multi-document summarization by sentence extraction.</title>
<date>2000</date>
<booktitle>Proceedings of the ANLP/NAACL Workshop on Automatic Summarization,</booktitle>
<pages>40--48</pages>
<contexts>
<context position="1683" citStr="Goldstein et al., 2000" startWordPosition="245" endWordPosition="248">ystems are typically extractive or abstractive. Since abstraction is quite hard, the most successful systems tested at the Text Analysis Conference (TAC) and Document Understanding Conference (DUC)1, for example, are extractive. In particular, sentence selection represents a reasonable trade-off between linguistic quality, guaranteed by longer textual units, and summary content, often improved with shorter units. Whereas the majority of approaches employ a greedy search to find a set of sentences that is 1TAC is a continuation of DUC, which ran from 2001-2007. both relevant and non-redundant (Goldstein et al., 2000; Nenkova and Vanderwende, 2005), some recent work focuses on improved search (McDonald, 2007; Yih et al., 2007). Among them, McDonald is the first to consider a non-approximated maximization of an objective function through Integer Linear Programming (ILP), which improves on a greedy search by 4-12%. His formulation assumes that the quality of a summary is proportional to the sum of the relevance scores of the selected sentences, penalized by the sum of the redundancy scores of all pairs of selected sentences. Under a maximum summary length constraint, this problem can be expressed as a quadr</context>
</contexts>
<marker>Goldstein, Mittal, Carbonell, Kantrowitz, 2000</marker>
<rawString>Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and Mark Kantrowitz. 2000. Multi-document summarization by sentence extraction. Proceedings of the ANLP/NAACL Workshop on Automatic Summarization, pages 40–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Hassel</author>
<author>Jonas Sj¨obergh</author>
</authors>
<title>Towards holistic summarization: Selecting summaries, not sentences.</title>
<date>2006</date>
<booktitle>In Proceedings of Language Resources and Evaluation.</booktitle>
<marker>Hassel, Sj¨obergh, 2006</marker>
<rawString>Martin Hassel and Jonas Sj¨obergh. 2006. Towards holistic summarization: Selecting summaries, not sentences. In Proceedings of Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D S Hochbaum</author>
</authors>
<title>Approximating covering and packing problems: set cover, vertex cover, independent set, and related problems.</title>
<date>1996</date>
<pages>94--143</pages>
<publisher>PWS Publishing Co.</publisher>
<location>Boston, MA, USA,</location>
<contexts>
<context position="13518" citStr="Hochbaum, 1996" startWordPosition="2245" endWordPosition="2246">able 2: Scores for both systems and a baseline on TAC 2008 data (Set A) for ROUGE-2 and Pyramid evaluations. 5 Scalability McDonald’s sentence-level formulation corresponds to a quadratic knapsack, and he shows his particular variant is NP-hard by reduction to 3-D matching. The concept-level formulation is similar in spirit to the classical maximum coverage problem: Given a set of items X, a set of subsets 5 of X, and an integer k, the goal is to pick at most k subsets from 5 that maximizes the size of their union. Maximum coverage is known to be NP-hard by reduction to the set cover problem (Hochbaum, 1996). Perhaps the simplest way to show that our formulation is NP-hard is by reduction to the knapsack problem (Karp, 1972). Consider the special case where sentences do not share any overlapping concepts. Then, the value of each sentence to the summary is independent of every other sentence. This is a knapsack problem: trying to maximize the value in a container of limited size. Given a solver for our problem, we could solve all knapsack problem instances, so our problem must also be NP-hard. With n input sentences and m concepts, both formulations generate a quadratic number of constraints. Howe</context>
</contexts>
<marker>Hochbaum, 1996</marker>
<rawString>D.S. Hochbaum. 1996. Approximating covering and packing problems: set cover, vertex cover, independent set, and related problems. PWS Publishing Co. Boston, MA, USA, pages 94–143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
<author>C Y Lin</author>
<author>L Zhou</author>
</authors>
<title>A BE-based multi-document summarizer with sentence compression.</title>
<date>2005</date>
<booktitle>In Proceedings of Multilingual Summarization Evaluation.</booktitle>
<contexts>
<context position="16753" citStr="Hovy et al., 2005" startWordPosition="2779" endWordPosition="2782">not experimented with much larger documents, approximate methods will likely be valuable in bridging the performance gap for complex problems. Preliminary experiments with local search methods are promising in this regard. 6 Extensions Here we describe how our ILP formulation can be extended with additional constraints to incorporate sentence compression. In particular, we are interested in creating compressed alternatives for the original sentence by manipulating its parse tree (Knight and Marcu, 2000). This idea has been applied with some success to summarization (Turner and Charniak, 2005; Hovy et al., 2005; Nenkova, 2008) with the goal of removing irrelevant or redundant details, thus freeing space for more relevant information. One way to achieve this end is to generate compressed candidates for each sentence, creating an expanded pool of input sentences, and emNumber of Sentences Figure 2: A comparison of ILP run-times (on an AMD 1.8Ghz desktop machine) of McDonald’s sentence-based formulation and our concept-based formulation with an increasing number of input sentences. ploy some redundancy removal on the final selection (Madnani et al., 2007). We adapt this approach to fit the ILP formulat</context>
</contexts>
<marker>Hovy, Lin, Zhou, 2005</marker>
<rawString>E. Hovy, C.Y. Lin, and L. Zhou. 2005. A BE-based multi-document summarizer with sentence compression. In Proceedings of Multilingual Summarization Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fatma Jaoua Kallel</author>
<author>Maher Jaoua</author>
<author>Lamia Belguith Hadrich</author>
<author>Abdelmajid Ben Hamadou</author>
</authors>
<title>Summarization at LARIS Laboratory.</title>
<date>2004</date>
<booktitle>In Proceedings of the Document Understanding Conference.</booktitle>
<marker>Kallel, Jaoua, Hadrich, Hamadou, 2004</marker>
<rawString>Fatma Jaoua Kallel, Maher Jaoua, Lamia Belguith Hadrich, and Abdelmajid Ben Hamadou. 2004. Summarization at LARIS Laboratory. In Proceedings of the Document Understanding Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Manning Karp</author>
</authors>
<title>Reducibility among combinatorial problems.</title>
<date>1972</date>
<journal>Complexity of Computer Computations,</journal>
<pages>43--85</pages>
<contexts>
<context position="13637" citStr="Karp, 1972" startWordPosition="2266" endWordPosition="2267">McDonald’s sentence-level formulation corresponds to a quadratic knapsack, and he shows his particular variant is NP-hard by reduction to 3-D matching. The concept-level formulation is similar in spirit to the classical maximum coverage problem: Given a set of items X, a set of subsets 5 of X, and an integer k, the goal is to pick at most k subsets from 5 that maximizes the size of their union. Maximum coverage is known to be NP-hard by reduction to the set cover problem (Hochbaum, 1996). Perhaps the simplest way to show that our formulation is NP-hard is by reduction to the knapsack problem (Karp, 1972). Consider the special case where sentences do not share any overlapping concepts. Then, the value of each sentence to the summary is independent of every other sentence. This is a knapsack problem: trying to maximize the value in a container of limited size. Given a solver for our problem, we could solve all knapsack problem instances, so our problem must also be NP-hard. With n input sentences and m concepts, both formulations generate a quadratic number of constraints. However, McDonald’s has O(n2) variables while ours has O(n + m). In practice, scalability is largely determined by the spar</context>
</contexts>
<marker>Karp, 1972</marker>
<rawString>Richard Manning Karp. 1972. Reducibility among combinatorial problems. Complexity of Computer Computations, 43:85–103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tibor Kiss</author>
<author>Jan Strunk</author>
</authors>
<title>Unsupervised multilingual sentence boundary detection.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<contexts>
<context position="9366" citStr="Kiss and Strunk, 2006" startWordPosition="1542" endWordPosition="1545">, each on a given topic. While the 2008 edition of TAC also includes an update task—additional summaries assuming some prior knowledge—we focus only on the standard task. This includes 48 topics, averaging 235 input sentences (ranging from 47 to 652). Since the mean sentence length is around 25 words, a typical summary consists of 4 sentences. In order to facilitate comparison, we generate summaries from both models using a common pipeline: 1. Clean input documents. A simple set of rules removes headers and formatting markup. 2. Split text into sentences. We use the unsupervised Punkt system (Kiss and Strunk, 2006). 3. Prune sentences shorter than 5 words. 4. Compute parameters needed by the models. 5. Map to ILP format and solve. We use an open source solver3. 6. Order sentences picked by the ILP for inclusion in the summary. The specifics of step 4 are described in detail in (McDonald, 2007) and (Gillick et al., 2008). McDonald’s sentence relevance combines word-level cosine similarity with the source document and the inverse of its position (early sentences tend to be more important). Redundancy between a pair of sentences is their cosine similarity. For sentence i in document D, Reli = cosine(i, D) </context>
</contexts>
<marker>Kiss, Strunk, 2006</marker>
<rawString>Tibor Kiss and Jan Strunk. 2006. Unsupervised multilingual sentence boundary detection. Computational Linguistics, 32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>D Marcu</author>
</authors>
<title>Statistics-Based Summarization-Step One: Sentence Compression.</title>
<date>2000</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence,</booktitle>
<pages>703--710</pages>
<publisher>AAAI Press; MIT Press;</publisher>
<location>Menlo Park, CA; Cambridge, MA; London;</location>
<contexts>
<context position="16644" citStr="Knight and Marcu, 2000" startWordPosition="2761" endWordPosition="2764">LP solution marks an upper bound on performance but is subject to the perils of exponential scaling. While we have not experimented with much larger documents, approximate methods will likely be valuable in bridging the performance gap for complex problems. Preliminary experiments with local search methods are promising in this regard. 6 Extensions Here we describe how our ILP formulation can be extended with additional constraints to incorporate sentence compression. In particular, we are interested in creating compressed alternatives for the original sentence by manipulating its parse tree (Knight and Marcu, 2000). This idea has been applied with some success to summarization (Turner and Charniak, 2005; Hovy et al., 2005; Nenkova, 2008) with the goal of removing irrelevant or redundant details, thus freeing space for more relevant information. One way to achieve this end is to generate compressed candidates for each sentence, creating an expanded pool of input sentences, and emNumber of Sentences Figure 2: A comparison of ILP run-times (on an AMD 1.8Ghz desktop machine) of McDonald’s sentence-based formulation and our concept-based formulation with an increasing number of input sentences. ploy some red</context>
</contexts>
<marker>Knight, Marcu, 2000</marker>
<rawString>K. Knight and D. Marcu. 2000. Statistics-Based Summarization-Step One: Sentence Compression. In Proceedings of the National Conference on Artificial Intelligence, pages 703–710. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>ROUGE: A Package for Automatic Evaluation of Summaries.</title>
<date>2004</date>
<booktitle>In Proceedings of the Workshop on Text Summarization Branches Out (WAS</booktitle>
<pages>25--26</pages>
<contexts>
<context position="11699" citStr="Lin, 2004" startWordPosition="1948" endWordPosition="1949">elationship between the document frequency of input bigrams and the fraction of those bigrams that appear in the human generated “gold” set: Let di be document frequency i and pi be the percent of input bigrams with di that are actually in the gold set. Then the correlation ρ(D, P) = 0.95 for DUC 2007 and 0.97 for DUC 2006. Data here averaged over all problems in DUC 2007. The summaries produced by the two systems have been evaluated automatically with ROUGE and manually with the Pyramid metric. In particular, ROUGE-2 is the recall in bigrams with a set of human-written abstractive summaries (Lin, 2004). The Pyramid score arises from a manual alignment of basic facts from the reference summaries, called Summary Content Units (SCUs), in a hypothesis summary (Nenkova and Passonneau, 2004). We used the SCUs provided by the TAC evaluation. Table 2 compares these results, alongside a baseline that uses the first 100 words of the most recent document. All the scores are significantly different, showing that according to both human and automatic content evaluation, the conceptbased model outperforms McDonald’s sentencebased model, which in turn outperforms the baseline. Of course, the relevance and</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Proceedings of the Workshop on Text Summarization Branches Out (WAS 2004), pages 25–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Liu</author>
<author>Y Wang</author>
<author>C Liu</author>
<author>Z Wang</author>
</authors>
<date>2006</date>
<booktitle>Multiple Documents Summarization Based on Genetic Algorithm. Lecture Notes in Computer Science,</booktitle>
<pages>4223--355</pages>
<marker>Liu, Wang, Liu, Wang, 2006</marker>
<rawString>D. Liu, Y. Wang, C. Liu, and Z. Wang. 2006. Multiple Documents Summarization Based on Genetic Algorithm. Lecture Notes in Computer Science, 4223:355.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Madnani</author>
<author>D Zajic</author>
<author>B Dorr</author>
<author>N F Ayan</author>
<author>J Lin</author>
</authors>
<title>Multiple Alternative Sentence Compressions for Automatic Text Summarization.</title>
<date>2007</date>
<booktitle>In Proceedings of the Document Understanding Conference at NLT/NAACL.</booktitle>
<contexts>
<context position="17305" citStr="Madnani et al., 2007" startWordPosition="2868" endWordPosition="2871">uccess to summarization (Turner and Charniak, 2005; Hovy et al., 2005; Nenkova, 2008) with the goal of removing irrelevant or redundant details, thus freeing space for more relevant information. One way to achieve this end is to generate compressed candidates for each sentence, creating an expanded pool of input sentences, and emNumber of Sentences Figure 2: A comparison of ILP run-times (on an AMD 1.8Ghz desktop machine) of McDonald’s sentence-based formulation and our concept-based formulation with an increasing number of input sentences. ploy some redundancy removal on the final selection (Madnani et al., 2007). We adapt this approach to fit the ILP formulations so that the optimization procedure decides which compressed alternatives to pick. Formally, each compression candidate belongs to a group gk corresponding to its original sentence. We can then craft a constraint to ensure that at most one sentence can be selected from group gk, which also includes the original: � si G 1, bgk i∈gk Assuming that all the compressed candidates are themselves well-formed, meaningful sentences, we would expect this approach to generate higher quality summaries. In general, however, compression algorithms can gener</context>
</contexts>
<marker>Madnani, Zajic, Dorr, Ayan, Lin, 2007</marker>
<rawString>N. Madnani, D. Zajic, B. Dorr, N.F. Ayan, and J. Lin. 2007. Multiple Alternative Sentence Compressions for Automatic Text Summarization. In Proceedings of the Document Understanding Conference at NLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
</authors>
<title>Discriminative sentence compression with soft syntactic constraints.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th EACL,</booktitle>
<pages>297--304</pages>
<contexts>
<context position="28672" citStr="McDonald, 2006" startWordPosition="4792" endWordPosition="4794">quency heuristic by learning bigram values have not produced significant gains. Better features may be necessary. However, since the ILP gives optimal solutions so quickly, we are more interested in discriminative training where we learn weights for features that push the resulting summaries in the right direction, as opposed to the individual concept values. Third, our rule-based sentence compression is more of a proof of concept, showing that joint compression and optimal selection is feasible. Better statistical methods have been developed for producing high quality compression candidates (McDonald, 2006), that maintain linguistic quality, some recent work even uses ILPs for exact inference (Clarke and Lapata, 2008). The addition of compressed sentences tends to yield less coherent summaries, making sentence ordering more important. We would like to add constraints on sentence ordering to the ILP formulation to address this issue. Acknowledgments This work is supported by the Defense Advanced Research Projects Agency (DARPA) GALE project, under Contract No. HR0011-06-C-0023. Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not ne</context>
</contexts>
<marker>McDonald, 2006</marker>
<rawString>R. McDonald. 2006. Discriminative sentence compression with soft syntactic constraints. In Proceedings of the 11th EACL, pages 297–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
</authors>
<title>A Study of Global Inference Algorithms in Multi-document Summarization.</title>
<date>2007</date>
<journal>Lecture Notes in Computer Science,</journal>
<pages>4425--557</pages>
<contexts>
<context position="1776" citStr="McDonald, 2007" startWordPosition="261" endWordPosition="262">systems tested at the Text Analysis Conference (TAC) and Document Understanding Conference (DUC)1, for example, are extractive. In particular, sentence selection represents a reasonable trade-off between linguistic quality, guaranteed by longer textual units, and summary content, often improved with shorter units. Whereas the majority of approaches employ a greedy search to find a set of sentences that is 1TAC is a continuation of DUC, which ran from 2001-2007. both relevant and non-redundant (Goldstein et al., 2000; Nenkova and Vanderwende, 2005), some recent work focuses on improved search (McDonald, 2007; Yih et al., 2007). Among them, McDonald is the first to consider a non-approximated maximization of an objective function through Integer Linear Programming (ILP), which improves on a greedy search by 4-12%. His formulation assumes that the quality of a summary is proportional to the sum of the relevance scores of the selected sentences, penalized by the sum of the redundancy scores of all pairs of selected sentences. Under a maximum summary length constraint, this problem can be expressed as a quadratic knapsack (Gallo et al., 1980) and many methods are available to solve it (Pisinger et al</context>
<context position="4753" citStr="McDonald (2007)" startWordPosition="741" endWordPosition="742"> can be mapped to an ILP that can be solved efficiently with standard software. We begin by comparing our model to McDonald’s (section 2) and detail the differences between the resulting ILP formulations (section 3), showing that ours can give competitive results (section 4) and offer better scalability2 (section 5). Next we demonstrate how our ILP formulation can be extended to include efficient parse-tree-based sentence compression (section 6). We review related work (section 7) and conclude with a discussion of potential improvements to the model (section 8). 2 Models The model proposed by McDonald (2007) considers information and redundancy at the sentence level. The score of a summary is defined as the sum of the relevance scores of the sentences it contains minus the sum of the redundancy scores of each pair of these sentences. If si is an indicator for the presence of sentence i in the summary, Reli is its relevance, and Redij is its redundancy with sentence j, then a summary is scored according to: � �Relisi − Redijsisj i ij Generating a summary under this model involves maximizing this objective function, subject to a 2Strictly speaking, exact inference for the models discussed in this p</context>
<context position="9650" citStr="McDonald, 2007" startWordPosition="1595" endWordPosition="1596">d 25 words, a typical summary consists of 4 sentences. In order to facilitate comparison, we generate summaries from both models using a common pipeline: 1. Clean input documents. A simple set of rules removes headers and formatting markup. 2. Split text into sentences. We use the unsupervised Punkt system (Kiss and Strunk, 2006). 3. Prune sentences shorter than 5 words. 4. Compute parameters needed by the models. 5. Map to ILP format and solve. We use an open source solver3. 6. Order sentences picked by the ILP for inclusion in the summary. The specifics of step 4 are described in detail in (McDonald, 2007) and (Gillick et al., 2008). McDonald’s sentence relevance combines word-level cosine similarity with the source document and the inverse of its position (early sentences tend to be more important). Redundancy between a pair of sentences is their cosine similarity. For sentence i in document D, Reli = cosine(i, D) + 1/pos(i, D) Redij = cosine(i, j) In our concept-based model, we use word bigrams, weighted by the number of input documents in which they appear. While word bigrams stretch the notion of a concept a bit thin, they are easily extracted and matched (we use stemming to allow slightly </context>
<context position="24888" citStr="McDonald (2007)" startWordPosition="4201" endWordPosition="4202">nd about 60% of the summaries contain ungrammatical sentences. This is confirmed by the linguistic quality4 score drop for this system. The poor quality of the compressed sentences explains the reduction in Pyramid scores: Human judges tend to not give credit to ungrammatical sentences because they obscure the SCUs. We have shown in this section how sentence compression can be implemented in a more scalable way under the concept-based model, but it remains to be shown that such a technique can improve summary quality. 7 Related work In addition to proposing an ILP for the sentencelevel model, McDonald (2007) discusses a kind of summary-level model: The score of a summary is 4As measured according to the TAC’08 guidelines. determined by its cosine similarity to the collection of input documents. Though this idea is only implemented with approximate methods, it is similar in spirit to our concept-based model since it relies on weights for individual summary words rather than sentences. Using a maximum coverage model for summarization is not new. Filatova (2004) formalizes the idea, discussing its similarity to the classical NPhard problem, but in the end uses a greedy approximation to generate summ</context>
</contexts>
<marker>McDonald, 2007</marker>
<rawString>R. McDonald. 2007. A Study of Global Inference Algorithms in Multi-document Summarization. Lecture Notes in Computer Science, 4425:557.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Rebecca Passonneau</author>
</authors>
<title>Evaluating content selection in summarization: The pyramid method.</title>
<date>2004</date>
<booktitle>In Proceedings ofHLT-NAACL.</booktitle>
<contexts>
<context position="11886" citStr="Nenkova and Passonneau, 2004" startWordPosition="1974" endWordPosition="1977">y i and pi be the percent of input bigrams with di that are actually in the gold set. Then the correlation ρ(D, P) = 0.95 for DUC 2007 and 0.97 for DUC 2006. Data here averaged over all problems in DUC 2007. The summaries produced by the two systems have been evaluated automatically with ROUGE and manually with the Pyramid metric. In particular, ROUGE-2 is the recall in bigrams with a set of human-written abstractive summaries (Lin, 2004). The Pyramid score arises from a manual alignment of basic facts from the reference summaries, called Summary Content Units (SCUs), in a hypothesis summary (Nenkova and Passonneau, 2004). We used the SCUs provided by the TAC evaluation. Table 2 compares these results, alongside a baseline that uses the first 100 words of the most recent document. All the scores are significantly different, showing that according to both human and automatic content evaluation, the conceptbased model outperforms McDonald’s sentencebased model, which in turn outperforms the baseline. Of course, the relevance and redundancy functions used for McDonald’s formulation in this experiment are rather primitive, and results would likely improve with better relevance features as used in many TAC systems.</context>
</contexts>
<marker>Nenkova, Passonneau, 2004</marker>
<rawString>Ani Nenkova and Rebecca Passonneau. 2004. Evaluating content selection in summarization: The pyramid method. In Proceedings ofHLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nenkova</author>
<author>L Vanderwende</author>
</authors>
<title>The impact of frequency on summarization.</title>
<date>2005</date>
<tech>Technical Report MSRTR-2005-101,</tech>
<institution>Microsoft Research,</institution>
<location>Redmond, Washington.</location>
<contexts>
<context position="1715" citStr="Nenkova and Vanderwende, 2005" startWordPosition="249" endWordPosition="252">ractive or abstractive. Since abstraction is quite hard, the most successful systems tested at the Text Analysis Conference (TAC) and Document Understanding Conference (DUC)1, for example, are extractive. In particular, sentence selection represents a reasonable trade-off between linguistic quality, guaranteed by longer textual units, and summary content, often improved with shorter units. Whereas the majority of approaches employ a greedy search to find a set of sentences that is 1TAC is a continuation of DUC, which ran from 2001-2007. both relevant and non-redundant (Goldstein et al., 2000; Nenkova and Vanderwende, 2005), some recent work focuses on improved search (McDonald, 2007; Yih et al., 2007). Among them, McDonald is the first to consider a non-approximated maximization of an objective function through Integer Linear Programming (ILP), which improves on a greedy search by 4-12%. His formulation assumes that the quality of a summary is proportional to the sum of the relevance scores of the selected sentences, penalized by the sum of the redundancy scores of all pairs of selected sentences. Under a maximum summary length constraint, this problem can be expressed as a quadratic knapsack (Gallo et al., 198</context>
<context position="26042" citStr="Nenkova and Vanderwende, 2005" startWordPosition="4383" endWordPosition="4386"> NPhard problem, but in the end uses a greedy approximation to generate summaries. More recently, Yih et al. (2007) employ a similar model and uses a stack decoder to improve on a greedy search. Globally optimal summaries are also discussed by Liu (2006) and Jaoua Kallel (2004) who apply genetic algorithms for finding selections of sentences that maximize summary-level metrics. Hassel (2006) uses hill climbing to build summaries that maximize a global information criterion based on random indexing. The general idea of concept-level scoring for summarization is employed in the SumBasic system (Nenkova and Vanderwende, 2005), which chooses sentences greedily according to the sum of their word values (values are derived from fre16 quency). Conroy (2006) describes a bag-of-words model, with the goal of approximating the distribution of words from the input documents in the summary. Others, like (Yih et al., 2007) train a model to learn the value of each word from a set of features including frequency and position. Filatova’s model is most theoretically similar to ours, though the concepts she chooses are “events”. 8 Conclusion and Future Work We have synthesized a number of ideas from the field of automatic summari</context>
</contexts>
<marker>Nenkova, Vanderwende, 2005</marker>
<rawString>A. Nenkova and L. Vanderwende. 2005. The impact of frequency on summarization. Technical Report MSRTR-2005-101, Microsoft Research, Redmond, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nenkova</author>
</authors>
<title>Entity-driven rewrite for multidocument summarization.</title>
<date>2008</date>
<booktitle>Proceedings ofIJCNLP.</booktitle>
<contexts>
<context position="16769" citStr="Nenkova, 2008" startWordPosition="2783" endWordPosition="2784">th much larger documents, approximate methods will likely be valuable in bridging the performance gap for complex problems. Preliminary experiments with local search methods are promising in this regard. 6 Extensions Here we describe how our ILP formulation can be extended with additional constraints to incorporate sentence compression. In particular, we are interested in creating compressed alternatives for the original sentence by manipulating its parse tree (Knight and Marcu, 2000). This idea has been applied with some success to summarization (Turner and Charniak, 2005; Hovy et al., 2005; Nenkova, 2008) with the goal of removing irrelevant or redundant details, thus freeing space for more relevant information. One way to achieve this end is to generate compressed candidates for each sentence, creating an expanded pool of input sentences, and emNumber of Sentences Figure 2: A comparison of ILP run-times (on an AMD 1.8Ghz desktop machine) of McDonald’s sentence-based formulation and our concept-based formulation with an increasing number of input sentences. ploy some redundancy removal on the final selection (Madnani et al., 2007). We adapt this approach to fit the ILP formulations so that the</context>
</contexts>
<marker>Nenkova, 2008</marker>
<rawString>A. Nenkova. 2008. Entity-driven rewrite for multidocument summarization. Proceedings ofIJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Learning and inference for hierarchically split PCFGs.</title>
<date>2007</date>
<booktitle>In AAAI</booktitle>
<note>(Nectar Track).</note>
<contexts>
<context position="21674" citStr="Petrov and Klein, 2007" startWordPosition="3660" endWordPosition="3663">s in the objective function. We set Occix to represent the occurrence of concept i in node x as a direct child. Let lx be the length contributed to node x as direct children. The resulting ILP for performing sentence compression jointly with sentence selection is: EMaximize: wici i ESubject to: lxnx ≤ L j nxOccix ≤ ci, ∀i, x E nxOccix ≥ ci ∀i x idem constraints (3) to (6) ci ∈ {0, 1} ∀i nx ∈ {0, 1} ∀x While this framework can be used to implement a wide range of compression techniques, we choose to derive the compression tree from the sentence’s parse tree, extracted with the Berkeley parser (Petrov and Klein, 2007), and use a set of rules to label parse tree nodes with compression operations. For example, declarative clauses containing a subject and a verb are labeled with the extract (E) operation; adverbial clauses and non-mandatory prepositional clauses are labeled with the remove (R) operation; Acronyms can be replaced by their full form by using substitution (S) operations and a primitive form of co-reference resolution is used to allow the substitution of noun phrases by their referent. System R-2 Pyr. LQ No comp. 0.110 0.345 2.479 Comp. 0.111 0.323 2.021 Table 3: Scores of the system with and wit</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Learning and inference for hierarchically split PCFGs. In AAAI 2007 (Nectar Track).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Pisinger</author>
<author>A B Rasmussen</author>
<author>R Sandvik</author>
</authors>
<title>Solution of large-sized quadratic knapsack problems through aggressive reduction.</title>
<date>2005</date>
<journal>INFORMS Journal on Computing.</journal>
<contexts>
<context position="2384" citStr="Pisinger et al., 2005" startWordPosition="360" endWordPosition="363">McDonald, 2007; Yih et al., 2007). Among them, McDonald is the first to consider a non-approximated maximization of an objective function through Integer Linear Programming (ILP), which improves on a greedy search by 4-12%. His formulation assumes that the quality of a summary is proportional to the sum of the relevance scores of the selected sentences, penalized by the sum of the redundancy scores of all pairs of selected sentences. Under a maximum summary length constraint, this problem can be expressed as a quadratic knapsack (Gallo et al., 1980) and many methods are available to solve it (Pisinger et al., 2005). However, McDonald reports that the method is not scalable above 100 input sentences and discusses more practical approximations. Still, an ILP formulation is appealing because it gives exact solutions and lends itself well to extensions through additional constraints. Methods like McDonald’s, including the wellknown Maximal Marginal Relevance (MMR) algorithm (Goldstein et al., 2000), are subject to another problem: Summary-level redundancy is not always well modeled by pairwise sentence-level redundancy. Figure 1 shows an example where the combination of sentences (1) and (2) overlaps comple</context>
</contexts>
<marker>Pisinger, Rasmussen, Sandvik, 2005</marker>
<rawString>D. Pisinger, A.B. Rasmussen, and R. Sandvik. 2005. Solution of large-sized quadratic knapsack problems through aggressive reduction. INFORMS Journal on Computing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>C Brockett</author>
<author>M Gamon</author>
<author>J Jagarlamudi</author>
<author>H Suzuki</author>
<author>L Vanderwende</author>
</authors>
<title>The Pythy Summarization System: Microsoft Research at DUC</title>
<date>2007</date>
<booktitle>In Proceedings of the Document Understanding Conference.</booktitle>
<contexts>
<context position="27416" citStr="Toutanova et al., 2007" startWordPosition="4600" endWordPosition="4603"> While an ILP formulation for summarization is not novel, ours provides reasonably scalable, efficient solutions for practical problems, including those in recent TAC and DUC evaluations. We have also shown how it can be extended to perform sentence compression and sentence selection jointly. In ROUGE and Pyramid evaluation, our system significantly outperformed McDonald’s ILP system. However, we would note that better design of sentence-level scoring would likely yield better results as suggested by the success of greedy sentencebased methods at the DUC and TAC conferences (see for instance (Toutanova et al., 2007)). Still, the performance of our system, on par with the current state-of-the-art, is encouraging. There are three principal directions for future work. First, word bigram concepts are convenient, but semantically unappealing. We plan to explore concepts derived from parse trees, where weights may be a function of frequency as well as hierarchical relationships. Second, our current approach relies entirely on word frequency, a reasonable proxy for relevance, but likely inferior to learning weights from training data. A number of systems have shown improvements by learning word values, though p</context>
</contexts>
<marker>Toutanova, Brockett, Gamon, Jagarlamudi, Suzuki, Vanderwende, 2007</marker>
<rawString>K. Toutanova, C. Brockett, M. Gamon, J. Jagarlamudi, H. Suzuki, and L. Vanderwende. 2007. The Pythy Summarization System: Microsoft Research at DUC 2007. In Proceedings of the Document Understanding Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Turner</author>
<author>E Charniak</author>
</authors>
<title>Supervised and Unsupervised Learning for Sentence Compression.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="16734" citStr="Turner and Charniak, 2005" startWordPosition="2775" endWordPosition="2778">ial scaling. While we have not experimented with much larger documents, approximate methods will likely be valuable in bridging the performance gap for complex problems. Preliminary experiments with local search methods are promising in this regard. 6 Extensions Here we describe how our ILP formulation can be extended with additional constraints to incorporate sentence compression. In particular, we are interested in creating compressed alternatives for the original sentence by manipulating its parse tree (Knight and Marcu, 2000). This idea has been applied with some success to summarization (Turner and Charniak, 2005; Hovy et al., 2005; Nenkova, 2008) with the goal of removing irrelevant or redundant details, thus freeing space for more relevant information. One way to achieve this end is to generate compressed candidates for each sentence, creating an expanded pool of input sentences, and emNumber of Sentences Figure 2: A comparison of ILP run-times (on an AMD 1.8Ghz desktop machine) of McDonald’s sentence-based formulation and our concept-based formulation with an increasing number of input sentences. ploy some redundancy removal on the final selection (Madnani et al., 2007). We adapt this approach to f</context>
</contexts>
<marker>Turner, Charniak, 2005</marker>
<rawString>J. Turner and E. Charniak. 2005. Supervised and Unsupervised Learning for Sentence Compression. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Yih</author>
<author>J Goodman</author>
<author>L Vanderwende</author>
<author>H Suzuki</author>
</authors>
<title>Multi-document summarization by maximizing informative content-words.</title>
<date>2007</date>
<booktitle>In International Joint Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="1795" citStr="Yih et al., 2007" startWordPosition="263" endWordPosition="266">t the Text Analysis Conference (TAC) and Document Understanding Conference (DUC)1, for example, are extractive. In particular, sentence selection represents a reasonable trade-off between linguistic quality, guaranteed by longer textual units, and summary content, often improved with shorter units. Whereas the majority of approaches employ a greedy search to find a set of sentences that is 1TAC is a continuation of DUC, which ran from 2001-2007. both relevant and non-redundant (Goldstein et al., 2000; Nenkova and Vanderwende, 2005), some recent work focuses on improved search (McDonald, 2007; Yih et al., 2007). Among them, McDonald is the first to consider a non-approximated maximization of an objective function through Integer Linear Programming (ILP), which improves on a greedy search by 4-12%. His formulation assumes that the quality of a summary is proportional to the sum of the relevance scores of the selected sentences, penalized by the sum of the redundancy scores of all pairs of selected sentences. Under a maximum summary length constraint, this problem can be expressed as a quadratic knapsack (Gallo et al., 1980) and many methods are available to solve it (Pisinger et al., 2005). However, </context>
<context position="25527" citStr="Yih et al. (2007)" startWordPosition="4303" endWordPosition="4306">ummary-level model: The score of a summary is 4As measured according to the TAC’08 guidelines. determined by its cosine similarity to the collection of input documents. Though this idea is only implemented with approximate methods, it is similar in spirit to our concept-based model since it relies on weights for individual summary words rather than sentences. Using a maximum coverage model for summarization is not new. Filatova (2004) formalizes the idea, discussing its similarity to the classical NPhard problem, but in the end uses a greedy approximation to generate summaries. More recently, Yih et al. (2007) employ a similar model and uses a stack decoder to improve on a greedy search. Globally optimal summaries are also discussed by Liu (2006) and Jaoua Kallel (2004) who apply genetic algorithms for finding selections of sentences that maximize summary-level metrics. Hassel (2006) uses hill climbing to build summaries that maximize a global information criterion based on random indexing. The general idea of concept-level scoring for summarization is employed in the SumBasic system (Nenkova and Vanderwende, 2005), which chooses sentences greedily according to the sum of their word values (values </context>
</contexts>
<marker>Yih, Goodman, Vanderwende, Suzuki, 2007</marker>
<rawString>W. Yih, J. Goodman, L. Vanderwende, and H. Suzuki. 2007. Multi-document summarization by maximizing informative content-words. In International Joint Conference on Artificial Intelligence.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>