<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007416">
<title confidence="0.995665">
Variable Bit Quantisation for LSH
</title>
<author confidence="0.998682">
Sean Moran
</author>
<affiliation confidence="0.996692">
School of Informatics
The University of Edinburgh
</affiliation>
<address confidence="0.965611">
EH8 9AB, Edinburgh, UK
</address>
<email confidence="0.999066">
sean.moran@ed.ac.uk
</email>
<author confidence="0.996346">
Victor Lavrenko
</author>
<affiliation confidence="0.995875">
School of Informatics
The University of Edinburgh
</affiliation>
<address confidence="0.964968">
EH8 9AB, Edinburgh, UK
</address>
<email confidence="0.998991">
vlavrenk@inf.ed.ac.uk
</email>
<author confidence="0.968624">
Miles Osborne
</author>
<affiliation confidence="0.984123">
School of Informatics
The University of Edinburgh
</affiliation>
<address confidence="0.956134">
EH8 9AB, Edinburgh, UK
</address>
<email confidence="0.999376">
miles@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.993908" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999953857142857">
We introduce a scheme for optimally al-
locating a variable number of bits per
LSH hyperplane. Previous approaches as-
sign a constant number of bits per hyper-
plane. This neglects the fact that a subset
of hyperplanes may be more informative
than others. Our method, dubbed Variable
Bit Quantisation (VBQ), provides a data-
driven non-uniform bit allocation across
hyperplanes. Despite only using a fraction
of the available hyperplanes, VBQ outper-
forms uniform quantisation by up to 168%
for retrieval across standard text and image
datasets.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999983901960785">
The task of retrieving the nearest neighbours to a
given query document permeates the field of Nat-
ural Language Processing (NLP). Nearest neigh-
bour search has been used for applications as di-
verse as automatically detecting document transla-
tion pairs for the purposes of training a statistical
machine translation system (SMT) (Krstovski and
Smith, 2011), the large-scale generation of noun
similarity lists (Ravichandran et al., 2005) to an
unsupervised method for extracting domain spe-
cific lexical variants (Stephan Gouws and Metzle,
2011).
There are two broad approaches to nearest
neighbour based search: exact and approximate
techniques, which are differentiated by their abil-
ity to return completely correct nearest neighbours
(the exact approach) or have some possibility of
returning points that are not true nearest neigh-
bours (the approximate approach). Approximate
nearest neighbour (ANN) search using hashing
techniques has recently gained prominence within
NLP. The hashing-based approach maps the data
into a substantially more compact representation
referred to as a fingerprint, that is more efficient
for performing similarity computations. The re-
sulting compact binary representation radically re-
duces memory requirements while also permitting
fast sub-linear time retrieval of approximate near-
est neighbours.
Hashing-based ANN techniques generally com-
prise two main steps: a projection stage followed
by a quantisation stage. The projection stage
performs a neighbourhood preserving embedding,
mapping the input data into a lower-dimensional
representation. The quantisation stage subse-
quently reduces the cardinality of this represen-
tation by converting the real-valued projections
to binary. Quantisation is a lossy transformation
which can have a significant impact on the result-
ing quality of the binary encoding.
Previous work has quantised each projected di-
mension into a uniform number of bits (Indyk and
Motwani, 1998) (Kong and Li, 2012) (Kong et al.,
2012) (Moran et al., 2013). We demonstrate that
uniform allocation of bits is sub-optimal and pro-
pose a data-driven scheme for variable bit alloca-
tion. Our approach is distinct from previous work
in that it provides a general objective function for
bit allocation. VBQ makes no assumptions on the
data and, in addition to LSH, it applies to a broad
range of other projection functions.
</bodyText>
<sectionHeader confidence="0.999766" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999800181818182">
Locality sensitive hashing (LSH) (Indyk and Mot-
wani, 1998) is an example of an approximate
nearest neighbour search technique that has been
widely used within the field of NLP to preserve the
Cosine distances between documents (Charikar,
2002). LSH for cosine distance draws a large
number of random hyperplanes within the input
feature space, effectively dividing the space into
non-overlapping regions (or buckets). Each hy-
perplane contributes one bit to the encoding, the
value (0 or 1) of which is determined by comput-
</bodyText>
<page confidence="0.986981">
753
</page>
<note confidence="0.46505">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 753–758,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<figure confidence="0.965665">
2 bits assigned: 3 thresholds perfectly preserve neighbourhood structure
Y
n,
h,
It
10
h,
n2
00 01
X
Fl score: 1.00
00 01 10
11
Fl score: 0.25
[a] [b] 0 bits assigned: 0 thresholds do nearly as well as one or more thresholds
</figure>
<figureCaption confidence="0.99961">
Figure 1: Left: Data points with identical shapes are 1-NN. Two hyperplanes h1, h2 are shown alongside
</figureCaption>
<bodyText confidence="0.970763634615385">
their associated normal vectors (n1, n2). Right top: Projection of points onto the normal vectors n1
and n2 of the hyperplanes (arrows denote projections). Right middle: Positioning of the points along
normal vector n2. Three quantisation thresholds (t1, t2, t3, and consequently 2 bits) can maintain the
neighbourhood structure. Right bottom: the high degree of mixing between the 1-NN means that this
hyperplane (h1) is likely to have 0 bits assigned (and therefore be discarded entirely).
ing the dot product of a data-point (x) with the
normal vector to the hyperplane (ni): that is, if
x.ni &lt; 0, i ∈ {1 ... k}, then the i-th bit is set
to 0, and 1 otherwise. This encoding scheme is
known as single bit quantisation (SBQ). More re-
cent hashing work has sought to inject a degree
of data-dependency into the positioning of the hy-
perplanes, for example, by using the principal di-
rections of the data (Wang et al., 2012) (Weiss
et al., 2008) or by training a stack of restricted
Boltzmann machines (Salakhutdinov and Hinton,
2009).
Existing quantisation schemes for LSH allocate
either one bit per hyperplane (Indyk and Motwani,
1998) or multiple bits per hyperplane (Kong et al.,
2012) (Kong and Li, 2012) (Moran et al., 2013).
For example, (Kong et al., 2012) recently pro-
posed the Manhattan Hashing (MQ) quantisation
technique where each projected dimension is en-
coded with multiple bits of natural binary code
(NBC). The Manhattan distance between the NBC
encoded data points is then used for nearest neigh-
bour search. The authors demonstrated that MQ
could better preserve the neighbourhood structure
between the data points as compared to SBQ with
Hamming distance.
Other recent quantisation work has focused on
the setting of the quantisation thresholds: for ex-
ample (Kong and Li, 2012) suggested encoding
each dimension into two bits and using an adaptive
thresholding scheme to set the threshold positions.
Their technique dubbed, Double Bit Quantisation
(DBQ), attempts to avoid placing thresholds be-
tween data points with similar projected values. In
other work (Moran et al., 2013) demonstrated that
retrieval accuracy could be enhanced by using a
topological quantisation matrix to guide the quan-
tisation threshold placement along the projected
dimensions. This topological quantisation matrix
specified pairs of c-nearest neighbours in the orig-
inal feature space. Their approach, Neighbour-
hood Preserving Quantisation (NPQ), was shown
to achieve significant increases in retrieval accu-
racy over SBQ, MQ and DBQ for the task of image
retrieval. In all of these cases the bit allocation is
uniform: each hyperplane is assigned an identical
number of bits.
</bodyText>
<sectionHeader confidence="0.971737" genericHeader="method">
3 Variable Bit Quantisation
</sectionHeader>
<bodyText confidence="0.995466">
Our proposed quantisation scheme, Variable Bit
Quantisation (VBQ), assigns a variable number of
bits to each hyperplane subject to a maximum up-
per limit on the total number of bits1. To do so,
VBQ computes an F-measure based directly on the
positioning of the quantisation thresholds along a
projected dimension. The higher the F-measure
for a given hyperplane, the better that hyperplane
is at preserving the neighbourhood structure be-
tween the data points, and the more bits the hyper-
plane should be afforded from the bit budget B.
Figure 1(a) illustrates the original 2-
dimensional feature space for a toy example.
</bodyText>
<footnote confidence="0.985197">
1Referred to as the bit budget B, typically 32 or 64 bits.
</footnote>
<page confidence="0.997802">
754
</page>
<bodyText confidence="0.999761571428571">
The space is divided into 4 buckets by two
random LSH hyperplanes. The circles, diamonds,
squares and stars denote 1-nearest neighbours
(1-NN). Quantisation for LSH is performed by
projecting the data points onto the normal vectors
(n1, n2) to the hyperplanes (h1, h2). This leads
to two projected dimensions. Thresholding these
projected dimensions at zero, and determining
which side of zero a given data-point falls, yields
the bit encoding for a given data-point.
Figure 1(b) demonstrates our proposed quanti-
sation scheme. Similar to vanilla LSH, the data-
points are projected onto the normal vectors, to
yield two projected dimensions. This is illustrated
on the topmost diagram in Figure 1(b). VBQ dif-
fers in how these projected dimensions are thresh-
olded to yield the bit encoding: rather than one
threshold situated at zero, VBQ employs one or
more thresholds and positions these thresholds in
an adaptive manner based upon maximisation of
an F-measure. Using multiple thresholds enables
more than one bit to be assigned per hyperplane2.
Figure 1(b) (middle, bottom) depicts the F-
measure driven threshold optimisation along the
projected dimensions. We define as a positive
pair, those pairs of data points in the original fea-
ture space that are c-nearest neighbours (c-NN),
and a negative pair otherwise. In our toy exam-
ple, data points with the same shape symbol form
a positive pair, while points with different sym-
bols are negative pairs. Intuitively, the thresholds
should be positioned in such a way as to maxi-
mize the number of positive pairs that fall within
the same thresholded region, while also ensuring
the negative pairs fall into different regions.
This intuition can be captured by an F-measure
which counts the number of positive pairs that are
found within the same thresholded regions (true
positives, TP), the number of negative pairs found
within the same regions (false positives, FP), and
the number of positive pairs found in different re-
gions of the threshold partitioned dimension (false
negatives, FN). For n2, three thresholds are opti-
mal, given they perfectly preserve the neighbour-
hood structure. For n1, no thresholds can provide a
neighbourhood preserving quantisation and there-
fore it is better to discard the hyperplane h1. VBQ
uses random restarts to optimise the F-measure3.
The computed F-measure scores per hyper-
</bodyText>
<footnote confidence="0.993362">
2b bits, requires 26 − 1 thresholds.
3More details on the computation of the F-measure per
hyperplane can be found in (Moran et al., 2013).
</footnote>
<bodyText confidence="0.999813642857143">
plane (h), per bit count (b) are an effective sig-
nal for bit allocation: more informative hyper-
planes tend to have higher F-measure, for higher
bit counts. VBQ applies a binary integer linear
program (BILP) on top of the F-measure scores
to obtain the bit allocation. To do so, the algo-
rithm collates the scores in a matrix F with ele-
ments Fb,h, where b E {0, ... , k} 4 indexes the
rows, with k being the maximum number of bits
allowable for any given hyperplane (set to 4 in this
work), and h E {1... , B} indexes the columns.
The BILP uses F to find the bit allocation that
maximises the cumulative F-measure across the B
hyperplanes (Equation 1).
</bodyText>
<equation confidence="0.920281">
max IIF o ZII
subject to IIZhII = 1 h E {1... B}
IIZoDII &lt; B
Z is binary
</equation>
<bodyText confidence="0.999894538461539">
II.II denotes the Frobenius L1 norm, o the
Hadamard product and D is a constraint matrix,
with Db,h = b, ensuring that the bit allocation
remains within the bit budget B. The BILP is
solved using the standard branch and bound op-
timization algorithm (Land and Doig, 1960). The
output from the BILP is an indicator matrix Z E
{0,1}(k+1)xB whose columns specify the optimal
bit allocation for a given hyperplane i.e. Zb,h = 1
if the BILP decided to allocate b bits for hyper-
plane h, and zero otherwise. Example matrices for
the toy problem in Figure 1 are given hereunder (in
this example, k = 2 and B = 2).
</bodyText>
<table confidence="0.9957925">
F h1 h2 D Z
b0 0.25 0.25 ⎞ ⎛ 0 0 ⎞ ⎛ 1 0
⎛ ⎞
b1 0.35 0.50 ⎠ ⎝ 1 1 ⎠ ⎝ 0 0
1 ⎠
⎝b2 0.40 1.00 2 2 0
</table>
<bodyText confidence="0.9996196">
Notice how the indicator matrix Z specifies an
assignment of 0 bits for hyperplane h1 and 2 bits
for hyperplane h2 as this yields the highest cu-
mulative F-measure across hyperplanes while also
meeting the bit budget. VBQ is therefore a princi-
pled method to select a discriminative subset of
hyperplanes, and simultaneously allocate bits to
the remaining hyperplanes, given a fixed overall
bit budget B, while maximizing cumulative F-
measure.
</bodyText>
<footnote confidence="0.9769945">
4For 0 bits, we compute the F-measure without any
thresholds along the projected dimension.
</footnote>
<equation confidence="0.670643">
(1)
</equation>
<page confidence="0.95505">
755
</page>
<table confidence="0.999870142857143">
Dataset CIFAR-10 TDT-2 Reuters-21578
SBQ MQ DBQ NPQ VBQ SBQ MQ DBQ VBQ SBQ MQ DBQ VBQ
SIKH 0.042 0.063 0.047 0.090 0.161 0.034 0.045 0.031 0.092 0.102 0.112 0.087 0.389
LSH 0.119 0.093 0.066 0.153 0.207 0.189 0.097 0.089 0.229 0.276 0.201 0.175 0.538
BLSI 0.038 0.135 0.111 0.155 0.231 0.283 0.210 0.087 0.396 0.100 0.030 0.030 0.156
SH 0.051 0.135 0.111 0.167 0.202 0.146 0.212 0.167 0.370 0.033 0.028 0.030 0.154
PCAH 0.036 0.137 0.107 0.153 0.219 0.281 0.208 0.094 0.374 0.095 0.034 0.027 0.154
</table>
<tableCaption confidence="0.991496">
Table 1: Area under the Precision Recall curve (AUPRC) for all five projection methods. Results are for
32 bits (images) and at 128 bits (text). The best overall score for each dataset is shown in bold face.
</tableCaption>
<sectionHeader confidence="0.997593" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.920242">
4.1 Datasets
</subsectionHeader>
<bodyText confidence="0.999975142857143">
Our text datasets are Reuters-21578 and TDT-2.
The original Reuters-21578 corpus contains 21578
documents in 135 categories. We use the ModApte
version and discard those documents with multi-
ple category labels. This leaves 8,293 documents
in 65 categories. The corpus contains 18,933 dis-
tinct terms. The TDT-2 corpus consists of 11,201
on-topic documents which are classified into 96
semantic categories. We remove those documents
appearing in two or more categories and keep only
the largest 30 categories. This leaves 9,394 docu-
ments in total with 36,771 distinct terms. Both text
datasets are TF-IDF and L2 norm weighted. To
demonstrate the generality of VBQ we also evalu-
ate on the CIFAR-10 image dataset (Krizhevsky,
2009), which consists of 60,000 images repre-
sented as 512 dimensional Gist descriptors (Oliva
and Torralba, 2001). All of the datasets are identi-
cal to those that have been used in previous ANN
hashing work (Zhang et al., 2010) (Kong and Li,
2012) and are publicly available on the Internet.
</bodyText>
<subsectionHeader confidence="0.872931">
4.2 Projection Methods
</subsectionHeader>
<bodyText confidence="0.998573625">
VBQ is independent of the projection stage and
therefore can be used the quantise the projections
from a wide range of different projection func-
tions, including LSH. In our evaluation we take
a sample of the more popular data-independent
(LSH, SIKH) and data-dependent (SH, PCAH,
BLSI) projection functions used in recent hashing
work:
</bodyText>
<listItem confidence="0.857881708333333">
• SIKH: Shift-Invariant Kernel Hashing
(SIKH) uses random projections that approx-
imate shift invariant kernels (Raginsky and
Lazebnik, 2009). We follow previous work
and use a Gaussian kernel with a bandwidth
set to the average distance to the 50th nearest
neighbour (Kong et al., 2012) (Raginsky and
Lazebnik, 2009).
• LSH: Locality Sensitive Hashing uses a
Gaussian random matrix for projection (In-
dyk and Motwani, 1998) (Charikar, 2002).
• BLSI: Binarised Latent Semantic Indexing
(BLSI) forms projections through Singular
Value Decomposition (SVD) (Salakhutdinov
and Hinton, 2009).
• SH: Spectral Hashing (SH) uses the eigen-
functions computed along the principal com-
ponent directions of the data for projec-
tion (Weiss et al., 2008).
• PCAH: Principal Component Analysis
Hashing (PCAH) employs the eigenvectors
corresponding the the largest eigenvalues of
the covariance matrix for projection (Wang
et al., 2012).
</listItem>
<subsectionHeader confidence="0.998532">
4.3 Baselines
</subsectionHeader>
<bodyText confidence="0.999150818181818">
Single Bit Quantisation (SBQ) (Indyk and Mot-
wani, 1998), Manhattan Hashing (MQ) (Kong et
al., 2012), Double Bit Quantisation (DBQ) (Kong
and Li, 2012) and Neighbourhood Preserving
Quantisation (NPQ) (Moran et al., 2013). MQ,
DBQ and NPQ all assign 2 bits per hyperplane,
while SBQ assigns 1 bit per hyperplane. All meth-
ods, including VBQ, are constrained to be within
the allocated bit budget B. If a method assigns
more bits to one hyperplane, then it either dis-
cards, or assigns less bits to other hyperplanes.
</bodyText>
<subsectionHeader confidence="0.990464">
4.4 Evaluation Protocol
</subsectionHeader>
<bodyText confidence="0.9999118">
We adopt the standard Hamming ranking evalua-
tion paradigm (Kong et al., 2012). We randomly
select 1000 query data points per run. Our re-
sults are averaged over 10 runs, and the average
reported. The ǫ-neighbours of each query point
</bodyText>
<page confidence="0.993749">
756
</page>
<figure confidence="0.99985911235955">
[2]
0 0.1 0.2 0.3 0.4 0.5 0.6 0.� 0.8 0.9
Re.W1
0.1
0.05
0
8 16 24 32 40 48 56 64
[1] Number of Bits
1
SBQ MH VBQ
0.9
0.8
0.]
Precision
0.6
0.5
0.4
0.3
0.2
0.1
0
SBQ MQ VBQ
AUPRC
0.2
0.15
0.35
0.3
0.25
SBQ MQ VBQ
0.05
0
0.5
0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.2
0.1
0
SBQ MQ VBQ
0.7
0.6
0.5
0.4
0.3
AUPRC
AUPRC
0 0.1 0.2 0.3 0.4 0.5 0.6 0.] 0.8 0.9 1
Ren11
0 0.1 0.2 0.3 0.4 0.5 0.6 0.] 0.8 0.9 1
Ren11
[4]
[6]
32 �� �� �� 126 256
Number of Bits
1
SBQ MH VBQ
0.9
0.8
0.]
0.6
0.5
0.4
0.3
0.2
0.1
0
32 48 64 96 128 256
Number of Bits
1
SBQ MH VBQ
0.8
0.]
0.6
0.5
0.4
0.3
0.2
0.1
0
[3]
[5]
Precision
Precision
0.9
</figure>
<figureCaption confidence="0.940732666666667">
Figure 2: [1] LSH AUPRC vs bits for CIFAR-10 [2] LSH Precision-Recall curve for CIFAR-10 [3]
LSH AUPRC vs bits for TDT-2 [4] LSH Precision-Recall curve for TDT-2 [5] LSH AUPRC vs bits for
Reuters-21578 [6] LSH Precision-Recall curve for Reuters-21578
</figureCaption>
<bodyText confidence="0.9989210625">
form the ground truth for evaluation. The thresh-
old E is computed by sampling 100 training data-
points at random from the training dataset and de-
termining the distance at which these points have
50 nearest neighbours on average. Positive pairs
and negative pairs for F-measure computation are
computed by thresholding the training dataset
Euclidean distance matrix by E. We adopt the
Manhattan distance and multi-bit binary encoding
method as suggested in (Kong et al., 2012). The
F-measure we use for threshold optimisation is:
FQ = (1+02)TP/((1+02)TP +02FN +FP).
We select the parameter 0 on a held-out valida-
tion dataset. The area under the precision-recall
curve (AUPRC) is used to evaluate the quality of
retrieval.
</bodyText>
<sectionHeader confidence="0.707163" genericHeader="evaluation">
4.5 Results
</sectionHeader>
<bodyText confidence="0.9998556">
Table 1 presents our results. For LSH on text
(Reuters-21578) at 128 bits we find a substantial
95% gain in retrieval performance over uniformly
assigning 1 bit per hyperplane (SBQ) and a 168%
gain over uniformly assigning 2 bits per hyper-
plane (MQ). VBQ gain over SBQ at 128 bits is sta-
tistically significant based upon a paired Wilcoxon
signed rank test across 10 random train/test parti-
tions (p-value: &lt; 0.0054). This pattern is repeated
on TDT-2 (for 128 bits, SBQ vs VBQ: p-value
&lt; 0.0054) and CIFAR-10 (for 32 bits, SBQ vs
VBQ: p-value: &lt; 0.0054). VBQ also reaps sub-
stantial gains for the Eigendecomposition based
projections (PCAH, SH, BLSI) effectively exploit-
ing the imbalanced variance across hyperplanes -
that is, those hyperplanes capturing higher propor-
tions of the variance in the data are allocated more
bits from the fixed bit budget. Figure 2 (top row)
illustrates that VBQ is effective across a range of
bit budgets. Figure 2 (bottom row) presents the
precision-recall (PR) curves at 32 bits (CIFAR-10)
and 128 bits (TDT-2, Reuters-21578). We confirm
our hypothesis that judicious allocation of variable
bits is significantly more effective than uniform al-
location.
</bodyText>
<sectionHeader confidence="0.999567" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999992166666667">
Our proposed quantisation scheme computes a
non-uniform bit assignment across LSH hyper-
planes. The novelty of our approach is centred
upon a binary integer linear program driven by a
novel F-measure based objective function that de-
termines the most appropriate bit allocation: hy-
perplanes that better preserve the neighbourhood
structure of the input data points are awarded more
bits from a fixed bit budget. Our evaluation on
standard datasets demonstrated that VBQ can sub-
stantially enhance the retrieval accuracy of a se-
lection of popular hashing techniques across two
distinct modalities (text and images). In this paper
we concentrated on the hamming ranking based
scenario for hashing. In the future, we would like
to examine the performance of VBQ in the lookup
based hashing scenario where hash tables are used
for fast retrieval.
</bodyText>
<sectionHeader confidence="0.996442" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999927137931035">
Moses Charikar. 2002. Similarity estimation tech-
niques from rounding algorithms. In STOC, pages
380–388.
Piotr Indyk and Rajeev Motwani. 1998. Approximate
nearest neighbors: towards removing the curse of di-
mensionality. In Proceedings of the thirtieth annual
ACM symposium on Theory of computing, STOC
’98, pages 604–613, New York, NY, USA. ACM.
Weihao Kong and Wu-Jun Li. 2012. Double-bit quan-
tization for hashing. In AAAI.
Weihao Kong, Wu-Jun Li, and Minyi Guo. 2012. Man-
hattan hashing for large-scale image retrieval. SI-
GIR ’12, pages 45–54.
Alex Krizhevsky. 2009. Learning Multiple Layers of
Features from Tiny Images. Master’s thesis.
Kriste Krstovski and David A. Smith. 2011. A Mini-
mally Supervised Approach for Detecting and Rank-
ing Document Translation Pairs. In Proceedings of
the Sixth Workshop on Statistical Machine Transla-
tion, Edinburgh, Scotland. Association for Compu-
tational Linguistics.
A. H. Land and A. G. Doig. 1960. An automatic
method of solving discrete programming problems.
Econometrica, 28:pp. 497–520.
Sean Moran, Victor Lavrenko, and Miles Osborne.
2013. Neighbourhood preserving quantisation for
lsh. In 36th Annual International ACM Conference
on Research and Development in Information Re-
trieval (SIGIR), Dublin, Ireland, 07/2013.
Aude Oliva and Antonio Torralba. 2001. Modeling the
shape of the scene: A holistic representation of the
spatial envelope. International Journal of Computer
Vision, 42(3):145–175.
Maxim Raginsky and Svetlana Lazebnik. 2009.
Locality-sensitive binary codes from shift-invariant
kernels. In NIPS ’09, pages 1509–1517.
Deepak Ravichandran, Patrick Pantel, and Eduard
Hovy. 2005. Randomized algorithms and nlp: using
locality sensitive hash function for high speed noun
clustering. ACL ’05, pages 622–629. Association
for Computational Linguistics.
Ruslan Salakhutdinov and Geoffrey Hinton. 2009.
Semantic hashing. Int. J. Approx. Reasoning,
50(7):969–978.
Dirk Hovy Stephan Gouws and Donald Metzle. 2011.
Unsupervised mining of lexical variants from noisy
text. In Proceedings of the First workshop on Unsu-
pervised Learning in NLP, EMNLP ’11, page 8290.
Association for Computational Linguistics.
Jun Wang, S. Kumar, and Shih-Fu Chang. 2012. Semi-
supervised hashing for large-scale search. IEEE
Transactions on Pattern Analysis and Machine In-
telligence, 34(12):2393–2406.
Yair Weiss, Antonio B. Torralba, and Robert Fergus.
2008. Spectral hashing. In NIPS, pages 1753–1760.
Dell Zhang, Jun Wang, Deng Cai, and Jinsong Lu.
2010. Self-taught hashing for fast similarity search.
In SIGIR, pages 18–25.
</reference>
<page confidence="0.99713">
758
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.646586">
<title confidence="0.99987">Variable Bit Quantisation for LSH</title>
<author confidence="0.999376">Sean</author>
<affiliation confidence="0.9960615">School of The University of</affiliation>
<address confidence="0.961517">EH8 9AB, Edinburgh,</address>
<email confidence="0.999235">sean.moran@ed.ac.uk</email>
<author confidence="0.988059">Victor</author>
<affiliation confidence="0.997414">School of The University of</affiliation>
<address confidence="0.959581">EH8 9AB, Edinburgh,</address>
<email confidence="0.998607">vlavrenk@inf.ed.ac.uk</email>
<author confidence="0.926882">Miles</author>
<affiliation confidence="0.996885">School of The University of</affiliation>
<address confidence="0.895495">EH8 9AB, Edinburgh,</address>
<email confidence="0.99961">miles@inf.ed.ac.uk</email>
<abstract confidence="0.990091466666667">We introduce a scheme for optimally allocating a variable number of bits per LSH hyperplane. Previous approaches assign a constant number of bits per hyperplane. This neglects the fact that a subset of hyperplanes may be more informative than others. Our method, dubbed Variable Bit Quantisation (VBQ), provides a datadriven non-uniform bit allocation across hyperplanes. Despite only using a fraction of the available hyperplanes, VBQ outperforms uniform quantisation by up to 168% for retrieval across standard text and image datasets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Moses Charikar</author>
</authors>
<title>Similarity estimation techniques from rounding algorithms.</title>
<date>2002</date>
<booktitle>In STOC,</booktitle>
<pages>380--388</pages>
<contexts>
<context position="3542" citStr="Charikar, 2002" startWordPosition="527" endWordPosition="528">e demonstrate that uniform allocation of bits is sub-optimal and propose a data-driven scheme for variable bit allocation. Our approach is distinct from previous work in that it provides a general objective function for bit allocation. VBQ makes no assumptions on the data and, in addition to LSH, it applies to a broad range of other projection functions. 2 Related Work Locality sensitive hashing (LSH) (Indyk and Motwani, 1998) is an example of an approximate nearest neighbour search technique that has been widely used within the field of NLP to preserve the Cosine distances between documents (Charikar, 2002). LSH for cosine distance draws a large number of random hyperplanes within the input feature space, effectively dividing the space into non-overlapping regions (or buckets). Each hyperplane contributes one bit to the encoding, the value (0 or 1) of which is determined by comput753 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 753–758, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 2 bits assigned: 3 thresholds perfectly preserve neighbourhood structure Y n, h, It 10 h, n2 00 01 X Fl score: 1.00 00 01 10 11 Fl</context>
<context position="14729" citStr="Charikar, 2002" startWordPosition="2398" endWordPosition="2399"> LSH. In our evaluation we take a sample of the more popular data-independent (LSH, SIKH) and data-dependent (SH, PCAH, BLSI) projection functions used in recent hashing work: • SIKH: Shift-Invariant Kernel Hashing (SIKH) uses random projections that approximate shift invariant kernels (Raginsky and Lazebnik, 2009). We follow previous work and use a Gaussian kernel with a bandwidth set to the average distance to the 50th nearest neighbour (Kong et al., 2012) (Raginsky and Lazebnik, 2009). • LSH: Locality Sensitive Hashing uses a Gaussian random matrix for projection (Indyk and Motwani, 1998) (Charikar, 2002). • BLSI: Binarised Latent Semantic Indexing (BLSI) forms projections through Singular Value Decomposition (SVD) (Salakhutdinov and Hinton, 2009). • SH: Spectral Hashing (SH) uses the eigenfunctions computed along the principal component directions of the data for projection (Weiss et al., 2008). • PCAH: Principal Component Analysis Hashing (PCAH) employs the eigenvectors corresponding the the largest eigenvalues of the covariance matrix for projection (Wang et al., 2012). 4.3 Baselines Single Bit Quantisation (SBQ) (Indyk and Motwani, 1998), Manhattan Hashing (MQ) (Kong et al., 2012), Double </context>
</contexts>
<marker>Charikar, 2002</marker>
<rawString>Moses Charikar. 2002. Similarity estimation techniques from rounding algorithms. In STOC, pages 380–388.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Piotr Indyk</author>
<author>Rajeev Motwani</author>
</authors>
<title>Approximate nearest neighbors: towards removing the curse of dimensionality.</title>
<date>1998</date>
<booktitle>In Proceedings of the thirtieth annual ACM symposium on Theory of computing, STOC ’98,</booktitle>
<pages>604--613</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="2863" citStr="Indyk and Motwani, 1998" startWordPosition="412" endWordPosition="415">bours. Hashing-based ANN techniques generally comprise two main steps: a projection stage followed by a quantisation stage. The projection stage performs a neighbourhood preserving embedding, mapping the input data into a lower-dimensional representation. The quantisation stage subsequently reduces the cardinality of this representation by converting the real-valued projections to binary. Quantisation is a lossy transformation which can have a significant impact on the resulting quality of the binary encoding. Previous work has quantised each projected dimension into a uniform number of bits (Indyk and Motwani, 1998) (Kong and Li, 2012) (Kong et al., 2012) (Moran et al., 2013). We demonstrate that uniform allocation of bits is sub-optimal and propose a data-driven scheme for variable bit allocation. Our approach is distinct from previous work in that it provides a general objective function for bit allocation. VBQ makes no assumptions on the data and, in addition to LSH, it applies to a broad range of other projection functions. 2 Related Work Locality sensitive hashing (LSH) (Indyk and Motwani, 1998) is an example of an approximate nearest neighbour search technique that has been widely used within the f</context>
<context position="5475" citStr="Indyk and Motwani, 1998" startWordPosition="848" endWordPosition="851"> of a data-point (x) with the normal vector to the hyperplane (ni): that is, if x.ni &lt; 0, i ∈ {1 ... k}, then the i-th bit is set to 0, and 1 otherwise. This encoding scheme is known as single bit quantisation (SBQ). More recent hashing work has sought to inject a degree of data-dependency into the positioning of the hyperplanes, for example, by using the principal directions of the data (Wang et al., 2012) (Weiss et al., 2008) or by training a stack of restricted Boltzmann machines (Salakhutdinov and Hinton, 2009). Existing quantisation schemes for LSH allocate either one bit per hyperplane (Indyk and Motwani, 1998) or multiple bits per hyperplane (Kong et al., 2012) (Kong and Li, 2012) (Moran et al., 2013). For example, (Kong et al., 2012) recently proposed the Manhattan Hashing (MQ) quantisation technique where each projected dimension is encoded with multiple bits of natural binary code (NBC). The Manhattan distance between the NBC encoded data points is then used for nearest neighbour search. The authors demonstrated that MQ could better preserve the neighbourhood structure between the data points as compared to SBQ with Hamming distance. Other recent quantisation work has focused on the setting of t</context>
<context position="14712" citStr="Indyk and Motwani, 1998" startWordPosition="2393" endWordPosition="2397">ction functions, including LSH. In our evaluation we take a sample of the more popular data-independent (LSH, SIKH) and data-dependent (SH, PCAH, BLSI) projection functions used in recent hashing work: • SIKH: Shift-Invariant Kernel Hashing (SIKH) uses random projections that approximate shift invariant kernels (Raginsky and Lazebnik, 2009). We follow previous work and use a Gaussian kernel with a bandwidth set to the average distance to the 50th nearest neighbour (Kong et al., 2012) (Raginsky and Lazebnik, 2009). • LSH: Locality Sensitive Hashing uses a Gaussian random matrix for projection (Indyk and Motwani, 1998) (Charikar, 2002). • BLSI: Binarised Latent Semantic Indexing (BLSI) forms projections through Singular Value Decomposition (SVD) (Salakhutdinov and Hinton, 2009). • SH: Spectral Hashing (SH) uses the eigenfunctions computed along the principal component directions of the data for projection (Weiss et al., 2008). • PCAH: Principal Component Analysis Hashing (PCAH) employs the eigenvectors corresponding the the largest eigenvalues of the covariance matrix for projection (Wang et al., 2012). 4.3 Baselines Single Bit Quantisation (SBQ) (Indyk and Motwani, 1998), Manhattan Hashing (MQ) (Kong et al</context>
</contexts>
<marker>Indyk, Motwani, 1998</marker>
<rawString>Piotr Indyk and Rajeev Motwani. 1998. Approximate nearest neighbors: towards removing the curse of dimensionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing, STOC ’98, pages 604–613, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weihao Kong</author>
<author>Wu-Jun Li</author>
</authors>
<title>Double-bit quantization for hashing.</title>
<date>2012</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="2883" citStr="Kong and Li, 2012" startWordPosition="416" endWordPosition="419">echniques generally comprise two main steps: a projection stage followed by a quantisation stage. The projection stage performs a neighbourhood preserving embedding, mapping the input data into a lower-dimensional representation. The quantisation stage subsequently reduces the cardinality of this representation by converting the real-valued projections to binary. Quantisation is a lossy transformation which can have a significant impact on the resulting quality of the binary encoding. Previous work has quantised each projected dimension into a uniform number of bits (Indyk and Motwani, 1998) (Kong and Li, 2012) (Kong et al., 2012) (Moran et al., 2013). We demonstrate that uniform allocation of bits is sub-optimal and propose a data-driven scheme for variable bit allocation. Our approach is distinct from previous work in that it provides a general objective function for bit allocation. VBQ makes no assumptions on the data and, in addition to LSH, it applies to a broad range of other projection functions. 2 Related Work Locality sensitive hashing (LSH) (Indyk and Motwani, 1998) is an example of an approximate nearest neighbour search technique that has been widely used within the field of NLP to prese</context>
<context position="5547" citStr="Kong and Li, 2012" startWordPosition="861" endWordPosition="864">f x.ni &lt; 0, i ∈ {1 ... k}, then the i-th bit is set to 0, and 1 otherwise. This encoding scheme is known as single bit quantisation (SBQ). More recent hashing work has sought to inject a degree of data-dependency into the positioning of the hyperplanes, for example, by using the principal directions of the data (Wang et al., 2012) (Weiss et al., 2008) or by training a stack of restricted Boltzmann machines (Salakhutdinov and Hinton, 2009). Existing quantisation schemes for LSH allocate either one bit per hyperplane (Indyk and Motwani, 1998) or multiple bits per hyperplane (Kong et al., 2012) (Kong and Li, 2012) (Moran et al., 2013). For example, (Kong et al., 2012) recently proposed the Manhattan Hashing (MQ) quantisation technique where each projected dimension is encoded with multiple bits of natural binary code (NBC). The Manhattan distance between the NBC encoded data points is then used for nearest neighbour search. The authors demonstrated that MQ could better preserve the neighbourhood structure between the data points as compared to SBQ with Hamming distance. Other recent quantisation work has focused on the setting of the quantisation thresholds: for example (Kong and Li, 2012) suggested en</context>
<context position="13886" citStr="Kong and Li, 2012" startWordPosition="2265" endWordPosition="2268">nts which are classified into 96 semantic categories. We remove those documents appearing in two or more categories and keep only the largest 30 categories. This leaves 9,394 documents in total with 36,771 distinct terms. Both text datasets are TF-IDF and L2 norm weighted. To demonstrate the generality of VBQ we also evaluate on the CIFAR-10 image dataset (Krizhevsky, 2009), which consists of 60,000 images represented as 512 dimensional Gist descriptors (Oliva and Torralba, 2001). All of the datasets are identical to those that have been used in previous ANN hashing work (Zhang et al., 2010) (Kong and Li, 2012) and are publicly available on the Internet. 4.2 Projection Methods VBQ is independent of the projection stage and therefore can be used the quantise the projections from a wide range of different projection functions, including LSH. In our evaluation we take a sample of the more popular data-independent (LSH, SIKH) and data-dependent (SH, PCAH, BLSI) projection functions used in recent hashing work: • SIKH: Shift-Invariant Kernel Hashing (SIKH) uses random projections that approximate shift invariant kernels (Raginsky and Lazebnik, 2009). We follow previous work and use a Gaussian kernel with</context>
<context position="15371" citStr="Kong and Li, 2012" startWordPosition="2491" endWordPosition="2494">nt Semantic Indexing (BLSI) forms projections through Singular Value Decomposition (SVD) (Salakhutdinov and Hinton, 2009). • SH: Spectral Hashing (SH) uses the eigenfunctions computed along the principal component directions of the data for projection (Weiss et al., 2008). • PCAH: Principal Component Analysis Hashing (PCAH) employs the eigenvectors corresponding the the largest eigenvalues of the covariance matrix for projection (Wang et al., 2012). 4.3 Baselines Single Bit Quantisation (SBQ) (Indyk and Motwani, 1998), Manhattan Hashing (MQ) (Kong et al., 2012), Double Bit Quantisation (DBQ) (Kong and Li, 2012) and Neighbourhood Preserving Quantisation (NPQ) (Moran et al., 2013). MQ, DBQ and NPQ all assign 2 bits per hyperplane, while SBQ assigns 1 bit per hyperplane. All methods, including VBQ, are constrained to be within the allocated bit budget B. If a method assigns more bits to one hyperplane, then it either discards, or assigns less bits to other hyperplanes. 4.4 Evaluation Protocol We adopt the standard Hamming ranking evaluation paradigm (Kong et al., 2012). We randomly select 1000 query data points per run. Our results are averaged over 10 runs, and the average reported. The ǫ-neighbours o</context>
</contexts>
<marker>Kong, Li, 2012</marker>
<rawString>Weihao Kong and Wu-Jun Li. 2012. Double-bit quantization for hashing. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weihao Kong</author>
<author>Wu-Jun Li</author>
<author>Minyi Guo</author>
</authors>
<title>Manhattan hashing for large-scale image retrieval.</title>
<date>2012</date>
<journal>SIGIR</journal>
<volume>12</volume>
<pages>45--54</pages>
<contexts>
<context position="2903" citStr="Kong et al., 2012" startWordPosition="420" endWordPosition="423">comprise two main steps: a projection stage followed by a quantisation stage. The projection stage performs a neighbourhood preserving embedding, mapping the input data into a lower-dimensional representation. The quantisation stage subsequently reduces the cardinality of this representation by converting the real-valued projections to binary. Quantisation is a lossy transformation which can have a significant impact on the resulting quality of the binary encoding. Previous work has quantised each projected dimension into a uniform number of bits (Indyk and Motwani, 1998) (Kong and Li, 2012) (Kong et al., 2012) (Moran et al., 2013). We demonstrate that uniform allocation of bits is sub-optimal and propose a data-driven scheme for variable bit allocation. Our approach is distinct from previous work in that it provides a general objective function for bit allocation. VBQ makes no assumptions on the data and, in addition to LSH, it applies to a broad range of other projection functions. 2 Related Work Locality sensitive hashing (LSH) (Indyk and Motwani, 1998) is an example of an approximate nearest neighbour search technique that has been widely used within the field of NLP to preserve the Cosine dista</context>
<context position="5527" citStr="Kong et al., 2012" startWordPosition="857" endWordPosition="860">ane (ni): that is, if x.ni &lt; 0, i ∈ {1 ... k}, then the i-th bit is set to 0, and 1 otherwise. This encoding scheme is known as single bit quantisation (SBQ). More recent hashing work has sought to inject a degree of data-dependency into the positioning of the hyperplanes, for example, by using the principal directions of the data (Wang et al., 2012) (Weiss et al., 2008) or by training a stack of restricted Boltzmann machines (Salakhutdinov and Hinton, 2009). Existing quantisation schemes for LSH allocate either one bit per hyperplane (Indyk and Motwani, 1998) or multiple bits per hyperplane (Kong et al., 2012) (Kong and Li, 2012) (Moran et al., 2013). For example, (Kong et al., 2012) recently proposed the Manhattan Hashing (MQ) quantisation technique where each projected dimension is encoded with multiple bits of natural binary code (NBC). The Manhattan distance between the NBC encoded data points is then used for nearest neighbour search. The authors demonstrated that MQ could better preserve the neighbourhood structure between the data points as compared to SBQ with Hamming distance. Other recent quantisation work has focused on the setting of the quantisation thresholds: for example (Kong and Li</context>
<context position="14576" citStr="Kong et al., 2012" startWordPosition="2373" endWordPosition="2376">is independent of the projection stage and therefore can be used the quantise the projections from a wide range of different projection functions, including LSH. In our evaluation we take a sample of the more popular data-independent (LSH, SIKH) and data-dependent (SH, PCAH, BLSI) projection functions used in recent hashing work: • SIKH: Shift-Invariant Kernel Hashing (SIKH) uses random projections that approximate shift invariant kernels (Raginsky and Lazebnik, 2009). We follow previous work and use a Gaussian kernel with a bandwidth set to the average distance to the 50th nearest neighbour (Kong et al., 2012) (Raginsky and Lazebnik, 2009). • LSH: Locality Sensitive Hashing uses a Gaussian random matrix for projection (Indyk and Motwani, 1998) (Charikar, 2002). • BLSI: Binarised Latent Semantic Indexing (BLSI) forms projections through Singular Value Decomposition (SVD) (Salakhutdinov and Hinton, 2009). • SH: Spectral Hashing (SH) uses the eigenfunctions computed along the principal component directions of the data for projection (Weiss et al., 2008). • PCAH: Principal Component Analysis Hashing (PCAH) employs the eigenvectors corresponding the the largest eigenvalues of the covariance matrix for p</context>
<context position="15835" citStr="Kong et al., 2012" startWordPosition="2569" endWordPosition="2572">nes Single Bit Quantisation (SBQ) (Indyk and Motwani, 1998), Manhattan Hashing (MQ) (Kong et al., 2012), Double Bit Quantisation (DBQ) (Kong and Li, 2012) and Neighbourhood Preserving Quantisation (NPQ) (Moran et al., 2013). MQ, DBQ and NPQ all assign 2 bits per hyperplane, while SBQ assigns 1 bit per hyperplane. All methods, including VBQ, are constrained to be within the allocated bit budget B. If a method assigns more bits to one hyperplane, then it either discards, or assigns less bits to other hyperplanes. 4.4 Evaluation Protocol We adopt the standard Hamming ranking evaluation paradigm (Kong et al., 2012). We randomly select 1000 query data points per run. Our results are averaged over 10 runs, and the average reported. The ǫ-neighbours of each query point 756 [2] 0 0.1 0.2 0.3 0.4 0.5 0.6 0.� 0.8 0.9 Re.W1 0.1 0.05 0 8 16 24 32 40 48 56 64 [1] Number of Bits 1 SBQ MH VBQ 0.9 0.8 0.] Precision 0.6 0.5 0.4 0.3 0.2 0.1 0 SBQ MQ VBQ AUPRC 0.2 0.15 0.35 0.3 0.25 SBQ MQ VBQ 0.05 0 0.5 0.45 0.4 0.35 0.3 0.25 0.2 0.15 0.1 0.2 0.1 0 SBQ MQ VBQ 0.7 0.6 0.5 0.4 0.3 AUPRC AUPRC 0 0.1 0.2 0.3 0.4 0.5 0.6 0.] 0.8 0.9 1 Ren11 0 0.1 0.2 0.3 0.4 0.5 0.6 0.] 0.8 0.9 1 Ren11 [4] [6] 32 �� �� �� 126 256 Number o</context>
<context position="17332" citStr="Kong et al., 2012" startWordPosition="2868" endWordPosition="2871">its for TDT-2 [4] LSH Precision-Recall curve for TDT-2 [5] LSH AUPRC vs bits for Reuters-21578 [6] LSH Precision-Recall curve for Reuters-21578 form the ground truth for evaluation. The threshold E is computed by sampling 100 training datapoints at random from the training dataset and determining the distance at which these points have 50 nearest neighbours on average. Positive pairs and negative pairs for F-measure computation are computed by thresholding the training dataset Euclidean distance matrix by E. We adopt the Manhattan distance and multi-bit binary encoding method as suggested in (Kong et al., 2012). The F-measure we use for threshold optimisation is: FQ = (1+02)TP/((1+02)TP +02FN +FP). We select the parameter 0 on a held-out validation dataset. The area under the precision-recall curve (AUPRC) is used to evaluate the quality of retrieval. 4.5 Results Table 1 presents our results. For LSH on text (Reuters-21578) at 128 bits we find a substantial 95% gain in retrieval performance over uniformly assigning 1 bit per hyperplane (SBQ) and a 168% gain over uniformly assigning 2 bits per hyperplane (MQ). VBQ gain over SBQ at 128 bits is statistically significant based upon a paired Wilcoxon sig</context>
</contexts>
<marker>Kong, Li, Guo, 2012</marker>
<rawString>Weihao Kong, Wu-Jun Li, and Minyi Guo. 2012. Manhattan hashing for large-scale image retrieval. SIGIR ’12, pages 45–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Krizhevsky</author>
</authors>
<title>Learning Multiple Layers of Features from Tiny Images. Master’s thesis.</title>
<date>2009</date>
<contexts>
<context position="13644" citStr="Krizhevsky, 2009" startWordPosition="2225" endWordPosition="2226">35 categories. We use the ModApte version and discard those documents with multiple category labels. This leaves 8,293 documents in 65 categories. The corpus contains 18,933 distinct terms. The TDT-2 corpus consists of 11,201 on-topic documents which are classified into 96 semantic categories. We remove those documents appearing in two or more categories and keep only the largest 30 categories. This leaves 9,394 documents in total with 36,771 distinct terms. Both text datasets are TF-IDF and L2 norm weighted. To demonstrate the generality of VBQ we also evaluate on the CIFAR-10 image dataset (Krizhevsky, 2009), which consists of 60,000 images represented as 512 dimensional Gist descriptors (Oliva and Torralba, 2001). All of the datasets are identical to those that have been used in previous ANN hashing work (Zhang et al., 2010) (Kong and Li, 2012) and are publicly available on the Internet. 4.2 Projection Methods VBQ is independent of the projection stage and therefore can be used the quantise the projections from a wide range of different projection functions, including LSH. In our evaluation we take a sample of the more popular data-independent (LSH, SIKH) and data-dependent (SH, PCAH, BLSI) proj</context>
</contexts>
<marker>Krizhevsky, 2009</marker>
<rawString>Alex Krizhevsky. 2009. Learning Multiple Layers of Features from Tiny Images. Master’s thesis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kriste Krstovski</author>
<author>David A Smith</author>
</authors>
<title>A Minimally Supervised Approach for Detecting and Ranking Document Translation Pairs.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland.</location>
<contexts>
<context position="1271" citStr="Krstovski and Smith, 2011" startWordPosition="186" endWordPosition="189">t Quantisation (VBQ), provides a datadriven non-uniform bit allocation across hyperplanes. Despite only using a fraction of the available hyperplanes, VBQ outperforms uniform quantisation by up to 168% for retrieval across standard text and image datasets. 1 Introduction The task of retrieving the nearest neighbours to a given query document permeates the field of Natural Language Processing (NLP). Nearest neighbour search has been used for applications as diverse as automatically detecting document translation pairs for the purposes of training a statistical machine translation system (SMT) (Krstovski and Smith, 2011), the large-scale generation of noun similarity lists (Ravichandran et al., 2005) to an unsupervised method for extracting domain specific lexical variants (Stephan Gouws and Metzle, 2011). There are two broad approaches to nearest neighbour based search: exact and approximate techniques, which are differentiated by their ability to return completely correct nearest neighbours (the exact approach) or have some possibility of returning points that are not true nearest neighbours (the approximate approach). Approximate nearest neighbour (ANN) search using hashing techniques has recently gained p</context>
</contexts>
<marker>Krstovski, Smith, 2011</marker>
<rawString>Kriste Krstovski and David A. Smith. 2011. A Minimally Supervised Approach for Detecting and Ranking Document Translation Pairs. In Proceedings of the Sixth Workshop on Statistical Machine Translation, Edinburgh, Scotland. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A H Land</author>
<author>A G Doig</author>
</authors>
<title>An automatic method of solving discrete programming problems.</title>
<date>1960</date>
<journal>Econometrica,</journal>
<pages>28--497</pages>
<contexts>
<context position="11210" citStr="Land and Doig, 1960" startWordPosition="1793" endWordPosition="1796">exes the rows, with k being the maximum number of bits allowable for any given hyperplane (set to 4 in this work), and h E {1... , B} indexes the columns. The BILP uses F to find the bit allocation that maximises the cumulative F-measure across the B hyperplanes (Equation 1). max IIF o ZII subject to IIZhII = 1 h E {1... B} IIZoDII &lt; B Z is binary II.II denotes the Frobenius L1 norm, o the Hadamard product and D is a constraint matrix, with Db,h = b, ensuring that the bit allocation remains within the bit budget B. The BILP is solved using the standard branch and bound optimization algorithm (Land and Doig, 1960). The output from the BILP is an indicator matrix Z E {0,1}(k+1)xB whose columns specify the optimal bit allocation for a given hyperplane i.e. Zb,h = 1 if the BILP decided to allocate b bits for hyperplane h, and zero otherwise. Example matrices for the toy problem in Figure 1 are given hereunder (in this example, k = 2 and B = 2). F h1 h2 D Z b0 0.25 0.25 ⎞ ⎛ 0 0 ⎞ ⎛ 1 0 ⎛ ⎞ b1 0.35 0.50 ⎠ ⎝ 1 1 ⎠ ⎝ 0 0 1 ⎠ ⎝b2 0.40 1.00 2 2 0 Notice how the indicator matrix Z specifies an assignment of 0 bits for hyperplane h1 and 2 bits for hyperplane h2 as this yields the highest cumulative F-measure acro</context>
</contexts>
<marker>Land, Doig, 1960</marker>
<rawString>A. H. Land and A. G. Doig. 1960. An automatic method of solving discrete programming problems. Econometrica, 28:pp. 497–520.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sean Moran</author>
<author>Victor Lavrenko</author>
<author>Miles Osborne</author>
</authors>
<title>Neighbourhood preserving quantisation for lsh.</title>
<date>2013</date>
<booktitle>In 36th Annual International ACM Conference on Research and Development in Information Retrieval (SIGIR),</booktitle>
<pages>07--2013</pages>
<location>Dublin, Ireland,</location>
<contexts>
<context position="2924" citStr="Moran et al., 2013" startWordPosition="424" endWordPosition="427">eps: a projection stage followed by a quantisation stage. The projection stage performs a neighbourhood preserving embedding, mapping the input data into a lower-dimensional representation. The quantisation stage subsequently reduces the cardinality of this representation by converting the real-valued projections to binary. Quantisation is a lossy transformation which can have a significant impact on the resulting quality of the binary encoding. Previous work has quantised each projected dimension into a uniform number of bits (Indyk and Motwani, 1998) (Kong and Li, 2012) (Kong et al., 2012) (Moran et al., 2013). We demonstrate that uniform allocation of bits is sub-optimal and propose a data-driven scheme for variable bit allocation. Our approach is distinct from previous work in that it provides a general objective function for bit allocation. VBQ makes no assumptions on the data and, in addition to LSH, it applies to a broad range of other projection functions. 2 Related Work Locality sensitive hashing (LSH) (Indyk and Motwani, 1998) is an example of an approximate nearest neighbour search technique that has been widely used within the field of NLP to preserve the Cosine distances between document</context>
<context position="5568" citStr="Moran et al., 2013" startWordPosition="865" endWordPosition="868">.. k}, then the i-th bit is set to 0, and 1 otherwise. This encoding scheme is known as single bit quantisation (SBQ). More recent hashing work has sought to inject a degree of data-dependency into the positioning of the hyperplanes, for example, by using the principal directions of the data (Wang et al., 2012) (Weiss et al., 2008) or by training a stack of restricted Boltzmann machines (Salakhutdinov and Hinton, 2009). Existing quantisation schemes for LSH allocate either one bit per hyperplane (Indyk and Motwani, 1998) or multiple bits per hyperplane (Kong et al., 2012) (Kong and Li, 2012) (Moran et al., 2013). For example, (Kong et al., 2012) recently proposed the Manhattan Hashing (MQ) quantisation technique where each projected dimension is encoded with multiple bits of natural binary code (NBC). The Manhattan distance between the NBC encoded data points is then used for nearest neighbour search. The authors demonstrated that MQ could better preserve the neighbourhood structure between the data points as compared to SBQ with Hamming distance. Other recent quantisation work has focused on the setting of the quantisation thresholds: for example (Kong and Li, 2012) suggested encoding each dimension</context>
<context position="10214" citStr="Moran et al., 2013" startWordPosition="1604" endWordPosition="1607">he same regions (false positives, FP), and the number of positive pairs found in different regions of the threshold partitioned dimension (false negatives, FN). For n2, three thresholds are optimal, given they perfectly preserve the neighbourhood structure. For n1, no thresholds can provide a neighbourhood preserving quantisation and therefore it is better to discard the hyperplane h1. VBQ uses random restarts to optimise the F-measure3. The computed F-measure scores per hyper2b bits, requires 26 − 1 thresholds. 3More details on the computation of the F-measure per hyperplane can be found in (Moran et al., 2013). plane (h), per bit count (b) are an effective signal for bit allocation: more informative hyperplanes tend to have higher F-measure, for higher bit counts. VBQ applies a binary integer linear program (BILP) on top of the F-measure scores to obtain the bit allocation. To do so, the algorithm collates the scores in a matrix F with elements Fb,h, where b E {0, ... , k} 4 indexes the rows, with k being the maximum number of bits allowable for any given hyperplane (set to 4 in this work), and h E {1... , B} indexes the columns. The BILP uses F to find the bit allocation that maximises the cumulat</context>
<context position="15440" citStr="Moran et al., 2013" startWordPosition="2500" endWordPosition="2503"> Decomposition (SVD) (Salakhutdinov and Hinton, 2009). • SH: Spectral Hashing (SH) uses the eigenfunctions computed along the principal component directions of the data for projection (Weiss et al., 2008). • PCAH: Principal Component Analysis Hashing (PCAH) employs the eigenvectors corresponding the the largest eigenvalues of the covariance matrix for projection (Wang et al., 2012). 4.3 Baselines Single Bit Quantisation (SBQ) (Indyk and Motwani, 1998), Manhattan Hashing (MQ) (Kong et al., 2012), Double Bit Quantisation (DBQ) (Kong and Li, 2012) and Neighbourhood Preserving Quantisation (NPQ) (Moran et al., 2013). MQ, DBQ and NPQ all assign 2 bits per hyperplane, while SBQ assigns 1 bit per hyperplane. All methods, including VBQ, are constrained to be within the allocated bit budget B. If a method assigns more bits to one hyperplane, then it either discards, or assigns less bits to other hyperplanes. 4.4 Evaluation Protocol We adopt the standard Hamming ranking evaluation paradigm (Kong et al., 2012). We randomly select 1000 query data points per run. Our results are averaged over 10 runs, and the average reported. The ǫ-neighbours of each query point 756 [2] 0 0.1 0.2 0.3 0.4 0.5 0.6 0.� 0.8 0.9 Re.W</context>
</contexts>
<marker>Moran, Lavrenko, Osborne, 2013</marker>
<rawString>Sean Moran, Victor Lavrenko, and Miles Osborne. 2013. Neighbourhood preserving quantisation for lsh. In 36th Annual International ACM Conference on Research and Development in Information Retrieval (SIGIR), Dublin, Ireland, 07/2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aude Oliva</author>
<author>Antonio Torralba</author>
</authors>
<title>Modeling the shape of the scene: A holistic representation of the spatial envelope.</title>
<date>2001</date>
<journal>International Journal of Computer Vision,</journal>
<volume>42</volume>
<issue>3</issue>
<contexts>
<context position="13752" citStr="Oliva and Torralba, 2001" startWordPosition="2239" endWordPosition="2242"> This leaves 8,293 documents in 65 categories. The corpus contains 18,933 distinct terms. The TDT-2 corpus consists of 11,201 on-topic documents which are classified into 96 semantic categories. We remove those documents appearing in two or more categories and keep only the largest 30 categories. This leaves 9,394 documents in total with 36,771 distinct terms. Both text datasets are TF-IDF and L2 norm weighted. To demonstrate the generality of VBQ we also evaluate on the CIFAR-10 image dataset (Krizhevsky, 2009), which consists of 60,000 images represented as 512 dimensional Gist descriptors (Oliva and Torralba, 2001). All of the datasets are identical to those that have been used in previous ANN hashing work (Zhang et al., 2010) (Kong and Li, 2012) and are publicly available on the Internet. 4.2 Projection Methods VBQ is independent of the projection stage and therefore can be used the quantise the projections from a wide range of different projection functions, including LSH. In our evaluation we take a sample of the more popular data-independent (LSH, SIKH) and data-dependent (SH, PCAH, BLSI) projection functions used in recent hashing work: • SIKH: Shift-Invariant Kernel Hashing (SIKH) uses random proj</context>
</contexts>
<marker>Oliva, Torralba, 2001</marker>
<rawString>Aude Oliva and Antonio Torralba. 2001. Modeling the shape of the scene: A holistic representation of the spatial envelope. International Journal of Computer Vision, 42(3):145–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maxim Raginsky</author>
<author>Svetlana Lazebnik</author>
</authors>
<title>Locality-sensitive binary codes from shift-invariant kernels.</title>
<date>2009</date>
<booktitle>In NIPS ’09,</booktitle>
<pages>1509--1517</pages>
<contexts>
<context position="14430" citStr="Raginsky and Lazebnik, 2009" startWordPosition="2347" endWordPosition="2350">have been used in previous ANN hashing work (Zhang et al., 2010) (Kong and Li, 2012) and are publicly available on the Internet. 4.2 Projection Methods VBQ is independent of the projection stage and therefore can be used the quantise the projections from a wide range of different projection functions, including LSH. In our evaluation we take a sample of the more popular data-independent (LSH, SIKH) and data-dependent (SH, PCAH, BLSI) projection functions used in recent hashing work: • SIKH: Shift-Invariant Kernel Hashing (SIKH) uses random projections that approximate shift invariant kernels (Raginsky and Lazebnik, 2009). We follow previous work and use a Gaussian kernel with a bandwidth set to the average distance to the 50th nearest neighbour (Kong et al., 2012) (Raginsky and Lazebnik, 2009). • LSH: Locality Sensitive Hashing uses a Gaussian random matrix for projection (Indyk and Motwani, 1998) (Charikar, 2002). • BLSI: Binarised Latent Semantic Indexing (BLSI) forms projections through Singular Value Decomposition (SVD) (Salakhutdinov and Hinton, 2009). • SH: Spectral Hashing (SH) uses the eigenfunctions computed along the principal component directions of the data for projection (Weiss et al., 2008). • P</context>
</contexts>
<marker>Raginsky, Lazebnik, 2009</marker>
<rawString>Maxim Raginsky and Svetlana Lazebnik. 2009. Locality-sensitive binary codes from shift-invariant kernels. In NIPS ’09, pages 1509–1517.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepak Ravichandran</author>
<author>Patrick Pantel</author>
<author>Eduard Hovy</author>
</authors>
<title>Randomized algorithms and nlp: using locality sensitive hash function for high speed noun clustering.</title>
<date>2005</date>
<journal>ACL</journal>
<volume>05</volume>
<pages>622--629</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1352" citStr="Ravichandran et al., 2005" startWordPosition="197" endWordPosition="200">erplanes. Despite only using a fraction of the available hyperplanes, VBQ outperforms uniform quantisation by up to 168% for retrieval across standard text and image datasets. 1 Introduction The task of retrieving the nearest neighbours to a given query document permeates the field of Natural Language Processing (NLP). Nearest neighbour search has been used for applications as diverse as automatically detecting document translation pairs for the purposes of training a statistical machine translation system (SMT) (Krstovski and Smith, 2011), the large-scale generation of noun similarity lists (Ravichandran et al., 2005) to an unsupervised method for extracting domain specific lexical variants (Stephan Gouws and Metzle, 2011). There are two broad approaches to nearest neighbour based search: exact and approximate techniques, which are differentiated by their ability to return completely correct nearest neighbours (the exact approach) or have some possibility of returning points that are not true nearest neighbours (the approximate approach). Approximate nearest neighbour (ANN) search using hashing techniques has recently gained prominence within NLP. The hashing-based approach maps the data into a substantial</context>
</contexts>
<marker>Ravichandran, Pantel, Hovy, 2005</marker>
<rawString>Deepak Ravichandran, Patrick Pantel, and Eduard Hovy. 2005. Randomized algorithms and nlp: using locality sensitive hash function for high speed noun clustering. ACL ’05, pages 622–629. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruslan Salakhutdinov</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Semantic hashing.</title>
<date>2009</date>
<journal>Int. J. Approx. Reasoning,</journal>
<volume>50</volume>
<issue>7</issue>
<contexts>
<context position="5371" citStr="Salakhutdinov and Hinton, 2009" startWordPosition="833" endWordPosition="836">is hyperplane (h1) is likely to have 0 bits assigned (and therefore be discarded entirely). ing the dot product of a data-point (x) with the normal vector to the hyperplane (ni): that is, if x.ni &lt; 0, i ∈ {1 ... k}, then the i-th bit is set to 0, and 1 otherwise. This encoding scheme is known as single bit quantisation (SBQ). More recent hashing work has sought to inject a degree of data-dependency into the positioning of the hyperplanes, for example, by using the principal directions of the data (Wang et al., 2012) (Weiss et al., 2008) or by training a stack of restricted Boltzmann machines (Salakhutdinov and Hinton, 2009). Existing quantisation schemes for LSH allocate either one bit per hyperplane (Indyk and Motwani, 1998) or multiple bits per hyperplane (Kong et al., 2012) (Kong and Li, 2012) (Moran et al., 2013). For example, (Kong et al., 2012) recently proposed the Manhattan Hashing (MQ) quantisation technique where each projected dimension is encoded with multiple bits of natural binary code (NBC). The Manhattan distance between the NBC encoded data points is then used for nearest neighbour search. The authors demonstrated that MQ could better preserve the neighbourhood structure between the data points </context>
<context position="14874" citStr="Salakhutdinov and Hinton, 2009" startWordPosition="2414" endWordPosition="2417">jection functions used in recent hashing work: • SIKH: Shift-Invariant Kernel Hashing (SIKH) uses random projections that approximate shift invariant kernels (Raginsky and Lazebnik, 2009). We follow previous work and use a Gaussian kernel with a bandwidth set to the average distance to the 50th nearest neighbour (Kong et al., 2012) (Raginsky and Lazebnik, 2009). • LSH: Locality Sensitive Hashing uses a Gaussian random matrix for projection (Indyk and Motwani, 1998) (Charikar, 2002). • BLSI: Binarised Latent Semantic Indexing (BLSI) forms projections through Singular Value Decomposition (SVD) (Salakhutdinov and Hinton, 2009). • SH: Spectral Hashing (SH) uses the eigenfunctions computed along the principal component directions of the data for projection (Weiss et al., 2008). • PCAH: Principal Component Analysis Hashing (PCAH) employs the eigenvectors corresponding the the largest eigenvalues of the covariance matrix for projection (Wang et al., 2012). 4.3 Baselines Single Bit Quantisation (SBQ) (Indyk and Motwani, 1998), Manhattan Hashing (MQ) (Kong et al., 2012), Double Bit Quantisation (DBQ) (Kong and Li, 2012) and Neighbourhood Preserving Quantisation (NPQ) (Moran et al., 2013). MQ, DBQ and NPQ all assign 2 bit</context>
</contexts>
<marker>Salakhutdinov, Hinton, 2009</marker>
<rawString>Ruslan Salakhutdinov and Geoffrey Hinton. 2009. Semantic hashing. Int. J. Approx. Reasoning, 50(7):969–978.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dirk Hovy Stephan Gouws</author>
<author>Donald Metzle</author>
</authors>
<title>Unsupervised mining of lexical variants from noisy text.</title>
<date>2011</date>
<booktitle>In Proceedings of the First workshop on Unsupervised Learning in NLP, EMNLP ’11,</booktitle>
<pages>8290</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="1459" citStr="Gouws and Metzle, 2011" startWordPosition="213" endWordPosition="216">up to 168% for retrieval across standard text and image datasets. 1 Introduction The task of retrieving the nearest neighbours to a given query document permeates the field of Natural Language Processing (NLP). Nearest neighbour search has been used for applications as diverse as automatically detecting document translation pairs for the purposes of training a statistical machine translation system (SMT) (Krstovski and Smith, 2011), the large-scale generation of noun similarity lists (Ravichandran et al., 2005) to an unsupervised method for extracting domain specific lexical variants (Stephan Gouws and Metzle, 2011). There are two broad approaches to nearest neighbour based search: exact and approximate techniques, which are differentiated by their ability to return completely correct nearest neighbours (the exact approach) or have some possibility of returning points that are not true nearest neighbours (the approximate approach). Approximate nearest neighbour (ANN) search using hashing techniques has recently gained prominence within NLP. The hashing-based approach maps the data into a substantially more compact representation referred to as a fingerprint, that is more efficient for performing similari</context>
</contexts>
<marker>Gouws, Metzle, 2011</marker>
<rawString>Dirk Hovy Stephan Gouws and Donald Metzle. 2011. Unsupervised mining of lexical variants from noisy text. In Proceedings of the First workshop on Unsupervised Learning in NLP, EMNLP ’11, page 8290. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Wang</author>
<author>S Kumar</author>
<author>Shih-Fu Chang</author>
</authors>
<title>Semisupervised hashing for large-scale search.</title>
<date>2012</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>34</volume>
<issue>12</issue>
<contexts>
<context position="5261" citStr="Wang et al., 2012" startWordPosition="816" endWordPosition="819">e neighbourhood structure. Right bottom: the high degree of mixing between the 1-NN means that this hyperplane (h1) is likely to have 0 bits assigned (and therefore be discarded entirely). ing the dot product of a data-point (x) with the normal vector to the hyperplane (ni): that is, if x.ni &lt; 0, i ∈ {1 ... k}, then the i-th bit is set to 0, and 1 otherwise. This encoding scheme is known as single bit quantisation (SBQ). More recent hashing work has sought to inject a degree of data-dependency into the positioning of the hyperplanes, for example, by using the principal directions of the data (Wang et al., 2012) (Weiss et al., 2008) or by training a stack of restricted Boltzmann machines (Salakhutdinov and Hinton, 2009). Existing quantisation schemes for LSH allocate either one bit per hyperplane (Indyk and Motwani, 1998) or multiple bits per hyperplane (Kong et al., 2012) (Kong and Li, 2012) (Moran et al., 2013). For example, (Kong et al., 2012) recently proposed the Manhattan Hashing (MQ) quantisation technique where each projected dimension is encoded with multiple bits of natural binary code (NBC). The Manhattan distance between the NBC encoded data points is then used for nearest neighbour searc</context>
<context position="15205" citStr="Wang et al., 2012" startWordPosition="2465" endWordPosition="2468">and Lazebnik, 2009). • LSH: Locality Sensitive Hashing uses a Gaussian random matrix for projection (Indyk and Motwani, 1998) (Charikar, 2002). • BLSI: Binarised Latent Semantic Indexing (BLSI) forms projections through Singular Value Decomposition (SVD) (Salakhutdinov and Hinton, 2009). • SH: Spectral Hashing (SH) uses the eigenfunctions computed along the principal component directions of the data for projection (Weiss et al., 2008). • PCAH: Principal Component Analysis Hashing (PCAH) employs the eigenvectors corresponding the the largest eigenvalues of the covariance matrix for projection (Wang et al., 2012). 4.3 Baselines Single Bit Quantisation (SBQ) (Indyk and Motwani, 1998), Manhattan Hashing (MQ) (Kong et al., 2012), Double Bit Quantisation (DBQ) (Kong and Li, 2012) and Neighbourhood Preserving Quantisation (NPQ) (Moran et al., 2013). MQ, DBQ and NPQ all assign 2 bits per hyperplane, while SBQ assigns 1 bit per hyperplane. All methods, including VBQ, are constrained to be within the allocated bit budget B. If a method assigns more bits to one hyperplane, then it either discards, or assigns less bits to other hyperplanes. 4.4 Evaluation Protocol We adopt the standard Hamming ranking evaluatio</context>
</contexts>
<marker>Wang, Kumar, Chang, 2012</marker>
<rawString>Jun Wang, S. Kumar, and Shih-Fu Chang. 2012. Semisupervised hashing for large-scale search. IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(12):2393–2406.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yair Weiss</author>
<author>Antonio B Torralba</author>
<author>Robert Fergus</author>
</authors>
<title>Spectral hashing.</title>
<date>2008</date>
<booktitle>In NIPS,</booktitle>
<pages>1753--1760</pages>
<contexts>
<context position="5282" citStr="Weiss et al., 2008" startWordPosition="820" endWordPosition="823">cture. Right bottom: the high degree of mixing between the 1-NN means that this hyperplane (h1) is likely to have 0 bits assigned (and therefore be discarded entirely). ing the dot product of a data-point (x) with the normal vector to the hyperplane (ni): that is, if x.ni &lt; 0, i ∈ {1 ... k}, then the i-th bit is set to 0, and 1 otherwise. This encoding scheme is known as single bit quantisation (SBQ). More recent hashing work has sought to inject a degree of data-dependency into the positioning of the hyperplanes, for example, by using the principal directions of the data (Wang et al., 2012) (Weiss et al., 2008) or by training a stack of restricted Boltzmann machines (Salakhutdinov and Hinton, 2009). Existing quantisation schemes for LSH allocate either one bit per hyperplane (Indyk and Motwani, 1998) or multiple bits per hyperplane (Kong et al., 2012) (Kong and Li, 2012) (Moran et al., 2013). For example, (Kong et al., 2012) recently proposed the Manhattan Hashing (MQ) quantisation technique where each projected dimension is encoded with multiple bits of natural binary code (NBC). The Manhattan distance between the NBC encoded data points is then used for nearest neighbour search. The authors demons</context>
<context position="15025" citStr="Weiss et al., 2008" startWordPosition="2440" endWordPosition="2443">nsky and Lazebnik, 2009). We follow previous work and use a Gaussian kernel with a bandwidth set to the average distance to the 50th nearest neighbour (Kong et al., 2012) (Raginsky and Lazebnik, 2009). • LSH: Locality Sensitive Hashing uses a Gaussian random matrix for projection (Indyk and Motwani, 1998) (Charikar, 2002). • BLSI: Binarised Latent Semantic Indexing (BLSI) forms projections through Singular Value Decomposition (SVD) (Salakhutdinov and Hinton, 2009). • SH: Spectral Hashing (SH) uses the eigenfunctions computed along the principal component directions of the data for projection (Weiss et al., 2008). • PCAH: Principal Component Analysis Hashing (PCAH) employs the eigenvectors corresponding the the largest eigenvalues of the covariance matrix for projection (Wang et al., 2012). 4.3 Baselines Single Bit Quantisation (SBQ) (Indyk and Motwani, 1998), Manhattan Hashing (MQ) (Kong et al., 2012), Double Bit Quantisation (DBQ) (Kong and Li, 2012) and Neighbourhood Preserving Quantisation (NPQ) (Moran et al., 2013). MQ, DBQ and NPQ all assign 2 bits per hyperplane, while SBQ assigns 1 bit per hyperplane. All methods, including VBQ, are constrained to be within the allocated bit budget B. If a met</context>
</contexts>
<marker>Weiss, Torralba, Fergus, 2008</marker>
<rawString>Yair Weiss, Antonio B. Torralba, and Robert Fergus. 2008. Spectral hashing. In NIPS, pages 1753–1760.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dell Zhang</author>
<author>Jun Wang</author>
<author>Deng Cai</author>
<author>Jinsong Lu</author>
</authors>
<title>Self-taught hashing for fast similarity search.</title>
<date>2010</date>
<booktitle>In SIGIR,</booktitle>
<pages>18--25</pages>
<contexts>
<context position="13866" citStr="Zhang et al., 2010" startWordPosition="2261" endWordPosition="2264">1,201 on-topic documents which are classified into 96 semantic categories. We remove those documents appearing in two or more categories and keep only the largest 30 categories. This leaves 9,394 documents in total with 36,771 distinct terms. Both text datasets are TF-IDF and L2 norm weighted. To demonstrate the generality of VBQ we also evaluate on the CIFAR-10 image dataset (Krizhevsky, 2009), which consists of 60,000 images represented as 512 dimensional Gist descriptors (Oliva and Torralba, 2001). All of the datasets are identical to those that have been used in previous ANN hashing work (Zhang et al., 2010) (Kong and Li, 2012) and are publicly available on the Internet. 4.2 Projection Methods VBQ is independent of the projection stage and therefore can be used the quantise the projections from a wide range of different projection functions, including LSH. In our evaluation we take a sample of the more popular data-independent (LSH, SIKH) and data-dependent (SH, PCAH, BLSI) projection functions used in recent hashing work: • SIKH: Shift-Invariant Kernel Hashing (SIKH) uses random projections that approximate shift invariant kernels (Raginsky and Lazebnik, 2009). We follow previous work and use a </context>
</contexts>
<marker>Zhang, Wang, Cai, Lu, 2010</marker>
<rawString>Dell Zhang, Jun Wang, Deng Cai, and Jinsong Lu. 2010. Self-taught hashing for fast similarity search. In SIGIR, pages 18–25.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>