<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002043">
<title confidence="0.999118">
KUL-Eval: A Combinatory Categorial Grammar Approach for
Improving Semantic Parsing of Robot Commands using Spatial Context
</title>
<author confidence="0.99968">
Willem Mattelaer, Mathias Verbeke and Davide Nitti
</author>
<affiliation confidence="0.999462">
Department of Computer Science, KU Leuven, Belgium
</affiliation>
<email confidence="0.962727333333333">
willem.mattelaer@gmail.com
mathias.verbeke@cs.kuleuven.be
davide.nitti@cs.kuleuven.be
</email>
<sectionHeader confidence="0.993494" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998405">
When executing commands, a robot has
a certain level of contextual knowledge
about the environment in which it oper-
ates. Taking this knowledge into account
can be beneficial to disambiguate com-
mands with multiple interpretations. We
present an approach that uses combina-
tory categorial grammars for improving
the semantic parsing of robot commands
that takes into account the spatial context
of the robot. The results indicate a clear
improvement over non-contextual seman-
tic parsing. This work was done in the
context of the SemEval-2014 task on su-
pervised semantic parsing of spatial robot
commands.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.942068615384615">
One of the long-standing goals of robotics is to
build autonomous robots that are able to perform
everyday tasks. Two important requirements to
achieve this are an efficient way of communicating
with the robot, and transforming these commands
such that the robot is able to capture their mean-
ing. Furthermore, this needs to be consistent with
the context in which the robot is operating, i.e., the
robot’s belief.
Semantic parsing focuses on translating natural
language (NL) into a formal representation that
captures the meaning of the sentence. Most of
the current semantic parsing approaches are non-
contextual, i.e., they do not take into account the
context in which the command sentence should be
executed. This can lead to erroneous parses, most
often due to ambiguity in the original sentence.
Consider the following example sentence “Move
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
the pyramid on the blue cube on the gray cube”.
This sentence has two valid interpretations. Either
the robot needs to move the pyramid that is cur-
rently standing on the blue cube and put it on the
gray cube, or move the pyramid and place it on the
blue cube that is standing on the gray cube.
Humans will decide on the correct interpreta-
tion by taking into account the context. For in-
stance, by looking at Figure 1, it is clear that the
second interpretation is not possible, because there
is no blue cube on top of a gray cube. However,
there is a pyramid on top of a blue cube, making
the first interpretation possible. The goal of this
paper is to improve on non-contextual semantic
parsing by tailoring the context to guide the parser.
In this way, part of the ambiguity that causes mul-
tiple interpretations can be resolved.
</bodyText>
<figureCaption confidence="0.733654">
Figure 1: Possible situation (taken from (Dukes,
2013b)).
</figureCaption>
<bodyText confidence="0.999969444444444">
Our approach consists of two steps. First, non-
contextual semantic parsing using combinatory
categorial grammars (CCG) (Steedman, 1996;
Steedman, 2000) is performed on the sentence.
This returns multiple possible parses, each with an
attached likelihood of correctness. Subsequently,
each parse is checked against the current context.
The parse with the highest score that is possible
given the current context is returned.
</bodyText>
<page confidence="0.988726">
385
</page>
<note confidence="0.7306375">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 385–390,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.99972575">
This paper is organized as follows. In Section 2
we discuss related work, followed by a detailed
description of our approach in Section 3. In Sec-
tion 4, the approach is evaluated and compared to
non-contextual parsing. Finally, in Section 5 we
conclude and outline directions for future work.
The software is available from https://
github.com/wmattelaer/Thesis.
</bodyText>
<sectionHeader confidence="0.999725" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999946815789474">
There is a significant body of previous work on
learning semantic parsers. We will first review
approaches that translate NL sentences into a for-
mal representation without taking context into ac-
count, followed by related techniques that use the
context to improve the parsing.
Our approach is inspired by the work of
Kwiatkowski et al. (2010). The authors present
a supervised CCG approach to parse queries to
a geographical information database and a flight-
booking system. This differs from the current set-
ting in that the database querying does not require
to take the context of the environment into ac-
count, as is the case when executing robot com-
mands. SILT (Kate et al., 2005) uses transfor-
mation rules to translate the NL sentence to a
query for the robot. This approach was extended
to tailor support vector machines with string ker-
nels (KRISP) (Kate and Mooney, 2006) and statis-
tical machine learning (WASP) (Mooney, 2007).
Also unsupervised approaches exist. Poon (2013)
solves this lack of supervision by 1) inferring su-
pervision using the target database, which con-
strains the search space, and 2) by using aug-
mented dependency trees.
Artzi and Zettlemoyer (2013) study the use of
grounded CCG semantic parsing using weak su-
pervision for interpreting navigational robot com-
mands. Their approach is similar to ours, but in-
stead of postprocessing the results in a verification
step, the context (or state) is added to the training
data. Krishnamurthy and Kollar (2013) use CCGs
as a foundation, but match it to the context using
an evaluation function. This evaluation function
scores a denotation, i.e., the set of entity referents
for the entire sentence, given a logical form and a
knowledge base, which is considered as the con-
text.
</bodyText>
<sectionHeader confidence="0.997496" genericHeader="method">
3 Methodology
</sectionHeader>
<bodyText confidence="0.9999849">
Our approach consists of two steps: a parse step
and a verification step. Before these steps can
be executed, a Combinatory Categorial Grammar
needs to be trained. The training data for this
grammar consists of typed A-expressions (Car-
penter, 1997) that are annotated with their cor-
responding NL sentences. As the input data for
the SemEval-2014 task consists of Robot Control
Language (RCL) expressions (Dukes, 2013a)1, the
data needs to be preprocessed first.
</bodyText>
<subsectionHeader confidence="0.999428">
3.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999988647058824">
During preprocessing, the RCL expressions are
transformed into equivalent A-expressions. In the
A-expressions, each entity is represented by a
lambda term where the variable is a reference to
the object. The properties of an entity are defined
by a conjunction of literals with two arguments.
The predicate details the property that is being de-
fined. An example entity, a blue cube, can be rep-
resented as Ax.color(x, blue), type(x, cube). A
spatial relation between two entities is a literal
with three arguments: the variable of the first en-
tity, the type of relation and the second entity. The
latter is given by its lambda term. This lambda
term has to be wrapped in a definite determiner,
det, that selects a single element from the set cre-
ated by the lambda term (Artzi and Zettlemoyer,
2013). For example: the RCL expression
</bodyText>
<equation confidence="0.8801983">
(entity:
(type: prism)
(spatial-relation:
(relation: above)
(entity:
(color: blue)
(type: cube))))
is transformed to the A-calculus expression
Ax.type(x,prism), relation(x, above,
det(Ay.color(y, blue), type(y, cube)))
</equation>
<bodyText confidence="0.999882857142857">
Events are contained in one lambda term with
one variable per event. There are three possible
event predicates. The action predicate defines the
action by detailing the action type and the object
entity. The destination predicate will set the des-
tination of the object2. Finally, the sequence pred-
icate is necessary to detail the order of the events.
</bodyText>
<footnote confidence="0.992478">
1RCL is a linguistically-oriented formal language for
controlling a robot arm, that represents entities, attributes,
anaphora, ellipsis and qualitative spatial relations.
2Note that this event is not always necessary, e.g., in the
case of a take action, the robot will not release the object.
</footnote>
<page confidence="0.99731">
386
</page>
<bodyText confidence="0.9812514375">
An example of this can be seen at the bottom of
Figure 2.
Besides transforming the RCL expressions to A-
calculus, also the action types of the events are
checked. If an event has an action of type move
or drop, it is changed to the combined move &amp;
drop action type. This change was introduced be-
cause the actual verbs that are used to instruct the
robot to perform one of these two actions are often
the same. To illustrate this, consider the following
two sentences taken from the training data: “place
blue block on top of single red block” and “place
green block on top of blue block”. In the former,
the intended action is a drop action, while in the
latter the action should be a move action. During
parsing, the correct action can be selected by look-
ing at the context it has to be executed in. If the
robot is currently grasping an object, the intended
action is a drop action, otherwise it is a move ac-
tion.
Furthermore, the anaphoric references are
resolved in the natural language sentences.
Anaphoric references are words that refer to one or
more words mentioned earlier in the sentence. The
sentences of the dataset are annotated with mark-
ers that capture the references in the sentence. The
markers that are used are [1], (1) and {1} and
are placed right after the word that is used for the
reference. [1] is used to mark a word that is re-
ferred to by another word, whereas (1) is used
to detail a word that refers to another word, e.g.,
it. Finally, {1} marks a word that refers to the
type of an earlier entity, e.g., one. The numbers
in these markers can increase if there are differ-
ent references in one sentence, but the sentences
of this dataset do not contain different references.
For instance, the sentence Pick the blue block and
place it above the gray one is transformed to the
sentence Pick the blue block [1] and place it (1)
above the gray one {1}.
The anaphoric references are found using
the coreference resolution system of Stanford
CoreNLP (Recasens et al., 2013; Lee et al., 2013;
Lee et al., 2011; Raghunathan et al., 2010). How-
ever, it is not capable of finding references that use
one. This can be solved by letting the one always
refer to the first entity of the sentence, because of
the simplicity of the sentences.
</bodyText>
<subsectionHeader confidence="0.999224">
3.2 Parsing
</subsectionHeader>
<bodyText confidence="0.999987357142857">
To parse the robot commands, a Probabilis-
tic Combinatory Categorial Grammar (PCCG)
(Kwiatkowski et al., 2010) is used. Regular CCGs
consist out of two sets: a lexicon of lexical items
and a set of operations. A lexical entry combines
a word or phrase with its meaning. This meaning
is represented by a category. A category captures
the syntactic as well as the the semantic informa-
tion of a word. A number of primitive symbols, a
subset of the part-of-speech tags, are used to rep-
resent the syntax. These primitive symbols can be
combined using specific operator symbols (/, \).
The semantics are represented by a A-expression.
Some example lexical entries are:
</bodyText>
<equation confidence="0.581920333333333">
blue ` ADJ : Ax.color(x, blue)
pyramid ` N : Ax.type(x, prism)
pickup ` S/NP : AyAx.action(x, take, y)
</equation>
<bodyText confidence="0.997151555555555">
The operator symbols can now be used to de-
termine how the categories can be combined using
operations. The operations that are used by the
CCG take one or two categories as input and re-
turn one category as output. These operations will
simultaneously address syntax and semantics. The
two most frequently used operations are the appli-
cation operations, i.e., forward (&gt;) and backward
(&lt;):
</bodyText>
<equation confidence="0.745377">
X/Y : f Y : g ⇒ X : f(g) (&gt;)
Y : g X\Y : f ⇒ X : f(g) (&lt;)
</equation>
<bodyText confidence="0.9993127">
The forward application takes as input a CCG
category with syntax X/Y and A-expression f
followed by a category with syntax Y and A-
expression g and returns a CGG category with syn-
tax X and A-expression f(g).
The operations will derive syntactic and seman-
tic information, while keeping track of the word
order that is encoded using the slash direction.
Another important operation deals with the def-
inite determiner in the A-expressions:
</bodyText>
<equation confidence="0.713612">
N : f ⇒ NP : det(f)
</equation>
<bodyText confidence="0.997392571428572">
This operation takes a single noun (N) category
as input and returns an noun phrase (NP) category
where the original A-expression is wrapped in a
determiner. A complete parsing example is shown
in Figure 2.
CCGs will usually have multiple possible parses
for a sentence given a certain lexicon for which it
</bodyText>
<page confidence="0.986343">
387
</page>
<figure confidence="0.964331882352941">
pyramid
N
Ax.type(x, prism)
Complete Partial Without context
Correct 71.29% 78.58% 57.76%
Wrong 11.66% 4.37% 27.72%
No result 17.05% 17.05% 14.52%
Take the
S/NP N/N
AzAy.action(y, take, z) Ax.x
&gt;
N
Ax.type(x, prism)
NP
det(Ax.type(x, prism))
S
Ay.action(y, take, det(Ax.type(x, prism)))
</figure>
<figureCaption confidence="0.996064">
Figure 2: A possible parse for the sentence “Take
the pyramid”.
</figureCaption>
<bodyText confidence="0.9998893">
is not possible to determine which of these is best.
To alleviate this problem, PCCGs have been intro-
duced (Kwiatkowski et al., 2010). PCCGs will re-
turn the most likely parse using a log-linear model
that contains a parameter vector 0, estimated us-
ing stochastic gradient updates. The joint proba-
bility of a A-calculus expression z and a parse y is
given by P(y, zlx; 0, A), with A being the entire
lexicon. The most likely A-calculus expression z
given a sentence x can then be found by:
</bodyText>
<equation confidence="0.99891">
f(x) = arg max P(zlx; 0, A)
Z
</equation>
<bodyText confidence="0.999945">
where the probability of z is equal to the sum of
the probabilities of all parses that produce z:
</bodyText>
<equation confidence="0.9963635">
P(zlx; 0, A) = � P(y, z|x; 0, A)
Y
</equation>
<bodyText confidence="0.9999443">
For training the PCCGs, the algorithm as de-
scribed by Kwiatkowski et al. (2010) was used. It
consists of two steps. In the first step the lexicon is
expanded with new lexical items. The second step
will update the parameters of the grammar using
stochastic gradient updates (LeCun et al., 1998).
All parameters are associated with a feature. The
system uses lexical features: for each item in the
lexicon a feature is added that fires when the item
is used.
</bodyText>
<subsectionHeader confidence="0.994831">
3.3 Verification
</subsectionHeader>
<bodyText confidence="0.999979222222222">
The parser will return multiple A-expressions,
each with an attached likelihood score. In the
verification step, these resulting expressions are
checked against the context. These A-expressions
are first transformed to RCL expressions3. Next,
the entities are extracted from the RCL expres-
sions and for each entity a corresponding object
is searched using a spatial planner, provided by
the task organizer. This spatial planner will, given
</bodyText>
<footnote confidence="0.990185666666667">
3Note that during pre- and postprocessing no information
is lost, as the mapping between A-calculus and RCL is a one-
to-one function.
</footnote>
<tableCaption confidence="0.997325">
Table 1: Results.
</tableCaption>
<bodyText confidence="0.999595333333333">
an entity description in RCL, return the objects in
the context that satisfy that description. RCL ex-
pressions with entities that have no corresponding
object in the context are discarded. From the re-
maining RCL expressions the one with the highest
likelihood is returned.
</bodyText>
<sectionHeader confidence="0.996524" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999968666666667">
The provided dataset for the task was crowd-
sourced using Train Robots, an online game in
which players were given before and after im-
ages of a scene and were asked to give the NL
command that the robot had executed (Dukes,
2013a). Each scene is a formal description of a
discrete 8x8x8 3D game board consisting of col-
ored blocks. The entire dataset consists of 3409
annotated examples, and was split in a training and
test set of 2500 and 909 sentences respectively.
The results are listed in Table 1. The first col-
umn (“Complete”) contains the results when the
resulting RCL expression is exactly the same as
the ground-truth RCL expression. Next to the full
matching scores, we also provide the scores for
partial matching of the RCL expressions (“Par-
tial”), based on the Parseval metric (Black et al.,
1991). Each RCL expression is scored between 0
and 1 according to the resemblance with the ex-
pected expression. The tree representations of the
RCL expressions are compared and the number of
correct nodes in the actual expression are divided
by the number of nodes in the tree of the expected
expression to calculate the score. A node is correct
if it is present at the same position in both trees and
if all children are correct.
The last column (“Without context”) contains
the results when using the parser without the veri-
fication step. This can be considered a baseline.
It may be clear that the use of contextual parsing
is advantageous when comparing the contextual
with the non-contextual setting, with an increase
of 13% in the number of correct results.
</bodyText>
<subsectionHeader confidence="0.958018">
Error Analysis
</subsectionHeader>
<bodyText confidence="0.831168">
When inspecting the wrong parses, it could be ob-
served that the wrong results were usually mini-
mally wrong. Either the value of a certain element
&gt;
</bodyText>
<page confidence="0.783013">
388
</page>
<figure confidence="0.918352666666667">
Expected Actual Occurrences
17
15
8
7
6
</figure>
<tableCaption confidence="0.991149">
Table 2: Wrong values.
</tableCaption>
<bodyText confidence="0.99954215625">
was wrong, an unnecessary element was added
to the expression or a required element was not
present in the resulting expression. This is also
clear when comparing the complete with the par-
tial match results, from which it can be seen that
66 sentences were only partially incorrect. Some
of the most commonly wrong values are listed in
Table 2. A final common reason for a wrong parse
was that a sequence of a take and a drop action
is considered as a single move action. There are
6 occurrences of this final case of which 5 would
result in the same end state.
One of the most common reasons that the parser
returned no result for a sentence, is because one
type of sentences was not present in the training
set. Sentences of the form “pick up red block. put
it on grey block” were completely absent from the
training data, but did appear 34 times in the test
set. Their structure is quite simple and should not
present a problem, but the parser was only trained
on sentences that combined the two actions with
an “and” connective. This is a problem because
the trained grammar is very dependent on the pro-
vided training data. Another difficult type of sen-
tences are the ones that contain measures. Only
17 of these were parsed correctly, while 70 had no
result and 3 were wrong.
Without considering the context, the combined
move &amp; drop action is not possible, since the con-
text is required to decide afterwards which specific
action has to be executed. 59 sentences (6.5%)
were wrong because a wrong action was selected.
</bodyText>
<sectionHeader confidence="0.998281" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.9948335">
In this paper we have presented an improved se-
mantic parsing approach for robot commands by
integrating spatial context. It consists of two steps.
First, the sentence is parsed using a Probabilis-
tic Combinatory Categorial Grammar. Next, the
parses are checked against the context. The re-
sulting parse is the one with the highest likeli-
hood that is valid given the context. This ap-
proach was evaluated on the SemEval-2014 Task
6 dataset. The results indicate that integrating
contextual knowledge is advantageous for parsing
spatial robot commands.
In future work, we will perform an in-depth
analysis of our system in comparison with the
other participating systems. Furthermore, we will
extend our approach to contexts that also contain
probabilistic facts, in order to be able to handle
noisy sensor data.
</bodyText>
<sectionHeader confidence="0.998304" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998396317073171">
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping
instructions to actions. Transactions of the ACL,
1:49–62.
Ezra W. Black, Steven P. Abney, Daniel P. Flickenger,
Claudia Gdaniec, Ralph Grishman, Philip Harri-
son, Donald Hindle, Robert J. P. Ingria, Freder-
ick Jelinek, Judith L. Klavans, Mark Y. Liberman,
Mitchell P. Marcus, Salim Roukos, Beatrice San-
torini, and Tomek Strzalkowski. 1991. A procedure
for quantitatively comparing the syntactic coverage
of English grammars. In HLT. Morgan Kaufmann.
Bob Carpenter. 1997. Type-Logical Semantics. The
MIT Press.
Kais Dukes. 2013a. Semantic Annotation of Robotic
Spatial Commands. In Language and Technology
Conference.
Kais Dukes. 2013b. Supervised semantic parsing of
robotic spatial commands. http://alt.qcri.
org/semeval2014/task6/.
Rohit J. Kate and Raymond J. Mooney. 2006. Us-
ing string-kernels for learning semantic parsers. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th An-
nual Meeting of the ACL, ACL-44, pages 913–920,
Stroudsburg, PA, USA. ACL.
Rohit J. Kate, Yuk Wah Wong, and Raymond J.
Mooney. 2005. Learning to transform natural to
formal languages. In Proceedings of the 20th Na-
tional Conference on Artificial Intelligence - Volume
3, AAAI’05, pages 1062–1068. AAAI Press.
Jayant Krishnamurthy and Thomas Kollar. 2013.
Jointly Learning to Parse and Perceive : Connecting
Natural Language to the Physical World. In Trans-
actions ofACL, volume 1, pages 193–206.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2010. Lexical general-
ization in ccg grammar induction for semantic pars-
ing. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
’11, pages 1512–1523, Stroudsburg, PA, USA. ACL.
</reference>
<figure confidence="0.7187811">
edge
above
right
left
within
region
within
left
front
above
</figure>
<page confidence="0.988657">
389
</page>
<reference confidence="0.999734714285714">
Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick
Haffner. 1998. Gradient-based learning applied to
document recognition. Proceedings of the IEEE,
86(11):2278–2324.
Heeyoung Lee, Yves Peirsman, Angel Chang,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2011. Stanford’s multi-pass sieve corefer-
ence resolution system at the CoNLL-2011 shared
task. In Proceedings of the CoNLL-11 Shared Task.
Heeyoung Lee, Angel Chang, Yves Peirsman,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2013. Deterministic coreference resolu-
tion based on entity-centric, precision-ranked rules.
Computational Linguistics, 39(4).
Raymond J. Mooney. 2007. Learning for semantic
parsing. In Alexander Gelbukh, editor, Computa-
tional Linguistics and Intelligent Text Processing,
volume 4394 of Lecture Notes in Computer Science,
pages 311–324. Springer Berlin Heidelberg.
Hoifung Poon. 2013. Grounded unsupervised seman-
tic parsing. In ACL (1), pages 933–943. ACL.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A multi-
pass sieve for coreference resolution. EMNLP-
2010, Boston, USA.
Marta Recasens, Marie-Catherine de Marneffe, and
Christopher Potts. 2013. The life and death of dis-
course entities: Identifying singleton mentions. In
Proceedings of NAACL 2013, pages 627–633. ACL.
Mark Steedman. 1996. Surface structure and inter-
pretation. Linguistic inquiry monographs. The MIT
Press, Cambridge, MA, USA.
Mark Steedman. 2000. The Syntactic Process. The
MIT Press, Cambridge, MA, USA.
</reference>
<page confidence="0.998275">
390
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.412879">
<title confidence="0.9970685">KUL-Eval: A Combinatory Categorial Grammar Approach Improving Semantic Parsing of Robot Commands using Spatial Context</title>
<author confidence="0.999793">Willem Mattelaer</author>
<author confidence="0.999793">Mathias Verbeke</author>
<author confidence="0.999793">Davide Nitti</author>
<affiliation confidence="0.999498">Department of Computer Science, KU Leuven, Belgium</affiliation>
<email confidence="0.703058333333333">willem.mattelaer@gmail.commathias.verbeke@cs.kuleuven.bedavide.nitti@cs.kuleuven.be</email>
<abstract confidence="0.993433176470588">When executing commands, a robot has a certain level of contextual knowledge about the environment in which it operates. Taking this knowledge into account can be beneficial to disambiguate commands with multiple interpretations. We present an approach that uses combinatory categorial grammars for improving the semantic parsing of robot commands that takes into account the spatial context of the robot. The results indicate a clear improvement over non-contextual semantic parsing. This work was done in the context of the SemEval-2014 task on supervised semantic parsing of spatial robot commands.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoav Artzi</author>
<author>Luke Zettlemoyer</author>
</authors>
<title>Weakly supervised learning of semantic parsers for mapping instructions to actions.</title>
<date>2013</date>
<journal>Transactions of the ACL,</journal>
<pages>1--49</pages>
<contexts>
<context position="5002" citStr="Artzi and Zettlemoyer (2013)" startWordPosition="782" endWordPosition="785">not require to take the context of the environment into account, as is the case when executing robot commands. SILT (Kate et al., 2005) uses transformation rules to translate the NL sentence to a query for the robot. This approach was extended to tailor support vector machines with string kernels (KRISP) (Kate and Mooney, 2006) and statistical machine learning (WASP) (Mooney, 2007). Also unsupervised approaches exist. Poon (2013) solves this lack of supervision by 1) inferring supervision using the target database, which constrains the search space, and 2) by using augmented dependency trees. Artzi and Zettlemoyer (2013) study the use of grounded CCG semantic parsing using weak supervision for interpreting navigational robot commands. Their approach is similar to ours, but instead of postprocessing the results in a verification step, the context (or state) is added to the training data. Krishnamurthy and Kollar (2013) use CCGs as a foundation, but match it to the context using an evaluation function. This evaluation function scores a denotation, i.e., the set of entity referents for the entire sentence, given a logical form and a knowledge base, which is considered as the context. 3 Methodology Our approach c</context>
<context position="6865" citStr="Artzi and Zettlemoyer, 2013" startWordPosition="1089" endWordPosition="1092">able is a reference to the object. The properties of an entity are defined by a conjunction of literals with two arguments. The predicate details the property that is being defined. An example entity, a blue cube, can be represented as Ax.color(x, blue), type(x, cube). A spatial relation between two entities is a literal with three arguments: the variable of the first entity, the type of relation and the second entity. The latter is given by its lambda term. This lambda term has to be wrapped in a definite determiner, det, that selects a single element from the set created by the lambda term (Artzi and Zettlemoyer, 2013). For example: the RCL expression (entity: (type: prism) (spatial-relation: (relation: above) (entity: (color: blue) (type: cube)))) is transformed to the A-calculus expression Ax.type(x,prism), relation(x, above, det(Ay.color(y, blue), type(y, cube))) Events are contained in one lambda term with one variable per event. There are three possible event predicates. The action predicate defines the action by detailing the action type and the object entity. The destination predicate will set the destination of the object2. Finally, the sequence predicate is necessary to detail the order of the even</context>
</contexts>
<marker>Artzi, Zettlemoyer, 2013</marker>
<rawString>Yoav Artzi and Luke Zettlemoyer. 2013. Weakly supervised learning of semantic parsers for mapping instructions to actions. Transactions of the ACL, 1:49–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ezra W Black</author>
<author>Steven P Abney</author>
<author>Daniel P Flickenger</author>
<author>Claudia Gdaniec</author>
<author>Ralph Grishman</author>
<author>Philip Harrison</author>
<author>Donald Hindle</author>
<author>Robert J P Ingria</author>
<author>Frederick Jelinek</author>
<author>Judith L Klavans</author>
<author>Mark Y Liberman</author>
<author>Mitchell P Marcus</author>
<author>Salim Roukos</author>
<author>Beatrice Santorini</author>
<author>Tomek Strzalkowski</author>
</authors>
<title>A procedure for quantitatively comparing the syntactic coverage of English grammars. In HLT.</title>
<date>1991</date>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="15177" citStr="Black et al., 1991" startWordPosition="2522" endWordPosition="2525"> the robot had executed (Dukes, 2013a). Each scene is a formal description of a discrete 8x8x8 3D game board consisting of colored blocks. The entire dataset consists of 3409 annotated examples, and was split in a training and test set of 2500 and 909 sentences respectively. The results are listed in Table 1. The first column (“Complete”) contains the results when the resulting RCL expression is exactly the same as the ground-truth RCL expression. Next to the full matching scores, we also provide the scores for partial matching of the RCL expressions (“Partial”), based on the Parseval metric (Black et al., 1991). Each RCL expression is scored between 0 and 1 according to the resemblance with the expected expression. The tree representations of the RCL expressions are compared and the number of correct nodes in the actual expression are divided by the number of nodes in the tree of the expected expression to calculate the score. A node is correct if it is present at the same position in both trees and if all children are correct. The last column (“Without context”) contains the results when using the parser without the verification step. This can be considered a baseline. It may be clear that the use </context>
</contexts>
<marker>Black, Abney, Flickenger, Gdaniec, Grishman, Harrison, Hindle, Ingria, Jelinek, Klavans, Liberman, Marcus, Roukos, Santorini, Strzalkowski, 1991</marker>
<rawString>Ezra W. Black, Steven P. Abney, Daniel P. Flickenger, Claudia Gdaniec, Ralph Grishman, Philip Harrison, Donald Hindle, Robert J. P. Ingria, Frederick Jelinek, Judith L. Klavans, Mark Y. Liberman, Mitchell P. Marcus, Salim Roukos, Beatrice Santorini, and Tomek Strzalkowski. 1991. A procedure for quantitatively comparing the syntactic coverage of English grammars. In HLT. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Carpenter</author>
</authors>
<title>Type-Logical Semantics.</title>
<date>1997</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="5836" citStr="Carpenter, 1997" startWordPosition="920" endWordPosition="922">the context (or state) is added to the training data. Krishnamurthy and Kollar (2013) use CCGs as a foundation, but match it to the context using an evaluation function. This evaluation function scores a denotation, i.e., the set of entity referents for the entire sentence, given a logical form and a knowledge base, which is considered as the context. 3 Methodology Our approach consists of two steps: a parse step and a verification step. Before these steps can be executed, a Combinatory Categorial Grammar needs to be trained. The training data for this grammar consists of typed A-expressions (Carpenter, 1997) that are annotated with their corresponding NL sentences. As the input data for the SemEval-2014 task consists of Robot Control Language (RCL) expressions (Dukes, 2013a)1, the data needs to be preprocessed first. 3.1 Preprocessing During preprocessing, the RCL expressions are transformed into equivalent A-expressions. In the A-expressions, each entity is represented by a lambda term where the variable is a reference to the object. The properties of an entity are defined by a conjunction of literals with two arguments. The predicate details the property that is being defined. An example entity</context>
</contexts>
<marker>Carpenter, 1997</marker>
<rawString>Bob Carpenter. 1997. Type-Logical Semantics. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kais Dukes</author>
</authors>
<title>Semantic Annotation of Robotic Spatial Commands.</title>
<date>2013</date>
<booktitle>In Language and Technology Conference.</booktitle>
<contexts>
<context position="2880" citStr="Dukes, 2013" startWordPosition="451" endWordPosition="452">ing on the gray cube. Humans will decide on the correct interpretation by taking into account the context. For instance, by looking at Figure 1, it is clear that the second interpretation is not possible, because there is no blue cube on top of a gray cube. However, there is a pyramid on top of a blue cube, making the first interpretation possible. The goal of this paper is to improve on non-contextual semantic parsing by tailoring the context to guide the parser. In this way, part of the ambiguity that causes multiple interpretations can be resolved. Figure 1: Possible situation (taken from (Dukes, 2013b)). Our approach consists of two steps. First, noncontextual semantic parsing using combinatory categorial grammars (CCG) (Steedman, 1996; Steedman, 2000) is performed on the sentence. This returns multiple possible parses, each with an attached likelihood of correctness. Subsequently, each parse is checked against the current context. The parse with the highest score that is possible given the current context is returned. 385 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 385–390, Dublin, Ireland, August 23-24, 2014. This paper is organized as foll</context>
<context position="6004" citStr="Dukes, 2013" startWordPosition="947" endWordPosition="948">s evaluation function scores a denotation, i.e., the set of entity referents for the entire sentence, given a logical form and a knowledge base, which is considered as the context. 3 Methodology Our approach consists of two steps: a parse step and a verification step. Before these steps can be executed, a Combinatory Categorial Grammar needs to be trained. The training data for this grammar consists of typed A-expressions (Carpenter, 1997) that are annotated with their corresponding NL sentences. As the input data for the SemEval-2014 task consists of Robot Control Language (RCL) expressions (Dukes, 2013a)1, the data needs to be preprocessed first. 3.1 Preprocessing During preprocessing, the RCL expressions are transformed into equivalent A-expressions. In the A-expressions, each entity is represented by a lambda term where the variable is a reference to the object. The properties of an entity are defined by a conjunction of literals with two arguments. The predicate details the property that is being defined. An example entity, a blue cube, can be represented as Ax.color(x, blue), type(x, cube). A spatial relation between two entities is a literal with three arguments: the variable of the fi</context>
<context position="14594" citStr="Dukes, 2013" startWordPosition="2425" endWordPosition="2426">ion is lost, as the mapping between A-calculus and RCL is a oneto-one function. Table 1: Results. an entity description in RCL, return the objects in the context that satisfy that description. RCL expressions with entities that have no corresponding object in the context are discarded. From the remaining RCL expressions the one with the highest likelihood is returned. 4 Evaluation The provided dataset for the task was crowdsourced using Train Robots, an online game in which players were given before and after images of a scene and were asked to give the NL command that the robot had executed (Dukes, 2013a). Each scene is a formal description of a discrete 8x8x8 3D game board consisting of colored blocks. The entire dataset consists of 3409 annotated examples, and was split in a training and test set of 2500 and 909 sentences respectively. The results are listed in Table 1. The first column (“Complete”) contains the results when the resulting RCL expression is exactly the same as the ground-truth RCL expression. Next to the full matching scores, we also provide the scores for partial matching of the RCL expressions (“Partial”), based on the Parseval metric (Black et al., 1991). Each RCL expres</context>
</contexts>
<marker>Dukes, 2013</marker>
<rawString>Kais Dukes. 2013a. Semantic Annotation of Robotic Spatial Commands. In Language and Technology Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kais Dukes</author>
</authors>
<title>Supervised semantic parsing of robotic spatial commands.</title>
<date>2013</date>
<note>http://alt.qcri. org/semeval2014/task6/.</note>
<contexts>
<context position="2880" citStr="Dukes, 2013" startWordPosition="451" endWordPosition="452">ing on the gray cube. Humans will decide on the correct interpretation by taking into account the context. For instance, by looking at Figure 1, it is clear that the second interpretation is not possible, because there is no blue cube on top of a gray cube. However, there is a pyramid on top of a blue cube, making the first interpretation possible. The goal of this paper is to improve on non-contextual semantic parsing by tailoring the context to guide the parser. In this way, part of the ambiguity that causes multiple interpretations can be resolved. Figure 1: Possible situation (taken from (Dukes, 2013b)). Our approach consists of two steps. First, noncontextual semantic parsing using combinatory categorial grammars (CCG) (Steedman, 1996; Steedman, 2000) is performed on the sentence. This returns multiple possible parses, each with an attached likelihood of correctness. Subsequently, each parse is checked against the current context. The parse with the highest score that is possible given the current context is returned. 385 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 385–390, Dublin, Ireland, August 23-24, 2014. This paper is organized as foll</context>
<context position="6004" citStr="Dukes, 2013" startWordPosition="947" endWordPosition="948">s evaluation function scores a denotation, i.e., the set of entity referents for the entire sentence, given a logical form and a knowledge base, which is considered as the context. 3 Methodology Our approach consists of two steps: a parse step and a verification step. Before these steps can be executed, a Combinatory Categorial Grammar needs to be trained. The training data for this grammar consists of typed A-expressions (Carpenter, 1997) that are annotated with their corresponding NL sentences. As the input data for the SemEval-2014 task consists of Robot Control Language (RCL) expressions (Dukes, 2013a)1, the data needs to be preprocessed first. 3.1 Preprocessing During preprocessing, the RCL expressions are transformed into equivalent A-expressions. In the A-expressions, each entity is represented by a lambda term where the variable is a reference to the object. The properties of an entity are defined by a conjunction of literals with two arguments. The predicate details the property that is being defined. An example entity, a blue cube, can be represented as Ax.color(x, blue), type(x, cube). A spatial relation between two entities is a literal with three arguments: the variable of the fi</context>
<context position="14594" citStr="Dukes, 2013" startWordPosition="2425" endWordPosition="2426">ion is lost, as the mapping between A-calculus and RCL is a oneto-one function. Table 1: Results. an entity description in RCL, return the objects in the context that satisfy that description. RCL expressions with entities that have no corresponding object in the context are discarded. From the remaining RCL expressions the one with the highest likelihood is returned. 4 Evaluation The provided dataset for the task was crowdsourced using Train Robots, an online game in which players were given before and after images of a scene and were asked to give the NL command that the robot had executed (Dukes, 2013a). Each scene is a formal description of a discrete 8x8x8 3D game board consisting of colored blocks. The entire dataset consists of 3409 annotated examples, and was split in a training and test set of 2500 and 909 sentences respectively. The results are listed in Table 1. The first column (“Complete”) contains the results when the resulting RCL expression is exactly the same as the ground-truth RCL expression. Next to the full matching scores, we also provide the scores for partial matching of the RCL expressions (“Partial”), based on the Parseval metric (Black et al., 1991). Each RCL expres</context>
</contexts>
<marker>Dukes, 2013</marker>
<rawString>Kais Dukes. 2013b. Supervised semantic parsing of robotic spatial commands. http://alt.qcri. org/semeval2014/task6/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohit J Kate</author>
<author>Raymond J Mooney</author>
</authors>
<title>Using string-kernels for learning semantic parsers.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the ACL, ACL-44,</booktitle>
<pages>913--920</pages>
<publisher>ACL.</publisher>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4703" citStr="Kate and Mooney, 2006" startWordPosition="736" endWordPosition="739">o improve the parsing. Our approach is inspired by the work of Kwiatkowski et al. (2010). The authors present a supervised CCG approach to parse queries to a geographical information database and a flightbooking system. This differs from the current setting in that the database querying does not require to take the context of the environment into account, as is the case when executing robot commands. SILT (Kate et al., 2005) uses transformation rules to translate the NL sentence to a query for the robot. This approach was extended to tailor support vector machines with string kernels (KRISP) (Kate and Mooney, 2006) and statistical machine learning (WASP) (Mooney, 2007). Also unsupervised approaches exist. Poon (2013) solves this lack of supervision by 1) inferring supervision using the target database, which constrains the search space, and 2) by using augmented dependency trees. Artzi and Zettlemoyer (2013) study the use of grounded CCG semantic parsing using weak supervision for interpreting navigational robot commands. Their approach is similar to ours, but instead of postprocessing the results in a verification step, the context (or state) is added to the training data. Krishnamurthy and Kollar (201</context>
</contexts>
<marker>Kate, Mooney, 2006</marker>
<rawString>Rohit J. Kate and Raymond J. Mooney. 2006. Using string-kernels for learning semantic parsers. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the ACL, ACL-44, pages 913–920, Stroudsburg, PA, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohit J Kate</author>
<author>Yuk Wah Wong</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to transform natural to formal languages.</title>
<date>2005</date>
<booktitle>In Proceedings of the 20th National Conference on Artificial Intelligence - Volume 3, AAAI’05,</booktitle>
<pages>1062--1068</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="4509" citStr="Kate et al., 2005" startWordPosition="703" endWordPosition="706">c parsers. We will first review approaches that translate NL sentences into a formal representation without taking context into account, followed by related techniques that use the context to improve the parsing. Our approach is inspired by the work of Kwiatkowski et al. (2010). The authors present a supervised CCG approach to parse queries to a geographical information database and a flightbooking system. This differs from the current setting in that the database querying does not require to take the context of the environment into account, as is the case when executing robot commands. SILT (Kate et al., 2005) uses transformation rules to translate the NL sentence to a query for the robot. This approach was extended to tailor support vector machines with string kernels (KRISP) (Kate and Mooney, 2006) and statistical machine learning (WASP) (Mooney, 2007). Also unsupervised approaches exist. Poon (2013) solves this lack of supervision by 1) inferring supervision using the target database, which constrains the search space, and 2) by using augmented dependency trees. Artzi and Zettlemoyer (2013) study the use of grounded CCG semantic parsing using weak supervision for interpreting navigational robot </context>
</contexts>
<marker>Kate, Wong, Mooney, 2005</marker>
<rawString>Rohit J. Kate, Yuk Wah Wong, and Raymond J. Mooney. 2005. Learning to transform natural to formal languages. In Proceedings of the 20th National Conference on Artificial Intelligence - Volume 3, AAAI’05, pages 1062–1068. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jayant Krishnamurthy</author>
<author>Thomas Kollar</author>
</authors>
<title>Jointly Learning to Parse and Perceive : Connecting Natural Language to the Physical World.</title>
<date>2013</date>
<booktitle>In Transactions ofACL,</booktitle>
<volume>1</volume>
<pages>193--206</pages>
<contexts>
<context position="5305" citStr="Krishnamurthy and Kollar (2013)" startWordPosition="831" endWordPosition="834">RISP) (Kate and Mooney, 2006) and statistical machine learning (WASP) (Mooney, 2007). Also unsupervised approaches exist. Poon (2013) solves this lack of supervision by 1) inferring supervision using the target database, which constrains the search space, and 2) by using augmented dependency trees. Artzi and Zettlemoyer (2013) study the use of grounded CCG semantic parsing using weak supervision for interpreting navigational robot commands. Their approach is similar to ours, but instead of postprocessing the results in a verification step, the context (or state) is added to the training data. Krishnamurthy and Kollar (2013) use CCGs as a foundation, but match it to the context using an evaluation function. This evaluation function scores a denotation, i.e., the set of entity referents for the entire sentence, given a logical form and a knowledge base, which is considered as the context. 3 Methodology Our approach consists of two steps: a parse step and a verification step. Before these steps can be executed, a Combinatory Categorial Grammar needs to be trained. The training data for this grammar consists of typed A-expressions (Carpenter, 1997) that are annotated with their corresponding NL sentences. As the inp</context>
</contexts>
<marker>Krishnamurthy, Kollar, 2013</marker>
<rawString>Jayant Krishnamurthy and Thomas Kollar. 2013. Jointly Learning to Parse and Perceive : Connecting Natural Language to the Physical World. In Transactions ofACL, volume 1, pages 193–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Kwiatkowski</author>
<author>Luke Zettlemoyer</author>
<author>Sharon Goldwater</author>
<author>Mark Steedman</author>
</authors>
<title>Lexical generalization in ccg grammar induction for semantic parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>1512--1523</pages>
<publisher>ACL.</publisher>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4169" citStr="Kwiatkowski et al. (2010)" startWordPosition="644" endWordPosition="647">d description of our approach in Section 3. In Section 4, the approach is evaluated and compared to non-contextual parsing. Finally, in Section 5 we conclude and outline directions for future work. The software is available from https:// github.com/wmattelaer/Thesis. 2 Related Work There is a significant body of previous work on learning semantic parsers. We will first review approaches that translate NL sentences into a formal representation without taking context into account, followed by related techniques that use the context to improve the parsing. Our approach is inspired by the work of Kwiatkowski et al. (2010). The authors present a supervised CCG approach to parse queries to a geographical information database and a flightbooking system. This differs from the current setting in that the database querying does not require to take the context of the environment into account, as is the case when executing robot commands. SILT (Kate et al., 2005) uses transformation rules to translate the NL sentence to a query for the robot. This approach was extended to tailor support vector machines with string kernels (KRISP) (Kate and Mooney, 2006) and statistical machine learning (WASP) (Mooney, 2007). Also unsu</context>
<context position="10141" citStr="Kwiatkowski et al., 2010" startWordPosition="1651" endWordPosition="1654">nd place it above the gray one is transformed to the sentence Pick the blue block [1] and place it (1) above the gray one {1}. The anaphoric references are found using the coreference resolution system of Stanford CoreNLP (Recasens et al., 2013; Lee et al., 2013; Lee et al., 2011; Raghunathan et al., 2010). However, it is not capable of finding references that use one. This can be solved by letting the one always refer to the first entity of the sentence, because of the simplicity of the sentences. 3.2 Parsing To parse the robot commands, a Probabilistic Combinatory Categorial Grammar (PCCG) (Kwiatkowski et al., 2010) is used. Regular CCGs consist out of two sets: a lexicon of lexical items and a set of operations. A lexical entry combines a word or phrase with its meaning. This meaning is represented by a category. A category captures the syntactic as well as the the semantic information of a word. A number of primitive symbols, a subset of the part-of-speech tags, are used to represent the syntax. These primitive symbols can be combined using specific operator symbols (/, \). The semantics are represented by a A-expression. Some example lexical entries are: blue ` ADJ : Ax.color(x, blue) pyramid ` N : Ax</context>
<context position="12496" citStr="Kwiatkowski et al., 2010" startWordPosition="2060" endWordPosition="2063">arsing example is shown in Figure 2. CCGs will usually have multiple possible parses for a sentence given a certain lexicon for which it 387 pyramid N Ax.type(x, prism) Complete Partial Without context Correct 71.29% 78.58% 57.76% Wrong 11.66% 4.37% 27.72% No result 17.05% 17.05% 14.52% Take the S/NP N/N AzAy.action(y, take, z) Ax.x &gt; N Ax.type(x, prism) NP det(Ax.type(x, prism)) S Ay.action(y, take, det(Ax.type(x, prism))) Figure 2: A possible parse for the sentence “Take the pyramid”. is not possible to determine which of these is best. To alleviate this problem, PCCGs have been introduced (Kwiatkowski et al., 2010). PCCGs will return the most likely parse using a log-linear model that contains a parameter vector 0, estimated using stochastic gradient updates. The joint probability of a A-calculus expression z and a parse y is given by P(y, zlx; 0, A), with A being the entire lexicon. The most likely A-calculus expression z given a sentence x can then be found by: f(x) = arg max P(zlx; 0, A) Z where the probability of z is equal to the sum of the probabilities of all parses that produce z: P(zlx; 0, A) = � P(y, z|x; 0, A) Y For training the PCCGs, the algorithm as described by Kwiatkowski et al. (2010) w</context>
</contexts>
<marker>Kwiatkowski, Zettlemoyer, Goldwater, Steedman, 2010</marker>
<rawString>Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwater, and Mark Steedman. 2010. Lexical generalization in ccg grammar induction for semantic parsing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 1512–1523, Stroudsburg, PA, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yann LeCun</author>
<author>L´eon Bottou</author>
<author>Yoshua Bengio</author>
<author>Patrick Haffner</author>
</authors>
<title>Gradient-based learning applied to document recognition.</title>
<date>1998</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<pages>86--11</pages>
<contexts>
<context position="13309" citStr="LeCun et al., 1998" startWordPosition="2213" endWordPosition="2216">on z and a parse y is given by P(y, zlx; 0, A), with A being the entire lexicon. The most likely A-calculus expression z given a sentence x can then be found by: f(x) = arg max P(zlx; 0, A) Z where the probability of z is equal to the sum of the probabilities of all parses that produce z: P(zlx; 0, A) = � P(y, z|x; 0, A) Y For training the PCCGs, the algorithm as described by Kwiatkowski et al. (2010) was used. It consists of two steps. In the first step the lexicon is expanded with new lexical items. The second step will update the parameters of the grammar using stochastic gradient updates (LeCun et al., 1998). All parameters are associated with a feature. The system uses lexical features: for each item in the lexicon a feature is added that fires when the item is used. 3.3 Verification The parser will return multiple A-expressions, each with an attached likelihood score. In the verification step, these resulting expressions are checked against the context. These A-expressions are first transformed to RCL expressions3. Next, the entities are extracted from the RCL expressions and for each entity a corresponding object is searched using a spatial planner, provided by the task organizer. This spatial</context>
</contexts>
<marker>LeCun, Bottou, Bengio, Haffner, 1998</marker>
<rawString>Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. 1998. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Yves Peirsman</author>
<author>Angel Chang</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Stanford’s multi-pass sieve coreference resolution system at the CoNLL-2011 shared task.</title>
<date>2011</date>
<booktitle>In Proceedings of the CoNLL-11 Shared Task.</booktitle>
<contexts>
<context position="9796" citStr="Lee et al., 2011" startWordPosition="1593" endWordPosition="1596">d that refers to another word, e.g., it. Finally, {1} marks a word that refers to the type of an earlier entity, e.g., one. The numbers in these markers can increase if there are different references in one sentence, but the sentences of this dataset do not contain different references. For instance, the sentence Pick the blue block and place it above the gray one is transformed to the sentence Pick the blue block [1] and place it (1) above the gray one {1}. The anaphoric references are found using the coreference resolution system of Stanford CoreNLP (Recasens et al., 2013; Lee et al., 2013; Lee et al., 2011; Raghunathan et al., 2010). However, it is not capable of finding references that use one. This can be solved by letting the one always refer to the first entity of the sentence, because of the simplicity of the sentences. 3.2 Parsing To parse the robot commands, a Probabilistic Combinatory Categorial Grammar (PCCG) (Kwiatkowski et al., 2010) is used. Regular CCGs consist out of two sets: a lexicon of lexical items and a set of operations. A lexical entry combines a word or phrase with its meaning. This meaning is represented by a category. A category captures the syntactic as well as the the</context>
</contexts>
<marker>Lee, Peirsman, Chang, Chambers, Surdeanu, Jurafsky, 2011</marker>
<rawString>Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011. Stanford’s multi-pass sieve coreference resolution system at the CoNLL-2011 shared task. In Proceedings of the CoNLL-11 Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Angel Chang</author>
<author>Yves Peirsman</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Deterministic coreference resolution based on entity-centric, precision-ranked rules.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>4</issue>
<contexts>
<context position="9778" citStr="Lee et al., 2013" startWordPosition="1589" endWordPosition="1592">ed to detail a word that refers to another word, e.g., it. Finally, {1} marks a word that refers to the type of an earlier entity, e.g., one. The numbers in these markers can increase if there are different references in one sentence, but the sentences of this dataset do not contain different references. For instance, the sentence Pick the blue block and place it above the gray one is transformed to the sentence Pick the blue block [1] and place it (1) above the gray one {1}. The anaphoric references are found using the coreference resolution system of Stanford CoreNLP (Recasens et al., 2013; Lee et al., 2013; Lee et al., 2011; Raghunathan et al., 2010). However, it is not capable of finding references that use one. This can be solved by letting the one always refer to the first entity of the sentence, because of the simplicity of the sentences. 3.2 Parsing To parse the robot commands, a Probabilistic Combinatory Categorial Grammar (PCCG) (Kwiatkowski et al., 2010) is used. Regular CCGs consist out of two sets: a lexicon of lexical items and a set of operations. A lexical entry combines a word or phrase with its meaning. This meaning is represented by a category. A category captures the syntactic </context>
</contexts>
<marker>Lee, Chang, Peirsman, Chambers, Surdeanu, Jurafsky, 2013</marker>
<rawString>Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2013. Deterministic coreference resolution based on entity-centric, precision-ranked rules. Computational Linguistics, 39(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raymond J Mooney</author>
</authors>
<title>Learning for semantic parsing.</title>
<date>2007</date>
<booktitle>Computational Linguistics and Intelligent Text Processing,</booktitle>
<volume>4394</volume>
<pages>311--324</pages>
<editor>In Alexander Gelbukh, editor,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="4758" citStr="Mooney, 2007" startWordPosition="746" endWordPosition="747">wiatkowski et al. (2010). The authors present a supervised CCG approach to parse queries to a geographical information database and a flightbooking system. This differs from the current setting in that the database querying does not require to take the context of the environment into account, as is the case when executing robot commands. SILT (Kate et al., 2005) uses transformation rules to translate the NL sentence to a query for the robot. This approach was extended to tailor support vector machines with string kernels (KRISP) (Kate and Mooney, 2006) and statistical machine learning (WASP) (Mooney, 2007). Also unsupervised approaches exist. Poon (2013) solves this lack of supervision by 1) inferring supervision using the target database, which constrains the search space, and 2) by using augmented dependency trees. Artzi and Zettlemoyer (2013) study the use of grounded CCG semantic parsing using weak supervision for interpreting navigational robot commands. Their approach is similar to ours, but instead of postprocessing the results in a verification step, the context (or state) is added to the training data. Krishnamurthy and Kollar (2013) use CCGs as a foundation, but match it to the contex</context>
</contexts>
<marker>Mooney, 2007</marker>
<rawString>Raymond J. Mooney. 2007. Learning for semantic parsing. In Alexander Gelbukh, editor, Computational Linguistics and Intelligent Text Processing, volume 4394 of Lecture Notes in Computer Science, pages 311–324. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
</authors>
<title>Grounded unsupervised semantic parsing.</title>
<date>2013</date>
<journal>In ACL</journal>
<volume>1</volume>
<pages>933--943</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="4807" citStr="Poon (2013)" startWordPosition="752" endWordPosition="753">ervised CCG approach to parse queries to a geographical information database and a flightbooking system. This differs from the current setting in that the database querying does not require to take the context of the environment into account, as is the case when executing robot commands. SILT (Kate et al., 2005) uses transformation rules to translate the NL sentence to a query for the robot. This approach was extended to tailor support vector machines with string kernels (KRISP) (Kate and Mooney, 2006) and statistical machine learning (WASP) (Mooney, 2007). Also unsupervised approaches exist. Poon (2013) solves this lack of supervision by 1) inferring supervision using the target database, which constrains the search space, and 2) by using augmented dependency trees. Artzi and Zettlemoyer (2013) study the use of grounded CCG semantic parsing using weak supervision for interpreting navigational robot commands. Their approach is similar to ours, but instead of postprocessing the results in a verification step, the context (or state) is added to the training data. Krishnamurthy and Kollar (2013) use CCGs as a foundation, but match it to the context using an evaluation function. This evaluation f</context>
</contexts>
<marker>Poon, 2013</marker>
<rawString>Hoifung Poon. 2013. Grounded unsupervised semantic parsing. In ACL (1), pages 933–943. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karthik Raghunathan</author>
<author>Heeyoung Lee</author>
<author>Sudarshan Rangarajan</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
<author>Christopher Manning</author>
</authors>
<title>A multipass sieve for coreference resolution. EMNLP2010,</title>
<date>2010</date>
<location>Boston, USA.</location>
<contexts>
<context position="9823" citStr="Raghunathan et al., 2010" startWordPosition="1597" endWordPosition="1600">nother word, e.g., it. Finally, {1} marks a word that refers to the type of an earlier entity, e.g., one. The numbers in these markers can increase if there are different references in one sentence, but the sentences of this dataset do not contain different references. For instance, the sentence Pick the blue block and place it above the gray one is transformed to the sentence Pick the blue block [1] and place it (1) above the gray one {1}. The anaphoric references are found using the coreference resolution system of Stanford CoreNLP (Recasens et al., 2013; Lee et al., 2013; Lee et al., 2011; Raghunathan et al., 2010). However, it is not capable of finding references that use one. This can be solved by letting the one always refer to the first entity of the sentence, because of the simplicity of the sentences. 3.2 Parsing To parse the robot commands, a Probabilistic Combinatory Categorial Grammar (PCCG) (Kwiatkowski et al., 2010) is used. Regular CCGs consist out of two sets: a lexicon of lexical items and a set of operations. A lexical entry combines a word or phrase with its meaning. This meaning is represented by a category. A category captures the syntactic as well as the the semantic information of a </context>
</contexts>
<marker>Raghunathan, Lee, Rangarajan, Chambers, Surdeanu, Jurafsky, Manning, 2010</marker>
<rawString>Karthik Raghunathan, Heeyoung Lee, Sudarshan Rangarajan, Nathanael Chambers, Mihai Surdeanu, Dan Jurafsky, and Christopher Manning. 2010. A multipass sieve for coreference resolution. EMNLP2010, Boston, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Recasens</author>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher Potts</author>
</authors>
<title>The life and death of discourse entities: Identifying singleton mentions.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL 2013,</booktitle>
<pages>627--633</pages>
<publisher>ACL.</publisher>
<marker>Recasens, de Marneffe, Potts, 2013</marker>
<rawString>Marta Recasens, Marie-Catherine de Marneffe, and Christopher Potts. 2013. The life and death of discourse entities: Identifying singleton mentions. In Proceedings of NAACL 2013, pages 627–633. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>Surface structure and interpretation. Linguistic inquiry monographs.</title>
<date>1996</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="3018" citStr="Steedman, 1996" startWordPosition="469" endWordPosition="470">Figure 1, it is clear that the second interpretation is not possible, because there is no blue cube on top of a gray cube. However, there is a pyramid on top of a blue cube, making the first interpretation possible. The goal of this paper is to improve on non-contextual semantic parsing by tailoring the context to guide the parser. In this way, part of the ambiguity that causes multiple interpretations can be resolved. Figure 1: Possible situation (taken from (Dukes, 2013b)). Our approach consists of two steps. First, noncontextual semantic parsing using combinatory categorial grammars (CCG) (Steedman, 1996; Steedman, 2000) is performed on the sentence. This returns multiple possible parses, each with an attached likelihood of correctness. Subsequently, each parse is checked against the current context. The parse with the highest score that is possible given the current context is returned. 385 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 385–390, Dublin, Ireland, August 23-24, 2014. This paper is organized as follows. In Section 2 we discuss related work, followed by a detailed description of our approach in Section 3. In Section 4, the approach is </context>
</contexts>
<marker>Steedman, 1996</marker>
<rawString>Mark Steedman. 1996. Surface structure and interpretation. Linguistic inquiry monographs. The MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="3035" citStr="Steedman, 2000" startWordPosition="471" endWordPosition="472">clear that the second interpretation is not possible, because there is no blue cube on top of a gray cube. However, there is a pyramid on top of a blue cube, making the first interpretation possible. The goal of this paper is to improve on non-contextual semantic parsing by tailoring the context to guide the parser. In this way, part of the ambiguity that causes multiple interpretations can be resolved. Figure 1: Possible situation (taken from (Dukes, 2013b)). Our approach consists of two steps. First, noncontextual semantic parsing using combinatory categorial grammars (CCG) (Steedman, 1996; Steedman, 2000) is performed on the sentence. This returns multiple possible parses, each with an attached likelihood of correctness. Subsequently, each parse is checked against the current context. The parse with the highest score that is possible given the current context is returned. 385 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 385–390, Dublin, Ireland, August 23-24, 2014. This paper is organized as follows. In Section 2 we discuss related work, followed by a detailed description of our approach in Section 3. In Section 4, the approach is evaluated and com</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The Syntactic Process. The MIT Press, Cambridge, MA, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>