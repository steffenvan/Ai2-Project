<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000014">
<title confidence="0.967248">
Nonparametric Method for Data-driven Image Captioning
</title>
<note confidence="0.449569">
Rebecca Mason and Eugene Charniak
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University, Providence, RI 02912
</note>
<email confidence="0.990517">
{rebecca,ec}@cs.brown.edu
</email>
<sectionHeader confidence="0.997287" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999866130434783">
We present a nonparametric density esti-
mation technique for image caption gener-
ation. Data-driven matching methods have
shown to be effective for a variety of com-
plex problems in Computer Vision. These
methods reduce an inference problem for
an unknown image to finding an exist-
ing labeled image which is semantically
similar. However, related approaches for
image caption generation (Ordonez et al.,
2011; Kuznetsova et al., 2012) are ham-
pered by noisy estimations of visual con-
tent and poor alignment between images
and human-written captions. Our work
addresses this challenge by estimating a
word frequency representation of the vi-
sual content of a query image. This al-
lows us to cast caption generation as an
extractive summarization problem. Our
model strongly outperforms two state-of-
the-art caption extraction systems accord-
ing to human judgments of caption rele-
vance.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999862785714286">
Automatic image captioning is a much studied
topic in both the Natural Language Processing
(NLP) and Computer Vision (CV) areas of re-
search. The task is to identify the visual content
of the input image, and to output a relevant natural
language caption.
Much prior work treats image captioning as
a retrieval problem (see Section 2). These ap-
proaches use CV algorithms to retrieve similar im-
ages from a large database of captioned images,
and then transfer text from the captions of those
images to the query image. This is a challenging
problem for two main reasons. First, visual simi-
larity measures do not perform reliably and do not
</bodyText>
<listItem confidence="0.950834714285714">
Query Image: Captioned Images:
1.) 3 month old baby girl with blue eyes in her crib
2.) A photo from the Ismail’s portrait shoot
3.) A portrait of a man, in black and white
4.) Portrait in black and white with the red rose
5.) I apparently had this saved in black and white as well
6.) Portrait in black and white
</listItem>
<tableCaption confidence="0.882767">
Table 1: Example of a query image from the SBU-
</tableCaption>
<bodyText confidence="0.988597380952381">
Flickr dataset (Ordonez et al., 2011), along with
scene-based estimates of visually similar images.
Our system models visual content using words that
are frequent in these captions (highlighted) and ex-
tracts a single output caption.
capture all of the relevant details which humans
might describe. Second, image captions collected
from the web often contain contextual or back-
ground information which is not visually relevant
to the image being described.
In this paper, we propose a system for transfer-
based image captioning which is designed to ad-
dress these challenges. Instead of selecting an out-
put caption according to a single noisy estimate
of visual similarity, our system uses a word fre-
quency model to find a smoothed estimate of vi-
sual content across multiple captions, as Table 1
illustrates. It then generates a description of the
query image by extracting the caption which best
represents the mutually shared content.
The contributions of this paper are as follows:
</bodyText>
<equation confidence="0.7671095">
4. 5. 6.
1. 2. 3.
</equation>
<page confidence="0.936554">
592
</page>
<bodyText confidence="0.246547">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 592–598,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</bodyText>
<listItem confidence="0.994901588235294">
1. Our caption generation system effectively lever-
ages information from the massive amounts of
human-written image captions on the internet. In
particular, it exhibits strong performance on the
SBU-Flickr dataset (Ordonez et al., 2011), a noisy
corpus of one million captioned images collected
from the web. We achieve a remarkable 34%
improvement in human relevance scores over a
recent state-of-the-art image captioning system
(Kuznetsova et al., 2012), and 48% improvement
over a scene-based retrieval system (Patterson et
al., 2014) using the same computed image fea-
tures.
2. Our approach uses simple models which can
be easily reproduced by both CV and NLP re-
searchers. We provide resources to enable com-
parison against future systems.1
</listItem>
<sectionHeader confidence="0.804236" genericHeader="method">
2 Image Captioning by Transfer
</sectionHeader>
<bodyText confidence="0.999980285714286">
The IM2TEXT model by Ordonez et al. (2011)
presents the first web-scale approach to image cap-
tion generation. IM2TEXT retrieves the image
which is the closest visual match to the query im-
age, and transfers its description to the query im-
age. The COLLECTIVE model by Kuznetsova et
al. (2012) is a related approach which uses trained
CV recognition systems to detect a variety of vi-
sual entities in the query image. A separate de-
scription is retrieved for each visual entity, which
are then fused into a single output caption. Like
IM2TEXT, their approach uses visual similarity as
a proxy for textual relevance.
Other related work models the text more di-
rectly, but is more restrictive about the source
and quality of the human-written training data.
Farhadi et al. (2010) and Hodosh et al. (2013)
learn joint representations for images and cap-
tions, but can only be trained on data with very
strong alignment between images and descriptions
(i.e. captions written by Mechanical Turkers). An-
other line of related work (Fan et al., 2010; Aker
and Gaizauskas, 2010; Feng and Lapata, 2010)
generates captions by extracting sentences from
documents which are related to the query image.
These approaches are tailored toward specific do-
mains, such as travel and news, where images tend
to appear with corresponding text.
</bodyText>
<footnote confidence="0.955703666666667">
1See http://bllip.cs.brown.edu/
download/captioning_resources.zip or ACL
Anthology.
</footnote>
<sectionHeader confidence="0.995364" genericHeader="method">
3 Dataset
</sectionHeader>
<bodyText confidence="0.999788428571429">
In this paper, we use the SBU-Flickr dataset2. Or-
donez et al. (2011) query Flickr.com using a
huge number of words which describe visual en-
tities, in order to build a corpus of one million
images with captions which refer to image con-
tent. However, further analysis by Hodosh et al.
(2013) shows that many captions in SBU-Flickr
(-67%) describe information that cannot be ob-
tained from the image itself, while a substantial
fraction (-23%) contain almost no visually rel-
evant information. Nevertheless, this dataset is
the only web-scale collection of captioned images,
and has enabled notable research in both CV and
NLP.3
</bodyText>
<sectionHeader confidence="0.984398" genericHeader="method">
4 Our Approach
</sectionHeader>
<subsectionHeader confidence="0.958738">
4.1 Overview
</subsectionHeader>
<bodyText confidence="0.999968777777778">
For a query image IQ, our task is to generate a rele-
vant description by selecting a single caption from
C, a large dataset of images with human-written
captions. In this section, we first define the feature
space for visual similarity, then formulate a den-
sity estimation problem with the aim of modeling
the words which are used to describe visually sim-
ilar images to IQ. We also explore methods for
extractive caption generation.
</bodyText>
<subsectionHeader confidence="0.999325">
4.2 Measuring Visual Similarity
</subsectionHeader>
<bodyText confidence="0.999886">
Data-driven matching methods have shown to be
very effective for a variety of challenging prob-
lems (Hays and Efros, 2008; Makadia et al.,
2008; Tighe and Lazebnik, 2010). Typically these
methods compute global (scene-based) descriptors
rather than object and entity detections. Scene-
based techniques in CV are generally more robust,
and can be computed more efficiently on large
datasets.
The basic IM2TEXT model uses an equally
weighted average of GIST (Oliva and Torralba,
2001) and TinyImage (Torralba et al., 2008) fea-
tures, which coarsely localize low-level features
in scenes. The output is a multi-dimensional
image space where semantically similar scenes
(e.g. streets, beaches, highways) are projected
near each other.
</bodyText>
<footnote confidence="0.9971566">
2http://tamaraberg.com/CLSP11/
3In particular, papers stemming from the 2011 JHU-CLSP
Summer Workshop (Berg et al., 2012; Dodge et al., 2012;
Mitchell et al., 2012) and more recently, the best paper award
winner at ICCV (Ordonez et al., 2013).
</footnote>
<page confidence="0.998276">
593
</page>
<bodyText confidence="0.999974666666667">
Patterson and Hays (2012) present “scene at-
tribute” representations which are characterized
using low-level perceptual attributes as used by
GIST (e.g. openness, ruggedness, naturalness),
as well as high-level attributes informed by open-
ended crowd-sourced image descriptions (e.g., in-
door lighting, running water, places for learning).
Follow-up work (Patterson et al., 2014) shows
that their attributes provide improved matching for
image captioning over IM2TEXT baseline. We
use their publicly available4 scene attributes for
our experiments. Training set and query images
are represented using 102-dimensional real-valued
vectors, and similarity between images is mea-
sured using the Euclidean distance.
</bodyText>
<subsectionHeader confidence="0.997865">
4.3 Density Estimation
</subsectionHeader>
<bodyText confidence="0.999969">
As shown in Bishop (2006), probability density
estimates at a particular point can be obtained by
considering points in the training data within some
local neighborhood. In our case, we define some
region R in the image space which contains Iq.
The probability mass of that space is
</bodyText>
<equation confidence="0.999531">
P = Lp(Iq)dIq (1)
</equation>
<bodyText confidence="0.994183666666667">
and if we assume that R is small enough such that
p(Iq) is roughly constant in R, we can approxi-
mate
</bodyText>
<equation confidence="0.858997333333333">
kimg
p(Iq) ≈ (2)
nimgV img
</equation>
<bodyText confidence="0.999894909090909">
where kimg is the number of images within R in
the training data, nimg is the total number of im-
ages in the training data, and Vimg is the volume
of R. In this paper, we fix kimg to a constant value,
so that Vimg is determined by the training data
around the query image.5
At this point, we extend the density estima-
tion technique in order to estimate a smoothed
model of descriptive text. Let us begin by consid-
ering p(w|Iq), the conditional probability of the
word6 w given Iq. This can be described using a
</bodyText>
<footnote confidence="0.949041166666667">
4https://github.com/genp/sun_
attributes
5As an alternate approach, one could fix the value of
V img and determine kimg from the number of points in R,
giving rise to the kernel density approach (a.k.a. Parzen
windows). However we believe the KNN approach is more
appropriate here, because the number of samples is nearly
10000 times greater than the number of dimensions in the
image representation.
6Here, we use word to refer to non-function words, and
assume all function words have been removed from the cap-
tions.
</footnote>
<equation confidence="0.903197333333333">
Bayesian model:
p(w|Iq) = p(Iq|w))(w) (3)
p(Iq
</equation>
<bodyText confidence="0.99473">
The prior for w is simply its unigram frequency in
C, where ntxt
</bodyText>
<equation confidence="0.9047302">
w and ntxt are word token counts:
ntxt
w
p(w) = (4)
ntxt
</equation>
<bodyText confidence="0.999775333333333">
Note that ntxt is not the same as nimg because a
single captioned image can have multiple words
in its caption. Likewise, the conditional density
</bodyText>
<equation confidence="0.9566015">
ktxt
p(Iq |w) ≈w(5)
ntxtVimg (5)
w
</equation>
<bodyText confidence="0.927194111111111">
considers instances of observed words within R,
although the volume of R is still defined by the
image space. ktxt
w is the number of times w is used
within R while ntxt
w is the total number of times w
is observed in C.
Combining Equations 2, 4, and 5 and canceling
out terms gives us the posterior probability:
</bodyText>
<equation confidence="0.931446">
ktxt nimg
w
p(w|Iq) = · (6)
kimg ntxt
</equation>
<bodyText confidence="0.99963075">
If the number of words in each caption is inde-
pendent of its image’s location in the image space,
then p(w|Iq) is approximately the observed uni-
gram frequency for the captions inside R.
</bodyText>
<subsectionHeader confidence="0.983331">
4.4 Extractive Caption Generation
</subsectionHeader>
<bodyText confidence="0.955466">
We compare two selection methods for extractive
caption generation:
</bodyText>
<listItem confidence="0.751117125">
1. SumBasic SumBasic (Nenkova and Vander-
wende, 2005) is a sentence selection algorithm for
extractive multi-document summarization which
exclusively maximizes the appearance of words
which have high frequency in the original docu-
ments. Here, we adapt SumBasic to maximize the
average value of p(w|Iq) in a single extracted cap-
tion:
</listItem>
<equation confidence="0.788499">
�output = arg max 1
ctxtEIZ wEctxt |ctxt |p(w|Iq) (7)
</equation>
<bodyText confidence="0.999833">
The candidate captions ctxt do not necessarily
have to be observed in R, but in practice we did
not find increasing the number of candidate cap-
tions to be more effective than increasing the size
of R directly.
</bodyText>
<page confidence="0.986961">
594
</page>
<table confidence="0.9991406">
System Relevance
COLLECTIVE 2.38 (Q = 1.45)
SCENE ATTRIBUTES 2.15 (Q = 1.45)
SYSTEM 3.19 (Q = 1.50)
HUMAN 4.09 (Q = 1.14)
</table>
<tableCaption confidence="0.994749">
Table 2: Human evaluations of relevance: mean
ratings and standard deviations. See Section 5.2.
</tableCaption>
<figure confidence="0.556952">
5.2 Human Evaluation
</figure>
<figureCaption confidence="0.9708765">
Figure 1: BLEU scores vs k for SumBasic extrac-
tion.
</figureCaption>
<listItem confidence="0.602817125">
2. KL Divergence We also consider a KL
Divergence selection method. This method out-
performs the SumBasic selection method for ex-
tractive multi-document summarization (Haghighi
and Vanderwende, 2009). It also generates the best
extractive captions for Feng and Lapata (2010),
who caption images by extracting text from a re-
lated news article. The KL Divergence method is
</listItem>
<equation confidence="0.9773924">
1: p(w|Iq) log p(w|Iq)
output = arg min
p(w|ctxt)
ctxt∈R w
(8)
</equation>
<sectionHeader confidence="0.99952" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.998391">
5.1 Automatic Evaluation
</subsectionHeader>
<bodyText confidence="0.999722627118645">
Although BLEU (Papineni et al., 2002) scores
are widely used for image caption evaluation, we
find them to be poor indicators of the quality of
our model. As shown in Figure 1, our system’s
BLEU scores increase rapidly until about k = 25.
Past this point we observe the density estimation
seems to get washed out by oversmoothing, but the
BLEU scores continue to improve until k = 500
but only because the generated captions become
increasingly shorter. Furthermore, although we
observe that our SumBasic extracted captions ob-
tain consistently higher BLEU scores, our per-
sonal observations find KL Divergence captions to
be better at balancing recall and precision. Never-
theless, BLEU scores are the accepted metric for
recent work, and our KL Divergence captions with
k = 25 still outperform all other previously pub-
lished systems and baselines. We omit full results
here due to space, but make our BLEU setup with
captions for all systems and baselines available for
documentary purposes.
We perform our human evaluation of caption rele-
vance using a similar setup to that of Kuznetsova
et al. (2012), who have humans rate the image cap-
tions on a 1-5 scale (5: perfect, 4: almost per-
fect, 3: 70-80% good, 2: 50-70% good, 1: to-
tally bad). Evaluation is performed using Amazon
Mechanical Turk. Evaluators are shown both the
caption and the query image, and are specifically
instructed to ignore errors in grammaticality and
coherence.
We generate captions using our system with KL
Divergence sentence selection and k = 25. We
also evaluate the original HUMAN captions for
the query image, as well as generated captions
from two recently published caption transfer sys-
tems. First, we consider the SCENE ATTRIBUTES
system (Patterson et al., 2014), which represents
both the best scene-based transfer model and a
k = 1 nearest-neighbor baseline for our system.
We also compare against the COLLECTIVE system
(Kuznetsova et al., 2012), which is the best object-
based transfer model.
In order to facilitate comparison, we use the
same test/train split that is used in the publicly
available system output for the COLLECTIVE sys-
tem7. However, we remove some query images
which have contamination between the train and
test set (this occurs when a photographer takes
multiple shots of the same scene and gives all the
images the exact same caption). We also note that
their test set is selected based on images where
their object detection systems had good perfor-
mance, and may not be indicative of their perfor-
mance on other query images.
Table 2 shows the results of our human study.
Captions generated by our system have 48%
improvement in relevance over the SCENE AT-
TRIBUTES system captions, and 34% improve-
</bodyText>
<footnote confidence="0.981662333333333">
7http://www.cs.sunysb.edu/
˜pkuznetsova/generation/cogn/captions.
html
</footnote>
<page confidence="0.982721">
595
</page>
<bodyText confidence="0.798306333333333">
COLLECTIVE: One of the birds seen in View of this woman sit- Found this mother bird Found in floating grass
company of female and ting on the sidewalk in feeding her babies in spotted alongside the
juvenile. Mumbai by the stained our maple tree on the scenic North Cascades
glass. The boy walk- phone. Hwy near Ruby arm a
ing by next to match- black bear.
ing color walls in gov t
</bodyText>
<table confidence="0.898045615384615">
building.
SCENE This small bird is pretty me and allison in front The sand in this beach Not the green one, but
ATTRIBUTES: much only found in the of the white house was black...I repeat the almost ghost-like
ancient Caledonian pine BLACK SAND white one in front of it.
forests of the Scottish
Highlands.
SYSTEM: White bird found in by the white house pine tree covered in ice Pink flower in garden w/
park standing on brick :) moth
wall
HUMAN: Some black head bird Us girls in front of the Male cardinal in snowy Black bear by the road
taken in bray head. white house tree knots between Ucluelet and
Port Alberni, B.C.,
Canada
</table>
<tableCaption confidence="0.999915">
Table 3: Example query images and generated captions.
</tableCaption>
<bodyText confidence="0.9999392">
ment over the COLLECTIVE system captions. Al-
though our system captions score lower than the
human captions on average, there are some in-
stances of our system captions being judged as
more relevant than the human-written captions.
</bodyText>
<sectionHeader confidence="0.992251" genericHeader="conclusions">
6 Discussion and Examples
</sectionHeader>
<bodyText confidence="0.999970290322581">
Example captions are shown in Table 3. In many
instances, scene-based image descriptors provide
enough information to generate a complete de-
scription of the image, or at least a sufficiently
good one. However, there are some kinds of
images for which scene-based features alone are
insufficient. For example, the last example de-
scribes the small pink flowers in the background,
but misses the bear.
Image captioning is a relatively novel task for
which the most compelling applications are prob-
ably not yet known. Much previous work in im-
age captioning focuses on generating captions that
concretely describe detected objects and entities
(Kulkarni et al., 2011; Yang et al., 2011; Mitchell
et al., 2012; Yu and Siskind, 2013). However,
human-generated captions and annotations also
describe perceptual features, contextual informa-
tion, and other types of content. Additionally, our
system is robust to instances where entity detec-
tion systems fail to perform. However, one could
consider combined approaches which incorporate
more regional content structures. For example,
previous work in nonparametric hierarchical topic
modeling (Blei et al., 2010) and scene labeling
(Liu et al., 2011) may provide avenues for further
improvement of this model. Compression meth-
ods for removing visually irrelevant information
(Kuznetsova et al., 2013) may also help increase
the relevance of extracted captions. We leave these
ideas for future work.
</bodyText>
<sectionHeader confidence="0.987861" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.990244588235294">
Ahmet Aker and Robert Gaizauskas. 2010. Generating
image descriptions using dependency relational pat-
terns. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, ACL
’10, pages 1250–1258, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Alexander C Berg, Tamara L Berg, Hal Daume, Jesse
Dodge, Amit Goyal, Xufeng Han, Alyssa Mensch,
Margaret Mitchell, Aneesh Sood, Karl Stratos, et al.
2012. Understanding and predicting importance in
images. In Computer Vision and Pattern Recog-
nition (CVPR), 2012 IEEE Conference on, pages
3562–3569. IEEE.
Christopher M Bishop. 2006. Pattern recognition and
machine learning, volume 1. Springer New York.
David M. Blei, Thomas L. Griffiths, and Michael I. Jor-
dan. 2010. The nested chinese restaurant process
</reference>
<page confidence="0.994252">
596
</page>
<reference confidence="0.997947227272727">
and bayesian nonparametric inference of topic hier-
archies. J. ACM, 57(2):7:1–7:30, February.
Jesse Dodge, Amit Goyal, Xufeng Han, Alyssa Men-
sch, Margaret Mitchell, Karl Stratos, Kota Yam-
aguchi, Yejin Choi, Hal Daum´e III, Alexander C.
Berg, and Tamara L. Berg. 2012. Detecting visual
text. In North American Chapter of the Association
for Computational Linguistics (NAACL).
Xin Fan, Ahmet Aker, Martin Tomko, Philip Smart,
Mark Sanderson, and Robert Gaizauskas. 2010.
Automatic image captioning from the web for gps
photographs. In Proceedings of the international
conference on Multimedia information retrieval,
MIR ’10, pages 445–448, New York, NY, USA.
ACM.
Ali Farhadi, Mohsen Hejrati, Mohammad Amin
Sadeghi, Peter Young, Cyrus Rashtchian, Julia
Hockenmaier, and David Forsyth. 2010. Every pic-
ture tells a story: generating sentences from images.
In Proceedings of the 11th European conference on
Computer vision: Part IV, ECCV’10, pages 15–29,
Berlin, Heidelberg. Springer-Verlag.
Yansong Feng and Mirella Lapata. 2010. How many
words is a picture worth? automatic caption genera-
tion for news images. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, ACL ’10, pages 1239–1249, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Aria Haghighi and Lucy Vanderwende. 2009. Ex-
ploring content models for multi-document summa-
rization. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 362–370. Association for
Computational Linguistics.
James Hays and Alexei A Efros. 2008. Im2gps: esti-
mating geographic information from a single image.
In Computer Vision and Pattern Recognition, 2008.
CVPR 2008. IEEE Conference on, pages 1–8. IEEE.
Micah Hodosh, Peter Young, and Julia Hockenmaier.
2013. Framing image description as a ranking task:
Data, models and evaluation metrics. Journal of Ar-
tificial Intelligence Research, 47:853–899.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming
Li, Yejin Choi, Alexander C. Berg, and Tamara L.
Berg. 2011. Baby talk: Understanding and gener-
ating simple image descriptions. In CVPR, pages
1601–1608.
Polina Kuznetsova, Vicente Ordonez, Alexander C.
Berg, Tamara L. Berg, and Yejin Choi. 2012. Col-
lective generation of natural image descriptions. In
ACL.
Polina Kuznetsova, Vicente Ordonez, Alexander Berg,
Tamara Berg, and Yejin Choi. 2013. Generalizing
image captions for image-text parallel corpus. In
ACL.
Ce Liu, Jenny Yuen, and Antonio Torralba. 2011.
Nonparametric scene parsing via label transfer.
Pattern Analysis and Machine Intelligence, IEEE
Transactions on, 33(12):2368–2382.
Ameesh Makadia, Vladimir Pavlovic, and Sanjiv Ku-
mar. 2008. A new baseline for image annotation.
In Computer Vision–ECCV 2008, pages 316–329.
Springer.
Margaret Mitchell, Jesse Dodge, Amit Goyal, Kota Ya-
maguchi, Karl Stratos, Xufeng Han, Alyssa Men-
sch, Alexander C. Berg, Tamara L. Berg, and Hal
Daum´e III. 2012. Midge: Generating image de-
scriptions from computer vision detections. In Euro-
pean Chapter of the Association for Computational
Linguistics (EACL).
Ani Nenkova and Lucy Vanderwende. 2005. The im-
pact of frequency on summarization.
Aude Oliva and Antonio Torralba. 2001. Modeling the
shape of the scene: A holistic representation of the
spatial envelope. International Journal of Computer
Vision, 42:145–175.
V. Ordonez, G. Kulkarni, and T.L. Berg. 2011.
Im2text: Describing images using 1 million cap-
tioned photographs. In NIPS.
Vicente Ordonez, Jia Deng, Yejin Choi, Alexander C
Berg, and Tamara L Berg. 2013. From large scale
image categorization to entry-level categories. In In-
ternational Conference on Computer Vision.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ’02, pages 311–318,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Genevieve Patterson and James Hays. 2012. Sun at-
tribute database: Discovering, annotating, and rec-
ognizing scene attributes. In Computer Vision and
Pattern Recognition (CVPR), 2012 IEEE Confer-
ence on, pages 2751–2758. IEEE.
Genevieve Patterson, Chen Xu, Hang Su, and James
Hays. 2014. The sun attribute database: Beyond
categories for deeper scene understanding. Interna-
tional Journal of Computer Vision.
Joseph Tighe and Svetlana Lazebnik. 2010. Su-
perparsing: scalable nonparametric image parsing
with superpixels. In Computer Vision–ECCV 2010,
pages 352–365. Springer.
Antonio Torralba, Robert Fergus, and William T Free-
man. 2008. 80 million tiny images: A large data
set for nonparametric object and scene recognition.
Pattern Analysis and Machine Intelligence, IEEE
Transactions on, 30(11):1958–1970.
</reference>
<page confidence="0.975081">
597
</page>
<reference confidence="0.998820583333333">
Yezhou Yang, Ching Lik Teo, Hal Daum´e III, and Yian-
nis Aloimonos. 2011. Corpus-guided sentence gen-
eration of natural images. In Empirical Methods
in Natural Language Processing (EMNLP), Edin-
burgh, Scotland.
Haonan Yu and Jeffrey Mark Siskind. 2013. Grounded
language learning from video described with sen-
tences. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers), volume 1, pages 53–63,
Sofia, Bulgaria. Association for Computational Lin-
guistics.
</reference>
<page confidence="0.997017">
598
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.412132">
<title confidence="0.999929">Nonparametric Method for Data-driven Image Captioning</title>
<author confidence="0.935621">Mason</author>
<affiliation confidence="0.7493795">Brown Laboratory for Linguistic Information Processing Brown University, Providence, RI</affiliation>
<abstract confidence="0.990366541666667">We present a nonparametric density estimation technique for image caption generation. Data-driven matching methods have shown to be effective for a variety of complex problems in Computer Vision. These methods reduce an inference problem for an unknown image to finding an existing labeled image which is semantically similar. However, related approaches for image caption generation (Ordonez et al., 2011; Kuznetsova et al., 2012) are hampered by noisy estimations of visual content and poor alignment between images and human-written captions. Our work addresses this challenge by estimating a word frequency representation of the visual content of a query image. This allows us to cast caption generation as an extractive summarization problem. Our model strongly outperforms two state-ofthe-art caption extraction systems according to human judgments of caption relevance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ahmet Aker</author>
<author>Robert Gaizauskas</author>
</authors>
<title>Generating image descriptions using dependency relational patterns.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>1250--1258</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5164" citStr="Aker and Gaizauskas, 2010" startWordPosition="830" endWordPosition="833"> is retrieved for each visual entity, which are then fused into a single output caption. Like IM2TEXT, their approach uses visual similarity as a proxy for textual relevance. Other related work models the text more directly, but is more restrictive about the source and quality of the human-written training data. Farhadi et al. (2010) and Hodosh et al. (2013) learn joint representations for images and captions, but can only be trained on data with very strong alignment between images and descriptions (i.e. captions written by Mechanical Turkers). Another line of related work (Fan et al., 2010; Aker and Gaizauskas, 2010; Feng and Lapata, 2010) generates captions by extracting sentences from documents which are related to the query image. These approaches are tailored toward specific domains, such as travel and news, where images tend to appear with corresponding text. 1See http://bllip.cs.brown.edu/ download/captioning_resources.zip or ACL Anthology. 3 Dataset In this paper, we use the SBU-Flickr dataset2. Ordonez et al. (2011) query Flickr.com using a huge number of words which describe visual entities, in order to build a corpus of one million images with captions which refer to image content. However, fur</context>
</contexts>
<marker>Aker, Gaizauskas, 2010</marker>
<rawString>Ahmet Aker and Robert Gaizauskas. 2010. Generating image descriptions using dependency relational patterns. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 1250–1258, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander C Berg</author>
<author>Tamara L Berg</author>
<author>Hal Daume</author>
<author>Jesse Dodge</author>
<author>Amit Goyal</author>
<author>Xufeng Han</author>
<author>Alyssa Mensch</author>
<author>Margaret Mitchell</author>
<author>Aneesh Sood</author>
<author>Karl Stratos</author>
</authors>
<title>Understanding and predicting importance in images.</title>
<date>2012</date>
<booktitle>In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on,</booktitle>
<pages>3562--3569</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="7476" citStr="Berg et al., 2012" startWordPosition="1187" endWordPosition="1190">tors rather than object and entity detections. Scenebased techniques in CV are generally more robust, and can be computed more efficiently on large datasets. The basic IM2TEXT model uses an equally weighted average of GIST (Oliva and Torralba, 2001) and TinyImage (Torralba et al., 2008) features, which coarsely localize low-level features in scenes. The output is a multi-dimensional image space where semantically similar scenes (e.g. streets, beaches, highways) are projected near each other. 2http://tamaraberg.com/CLSP11/ 3In particular, papers stemming from the 2011 JHU-CLSP Summer Workshop (Berg et al., 2012; Dodge et al., 2012; Mitchell et al., 2012) and more recently, the best paper award winner at ICCV (Ordonez et al., 2013). 593 Patterson and Hays (2012) present “scene attribute” representations which are characterized using low-level perceptual attributes as used by GIST (e.g. openness, ruggedness, naturalness), as well as high-level attributes informed by openended crowd-sourced image descriptions (e.g., indoor lighting, running water, places for learning). Follow-up work (Patterson et al., 2014) shows that their attributes provide improved matching for image captioning over IM2TEXT baselin</context>
</contexts>
<marker>Berg, Berg, Daume, Dodge, Goyal, Han, Mensch, Mitchell, Sood, Stratos, 2012</marker>
<rawString>Alexander C Berg, Tamara L Berg, Hal Daume, Jesse Dodge, Amit Goyal, Xufeng Han, Alyssa Mensch, Margaret Mitchell, Aneesh Sood, Karl Stratos, et al. 2012. Understanding and predicting importance in images. In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pages 3562–3569. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher M Bishop</author>
</authors>
<title>Pattern recognition and machine learning,</title>
<date>2006</date>
<volume>1</volume>
<publisher>Springer</publisher>
<location>New York.</location>
<contexts>
<context position="8359" citStr="Bishop (2006)" startWordPosition="1316" endWordPosition="1317">IST (e.g. openness, ruggedness, naturalness), as well as high-level attributes informed by openended crowd-sourced image descriptions (e.g., indoor lighting, running water, places for learning). Follow-up work (Patterson et al., 2014) shows that their attributes provide improved matching for image captioning over IM2TEXT baseline. We use their publicly available4 scene attributes for our experiments. Training set and query images are represented using 102-dimensional real-valued vectors, and similarity between images is measured using the Euclidean distance. 4.3 Density Estimation As shown in Bishop (2006), probability density estimates at a particular point can be obtained by considering points in the training data within some local neighborhood. In our case, we define some region R in the image space which contains Iq. The probability mass of that space is P = Lp(Iq)dIq (1) and if we assume that R is small enough such that p(Iq) is roughly constant in R, we can approximate kimg p(Iq) ≈ (2) nimgV img where kimg is the number of images within R in the training data, nimg is the total number of images in the training data, and Vimg is the volume of R. In this paper, we fix kimg to a constant val</context>
</contexts>
<marker>Bishop, 2006</marker>
<rawString>Christopher M Bishop. 2006. Pattern recognition and machine learning, volume 1. Springer New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Thomas L Griffiths</author>
<author>Michael I Jordan</author>
</authors>
<title>The nested chinese restaurant process and bayesian nonparametric inference of topic hierarchies.</title>
<date>2010</date>
<journal>J. ACM,</journal>
<volume>57</volume>
<issue>2</issue>
<marker>Blei, Griffiths, Jordan, 2010</marker>
<rawString>David M. Blei, Thomas L. Griffiths, and Michael I. Jordan. 2010. The nested chinese restaurant process and bayesian nonparametric inference of topic hierarchies. J. ACM, 57(2):7:1–7:30, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jesse Dodge</author>
<author>Amit Goyal</author>
<author>Xufeng Han</author>
<author>Alyssa Mensch</author>
<author>Margaret Mitchell</author>
<author>Karl Stratos</author>
</authors>
<title>Detecting visual text.</title>
<date>2012</date>
<booktitle>In North American Chapter of the Association for Computational Linguistics (NAACL).</booktitle>
<location>Kota Yamaguchi, Yejin</location>
<contexts>
<context position="7496" citStr="Dodge et al., 2012" startWordPosition="1191" endWordPosition="1194">ject and entity detections. Scenebased techniques in CV are generally more robust, and can be computed more efficiently on large datasets. The basic IM2TEXT model uses an equally weighted average of GIST (Oliva and Torralba, 2001) and TinyImage (Torralba et al., 2008) features, which coarsely localize low-level features in scenes. The output is a multi-dimensional image space where semantically similar scenes (e.g. streets, beaches, highways) are projected near each other. 2http://tamaraberg.com/CLSP11/ 3In particular, papers stemming from the 2011 JHU-CLSP Summer Workshop (Berg et al., 2012; Dodge et al., 2012; Mitchell et al., 2012) and more recently, the best paper award winner at ICCV (Ordonez et al., 2013). 593 Patterson and Hays (2012) present “scene attribute” representations which are characterized using low-level perceptual attributes as used by GIST (e.g. openness, ruggedness, naturalness), as well as high-level attributes informed by openended crowd-sourced image descriptions (e.g., indoor lighting, running water, places for learning). Follow-up work (Patterson et al., 2014) shows that their attributes provide improved matching for image captioning over IM2TEXT baseline. We use their publ</context>
</contexts>
<marker>Dodge, Goyal, Han, Mensch, Mitchell, Stratos, 2012</marker>
<rawString>Jesse Dodge, Amit Goyal, Xufeng Han, Alyssa Mensch, Margaret Mitchell, Karl Stratos, Kota Yamaguchi, Yejin Choi, Hal Daum´e III, Alexander C. Berg, and Tamara L. Berg. 2012. Detecting visual text. In North American Chapter of the Association for Computational Linguistics (NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Fan</author>
<author>Ahmet Aker</author>
<author>Martin Tomko</author>
<author>Philip Smart</author>
<author>Mark Sanderson</author>
<author>Robert Gaizauskas</author>
</authors>
<title>Automatic image captioning from the web for gps photographs.</title>
<date>2010</date>
<booktitle>In Proceedings of the international conference on Multimedia information retrieval, MIR ’10,</booktitle>
<pages>445--448</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="5137" citStr="Fan et al., 2010" startWordPosition="826" endWordPosition="829">parate description is retrieved for each visual entity, which are then fused into a single output caption. Like IM2TEXT, their approach uses visual similarity as a proxy for textual relevance. Other related work models the text more directly, but is more restrictive about the source and quality of the human-written training data. Farhadi et al. (2010) and Hodosh et al. (2013) learn joint representations for images and captions, but can only be trained on data with very strong alignment between images and descriptions (i.e. captions written by Mechanical Turkers). Another line of related work (Fan et al., 2010; Aker and Gaizauskas, 2010; Feng and Lapata, 2010) generates captions by extracting sentences from documents which are related to the query image. These approaches are tailored toward specific domains, such as travel and news, where images tend to appear with corresponding text. 1See http://bllip.cs.brown.edu/ download/captioning_resources.zip or ACL Anthology. 3 Dataset In this paper, we use the SBU-Flickr dataset2. Ordonez et al. (2011) query Flickr.com using a huge number of words which describe visual entities, in order to build a corpus of one million images with captions which refer to </context>
</contexts>
<marker>Fan, Aker, Tomko, Smart, Sanderson, Gaizauskas, 2010</marker>
<rawString>Xin Fan, Ahmet Aker, Martin Tomko, Philip Smart, Mark Sanderson, and Robert Gaizauskas. 2010. Automatic image captioning from the web for gps photographs. In Proceedings of the international conference on Multimedia information retrieval, MIR ’10, pages 445–448, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ali Farhadi</author>
<author>Mohsen Hejrati</author>
<author>Mohammad Amin Sadeghi</author>
<author>Peter Young</author>
<author>Cyrus Rashtchian</author>
<author>Julia Hockenmaier</author>
<author>David Forsyth</author>
</authors>
<title>Every picture tells a story: generating sentences from images.</title>
<date>2010</date>
<booktitle>In Proceedings of the 11th European conference on Computer vision: Part IV, ECCV’10,</booktitle>
<pages>15--29</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="4874" citStr="Farhadi et al. (2010)" startWordPosition="782" endWordPosition="785"> closest visual match to the query image, and transfers its description to the query image. The COLLECTIVE model by Kuznetsova et al. (2012) is a related approach which uses trained CV recognition systems to detect a variety of visual entities in the query image. A separate description is retrieved for each visual entity, which are then fused into a single output caption. Like IM2TEXT, their approach uses visual similarity as a proxy for textual relevance. Other related work models the text more directly, but is more restrictive about the source and quality of the human-written training data. Farhadi et al. (2010) and Hodosh et al. (2013) learn joint representations for images and captions, but can only be trained on data with very strong alignment between images and descriptions (i.e. captions written by Mechanical Turkers). Another line of related work (Fan et al., 2010; Aker and Gaizauskas, 2010; Feng and Lapata, 2010) generates captions by extracting sentences from documents which are related to the query image. These approaches are tailored toward specific domains, such as travel and news, where images tend to appear with corresponding text. 1See http://bllip.cs.brown.edu/ download/captioning_reso</context>
</contexts>
<marker>Farhadi, Hejrati, Sadeghi, Young, Rashtchian, Hockenmaier, Forsyth, 2010</marker>
<rawString>Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi, Peter Young, Cyrus Rashtchian, Julia Hockenmaier, and David Forsyth. 2010. Every picture tells a story: generating sentences from images. In Proceedings of the 11th European conference on Computer vision: Part IV, ECCV’10, pages 15–29, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yansong Feng</author>
<author>Mirella Lapata</author>
</authors>
<title>How many words is a picture worth? automatic caption generation for news images.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>1239--1249</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5188" citStr="Feng and Lapata, 2010" startWordPosition="834" endWordPosition="837">al entity, which are then fused into a single output caption. Like IM2TEXT, their approach uses visual similarity as a proxy for textual relevance. Other related work models the text more directly, but is more restrictive about the source and quality of the human-written training data. Farhadi et al. (2010) and Hodosh et al. (2013) learn joint representations for images and captions, but can only be trained on data with very strong alignment between images and descriptions (i.e. captions written by Mechanical Turkers). Another line of related work (Fan et al., 2010; Aker and Gaizauskas, 2010; Feng and Lapata, 2010) generates captions by extracting sentences from documents which are related to the query image. These approaches are tailored toward specific domains, such as travel and news, where images tend to appear with corresponding text. 1See http://bllip.cs.brown.edu/ download/captioning_resources.zip or ACL Anthology. 3 Dataset In this paper, we use the SBU-Flickr dataset2. Ordonez et al. (2011) query Flickr.com using a huge number of words which describe visual entities, in order to build a corpus of one million images with captions which refer to image content. However, further analysis by Hodosh </context>
<context position="11945" citStr="Feng and Lapata (2010)" startWordPosition="1938" endWordPosition="1941">ore effective than increasing the size of R directly. 594 System Relevance COLLECTIVE 2.38 (Q = 1.45) SCENE ATTRIBUTES 2.15 (Q = 1.45) SYSTEM 3.19 (Q = 1.50) HUMAN 4.09 (Q = 1.14) Table 2: Human evaluations of relevance: mean ratings and standard deviations. See Section 5.2. 5.2 Human Evaluation Figure 1: BLEU scores vs k for SumBasic extraction. 2. KL Divergence We also consider a KL Divergence selection method. This method outperforms the SumBasic selection method for extractive multi-document summarization (Haghighi and Vanderwende, 2009). It also generates the best extractive captions for Feng and Lapata (2010), who caption images by extracting text from a related news article. The KL Divergence method is 1: p(w|Iq) log p(w|Iq) output = arg min p(w|ctxt) ctxt∈R w (8) 5 Evaluation 5.1 Automatic Evaluation Although BLEU (Papineni et al., 2002) scores are widely used for image caption evaluation, we find them to be poor indicators of the quality of our model. As shown in Figure 1, our system’s BLEU scores increase rapidly until about k = 25. Past this point we observe the density estimation seems to get washed out by oversmoothing, but the BLEU scores continue to improve until k = 500 but only because </context>
</contexts>
<marker>Feng, Lapata, 2010</marker>
<rawString>Yansong Feng and Mirella Lapata. 2010. How many words is a picture worth? automatic caption generation for news images. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 1239–1249, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Exploring content models for multi-document summarization.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>362--370</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11870" citStr="Haghighi and Vanderwende, 2009" startWordPosition="1926" endWordPosition="1929"> but in practice we did not find increasing the number of candidate captions to be more effective than increasing the size of R directly. 594 System Relevance COLLECTIVE 2.38 (Q = 1.45) SCENE ATTRIBUTES 2.15 (Q = 1.45) SYSTEM 3.19 (Q = 1.50) HUMAN 4.09 (Q = 1.14) Table 2: Human evaluations of relevance: mean ratings and standard deviations. See Section 5.2. 5.2 Human Evaluation Figure 1: BLEU scores vs k for SumBasic extraction. 2. KL Divergence We also consider a KL Divergence selection method. This method outperforms the SumBasic selection method for extractive multi-document summarization (Haghighi and Vanderwende, 2009). It also generates the best extractive captions for Feng and Lapata (2010), who caption images by extracting text from a related news article. The KL Divergence method is 1: p(w|Iq) log p(w|Iq) output = arg min p(w|ctxt) ctxt∈R w (8) 5 Evaluation 5.1 Automatic Evaluation Although BLEU (Papineni et al., 2002) scores are widely used for image caption evaluation, we find them to be poor indicators of the quality of our model. As shown in Figure 1, our system’s BLEU scores increase rapidly until about k = 25. Past this point we observe the density estimation seems to get washed out by oversmoothi</context>
</contexts>
<marker>Haghighi, Vanderwende, 2009</marker>
<rawString>Aria Haghighi and Lucy Vanderwende. 2009. Exploring content models for multi-document summarization. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 362–370. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Hays</author>
<author>Alexei A Efros</author>
</authors>
<title>Im2gps: estimating geographic information from a single image.</title>
<date>2008</date>
<booktitle>In Computer Vision and Pattern Recognition,</booktitle>
<pages>1--8</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="6748" citStr="Hays and Efros, 2008" startWordPosition="1083" endWordPosition="1086">d NLP.3 4 Our Approach 4.1 Overview For a query image IQ, our task is to generate a relevant description by selecting a single caption from C, a large dataset of images with human-written captions. In this section, we first define the feature space for visual similarity, then formulate a density estimation problem with the aim of modeling the words which are used to describe visually similar images to IQ. We also explore methods for extractive caption generation. 4.2 Measuring Visual Similarity Data-driven matching methods have shown to be very effective for a variety of challenging problems (Hays and Efros, 2008; Makadia et al., 2008; Tighe and Lazebnik, 2010). Typically these methods compute global (scene-based) descriptors rather than object and entity detections. Scenebased techniques in CV are generally more robust, and can be computed more efficiently on large datasets. The basic IM2TEXT model uses an equally weighted average of GIST (Oliva and Torralba, 2001) and TinyImage (Torralba et al., 2008) features, which coarsely localize low-level features in scenes. The output is a multi-dimensional image space where semantically similar scenes (e.g. streets, beaches, highways) are projected near each</context>
</contexts>
<marker>Hays, Efros, 2008</marker>
<rawString>James Hays and Alexei A Efros. 2008. Im2gps: estimating geographic information from a single image. In Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on, pages 1–8. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micah Hodosh</author>
<author>Peter Young</author>
<author>Julia Hockenmaier</author>
</authors>
<title>Framing image description as a ranking task: Data, models and evaluation metrics.</title>
<date>2013</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>47--853</pages>
<contexts>
<context position="4899" citStr="Hodosh et al. (2013)" startWordPosition="787" endWordPosition="790">he query image, and transfers its description to the query image. The COLLECTIVE model by Kuznetsova et al. (2012) is a related approach which uses trained CV recognition systems to detect a variety of visual entities in the query image. A separate description is retrieved for each visual entity, which are then fused into a single output caption. Like IM2TEXT, their approach uses visual similarity as a proxy for textual relevance. Other related work models the text more directly, but is more restrictive about the source and quality of the human-written training data. Farhadi et al. (2010) and Hodosh et al. (2013) learn joint representations for images and captions, but can only be trained on data with very strong alignment between images and descriptions (i.e. captions written by Mechanical Turkers). Another line of related work (Fan et al., 2010; Aker and Gaizauskas, 2010; Feng and Lapata, 2010) generates captions by extracting sentences from documents which are related to the query image. These approaches are tailored toward specific domains, such as travel and news, where images tend to appear with corresponding text. 1See http://bllip.cs.brown.edu/ download/captioning_resources.zip or ACL Antholog</context>
</contexts>
<marker>Hodosh, Young, Hockenmaier, 2013</marker>
<rawString>Micah Hodosh, Peter Young, and Julia Hockenmaier. 2013. Framing image description as a ranking task: Data, models and evaluation metrics. Journal of Artificial Intelligence Research, 47:853–899.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Girish Kulkarni</author>
<author>Visruth Premraj</author>
<author>Sagnik Dhar</author>
<author>Siming Li</author>
<author>Yejin Choi</author>
<author>Alexander C Berg</author>
<author>Tamara L Berg</author>
</authors>
<title>Baby talk: Understanding and generating simple image descriptions.</title>
<date>2011</date>
<booktitle>In CVPR,</booktitle>
<pages>1601--1608</pages>
<contexts>
<context position="16899" citStr="Kulkarni et al., 2011" startWordPosition="2767" endWordPosition="2770">n many instances, scene-based image descriptors provide enough information to generate a complete description of the image, or at least a sufficiently good one. However, there are some kinds of images for which scene-based features alone are insufficient. For example, the last example describes the small pink flowers in the background, but misses the bear. Image captioning is a relatively novel task for which the most compelling applications are probably not yet known. Much previous work in image captioning focuses on generating captions that concretely describe detected objects and entities (Kulkarni et al., 2011; Yang et al., 2011; Mitchell et al., 2012; Yu and Siskind, 2013). However, human-generated captions and annotations also describe perceptual features, contextual information, and other types of content. Additionally, our system is robust to instances where entity detection systems fail to perform. However, one could consider combined approaches which incorporate more regional content structures. For example, previous work in nonparametric hierarchical topic modeling (Blei et al., 2010) and scene labeling (Liu et al., 2011) may provide avenues for further improvement of this model. Compression</context>
</contexts>
<marker>Kulkarni, Premraj, Dhar, Li, Choi, Berg, Berg, 2011</marker>
<rawString>Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming Li, Yejin Choi, Alexander C. Berg, and Tamara L. Berg. 2011. Baby talk: Understanding and generating simple image descriptions. In CVPR, pages 1601–1608.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Polina Kuznetsova</author>
<author>Vicente Ordonez</author>
<author>Alexander C Berg</author>
<author>Tamara L Berg</author>
<author>Yejin Choi</author>
</authors>
<title>Collective generation of natural image descriptions.</title>
<date>2012</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="656" citStr="Kuznetsova et al., 2012" startWordPosition="88" endWordPosition="91">iven Image Captioning Rebecca Mason and Eugene Charniak Brown Laboratory for Linguistic Information Processing (BLLIP) Brown University, Providence, RI 02912 {rebecca,ec}@cs.brown.edu Abstract We present a nonparametric density estimation technique for image caption generation. Data-driven matching methods have shown to be effective for a variety of complex problems in Computer Vision. These methods reduce an inference problem for an unknown image to finding an existing labeled image which is semantically similar. However, related approaches for image caption generation (Ordonez et al., 2011; Kuznetsova et al., 2012) are hampered by noisy estimations of visual content and poor alignment between images and human-written captions. Our work addresses this challenge by estimating a word frequency representation of the visual content of a query image. This allows us to cast caption generation as an extractive summarization problem. Our model strongly outperforms two state-ofthe-art caption extraction systems according to human judgments of caption relevance. 1 Introduction Automatic image captioning is a much studied topic in both the Natural Language Processing (NLP) and Computer Vision (CV) areas of research</context>
<context position="3784" citStr="Kuznetsova et al., 2012" startWordPosition="599" endWordPosition="602"> the Association for Computational Linguistics (Short Papers), pages 592–598, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics 1. Our caption generation system effectively leverages information from the massive amounts of human-written image captions on the internet. In particular, it exhibits strong performance on the SBU-Flickr dataset (Ordonez et al., 2011), a noisy corpus of one million captioned images collected from the web. We achieve a remarkable 34% improvement in human relevance scores over a recent state-of-the-art image captioning system (Kuznetsova et al., 2012), and 48% improvement over a scene-based retrieval system (Patterson et al., 2014) using the same computed image features. 2. Our approach uses simple models which can be easily reproduced by both CV and NLP researchers. We provide resources to enable comparison against future systems.1 2 Image Captioning by Transfer The IM2TEXT model by Ordonez et al. (2011) presents the first web-scale approach to image caption generation. IM2TEXT retrieves the image which is the closest visual match to the query image, and transfers its description to the query image. The COLLECTIVE model by Kuznetsova et a</context>
<context position="13244" citStr="Kuznetsova et al. (2012)" startWordPosition="2155" endWordPosition="2158">e observe that our SumBasic extracted captions obtain consistently higher BLEU scores, our personal observations find KL Divergence captions to be better at balancing recall and precision. Nevertheless, BLEU scores are the accepted metric for recent work, and our KL Divergence captions with k = 25 still outperform all other previously published systems and baselines. We omit full results here due to space, but make our BLEU setup with captions for all systems and baselines available for documentary purposes. We perform our human evaluation of caption relevance using a similar setup to that of Kuznetsova et al. (2012), who have humans rate the image captions on a 1-5 scale (5: perfect, 4: almost perfect, 3: 70-80% good, 2: 50-70% good, 1: totally bad). Evaluation is performed using Amazon Mechanical Turk. Evaluators are shown both the caption and the query image, and are specifically instructed to ignore errors in grammaticality and coherence. We generate captions using our system with KL Divergence sentence selection and k = 25. We also evaluate the original HUMAN captions for the query image, as well as generated captions from two recently published caption transfer systems. First, we consider the SCENE </context>
</contexts>
<marker>Kuznetsova, Ordonez, Berg, Berg, Choi, 2012</marker>
<rawString>Polina Kuznetsova, Vicente Ordonez, Alexander C. Berg, Tamara L. Berg, and Yejin Choi. 2012. Collective generation of natural image descriptions. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Polina Kuznetsova</author>
<author>Vicente Ordonez</author>
<author>Alexander Berg</author>
<author>Tamara Berg</author>
<author>Yejin Choi</author>
</authors>
<title>Generalizing image captions for image-text parallel corpus.</title>
<date>2013</date>
<booktitle>In ACL.</booktitle>
<marker>Kuznetsova, Ordonez, Berg, Berg, Choi, 2013</marker>
<rawString>Polina Kuznetsova, Vicente Ordonez, Alexander Berg, Tamara Berg, and Yejin Choi. 2013. Generalizing image captions for image-text parallel corpus. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ce Liu</author>
<author>Jenny Yuen</author>
<author>Antonio Torralba</author>
</authors>
<title>Nonparametric scene parsing via label transfer. Pattern Analysis and Machine Intelligence,</title>
<date>2011</date>
<journal>IEEE Transactions on,</journal>
<volume>33</volume>
<issue>12</issue>
<marker>Liu, Yuen, Torralba, 2011</marker>
<rawString>Ce Liu, Jenny Yuen, and Antonio Torralba. 2011. Nonparametric scene parsing via label transfer. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 33(12):2368–2382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ameesh Makadia</author>
<author>Vladimir Pavlovic</author>
<author>Sanjiv Kumar</author>
</authors>
<title>A new baseline for image annotation. In Computer Vision–ECCV</title>
<date>2008</date>
<pages>316--329</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="6770" citStr="Makadia et al., 2008" startWordPosition="1087" endWordPosition="1090"> 4.1 Overview For a query image IQ, our task is to generate a relevant description by selecting a single caption from C, a large dataset of images with human-written captions. In this section, we first define the feature space for visual similarity, then formulate a density estimation problem with the aim of modeling the words which are used to describe visually similar images to IQ. We also explore methods for extractive caption generation. 4.2 Measuring Visual Similarity Data-driven matching methods have shown to be very effective for a variety of challenging problems (Hays and Efros, 2008; Makadia et al., 2008; Tighe and Lazebnik, 2010). Typically these methods compute global (scene-based) descriptors rather than object and entity detections. Scenebased techniques in CV are generally more robust, and can be computed more efficiently on large datasets. The basic IM2TEXT model uses an equally weighted average of GIST (Oliva and Torralba, 2001) and TinyImage (Torralba et al., 2008) features, which coarsely localize low-level features in scenes. The output is a multi-dimensional image space where semantically similar scenes (e.g. streets, beaches, highways) are projected near each other. 2http://tamara</context>
</contexts>
<marker>Makadia, Pavlovic, Kumar, 2008</marker>
<rawString>Ameesh Makadia, Vladimir Pavlovic, and Sanjiv Kumar. 2008. A new baseline for image annotation. In Computer Vision–ECCV 2008, pages 316–329. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret Mitchell</author>
<author>Jesse Dodge</author>
<author>Amit Goyal</author>
<author>Kota Yamaguchi</author>
<author>Karl Stratos</author>
<author>Xufeng Han</author>
<author>Alyssa Mensch</author>
<author>Alexander C Berg</author>
<author>Tamara L Berg</author>
<author>Hal Daum´e</author>
</authors>
<title>Midge: Generating image descriptions from computer vision detections.</title>
<date>2012</date>
<booktitle>In European Chapter of the Association for Computational Linguistics (EACL).</booktitle>
<marker>Mitchell, Dodge, Goyal, Yamaguchi, Stratos, Han, Mensch, Berg, Berg, Daum´e, 2012</marker>
<rawString>Margaret Mitchell, Jesse Dodge, Amit Goyal, Kota Yamaguchi, Karl Stratos, Xufeng Han, Alyssa Mensch, Alexander C. Berg, Tamara L. Berg, and Hal Daum´e III. 2012. Midge: Generating image descriptions from computer vision detections. In European Chapter of the Association for Computational Linguistics (EACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Lucy Vanderwende</author>
</authors>
<title>The impact of frequency on summarization.</title>
<date>2005</date>
<contexts>
<context position="10833" citStr="Nenkova and Vanderwende, 2005" startWordPosition="1756" endWordPosition="1760">volume of R is still defined by the image space. ktxt w is the number of times w is used within R while ntxt w is the total number of times w is observed in C. Combining Equations 2, 4, and 5 and canceling out terms gives us the posterior probability: ktxt nimg w p(w|Iq) = · (6) kimg ntxt If the number of words in each caption is independent of its image’s location in the image space, then p(w|Iq) is approximately the observed unigram frequency for the captions inside R. 4.4 Extractive Caption Generation We compare two selection methods for extractive caption generation: 1. SumBasic SumBasic (Nenkova and Vanderwende, 2005) is a sentence selection algorithm for extractive multi-document summarization which exclusively maximizes the appearance of words which have high frequency in the original documents. Here, we adapt SumBasic to maximize the average value of p(w|Iq) in a single extracted caption: �output = arg max 1 ctxtEIZ wEctxt |ctxt |p(w|Iq) (7) The candidate captions ctxt do not necessarily have to be observed in R, but in practice we did not find increasing the number of candidate captions to be more effective than increasing the size of R directly. 594 System Relevance COLLECTIVE 2.38 (Q = 1.45) SCENE AT</context>
</contexts>
<marker>Nenkova, Vanderwende, 2005</marker>
<rawString>Ani Nenkova and Lucy Vanderwende. 2005. The impact of frequency on summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aude Oliva</author>
<author>Antonio Torralba</author>
</authors>
<title>Modeling the shape of the scene: A holistic representation of the spatial envelope.</title>
<date>2001</date>
<journal>International Journal of Computer Vision,</journal>
<pages>42--145</pages>
<contexts>
<context position="7108" citStr="Oliva and Torralba, 2001" startWordPosition="1137" endWordPosition="1140">h are used to describe visually similar images to IQ. We also explore methods for extractive caption generation. 4.2 Measuring Visual Similarity Data-driven matching methods have shown to be very effective for a variety of challenging problems (Hays and Efros, 2008; Makadia et al., 2008; Tighe and Lazebnik, 2010). Typically these methods compute global (scene-based) descriptors rather than object and entity detections. Scenebased techniques in CV are generally more robust, and can be computed more efficiently on large datasets. The basic IM2TEXT model uses an equally weighted average of GIST (Oliva and Torralba, 2001) and TinyImage (Torralba et al., 2008) features, which coarsely localize low-level features in scenes. The output is a multi-dimensional image space where semantically similar scenes (e.g. streets, beaches, highways) are projected near each other. 2http://tamaraberg.com/CLSP11/ 3In particular, papers stemming from the 2011 JHU-CLSP Summer Workshop (Berg et al., 2012; Dodge et al., 2012; Mitchell et al., 2012) and more recently, the best paper award winner at ICCV (Ordonez et al., 2013). 593 Patterson and Hays (2012) present “scene attribute” representations which are characterized using low-le</context>
</contexts>
<marker>Oliva, Torralba, 2001</marker>
<rawString>Aude Oliva and Antonio Torralba. 2001. Modeling the shape of the scene: A holistic representation of the spatial envelope. International Journal of Computer Vision, 42:145–175.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ordonez</author>
<author>G Kulkarni</author>
<author>T L Berg</author>
</authors>
<title>Im2text: Describing images using 1 million captioned photographs.</title>
<date>2011</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="630" citStr="Ordonez et al., 2011" startWordPosition="84" endWordPosition="87">ric Method for Data-driven Image Captioning Rebecca Mason and Eugene Charniak Brown Laboratory for Linguistic Information Processing (BLLIP) Brown University, Providence, RI 02912 {rebecca,ec}@cs.brown.edu Abstract We present a nonparametric density estimation technique for image caption generation. Data-driven matching methods have shown to be effective for a variety of complex problems in Computer Vision. These methods reduce an inference problem for an unknown image to finding an existing labeled image which is semantically similar. However, related approaches for image caption generation (Ordonez et al., 2011; Kuznetsova et al., 2012) are hampered by noisy estimations of visual content and poor alignment between images and human-written captions. Our work addresses this challenge by estimating a word frequency representation of the visual content of a query image. This allows us to cast caption generation as an extractive summarization problem. Our model strongly outperforms two state-ofthe-art caption extraction systems according to human judgments of caption relevance. 1 Introduction Automatic image captioning is a much studied topic in both the Natural Language Processing (NLP) and Computer Vis</context>
<context position="2152" citStr="Ordonez et al., 2011" startWordPosition="347" endWordPosition="350">base of captioned images, and then transfer text from the captions of those images to the query image. This is a challenging problem for two main reasons. First, visual similarity measures do not perform reliably and do not Query Image: Captioned Images: 1.) 3 month old baby girl with blue eyes in her crib 2.) A photo from the Ismail’s portrait shoot 3.) A portrait of a man, in black and white 4.) Portrait in black and white with the red rose 5.) I apparently had this saved in black and white as well 6.) Portrait in black and white Table 1: Example of a query image from the SBUFlickr dataset (Ordonez et al., 2011), along with scene-based estimates of visually similar images. Our system models visual content using words that are frequent in these captions (highlighted) and extracts a single output caption. capture all of the relevant details which humans might describe. Second, image captions collected from the web often contain contextual or background information which is not visually relevant to the image being described. In this paper, we propose a system for transferbased image captioning which is designed to address these challenges. Instead of selecting an output caption according to a single noi</context>
<context position="3565" citStr="Ordonez et al., 2011" startWordPosition="566" endWordPosition="569">cription of the query image by extracting the caption which best represents the mutually shared content. The contributions of this paper are as follows: 4. 5. 6. 1. 2. 3. 592 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 592–598, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics 1. Our caption generation system effectively leverages information from the massive amounts of human-written image captions on the internet. In particular, it exhibits strong performance on the SBU-Flickr dataset (Ordonez et al., 2011), a noisy corpus of one million captioned images collected from the web. We achieve a remarkable 34% improvement in human relevance scores over a recent state-of-the-art image captioning system (Kuznetsova et al., 2012), and 48% improvement over a scene-based retrieval system (Patterson et al., 2014) using the same computed image features. 2. Our approach uses simple models which can be easily reproduced by both CV and NLP researchers. We provide resources to enable comparison against future systems.1 2 Image Captioning by Transfer The IM2TEXT model by Ordonez et al. (2011) presents the first </context>
<context position="5580" citStr="Ordonez et al. (2011)" startWordPosition="889" endWordPosition="893">n only be trained on data with very strong alignment between images and descriptions (i.e. captions written by Mechanical Turkers). Another line of related work (Fan et al., 2010; Aker and Gaizauskas, 2010; Feng and Lapata, 2010) generates captions by extracting sentences from documents which are related to the query image. These approaches are tailored toward specific domains, such as travel and news, where images tend to appear with corresponding text. 1See http://bllip.cs.brown.edu/ download/captioning_resources.zip or ACL Anthology. 3 Dataset In this paper, we use the SBU-Flickr dataset2. Ordonez et al. (2011) query Flickr.com using a huge number of words which describe visual entities, in order to build a corpus of one million images with captions which refer to image content. However, further analysis by Hodosh et al. (2013) shows that many captions in SBU-Flickr (-67%) describe information that cannot be obtained from the image itself, while a substantial fraction (-23%) contain almost no visually relevant information. Nevertheless, this dataset is the only web-scale collection of captioned images, and has enabled notable research in both CV and NLP.3 4 Our Approach 4.1 Overview For a query imag</context>
</contexts>
<marker>Ordonez, Kulkarni, Berg, 2011</marker>
<rawString>V. Ordonez, G. Kulkarni, and T.L. Berg. 2011. Im2text: Describing images using 1 million captioned photographs. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vicente Ordonez</author>
<author>Jia Deng</author>
<author>Yejin Choi</author>
<author>Alexander C Berg</author>
<author>Tamara L Berg</author>
</authors>
<title>From large scale image categorization to entry-level categories.</title>
<date>2013</date>
<booktitle>In International Conference on Computer Vision.</booktitle>
<contexts>
<context position="7598" citStr="Ordonez et al., 2013" startWordPosition="1209" endWordPosition="1212">ted more efficiently on large datasets. The basic IM2TEXT model uses an equally weighted average of GIST (Oliva and Torralba, 2001) and TinyImage (Torralba et al., 2008) features, which coarsely localize low-level features in scenes. The output is a multi-dimensional image space where semantically similar scenes (e.g. streets, beaches, highways) are projected near each other. 2http://tamaraberg.com/CLSP11/ 3In particular, papers stemming from the 2011 JHU-CLSP Summer Workshop (Berg et al., 2012; Dodge et al., 2012; Mitchell et al., 2012) and more recently, the best paper award winner at ICCV (Ordonez et al., 2013). 593 Patterson and Hays (2012) present “scene attribute” representations which are characterized using low-level perceptual attributes as used by GIST (e.g. openness, ruggedness, naturalness), as well as high-level attributes informed by openended crowd-sourced image descriptions (e.g., indoor lighting, running water, places for learning). Follow-up work (Patterson et al., 2014) shows that their attributes provide improved matching for image captioning over IM2TEXT baseline. We use their publicly available4 scene attributes for our experiments. Training set and query images are represented us</context>
</contexts>
<marker>Ordonez, Deng, Choi, Berg, Berg, 2013</marker>
<rawString>Vicente Ordonez, Jia Deng, Yejin Choi, Alexander C Berg, and Tamara L Berg. 2013. From large scale image categorization to entry-level categories. In International Conference on Computer Vision.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="12180" citStr="Papineni et al., 2002" startWordPosition="1978" endWordPosition="1981">nd standard deviations. See Section 5.2. 5.2 Human Evaluation Figure 1: BLEU scores vs k for SumBasic extraction. 2. KL Divergence We also consider a KL Divergence selection method. This method outperforms the SumBasic selection method for extractive multi-document summarization (Haghighi and Vanderwende, 2009). It also generates the best extractive captions for Feng and Lapata (2010), who caption images by extracting text from a related news article. The KL Divergence method is 1: p(w|Iq) log p(w|Iq) output = arg min p(w|ctxt) ctxt∈R w (8) 5 Evaluation 5.1 Automatic Evaluation Although BLEU (Papineni et al., 2002) scores are widely used for image caption evaluation, we find them to be poor indicators of the quality of our model. As shown in Figure 1, our system’s BLEU scores increase rapidly until about k = 25. Past this point we observe the density estimation seems to get washed out by oversmoothing, but the BLEU scores continue to improve until k = 500 but only because the generated captions become increasingly shorter. Furthermore, although we observe that our SumBasic extracted captions obtain consistently higher BLEU scores, our personal observations find KL Divergence captions to be better at bal</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 311–318, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Genevieve Patterson</author>
<author>James Hays</author>
</authors>
<title>Sun attribute database: Discovering, annotating, and recognizing scene attributes.</title>
<date>2012</date>
<booktitle>In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on,</booktitle>
<pages>2751--2758</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="7629" citStr="Patterson and Hays (2012)" startWordPosition="1214" endWordPosition="1217">ge datasets. The basic IM2TEXT model uses an equally weighted average of GIST (Oliva and Torralba, 2001) and TinyImage (Torralba et al., 2008) features, which coarsely localize low-level features in scenes. The output is a multi-dimensional image space where semantically similar scenes (e.g. streets, beaches, highways) are projected near each other. 2http://tamaraberg.com/CLSP11/ 3In particular, papers stemming from the 2011 JHU-CLSP Summer Workshop (Berg et al., 2012; Dodge et al., 2012; Mitchell et al., 2012) and more recently, the best paper award winner at ICCV (Ordonez et al., 2013). 593 Patterson and Hays (2012) present “scene attribute” representations which are characterized using low-level perceptual attributes as used by GIST (e.g. openness, ruggedness, naturalness), as well as high-level attributes informed by openended crowd-sourced image descriptions (e.g., indoor lighting, running water, places for learning). Follow-up work (Patterson et al., 2014) shows that their attributes provide improved matching for image captioning over IM2TEXT baseline. We use their publicly available4 scene attributes for our experiments. Training set and query images are represented using 102-dimensional real-valued</context>
</contexts>
<marker>Patterson, Hays, 2012</marker>
<rawString>Genevieve Patterson and James Hays. 2012. Sun attribute database: Discovering, annotating, and recognizing scene attributes. In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pages 2751–2758. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Genevieve Patterson</author>
<author>Chen Xu</author>
<author>Hang Su</author>
<author>James Hays</author>
</authors>
<title>The sun attribute database: Beyond categories for deeper scene understanding.</title>
<date>2014</date>
<journal>International Journal of Computer Vision.</journal>
<contexts>
<context position="3866" citStr="Patterson et al., 2014" startWordPosition="611" endWordPosition="614">more, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics 1. Our caption generation system effectively leverages information from the massive amounts of human-written image captions on the internet. In particular, it exhibits strong performance on the SBU-Flickr dataset (Ordonez et al., 2011), a noisy corpus of one million captioned images collected from the web. We achieve a remarkable 34% improvement in human relevance scores over a recent state-of-the-art image captioning system (Kuznetsova et al., 2012), and 48% improvement over a scene-based retrieval system (Patterson et al., 2014) using the same computed image features. 2. Our approach uses simple models which can be easily reproduced by both CV and NLP researchers. We provide resources to enable comparison against future systems.1 2 Image Captioning by Transfer The IM2TEXT model by Ordonez et al. (2011) presents the first web-scale approach to image caption generation. IM2TEXT retrieves the image which is the closest visual match to the query image, and transfers its description to the query image. The COLLECTIVE model by Kuznetsova et al. (2012) is a related approach which uses trained CV recognition systems to detec</context>
<context position="7980" citStr="Patterson et al., 2014" startWordPosition="1261" endWordPosition="1264">http://tamaraberg.com/CLSP11/ 3In particular, papers stemming from the 2011 JHU-CLSP Summer Workshop (Berg et al., 2012; Dodge et al., 2012; Mitchell et al., 2012) and more recently, the best paper award winner at ICCV (Ordonez et al., 2013). 593 Patterson and Hays (2012) present “scene attribute” representations which are characterized using low-level perceptual attributes as used by GIST (e.g. openness, ruggedness, naturalness), as well as high-level attributes informed by openended crowd-sourced image descriptions (e.g., indoor lighting, running water, places for learning). Follow-up work (Patterson et al., 2014) shows that their attributes provide improved matching for image captioning over IM2TEXT baseline. We use their publicly available4 scene attributes for our experiments. Training set and query images are represented using 102-dimensional real-valued vectors, and similarity between images is measured using the Euclidean distance. 4.3 Density Estimation As shown in Bishop (2006), probability density estimates at a particular point can be obtained by considering points in the training data within some local neighborhood. In our case, we define some region R in the image space which contains Iq. T</context>
<context position="13886" citStr="Patterson et al., 2014" startWordPosition="2261" endWordPosition="2264">ate the image captions on a 1-5 scale (5: perfect, 4: almost perfect, 3: 70-80% good, 2: 50-70% good, 1: totally bad). Evaluation is performed using Amazon Mechanical Turk. Evaluators are shown both the caption and the query image, and are specifically instructed to ignore errors in grammaticality and coherence. We generate captions using our system with KL Divergence sentence selection and k = 25. We also evaluate the original HUMAN captions for the query image, as well as generated captions from two recently published caption transfer systems. First, we consider the SCENE ATTRIBUTES system (Patterson et al., 2014), which represents both the best scene-based transfer model and a k = 1 nearest-neighbor baseline for our system. We also compare against the COLLECTIVE system (Kuznetsova et al., 2012), which is the best objectbased transfer model. In order to facilitate comparison, we use the same test/train split that is used in the publicly available system output for the COLLECTIVE system7. However, we remove some query images which have contamination between the train and test set (this occurs when a photographer takes multiple shots of the same scene and gives all the images the exact same caption). We </context>
</contexts>
<marker>Patterson, Xu, Su, Hays, 2014</marker>
<rawString>Genevieve Patterson, Chen Xu, Hang Su, and James Hays. 2014. The sun attribute database: Beyond categories for deeper scene understanding. International Journal of Computer Vision.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Tighe</author>
<author>Svetlana Lazebnik</author>
</authors>
<title>Superparsing: scalable nonparametric image parsing with superpixels.</title>
<date>2010</date>
<booktitle>In Computer Vision–ECCV</booktitle>
<pages>352--365</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="6797" citStr="Tighe and Lazebnik, 2010" startWordPosition="1091" endWordPosition="1094">ery image IQ, our task is to generate a relevant description by selecting a single caption from C, a large dataset of images with human-written captions. In this section, we first define the feature space for visual similarity, then formulate a density estimation problem with the aim of modeling the words which are used to describe visually similar images to IQ. We also explore methods for extractive caption generation. 4.2 Measuring Visual Similarity Data-driven matching methods have shown to be very effective for a variety of challenging problems (Hays and Efros, 2008; Makadia et al., 2008; Tighe and Lazebnik, 2010). Typically these methods compute global (scene-based) descriptors rather than object and entity detections. Scenebased techniques in CV are generally more robust, and can be computed more efficiently on large datasets. The basic IM2TEXT model uses an equally weighted average of GIST (Oliva and Torralba, 2001) and TinyImage (Torralba et al., 2008) features, which coarsely localize low-level features in scenes. The output is a multi-dimensional image space where semantically similar scenes (e.g. streets, beaches, highways) are projected near each other. 2http://tamaraberg.com/CLSP11/ 3In partic</context>
</contexts>
<marker>Tighe, Lazebnik, 2010</marker>
<rawString>Joseph Tighe and Svetlana Lazebnik. 2010. Superparsing: scalable nonparametric image parsing with superpixels. In Computer Vision–ECCV 2010, pages 352–365. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Torralba</author>
<author>Robert Fergus</author>
<author>William T Freeman</author>
</authors>
<title>80 million tiny images: A large data set for nonparametric object and scene recognition. Pattern Analysis and Machine Intelligence,</title>
<date>2008</date>
<journal>IEEE Transactions on,</journal>
<volume>30</volume>
<issue>11</issue>
<contexts>
<context position="7146" citStr="Torralba et al., 2008" startWordPosition="1143" endWordPosition="1146">mages to IQ. We also explore methods for extractive caption generation. 4.2 Measuring Visual Similarity Data-driven matching methods have shown to be very effective for a variety of challenging problems (Hays and Efros, 2008; Makadia et al., 2008; Tighe and Lazebnik, 2010). Typically these methods compute global (scene-based) descriptors rather than object and entity detections. Scenebased techniques in CV are generally more robust, and can be computed more efficiently on large datasets. The basic IM2TEXT model uses an equally weighted average of GIST (Oliva and Torralba, 2001) and TinyImage (Torralba et al., 2008) features, which coarsely localize low-level features in scenes. The output is a multi-dimensional image space where semantically similar scenes (e.g. streets, beaches, highways) are projected near each other. 2http://tamaraberg.com/CLSP11/ 3In particular, papers stemming from the 2011 JHU-CLSP Summer Workshop (Berg et al., 2012; Dodge et al., 2012; Mitchell et al., 2012) and more recently, the best paper award winner at ICCV (Ordonez et al., 2013). 593 Patterson and Hays (2012) present “scene attribute” representations which are characterized using low-level perceptual attributes as used by G</context>
</contexts>
<marker>Torralba, Fergus, Freeman, 2008</marker>
<rawString>Antonio Torralba, Robert Fergus, and William T Freeman. 2008. 80 million tiny images: A large data set for nonparametric object and scene recognition. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 30(11):1958–1970.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yezhou Yang</author>
</authors>
<title>Ching Lik Teo, Hal Daum´e III, and Yiannis Aloimonos.</title>
<date>2011</date>
<booktitle>In Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<location>Edinburgh, Scotland.</location>
<marker>Yang, 2011</marker>
<rawString>Yezhou Yang, Ching Lik Teo, Hal Daum´e III, and Yiannis Aloimonos. 2011. Corpus-guided sentence generation of natural images. In Empirical Methods in Natural Language Processing (EMNLP), Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haonan Yu</author>
<author>Jeffrey Mark Siskind</author>
</authors>
<title>Grounded language learning from video described with sentences.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<volume>1</volume>
<pages>53--63</pages>
<institution>Sofia, Bulgaria. Association for Computational Linguistics.</institution>
<contexts>
<context position="16964" citStr="Yu and Siskind, 2013" startWordPosition="2779" endWordPosition="2782">formation to generate a complete description of the image, or at least a sufficiently good one. However, there are some kinds of images for which scene-based features alone are insufficient. For example, the last example describes the small pink flowers in the background, but misses the bear. Image captioning is a relatively novel task for which the most compelling applications are probably not yet known. Much previous work in image captioning focuses on generating captions that concretely describe detected objects and entities (Kulkarni et al., 2011; Yang et al., 2011; Mitchell et al., 2012; Yu and Siskind, 2013). However, human-generated captions and annotations also describe perceptual features, contextual information, and other types of content. Additionally, our system is robust to instances where entity detection systems fail to perform. However, one could consider combined approaches which incorporate more regional content structures. For example, previous work in nonparametric hierarchical topic modeling (Blei et al., 2010) and scene labeling (Liu et al., 2011) may provide avenues for further improvement of this model. Compression methods for removing visually irrelevant information (Kuznetsova</context>
</contexts>
<marker>Yu, Siskind, 2013</marker>
<rawString>Haonan Yu and Jeffrey Mark Siskind. 2013. Grounded language learning from video described with sentences. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 53–63, Sofia, Bulgaria. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>