<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000014">
<title confidence="0.9915075">
Why-Question Answering
using Intra- and Inter-Sentential Causal Relations
</title>
<author confidence="0.8219145">
Jong-Hoon Oh* Kentaro Torisawat Chikara Hashimoto t Motoki Sano§
Stijn De Saeger¶ Kiyonori Ohtakell
</author>
<affiliation confidence="0.994065666666667">
Information Analysis Laboratory
Universal Communication Research Institute
National Institute of Information and Communications Technology (NICT)
</affiliation>
<email confidence="0.969358">
{* rovellia,† torisawa,$ ch,§ msano,¶stijn,ll kiyonori.ohtake}@nict.go.jp
</email>
<sectionHeader confidence="0.993025" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999615476190476">
In this paper, we explore the utility of
intra- and inter-sentential causal relations
between terms or clauses as evidence for
answering why-questions. To the best of
our knowledge, this is the first work that
uses both intra- and inter-sentential causal
relations for why-QA. We also propose
a method for assessing the appropriate-
ness of causal relations as answers to a
given question using the semantic orienta-
tion of excitation proposed by Hashimoto
et al. (2012). By applying these ideas
to Japanese why-QA, we improved preci-
sion by 4.4% against all the questions in
our test set over the current state-of-the-
art system for Japanese why-QA. In addi-
tion, unlike the state-of-the-art system, our
system could achieve very high precision
(83.2%) for 25% of all the questions in the
test set by restricting its output to the con-
fident answers only.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999599375">
“Why-question answering” (why-QA) is a task to
retrieve answers from a given text archive for a
why-question, such as “Why are tsunamis gen-
erated?” The answers are usually text fragments
consisting of one or more sentences. Although
much research exists on this task (Gir u, 2003;
Higashinaka and Isozaki, 2008; Verberne et al.,
2008; Verberne et al., 2011; Oh et al., 2012), its
performance remains much lower than that of the
state-of-the-art factoid QA systems, such as IBM’s
Watson (Ferrucci et al., 2010).
In this work, we propose a quite straightfor-
ward but novel approach for such difficult why-
QA task. Consider the sentence A1 in Table 1,
which represents the causal relation between the
cause, “the ocean’s water mass ..., waves are gen-
</bodyText>
<table confidence="0.735623125">
A1 [Tsunamis that can cause large coastal inundation
are generated]effect because [the ocean’s water
mass is displaced and, much like throwing a stone
into a pond, waves are generated.]cause
A2 [Earthquake causes seismic waves which set up
the water in motion with a large force.]cause
This causes [a tsunami.]effect
A3 [Tsunamis]effect are caused by [the sudden dis-
placement of huge volumes of water.]cause
A4 [Tsunamis weaken as they pass through
forests]effect because [the hydraulic resistance of
the trees diminish their energy.]cause
A5 [Automakers in Japan suspended production for an
array of vehicles]effect because [the magnitude 9
earthquake and tsunami hit their country on Friday,
March 11, 2011.]cause
</table>
<tableCaption confidence="0.942165">
Table 1: Examples of intra/inter-sentential causal
</tableCaption>
<bodyText confidence="0.993695148148148">
relations. Cause and effect parts of each causal re-
lation, marked with [..]cause and [..]effect, are con-
nected by the underlined cue phrases for causality,
such as because, this causes, and are caused by.
erated,” and its effect, “Tsunamis ... are gener-
ated.” This is a good answer to the question, “Why
are tsunamis generated?”, since the effect part is
more or less equivalent to the (propositional) con-
tent of the question. Our method finds text frag-
ments that include such causal relations with an
effect part that resembles a given question and pro-
vides them as answers.
Since this idea looks quite intuitive, many peo-
ple would probably consider it as a solution to
why-QA. However, to our surprise, we could not
find any previous work on why-QA that took this
approach. Some methods utilized the causal re-
lations between terms as evidence for finding an-
swers (i.e., matching a cause term with an answer
text and its effect term with a question) (Gir u,
2003; Higashinaka and Isozaki, 2008). Other ap-
proaches utilized such clue terms for causality as
“because” as evidence for finding answers (Mu-
rata et al., 2007). However, these algorithms did
not check whether an answer candidate, i.e., a text
fragment that may be provided as an answer, ex-
plicitly contains a complex causal relation sen-
</bodyText>
<page confidence="0.859306">
1733
</page>
<note confidence="0.913412">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1733–1743,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999912970149254">
tence with the effect part that resembles a ques-
tion. For example, A5 in Table 1 is an incorrect an-
swer to “Why are tsunamis generated?”, but these
previous approaches would probably choose it as a
proper answer due to “because” and “earthquake”
(i.e., a cause of tsunamis). At least in our exper-
imental setting, our approach outperformed these
simpler causality-based QA systems.
Perhaps this approach was previously deemed
infeasible due to two non-trivial technical chal-
lenges. The first challenge is to accurately iden-
tify a wide range of causal relations like those in
Table 1 in answer candidates. To meet this chal-
lenge, we developed a sequence labeling method
that identifies not only intra-sentential causal re-
lations, i.e., the causal relations between two
terms/phrases/clauses expressed in a single sen-
tence (e.g., A1 in Table 1), but also the inter-
sentential causal relations, which are the causal
relations between two terms/phrases/clauses ex-
pressed in two adjacent sentences (e.g., A2) in a
given text fragment.
The second challenge is assessing the appropri-
ateness of each identified causal relation as an an-
swer to a given question. This is important since
the causal relations identified in the answer candi-
dates may have nothing to do with a given ques-
tion. In this case, we have to reject these causal
relations because they are inappropriate as an an-
swer to the question. When a single answer candi-
date contains many causal relations, we also have
to select the appropriate ones. Consider the causal
relations in A1–A4. Those in A1–A3 are appro-
priate answers to “Why are tsunamis generated?”,
but not the one in A4. To assess the appropri-
ateness, the system must recognize textual entail-
ment, i.e., “tsunamis (are) generated” in the ques-
tion is entailed by all “tsunamis are generated” in
A1, “cause a tsunami” in A2 and “tsunamis are
caused” in A3 but not by “tsunamis weaken” in
A4. This quite difficult task is currently being
studied by many researchers in the RTE field (An-
droutsopoulos and Malakasiotis, 2010; Dagan et
al., 2010; Shima et al., 2011; Bentivogli et al.,
2011). To meet this challenge, we developed a
relatively simple method that can be seen as a
lightweight approximation for this difficult RTE
task, using excitation polarities (Hashimoto et al.,
2012).
Through our experiments on Japanese why-QA,
we show that a combination of the above methods
can improve why-QA accuracy. In addition, our
proposed method can be successfully combined
with other approaches to why-QA and can con-
tribute to higher accuracy. As a final result, we im-
proved the precision by 4.4% against all the ques-
tions in our test set over the current state-of-the-art
system of Japanese why-QA (Oh et al., 2012). The
difference in the performance became much larger
when we only compared the highly confident an-
swers of each system. When we made our sys-
tem provide only its confident answers according
to their confidence score given by our system, the
precision of these confident answers was 83.2%
for 25% of all the questions in our test set. In the
same setting, the precision of the state-of-the-art
system (Oh et al., 2012) was only 62.4%.
</bodyText>
<sectionHeader confidence="0.999782" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999899727272727">
Although there were many previous works on the
acquisition of intra- and inter-sentential causal re-
lations from texts (Khoo et al., 2000; Girju, 2003;
Inui and Okumura, 2005; Chang and Choi, 2006;
Torisawa, 2006; Blanco et al., 2008; De Saeger et
al., 2009; De Saeger et al., 2011; Riaz and Girju,
2010; Do et al., 2011; Radinsky et al., 2012), their
application to why-QA was limited to causal re-
lations between terms (Girju, 2003; Higashinaka
and Isozaki, 2008).
As previous attempts to improve why-QA per-
formance, such semantic knowledge as Word-
Net synsets (Verberne et al., 2011), semantic
word classes (Oh et al., 2012), sentiment analy-
sis (Oh et al., 2012), and causal relations between
terms (Girju, 2003; Higashinaka and Isozaki,
2008) has been used. These previous studies took
basically bag-of-words approaches and used the
semantic knowledge to identify certain seman-
tic associations using terms and n-grams. On
the other hand, our method explicitly identifies
intra- and inter-sentential causal relations between
terms/phrases/clauses that have complex struc-
tures and uses the identified relations to answer
a why-question. In other words, our method
considers more complex linguistic structures than
those used in the previous studies. Note that our
method can complement the previous approaches.
Through our experiments, we showed that it is
possible to achieve a higher precision by combin-
ing our proposed method with bag-of-words ap-
proaches considering semantic word classes and
sentiment analysis in our previous work (Oh et al.,
</bodyText>
<page confidence="0.992169">
1734
</page>
<figure confidence="0.653964666666667">
tap-n answer
candidates by answer
re--‐ranking
</figure>
<figureCaption confidence="0.999919">
Figure 1: System architecture
</figureCaption>
<bodyText confidence="0.877747">
2012).
</bodyText>
<sectionHeader confidence="0.982492" genericHeader="method">
3 System Architecture
</sectionHeader>
<bodyText confidence="0.992336961538462">
We first describe the system architecture of
our QA system before describing our proposed
method. It is composed of two components: an-
swer candidate extraction and answer re-ranking
(Fig. 1). This architecture is basically the same as
that used in our previous work (Oh et al., 2012).
We extended our previous work by introducing
causal relations recognized from answer candi-
dates to the answer re-ranking. The features used
in our previous work are very different from those
in this work, and we found that combining both
improves accuracy.
Answer candidate extraction: In our previous
work, we implemented the method of Murata et
al. (2007) for our answer candidate extractor. We
retrieved documents from Japanese web texts us-
ing Boolean AND and OR queries generated from
the content words in why-questions. Then we ex-
tracted passages of five sentences from these re-
trieved documents and ranked them with the rank-
ing function proposed by Murata et al. (2007).
This method ranks a passage higher when it con-
tains more query terms that are closer to each other
in the passage. We used a set of clue terms, includ-
ing the Japanese counterparts of cause and reason,
as query terms for the ranking. The top ranked
passages are regarded as answer candidates in the
answer re-ranking. See Murata et al. (2007) for
more details.
Answer re-ranking: Re-ranking the answer
candidates is done by a supervised classifier
(SVMs) (Vapnik, 1995). In our previous work, we
employed three types of features for training the
re-ranker: morphosyntactic features (n-grams of
morphemes and syntactic dependency chains), se-
mantic word class features (semantic word classes
obtained by automatic word clustering (Kazama
and Torisawa, 2008)) and sentiment polarity fea-
tures (word and phrase polarities). Here, we used
semantic word classes and sentiment polarities for
identifying such semantic associations between a
why-question and its answer as “if a disease’s
name appears in a question, then answers that in-
clude nutrient names are more likely to be correct”
by semantic word classes and “if something un-
desirable happens, the reason is often also some-
thing undesirable” by sentiment polarities. In this
work, we propose causal relation features gener-
ated from intra- and inter-sentential causal rela-
tions in answer candidates and use them along
with the features proposed in our previous work
for training our re-ranker.
</bodyText>
<sectionHeader confidence="0.987711" genericHeader="method">
4 Causal Relations for Why-QA
</sectionHeader>
<bodyText confidence="0.999981333333333">
We describe causal relation recognition in Sec-
tion 4.1 and describe the features (of our re-ranker)
generated from causal relations in Section 4.2.
</bodyText>
<subsectionHeader confidence="0.996409">
4.1 Causal Relation Recognition
</subsectionHeader>
<bodyText confidence="0.999973875">
We restrict causal relations to those expressed by
such cue phrases for causality as (the Japanese
counterparts of) because and as a result like in
the previous work (Khoo et al., 2000; Inui and
Okumura, 2005) and recognize them in the fol-
lowing two steps: extracting causal relation candi-
dates and recognizing causal relations from these
candidates.
</bodyText>
<subsectionHeader confidence="0.943521">
4.1.1 Extracting Causal Relation Candidates
</subsectionHeader>
<bodyText confidence="0.999989533333333">
We identify cue phrases for causality in answer
candidates using the regular expressions in Ta-
ble 2. Then, for each identified cue phrase, we
extract three sentences as a causal relation candi-
date, where one contains the cue phrase and the
other two are the previous and next sentences in
the answer candidates. When there is more than
one cue phrase in an answer candidate, we use
all of them for extracting the causal relation can-
didates, assuming that each of the cue phrases is
linked to different causal relations. We call a cue
phrase used for extracting a causal relation candi-
date a c-marker (causality marker) of the candi-
date to distinguish it from the other cue phrases in
the same causal relation candidate.
</bodyText>
<figure confidence="0.992806857142857">
Why-question
Answer candidate extraction
Answer candidate extraction
from the retrieved documents
Document retrieval from
Japanese web texts
Answer re-ranking
Answer re-ranker
tap-n answer
candidates
recognition
training
recognition
training
Causal relation
recognition model
Training data for
answer re-ranking
Training data for
causal relation
recognition
</figure>
<page confidence="0.933058">
1735
</page>
<table confidence="0.994689818181818">
Regular expressions Examples
(D|の)? ため P? ため (for), のため (for), そのため
(as a result), のために (for)
ので ので (since or because of)
こと (から|で) ことから (from the fact that), こと
で (by the fact that)
(から|ため) C からだ (because), ためだ (It is be-
cause)
D? RCT (P|C)+ 理 由 は (the reason is), 原 ® だ
(is the cause),この理由から (from
this reason)
</table>
<tableCaption confidence="0.991702">
Table 2: Regular expressions for identifying cue
</tableCaption>
<bodyText confidence="0.9032965">
phrases for causality. D, P and C represent
demonstratives (e.g., この (this) and その (that)),
postpositions (including case markers such as が
(nominative), の (genitive)), and copula (e.g., で
す (is) and である (is)) in Japanese, respectively.
RCT, which represents Japanese terms meaning
reason, cause, or thanks to, is defined as fol-
lows: RCT = {理由 (reason), 原® (cause), 要
® (cause), 引き金 (cause), おかげ (thanks to),
せい (thanks to), わけ (reason) }.
</bodyText>
<subsectionHeader confidence="0.839053">
4.1.2 Recognizing Causal Relations
</subsectionHeader>
<bodyText confidence="0.995716266666667">
Next, we recognize the spans of the cause and ef-
fect parts of a causal relation linked to a c-marker.
We regard this task as a sequence labeling problem
and use Conditional Random Fields (CRFs) (Laf-
ferty et al., 2001) as a machine learning frame-
work. In our task, CRFs take three sentences
of a causal relation candidate as input and gen-
erate their cause-effect annotations with a set of
possible cause-effect IOB labels, including Begin-
Cause (B-C), Inside-Cause (I-C), Begin-Effect (B-
E), Inside-Effect (I-E), and Outside (O). Fig 2
shows an example of such sequence labeling. Al-
though this example is about sequential labeling
shown on English sentences for ease of explana-
tion, it was actually done on Japanese sentences.
We used the three types of feature sets in Table 3
for training the CRFs, where j is in the range of
i − 4 ≤ j ≤ i + 4 for current position i in a causal
relation candidate.
Type Features
Morphological feature mj, mj+1
j , posj, posj+1
j
Syntactic feature j+1
sj, sj , bj, bj+1
j
C-marker feature (mj, cm), (mj+1
j , cm)
(sj, cm), (sj+1
j , cm)
</bodyText>
<tableCaption confidence="0.671772">
Table 3: Features for training CRFs, where
</tableCaption>
<equation confidence="0.937267">
xj+1
j = xjxj+1
</equation>
<bodyText confidence="0.984887">
Morphological features: mj and posj in Ta-
ble 3 represent the jth morpheme and the POS tag.
</bodyText>
<figure confidence="0.782887571428571">
CRFs
S1 Earthquake causes ... with I-­‐C a large force . EOS
IOB B--‐C I-­‐C I-­‐C I-­‐C I-­‐C I-­‐C I-­‐C O
S2 This causes a tsunami . EOS
IOB O O B--‐E I-­‐E I-­‐E O
S3 EOA
IOB O
</figure>
<figureCaption confidence="0.996021">
Figure 2: Recognizing causal relations by se-
quence labeling: Underlined text This causes rep-
resents a c-marker, and EOS and EOA represent
end-of-sentence and end-of-answer candidates.
</figureCaption>
<figure confidence="0.972642571428571">
root
c-marker node
増加するため、
なると
水が 氷に なると 体積が 増加するため、 氷山は 水に 浮くことができる
water ice if (it) its because (it) an water float on (water)
becomes volume increases iceberg
Subtree informa�on used for syntac�c features
subtree subtree child child c-marker subtree subtree parent
-­‐of-­‐ -­‐of-­‐
parent parent
[水が氷になると体積が増加する]causeため、[氷山は水に浮くことができる]effect
(Because [the volume of the water increases if it becomes ice]cause, [an iceberg floats
on water]effect.)
</figure>
<figureCaption confidence="0.980504">
Figure 3: Example of syntactic information related
to a c-marker used for syntactic features
</figureCaption>
<bodyText confidence="0.99959295">
We use JUMAN1, a Japanese morphological ana-
lyzer, for generating our morphological features.
Syntactic features: The span of the causal rela-
tions in a given causal relation candidate strongly
depends on the c-marker in the candidate. Es-
pecially for intra-sentential causal relations, their
cause and effect parts often appear in the subtrees
of the c-marker’s node or those of the c-marker’s
parent node in a syntactic dependency tree struc-
ture. Fig. 3 shows an example that follows this ob-
servation, where the c-marker node is represented
in a hexagon and the other nodes are in a rectan-
gle. Note that each node in Fig. 3 is a word phrase
(called a bunsetsu), which is the smallest unit of
syntactic analysis in Japanese. A bunsetsu is a
syntactic constituent composed of a content word
and several function words such as postpositions
and case markers. Syntactic dependency is repre-
sented by an arrow in Fig. 3. For example, there
is syntactic dependency from word phrase 水が
</bodyText>
<footnote confidence="0.88533">
1 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN
</footnote>
<table confidence="0.808548">
A causal relation candidate from A2
S1: Earthquake causes seismic waves which set up
the water in motion with a large force. EOS
S2: This causes a tsunami. EOS
S3: EOA
浮くことができる
</table>
<page confidence="0.794137">
1736
</page>
<bodyText confidence="0.936699954545454">
(water) to ��,BL (if (it) becomes), i.e., 水hS dep
−−→
�r,BL. We encode this subtree information into
sj, which is the syntactic information of a word
phrase to which the jth morpheme belongs. sj
only has one of six values: 1) the c-marker’s node
(c-marker), 2) the c-marker’s child node (child),
3) the c-marker’s parent node (parent), 4) in the c-
marker’s subtree but not the c-marker’s child node
(subtree), 5) in the subtree of the c-marker’s par-
ent node but not the c-marker’s node (subtree-of-
parent) and 6) the others (others). bj is the word
phrase information of the jth morpheme (mj) that
represents whether mj is in the beginning or in-
side a word phrase. For generating our syntactic
features, we use KNP2, a Japanese syntactic de-
pendency parser.
C-marker features: As our c-marker features,
we use a pair composed of c-marker cm and one
of the following: mj, mj+1
j , sj, or sj+1
j .
</bodyText>
<subsectionHeader confidence="0.985514">
4.2 Causal Relation Features
</subsectionHeader>
<bodyText confidence="0.999895777777778">
We use terms, partial trees (in a syntactic depen-
dency tree structure), and the semantic orienta-
tion of excitation (Hashimoto et al., 2012) to as-
sess the appropriateness of each causal relation ob-
tained by our causal relation recognizer as an an-
swer to a given question. Finding answers with
term matching and partial tree matching has been
used in the literature of question answering (Girju,
2003; Narayanan and Harabagiu, 2004; Moschitti
et al., 2007; Higashinaka and Isozaki, 2008; Ver-
berne et al., 2008; Surdeanu et al., 2011; Verberne
et al., 2011; Oh et al., 2012), while that with the
excitation polarity is proposed in this work.
We use three types of features. Each fea-
ture type expresses the causal relations in an an-
swer candidate that are determined to be appro-
priate as answers to a given question by term
matching (tf1–tf4), partial tree matching (pf1–
pf4) and excitation polarity matching (ef1–ef4).
We call these causal relations used for generating
our causal relation features candidates of an ap-
propriate causal relation in this section. Note that
if one answer candidate has more than one candi-
date of an appropriate causal relation found by one
matching method, we generated features for each
appropriate candidate and merged all of them for
the answer candidate.
</bodyText>
<footnote confidence="0.952">
2 http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KNP
</footnote>
<table confidence="0.99931076">
Type Description
tf1 word n-grams of causal relations
tf2 word class version of tf1
tf3 indicator for the existence of candidates of an
tf4 appropriate causal relation identified by term
matching in an answer candidate
number of matched terms in candidates of an ap-
propriate causal relation
pf1 syntactic dependency n-grams (n dependency
pf2 chain) of causal relations
pf3 word class version of pf1
pf4 indicator for the existence of candidates of an ap-
propriate causal relation identified by partial tree
matching in an answer candidate
number of matched partial trees in candidates of
an appropriate causal relation
ef1 types of noun-polarity pairs shared by causal re-
ef2 lations and the question
ef3 ef1 coupled with each noun’s word class
ef4 indicator for the existence of candidates of an ap-
propriate causal relation identified by excitation
polarity matching in an answer candidate
number of noun-polarity pairs shared by the
question and the candidates of an appropriate
causal relation
</table>
<tableCaption confidence="0.836593">
Table 4: Causal relation features: n in n-grams
is n = {2,3} and n-grams in an effect part are
distinguished from those in a cause part.
</tableCaption>
<subsubsectionHeader confidence="0.564328">
4.2.1 Term Matching
</subsubsectionHeader>
<bodyText confidence="0.999981642857143">
Our term matching method judges that a causal re-
lation is a candidate of an appropriate causal rela-
tion if its effect part contains at least one content
word (nouns, verbs, and adjectives) in the ques-
tion. For example, all the causal relations of A1–
A4 in Table 1 are candidates of an appropriate
causal relation to the question, “Why is a tsunami
generated?”, by term matching with question term
tsunami.
tf1–tf4 are generated from candidates of an ap-
propriate causal relation identified by term match-
ing. The n-grams of tf1 and tf2 are restricted
to those containing at least one content word in
a question. We distinguish this matched word
from the other words by replacing it with QW, a
special symbol representing a word in the ques-
tion. For example, word 3-gram “this/cause/QW”
is extracted from This causes tsunamis in A2 for
“Why is a tsunami generated?” Further, we cre-
ate a word class version of word n-grams by con-
verting the words in these word n-grams into their
corresponding word class using the semantic word
classes (500 classes for 5.5 million nouns) from
our previous work (Oh et al., 2012). These word
classes were created by applying the automatic
word clustering method of Kazama and Torisawa
(2008) to 600 million Japanese web pages. For
example, the word class version of word 3-gram
</bodyText>
<page confidence="0.96929">
1737
</page>
<bodyText confidence="0.999811888888889">
“this/cause/QW” is “this/cause/QW,WCtsunami”,
where WCtsunami represents the word class of
a tsunami. tf3 is a binary feature that indi-
cates the existence of candidates of an appropri-
ate causal relation identified by term matching in
an answer candidate. tf4 represents the degree
of the relevance of the candidates of an appro-
priate causal relation measured by the number of
matched terms: one, two, and more than two.
</bodyText>
<subsectionHeader confidence="0.77465">
4.2.2 Partial Tree Matching
</subsectionHeader>
<bodyText confidence="0.939365375">
Our partial tree matching method judges a causal
relation as a candidate of an appropriate causal re-
lation if its effect part contains at least one par-
tial tree in a question, where the partial tree covers
more than one content word. For example, only
the causal relation A1 among A1–A4 is a can-
didate of an appropriate causal relation for ques-
tion “Why are tsunamis generated?” by partial
tree matching because only its effect part contains
ep
partial tree “tsunamis d (are) generated” of the
question.
pf1–pf4 are generated from candidates of an
appropriate causal relation identified by the par-
tial tree matching. The syntactic dependency n-
grams in pf1 and pf2 are restricted to those that
contain at least one content word in a question. We
distinguish this matched content word from the
other content words in the n-gram by converting
it to QW, which represents a content word in the
question. For example, syntactic dependency 2-
gram “QW dep −−→ cause” and its word class version
ep
“QW,WCtsunami d cause” are extracted from
Tsunamis that can cause in A1. pf3 is a binary
feature that indicates whether an answer candidate
contains candidates of an appropriate causal rela-
tion identified by partial tree matching. pf4 rep-
resents the degree of the relevance of the candi-
date of an appropriate causal relation measured by
the number of matched partial trees: one, two, and
more than two.
</bodyText>
<subsectionHeader confidence="0.677102">
4.2.3 Excitation Polarity Matching
</subsectionHeader>
<bodyText confidence="0.999964169811321">
Hashimoto et al. (2012) proposed a semantic ori-
entation called excitation polarities. It classifies
predicates with their argument position (called
templates) into excitatory, inhibitory and neu-
tral. In the following, we denote a template
as “[argument position,predicate].” According to
Hashimoto’s definition, excitatory templates im-
ply that the function, effect, purpose, or the role of
an entity filling an argument position in the tem-
plates is activated/enhanced. On the contrary, in-
hibitory templates imply that the effect, purpose
or the role of an entity is deactivated/suppressed.
Neutral templates are those that neither activate
nor suppress the function of an argument.
We assume that the meanings of a text can
be roughly captured by checking whether each
noun in the text is activated or suppressed in the
sense of the excitation polarity framework, where
the activation and suppression of each entity (or
noun) can be detected by looking at the excita-
tion polarities of the templates that are filled by
the entity. For instance, effect part “tsunamis
that can cause large coastal inundation are gen-
erated” of A1 roughly means that “tsunamis” are
activated and “inundation” is (or can be) acti-
vated. This activation/suppression configuration
of the nouns is consistent with sentence “tsunamis
are caused” in which “tsunamis” are activated.
This consistency suggests that A1 is a good an-
swer to question “Why are tsunamis caused?”, al-
though the “tsunamis” are modified by different
predicates; “cause” and “generate.” On the other
hand, effect part “tsunamis weaken as they pass
through forests” of A4 implies that “tsunamis”
are suppressed. This suggests that A4 is not
a good answer to “Why are tsunamis caused?”
Note that the consistency checking between ac-
tivation/suppression configurations of nouns3 in
texts can be seen as a rough but lightweight ap-
proximation of the recognition of textual entail-
ments or paraphrases.
Following the definition of excitation polarity
in Hashimoto et al. (2012), we manually classi-
fied templates4 to each polarity type and obtained
8,464 excitatory templates, such as [hS, 増t5]
([subject, increase]) and [hS, 向上-�5] ([sub-
ject, improve]), 2,262 inhibitory templates, such
as [;�-, 防&lt;°] ([object, prevent]) and [hS, 死&amp;]
([subject, die]), and 7,230 neutral templates such
as [;�-, 考t5] ([object, consider]). With these
templates, we obtain activation/suppression con-
figurations (including neutral) for the nouns in the
causal relations in the answer candidates and ques-
</bodyText>
<footnote confidence="0.990071625">
3 Because the activation/suppression configurations of
nouns come from an excitation polarity of templates, “[argu-
ment position,predicate],” the semantics of verbs in the tem-
plates are implicitly considered in this consistency checking.
4 Varga et al. (2013) has used the same templates as ours,
except they restricted their excitation/inhibitory templates to
those whose polarity is consistent with that given by the au-
tomatic acquisition method of Hashimoto et al. (2012).
</footnote>
<page confidence="0.996036">
1738
</page>
<bodyText confidence="0.999758903225806">
tions.
Next, we assume that a causal relation is ap-
propriate as an answer to a question if the effect
part of the causal relation and the question share
at least one common noun with the same polarity.
More detailed information concerning the config-
urations of all the nouns in all the candidates of an
appropriate causal relation (including their cause
parts) and the question are encoded into our fea-
ture set ef1–ef4 in Table 4 and the final judgment
is done by our re-ranker.
For generating ef1 and ef2, we classified all the
nouns coupled with activation/suppression/neutral
polarities in a causal relation into three types:
SAME (the question contains the same noun with
the same polarity), DiffPOL (the question con-
tains the same noun with different polarity), and
OTHER (the others). ef1 indicates whether each
type of noun-polarity pair exists in a causal rela-
tion. Note that the types for the effect and cause
parts are represented in distinct features. ef2 is the
same as ef1 except that the types are augmented
with the word classes of the corresponding nouns.
In other words, ef2 indicates whether each type
of noun-polarity pair exists in the causal relation
for each word class. ef3 indicates the existence of
candidates of an appropriate causal relation iden-
tified by this matching scheme, and ef4 repre-
sents the number of noun-polarity pairs shared by
the question and the candidates of an appropriate
causal relations (one, two, and more than two).
</bodyText>
<sectionHeader confidence="0.998691" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999942">
We experimented with causal relation recognition
and why-QA with our causal relation features.
</bodyText>
<subsectionHeader confidence="0.998242">
5.1 Data Set for Why-Question Answering
</subsectionHeader>
<bodyText confidence="0.99998836">
For our experiments, we used the same why-QA
data set as the one used in our previous work (Oh
et al., 2012). This why-QA data set is composed
of 850 Japanese why-questions and their top-20
answer candidates obtained by answer candidate
extraction from 600 million Japanese web pages.
Three annotators checked the top-20 answer can-
didates of these 850 questions and the final judg-
ment was made by their majority vote. Their inter-
rater agreement by Fleiss’ kappa reported in Oh et
al. (2012) was substantial (n = 0.634). Among the
850 questions, 250 why-questions were extracted
from the Japanese version of Yahoo! Answers,
and another 250 were created by annotators. In
our previous work, we evaluated the system with
these 500 questions and their answer candidates as
training and test data in 10-fold cross-validation.
The other 350 why-questions were manually built
from passages describing the causes or reasons of
events/phenomena. These questions and their an-
swer candidates were used as additional training
data for testing subsamples in each fold during the
10-fold cross-validation. In our why-QA experi-
ments, we evaluated our why-QA system with the
same settings.
</bodyText>
<subsectionHeader confidence="0.999558">
5.2 Data Set for Causal Relation Recognition
</subsectionHeader>
<bodyText confidence="0.999932318181818">
We built a data set composed of manually anno-
tated causal relations for evaluating our causal re-
lation recognition. As source data for this data set,
we used the same 10-fold data that we used for
evaluating our why-QA (500 questions and their
answer candidates). We extracted the causal re-
lation candidates from the answer candidates in
each fold, and then our annotator (not an author)
manually marked the span of the cause and effect
parts of a causal relation for each causal relation
candidate, keeping in mind that the causal rela-
tion must be expressed in terms of a c-marker in
a given causal relation candidate. Finally, we had
a data set made of 16,051 causal relation candi-
dates, 8,117 of which had a true causal relation;
the number of intra- and inter-sentential causal re-
lations were 7,120 and 997, respectively.
Note that this data set can be partitioned into ten
folds by using the 10-fold partition of its source
data. We performed 10-fold cross validation to
evaluate our causal relation recognition with this
10-fold data.
</bodyText>
<subsectionHeader confidence="0.998789">
5.3 Causal Relation Recognition
</subsectionHeader>
<bodyText confidence="0.999732833333333">
We used CRF++5 for training our causal relation
recognizer. In our evaluation, we judged a sys-
tem’s output as correct if both spans of the cause
and effect parts overlapped those in the gold stan-
dard. Evaluation was done by precision, recall,
and F1.
</bodyText>
<table confidence="0.9997536">
Precision Recall Fl
BASELINE 41.9 61.0 49.7
INTRA-SENT 84.5 75.4 79.7
INTER-SENT 80.2 52.6 63.6
ALL 83.8 71.1 77.0
</table>
<tableCaption confidence="0.9998845">
Table 5: Results of causal relation recognition (%)
Table 5 shows the result. BASELINE represents
</tableCaption>
<footnote confidence="0.937967">
5 http://code.google.com/p/crfpp/
</footnote>
<page confidence="0.996247">
1739
</page>
<bodyText confidence="0.999765388888889">
the result for our baseline system that recognizes
a causal relation by simply taking the two phrases
adjacent to a c-marker (i.e., before and after) as
cause and effect parts of the causal relation. We
assumed that the system had an oracle for judging
correctly whether each phrase is a cause part or an
effect part. In other words, we judged that a causal
relation recognized by BASELINE is correct if both
cause and effect parts in the gold standard are adja-
cent to a c-marker. INTRA-SENT and INTER-SENT
represent the results for intra- and inter-sentential
causal relations and ALL represents the result for
the both causal relations by our method. From
these results, we confirmed that our method rec-
ognized both intra- and inter-sentential causal rela-
tions with over 80% precision, and it significantly
outperformed our baseline system in both preci-
sion and recall rates.
</bodyText>
<table confidence="0.9998682">
Precision Recall Fl
ALL-“MORPH” 80.8 66.4 72.9
ALL-“SYNTACTIC” 82.9 67.0 74.1
ALL-“C-MARKER” 76.3 51.4 61.4
ALL 83.8 71.1 77.0
</table>
<tableCaption confidence="0.897277">
Table 6: Ablation test results for causal relation
recognition (%)
</tableCaption>
<bodyText confidence="0.999977076923077">
We also investigated the contribution of the
three types of features used in our causal rela-
tion recognition to the performance. We evalu-
ated the performance when we removed one of
the three types of features (ALL-“MORPH”, ALL-
“SYNTACTIC” and ALL-“C-MARKER”) and com-
pared the results in these settings with the one
when all the feature sets were used (ALL). Ta-
ble 6 shows the result. We confirmed that all the
feature sets improved the performance, and we got
the best performance when using all of them. We
used the causal relations obtained from the 10-fold
cross validation for our why-QA experiments.
</bodyText>
<subsectionHeader confidence="0.994658">
5.4 Why-Question Answering
</subsectionHeader>
<bodyText confidence="0.996664630434783">
We performed why-QA experiments to confirm
the effectiveness of intra- and inter-sentential
causal relations in a why-QA task. In
this experiment, we compared five systems:
four baseline systems (MURATA, OURCF, OH
and OH+PREVCF) and our proposed method
(PROPOSED).
MURATA corresponds to our answer candidate
extraction.
OURCF uses a re-ranker trained with only our
causal relation features.
OH, which represents our previous work (Oh et
al., 2012), has a re-ranker trained with mor-
phosyntactic, semantic word class, and senti-
ment polarity features.
OH+PREVCF is a system with a re-ranker
trained with the features used in OH and with
the causal relation feature proposed in Hi-
gashinaka and Isozaki (2008). The causal re-
lation feature includes an indicator that deter-
mines whether the causal relations between
two terms appear in a question-answer pair;
cause in an answer and its effect in a question.
We acquired the causal relation instances (be-
tween terms) from 600 million Japanese web
pages using the method of De Saeger et al.
(2009) and exploited the top-100,000 causal
relation instances in this system.
PROPOSED has a re-ranker trained with our
causal relation features as well as the three
types of features proposed in Oh et al. (2012).
Comparison between OH and PROPOSED re-
veals the contribution of our causal relation
features to why-QA.
We used TinySVM6 with a linear kernel
for training the re-rankers in OURCF, OH,
OH+PREVCF and PROPOSED. Evaluation was
done by P@1 (Precision of the top-answer) and
Mean Average Precision (MAP); they are the same
measures used in Oh et al. (2012). P@1 measures
how many questions have a correct top-answer
candidate. MAP measures the overall quality of
the top-20 answer candidates. As mentioned in
Section 5.1, we used 10-fold cross-validation with
the same setting as the one used in Oh et al. (2012)
for our experiments.
</bodyText>
<table confidence="0.989800166666667">
P@1 MAP
MURATA 22.2 27.0
OURCF 27.8 31.4
OH 37.4 39.1
OH+PREVCF 37.4 38.9
PROPOSED 41.8 41.0
</table>
<tableCaption confidence="0.996674">
Table 7: Why-QA results (%)
</tableCaption>
<bodyText confidence="0.91579525">
Table 7 shows the evaluation results. Our pro-
posed method outperformed the other four sys-
tems and improved P@1 by 4.4% over OH, which
is the-state-of-the-art system for Japanese why-
</bodyText>
<footnote confidence="0.933091">
6 http://chasen.org/∼taku/software/TinySVM/
</footnote>
<page confidence="0.987492">
1740
</page>
<bodyText confidence="0.999543888888889">
QA. OURCF showed the performance improve-
ment over MURATA. Although this suggests the
effectiveness of our causal relation features, the
overall performance of OURCF was lower than
that of OH. OH+PREVCF outperformed neither
OH nor PROPOSED. This suggests that our ap-
proach is more effective than previous causality-
based approaches (Girju, 2003; Higashinaka and
Isozaki, 2008), at least in our setting.
</bodyText>
<figure confidence="0.980204">
10 20 30 40 50 60 70 80 90 100
% of questions
</figure>
<figureCaption confidence="0.984157">
Figure 4: Effect of causal relation features on the
top-answers
</figureCaption>
<bodyText confidence="0.999797675">
We also compared confident answers of
OURCF, OH, and PROPOSED by making each sys-
tem provide only the k confident top-answers (for
k questions) selected by their SVM scores given
by each system’s re-ranker. This reduces the num-
ber of questions that can be answered by a system,
but the top-answers become more reliable as k de-
creases. Fig. 4 shows this result, where the x axis
represents the percentage of questions (against all
the questions in our test set) whose top-answers
are given by each system, and the y axis repre-
sents the precision of the top-answers at a certain
point on the x axis. When both systems provided
top-answers for 25% of all the questions in our test
set, our method achieved 83.2% precision, which
is much higher than OH’s (62.4%). This exper-
iment confirmed that our causal relation features
were also effective in improving the quality of the
highly confident answers.
However, the high precision by our method was
bound to confident answers for a small number
of questions, and the difference in the precision
between OH and PROPOSED in Fig. 4 became
smaller as we considered more answers with lower
confidence. We think that one of the reasons is the
relatively small coverage of the excitation polarity
lexicon, a core resource in our excitation polarity
matching. We are planning to enlarge the lexicon
to deal with this problem.
Next, we investigated the contribution of the
intra- and inter-sentential causal relations to the
performance of our method. We used only one
of the two types of causal relations for generating
causal relation features (INTRA-SENT and INTER-
SENT) for training our re-ranker and compared the
results in these settings with the one when both
were used (ALL (PROPOSED)). Table 8 shows
the result. Both intra- and inter-sentential causal
relations contributed to the performance improve-
ment.
</bodyText>
<table confidence="0.992661">
P@1 MAP
INTER-SENT 39.0 39.7
INTRA-SENT 40.4 40.5
ALL (PROPOSED) 41.8 41.0
</table>
<tableCaption confidence="0.9761925">
Table 8: Results with/without intra- and inter-
sentential causal relations (%)
</tableCaption>
<bodyText confidence="0.999770428571429">
We also investigated the contributions of the
three types of causal relation features by ablation
tests (Table 9). When we do not use the fea-
tures by excitation polarity matching (ALL-{ef1–
ef4}), the performance is the worst. This implies
that the contribution of excitation polarity match-
ing exceeds the other two.
</bodyText>
<table confidence="0.9943016">
P@1 MAP
ALL-{tf1–tf4} 40.8 40.7
ALL-{pf1–pf4} 41.0 40.9
ALL-{ef1–ef4} 39.6 40.5
ALL (PROPOSED) 41.8 41.0
</table>
<tableCaption confidence="0.99993">
Table 9: Ablation test results for why-QA (%)
</tableCaption>
<sectionHeader confidence="0.998096" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999933071428571">
In this paper, we explored the utility of intra- and
inter-sentential causal relations for ranking answer
candidates to why-questions. We also proposed a
method for assessing the appropriateness of causal
relations as answers to a given question using the
semantic orientation of excitation. Through ex-
periments, we confirmed that these ideas are ef-
fective for improving why-QA, and our proposed
method achieved 41.8% P@1, which is 4.4% im-
provement over the current state-of-the-art system
of Japanese why-QA. We also showed that our
system achieved 83.2% precision for its confident
answers, when it only provided its confident an-
swers for 25% of all the questions in our test set.
</bodyText>
<figure confidence="0.997747384615385">
Precision (%)
100
80
50
40
30
20
90
70
60
PROPOSED
OH
OurCF
</figure>
<page confidence="0.977696">
1741
</page>
<bodyText confidence="0.991281">
Takashi Inui and Manabu Okumura. 2005. Investigat-
ing the characteristics of causal relations in Japanese
text. In In Annual Meeting of the Association
for Computational Linguistics (ACL) Workshop on
Frontiers in Corpus Annotations II: Pie in the Sky.
</bodyText>
<sectionHeader confidence="0.885083" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.77116325">
Ion Androutsopoulos and Prodromos Malakasiotis.
2010. A survey of paraphrasing and textual entail-
ment methods. Journal of Arti�cial Intelligence Re-
search (JAIR), 38(1):135–187.
</bodyText>
<reference confidence="0.999784354166667">
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Dang,
and Danilo Giampiccolo. 2011. The seventh pascal
recognizing textual entailment challenge. In Pro-
ceedings of TAC.
E. Blanco, N. Castell, and Dan I. Moldovan. 2008.
Causal relation extraction. In Proceedings of
LREC’08.
Du-Seong Chang and Key-Sun Choi. 2006. Incremen-
tal cue phrase learning and bootstrapping method for
causality extraction using cue phrase and word pair
probabilities. Information Processing and Manage-
ment, 42(3):662–678.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan
Roth. 2010. Recognizing textual entailment: Ratio-
nal, evaluation and approaches. Natural Language
Engineering, 16(1):1–17.
Stijn De Saeger, Kentaro Torisawa, Jun’ichi Kazama,
Kow Kuroda, and Masaki Murata. 2009. Large
scale relation acquisition using class dependent pat-
terns. In Proceedings of ICDM ’09, pages 764–769.
Stijn De Saeger, Kentaro Torisawa, Masaaki Tsuchida,
Jun’ichi Kazama, Chikara Hashimoto, Ichiro Ya-
mada, Jong Hoon Oh, István Varga, and Yulan Yan.
2011. Relation acquisition using word classes and
partial patterns. In Proceedings of EMNLP ’11,
pages 825–835.
Quang Xuan Do, Yee Seng Chan, and Dan Roth. 2011.
Minimally supervised event causality identification.
In Proceedings of EMNLP ’11, pages 294–303.
David A. Ferrucci, Eric W. Brown, Jennifer Chu-
Carroll, James Fan, David Gondek, Aditya Kalyan-
pur, Adam Lally, J. William Murdock, Eric Nyberg,
John M. Prager, Nico Schlaefer, and Christopher A.
Welty. 2010. Building Watson: An overview of the
DeepQA project. AI Magazine, 31(3):59–79.
Roxana Girju. 2003. Automatic detection of causal
relations for question answering. In Proceedings of
the ACL 2003 workshop on Multilingual summariza-
tion and question answering, pages 76–83.
Chikara Hashimoto, Kentaro Torisawa, Stijn De
Saeger, Jong-Hoon Oh, and Jun’ichi Kazama. 2012.
Excitatory or inhibitory: A new semantic orientation
extracts contradiction and causality from the web. In
Proceedings of EMNLP-CoNLL ’12.
Ryuichiro Higashinaka and Hideki Isozaki. 2008.
Corpus-based question answering for why-
questions. In Proceedings of IJCNLP ’08, pages
418–425.
Jun’ichi Kazama and Kentaro Torisawa. 2008. In-
ducing gazetteers for named entity recognition by
large-scale clustering of dependency relations. In
Proceedings ofACL-08: HLT, pages 407–415.
Christopher S. G. Khoo, Syin Chan, and Yun Niu.
2000. Extracting causal knowledge from a medical
database using graphical patterns. In Proceedings of
ACL ’00, pages 336–343.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of ICML ’01, pages
282–289.
Alessandro Moschitti, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploiting
syntactic and shallow semantic kernels for question
answer classification. In Proceedings of ACL ’07,
pages 776–783.
Masaki Murata, Sachiyo Tsukawaki, Toshiyuki Kana-
maru, Qing Ma, and Hitoshi Isahara. 2007. A sys-
tem for answering non-factoid Japanese questions
by using passage retrieval weighted based on type
of answer. In Proceedings of NTCIR-6.
Srini Narayanan and Sanda Harabagiu. 2004. Ques-
tion answering based on semantic structures. In Pro-
ceedings of COLING ’04, pages 693–701.
Jong-Hoon Oh, Kentaro Torisawa, Chikara Hashimoto,
Takuya Kawada, Stijn De Saeger, Jun’ichi Kazama,
and Yiou Wang. 2012. Why question answering
using sentiment analysis and word classes. In Pro-
ceedings of EMNLP-CoNLL ’12, pages 368–378.
Kira Radinsky, Sagie Davidovich, and Shaul
Markovitch. 2012. Learning causality for
news events prediction. In Proceedings of WWW
’12, pages 909–918.
Mehwish Riaz and Roxana Girju. 2010. Another look
at causality: Discovering scenario-specific contin-
gency relationships with no supervision. In ICSC
’10, pages 361–368.
Hideki Shima, Hiroshi Kanayama, Cheng wei Lee,
Chuan jie Lin, Teruko Mitamura, Yusuke Miyao,
Shuming Shi, and Koichi Takeda. 2011. Overview
of NTCIR-9 RITE: Recognizing Inference in TExt.
In Proceedings of NTCIR-9.
Mihai Surdeanu, Massimiliano Ciaramita, and Hugo
Zaragoza. 2011. Learning to rank answers to non-
factoid questions from web collections. Computa-
tional Linguistics, 37(2):351–383.
</reference>
<page confidence="0.852758">
1742
</page>
<reference confidence="0.993511105263158">
Kentaro Torisawa. 2006. Acquiring inference rules
with temporal constraints by using japanese coordi-
nated sentences and noun-verb co-occurrences. In
Proceedings of HLT-NAACL ’06, pages 57–64.
Vladimir N. Vapnik. 1995. The nature of statistical
learning theory. Springer-Verlag New York, Inc.,
New York, NY, USA.
Istvan Varga, Motoki Sano, Kentaro Torisawa, Chikara
Hashimoto, Kiyonori Ohtake, Takao Kawai, Jong-
Hoon Oh, and Stijn De Saeger. 2013. Aid is out
there: Looking for help from tweets during a large
scale disaster. In Proceedings of ACL ’13.
Suzan Verberne, Lou Boves, Nelleke Oostdijk, and
Peter-Arno Coppen. 2008. Using syntactic infor-
mation for improving why-question answering. In
Proceedings of COLING ’08, pages 953–960.
Suzan Verberne, Lou Boves, and Wessel Kraaij. 2011.
Bringing why-qa to web search. In Proceedings of
ECIR ’11, pages 491–496.
</reference>
<page confidence="0.975755">
1743
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.674855">
<title confidence="0.9995685">Why-Question Answering using Intraand Inter-Sentential Causal Relations</title>
<author confidence="0.985228">Kentaro Hashimoto tMotoki</author>
<affiliation confidence="0.9324965">De Information Analysis Universal Communication Research Institute National Institute of Information and Communications Technology (NICT)</affiliation>
<abstract confidence="0.995492318181818">In this paper, we explore the utility of causal relations between terms or clauses as evidence for answering why-questions. To the best of our knowledge, this is the first work that uses both intraand inter-sentential causal relations for why-QA. We also propose a method for assessing the appropriateness of causal relations as answers to a given question using the semantic orientaof by Hashimoto et al. (2012). By applying these ideas to Japanese why-QA, we improved precision by 4.4% against all the questions in our test set over the current state-of-theart system for Japanese why-QA. In addition, unlike the state-of-the-art system, our system could achieve very high precision (83.2%) for 25% of all the questions in the test set by restricting its output to the confident answers only.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Luisa Bentivogli</author>
<author>Peter Clark</author>
<author>Ido Dagan</author>
<author>Hoa Dang</author>
<author>Danilo Giampiccolo</author>
</authors>
<title>The seventh pascal recognizing textual entailment challenge.</title>
<date>2011</date>
<booktitle>In Proceedings of TAC.</booktitle>
<contexts>
<context position="6372" citStr="Bentivogli et al., 2011" startWordPosition="1015" endWordPosition="1018">t the appropriate ones. Consider the causal relations in A1–A4. Those in A1–A3 are appropriate answers to “Why are tsunamis generated?”, but not the one in A4. To assess the appropriateness, the system must recognize textual entailment, i.e., “tsunamis (are) generated” in the question is entailed by all “tsunamis are generated” in A1, “cause a tsunami” in A2 and “tsunamis are caused” in A3 but not by “tsunamis weaken” in A4. This quite difficult task is currently being studied by many researchers in the RTE field (Androutsopoulos and Malakasiotis, 2010; Dagan et al., 2010; Shima et al., 2011; Bentivogli et al., 2011). To meet this challenge, we developed a relatively simple method that can be seen as a lightweight approximation for this difficult RTE task, using excitation polarities (Hashimoto et al., 2012). Through our experiments on Japanese why-QA, we show that a combination of the above methods can improve why-QA accuracy. In addition, our proposed method can be successfully combined with other approaches to why-QA and can contribute to higher accuracy. As a final result, we improved the precision by 4.4% against all the questions in our test set over the current state-of-the-art system of Japanese w</context>
</contexts>
<marker>Bentivogli, Clark, Dagan, Dang, Giampiccolo, 2011</marker>
<rawString>Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Dang, and Danilo Giampiccolo. 2011. The seventh pascal recognizing textual entailment challenge. In Proceedings of TAC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Blanco</author>
<author>N Castell</author>
<author>Dan I Moldovan</author>
</authors>
<title>Causal relation extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of LREC’08.</booktitle>
<contexts>
<context position="7677" citStr="Blanco et al., 2008" startWordPosition="1233" endWordPosition="1236">only compared the highly confident answers of each system. When we made our system provide only its confident answers according to their confidence score given by our system, the precision of these confident answers was 83.2% for 25% of all the questions in our test set. In the same setting, the precision of the state-of-the-art system (Oh et al., 2012) was only 62.4%. 2 Related Work Although there were many previous works on the acquisition of intra- and inter-sentential causal relations from texts (Khoo et al., 2000; Girju, 2003; Inui and Okumura, 2005; Chang and Choi, 2006; Torisawa, 2006; Blanco et al., 2008; De Saeger et al., 2009; De Saeger et al., 2011; Riaz and Girju, 2010; Do et al., 2011; Radinsky et al., 2012), their application to why-QA was limited to causal relations between terms (Girju, 2003; Higashinaka and Isozaki, 2008). As previous attempts to improve why-QA performance, such semantic knowledge as WordNet synsets (Verberne et al., 2011), semantic word classes (Oh et al., 2012), sentiment analysis (Oh et al., 2012), and causal relations between terms (Girju, 2003; Higashinaka and Isozaki, 2008) has been used. These previous studies took basically bag-of-words approaches and used th</context>
</contexts>
<marker>Blanco, Castell, Moldovan, 2008</marker>
<rawString>E. Blanco, N. Castell, and Dan I. Moldovan. 2008. Causal relation extraction. In Proceedings of LREC’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Du-Seong Chang</author>
<author>Key-Sun Choi</author>
</authors>
<title>Incremental cue phrase learning and bootstrapping method for causality extraction using cue phrase and word pair probabilities.</title>
<date>2006</date>
<booktitle>Information Processing and Management,</booktitle>
<volume>42</volume>
<issue>3</issue>
<contexts>
<context position="7640" citStr="Chang and Choi, 2006" startWordPosition="1227" endWordPosition="1230">erformance became much larger when we only compared the highly confident answers of each system. When we made our system provide only its confident answers according to their confidence score given by our system, the precision of these confident answers was 83.2% for 25% of all the questions in our test set. In the same setting, the precision of the state-of-the-art system (Oh et al., 2012) was only 62.4%. 2 Related Work Although there were many previous works on the acquisition of intra- and inter-sentential causal relations from texts (Khoo et al., 2000; Girju, 2003; Inui and Okumura, 2005; Chang and Choi, 2006; Torisawa, 2006; Blanco et al., 2008; De Saeger et al., 2009; De Saeger et al., 2011; Riaz and Girju, 2010; Do et al., 2011; Radinsky et al., 2012), their application to why-QA was limited to causal relations between terms (Girju, 2003; Higashinaka and Isozaki, 2008). As previous attempts to improve why-QA performance, such semantic knowledge as WordNet synsets (Verberne et al., 2011), semantic word classes (Oh et al., 2012), sentiment analysis (Oh et al., 2012), and causal relations between terms (Girju, 2003; Higashinaka and Isozaki, 2008) has been used. These previous studies took basicall</context>
</contexts>
<marker>Chang, Choi, 2006</marker>
<rawString>Du-Seong Chang and Key-Sun Choi. 2006. Incremental cue phrase learning and bootstrapping method for causality extraction using cue phrase and word pair probabilities. Information Processing and Management, 42(3):662–678.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Bill Dolan</author>
<author>Bernardo Magnini</author>
<author>Dan Roth</author>
</authors>
<title>Recognizing textual entailment: Rational, evaluation and approaches.</title>
<date>2010</date>
<journal>Natural Language Engineering,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="6326" citStr="Dagan et al., 2010" startWordPosition="1007" endWordPosition="1010"> causal relations, we also have to select the appropriate ones. Consider the causal relations in A1–A4. Those in A1–A3 are appropriate answers to “Why are tsunamis generated?”, but not the one in A4. To assess the appropriateness, the system must recognize textual entailment, i.e., “tsunamis (are) generated” in the question is entailed by all “tsunamis are generated” in A1, “cause a tsunami” in A2 and “tsunamis are caused” in A3 but not by “tsunamis weaken” in A4. This quite difficult task is currently being studied by many researchers in the RTE field (Androutsopoulos and Malakasiotis, 2010; Dagan et al., 2010; Shima et al., 2011; Bentivogli et al., 2011). To meet this challenge, we developed a relatively simple method that can be seen as a lightweight approximation for this difficult RTE task, using excitation polarities (Hashimoto et al., 2012). Through our experiments on Japanese why-QA, we show that a combination of the above methods can improve why-QA accuracy. In addition, our proposed method can be successfully combined with other approaches to why-QA and can contribute to higher accuracy. As a final result, we improved the precision by 4.4% against all the questions in our test set over the</context>
</contexts>
<marker>Dagan, Dolan, Magnini, Roth, 2010</marker>
<rawString>Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth. 2010. Recognizing textual entailment: Rational, evaluation and approaches. Natural Language Engineering, 16(1):1–17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stijn De Saeger</author>
<author>Kentaro Torisawa</author>
<author>Jun’ichi Kazama</author>
<author>Kow Kuroda</author>
<author>Masaki Murata</author>
</authors>
<title>Large scale relation acquisition using class dependent patterns.</title>
<date>2009</date>
<booktitle>In Proceedings of ICDM ’09,</booktitle>
<pages>764--769</pages>
<marker>De Saeger, Torisawa, Kazama, Kuroda, Murata, 2009</marker>
<rawString>Stijn De Saeger, Kentaro Torisawa, Jun’ichi Kazama, Kow Kuroda, and Masaki Murata. 2009. Large scale relation acquisition using class dependent patterns. In Proceedings of ICDM ’09, pages 764–769.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stijn De Saeger</author>
</authors>
<title>Kentaro Torisawa, Masaaki Tsuchida, Jun’ichi Kazama, Chikara Hashimoto, Ichiro Yamada, Jong Hoon Oh, István Varga, and Yulan Yan.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP ’11,</booktitle>
<pages>825--835</pages>
<marker>De Saeger, 2011</marker>
<rawString>Stijn De Saeger, Kentaro Torisawa, Masaaki Tsuchida, Jun’ichi Kazama, Chikara Hashimoto, Ichiro Yamada, Jong Hoon Oh, István Varga, and Yulan Yan. 2011. Relation acquisition using word classes and partial patterns. In Proceedings of EMNLP ’11, pages 825–835.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quang Xuan Do</author>
<author>Yee Seng Chan</author>
<author>Dan Roth</author>
</authors>
<title>Minimally supervised event causality identification.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP ’11,</booktitle>
<pages>294--303</pages>
<contexts>
<context position="7764" citStr="Do et al., 2011" startWordPosition="1251" endWordPosition="1254">only its confident answers according to their confidence score given by our system, the precision of these confident answers was 83.2% for 25% of all the questions in our test set. In the same setting, the precision of the state-of-the-art system (Oh et al., 2012) was only 62.4%. 2 Related Work Although there were many previous works on the acquisition of intra- and inter-sentential causal relations from texts (Khoo et al., 2000; Girju, 2003; Inui and Okumura, 2005; Chang and Choi, 2006; Torisawa, 2006; Blanco et al., 2008; De Saeger et al., 2009; De Saeger et al., 2011; Riaz and Girju, 2010; Do et al., 2011; Radinsky et al., 2012), their application to why-QA was limited to causal relations between terms (Girju, 2003; Higashinaka and Isozaki, 2008). As previous attempts to improve why-QA performance, such semantic knowledge as WordNet synsets (Verberne et al., 2011), semantic word classes (Oh et al., 2012), sentiment analysis (Oh et al., 2012), and causal relations between terms (Girju, 2003; Higashinaka and Isozaki, 2008) has been used. These previous studies took basically bag-of-words approaches and used the semantic knowledge to identify certain semantic associations using terms and n-grams.</context>
</contexts>
<marker>Do, Chan, Roth, 2011</marker>
<rawString>Quang Xuan Do, Yee Seng Chan, and Dan Roth. 2011. Minimally supervised event causality identification. In Proceedings of EMNLP ’11, pages 294–303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Ferrucci</author>
<author>Eric W Brown</author>
<author>Jennifer ChuCarroll</author>
<author>James Fan</author>
<author>David Gondek</author>
<author>Aditya Kalyanpur</author>
<author>Adam Lally</author>
<author>J William Murdock</author>
<author>Eric Nyberg</author>
<author>John M Prager</author>
<author>Nico Schlaefer</author>
<author>Christopher A Welty</author>
</authors>
<title>Building Watson: An overview of the DeepQA project.</title>
<date>2010</date>
<journal>AI Magazine,</journal>
<volume>31</volume>
<issue>3</issue>
<contexts>
<context position="1777" citStr="Ferrucci et al., 2010" startWordPosition="265" endWordPosition="268">25% of all the questions in the test set by restricting its output to the confident answers only. 1 Introduction “Why-question answering” (why-QA) is a task to retrieve answers from a given text archive for a why-question, such as “Why are tsunamis generated?” The answers are usually text fragments consisting of one or more sentences. Although much research exists on this task (Gir u, 2003; Higashinaka and Isozaki, 2008; Verberne et al., 2008; Verberne et al., 2011; Oh et al., 2012), its performance remains much lower than that of the state-of-the-art factoid QA systems, such as IBM’s Watson (Ferrucci et al., 2010). In this work, we propose a quite straightforward but novel approach for such difficult whyQA task. Consider the sentence A1 in Table 1, which represents the causal relation between the cause, “the ocean’s water mass ..., waves are genA1 [Tsunamis that can cause large coastal inundation are generated]effect because [the ocean’s water mass is displaced and, much like throwing a stone into a pond, waves are generated.]cause A2 [Earthquake causes seismic waves which set up the water in motion with a large force.]cause This causes [a tsunami.]effect A3 [Tsunamis]effect are caused by [the sudden d</context>
</contexts>
<marker>Ferrucci, Brown, ChuCarroll, Fan, Gondek, Kalyanpur, Lally, Murdock, Nyberg, Prager, Schlaefer, Welty, 2010</marker>
<rawString>David A. Ferrucci, Eric W. Brown, Jennifer ChuCarroll, James Fan, David Gondek, Aditya Kalyanpur, Adam Lally, J. William Murdock, Eric Nyberg, John M. Prager, Nico Schlaefer, and Christopher A. Welty. 2010. Building Watson: An overview of the DeepQA project. AI Magazine, 31(3):59–79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
</authors>
<title>Automatic detection of causal relations for question answering.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL</booktitle>
<pages>76--83</pages>
<contexts>
<context position="7594" citStr="Girju, 2003" startWordPosition="1221" endWordPosition="1222">t al., 2012). The difference in the performance became much larger when we only compared the highly confident answers of each system. When we made our system provide only its confident answers according to their confidence score given by our system, the precision of these confident answers was 83.2% for 25% of all the questions in our test set. In the same setting, the precision of the state-of-the-art system (Oh et al., 2012) was only 62.4%. 2 Related Work Although there were many previous works on the acquisition of intra- and inter-sentential causal relations from texts (Khoo et al., 2000; Girju, 2003; Inui and Okumura, 2005; Chang and Choi, 2006; Torisawa, 2006; Blanco et al., 2008; De Saeger et al., 2009; De Saeger et al., 2011; Riaz and Girju, 2010; Do et al., 2011; Radinsky et al., 2012), their application to why-QA was limited to causal relations between terms (Girju, 2003; Higashinaka and Isozaki, 2008). As previous attempts to improve why-QA performance, such semantic knowledge as WordNet synsets (Verberne et al., 2011), semantic word classes (Oh et al., 2012), sentiment analysis (Oh et al., 2012), and causal relations between terms (Girju, 2003; Higashinaka and Isozaki, 2008) has b</context>
<context position="18692" citStr="Girju, 2003" startWordPosition="3041" endWordPosition="3042">KNP2, a Japanese syntactic dependency parser. C-marker features: As our c-marker features, we use a pair composed of c-marker cm and one of the following: mj, mj+1 j , sj, or sj+1 j . 4.2 Causal Relation Features We use terms, partial trees (in a syntactic dependency tree structure), and the semantic orientation of excitation (Hashimoto et al., 2012) to assess the appropriateness of each causal relation obtained by our causal relation recognizer as an answer to a given question. Finding answers with term matching and partial tree matching has been used in the literature of question answering (Girju, 2003; Narayanan and Harabagiu, 2004; Moschitti et al., 2007; Higashinaka and Isozaki, 2008; Verberne et al., 2008; Surdeanu et al., 2011; Verberne et al., 2011; Oh et al., 2012), while that with the excitation polarity is proposed in this work. We use three types of features. Each feature type expresses the causal relations in an answer candidate that are determined to be appropriate as answers to a given question by term matching (tf1–tf4), partial tree matching (pf1– pf4) and excitation polarity matching (ef1–ef4). We call these causal relations used for generating our causal relation features c</context>
<context position="35652" citStr="Girju, 2003" startWordPosition="5783" endWordPosition="5784"> 41.8 41.0 Table 7: Why-QA results (%) Table 7 shows the evaluation results. Our proposed method outperformed the other four systems and improved P@1 by 4.4% over OH, which is the-state-of-the-art system for Japanese why6 http://chasen.org/∼taku/software/TinySVM/ 1740 QA. OURCF showed the performance improvement over MURATA. Although this suggests the effectiveness of our causal relation features, the overall performance of OURCF was lower than that of OH. OH+PREVCF outperformed neither OH nor PROPOSED. This suggests that our approach is more effective than previous causalitybased approaches (Girju, 2003; Higashinaka and Isozaki, 2008), at least in our setting. 10 20 30 40 50 60 70 80 90 100 % of questions Figure 4: Effect of causal relation features on the top-answers We also compared confident answers of OURCF, OH, and PROPOSED by making each system provide only the k confident top-answers (for k questions) selected by their SVM scores given by each system’s re-ranker. This reduces the number of questions that can be answered by a system, but the top-answers become more reliable as k decreases. Fig. 4 shows this result, where the x axis represents the percentage of questions (against all th</context>
</contexts>
<marker>Girju, 2003</marker>
<rawString>Roxana Girju. 2003. Automatic detection of causal relations for question answering. In Proceedings of the ACL 2003 workshop on Multilingual summarization and question answering, pages 76–83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chikara Hashimoto</author>
<author>Kentaro Torisawa</author>
<author>Stijn De Saeger</author>
<author>Jong-Hoon Oh</author>
<author>Jun’ichi Kazama</author>
</authors>
<title>Excitatory or inhibitory: A new semantic orientation extracts contradiction and causality from the web.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP-CoNLL</booktitle>
<pages>12</pages>
<marker>Hashimoto, Torisawa, De Saeger, Oh, Kazama, 2012</marker>
<rawString>Chikara Hashimoto, Kentaro Torisawa, Stijn De Saeger, Jong-Hoon Oh, and Jun’ichi Kazama. 2012. Excitatory or inhibitory: A new semantic orientation extracts contradiction and causality from the web. In Proceedings of EMNLP-CoNLL ’12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryuichiro Higashinaka</author>
<author>Hideki Isozaki</author>
</authors>
<title>Corpus-based question answering for whyquestions.</title>
<date>2008</date>
<booktitle>In Proceedings of IJCNLP ’08,</booktitle>
<pages>418--425</pages>
<contexts>
<context position="1578" citStr="Higashinaka and Isozaki, 2008" startWordPosition="232" endWordPosition="235">inst all the questions in our test set over the current state-of-theart system for Japanese why-QA. In addition, unlike the state-of-the-art system, our system could achieve very high precision (83.2%) for 25% of all the questions in the test set by restricting its output to the confident answers only. 1 Introduction “Why-question answering” (why-QA) is a task to retrieve answers from a given text archive for a why-question, such as “Why are tsunamis generated?” The answers are usually text fragments consisting of one or more sentences. Although much research exists on this task (Gir u, 2003; Higashinaka and Isozaki, 2008; Verberne et al., 2008; Verberne et al., 2011; Oh et al., 2012), its performance remains much lower than that of the state-of-the-art factoid QA systems, such as IBM’s Watson (Ferrucci et al., 2010). In this work, we propose a quite straightforward but novel approach for such difficult whyQA task. Consider the sentence A1 in Table 1, which represents the causal relation between the cause, “the ocean’s water mass ..., waves are genA1 [Tsunamis that can cause large coastal inundation are generated]effect because [the ocean’s water mass is displaced and, much like throwing a stone into a pond, w</context>
<context position="3775" citStr="Higashinaka and Isozaki, 2008" startWordPosition="590" endWordPosition="593">ss equivalent to the (propositional) content of the question. Our method finds text fragments that include such causal relations with an effect part that resembles a given question and provides them as answers. Since this idea looks quite intuitive, many people would probably consider it as a solution to why-QA. However, to our surprise, we could not find any previous work on why-QA that took this approach. Some methods utilized the causal relations between terms as evidence for finding answers (i.e., matching a cause term with an answer text and its effect term with a question) (Gir u, 2003; Higashinaka and Isozaki, 2008). Other approaches utilized such clue terms for causality as “because” as evidence for finding answers (Murata et al., 2007). However, these algorithms did not check whether an answer candidate, i.e., a text fragment that may be provided as an answer, explicitly contains a complex causal relation sen1733 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1733–1743, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics tence with the effect part that resembles a question. For example, A5 in Table 1 is an incorrect answer t</context>
<context position="7908" citStr="Higashinaka and Isozaki, 2008" startWordPosition="1273" endWordPosition="1276">83.2% for 25% of all the questions in our test set. In the same setting, the precision of the state-of-the-art system (Oh et al., 2012) was only 62.4%. 2 Related Work Although there were many previous works on the acquisition of intra- and inter-sentential causal relations from texts (Khoo et al., 2000; Girju, 2003; Inui and Okumura, 2005; Chang and Choi, 2006; Torisawa, 2006; Blanco et al., 2008; De Saeger et al., 2009; De Saeger et al., 2011; Riaz and Girju, 2010; Do et al., 2011; Radinsky et al., 2012), their application to why-QA was limited to causal relations between terms (Girju, 2003; Higashinaka and Isozaki, 2008). As previous attempts to improve why-QA performance, such semantic knowledge as WordNet synsets (Verberne et al., 2011), semantic word classes (Oh et al., 2012), sentiment analysis (Oh et al., 2012), and causal relations between terms (Girju, 2003; Higashinaka and Isozaki, 2008) has been used. These previous studies took basically bag-of-words approaches and used the semantic knowledge to identify certain semantic associations using terms and n-grams. On the other hand, our method explicitly identifies intra- and inter-sentential causal relations between terms/phrases/clauses that have comple</context>
<context position="18778" citStr="Higashinaka and Isozaki, 2008" startWordPosition="3051" endWordPosition="3054">our c-marker features, we use a pair composed of c-marker cm and one of the following: mj, mj+1 j , sj, or sj+1 j . 4.2 Causal Relation Features We use terms, partial trees (in a syntactic dependency tree structure), and the semantic orientation of excitation (Hashimoto et al., 2012) to assess the appropriateness of each causal relation obtained by our causal relation recognizer as an answer to a given question. Finding answers with term matching and partial tree matching has been used in the literature of question answering (Girju, 2003; Narayanan and Harabagiu, 2004; Moschitti et al., 2007; Higashinaka and Isozaki, 2008; Verberne et al., 2008; Surdeanu et al., 2011; Verberne et al., 2011; Oh et al., 2012), while that with the excitation polarity is proposed in this work. We use three types of features. Each feature type expresses the causal relations in an answer candidate that are determined to be appropriate as answers to a given question by term matching (tf1–tf4), partial tree matching (pf1– pf4) and excitation polarity matching (ef1–ef4). We call these causal relations used for generating our causal relation features candidates of an appropriate causal relation in this section. Note that if one answer c</context>
<context position="33788" citStr="Higashinaka and Isozaki (2008)" startWordPosition="5479" endWordPosition="5483"> inter-sentential causal relations in a why-QA task. In this experiment, we compared five systems: four baseline systems (MURATA, OURCF, OH and OH+PREVCF) and our proposed method (PROPOSED). MURATA corresponds to our answer candidate extraction. OURCF uses a re-ranker trained with only our causal relation features. OH, which represents our previous work (Oh et al., 2012), has a re-ranker trained with morphosyntactic, semantic word class, and sentiment polarity features. OH+PREVCF is a system with a re-ranker trained with the features used in OH and with the causal relation feature proposed in Higashinaka and Isozaki (2008). The causal relation feature includes an indicator that determines whether the causal relations between two terms appear in a question-answer pair; cause in an answer and its effect in a question. We acquired the causal relation instances (between terms) from 600 million Japanese web pages using the method of De Saeger et al. (2009) and exploited the top-100,000 causal relation instances in this system. PROPOSED has a re-ranker trained with our causal relation features as well as the three types of features proposed in Oh et al. (2012). Comparison between OH and PROPOSED reveals the contribut</context>
<context position="35684" citStr="Higashinaka and Isozaki, 2008" startWordPosition="5785" endWordPosition="5788">ble 7: Why-QA results (%) Table 7 shows the evaluation results. Our proposed method outperformed the other four systems and improved P@1 by 4.4% over OH, which is the-state-of-the-art system for Japanese why6 http://chasen.org/∼taku/software/TinySVM/ 1740 QA. OURCF showed the performance improvement over MURATA. Although this suggests the effectiveness of our causal relation features, the overall performance of OURCF was lower than that of OH. OH+PREVCF outperformed neither OH nor PROPOSED. This suggests that our approach is more effective than previous causalitybased approaches (Girju, 2003; Higashinaka and Isozaki, 2008), at least in our setting. 10 20 30 40 50 60 70 80 90 100 % of questions Figure 4: Effect of causal relation features on the top-answers We also compared confident answers of OURCF, OH, and PROPOSED by making each system provide only the k confident top-answers (for k questions) selected by their SVM scores given by each system’s re-ranker. This reduces the number of questions that can be answered by a system, but the top-answers become more reliable as k decreases. Fig. 4 shows this result, where the x axis represents the percentage of questions (against all the questions in our test set) who</context>
</contexts>
<marker>Higashinaka, Isozaki, 2008</marker>
<rawString>Ryuichiro Higashinaka and Hideki Isozaki. 2008. Corpus-based question answering for whyquestions. In Proceedings of IJCNLP ’08, pages 418–425.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun’ichi Kazama</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Inducing gazetteers for named entity recognition by large-scale clustering of dependency relations.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08: HLT,</booktitle>
<pages>407--415</pages>
<contexts>
<context position="10819" citStr="Kazama and Torisawa, 2008" startWordPosition="1727" endWordPosition="1730"> of clue terms, including the Japanese counterparts of cause and reason, as query terms for the ranking. The top ranked passages are regarded as answer candidates in the answer re-ranking. See Murata et al. (2007) for more details. Answer re-ranking: Re-ranking the answer candidates is done by a supervised classifier (SVMs) (Vapnik, 1995). In our previous work, we employed three types of features for training the re-ranker: morphosyntactic features (n-grams of morphemes and syntactic dependency chains), semantic word class features (semantic word classes obtained by automatic word clustering (Kazama and Torisawa, 2008)) and sentiment polarity features (word and phrase polarities). Here, we used semantic word classes and sentiment polarities for identifying such semantic associations between a why-question and its answer as “if a disease’s name appears in a question, then answers that include nutrient names are more likely to be correct” by semantic word classes and “if something undesirable happens, the reason is often also something undesirable” by sentiment polarities. In this work, we propose causal relation features generated from intra- and inter-sentential causal relations in answer candidates and use</context>
<context position="22005" citStr="Kazama and Torisawa (2008)" startWordPosition="3584" endWordPosition="3587">in a question. We distinguish this matched word from the other words by replacing it with QW, a special symbol representing a word in the question. For example, word 3-gram “this/cause/QW” is extracted from This causes tsunamis in A2 for “Why is a tsunami generated?” Further, we create a word class version of word n-grams by converting the words in these word n-grams into their corresponding word class using the semantic word classes (500 classes for 5.5 million nouns) from our previous work (Oh et al., 2012). These word classes were created by applying the automatic word clustering method of Kazama and Torisawa (2008) to 600 million Japanese web pages. For example, the word class version of word 3-gram 1737 “this/cause/QW” is “this/cause/QW,WCtsunami”, where WCtsunami represents the word class of a tsunami. tf3 is a binary feature that indicates the existence of candidates of an appropriate causal relation identified by term matching in an answer candidate. tf4 represents the degree of the relevance of the candidates of an appropriate causal relation measured by the number of matched terms: one, two, and more than two. 4.2.2 Partial Tree Matching Our partial tree matching method judges a causal relation as</context>
</contexts>
<marker>Kazama, Torisawa, 2008</marker>
<rawString>Jun’ichi Kazama and Kentaro Torisawa. 2008. Inducing gazetteers for named entity recognition by large-scale clustering of dependency relations. In Proceedings ofACL-08: HLT, pages 407–415.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher S G Khoo</author>
<author>Syin Chan</author>
<author>Yun Niu</author>
</authors>
<title>Extracting causal knowledge from a medical database using graphical patterns.</title>
<date>2000</date>
<booktitle>In Proceedings of ACL ’00,</booktitle>
<pages>336--343</pages>
<contexts>
<context position="7581" citStr="Khoo et al., 2000" startWordPosition="1217" endWordPosition="1220">panese why-QA (Oh et al., 2012). The difference in the performance became much larger when we only compared the highly confident answers of each system. When we made our system provide only its confident answers according to their confidence score given by our system, the precision of these confident answers was 83.2% for 25% of all the questions in our test set. In the same setting, the precision of the state-of-the-art system (Oh et al., 2012) was only 62.4%. 2 Related Work Although there were many previous works on the acquisition of intra- and inter-sentential causal relations from texts (Khoo et al., 2000; Girju, 2003; Inui and Okumura, 2005; Chang and Choi, 2006; Torisawa, 2006; Blanco et al., 2008; De Saeger et al., 2009; De Saeger et al., 2011; Riaz and Girju, 2010; Do et al., 2011; Radinsky et al., 2012), their application to why-QA was limited to causal relations between terms (Girju, 2003; Higashinaka and Isozaki, 2008). As previous attempts to improve why-QA performance, such semantic knowledge as WordNet synsets (Verberne et al., 2011), semantic word classes (Oh et al., 2012), sentiment analysis (Oh et al., 2012), and causal relations between terms (Girju, 2003; Higashinaka and Isozaki</context>
<context position="11901" citStr="Khoo et al., 2000" startWordPosition="1899" endWordPosition="1902">this work, we propose causal relation features generated from intra- and inter-sentential causal relations in answer candidates and use them along with the features proposed in our previous work for training our re-ranker. 4 Causal Relations for Why-QA We describe causal relation recognition in Section 4.1 and describe the features (of our re-ranker) generated from causal relations in Section 4.2. 4.1 Causal Relation Recognition We restrict causal relations to those expressed by such cue phrases for causality as (the Japanese counterparts of) because and as a result like in the previous work (Khoo et al., 2000; Inui and Okumura, 2005) and recognize them in the following two steps: extracting causal relation candidates and recognizing causal relations from these candidates. 4.1.1 Extracting Causal Relation Candidates We identify cue phrases for causality in answer candidates using the regular expressions in Table 2. Then, for each identified cue phrase, we extract three sentences as a causal relation candidate, where one contains the cue phrase and the other two are the previous and next sentences in the answer candidates. When there is more than one cue phrase in an answer candidate, we use all of </context>
</contexts>
<marker>Khoo, Chan, Niu, 2000</marker>
<rawString>Christopher S. G. Khoo, Syin Chan, and Yun Niu. 2000. Extracting causal knowledge from a medical database using graphical patterns. In Proceedings of ACL ’00, pages 336–343.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of ICML ’01,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="14248" citStr="Lafferty et al., 2001" startWordPosition="2283" endWordPosition="2287">this) and その (that)), postpositions (including case markers such as が (nominative), の (genitive)), and copula (e.g., で す (is) and である (is)) in Japanese, respectively. RCT, which represents Japanese terms meaning reason, cause, or thanks to, is defined as follows: RCT = {理由 (reason), 原® (cause), 要 ® (cause), 引き金 (cause), おかげ (thanks to), せい (thanks to), わけ (reason) }. 4.1.2 Recognizing Causal Relations Next, we recognize the spans of the cause and effect parts of a causal relation linked to a c-marker. We regard this task as a sequence labeling problem and use Conditional Random Fields (CRFs) (Lafferty et al., 2001) as a machine learning framework. In our task, CRFs take three sentences of a causal relation candidate as input and generate their cause-effect annotations with a set of possible cause-effect IOB labels, including BeginCause (B-C), Inside-Cause (I-C), Begin-Effect (BE), Inside-Effect (I-E), and Outside (O). Fig 2 shows an example of such sequence labeling. Although this example is about sequential labeling shown on English sentences for ease of explanation, it was actually done on Japanese sentences. We used the three types of feature sets in Table 3 for training the CRFs, where j is in the r</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of ICML ’01, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Silvia Quarteroni</author>
<author>Roberto Basili</author>
<author>Suresh Manandhar</author>
</authors>
<title>Exploiting syntactic and shallow semantic kernels for question answer classification.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL ’07,</booktitle>
<pages>776--783</pages>
<contexts>
<context position="18747" citStr="Moschitti et al., 2007" startWordPosition="3047" endWordPosition="3050">. C-marker features: As our c-marker features, we use a pair composed of c-marker cm and one of the following: mj, mj+1 j , sj, or sj+1 j . 4.2 Causal Relation Features We use terms, partial trees (in a syntactic dependency tree structure), and the semantic orientation of excitation (Hashimoto et al., 2012) to assess the appropriateness of each causal relation obtained by our causal relation recognizer as an answer to a given question. Finding answers with term matching and partial tree matching has been used in the literature of question answering (Girju, 2003; Narayanan and Harabagiu, 2004; Moschitti et al., 2007; Higashinaka and Isozaki, 2008; Verberne et al., 2008; Surdeanu et al., 2011; Verberne et al., 2011; Oh et al., 2012), while that with the excitation polarity is proposed in this work. We use three types of features. Each feature type expresses the causal relations in an answer candidate that are determined to be appropriate as answers to a given question by term matching (tf1–tf4), partial tree matching (pf1– pf4) and excitation polarity matching (ef1–ef4). We call these causal relations used for generating our causal relation features candidates of an appropriate causal relation in this sec</context>
</contexts>
<marker>Moschitti, Quarteroni, Basili, Manandhar, 2007</marker>
<rawString>Alessandro Moschitti, Silvia Quarteroni, Roberto Basili, and Suresh Manandhar. 2007. Exploiting syntactic and shallow semantic kernels for question answer classification. In Proceedings of ACL ’07, pages 776–783.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaki Murata</author>
<author>Sachiyo Tsukawaki</author>
<author>Toshiyuki Kanamaru</author>
<author>Qing Ma</author>
<author>Hitoshi Isahara</author>
</authors>
<title>A system for answering non-factoid Japanese questions by using passage retrieval weighted based on type of answer.</title>
<date>2007</date>
<booktitle>In Proceedings of NTCIR-6.</booktitle>
<contexts>
<context position="3899" citStr="Murata et al., 2007" startWordPosition="610" endWordPosition="614">an effect part that resembles a given question and provides them as answers. Since this idea looks quite intuitive, many people would probably consider it as a solution to why-QA. However, to our surprise, we could not find any previous work on why-QA that took this approach. Some methods utilized the causal relations between terms as evidence for finding answers (i.e., matching a cause term with an answer text and its effect term with a question) (Gir u, 2003; Higashinaka and Isozaki, 2008). Other approaches utilized such clue terms for causality as “because” as evidence for finding answers (Murata et al., 2007). However, these algorithms did not check whether an answer candidate, i.e., a text fragment that may be provided as an answer, explicitly contains a complex causal relation sen1733 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1733–1743, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics tence with the effect part that resembles a question. For example, A5 in Table 1 is an incorrect answer to “Why are tsunamis generated?”, but these previous approaches would probably choose it as a proper answer due to “because” </context>
<context position="9745" citStr="Murata et al. (2007)" startWordPosition="1554" endWordPosition="1557">system architecture of our QA system before describing our proposed method. It is composed of two components: answer candidate extraction and answer re-ranking (Fig. 1). This architecture is basically the same as that used in our previous work (Oh et al., 2012). We extended our previous work by introducing causal relations recognized from answer candidates to the answer re-ranking. The features used in our previous work are very different from those in this work, and we found that combining both improves accuracy. Answer candidate extraction: In our previous work, we implemented the method of Murata et al. (2007) for our answer candidate extractor. We retrieved documents from Japanese web texts using Boolean AND and OR queries generated from the content words in why-questions. Then we extracted passages of five sentences from these retrieved documents and ranked them with the ranking function proposed by Murata et al. (2007). This method ranks a passage higher when it contains more query terms that are closer to each other in the passage. We used a set of clue terms, including the Japanese counterparts of cause and reason, as query terms for the ranking. The top ranked passages are regarded as answer </context>
</contexts>
<marker>Murata, Tsukawaki, Kanamaru, Ma, Isahara, 2007</marker>
<rawString>Masaki Murata, Sachiyo Tsukawaki, Toshiyuki Kanamaru, Qing Ma, and Hitoshi Isahara. 2007. A system for answering non-factoid Japanese questions by using passage retrieval weighted based on type of answer. In Proceedings of NTCIR-6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srini Narayanan</author>
<author>Sanda Harabagiu</author>
</authors>
<title>Question answering based on semantic structures.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING ’04,</booktitle>
<pages>693--701</pages>
<contexts>
<context position="18723" citStr="Narayanan and Harabagiu, 2004" startWordPosition="3043" endWordPosition="3046">ese syntactic dependency parser. C-marker features: As our c-marker features, we use a pair composed of c-marker cm and one of the following: mj, mj+1 j , sj, or sj+1 j . 4.2 Causal Relation Features We use terms, partial trees (in a syntactic dependency tree structure), and the semantic orientation of excitation (Hashimoto et al., 2012) to assess the appropriateness of each causal relation obtained by our causal relation recognizer as an answer to a given question. Finding answers with term matching and partial tree matching has been used in the literature of question answering (Girju, 2003; Narayanan and Harabagiu, 2004; Moschitti et al., 2007; Higashinaka and Isozaki, 2008; Verberne et al., 2008; Surdeanu et al., 2011; Verberne et al., 2011; Oh et al., 2012), while that with the excitation polarity is proposed in this work. We use three types of features. Each feature type expresses the causal relations in an answer candidate that are determined to be appropriate as answers to a given question by term matching (tf1–tf4), partial tree matching (pf1– pf4) and excitation polarity matching (ef1–ef4). We call these causal relations used for generating our causal relation features candidates of an appropriate cau</context>
</contexts>
<marker>Narayanan, Harabagiu, 2004</marker>
<rawString>Srini Narayanan and Sanda Harabagiu. 2004. Question answering based on semantic structures. In Proceedings of COLING ’04, pages 693–701.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jong-Hoon Oh</author>
</authors>
<title>Kentaro Torisawa, Chikara Hashimoto, Takuya Kawada, Stijn De Saeger, Jun’ichi Kazama, and Yiou Wang.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP-CoNLL ’12,</booktitle>
<pages>368--378</pages>
<marker>Oh, 2012</marker>
<rawString>Jong-Hoon Oh, Kentaro Torisawa, Chikara Hashimoto, Takuya Kawada, Stijn De Saeger, Jun’ichi Kazama, and Yiou Wang. 2012. Why question answering using sentiment analysis and word classes. In Proceedings of EMNLP-CoNLL ’12, pages 368–378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kira Radinsky</author>
<author>Sagie Davidovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Learning causality for news events prediction.</title>
<date>2012</date>
<booktitle>In Proceedings of WWW ’12,</booktitle>
<pages>909--918</pages>
<contexts>
<context position="7788" citStr="Radinsky et al., 2012" startWordPosition="1255" endWordPosition="1258">t answers according to their confidence score given by our system, the precision of these confident answers was 83.2% for 25% of all the questions in our test set. In the same setting, the precision of the state-of-the-art system (Oh et al., 2012) was only 62.4%. 2 Related Work Although there were many previous works on the acquisition of intra- and inter-sentential causal relations from texts (Khoo et al., 2000; Girju, 2003; Inui and Okumura, 2005; Chang and Choi, 2006; Torisawa, 2006; Blanco et al., 2008; De Saeger et al., 2009; De Saeger et al., 2011; Riaz and Girju, 2010; Do et al., 2011; Radinsky et al., 2012), their application to why-QA was limited to causal relations between terms (Girju, 2003; Higashinaka and Isozaki, 2008). As previous attempts to improve why-QA performance, such semantic knowledge as WordNet synsets (Verberne et al., 2011), semantic word classes (Oh et al., 2012), sentiment analysis (Oh et al., 2012), and causal relations between terms (Girju, 2003; Higashinaka and Isozaki, 2008) has been used. These previous studies took basically bag-of-words approaches and used the semantic knowledge to identify certain semantic associations using terms and n-grams. On the other hand, our </context>
</contexts>
<marker>Radinsky, Davidovich, Markovitch, 2012</marker>
<rawString>Kira Radinsky, Sagie Davidovich, and Shaul Markovitch. 2012. Learning causality for news events prediction. In Proceedings of WWW ’12, pages 909–918.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehwish Riaz</author>
<author>Roxana Girju</author>
</authors>
<title>Another look at causality: Discovering scenario-specific contingency relationships with no supervision.</title>
<date>2010</date>
<booktitle>In ICSC ’10,</booktitle>
<pages>361--368</pages>
<contexts>
<context position="7747" citStr="Riaz and Girju, 2010" startWordPosition="1247" endWordPosition="1250">de our system provide only its confident answers according to their confidence score given by our system, the precision of these confident answers was 83.2% for 25% of all the questions in our test set. In the same setting, the precision of the state-of-the-art system (Oh et al., 2012) was only 62.4%. 2 Related Work Although there were many previous works on the acquisition of intra- and inter-sentential causal relations from texts (Khoo et al., 2000; Girju, 2003; Inui and Okumura, 2005; Chang and Choi, 2006; Torisawa, 2006; Blanco et al., 2008; De Saeger et al., 2009; De Saeger et al., 2011; Riaz and Girju, 2010; Do et al., 2011; Radinsky et al., 2012), their application to why-QA was limited to causal relations between terms (Girju, 2003; Higashinaka and Isozaki, 2008). As previous attempts to improve why-QA performance, such semantic knowledge as WordNet synsets (Verberne et al., 2011), semantic word classes (Oh et al., 2012), sentiment analysis (Oh et al., 2012), and causal relations between terms (Girju, 2003; Higashinaka and Isozaki, 2008) has been used. These previous studies took basically bag-of-words approaches and used the semantic knowledge to identify certain semantic associations using t</context>
</contexts>
<marker>Riaz, Girju, 2010</marker>
<rawString>Mehwish Riaz and Roxana Girju. 2010. Another look at causality: Discovering scenario-specific contingency relationships with no supervision. In ICSC ’10, pages 361–368.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Shima</author>
<author>Hiroshi Kanayama</author>
<author>Cheng wei Lee</author>
</authors>
<title>Chuan jie Lin, Teruko Mitamura, Yusuke Miyao, Shuming Shi, and Koichi Takeda.</title>
<date>2011</date>
<booktitle>In Proceedings of NTCIR-9.</booktitle>
<contexts>
<context position="6346" citStr="Shima et al., 2011" startWordPosition="1011" endWordPosition="1014">e also have to select the appropriate ones. Consider the causal relations in A1–A4. Those in A1–A3 are appropriate answers to “Why are tsunamis generated?”, but not the one in A4. To assess the appropriateness, the system must recognize textual entailment, i.e., “tsunamis (are) generated” in the question is entailed by all “tsunamis are generated” in A1, “cause a tsunami” in A2 and “tsunamis are caused” in A3 but not by “tsunamis weaken” in A4. This quite difficult task is currently being studied by many researchers in the RTE field (Androutsopoulos and Malakasiotis, 2010; Dagan et al., 2010; Shima et al., 2011; Bentivogli et al., 2011). To meet this challenge, we developed a relatively simple method that can be seen as a lightweight approximation for this difficult RTE task, using excitation polarities (Hashimoto et al., 2012). Through our experiments on Japanese why-QA, we show that a combination of the above methods can improve why-QA accuracy. In addition, our proposed method can be successfully combined with other approaches to why-QA and can contribute to higher accuracy. As a final result, we improved the precision by 4.4% against all the questions in our test set over the current state-of-th</context>
</contexts>
<marker>Shima, Kanayama, Lee, 2011</marker>
<rawString>Hideki Shima, Hiroshi Kanayama, Cheng wei Lee, Chuan jie Lin, Teruko Mitamura, Yusuke Miyao, Shuming Shi, and Koichi Takeda. 2011. Overview of NTCIR-9 RITE: Recognizing Inference in TExt. In Proceedings of NTCIR-9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Massimiliano Ciaramita</author>
<author>Hugo Zaragoza</author>
</authors>
<title>Learning to rank answers to nonfactoid questions from web collections.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>2</issue>
<contexts>
<context position="18824" citStr="Surdeanu et al., 2011" startWordPosition="3060" endWordPosition="3063">ker cm and one of the following: mj, mj+1 j , sj, or sj+1 j . 4.2 Causal Relation Features We use terms, partial trees (in a syntactic dependency tree structure), and the semantic orientation of excitation (Hashimoto et al., 2012) to assess the appropriateness of each causal relation obtained by our causal relation recognizer as an answer to a given question. Finding answers with term matching and partial tree matching has been used in the literature of question answering (Girju, 2003; Narayanan and Harabagiu, 2004; Moschitti et al., 2007; Higashinaka and Isozaki, 2008; Verberne et al., 2008; Surdeanu et al., 2011; Verberne et al., 2011; Oh et al., 2012), while that with the excitation polarity is proposed in this work. We use three types of features. Each feature type expresses the causal relations in an answer candidate that are determined to be appropriate as answers to a given question by term matching (tf1–tf4), partial tree matching (pf1– pf4) and excitation polarity matching (ef1–ef4). We call these causal relations used for generating our causal relation features candidates of an appropriate causal relation in this section. Note that if one answer candidate has more than one candidate of an app</context>
</contexts>
<marker>Surdeanu, Ciaramita, Zaragoza, 2011</marker>
<rawString>Mihai Surdeanu, Massimiliano Ciaramita, and Hugo Zaragoza. 2011. Learning to rank answers to nonfactoid questions from web collections. Computational Linguistics, 37(2):351–383.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kentaro Torisawa</author>
</authors>
<title>Acquiring inference rules with temporal constraints by using japanese coordinated sentences and noun-verb co-occurrences.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL ’06,</booktitle>
<pages>57--64</pages>
<contexts>
<context position="7656" citStr="Torisawa, 2006" startWordPosition="1231" endWordPosition="1232"> larger when we only compared the highly confident answers of each system. When we made our system provide only its confident answers according to their confidence score given by our system, the precision of these confident answers was 83.2% for 25% of all the questions in our test set. In the same setting, the precision of the state-of-the-art system (Oh et al., 2012) was only 62.4%. 2 Related Work Although there were many previous works on the acquisition of intra- and inter-sentential causal relations from texts (Khoo et al., 2000; Girju, 2003; Inui and Okumura, 2005; Chang and Choi, 2006; Torisawa, 2006; Blanco et al., 2008; De Saeger et al., 2009; De Saeger et al., 2011; Riaz and Girju, 2010; Do et al., 2011; Radinsky et al., 2012), their application to why-QA was limited to causal relations between terms (Girju, 2003; Higashinaka and Isozaki, 2008). As previous attempts to improve why-QA performance, such semantic knowledge as WordNet synsets (Verberne et al., 2011), semantic word classes (Oh et al., 2012), sentiment analysis (Oh et al., 2012), and causal relations between terms (Girju, 2003; Higashinaka and Isozaki, 2008) has been used. These previous studies took basically bag-of-words a</context>
</contexts>
<marker>Torisawa, 2006</marker>
<rawString>Kentaro Torisawa. 2006. Acquiring inference rules with temporal constraints by using japanese coordinated sentences and noun-verb co-occurrences. In Proceedings of HLT-NAACL ’06, pages 57–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>The nature of statistical learning theory.</title>
<date>1995</date>
<publisher>Springer-Verlag</publisher>
<location>New York,</location>
<contexts>
<context position="10533" citStr="Vapnik, 1995" startWordPosition="1689" endWordPosition="1690"> extracted passages of five sentences from these retrieved documents and ranked them with the ranking function proposed by Murata et al. (2007). This method ranks a passage higher when it contains more query terms that are closer to each other in the passage. We used a set of clue terms, including the Japanese counterparts of cause and reason, as query terms for the ranking. The top ranked passages are regarded as answer candidates in the answer re-ranking. See Murata et al. (2007) for more details. Answer re-ranking: Re-ranking the answer candidates is done by a supervised classifier (SVMs) (Vapnik, 1995). In our previous work, we employed three types of features for training the re-ranker: morphosyntactic features (n-grams of morphemes and syntactic dependency chains), semantic word class features (semantic word classes obtained by automatic word clustering (Kazama and Torisawa, 2008)) and sentiment polarity features (word and phrase polarities). Here, we used semantic word classes and sentiment polarities for identifying such semantic associations between a why-question and its answer as “if a disease’s name appears in a question, then answers that include nutrient names are more likely to b</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>Vladimir N. Vapnik. 1995. The nature of statistical learning theory. Springer-Verlag New York, Inc., New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Istvan Varga</author>
<author>Motoki Sano</author>
</authors>
<title>Kentaro Torisawa, Chikara Hashimoto, Kiyonori Ohtake, Takao Kawai, JongHoon Oh, and Stijn De Saeger.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL ’13.</booktitle>
<marker>Varga, Sano, 2013</marker>
<rawString>Istvan Varga, Motoki Sano, Kentaro Torisawa, Chikara Hashimoto, Kiyonori Ohtake, Takao Kawai, JongHoon Oh, and Stijn De Saeger. 2013. Aid is out there: Looking for help from tweets during a large scale disaster. In Proceedings of ACL ’13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suzan Verberne</author>
<author>Lou Boves</author>
<author>Nelleke Oostdijk</author>
<author>Peter-Arno Coppen</author>
</authors>
<title>Using syntactic information for improving why-question answering.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING ’08,</booktitle>
<pages>953--960</pages>
<contexts>
<context position="1601" citStr="Verberne et al., 2008" startWordPosition="236" endWordPosition="239">est set over the current state-of-theart system for Japanese why-QA. In addition, unlike the state-of-the-art system, our system could achieve very high precision (83.2%) for 25% of all the questions in the test set by restricting its output to the confident answers only. 1 Introduction “Why-question answering” (why-QA) is a task to retrieve answers from a given text archive for a why-question, such as “Why are tsunamis generated?” The answers are usually text fragments consisting of one or more sentences. Although much research exists on this task (Gir u, 2003; Higashinaka and Isozaki, 2008; Verberne et al., 2008; Verberne et al., 2011; Oh et al., 2012), its performance remains much lower than that of the state-of-the-art factoid QA systems, such as IBM’s Watson (Ferrucci et al., 2010). In this work, we propose a quite straightforward but novel approach for such difficult whyQA task. Consider the sentence A1 in Table 1, which represents the causal relation between the cause, “the ocean’s water mass ..., waves are genA1 [Tsunamis that can cause large coastal inundation are generated]effect because [the ocean’s water mass is displaced and, much like throwing a stone into a pond, waves are generated.]cau</context>
<context position="18801" citStr="Verberne et al., 2008" startWordPosition="3055" endWordPosition="3059"> pair composed of c-marker cm and one of the following: mj, mj+1 j , sj, or sj+1 j . 4.2 Causal Relation Features We use terms, partial trees (in a syntactic dependency tree structure), and the semantic orientation of excitation (Hashimoto et al., 2012) to assess the appropriateness of each causal relation obtained by our causal relation recognizer as an answer to a given question. Finding answers with term matching and partial tree matching has been used in the literature of question answering (Girju, 2003; Narayanan and Harabagiu, 2004; Moschitti et al., 2007; Higashinaka and Isozaki, 2008; Verberne et al., 2008; Surdeanu et al., 2011; Verberne et al., 2011; Oh et al., 2012), while that with the excitation polarity is proposed in this work. We use three types of features. Each feature type expresses the causal relations in an answer candidate that are determined to be appropriate as answers to a given question by term matching (tf1–tf4), partial tree matching (pf1– pf4) and excitation polarity matching (ef1–ef4). We call these causal relations used for generating our causal relation features candidates of an appropriate causal relation in this section. Note that if one answer candidate has more than </context>
</contexts>
<marker>Verberne, Boves, Oostdijk, Coppen, 2008</marker>
<rawString>Suzan Verberne, Lou Boves, Nelleke Oostdijk, and Peter-Arno Coppen. 2008. Using syntactic information for improving why-question answering. In Proceedings of COLING ’08, pages 953–960.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suzan Verberne</author>
<author>Lou Boves</author>
<author>Wessel Kraaij</author>
</authors>
<title>Bringing why-qa to web search.</title>
<date>2011</date>
<booktitle>In Proceedings of ECIR ’11,</booktitle>
<pages>491--496</pages>
<contexts>
<context position="1624" citStr="Verberne et al., 2011" startWordPosition="240" endWordPosition="243">t state-of-theart system for Japanese why-QA. In addition, unlike the state-of-the-art system, our system could achieve very high precision (83.2%) for 25% of all the questions in the test set by restricting its output to the confident answers only. 1 Introduction “Why-question answering” (why-QA) is a task to retrieve answers from a given text archive for a why-question, such as “Why are tsunamis generated?” The answers are usually text fragments consisting of one or more sentences. Although much research exists on this task (Gir u, 2003; Higashinaka and Isozaki, 2008; Verberne et al., 2008; Verberne et al., 2011; Oh et al., 2012), its performance remains much lower than that of the state-of-the-art factoid QA systems, such as IBM’s Watson (Ferrucci et al., 2010). In this work, we propose a quite straightforward but novel approach for such difficult whyQA task. Consider the sentence A1 in Table 1, which represents the causal relation between the cause, “the ocean’s water mass ..., waves are genA1 [Tsunamis that can cause large coastal inundation are generated]effect because [the ocean’s water mass is displaced and, much like throwing a stone into a pond, waves are generated.]cause A2 [Earthquake cause</context>
<context position="8028" citStr="Verberne et al., 2011" startWordPosition="1292" endWordPosition="1295">, 2012) was only 62.4%. 2 Related Work Although there were many previous works on the acquisition of intra- and inter-sentential causal relations from texts (Khoo et al., 2000; Girju, 2003; Inui and Okumura, 2005; Chang and Choi, 2006; Torisawa, 2006; Blanco et al., 2008; De Saeger et al., 2009; De Saeger et al., 2011; Riaz and Girju, 2010; Do et al., 2011; Radinsky et al., 2012), their application to why-QA was limited to causal relations between terms (Girju, 2003; Higashinaka and Isozaki, 2008). As previous attempts to improve why-QA performance, such semantic knowledge as WordNet synsets (Verberne et al., 2011), semantic word classes (Oh et al., 2012), sentiment analysis (Oh et al., 2012), and causal relations between terms (Girju, 2003; Higashinaka and Isozaki, 2008) has been used. These previous studies took basically bag-of-words approaches and used the semantic knowledge to identify certain semantic associations using terms and n-grams. On the other hand, our method explicitly identifies intra- and inter-sentential causal relations between terms/phrases/clauses that have complex structures and uses the identified relations to answer a why-question. In other words, our method considers more compl</context>
<context position="18847" citStr="Verberne et al., 2011" startWordPosition="3064" endWordPosition="3067">ollowing: mj, mj+1 j , sj, or sj+1 j . 4.2 Causal Relation Features We use terms, partial trees (in a syntactic dependency tree structure), and the semantic orientation of excitation (Hashimoto et al., 2012) to assess the appropriateness of each causal relation obtained by our causal relation recognizer as an answer to a given question. Finding answers with term matching and partial tree matching has been used in the literature of question answering (Girju, 2003; Narayanan and Harabagiu, 2004; Moschitti et al., 2007; Higashinaka and Isozaki, 2008; Verberne et al., 2008; Surdeanu et al., 2011; Verberne et al., 2011; Oh et al., 2012), while that with the excitation polarity is proposed in this work. We use three types of features. Each feature type expresses the causal relations in an answer candidate that are determined to be appropriate as answers to a given question by term matching (tf1–tf4), partial tree matching (pf1– pf4) and excitation polarity matching (ef1–ef4). We call these causal relations used for generating our causal relation features candidates of an appropriate causal relation in this section. Note that if one answer candidate has more than one candidate of an appropriate causal relatio</context>
</contexts>
<marker>Verberne, Boves, Kraaij, 2011</marker>
<rawString>Suzan Verberne, Lou Boves, and Wessel Kraaij. 2011. Bringing why-qa to web search. In Proceedings of ECIR ’11, pages 491–496.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>