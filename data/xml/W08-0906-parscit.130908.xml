<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000679">
<title confidence="0.874119666666667">
Answering Learners’ Questions by Retrieving Question Paraphrases
from Social Q&amp;A Sites
Delphine Bernhard and Iryna Gurevych
</title>
<author confidence="0.47852">
Ubiquitous Knowledge Processing Lab
</author>
<affiliation confidence="0.627171">
Computer Science Department
Technische Universit¨at Darmstadt, Hochschulstraße 10
</affiliation>
<address confidence="0.69708">
D-64289 Darmstadt, Germany
</address>
<email confidence="0.998369">
{delphine|gurevych}@tk.informatik.tu-darmstadt.de
</email>
<sectionHeader confidence="0.998595" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999782333333333">
Information overload is a well-known prob-
lem which can be particularly detrimental to
learners. In this paper, we propose a method
to support learners in the information seek-
ing process which consists in answering their
questions by retrieving question paraphrases
and their corresponding answers from social
Q&amp;A sites. Given the novelty of this kind of
data, it is crucial to get a better understand-
ing of how questions in social Q&amp;A sites can
be automatically analysed and retrieved. We
discuss and evaluate several pre-processing
strategies and question similarity metrics, us-
ing a new question paraphrase corpus col-
lected from the WikiAnswers Q&amp;A site. The
results show that viable performance levels of
more than 80% accuracy can be obtained for
the task of question paraphrase retrieval.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999813458333334">
Question asking is an important component of effi-
cient learning. However, instructors are often over-
whelmed with students’ questions and are therefore
unable to provide timely answers (Feng et al., 2006).
Information seeking is also rendered difficult by the
sheer amount of learning material available, espe-
cially online. The use of advanced information re-
trieval and natural language processing techniques
to answer learners’ questions and reduce the diffi-
culty of information seeking is henceforth particu-
larly promising. Question Answering (QA) systems
seem well suited for this task since they aim at gen-
erating precise answers to natural language ques-
tions instead of merely returning documents con-
taining answers. However, QA systems have to be
adapted to meet learners’ needs. Indeed, learners
do not merely ask concrete or factoid questions, but
rather open-ended, explanatory or methodological
questions which cannot be answered by a single sen-
tence (Baram-Tsabari et al., 2006). Despite a recent
trend to render the tasks more complex at large scale
QA evaluation campaigns such as TREC or CLEF,
current QA systems are still ill-suited to meet these
requirements.
A first alternative to full-fledged QA consists in
making use of already available question and answer
pairs extracted from archived discussions. For in-
stance, Feng et al. (2006) describe an intelligent dis-
cussion bot for answering student questions in fo-
rums which relies on answers retrieved from an an-
notated corpus of discussions. This renders the task
of QA easier since answers do not have to be gener-
ated from heterogeneous documents by the system.
The scope of such a discussion bot is however inher-
ently limited since it relies on manually annotated
data, taken from forums within a specific domain.
We propose a different solution which consists in
tapping into the wisdom of crowds to answer learn-
ers’ questions. This approach provides the com-
pelling advantage that it utilises the wealth of al-
ready answered questions available in online social
Q&amp;A sites. The task of Question Answering can
then be boiled down to the problem of finding ques-
tion paraphrases in a database of answered ques-
tions. Question paraphrases are questions which
have identical meanings and expect the same answer
while presenting alternate wordings. Several meth-
ods have already been proposed to identify question
</bodyText>
<page confidence="0.98918">
44
</page>
<note confidence="0.7396365">
Proceedings of the Third ACL Workshop on Innovative Use of NLP for Building Educational Applications, pages 44–52,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</note>
<bodyText confidence="0.998922935483871">
paraphrases mostly in FAQs (Tomuro and Lytinen,
2004) or search engine logs (Zhao et al., 2007).
In this paper, we focus on the problem of question
paraphrase identification in social Q&amp;A sites within
a realistic information seeking scenario: given a user
question, we want to retrieve the best matching ques-
tion paraphrase from a database of previously an-
swered questions in order to display the correspond-
ing answer. The use of social Q&amp;A sites for ed-
ucational applications brings about new challenges
linked to the variable quality of social media content.
As opposed to questions in FAQs, which are subject
to editorial control, questions in social Q&amp;A sites
are often ill-formed or contain spelling errors. It is
therefore crucial to get a better understanding of how
they can be automatically analysed and retrieved. In
this work, we focus on several pre-processing strate-
gies and question similarity measures applied to the
task of identifying question paraphrases in a social
Q&amp;A site. We chose WikiAnswers which has been
ranked by comScore as the first fastest growing do-
main of the top 1,500 in the U.S. in 2007.
The remainder of the paper is organised as fol-
lows. Section 2 first discusses related work on
paraphrase identification and question paraphrasing.
Section 3 then presents question and answer repos-
itories with special emphasis on social Q&amp;A sites.
Our methods to identify question paraphrases are de-
tailed in section 4. Finally, we present and analyse
the experimental results obtained in section 5 and
conclude in section 6.
</bodyText>
<sectionHeader confidence="0.999892" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.995173833333333">
The identification of question paraphrases in ques-
tion and answer repositories is related to research
focusing on sentence paraphrase identification (sec-
tion 2.1) and query paraphrasing (section 2.2). The
specific features of question paraphrasing have also
already been investigated (section 2.3).
</bodyText>
<subsectionHeader confidence="0.998029">
2.1 Sentence Paraphrase Identification
</subsectionHeader>
<bodyText confidence="0.99964534375">
Paraphrases are alternative ways to convey the same
information (Barzilay and McKeown, 2001). Para-
phrases can be found at different levels of lin-
guistic structure: words, phrases and whole sen-
tences. While word and phrasal paraphrases can
be assimilated to the well-studied notion of syn-
onymy, sentence level paraphrasing is more difficult
to grasp and cannot be equated with word-for-word
or phrase-by-phrase substitution since it might en-
tail changes in the structure of the sentence (Barzi-
lay and Lee, 2003). In practice, sentence para-
phrases are identified using various string and se-
mantic similarity measures which aim at captur-
ing the semantic equivalence of the sentences being
compared. String similarity metrics, when applied
to sentences, consist in comparing the words con-
tained in the sentences. There exist many different
string similarity measures: word overlap (Tomuro
and Lytinen, 2004), longest common subsequence
(Islam and Inkpen, 2007), Levenshtein edit distance
(Dolan et al., 2004), word n-gram overlap (Barzilay
and Lee, 2003) etc. Semantic similarity measures
are obtained by first computing the semantic simi-
larity of the words contained in the sentences being
compared. Mihalcea et al. (2006) use both corpus-
based and knowledge-based measures of the seman-
tic similarity between words. Both string similarity
and semantic similarity might be combined: for in-
stance, Islam and Inkpen (2007) combine semantic
similarity with longest common subsequence string
similarity, while Li et al. (2006) make additional use
of word order similarity.
</bodyText>
<subsectionHeader confidence="0.9996">
2.2 Query Paraphrasing
</subsectionHeader>
<bodyText confidence="0.999962615384615">
In Information Retrieval, research on paraphrasing
is dedicated to query paraphrasing which consists in
identifying semantically similar queries. The over-
all objective is to discover frequently asked ques-
tions and popular topics (Wen et al., 2002) or sug-
gest related queries to users (Sahami and Heilman,
2006). Traditional string similarity metrics are usu-
ally deemed inefficient for such short text snip-
pets and alternative similarity metrics have therefore
been proposed. For instance, Wen et al. (2002) rely
on user click logs, based on the idea that queries and
questions which result in identical document clicks
are bound to be similar.
</bodyText>
<subsectionHeader confidence="0.999709">
2.3 Question Paraphrasing
</subsectionHeader>
<bodyText confidence="0.9995796">
Following previous research in this domain, we de-
fine question paraphrases as questions which have
all the following properties: (a) they have identi-
cal meanings, (b) they have the same answers, and
(c) they present alternate wordings. Question para-
</bodyText>
<page confidence="0.998665">
45
</page>
<bodyText confidence="0.999678979166667">
phrases differ from sentence paraphrases by the ad-
ditional condition (b). This definition encompasses
the following questions, taken from the WikiAn-
swers web site: How many ounces are there in a
pound?, What’s the number of ounces per pound?,
How many oz. in a lb.?
Question paraphrases share some properties both
with declarative sentence paraphrases and query
paraphrases. On the one hand, questions are com-
plete sentences which differ from declarative sen-
tences by their specific word order and the presence
of question words and a question focus. On the other
hand, questions are usually associated with answers,
which makes them similar to queries associated with
documents. Accordingly, research on the identifi-
cation of question paraphrases in Q&amp;A repositories
builds upon both sentence and query paraphrasing.
Zhao et al. (2007) propose to utilise user click
logs from the Encarta web site to identify question
paraphrases. Jeon et al. (2005) employ a related
method, in that they identify similar answers in the
Naver Question and Answer database to retrieve se-
mantically similar questions, while Jijkoun and de
Rijke (2005) include the answer in the retrieval pro-
cess to return a ranked list of QA pairs in response
to a user’s question. Lytinen and Tomuro (2002)
suggest yet another feature to identify question para-
phrases, namely question type similarity, which con-
sists in determining a question’s category in order to
match questions only if they belong to the same cat-
egory.
Our focus is on question paraphrase identification
in social Q&amp;A sites. Previous research was mostly
based on question paraphrase identification in FAQs
(Lytinen and Tomuro, 2002; Tomuro and Lytinen,
2004; Jijkoun and de Rijke, 2005). In FAQs, ques-
tions and answers are edited by expert information
suppliers, which guarantees stricter conformance to
conventional writing rules. In social Q&amp;A sites,
questions and answers are written by users and may
hence be error-prone. Question paraphrase identi-
fication in social Q&amp;A sites has been little investi-
gated. To our knowledge, only Jeon et al. (2005)
have used data from a Q&amp;A site, namely the Korean
Naver portal, to find semantically similar questions.
Our work is related to the latter since it employs a
similar dataset, yet in English and from a different
social Q&amp;A site.
</bodyText>
<sectionHeader confidence="0.915734" genericHeader="method">
3 Question and Answer Repositories
</sectionHeader>
<subsectionHeader confidence="0.999589">
3.1 Properties of Q&amp;A Repositories
</subsectionHeader>
<bodyText confidence="0.989418388888889">
Question and answer repositories have existed for a
long time on the Internet. Their form has evolved
from Frequently Asked Questions (FAQs) to Ask-
an-expert services (Baram-Tsabari et al., 2006) and,
even more recently, social Q&amp;A sites. The latest,
which include web sites such as Yahoo! Answers
and AnswerBag, provide portals where users can
ask their own questions as well as answer ques-
tions from other users. Social Q&amp;A sites are in-
creasingly popular. For instance, in December 2006
Yahoo! Answers was the second-most visited edu-
cation/reference site on the Internet after Wikipedia
according to the Hitwise company (Prescott, 2006).
Even more strikingly, the Q&amp;A portal Naver is the
leader of Internet search in South Korea, well ahead
of Google (Sang-Hun, 2007).
Several factors might explain the success of social
Q&amp;A sites:
</bodyText>
<listItem confidence="0.9609203">
• they provide answers to questions which are
difficult to answer with a traditional Web search
or using static reference sites like Wikipedia,
for instance opinions or advice about a specific
family situation or a relationship problem;
• questions can be asked anonymously;
• users do not have to browse a list of documents
but rather obtain a complete answer;
• the answers are almost instantaneous and nu-
merous, due to the large number of users.
</listItem>
<bodyText confidence="0.999547545454545">
Social Q&amp;A sites record the questions and their
answers online, and thus constitute a formidable
repository of collective intelligence, including an-
swers to complex questions. Moreover, they make
it possible for learners to reach other people world-
wide. The relevance of social Q&amp;A sites for learning
has been little investigated. To our knowledge, there
has been only one study which has shown that Ko-
rean users of the Naver Question and Answer plat-
form consider that social Q&amp;A sites can satisfacto-
rily and reliably support learning (Lee, 2006).
</bodyText>
<subsectionHeader confidence="0.998372">
3.2 WikiAnswers
</subsectionHeader>
<bodyText confidence="0.9995935">
For our experiments we collected a dataset of ques-
tions and their paraphrases from the WikiAnswers
</bodyText>
<page confidence="0.996046">
46
</page>
<bodyText confidence="0.999795222222222">
web site. WikiAnswers1 is a social Q&amp;A site similar
to Yahoo! Answers and AnswerBag. As of Febru-
ary 2008, it contained 1,807,600 questions, sorted in
2,404 categories (Answers Corporation, 2008).
Compared with its competitors, the main origi-
nality of WikiAnswers is that it relies on the wiki
technology used in Wikipedia, which means that an-
swers can be edited and improved over time by all
contributors. Moreover, the Answers Corporation,
which owns the WikiAnswers site, explicitly tar-
gets educational uses and even provides an educator
toolkit.2 Another interesting property of WikiAn-
swers is that users might manually tag question re-
formulations in order to prevent the duplication of
questions asking the same thing in a different way.
When a user enters a question which is not already
part of the question repository, the web site dis-
plays a list of questions already existing on the site
and similar to the one just asked by the user. The
user may then freely select the question which para-
phrases her question, if available, or choose to view
one of the proposed alternatives without labelling it
as a paraphrase. The user-labelled question refor-
mulations are stored in order to retrieve the same
answer when the question rephrasing is asked again.
The wiki principle holds for the stored reformula-
tions too, since they can subsequently be edited by
other users if they consider that they correspond to
another existing question or actually ask an entirely
new question. It should be noted that contributors
get not reward in terms of trust points for providing
or editing alternate wordings for questions.
We use the wealth of question paraphrases avail-
able on the WikiAnswers website as the so called
user generated gold standard in our question para-
phrasing experiments. User generated gold stan-
dards have been increasingly used in recent years
for research evaluation purposes, since they can be
easily created from user annotated content. For
instance, Mihalcea and Csomai (2007) use manu-
ally annotated keywords (links to other articles) in
Wikipedia articles to evaluate their automatic key-
word extraction and word sense disambiguation al-
gorithms. Similarly, quality assessments provided
by users in social media have been used as gold
</bodyText>
<footnote confidence="0.9998935">
1http://wiki.answers.com/
2http://educator.answers.com/
</footnote>
<bodyText confidence="0.999916842105263">
standards for the automatic assessment of post qual-
ity in forum discussions (Weimer et al., 2007). It
should however be kept in mind that user generated
gold standards are not perfect, as already noticed by
(Mihalcea and Csomai, 2007), and thus constitute a
trade-off solution.
For the experiments described hereafter, we ran-
domly extracted a collection of 1,000 questions
along with their paraphrases (totalling 7,434 ques-
tion paraphrases) from 100 randomly selected FAQ
files in the Education category of the WikiAnswers
web site. In what follows, the corpus of 1,000 ques-
tions is called the target questions collection, while
the 7,434 question paraphrases constitute the input
questions collection. The objective of the task is to
retrieve the corresponding target question for each
input question. The target question selected is the
one which maximises the question similarity value
(see section 4.2).
</bodyText>
<sectionHeader confidence="0.995495" genericHeader="method">
4 Method
</sectionHeader>
<bodyText confidence="0.99992775">
In order to rate the similarity of input and target
questions, we have first pre-processed both the in-
put and target questions and then experimented with
several question similarity measures.
</bodyText>
<subsectionHeader confidence="0.999397">
4.1 Pre-processing
</subsectionHeader>
<bodyText confidence="0.911672">
We employ the following steps in pre-processing the
questions:
Stop words elimination however, we keep ques-
tion words such as how, why, what, etc. since these
make it possible to implicitly identify the question
type (Lytinen and Tomuro, 2002; Jijkoun and de Ri-
jke, 2005)
Stemming using the Porter Stemmer3
Lemmatisation using the TreeTagger4
Spelling correction using a statistical system
based on language modelling (Norvig, 2007).5
</bodyText>
<footnote confidence="0.980066857142857">
3http://snowball.tartarus.org/
4http://www.ims.uni-stuttgart.de/
projekte/corplex/TreeTagger/
5We used a Java implementation of the system, jSpell-
Correct available at http://developer.gauner.org/
jspellcorrect/, trained with the default English training
data, to which we appended the myspell English dictionaries.
</footnote>
<page confidence="0.998968">
47
</page>
<bodyText confidence="0.999626466666667">
Stop words were eliminated in all the experi-
mental settings, while stemming and lemmatisation
were optionally performed to evaluate the effects
of these pre-processing steps on the identification
of question paraphrases. We added spelling correc-
tion to the conventional pre-processing steps, since
we target paraphrasing of questions which often
contain spelling errors, such as When was indoor
pluming invented? or What is the largest countery
in the western Hemipher? Other related endeav-
ours at retrieving question paraphrases have identi-
fied spelling mistakes in questions as a significant
source of errors in the retrieval process, but have not
attempted to solve this problem (Jijkoun and de Ri-
jke, 2005; Zhao et al., 2007).
</bodyText>
<subsectionHeader confidence="0.997764">
4.2 Question Similarity Measures
</subsectionHeader>
<bodyText confidence="0.999381">
We have experimented with several kinds of ques-
tion similarity measures, belonging to two different
families of measures: string similarity measures and
vector space measures.
</bodyText>
<subsectionHeader confidence="0.999901">
4.3 String Similarity Measures
</subsectionHeader>
<bodyText confidence="0.997650090909091">
Basic string similarity measures compare the words
contained in the questions without taking word fre-
quency into account.
Matching coefficient The matching coefficient of
two questions q1 and q2 represented by the set of
distinct words Q1 and Q2 they contain is computed
as follows (Manning and Sch¨utze, 1999):
matching coefficient = |Q1 n Q2 |
Overlap coefficient The overlap coefficient is
computed according to the following formula (Man-
ning and Sch¨utze, 1999):
</bodyText>
<equation confidence="0.9288575">
overlap coefficient =  |Q1 n Q2 |
min( |Q1 |, |Q2 |)
</equation>
<bodyText confidence="0.9983875">
Normalised Edit Distance The edit distance of
two questions is the number of words that need to be
substituted, inserted, or deleted, to transform q1 into
q2. In order to be able to compare the edit distance
with the other metrics, we have used the follow-
ing formula (Wen et al., 2002) which normalises the
minimum edit distance by the length of the longest
question and transforms it into a similarity metric:
</bodyText>
<equation confidence="0.978753">
edit dist(q1, q2)
normalised edit distance = 1 −
max( |q1 |, |q2 |)
</equation>
<bodyText confidence="0.952882">
Word Ngram Overlap This metric compares the
word n-grams in both questions:
where Gn(q) is the set of n-grams of length n in
question q and N usually equals 4 (Barzilay and
Lee, 2003; Cordeiro et al., 2007).
</bodyText>
<subsectionHeader confidence="0.996319">
4.4 Vector Space Based Measures
</subsectionHeader>
<bodyText confidence="0.975246571428571">
Vector space measures represent questions as real-
valued vectors by taking word frequency into ac-
count.
Term Vector Similarity Questions are repre-
sented as term vectors V1 and V2. The feature val-
ues of the vectors are the tf.idf scores of the corre-
sponding terms:
</bodyText>
<equation confidence="0.977705333333333">
N + 1
tf.idf = (1 + log(tf)) * log
df
</equation>
<bodyText confidence="0.999843222222222">
where tf is equal to the frequency of the term in
the question, N is the number of target questions
and df is the number of target questions in which
the term occurs, computed by considering the in-
put question as part of the target questions collection
(Lytinen and Tomuro, 2002).
The similarity of an input question vector and a
target question vector is determined by the cosine
coefficient:
</bodyText>
<equation confidence="0.998104333333333">
V1 V2
cosine coefficient =
 |V1   ||V2 |
</equation>
<bodyText confidence="0.9892508">
Lucene’s Extended Boolean Model The prob-
lem of question paraphrase identification can be
cast as an Information Retrieval problem, since in
real-world applications the user posts a question
and the system returns the best matching questions
from its database. We have therefore tested the re-
sults obtained using an Information Retrieval sys-
tem, namely Lucene6, which combines the Vector
Space Model and the Boolean model. Lucene has
already been successfully used by Jijkoun and de Ri-
jke (2005) to retrieve answers from FAQ web pages
by combining several fields: question text, answer
text and the whole FAQ page. The target questions
are indexed as documents and retrieved by trans-
forming the input questions into queries.
</bodyText>
<equation confidence="0.43299875">
6http://lucene.apache.org/java/docs/
ngram overlap = 1 N  |Gn(q1) n Gn(q2) |
N E
min( |Gn(q1) |, |Gn(q2) |)
</equation>
<page confidence="0.8859">
48
</page>
<figure confidence="0.99810175">
Accuracy
100
90
80
70
60
50
MRR
0.9
0.8
0.7
0.6
0.5
1.0
Matching coefficient
Overlap coefficient
Normalised edit distance
Ngram Overlap
Term vector similarity
Lucene
T T S S L L T T S S L L
-SW -SW -SW -SW -SW -SW -SW -SW -SW -SW -SW -SW
+SC +SC +SC +SC
Preprocessing Preprocessing
</figure>
<figureCaption confidence="0.981304">
Figure 1: Accuracy (%) and Mean Reciprocal Rank obtained for different question similarity measures and pre-
processing strategies: tokens (T), stemming (S), lemmatisation (L), stop words removal (-SW), spelling correction
(+SC).
</figureCaption>
<sectionHeader confidence="0.990879" genericHeader="method">
5 Evaluation and Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.970098">
5.1 Evaluation Measures
</subsectionHeader>
<bodyText confidence="0.989815666666667">
We use the following evaluation measures for evalu-
ating the results:
Mean Reciprocal Rank For a question, the recip-
rocal rank RR is r where r is the rank of the correct
target question, or zero if the target question was not
found. The Mean Reciprocal Rank (MRR) is the
mean of the reciprocal ranks over all the input ques-
tions.
Accuracy We define accuracy as Success@1,
which is the percentage of input questions for which
the correct target question has been retrieved at rank
1.
</bodyText>
<subsectionHeader confidence="0.998697">
5.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.997960846153846">
Figure 1 displays the accuracy and the mean recip-
rocal ranks obtained with the different question sim-
ilarity measures and pre-processing strategies. As
could be expected, vector space based similarity
measures are consistently more accurate than sim-
ple string similarity measures. Moreover, both the
accuracy and the MRR are rather high for vector
space metrics (accuracy around 80-85% and MRR
around 0.85-0.9), which shows that good results can
be obtained with these retrieval mechanisms. Addi-
tional pre-processing, i.e. stemming, lemmatisation
and spelling correction, does not ameliorate the to-
kens minus stop words (T -SW) baseline.
</bodyText>
<subsectionHeader confidence="0.998858">
5.3 Detailed Error Analysis
</subsectionHeader>
<bodyText confidence="0.995103384615385">
Stemming and lemmatisation Morphological
pre-processing brings about mitigated improve-
ments over the tokens-only baseline. On the one
hand, it improves paraphrase retrieval for ques-
tions containing morphological variants of the same
words such as What are analogies for mitochondria?
and What is an analogy for mitochondrion? On the
other hand, it also leads to false positives, such has
How was calculus started?, stemmed as How was
calculus start? and lemmatised as How be calculus
start?, which is mapped by Lucene to the question
How could you start your MA English studies?
instead of Who developed calculus?. The negative
effect of stemming has already been identified by
(Jijkoun and de Rijke, 2005) and our results are
consistent with this previous finding.
Spelling correction We expected that spelling
correction would have a positive impact on the re-
sults. There are indeed cases when spelling correc-
tion helps. For instance, given the question How do
you become an anestesiologist?, it is impossible to
retrieve the target question How many years of med-
ical school do you need to be an anesthesiolgist?
without spelling correction since anesthesiologist is
ill-spelled both in the paraphrase and the target ques-
tion.
</bodyText>
<page confidence="0.99813">
49
</page>
<table confidence="0.999515166666667">
Token + Stop words
Token + Stop words + Spelling correction
Stem + Stop words
Stem + Stop words + Spelling correction
Lemma + Stop words
Lemma + Stop words + Spelling correction
</table>
<subsectionHeader confidence="0.852360166666667">
Edit distance
Matching coefficient
Overlap coefficient
Word Ngram overlap
Term Vector similarity
Lucene
</subsectionHeader>
<figure confidence="0.986198">
(b)
</figure>
<figureCaption confidence="0.9408755">
Figure 2: Comparison of the different pre-processing strategies 2(a) and methods 2(b) for 50 input questions. For the
pre-processing comparison, the Lucene retrieval method has been used, while the methods have been compared using
baseline pre-processing (tokens minus stop words). A filled square indicates that the target question has been retrieved
at rank 1, while a blank square indicates that the target question has not been retrieved at rank 1.
</figureCaption>
<figure confidence="0.880815">
(a)
</figure>
<bodyText confidence="0.998535326923077">
There are however cases when spelling correction
induces worse results, since it is accurate in only ap-
proximately 70% of the cases (Norvig, 2007). A
major source of errors lies in named entities and ab-
breviations, which are recognised as spelling errors
when they are not part of the training lexicon. For
instance, the question What are the GRE score re-
quired to get into top100 US universities? (where
GRE stands for Graduate Record Examination) is
badly corrected as What are the are score required
to get into top100 US universities?.
Spelling correction also induces an unexpected
side effect, when the spelling error does not affect
the question’s focus. For instance, consider the fol-
lowing question, with a spelling error: What events
occured in 1919?, which gets correctly mapped to
the target question What important events happened
in 1919? by Lucene; however, after spelling correc-
tion (What events occurred in 1919?), it has a big-
ger overlap with an entirely different question: What
events occurred in colonial South Carolina 1674-
1775?.
The latter example also points at another limita-
tion of the evaluated methods, which do not identify
semantically similar words, such as occurred and
happened.
Errors in the gold standard Some errors can ac-
tually be traced back to inaccuracies in the gold stan-
dard: some question pairs which have been flagged
as paraphrases by the WikiAnswers contributors are
actually distantly related. For instance, the questions
When was the first painting made? and Where did
leanardo da vinci live? are marked as reformula-
tions of the question What is the secret about mona
lisa? Though these questions all share a common
broad topic, they cannot be considered as relevant
paraphrases.
We can deduce several possible improvements
from what precedes. First, named entities and ab-
breviations play an important role in questions and
should therefore be identified and treated differently
from other kinds of tokens. This could be achieved
by using a named entity recognition component
during pre-processing and then assigning a higher
weight to named entities in the retrieval process.
This should also improve the results of spelling cor-
rection since named entities and abbreviations could
be excluded from the correction. Second, seman-
tic errors could be dealt with by using a semantic
similarity metric similar to those used in declarative
sentence paraphrase identification (Li et al., 2006;
Mihalcea et al., 2006; Islam and Inkpen, 2007).
</bodyText>
<sectionHeader confidence="0.8780125" genericHeader="method">
5.4 Comparison and Combination of the
Methods
</sectionHeader>
<bodyText confidence="0.9984275">
In a second part of the experiment, we investigated
whether the evaluated methods display independent
</bodyText>
<page confidence="0.985038">
50
</page>
<bodyText confidence="0.999866923076923">
error patterns, as suggested by our detailed results
analysis. Figure 2 confirms that the pre-processing
techniques as well as the methods employed result
in dissimilar error patterns. We therefore combined
several methods and pre-processing techniques in
order to verify if we could improve accuracy.
We obtained the best results by performing a ma-
jority vote combination of the following methods
and pre-processing strategies: Lucene, Term Vector
Similarity with stemming and Ngram Overlap with
spelling correction. The combination yielded an ac-
curacy of 88.3%, that is 0.9% over the best Lucene
results with an accuracy of 87.4%.
</bodyText>
<sectionHeader confidence="0.997972" genericHeader="conclusions">
6 Conclusion and Outlook
</sectionHeader>
<bodyText confidence="0.999921653846154">
In this paper, we have shown that it is feasible to an-
swer learners’ questions by retrieving question para-
phrases from social Q&amp;A sites. As a first step to-
wards this objective, we investigated several ques-
tion similarity metrics and pre-processing strategies,
using WikiAnswers as input data and user generated
gold standard. The approach is however not limited
to this dataset and can be easily applied to retrieve
question paraphrases from other social Q&amp;A sites.
We also performed an extended failure analysis
which provided useful insights on how results could
be further improved by performing named entity
analysis and using semantic similarity metrics.
Another important challenge in using social Q&amp;A
sites for educational purposes lies in the quality of
the answers retrieved from such sites. Previous re-
search on the identification of high quality content in
social Q&amp;A sites has defined answer quality in terms
of correctness, well-formedness, readability, objec-
tivity, relevance, utility and interestingness (Jeon et
al., 2006; Agichtein et al., 2008). It is obvious that
all these elements play an important role in the ac-
ceptance of the answers by learners. We therefore
plan to integrate quality measures in the retrieval
process and to perform evaluations in a real educa-
tional setting.
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998437">
This work was supported by the Emmy Noether Pro-
gramme of the German Research Foundation (DFG)
under grant No. GU 798/3-1.
</bodyText>
<sectionHeader confidence="0.995364" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998848666666666">
Eugene Agichtein, Carlos Castillo, Debora Donato, Aris-
tides Gionis, and Gilad Mishne. 2008. Finding
high-quality content in social media. In WSDM ’08:
Proceedings of the international conference on Web
search and web data mining, pages 183–194.
Answers Corporation. 2008. WikiAnswers Jour-
nalist Quick Guide. [Online; visited March
4, 2008]. http://site.wikianswers.com/
resources/WikiAnswers_1-pager.pdf.
Ayelet Baram-Tsabari, Ricky J. Sethi, Lynn Bry, and
Anat Yarden. 2006. Using questions sent to an Ask-A-
Scientist site to identify children’s interests in science.
Science Education, 90(6):1050–1072.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: an unsupervised approach using multiple-
sequence alignment. In Proceedings of NAACL-HLT
2003, pages 16–23. Association for Computational
Linguistics.
Regina Barzilay and Kathleen R. McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In ACL
’01: Proceedings of the 39th Annual Meeting of the
Association for Computational Linguistics, pages 50–
57. Association for Computational Linguistics.
Jo˜ao Cordeiro, Ga¨el Dias, and Pavel Brazdil. 2007.
Learning Paraphrases from WNS Corpora. In David
Wilson and Geoff Sutcliffe, editors, Proceedings of
the Twentieth International Florida Artificial Intelli-
gence Research Society Conference (FLAIRS), pages
193–198, Key West, Florida, USA, May 7-9. AAAI
Press.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
exploiting massively parallel news sources. In COL-
ING ’04: Proceedings of the 20th international con-
ference on Computational Linguistics, pages 350–356.
Association for Computational Linguistics.
Donghui Feng, Erin Shaw, Jihie Kim, and Eduard Hovy.
2006. An Intelligent Discussion-Bot for Answering
Student Queries in Threaded Discussions. In Proceed-
ings of the 11th international conference on Intelligent
user interfaces (IUI’06), pages 171–177.
Aminul Islam and Diana Inkpen. 2007. Semantic Sim-
ilarity of Short Texts. In Proceedings of the Interna-
tional Conference on RecentAdvances in Natural Lan-
guage Processing (RANLP 2007), Borovets, Bulgaria,
September.
Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005.
Finding similar questions in large question and answer
archives. In CIKM ’05: Proceedings of the 14th ACM
international conference on Information and knowl-
edge management, pages 84–90.
</reference>
<page confidence="0.979831">
51
</page>
<reference confidence="0.999815202531645">
Jiwoon Jeon, W. Bruce Croft, Joon Ho Lee, and Soyeon
Park. 2006. A framework to predict the quality of
answers with non-textual features. In SIGIR ’06: Pro-
ceedings of the 29th annual international ACM SIGIR
conference on Research and development in informa-
tion retrieval, pages 228–235.
Valentin Jijkoun and Maarten de Rijke. 2005. Retrieving
answers from frequently asked questions pages on the
web. In CIKM ’05: Proceedings of the 14th ACM in-
ternational conference on Information and knowledge
management, pages 76–83.
Yu Sun Lee. 2006. Toward a New Knowledge Shar-
ing Community: Collective Intelligence and Learn-
ing through Web-Portal-Based Question-Answer Ser-
vices. Masters of arts in communication, culture &amp;
technology, Faculty of the Graduate School of Arts
and Sciences of Georgetown University, May. [On-
line; visited February 15, 2008], http://hdl.
handle.net/1961/3701.
Yuhua Li, David McLean, Zuhair A. Bandar, James D.
O’Shea, and Keeley Crockett. 2006. Sentence Simi-
larity Based on Semantic Nets and Corpus Statistics.
IEEE Transactions on Knowledge and Data Engineer-
ing, 18(8):1138–1150.
Steven L. Lytinen and Noriko Tomuro. 2002. The Use
of Question Types to Match Questions in FAQFinder.
In Proceedings of the 2002 AAAI Spring Symposium
on Mining Answers from Texts and Knowledge Bases,
pages 46–53.
Christopher D. Manning and Hinrich Sch¨utze. 1999.
Foundations of Statistical Natural Language Process-
ing. The MIT Press, Cambridge, Massachusetts.
Rada Mihalcea and Andras Csomai. 2007. Wikify!: link-
ing documents to encyclopedic knowledge. In CIKM
’07: Proceedings of the sixteenth ACM conference on
information and knowledge management, pages 233–
242.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and Knowledge-based Measures
of Text Semantic Similarity. In Proceedings of AAAI
2006, Boston, July.
Peter Norvig. 2007. How to Write a Spelling Correc-
tor. [Online; visited February 22, 2008]. http:
//norvig.com/spell-correct.html.
Lee Ann Prescott. 2006. Yahoo! Answers Cap-
tures 96% of Q and A Market Share. Hit-
wise Intelligence [Online; visited February
26, 2008]. http://weblogs.hitwise.
com/leeann-prescott/2006/12/yahoo_
answers_captures_96_of_q.html.
Mehran Sahami and Timothy D. Heilman. 2006. A web-
based kernel function for measuring the similarity of
short text snippets. In WWW ’06: Proceedings of
the 15th international conference on World Wide Web,
pages 377–386.
Choe Sang-Hun. 2007. To outdo Google,
Naver taps into Korea’s collective wis-
dom. International Herald Tribune, July 4.
http://www.iht.com/articles/2007/
07/04/technology/naver.php.
Noriko Tomuro and Steven Lytinen. 2004. Retrieval
Models and Q&amp;A Learning with FAQ Files. In
Mark T. Maybury, editor, New Directions in Question
Answering, pages 183–194. AAAI Press.
Markus Weimer, Iryna Gurevych, and Max M¨uhlh¨auser.
2007. Automatically Assessing the Post Quality in
Online Discussions on Software. In Proceedings of the
Demo and Poster Sessions of the 45th Annual Meet-
ing of the Association for Computational Linguistics,
pages 125–128, Prague, Czech Republic, June. Asso-
ciation for Computational Linguistics.
Ji-Rong Wen, Jian-Yun Nie, and Hong-Jiang Zhang.
2002. Query clustering using user logs. ACM Trans.
Inf. Syst., 20(1):59–81.
Shiqi Zhao, Ming Zhou, and Ting Liu. 2007. Learn-
ing Question Paraphrases for QA from Encarta Logs.
In Proceedings of the 20th International Joint Confer-
ence on Artificial Intelligence, pages 1795–1801, Hy-
derabad, India, January 6-12.
</reference>
<page confidence="0.998856">
52
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.486413">
<title confidence="0.9694395">Answering Learners’ Questions by Retrieving Question from Social Q&amp;A Sites</title>
<author confidence="0.818747">Bernhard</author>
<affiliation confidence="0.842274333333333">Ubiquitous Knowledge Processing Computer Science Technische Universit¨at Darmstadt, Hochschulstraße</affiliation>
<address confidence="0.968416">D-64289 Darmstadt,</address>
<abstract confidence="0.998605684210526">Information overload is a well-known problem which can be particularly detrimental to learners. In this paper, we propose a method to support learners in the information seeking process which consists in answering their questions by retrieving question paraphrases and their corresponding answers from social Q&amp;A sites. Given the novelty of this kind of data, it is crucial to get a better understanding of how questions in social Q&amp;A sites can be automatically analysed and retrieved. We discuss and evaluate several pre-processing strategies and question similarity metrics, using a new question paraphrase corpus collected from the WikiAnswers Q&amp;A site. The results show that viable performance levels of more than 80% accuracy can be obtained for the task of question paraphrase retrieval.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Agichtein</author>
<author>Carlos Castillo</author>
<author>Debora Donato</author>
<author>Aristides Gionis</author>
<author>Gilad Mishne</author>
</authors>
<title>Finding high-quality content in social media.</title>
<date>2008</date>
<booktitle>In WSDM ’08: Proceedings of the international conference on Web</booktitle>
<pages>183--194</pages>
<marker>Agichtein, Castillo, Donato, Gionis, Mishne, 2008</marker>
<rawString>Eugene Agichtein, Carlos Castillo, Debora Donato, Aristides Gionis, and Gilad Mishne. 2008. Finding high-quality content in social media. In WSDM ’08: Proceedings of the international conference on Web search and web data mining, pages 183–194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Answers Corporation</author>
</authors>
<date>2008</date>
<journal>WikiAnswers Journalist Quick Guide. [Online; visited March</journal>
<volume>4</volume>
<note>http://site.wikianswers.com/ resources/WikiAnswers_1-pager.pdf.</note>
<contexts>
<context position="12651" citStr="Corporation, 2008" startWordPosition="1969" endWordPosition="1970"> worldwide. The relevance of social Q&amp;A sites for learning has been little investigated. To our knowledge, there has been only one study which has shown that Korean users of the Naver Question and Answer platform consider that social Q&amp;A sites can satisfactorily and reliably support learning (Lee, 2006). 3.2 WikiAnswers For our experiments we collected a dataset of questions and their paraphrases from the WikiAnswers 46 web site. WikiAnswers1 is a social Q&amp;A site similar to Yahoo! Answers and AnswerBag. As of February 2008, it contained 1,807,600 questions, sorted in 2,404 categories (Answers Corporation, 2008). Compared with its competitors, the main originality of WikiAnswers is that it relies on the wiki technology used in Wikipedia, which means that answers can be edited and improved over time by all contributors. Moreover, the Answers Corporation, which owns the WikiAnswers site, explicitly targets educational uses and even provides an educator toolkit.2 Another interesting property of WikiAnswers is that users might manually tag question reformulations in order to prevent the duplication of questions asking the same thing in a different way. When a user enters a question which is not already p</context>
</contexts>
<marker>Corporation, 2008</marker>
<rawString>Answers Corporation. 2008. WikiAnswers Journalist Quick Guide. [Online; visited March 4, 2008]. http://site.wikianswers.com/ resources/WikiAnswers_1-pager.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ayelet Baram-Tsabari</author>
<author>Ricky J Sethi</author>
<author>Lynn Bry</author>
<author>Anat Yarden</author>
</authors>
<title>Using questions sent to an Ask-AScientist site to identify children’s interests in science.</title>
<date>2006</date>
<journal>Science Education,</journal>
<volume>90</volume>
<issue>6</issue>
<contexts>
<context position="2125" citStr="Baram-Tsabari et al., 2006" startWordPosition="305" endWordPosition="308">val and natural language processing techniques to answer learners’ questions and reduce the difficulty of information seeking is henceforth particularly promising. Question Answering (QA) systems seem well suited for this task since they aim at generating precise answers to natural language questions instead of merely returning documents containing answers. However, QA systems have to be adapted to meet learners’ needs. Indeed, learners do not merely ask concrete or factoid questions, but rather open-ended, explanatory or methodological questions which cannot be answered by a single sentence (Baram-Tsabari et al., 2006). Despite a recent trend to render the tasks more complex at large scale QA evaluation campaigns such as TREC or CLEF, current QA systems are still ill-suited to meet these requirements. A first alternative to full-fledged QA consists in making use of already available question and answer pairs extracted from archived discussions. For instance, Feng et al. (2006) describe an intelligent discussion bot for answering student questions in forums which relies on answers retrieved from an annotated corpus of discussions. This renders the task of QA easier since answers do not have to be generated f</context>
<context position="10703" citStr="Baram-Tsabari et al., 2006" startWordPosition="1652" endWordPosition="1655">r-prone. Question paraphrase identification in social Q&amp;A sites has been little investigated. To our knowledge, only Jeon et al. (2005) have used data from a Q&amp;A site, namely the Korean Naver portal, to find semantically similar questions. Our work is related to the latter since it employs a similar dataset, yet in English and from a different social Q&amp;A site. 3 Question and Answer Repositories 3.1 Properties of Q&amp;A Repositories Question and answer repositories have existed for a long time on the Internet. Their form has evolved from Frequently Asked Questions (FAQs) to Askan-expert services (Baram-Tsabari et al., 2006) and, even more recently, social Q&amp;A sites. The latest, which include web sites such as Yahoo! Answers and AnswerBag, provide portals where users can ask their own questions as well as answer questions from other users. Social Q&amp;A sites are increasingly popular. For instance, in December 2006 Yahoo! Answers was the second-most visited education/reference site on the Internet after Wikipedia according to the Hitwise company (Prescott, 2006). Even more strikingly, the Q&amp;A portal Naver is the leader of Internet search in South Korea, well ahead of Google (Sang-Hun, 2007). Several factors might ex</context>
</contexts>
<marker>Baram-Tsabari, Sethi, Bry, Yarden, 2006</marker>
<rawString>Ayelet Baram-Tsabari, Ricky J. Sethi, Lynn Bry, and Anat Yarden. 2006. Using questions sent to an Ask-AScientist site to identify children’s interests in science. Science Education, 90(6):1050–1072.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Learning to paraphrase: an unsupervised approach using multiplesequence alignment.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL-HLT 2003,</booktitle>
<pages>16--23</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6122" citStr="Barzilay and Lee, 2003" startWordPosition="938" endWordPosition="942"> question paraphrasing have also already been investigated (section 2.3). 2.1 Sentence Paraphrase Identification Paraphrases are alternative ways to convey the same information (Barzilay and McKeown, 2001). Paraphrases can be found at different levels of linguistic structure: words, phrases and whole sentences. While word and phrasal paraphrases can be assimilated to the well-studied notion of synonymy, sentence level paraphrasing is more difficult to grasp and cannot be equated with word-for-word or phrase-by-phrase substitution since it might entail changes in the structure of the sentence (Barzilay and Lee, 2003). In practice, sentence paraphrases are identified using various string and semantic similarity measures which aim at capturing the semantic equivalence of the sentences being compared. String similarity metrics, when applied to sentences, consist in comparing the words contained in the sentences. There exist many different string similarity measures: word overlap (Tomuro and Lytinen, 2004), longest common subsequence (Islam and Inkpen, 2007), Levenshtein edit distance (Dolan et al., 2004), word n-gram overlap (Barzilay and Lee, 2003) etc. Semantic similarity measures are obtained by first com</context>
<context position="18797" citStr="Barzilay and Lee, 2003" startWordPosition="2925" endWordPosition="2928">nce of two questions is the number of words that need to be substituted, inserted, or deleted, to transform q1 into q2. In order to be able to compare the edit distance with the other metrics, we have used the following formula (Wen et al., 2002) which normalises the minimum edit distance by the length of the longest question and transforms it into a similarity metric: edit dist(q1, q2) normalised edit distance = 1 − max( |q1 |, |q2 |) Word Ngram Overlap This metric compares the word n-grams in both questions: where Gn(q) is the set of n-grams of length n in question q and N usually equals 4 (Barzilay and Lee, 2003; Cordeiro et al., 2007). 4.4 Vector Space Based Measures Vector space measures represent questions as realvalued vectors by taking word frequency into account. Term Vector Similarity Questions are represented as term vectors V1 and V2. The feature values of the vectors are the tf.idf scores of the corresponding terms: N + 1 tf.idf = (1 + log(tf)) * log df where tf is equal to the frequency of the term in the question, N is the number of target questions and df is the number of target questions in which the term occurs, computed by considering the input question as part of the target questions</context>
</contexts>
<marker>Barzilay, Lee, 2003</marker>
<rawString>Regina Barzilay and Lillian Lee. 2003. Learning to paraphrase: an unsupervised approach using multiplesequence alignment. In Proceedings of NAACL-HLT 2003, pages 16–23. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Extracting paraphrases from a parallel corpus.</title>
<date>2001</date>
<booktitle>In ACL ’01: Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>pages</pages>
<contexts>
<context position="5704" citStr="Barzilay and McKeown, 2001" startWordPosition="872" endWordPosition="875">es. Our methods to identify question paraphrases are detailed in section 4. Finally, we present and analyse the experimental results obtained in section 5 and conclude in section 6. 2 Related Work The identification of question paraphrases in question and answer repositories is related to research focusing on sentence paraphrase identification (section 2.1) and query paraphrasing (section 2.2). The specific features of question paraphrasing have also already been investigated (section 2.3). 2.1 Sentence Paraphrase Identification Paraphrases are alternative ways to convey the same information (Barzilay and McKeown, 2001). Paraphrases can be found at different levels of linguistic structure: words, phrases and whole sentences. While word and phrasal paraphrases can be assimilated to the well-studied notion of synonymy, sentence level paraphrasing is more difficult to grasp and cannot be equated with word-for-word or phrase-by-phrase substitution since it might entail changes in the structure of the sentence (Barzilay and Lee, 2003). In practice, sentence paraphrases are identified using various string and semantic similarity measures which aim at capturing the semantic equivalence of the sentences being compar</context>
</contexts>
<marker>Barzilay, McKeown, 2001</marker>
<rawString>Regina Barzilay and Kathleen R. McKeown. 2001. Extracting paraphrases from a parallel corpus. In ACL ’01: Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics, pages 50– 57. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jo˜ao Cordeiro</author>
<author>Ga¨el Dias</author>
<author>Pavel Brazdil</author>
</authors>
<title>Learning Paraphrases from WNS Corpora.</title>
<date>2007</date>
<booktitle>Proceedings of the Twentieth International Florida Artificial Intelligence Research Society Conference (FLAIRS),</booktitle>
<pages>193--198</pages>
<editor>In David Wilson and Geoff Sutcliffe, editors,</editor>
<publisher>AAAI Press.</publisher>
<location>Key West, Florida, USA,</location>
<marker>Cordeiro, Ga¨el Dias, Brazdil, 2007</marker>
<rawString>Jo˜ao Cordeiro, Ga¨el Dias, and Pavel Brazdil. 2007. Learning Paraphrases from WNS Corpora. In David Wilson and Geoff Sutcliffe, editors, Proceedings of the Twentieth International Florida Artificial Intelligence Research Society Conference (FLAIRS), pages 193–198, Key West, Florida, USA, May 7-9. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill Dolan</author>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
</authors>
<title>Unsupervised construction of large paraphrase corpora: exploiting massively parallel news sources.</title>
<date>2004</date>
<booktitle>In COLING ’04: Proceedings of the 20th international conference on Computational Linguistics,</booktitle>
<pages>350--356</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6616" citStr="Dolan et al., 2004" startWordPosition="1011" endWordPosition="1014">word or phrase-by-phrase substitution since it might entail changes in the structure of the sentence (Barzilay and Lee, 2003). In practice, sentence paraphrases are identified using various string and semantic similarity measures which aim at capturing the semantic equivalence of the sentences being compared. String similarity metrics, when applied to sentences, consist in comparing the words contained in the sentences. There exist many different string similarity measures: word overlap (Tomuro and Lytinen, 2004), longest common subsequence (Islam and Inkpen, 2007), Levenshtein edit distance (Dolan et al., 2004), word n-gram overlap (Barzilay and Lee, 2003) etc. Semantic similarity measures are obtained by first computing the semantic similarity of the words contained in the sentences being compared. Mihalcea et al. (2006) use both corpusbased and knowledge-based measures of the semantic similarity between words. Both string similarity and semantic similarity might be combined: for instance, Islam and Inkpen (2007) combine semantic similarity with longest common subsequence string similarity, while Li et al. (2006) make additional use of word order similarity. 2.2 Query Paraphrasing In Information Re</context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised construction of large paraphrase corpora: exploiting massively parallel news sources. In COLING ’04: Proceedings of the 20th international conference on Computational Linguistics, pages 350–356. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donghui Feng</author>
<author>Erin Shaw</author>
<author>Jihie Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>An Intelligent Discussion-Bot for Answering Student Queries in Threaded Discussions.</title>
<date>2006</date>
<booktitle>In Proceedings of the 11th international conference on Intelligent user interfaces (IUI’06),</booktitle>
<pages>171--177</pages>
<contexts>
<context position="1340" citStr="Feng et al., 2006" startWordPosition="187" endWordPosition="190">rstanding of how questions in social Q&amp;A sites can be automatically analysed and retrieved. We discuss and evaluate several pre-processing strategies and question similarity metrics, using a new question paraphrase corpus collected from the WikiAnswers Q&amp;A site. The results show that viable performance levels of more than 80% accuracy can be obtained for the task of question paraphrase retrieval. 1 Introduction Question asking is an important component of efficient learning. However, instructors are often overwhelmed with students’ questions and are therefore unable to provide timely answers (Feng et al., 2006). Information seeking is also rendered difficult by the sheer amount of learning material available, especially online. The use of advanced information retrieval and natural language processing techniques to answer learners’ questions and reduce the difficulty of information seeking is henceforth particularly promising. Question Answering (QA) systems seem well suited for this task since they aim at generating precise answers to natural language questions instead of merely returning documents containing answers. However, QA systems have to be adapted to meet learners’ needs. Indeed, learners d</context>
</contexts>
<marker>Feng, Shaw, Kim, Hovy, 2006</marker>
<rawString>Donghui Feng, Erin Shaw, Jihie Kim, and Eduard Hovy. 2006. An Intelligent Discussion-Bot for Answering Student Queries in Threaded Discussions. In Proceedings of the 11th international conference on Intelligent user interfaces (IUI’06), pages 171–177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aminul Islam</author>
<author>Diana Inkpen</author>
</authors>
<title>Semantic Similarity of Short Texts.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Conference on RecentAdvances in Natural Language Processing (RANLP</booktitle>
<location>Borovets, Bulgaria,</location>
<contexts>
<context position="6568" citStr="Islam and Inkpen, 2007" startWordPosition="1004" endWordPosition="1007">ficult to grasp and cannot be equated with word-for-word or phrase-by-phrase substitution since it might entail changes in the structure of the sentence (Barzilay and Lee, 2003). In practice, sentence paraphrases are identified using various string and semantic similarity measures which aim at capturing the semantic equivalence of the sentences being compared. String similarity metrics, when applied to sentences, consist in comparing the words contained in the sentences. There exist many different string similarity measures: word overlap (Tomuro and Lytinen, 2004), longest common subsequence (Islam and Inkpen, 2007), Levenshtein edit distance (Dolan et al., 2004), word n-gram overlap (Barzilay and Lee, 2003) etc. Semantic similarity measures are obtained by first computing the semantic similarity of the words contained in the sentences being compared. Mihalcea et al. (2006) use both corpusbased and knowledge-based measures of the semantic similarity between words. Both string similarity and semantic similarity might be combined: for instance, Islam and Inkpen (2007) combine semantic similarity with longest common subsequence string similarity, while Li et al. (2006) make additional use of word order simi</context>
<context position="26625" citStr="Islam and Inkpen, 2007" startWordPosition="4194" endWordPosition="4197">questions and should therefore be identified and treated differently from other kinds of tokens. This could be achieved by using a named entity recognition component during pre-processing and then assigning a higher weight to named entities in the retrieval process. This should also improve the results of spelling correction since named entities and abbreviations could be excluded from the correction. Second, semantic errors could be dealt with by using a semantic similarity metric similar to those used in declarative sentence paraphrase identification (Li et al., 2006; Mihalcea et al., 2006; Islam and Inkpen, 2007). 5.4 Comparison and Combination of the Methods In a second part of the experiment, we investigated whether the evaluated methods display independent 50 error patterns, as suggested by our detailed results analysis. Figure 2 confirms that the pre-processing techniques as well as the methods employed result in dissimilar error patterns. We therefore combined several methods and pre-processing techniques in order to verify if we could improve accuracy. We obtained the best results by performing a majority vote combination of the following methods and pre-processing strategies: Lucene, Term Vecto</context>
</contexts>
<marker>Islam, Inkpen, 2007</marker>
<rawString>Aminul Islam and Diana Inkpen. 2007. Semantic Similarity of Short Texts. In Proceedings of the International Conference on RecentAdvances in Natural Language Processing (RANLP 2007), Borovets, Bulgaria, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiwoon Jeon</author>
<author>W Bruce Croft</author>
<author>Joon Ho Lee</author>
</authors>
<title>Finding similar questions in large question and answer archives.</title>
<date>2005</date>
<booktitle>In CIKM ’05: Proceedings of the 14th ACM international conference on Information and knowledge management,</booktitle>
<pages>84--90</pages>
<contexts>
<context position="9071" citStr="Jeon et al. (2005)" startWordPosition="1390" endWordPosition="1393">entence paraphrases and query paraphrases. On the one hand, questions are complete sentences which differ from declarative sentences by their specific word order and the presence of question words and a question focus. On the other hand, questions are usually associated with answers, which makes them similar to queries associated with documents. Accordingly, research on the identification of question paraphrases in Q&amp;A repositories builds upon both sentence and query paraphrasing. Zhao et al. (2007) propose to utilise user click logs from the Encarta web site to identify question paraphrases. Jeon et al. (2005) employ a related method, in that they identify similar answers in the Naver Question and Answer database to retrieve semantically similar questions, while Jijkoun and de Rijke (2005) include the answer in the retrieval process to return a ranked list of QA pairs in response to a user’s question. Lytinen and Tomuro (2002) suggest yet another feature to identify question paraphrases, namely question type similarity, which consists in determining a question’s category in order to match questions only if they belong to the same category. Our focus is on question paraphrase identification in socia</context>
</contexts>
<marker>Jeon, Croft, Lee, 2005</marker>
<rawString>Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005. Finding similar questions in large question and answer archives. In CIKM ’05: Proceedings of the 14th ACM international conference on Information and knowledge management, pages 84–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiwoon Jeon</author>
<author>W Bruce Croft</author>
<author>Joon Ho Lee</author>
<author>Soyeon Park</author>
</authors>
<title>A framework to predict the quality of answers with non-textual features.</title>
<date>2006</date>
<booktitle>In SIGIR ’06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>228--235</pages>
<marker>Jeon, Croft, Lee, Park, 2006</marker>
<rawString>Jiwoon Jeon, W. Bruce Croft, Joon Ho Lee, and Soyeon Park. 2006. A framework to predict the quality of answers with non-textual features. In SIGIR ’06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 228–235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin Jijkoun</author>
<author>Maarten de Rijke</author>
</authors>
<title>Retrieving answers from frequently asked questions pages on the web. In</title>
<date>2005</date>
<booktitle>CIKM ’05: Proceedings of the 14th ACM international conference on Information and knowledge management,</booktitle>
<pages>76--83</pages>
<marker>Jijkoun, de Rijke, 2005</marker>
<rawString>Valentin Jijkoun and Maarten de Rijke. 2005. Retrieving answers from frequently asked questions pages on the web. In CIKM ’05: Proceedings of the 14th ACM international conference on Information and knowledge management, pages 76–83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yu Sun Lee</author>
</authors>
<title>Toward a New Knowledge Sharing Community: Collective Intelligence and Learning through Web-Portal-Based Question-Answer Services. Masters of arts</title>
<date>2006</date>
<booktitle>in communication, culture &amp; technology, Faculty of the Graduate School of Arts and Sciences of</booktitle>
<institution>Georgetown University,</institution>
<note>Online; visited</note>
<contexts>
<context position="12337" citStr="Lee, 2006" startWordPosition="1920" endWordPosition="1921">t instantaneous and numerous, due to the large number of users. Social Q&amp;A sites record the questions and their answers online, and thus constitute a formidable repository of collective intelligence, including answers to complex questions. Moreover, they make it possible for learners to reach other people worldwide. The relevance of social Q&amp;A sites for learning has been little investigated. To our knowledge, there has been only one study which has shown that Korean users of the Naver Question and Answer platform consider that social Q&amp;A sites can satisfactorily and reliably support learning (Lee, 2006). 3.2 WikiAnswers For our experiments we collected a dataset of questions and their paraphrases from the WikiAnswers 46 web site. WikiAnswers1 is a social Q&amp;A site similar to Yahoo! Answers and AnswerBag. As of February 2008, it contained 1,807,600 questions, sorted in 2,404 categories (Answers Corporation, 2008). Compared with its competitors, the main originality of WikiAnswers is that it relies on the wiki technology used in Wikipedia, which means that answers can be edited and improved over time by all contributors. Moreover, the Answers Corporation, which owns the WikiAnswers site, explic</context>
</contexts>
<marker>Lee, 2006</marker>
<rawString>Yu Sun Lee. 2006. Toward a New Knowledge Sharing Community: Collective Intelligence and Learning through Web-Portal-Based Question-Answer Services. Masters of arts in communication, culture &amp; technology, Faculty of the Graduate School of Arts and Sciences of Georgetown University, May. [Online; visited February 15, 2008], http://hdl. handle.net/1961/3701.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuhua Li</author>
<author>David McLean</author>
<author>Zuhair A Bandar</author>
<author>James D O’Shea</author>
<author>Keeley Crockett</author>
</authors>
<title>Sentence Similarity Based on Semantic Nets and Corpus Statistics.</title>
<date>2006</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>18</volume>
<issue>8</issue>
<marker>Li, McLean, Bandar, O’Shea, Crockett, 2006</marker>
<rawString>Yuhua Li, David McLean, Zuhair A. Bandar, James D. O’Shea, and Keeley Crockett. 2006. Sentence Similarity Based on Semantic Nets and Corpus Statistics. IEEE Transactions on Knowledge and Data Engineering, 18(8):1138–1150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven L Lytinen</author>
<author>Noriko Tomuro</author>
</authors>
<title>The Use of Question Types to Match Questions in FAQFinder.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 AAAI Spring Symposium on Mining Answers from Texts and Knowledge Bases,</booktitle>
<pages>46--53</pages>
<contexts>
<context position="9394" citStr="Lytinen and Tomuro (2002)" startWordPosition="1445" endWordPosition="1448">ries associated with documents. Accordingly, research on the identification of question paraphrases in Q&amp;A repositories builds upon both sentence and query paraphrasing. Zhao et al. (2007) propose to utilise user click logs from the Encarta web site to identify question paraphrases. Jeon et al. (2005) employ a related method, in that they identify similar answers in the Naver Question and Answer database to retrieve semantically similar questions, while Jijkoun and de Rijke (2005) include the answer in the retrieval process to return a ranked list of QA pairs in response to a user’s question. Lytinen and Tomuro (2002) suggest yet another feature to identify question paraphrases, namely question type similarity, which consists in determining a question’s category in order to match questions only if they belong to the same category. Our focus is on question paraphrase identification in social Q&amp;A sites. Previous research was mostly based on question paraphrase identification in FAQs (Lytinen and Tomuro, 2002; Tomuro and Lytinen, 2004; Jijkoun and de Rijke, 2005). In FAQs, questions and answers are edited by expert information suppliers, which guarantees stricter conformance to conventional writing rules. In </context>
<context position="16135" citStr="Lytinen and Tomuro, 2002" startWordPosition="2515" endWordPosition="2518">e corresponding target question for each input question. The target question selected is the one which maximises the question similarity value (see section 4.2). 4 Method In order to rate the similarity of input and target questions, we have first pre-processed both the input and target questions and then experimented with several question similarity measures. 4.1 Pre-processing We employ the following steps in pre-processing the questions: Stop words elimination however, we keep question words such as how, why, what, etc. since these make it possible to implicitly identify the question type (Lytinen and Tomuro, 2002; Jijkoun and de Rijke, 2005) Stemming using the Porter Stemmer3 Lemmatisation using the TreeTagger4 Spelling correction using a statistical system based on language modelling (Norvig, 2007).5 3http://snowball.tartarus.org/ 4http://www.ims.uni-stuttgart.de/ projekte/corplex/TreeTagger/ 5We used a Java implementation of the system, jSpellCorrect available at http://developer.gauner.org/ jspellcorrect/, trained with the default English training data, to which we appended the myspell English dictionaries. 47 Stop words were eliminated in all the experimental settings, while stemming and lemmatisa</context>
<context position="19435" citStr="Lytinen and Tomuro, 2002" startWordPosition="3041" endWordPosition="3044"> al., 2007). 4.4 Vector Space Based Measures Vector space measures represent questions as realvalued vectors by taking word frequency into account. Term Vector Similarity Questions are represented as term vectors V1 and V2. The feature values of the vectors are the tf.idf scores of the corresponding terms: N + 1 tf.idf = (1 + log(tf)) * log df where tf is equal to the frequency of the term in the question, N is the number of target questions and df is the number of target questions in which the term occurs, computed by considering the input question as part of the target questions collection (Lytinen and Tomuro, 2002). The similarity of an input question vector and a target question vector is determined by the cosine coefficient: V1 V2 cosine coefficient = |V1 ||V2 | Lucene’s Extended Boolean Model The problem of question paraphrase identification can be cast as an Information Retrieval problem, since in real-world applications the user posts a question and the system returns the best matching questions from its database. We have therefore tested the results obtained using an Information Retrieval system, namely Lucene6, which combines the Vector Space Model and the Boolean model. Lucene has already been s</context>
</contexts>
<marker>Lytinen, Tomuro, 2002</marker>
<rawString>Steven L. Lytinen and Noriko Tomuro. 2002. The Use of Question Types to Match Questions in FAQFinder. In Proceedings of the 2002 AAAI Spring Symposium on Mining Answers from Texts and Knowledge Bases, pages 46–53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing.</title>
<date>1999</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Christopher D. Manning and Hinrich Sch¨utze. 1999. Foundations of Statistical Natural Language Processing. The MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Andras Csomai</author>
</authors>
<title>Wikify!: linking documents to encyclopedic knowledge.</title>
<date>2007</date>
<booktitle>In CIKM ’07: Proceedings of the sixteenth ACM conference on information and knowledge management,</booktitle>
<pages>233--242</pages>
<contexts>
<context position="14451" citStr="Mihalcea and Csomai (2007)" startWordPosition="2261" endWordPosition="2264">ers if they consider that they correspond to another existing question or actually ask an entirely new question. It should be noted that contributors get not reward in terms of trust points for providing or editing alternate wordings for questions. We use the wealth of question paraphrases available on the WikiAnswers website as the so called user generated gold standard in our question paraphrasing experiments. User generated gold standards have been increasingly used in recent years for research evaluation purposes, since they can be easily created from user annotated content. For instance, Mihalcea and Csomai (2007) use manually annotated keywords (links to other articles) in Wikipedia articles to evaluate their automatic keyword extraction and word sense disambiguation algorithms. Similarly, quality assessments provided by users in social media have been used as gold 1http://wiki.answers.com/ 2http://educator.answers.com/ standards for the automatic assessment of post quality in forum discussions (Weimer et al., 2007). It should however be kept in mind that user generated gold standards are not perfect, as already noticed by (Mihalcea and Csomai, 2007), and thus constitute a trade-off solution. For the </context>
</contexts>
<marker>Mihalcea, Csomai, 2007</marker>
<rawString>Rada Mihalcea and Andras Csomai. 2007. Wikify!: linking documents to encyclopedic knowledge. In CIKM ’07: Proceedings of the sixteenth ACM conference on information and knowledge management, pages 233– 242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and Knowledge-based Measures of Text Semantic Similarity.</title>
<date>2006</date>
<booktitle>In Proceedings of AAAI 2006,</booktitle>
<location>Boston,</location>
<contexts>
<context position="6831" citStr="Mihalcea et al. (2006)" startWordPosition="1044" endWordPosition="1047">milarity measures which aim at capturing the semantic equivalence of the sentences being compared. String similarity metrics, when applied to sentences, consist in comparing the words contained in the sentences. There exist many different string similarity measures: word overlap (Tomuro and Lytinen, 2004), longest common subsequence (Islam and Inkpen, 2007), Levenshtein edit distance (Dolan et al., 2004), word n-gram overlap (Barzilay and Lee, 2003) etc. Semantic similarity measures are obtained by first computing the semantic similarity of the words contained in the sentences being compared. Mihalcea et al. (2006) use both corpusbased and knowledge-based measures of the semantic similarity between words. Both string similarity and semantic similarity might be combined: for instance, Islam and Inkpen (2007) combine semantic similarity with longest common subsequence string similarity, while Li et al. (2006) make additional use of word order similarity. 2.2 Query Paraphrasing In Information Retrieval, research on paraphrasing is dedicated to query paraphrasing which consists in identifying semantically similar queries. The overall objective is to discover frequently asked questions and popular topics (We</context>
<context position="26600" citStr="Mihalcea et al., 2006" startWordPosition="4190" endWordPosition="4193">y an important role in questions and should therefore be identified and treated differently from other kinds of tokens. This could be achieved by using a named entity recognition component during pre-processing and then assigning a higher weight to named entities in the retrieval process. This should also improve the results of spelling correction since named entities and abbreviations could be excluded from the correction. Second, semantic errors could be dealt with by using a semantic similarity metric similar to those used in declarative sentence paraphrase identification (Li et al., 2006; Mihalcea et al., 2006; Islam and Inkpen, 2007). 5.4 Comparison and Combination of the Methods In a second part of the experiment, we investigated whether the evaluated methods display independent 50 error patterns, as suggested by our detailed results analysis. Figure 2 confirms that the pre-processing techniques as well as the methods employed result in dissimilar error patterns. We therefore combined several methods and pre-processing techniques in order to verify if we could improve accuracy. We obtained the best results by performing a majority vote combination of the following methods and pre-processing strat</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 2006. Corpus-based and Knowledge-based Measures of Text Semantic Similarity. In Proceedings of AAAI 2006, Boston, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Norvig</author>
</authors>
<title>How to Write a Spelling Corrector.</title>
<date>2007</date>
<journal>Online; visited February</journal>
<volume>22</volume>
<note>http: //norvig.com/spell-correct.html.</note>
<contexts>
<context position="16325" citStr="Norvig, 2007" startWordPosition="2544" endWordPosition="2545">y of input and target questions, we have first pre-processed both the input and target questions and then experimented with several question similarity measures. 4.1 Pre-processing We employ the following steps in pre-processing the questions: Stop words elimination however, we keep question words such as how, why, what, etc. since these make it possible to implicitly identify the question type (Lytinen and Tomuro, 2002; Jijkoun and de Rijke, 2005) Stemming using the Porter Stemmer3 Lemmatisation using the TreeTagger4 Spelling correction using a statistical system based on language modelling (Norvig, 2007).5 3http://snowball.tartarus.org/ 4http://www.ims.uni-stuttgart.de/ projekte/corplex/TreeTagger/ 5We used a Java implementation of the system, jSpellCorrect available at http://developer.gauner.org/ jspellcorrect/, trained with the default English training data, to which we appended the myspell English dictionaries. 47 Stop words were eliminated in all the experimental settings, while stemming and lemmatisation were optionally performed to evaluate the effects of these pre-processing steps on the identification of question paraphrases. We added spelling correction to the conventional pre-proce</context>
<context position="24288" citStr="Norvig, 2007" startWordPosition="3823" endWordPosition="3824">ucene (b) Figure 2: Comparison of the different pre-processing strategies 2(a) and methods 2(b) for 50 input questions. For the pre-processing comparison, the Lucene retrieval method has been used, while the methods have been compared using baseline pre-processing (tokens minus stop words). A filled square indicates that the target question has been retrieved at rank 1, while a blank square indicates that the target question has not been retrieved at rank 1. (a) There are however cases when spelling correction induces worse results, since it is accurate in only approximately 70% of the cases (Norvig, 2007). A major source of errors lies in named entities and abbreviations, which are recognised as spelling errors when they are not part of the training lexicon. For instance, the question What are the GRE score required to get into top100 US universities? (where GRE stands for Graduate Record Examination) is badly corrected as What are the are score required to get into top100 US universities?. Spelling correction also induces an unexpected side effect, when the spelling error does not affect the question’s focus. For instance, consider the following question, with a spelling error: What events oc</context>
</contexts>
<marker>Norvig, 2007</marker>
<rawString>Peter Norvig. 2007. How to Write a Spelling Corrector. [Online; visited February 22, 2008]. http: //norvig.com/spell-correct.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lee Ann Prescott</author>
</authors>
<date>2006</date>
<journal>Yahoo! Answers Captures 96% of Q and A Market Share. Hitwise Intelligence [Online; visited February</journal>
<volume>26</volume>
<note>http://weblogs.hitwise. com/leeann-prescott/2006/12/yahoo_ answers_captures_96_of_q.html.</note>
<contexts>
<context position="11146" citStr="Prescott, 2006" startWordPosition="1724" endWordPosition="1725">positories have existed for a long time on the Internet. Their form has evolved from Frequently Asked Questions (FAQs) to Askan-expert services (Baram-Tsabari et al., 2006) and, even more recently, social Q&amp;A sites. The latest, which include web sites such as Yahoo! Answers and AnswerBag, provide portals where users can ask their own questions as well as answer questions from other users. Social Q&amp;A sites are increasingly popular. For instance, in December 2006 Yahoo! Answers was the second-most visited education/reference site on the Internet after Wikipedia according to the Hitwise company (Prescott, 2006). Even more strikingly, the Q&amp;A portal Naver is the leader of Internet search in South Korea, well ahead of Google (Sang-Hun, 2007). Several factors might explain the success of social Q&amp;A sites: • they provide answers to questions which are difficult to answer with a traditional Web search or using static reference sites like Wikipedia, for instance opinions or advice about a specific family situation or a relationship problem; • questions can be asked anonymously; • users do not have to browse a list of documents but rather obtain a complete answer; • the answers are almost instantaneous and</context>
</contexts>
<marker>Prescott, 2006</marker>
<rawString>Lee Ann Prescott. 2006. Yahoo! Answers Captures 96% of Q and A Market Share. Hitwise Intelligence [Online; visited February 26, 2008]. http://weblogs.hitwise. com/leeann-prescott/2006/12/yahoo_ answers_captures_96_of_q.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehran Sahami</author>
<author>Timothy D Heilman</author>
</authors>
<title>A webbased kernel function for measuring the similarity of short text snippets.</title>
<date>2006</date>
<booktitle>In WWW ’06: Proceedings of the 15th international conference on World Wide Web,</booktitle>
<pages>377--386</pages>
<contexts>
<context position="7509" citStr="Sahami and Heilman, 2006" startWordPosition="1145" endWordPosition="1148"> the semantic similarity between words. Both string similarity and semantic similarity might be combined: for instance, Islam and Inkpen (2007) combine semantic similarity with longest common subsequence string similarity, while Li et al. (2006) make additional use of word order similarity. 2.2 Query Paraphrasing In Information Retrieval, research on paraphrasing is dedicated to query paraphrasing which consists in identifying semantically similar queries. The overall objective is to discover frequently asked questions and popular topics (Wen et al., 2002) or suggest related queries to users (Sahami and Heilman, 2006). Traditional string similarity metrics are usually deemed inefficient for such short text snippets and alternative similarity metrics have therefore been proposed. For instance, Wen et al. (2002) rely on user click logs, based on the idea that queries and questions which result in identical document clicks are bound to be similar. 2.3 Question Paraphrasing Following previous research in this domain, we define question paraphrases as questions which have all the following properties: (a) they have identical meanings, (b) they have the same answers, and (c) they present alternate wordings. Ques</context>
</contexts>
<marker>Sahami, Heilman, 2006</marker>
<rawString>Mehran Sahami and Timothy D. Heilman. 2006. A webbased kernel function for measuring the similarity of short text snippets. In WWW ’06: Proceedings of the 15th international conference on World Wide Web, pages 377–386.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Choe Sang-Hun</author>
</authors>
<title>To outdo Google, Naver taps into Korea’s collective wisdom. International Herald Tribune,</title>
<date>2007</date>
<note>http://www.iht.com/articles/2007/ 07/04/technology/naver.php.</note>
<contexts>
<context position="11277" citStr="Sang-Hun, 2007" startWordPosition="1746" endWordPosition="1747">ert services (Baram-Tsabari et al., 2006) and, even more recently, social Q&amp;A sites. The latest, which include web sites such as Yahoo! Answers and AnswerBag, provide portals where users can ask their own questions as well as answer questions from other users. Social Q&amp;A sites are increasingly popular. For instance, in December 2006 Yahoo! Answers was the second-most visited education/reference site on the Internet after Wikipedia according to the Hitwise company (Prescott, 2006). Even more strikingly, the Q&amp;A portal Naver is the leader of Internet search in South Korea, well ahead of Google (Sang-Hun, 2007). Several factors might explain the success of social Q&amp;A sites: • they provide answers to questions which are difficult to answer with a traditional Web search or using static reference sites like Wikipedia, for instance opinions or advice about a specific family situation or a relationship problem; • questions can be asked anonymously; • users do not have to browse a list of documents but rather obtain a complete answer; • the answers are almost instantaneous and numerous, due to the large number of users. Social Q&amp;A sites record the questions and their answers online, and thus constitute a </context>
</contexts>
<marker>Sang-Hun, 2007</marker>
<rawString>Choe Sang-Hun. 2007. To outdo Google, Naver taps into Korea’s collective wisdom. International Herald Tribune, July 4. http://www.iht.com/articles/2007/ 07/04/technology/naver.php.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noriko Tomuro</author>
<author>Steven Lytinen</author>
</authors>
<title>Retrieval Models and Q&amp;A Learning with FAQ Files.</title>
<date>2004</date>
<booktitle>New Directions in Question Answering,</booktitle>
<pages>183--194</pages>
<editor>In Mark T. Maybury, editor,</editor>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="3764" citStr="Tomuro and Lytinen, 2004" startWordPosition="567" endWordPosition="570">ns available in online social Q&amp;A sites. The task of Question Answering can then be boiled down to the problem of finding question paraphrases in a database of answered questions. Question paraphrases are questions which have identical meanings and expect the same answer while presenting alternate wordings. Several methods have already been proposed to identify question 44 Proceedings of the Third ACL Workshop on Innovative Use of NLP for Building Educational Applications, pages 44–52, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics paraphrases mostly in FAQs (Tomuro and Lytinen, 2004) or search engine logs (Zhao et al., 2007). In this paper, we focus on the problem of question paraphrase identification in social Q&amp;A sites within a realistic information seeking scenario: given a user question, we want to retrieve the best matching question paraphrase from a database of previously answered questions in order to display the corresponding answer. The use of social Q&amp;A sites for educational applications brings about new challenges linked to the variable quality of social media content. As opposed to questions in FAQs, which are subject to editorial control, questions in social </context>
<context position="6515" citStr="Tomuro and Lytinen, 2004" startWordPosition="997" endWordPosition="1000">on of synonymy, sentence level paraphrasing is more difficult to grasp and cannot be equated with word-for-word or phrase-by-phrase substitution since it might entail changes in the structure of the sentence (Barzilay and Lee, 2003). In practice, sentence paraphrases are identified using various string and semantic similarity measures which aim at capturing the semantic equivalence of the sentences being compared. String similarity metrics, when applied to sentences, consist in comparing the words contained in the sentences. There exist many different string similarity measures: word overlap (Tomuro and Lytinen, 2004), longest common subsequence (Islam and Inkpen, 2007), Levenshtein edit distance (Dolan et al., 2004), word n-gram overlap (Barzilay and Lee, 2003) etc. Semantic similarity measures are obtained by first computing the semantic similarity of the words contained in the sentences being compared. Mihalcea et al. (2006) use both corpusbased and knowledge-based measures of the semantic similarity between words. Both string similarity and semantic similarity might be combined: for instance, Islam and Inkpen (2007) combine semantic similarity with longest common subsequence string similarity, while Li</context>
<context position="9816" citStr="Tomuro and Lytinen, 2004" startWordPosition="1510" endWordPosition="1513">antically similar questions, while Jijkoun and de Rijke (2005) include the answer in the retrieval process to return a ranked list of QA pairs in response to a user’s question. Lytinen and Tomuro (2002) suggest yet another feature to identify question paraphrases, namely question type similarity, which consists in determining a question’s category in order to match questions only if they belong to the same category. Our focus is on question paraphrase identification in social Q&amp;A sites. Previous research was mostly based on question paraphrase identification in FAQs (Lytinen and Tomuro, 2002; Tomuro and Lytinen, 2004; Jijkoun and de Rijke, 2005). In FAQs, questions and answers are edited by expert information suppliers, which guarantees stricter conformance to conventional writing rules. In social Q&amp;A sites, questions and answers are written by users and may hence be error-prone. Question paraphrase identification in social Q&amp;A sites has been little investigated. To our knowledge, only Jeon et al. (2005) have used data from a Q&amp;A site, namely the Korean Naver portal, to find semantically similar questions. Our work is related to the latter since it employs a similar dataset, yet in English and from a diff</context>
</contexts>
<marker>Tomuro, Lytinen, 2004</marker>
<rawString>Noriko Tomuro and Steven Lytinen. 2004. Retrieval Models and Q&amp;A Learning with FAQ Files. In Mark T. Maybury, editor, New Directions in Question Answering, pages 183–194. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Weimer</author>
<author>Iryna Gurevych</author>
<author>Max M¨uhlh¨auser</author>
</authors>
<title>Automatically Assessing the Post Quality in Online Discussions on Software.</title>
<date>2007</date>
<booktitle>In Proceedings of the Demo and Poster Sessions of the 45th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>125--128</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<marker>Weimer, Gurevych, M¨uhlh¨auser, 2007</marker>
<rawString>Markus Weimer, Iryna Gurevych, and Max M¨uhlh¨auser. 2007. Automatically Assessing the Post Quality in Online Discussions on Software. In Proceedings of the Demo and Poster Sessions of the 45th Annual Meeting of the Association for Computational Linguistics, pages 125–128, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ji-Rong Wen</author>
<author>Jian-Yun Nie</author>
<author>Hong-Jiang Zhang</author>
</authors>
<title>Query clustering using user logs.</title>
<date>2002</date>
<journal>ACM Trans. Inf. Syst.,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="7446" citStr="Wen et al., 2002" startWordPosition="1134" endWordPosition="1137">6) use both corpusbased and knowledge-based measures of the semantic similarity between words. Both string similarity and semantic similarity might be combined: for instance, Islam and Inkpen (2007) combine semantic similarity with longest common subsequence string similarity, while Li et al. (2006) make additional use of word order similarity. 2.2 Query Paraphrasing In Information Retrieval, research on paraphrasing is dedicated to query paraphrasing which consists in identifying semantically similar queries. The overall objective is to discover frequently asked questions and popular topics (Wen et al., 2002) or suggest related queries to users (Sahami and Heilman, 2006). Traditional string similarity metrics are usually deemed inefficient for such short text snippets and alternative similarity metrics have therefore been proposed. For instance, Wen et al. (2002) rely on user click logs, based on the idea that queries and questions which result in identical document clicks are bound to be similar. 2.3 Question Paraphrasing Following previous research in this domain, we define question paraphrases as questions which have all the following properties: (a) they have identical meanings, (b) they have </context>
<context position="18421" citStr="Wen et al., 2002" startWordPosition="2857" endWordPosition="2860"> and q2 represented by the set of distinct words Q1 and Q2 they contain is computed as follows (Manning and Sch¨utze, 1999): matching coefficient = |Q1 n Q2 | Overlap coefficient The overlap coefficient is computed according to the following formula (Manning and Sch¨utze, 1999): overlap coefficient = |Q1 n Q2 | min( |Q1 |, |Q2 |) Normalised Edit Distance The edit distance of two questions is the number of words that need to be substituted, inserted, or deleted, to transform q1 into q2. In order to be able to compare the edit distance with the other metrics, we have used the following formula (Wen et al., 2002) which normalises the minimum edit distance by the length of the longest question and transforms it into a similarity metric: edit dist(q1, q2) normalised edit distance = 1 − max( |q1 |, |q2 |) Word Ngram Overlap This metric compares the word n-grams in both questions: where Gn(q) is the set of n-grams of length n in question q and N usually equals 4 (Barzilay and Lee, 2003; Cordeiro et al., 2007). 4.4 Vector Space Based Measures Vector space measures represent questions as realvalued vectors by taking word frequency into account. Term Vector Similarity Questions are represented as term vector</context>
</contexts>
<marker>Wen, Nie, Zhang, 2002</marker>
<rawString>Ji-Rong Wen, Jian-Yun Nie, and Hong-Jiang Zhang. 2002. Query clustering using user logs. ACM Trans. Inf. Syst., 20(1):59–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shiqi Zhao</author>
<author>Ming Zhou</author>
<author>Ting Liu</author>
</authors>
<title>Learning Question Paraphrases for QA from Encarta Logs.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1795--1801</pages>
<location>Hyderabad, India,</location>
<contexts>
<context position="3806" citStr="Zhao et al., 2007" startWordPosition="575" endWordPosition="578"> of Question Answering can then be boiled down to the problem of finding question paraphrases in a database of answered questions. Question paraphrases are questions which have identical meanings and expect the same answer while presenting alternate wordings. Several methods have already been proposed to identify question 44 Proceedings of the Third ACL Workshop on Innovative Use of NLP for Building Educational Applications, pages 44–52, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics paraphrases mostly in FAQs (Tomuro and Lytinen, 2004) or search engine logs (Zhao et al., 2007). In this paper, we focus on the problem of question paraphrase identification in social Q&amp;A sites within a realistic information seeking scenario: given a user question, we want to retrieve the best matching question paraphrase from a database of previously answered questions in order to display the corresponding answer. The use of social Q&amp;A sites for educational applications brings about new challenges linked to the variable quality of social media content. As opposed to questions in FAQs, which are subject to editorial control, questions in social Q&amp;A sites are often ill-formed or contain </context>
<context position="8957" citStr="Zhao et al. (2007)" startWordPosition="1371" endWordPosition="1374">er of ounces per pound?, How many oz. in a lb.? Question paraphrases share some properties both with declarative sentence paraphrases and query paraphrases. On the one hand, questions are complete sentences which differ from declarative sentences by their specific word order and the presence of question words and a question focus. On the other hand, questions are usually associated with answers, which makes them similar to queries associated with documents. Accordingly, research on the identification of question paraphrases in Q&amp;A repositories builds upon both sentence and query paraphrasing. Zhao et al. (2007) propose to utilise user click logs from the Encarta web site to identify question paraphrases. Jeon et al. (2005) employ a related method, in that they identify similar answers in the Naver Question and Answer database to retrieve semantically similar questions, while Jijkoun and de Rijke (2005) include the answer in the retrieval process to return a ranked list of QA pairs in response to a user’s question. Lytinen and Tomuro (2002) suggest yet another feature to identify question paraphrases, namely question type similarity, which consists in determining a question’s category in order to mat</context>
<context position="17375" citStr="Zhao et al., 2007" startWordPosition="2687" endWordPosition="2690">erformed to evaluate the effects of these pre-processing steps on the identification of question paraphrases. We added spelling correction to the conventional pre-processing steps, since we target paraphrasing of questions which often contain spelling errors, such as When was indoor pluming invented? or What is the largest countery in the western Hemipher? Other related endeavours at retrieving question paraphrases have identified spelling mistakes in questions as a significant source of errors in the retrieval process, but have not attempted to solve this problem (Jijkoun and de Rijke, 2005; Zhao et al., 2007). 4.2 Question Similarity Measures We have experimented with several kinds of question similarity measures, belonging to two different families of measures: string similarity measures and vector space measures. 4.3 String Similarity Measures Basic string similarity measures compare the words contained in the questions without taking word frequency into account. Matching coefficient The matching coefficient of two questions q1 and q2 represented by the set of distinct words Q1 and Q2 they contain is computed as follows (Manning and Sch¨utze, 1999): matching coefficient = |Q1 n Q2 | Overlap coef</context>
</contexts>
<marker>Zhao, Zhou, Liu, 2007</marker>
<rawString>Shiqi Zhao, Ming Zhou, and Ting Liu. 2007. Learning Question Paraphrases for QA from Encarta Logs. In Proceedings of the 20th International Joint Conference on Artificial Intelligence, pages 1795–1801, Hyderabad, India, January 6-12.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>