<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002934">
<title confidence="0.884895">
Use of Support Vector Machines in Extended Named Entity
Recognition
</title>
<author confidence="0.584471">
Koichi Takeuchi and Nigel Collier
</author>
<affiliation confidence="0.669984">
National Institute of Informatics
</affiliation>
<address confidence="0.965423">
2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo, 101-8430 Japan
</address>
<email confidence="0.683672">
.ac. jp
</email>
<sectionHeader confidence="0.969304" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999363125">
This paper explores the use of Sup-
port Vector Machines (SVMs) for an
extended named entity task. We inves-
tigate the identification and classifica-
tion of technical terms in the molecular
biology domain and contrast this to re-
sults obtained for traditional NE recog-
nition on the MUC-6 data set. Fur-
thermore we compare the performance
of the SVM model to a standard Ell\ilM
bigram model. Results show that the
SVM utilizing a rich feature set of a
+3 context window and POS features
(MUC-6 only) had a significant perfor-
mance advantage on both the MUC-6
and molecular biology data sets.
</bodyText>
<sectionHeader confidence="0.996342" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.993492857142857">
Named entity (NE) extraction is now firmly es-
tablished as a core technology for understanding
low level semantics of texts. NE was formalized
in the DARPA-sponsored Message Understand-
ing Conference (MUC)-6 (MUC, 1995) and since
then several methodologies have been widely ex-
plored:
</bodyText>
<listItem confidence="0.969317714285714">
• heuristics-based, using rules written by
human experts after inspecting examples
(Fukuda et al., 1998);
• supervised such as (Bikel et al., 1997) using
labelled training examples;
• non-supervised methods such as (Collins
and Singer, 1999).
</listItem>
<bodyText confidence="0.999944827160494">
NE&apos;s main role has been to identify expres-
sions such as the names of people, places and or-
ganizations as well as date and time expressions.
Such expressions are hard to analyze using tra-
ditional natural language processing (NLP) be-
cause they belong to the open class of expres-
sions, i.e. there is an infinite variety and new
expressions are constantly being invented.
The application of NE to non-news domains
requires us to consider extending NE so that it
can capture types, i.e. instances of conceptual
classes as well as individuals. To distinguish be-
tween traditional NE and extended NE we refer
to the later as NE-i-. There are several issues
that may mean that NE+ is more challenging
that NE. The most important is the number
of variants of NE+ expressions due to graph-
ical, morphological, shallow syntactic and dis-
course variations. For example the use of head
sharing combined with embedded abbreviations
in unliganded (apo)- and liganded (holo)-LBD.
Such expressions will require syntactic analysis
beyond simple noun phrase chunking if they are
to be successfully captured. NE+ expressions
may also require richer contextual evidence than
is needed for regular NEs - for example knowl-
edge of the head noun or the predicate. At the
ontology level there are complex issues related
to granularity when deciding on which class a
possible NE+ expression should be assigned to.
NE+ expressions will typically belong to a
much richer taxonomy than NE, opening up the
possibility of combining information extraction
(IE) with deep knowledge representations such
as ontologies. This is an area we are currently
exploring (Collier et al., 2002). Examples of
NE+ classes include, a person&apos;s name, a pro-
tein name, a chemical formula or a computer
product code. All of these may be valid candi-
dates for tagging depending on whether they are
contained in the ontology.
NE+ can be viewed as a type of multiple
classification task and there are several effective
and well studied learning algorithms available
for this such as Hidden Markov Models (HA/11\4s)
(Rabiner and Juang, 1986) and transformation-
based error-driven learning (TBL) (Brill, 1995).
Recently a new learning paradigm called sup-
port vector machines (SV1&apos;v4s) (Vapnik, 1995)
has been the focus of intensive research in ma-
chine learning due to its capacity to learn effec-
tively from large feature sets. SVMs have been
applied very successfully in the past to several
traditional classification tasks such as text clas-
sification. Promising results have been reported
for NLP tasks such as part of speech tagging and
chunking, e.g. (Kudoh and Matsumoto, 2000).
We have implemented and compared two
learning methods (SVM, HA/11\4) and tested
them on two data sets. The comparison between
these models is informative because of the dif-
ferent nature of the two learning methods. In
the case of the HAIM the learning approach is
generative, i.e. it makes use of positive exam-
ples to build a model of NE classes and then
evaluates each unseen sentence to see how well
each of the words &apos;fits&apos; the model. The SVM
on the other hand is a discriminative approach
and makes use of both positive and negative ex-
amples to learn the distinction between the two
classes. Another major difference is that the
SVM outputs a measure of distance from the
classification function whereas the HAIM uses
the Viterbi algorithm (Viterbi, 1967) to decode
using maximum likelihood probabilities. Basi-
cally we expect the models to have quite dif-
ferent strengths and weaknesses and hopefully
these can be complementary, allowing us even-
tually to combine the approaches to achieve a
composite model. The two models are described
further below along with their performance.
</bodyText>
<sectionHeader confidence="0.889097" genericHeader="introduction">
2 Method
</sectionHeader>
<subsectionHeader confidence="0.883259">
2.1 SVM
</subsectionHeader>
<bodyText confidence="0.999797186046512">
We developed our method using the Tiny SVM
package from NAIST 1 which is an implementa-
tion of Vladimir Vapnik&apos;s SVM combined with
an optimization algorithm (Joachims, 1999).
SVMs like other inductive-learning ap-
proaches take as input a set of training exam-
ples (given as binary valued feature vectors) and
finds a classification function that maps them
to a class. There are several points about SVM
models that are worth summarizing here. The
first is that SVMs are known to robustly han-
dle large feature sets and to develop models
that maximize their generalizability. This makes
them an ideal model for the NE+ task. General-
izability in SVMs is based on statistical learning
theory and the observation that it is useful some-
times to misclassify some of the training data so
that the margin between other training points is
maximized (Cortes and Vapnik, 1995). This is
particularly useful for real world data sets that
often contain inseparable data points. Secondly,
although training is generally slow, the resulting
model is usually small and runs quickly as only
the support vectors need to be retained, i.e. the
patterns that help define the function which sep-
arates positive from negative examples. Thirdly
is that SVMs are binary classifiers and so we
need to combine SVM models to obtain a multi-
class classifier.
Formally then we can consider the purpose
of the SVM to be to estimate a classification
function f : x 1+11 using training examples
from x x 1+11 so that error on unseen exam-
ples is minimized. The classification function
returns either +1 if the test data is a mem-
ber of the class, or —1 if it is not. Although
SVMs learn what are essentially linear decision
functions, the effectiveness of the strategy is en-
sured by mapping the input patterns x to a fea-
ture space F using a nonlinear mapping function
: x F. Since the algorithm is well described
in the literature cited earlier we will focus our
description from now on the features we used for
</bodyText>
<footnote confidence="0.9602025">
1-Tiny SVM is available from http:// http://cl.aist-
nara.ac.jp/ taku-ku/software/ TinySVM/
</footnote>
<figure confidence="0.96891075">
Word
POS
Orthography
Class
</figure>
<figureCaption confidence="0.9670236">
Figure 1: Lexical features and context consid-
ered by the SVM model when deciding the class
tag T at the focus position t includes surface
word forms, part of speech, orthographic fea-
tures and previous word class tags.
</figureCaption>
<bodyText confidence="0.997343421052632">
training.
In our implementation each training pattern
is given as a vector which represents certain lex-
ical features. All models use a surface word,
an orthographic feature (Collier et al., 2000)
and previous class assignments, but our experi-
ments with part of speech (POS) features (Brill,
1992) showed that POS features actually inhib-
ited performance in the molecular biology data
set which we present below. This is probably
because the POS tagger was trained on news
texts. Therefore POS features are used only for
the MUC-6 news data set where we show a com-
parison with and without these features. The
form of the vector is basically a bag of words,
i.e. word positions or ordering are not recorded.
In the experiments we report below we use fea-
ture vectors consisting of differing amounts of
&apos;context&apos; by varying the window around the fo-
cus word which is to be classified into one of the
NE+ classes. The full window of context consid-
ered in these experiments is +3 about the focus
word as shown in Figure 1. In pattern formation
we took an JOB based approach to NE+ chunk
identification in which each word was assigned
a class tag from {I _c, B_Ct, 0} where Ct is the
class, B stands for a beginning of chunk tag,
I stands for an in-chunk tag, and 0 stands for
outside of chunk, i.e. not a member of one of
the given classes.
Due to the nature of the SVM as a binary
classifier it is necessary in a multi-class task to
consider the strategy for combining several clas-
sifiers. In our experiments with Tiny SVM the
strategy used was one-against-one rather than
one-against-the-rest. For example, if we have
four classes A, B, C and D then Tiny SVM
builds classifiers for (1) A against (B, C, D),
(2) B against (C, D), and (3) C against D. The
winning class is the one which obtains the most
votes of the pairwise classifiers.
We implemented two versions of the SVM.
SVM&apos; uses a +3 window about the focus word
and is implemented with the polynomial (poly)
kernel function. SVM2 is used to directly com-
pare the performance of the SVM with the Ell\iIM
model described below and here we use only fea-
tures for the focus word and previous word, i.e.
a more limited context. Due to effects of data
sparseness the Ell\iIM would be very difficult to
train using a wider context window - this is one
of the advantages we hope to test in SVM&apos;.
The kernel function k:xxx mentioned
above basically defines the feature space f by
computing the inner product of pairs of data
points. For e x we explored the simple poly-
nomial function k(xi,xj) = (xi • xj +1)(1.
</bodyText>
<subsectionHeader confidence="0.893351">
2.2 HMM
</subsectionHeader>
<bodyText confidence="0.999276466666667">
The Ell\iIM we implemented for comparison with
the SVM was the version fully described in (Col-
lier et al., 2000)2. Basically this is a linear in-
terpolating Ell\iIM trained using maximum likeli-
hood estimates from bigrams of the surface word
and an orthographic feature which is determin-
istically chosen. No part of speech was used in
the formulation of this model.
We consider words to be ordered pairs consist-
ing of a surface word, W, and a word feature,
F, given as &lt; W, F &gt;. As is common prac-
tice, we need to calculate the probabilities for a
word sequence for the first word&apos;s name class Co
and every other word differently since we have
no initial name-class to make a transition from.
</bodyText>
<footnote confidence="0.993423333333333">
2For purposes of comparison we note that in further
tests Nobata et al. (2000) found this HMM to be superior
to the C4.5 decision tree rule learner.
</footnote>
<equation confidence="0.750566">
t-3 t-2 1-1 t t+1 t+2 t+3
It
</equation>
<bodyText confidence="0.9951185">
Accordingly we use the following equation to cal-
culate the initial name class probability,
</bodyText>
<equation confidence="0.95031725">
Pr(Col &lt; Wo, Fo &gt;) =
go f (Co &lt; Wo, Fo &gt;) +
alf(Col &lt; F0 &gt;) +
a2f (Co) (1)
</equation>
<bodyText confidence="0.939725">
and for all other words and their name classes
Ct as follows:
</bodyText>
<figure confidence="0.712427714285714">
Pr (Ctl Wt, Ft &gt;,&lt; T/Vt-1, Ft—) &gt;,C_1) =
Ao f (Ctl &lt; Wt, Ft &gt;,&lt; Wt-1, Ft—) &gt;,Ct-1)
Al f (Ctl &lt;,F &gt;,&lt; Wt-1, Ft—i &gt;,C_1)
A2 f (Ctl &lt; Wt, Ft &gt;, &lt; Ft-1 &gt;,Ct-1)
f (Ctl &lt; Ft &gt; , &lt; Ft—) &gt;,Ct-1)
f (Ctrt-1)
f (Ct) (2)
</figure>
<bodyText confidence="0.999895266666667">
where f (I) is calculated with maximum-
likelihood estimates from counts on training
data.
In our current system we set the constants Ai
and ai by hand and let E cri = 1.0, E Ai = 1.0,
O o &gt; Ui &gt; Cr2, AO &gt; Ai &gt; A5. The current
name-class Ct is conditioned on the current word
and feature, the previous name-class, Ct_i, and
previous word and feature.
Once the state transition probabilities have
been calculated according to Equations 1 and
2, the Viterbi algorithm (Viterbi, 1967) is used
to search the state space of possible name class
assignments in linear time to find the highest
probability path, i.e. to maximize Pr(1/17, C).
</bodyText>
<subsectionHeader confidence="0.99321">
2.3 Data Sets
</subsectionHeader>
<bodyText confidence="0.9975317">
We used two data sets in our study one for
NE+ and the other for traditional NE. The NE+
collection (Bio1) consists of 100 MEDLINE ab-
stracts (23586 words) in the domain of molec-
ular biology annotated for the names of genes
and gene products (Tateishi et al., 2000). The
second (MUC-6) is the collection of 60 executive
succession texts (24617 words) used in MUC-6
for dryrun and testing. Details are shown in Ta-
bles 1 and 2.
</bodyText>
<table confidence="0.9995273125">
Class # Description
PROTEIN 2125 proteins, protein
groups,families,
complexes and
substructures
DNA 358 DNAs, DNA groups,
regions and genes
RNA 30 RNAs, RNA groups,
regions and genes
SOURCE.c1 93 cell line
SOURCE.ct 417 cell type
SOURCE.mo 21 mono-organism
SOURCE.mu 64 multi-celled organism
SOURCE.vi 90 viruses
SOURCE.s1 77 sublocation
SOURCE.ti 37 tissue
</table>
<tableCaption confidence="0.758171">
Table 1: Markup classes used in Bio1 with the
number of word tokens for each class.
</tableCaption>
<sectionHeader confidence="0.994942" genericHeader="related work">
3 Results and Analysis
</sectionHeader>
<bodyText confidence="0.998621181818182">
Results are given as F-scores (van Rijsbergen,
1979) and calculated using the CoNLL eval-
uation script3. F-score is defined as F =
(2P R)I (P R). where P denotes Precision and
R Recall. P is the ratio of the number of cor-
rectly found NE chunks to the number of found
NE chunks, and R is the ratio of the number
of correctly found NE chunks to the number of
true NE chunks.
Table 3 shows the overall F-score for the three
models and two collections, calculated using 10-
</bodyText>
<table confidence="0.7112219">
3 Available from http://leg-www.uia.ac.be/
con112002/ner/bin/
Class #
DATE 542
LOCATION 390
ORGANIZATION 1783
MONEY 423
PERCENT 108
PERSON 838
TIME 3
</table>
<tableCaption confidence="0.928729">
Table 2: Markup classes used in MUC-6 with
the number of word tokens for class label.
</tableCaption>
<bodyText confidence="0.999837087719298">
fold cross validation on the total test collection.
Due to the size of the collections we did not ob-
serve an optimal result for each model but we
found a clear and sustained advantage by SVM&apos;
over the Ell\ilM for the NE task in MUC-6 and
the NE+ task in Biol. The only drawback we
observed with SVM2 was that it seemed to be
quite weak for the very low frequency classes
such as RNA, SOURCE.mo or TIME where the
Ell\ilM usually proved to be more robust. SVM2
was the weakest model that we tested and we
can conclude that when trained with similar
knowledge to the Ell\ilM the SVM has no partic-
ular performance advantage that we could ob-
serve. However by exploiting the SVMs capabil-
ity to easily handle large feature sets including a
wide context window and POS tags the results
suggest that the SVM will perform at a signif-
icantly higher level than the 11MM. A detailed
break down of results by class is shown in Table
4.
What is not obvious from the tables is the
effect we found of tokenization. In all the exper-
iments reported for SVM in Table 3 we used the
FDG parser (Tapanainen and Jarvinen, 1997)
which we found gave much better results for
Biol than a simple tokenization strategy that
simply split each word at spaces or punctuation
marks. On MUC-6 the advantage was less clear
and we concluded that the frequent and ambigu-
ous use of hyphen in Biol was the key factor.
On the NE+ task in Biol we found that SVM&apos;
slightly but clearly outperformed the El1\i1M. In
analysis of SVM&apos; results we identified several
types of error. The first and perhaps most seri-
ous type was caused by local syntactic ambigui-
ties such as head sharing in 39-kD SH2, 8113 do-
main which should have been classed as a PRO-
TEIN, but the SVM split it into two PROTEIN
expressions 8II2 and 8113 domain. In partic-
ular the ambiguous use of hyphen, e.g. 14E1
single-chain (sc) Fe antibody, and parentheses,
e.g. scFy (14E1), seemed to cause the SVM
more difficulties than the 11l\i1M. It is likely that
the limited contextual information we gave to
the SVM was the cause of this and can be im-
proved on using grammatical features such as
head noun or main verb. Ell\ilM seems to gain
an advantage through the Viterbi algorithm by
being able to partially consider evidence over
the entire sentence. A second minor type of er-
ror seemed to be the result of inconsistencies in
the annotation scheme for Biol such as the in-
clusion of a definite description in a term name
for UT7/TPO cells, a thrombopoietin- dependent
megakaryocytic cell line which was all considered
to be a SOURCE.ct expression.
</bodyText>
<sectionHeader confidence="0.992066" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999915527777778">
There are many more kernel parameters to ex-
plore than can be dealt with here and we will
be continuing our investigation by tuning the
SVM parameters. The results do however pro-
vide an indication of performance trends: the
first is that SVM will outperform the Ell\ilM by a
significant margin on both the MUC-6 and Biol
data sets if it is given a wide context window
(+3) and a rich feature set. The second is that
the SVM lacked sufficient knowledge about com-
plex structures in NE+ expressions to achieve
its best performance on Biol. We believe that
with further tuning our SVM model will prove
more useful in NE+ and allow us to combine ev-
idence from large feature sets in order to model
local structure and context. Furthermore, if a
training set was developed for the POS tagger
in the NE+ domain it seems likely that the SVM
would strongly benefit from this. In its current
configuration SVM&apos; could be combined with the
Ell\ilM on the Biol data set to achieve better per-
formance in 5 of the 10 classes.
While acknowledging the danger of drawing
broad conclusions about the NE+ task from one
domain-based data set, pending further analy-
sis, we can cautiously say that performance on
the two data sets has shown that the MUC-6 NE
task is somewhat easier than the Biol NE+ task.
Despite a few investigations into the nature of
the NE task (Palmer and Day, 1997) (Nobata
et al., 2000) the information theoretical aspects
of the knowledge required for the task are still
not well understood and this must be consid-
ered as a key area for future research. In order
to test our method more accurately and develop
a composite model we are now building a more
</bodyText>
<table confidence="0.998098571428572">
Model
Data set Ell\ilM SVM&apos; (poly) SVM2
degree d= degree d=
1 2 3 4 2
Biolt 70.97 71.33 71.78 68.54 65.09 65.63
MUC-6t 70.38 72.86 73.21 69.22 65.12 65.94
MUC-61 - 74.80 74.66 72.68 67.92 68.83
</table>
<tableCaption confidence="0.840495">
Table 3: Overall F-scores for each of the learning methods on the two test sets using 10-fold cross
validation on all data. SVM1(poly) denotes the SVM trained using a polynomial kernel function
</tableCaption>
<bodyText confidence="0.992299">
and a 13 context window; SVM2 results when a context window of -1 and the focus were used
so that direct comparisons with the Ell\tM can be made. t Results for models using surface word
and orthographic features but no part of speech features; Results for models using surface word,
orthographic and part of speech features.
realistic data set for molecular biology from full
journal articles.
</bodyText>
<sectionHeader confidence="0.996904" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999957333333333">
This work was supported in part by the
Japanese Ministry of Education and Science
(grant no. 14701020). We would like to thank
Jun-ichi Tsujii for providing the data set Bio1
and to the anonymous referees whose comments
have helped to improve the paper.
</bodyText>
<sectionHeader confidence="0.998869" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.986569237288136">
D. Bikel. S. Miller. R. Schwartz. and R. Wesichedel.
1997. Nymble: a high-performance learning name-
finder. In Proceedings of the Fifth Conference on
Applied Natural Language Processing (ANLP&apos;97)„
Washington D. C., USA.. pages 194-201.31 March
- 3 April.
E. Brill. 1992. A simple rule-based part of speech
tagger. In Third Conference on Applied Natural
Language Processing - Association for Computa-
tional Linguistics, Trento, Italy. pages 152-155.
31st March - 3rd April.
E. Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part-of-speech tagging. Computational
Linguistics. 21:543-565.
N. Collier. C. Nobata,. and J. Tsujii. 2000. Extract-
ing the names of genes and gene products with a
hidden Markov model. In Proceedings of the 18th
International Conference on Computational Lin-
guistics (COLING &apos;2000, Saarbrucken, Germany,
July 31st-August 4th.
N. Collier. K. Takeuchi. C. Nobata,. J. Fukumoto.
and N. Oga,ta,. 2002. Progress on multi-lingual
named entity annotation guidelines using RDF(S).
In Proceedings of the Third International Con-
ference on Language Resources and Evaluation
(LREC&apos;2002), Las Palmas, Spain. pages 2074-
2081. May 27th - June 2nd.
M. Collins and Y. Singer. 1999. Unsupervised mod-
els for named entity classification. In Proceedings
of the 1999 Joint SIGDAT Conference on Empir-
ical Methods in Natural Language Processing and
Very Large Corpora.
C. Cortes and V. Va,pnik. 1995. Support-vector net-
works. Machine Learning. 20:273-297. November.
K. Fukuda. T. Tsunoda. A. Tamura. and T. Takagi.
1998. Toward information extraction: identifying
protein names from biological papers. In Proceed-
ings of the Pacific Symposium on Biocomputing&apos;98
(PSB&apos;98). pages 707-718. January.
T. Joa,chims. 1999. Making large-scale SVM learn-
ing practical. In B. Scholkopf. C. Burges. and
A. Smola. editors. Advances in Kernel Methods
- Support Vector Learning. MIT Press.
T. Kudoh and Y. Matsumoto. 2000. Use of sup-
port vector learning for chunk identification. In
Proceedings of the Fourth Conference on Natural
Language Learning (CoNLL-2000„ Lisbon, Por-
tugal. pages 142-144.
DARPA. 1995. Proceedings of the Sixth Message Un-
derstanding Conference(MUC-6). Columbia. MD.
USA. November. Morgan Kaufmann.
C. Nobata,. N. Collier. and J. Tsujii. 2000. Compar-
ison between tagged corpora for the named entity
Model
Class SVM]-(po1y)l- HlVIM
d=2
F13=1 F13=1
Data set Biol
</reference>
<table confidence="0.99927275">
PROTEIN 75.99 78.87 77.40 78.84 78.81 78.82
DNA 69.32 48.88 57.33 48.80 45.79 47.25
RNA 80.00 13.79 23.53 41.67 16.67 23.81
SOURCE.c1 75.00 48.91 59.21 50.00 38.71 43.64
SOURCE.ct 78.89 54.68 64.59 70.60 64.51 67.42
SOURCE.mo 0.00 0.00 0.00 77.78 35.00 48.28
SOURCE.mu 72.50 45.31 55.77 60.00 32.81 42.42
SOURCE.vi 91.80 60.87 73.20 78.08 62.64 69.51
SOURCE.s1 72.41 54.55 62.22 65.67 57.14 61.11
SOURCE.ti 50.00 2.70 5.13 47.06 21.62 29.63
All 75.89 68.09 71.78 73.10 58.95 70.97
Data set MUC-6
DATE 84.52 62.56 71.90 74.47 61.67 67.47
LOCATION 67.78 48.22 56.35 66.23 40.32 50.12
ORG. 70.17 81.19 75.28 65.33 66.01 65.67
MONEY 83.11 76.88 79.87 77.71 80.62 79.14
PERCENT 75.68 51.85 61.54 85.48 98.15 91.38
PERSON 88.62 76.70 82.23 87.06 80.46 83.63
TIME 0.00 0.00 0.00 0.00 0.00 0.00
All 76.13 73.24 74.66 73.09 67.87 70.38
</table>
<tableCaption confidence="0.979001">
Table 4: Recall, precision and F-scores by class for Bio1 and MUC-6. Results were calculated using
</tableCaption>
<reference confidence="0.978271117647059">
10-fold cross validation on all the available data. t Results are shown for the best performing SVM
models, i.e. without POS features for Bio1 and with POS features for MUC-6.
task. In A. Kilgarriff and T. Berber Sardinha, ed-
itors, Proceedings of the Association for Computa-
tional Linguistics (ACL &apos;2000) Workshop on Com-
paring Corpora, Hong Kong, pages 20-27, October
7th.
D. Palmer and D. Day. 1997. A statistical pro-
file of the named entity task. In Proceedings of
the Fifth Conference on Applied Natural Language
Processing (A NL P 97), Washington D. C., USA.,
31 March - 3 April.
L. Rabiner and B. Juang. 1986. An introduction to
hidden Markov models. IEEE ASSP Magazine,
pages 4-16, January.
P. Tapanainen and T. Jarvinen. 1997. A non-
projective dependency parser. In Proceedings
of the 5th Conference on Applied Natural Lan-
guage Processing, Washington D.C., Association
of Computational Linguistics, pages 64-71.
Y. Tateishi, T. Ohta, N. Collier, C. Nobata,
K. Ibushi, and J. Tsujii. 2000. Building an an-
notated corpus in the molecular-biology domain.
In COLING&apos;2000 Workshop on Semantic Annota-
tion and Intelligent Content, Luxemburg, 5th-6th
August.
C. J. van Rijsbergen. 1979. Information Retrieval.
Butterworths, London.
V. N. Vapnik. 1995. The Nature of Statistical Learn-
ing Theory. Springer-Verlag, New York.
A. J. Viterbi. 1967. Error bounds for convolutions
codes and an asymptotically optimum decoding al-
gorithm IEEE Transactions on Information The-
ory, IT-13 (2): 260-269.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.805951">
<title confidence="0.998725">of Support Vector Machines in Recognition</title>
<author confidence="0.998437">Koichi Takeuchi</author>
<author confidence="0.998437">Nigel Collier</author>
<affiliation confidence="0.999847">National Institute of Informatics</affiliation>
<address confidence="0.970831">2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo, 101-8430 Japan</address>
<email confidence="0.834865">.ac.jp</email>
<abstract confidence="0.999113647058824">This paper explores the use of Support Vector Machines (SVMs) for an extended named entity task. We investigate the identification and classification of technical terms in the molecular biology domain and contrast this to results obtained for traditional NE recognition on the MUC-6 data set. Furthermore we compare the performance of the SVM model to a standard Ell\ilM bigram model. Results show that the SVM utilizing a rich feature set of a +3 context window and POS features (MUC-6 only) had a significant performance advantage on both the MUC-6 and molecular biology data sets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Miller R Schwartz</author>
<author>R Wesichedel</author>
</authors>
<title>Nymble: a high-performance learning namefinder.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth Conference on Applied Natural Language Processing (ANLP&apos;97)„ Washington D. C., USA..</booktitle>
<volume>3</volume>
<pages>194--201</pages>
<marker>Schwartz, Wesichedel, 1997</marker>
<rawString>D. Bikel. S. Miller. R. Schwartz. and R. Wesichedel. 1997. Nymble: a high-performance learning namefinder. In Proceedings of the Fifth Conference on Applied Natural Language Processing (ANLP&apos;97)„ Washington D. C., USA.. pages 194-201.31 March - 3 April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>A simple rule-based part of speech tagger.</title>
<date>1992</date>
<booktitle>In Third Conference on Applied Natural Language Processing - Association for Computational Linguistics,</booktitle>
<pages>152--155</pages>
<location>Trento,</location>
<contexts>
<context position="7653" citStr="Brill, 1992" startWordPosition="1257" endWordPosition="1258">able from http:// http://cl.aistnara.ac.jp/ taku-ku/software/ TinySVM/ Word POS Orthography Class Figure 1: Lexical features and context considered by the SVM model when deciding the class tag T at the focus position t includes surface word forms, part of speech, orthographic features and previous word class tags. training. In our implementation each training pattern is given as a vector which represents certain lexical features. All models use a surface word, an orthographic feature (Collier et al., 2000) and previous class assignments, but our experiments with part of speech (POS) features (Brill, 1992) showed that POS features actually inhibited performance in the molecular biology data set which we present below. This is probably because the POS tagger was trained on news texts. Therefore POS features are used only for the MUC-6 news data set where we show a comparison with and without these features. The form of the vector is basically a bag of words, i.e. word positions or ordering are not recorded. In the experiments we report below we use feature vectors consisting of differing amounts of &apos;context&apos; by varying the window around the focus word which is to be classified into one of the NE</context>
</contexts>
<marker>Brill, 1992</marker>
<rawString>E. Brill. 1992. A simple rule-based part of speech tagger. In Third Conference on Applied Natural Language Processing - Association for Computational Linguistics, Trento, Italy. pages 152-155. 31st March - 3rd April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging.</title>
<date>1995</date>
<journal>Computational Linguistics.</journal>
<pages>21--543</pages>
<contexts>
<context position="3477" citStr="Brill, 1995" startWordPosition="557" endWordPosition="558"> knowledge representations such as ontologies. This is an area we are currently exploring (Collier et al., 2002). Examples of NE+ classes include, a person&apos;s name, a protein name, a chemical formula or a computer product code. All of these may be valid candidates for tagging depending on whether they are contained in the ontology. NE+ can be viewed as a type of multiple classification task and there are several effective and well studied learning algorithms available for this such as Hidden Markov Models (HA/11\4s) (Rabiner and Juang, 1986) and transformationbased error-driven learning (TBL) (Brill, 1995). Recently a new learning paradigm called support vector machines (SV1&apos;v4s) (Vapnik, 1995) has been the focus of intensive research in machine learning due to its capacity to learn effectively from large feature sets. SVMs have been applied very successfully in the past to several traditional classification tasks such as text classification. Promising results have been reported for NLP tasks such as part of speech tagging and chunking, e.g. (Kudoh and Matsumoto, 2000). We have implemented and compared two learning methods (SVM, HA/11\4) and tested them on two data sets. The comparison between </context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>E. Brill. 1995. Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging. Computational Linguistics. 21:543-565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tsujii</author>
</authors>
<title>Extracting the names of genes and gene products with a hidden Markov model.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics (COLING &apos;2000,</booktitle>
<pages>4</pages>
<location>Saarbrucken, Germany,</location>
<marker>Tsujii, 2000</marker>
<rawString>N. Collier. C. Nobata,. and J. Tsujii. 2000. Extracting the names of genes and gene products with a hidden Markov model. In Proceedings of the 18th International Conference on Computational Linguistics (COLING &apos;2000, Saarbrucken, Germany, July 31st-August 4th.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Fukumoto</author>
<author>N Oga</author>
<author>ta</author>
</authors>
<title>Progress on multi-lingual named entity annotation guidelines using RDF(S).</title>
<date>2002</date>
<booktitle>In Proceedings of the Third International Conference on Language Resources and Evaluation (LREC&apos;2002),</booktitle>
<pages>2074--2081</pages>
<location>Las Palmas,</location>
<marker>Fukumoto, Oga, ta, 2002</marker>
<rawString>N. Collier. K. Takeuchi. C. Nobata,. J. Fukumoto. and N. Oga,ta,. 2002. Progress on multi-lingual named entity annotation guidelines using RDF(S). In Proceedings of the Third International Conference on Language Resources and Evaluation (LREC&apos;2002), Las Palmas, Spain. pages 2074-2081. May 27th - June 2nd.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>Y Singer</author>
</authors>
<title>Unsupervised models for named entity classification.</title>
<date>1999</date>
<booktitle>In Proceedings of the 1999 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora.</booktitle>
<contexts>
<context position="1329" citStr="Collins and Singer, 1999" startWordPosition="206" endWordPosition="209">a significant performance advantage on both the MUC-6 and molecular biology data sets. 1 Introduction Named entity (NE) extraction is now firmly established as a core technology for understanding low level semantics of texts. NE was formalized in the DARPA-sponsored Message Understanding Conference (MUC)-6 (MUC, 1995) and since then several methodologies have been widely explored: • heuristics-based, using rules written by human experts after inspecting examples (Fukuda et al., 1998); • supervised such as (Bikel et al., 1997) using labelled training examples; • non-supervised methods such as (Collins and Singer, 1999). NE&apos;s main role has been to identify expressions such as the names of people, places and organizations as well as date and time expressions. Such expressions are hard to analyze using traditional natural language processing (NLP) because they belong to the open class of expressions, i.e. there is an infinite variety and new expressions are constantly being invented. The application of NE to non-news domains requires us to consider extending NE so that it can capture types, i.e. instances of conceptual classes as well as individuals. To distinguish between traditional NE and extended NE we ref</context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>M. Collins and Y. Singer. 1999. Unsupervised models for named entity classification. In Proceedings of the 1999 Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cortes</author>
<author>V Va</author>
<author>pnik</author>
</authors>
<title>Support-vector networks.</title>
<date>1995</date>
<journal>Machine Learning.</journal>
<pages>20--273</pages>
<marker>Cortes, Va, pnik, 1995</marker>
<rawString>C. Cortes and V. Va,pnik. 1995. Support-vector networks. Machine Learning. 20:273-297. November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Tsunoda A Tamura</author>
<author>T Takagi</author>
</authors>
<title>Toward information extraction: identifying protein names from biological papers.</title>
<date>1998</date>
<booktitle>In Proceedings of the Pacific Symposium on Biocomputing&apos;98 (PSB&apos;98).</booktitle>
<pages>707--718</pages>
<marker>Tamura, Takagi, 1998</marker>
<rawString>K. Fukuda. T. Tsunoda. A. Tamura. and T. Takagi. 1998. Toward information extraction: identifying protein names from biological papers. In Proceedings of the Pacific Symposium on Biocomputing&apos;98 (PSB&apos;98). pages 707-718. January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joa</author>
<author>chims</author>
</authors>
<title>Making large-scale SVM learning practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods - Support Vector Learning.</booktitle>
<editor>In B. Scholkopf. C. Burges. and A. Smola. editors.</editor>
<publisher>MIT Press.</publisher>
<marker>Joa, chims, 1999</marker>
<rawString>T. Joa,chims. 1999. Making large-scale SVM learning practical. In B. Scholkopf. C. Burges. and A. Smola. editors. Advances in Kernel Methods - Support Vector Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kudoh</author>
<author>Y Matsumoto</author>
</authors>
<title>Use of support vector learning for chunk identification.</title>
<date>2000</date>
<booktitle>In Proceedings of the Fourth Conference on Natural Language Learning (CoNLL-2000„</booktitle>
<pages>142--144</pages>
<location>Lisbon,</location>
<contexts>
<context position="3949" citStr="Kudoh and Matsumoto, 2000" startWordPosition="631" endWordPosition="634">hms available for this such as Hidden Markov Models (HA/11\4s) (Rabiner and Juang, 1986) and transformationbased error-driven learning (TBL) (Brill, 1995). Recently a new learning paradigm called support vector machines (SV1&apos;v4s) (Vapnik, 1995) has been the focus of intensive research in machine learning due to its capacity to learn effectively from large feature sets. SVMs have been applied very successfully in the past to several traditional classification tasks such as text classification. Promising results have been reported for NLP tasks such as part of speech tagging and chunking, e.g. (Kudoh and Matsumoto, 2000). We have implemented and compared two learning methods (SVM, HA/11\4) and tested them on two data sets. The comparison between these models is informative because of the different nature of the two learning methods. In the case of the HAIM the learning approach is generative, i.e. it makes use of positive examples to build a model of NE classes and then evaluates each unseen sentence to see how well each of the words &apos;fits&apos; the model. The SVM on the other hand is a discriminative approach and makes use of both positive and negative examples to learn the distinction between the two classes. An</context>
</contexts>
<marker>Kudoh, Matsumoto, 2000</marker>
<rawString>T. Kudoh and Y. Matsumoto. 2000. Use of support vector learning for chunk identification. In Proceedings of the Fourth Conference on Natural Language Learning (CoNLL-2000„ Lisbon, Portugal. pages 142-144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>DARPA</author>
</authors>
<date>1995</date>
<booktitle>Proceedings of the Sixth Message Understanding Conference(MUC-6).</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<location>Columbia. MD. USA.</location>
<marker>DARPA, 1995</marker>
<rawString>DARPA. 1995. Proceedings of the Sixth Message Understanding Conference(MUC-6). Columbia. MD. USA. November. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tsujii</author>
</authors>
<title>Comparison between tagged corpora for the named entity Model Class SVM]-(po1y)l- HlVIM</title>
<date>2000</date>
<note>d=2 F13=1 F13=1</note>
<marker>Tsujii, 2000</marker>
<rawString>C. Nobata,. N. Collier. and J. Tsujii. 2000. Comparison between tagged corpora for the named entity Model Class SVM]-(po1y)l- HlVIM d=2 F13=1 F13=1</rawString>
</citation>
<citation valid="false">
<title>Data set Biol 10-fold cross validation on all the available data. t Results are shown for the best performing SVM models, i.e. without POS features for Bio1 and with POS features for MUC-6.</title>
<marker></marker>
<rawString>Data set Biol 10-fold cross validation on all the available data. t Results are shown for the best performing SVM models, i.e. without POS features for Bio1 and with POS features for MUC-6.</rawString>
</citation>
<citation valid="true">
<date></date>
<booktitle>Proceedings of the Association for Computational Linguistics (ACL &apos;2000) Workshop on Comparing Corpora, Hong Kong,</booktitle>
<pages>20--27</pages>
<editor>task. In A. Kilgarriff and T. Berber Sardinha, editors,</editor>
<marker></marker>
<rawString>task. In A. Kilgarriff and T. Berber Sardinha, editors, Proceedings of the Association for Computational Linguistics (ACL &apos;2000) Workshop on Comparing Corpora, Hong Kong, pages 20-27, October 7th.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Palmer</author>
<author>D Day</author>
</authors>
<title>A statistical profile of the named entity task.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth Conference on Applied Natural Language Processing (A NL P 97),</booktitle>
<volume>3</volume>
<location>Washington D. C., USA., 31</location>
<contexts>
<context position="17407" citStr="Palmer and Day, 1997" startWordPosition="3033" endWordPosition="3036">ing set was developed for the POS tagger in the NE+ domain it seems likely that the SVM would strongly benefit from this. In its current configuration SVM&apos; could be combined with the Ell\ilM on the Biol data set to achieve better performance in 5 of the 10 classes. While acknowledging the danger of drawing broad conclusions about the NE+ task from one domain-based data set, pending further analysis, we can cautiously say that performance on the two data sets has shown that the MUC-6 NE task is somewhat easier than the Biol NE+ task. Despite a few investigations into the nature of the NE task (Palmer and Day, 1997) (Nobata et al., 2000) the information theoretical aspects of the knowledge required for the task are still not well understood and this must be considered as a key area for future research. In order to test our method more accurately and develop a composite model we are now building a more Model Data set Ell\ilM SVM&apos; (poly) SVM2 degree d= degree d= 1 2 3 4 2 Biolt 70.97 71.33 71.78 68.54 65.09 65.63 MUC-6t 70.38 72.86 73.21 69.22 65.12 65.94 MUC-61 - 74.80 74.66 72.68 67.92 68.83 Table 3: Overall F-scores for each of the learning methods on the two test sets using 10-fold cross validation on </context>
</contexts>
<marker>Palmer, Day, 1997</marker>
<rawString>D. Palmer and D. Day. 1997. A statistical profile of the named entity task. In Proceedings of the Fifth Conference on Applied Natural Language Processing (A NL P 97), Washington D. C., USA., 31 March - 3 April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Rabiner</author>
<author>B Juang</author>
</authors>
<title>An introduction to hidden Markov models.</title>
<date>1986</date>
<journal>IEEE ASSP Magazine,</journal>
<pages>4--16</pages>
<contexts>
<context position="3411" citStr="Rabiner and Juang, 1986" startWordPosition="547" endWordPosition="550"> opening up the possibility of combining information extraction (IE) with deep knowledge representations such as ontologies. This is an area we are currently exploring (Collier et al., 2002). Examples of NE+ classes include, a person&apos;s name, a protein name, a chemical formula or a computer product code. All of these may be valid candidates for tagging depending on whether they are contained in the ontology. NE+ can be viewed as a type of multiple classification task and there are several effective and well studied learning algorithms available for this such as Hidden Markov Models (HA/11\4s) (Rabiner and Juang, 1986) and transformationbased error-driven learning (TBL) (Brill, 1995). Recently a new learning paradigm called support vector machines (SV1&apos;v4s) (Vapnik, 1995) has been the focus of intensive research in machine learning due to its capacity to learn effectively from large feature sets. SVMs have been applied very successfully in the past to several traditional classification tasks such as text classification. Promising results have been reported for NLP tasks such as part of speech tagging and chunking, e.g. (Kudoh and Matsumoto, 2000). We have implemented and compared two learning methods (SVM, </context>
</contexts>
<marker>Rabiner, Juang, 1986</marker>
<rawString>L. Rabiner and B. Juang. 1986. An introduction to hidden Markov models. IEEE ASSP Magazine, pages 4-16, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Tapanainen</author>
<author>T Jarvinen</author>
</authors>
<title>A nonprojective dependency parser.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th Conference on Applied Natural Language Processing, Washington D.C., Association of Computational Linguistics,</booktitle>
<pages>64--71</pages>
<contexts>
<context position="14566" citStr="Tapanainen and Jarvinen, 1997" startWordPosition="2528" endWordPosition="2531">t model that we tested and we can conclude that when trained with similar knowledge to the Ell\ilM the SVM has no particular performance advantage that we could observe. However by exploiting the SVMs capability to easily handle large feature sets including a wide context window and POS tags the results suggest that the SVM will perform at a significantly higher level than the 11MM. A detailed break down of results by class is shown in Table 4. What is not obvious from the tables is the effect we found of tokenization. In all the experiments reported for SVM in Table 3 we used the FDG parser (Tapanainen and Jarvinen, 1997) which we found gave much better results for Biol than a simple tokenization strategy that simply split each word at spaces or punctuation marks. On MUC-6 the advantage was less clear and we concluded that the frequent and ambiguous use of hyphen in Biol was the key factor. On the NE+ task in Biol we found that SVM&apos; slightly but clearly outperformed the El1\i1M. In analysis of SVM&apos; results we identified several types of error. The first and perhaps most serious type was caused by local syntactic ambiguities such as head sharing in 39-kD SH2, 8113 domain which should have been classed as a PROT</context>
</contexts>
<marker>Tapanainen, Jarvinen, 1997</marker>
<rawString>P. Tapanainen and T. Jarvinen. 1997. A nonprojective dependency parser. In Proceedings of the 5th Conference on Applied Natural Language Processing, Washington D.C., Association of Computational Linguistics, pages 64-71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Tateishi</author>
<author>T Ohta</author>
<author>N Collier</author>
<author>C Nobata</author>
<author>K Ibushi</author>
<author>J Tsujii</author>
</authors>
<title>Building an annotated corpus in the molecular-biology domain.</title>
<date>2000</date>
<booktitle>In COLING&apos;2000 Workshop on Semantic Annotation and Intelligent Content,</booktitle>
<location>Luxemburg, 5th-6th</location>
<contexts>
<context position="12132" citStr="Tateishi et al., 2000" startWordPosition="2096" endWordPosition="2099">he previous name-class, Ct_i, and previous word and feature. Once the state transition probabilities have been calculated according to Equations 1 and 2, the Viterbi algorithm (Viterbi, 1967) is used to search the state space of possible name class assignments in linear time to find the highest probability path, i.e. to maximize Pr(1/17, C). 2.3 Data Sets We used two data sets in our study one for NE+ and the other for traditional NE. The NE+ collection (Bio1) consists of 100 MEDLINE abstracts (23586 words) in the domain of molecular biology annotated for the names of genes and gene products (Tateishi et al., 2000). The second (MUC-6) is the collection of 60 executive succession texts (24617 words) used in MUC-6 for dryrun and testing. Details are shown in Tables 1 and 2. Class # Description PROTEIN 2125 proteins, protein groups,families, complexes and substructures DNA 358 DNAs, DNA groups, regions and genes RNA 30 RNAs, RNA groups, regions and genes SOURCE.c1 93 cell line SOURCE.ct 417 cell type SOURCE.mo 21 mono-organism SOURCE.mu 64 multi-celled organism SOURCE.vi 90 viruses SOURCE.s1 77 sublocation SOURCE.ti 37 tissue Table 1: Markup classes used in Bio1 with the number of word tokens for each clas</context>
</contexts>
<marker>Tateishi, Ohta, Collier, Nobata, Ibushi, Tsujii, 2000</marker>
<rawString>Y. Tateishi, T. Ohta, N. Collier, C. Nobata, K. Ibushi, and J. Tsujii. 2000. Building an annotated corpus in the molecular-biology domain. In COLING&apos;2000 Workshop on Semantic Annotation and Intelligent Content, Luxemburg, 5th-6th August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J van Rijsbergen</author>
</authors>
<title>Information Retrieval.</title>
<date>1979</date>
<publisher>Butterworths,</publisher>
<location>London.</location>
<marker>van Rijsbergen, 1979</marker>
<rawString>C. J. van Rijsbergen. 1979. Information Retrieval. Butterworths, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V N Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer-Verlag,</publisher>
<location>New York.</location>
<contexts>
<context position="3567" citStr="Vapnik, 1995" startWordPosition="570" endWordPosition="571"> (Collier et al., 2002). Examples of NE+ classes include, a person&apos;s name, a protein name, a chemical formula or a computer product code. All of these may be valid candidates for tagging depending on whether they are contained in the ontology. NE+ can be viewed as a type of multiple classification task and there are several effective and well studied learning algorithms available for this such as Hidden Markov Models (HA/11\4s) (Rabiner and Juang, 1986) and transformationbased error-driven learning (TBL) (Brill, 1995). Recently a new learning paradigm called support vector machines (SV1&apos;v4s) (Vapnik, 1995) has been the focus of intensive research in machine learning due to its capacity to learn effectively from large feature sets. SVMs have been applied very successfully in the past to several traditional classification tasks such as text classification. Promising results have been reported for NLP tasks such as part of speech tagging and chunking, e.g. (Kudoh and Matsumoto, 2000). We have implemented and compared two learning methods (SVM, HA/11\4) and tested them on two data sets. The comparison between these models is informative because of the different nature of the two learning methods. I</context>
<context position="5913" citStr="Vapnik, 1995" startWordPosition="962" endWordPosition="963">ing examples (given as binary valued feature vectors) and finds a classification function that maps them to a class. There are several points about SVM models that are worth summarizing here. The first is that SVMs are known to robustly handle large feature sets and to develop models that maximize their generalizability. This makes them an ideal model for the NE+ task. Generalizability in SVMs is based on statistical learning theory and the observation that it is useful sometimes to misclassify some of the training data so that the margin between other training points is maximized (Cortes and Vapnik, 1995). This is particularly useful for real world data sets that often contain inseparable data points. Secondly, although training is generally slow, the resulting model is usually small and runs quickly as only the support vectors need to be retained, i.e. the patterns that help define the function which separates positive from negative examples. Thirdly is that SVMs are binary classifiers and so we need to combine SVM models to obtain a multiclass classifier. Formally then we can consider the purpose of the SVM to be to estimate a classification function f : x 1+11 using training examples from x</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>V. N. Vapnik. 1995. The Nature of Statistical Learning Theory. Springer-Verlag, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J Viterbi</author>
</authors>
<title>Error bounds for convolutions codes and an asymptotically optimum decoding algorithm</title>
<date>1967</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>13</volume>
<issue>2</issue>
<pages>260--269</pages>
<contexts>
<context position="4710" citStr="Viterbi, 1967" startWordPosition="764" endWordPosition="765">ormative because of the different nature of the two learning methods. In the case of the HAIM the learning approach is generative, i.e. it makes use of positive examples to build a model of NE classes and then evaluates each unseen sentence to see how well each of the words &apos;fits&apos; the model. The SVM on the other hand is a discriminative approach and makes use of both positive and negative examples to learn the distinction between the two classes. Another major difference is that the SVM outputs a measure of distance from the classification function whereas the HAIM uses the Viterbi algorithm (Viterbi, 1967) to decode using maximum likelihood probabilities. Basically we expect the models to have quite different strengths and weaknesses and hopefully these can be complementary, allowing us eventually to combine the approaches to achieve a composite model. The two models are described further below along with their performance. 2 Method 2.1 SVM We developed our method using the Tiny SVM package from NAIST 1 which is an implementation of Vladimir Vapnik&apos;s SVM combined with an optimization algorithm (Joachims, 1999). SVMs like other inductive-learning approaches take as input a set of training exampl</context>
<context position="11701" citStr="Viterbi, 1967" startWordPosition="2020" endWordPosition="2021"> &gt;,Ct-1) Al f (Ctl &lt;,F &gt;,&lt; Wt-1, Ft—i &gt;,C_1) A2 f (Ctl &lt; Wt, Ft &gt;, &lt; Ft-1 &gt;,Ct-1) f (Ctl &lt; Ft &gt; , &lt; Ft—) &gt;,Ct-1) f (Ctrt-1) f (Ct) (2) where f (I) is calculated with maximumlikelihood estimates from counts on training data. In our current system we set the constants Ai and ai by hand and let E cri = 1.0, E Ai = 1.0, O o &gt; Ui &gt; Cr2, AO &gt; Ai &gt; A5. The current name-class Ct is conditioned on the current word and feature, the previous name-class, Ct_i, and previous word and feature. Once the state transition probabilities have been calculated according to Equations 1 and 2, the Viterbi algorithm (Viterbi, 1967) is used to search the state space of possible name class assignments in linear time to find the highest probability path, i.e. to maximize Pr(1/17, C). 2.3 Data Sets We used two data sets in our study one for NE+ and the other for traditional NE. The NE+ collection (Bio1) consists of 100 MEDLINE abstracts (23586 words) in the domain of molecular biology annotated for the names of genes and gene products (Tateishi et al., 2000). The second (MUC-6) is the collection of 60 executive succession texts (24617 words) used in MUC-6 for dryrun and testing. Details are shown in Tables 1 and 2. Class # </context>
</contexts>
<marker>Viterbi, 1967</marker>
<rawString>A. J. Viterbi. 1967. Error bounds for convolutions codes and an asymptotically optimum decoding algorithm IEEE Transactions on Information Theory, IT-13 (2): 260-269.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>