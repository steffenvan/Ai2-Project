<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.513719">
An Information Structural Approach
to Spoken Language Generation
</title>
<author confidence="0.574215">
Scott Prevost
</author>
<affiliation confidence="0.8621245">
The Media Laboratory
Massachusetts Institute of Technology
</affiliation>
<address confidence="0.93415">
20 Ames Street
</address>
<bodyText confidence="0.958969607142857">
Cambridge, Massachusetts 02139-4307 USA
prevostemedia.mit .edu
Abstract The lower tier in the information structure repre-
sentation specifies the semantic material that is in
&amp;quot;focus&amp;quot; within themes and rhemes. Material may be
in focus for a variety of reasons, such as to empha-
size its &amp;quot;new&amp;quot; status in the discourse, or to contrast
it with other salient material. Such focal distinc-
tions may affect the linguistic presentation of infor-
mation. For example, the it-cleft in (1) may mark
John as standing in contrast to some other recently
mentioned person. Similarly, in (2), the pitch accent
on red may mark the referenced car as standing in
contrast to some other car inferable from the dis-
course context.&apos;
This paper presents an architecture for the
generation of spoken monologues with con-
textually appropriate intonation. A two-
tiered information structure representation
is used in the high-level content planning
and sentence planning stages of generation
to produce efficient, coherent speech that
makes certain discourse relationships, such
as explicit contrasts, appropriately salient.
The system is able to produce appropriate
intonational patterns that cannot be gen-
erated by other systems which rely solely
on word class and given/new distinctions.
</bodyText>
<sectionHeader confidence="0.999069" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999775846153846">
While research on generating coherent written text
has flourished within the computational linguistics
and artificial intelligence communities, research on
the generation of spoken language, and particularly
intonation, has received somewhat less attention.
In this paper, we argue that commonly employed
models of text organization, such as schemata and
rhetorical structure theory (RST), do not adequately
address many of the issues involved in generating
spoken language. Such approaches fail to consider
contextually bound focal distinctions that are mani-
fest through a variety of different linguistic and par-
alinguistic devices, depending on the language.
In order to account for such distinctions of fo-
cus, we employ a two-tiered information structure
representation as a framework for maintaining lo-
cal coherence in the generation of natural language.
The higher tier, which delineates the theme, that
which links the utterance to prior utterances, and
the rheme, that which forms the core contribution of
the utterance to the discourse, is instrumental in de-
termining the high-level organization of information
within a discourse segment. Dividing semantic rep-
resentations into their thematic and rhematic parts
allows propositions to be presented in a way that
maximizes the shared material between utterances.
</bodyText>
<listItem confidence="0.9943075">
(1) It was John who spoke first.
(2) Q: Which car did Mary drive?
</listItem>
<bodyText confidence="0.904189">
A: (MARY drove)th (the RED car.)rh
L+H* LH(%) II* LIS
By appealing to the notion that the simple rise-fall
tune (II* LL%) very often accompanies the rhematic
material in an utterance and the rise-fall-rise tune of-
ten accompanies the thematic material (Steedman,
1991; Prevost and Steedman, 1994), we present a
spoken language generation architecture for produc-
ing short spoken monologues with contextually ap-
propriate intonation.
</bodyText>
<sectionHeader confidence="0.991945" genericHeader="method">
2 Information Structure
</sectionHeader>
<bodyText confidence="0.988950466666667">
Information Structure refers to the organization of
information within an utterance. In particular, it
&apos;In this example, and throughout the remainder of
the paper, the intonation contour is informally noted
by placing prosodic phrases in parentheses and marking
pitch accented words with capital letters. The tunes are
more formally annotated with a variant of (Pierrehum-
bert, 1980) notation described in (Prevost, 1995). Three
different pause lengths are associated with boundaries
in the modified notation. &apos;(%)&apos; marks intra-utterance
boundaries with very little pausing, &apos;%&apos; marks intra-
utterance boundaries associated with clauses demar-
cated by commas, and &apos;$&apos; marks utterance-final bound-
aries. For the purposes of generation and synthesis, these
distinctions are crucial.
</bodyText>
<page confidence="0.995433">
294
</page>
<figure confidence="0.994117266666667">
(3) Q: I know the AMERICAN amplifier produces MUDDY treble,
(But WHAT) (does the BRITISH amplifier produce?)
Li-Ht L(H%) II* LL$
A: (The BRITISH amplifier produces) (CLEAN treble.)
theme-focus L(H%) II* LL$
rherrze-focus
Theme Rheme
(4) Q: I know the AMERICAN amplifier produces MUDDY treble,
(But WHAT) (produces CLEAN treble?)
Ld-H* L(H%) H* LL$
A: (The BRITISH amplifier) (produces CLEAN treble.)
II* L(L%) L-FH* LH$
rherne-focus theme-focus
Rheme
Theme
</figure>
<bodyText confidence="0.999677">
defines how the information conveyed by a sentence
is related to the knowledge of the interlocutors and
the structure of their discourse. Sentences conveying
the same propositional content in different contexts
need not share the same information structure. That
is, information structure refers to how the semantic
content of an utterance is packaged, and amounts
to instructions for updating the models of the dis-
course participants. The realization of information
structure in a sentence, however, differs from lan-
guage to language. In English, for example, intona-
tion carries much of the burden of information struc-
ture, while languages with freer word order, such as
Catalan (Engdahl and Vallduvi, 1994) and Turkish
(Hoffman, 1995) convey information structure syn-
tactically.
</bodyText>
<subsectionHeader confidence="0.999486">
2.1 Information Structure and Intonation
</subsectionHeader>
<bodyText confidence="0.992016704545454">
The relationship between intonational structure and
information structure is illustrated by (3) and (4). In
each of these examples, the answer contains the same
string words but different intonational patterns and
information structural representations. The theme
of each utterance is considered to be represented by
the material repeated from the question. That is,
the theme of the answer is what links it to the ques-
tion and defines what the utterance is about. The
rheme of each utterance is considered to be repre-
sented by the material that is new or forms the core
contribution of the utterance to the discourse. By
mapping the rise-fall tune (II* LL%) onto rhemes
and the rise-fall-rise tune (L-FH* LH%) onto themes
(Steedman, 1991; Prevost and Steedman, 1994), we
can easily identify the string of words over which
these two prominent tunes occur directly from the
information structure. While this mapping is cer-
tainly overly simplistic, the results presented in Sec-
tion 4.3 demonstrate its appropriateness for the class
of simple declarative sentences under investigation.
Knowing the strings of words to which these two
tunes are to be assigned, however, does not pro-
vide enough information to determine the location of
the pitch accents (II* and L+H*) within the tunes.
Moreover, the simple mapping described above does
not account for the frequently occurring cases in
which thematic material bears no pitch accents and
is consequently unmarked intonationally. Previous
approaches to the problem of determining where
to place accents have utilized heuristics based on
&amp;quot;givenness.&amp;quot; That is, content-bearing words (e.g.
nouns and verbs) which had not been previously
mentioned (or whose roots had not been previ-
ously mentioned) were assigned accents, while func-
tion words were de-accented (Davis and Hirschberg,
1988; Hirschberg, 1990). While these heuristics ac-
count for a broad range of intonational possibilities,
they fail to account for accentual patterns that serve
to contrast entities or propositions that were previ-
ously &amp;quot;given&amp;quot; in the discourse. Consider, for ex-
ample the intonational pattern in (5), in which the
pitch accent on amplifier in the response cannot be
attributed to its being &amp;quot;new&amp;quot; to the discourse.
</bodyText>
<listItem confidence="0.634173">
(5) Q: Do critics prefer the BRITISH amplifier
L*
or the AMERICAN amplifier?
</listItem>
<sectionHeader confidence="0.790242" genericHeader="method">
II* LL$
</sectionHeader>
<bodyText confidence="0.96883">
A: They prefer the AMERICAN amplifier.
</bodyText>
<equation confidence="0.395906">
H* LL$
</equation>
<bodyText confidence="0.998941571428571">
For the determination of pitch accent placement,
we rely on a secondary tier of information structure
which identifies focused properties within themes
and rhemes. The theme-foci and the rheme-foci
mark the information that differentiates properties
or entities in the current utterance from properties
or entities established in prior utterances. Conse-
</bodyText>
<page confidence="0.996796">
295
</page>
<bodyText confidence="0.963719266666667">
quently, the semantic material bearing &amp;quot;new&amp;quot; infor-
mation is considered to be in focus. Furthermore,
the focus may include semantic material that serves
to contrast an entity or proposition from alterna-
tive entities or propositions already established in
the discourse. While the types of pitch accents (II*
or L-FH*) are determined by the theme/rheme delin-
eation and the aforementioned mapping onto tunes,
the locations of pitch accents are determined by the
assignment of foci within the theme and rheme, as
illustrated in (3) and (4). Note that it is in pre-
cisely those cases where thematic material, which is
&amp;quot;given&amp;quot; by default, does not contrast with any other
previously established properties or entities that this
material is intonationally unmarked, as in (6).
</bodyText>
<listItem confidence="0.79665875">
(6) Q: Which amplifier does Scott PREFER?
II* LL$
A: (He prefers)th (the BRITISH amplifier.)rh
H* LL$
</listItem>
<subsectionHeader confidence="0.999298">
2.2 Contrastive Focus Algorithm
</subsectionHeader>
<bodyText confidence="0.975845512195122">
The determination of contrastive focus, and con-
sequently the determination of pitch accent loca-
tions, is based on the premise that each object in
the knowledge base is associated with a set of alter-
natives from which it must be distinguished if refer-
ence is to succeed. The set of alternatives is deter-
mined by the hierarchical structure of the knowledge
base. For the present implementation, only proper-
ties with the same parent or grandparent class are
considered to be alternatives to one another.
Given an entity x and a referring expression for x,
the contrastive focus feature for its semantic repre-
sentation is computed on the basis of the contrastive
focus algorithm described in (7), (8) and (9). The
data structures and notational conventions are given
below.
(7) DElist: a collection of discourse entities
that have been evoked in prior dis-
course, ordered by recency. The list
may be limited to some size k so that
only the k most recent discourse enti-
ties pushed onto the list are retrievable.
ASet(x): the set of alternatives for object
x, i.e. those objects that belong to
the same class as x, as defined in the
knowledge base.
RSet(x, S): the set of alternatives for ob-
ject x as restricted by the referring ex-
pressions in DElist and the set of prop-
erties S.
CSet(x, S): the subset of properties of S to
be accented for contrastive purposes.
Props(x): a list of properties for object x,
ordered by the grammar so that nomi-
nal properties take precedence over ad-
jectival properties.
The algorithm, which assigns contrastive focus
in both thematic and rhematic constituents, begins
by isolating the discourse entities in the given con-
stituent. For each such entity x, the structures de-
fined above are initialized as follows:
</bodyText>
<equation confidence="0.9015568">
(8) Props(x) := [P I P(x) is true in KB]
ASet(x) := {y I alt(x,y)}, x&apos;s alternatives
RSet(x,{}) := {x} U y E ASet(x) &amp; y E
DElist}, evoked alternatives
CSet(x,{}) := {}
The algorithm appears in pseudo-code in (9).2
(9) S:={}
for each P in Props(x)
RSet(x, S U {P})
fy I y E RSet(x, S) &amp; P(Y)}
</equation>
<construct confidence="0.749508">
if RSet(x, S U {P}) RSet(x, S) then
</construct>
<figure confidence="0.913111">
% no restrictions were made
% based on property P.
CSet(x, S U {P}) := CSet(x, S)
else
% property P eliminated some
% members of the RSet.
CSet(x, S U {P}) := CSet(x, S)U {P}
endif
S:=SU{P}
endfor
</figure>
<bodyText confidence="0.99977796">
In other words, given an object x, a list of its prop-
erties and a set of alternatives, the set of alternatives
is restricted by including in the initial RSet only x
and those objects that are explicitly referenced in the
prior discourse. Initially, the set of properties to be
contrasted (CSet) is empty. Then, for each property
of x in turn, the RSet is restricted to include only
those objects satisfying the given property in the
knowledge base. If imposing this restriction on the
RSet for a given property decreases the cardinality
of the RSet, then the property serves to distinguish
x from other salient alternatives evoked in the prior
discourse, and is therefore added to the contrast set.
Conversely, if imposing the restriction on the RSet
for a given property does not change the RSei, the
property is not necessary for distinguishing x from
its alternatives, and is not added to the CSei.
Based on this contrastive focus algorithm and the
mapping between information structure and into-
nation described above, we can view information
structure as the representational bridge between dis-
course and intonational variability. The following
sections elucidate how such a formalism can be in-
tegrated into the computational task of generating
spoken language.
</bodyText>
<footnote confidence="0.8182285">
2An in-depth discussion of the algorithm and numer-
ous examples are presented in (Prevost, 1995).
</footnote>
<page confidence="0.997366">
296
</page>
<sectionHeader confidence="0.971695" genericHeader="method">
3 Generation Architecture
</sectionHeader>
<bodyText confidence="0.9999814">
The task of natural language generation (NLG) has
often been divided into three stages: content plan-
ning, in which high-level goals are satisfied and dis-
course structure is determined, sentence planning, in
which high-level abstract semantic representations
are mapped onto representations that more fully
constrain the possible sentential realizations (Ram-
bow and Korelsky, 1992; Reiter and Mellish, 1992;
Meteer, 1991), and surface generation, in which the
high-level propositions are converted into sentences.
The selection and organization of propositions
and their divisions into theme and rheme are de-
termined by the content planner, which maintains
discourse coherence by stipulating that semantic in-
formation must be shared between consecutive utter-
ances whenever possible. That is, the content plan-
ner ensures that the theme of an utterance links it
to material in prior utterances.
The process of determining foci within themes and
rhemes can be divided into two tasks: determining
which discourse entities or propositions are in fo-
cus, and determining how their linguistic realizations
should be marked to convey that focus. The first
of these tasks can be handled in the content phase
of the NLG model described above. The second of
these tasks, however, relies on information, such as
the construction of referring expressions, that is of-
ten considered the domain of the sentence planning
stage. For example, although two discourse entities
el and e2 can be determined to stand in contrast
to one another by appealing only to the discourse
model and the salient pool of knowledge, the method
of contrastively distinguishing between them by the
placement of pitch accents cannot be resolved until
the choice of referring expressions has been made.
Since referring expressions are generally taken to be
in the domain of the sentence planner (Dale and
Haddock, 1991), the present approach resolves is-
sues of contrastive focus assignment at the sentence
processing stage as well.
During the content generation phase, the content
of the utterance is planned based on the previous
discourse. While template-based systems (McKe-
own, 1985) have been widely used, rhetorical struc-
ture theory (RST) approaches (Mann and Thomp-
son, 1986; Hovy, 1993), which organize texts by
identifying rhetorical relations between clause-level
propositions from a knowledge base, have recently
flourished. Sibun (Sibun, 1991) offers yet another
alternative in which propositions are linked to one
another not by rhetorical relations or pre-planned
templates, but rather by physical and spatial prop-
erties represented in the knowledge-base.
The present framework for organizing the con-
tent of a monologue is a hybrid of the template
and RST approaches. The implementation, which
is presented in the following section, produces de-
scriptions of objects from a knowledge base with
context-appropriate intonation that makes proper
distinctions of contrast between alternative, salient
discourse entities. Certain constraints, such as the
requirement that objects be identified or defined at
the beginning of a description, are reminiscent of
McKeown&apos;s schemata. Rather than imposing strict
rules on the order in which information is presented,
the order is determined by domain specific knowl-
edge, the communicative intentions of the speaker,
and beliefs about the hearer&apos;s knowledge. Finally,
the system includes a set of rhetorical constraints
that may rearrange the order of presentation for in-
formation in order to make certain rhetorical rela-
tionships salient. While this approach has proven
effective in the present implementation, further re-
search is required to determine its usefulness for a
broader range of discourse types.
</bodyText>
<sectionHeader confidence="0.984374" genericHeader="method">
4 The Prolog Implementation
</sectionHeader>
<bodyText confidence="0.999986142857143">
The monologue generation program produces text
and contextually-appropriate intonation contours to
describe an object from the knowledge base. The
system exhibits the ability to intonationally contrast
alternative entities and properties that have been
explicitly evoked in the discourse even when they
occur with several intervening sentences.
</bodyText>
<subsectionHeader confidence="0.9953">
4.1 Content Generation
</subsectionHeader>
<bodyText confidence="0.999517875">
The architecture for the monologue generation pro-
gram is shown in Figure 1, in which arrows repre-
sent the computational flow and lines represent de-
pendencies among modules. The remainder of this
section contains a description of the computational
path through the system with respect to a single
example. The input to the program is a goal to de-
scribe an object from the knowledge base, which in
this case contains a variety of facts about hypothet-
ical stereo components. In addition, the input pro-
vides a communicative intention for the goal which
may affect its ultimate realization, as shown in (10).
For example, given the goal describe(x), the in-
tention persuade-to-buy(hearer,x) may result in
a radically different monologue than the intention
persuade-to-sell(hearer,x).
</bodyText>
<listItem confidence="0.498343">
(10) Goal: describe el
Input: generate(intention(bel(hi,
good-to-buy(e1)))
</listItem>
<bodyText confidence="0.999461555555555">
Information from the knowledge base is selected to
be included in the output by a set of relations that
determines the degree to which knowledge base facts
and rules support the communicative intention of
the speaker. For example, suppose the system &amp;quot;be-
lieves&amp;quot; that conveying the proposition in (11) mod-
erately supports the intention of making hearer hl
want to buy el, and further that the rule in (12) is
known by hl.
</bodyText>
<page confidence="0.967778">
297
</page>
<figure confidence="0.98399775">
Communicative Goals and Intentions
Prosodic* Annotated Monologue
Speech Synthesizer
Spoken Output
</figure>
<figureCaption confidence="0.934496">
Figure 1: An Architecture for Monologue Genera-
tion
</figureCaption>
<listItem confidence="0.853169666666667">
(11) bel(h1, holds(rating(X, powerful)))
(12) holds(rating(X, powerful)) :-
holds(produce(X, Y)),
holds(isa(Y, watts-per-channel)),
holds(amount(Y, Z)),
number(Z),
</listItem>
<equation confidence="0.88521">
Z &gt;= 100.
</equation>
<bodyText confidence="0.9999696">
The program then consults the facts in the knowl-
edge base, verifies that the property does indeed hold
and consequently includes the corresponding facts in
the set of properties to be conveyed to the hearer,
as shown in (13).
</bodyText>
<equation confidence="0.991878666666667">
(13) holds(produce(e1, e7)).
holds(isa(e7, watts-per-channel)).
holds(amount(e7, 100) ) .
</equation>
<bodyText confidence="0.999991636363636">
The content generator starts with a simple de-
scription template that specifies that an object is to
be explicitly identified or defined before other propo-
sitions concerning it are put forth. Other relevant
propositions concerning the object in question are
then linearly organized according to beliefs about
how well they contribute to the overall intention. Fi-
nally, a small set of rhetorical predicates rearranges
the linear ordering of propositions so that sets of
sentences that stand in some interesting rhetorical
relationship to one another will be realized together
in the output. These rhetorical predicates employ
information structure to assist in maintaining the
coherence of the output. For example, the conjunc-
tion predicate specifies that propositions sharing the
same theme or rheme be realized together in order
to avoid excessive topic shifting. The contrast pred-
icate specifies that pairs of themes or rhemes that
explicitly contrast with one another be realized to-
gether. The result is a set of properties roughly or-
dered by the degree to which they support the given
intention, as shown in (14).
</bodyText>
<equation confidence="0.9876865">
(14) holds(defn(isa(e1,amplifier)))
holds(design(e1,solid-state),pres)
holds(cost(e1,e9),pres)
holds(produce(el,e7),pres)
holds(contrast(praise(e4,e1),
revile(e5,e1)),past)
</equation>
<bodyText confidence="0.999983379310345">
The top-level propositions shown in (14) were se-
lected by the program because the hearer (hi) is
believed to be interested in the design of the am-
plifier and the reviews the amplifier has received.
Moreover, the belief that the hearer is interested in
buying an expensive, powerful amplifier justifies in-
cluding information about its cost and power rat-
ing. Different sets of propositions would be gener-
ated for other (perhaps thriftier) hearers. Addition-
ally, note that the propositions praise (e4, el) and
revile(e5,e1) are combined into the larger propo-
sition contrast (praise ( e4 , el) ,revile(e5 , el ) ).
This is accomplished by the rhetorical constraints
that determine the two propositions to be con-
trastive because e4 and eS belong to the same set
of alternative entities in the knowledge base and
praise and revile belong to the same set of al-
ternative propositions in the knowledge base.
The next phase of content generation recognizes
the dependency relationships between the proper-
ties to be conveyed based on shared discourse enti-
ties. This phase, which represents an extension of
the rhetorical constraints, arranges propositions to
ensure that consecutive utterances share semantic
material (cf. (McKeown et al., 1994)). This rule,
which in effect imposes a strong bias for Centering
Theory&apos;s continue and retain transitions (Grosz et
al., 1986) determines the theme-rheme segmentation
for each proposition.
</bodyText>
<subsectionHeader confidence="0.995026">
4.2 Sentence Planning
</subsectionHeader>
<bodyText confidence="0.999986277777778">
After the coherence constraints from the previous
section are applied, the sentence planner is respon-
sible for making decisions concerning the form in
which propositions are realized. This is accom-
plished by the following simple set of rules. First,
Definitional isa properties are realized by the ma-
trix verb. Other isa properties are realized by nouns
or noun phrases. Top-level properties (such as those
in (14)) are realized by the matrix verb. Finally,
embedded properties (those evoked for building re-
ferring expressions for discourse entities) are realized
by adjectival modifiers if possible and otherwise by
relative clauses.
While there are certainly a number of linguis-
tically interesting aspects to the sentence planner,
the most important aspect for the present purposes
is the determination of theme-foci and rheme-foci.
The focus assignment algorithm employed by the
</bodyText>
<page confidence="0.994614">
298
</page>
<bodyText confidence="0.999936933333333">
sentence planner, which has access to both the dis-
course model and the knowledge base, works as fol-
lows. First, each property or discourse entity in the
semantic and information structural representations
is marked as either previously mentioned or new to
the discourse. This assignment is made with re-
spect to two data structures, the discourse entity
list (DEList), which tracks the succession of entities
through the discourse, and a similar structure for
evoked properties. Certain aspects of the semantic
form are considered unaccentable because they cor-
respond to the interpretations of closed-class items
such as function words. Items that are assigned fo-
cus based on their &amp;quot;newness&amp;quot; are assigned the o focus
operator, as shown in (15).
</bodyText>
<table confidence="0.6257998">
(15) Semantics: defn(isa(oel, oc1))
Theme: oe 1
Rheme: Ax .isa(x, ocl)
Supporting Props: isa(cl, °amplifier)
o design(cl, osolidstate)
</table>
<bodyText confidence="0.99989840625">
The second step in the focus assignment algorithm
checks for the presence of contrasting propositions
in the ISStore, a structure that stores a history of
information structure representations. Propositions
are considered contrastive if they contain two con-
trasting pairs of discourse entities, or if they contain
one contrasting pair of discourse entities as well as
contrasting functors.
Discourse entities are determined to be contrastive
if they belong to the same set of alternatives in the
knowledge base, where such sets are inferred from
the isa-links that define class hierarchies. While the
present implementation only considers entities with
the same parent or grandparent class to be alterna-
tives for the purposes of contrastive stress, a gradu-
ated approach that entails degrees of contrastiveness
may also be possible.
The effects of the focus assignment algorithm are
easily shown by examining the generation of an ut-
terance that contrasts with the utterance shown
in (15). That is, suppose the generation program
has finished generating the output corresponding to
the examples in (10) through (15) and is assigned
the new goal of describing entity e2, a different am-
plifier. After applying the second step on the focus
assignment algorithm, contrasting discourse entities
are marked with the • contrastive focus operator, as
shown in (16). Since el and e2 are both instances of
the class amplifiers and cl and c2 both describe
the class amplifiers itself, these two pairs of dis-
course entities are considered to stand in contrastive
relationships.
</bodyText>
<equation confidence="0.4808412">
(16) Semantics: defn(isa(*e2, *c2))
Theme: .e2
Rheme: As .isa(x , *c2)
Supporting Props: class(c2, amplifier)
design (c2, otube)
</equation>
<bodyText confidence="0.9999165">
While the previous step of the algorithm deter-
mined which abstract discourse entities and proper-
ties stand in contrast, the third step uses the con-
trastive focus algorithm described in Section 2 to
determine which elements need to be contrastively
focused for reference to succeed. This algorithm de-
termines the minimal set of properties of an entity
that must be &amp;quot;focused&amp;quot; in order to distinguish it
from other salient entities. For example, although
the representation in (16) specifies that e2 stands
in contrast to some other entity, it is the property
of e2 having a tube design rather than a solid-state
design that needs to be conveyed to the hearer. Af-
ter applying the third step of the focus assignment
to (16), the result appears as shown in (17), with
&amp;quot;tube&amp;quot; contrastively focused as desired.
</bodyText>
<equation confidence="0.9717928">
(17) Semantics: defn(isa(*e2, oc2))
Theme: oe2
Rheme: Ax isa(x , oc2)
Supporting Props: isa(c2, amplifier)
design(c2, *tube)
</equation>
<bodyText confidence="0.999405">
The final step in the sentence planning phase of
generation is to compute a representation that can
serve as input to a surface form generator based on
Combinatory Categorial Grammar (CCG) (Steed-
man, 1991), as shown in (18).3
</bodyText>
<equation confidence="0.811733333333333">
(18) Theme: np(3, s) :
(el&amp;quot; Sr def (el, ex5(e1)&amp;S)Ouirh
Rheme: s:
(act&amp;quot; pres)&amp;quot; indef (el, (arriplifier(c1)8z
•tube(c1))&amp;isa(e1, cl))\np(3, s) : elOrh
4.3 Results
</equation>
<bodyText confidence="0.935228523809524">
Given the focus-marked output of the sentence
planner, the surface generation module consults a
CCG grammar which encodes the information struc-
ture/intonation mapping and dictates the genera-
tion of both the syntactic and prosodic constituents.
The result is a string of words and the appropriate
prosodic annotations, as shown in (19). The output
of this module is easily translated into a form suit-
able for a speech synthesizer, which produces spoken
output with the desired intonation.4
(19) The X5 is a TUBE amplifier.
L(H%) H*, LL$
The modules described above and shown in Fig-
ure 1 are implemented in Quintus Prolog. The sys-
tem produces the types of output shown in (20) and
3A complete description of the CCG generator can
be found in (Prevost and Steedman, 1993). CCG was
chosen as the grammatical formalism because it licenses
non-traditional syntactic constituents that are congruent
with the bracketings imposed by information structure
and intonational phrasing, as illustrated in (3).
</bodyText>
<footnote confidence="0.990959666666667">
4The system currently uses the AT&amp;T Bell Laborato-
ries TTS system, but the implementation is easily adapt-
able to other synthesizers.
</footnote>
<page confidence="0.997813">
299
</page>
<bodyText confidence="0.99904125">
(21), which should be interpreted as a single (two
paragraph) monologue satisfying a goal to describe
two different objects.5 Note that both paragraphs
include very similar types of information, but radi-
cally different intonational contours, due to the dis-
course context. In fact, if the intonational patterns
of the two examples are interchanged, the resulting
speech sounds highly unnatural.
</bodyText>
<table confidence="0.952768103448276">
(20) a. Describe the x4.
b. The X4
L-FH* L(H%)
is a SOLID-state AMPLIFIER.
H* II* LL$
It COSTS EIGHT HUNDRED DOLLARS,
H* H* II* H* LL%
and PRODUCES
H*
ONE hundred watts-per-CHANNEL.
H* H* LL$
It was PRAISED by STEREOFOOL,
!H: LH%
an AUDIO JOURNAL,
H* H* LH%
but was REVILED by AUDIOFAD,
H: !H: LH%
ANOTHER audio journal.
H* LL$
(21) a. Describe the x5.
b. The X5 is a TUBE amplifier.
L(H%) H: LL$
IT costs NINE hundred dollars,
L-FH: L(H%) H: LH%
produces TWO hundred watts-per-channel.
H: LH%
and was praised
by Stereofool AND Audiofad.
H: LL$
</table>
<bodyText confidence="0.996271384615385">
Several aspects of the output shown above are
worth noting. Initially, the program assumes that
the hearer has no specific knowledge of any partic-
ular objects in the knowledge base. Note however,
that every proposition put forth by the generator
is assumed to be incorporated into the hearer&apos;s set
of beliefs. Consequently, the descriptive phrase &amp;quot;an
audio journal,&amp;quot; which is new information in the first
paragraph, is omitted from the second. Additionally,
when presenting the proposition Audiofad is an au-
dio journal,&apos; the generator is able to recognize the
similarity with the corresponding proposition about
Stereofool (i.e. both propositions are abstractions
over the single variable open proposition &apos;X is an au-
dio journal&apos;). The program therefore interjects the
other property and produces &amp;quot;another audio jour-
nal.&amp;quot;
&apos;The implementation assigns slightly higher pitch to
accents bearing the subscript c (e.g. H:), which mark
contrastive focus as determined by the algorithm de-
scribe above and in (Prevost, 1995).
Several aspects of the contrastive intonational ef-
fects in these examples also deserve attention. Be-
cause of the content generator&apos;s use of the rhetorical
contrast predicate, items are eligible to receive stress
in order to convey contrast before the contrast-
ing items are even mentioned. This phenomenon
is clearly illustrated by the clause &amp;quot;PRAISED by
STEREOFOOL&amp;quot; in (20), which is contrastively
stressed before &amp;quot;REVILED by AUDIOFAD&amp;quot; is ut-
tered. Such situations are produced only when the
contrasting propositions are gathered by the content
planner in a single invocation of the generator and
identified as contrastive when the rhetorical predi-
cates are applied. Moreover, unlike systems that rely
solely on word class and given/new distinctions for
determining accentual patterns, the system is able
to produce contrastive accents on pronouns despite
their &amp;quot;given&amp;quot; status, as shown in (21).
</bodyText>
<sectionHeader confidence="0.999717" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999876083333333">
The generation architecture described above and im-
plemented in Quintus Prolog produces paragraph-
length, spoken monologues concerning objects in a
simple knowledge base. The architecture relies on
a mapping between a two-tiered information struc-
ture representation and intonational tunes to pro-
duce speech that makes appropriate contrastive dis-
tinctions prosodically. The process of natural lan-
guage generation, in accordance with much of the re-
cent literature in the field, is divided into three pro-
cesses: high-level content planning, sentence plan-
ning, and surface generation. Two points concern-
ing the role of intonation in the generation process
are emphasized. First, since intonational phrasing is
dependent on the division of utterances into theme
and rheme, and since this division relates consecu-
tive sentences to one another, matters of information
structure (and hence intonational phrasing) must
be largely resolved during the high-level planning
phase. Second, since accentual decisions are made
with respect to the particular linguistic realizations
of discourse properties and entities (e.g. the choice
of referring expressions), these matters cannot be
fully resolved until the sentence planning phase.
</bodyText>
<sectionHeader confidence="0.999514" genericHeader="acknowledgments">
6 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999797454545455">
The author is grateful for the advice and help-
ful suggestions of Mark Steedman, Justine Cassell,
Kathy McKeown, Aravind Joshi, Ellen Prince, Mark
Liberman, Matthew Stone, Beryl Hoffman and Kris
ThOrisson as well as the anonymous ACL review-
ers. Without the AT&amp;T Bell Laboratories TTS sys-
tem, and the patient advice on its use from Julia
Hirschberg and Richard Sproat, this work would not
have been possible. This research was funded by
NSF grants IR191-17110 and IR195-04372 and the
generous sponsors of the MIT Media Laboratory.
</bodyText>
<page confidence="0.996009">
300
</page>
<sectionHeader confidence="0.98889" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.950587723404255">
Bolinger, D. (1989). Intonation and Its Uses. Stan-
ford University Press.
Culicover, P. and Rochemont, M. (1983). Stress and
focus in English. Language, 59:123-165.
Dale, R. and Haddock, N. (1991). Content determi-
nation in the generation of referring expressions.
Computational Intelligence, 7(4):252-265.
Davis, J. and Hirschberg, J. (1988). Assigning into-
national features in synthesized spoken discourse.
In Proceedings of the 26th Annual Meeting of the
Association for Computational Linguistics, pages
187-193, Buffalo.
Engdahl, E. and Vallduvi, E.
(1994). Information packaging and grammar ar-
chitecture: A constraint-based approach. In Eng-
dahl, E., editor, Integrating Information Structure
into Constraint-Based and Categorial Approaches
(DYANA-2 Report R.1.3.B). CLLI, Amsterdam.
Grosz, B. J., Joshi, A. K., and Weinstein, S. (1986).
Towards a computational theory of discourse in-
terpretation. Unpublished manuscript.
Gussenhoven, C. (1983b). On the Grammar and Se-
mantics of Sentence Accent. Foris, Dodrecht.
Halliday, M. (1970). Language structure and lan-
guage function. In Lyons, J., editor, New Horizons
in Linguistics, pages 140-165. Penguin.
Hirschberg, J. (1990). Accent and discourse context:
Assigning pitch accent in synthetic speech. In Pro-
ceedings of the Eighth National Conference on Ar-
tificial Intelligence, pages 952-957.
Hoffman, B. (1995). The Computational Analysis of
the Syntax and Interpretation of &apos;Free&apos; Word Or-
der in Turkish. PhD thesis, University of Pennsyl-
vania, Philadelphia.
Hovy, E. (1993). Automated discourse generation us-
ing discourse structure relations. Artificial Intelli-
gence, 63:341-385.
Mann, W. and Thompson, S. (1986). Rhetorical
structure theory: Description and construction of
text structures. In Kempen, G., editor, Natural
Language Generation: New Results in Artificial
Intelligence, Psychology and Linguistics, pages
279-300. Kluwer Academic Publishers, Boston.
McKeown, K., Kukich, K., and Shaw, J. (1994).
Practical issues in automatic documentation gen-
eration. In Proceedings of the Fourth ACL Con-
ference on Applied Natural Language Processing,
pages 7-14, Stuttgart. Association for Computa-
tional Linguistics.
McKeown, K. R. (1985). Text Generation: Us-
ing Discourse Strategies and Focus Constraints to
Generate Natural Language Text. Cambridge Uni-
versity Press, Cambridge.
Meteer, M. (1991). Bridging the generation gap
between text planning and linguistic realization.
Computational Intelligence, 7(4):296-304.
Pierrehumbert, J. (1980). The Phonology and Pho-
netics of English Intonation. PhD thesis, Mas-
sachusetts Institute of Technology. Distributed by
Indiana University Linguistics Club, Blooming-
ton, IN.
Prevost, S. (1995). A Semantics of Contrast and In-
formation Structure for Specifying Intonation in
Spoken Language Generation. PhD Thesis, Uni-
versity of Pennsylvania.
Prevost, S. and Steedman, M. (1993). Generating
contextually appropriate intonation. In Proceed-
ings of the 6th Conference of the European Chap-
ter of the Association for Computational Linguis-
tics, pages 332-340, Utrecht.
Prevost, S. and Steedman, M. (1994). Specifying in-
tonation from context for speech synthesis. Speech
Communication, 15:139-153.
Rambow, 0. and Korelsky, T. (1992). Applied text
generation. In Proceedings of the Third Conference
on Applied Natural Language Processing (ANLP-
1992), pages 40-47.
Reiter, E. and Mellish, C. (1992). Using classifica-
tion to generate text. In Proceedings of the 30th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 265-272.
Robin, J. (1993). A revision-based generation ar-
chitecture for reporting facts in their historical
context. In Horacek, H. and Zock, M., editors,
New Concepts in Natural Language Generation:
Planning, Realization and Systems, pages 238-
265. Pinter Publishers, New York.
Rochemont, M. (1986). Focus in Generative Gram-
mar. John Benjamins, Philadelphia.
Sibun, P. (1991). The Local Organization and Incre-
mental Generation of Text. PhD thesis, University
of Massachusetts.
Steedman, M. (1991a). Structure and intonation.
Language, pages 260-296.
</reference>
<page confidence="0.998894">
301
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.570568">
<title confidence="0.987311">An Information Structural Approach to Spoken Language Generation</title>
<author confidence="0.953539">Prevost</author>
<affiliation confidence="0.999801">The Media Laboratory Massachusetts Institute of Technology</affiliation>
<address confidence="0.9999065">20 Ames Street Cambridge, Massachusetts 02139-4307 USA</address>
<abstract confidence="0.985292222222222">prevostemedia.mit .edu lower tier in the information structure representation specifies the semantic material that is in &amp;quot;focus&amp;quot; within themes and rhemes. Material may be in focus for a variety of reasons, such as to emphasize its &amp;quot;new&amp;quot; status in the discourse, or to contrast it with other salient material. Such focal distinctions may affect the linguistic presentation of information. For example, the it-cleft in (1) may mark standing in contrast to some other recently mentioned person. Similarly, in (2), the pitch accent on red may mark the referenced car as standing in contrast to some other car inferable from the discourse context.&apos; This paper presents an architecture for the generation of spoken monologues with contextually appropriate intonation. A twostructure is used in the high-level content planning and sentence planning stages of generation to produce efficient, coherent speech that makes certain discourse relationships, such as explicit contrasts, appropriately salient. The system is able to produce appropriate intonational patterns that cannot be generated by other systems which rely solely on word class and given/new distinctions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Bolinger</author>
</authors>
<title>Intonation and Its Uses.</title>
<date>1989</date>
<publisher>Stanford University Press.</publisher>
<marker>Bolinger, 1989</marker>
<rawString>Bolinger, D. (1989). Intonation and Its Uses. Stanford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Culicover</author>
<author>M Rochemont</author>
</authors>
<date>1983</date>
<booktitle>Stress and focus in English. Language,</booktitle>
<pages>59--123</pages>
<marker>Culicover, Rochemont, 1983</marker>
<rawString>Culicover, P. and Rochemont, M. (1983). Stress and focus in English. Language, 59:123-165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dale</author>
<author>N Haddock</author>
</authors>
<title>Content determination in the generation of referring expressions.</title>
<date>1991</date>
<journal>Computational Intelligence,</journal>
<pages>7--4</pages>
<contexts>
<context position="14481" citStr="Dale and Haddock, 1991" startWordPosition="2275" endWordPosition="2278">sks, however, relies on information, such as the construction of referring expressions, that is often considered the domain of the sentence planning stage. For example, although two discourse entities el and e2 can be determined to stand in contrast to one another by appealing only to the discourse model and the salient pool of knowledge, the method of contrastively distinguishing between them by the placement of pitch accents cannot be resolved until the choice of referring expressions has been made. Since referring expressions are generally taken to be in the domain of the sentence planner (Dale and Haddock, 1991), the present approach resolves issues of contrastive focus assignment at the sentence processing stage as well. During the content generation phase, the content of the utterance is planned based on the previous discourse. While template-based systems (McKeown, 1985) have been widely used, rhetorical structure theory (RST) approaches (Mann and Thompson, 1986; Hovy, 1993), which organize texts by identifying rhetorical relations between clause-level propositions from a knowledge base, have recently flourished. Sibun (Sibun, 1991) offers yet another alternative in which propositions are linked t</context>
</contexts>
<marker>Dale, Haddock, 1991</marker>
<rawString>Dale, R. and Haddock, N. (1991). Content determination in the generation of referring expressions. Computational Intelligence, 7(4):252-265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Davis</author>
<author>J Hirschberg</author>
</authors>
<title>Assigning intonational features in synthesized spoken discourse.</title>
<date>1988</date>
<booktitle>In Proceedings of the 26th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>187--193</pages>
<location>Buffalo.</location>
<contexts>
<context position="7151" citStr="Davis and Hirschberg, 1988" startWordPosition="1074" endWordPosition="1077"> to determine the location of the pitch accents (II* and L+H*) within the tunes. Moreover, the simple mapping described above does not account for the frequently occurring cases in which thematic material bears no pitch accents and is consequently unmarked intonationally. Previous approaches to the problem of determining where to place accents have utilized heuristics based on &amp;quot;givenness.&amp;quot; That is, content-bearing words (e.g. nouns and verbs) which had not been previously mentioned (or whose roots had not been previously mentioned) were assigned accents, while function words were de-accented (Davis and Hirschberg, 1988; Hirschberg, 1990). While these heuristics account for a broad range of intonational possibilities, they fail to account for accentual patterns that serve to contrast entities or propositions that were previously &amp;quot;given&amp;quot; in the discourse. Consider, for example the intonational pattern in (5), in which the pitch accent on amplifier in the response cannot be attributed to its being &amp;quot;new&amp;quot; to the discourse. (5) Q: Do critics prefer the BRITISH amplifier L* or the AMERICAN amplifier? II* LL$ A: They prefer the AMERICAN amplifier. H* LL$ For the determination of pitch accent placement, we rely on a</context>
</contexts>
<marker>Davis, Hirschberg, 1988</marker>
<rawString>Davis, J. and Hirschberg, J. (1988). Assigning intonational features in synthesized spoken discourse. In Proceedings of the 26th Annual Meeting of the Association for Computational Linguistics, pages 187-193, Buffalo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Engdahl</author>
<author>E Vallduvi</author>
</authors>
<title>Information packaging and grammar architecture: A constraint-based approach.</title>
<date>1994</date>
<booktitle>Integrating Information Structure into Constraint-Based and Categorial Approaches (DYANA-2 Report R.1.3.B). CLLI,</booktitle>
<editor>In Engdahl, E., editor,</editor>
<location>Amsterdam.</location>
<contexts>
<context position="5210" citStr="Engdahl and Vallduvi, 1994" startWordPosition="774" endWordPosition="777"> the interlocutors and the structure of their discourse. Sentences conveying the same propositional content in different contexts need not share the same information structure. That is, information structure refers to how the semantic content of an utterance is packaged, and amounts to instructions for updating the models of the discourse participants. The realization of information structure in a sentence, however, differs from language to language. In English, for example, intonation carries much of the burden of information structure, while languages with freer word order, such as Catalan (Engdahl and Vallduvi, 1994) and Turkish (Hoffman, 1995) convey information structure syntactically. 2.1 Information Structure and Intonation The relationship between intonational structure and information structure is illustrated by (3) and (4). In each of these examples, the answer contains the same string words but different intonational patterns and information structural representations. The theme of each utterance is considered to be represented by the material repeated from the question. That is, the theme of the answer is what links it to the question and defines what the utterance is about. The rheme of each utt</context>
</contexts>
<marker>Engdahl, Vallduvi, 1994</marker>
<rawString>Engdahl, E. and Vallduvi, E. (1994). Information packaging and grammar architecture: A constraint-based approach. In Engdahl, E., editor, Integrating Information Structure into Constraint-Based and Categorial Approaches (DYANA-2 Report R.1.3.B). CLLI, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Grosz</author>
<author>A K Joshi</author>
<author>S Weinstein</author>
</authors>
<title>Towards a computational theory of discourse interpretation.</title>
<date>1986</date>
<note>Unpublished manuscript.</note>
<contexts>
<context position="21283" citStr="Grosz et al., 1986" startWordPosition="3290" endWordPosition="3293">o the same set of alternative entities in the knowledge base and praise and revile belong to the same set of alternative propositions in the knowledge base. The next phase of content generation recognizes the dependency relationships between the properties to be conveyed based on shared discourse entities. This phase, which represents an extension of the rhetorical constraints, arranges propositions to ensure that consecutive utterances share semantic material (cf. (McKeown et al., 1994)). This rule, which in effect imposes a strong bias for Centering Theory&apos;s continue and retain transitions (Grosz et al., 1986) determines the theme-rheme segmentation for each proposition. 4.2 Sentence Planning After the coherence constraints from the previous section are applied, the sentence planner is responsible for making decisions concerning the form in which propositions are realized. This is accomplished by the following simple set of rules. First, Definitional isa properties are realized by the matrix verb. Other isa properties are realized by nouns or noun phrases. Top-level properties (such as those in (14)) are realized by the matrix verb. Finally, embedded properties (those evoked for building referring </context>
</contexts>
<marker>Grosz, Joshi, Weinstein, 1986</marker>
<rawString>Grosz, B. J., Joshi, A. K., and Weinstein, S. (1986). Towards a computational theory of discourse interpretation. Unpublished manuscript.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Gussenhoven</author>
</authors>
<title>On the Grammar and Semantics of Sentence Accent.</title>
<date>1983</date>
<location>Foris, Dodrecht.</location>
<marker>Gussenhoven, 1983</marker>
<rawString>Gussenhoven, C. (1983b). On the Grammar and Semantics of Sentence Accent. Foris, Dodrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Halliday</author>
</authors>
<title>Language structure and language function.</title>
<date>1970</date>
<booktitle>New Horizons in Linguistics,</booktitle>
<pages>140--165</pages>
<editor>In Lyons, J., editor,</editor>
<publisher>Penguin.</publisher>
<marker>Halliday, 1970</marker>
<rawString>Halliday, M. (1970). Language structure and language function. In Lyons, J., editor, New Horizons in Linguistics, pages 140-165. Penguin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hirschberg</author>
</authors>
<title>Accent and discourse context: Assigning pitch accent in synthetic speech.</title>
<date>1990</date>
<booktitle>In Proceedings of the Eighth National Conference on Artificial Intelligence,</booktitle>
<pages>952--957</pages>
<contexts>
<context position="7170" citStr="Hirschberg, 1990" startWordPosition="1078" endWordPosition="1079">f the pitch accents (II* and L+H*) within the tunes. Moreover, the simple mapping described above does not account for the frequently occurring cases in which thematic material bears no pitch accents and is consequently unmarked intonationally. Previous approaches to the problem of determining where to place accents have utilized heuristics based on &amp;quot;givenness.&amp;quot; That is, content-bearing words (e.g. nouns and verbs) which had not been previously mentioned (or whose roots had not been previously mentioned) were assigned accents, while function words were de-accented (Davis and Hirschberg, 1988; Hirschberg, 1990). While these heuristics account for a broad range of intonational possibilities, they fail to account for accentual patterns that serve to contrast entities or propositions that were previously &amp;quot;given&amp;quot; in the discourse. Consider, for example the intonational pattern in (5), in which the pitch accent on amplifier in the response cannot be attributed to its being &amp;quot;new&amp;quot; to the discourse. (5) Q: Do critics prefer the BRITISH amplifier L* or the AMERICAN amplifier? II* LL$ A: They prefer the AMERICAN amplifier. H* LL$ For the determination of pitch accent placement, we rely on a secondary tier of </context>
</contexts>
<marker>Hirschberg, 1990</marker>
<rawString>Hirschberg, J. (1990). Accent and discourse context: Assigning pitch accent in synthetic speech. In Proceedings of the Eighth National Conference on Artificial Intelligence, pages 952-957.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Hoffman</author>
</authors>
<title>The Computational Analysis of the Syntax and Interpretation of &apos;Free&apos; Word Order in Turkish.</title>
<date>1995</date>
<tech>PhD thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia.</location>
<contexts>
<context position="5238" citStr="Hoffman, 1995" startWordPosition="780" endWordPosition="781">heir discourse. Sentences conveying the same propositional content in different contexts need not share the same information structure. That is, information structure refers to how the semantic content of an utterance is packaged, and amounts to instructions for updating the models of the discourse participants. The realization of information structure in a sentence, however, differs from language to language. In English, for example, intonation carries much of the burden of information structure, while languages with freer word order, such as Catalan (Engdahl and Vallduvi, 1994) and Turkish (Hoffman, 1995) convey information structure syntactically. 2.1 Information Structure and Intonation The relationship between intonational structure and information structure is illustrated by (3) and (4). In each of these examples, the answer contains the same string words but different intonational patterns and information structural representations. The theme of each utterance is considered to be represented by the material repeated from the question. That is, the theme of the answer is what links it to the question and defines what the utterance is about. The rheme of each utterance is considered to be r</context>
</contexts>
<marker>Hoffman, 1995</marker>
<rawString>Hoffman, B. (1995). The Computational Analysis of the Syntax and Interpretation of &apos;Free&apos; Word Order in Turkish. PhD thesis, University of Pennsylvania, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
</authors>
<title>Automated discourse generation using discourse structure relations.</title>
<date>1993</date>
<journal>Artificial Intelligence,</journal>
<pages>63--341</pages>
<contexts>
<context position="14854" citStr="Hovy, 1993" startWordPosition="2334" endWordPosition="2335">etween them by the placement of pitch accents cannot be resolved until the choice of referring expressions has been made. Since referring expressions are generally taken to be in the domain of the sentence planner (Dale and Haddock, 1991), the present approach resolves issues of contrastive focus assignment at the sentence processing stage as well. During the content generation phase, the content of the utterance is planned based on the previous discourse. While template-based systems (McKeown, 1985) have been widely used, rhetorical structure theory (RST) approaches (Mann and Thompson, 1986; Hovy, 1993), which organize texts by identifying rhetorical relations between clause-level propositions from a knowledge base, have recently flourished. Sibun (Sibun, 1991) offers yet another alternative in which propositions are linked to one another not by rhetorical relations or pre-planned templates, but rather by physical and spatial properties represented in the knowledge-base. The present framework for organizing the content of a monologue is a hybrid of the template and RST approaches. The implementation, which is presented in the following section, produces descriptions of objects from a knowled</context>
</contexts>
<marker>Hovy, 1993</marker>
<rawString>Hovy, E. (1993). Automated discourse generation using discourse structure relations. Artificial Intelligence, 63:341-385.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Mann</author>
<author>S Thompson</author>
</authors>
<title>Rhetorical structure theory: Description and construction of text structures.</title>
<date>1986</date>
<booktitle>Natural Language Generation: New Results in Artificial Intelligence, Psychology and Linguistics,</booktitle>
<pages>279--300</pages>
<editor>In Kempen, G., editor,</editor>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Boston.</location>
<contexts>
<context position="14841" citStr="Mann and Thompson, 1986" startWordPosition="2329" endWordPosition="2333">astively distinguishing between them by the placement of pitch accents cannot be resolved until the choice of referring expressions has been made. Since referring expressions are generally taken to be in the domain of the sentence planner (Dale and Haddock, 1991), the present approach resolves issues of contrastive focus assignment at the sentence processing stage as well. During the content generation phase, the content of the utterance is planned based on the previous discourse. While template-based systems (McKeown, 1985) have been widely used, rhetorical structure theory (RST) approaches (Mann and Thompson, 1986; Hovy, 1993), which organize texts by identifying rhetorical relations between clause-level propositions from a knowledge base, have recently flourished. Sibun (Sibun, 1991) offers yet another alternative in which propositions are linked to one another not by rhetorical relations or pre-planned templates, but rather by physical and spatial properties represented in the knowledge-base. The present framework for organizing the content of a monologue is a hybrid of the template and RST approaches. The implementation, which is presented in the following section, produces descriptions of objects f</context>
</contexts>
<marker>Mann, Thompson, 1986</marker>
<rawString>Mann, W. and Thompson, S. (1986). Rhetorical structure theory: Description and construction of text structures. In Kempen, G., editor, Natural Language Generation: New Results in Artificial Intelligence, Psychology and Linguistics, pages 279-300. Kluwer Academic Publishers, Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K McKeown</author>
<author>K Kukich</author>
<author>J Shaw</author>
</authors>
<title>Practical issues in automatic documentation generation.</title>
<date>1994</date>
<booktitle>In Proceedings of the Fourth ACL Conference on Applied Natural Language Processing,</booktitle>
<pages>7--14</pages>
<institution>Stuttgart. Association for Computational Linguistics.</institution>
<contexts>
<context position="21156" citStr="McKeown et al., 1994" startWordPosition="3270" endWordPosition="3273">is is accomplished by the rhetorical constraints that determine the two propositions to be contrastive because e4 and eS belong to the same set of alternative entities in the knowledge base and praise and revile belong to the same set of alternative propositions in the knowledge base. The next phase of content generation recognizes the dependency relationships between the properties to be conveyed based on shared discourse entities. This phase, which represents an extension of the rhetorical constraints, arranges propositions to ensure that consecutive utterances share semantic material (cf. (McKeown et al., 1994)). This rule, which in effect imposes a strong bias for Centering Theory&apos;s continue and retain transitions (Grosz et al., 1986) determines the theme-rheme segmentation for each proposition. 4.2 Sentence Planning After the coherence constraints from the previous section are applied, the sentence planner is responsible for making decisions concerning the form in which propositions are realized. This is accomplished by the following simple set of rules. First, Definitional isa properties are realized by the matrix verb. Other isa properties are realized by nouns or noun phrases. Top-level propert</context>
</contexts>
<marker>McKeown, Kukich, Shaw, 1994</marker>
<rawString>McKeown, K., Kukich, K., and Shaw, J. (1994). Practical issues in automatic documentation generation. In Proceedings of the Fourth ACL Conference on Applied Natural Language Processing, pages 7-14, Stuttgart. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K R McKeown</author>
</authors>
<title>Text Generation: Using Discourse Strategies and Focus Constraints to Generate Natural Language Text.</title>
<date>1985</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="14748" citStr="McKeown, 1985" startWordPosition="2316" endWordPosition="2318">g only to the discourse model and the salient pool of knowledge, the method of contrastively distinguishing between them by the placement of pitch accents cannot be resolved until the choice of referring expressions has been made. Since referring expressions are generally taken to be in the domain of the sentence planner (Dale and Haddock, 1991), the present approach resolves issues of contrastive focus assignment at the sentence processing stage as well. During the content generation phase, the content of the utterance is planned based on the previous discourse. While template-based systems (McKeown, 1985) have been widely used, rhetorical structure theory (RST) approaches (Mann and Thompson, 1986; Hovy, 1993), which organize texts by identifying rhetorical relations between clause-level propositions from a knowledge base, have recently flourished. Sibun (Sibun, 1991) offers yet another alternative in which propositions are linked to one another not by rhetorical relations or pre-planned templates, but rather by physical and spatial properties represented in the knowledge-base. The present framework for organizing the content of a monologue is a hybrid of the template and RST approaches. The im</context>
</contexts>
<marker>McKeown, 1985</marker>
<rawString>McKeown, K. R. (1985). Text Generation: Using Discourse Strategies and Focus Constraints to Generate Natural Language Text. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Meteer</author>
</authors>
<title>Bridging the generation gap between text planning and linguistic realization.</title>
<date>1991</date>
<journal>Computational Intelligence,</journal>
<pages>7--4</pages>
<contexts>
<context position="13025" citStr="Meteer, 1991" startWordPosition="2046" endWordPosition="2047">grated into the computational task of generating spoken language. 2An in-depth discussion of the algorithm and numerous examples are presented in (Prevost, 1995). 296 3 Generation Architecture The task of natural language generation (NLG) has often been divided into three stages: content planning, in which high-level goals are satisfied and discourse structure is determined, sentence planning, in which high-level abstract semantic representations are mapped onto representations that more fully constrain the possible sentential realizations (Rambow and Korelsky, 1992; Reiter and Mellish, 1992; Meteer, 1991), and surface generation, in which the high-level propositions are converted into sentences. The selection and organization of propositions and their divisions into theme and rheme are determined by the content planner, which maintains discourse coherence by stipulating that semantic information must be shared between consecutive utterances whenever possible. That is, the content planner ensures that the theme of an utterance links it to material in prior utterances. The process of determining foci within themes and rhemes can be divided into two tasks: determining which discourse entities or </context>
</contexts>
<marker>Meteer, 1991</marker>
<rawString>Meteer, M. (1991). Bridging the generation gap between text planning and linguistic realization. Computational Intelligence, 7(4):296-304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pierrehumbert</author>
</authors>
<title>The Phonology and Phonetics of English Intonation.</title>
<date>1980</date>
<tech>PhD thesis,</tech>
<institution>Massachusetts Institute of Technology. Distributed by Indiana University Linguistics Club,</institution>
<location>Bloomington, IN.</location>
<contexts>
<context position="3647" citStr="Pierrehumbert, 1980" startWordPosition="542" endWordPosition="544"> accompanies the thematic material (Steedman, 1991; Prevost and Steedman, 1994), we present a spoken language generation architecture for producing short spoken monologues with contextually appropriate intonation. 2 Information Structure Information Structure refers to the organization of information within an utterance. In particular, it &apos;In this example, and throughout the remainder of the paper, the intonation contour is informally noted by placing prosodic phrases in parentheses and marking pitch accented words with capital letters. The tunes are more formally annotated with a variant of (Pierrehumbert, 1980) notation described in (Prevost, 1995). Three different pause lengths are associated with boundaries in the modified notation. &apos;(%)&apos; marks intra-utterance boundaries with very little pausing, &apos;%&apos; marks intrautterance boundaries associated with clauses demarcated by commas, and &apos;$&apos; marks utterance-final boundaries. For the purposes of generation and synthesis, these distinctions are crucial. 294 (3) Q: I know the AMERICAN amplifier produces MUDDY treble, (But WHAT) (does the BRITISH amplifier produce?) Li-Ht L(H%) II* LL$ A: (The BRITISH amplifier produces) (CLEAN treble.) theme-focus L(H%) II*</context>
</contexts>
<marker>Pierrehumbert, 1980</marker>
<rawString>Pierrehumbert, J. (1980). The Phonology and Phonetics of English Intonation. PhD thesis, Massachusetts Institute of Technology. Distributed by Indiana University Linguistics Club, Bloomington, IN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Prevost</author>
</authors>
<title>A Semantics of Contrast and Information Structure for Specifying Intonation in Spoken Language Generation.</title>
<date>1995</date>
<tech>PhD Thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="3685" citStr="Prevost, 1995" startWordPosition="548" endWordPosition="549">, 1991; Prevost and Steedman, 1994), we present a spoken language generation architecture for producing short spoken monologues with contextually appropriate intonation. 2 Information Structure Information Structure refers to the organization of information within an utterance. In particular, it &apos;In this example, and throughout the remainder of the paper, the intonation contour is informally noted by placing prosodic phrases in parentheses and marking pitch accented words with capital letters. The tunes are more formally annotated with a variant of (Pierrehumbert, 1980) notation described in (Prevost, 1995). Three different pause lengths are associated with boundaries in the modified notation. &apos;(%)&apos; marks intra-utterance boundaries with very little pausing, &apos;%&apos; marks intrautterance boundaries associated with clauses demarcated by commas, and &apos;$&apos; marks utterance-final boundaries. For the purposes of generation and synthesis, these distinctions are crucial. 294 (3) Q: I know the AMERICAN amplifier produces MUDDY treble, (But WHAT) (does the BRITISH amplifier produce?) Li-Ht L(H%) II* LL$ A: (The BRITISH amplifier produces) (CLEAN treble.) theme-focus L(H%) II* LL$ rherrze-focus Theme Rheme (4) Q: </context>
<context position="12573" citStr="Prevost, 1995" startWordPosition="1982" endWordPosition="1983">Set for a given property does not change the RSei, the property is not necessary for distinguishing x from its alternatives, and is not added to the CSei. Based on this contrastive focus algorithm and the mapping between information structure and intonation described above, we can view information structure as the representational bridge between discourse and intonational variability. The following sections elucidate how such a formalism can be integrated into the computational task of generating spoken language. 2An in-depth discussion of the algorithm and numerous examples are presented in (Prevost, 1995). 296 3 Generation Architecture The task of natural language generation (NLG) has often been divided into three stages: content planning, in which high-level goals are satisfied and discourse structure is determined, sentence planning, in which high-level abstract semantic representations are mapped onto representations that more fully constrain the possible sentential realizations (Rambow and Korelsky, 1992; Reiter and Mellish, 1992; Meteer, 1991), and surface generation, in which the high-level propositions are converted into sentences. The selection and organization of propositions and thei</context>
<context position="29229" citStr="Prevost, 1995" startWordPosition="4546" endWordPosition="4547">rst paragraph, is omitted from the second. Additionally, when presenting the proposition Audiofad is an audio journal,&apos; the generator is able to recognize the similarity with the corresponding proposition about Stereofool (i.e. both propositions are abstractions over the single variable open proposition &apos;X is an audio journal&apos;). The program therefore interjects the other property and produces &amp;quot;another audio journal.&amp;quot; &apos;The implementation assigns slightly higher pitch to accents bearing the subscript c (e.g. H:), which mark contrastive focus as determined by the algorithm describe above and in (Prevost, 1995). Several aspects of the contrastive intonational effects in these examples also deserve attention. Because of the content generator&apos;s use of the rhetorical contrast predicate, items are eligible to receive stress in order to convey contrast before the contrasting items are even mentioned. This phenomenon is clearly illustrated by the clause &amp;quot;PRAISED by STEREOFOOL&amp;quot; in (20), which is contrastively stressed before &amp;quot;REVILED by AUDIOFAD&amp;quot; is uttered. Such situations are produced only when the contrasting propositions are gathered by the content planner in a single invocation of the generator and id</context>
</contexts>
<marker>Prevost, 1995</marker>
<rawString>Prevost, S. (1995). A Semantics of Contrast and Information Structure for Specifying Intonation in Spoken Language Generation. PhD Thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Prevost</author>
<author>M Steedman</author>
</authors>
<title>Generating contextually appropriate intonation.</title>
<date>1993</date>
<booktitle>In Proceedings of the 6th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>332--340</pages>
<location>Utrecht.</location>
<contexts>
<context position="26911" citStr="Prevost and Steedman, 1993" startWordPosition="4176" endWordPosition="4179">ucture/intonation mapping and dictates the generation of both the syntactic and prosodic constituents. The result is a string of words and the appropriate prosodic annotations, as shown in (19). The output of this module is easily translated into a form suitable for a speech synthesizer, which produces spoken output with the desired intonation.4 (19) The X5 is a TUBE amplifier. L(H%) H*, LL$ The modules described above and shown in Figure 1 are implemented in Quintus Prolog. The system produces the types of output shown in (20) and 3A complete description of the CCG generator can be found in (Prevost and Steedman, 1993). CCG was chosen as the grammatical formalism because it licenses non-traditional syntactic constituents that are congruent with the bracketings imposed by information structure and intonational phrasing, as illustrated in (3). 4The system currently uses the AT&amp;T Bell Laboratories TTS system, but the implementation is easily adaptable to other synthesizers. 299 (21), which should be interpreted as a single (two paragraph) monologue satisfying a goal to describe two different objects.5 Note that both paragraphs include very similar types of information, but radically different intonational cont</context>
</contexts>
<marker>Prevost, Steedman, 1993</marker>
<rawString>Prevost, S. and Steedman, M. (1993). Generating contextually appropriate intonation. In Proceedings of the 6th Conference of the European Chapter of the Association for Computational Linguistics, pages 332-340, Utrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Prevost</author>
<author>M Steedman</author>
</authors>
<title>Specifying intonation from context for speech synthesis.</title>
<date>1994</date>
<journal>Speech Communication,</journal>
<pages>15--139</pages>
<contexts>
<context position="3106" citStr="Prevost and Steedman, 1994" startWordPosition="463" endWordPosition="466">mental in determining the high-level organization of information within a discourse segment. Dividing semantic representations into their thematic and rhematic parts allows propositions to be presented in a way that maximizes the shared material between utterances. (1) It was John who spoke first. (2) Q: Which car did Mary drive? A: (MARY drove)th (the RED car.)rh L+H* LH(%) II* LIS By appealing to the notion that the simple rise-fall tune (II* LL%) very often accompanies the rhematic material in an utterance and the rise-fall-rise tune often accompanies the thematic material (Steedman, 1991; Prevost and Steedman, 1994), we present a spoken language generation architecture for producing short spoken monologues with contextually appropriate intonation. 2 Information Structure Information Structure refers to the organization of information within an utterance. In particular, it &apos;In this example, and throughout the remainder of the paper, the intonation contour is informally noted by placing prosodic phrases in parentheses and marking pitch accented words with capital letters. The tunes are more formally annotated with a variant of (Pierrehumbert, 1980) notation described in (Prevost, 1995). Three different pau</context>
<context position="6091" citStr="Prevost and Steedman, 1994" startWordPosition="912" endWordPosition="915">er contains the same string words but different intonational patterns and information structural representations. The theme of each utterance is considered to be represented by the material repeated from the question. That is, the theme of the answer is what links it to the question and defines what the utterance is about. The rheme of each utterance is considered to be represented by the material that is new or forms the core contribution of the utterance to the discourse. By mapping the rise-fall tune (II* LL%) onto rhemes and the rise-fall-rise tune (L-FH* LH%) onto themes (Steedman, 1991; Prevost and Steedman, 1994), we can easily identify the string of words over which these two prominent tunes occur directly from the information structure. While this mapping is certainly overly simplistic, the results presented in Section 4.3 demonstrate its appropriateness for the class of simple declarative sentences under investigation. Knowing the strings of words to which these two tunes are to be assigned, however, does not provide enough information to determine the location of the pitch accents (II* and L+H*) within the tunes. Moreover, the simple mapping described above does not account for the frequently occu</context>
</contexts>
<marker>Prevost, Steedman, 1994</marker>
<rawString>Prevost, S. and Steedman, M. (1994). Specifying intonation from context for speech synthesis. Speech Communication, 15:139-153.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Korelsky</author>
</authors>
<title>Applied text generation.</title>
<date>1992</date>
<booktitle>In Proceedings of the Third Conference on Applied Natural Language Processing (ANLP1992),</booktitle>
<pages>40--47</pages>
<contexts>
<context position="12984" citStr="Korelsky, 1992" startWordPosition="2040" endWordPosition="2041">elucidate how such a formalism can be integrated into the computational task of generating spoken language. 2An in-depth discussion of the algorithm and numerous examples are presented in (Prevost, 1995). 296 3 Generation Architecture The task of natural language generation (NLG) has often been divided into three stages: content planning, in which high-level goals are satisfied and discourse structure is determined, sentence planning, in which high-level abstract semantic representations are mapped onto representations that more fully constrain the possible sentential realizations (Rambow and Korelsky, 1992; Reiter and Mellish, 1992; Meteer, 1991), and surface generation, in which the high-level propositions are converted into sentences. The selection and organization of propositions and their divisions into theme and rheme are determined by the content planner, which maintains discourse coherence by stipulating that semantic information must be shared between consecutive utterances whenever possible. That is, the content planner ensures that the theme of an utterance links it to material in prior utterances. The process of determining foci within themes and rhemes can be divided into two tasks:</context>
</contexts>
<marker>Korelsky, 1992</marker>
<rawString>Rambow, 0. and Korelsky, T. (1992). Applied text generation. In Proceedings of the Third Conference on Applied Natural Language Processing (ANLP1992), pages 40-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Reiter</author>
<author>C Mellish</author>
</authors>
<title>Using classification to generate text.</title>
<date>1992</date>
<booktitle>In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>265--272</pages>
<contexts>
<context position="13010" citStr="Reiter and Mellish, 1992" startWordPosition="2042" endWordPosition="2045">ch a formalism can be integrated into the computational task of generating spoken language. 2An in-depth discussion of the algorithm and numerous examples are presented in (Prevost, 1995). 296 3 Generation Architecture The task of natural language generation (NLG) has often been divided into three stages: content planning, in which high-level goals are satisfied and discourse structure is determined, sentence planning, in which high-level abstract semantic representations are mapped onto representations that more fully constrain the possible sentential realizations (Rambow and Korelsky, 1992; Reiter and Mellish, 1992; Meteer, 1991), and surface generation, in which the high-level propositions are converted into sentences. The selection and organization of propositions and their divisions into theme and rheme are determined by the content planner, which maintains discourse coherence by stipulating that semantic information must be shared between consecutive utterances whenever possible. That is, the content planner ensures that the theme of an utterance links it to material in prior utterances. The process of determining foci within themes and rhemes can be divided into two tasks: determining which discour</context>
</contexts>
<marker>Reiter, Mellish, 1992</marker>
<rawString>Reiter, E. and Mellish, C. (1992). Using classification to generate text. In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics, pages 265-272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Robin</author>
</authors>
<title>A revision-based generation architecture for reporting facts in their historical context.</title>
<date>1993</date>
<booktitle>New Concepts in Natural Language Generation: Planning, Realization and Systems,</booktitle>
<pages>238--265</pages>
<editor>In Horacek, H. and Zock, M., editors,</editor>
<publisher>Pinter Publishers,</publisher>
<location>New York.</location>
<marker>Robin, 1993</marker>
<rawString>Robin, J. (1993). A revision-based generation architecture for reporting facts in their historical context. In Horacek, H. and Zock, M., editors, New Concepts in Natural Language Generation: Planning, Realization and Systems, pages 238-265. Pinter Publishers, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Rochemont</author>
</authors>
<title>Focus in Generative Grammar.</title>
<date>1986</date>
<publisher>John Benjamins,</publisher>
<location>Philadelphia.</location>
<marker>Rochemont, 1986</marker>
<rawString>Rochemont, M. (1986). Focus in Generative Grammar. John Benjamins, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Sibun</author>
</authors>
<title>The Local Organization and Incremental Generation of Text.</title>
<date>1991</date>
<tech>PhD thesis,</tech>
<institution>University of Massachusetts.</institution>
<contexts>
<context position="15015" citStr="Sibun, 1991" startWordPosition="2354" endWordPosition="2355">lly taken to be in the domain of the sentence planner (Dale and Haddock, 1991), the present approach resolves issues of contrastive focus assignment at the sentence processing stage as well. During the content generation phase, the content of the utterance is planned based on the previous discourse. While template-based systems (McKeown, 1985) have been widely used, rhetorical structure theory (RST) approaches (Mann and Thompson, 1986; Hovy, 1993), which organize texts by identifying rhetorical relations between clause-level propositions from a knowledge base, have recently flourished. Sibun (Sibun, 1991) offers yet another alternative in which propositions are linked to one another not by rhetorical relations or pre-planned templates, but rather by physical and spatial properties represented in the knowledge-base. The present framework for organizing the content of a monologue is a hybrid of the template and RST approaches. The implementation, which is presented in the following section, produces descriptions of objects from a knowledge base with context-appropriate intonation that makes proper distinctions of contrast between alternative, salient discourse entities. Certain constraints, such</context>
</contexts>
<marker>Sibun, 1991</marker>
<rawString>Sibun, P. (1991). The Local Organization and Incremental Generation of Text. PhD thesis, University of Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steedman</author>
</authors>
<title>Structure and intonation.</title>
<date>1991</date>
<journal>Language,</journal>
<pages>260--296</pages>
<contexts>
<context position="3077" citStr="Steedman, 1991" startWordPosition="461" endWordPosition="462">ourse, is instrumental in determining the high-level organization of information within a discourse segment. Dividing semantic representations into their thematic and rhematic parts allows propositions to be presented in a way that maximizes the shared material between utterances. (1) It was John who spoke first. (2) Q: Which car did Mary drive? A: (MARY drove)th (the RED car.)rh L+H* LH(%) II* LIS By appealing to the notion that the simple rise-fall tune (II* LL%) very often accompanies the rhematic material in an utterance and the rise-fall-rise tune often accompanies the thematic material (Steedman, 1991; Prevost and Steedman, 1994), we present a spoken language generation architecture for producing short spoken monologues with contextually appropriate intonation. 2 Information Structure Information Structure refers to the organization of information within an utterance. In particular, it &apos;In this example, and throughout the remainder of the paper, the intonation contour is informally noted by placing prosodic phrases in parentheses and marking pitch accented words with capital letters. The tunes are more formally annotated with a variant of (Pierrehumbert, 1980) notation described in (Prevos</context>
<context position="6062" citStr="Steedman, 1991" startWordPosition="910" endWordPosition="911">amples, the answer contains the same string words but different intonational patterns and information structural representations. The theme of each utterance is considered to be represented by the material repeated from the question. That is, the theme of the answer is what links it to the question and defines what the utterance is about. The rheme of each utterance is considered to be represented by the material that is new or forms the core contribution of the utterance to the discourse. By mapping the rise-fall tune (II* LL%) onto rhemes and the rise-fall-rise tune (L-FH* LH%) onto themes (Steedman, 1991; Prevost and Steedman, 1994), we can easily identify the string of words over which these two prominent tunes occur directly from the information structure. While this mapping is certainly overly simplistic, the results presented in Section 4.3 demonstrate its appropriateness for the class of simple declarative sentences under investigation. Knowing the strings of words to which these two tunes are to be assigned, however, does not provide enough information to determine the location of the pitch accents (II* and L+H*) within the tunes. Moreover, the simple mapping described above does not ac</context>
<context position="25960" citStr="Steedman, 1991" startWordPosition="4021" endWordPosition="4023">some other entity, it is the property of e2 having a tube design rather than a solid-state design that needs to be conveyed to the hearer. After applying the third step of the focus assignment to (16), the result appears as shown in (17), with &amp;quot;tube&amp;quot; contrastively focused as desired. (17) Semantics: defn(isa(*e2, oc2)) Theme: oe2 Rheme: Ax isa(x , oc2) Supporting Props: isa(c2, amplifier) design(c2, *tube) The final step in the sentence planning phase of generation is to compute a representation that can serve as input to a surface form generator based on Combinatory Categorial Grammar (CCG) (Steedman, 1991), as shown in (18).3 (18) Theme: np(3, s) : (el&amp;quot; Sr def (el, ex5(e1)&amp;S)Ouirh Rheme: s: (act&amp;quot; pres)&amp;quot; indef (el, (arriplifier(c1)8z •tube(c1))&amp;isa(e1, cl))\np(3, s) : elOrh 4.3 Results Given the focus-marked output of the sentence planner, the surface generation module consults a CCG grammar which encodes the information structure/intonation mapping and dictates the generation of both the syntactic and prosodic constituents. The result is a string of words and the appropriate prosodic annotations, as shown in (19). The output of this module is easily translated into a form suitable for a speech </context>
</contexts>
<marker>Steedman, 1991</marker>
<rawString>Steedman, M. (1991a). Structure and intonation. Language, pages 260-296.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>