<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000007">
<title confidence="0.9722895">
Using a Support-Vector Machine for Japanese-to-English
Translation of Tense, Aspect, and Modality
</title>
<author confidence="0.950751">
Masaki Murata, Kiyotaka Uchimoto, Qing Ma, and Hitoshi Isahara
</author>
<affiliation confidence="0.921424">
Communications Research Laboratory
</affiliation>
<address confidence="0.973261">
2-2-2 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0289, Japan
</address>
<email confidence="0.979382">
fmurata,uchimoto,qma,isaharal@crl.go.jp
</email>
<bodyText confidence="0.886613875">
g o)-Ztite,A.if:_—.),1..)b)1;■Iv[gri;t,10.
This child always talks back to me, and this &lt;v&gt;is&lt;/v&gt;
why I hate him.
d f/AMt-3&lt;ZY.t
I &lt;v&gt;did not think&lt;/v&gt; he was so timid.
c EU
Such a busy man as he &lt;v&gt;cannot have&lt;/v&gt; any spare
time.
</bodyText>
<figureCaption confidence="0.996434">
Figure 1: Part of the modality corpus
</figureCaption>
<sectionHeader confidence="0.932643" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999778">
This paper describes experiments
carried out using a variety of
machine-learning methods, includ-
ing the k-nearest neighborhood
method that was used in a pre-
vious study, for the translation of
tense, aspect, and modality. It was
found that the support-vector ma-
chine method was the most precise
of all the methods tested.
</bodyText>
<sectionHeader confidence="0.998524" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999897916666667">
Tense, aspect, and modality are known to
cause problems in machine translation. In
traditional approaches tense, aspect, and
modality have been translated using man-
ually constructed heuristic rules. Recently,
however, corpus-based approaches such as the
example-based method (k-nearest neighbor-
hood method) have also been used (Murata
et al., 1999). For our study, we carried
out experiments on the translation of tense,
aspect, and modality by using a variety of
machine-learning methods, in addition to the
k-nearest neighborhood method, and then de-
termined which method was the most precise.
In our previous research, in which we stud-
ied the utilization of the k-nearest neighbor-
hood method, only the strings at the ends
of sentences were used to translate tense, as-
pect, and modality. In this study, however,
we used all of the morphemes from each of
the sentences as information, as well as used
the strings at the ends of the sentences.
In connection with our approach, we would
like to emphasize the following points:
</bodyText>
<listItem confidence="0.633359388888889">
• We obtained better translation of tense,
aspect, and modality by using a support-
vector machine method than we could
by using the k-nearest neighborhood
method that we used in our previous
study (Murata et al., 1999).
• In our previous study (Murata et al.,
1999) we only used the strings at the ends
of sentences to translate tense, aspect,
and modality. Here, however, we used all
of the morphemes in each of the sentences
as information, in addition to the strings
at the ends of the sentences. Using a sta-
tistical test, we were able to confirm that
adding the morpheme information from
each of the sentences was significantly ef-
fective in improving the precision of the
translations.
</listItem>
<sectionHeader confidence="0.985095" genericHeader="method">
2 Task Descriptions
</sectionHeader>
<bodyText confidence="0.998861125">
For this study we used the modality corpus
described in one of our previous papers (Mu-
rata et al., 2001). Part of this modelity corpus
is shown in Figure 1. It consists of a Japanese-
English bilingual corpus, and the main verb
phrase in each English sentence is tagged with
&lt;v&gt;. The symbols placed at the beginning
of each Japanese sentence, such as &amp;quot;c&amp;quot; and
</bodyText>
<tableCaption confidence="0.996868">
Table 1: Occurrence rates of categories
</tableCaption>
<table confidence="0.997275333333333">
Category Kodansha White paper
present 0.42 0.41
past 0.36 0.21
imperative 0.05 0.00
perfect 0.04 0.11
&amp;quot;will&amp;quot; 0.03 0.06
progressive 0.03 0.10
&amp;quot;can&amp;quot; 0.02 0.04
others 0.05 0.07
</table>
<bodyText confidence="0.943199333333333">
&amp;quot;d,&amp;quot; indicate categories of tense, aspect, and
modality for the sentence. (For example, &amp;quot;c&amp;quot;
and &amp;quot;d&amp;quot; indicate &amp;quot;can&amp;quot; and past tense, respec-
</bodyText>
<equation confidence="0.512776">
tively.)
</equation>
<bodyText confidence="0.9995605">
The following categories were used for
tense, aspect, and modality.
</bodyText>
<listItem confidence="0.998729375">
1. All combinations of each auxiliary verb (&amp;quot;be
able to,&amp;quot; &amp;quot;be going to,&amp;quot; &amp;quot;can,&amp;quot; &amp;quot;have to,&amp;quot;
&amp;quot;had better,&amp;quot; &amp;quot;may,&amp;quot; &amp;quot;must,&amp;quot; &amp;quot;need,&amp;quot; &amp;quot;ought,&amp;quot;
&amp;quot;shall,&amp;quot; &amp;quot;used to,&amp;quot; and &amp;quot;will&amp;quot;) and forms for
{present tense, past tense}, {progressive, non-
progressive}, {perfect, non-perfect} (215 cate-
gories)
2. Imperative mood (1 category)
</listItem>
<bodyText confidence="0.999635666666666">
These categories of tense, aspect, and
modality are defined on the basis of the
surface expressions of the English sentences.
Therefore, if we are able to determine the cor-
rect category from a Japanese sentence, we
should also be able to translate the Japanese
tense, aspect, and modality into English. In
this study, only the tags indicating the cate-
gories of tense, aspect, and modality and the
Japanese sentences were used.
The following two types of corpora were
used to construct the modality corpus.
</bodyText>
<listItem confidence="0.8505598">
• Example sentences in the Kodansha Japanese-
English dictionary
(39,660 sentences, 46 categories)
• White papers
(5,805 sentences, 30 categories)
</listItem>
<bodyText confidence="0.999951">
The occurrence rates of major categories
are shown in Table 1. As can be seen in the ta-
ble, the present tense occurs most frequently.
</bodyText>
<sectionHeader confidence="0.999572" genericHeader="method">
3 Machine-Learning Methods
</sectionHeader>
<bodyText confidence="0.986845">
We used the following four machine-learning
method for our study.&apos;
&apos;Although there are also decision-tree learning
methods such as C4.5, we did not use them for the
</bodyText>
<listItem confidence="0.99974525">
• k-nearest neighborhood method
• decision-list method
• maximum-entropy method
• support-vector machine method
</listItem>
<bodyText confidence="0.9979645">
In this next section, we will be explaining
each of these machine-learning methods.
</bodyText>
<subsectionHeader confidence="0.997884">
3.1 k-nearest neighborhood method
</subsectionHeader>
<bodyText confidence="0.999990461538462">
The domain of machine translation includes a
method called an example-based method. In
this method, the example most similar to the
input sentence is searched for, and the cat-
egory of the input sentence is chosen based
on the example. However, this method only
uses one example, so it is weak with respect to
noise (i.e. errors in the corpus or other excep-
tional phenomenon). The k-nearest neighbor-
hood method prevents this problem by using
the most similar examples (a total of k ex-
amples) instead of using only the most sim-
ilar example. The category is chosen on the
basis of &amp;quot;voting&amp;quot;2 on k examples. Since this
method uses multiple examples, it is capable
of providing a stable solution even if a corpus
includes noise.
In the k-nearest neighborhood method,
since it is necessary to collect similar exam-
ples, it is also necessary to define the sim-
ilarity between each pair of examples. The
definition of similarity used in this paper is
discussed in the section on features (Section
4). When there is an example that has the
same similarity as the selected k examples,
that example is also used in the &amp;quot;voting.&amp;quot;
</bodyText>
<subsectionHeader confidence="0.987069">
3.2 Decision-List Method
</subsectionHeader>
<bodyText confidence="0.975345769230769">
In this method, the probability of each cat-
egory is calculated using one feature f i(E
F,1 &lt; j &lt; k), and the category with the
highest probability is judged to be the cor-
rect category. The probability that produces
following two reasons. First, a decision-tree learning
method performs worse than other methods for several
tasks (Murata et al., 2000; Taira and Haruno, 2000).
Second, the number of attributes used in this study
was too large, and the performance of C4.5 would be-
come evne worse if the number of attributes was de-
creased so that it could be used.
2 &amp;quot;Voting&amp;quot; means a decision made by the majority.
</bodyText>
<subsectionHeader confidence="0.968899">
Small Margin Large Margin
</subsectionHeader>
<bodyText confidence="0.973731875">
syn (E aiyiK (x , x) (5)
max i,y, ibi +
2
EaiyiK(xi, xi),
where x is the context (a set of features) of
an input example, x, and y, (i = 1, y, E
{1, —1}) indicate the context of the training
data and its category, and the function sgn is
</bodyText>
<equation confidence="0.988627">
sgn(x) = 1 (x &gt; 0), (6)
—1 (otherwise).
</equation>
<bodyText confidence="0.98305325">
Each a, (i = 1,2...) is fixed as the value of a,
when the value of L(a) in Equation (7) is at
its maximum under the conditions of Equa-
tions (8) and (9).
</bodyText>
<equation confidence="0.999385">
E ai yiy) K xi , xj) (7)
i , j 1
0 &lt; a, &lt; C (i = 1,...,/) (8)
=0 (9)
</equation>
<bodyText confidence="0.999984461538462">
In this method, for data consisting of N cat-
egories, pairs of two different categories (N(N-
1)/2 pairs) are constructed. The better cate-
gory is determined using a 2-category classi-
fier. In this paper, a support-vector machine4
was used as the 2-category classifier. Finally,
the correct category is determined on the ba-
sis of &amp;quot;voting&amp;quot; on the N(N-1)/2 pairs analyzed
by the 2-category classifier.
The support-vector machine method used
in this paper was performed by combining the
support-vector machine method and the pair-
wise method described above.
</bodyText>
<sectionHeader confidence="0.7901255" genericHeader="method">
4 Features (information used in
classification)
</sectionHeader>
<bodyText confidence="0.999987416666667">
Although we have already explained the four
machine-learning methods in the previous sec-
tion, we must also define the features (infor-
mation used in classification). In this section,
we will explain these features.
As mentioned in Section 2, when a Japanese
sentence is input, we then output the category
of the tense, aspect, and modality. There-
fore, features are extracted from the input
Japanese sentence.
We tested the following three kinds of fea-
ture sets in our experiments.
</bodyText>
<listItem confidence="0.892382">
• Feature-set 1
</listItem>
<equation confidence="0.998921666666667">
L(a) = 1
2
K (x, y) = (x • y + 1)d .
</equation>
<figureCaption confidence="0.99341725">
Feature-set 1 consists of 1-gram to 10-gram
strings at the ends of the input Japanese sen-
tences and all of the morphemes from each of
the sentences.
</figureCaption>
<bodyText confidence="0.928517047619048">
e.g. &amp;quot;U tov,&amp;quot; (do not), &amp;quot;4- Ef &amp;quot; (today).
(The number of features is 230,134 in the Ko-
dansha Japanese-English dictionary and 25,958
(10) in the white papers.)
Although the function K is called a kernel
function and various types of kernel functions
are used, we used the following polynomial
function:
C and d are constants set by experimenta-
tion, and in this paper, C is fixed as 1 for all
of the experiments. Two values, d = 1 and
d = 2, are used for d. A set of x, that sat-
isfies a, &gt; 0 is called a support vector, and
the portion performing the sum in Equation
(5) is calculated using only examples that are
support vectors.
Support-vector machine methods are capa-
ble of handling data consisting of two cate-
gories. In general, data consisting of more
than two categories is handled using the pair-
wise method (Kudoh and Matsumoto, 2000).
</bodyText>
<listItem confidence="0.646472">
• Feature-set 2
</listItem>
<figureCaption confidence="0.733441333333333">
Feature-set 2 consists of 1-gram to 10-gram
strings at the ends of the input Japanese sen-
tences.
</figureCaption>
<bodyText confidence="0.920533">
e.g. &amp;quot;U tolo&amp;quot; (do not), &amp;quot;U t/-&amp;quot; (did not).
(The number of features is 199,199 in the Ko-
dansha Japanese-English dictionary and 16,610
in the white papers.)
</bodyText>
<listItem confidence="0.888683">
• Feature-set 3
</listItem>
<footnote confidence="0.9844315">
Feature-set 3 consists of all of the morphemes
from each of the sentences.
4We used the software TinySVM (Kudoh, 2000) by
Kudo as a support-vector machine.
</footnote>
<tableCaption confidence="0.839495">
Table 2: Precisions for the Kodansha data (values in parentheses are for the closed experiments)
</tableCaption>
<table confidence="0.9996161">
Method Feature-set 1 Feature-set 2 Feature-set 3
knn (k=1) 79.36% (98.50%)
knn (k=3) 80.35% (83.94%) —
knn (k=5) 80.43% (82.39%) —
knn (k=7) 80.39% (81.71%) —
knn (k=9) 80.22% (81.30%) —
decision list 74.19% (98.21%) 80.23% (98.18%) 67.90% (86.58%)
max. ent. 80.37% (88.87%) 81.16% (83.85%) 75.35% (84.15%)
support vec. (d=1) 82.48% (98.70%) 81.93% (98.50%) 78.68% (96.68%)
support vec. (d=2) 82.28% (98.48%) 81.37% (98.48%) 79.01% (98.74%)
</table>
<equation confidence="0.647195333333333">
baseline = 73.88%.
e.g. &amp;quot;4&amp;quot; Er (today), &amp;quot;t&amp;quot; (I), &amp;quot;it&amp;quot; (topic-marker
particle) &amp;quot;± &amp;quot; (run).
</equation>
<bodyText confidence="0.992605382352941">
(The number of features is 30,935 in the Kodan-
sha Japanese-English dictionary and 9,348 in the
white papers.)
The Japanese morphological analyzer JU-
MAN (Kurohashi and Nagao, 1998) was used
to divide the input sentences into morphemes.
Feature-set 1 is the combination of Feature-
sets 2 and 3. Feature-set 2 was constructed
based on our previous research (Murata et al.,
1999). In Japanese sentences the tense, as-
pect, and modality are often indicated by the
verbs at the ends of sentences.5 Therefore, in
our previous study, the strings at the ends of
the sentences were used as features. Feature-
set 3 was constructed by taking into consid-
eration the fact that adverbs such as &amp;quot;tomor-
row&amp;quot; and &amp;quot;yesterday&amp;quot; can also indicate tense,
aspect, and modality, and must therefore be
used.
Defining the feature sets is sufficient for
enabling the use of decision-list, maximum-
entropy, and support-vector machine meth-
ods. For the k-nearest neighborhood method,
however, it is also necessary to define the sim-
ilarities between examples, in addition to the
feature sets. For Feature-sets 1 and 3, which
use all of the morphemes from the entire input
sentence, it is difficult to define the similarity.
Therefore, we decided to only use Feature-set
2 for the k-nearest neighborhood method. In
terms of defining similarity for Feature-set 2,
when two examples match for x-gram charac-
ters, the value of the similarity between them
is x.
</bodyText>
<footnote confidence="0.9203705">
5The Japanese language is of the type SOV, so verb
phrases appear at the ends of sentences.
</footnote>
<sectionHeader confidence="0.994502" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999818875">
This section describes our experiments on
the translation of tense, aspect, and modal-
ity that were conducted using the machine-
learning methods described in Section 3 with
the feature sets described in Section 4 for the
tasks described in Section 2.
First, we conducted experiments using the
example sentences in the Kodansha Japanese-
English dictionary. The results for these ex-
periments are shown in Table 2. We con-
ducted two types of experiments, closed and
open6. The open experiments were performed
using 10-fold cross-validation. In the table,
the values in parentheses indicate the preci-
sions for the closed experiments and the val-
ues outside the parentheses are for the preci-
sions for the open experiments. (We used the
baseline method for comparison. This method
is used to judge cases in which the end of the
sentence is &amp;quot;ta ,&amp;quot; which is a Japanese particle
used for the past tense, as the past tense, and
judges other cases to be the present tense.)
We were able to learn the following from
the experimental results.
</bodyText>
<listItem confidence="0.7402332">
• The cases of k&gt; 1 performed better than
the case of k = 1, which is the example-
based method. We thus found that the k-
nearest neighborhood method was more
precise than the example-based method.
(This had also been confirmed in our pre-
vious research (Murata et al., 1999).)
• The decision-list method had almost the
same precisions as the k-nearest neigh-
6 Closed means experiments that uses the tested
data when learning. Open means experiments that
do not use the tested data when learning.
borhood method when Feature-set 2 was
used.
• The maximum-entropy method was more
precise than both the k-nearest neighbor-
hood and decision-list methods.
• The support-vector machine method ob-
tained higher precisions than all the other
methods.
• In terms of comparing feature sets,
the maximum-entropy and decision-list
methods obtained their highest preci-
sions when Feature-set 2 was used. They
produced lower precisions when mor-
pheme information was added, as in
Feature-set 1. This would be because the
number of unnecessary features increases
as the total number of features increases.
• In terms of comparing feature sets
</listItem>
<bodyText confidence="0.993453303030303">
for the support-vector machine method,
Feature-set 1 obtained the highest pre-
cisions. This indicates that adding the
morpheme information was effective in
improving precision. Since adding the
morpheme information produced lower
precisions for the other methods, we as-
sume that the support-vector machine
method is more capable of eliminating
unnecessary features and selecting effec-
tive features than the other methods. We
can provide the following explanations
for these results based on the theoretical
aspects of each of the methods.
Because the decision-list method chooses
the category by using only one feature, it
is likely that only one unnecessary feature
will be used, and that the precisions are
likely to decrease when there are many un-
necessary features.
Because the maximum-entropy method al-
ways uses almost all of the features, the
precisions decrease when there are many
unnecessary features.
However, because the support-vector ma-
chine includes a function that eliminates
examples, such that it only uses examples
that are support vectors and does not use
any other examples, it eliminates many un-
necessary features along with these exam-
ples. The precisions are thus unlikely to de-
crease even if there are many unnecessary
features.
</bodyText>
<tableCaption confidence="0.997631">
Table 3: Effective morpheme features
</tableCaption>
<table confidence="0.998872">
Frequency Morpheme feature
221 M (subject-case particle)
121 1,N 6 (is doing)
89 tolo (is not)
28 i 1- (do)
23 to 1-‘0 (if)
23 fc. (came)
22 t &apos;5 (already or yet)
19 IV (between)
18 AA (recently)
16 t-Z. &apos;5 (will)
16 i 1., t::-. (did)
14 i f-Z. (yet)
13 ti, 6 (can)
12 to il 4-1, if (if not)
11 i 1, .t &apos;5 (let&apos;s)
11 1- -0 h) 9 (entirely)
10 1&apos;04% (is done)
8 1- (tomorrow)
8 /AC (how)
</table>
<listItem confidence="0.62960875">
• The method with the higest precision
among all the methods was the support-
vector machine method using d = 1 and
Feature-set 1.
</listItem>
<bodyText confidence="0.999865333333333">
To confirm that using Feature-set 1 was
better than using Feature-set 2, (in other
words, to confirm that adding the morpheme
information was effective) we conducted a sign
test. This was done for the case of d = 1,
which produced better results than d = 2.
Among all of the 39,660 examples, the number
for which the category was chosen incorrectly
with Feature-set 1 and correctly with Feature-
set 2 was 648. For the opposite case (chosen
incorrectly with Feature-set 2 and correctly
with Feature-set 1), the number was 427. We
performed the sign test by using this statisti-
cal data and obtained results showing that a
significant difference existed at a significance
level of 1%. We can thus be almost completely
sure that adding the morpheme information
was effective.
Next, we examined which features were ef-
fective among all the features using the mor-
pheme information. This was done by exam-
ining the features that appeared relatively fre-
quently among the 648 examples for which the
category was chosen incorrectly with Feature-
set 1 and correctly with Feature-set 2. We
used a binomial test to choose an example
whose occurrence rate among the 648 ex-
</bodyText>
<tableCaption confidence="0.986281">
Table 4: Precisions for the white papers (values in parentheses are for the closed experiments)
</tableCaption>
<table confidence="0.9965525">
Method Feature-set 1 Feature-set 2 Feature-set 3
Support Vec. (d=1) 60.10% (99.81%) 56.61% (89.87%) 56.14% (96.67%)
Support Vec. (d=2) 64.67% (99.81%) 56.74% (89.87%) 62.07% (99.83%)
baseline = 49.77%.
</table>
<tableCaption confidence="0.783373">
Table 5: Precisions for the Kodansha data and the white papers (values in parentheses are for
the closed experiments)
</tableCaption>
<table confidence="0.997141333333333">
Training data Test data
Kodansha and White Kodansha only White papers only
Kodansha data (d=1) 82.44% (98.71%) 82.48% (98.70%) 65.31%
Kodansha data (d=2) 82.31% (98.74%) 82.28% (98.48%) 51.92%
White papers (d=1) 60.02% (99.79%) 47.65% 60.10% (99.81%)
White papers (d=2) 64.01% (99.83%) 49.53% 64.67% (99.81%)
</table>
<bodyText confidence="0.99123273015873">
amples was significantly larger than among
all the examples at a significant level of 1%
as a relatively frequently appearing example.
The 20 most frequently occurring features are
listed in Table 3. As shown in the table, we
were able to obtain features that are thought
to be effective in determining tense, aspect,
and modality, such as &amp;quot;t &amp;quot; (already), &amp;quot;A
A&amp;quot; (recently), &amp;quot;tZ 6 &amp;quot; (will), &amp;quot;t ti&amp;quot; (yet),
&amp;quot; (if not), &amp;quot;tU A &amp;quot; (let&apos;s) and &amp;quot;A
t&amp;quot; (tomorrow), and we believe that such fea-
tures improved precisions.
Next, we carried out experiments using
the white papers. These experiments were
performed using the support-vector machine
method that produced good precisions for the
Kodansha data. The open precisions were cal-
culated using 10-fold cross-validation in these
experiments as well. The experimental results
are shown in Table 4.
We learned the following from the results.
• The highest precision for the white paper
data was 64.67%.
• Feature-set 1 produced higher precisions
than Feature-set 2. Moreover, Feature-
set 3 also produced higher precisions than
Feature-set 2. These results again con-
firmed that adding the morpheme infor-
mation for each of the sentences was ef-
fective in improving precisions.
We next performed experiments in which
different-domain data was used as training
data, such that the Kodansha data was used
as the training data and the white paper data
was used as the test data.7 We then examined
how the precisions changed under these con-
ditions. These experiments were performed
using support-vector machine methods with
d=1 or d=2, which both obtained good pre-
cisions. When the training data and the test
data overlapped, 10-fold cross-validation was
used for the overlapping part. The experi-
mental results are shown in Table 5.
We learned the following from these results.
• When we used different-domain data as
training data, the precisions greatly de-
creased. When the Kodansha data was
analyzed using the White paper data as
training data or the white paper data
was analyzed using the Kodansha data
as training data, the precisions decreased
about 15% (82.48% 65.31% or 64.67%
49.53%).
We thus found that using same-domain
data is more effective in terms of preci-
sion. It is difficult to construct a system
adapted for different-domain data with
a method that uses hand-written rules.
However, for methods using machine-
learning, such as those described in this
paper, since it is easy to change the train-
ing data to different-domain data and
then have the data learned again, it is
</bodyText>
<footnote confidence="0.786112">
7 Sekine had carried out domain-dependent/
domain-independent experiments on parsing (Sekine,
1997).
</footnote>
<bodyText confidence="0.994947333333333">
easy to construct a system adapted for
different-domain data.
• When both the Kodansha and the white
paper data were used as training data,
the precisions were almost the same or
slightly decreased. We thus found in-
creasing the size of training data is not al-
ways better and adding different-domain
data is not effective.
</bodyText>
<sectionHeader confidence="0.998481" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999996953488372">
Tense, aspect, and modality are known to
present difficult problems in machine transla-
tion. In traditional approaches, tense, aspect,
and modality have been translated using man-
ually constructed heuristic rules. Recently,
however, corpus-based approaches such as the
example-based method (k-nearest neighbor-
hood method) have also been applied (Murata
et al., 1999). We carried out experiments on
the translation of tense, aspect, and modality
by using a variety of machine-learning meth-
ods, as well as the k-nearest neighborhood
method, and we determined which method
was the most precise. In our previous re-
search, in which we used the k-nearest neigh-
borhood method, only the strings at the ends
of sentences were used to translate tense, as-
pect, and modality. However, in this study we
used all of the morphemes in each of the sen-
tences as information, as well as the strings at
the ends of each of the sentences.
The support-vector machine method was
found to produce the highest precisions of all
the methods we tested. We were also able
to obtained better translations of tense, as-
pect and modality than we could by using
the k-nearest neighborhood method. Further-
more, we used a statistical test to confirm that
adding the morpheme information for the en-
tire sentence, which was not used in our pre-
vious study, was effective in improving preci-
sion. We also carried out experiments using
a different-domain corpus. In these experi-
ments, we confirmed that using a different-
domain corpus as the training data produced
very low precisions, and that we must con-
struct a system for translating the tense, as-
pect, and modality for each domain. This
also indicates that approaches using machine-
learning methods, such as those described in
this paper, are appropriate because it would
be too difficult to construct systems adapted
for different domains by hand.
</bodyText>
<sectionHeader confidence="0.992026" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999482139534884">
Nello Cristianini and John Shawe-Taylor. 2000. An
Introduction to Support Vector Machines and Other
Kernel-based Learning Methods. Cambridge Uni-
versity Press.
Taku Kudoh and Yuji Matsumoto. 2000. Use of
support vector learning for chunk identification.
CoNLL-2000.
Taku Kudoh. 2000. Tinysvm: Support vec-
tor machines. http://cl.aist-nara.ac.jp/ taku-
ku//software/Tiny SVM/index.html.
Sadao Kurohashi and Makoto Nagao, 1998. Japanese
Morphological Analysis System JUMAN version
3.5. Department of Informatics, Kyoto University.
(in Japanese).
Masaki Murata, Qing Ma, Kiyotaka Uchimoto, and
Hitoshi Isahara. 1999. An example-based approach
to Japanese-to-English translation of tense, aspect,
and modality. In TMI &apos;99, pages 66-76.
Masaki Murata, Kiyotaka Uchimoto, Qing Ma, and
Hitoshi Isahara. 2000. Bunsetsu identification us-
ing category-exclusive rules. In COLING 2000,
pages 565-571.
Masaki Murata, Masao Utiyama, Kiyotaka Uchimoto,
Qing Ma, and Hitoshi Isahara. 2001. Correction of
the modality corpus for machine translation based
on machine-learning method. 7th Annual Meeting
of the Association for Natural Language Processing.
(in Japanese; the English translation of this paper
is available at http://arXiv.org/abs/cs/0105001).
Eric Sven Ristad. 1997. Maximum Entropy Modeling
for Natural Language. ACL/EACL Tutorial Pro-
gram, Madrid.
Eric Sven Ristad. 1998. Maximum Entropy Modeling
Toolkit, Release 1.6 beta. http://www.mnemonic
.com/software/memt.
Satoshi Sekine. 1997. The domain dependence of
parsing. In Proceedings of the Fifth Conference
on Applied Natural Language Processing, pages 96-
102.
Hirotoshi Taira and Masahiko Haruno. 2000. Fea-
ture selection in svm text categorization. Transac-
tions of Information Processing Society of Japan,
41(4):1113-1123. (in Japanese).
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.334389">
<title confidence="0.9995945">Using a Support-Vector Machine for Translation of Tense, Aspect, and Modality</title>
<author confidence="0.994145">Masaki Murata</author>
<author confidence="0.994145">Kiyotaka Uchimoto</author>
<author confidence="0.994145">Qing Ma</author>
<author confidence="0.994145">Hitoshi</author>
<affiliation confidence="0.994328">Communications Research</affiliation>
<address confidence="0.78569">2-2-2 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0289,</address>
<email confidence="0.799187">fmurata,uchimoto,qma,isaharal@crl.go.jp</email>
<abstract confidence="0.991112">This child always talks back to me, and this &lt;v&gt;is&lt;/v&gt; why I hate him. d f/AMt-3&lt;ZY.t I &lt;v&gt;did not think&lt;/v&gt; he was so timid. Such a busy man as he &lt;v&gt;cannot have&lt;/v&gt; any spare time.</abstract>
<note confidence="0.501012">Figure 1: Part of the modality corpus</note>
<abstract confidence="0.998841727272727">This paper describes experiments carried out using a variety of machine-learning methods, including the k-nearest neighborhood method that was used in a previous study, for the translation of tense, aspect, and modality. It was found that the support-vector machine method was the most precise of all the methods tested.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nello Cristianini</author>
<author>John Shawe-Taylor</author>
</authors>
<title>An Introduction to Support Vector Machines and Other Kernel-based Learning Methods.</title>
<date>2000</date>
<publisher>Cambridge University Press.</publisher>
<marker>Cristianini, Shawe-Taylor, 2000</marker>
<rawString>Nello Cristianini and John Shawe-Taylor. 2000. An Introduction to Support Vector Machines and Other Kernel-based Learning Methods. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudoh</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Use of support vector learning for chunk identification.</title>
<date>2000</date>
<journal>CoNLL-2000.</journal>
<contexts>
<context position="9351" citStr="Kudoh and Matsumoto, 2000" startWordPosition="1580" endWordPosition="1583">d various types of kernel functions are used, we used the following polynomial function: C and d are constants set by experimentation, and in this paper, C is fixed as 1 for all of the experiments. Two values, d = 1 and d = 2, are used for d. A set of x, that satisfies a, &gt; 0 is called a support vector, and the portion performing the sum in Equation (5) is calculated using only examples that are support vectors. Support-vector machine methods are capable of handling data consisting of two categories. In general, data consisting of more than two categories is handled using the pairwise method (Kudoh and Matsumoto, 2000). • Feature-set 2 Feature-set 2 consists of 1-gram to 10-gram strings at the ends of the input Japanese sentences. e.g. &amp;quot;U tolo&amp;quot; (do not), &amp;quot;U t/-&amp;quot; (did not). (The number of features is 199,199 in the Kodansha Japanese-English dictionary and 16,610 in the white papers.) • Feature-set 3 Feature-set 3 consists of all of the morphemes from each of the sentences. 4We used the software TinySVM (Kudoh, 2000) by Kudo as a support-vector machine. Table 2: Precisions for the Kodansha data (values in parentheses are for the closed experiments) Method Feature-set 1 Feature-set 2 Feature-set 3 knn (k=1) 79</context>
</contexts>
<marker>Kudoh, Matsumoto, 2000</marker>
<rawString>Taku Kudoh and Yuji Matsumoto. 2000. Use of support vector learning for chunk identification. CoNLL-2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudoh</author>
</authors>
<title>Tinysvm: Support vector machines.</title>
<date>2000</date>
<note>http://cl.aist-nara.ac.jp/ takuku//software/Tiny SVM/index.html.</note>
<contexts>
<context position="9755" citStr="Kudoh, 2000" startWordPosition="1652" endWordPosition="1653">pport-vector machine methods are capable of handling data consisting of two categories. In general, data consisting of more than two categories is handled using the pairwise method (Kudoh and Matsumoto, 2000). • Feature-set 2 Feature-set 2 consists of 1-gram to 10-gram strings at the ends of the input Japanese sentences. e.g. &amp;quot;U tolo&amp;quot; (do not), &amp;quot;U t/-&amp;quot; (did not). (The number of features is 199,199 in the Kodansha Japanese-English dictionary and 16,610 in the white papers.) • Feature-set 3 Feature-set 3 consists of all of the morphemes from each of the sentences. 4We used the software TinySVM (Kudoh, 2000) by Kudo as a support-vector machine. Table 2: Precisions for the Kodansha data (values in parentheses are for the closed experiments) Method Feature-set 1 Feature-set 2 Feature-set 3 knn (k=1) 79.36% (98.50%) knn (k=3) 80.35% (83.94%) — knn (k=5) 80.43% (82.39%) — knn (k=7) 80.39% (81.71%) — knn (k=9) 80.22% (81.30%) — decision list 74.19% (98.21%) 80.23% (98.18%) 67.90% (86.58%) max. ent. 80.37% (88.87%) 81.16% (83.85%) 75.35% (84.15%) support vec. (d=1) 82.48% (98.70%) 81.93% (98.50%) 78.68% (96.68%) support vec. (d=2) 82.28% (98.48%) 81.37% (98.48%) 79.01% (98.74%) baseline = 73.88%. e.g. </context>
</contexts>
<marker>Kudoh, 2000</marker>
<rawString>Taku Kudoh. 2000. Tinysvm: Support vector machines. http://cl.aist-nara.ac.jp/ takuku//software/Tiny SVM/index.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
<author>Makoto Nagao</author>
</authors>
<date>1998</date>
<booktitle>Japanese Morphological Analysis System JUMAN version 3.5.</booktitle>
<institution>Department of Informatics, Kyoto University.</institution>
<note>(in Japanese).</note>
<contexts>
<context position="10600" citStr="Kurohashi and Nagao, 1998" startWordPosition="1779" endWordPosition="1782"> 80.35% (83.94%) — knn (k=5) 80.43% (82.39%) — knn (k=7) 80.39% (81.71%) — knn (k=9) 80.22% (81.30%) — decision list 74.19% (98.21%) 80.23% (98.18%) 67.90% (86.58%) max. ent. 80.37% (88.87%) 81.16% (83.85%) 75.35% (84.15%) support vec. (d=1) 82.48% (98.70%) 81.93% (98.50%) 78.68% (96.68%) support vec. (d=2) 82.28% (98.48%) 81.37% (98.48%) 79.01% (98.74%) baseline = 73.88%. e.g. &amp;quot;4&amp;quot; Er (today), &amp;quot;t&amp;quot; (I), &amp;quot;it&amp;quot; (topic-marker particle) &amp;quot;± &amp;quot; (run). (The number of features is 30,935 in the Kodansha Japanese-English dictionary and 9,348 in the white papers.) The Japanese morphological analyzer JUMAN (Kurohashi and Nagao, 1998) was used to divide the input sentences into morphemes. Feature-set 1 is the combination of Featuresets 2 and 3. Feature-set 2 was constructed based on our previous research (Murata et al., 1999). In Japanese sentences the tense, aspect, and modality are often indicated by the verbs at the ends of sentences.5 Therefore, in our previous study, the strings at the ends of the sentences were used as features. Featureset 3 was constructed by taking into consideration the fact that adverbs such as &amp;quot;tomorrow&amp;quot; and &amp;quot;yesterday&amp;quot; can also indicate tense, aspect, and modality, and must therefore be used. D</context>
</contexts>
<marker>Kurohashi, Nagao, 1998</marker>
<rawString>Sadao Kurohashi and Makoto Nagao, 1998. Japanese Morphological Analysis System JUMAN version 3.5. Department of Informatics, Kyoto University. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaki Murata</author>
<author>Qing Ma</author>
<author>Kiyotaka Uchimoto</author>
<author>Hitoshi Isahara</author>
</authors>
<title>An example-based approach to Japanese-to-English translation of tense, aspect, and modality.</title>
<date>1999</date>
<booktitle>In TMI &apos;99,</booktitle>
<pages>66--76</pages>
<contexts>
<context position="1260" citStr="Murata et al., 1999" startWordPosition="178" endWordPosition="181">machine-learning methods, including the k-nearest neighborhood method that was used in a previous study, for the translation of tense, aspect, and modality. It was found that the support-vector machine method was the most precise of all the methods tested. 1 Introduction Tense, aspect, and modality are known to cause problems in machine translation. In traditional approaches tense, aspect, and modality have been translated using manually constructed heuristic rules. Recently, however, corpus-based approaches such as the example-based method (k-nearest neighborhood method) have also been used (Murata et al., 1999). For our study, we carried out experiments on the translation of tense, aspect, and modality by using a variety of machine-learning methods, in addition to the k-nearest neighborhood method, and then determined which method was the most precise. In our previous research, in which we studied the utilization of the k-nearest neighborhood method, only the strings at the ends of sentences were used to translate tense, aspect, and modality. In this study, however, we used all of the morphemes from each of the sentences as information, as well as used the strings at the ends of the sentences. In co</context>
<context position="10795" citStr="Murata et al., 1999" startWordPosition="1812" endWordPosition="1815">.85%) 75.35% (84.15%) support vec. (d=1) 82.48% (98.70%) 81.93% (98.50%) 78.68% (96.68%) support vec. (d=2) 82.28% (98.48%) 81.37% (98.48%) 79.01% (98.74%) baseline = 73.88%. e.g. &amp;quot;4&amp;quot; Er (today), &amp;quot;t&amp;quot; (I), &amp;quot;it&amp;quot; (topic-marker particle) &amp;quot;± &amp;quot; (run). (The number of features is 30,935 in the Kodansha Japanese-English dictionary and 9,348 in the white papers.) The Japanese morphological analyzer JUMAN (Kurohashi and Nagao, 1998) was used to divide the input sentences into morphemes. Feature-set 1 is the combination of Featuresets 2 and 3. Feature-set 2 was constructed based on our previous research (Murata et al., 1999). In Japanese sentences the tense, aspect, and modality are often indicated by the verbs at the ends of sentences.5 Therefore, in our previous study, the strings at the ends of the sentences were used as features. Featureset 3 was constructed by taking into consideration the fact that adverbs such as &amp;quot;tomorrow&amp;quot; and &amp;quot;yesterday&amp;quot; can also indicate tense, aspect, and modality, and must therefore be used. Defining the feature sets is sufficient for enabling the use of decision-list, maximumentropy, and support-vector machine methods. For the k-nearest neighborhood method, however, it is also necess</context>
<context position="13242" citStr="Murata et al., 1999" startWordPosition="2227" endWordPosition="2230">precisions for the open experiments. (We used the baseline method for comparison. This method is used to judge cases in which the end of the sentence is &amp;quot;ta ,&amp;quot; which is a Japanese particle used for the past tense, as the past tense, and judges other cases to be the present tense.) We were able to learn the following from the experimental results. • The cases of k&gt; 1 performed better than the case of k = 1, which is the examplebased method. We thus found that the knearest neighborhood method was more precise than the example-based method. (This had also been confirmed in our previous research (Murata et al., 1999).) • The decision-list method had almost the same precisions as the k-nearest neigh6 Closed means experiments that uses the tested data when learning. Open means experiments that do not use the tested data when learning. borhood method when Feature-set 2 was used. • The maximum-entropy method was more precise than both the k-nearest neighborhood and decision-list methods. • The support-vector machine method obtained higher precisions than all the other methods. • In terms of comparing feature sets, the maximum-entropy and decision-list methods obtained their highest precisions when Feature-set</context>
<context position="21092" citStr="Murata et al., 1999" startWordPosition="3505" endWordPosition="3508">odansha and the white paper data were used as training data, the precisions were almost the same or slightly decreased. We thus found increasing the size of training data is not always better and adding different-domain data is not effective. 6 Conclusion Tense, aspect, and modality are known to present difficult problems in machine translation. In traditional approaches, tense, aspect, and modality have been translated using manually constructed heuristic rules. Recently, however, corpus-based approaches such as the example-based method (k-nearest neighborhood method) have also been applied (Murata et al., 1999). We carried out experiments on the translation of tense, aspect, and modality by using a variety of machine-learning methods, as well as the k-nearest neighborhood method, and we determined which method was the most precise. In our previous research, in which we used the k-nearest neighborhood method, only the strings at the ends of sentences were used to translate tense, aspect, and modality. However, in this study we used all of the morphemes in each of the sentences as information, as well as the strings at the ends of each of the sentences. The support-vector machine method was found to p</context>
</contexts>
<marker>Murata, Ma, Uchimoto, Isahara, 1999</marker>
<rawString>Masaki Murata, Qing Ma, Kiyotaka Uchimoto, and Hitoshi Isahara. 1999. An example-based approach to Japanese-to-English translation of tense, aspect, and modality. In TMI &apos;99, pages 66-76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaki Murata</author>
<author>Kiyotaka Uchimoto</author>
<author>Qing Ma</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Bunsetsu identification using category-exclusive rules.</title>
<date>2000</date>
<booktitle>In COLING</booktitle>
<pages>565--571</pages>
<contexts>
<context position="6442" citStr="Murata et al., 2000" startWordPosition="1041" endWordPosition="1044"> pair of examples. The definition of similarity used in this paper is discussed in the section on features (Section 4). When there is an example that has the same similarity as the selected k examples, that example is also used in the &amp;quot;voting.&amp;quot; 3.2 Decision-List Method In this method, the probability of each category is calculated using one feature f i(E F,1 &lt; j &lt; k), and the category with the highest probability is judged to be the correct category. The probability that produces following two reasons. First, a decision-tree learning method performs worse than other methods for several tasks (Murata et al., 2000; Taira and Haruno, 2000). Second, the number of attributes used in this study was too large, and the performance of C4.5 would become evne worse if the number of attributes was decreased so that it could be used. 2 &amp;quot;Voting&amp;quot; means a decision made by the majority. Small Margin Large Margin syn (E aiyiK (x , x) (5) max i,y, ibi + 2 EaiyiK(xi, xi), where x is the context (a set of features) of an input example, x, and y, (i = 1, y, E {1, —1}) indicate the context of the training data and its category, and the function sgn is sgn(x) = 1 (x &gt; 0), (6) —1 (otherwise). Each a, (i = 1,2...) is fixed as</context>
</contexts>
<marker>Murata, Uchimoto, Ma, Isahara, 2000</marker>
<rawString>Masaki Murata, Kiyotaka Uchimoto, Qing Ma, and Hitoshi Isahara. 2000. Bunsetsu identification using category-exclusive rules. In COLING 2000, pages 565-571.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaki Murata</author>
<author>Masao Utiyama</author>
<author>Kiyotaka Uchimoto</author>
<author>Qing Ma</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Correction of the modality corpus for machine translation based on machine-learning method. 7th Annual Meeting of the Association for Natural Language Processing. (in Japanese; the English translation of this paper is available at http://arXiv.org/abs/cs/0105001).</title>
<date>2001</date>
<contexts>
<context position="2750" citStr="Murata et al., 2001" startWordPosition="434" endWordPosition="438">dy (Murata et al., 1999). • In our previous study (Murata et al., 1999) we only used the strings at the ends of sentences to translate tense, aspect, and modality. Here, however, we used all of the morphemes in each of the sentences as information, in addition to the strings at the ends of the sentences. Using a statistical test, we were able to confirm that adding the morpheme information from each of the sentences was significantly effective in improving the precision of the translations. 2 Task Descriptions For this study we used the modality corpus described in one of our previous papers (Murata et al., 2001). Part of this modelity corpus is shown in Figure 1. It consists of a JapaneseEnglish bilingual corpus, and the main verb phrase in each English sentence is tagged with &lt;v&gt;. The symbols placed at the beginning of each Japanese sentence, such as &amp;quot;c&amp;quot; and Table 1: Occurrence rates of categories Category Kodansha White paper present 0.42 0.41 past 0.36 0.21 imperative 0.05 0.00 perfect 0.04 0.11 &amp;quot;will&amp;quot; 0.03 0.06 progressive 0.03 0.10 &amp;quot;can&amp;quot; 0.02 0.04 others 0.05 0.07 &amp;quot;d,&amp;quot; indicate categories of tense, aspect, and modality for the sentence. (For example, &amp;quot;c&amp;quot; and &amp;quot;d&amp;quot; indicate &amp;quot;can&amp;quot; and past tense, re</context>
</contexts>
<marker>Murata, Utiyama, Uchimoto, Ma, Isahara, 2001</marker>
<rawString>Masaki Murata, Masao Utiyama, Kiyotaka Uchimoto, Qing Ma, and Hitoshi Isahara. 2001. Correction of the modality corpus for machine translation based on machine-learning method. 7th Annual Meeting of the Association for Natural Language Processing. (in Japanese; the English translation of this paper is available at http://arXiv.org/abs/cs/0105001).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Sven Ristad</author>
</authors>
<title>Maximum Entropy Modeling for Natural Language. ACL/EACL Tutorial Program,</title>
<date>1997</date>
<location>Madrid.</location>
<marker>Ristad, 1997</marker>
<rawString>Eric Sven Ristad. 1997. Maximum Entropy Modeling for Natural Language. ACL/EACL Tutorial Program, Madrid.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Sven Ristad</author>
</authors>
<title>Maximum Entropy Modeling Toolkit, Release 1.6 beta. http://www.mnemonic .com/software/memt.</title>
<date>1998</date>
<marker>Ristad, 1998</marker>
<rawString>Eric Sven Ristad. 1998. Maximum Entropy Modeling Toolkit, Release 1.6 beta. http://www.mnemonic .com/software/memt.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
</authors>
<title>The domain dependence of parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth Conference on Applied Natural Language Processing,</booktitle>
<pages>96--102</pages>
<contexts>
<context position="20391" citStr="Sekine, 1997" startWordPosition="3399" endWordPosition="3400">alyzed using the Kodansha data as training data, the precisions decreased about 15% (82.48% 65.31% or 64.67% 49.53%). We thus found that using same-domain data is more effective in terms of precision. It is difficult to construct a system adapted for different-domain data with a method that uses hand-written rules. However, for methods using machinelearning, such as those described in this paper, since it is easy to change the training data to different-domain data and then have the data learned again, it is 7 Sekine had carried out domain-dependent/ domain-independent experiments on parsing (Sekine, 1997). easy to construct a system adapted for different-domain data. • When both the Kodansha and the white paper data were used as training data, the precisions were almost the same or slightly decreased. We thus found increasing the size of training data is not always better and adding different-domain data is not effective. 6 Conclusion Tense, aspect, and modality are known to present difficult problems in machine translation. In traditional approaches, tense, aspect, and modality have been translated using manually constructed heuristic rules. Recently, however, corpus-based approaches such as </context>
</contexts>
<marker>Sekine, 1997</marker>
<rawString>Satoshi Sekine. 1997. The domain dependence of parsing. In Proceedings of the Fifth Conference on Applied Natural Language Processing, pages 96-102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hirotoshi Taira</author>
<author>Masahiko Haruno</author>
</authors>
<title>Feature selection in svm text categorization.</title>
<date>2000</date>
<journal>Transactions of Information Processing Society of Japan,</journal>
<pages>41--4</pages>
<note>(in Japanese).</note>
<contexts>
<context position="6467" citStr="Taira and Haruno, 2000" startWordPosition="1045" endWordPosition="1048">e definition of similarity used in this paper is discussed in the section on features (Section 4). When there is an example that has the same similarity as the selected k examples, that example is also used in the &amp;quot;voting.&amp;quot; 3.2 Decision-List Method In this method, the probability of each category is calculated using one feature f i(E F,1 &lt; j &lt; k), and the category with the highest probability is judged to be the correct category. The probability that produces following two reasons. First, a decision-tree learning method performs worse than other methods for several tasks (Murata et al., 2000; Taira and Haruno, 2000). Second, the number of attributes used in this study was too large, and the performance of C4.5 would become evne worse if the number of attributes was decreased so that it could be used. 2 &amp;quot;Voting&amp;quot; means a decision made by the majority. Small Margin Large Margin syn (E aiyiK (x , x) (5) max i,y, ibi + 2 EaiyiK(xi, xi), where x is the context (a set of features) of an input example, x, and y, (i = 1, y, E {1, —1}) indicate the context of the training data and its category, and the function sgn is sgn(x) = 1 (x &gt; 0), (6) —1 (otherwise). Each a, (i = 1,2...) is fixed as the value of a, when the</context>
</contexts>
<marker>Taira, Haruno, 2000</marker>
<rawString>Hirotoshi Taira and Masahiko Haruno. 2000. Feature selection in svm text categorization. Transactions of Information Processing Society of Japan, 41(4):1113-1123. (in Japanese).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>