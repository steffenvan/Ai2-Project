<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003968">
<title confidence="0.574654">
UOW: Semantically Informed Text Similarity
</title>
<author confidence="0.950335">
Miguel Rios and Wilker Aziz
</author>
<affiliation confidence="0.973599">
Research Group in Computational Linguistics
University of Wolverhampton
</affiliation>
<address confidence="0.9394">
Stafford Street, Wolverhampton,
WV1 1SB, UK
</address>
<email confidence="0.996531">
{M.Rios, W.Aziz}@wlv.ac.uk
</email>
<author confidence="0.991568">
Lucia Specia
</author>
<affiliation confidence="0.998147">
Department of Computer Science
University of Sheffield
</affiliation>
<address confidence="0.9473615">
Regent Court, 211 Portobello,
Sheffield, S1 4DP, UK
</address>
<email confidence="0.998677">
L.Specia@sheffield.ac.uk
</email>
<sectionHeader confidence="0.998596" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999897058823529">
The UOW submissions to the Semantic Tex-
tual Similarity task at SemEval-2012 use a
supervised machine learning algorithm along
with features based on lexical, syntactic and
semantic similarity metrics to predict the se-
mantic equivalence between a pair of sen-
tences. The lexical metrics are based on word-
overlap. A shallow syntactic metric is based
on the overlap of base-phrase labels. The
semantically informed metrics are based on
the preservation of named entities and on the
alignment of verb predicates and the overlap
of argument roles using inexact matching. Our
submissions outperformed the official base-
line, with our best system ranked above aver-
age, but the contribution of the semantic met-
rics was not conclusive.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998838714285714">
We describe the UOW submissions to the Semantic
Textual Similarity (STS) task at SemEval-2012. Our
systems are based on combining similarity scores as
features using a regression algorithm to predict the
degree of semantic equivalence between a pair of
sentences. We train the regression algorithm with
different classes of similarity metrics: i) lexical,
ii) syntactic and iii) semantic. The lexical similar-
ity metrics are: i) cosine similarity using a bag-of-
words representation, and ii) precision, recall and
F-measure of content words. The syntactic metric
computes BLEU (Papineni et al., 2002), a machine
translation evaluation metric, over a labels of base-
phrases (chunks). Two semantic metrics are used: a
metric based on the preservation of Named Entities
and TINE (Rios et al., 2011). Named entities are
matched by type and content: while the type has to
match exactly, the content is compared with the as-
sistance of a distributional thesaurus. TINE is a met-
ric proposed to measure adequacy in machine trans-
lation and favors similar semantic frames. TINE
attempts to align verb predicates, assuming a one-
to-one correspondence between semantic roles, and
considering ontologies for inexact alignment. The
surface realization of the arguments is compared us-
ing a distributional thesaurus and the cosine similar-
ity metric. Finally, we use METEOR (Denkowski
and Lavie, 2010), also a common metric for ma-
chine translation evaluation, that also computes in-
exact word overlap as at way of measuring the im-
pact of our semantic metrics.
The lexical and syntactic metrics complement the
semantic metrics in dealing with the phenomena ob-
served in the task’s dataset. For instance, from the
MSRvid dataset:
</bodyText>
<equation confidence="0.9900655">
S1 Two men are playing football.
S2 Two men are practicing football.
</equation>
<bodyText confidence="0.9994945">
In this case, as typical of paraphrasing, the situa-
tion and participants are the same while the surface
realization differs, but playing can be considered
similar to practicing. From the SMT-eur dataset:
</bodyText>
<footnote confidence="0.98092775">
S3 The Council of Europe, along with the Court of
Human Rights, has a wealth of experience of
such forms of supervision, and we can build on
these.
</footnote>
<page confidence="0.962341">
673
</page>
<note confidence="0.540221">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 673–678,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.997699652173913">
S4 Just as the European Court of Human Rights, the
Council of Europe has also considerable expe-
rience with regard to these forms of control; we
can take as a basis.
Similarly, here although with different realiza-
tions, the Court of Human Rights and the European
Court of Human Rights represent the same entity.
Semantic metrics based on predicate-argument
structure can play a role in cases when different re-
alization have similar semantic roles:
S5 The right of a government arbitrarily to set aside
its own constitution is the defining characteris-
tic of a tyranny.
S6 The right for a government to draw aside its con-
stitution arbitrarily is the definition character-
istic of a tyranny.
In this work we attempt to exploit the fact that su-
perficial variations such the ones in these examples
should still render very similarity scores.
In Section 2 we describe the similarity metrics in
more detail. In Section 3 we show the results of our
three systems. In Section 4 we discuss these results
and in Section 5 we present some conclusions.
</bodyText>
<sectionHeader confidence="0.990392" genericHeader="method">
2 Similarity Metrics
</sectionHeader>
<subsectionHeader confidence="0.7833685">
The metrics used in this work are as follows:
2.1 Lexical metrics
</subsectionHeader>
<bodyText confidence="0.998892333333333">
All our lexical metrics use the same surface repre-
sentation: words. However, the cosine metric uses
bag-of-words, while all the other metrics use only
content words. We thus first represent the sentences
as bag-of-words. For example, given the pair of sen-
tences S7 and S8:
</bodyText>
<equation confidence="0.60547">
S7 A man is riding a bicycle.
S8 A man is riding a bike.
</equation>
<bodyText confidence="0.996445">
the bag-of-words are S7 = {A, man, is, riding, a,
bicycle,.} and S8 = {A, man, is, riding, a, bike, .},
and the bag-of-content-words are S7 = {man, riding,
bicycle} and S8 = {man, riding, bike}.
We compute similarity scores using the following
metrics between a pair of sentences A and B: cosine
distance (Equation 1), precision (Equation 2), recall
(Equation 3) and F-measure (Equation 4).
</bodyText>
<equation confidence="0.9970525">
cosine(A,B) = JAnB |B|(1)
precision(A, B) = |A n B |(2)
|B|
recall(A, B) = |A n B |(3)
|A|
F(A,B) = 2 · precision(A, B) · recall(A, B)
precision(A, B) + recall(A, B)
(4)
</equation>
<subsectionHeader confidence="0.997725">
2.2 BLEU over base-phrases
</subsectionHeader>
<bodyText confidence="0.99999025">
The BLEU metric is used for the automatic evalua-
tion of Machine Translation. The metric computes
the precision of exact matching of n-grams between
a hypothesis and reference translations. This sim-
ple procedure has limitations such as: the matching
of non-content words mixed with the counts of con-
tent words affects in a perfect matching that can hap-
pen even if the order of sequences of n-grams in the
hypothesis and reference translation are very differ-
ent, changing completely the meaning of the trans-
lation. To account for similarity in word order we
use BLEU over base-phrase labels instead of words,
leaving the lexical matching for other lexical and se-
mantic metrics. We compute the matchings of 1-
4-grams of base-phrase labels. This metric favors
similar syntactic order.
</bodyText>
<subsectionHeader confidence="0.992795">
2.3 Named Entities metric
</subsectionHeader>
<bodyText confidence="0.999955727272727">
The goal of the metric is to deal with synonym enti-
ties. First, named entities are grouped by class (e.g.
Organization), and then the content of the named en-
tities within the same classes is compared through
cosine similarity. If the surface realization is differ-
ent, we retrieve words that share the same context
with the named entity using Dekang Lin’s distribu-
tional thesaurus (Lin, 1998). Therefore, the cosine
similarity will have more information than just the
named entities themselves. For example, from the
sentence pair S9 and S10:
</bodyText>
<subsubsectionHeader confidence="0.469412">
S9 Companies include IBM Corp....
</subsubsectionHeader>
<page confidence="0.994767">
674
</page>
<bodyText confidence="0.955426142857143">
S10 Companies include International Business Ma-
chines ...
The entity from S9: IBM Corp. and the entity
from S10: International Business Machines have
the same tag Organization. The metric groups
them and adds words from the thesaurus result-
ing in the following bag-of-words. S9: {IBM
Corp.,... Microsoft, Intel, Sun Microsystems, Mo-
torola/Motorola, Hewlett-Packard/Hewlett-Packard,
Novell, Apple Computer...} and S10: {International
Business Machines,... Apple Computer, Yahoo, Mi-
crosoft, Alcoa...}. The metric then computes the co-
sine similarity between this expanded pair of bag-of-
words.
</bodyText>
<subsectionHeader confidence="0.945464">
2.4 METEOR
</subsectionHeader>
<bodyText confidence="0.9999942">
This metric is also a lexical metric based on uni-
gram matching between two sentences. However,
matches can be exact, using stems, synonyms, or
paraphrases of unigrams. The synonym matching is
computed using WordNet (Fellbaum, 1998) and the
paraphrase matching is computed using paraphrase
tables (Callison-Burch et al., 2010). The structure of
the sentences is not not directly considered, but sim-
ilar word orders are rewarded through higher scores
for the matching of longer fragments.
</bodyText>
<subsectionHeader confidence="0.994929">
2.5 Semantic Role Label metric
</subsectionHeader>
<bodyText confidence="0.997421666666667">
Rios et al. (2011) propose TINE, an automatic met-
ric based on the use semantic roles to align predi-
cates and their respective arguments in a pair of sen-
tences. The metric complements lexical matching
with a shallow semantic component to better address
adequacy in machine translation evaluation. The
main contribution of such a metric is to provide a
more flexible way of measuring the overlap between
shallow semantic representations (semantic role la-
bels) that considers both the semantic structure of
the sentence and the content of the semantic compo-
nents.
This metric allows to match synonym predicates
by using verb ontologies such as VerbNet (Schuler,
2006) and VerbOcean (Chklovski and Pantel, 2004)
and distributional semantics similarity metrics, such
as Dekang Lin’s thesaurus (Lin, 1998), where pre-
vious semantic metrics only perform exact match of
predicate structures and arguments. For example, in
VerbNet the verbs spook and terrify share the same
class amuse-31.1, and in VerbOcean the verb dress
is related to the verb wear, so these are considered
matches in TINE.
The main sources of errors in this metric are the
matching of unrelated verbs and the lack of coverage
of the ontologies. For example, for S11 and S12,
remain and say are (incorrectly) related as given by
VerbOcean.
S11 If snowfalls on the slopes this week, Christmas
will sell out too, says Schiefert.
S12 If the roads remain snowfall during the week,
the dates of Christmas will dry up, said
Schiefert.
For this work the matching of unrelated verbs is
a particularly crucial issue, since the sentences to be
compared are not necessarily similar, as it is the gen-
eral case in machine translation. We have thus mod-
ified the metric with a preliminary optimization step
which aligns the verb predicates by measuring two
degrees of similarity: i) how similar their arguments
are, and ii) how related the predicates’ realizations
are. Both scores are combined as shown in Equation
5 to score the similarity between the two predicates
(A„, B„) from a pair of sentences (A, B).
sim(Av, Bv) = (wlex x lexScore(A,,, B„))
where wlex and warg are the weights for each
component, argScore(Aarg, Barg) is the similarity,
which is computed as in Equation 7, of the argu-
ments between the predicates being compared and
lexScore(A„, B„) is the similarity score extracted
from the Dekang Lin’s thesaurus between the predi-
cates being compared. The Dekang Lin’s thesaurus
is an automatically built thesaurus, and for each
word it has an entry with the most similar words and
their similarity scores. If the verbs are related in the
thesaurus we use their similarity score as lexScore
otherwise lexScore = 0. The pair of predicates
with the maximum sim score is aligned. The align-
ment is an optimization problem where predicates
are aligned 1-1: we search for all 1-1 alignments that
lead to the maximum average sim for the pair of sen-
tences. For example, S13 and S14 have the follow-
ing list of predicates: S13 = {loaded, rose, ending}
</bodyText>
<equation confidence="0.848357">
5)
+(warg x argScore(Aarg,Barg)) (
</equation>
<page confidence="0.988659">
675
</page>
<bodyText confidence="0.999086571428571">
and S14 = {laced, climbed}. The metric compares
each pair of predicates and it aligns the predicates
rose and climbed because they are related in the the-
saurus with a similarity score lexScore = 0.796
and a argScore = 0.185 given that the weights are
set to 0.5 and sum up to 1 the predicates reach the
maximum sim = 0.429 score. The output of this
step results in a set of aligned verbs between a pair
of sentences.
S13 The tech - loaded Nasdaq composite rose 0
points to 0 , ending at its highest level for 0
months.
S14 The technology - laced Nasdaq Composite In-
dex IXIC climbed 0 points , or 0 percent , to
0.
The SRL similarity metric semanticRole be-
tween two sentences A and B is then defined as:
The verbScore in Equation 6 is computed over
the set of aligned predicates from the previous opti-
mization step and for each aligned predicate the ar-
gument similarity is computed by Equation 7.
</bodyText>
<equation confidence="0.982132333333333">
verbScore(A,,, B„) =
EargEArgAnArgB argScore(Aarg, Barg) (7)
|ArgB|
</equation>
<bodyText confidence="0.999294454545455">
In Equation 6, V is the set of verbs aligned between
the two sentences A and B, and |VB |is the num-
ber of verbs in one of the sentences.1 The similar-
ity between the arguments of a verb pair (A,,, B„)
in V is measured as defined in Equation 7, where
ArgA and ArgB are the sets of labeled arguments
of the first and the second sentences and |ArgB |is
the number of arguments of the verb in B.2 The
argScore(Aarg, Barg) computation is based on the
cosine similarity as in Equation 1. We treat the to-
kens in the argument as a bag-of-words.
</bodyText>
<footnote confidence="0.800483166666667">
1This is inherited from the use of the metric focusing on re-
call in machine translation, where the B is the reference trans-
lation. In this work a better approach could be to compute this
metric twice, in both directions.
2Again, from the analogy of a recall metric for machine
translation.
</footnote>
<sectionHeader confidence="0.986767" genericHeader="method">
3 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999331222222222">
We use the following state-of-the-art tools to pre-
process the data for feature extraction: i) Tree-
Tagger3 for lemmas and ii) SENNA (Collobert et
al., 2011)4 for Part-of-Speech tagging, Chunking,
Named Entity Recognition and Semantic Role La-
beling. SENNA has been reported to achieve an F-
measure of 75.79% for tagging semantic roles on the
CoNLL-2005 2 benchmark. The final feature set in-
cludes:
</bodyText>
<listItem confidence="0.999972">
• Lexical metrics
– Cosine metric over bag-of-words
– Precision over content words
– Recall over content words
– F-measure over content words
• BLEU metric over chunks
• METEOR metric over words (with stems, syn-
onyms and paraphrases)
• Named Entity metric
• Semantic Role Labeling metric
</listItem>
<bodyText confidence="0.9994425">
The Machine Learning algorithm used for re-
gression is the LIBSVM5 Support Vector Machine
(SVM) implementation using the radial basis kernel
function. We used a simple genetic algorithm (Back
et al., 1999) to tune the parameters of the SVM. The
configuration of the genetic algorithm is as follows:
</bodyText>
<listItem confidence="0.999315375">
• Fitness function: minimize the mean squared
error found by cross-validation
• Chromosome: real numbers for SVM parame-
ters &apos;y, cost and E
• Number of individuals: 80
• Number of generations: 100
• Selection method: roulette
• Crossover probability: 0.9
</listItem>
<footnote confidence="0.99992575">
3http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
4http://ml.nec-labs.com/senna/
2http://www.lsi.upc.edu/ srlconll/
5http://www.csie.ntu.edu.tw/ cjlin/libsvm/
</footnote>
<figure confidence="0.929605666666667">
R
semanticole(A B
, ) =
E,cV verbScore(A,,, B„)
|VB|
(6)
</figure>
<page confidence="0.988184">
676
</page>
<listItem confidence="0.954466">
• Mutation probability: 0.01
</listItem>
<bodyText confidence="0.99998916">
We submitted three system runs, each is a varia-
tion of the above feature set. For the official submis-
sion we used the systems with optimized SVM pa-
rameters. We trained SVM models with each of the
following task datasets: MSRpar, MSRvid, SMT-
eur and the combination of MSRpar+MSRvid. For
each test dataset we applied their respective training
models, except for the new test sets, not covered by
any training set: for On-WN we used the combina-
tion MSRpar+MSRvid, and for SMT-news we used
SMT-eur.
Tables 1 to 3 focus on the Pearson correlation
of our three systems/runs for individual datasets of
the predicted scores against human annotation, com-
pared against the official baseline, which uses a sim-
ple word overlap metric. Table 4 shows the aver-
age results over all five datasets, where ALL stands
for the Pearson correlation with the gold standard
for the five dataset, Rank is the absolute rank among
all submissions, ALLnrm is the Pearson correlation
when each dataset is fitted to the gold standard us-
ing least squares, RankNrm is the corresponding
rank and Mean is the weighted mean across the five
datasets, where the weight depends on the number
of sentence pairs in the dataset.
</bodyText>
<subsectionHeader confidence="0.999171">
3.1 Run 1: All except SRL features
</subsectionHeader>
<bodyText confidence="0.975211666666667">
Our first run uses the lexical, BLEU, METEOR and
Named Entities features, without the SRL feature.
Table 1 shows the results over the test set, where
Run 1-A is the version without SVM parameter op-
timization and Run 1-B are the official results with
optimized parameters for SVM.
</bodyText>
<table confidence="0.999084">
Task Run 1-A Run 1-B Baseline
MSRpar 0.455 0.455 0.433
MSRvid 0.706 0.362 0.300
SMT-eur 0.461 0.307 0.454
On-WN 0.514 0.281 0.586
SMT-news 0.386 0.208 0.390
</table>
<tableCaption confidence="0.995989666666667">
Table 1: Results for Run 1 using lexical, chunking,
named entities and METEOR as features. A is the non-
optimized version, B are the official results
</tableCaption>
<subsectionHeader confidence="0.998385">
3.2 Run 2: SRL feature
</subsectionHeader>
<bodyText confidence="0.974500142857143">
In this run we use only the SRL feature in order to
analyze whether this feature on its own could be suf-
ficient or lexical and other simpler features are im-
portant. Table 2 shows the results over the test set
without parameter optimization (Run 2-A) and the
official results with optimized parameters for SVM
(Run 2-B).
</bodyText>
<table confidence="0.999614166666667">
Task Run 2-A Run 2-B Baseline
MSRpar 0.335 0.300 0.433
MSRvid 0.264 0.291 0.300
SMT-eur 0.264 0.161 0.454
On-WN 0.281 0.257 0.586
SMT-news 0.189 0.221 0.390
</table>
<tableCaption confidence="0.983635">
Table 2: Results for Run 2 using the SRL feature only. A
is the non-optimized version, B are the official results
</tableCaption>
<subsectionHeader confidence="0.995651">
3.3 Run 3: All features
</subsectionHeader>
<bodyText confidence="0.9984">
In the last run we use all features. Table 3 shows
the results over the test set without parameter opti-
mization (Run 3-A) and the official results with op-
timized parameters for SVM (Run 3-B).
</bodyText>
<table confidence="0.997458833333333">
Task Run 3-A Run 3-B Baseline
MSRpar 0.472 0.353 0.433
MSRvid 0.705 0.572 0.300
SMT-eur 0.471 0.307 0.454
On-WN 0.511 0.264 0.586
SMT-news 0.410 0.116 0.390
</table>
<tableCaption confidence="0.989541">
Table 3: Results for Run 3 using all features. A is the
non-optimized version, B are the official results
</tableCaption>
<sectionHeader confidence="0.999653" genericHeader="method">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999975117647059">
Table 4 shows the ranking and normalized offi-
cial scores of our submissions compared against the
baseline. Our submissions outperform the official
baseline but significantly underperform the top sys-
tems in the shared task. The best system (Run 1)
achieved an above average ranking, but disappoint-
ingly the performance of our most complete system
(Run 3) using the semantic metric is poorer. Sur-
prisingly, the results of the non-optimized versions
outperform the optimized versions used in our offi-
cial submission. One possible reason for that is the
overfitting of the optimized models to the training
sets.
Run 1 and Run 3 have very similar results: the
overall correlation between all datasets of these two
systems is 0.98. One of the reasons for these results
is that the SRL metric is compromised by the length
</bodyText>
<page confidence="0.994705">
677
</page>
<table confidence="0.9990464">
System ALL Rank ALLnrm RankNrm Mean RankMean
Run 1 0.640 36 0.719 71 0.382 80
Run 2 0.536 59 0.629 88 0.257 88
Run 3 0.598 49 0.696 82 0.347 84
Baseline 0.311 87 0.673 85 0.436 70
</table>
<tableCaption confidence="0.999174">
Table 4: Official results and ranking over the test set for Runs 1-3 with SVM parameters optimized
</tableCaption>
<bodyText confidence="0.999913">
of the sentences. In the MSRvid dataset, where the
sentences are simple such as “Someone is drawing”,
resulting in a good semantic parsing, a high per-
formance for this metric is achieved. However, in
the SMT datasets, sentences are much longer (and
often ungrammatical, since they are produced by a
machine translation system) and the performance of
the metric drops. In addition, the SRL metric makes
mistakes such as judging as highly similar sentences
such as “A man is peeling a potato” and “A man is
slicing a potato”, where the arguments are the same
but the situations are different.
</bodyText>
<sectionHeader confidence="0.999764" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999980636363636">
We have presented our systems based on similar-
ity scores as features to train a regression algorithm
to predict the semantic similarity between a pair
of sentences. Our official submissions outperform
the baseline method, but have lower performance
than most participants, and a simpler version of the
systems without any parameter optimization proved
more robust. Disappointingly, our main contribu-
tion, the addition of a metric based on Semantic Role
Labels shows no improvement as compared to sim-
pler metrics.
</bodyText>
<sectionHeader confidence="0.998966" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999312">
This work was supported by the Mexican National
Council for Science and Technology (CONACYT),
scholarship reference 309261.
</bodyText>
<sectionHeader confidence="0.999634" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999778555555556">
Thomas Back, David B. Fogel, and Zbigniew
Michalewicz, editors. 1999. Evolutionary Com-
putation 1, Basic Algorithms and Operators. IOP
Publishing Ltd., Bristol, UK, 1st edition.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR,
pages 17–53, Uppsala, Sweden, July.
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the Web for Fine-Grained Semantic Verb
Relations. In Dekang Lin and Dekai Wu, editors, Pro-
ceedings of EMNLP 2004, pages 33–40, Barcelona,
Spain, July.
Ronan Collobert, Jason Weston, Leon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural Language Processing (almost) from Scratch.
Michael Denkowski and Alon Lavie. 2010. Meteor-next
and the meteor paraphrase tables: Improved evaluation
support for five target languages. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Transla-
tion and MetricsMATR, pages 339–342, July.
Christiane Fellbaum, editor. 1998. WordNet An Elec-
tronic Lexical Database. Cambridge, MA ; London,
May.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Linguis-
tics and 17th International Conference on Computa-
tional Linguistics - Volume 2, ACL ’98, pages 768–
774, Stroudsburg, PA, USA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ’02, pages 311–318, Strouds-
burg, PA, USA.
Miguel Rios, Wilker Aziz, and Lucia Specia. 2011. Tine:
A metric to assess mt adequacy. Proceedings of the
Sixth Workshop on Statistical Machine Translation.
Karin Kipper Schuler. 2006. VerbNet: A Broad-
Coverage, Comprehensive Verb Lexicon. Ph.D. thesis,
University of Pennsylvania.
</reference>
<page confidence="0.997728">
678
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.439685">
<title confidence="0.999139">UOW: Semantically Informed Text Similarity</title>
<author confidence="0.99979">Miguel Rios</author>
<author confidence="0.99979">Wilker</author>
<affiliation confidence="0.986872333333333">Research Group in Computational University of Stafford Street,</affiliation>
<address confidence="0.769447">WV1 1SB,</address>
<author confidence="0.732456">Lucia</author>
<affiliation confidence="0.9986685">Department of Computer University of</affiliation>
<address confidence="0.9106385">Regent Court, 211 Sheffield, S1 4DP,</address>
<email confidence="0.998991">L.Specia@sheffield.ac.uk</email>
<abstract confidence="0.995849555555556">The UOW submissions to the Semantic Textual Similarity task at SemEval-2012 use a supervised machine learning algorithm along with features based on lexical, syntactic and semantic similarity metrics to predict the semantic equivalence between a pair of sentences. The lexical metrics are based on wordoverlap. A shallow syntactic metric is based on the overlap of base-phrase labels. The semantically informed metrics are based on the preservation of named entities and on the alignment of verb predicates and the overlap of argument roles using inexact matching. Our submissions outperformed the official baseline, with our best system ranked above average, but the contribution of the semantic metrics was not conclusive.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Thomas Back</author>
<author>David B Fogel</author>
<author>Zbigniew Michalewicz</author>
<author>editors</author>
</authors>
<title>edition.</title>
<date>1999</date>
<booktitle>Evolutionary Computation 1, Basic Algorithms and Operators. IOP Publishing Ltd.,</booktitle>
<location>Bristol, UK,</location>
<contexts>
<context position="13698" citStr="Back et al., 1999" startWordPosition="2255" endWordPosition="2258">ed to achieve an Fmeasure of 75.79% for tagging semantic roles on the CoNLL-2005 2 benchmark. The final feature set includes: • Lexical metrics – Cosine metric over bag-of-words – Precision over content words – Recall over content words – F-measure over content words • BLEU metric over chunks • METEOR metric over words (with stems, synonyms and paraphrases) • Named Entity metric • Semantic Role Labeling metric The Machine Learning algorithm used for regression is the LIBSVM5 Support Vector Machine (SVM) implementation using the radial basis kernel function. We used a simple genetic algorithm (Back et al., 1999) to tune the parameters of the SVM. The configuration of the genetic algorithm is as follows: • Fitness function: minimize the mean squared error found by cross-validation • Chromosome: real numbers for SVM parameters &apos;y, cost and E • Number of individuals: 80 • Number of generations: 100 • Selection method: roulette • Crossover probability: 0.9 3http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/ 4http://ml.nec-labs.com/senna/ 2http://www.lsi.upc.edu/ srlconll/ 5http://www.csie.ntu.edu.tw/ cjlin/libsvm/ R semanticole(A B , ) = E,cV verbScore(A,,, B„) |VB| (6) 676 • Mutation probabili</context>
</contexts>
<marker>Back, Fogel, Michalewicz, editors, 1999</marker>
<rawString>Thomas Back, David B. Fogel, and Zbigniew Michalewicz, editors. 1999. Evolutionary Computation 1, Basic Algorithms and Operators. IOP Publishing Ltd., Bristol, UK, 1st edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Kay Peterson</author>
<author>Mark Przybocki</author>
<author>Omar Zaidan</author>
</authors>
<title>Findings of the 2010 joint workshop on statistical machine translation and metrics for machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>17--53</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="7763" citStr="Callison-Burch et al., 2010" startWordPosition="1237" endWordPosition="1240">osoft, Intel, Sun Microsystems, Motorola/Motorola, Hewlett-Packard/Hewlett-Packard, Novell, Apple Computer...} and S10: {International Business Machines,... Apple Computer, Yahoo, Microsoft, Alcoa...}. The metric then computes the cosine similarity between this expanded pair of bag-ofwords. 2.4 METEOR This metric is also a lexical metric based on unigram matching between two sentences. However, matches can be exact, using stems, synonyms, or paraphrases of unigrams. The synonym matching is computed using WordNet (Fellbaum, 1998) and the paraphrase matching is computed using paraphrase tables (Callison-Burch et al., 2010). The structure of the sentences is not not directly considered, but similar word orders are rewarded through higher scores for the matching of longer fragments. 2.5 Semantic Role Label metric Rios et al. (2011) propose TINE, an automatic metric based on the use semantic roles to align predicates and their respective arguments in a pair of sentences. The metric complements lexical matching with a shallow semantic component to better address adequacy in machine translation evaluation. The main contribution of such a metric is to provide a more flexible way of measuring the overlap between shall</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Peterson, Przybocki, Zaidan, 2010</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Kay Peterson, Mark Przybocki, and Omar Zaidan. 2010. Findings of the 2010 joint workshop on statistical machine translation and metrics for machine translation. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 17–53, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Chklovski</author>
<author>Patrick Pantel</author>
</authors>
<title>VerbOcean: Mining the Web for Fine-Grained Semantic Verb Relations.</title>
<date>2004</date>
<booktitle>In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004,</booktitle>
<pages>33--40</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="8663" citStr="Chklovski and Pantel, 2004" startWordPosition="1381" endWordPosition="1384">oles to align predicates and their respective arguments in a pair of sentences. The metric complements lexical matching with a shallow semantic component to better address adequacy in machine translation evaluation. The main contribution of such a metric is to provide a more flexible way of measuring the overlap between shallow semantic representations (semantic role labels) that considers both the semantic structure of the sentence and the content of the semantic components. This metric allows to match synonym predicates by using verb ontologies such as VerbNet (Schuler, 2006) and VerbOcean (Chklovski and Pantel, 2004) and distributional semantics similarity metrics, such as Dekang Lin’s thesaurus (Lin, 1998), where previous semantic metrics only perform exact match of predicate structures and arguments. For example, in VerbNet the verbs spook and terrify share the same class amuse-31.1, and in VerbOcean the verb dress is related to the verb wear, so these are considered matches in TINE. The main sources of errors in this metric are the matching of unrelated verbs and the lack of coverage of the ontologies. For example, for S11 and S12, remain and say are (incorrectly) related as given by VerbOcean. S11 If </context>
</contexts>
<marker>Chklovski, Pantel, 2004</marker>
<rawString>Timothy Chklovski and Patrick Pantel. 2004. VerbOcean: Mining the Web for Fine-Grained Semantic Verb Relations. In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 33–40, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>Leon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural Language Processing (almost) from Scratch.</title>
<date>2011</date>
<contexts>
<context position="12966" citStr="Collobert et al., 2011" startWordPosition="2137" endWordPosition="2140"> in B.2 The argScore(Aarg, Barg) computation is based on the cosine similarity as in Equation 1. We treat the tokens in the argument as a bag-of-words. 1This is inherited from the use of the metric focusing on recall in machine translation, where the B is the reference translation. In this work a better approach could be to compute this metric twice, in both directions. 2Again, from the analogy of a recall metric for machine translation. 3 Experiments and Results We use the following state-of-the-art tools to preprocess the data for feature extraction: i) TreeTagger3 for lemmas and ii) SENNA (Collobert et al., 2011)4 for Part-of-Speech tagging, Chunking, Named Entity Recognition and Semantic Role Labeling. SENNA has been reported to achieve an Fmeasure of 75.79% for tagging semantic roles on the CoNLL-2005 2 benchmark. The final feature set includes: • Lexical metrics – Cosine metric over bag-of-words – Precision over content words – Recall over content words – F-measure over content words • BLEU metric over chunks • METEOR metric over words (with stems, synonyms and paraphrases) • Named Entity metric • Semantic Role Labeling metric The Machine Learning algorithm used for regression is the LIBSVM5 Suppor</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, Leon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural Language Processing (almost) from Scratch.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor-next and the meteor paraphrase tables: Improved evaluation support for five target languages.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>339--342</pages>
<contexts>
<context position="2487" citStr="Denkowski and Lavie, 2010" startWordPosition="372" endWordPosition="375">d Entities and TINE (Rios et al., 2011). Named entities are matched by type and content: while the type has to match exactly, the content is compared with the assistance of a distributional thesaurus. TINE is a metric proposed to measure adequacy in machine translation and favors similar semantic frames. TINE attempts to align verb predicates, assuming a oneto-one correspondence between semantic roles, and considering ontologies for inexact alignment. The surface realization of the arguments is compared using a distributional thesaurus and the cosine similarity metric. Finally, we use METEOR (Denkowski and Lavie, 2010), also a common metric for machine translation evaluation, that also computes inexact word overlap as at way of measuring the impact of our semantic metrics. The lexical and syntactic metrics complement the semantic metrics in dealing with the phenomena observed in the task’s dataset. For instance, from the MSRvid dataset: S1 Two men are playing football. S2 Two men are practicing football. In this case, as typical of paraphrasing, the situation and participants are the same while the surface realization differs, but playing can be considered similar to practicing. From the SMT-eur dataset: S3</context>
</contexts>
<marker>Denkowski, Lavie, 2010</marker>
<rawString>Michael Denkowski and Alon Lavie. 2010. Meteor-next and the meteor paraphrase tables: Improved evaluation support for five target languages. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 339–342, July.</rawString>
</citation>
<citation valid="true">
<title>WordNet An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<location>Cambridge, MA ; London,</location>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet An Electronic Lexical Database. Cambridge, MA ; London, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics - Volume 2, ACL ’98,</booktitle>
<pages>768--774</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6648" citStr="Lin, 1998" startWordPosition="1077" endWordPosition="1078">instead of words, leaving the lexical matching for other lexical and semantic metrics. We compute the matchings of 1- 4-grams of base-phrase labels. This metric favors similar syntactic order. 2.3 Named Entities metric The goal of the metric is to deal with synonym entities. First, named entities are grouped by class (e.g. Organization), and then the content of the named entities within the same classes is compared through cosine similarity. If the surface realization is different, we retrieve words that share the same context with the named entity using Dekang Lin’s distributional thesaurus (Lin, 1998). Therefore, the cosine similarity will have more information than just the named entities themselves. For example, from the sentence pair S9 and S10: S9 Companies include IBM Corp.... 674 S10 Companies include International Business Machines ... The entity from S9: IBM Corp. and the entity from S10: International Business Machines have the same tag Organization. The metric groups them and adds words from the thesaurus resulting in the following bag-of-words. S9: {IBM Corp.,... Microsoft, Intel, Sun Microsystems, Motorola/Motorola, Hewlett-Packard/Hewlett-Packard, Novell, Apple Computer...} an</context>
<context position="8755" citStr="Lin, 1998" startWordPosition="1395" endWordPosition="1396">l matching with a shallow semantic component to better address adequacy in machine translation evaluation. The main contribution of such a metric is to provide a more flexible way of measuring the overlap between shallow semantic representations (semantic role labels) that considers both the semantic structure of the sentence and the content of the semantic components. This metric allows to match synonym predicates by using verb ontologies such as VerbNet (Schuler, 2006) and VerbOcean (Chklovski and Pantel, 2004) and distributional semantics similarity metrics, such as Dekang Lin’s thesaurus (Lin, 1998), where previous semantic metrics only perform exact match of predicate structures and arguments. For example, in VerbNet the verbs spook and terrify share the same class amuse-31.1, and in VerbOcean the verb dress is related to the verb wear, so these are considered matches in TINE. The main sources of errors in this metric are the matching of unrelated verbs and the lack of coverage of the ontologies. For example, for S11 and S12, remain and say are (incorrectly) related as given by VerbOcean. S11 If snowfalls on the slopes this week, Christmas will sell out too, says Schiefert. S12 If the r</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics - Volume 2, ACL ’98, pages 768– 774, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02,</booktitle>
<pages>311--318</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1706" citStr="Papineni et al., 2002" startWordPosition="248" endWordPosition="251">clusive. 1 Introduction We describe the UOW submissions to the Semantic Textual Similarity (STS) task at SemEval-2012. Our systems are based on combining similarity scores as features using a regression algorithm to predict the degree of semantic equivalence between a pair of sentences. We train the regression algorithm with different classes of similarity metrics: i) lexical, ii) syntactic and iii) semantic. The lexical similarity metrics are: i) cosine similarity using a bag-ofwords representation, and ii) precision, recall and F-measure of content words. The syntactic metric computes BLEU (Papineni et al., 2002), a machine translation evaluation metric, over a labels of basephrases (chunks). Two semantic metrics are used: a metric based on the preservation of Named Entities and TINE (Rios et al., 2011). Named entities are matched by type and content: while the type has to match exactly, the content is compared with the assistance of a distributional thesaurus. TINE is a metric proposed to measure adequacy in machine translation and favors similar semantic frames. TINE attempts to align verb predicates, assuming a oneto-one correspondence between semantic roles, and considering ontologies for inexact </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 311–318, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miguel Rios</author>
<author>Wilker Aziz</author>
<author>Lucia Specia</author>
</authors>
<title>Tine: A metric to assess mt adequacy.</title>
<date>2011</date>
<booktitle>Proceedings of the Sixth Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="1900" citStr="Rios et al., 2011" startWordPosition="280" endWordPosition="283">sion algorithm to predict the degree of semantic equivalence between a pair of sentences. We train the regression algorithm with different classes of similarity metrics: i) lexical, ii) syntactic and iii) semantic. The lexical similarity metrics are: i) cosine similarity using a bag-ofwords representation, and ii) precision, recall and F-measure of content words. The syntactic metric computes BLEU (Papineni et al., 2002), a machine translation evaluation metric, over a labels of basephrases (chunks). Two semantic metrics are used: a metric based on the preservation of Named Entities and TINE (Rios et al., 2011). Named entities are matched by type and content: while the type has to match exactly, the content is compared with the assistance of a distributional thesaurus. TINE is a metric proposed to measure adequacy in machine translation and favors similar semantic frames. TINE attempts to align verb predicates, assuming a oneto-one correspondence between semantic roles, and considering ontologies for inexact alignment. The surface realization of the arguments is compared using a distributional thesaurus and the cosine similarity metric. Finally, we use METEOR (Denkowski and Lavie, 2010), also a comm</context>
<context position="7974" citStr="Rios et al. (2011)" startWordPosition="1272" endWordPosition="1275">putes the cosine similarity between this expanded pair of bag-ofwords. 2.4 METEOR This metric is also a lexical metric based on unigram matching between two sentences. However, matches can be exact, using stems, synonyms, or paraphrases of unigrams. The synonym matching is computed using WordNet (Fellbaum, 1998) and the paraphrase matching is computed using paraphrase tables (Callison-Burch et al., 2010). The structure of the sentences is not not directly considered, but similar word orders are rewarded through higher scores for the matching of longer fragments. 2.5 Semantic Role Label metric Rios et al. (2011) propose TINE, an automatic metric based on the use semantic roles to align predicates and their respective arguments in a pair of sentences. The metric complements lexical matching with a shallow semantic component to better address adequacy in machine translation evaluation. The main contribution of such a metric is to provide a more flexible way of measuring the overlap between shallow semantic representations (semantic role labels) that considers both the semantic structure of the sentence and the content of the semantic components. This metric allows to match synonym predicates by using v</context>
</contexts>
<marker>Rios, Aziz, Specia, 2011</marker>
<rawString>Miguel Rios, Wilker Aziz, and Lucia Specia. 2011. Tine: A metric to assess mt adequacy. Proceedings of the Sixth Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karin Kipper Schuler</author>
</authors>
<title>VerbNet: A BroadCoverage, Comprehensive Verb Lexicon.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="8620" citStr="Schuler, 2006" startWordPosition="1377" endWordPosition="1378">ic based on the use semantic roles to align predicates and their respective arguments in a pair of sentences. The metric complements lexical matching with a shallow semantic component to better address adequacy in machine translation evaluation. The main contribution of such a metric is to provide a more flexible way of measuring the overlap between shallow semantic representations (semantic role labels) that considers both the semantic structure of the sentence and the content of the semantic components. This metric allows to match synonym predicates by using verb ontologies such as VerbNet (Schuler, 2006) and VerbOcean (Chklovski and Pantel, 2004) and distributional semantics similarity metrics, such as Dekang Lin’s thesaurus (Lin, 1998), where previous semantic metrics only perform exact match of predicate structures and arguments. For example, in VerbNet the verbs spook and terrify share the same class amuse-31.1, and in VerbOcean the verb dress is related to the verb wear, so these are considered matches in TINE. The main sources of errors in this metric are the matching of unrelated verbs and the lack of coverage of the ontologies. For example, for S11 and S12, remain and say are (incorrec</context>
</contexts>
<marker>Schuler, 2006</marker>
<rawString>Karin Kipper Schuler. 2006. VerbNet: A BroadCoverage, Comprehensive Verb Lexicon. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>