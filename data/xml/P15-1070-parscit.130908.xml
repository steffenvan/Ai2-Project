<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.023962">
<title confidence="0.9641">
Automatic disambiguation of English puns
</title>
<author confidence="0.808088">
Tristan Miller and Iryna Gurevych
</author>
<affiliation confidence="0.8218995">
Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Department of Computer Science, Technische Universit¨at Darmstadt
</affiliation>
<email confidence="0.698622">
https://www.ukp.tu-darmstadt.de/
</email>
<sectionHeader confidence="0.987655" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999926684210526">
Traditional approaches to word sense dis-
ambiguation (WSD) rest on the assump-
tion that there exists a single, unambigu-
ous communicative intention underlying
every word in a document. However, writ-
ers sometimes intend for a word to be in-
terpreted as simultaneously carrying mul-
tiple distinct meanings. This deliberate
use of lexical ambiguity—i.e., punning—
is a particularly common source of humour.
In this paper we describe how traditional,
language-agnostic WSD approaches can be
adapted to “disambiguate” puns, or rather
to identify their double meanings. We eval-
uate several such approaches on a manually
sense-annotated collection of English puns
and observe performance exceeding that
of some knowledge-based and supervised
baselines.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.981337492063492">
Word sense disambiguation, or WSD, is the task of
identifying a word’s meaning in context. No matter
whether it is performed by a human or a machine,
WSD usually rests on the assumption that there
is a single unambiguous communicative intention
underlying each word in the document.1 However,
there exists a class of language constructs known
1Under this assumption, lexical ambiguity arises due to
there being a plurality of words with the same surface form
but different meanings, and the task of the interpreter is to
select correctly among them. An alternative view is that each
word is a single lexical entry whose specific meaning is un-
derspecified until it is activated by the context (Ludlow, 1996).
In the case of systematically polysemous terms (i.e., words
that have several related senses shared in a systematic way by
a group of similar words), it may not be necessary to disam-
biguate them at all in order to interpret the communication
(Buitelaar, 2000). While there has been some research in mod-
elling intentional lexical-semantic underspecification (Jurgens,
2014), it is intended for closely related senses such as those of
systematically polysemous terms, not those of coarser-grained
homonyms which are the subject of this paper.
as paronomasia and syllepsis, or more generally as
puns, in which homonymic (i.e., coarse-grained)
lexical-semantic ambiguity is a deliberate effect of
the communication act. That is, the writer intends
for a certain word or other lexical item to be in-
terpreted as simultaneously carrying two or more
separate meanings, or alternatively for it to be un-
clear which meaning is the intended one. There are
a variety of motivations writers have for employing
such constructions, and in turn for why such uses
are worthy of scholarly investigation.
Perhaps surprisingly, this sort of intentional lex-
ical ambiguity has attracted little attention in the
fields of computational linguistics and natural lan-
guage processing. What little research has been
done is confined largely to computational mecha-
nisms for pun generation (in the context of natu-
ral language generation for computational humour)
and to computational analysis of phonological prop-
erties of puns. A fundamental problem which has
not yet been as widely studied is the automatic
detection and identification of intentional lexical
ambiguity—that is, given a text, does it contain
any lexical items which are used in a deliberately
ambiguous manner, and if so, what are the intended
meanings?
We consider these to be important research ques-
tions with a number of real-world applications.
For instance, puns are particularly common in ad-
vertising, where they are used not only to create
humour but also to induce in the audience a va-
lenced attitude toward the target (Valitutti et al.,
2008). Recognizing instances of such lexical am-
biguity and understanding their affective connota-
tions would be of benefit to systems performing
sentiment analysis on persuasive texts. Wordplay
is also a perennial topic of scholarship in literary
criticism and analysis. To give just one example,
puns are one of the most intensively studied as-
pects of Shakespeare’s rhetoric, and laborious man-
ual counts have shown their frequency in certain
</bodyText>
<page confidence="0.976379">
719
</page>
<note confidence="0.977945333333333">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 719–729,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999980023809524">
of his plays to range from 17 to 85 instances per
thousand lines (Keller, 2009). It is not hard to
image how computer-assisted detection, classifi-
cation, and analysis of puns could help scholars
in the digital humanities. Finally, computational
pun detection and understanding hold tremendous
potential for machine-assisted translation. Some of
the most widely disseminated and translated popu-
lar discourses—particularly television shows and
movies—feature puns and other forms of wordplay
as a recurrent and expected feature (Schr¨oter, 2005).
These pose particular challenges for translators,
who need not only to recognize and comprehend
each instance of humour-provoking ambiguity, but
also to select and implement an appropriate trans-
lation strategy.2 NLP systems could assist transla-
tors in flagging intentionally ambiguous words for
special attention, and where they are not directly
translatable (as is usually the case), the systems
may be able to propose ambiguity-preserving alter-
natives which best match the original pun’s double
meaning.
In the present work, we discuss the adaptation of
automatic word sense disambiguation techniques
to intentionally ambiguous text and evaluate these
adaptations in a controlled setting. We focus on
humorous puns, as these are by far the most com-
monly encountered and more readily available in
(and extractable from) existing text corpora.
The remainder of this paper is structured as fol-
lows: In the following section we give a brief intro-
duction to puns, WSD, and related previous work
on computational detection and comprehension of
humour. In §3 we describe the data set produced
for our experiments. In §§4 and 5 we describe how
disambiguation algorithms, evaluation metrics, and
baselines from traditional WSD can be adapted to
the task of pun identification, and in §6 we report
and discuss the performance of our adapted sys-
tems. Finally, we conclude in §7 with a review of
our research contributions and an outline of our
plans for future work.
</bodyText>
<sectionHeader confidence="0.995148" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.779287">
2.1 Puns
</subsectionHeader>
<bodyText confidence="0.992199">
Punning is a form of wordplay where a word is
used in such a way as to evoke several indepen-
dent meanings simultaneously. Humorous and non-
</bodyText>
<footnote confidence="0.936765333333333">
2The problem is compounded in audio-visual media such
as films; often one or both of the pun’s meanings appears in
the visual channel, and thus cannot be freely substituted.
</footnote>
<bodyText confidence="0.999273695652174">
humorous puns have been the subject of extensive
study in the humanities and social sciences, which
has led to insights into the nature of language-based
humour and wordplay, including their role in com-
merce, entertainment, and health care; how they are
processed in the brain; and how they vary over time
and across cultures (Monnot, 1982; Culler, 1988;
Lagerwerf, 2002; Bell et al., 2011; Bekinschtein et
al., 2011). Study of literary puns imparts a greater
understanding of the cultural or historical context
in which the literature was produced, which is of-
ten necessary to properly interpret and translate it
(Delabastita, 1997).
Puns can be classified in various ways (Attardo,
1994), though from the point of view of our par-
ticular natural language processing application the
most important distinction is between homographic
and homophonic puns. A homographic pun ex-
ploits distinct meanings of the same written word,
and a homophonic pun exploits distinct meanings
of the same spoken word. Puns can be homo-
graphic, homophonic, both, or neither, as the fol-
lowing examples illustrate:
</bodyText>
<listItem confidence="0.998266428571429">
(1) A lumberjack’s world revolves on its axes.
(2) She fell through the window but felt no
pane.
(3) A political prisoner is one who stands be-
hind her convictions.
(4) The sign at the nudist camp read, “Clothed
until April.”
</listItem>
<bodyText confidence="0.999920842105263">
In (1), the pun on axes is homographic but not
homophonic, since the two meanings (“more than
one axe” and “more than one axis”) share the same
spelling but have different pronunciations. In (2),
the pun on pane (“sheet of glass”) is homophonic
but not homographic, since the word for the sec-
ondary meaning (“feeling of injury”) is properly
spelled pain but pronounced the same. The pun on
convictions (“strongly held beliefs” and “findings
of criminal guilt”) in (3) is both homographic and
homophonic. Finally, the pun on clothed in (4) is
neither homographic nor homophonic, since the
word for the secondary meaning, closed, differs
in both spelling and pronunciation. Such puns are
commonly known as imperfect puns.
Other characteristics of puns important for our
work include whether they involve compounds,
multiword expressions, or proper names, and
whether the pun’s multiple meanings involve mul-
</bodyText>
<page confidence="0.990179">
720
</page>
<bodyText confidence="0.953997">
tiple parts of speech. We elaborate on the signifi-
cance of these characteristics in the next section.
</bodyText>
<subsectionHeader confidence="0.999784">
2.2 Word sense disambiguation
</subsectionHeader>
<bodyText confidence="0.999021293103448">
Word sense disambiguation (WSD) is the task of
determining which sense of a polysemous term is
the one intended when that term is used in a given
communicative act. Besides the target term itself,
a WSD system generally requires two inputs: the
context (i.e., the running text containing the target),
and a sense inventory which specifies all possible
senses of the target.
Approaches to WSD can be categorized accord-
ing to the type of knowledge sources used to help
discriminate senses. Knowledge-based approaches
restrict themselves to using pre-existing lexical-
semantic resources (LSRs), or such additional infor-
mation as can be automatically extracted or mined
from raw text corpora. Supervised approaches, on
the other hand, use manually sense-annotated cor-
pora as training data for a machine learning sys-
tem, or as seed data for a bootstrapping process.
Supervised WSD systems generally outperform
their knowledge-based counterparts, though this
comes at the considerable expense of having hu-
man annotators manually disambiguate hundreds
or thousands of example sentences. Moreover, su-
pervised approaches tend to be such that they can
disambiguate only those words for which they have
seen sufficient training examples to cover all senses.
That is, most of them cannot disambiguate words
which do not occur in the training data, nor can
they select the correct sense of a known word if
that sense was never observed in the training data.
Regardless of the approach, all WSD systems
work by extracting contextual information for the
target word and comparing it against the sense
information stored for that word. A seminal
knowledge-based example is the Lesk algorithm
(Lesk, 1986) which disambiguates a pair of tar-
get terms in context by comparing their respective
dictionary definitions and selecting the two with
the greatest number of words in common. Though
simple, the Lesk algorithm performs surprisingly
well, and has frequently served as the basis of more
sophisticated approaches. In recent years, Lesk
variants in which the contexts and definitions are
supplemented with entries from a distributional the-
saurus (Lin, 1998) have achieved state-of-the-art
performance for knowledge-based systems on stan-
dard data sets (Miller et al., 2012; Basile et al.,
2014).
In traditional word sense disambiguation, the
part of speech and lemma of the target word are
usually known a priori, or can be determined with
high accuracy using off-the-shelf natural language
processing tools. The pool of candidate senses can
therefore be restricted to those whose lexicaliza-
tions exactly match the target lemma and part of
speech. No such help is available for puns, at least
not in the general case. Take the following two
examples:
</bodyText>
<listItem confidence="0.996581">
(5) Tom moped.
(6) “I want a scooter,” Tom moped.
</listItem>
<bodyText confidence="0.999972952380953">
In the first of these sentences, the word moped
is unambiguously a verb with the lemma mope,
and would be correctly recognized as such by any
automatic lemmatizer and part-of-speech tagger.
The moped of the second example is a pun, one of
whose meanings is the same inflected form of the
verb mope (“to sulk”) and the other of which is the
noun moped (“motorized scooter”). For such cases
an automated pun identifier would therefore need
to account for all possible lemmas for all possible
parts of speech of the target word. The situation
becomes even more onerous for heterographic and
imperfect puns, which may require the use of pro-
nunciation dictionaries, and application of phono-
logical theories of punning, in order to recover the
lemmas (Hempelmann, 2003).
As our research interests are in lexical semantics
rather than phonology, we focus on puns which are
homographic and monolexemic. This allows us to
investigate the problem of pun identification in as
controlled a setting as possible.
</bodyText>
<subsectionHeader confidence="0.9653215">
2.3 Previous work
2.3.1 Computational humour
</subsectionHeader>
<bodyText confidence="0.999993538461538">
There is some previous research on computational
detection and comprehension of humour, though by
and large it is not concerned specifically with puns;
those studies which do analyze puns tend to have
a phonological or syntactic rather than semantic
bent. In this subsection we briefly review some
prior work which is relevant to ours.
Yokogawa (2002) describes a system for detect-
ing the presence of puns in Japanese text. However,
this work is concerned only with puns which are
both imperfect and ungrammatical, relying on syn-
tactic cues rather than the lexical-semantic informa-
tion we propose to use. Taylor and Mazlack (2004)
</bodyText>
<page confidence="0.986206">
721
</page>
<bodyText confidence="0.999992119047619">
describe an n-gram–based approach for recogniz-
ing when imperfect puns are used for humorous
effect in a certain narrow class of English knock-
knock jokes. Their focus on imperfect puns and
their use of a fixed syntactic context makes their
approach largely inapplicable to perfect puns in
running text. Mihalcea and Strapparava (2005)
treat humour recognition as a classification task,
employing various machine learning techniques
on humour-specific stylistic features such as al-
literation and antonymy. Of particular interest is
their follow-up analysis (Mihalcea and Strapparava,
2006), where they specifically point to their sys-
tem’s failure to resolve lexical-semantic ambiguity
as a stumbling block to better accuracy, and specu-
late that deeper semantic analysis of the text, such
as via word sense disambiguation or domain disam-
biguation, could aid in the detection of humorous
incongruity and opposition.
The previous work which is perhaps most rele-
vant to ours is that of Mihalcea et al. (2010). They
build a data set consisting of 150 joke set-ups, each
of which is followed by four possible “punchlines”,
only one of which is actually humorous (but not
necessarily due to a pun). They then compare
the set-ups against the punchlines using various
models of incongruity detection, including many
exploiting knowledge-based semantic relatedness
such as Lesk. The Lesk model had an accuracy
of 56%, which is lower than that of a naive pol-
ysemy model which simply selects the punchline
with the highest mean polysemy (66%) and even
of a random-choice baseline (62%). However, it
should be stressed here that the Lesk model did not
directly account for the possibility that any given
word might be ambiguous. Rather, for every word
in the setup, the Lesk measure was used to select a
word in the punchline such that the lexical overlap
between each one of their possible definitions was
maximized. The overlap scores for all word pairs
were then averaged, and the punchline with the low-
est average score selected as the most humorous.
</bodyText>
<subsectionHeader confidence="0.874889">
2.3.2 Corpora
</subsectionHeader>
<bodyText confidence="0.999989523809524">
There are a number of English-language corpora
of intentional lexical ambiguity which have been
used in past work, usually in linguistics or the so-
cial sciences. In their work on computer-generated
humour, Lessard et al. (2002) use a corpus of 374
“Tom Swifty” puns taken from the Internet, plus
a well-balanced corpus of 50 humorous and non-
humorous lexical ambiguities generated program-
matically (Venour, 1999). Hong and Ong (2009)
also study humour in natural language generation,
using a smaller data set of 27 punning riddles de-
rived from a mix of natural and artificial sources.
In their study of wordplay in religious advertis-
ing, Bell et al. (2011) compile a corpus of 373
puns taken from church marquees and literature,
and compare it against a general corpus of 1515
puns drawn from Internet websites and a special-
ized dictionary. Zwicky and Zwicky (1986) con-
duct a phonological analysis on a corpus of several
thousand puns, some of which they collected them-
selves from advertisements and catalogues, and the
remainder of which were taken from previously
published collections. Two studies on cognitive
strategies used by second language learners (Ka-
plan and Lucas, 2001; Lucas, 2004) used a data set
of 58 jokes compiled from newspaper comics, 32
of which rely on lexical ambiguity. Bucaria (2004)
conducts a linguistic analysis of a set of 135 hu-
morous newspaper headlines, about half of which
exploit lexical ambiguity.
Such data sets—particularly the larger ones—
provided us good evidence that intentionally lexical
ambiguous exemplars exist in sufficient numbers to
make a rigorous evaluation of our task feasible. Un-
fortunately, none of the above-mentioned corpora
have been published in full, and moreover many of
them contain (sometimes exclusively) the sort of
imperfect or otherwise heterographic puns which
we mean to exclude from consideration. This has
motivated us to produce our own corpus of puns,
the construction and analysis of which is described
in the following section.
</bodyText>
<sectionHeader confidence="0.967592" genericHeader="method">
3 Data set
</sectionHeader>
<bodyText confidence="0.999949888888889">
As in traditional WSD, a prerequisite for our re-
search is a corpus of examples, where one or more
human annotators have already identified the am-
biguous words and marked up their various mean-
ings with reference to a given sense inventory. Such
a corpus is sufficient for evaluating what we term
pun identification or pun disambiguation—that is,
identifying the senses of a term known a priori to
be a pun.
</bodyText>
<subsectionHeader confidence="0.998704">
3.1 Construction
</subsectionHeader>
<bodyText confidence="0.9997555">
Though several prior studies have produced corpora
of puns, none of them are systematically sense-
annotated. We therefore compiled our own corpus
by pooling together some of the aforementioned
</bodyText>
<page confidence="0.983737">
722
</page>
<bodyText confidence="0.999581191489361">
corpora, the user-submitted puns from the Pun of
the Day website,3 and private collections provided
to us by some professional humorists. This raw
collection of 7750 one-liners was then filtered by
trained human annotators to those instances meet-
ing the following four criteria:
One pun per instance: Of all the lexical units in
the instance, one and only one may be a pun.
(This criterion simplifies the task detecting the
presence and location of puns in a text, a clas-
sification task which we intend to investigate
in future work.)
One content word per pun: The lexical unit that
forms the pun must consist of, or contain, only
a single content word (i.e., a noun, verb, adjec-
tive, or adverb), excepting adverbial particles
of phrasal verbs. This criterion is important
because, in our observations, it is often only
one word which carries ambiguity in puns on
compounds and multi-word expressions. Ac-
cepting lexical units containing more than one
content word would have required our annota-
tors to laboriously partition the pun into (pos-
sibly overlapping) sense-bearing units and to
assign sense sets to each of them, inflating the
complexity of the annotation task to unaccept-
able levels.
Two meanings per pun: The pun must have ex-
actly two distinct meanings. Though many
sources state that puns have only two senses
(Redfern, 1984; Attardo, 1994), our annota-
tors identified a handful of corpus examples
where the pun could plausibly be analyzed as
carrying three distinct meanings. To simplify
our manual annotation procedure and our eval-
uation metrics we excluded these rare outliers.
Weak homography: The lexical units corre-
sponding to the two distinct meanings must be
spelled exactly the same way, except that parti-
cles and inflections may be disregarded. This
somewhat softer definition of homography al-
lows us to admit a good many morphologi-
cally interesting cases which were nonetheless
readily recognized by our human annotators.
The filtering reduced the number of instances
to 1652, whose puns two human judges anno-
tated with sense keys from WordNet 3.1 (Fellbaum,
</bodyText>
<footnote confidence="0.79964">
3http://www.punoftheday.com/
</footnote>
<bodyText confidence="0.999568083333333">
1998). Using an online annotation tool specially
constructed for this study, the annotators applied
two sets of sense keys to each instance, one for each
of the two meanings of the pun. For cases where the
distinction between WordNet’s fine-grained senses
was irrelevant, the annotators had the option of
labelling the meaning with more than one sense
key. Annotators also had the option of marking a
meaning as unassignable if WordNet had no cor-
responding sense key. Further details of our anno-
tation tool and its use can be found in Miller and
Turkovi´c (2015).
</bodyText>
<subsectionHeader confidence="0.998921">
3.2 Analysis
</subsectionHeader>
<bodyText confidence="0.999875428571428">
Our judges agreed on which word was the pun
in 1634 out of 1652 cases, a raw agreement of
98.91%. For the agreed cases, we used DKPro
Agreement (Meyer et al., 2014) to compute Krip-
pendorff’s α (Krippendorff, 1980) for the sense
annotations. This is a chance-correcting metric
of inter-annotator agreement ranging in (−1,1],
where 1 indicates perfect agreement, −1 perfect
disagreement, and 0 the expected score for ran-
dom labelling. Our distance metric for α is a
straightforward adaptation of the MASI set compar-
ison metric (Passonneau, 2006). Whereas standard
MASI, dm(A,B), compares two annotation sets A
and B, our annotations take the form of unordered
pairs of sets {A1,A2} and {B1,B2}. We therefore
find the mapping between elements of the two
pairs that gives the lowest total distance, and halve
it: dm,({A1,A2},{B1,B2}) = 21min(dm(A1,B1)+
dm(A2,B2),dm(A1,B2) + dm(A2,B1)). With this
method we observe a Krippendorff’s α of 0.777;
this is only slightly below the 0.8 threshold recom-
mended by Krippendorff, and far higher than what
has been reported in other sense annotation studies
(Passonneau et al., 2006; Jurgens and Klapaftis,
2013).
Where possible, we resolved sense annotation
disagreements automatically by taking the intersec-
tion of corresponding sense sets. Where the an-
notators’ sense sets were disjoint or contradictory
(including the cases where the annotators disagreed
on the pun word), we had a human adjudicator at-
tempt to resolve the disagreement in favour of one
annotator or the other. This left us with 1607 in-
stances,4 of which we retained only the 1298 that
had successful (i.e., not marked as unassignable)
</bodyText>
<footnote confidence="0.995171666666667">
4Pending clearance of the distribution rights, we will make
some or all of our annotated data set available on our website
athttps://www.ukp.tu-darmstadt.de/data/.
</footnote>
<page confidence="0.996853">
723
</page>
<bodyText confidence="0.99992316">
annotations for the present study. The contexts in
this data set range in length from 3 to 44 words,
with an average length of 11.9. The 2596 meanings
carry sense key annotations corresponding to any-
where from one to seven WordNet synsets, with
an average of 1.08. As expected, then, WordNet’s
sense granularity proved to be somewhat finer than
necessary to characterize the meanings in the data
set, though only marginally so.
Of the 2596 individual meanings, 1303 (50.2%)
were annotated with noun senses only, 877 (33.8%)
with verb senses only, 340 (13.1%) with adjective
senses only, and 41 (1.6%) with adverb senses only.
Only 35 individual meanings (1.3%) carry sense an-
notations corresponding to multiple parts of speech.
However, for 297 (22.9%) of our puns, the two
meanings had different parts of speech. Similarly,
sense annotations for each individual meaning cor-
respond to anywhere from one to four different
lemmas, with a mean of 1.25. These observations
confirm the concerns we raised in §2.2 that pun
disambiguators, unlike traditional WSD systems,
cannot always rely on the output of a lemmatizer
or part-of-speech tagger to narrow down the list of
sense candidates.
</bodyText>
<sectionHeader confidence="0.978366" genericHeader="method">
4 Pun disambiguation
</sectionHeader>
<bodyText confidence="0.999915328571429">
It has long been observed that gloss overlap–based
WSD systems, such as those based on the Lesk
algorithm, fail to distinguish between candidate
senses when their definitions have a similar over-
lap with the target word’s context. In some cases
this is because the overlap is negligible or nonexis-
tent; this is known as the lexical gap problem, and
various solutions to it are discussed in (inter alia)
Miller et al. (2012). In other cases, the indecision
arises because the definitions provided by the sense
inventory are too fine-grained; this problem has
been addressed, with varying degrees of success,
through sense clustering or coarsening techniques
(a short but reasonably comprehensive survey of
which appears in Matuschek et al. (2014)). A third
condition under which senses cannot be discrimi-
nated is when the target word is used in an under-
specified or intentionally ambiguous manner. We
hold that for this third scenario a disambiguator’s
inability to discriminate senses should not be seen
as a failure condition, but rather as a limitation
of the WSD task as traditionally defined. By re-
framing the task so as to permit the assignment
of multiple senses (or groups of senses), we can
allow disambiguation systems to sense-annotate in-
tentionally ambiguous constructions such as puns.
Many approaches to WSD, including Lesk-like
algorithms, involve computing some score for all
possible senses of a target word, and then select-
ing the single highest-scoring one as the “correct”
sense. The most straightforward modification of
these techniques to pun disambiguation, then, is
to have the systems select the two top-scoring
senses, one for each meaning of the pun. Accord-
ingly we applied this modification to the following
knowledge-based WSD algorithms:
Simplified Lesk (Kilgarriff and Rosenzweig,
2000) disambiguates a target word by examin-
ing the definitions5 for each of its candidate
senses and selecting the single sense—or in
our case, the two senses—which have the
greatest number of words in common with
the context. As we previously demonstrated
that puns often transcend part of speech, our
pool of candidate senses is constructed as
follows: we apply a morphological analyzer
to recover all possible lemmas of the target
word without respect to part of speech, and
for each lemma we add all its senses to the
pool.
Simplified extended Lesk (Ponzetto and Navigli,
2010) is similar to simplified Lesk, except that
the definition for each sense is concatenated
with those of neighbouring senses in Word-
Net’s semantic network.
Simplified lexically expanded Lesk (Miller et
al., 2012) is also based on simplified Lesk,
with the extension that every word in the
context and sense definitions is expanded with
up to 100 entries from a large distributional
thesaurus.
The above algorithms fail to make a sense as-
signment when more than two senses are tied for
the highest lexical overlap, or when there is a single
highest-scoring sense but multiple senses are tied
for the second-highest overlap. We therefore de-
vised two pun-specific tie-breaking strategies. The
first is motivated by the informal observation that,
though the two meanings of a pun may have dif-
ferent parts of speech, at least one of the parts
</bodyText>
<footnote confidence="0.994317666666667">
5In our implementation, the sense definitions are formed
by concatenating the synonyms, gloss, and example sentences
provided by WordNet.
</footnote>
<page confidence="0.997677">
724
</page>
<bodyText confidence="0.999986954545454">
of speech is grammatical in the context of the
sentence, and so would probably be the one as-
signed by a stochastic or rule-based POS tagger.
Our “POS” tie-breaker therefore preferentially se-
lects the best sense, or pair of senses, whose POS
matches the one applied to the target by the Stan-
ford POS tagger (Toutanova et al., 2003). For our
second tie-breaking strategy, we posit that since
humour derives from the resolution of semantic in-
congruity (Raskin, 1985; Attardo, 1994), puns are
more likely to exploit coarse-grained homonymy
than than fine-grained systematic polysemy. Thus,
following Matuschek et al. (2014), we induced a
clustering of WordNet senses by aligning WordNet
to the more coarse-grained OmegaWiki LSR.6 Our
“cluster” fallback works the same as the “POS” one,
with the addition that any remaining ties among
senses with the second-highest overlap are resolved
by preferentially selecting those which are not in
the same induced cluster as, and which in Word-
Net’s semantic network are at least three edges
distant from, the sense with the highest overlap.
</bodyText>
<sectionHeader confidence="0.997873" genericHeader="method">
5 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.984143">
5.1 Scoring
</subsectionHeader>
<bodyText confidence="0.99980732">
In traditional word sense disambiguation, in vitro
evaluations are conducted by comparing the senses
assigned by the disambiguation system to the gold-
standard senses assigned by the human annotators.
For the case that the system and gold-standard as-
signments consist of a single sense each, the exact-
match criterion is used: the system receives a score
of 1 if it chose the sense specified by the gold stan-
dard, and 0 otherwise. Where the system selects
a single sense for an instance for which there is
more than one correct gold standard sense, the mul-
tiple tags are interpreted disjunctively—that is, the
system receives a score of 1 if it chose any one of
the gold-standard senses, and 0 otherwise. Overall
performance is reported in terms of coverage (the
number of targets for which a sense assignment
was attempted), precision (the sum of scores di-
vided by the number of attempted targets), recall
(the sum of scores divided by the total number of
targets in the data set), and F1 (the harmonic mean
of precision and recall) (Palmer et al., 2006).
The traditional approach to scoring individual
targets is not usable as-is for pun disambiguation,
because each pun carries two disjoint but equally
valid sets of sense annotations. Instead, since our
</bodyText>
<footnote confidence="0.843785">
6http://www.omegawiki.org/
</footnote>
<bodyText confidence="0.999538888888889">
systems assign exactly one sense to each of the
pun’s two sense sets, we count this as a match
(scoring 1) only if each chosen sense can be found
in one of the gold-standard sense sets, and no two
gold-standard sense sets contain the same chosen
sense. (As with traditional WSD scoring, various
approaches could be used to assign credit for par-
tially correct assignments, though we leave explo-
ration of these to future work.)
</bodyText>
<subsectionHeader confidence="0.998488">
5.2 Baselines
</subsectionHeader>
<bodyText confidence="0.974090909090909">
System performance in WSD is normally inter-
preted with reference to one or more baselines. To
our knowledge, ours is the very first study of auto-
matic pun disambiguation on any scale, so at this
point there are no previous systems against which
to compare our results. However, traditional WSD
systems are often compared with two naive base-
lines (Gale et al., 1992) which can be adapted for
our purposes.
The first of these naive baselines is to randomly
select from among the candidate senses. In tradi-
tional WSD, the score for a random disambiguator
which selects a single sense for a given target t is
the number of gold-standard senses divided by the
number of candidate senses: score(t) = g(t)÷δ(t).
In our pun disambiguation task, however, a ran-
dom disambiguator must select two senses—one
for each of the sense sets g1(t) and g2(t)—and
these senses must be distinct. There are (δ(t)) pos-
2
sible ways of selecting two unique senses, so the
random score for any given instance is score(t) =
</bodyText>
<equation confidence="0.960677">
g1(t) · g2(t) ÷ (δ(t)).
2
</equation>
<bodyText confidence="0.9998509375">
The second naive baseline for WSD, known as
most frequent sense (MFS), is a supervised base-
line, meaning that it depends on a manually sense-
annotated background corpus. As its name sug-
gests, it involves always selecting from the candi-
dates that sense which has the highest frequency in
the corpus. As with our test algorithms, we adapt
this technique to pun disambiguation by having
it select the two most frequent senses (according
to WordNet’s built-in sense frequency counts). In
traditional WSD, MFS baselines are notoriously
difficult to beat, even for supervised disambigua-
tion systems, and since they rely on expensive
sense-tagged data they are not normally considered
a benchmark for the performance of knowledge-
based disambiguators.
</bodyText>
<page confidence="0.994662">
725
</page>
<table confidence="0.999677125">
system C P R F1
SL 35.52 19.74 7.01 10.35
SEL 42.45 19.96 8.47 11.90
SLEL 98.69 13.43 13.25 13.34
SEL+POS 59.94 21.21 12.71 15.90
SEL+cluster 68.10 20.70 14.10 16.77
random 100.00 9.31 9.31 9.31
MFS 100.00 13.25 13.25 13.25
</table>
<tableCaption confidence="0.998457">
Table 1: Coverage, precision, recall, and F1 for
various pun diasmbiguation algorithms.
</tableCaption>
<sectionHeader confidence="0.999293" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999976011627907">
Using the freely available DKPro WSD framework
(Miller et al., 2013), we implemented our pun dis-
ambiguation algorithms, ran them on our full data
set, and compared their annotations against those
of our manually produced gold standard. Table 1
shows the coverage, precision, recall, and F1 for
simplified Lesk (SL), simplified extended Lesk
(SEL), simplified lexically expanded Lesk (SLEL),
and the random and most frequent sense baselines;
for SEL we also report results for each of our pun-
specific tie-breaking strategies. All metrics are
reported as percentages, and the highest score for
each metric (excluding baseline coverage, which is
always 100%) is highlighted in boldface.
Accuracy for the random baseline annotator was
about 9%; for the MFS baseline it was just over
13%. These figures are considerably lower than
what is typically seen with traditional WSD cor-
pora, where random baselines achieve accuracies of
30 to 60%, and MFS baselines 65 to 80% (Palmer
et al., 2001; Snyder and Palmer, 2004; Navigli et
al., 2007). Our baselines’ low figures are the re-
sult of them having to consider senses from every
possible lemmatization and part of speech of the
target, and underscore the difficulty of our task.
The simplest knowledge-based algorithm we
tested, simplified Lesk, was over twice as accu-
rate as the random baseline in terms of precision
(19.74%), but predictably had very low cover-
age (35.52%), leading in turn to very low recall
(7.01%). Manual examination of the unassigned
instances confirmed that failure was usually due
to the lack of any lexical overlap whatsoever be-
tween the context and definitions. The use of a
tie-breaking strategy would not help much here,
though some way of bridging the lexical gap would.
This is, in fact, the strategy employed by the ex-
tended and lexically expanded variants of simpli-
fied Lesk, and we observed that both were success-
ful to some degree. Simplified lexically expanded
Lesk almost completely closed the lexical gap, with
nearly complete coverage (98.69%), though this
came at the expense of a large drop in precision (to
13.43%). Given the near-total coverage, use of a tie-
breaking strategy here would have no appreciable
effect on the accuracy.
Simplified extended Lesk, on the other hand,
saw significant increases in coverage, precision,
and recall (to 42.45%, 19.96%, and 8.47%, respec-
tively). Its recall is statistically indistinguishable7
from the random baseline, though spot-checks of
its unassigned instances show that the problem is
very frequently not the lexical gap but rather mul-
tiple senses tied for the greatest overlap with the
context. We therefore tested our two pun-specific
backoff strategies to break this system’s ties. Us-
ing the “POS” strategy increased coverage by 41%,
relatively speaking, and gave us our highest ob-
served precision of 21.21%. Our “cluster” strategy
effected a relative increase in coverage of over 60%,
and gave us the best recall (14.10%). This strategy
also had the best tradeoff between precision and
recall, with an F1 of 16.77%.
Significance testing shows the recall scores for
SLEL, SEL+POS, and SEL+cluster to be signifi-
cantly better than the random baseline, and statisti-
cally indistinguishable from that of MFS. This is
excellent news, especially in light of the fact that
supervised approaches (even baselines like MFS)
usually outperform their knowledge-based counter-
parts. Though the three knowledge-based systems
are not statistically distinguishable from each other
in terms of recall, they do show a statistically sig-
nificant improvement over SL and SEL, and the
two implementing pun-specific tie-breaking strate-
gies were markedly more accurate than SLEL for
those targets where they attempted an assignment.
These two systems would therefore be preferable
for applications where precision is more important
than recall.
We also examined the results of our gener-
ally best-performing system, SEL+cluster, to see
whether there was any relationship with the targets’
part of speech. We filtered the results according to
whether both gold-standard meanings of the pun
contain senses for nouns only, verbs only, adjec-
</bodyText>
<footnote confidence="0.942526">
7All significance statements in this section are based on
McNemar’s test at a confidence level of 5%.
</footnote>
<page confidence="0.988945">
726
</page>
<table confidence="0.998834285714286">
POS C P R Rrand
noun 66.60 20.89 13.91 10.44
verb 65.61 14.54 9.54 5.12
adj. 68.87 39.73 27.36 16.84
adv. 100.00 75.00 75.00 46.67
pure 66.77 21.44 14.31 9.56
mult. 72.58 18.43 13.38 12.18
</table>
<tableCaption confidence="0.849768333333333">
Table 2: Coverage, precision, and recall for
SEL+cluster, and random baseline recall, accord-
ing to part of speech.
</tableCaption>
<bodyText confidence="0.999777481481481">
tives only, or adverbs only; these amounted to 539,
346, 106, and 8 instances, respectively. These re-
sults are shown in Table 2. Also shown there is a
row which aggregates the 999 targets with “pure”
POS, and another for the remaining 608 instances
(“mult.”), where one or both of the two mean-
ings contain senses for multiple parts of speech,
or where the two meanings have different parts of
speech. The last column of each row shows the
recall of the random baseline for comparison.
Accuracy was lowest on the verbs, which had the
highest candidate polysemy (21.6) and are known
to be particularly difficult to disambiguate even in
traditional WSD. Still, as with all the other sin-
gle parts of speech, performance of SEL+cluster
exceeded the random baseline. While recall was
lower on targets with mixed POS than those with
pure POS, coverage was significantly higher. Nor-
mally such a disparity could be attributed to a dif-
ference in polysemy: Lesk-like systems are more
likely to attempt a sense assignment for highly pol-
ysemous targets, since there is a greater likelihood
of one of the candidate definitions matching the
context, though the probability of the assignment
being correct is reduced. In this case, however,
the multi-POS targets actually had lower average
polysemy than the single-POS ones (13.2 vs. 15.8).
</bodyText>
<sectionHeader confidence="0.998497" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999756441860465">
In this paper we have introduced the novel task of
pun disambiguation and have proposed and evalu-
ated several computational approaches for it. The
major contributions of this work are as follows:
First, we have produced a new data set consisting
of manually sense-annotated homographic puns.
The data set is large enough, and the manual an-
notations reliable enough, for a principled eval-
uation of automatic pun disambiguation systems.
Second, we have shown how evaluation metrics,
baselines, and disambiguation algorithms from tra-
ditional WSD can be adapted to the task of pun
disambiguation, and we have tested these adapta-
tions in a controlled experiment. The results show
pun disambiguation to be a particularly challeng-
ing task for NLP, with baseline results far below
what is commonly seen in traditional WSD. We
showed that knowledge-based disambiguation al-
gorithms naively adapted from traditional WSD
perform poorly, but that extending them with strate-
gies that rely on pun-specific features brings about
dramatic improvements in accuracy: their recall be-
comes comparable to that of a supervised baseline,
and their precision greatly exceeds it.
There are a number of avenues we intend to ex-
plore in future work. First, we would like to try
adapting and evaluating some additional WSD al-
gorithms for use with puns. Though our data set is
probably too small to use with machine learning–
based approaches, we are particularly interested
in testing knowledge-based disambiguators which
rely on measures of graph connectivity rather than
gloss overlaps. Second, we would like to investi-
gate alternative tie-breaking strategies, such as the
domain similarity measures used by Mihalcea et
al. (2010). Finally, whereas in this paper we have
treated only the task of sense disambiguation for
the case where a word is known a priori to be a
pun, we are interested in exploring the requisite
problem of pun detection, where the object is to
determine whether or not a given context contains
a pun, and more precisely whether any given word
in a context is a pun.
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9914425">
The work described in this paper is supported
by the Volkswagen Foundation as part of the
Lichtenberg Professorship Program under grant
No. I/82806. The authors thank John Black,
Matthew Collins, Don Hauptman, Christian
F. Hempelmann, Stan Kegel, Andrew Lamont,
Beatrice Santorini, Mladen Turkovi´c, and Andreas
Zimpfer for helping us build our data set.
</bodyText>
<sectionHeader confidence="0.998605" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9927385">
Salvatore Attardo. 1994. Linguistic Theories of Humor.
Mouton de Gruyter.
Pierpaolo Basile, Annalina Caputo, and Giovanni Se-
meraro. 2014. An enhanced Lesk word sense disam-
</reference>
<page confidence="0.986059">
727
</page>
<reference confidence="0.992473943396226">
biguation algorithm through a distributional seman-
tic model. In Proceedings of the 25th International
Conference on Computational Linguistics (COLING
2014), pages 1591–1600.
Tristan A. Bekinschtein, Matthew H. Davis, Jennifer M.
Rodd, and Adrian M. Owen. 2011. Why clowns
taste funny: The relationship between humor and
semantic ambiguity. The Journal of Neuroscience,
31(26):9665–9671, June.
Nancy D. Bell, Scott Crossley, and Christian F. Hempel-
mann. 2011. Wordplay in church marquees. Humor:
International Journal of Humor Research, 24(2):187–
202, April.
Chiara Bucaria. 2004. Lexical and syntactic ambigu-
ity as a source of humor: The case of newspaper
headlines. Humor: International Journal of Humor
Research, 17(3):279–309.
Paul Buitelaar. 2000. Reducing lexical semantic com-
plexity with systematic polysemous classes and un-
derspecification. In Proceedings of the 2000 NAACL-
ANLP Workshop on Syntactic and Semantic Com-
plexity in Natural Language Processing Systems, vol-
ume 1, pages 14–19.
Jonathan D. Culler, editor. 1988. On Puns: The Foun-
dation of Letters. Basil Blackwell, Oxford.
Dirk Delabastita, editor. 1997. Traductio: Essays on
Punning and Translation. St. Jerome, Manchester.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
MA.
William Gale, Kenneth Ward Church, and David
Yarowsky. 1992. Estimating upper and lower bounds
on the performance of word-sense disambiguation
programs. In Proceedings of the 30th Annual Meet-
ing of the Association of Computational Linguistics
(ACL 1992), pages 249–256.
Christian F. Hempelmann. 2003. Paronomasic Puns:
Target Recoverability Towards Automatic Generation.
Ph.D. thesis, Purdue University.
Bryan Anthony Hong and Ethel Ong. 2009. Automati-
cally extracting word relationships as templates for
pun generation. In Proceedings of the 1st Workshop
on Computational Approaches to Linguistic Creativ-
ity (CALC 2009), pages 24–31, June.
David Jurgens and Ioannis Klapaftis. 2013. SemEval-
2013 Task 13: Word sense induction for graded and
non-graded senses. In Proceedings of the 7th Interna-
tional Workshop on Semantic Evaluation (SemEval
2013), pages 290–299, June.
David Jurgens. 2014. An analysis of ambiguity in
word sense annotations. In Proceedings of the 9th In-
ternational Conference on Language Resources and
Evaluation (LREC 2014), pages 3006–3012, May.
Nora Kaplan and Teresa Lucas. 2001. Comprensi´on
del humorismo en ingl´es: Estudio de las estrategias
de inferencia utilizadas por estudiantes avanzados de
ingl´es como lengua extranjera en la interpretaci´on
de los retru´ecanos en historietas c´omicas en lengua
inglesa. Anales de la Universidad Metropolitana,
1(2):245–258.
Stefan Daniel Keller. 2009. The Development of Shake-
speare’s Rhetoric: A Study of Nine Plays, volume
136 of Swiss Studies in English. Narr, T¨ubingen.
Adam Kilgarriff and Joseph Rosenzweig. 2000. Frame-
work and results for English SENSEVAL. Computers
and the Humanities, 34:15–48.
Klaus Krippendorff. 1980. Content Analysis: An In-
troduction to its Methodology. Sage, Beverly Hills,
CA.
Luuk Lagerwerf. 2002. Deliberate ambiguity in slo-
gans: Recognition and appreciation. Document De-
sign, 3(3):245–260.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: How to tell a
pine cone from a ice cream cone. In Proceedings of
the 5th Annual International Conference of Systems
Documentation (SIGDOC 1986), pages 24–26.
Greg Lessard, Michael Levison, and Chris Venour.
2002. Cleverness versus funniness. In Proceedings
of the 20th Twente Workshop on Language Technol-
ogy, pages 137–145.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 36th An-
nual Meeting of the Association for Computational
Linguistics (ACL 1998) and the 17th International
Conference on Computational Linguistics (COLING
1998), volume 2, pages 768–774.
Teresa Lucas. 2004. Deciphering the Meaning of Puns
in Learning English as a Second Language: A Study
of Triadic Interaction. Ph.D. thesis, Florida State
University.
Peter J. Ludlow. 1996. Semantic Ambiguity and Under-
specification (review). Computational Linguistics,
3(23):476–482.
Michael Matuschek, Tristan Miller, and Iryna Gurevych.
2014. A language-independent sense clustering ap-
proach for enhanced WSD. In Proceedings of the
12th Konferenz zur Verarbeitung nat¨urlicher Sprache
(KONVENS 2014), pages 11–21, October.
Christian M. Meyer, Margot Mieskes, Christian Stab,
and Iryna Gurevych. 2014. DKPro Agreement: An
open-source Java library for measuring inter-rater
agreement. In Proceedings of the 25th International
Conference on Computational Linguistics (System
Demonstrations) (COLING 2014), pages 105–109,
August.
</reference>
<page confidence="0.973674">
728
</page>
<reference confidence="0.999802435185185">
Rada Mihalcea and Carlo Strapparava. 2005. Mak-
ing computers laugh: Investigations in automatic hu-
mor recognition. In Proceedings of the 11th Human
Language Technology Conference and the 10th Con-
ference on Empirical Methods in Natural Language
Processing (HLT-EMNLP 2005), pages 531–538, Oc-
tober.
Rada Mihalcea and Carlo Strapparava. 2006. Learn-
ing to laugh (automatically): Computational models
for humor recognition. Computational Intelligence,
22(2):126–142.
Rada Mihalcea, Carlo Strapparava, and Stephen Pul-
man. 2010. Computational models for incongruity
detection in humour. In Proceedings of the 11th
International Conference on Computational Linguis-
tics and Intelligent Text Processing (CICLing 2010),
volume 6008 of Lecture Notes in Computer Science,
pages 364–374. Springer, March.
Tristan Miller and Mladen Turkovi´c. 2015. Towards
the automatic detection and identification of English
puns. European Journal of Humour Research. To
appear.
Tristan Miller, Chris Biemann, Torsten Zesch, and Iryna
Gurevych. 2012. Using distributional similarity for
lexical expansion in knowledge-based word sense
disambiguation. In Proceedings of the 24th Inter-
national Conference on Computational Linguistics
(COLING 2012), pages 1781–1796, December.
Tristan Miller, Nicolai Erbs, Hans-Peter Zorn, Torsten
Zesch, and Iryna Gurevych. 2013. DKPro WSD: A
generalized UIMA-based framework for word sense
disambiguation. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (System Demonstrations) (ACL 2013), pages
37–42, August.
Michel Monnot. 1982. Puns in advertising: Ambiguity
as verbal aggression. Maledicta, 6:7–20.
Roberto Navigli, Kenneth C. Litkowski, and Orin Har-
graves. 2007. SemEval-2007 Task 07: Coarse-
grained English All-words Task. In Proceedings
of the 4th International Workshop on Semantic Eval-
uations (SemEval-2007), pages 30–35, June.
Martha Palmer, Christiane Fellbaum, Scott Cotton, Lau-
ren Delfs, and Hoa Trang Dang. 2001. English tasks:
All-words and verb lexical sample. In Proceedings
of Senseval-2: 2nd International Workshop on Eval-
uating Word Sense Disambiguation Systems, pages
21–24, July.
Martha Palmer, Hwee Tou Ng, and Hoa Trang Dang.
2006. Evaluation of WSD systems. In Eneko Agirre
and Philip Edmonds, editors, Word Sense Disam-
biguation: Algorithms and Applications, volume 33
of Text, Speech, and Language Technology. Springer.
Rebecca J. Passonneau, Nizar Habash, and Owen Ram-
bow. 2006. Inter-annotator agreement on a multilin-
gual semantic annotation task. In Proceedings of the
5th International Conference on Language Resources
and Evaluations (LREC 2006), pages 1951–1956.
Rebecca J. Passonneau. 2006. Measuring agreement on
set-valued items (MASI) for semantic and pragmatic
annotation. In Proceedings of the 5th International
Conference on Language Resources and Evaluations
(LREC 2006), pages 831–836.
Simone Paolo Ponzetto and Roberto Navigli. 2010.
Knowledge-rich word sense disambiguation rivaling
supervised systems. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics (ACL 2010), pages 1522–1531.
Vitor Raskin. 1985. Semantic Mechanisms of Humor.
D. Reidel, Dordrecht, the Netherlands.
Walter Redfern. 1984. Puns. Basil Blackwell, Oxford.
Thorsten Schr¨oter. 2005. Shun the Pun, Rescue the
Rhyme? The Dubbing and Subtitling of Language-
Play in Film. Ph.D. thesis, Karlstad University.
Benjamin Snyder and Martha Palmer. 2004. The En-
glish all-words task. In Proceedings of the 3rd In-
ternational Workshop on the Evaluation of Systems
for the Semantic Analysis of Text (Senseval-3), pages
41–43, July.
Julia M. Taylor and Lawrence J. Mazlack. 2004. Com-
putationally recognizing wordplay in jokes. In Pro-
ceedings of the 26th Annual Conference of the Cog-
nitive Science Society (CogSci 2004), pages 1315–
1320, August.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of the 3rd Conference of the North American
Chapter of the Association for Computational Lin-
guistics and the 9th Human Language Technologies
Conference (HLT-NAACL 2003), pages 252–259.
Alessandro Valitutti, Carlo Strapparava, and Oliviero
Stock. 2008. Textual affect sensing for computa-
tional advertising. In Proceedings of the AAAI Spring
Symposium on Creative Intelligent Systems, pages
117–122, March.
Chris Venour. 1999. The computational generation of
a class of puns. Master’s thesis, Queen’s University,
Kingston, ON.
Toshihiko Yokogawa. 2002. Japanese pun analyzer
using articulation similarities. In Proceedings of
the 11th IEEE International Conference on Fuzzy
Systems (FUZZ 2002), volume 2, pages 1114–1119,
May.
Arnold M. Zwicky and Elizabeth D. Zwicky. 1986.
Imperfect puns, markedness, and phonological simi-
larity: With fronds like these, who needs anemones?
Folia Linguistica, 20(3–4):493–503.
</reference>
<page confidence="0.998538">
729
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.635739">
<title confidence="0.999803">Automatic disambiguation of English puns</title>
<author confidence="0.917298">Tristan Miller</author>
<author confidence="0.917298">Iryna</author>
<affiliation confidence="0.8385425">Ubiquitous Knowledge Processing Lab Department of Computer Science, Technische Universit¨at</affiliation>
<web confidence="0.99944">https://www.ukp.tu-darmstadt.de/</web>
<abstract confidence="0.99722605">Traditional approaches to word sense disambiguation (WSD) rest on the assumption that there exists a single, unambiguous communicative intention underlying every word in a document. However, writers sometimes intend for a word to be interpreted as simultaneously carrying multiple distinct meanings. This deliberate of lexical punning— is a particularly common source of humour. In this paper we describe how traditional, language-agnostic WSD approaches can be adapted to “disambiguate” puns, or rather to identify their double meanings. We evaluate several such approaches on a manually sense-annotated collection of English puns and observe performance exceeding that of some knowledge-based and supervised baselines.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Salvatore Attardo</author>
</authors>
<title>Linguistic Theories of Humor. Mouton de Gruyter.</title>
<date>1994</date>
<contexts>
<context position="7510" citStr="Attardo, 1994" startWordPosition="1160" endWordPosition="1161">d social sciences, which has led to insights into the nature of language-based humour and wordplay, including their role in commerce, entertainment, and health care; how they are processed in the brain; and how they vary over time and across cultures (Monnot, 1982; Culler, 1988; Lagerwerf, 2002; Bell et al., 2011; Bekinschtein et al., 2011). Study of literary puns imparts a greater understanding of the cultural or historical context in which the literature was produced, which is often necessary to properly interpret and translate it (Delabastita, 1997). Puns can be classified in various ways (Attardo, 1994), though from the point of view of our particular natural language processing application the most important distinction is between homographic and homophonic puns. A homographic pun exploits distinct meanings of the same written word, and a homophonic pun exploits distinct meanings of the same spoken word. Puns can be homographic, homophonic, both, or neither, as the following examples illustrate: (1) A lumberjack’s world revolves on its axes. (2) She fell through the window but felt no pane. (3) A political prisoner is one who stands behind her convictions. (4) The sign at the nudist camp re</context>
<context position="19677" citStr="Attardo, 1994" startWordPosition="3123" endWordPosition="3124">verbs. This criterion is important because, in our observations, it is often only one word which carries ambiguity in puns on compounds and multi-word expressions. Accepting lexical units containing more than one content word would have required our annotators to laboriously partition the pun into (possibly overlapping) sense-bearing units and to assign sense sets to each of them, inflating the complexity of the annotation task to unacceptable levels. Two meanings per pun: The pun must have exactly two distinct meanings. Though many sources state that puns have only two senses (Redfern, 1984; Attardo, 1994), our annotators identified a handful of corpus examples where the pun could plausibly be analyzed as carrying three distinct meanings. To simplify our manual annotation procedure and our evaluation metrics we excluded these rare outliers. Weak homography: The lexical units corresponding to the two distinct meanings must be spelled exactly the same way, except that particles and inflections may be disregarded. This somewhat softer definition of homography allows us to admit a good many morphologically interesting cases which were nonetheless readily recognized by our human annotators. The filt</context>
<context position="27873" citStr="Attardo, 1994" startWordPosition="4436" endWordPosition="4437">r implementation, the sense definitions are formed by concatenating the synonyms, gloss, and example sentences provided by WordNet. 724 of speech is grammatical in the context of the sentence, and so would probably be the one assigned by a stochastic or rule-based POS tagger. Our “POS” tie-breaker therefore preferentially selects the best sense, or pair of senses, whose POS matches the one applied to the target by the Stanford POS tagger (Toutanova et al., 2003). For our second tie-breaking strategy, we posit that since humour derives from the resolution of semantic incongruity (Raskin, 1985; Attardo, 1994), puns are more likely to exploit coarse-grained homonymy than than fine-grained systematic polysemy. Thus, following Matuschek et al. (2014), we induced a clustering of WordNet senses by aligning WordNet to the more coarse-grained OmegaWiki LSR.6 Our “cluster” fallback works the same as the “POS” one, with the addition that any remaining ties among senses with the second-highest overlap are resolved by preferentially selecting those which are not in the same induced cluster as, and which in WordNet’s semantic network are at least three edges distant from, the sense with the highest overlap. 5</context>
</contexts>
<marker>Attardo, 1994</marker>
<rawString>Salvatore Attardo. 1994. Linguistic Theories of Humor. Mouton de Gruyter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierpaolo Basile</author>
<author>Annalina Caputo</author>
<author>Giovanni Semeraro</author>
</authors>
<title>An enhanced Lesk word sense disambiguation algorithm through a distributional semantic model.</title>
<date>2014</date>
<booktitle>In Proceedings of the 25th International Conference on Computational Linguistics (COLING 2014),</booktitle>
<pages>1591--1600</pages>
<contexts>
<context position="11452" citStr="Basile et al., 2014" startWordPosition="1782" endWordPosition="1785"> the Lesk algorithm (Lesk, 1986) which disambiguates a pair of target terms in context by comparing their respective dictionary definitions and selecting the two with the greatest number of words in common. Though simple, the Lesk algorithm performs surprisingly well, and has frequently served as the basis of more sophisticated approaches. In recent years, Lesk variants in which the contexts and definitions are supplemented with entries from a distributional thesaurus (Lin, 1998) have achieved state-of-the-art performance for knowledge-based systems on standard data sets (Miller et al., 2012; Basile et al., 2014). In traditional word sense disambiguation, the part of speech and lemma of the target word are usually known a priori, or can be determined with high accuracy using off-the-shelf natural language processing tools. The pool of candidate senses can therefore be restricted to those whose lexicalizations exactly match the target lemma and part of speech. No such help is available for puns, at least not in the general case. Take the following two examples: (5) Tom moped. (6) “I want a scooter,” Tom moped. In the first of these sentences, the word moped is unambiguously a verb with the lemma mope, </context>
</contexts>
<marker>Basile, Caputo, Semeraro, 2014</marker>
<rawString>Pierpaolo Basile, Annalina Caputo, and Giovanni Semeraro. 2014. An enhanced Lesk word sense disambiguation algorithm through a distributional semantic model. In Proceedings of the 25th International Conference on Computational Linguistics (COLING 2014), pages 1591–1600.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tristan A Bekinschtein</author>
<author>Matthew H Davis</author>
<author>Jennifer M Rodd</author>
<author>Adrian M Owen</author>
</authors>
<title>Why clowns taste funny: The relationship between humor and semantic ambiguity.</title>
<date>2011</date>
<journal>The Journal of Neuroscience,</journal>
<volume>31</volume>
<issue>26</issue>
<contexts>
<context position="7238" citStr="Bekinschtein et al., 2011" startWordPosition="1116" endWordPosition="1119">gs simultaneously. Humorous and non2The problem is compounded in audio-visual media such as films; often one or both of the pun’s meanings appears in the visual channel, and thus cannot be freely substituted. humorous puns have been the subject of extensive study in the humanities and social sciences, which has led to insights into the nature of language-based humour and wordplay, including their role in commerce, entertainment, and health care; how they are processed in the brain; and how they vary over time and across cultures (Monnot, 1982; Culler, 1988; Lagerwerf, 2002; Bell et al., 2011; Bekinschtein et al., 2011). Study of literary puns imparts a greater understanding of the cultural or historical context in which the literature was produced, which is often necessary to properly interpret and translate it (Delabastita, 1997). Puns can be classified in various ways (Attardo, 1994), though from the point of view of our particular natural language processing application the most important distinction is between homographic and homophonic puns. A homographic pun exploits distinct meanings of the same written word, and a homophonic pun exploits distinct meanings of the same spoken word. Puns can be homogra</context>
</contexts>
<marker>Bekinschtein, Davis, Rodd, Owen, 2011</marker>
<rawString>Tristan A. Bekinschtein, Matthew H. Davis, Jennifer M. Rodd, and Adrian M. Owen. 2011. Why clowns taste funny: The relationship between humor and semantic ambiguity. The Journal of Neuroscience, 31(26):9665–9671, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy D Bell</author>
<author>Scott Crossley</author>
<author>Christian F Hempelmann</author>
</authors>
<title>Wordplay in church marquees.</title>
<date>2011</date>
<journal>Humor: International Journal of Humor Research,</journal>
<volume>24</volume>
<issue>2</issue>
<pages>202</pages>
<contexts>
<context position="7210" citStr="Bell et al., 2011" startWordPosition="1112" endWordPosition="1115"> independent meanings simultaneously. Humorous and non2The problem is compounded in audio-visual media such as films; often one or both of the pun’s meanings appears in the visual channel, and thus cannot be freely substituted. humorous puns have been the subject of extensive study in the humanities and social sciences, which has led to insights into the nature of language-based humour and wordplay, including their role in commerce, entertainment, and health care; how they are processed in the brain; and how they vary over time and across cultures (Monnot, 1982; Culler, 1988; Lagerwerf, 2002; Bell et al., 2011; Bekinschtein et al., 2011). Study of literary puns imparts a greater understanding of the cultural or historical context in which the literature was produced, which is often necessary to properly interpret and translate it (Delabastita, 1997). Puns can be classified in various ways (Attardo, 1994), though from the point of view of our particular natural language processing application the most important distinction is between homographic and homophonic puns. A homographic pun exploits distinct meanings of the same written word, and a homophonic pun exploits distinct meanings of the same spok</context>
<context position="16347" citStr="Bell et al. (2011)" startWordPosition="2579" endWordPosition="2582">e corpora of intentional lexical ambiguity which have been used in past work, usually in linguistics or the social sciences. In their work on computer-generated humour, Lessard et al. (2002) use a corpus of 374 “Tom Swifty” puns taken from the Internet, plus a well-balanced corpus of 50 humorous and nonhumorous lexical ambiguities generated programmatically (Venour, 1999). Hong and Ong (2009) also study humour in natural language generation, using a smaller data set of 27 punning riddles derived from a mix of natural and artificial sources. In their study of wordplay in religious advertising, Bell et al. (2011) compile a corpus of 373 puns taken from church marquees and literature, and compare it against a general corpus of 1515 puns drawn from Internet websites and a specialized dictionary. Zwicky and Zwicky (1986) conduct a phonological analysis on a corpus of several thousand puns, some of which they collected themselves from advertisements and catalogues, and the remainder of which were taken from previously published collections. Two studies on cognitive strategies used by second language learners (Kaplan and Lucas, 2001; Lucas, 2004) used a data set of 58 jokes compiled from newspaper comics, </context>
</contexts>
<marker>Bell, Crossley, Hempelmann, 2011</marker>
<rawString>Nancy D. Bell, Scott Crossley, and Christian F. Hempelmann. 2011. Wordplay in church marquees. Humor: International Journal of Humor Research, 24(2):187– 202, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chiara Bucaria</author>
</authors>
<title>Lexical and syntactic ambiguity as a source of humor: The case of newspaper headlines.</title>
<date>2004</date>
<journal>Humor: International Journal of Humor Research,</journal>
<volume>17</volume>
<issue>3</issue>
<contexts>
<context position="17000" citStr="Bucaria (2004)" startWordPosition="2687" endWordPosition="2688">m church marquees and literature, and compare it against a general corpus of 1515 puns drawn from Internet websites and a specialized dictionary. Zwicky and Zwicky (1986) conduct a phonological analysis on a corpus of several thousand puns, some of which they collected themselves from advertisements and catalogues, and the remainder of which were taken from previously published collections. Two studies on cognitive strategies used by second language learners (Kaplan and Lucas, 2001; Lucas, 2004) used a data set of 58 jokes compiled from newspaper comics, 32 of which rely on lexical ambiguity. Bucaria (2004) conducts a linguistic analysis of a set of 135 humorous newspaper headlines, about half of which exploit lexical ambiguity. Such data sets—particularly the larger ones— provided us good evidence that intentionally lexical ambiguous exemplars exist in sufficient numbers to make a rigorous evaluation of our task feasible. Unfortunately, none of the above-mentioned corpora have been published in full, and moreover many of them contain (sometimes exclusively) the sort of imperfect or otherwise heterographic puns which we mean to exclude from consideration. This has motivated us to produce our own</context>
</contexts>
<marker>Bucaria, 2004</marker>
<rawString>Chiara Bucaria. 2004. Lexical and syntactic ambiguity as a source of humor: The case of newspaper headlines. Humor: International Journal of Humor Research, 17(3):279–309.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Buitelaar</author>
</authors>
<title>Reducing lexical semantic complexity with systematic polysemous classes and underspecification.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2000 NAACLANLP Workshop on Syntactic and Semantic Complexity in Natural Language Processing Systems,</booktitle>
<volume>1</volume>
<pages>14--19</pages>
<contexts>
<context position="1953" citStr="Buitelaar, 2000" startWordPosition="294" endWordPosition="295">his assumption, lexical ambiguity arises due to there being a plurality of words with the same surface form but different meanings, and the task of the interpreter is to select correctly among them. An alternative view is that each word is a single lexical entry whose specific meaning is underspecified until it is activated by the context (Ludlow, 1996). In the case of systematically polysemous terms (i.e., words that have several related senses shared in a systematic way by a group of similar words), it may not be necessary to disambiguate them at all in order to interpret the communication (Buitelaar, 2000). While there has been some research in modelling intentional lexical-semantic underspecification (Jurgens, 2014), it is intended for closely related senses such as those of systematically polysemous terms, not those of coarser-grained homonyms which are the subject of this paper. as paronomasia and syllepsis, or more generally as puns, in which homonymic (i.e., coarse-grained) lexical-semantic ambiguity is a deliberate effect of the communication act. That is, the writer intends for a certain word or other lexical item to be interpreted as simultaneously carrying two or more separate meanings</context>
</contexts>
<marker>Buitelaar, 2000</marker>
<rawString>Paul Buitelaar. 2000. Reducing lexical semantic complexity with systematic polysemous classes and underspecification. In Proceedings of the 2000 NAACLANLP Workshop on Syntactic and Semantic Complexity in Natural Language Processing Systems, volume 1, pages 14–19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan D Culler</author>
<author>editor</author>
</authors>
<title>On Puns: The Foundation of Letters.</title>
<date>1988</date>
<publisher>Basil Blackwell,</publisher>
<location>Oxford.</location>
<marker>Culler, editor, 1988</marker>
<rawString>Jonathan D. Culler, editor. 1988. On Puns: The Foundation of Letters. Basil Blackwell, Oxford.</rawString>
</citation>
<citation valid="true">
<date>1997</date>
<booktitle>Traductio: Essays on Punning and Translation. St.</booktitle>
<editor>Dirk Delabastita, editor.</editor>
<location>Jerome, Manchester.</location>
<marker>1997</marker>
<rawString>Dirk Delabastita, editor. 1997. Traductio: Essays on Punning and Translation. St. Jerome, Manchester.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Gale</author>
<author>Kenneth Ward Church</author>
<author>David Yarowsky</author>
</authors>
<title>Estimating upper and lower bounds on the performance of word-sense disambiguation programs.</title>
<date>1992</date>
<booktitle>In Proceedings of the 30th Annual Meeting of the Association of Computational Linguistics (ACL</booktitle>
<pages>249--256</pages>
<contexts>
<context position="30585" citStr="Gale et al., 1992" startWordPosition="4883" endWordPosition="4886">nd no two gold-standard sense sets contain the same chosen sense. (As with traditional WSD scoring, various approaches could be used to assign credit for partially correct assignments, though we leave exploration of these to future work.) 5.2 Baselines System performance in WSD is normally interpreted with reference to one or more baselines. To our knowledge, ours is the very first study of automatic pun disambiguation on any scale, so at this point there are no previous systems against which to compare our results. However, traditional WSD systems are often compared with two naive baselines (Gale et al., 1992) which can be adapted for our purposes. The first of these naive baselines is to randomly select from among the candidate senses. In traditional WSD, the score for a random disambiguator which selects a single sense for a given target t is the number of gold-standard senses divided by the number of candidate senses: score(t) = g(t)÷δ(t). In our pun disambiguation task, however, a random disambiguator must select two senses—one for each of the sense sets g1(t) and g2(t)—and these senses must be distinct. There are (δ(t)) pos2 sible ways of selecting two unique senses, so the random score for an</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>William Gale, Kenneth Ward Church, and David Yarowsky. 1992. Estimating upper and lower bounds on the performance of word-sense disambiguation programs. In Proceedings of the 30th Annual Meeting of the Association of Computational Linguistics (ACL 1992), pages 249–256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian F Hempelmann</author>
</authors>
<title>Paronomasic Puns: Target Recoverability Towards Automatic Generation.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>Purdue University.</institution>
<contexts>
<context position="12722" citStr="Hempelmann, 2003" startWordPosition="1997" endWordPosition="1998">matic lemmatizer and part-of-speech tagger. The moped of the second example is a pun, one of whose meanings is the same inflected form of the verb mope (“to sulk”) and the other of which is the noun moped (“motorized scooter”). For such cases an automated pun identifier would therefore need to account for all possible lemmas for all possible parts of speech of the target word. The situation becomes even more onerous for heterographic and imperfect puns, which may require the use of pronunciation dictionaries, and application of phonological theories of punning, in order to recover the lemmas (Hempelmann, 2003). As our research interests are in lexical semantics rather than phonology, we focus on puns which are homographic and monolexemic. This allows us to investigate the problem of pun identification in as controlled a setting as possible. 2.3 Previous work 2.3.1 Computational humour There is some previous research on computational detection and comprehension of humour, though by and large it is not concerned specifically with puns; those studies which do analyze puns tend to have a phonological or syntactic rather than semantic bent. In this subsection we briefly review some prior work which is r</context>
</contexts>
<marker>Hempelmann, 2003</marker>
<rawString>Christian F. Hempelmann. 2003. Paronomasic Puns: Target Recoverability Towards Automatic Generation. Ph.D. thesis, Purdue University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bryan Anthony Hong</author>
<author>Ethel Ong</author>
</authors>
<title>Automatically extracting word relationships as templates for pun generation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 1st Workshop on Computational Approaches to Linguistic Creativity (CALC</booktitle>
<pages>24--31</pages>
<contexts>
<context position="16124" citStr="Hong and Ong (2009)" startWordPosition="2540" endWordPosition="2543">possible definitions was maximized. The overlap scores for all word pairs were then averaged, and the punchline with the lowest average score selected as the most humorous. 2.3.2 Corpora There are a number of English-language corpora of intentional lexical ambiguity which have been used in past work, usually in linguistics or the social sciences. In their work on computer-generated humour, Lessard et al. (2002) use a corpus of 374 “Tom Swifty” puns taken from the Internet, plus a well-balanced corpus of 50 humorous and nonhumorous lexical ambiguities generated programmatically (Venour, 1999). Hong and Ong (2009) also study humour in natural language generation, using a smaller data set of 27 punning riddles derived from a mix of natural and artificial sources. In their study of wordplay in religious advertising, Bell et al. (2011) compile a corpus of 373 puns taken from church marquees and literature, and compare it against a general corpus of 1515 puns drawn from Internet websites and a specialized dictionary. Zwicky and Zwicky (1986) conduct a phonological analysis on a corpus of several thousand puns, some of which they collected themselves from advertisements and catalogues, and the remainder of </context>
</contexts>
<marker>Hong, Ong, 2009</marker>
<rawString>Bryan Anthony Hong and Ethel Ong. 2009. Automatically extracting word relationships as templates for pun generation. In Proceedings of the 1st Workshop on Computational Approaches to Linguistic Creativity (CALC 2009), pages 24–31, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Jurgens</author>
<author>Ioannis Klapaftis</author>
</authors>
<title>SemEval2013 Task 13: Word sense induction for graded and non-graded senses.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>290--299</pages>
<contexts>
<context position="22158" citStr="Jurgens and Klapaftis, 2013" startWordPosition="3516" endWordPosition="3519">metric (Passonneau, 2006). Whereas standard MASI, dm(A,B), compares two annotation sets A and B, our annotations take the form of unordered pairs of sets {A1,A2} and {B1,B2}. We therefore find the mapping between elements of the two pairs that gives the lowest total distance, and halve it: dm,({A1,A2},{B1,B2}) = 21min(dm(A1,B1)+ dm(A2,B2),dm(A1,B2) + dm(A2,B1)). With this method we observe a Krippendorff’s α of 0.777; this is only slightly below the 0.8 threshold recommended by Krippendorff, and far higher than what has been reported in other sense annotation studies (Passonneau et al., 2006; Jurgens and Klapaftis, 2013). Where possible, we resolved sense annotation disagreements automatically by taking the intersection of corresponding sense sets. Where the annotators’ sense sets were disjoint or contradictory (including the cases where the annotators disagreed on the pun word), we had a human adjudicator attempt to resolve the disagreement in favour of one annotator or the other. This left us with 1607 instances,4 of which we retained only the 1298 that had successful (i.e., not marked as unassignable) 4Pending clearance of the distribution rights, we will make some or all of our annotated data set availabl</context>
</contexts>
<marker>Jurgens, Klapaftis, 2013</marker>
<rawString>David Jurgens and Ioannis Klapaftis. 2013. SemEval2013 Task 13: Word sense induction for graded and non-graded senses. In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013), pages 290–299, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Jurgens</author>
</authors>
<title>An analysis of ambiguity in word sense annotations.</title>
<date>2014</date>
<booktitle>In Proceedings of the 9th International Conference on Language Resources and Evaluation (LREC 2014),</booktitle>
<pages>3006--3012</pages>
<contexts>
<context position="2066" citStr="Jurgens, 2014" startWordPosition="308" endWordPosition="309">ferent meanings, and the task of the interpreter is to select correctly among them. An alternative view is that each word is a single lexical entry whose specific meaning is underspecified until it is activated by the context (Ludlow, 1996). In the case of systematically polysemous terms (i.e., words that have several related senses shared in a systematic way by a group of similar words), it may not be necessary to disambiguate them at all in order to interpret the communication (Buitelaar, 2000). While there has been some research in modelling intentional lexical-semantic underspecification (Jurgens, 2014), it is intended for closely related senses such as those of systematically polysemous terms, not those of coarser-grained homonyms which are the subject of this paper. as paronomasia and syllepsis, or more generally as puns, in which homonymic (i.e., coarse-grained) lexical-semantic ambiguity is a deliberate effect of the communication act. That is, the writer intends for a certain word or other lexical item to be interpreted as simultaneously carrying two or more separate meanings, or alternatively for it to be unclear which meaning is the intended one. There are a variety of motivations wri</context>
</contexts>
<marker>Jurgens, 2014</marker>
<rawString>David Jurgens. 2014. An analysis of ambiguity in word sense annotations. In Proceedings of the 9th International Conference on Language Resources and Evaluation (LREC 2014), pages 3006–3012, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nora Kaplan</author>
<author>Teresa Lucas</author>
</authors>
<title>Comprensi´on del humorismo en ingl´es: Estudio de las estrategias de inferencia utilizadas por estudiantes avanzados de ingl´es como lengua extranjera en la interpretaci´on de los retru´ecanos en historietas c´omicas en lengua inglesa. Anales de la Universidad</title>
<date>2001</date>
<location>Metropolitana,</location>
<contexts>
<context position="16872" citStr="Kaplan and Lucas, 2001" startWordPosition="2662" endWordPosition="2666">l and artificial sources. In their study of wordplay in religious advertising, Bell et al. (2011) compile a corpus of 373 puns taken from church marquees and literature, and compare it against a general corpus of 1515 puns drawn from Internet websites and a specialized dictionary. Zwicky and Zwicky (1986) conduct a phonological analysis on a corpus of several thousand puns, some of which they collected themselves from advertisements and catalogues, and the remainder of which were taken from previously published collections. Two studies on cognitive strategies used by second language learners (Kaplan and Lucas, 2001; Lucas, 2004) used a data set of 58 jokes compiled from newspaper comics, 32 of which rely on lexical ambiguity. Bucaria (2004) conducts a linguistic analysis of a set of 135 humorous newspaper headlines, about half of which exploit lexical ambiguity. Such data sets—particularly the larger ones— provided us good evidence that intentionally lexical ambiguous exemplars exist in sufficient numbers to make a rigorous evaluation of our task feasible. Unfortunately, none of the above-mentioned corpora have been published in full, and moreover many of them contain (sometimes exclusively) the sort of</context>
</contexts>
<marker>Kaplan, Lucas, 2001</marker>
<rawString>Nora Kaplan and Teresa Lucas. 2001. Comprensi´on del humorismo en ingl´es: Estudio de las estrategias de inferencia utilizadas por estudiantes avanzados de ingl´es como lengua extranjera en la interpretaci´on de los retru´ecanos en historietas c´omicas en lengua inglesa. Anales de la Universidad Metropolitana, 1(2):245–258.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Daniel Keller</author>
</authors>
<title>The Development of Shakespeare’s Rhetoric:</title>
<date>2009</date>
<journal>A Study of Nine Plays,</journal>
<volume>136</volume>
<contexts>
<context position="4563" citStr="Keller, 2009" startWordPosition="697" endWordPosition="698">ive texts. Wordplay is also a perennial topic of scholarship in literary criticism and analysis. To give just one example, puns are one of the most intensively studied aspects of Shakespeare’s rhetoric, and laborious manual counts have shown their frequency in certain 719 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 719–729, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics of his plays to range from 17 to 85 instances per thousand lines (Keller, 2009). It is not hard to image how computer-assisted detection, classification, and analysis of puns could help scholars in the digital humanities. Finally, computational pun detection and understanding hold tremendous potential for machine-assisted translation. Some of the most widely disseminated and translated popular discourses—particularly television shows and movies—feature puns and other forms of wordplay as a recurrent and expected feature (Schr¨oter, 2005). These pose particular challenges for translators, who need not only to recognize and comprehend each instance of humour-provoking ambi</context>
</contexts>
<marker>Keller, 2009</marker>
<rawString>Stefan Daniel Keller. 2009. The Development of Shakespeare’s Rhetoric: A Study of Nine Plays, volume 136 of Swiss Studies in English. Narr, T¨ubingen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
<author>Joseph Rosenzweig</author>
</authors>
<date>2000</date>
<booktitle>Framework and results for English SENSEVAL. Computers and the Humanities,</booktitle>
<pages>34--15</pages>
<contexts>
<context position="25840" citStr="Kilgarriff and Rosenzweig, 2000" startWordPosition="4099" endWordPosition="4102">enses), we can allow disambiguation systems to sense-annotate intentionally ambiguous constructions such as puns. Many approaches to WSD, including Lesk-like algorithms, involve computing some score for all possible senses of a target word, and then selecting the single highest-scoring one as the “correct” sense. The most straightforward modification of these techniques to pun disambiguation, then, is to have the systems select the two top-scoring senses, one for each meaning of the pun. Accordingly we applied this modification to the following knowledge-based WSD algorithms: Simplified Lesk (Kilgarriff and Rosenzweig, 2000) disambiguates a target word by examining the definitions5 for each of its candidate senses and selecting the single sense—or in our case, the two senses—which have the greatest number of words in common with the context. As we previously demonstrated that puns often transcend part of speech, our pool of candidate senses is constructed as follows: we apply a morphological analyzer to recover all possible lemmas of the target word without respect to part of speech, and for each lemma we add all its senses to the pool. Simplified extended Lesk (Ponzetto and Navigli, 2010) is similar to simplifie</context>
</contexts>
<marker>Kilgarriff, Rosenzweig, 2000</marker>
<rawString>Adam Kilgarriff and Joseph Rosenzweig. 2000. Framework and results for English SENSEVAL. Computers and the Humanities, 34:15–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Krippendorff</author>
</authors>
<title>Content Analysis: An Introduction to its Methodology. Sage,</title>
<date>1980</date>
<location>Beverly Hills, CA.</location>
<contexts>
<context position="21225" citStr="Krippendorff, 1980" startWordPosition="3375" endWordPosition="3376">gs of the pun. For cases where the distinction between WordNet’s fine-grained senses was irrelevant, the annotators had the option of labelling the meaning with more than one sense key. Annotators also had the option of marking a meaning as unassignable if WordNet had no corresponding sense key. Further details of our annotation tool and its use can be found in Miller and Turkovi´c (2015). 3.2 Analysis Our judges agreed on which word was the pun in 1634 out of 1652 cases, a raw agreement of 98.91%. For the agreed cases, we used DKPro Agreement (Meyer et al., 2014) to compute Krippendorff’s α (Krippendorff, 1980) for the sense annotations. This is a chance-correcting metric of inter-annotator agreement ranging in (−1,1], where 1 indicates perfect agreement, −1 perfect disagreement, and 0 the expected score for random labelling. Our distance metric for α is a straightforward adaptation of the MASI set comparison metric (Passonneau, 2006). Whereas standard MASI, dm(A,B), compares two annotation sets A and B, our annotations take the form of unordered pairs of sets {A1,A2} and {B1,B2}. We therefore find the mapping between elements of the two pairs that gives the lowest total distance, and halve it: dm,(</context>
</contexts>
<marker>Krippendorff, 1980</marker>
<rawString>Klaus Krippendorff. 1980. Content Analysis: An Introduction to its Methodology. Sage, Beverly Hills, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luuk Lagerwerf</author>
</authors>
<title>Deliberate ambiguity in slogans: Recognition and appreciation.</title>
<date>2002</date>
<journal>Document Design,</journal>
<volume>3</volume>
<issue>3</issue>
<contexts>
<context position="7191" citStr="Lagerwerf, 2002" startWordPosition="1110" endWordPosition="1111"> to evoke several independent meanings simultaneously. Humorous and non2The problem is compounded in audio-visual media such as films; often one or both of the pun’s meanings appears in the visual channel, and thus cannot be freely substituted. humorous puns have been the subject of extensive study in the humanities and social sciences, which has led to insights into the nature of language-based humour and wordplay, including their role in commerce, entertainment, and health care; how they are processed in the brain; and how they vary over time and across cultures (Monnot, 1982; Culler, 1988; Lagerwerf, 2002; Bell et al., 2011; Bekinschtein et al., 2011). Study of literary puns imparts a greater understanding of the cultural or historical context in which the literature was produced, which is often necessary to properly interpret and translate it (Delabastita, 1997). Puns can be classified in various ways (Attardo, 1994), though from the point of view of our particular natural language processing application the most important distinction is between homographic and homophonic puns. A homographic pun exploits distinct meanings of the same written word, and a homophonic pun exploits distinct meanin</context>
</contexts>
<marker>Lagerwerf, 2002</marker>
<rawString>Luuk Lagerwerf. 2002. Deliberate ambiguity in slogans: Recognition and appreciation. Document Design, 3(3):245–260.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Lesk</author>
</authors>
<title>Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from a ice cream cone.</title>
<date>1986</date>
<booktitle>In Proceedings of the 5th Annual International Conference of Systems Documentation (SIGDOC</booktitle>
<pages>24--26</pages>
<contexts>
<context position="10864" citStr="Lesk, 1986" startWordPosition="1695" endWordPosition="1696">s. Moreover, supervised approaches tend to be such that they can disambiguate only those words for which they have seen sufficient training examples to cover all senses. That is, most of them cannot disambiguate words which do not occur in the training data, nor can they select the correct sense of a known word if that sense was never observed in the training data. Regardless of the approach, all WSD systems work by extracting contextual information for the target word and comparing it against the sense information stored for that word. A seminal knowledge-based example is the Lesk algorithm (Lesk, 1986) which disambiguates a pair of target terms in context by comparing their respective dictionary definitions and selecting the two with the greatest number of words in common. Though simple, the Lesk algorithm performs surprisingly well, and has frequently served as the basis of more sophisticated approaches. In recent years, Lesk variants in which the contexts and definitions are supplemented with entries from a distributional thesaurus (Lin, 1998) have achieved state-of-the-art performance for knowledge-based systems on standard data sets (Miller et al., 2012; Basile et al., 2014). In traditi</context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>Michael Lesk. 1986. Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from a ice cream cone. In Proceedings of the 5th Annual International Conference of Systems Documentation (SIGDOC 1986), pages 24–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Lessard</author>
<author>Michael Levison</author>
<author>Chris Venour</author>
</authors>
<title>Cleverness versus funniness.</title>
<date>2002</date>
<booktitle>In Proceedings of the 20th Twente Workshop on Language Technology,</booktitle>
<pages>137--145</pages>
<contexts>
<context position="15919" citStr="Lessard et al. (2002)" startWordPosition="2507" endWordPosition="2510">e possibility that any given word might be ambiguous. Rather, for every word in the setup, the Lesk measure was used to select a word in the punchline such that the lexical overlap between each one of their possible definitions was maximized. The overlap scores for all word pairs were then averaged, and the punchline with the lowest average score selected as the most humorous. 2.3.2 Corpora There are a number of English-language corpora of intentional lexical ambiguity which have been used in past work, usually in linguistics or the social sciences. In their work on computer-generated humour, Lessard et al. (2002) use a corpus of 374 “Tom Swifty” puns taken from the Internet, plus a well-balanced corpus of 50 humorous and nonhumorous lexical ambiguities generated programmatically (Venour, 1999). Hong and Ong (2009) also study humour in natural language generation, using a smaller data set of 27 punning riddles derived from a mix of natural and artificial sources. In their study of wordplay in religious advertising, Bell et al. (2011) compile a corpus of 373 puns taken from church marquees and literature, and compare it against a general corpus of 1515 puns drawn from Internet websites and a specialized</context>
</contexts>
<marker>Lessard, Levison, Venour, 2002</marker>
<rawString>Greg Lessard, Michael Levison, and Chris Venour. 2002. Cleverness versus funniness. In Proceedings of the 20th Twente Workshop on Language Technology, pages 137–145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics (ACL 1998) and the 17th International Conference on Computational Linguistics (COLING</booktitle>
<volume>2</volume>
<pages>768--774</pages>
<contexts>
<context position="11316" citStr="Lin, 1998" startWordPosition="1764" endWordPosition="1765"> for the target word and comparing it against the sense information stored for that word. A seminal knowledge-based example is the Lesk algorithm (Lesk, 1986) which disambiguates a pair of target terms in context by comparing their respective dictionary definitions and selecting the two with the greatest number of words in common. Though simple, the Lesk algorithm performs surprisingly well, and has frequently served as the basis of more sophisticated approaches. In recent years, Lesk variants in which the contexts and definitions are supplemented with entries from a distributional thesaurus (Lin, 1998) have achieved state-of-the-art performance for knowledge-based systems on standard data sets (Miller et al., 2012; Basile et al., 2014). In traditional word sense disambiguation, the part of speech and lemma of the target word are usually known a priori, or can be determined with high accuracy using off-the-shelf natural language processing tools. The pool of candidate senses can therefore be restricted to those whose lexicalizations exactly match the target lemma and part of speech. No such help is available for puns, at least not in the general case. Take the following two examples: (5) Tom</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics (ACL 1998) and the 17th International Conference on Computational Linguistics (COLING 1998), volume 2, pages 768–774.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Teresa Lucas</author>
</authors>
<title>Deciphering the Meaning of Puns in Learning English as a Second Language: A Study of Triadic Interaction.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>Florida State University.</institution>
<contexts>
<context position="16886" citStr="Lucas, 2004" startWordPosition="2667" endWordPosition="2668">. In their study of wordplay in religious advertising, Bell et al. (2011) compile a corpus of 373 puns taken from church marquees and literature, and compare it against a general corpus of 1515 puns drawn from Internet websites and a specialized dictionary. Zwicky and Zwicky (1986) conduct a phonological analysis on a corpus of several thousand puns, some of which they collected themselves from advertisements and catalogues, and the remainder of which were taken from previously published collections. Two studies on cognitive strategies used by second language learners (Kaplan and Lucas, 2001; Lucas, 2004) used a data set of 58 jokes compiled from newspaper comics, 32 of which rely on lexical ambiguity. Bucaria (2004) conducts a linguistic analysis of a set of 135 humorous newspaper headlines, about half of which exploit lexical ambiguity. Such data sets—particularly the larger ones— provided us good evidence that intentionally lexical ambiguous exemplars exist in sufficient numbers to make a rigorous evaluation of our task feasible. Unfortunately, none of the above-mentioned corpora have been published in full, and moreover many of them contain (sometimes exclusively) the sort of imperfect or </context>
</contexts>
<marker>Lucas, 2004</marker>
<rawString>Teresa Lucas. 2004. Deciphering the Meaning of Puns in Learning English as a Second Language: A Study of Triadic Interaction. Ph.D. thesis, Florida State University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter J Ludlow</author>
</authors>
<date>1996</date>
<booktitle>Semantic Ambiguity and Underspecification (review). Computational Linguistics,</booktitle>
<pages>3--23</pages>
<contexts>
<context position="1692" citStr="Ludlow, 1996" startWordPosition="250" endWordPosition="251"> matter whether it is performed by a human or a machine, WSD usually rests on the assumption that there is a single unambiguous communicative intention underlying each word in the document.1 However, there exists a class of language constructs known 1Under this assumption, lexical ambiguity arises due to there being a plurality of words with the same surface form but different meanings, and the task of the interpreter is to select correctly among them. An alternative view is that each word is a single lexical entry whose specific meaning is underspecified until it is activated by the context (Ludlow, 1996). In the case of systematically polysemous terms (i.e., words that have several related senses shared in a systematic way by a group of similar words), it may not be necessary to disambiguate them at all in order to interpret the communication (Buitelaar, 2000). While there has been some research in modelling intentional lexical-semantic underspecification (Jurgens, 2014), it is intended for closely related senses such as those of systematically polysemous terms, not those of coarser-grained homonyms which are the subject of this paper. as paronomasia and syllepsis, or more generally as puns, </context>
</contexts>
<marker>Ludlow, 1996</marker>
<rawString>Peter J. Ludlow. 1996. Semantic Ambiguity and Underspecification (review). Computational Linguistics, 3(23):476–482.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Matuschek</author>
<author>Tristan Miller</author>
<author>Iryna Gurevych</author>
</authors>
<title>A language-independent sense clustering approach for enhanced WSD.</title>
<date>2014</date>
<booktitle>In Proceedings of the 12th Konferenz zur Verarbeitung nat¨urlicher Sprache (KONVENS 2014),</booktitle>
<pages>11--21</pages>
<contexts>
<context position="24769" citStr="Matuschek et al. (2014)" startWordPosition="3932" endWordPosition="3935">guish between candidate senses when their definitions have a similar overlap with the target word’s context. In some cases this is because the overlap is negligible or nonexistent; this is known as the lexical gap problem, and various solutions to it are discussed in (inter alia) Miller et al. (2012). In other cases, the indecision arises because the definitions provided by the sense inventory are too fine-grained; this problem has been addressed, with varying degrees of success, through sense clustering or coarsening techniques (a short but reasonably comprehensive survey of which appears in Matuschek et al. (2014)). A third condition under which senses cannot be discriminated is when the target word is used in an underspecified or intentionally ambiguous manner. We hold that for this third scenario a disambiguator’s inability to discriminate senses should not be seen as a failure condition, but rather as a limitation of the WSD task as traditionally defined. By reframing the task so as to permit the assignment of multiple senses (or groups of senses), we can allow disambiguation systems to sense-annotate intentionally ambiguous constructions such as puns. Many approaches to WSD, including Lesk-like alg</context>
<context position="28014" citStr="Matuschek et al. (2014)" startWordPosition="4453" endWordPosition="4456">724 of speech is grammatical in the context of the sentence, and so would probably be the one assigned by a stochastic or rule-based POS tagger. Our “POS” tie-breaker therefore preferentially selects the best sense, or pair of senses, whose POS matches the one applied to the target by the Stanford POS tagger (Toutanova et al., 2003). For our second tie-breaking strategy, we posit that since humour derives from the resolution of semantic incongruity (Raskin, 1985; Attardo, 1994), puns are more likely to exploit coarse-grained homonymy than than fine-grained systematic polysemy. Thus, following Matuschek et al. (2014), we induced a clustering of WordNet senses by aligning WordNet to the more coarse-grained OmegaWiki LSR.6 Our “cluster” fallback works the same as the “POS” one, with the addition that any remaining ties among senses with the second-highest overlap are resolved by preferentially selecting those which are not in the same induced cluster as, and which in WordNet’s semantic network are at least three edges distant from, the sense with the highest overlap. 5 Evaluation 5.1 Scoring In traditional word sense disambiguation, in vitro evaluations are conducted by comparing the senses assigned by the </context>
</contexts>
<marker>Matuschek, Miller, Gurevych, 2014</marker>
<rawString>Michael Matuschek, Tristan Miller, and Iryna Gurevych. 2014. A language-independent sense clustering approach for enhanced WSD. In Proceedings of the 12th Konferenz zur Verarbeitung nat¨urlicher Sprache (KONVENS 2014), pages 11–21, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian M Meyer</author>
<author>Margot Mieskes</author>
<author>Christian Stab</author>
<author>Iryna Gurevych</author>
</authors>
<title>DKPro Agreement: An open-source Java library for measuring inter-rater agreement.</title>
<date>2014</date>
<booktitle>In Proceedings of the 25th International Conference on Computational Linguistics (System Demonstrations) (COLING 2014),</booktitle>
<pages>105--109</pages>
<contexts>
<context position="21176" citStr="Meyer et al., 2014" startWordPosition="3366" endWordPosition="3369"> to each instance, one for each of the two meanings of the pun. For cases where the distinction between WordNet’s fine-grained senses was irrelevant, the annotators had the option of labelling the meaning with more than one sense key. Annotators also had the option of marking a meaning as unassignable if WordNet had no corresponding sense key. Further details of our annotation tool and its use can be found in Miller and Turkovi´c (2015). 3.2 Analysis Our judges agreed on which word was the pun in 1634 out of 1652 cases, a raw agreement of 98.91%. For the agreed cases, we used DKPro Agreement (Meyer et al., 2014) to compute Krippendorff’s α (Krippendorff, 1980) for the sense annotations. This is a chance-correcting metric of inter-annotator agreement ranging in (−1,1], where 1 indicates perfect agreement, −1 perfect disagreement, and 0 the expected score for random labelling. Our distance metric for α is a straightforward adaptation of the MASI set comparison metric (Passonneau, 2006). Whereas standard MASI, dm(A,B), compares two annotation sets A and B, our annotations take the form of unordered pairs of sets {A1,A2} and {B1,B2}. We therefore find the mapping between elements of the two pairs that gi</context>
</contexts>
<marker>Meyer, Mieskes, Stab, Gurevych, 2014</marker>
<rawString>Christian M. Meyer, Margot Mieskes, Christian Stab, and Iryna Gurevych. 2014. DKPro Agreement: An open-source Java library for measuring inter-rater agreement. In Proceedings of the 25th International Conference on Computational Linguistics (System Demonstrations) (COLING 2014), pages 105–109, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Carlo Strapparava</author>
</authors>
<title>Making computers laugh: Investigations in automatic humor recognition.</title>
<date>2005</date>
<booktitle>In Proceedings of the 11th Human Language Technology Conference and the 10th Conference on Empirical Methods in Natural Language Processing (HLT-EMNLP</booktitle>
<pages>531--538</pages>
<contexts>
<context position="13971" citStr="Mihalcea and Strapparava (2005)" startWordPosition="2194" endWordPosition="2197">okogawa (2002) describes a system for detecting the presence of puns in Japanese text. However, this work is concerned only with puns which are both imperfect and ungrammatical, relying on syntactic cues rather than the lexical-semantic information we propose to use. Taylor and Mazlack (2004) 721 describe an n-gram–based approach for recognizing when imperfect puns are used for humorous effect in a certain narrow class of English knockknock jokes. Their focus on imperfect puns and their use of a fixed syntactic context makes their approach largely inapplicable to perfect puns in running text. Mihalcea and Strapparava (2005) treat humour recognition as a classification task, employing various machine learning techniques on humour-specific stylistic features such as alliteration and antonymy. Of particular interest is their follow-up analysis (Mihalcea and Strapparava, 2006), where they specifically point to their system’s failure to resolve lexical-semantic ambiguity as a stumbling block to better accuracy, and speculate that deeper semantic analysis of the text, such as via word sense disambiguation or domain disambiguation, could aid in the detection of humorous incongruity and opposition. The previous work whi</context>
</contexts>
<marker>Mihalcea, Strapparava, 2005</marker>
<rawString>Rada Mihalcea and Carlo Strapparava. 2005. Making computers laugh: Investigations in automatic humor recognition. In Proceedings of the 11th Human Language Technology Conference and the 10th Conference on Empirical Methods in Natural Language Processing (HLT-EMNLP 2005), pages 531–538, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Carlo Strapparava</author>
</authors>
<title>Learning to laugh (automatically): Computational models for humor recognition.</title>
<date>2006</date>
<journal>Computational Intelligence,</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="14225" citStr="Mihalcea and Strapparava, 2006" startWordPosition="2227" endWordPosition="2230">opose to use. Taylor and Mazlack (2004) 721 describe an n-gram–based approach for recognizing when imperfect puns are used for humorous effect in a certain narrow class of English knockknock jokes. Their focus on imperfect puns and their use of a fixed syntactic context makes their approach largely inapplicable to perfect puns in running text. Mihalcea and Strapparava (2005) treat humour recognition as a classification task, employing various machine learning techniques on humour-specific stylistic features such as alliteration and antonymy. Of particular interest is their follow-up analysis (Mihalcea and Strapparava, 2006), where they specifically point to their system’s failure to resolve lexical-semantic ambiguity as a stumbling block to better accuracy, and speculate that deeper semantic analysis of the text, such as via word sense disambiguation or domain disambiguation, could aid in the detection of humorous incongruity and opposition. The previous work which is perhaps most relevant to ours is that of Mihalcea et al. (2010). They build a data set consisting of 150 joke set-ups, each of which is followed by four possible “punchlines”, only one of which is actually humorous (but not necessarily due to a pun</context>
</contexts>
<marker>Mihalcea, Strapparava, 2006</marker>
<rawString>Rada Mihalcea and Carlo Strapparava. 2006. Learning to laugh (automatically): Computational models for humor recognition. Computational Intelligence, 22(2):126–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Carlo Strapparava</author>
<author>Stephen Pulman</author>
</authors>
<title>Computational models for incongruity detection in humour.</title>
<date>2010</date>
<booktitle>In Proceedings of the 11th International Conference on Computational Linguistics and Intelligent Text Processing (CICLing 2010),</booktitle>
<volume>6008</volume>
<pages>364--374</pages>
<publisher>Springer,</publisher>
<contexts>
<context position="14640" citStr="Mihalcea et al. (2010)" startWordPosition="2295" endWordPosition="2298">ask, employing various machine learning techniques on humour-specific stylistic features such as alliteration and antonymy. Of particular interest is their follow-up analysis (Mihalcea and Strapparava, 2006), where they specifically point to their system’s failure to resolve lexical-semantic ambiguity as a stumbling block to better accuracy, and speculate that deeper semantic analysis of the text, such as via word sense disambiguation or domain disambiguation, could aid in the detection of humorous incongruity and opposition. The previous work which is perhaps most relevant to ours is that of Mihalcea et al. (2010). They build a data set consisting of 150 joke set-ups, each of which is followed by four possible “punchlines”, only one of which is actually humorous (but not necessarily due to a pun). They then compare the set-ups against the punchlines using various models of incongruity detection, including many exploiting knowledge-based semantic relatedness such as Lesk. The Lesk model had an accuracy of 56%, which is lower than that of a naive polysemy model which simply selects the punchline with the highest mean polysemy (66%) and even of a random-choice baseline (62%). However, it should be stresse</context>
<context position="39901" citStr="Mihalcea et al. (2010)" startWordPosition="6384" endWordPosition="6387">to that of a supervised baseline, and their precision greatly exceeds it. There are a number of avenues we intend to explore in future work. First, we would like to try adapting and evaluating some additional WSD algorithms for use with puns. Though our data set is probably too small to use with machine learning– based approaches, we are particularly interested in testing knowledge-based disambiguators which rely on measures of graph connectivity rather than gloss overlaps. Second, we would like to investigate alternative tie-breaking strategies, such as the domain similarity measures used by Mihalcea et al. (2010). Finally, whereas in this paper we have treated only the task of sense disambiguation for the case where a word is known a priori to be a pun, we are interested in exploring the requisite problem of pun detection, where the object is to determine whether or not a given context contains a pun, and more precisely whether any given word in a context is a pun. Acknowledgments The work described in this paper is supported by the Volkswagen Foundation as part of the Lichtenberg Professorship Program under grant No. I/82806. The authors thank John Black, Matthew Collins, Don Hauptman, Christian F. H</context>
</contexts>
<marker>Mihalcea, Strapparava, Pulman, 2010</marker>
<rawString>Rada Mihalcea, Carlo Strapparava, and Stephen Pulman. 2010. Computational models for incongruity detection in humour. In Proceedings of the 11th International Conference on Computational Linguistics and Intelligent Text Processing (CICLing 2010), volume 6008 of Lecture Notes in Computer Science, pages 364–374. Springer, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tristan Miller</author>
<author>Mladen Turkovi´c</author>
</authors>
<title>Towards the automatic detection and identification of English puns.</title>
<date>2015</date>
<journal>European Journal of Humour Research.</journal>
<note>To appear.</note>
<marker>Miller, Turkovi´c, 2015</marker>
<rawString>Tristan Miller and Mladen Turkovi´c. 2015. Towards the automatic detection and identification of English puns. European Journal of Humour Research. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tristan Miller</author>
<author>Chris Biemann</author>
<author>Torsten Zesch</author>
<author>Iryna Gurevych</author>
</authors>
<title>Using distributional similarity for lexical expansion in knowledge-based word sense disambiguation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 24th International Conference on Computational Linguistics (COLING 2012),</booktitle>
<pages>1781--1796</pages>
<contexts>
<context position="11430" citStr="Miller et al., 2012" startWordPosition="1778" endWordPosition="1781">edge-based example is the Lesk algorithm (Lesk, 1986) which disambiguates a pair of target terms in context by comparing their respective dictionary definitions and selecting the two with the greatest number of words in common. Though simple, the Lesk algorithm performs surprisingly well, and has frequently served as the basis of more sophisticated approaches. In recent years, Lesk variants in which the contexts and definitions are supplemented with entries from a distributional thesaurus (Lin, 1998) have achieved state-of-the-art performance for knowledge-based systems on standard data sets (Miller et al., 2012; Basile et al., 2014). In traditional word sense disambiguation, the part of speech and lemma of the target word are usually known a priori, or can be determined with high accuracy using off-the-shelf natural language processing tools. The pool of candidate senses can therefore be restricted to those whose lexicalizations exactly match the target lemma and part of speech. No such help is available for puns, at least not in the general case. Take the following two examples: (5) Tom moped. (6) “I want a scooter,” Tom moped. In the first of these sentences, the word moped is unambiguously a verb</context>
<context position="24447" citStr="Miller et al. (2012)" startWordPosition="3885" endWordPosition="3888"> that pun disambiguators, unlike traditional WSD systems, cannot always rely on the output of a lemmatizer or part-of-speech tagger to narrow down the list of sense candidates. 4 Pun disambiguation It has long been observed that gloss overlap–based WSD systems, such as those based on the Lesk algorithm, fail to distinguish between candidate senses when their definitions have a similar overlap with the target word’s context. In some cases this is because the overlap is negligible or nonexistent; this is known as the lexical gap problem, and various solutions to it are discussed in (inter alia) Miller et al. (2012). In other cases, the indecision arises because the definitions provided by the sense inventory are too fine-grained; this problem has been addressed, with varying degrees of success, through sense clustering or coarsening techniques (a short but reasonably comprehensive survey of which appears in Matuschek et al. (2014)). A third condition under which senses cannot be discriminated is when the target word is used in an underspecified or intentionally ambiguous manner. We hold that for this third scenario a disambiguator’s inability to discriminate senses should not be seen as a failure condit</context>
<context position="26627" citStr="Miller et al., 2012" startWordPosition="4227" endWordPosition="4230">e greatest number of words in common with the context. As we previously demonstrated that puns often transcend part of speech, our pool of candidate senses is constructed as follows: we apply a morphological analyzer to recover all possible lemmas of the target word without respect to part of speech, and for each lemma we add all its senses to the pool. Simplified extended Lesk (Ponzetto and Navigli, 2010) is similar to simplified Lesk, except that the definition for each sense is concatenated with those of neighbouring senses in WordNet’s semantic network. Simplified lexically expanded Lesk (Miller et al., 2012) is also based on simplified Lesk, with the extension that every word in the context and sense definitions is expanded with up to 100 entries from a large distributional thesaurus. The above algorithms fail to make a sense assignment when more than two senses are tied for the highest lexical overlap, or when there is a single highest-scoring sense but multiple senses are tied for the second-highest overlap. We therefore devised two pun-specific tie-breaking strategies. The first is motivated by the informal observation that, though the two meanings of a pun may have different parts of speech, </context>
</contexts>
<marker>Miller, Biemann, Zesch, Gurevych, 2012</marker>
<rawString>Tristan Miller, Chris Biemann, Torsten Zesch, and Iryna Gurevych. 2012. Using distributional similarity for lexical expansion in knowledge-based word sense disambiguation. In Proceedings of the 24th International Conference on Computational Linguistics (COLING 2012), pages 1781–1796, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tristan Miller</author>
<author>Nicolai Erbs</author>
<author>Hans-Peter Zorn</author>
<author>Torsten Zesch</author>
<author>Iryna Gurevych</author>
</authors>
<title>DKPro WSD: A generalized UIMA-based framework for word sense disambiguation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (System Demonstrations) (ACL 2013),</booktitle>
<pages>37--42</pages>
<contexts>
<context position="32379" citStr="Miller et al., 2013" startWordPosition="5181" endWordPosition="5184"> notoriously difficult to beat, even for supervised disambiguation systems, and since they rely on expensive sense-tagged data they are not normally considered a benchmark for the performance of knowledgebased disambiguators. 725 system C P R F1 SL 35.52 19.74 7.01 10.35 SEL 42.45 19.96 8.47 11.90 SLEL 98.69 13.43 13.25 13.34 SEL+POS 59.94 21.21 12.71 15.90 SEL+cluster 68.10 20.70 14.10 16.77 random 100.00 9.31 9.31 9.31 MFS 100.00 13.25 13.25 13.25 Table 1: Coverage, precision, recall, and F1 for various pun diasmbiguation algorithms. 6 Results Using the freely available DKPro WSD framework (Miller et al., 2013), we implemented our pun disambiguation algorithms, ran them on our full data set, and compared their annotations against those of our manually produced gold standard. Table 1 shows the coverage, precision, recall, and F1 for simplified Lesk (SL), simplified extended Lesk (SEL), simplified lexically expanded Lesk (SLEL), and the random and most frequent sense baselines; for SEL we also report results for each of our punspecific tie-breaking strategies. All metrics are reported as percentages, and the highest score for each metric (excluding baseline coverage, which is always 100%) is highlight</context>
</contexts>
<marker>Miller, Erbs, Zorn, Zesch, Gurevych, 2013</marker>
<rawString>Tristan Miller, Nicolai Erbs, Hans-Peter Zorn, Torsten Zesch, and Iryna Gurevych. 2013. DKPro WSD: A generalized UIMA-based framework for word sense disambiguation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (System Demonstrations) (ACL 2013), pages 37–42, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Monnot</author>
</authors>
<title>Puns in advertising: Ambiguity as verbal aggression.</title>
<date>1982</date>
<journal>Maledicta,</journal>
<pages>6--7</pages>
<contexts>
<context position="7160" citStr="Monnot, 1982" startWordPosition="1106" endWordPosition="1107">ord is used in such a way as to evoke several independent meanings simultaneously. Humorous and non2The problem is compounded in audio-visual media such as films; often one or both of the pun’s meanings appears in the visual channel, and thus cannot be freely substituted. humorous puns have been the subject of extensive study in the humanities and social sciences, which has led to insights into the nature of language-based humour and wordplay, including their role in commerce, entertainment, and health care; how they are processed in the brain; and how they vary over time and across cultures (Monnot, 1982; Culler, 1988; Lagerwerf, 2002; Bell et al., 2011; Bekinschtein et al., 2011). Study of literary puns imparts a greater understanding of the cultural or historical context in which the literature was produced, which is often necessary to properly interpret and translate it (Delabastita, 1997). Puns can be classified in various ways (Attardo, 1994), though from the point of view of our particular natural language processing application the most important distinction is between homographic and homophonic puns. A homographic pun exploits distinct meanings of the same written word, and a homophon</context>
</contexts>
<marker>Monnot, 1982</marker>
<rawString>Michel Monnot. 1982. Puns in advertising: Ambiguity as verbal aggression. Maledicta, 6:7–20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Kenneth C Litkowski</author>
<author>Orin Hargraves</author>
</authors>
<title>SemEval-2007 Task 07: Coarsegrained English All-words Task.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>30--35</pages>
<contexts>
<context position="33342" citStr="Navigli et al., 2007" startWordPosition="5335" endWordPosition="5338">most frequent sense baselines; for SEL we also report results for each of our punspecific tie-breaking strategies. All metrics are reported as percentages, and the highest score for each metric (excluding baseline coverage, which is always 100%) is highlighted in boldface. Accuracy for the random baseline annotator was about 9%; for the MFS baseline it was just over 13%. These figures are considerably lower than what is typically seen with traditional WSD corpora, where random baselines achieve accuracies of 30 to 60%, and MFS baselines 65 to 80% (Palmer et al., 2001; Snyder and Palmer, 2004; Navigli et al., 2007). Our baselines’ low figures are the result of them having to consider senses from every possible lemmatization and part of speech of the target, and underscore the difficulty of our task. The simplest knowledge-based algorithm we tested, simplified Lesk, was over twice as accurate as the random baseline in terms of precision (19.74%), but predictably had very low coverage (35.52%), leading in turn to very low recall (7.01%). Manual examination of the unassigned instances confirmed that failure was usually due to the lack of any lexical overlap whatsoever between the context and definitions. T</context>
</contexts>
<marker>Navigli, Litkowski, Hargraves, 2007</marker>
<rawString>Roberto Navigli, Kenneth C. Litkowski, and Orin Hargraves. 2007. SemEval-2007 Task 07: Coarsegrained English All-words Task. In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 30–35, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Christiane Fellbaum</author>
<author>Scott Cotton</author>
<author>Lauren Delfs</author>
<author>Hoa Trang Dang</author>
</authors>
<title>English tasks: All-words and verb lexical sample.</title>
<date>2001</date>
<booktitle>In Proceedings of Senseval-2: 2nd International Workshop on Evaluating Word Sense Disambiguation Systems,</booktitle>
<pages>21--24</pages>
<contexts>
<context position="33294" citStr="Palmer et al., 2001" startWordPosition="5327" endWordPosition="5330">ally expanded Lesk (SLEL), and the random and most frequent sense baselines; for SEL we also report results for each of our punspecific tie-breaking strategies. All metrics are reported as percentages, and the highest score for each metric (excluding baseline coverage, which is always 100%) is highlighted in boldface. Accuracy for the random baseline annotator was about 9%; for the MFS baseline it was just over 13%. These figures are considerably lower than what is typically seen with traditional WSD corpora, where random baselines achieve accuracies of 30 to 60%, and MFS baselines 65 to 80% (Palmer et al., 2001; Snyder and Palmer, 2004; Navigli et al., 2007). Our baselines’ low figures are the result of them having to consider senses from every possible lemmatization and part of speech of the target, and underscore the difficulty of our task. The simplest knowledge-based algorithm we tested, simplified Lesk, was over twice as accurate as the random baseline in terms of precision (19.74%), but predictably had very low coverage (35.52%), leading in turn to very low recall (7.01%). Manual examination of the unassigned instances confirmed that failure was usually due to the lack of any lexical overlap w</context>
</contexts>
<marker>Palmer, Fellbaum, Cotton, Delfs, Dang, 2001</marker>
<rawString>Martha Palmer, Christiane Fellbaum, Scott Cotton, Lauren Delfs, and Hoa Trang Dang. 2001. English tasks: All-words and verb lexical sample. In Proceedings of Senseval-2: 2nd International Workshop on Evaluating Word Sense Disambiguation Systems, pages 21–24, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
</authors>
<title>Hwee Tou Ng, and Hoa Trang Dang.</title>
<date>2006</date>
<booktitle>In Eneko Agirre and Philip Edmonds, editors, Word Sense Disambiguation: Algorithms and Applications, volume 33 of Text, Speech, and Language Technology.</booktitle>
<publisher>Springer.</publisher>
<marker>Palmer, 2006</marker>
<rawString>Martha Palmer, Hwee Tou Ng, and Hoa Trang Dang. 2006. Evaluation of WSD systems. In Eneko Agirre and Philip Edmonds, editors, Word Sense Disambiguation: Algorithms and Applications, volume 33 of Text, Speech, and Language Technology. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca J Passonneau</author>
<author>Nizar Habash</author>
<author>Owen Rambow</author>
</authors>
<title>Inter-annotator agreement on a multilingual semantic annotation task.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluations (LREC</booktitle>
<pages>1951--1956</pages>
<contexts>
<context position="22128" citStr="Passonneau et al., 2006" startWordPosition="3512" endWordPosition="3515"> the MASI set comparison metric (Passonneau, 2006). Whereas standard MASI, dm(A,B), compares two annotation sets A and B, our annotations take the form of unordered pairs of sets {A1,A2} and {B1,B2}. We therefore find the mapping between elements of the two pairs that gives the lowest total distance, and halve it: dm,({A1,A2},{B1,B2}) = 21min(dm(A1,B1)+ dm(A2,B2),dm(A1,B2) + dm(A2,B1)). With this method we observe a Krippendorff’s α of 0.777; this is only slightly below the 0.8 threshold recommended by Krippendorff, and far higher than what has been reported in other sense annotation studies (Passonneau et al., 2006; Jurgens and Klapaftis, 2013). Where possible, we resolved sense annotation disagreements automatically by taking the intersection of corresponding sense sets. Where the annotators’ sense sets were disjoint or contradictory (including the cases where the annotators disagreed on the pun word), we had a human adjudicator attempt to resolve the disagreement in favour of one annotator or the other. This left us with 1607 instances,4 of which we retained only the 1298 that had successful (i.e., not marked as unassignable) 4Pending clearance of the distribution rights, we will make some or all of o</context>
</contexts>
<marker>Passonneau, Habash, Rambow, 2006</marker>
<rawString>Rebecca J. Passonneau, Nizar Habash, and Owen Rambow. 2006. Inter-annotator agreement on a multilingual semantic annotation task. In Proceedings of the 5th International Conference on Language Resources and Evaluations (LREC 2006), pages 1951–1956.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca J Passonneau</author>
</authors>
<title>Measuring agreement on set-valued items (MASI) for semantic and pragmatic annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluations (LREC</booktitle>
<pages>831--836</pages>
<contexts>
<context position="21555" citStr="Passonneau, 2006" startWordPosition="3425" endWordPosition="3426">ool and its use can be found in Miller and Turkovi´c (2015). 3.2 Analysis Our judges agreed on which word was the pun in 1634 out of 1652 cases, a raw agreement of 98.91%. For the agreed cases, we used DKPro Agreement (Meyer et al., 2014) to compute Krippendorff’s α (Krippendorff, 1980) for the sense annotations. This is a chance-correcting metric of inter-annotator agreement ranging in (−1,1], where 1 indicates perfect agreement, −1 perfect disagreement, and 0 the expected score for random labelling. Our distance metric for α is a straightforward adaptation of the MASI set comparison metric (Passonneau, 2006). Whereas standard MASI, dm(A,B), compares two annotation sets A and B, our annotations take the form of unordered pairs of sets {A1,A2} and {B1,B2}. We therefore find the mapping between elements of the two pairs that gives the lowest total distance, and halve it: dm,({A1,A2},{B1,B2}) = 21min(dm(A1,B1)+ dm(A2,B2),dm(A1,B2) + dm(A2,B1)). With this method we observe a Krippendorff’s α of 0.777; this is only slightly below the 0.8 threshold recommended by Krippendorff, and far higher than what has been reported in other sense annotation studies (Passonneau et al., 2006; Jurgens and Klapaftis, 20</context>
</contexts>
<marker>Passonneau, 2006</marker>
<rawString>Rebecca J. Passonneau. 2006. Measuring agreement on set-valued items (MASI) for semantic and pragmatic annotation. In Proceedings of the 5th International Conference on Language Resources and Evaluations (LREC 2006), pages 831–836.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
<author>Roberto Navigli</author>
</authors>
<title>Knowledge-rich word sense disambiguation rivaling supervised systems.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>1522--1531</pages>
<contexts>
<context position="26416" citStr="Ponzetto and Navigli, 2010" startWordPosition="4195" endWordPosition="4198">: Simplified Lesk (Kilgarriff and Rosenzweig, 2000) disambiguates a target word by examining the definitions5 for each of its candidate senses and selecting the single sense—or in our case, the two senses—which have the greatest number of words in common with the context. As we previously demonstrated that puns often transcend part of speech, our pool of candidate senses is constructed as follows: we apply a morphological analyzer to recover all possible lemmas of the target word without respect to part of speech, and for each lemma we add all its senses to the pool. Simplified extended Lesk (Ponzetto and Navigli, 2010) is similar to simplified Lesk, except that the definition for each sense is concatenated with those of neighbouring senses in WordNet’s semantic network. Simplified lexically expanded Lesk (Miller et al., 2012) is also based on simplified Lesk, with the extension that every word in the context and sense definitions is expanded with up to 100 entries from a large distributional thesaurus. The above algorithms fail to make a sense assignment when more than two senses are tied for the highest lexical overlap, or when there is a single highest-scoring sense but multiple senses are tied for the se</context>
</contexts>
<marker>Ponzetto, Navigli, 2010</marker>
<rawString>Simone Paolo Ponzetto and Roberto Navigli. 2010. Knowledge-rich word sense disambiguation rivaling supervised systems. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010), pages 1522–1531.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vitor Raskin</author>
</authors>
<title>Semantic Mechanisms of Humor.</title>
<date>1985</date>
<publisher>Puns. Basil Blackwell,</publisher>
<location>D. Reidel, Dordrecht, the Netherlands. Walter Redfern.</location>
<contexts>
<context position="27857" citStr="Raskin, 1985" startWordPosition="4434" endWordPosition="4435">e parts 5In our implementation, the sense definitions are formed by concatenating the synonyms, gloss, and example sentences provided by WordNet. 724 of speech is grammatical in the context of the sentence, and so would probably be the one assigned by a stochastic or rule-based POS tagger. Our “POS” tie-breaker therefore preferentially selects the best sense, or pair of senses, whose POS matches the one applied to the target by the Stanford POS tagger (Toutanova et al., 2003). For our second tie-breaking strategy, we posit that since humour derives from the resolution of semantic incongruity (Raskin, 1985; Attardo, 1994), puns are more likely to exploit coarse-grained homonymy than than fine-grained systematic polysemy. Thus, following Matuschek et al. (2014), we induced a clustering of WordNet senses by aligning WordNet to the more coarse-grained OmegaWiki LSR.6 Our “cluster” fallback works the same as the “POS” one, with the addition that any remaining ties among senses with the second-highest overlap are resolved by preferentially selecting those which are not in the same induced cluster as, and which in WordNet’s semantic network are at least three edges distant from, the sense with the hi</context>
</contexts>
<marker>Raskin, 1985</marker>
<rawString>Vitor Raskin. 1985. Semantic Mechanisms of Humor. D. Reidel, Dordrecht, the Netherlands. Walter Redfern. 1984. Puns. Basil Blackwell, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Schr¨oter</author>
</authors>
<title>Shun the Pun, Rescue the Rhyme? The Dubbing and Subtitling of LanguagePlay in Film.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>Karlstad University.</institution>
<marker>Schr¨oter, 2005</marker>
<rawString>Thorsten Schr¨oter. 2005. Shun the Pun, Rescue the Rhyme? The Dubbing and Subtitling of LanguagePlay in Film. Ph.D. thesis, Karlstad University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Martha Palmer</author>
</authors>
<title>The English all-words task.</title>
<date>2004</date>
<booktitle>In Proceedings of the 3rd International Workshop on the Evaluation of Systems for the Semantic Analysis of Text (Senseval-3),</booktitle>
<pages>41--43</pages>
<contexts>
<context position="33319" citStr="Snyder and Palmer, 2004" startWordPosition="5331" endWordPosition="5334">LEL), and the random and most frequent sense baselines; for SEL we also report results for each of our punspecific tie-breaking strategies. All metrics are reported as percentages, and the highest score for each metric (excluding baseline coverage, which is always 100%) is highlighted in boldface. Accuracy for the random baseline annotator was about 9%; for the MFS baseline it was just over 13%. These figures are considerably lower than what is typically seen with traditional WSD corpora, where random baselines achieve accuracies of 30 to 60%, and MFS baselines 65 to 80% (Palmer et al., 2001; Snyder and Palmer, 2004; Navigli et al., 2007). Our baselines’ low figures are the result of them having to consider senses from every possible lemmatization and part of speech of the target, and underscore the difficulty of our task. The simplest knowledge-based algorithm we tested, simplified Lesk, was over twice as accurate as the random baseline in terms of precision (19.74%), but predictably had very low coverage (35.52%), leading in turn to very low recall (7.01%). Manual examination of the unassigned instances confirmed that failure was usually due to the lack of any lexical overlap whatsoever between the con</context>
</contexts>
<marker>Snyder, Palmer, 2004</marker>
<rawString>Benjamin Snyder and Martha Palmer. 2004. The English all-words task. In Proceedings of the 3rd International Workshop on the Evaluation of Systems for the Semantic Analysis of Text (Senseval-3), pages 41–43, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia M Taylor</author>
<author>Lawrence J Mazlack</author>
</authors>
<title>Computationally recognizing wordplay in jokes.</title>
<date>2004</date>
<booktitle>In Proceedings of the 26th Annual Conference of the Cognitive Science Society (CogSci</booktitle>
<pages>1315--1320</pages>
<contexts>
<context position="13633" citStr="Taylor and Mazlack (2004)" startWordPosition="2140" endWordPosition="2143"> is some previous research on computational detection and comprehension of humour, though by and large it is not concerned specifically with puns; those studies which do analyze puns tend to have a phonological or syntactic rather than semantic bent. In this subsection we briefly review some prior work which is relevant to ours. Yokogawa (2002) describes a system for detecting the presence of puns in Japanese text. However, this work is concerned only with puns which are both imperfect and ungrammatical, relying on syntactic cues rather than the lexical-semantic information we propose to use. Taylor and Mazlack (2004) 721 describe an n-gram–based approach for recognizing when imperfect puns are used for humorous effect in a certain narrow class of English knockknock jokes. Their focus on imperfect puns and their use of a fixed syntactic context makes their approach largely inapplicable to perfect puns in running text. Mihalcea and Strapparava (2005) treat humour recognition as a classification task, employing various machine learning techniques on humour-specific stylistic features such as alliteration and antonymy. Of particular interest is their follow-up analysis (Mihalcea and Strapparava, 2006), where </context>
</contexts>
<marker>Taylor, Mazlack, 2004</marker>
<rawString>Julia M. Taylor and Lawrence J. Mazlack. 2004. Computationally recognizing wordplay in jokes. In Proceedings of the 26th Annual Conference of the Cognitive Science Society (CogSci 2004), pages 1315– 1320, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of the 3rd Conference of the North American Chapter of the Association for Computational Linguistics and the 9th Human Language Technologies Conference (HLT-NAACL</booktitle>
<pages>252--259</pages>
<contexts>
<context position="27725" citStr="Toutanova et al., 2003" startWordPosition="4412" endWordPosition="4415">he first is motivated by the informal observation that, though the two meanings of a pun may have different parts of speech, at least one of the parts 5In our implementation, the sense definitions are formed by concatenating the synonyms, gloss, and example sentences provided by WordNet. 724 of speech is grammatical in the context of the sentence, and so would probably be the one assigned by a stochastic or rule-based POS tagger. Our “POS” tie-breaker therefore preferentially selects the best sense, or pair of senses, whose POS matches the one applied to the target by the Stanford POS tagger (Toutanova et al., 2003). For our second tie-breaking strategy, we posit that since humour derives from the resolution of semantic incongruity (Raskin, 1985; Attardo, 1994), puns are more likely to exploit coarse-grained homonymy than than fine-grained systematic polysemy. Thus, following Matuschek et al. (2014), we induced a clustering of WordNet senses by aligning WordNet to the more coarse-grained OmegaWiki LSR.6 Our “cluster” fallback works the same as the “POS” one, with the addition that any remaining ties among senses with the second-highest overlap are resolved by preferentially selecting those which are not </context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proceedings of the 3rd Conference of the North American Chapter of the Association for Computational Linguistics and the 9th Human Language Technologies Conference (HLT-NAACL 2003), pages 252–259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Valitutti</author>
<author>Carlo Strapparava</author>
<author>Oliviero Stock</author>
</authors>
<title>Textual affect sensing for computational advertising.</title>
<date>2008</date>
<booktitle>In Proceedings of the AAAI Spring Symposium on Creative Intelligent Systems,</booktitle>
<pages>117--122</pages>
<contexts>
<context position="3782" citStr="Valitutti et al., 2008" startWordPosition="578" endWordPosition="581">phonological properties of puns. A fundamental problem which has not yet been as widely studied is the automatic detection and identification of intentional lexical ambiguity—that is, given a text, does it contain any lexical items which are used in a deliberately ambiguous manner, and if so, what are the intended meanings? We consider these to be important research questions with a number of real-world applications. For instance, puns are particularly common in advertising, where they are used not only to create humour but also to induce in the audience a valenced attitude toward the target (Valitutti et al., 2008). Recognizing instances of such lexical ambiguity and understanding their affective connotations would be of benefit to systems performing sentiment analysis on persuasive texts. Wordplay is also a perennial topic of scholarship in literary criticism and analysis. To give just one example, puns are one of the most intensively studied aspects of Shakespeare’s rhetoric, and laborious manual counts have shown their frequency in certain 719 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processi</context>
</contexts>
<marker>Valitutti, Strapparava, Stock, 2008</marker>
<rawString>Alessandro Valitutti, Carlo Strapparava, and Oliviero Stock. 2008. Textual affect sensing for computational advertising. In Proceedings of the AAAI Spring Symposium on Creative Intelligent Systems, pages 117–122, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Venour</author>
</authors>
<title>The computational generation of a class of puns. Master’s thesis,</title>
<date>1999</date>
<institution>Queen’s University,</institution>
<location>Kingston, ON.</location>
<contexts>
<context position="16103" citStr="Venour, 1999" startWordPosition="2538" endWordPosition="2539">h one of their possible definitions was maximized. The overlap scores for all word pairs were then averaged, and the punchline with the lowest average score selected as the most humorous. 2.3.2 Corpora There are a number of English-language corpora of intentional lexical ambiguity which have been used in past work, usually in linguistics or the social sciences. In their work on computer-generated humour, Lessard et al. (2002) use a corpus of 374 “Tom Swifty” puns taken from the Internet, plus a well-balanced corpus of 50 humorous and nonhumorous lexical ambiguities generated programmatically (Venour, 1999). Hong and Ong (2009) also study humour in natural language generation, using a smaller data set of 27 punning riddles derived from a mix of natural and artificial sources. In their study of wordplay in religious advertising, Bell et al. (2011) compile a corpus of 373 puns taken from church marquees and literature, and compare it against a general corpus of 1515 puns drawn from Internet websites and a specialized dictionary. Zwicky and Zwicky (1986) conduct a phonological analysis on a corpus of several thousand puns, some of which they collected themselves from advertisements and catalogues, </context>
</contexts>
<marker>Venour, 1999</marker>
<rawString>Chris Venour. 1999. The computational generation of a class of puns. Master’s thesis, Queen’s University, Kingston, ON.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Toshihiko Yokogawa</author>
</authors>
<title>Japanese pun analyzer using articulation similarities.</title>
<date>2002</date>
<booktitle>In Proceedings of the 11th IEEE International Conference on Fuzzy Systems (FUZZ</booktitle>
<volume>2</volume>
<pages>1114--1119</pages>
<contexts>
<context position="13354" citStr="Yokogawa (2002)" startWordPosition="2096" endWordPosition="2097"> interests are in lexical semantics rather than phonology, we focus on puns which are homographic and monolexemic. This allows us to investigate the problem of pun identification in as controlled a setting as possible. 2.3 Previous work 2.3.1 Computational humour There is some previous research on computational detection and comprehension of humour, though by and large it is not concerned specifically with puns; those studies which do analyze puns tend to have a phonological or syntactic rather than semantic bent. In this subsection we briefly review some prior work which is relevant to ours. Yokogawa (2002) describes a system for detecting the presence of puns in Japanese text. However, this work is concerned only with puns which are both imperfect and ungrammatical, relying on syntactic cues rather than the lexical-semantic information we propose to use. Taylor and Mazlack (2004) 721 describe an n-gram–based approach for recognizing when imperfect puns are used for humorous effect in a certain narrow class of English knockknock jokes. Their focus on imperfect puns and their use of a fixed syntactic context makes their approach largely inapplicable to perfect puns in running text. Mihalcea and S</context>
</contexts>
<marker>Yokogawa, 2002</marker>
<rawString>Toshihiko Yokogawa. 2002. Japanese pun analyzer using articulation similarities. In Proceedings of the 11th IEEE International Conference on Fuzzy Systems (FUZZ 2002), volume 2, pages 1114–1119, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arnold M Zwicky</author>
<author>Elizabeth D Zwicky</author>
</authors>
<title>Imperfect puns, markedness, and phonological similarity: With fronds like these, who needs anemones? Folia Linguistica,</title>
<date>1986</date>
<pages>20--3</pages>
<contexts>
<context position="16556" citStr="Zwicky and Zwicky (1986)" startWordPosition="2614" endWordPosition="2617"> of 374 “Tom Swifty” puns taken from the Internet, plus a well-balanced corpus of 50 humorous and nonhumorous lexical ambiguities generated programmatically (Venour, 1999). Hong and Ong (2009) also study humour in natural language generation, using a smaller data set of 27 punning riddles derived from a mix of natural and artificial sources. In their study of wordplay in religious advertising, Bell et al. (2011) compile a corpus of 373 puns taken from church marquees and literature, and compare it against a general corpus of 1515 puns drawn from Internet websites and a specialized dictionary. Zwicky and Zwicky (1986) conduct a phonological analysis on a corpus of several thousand puns, some of which they collected themselves from advertisements and catalogues, and the remainder of which were taken from previously published collections. Two studies on cognitive strategies used by second language learners (Kaplan and Lucas, 2001; Lucas, 2004) used a data set of 58 jokes compiled from newspaper comics, 32 of which rely on lexical ambiguity. Bucaria (2004) conducts a linguistic analysis of a set of 135 humorous newspaper headlines, about half of which exploit lexical ambiguity. Such data sets—particularly the</context>
</contexts>
<marker>Zwicky, Zwicky, 1986</marker>
<rawString>Arnold M. Zwicky and Elizabeth D. Zwicky. 1986. Imperfect puns, markedness, and phonological similarity: With fronds like these, who needs anemones? Folia Linguistica, 20(3–4):493–503.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>