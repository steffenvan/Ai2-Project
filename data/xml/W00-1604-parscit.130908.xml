<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010608">
<title confidence="0.988928">
Measuring efficiency in high-accuracy,
broad-coverage statistical parsing*
</title>
<author confidence="0.946177">
Brian Roark Eugene Charniak
</author>
<note confidence="0.749442333333333">
Brown Laboratory for Linguistic Information Processing (BLLIP), and
Cognitive and Linguistic Sciences Computer Science
Box 1978 Box 1910
</note>
<affiliation confidence="0.882473">
Brown University Brown University
</affiliation>
<address confidence="0.830039">
Providence, RI 02912 Providence, RI 02912
</address>
<email confidence="0.640614">
brian-roarkObrown.edu ec@cs . brown . edu
</email>
<sectionHeader confidence="0.933973" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.981889066666667">
Very little attention has been paid to the
comparison of efficiency between high accu-
racy statistical parsers. This paper proposes
one machine-independent metric that is general
enough to allow comparisons across very differ-
ent parsing architectures. This metric, which
we call &amp;quot;events considered&amp;quot;, measures the num-
ber of &amp;quot;events&amp;quot;, however they are defined for a
particular parser, for which a probability must
be calculated, in order to find the parse. It is
applicable to single-pass or multi-stage parsers.
We discuss the advantages of the metric, and
demonstrate its usefulness by using it to com-
pare two parsers which differ in several funda-
mental ways.
</bodyText>
<sectionHeader confidence="0.996299" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999946647058823">
The past five years have seen enormous im-
provements in broad-coverage parsing accuracy,
through the use of statistical techniques. The
parsers that perform at the highest level of ac-
curacy (Charniak (1997; 2000); Collins (1997;
2000); Ratnaparkhi, 1997) use probabilistic
models with a very large number of parame-
ters, which can be costly to use in evaluating
structures. Parsers that have been built for
this level of accuracy have generally been com-
pared only with respect to accuracy, not effi-
ciency. This is understandable: their great sell-
ing point is the high level of accuracy they are
able to achieve. In addition, these parsers are
difficult to compare with respect to efficiency:
the models are quite diverse, with very differ-
ent kinds of parameters and different estimation
</bodyText>
<footnote confidence="0.8127016">
* Thanks to everyone in the Brown Laboratory for Lin-
guistic Information Processing (BLLIP) for valuable dis-
cussion on these issues. This research was supported in
part by NSF IGERT Grant #DGE-9870676, and NSF
LIS Grant #SBR-9720368.
</footnote>
<bodyText confidence="0.99537017948718">
and smoothing techniques. Furthermore, the
search and pruning strategies have been quite
varied, from beam-search to best-first, and with
different numbers of distinct stages of process-
ing.
At a very general level, however, these ap-
proaches share some key characteristics, and it
is at this general level that we would like to ad-
dress the issue of efficiency. In each of these
approaches, scores or weights are calculated for
events, e.g. edges or other structures, or per-
haps constituent/head or even head/head rela-
tions. The scores for these events are compared
and &amp;quot;bad&amp;quot; events, i.e. events with relatively low
scores, are either discarded (as in beam search)
or sink to the bottom of the heap (as in best-
first). In fact, this general characterization is
basically what goes on at each of the stages
in multi-stage parsers, although the events that
are being weighted, and the models by which
they are scored, may change&apos;. In each parser&apos;s
final stage, the parse which emerges with the
best score is returned for evaluation.
We would like to propose an efficiency met-
ric which we call events considered. An event
is considered when a score is calculated for it.
Search and pruning techniques can be judged to
improve the efficiency of a parser if they reduce
the number of events that must be considered en
route to parses with the same level of accuracy.
Because an event must have a score for a statis-
tical parser to decide whether it should be re-
tained or discarded, there is no way to improve
this number without having improved either the
efficiency of the search (through, say, dynamic
programming) or the efficacy of the pruning
We will argue that this is not the case with
&apos;Even within the same stage, events can be heteroge-
neous. See the discussion of the EC parser below.
</bodyText>
<page confidence="0.997757">
29
</page>
<bodyText confidence="0.999835220779221">
competitor measures, such as time or total heap
operations, which can be improved through op-
timization techniques that do not change the
search space. This is not to say that these tech-
niques do not have a great deal of value; sim-
ply that, for comparisons between approaches
to statistical parsing, the implementations of
which may or may not have carried out the
same optimizations, they are less informative
than the metric we have proposed.
Some recent papers on efficiency in statisti-
cal parsing have looked at the number of pops
from a heap as the relevant measure of effi-
ciency (Caraballo and Charniak, 1998; Char-
niak, Goldwater, and Johnson, 1998; Blaheta
and Charniak, 1999), and have demonstrated
techniques for improving the scoring function so
that this number is dramatically reduced. This
is also a score that cannot be &amp;quot;artificially&amp;quot; re-
duced through optimization. It may very well
be, however, that some significant part of a
parser&apos;s function is not an operation on a heap.
For example, a parser could run a part-of-speech
(POS) tagger over the string as a first stage.
What is relevant for this first stage are the num-
ber of (POS,word) pairs that must be considered
by the tagger. Each of these pairs would have a
score calculated for them, and would hence be
an event considered. The events in the second
stage may be, for example, edges in the chart. A
parser&apos;s efficiency score would be the total num-
ber of these considered events across all stages.
The principle merits of this metric are that
it is general enough to cover different search
and pruning techniques (including exhaustive
parsing); that it is machine-independent; and
that it is, to a certain extent, implementation-
independent. The last of these might be what
recommends the metric most, insofar as it is not
the case for other simple metrics. For example,
using time as a metric is perfectly general, and
there are ways to normalize for processor differ-
ences (see Moore, 2000b). However, unless one
is comparing two implementations that are es-
sentially identical in all incidental ways, it is not
possible to normalize for certain specifics of the
implementation. For example, how probabili-
ties are accessed, upon which processing time
is very dependent, can differ from implementa-
tion to implementation (see discussion below).
Thus, while time may be ideal for highly con-
trolled studies of relatively similar algorithms
(as in Moore, 2000a), its applicability for com-
paring diverse parsers is problematic.
Let us consider a specific example: calculat-
ing scores from highly conditioned, interpolated
probability distributions. First we will discuss
conditional probability models, followed by an
illustration of interpolation.
A simple probabilistic context free grammar
(PUG) is a context free grammar with a prob-
ability assigned to each rule: the probability of
the righthand side of the rule given the lefthand
side of the rule. These probabilities can be esti-
mated via their relative frequency in a corpus of
trees. For instance, we can assign a probability
to the rule S —&gt; NP VP by counting the number
of occurrences of this rule in the corpus, and
dividing by the total number of S nodes in the
corpus. We can improve the probability model
if we add in more conditioning events beyond
the lefthand side of the rule. For example, if we
throw in the parent of the lefthand side in the
tree within which it appears, we can immedi-
ately see a dramatic improvement in the maxi-
mum likelihood parse (Johnson, 1998). That is,
instead of:
</bodyText>
<equation confidence="0.999148">
P(RHSILHS,Puris) =
P(LHS,Pmis)
</equation>
<bodyText confidence="0.999985736842105">
where Pm&apos;s is the parent above the lefthand
side of the rule. This additional conditioning
event allows us to capture the fact that the dis-
tribution of, say, S node expansions underneath
VPs is quite different than that of S nodes at
the root of the tree. The models that we will be
discussing in this paper condition on many such
events, somewhere between five and ten. This
can lead to sparse data problems, necessitating
some kind of smoothing - in these cases, deleted
interpolation.
The idea behind deleted interpolation (Je-
linek and Mercer, 1980) is simple: mix the em-
pirically observed probability using n condition-
ing events with lower order models. The mixing
coefficients, AT, , are functions of the frequency of
the joint occurrence of the conditioning events,
estimated from a held out portion of the cor-
pus. Let e0 be the event whose probability is
</bodyText>
<equation confidence="0.992563">
P(LHS,RHS)
P(RHS1LHS) =
P(LHS)
</equation>
<bodyText confidence="0.7330955">
the probability of the rule instance is:
P(LHS,RHS,PEris)
</bodyText>
<page confidence="0.97126">
30
</page>
<bodyText confidence="0.9981924">
to be conditioned, ei ... en the n conditioning
events used in the model, and P the empirically
observed conditional probability. Then the fol-
lowing is a recursive definition of the interpo-
lated probability:
</bodyText>
<equation confidence="0.998737">
P(eo en) = An(ei en) Neo en) +
(1—(ei en))P(eo en—i)
</equation>
<bodyText confidence="0.999977024390244">
This has been shown to be very effective in cir-
cumstances where sparse data requires smooth-
ing to avoid assigning a probability of zero to
a large number of possible events that happen
not to have been observed in the training data
with the n conditioning events.
Using such a mode12, the time to calculate a
particular conditional probability can be signif-
icant. There are a variety of techniques that
can be used to speed this up, such as pre-
compilation or caching. These techniques can
have a fairly large effect on the time of computa-
tion, but they contribute little to a comparison
between pruning techniques or issues of search.
More generally, optimization and lack of it is
something that can obscure algorithm similari-
ties or differences, over and above differences in
machine or platform. Researchers whose inter-
est lies in improving parser accuracy might not
care to improve the efficiency once it reaches an
acceptable level. This should not bar us from
trying to compare their techniques with regards
to efficiency.
Another such example contrasts our metric
with one that measures total heap operations.
Depending on the pruning method, it might be
possible to evaluate an event&apos;s probability and
throw it away if it falls below some threshold,
rather than pushing it onto the heap. Another
option in the same circumstance is to simply
push all analyses onto the heap, and let the heap
ranking decide if they ever surface again. Both
have their respective time trade-offs (the cost of
thresholding versus heap operations), and which
is chosen is an implementation issue that is or-
thogonal to the relative search efficiency that we
would like to evaluate.
In contrast to time or total heap operations,
there is no incidental optimization that allows
the parser to avoid calculating scores for analy-
ses. A statistical parser that prunes the search
</bodyText>
<footnote confidence="0.7552095">
2The same points hold for other smoothing methods,
such as backing off.
</footnote>
<bodyText confidence="0.99097574">
space cannot perform this pruning without scor-
ing events that must be either retained or dis-
carded. A reduction in events considered with-
out a loss of accuracy counts as a novel search
or pruning technique, and as such should be ex-
plicitly evaluated as a competitor strategy. The
basic point that we are making here is that our
metric measures that which is central to statisti-
cal parsing techniques, and not something that
can be incidentally improved.
In the next section, we outline two quite dif-
ferent statistical parsers, and present some re-
sults using our new metric.
2 Comparing statistical parsers
To illustrate the utility of this metric for com-
paring the efficiency of radically different ap-
proaches to broad-coverage parsing, we will
contrast some results from a two-stage best-
first parser (Charniak, 2000) with a single-pass
left-to-right, incremental beam-search parser
(Roark, 2000). Both of these parsers (which
we will refer to, henceforth, as the EC and BR
parsers, respectively) score between 85 and 90
percent average precision and recall; both con-
dition the probabilities of events on a large num-
ber of contextual parameters in more-or-less the
way outlined above; and both use boundary
statistics to assign partial structures a figure-
of-merit, which is the product of the probability
of the structure in its own right and a score for
its likelihood of integrating with its surrounding
context.
Both of the parsers also use parameterized
pruning strategies, which will be described
when the parsers are outlined. Results will be
presented for each parser at a range of parame-
ter values, to give a sense of the behavior of the
parser as more or fewer events are taken into
consideration. From this data, we shall be able
to see the degree to which the events consid-
ered score correlates with time, as well as the
convergence in accuracy.
The parsers were trained on sections 2-21
and tested on section 23 of the Penn Wall
St. Journal Treebank (Marcus, Santorini, and
Marcinkiewicz, 1993), which are the standards
in the statistical parsing literature. Accuracy
is reported in terms of average labelled pre-
cision and recall. Precision is the number of
correct constituents divided by the number of
</bodyText>
<page confidence="0.998566">
31
</page>
<table confidence="0.9963811">
section 23: 2416 sentences of length &lt; 100
Average length: 23.46 words/sentence
Times past Avg. Events Time in
first parse Prec/Rec Considered f secondst
21 89.7 212,014 26.7
13 89.6 107,221 14.0
7.5 89.1 48,606 6.7
2.5 86.8 9,621 1.5
2 85.6 6,826 1.1
per sentence
</table>
<tableCaption confidence="0.9513765">
Table 1: Results from the EC parser at different
initial parameter values
</tableCaption>
<bodyText confidence="0.999894285714286">
constituents proposed by the parser. Recall is
the number of correct constituents divided by
the number of constituents in the actual parse.
Labelled precision and recall counts only non-
part-of-speech non-terminal constituents. The
two numbers are generally quite close, and are
averaged to give a single composite score.
</bodyText>
<subsectionHeader confidence="0.973911">
2.1 EC parser
</subsectionHeader>
<bodyText confidence="0.999996775862069">
The EC parser first prunes the search space by
building a chart containing only the most likely
edges. Each new edge is assigned a figure-of-
merit (FOM) and pushed onto a heap. The
FOM is the product of the probability of the con-
stituent given the simple PCFG and the bound-
ary statistics. Edges that are popped from the
heap are put into the chart, and standard chart
building occurs, with new edges being pushed
onto the heap. This process continues until a
complete parse is found; hence this is a best-first
approach. Of course, the chart building does
not necessarily need to stop when the first parse
is found; it can continue until some stopping cri-
terion is met. The criterion that was used in the
trials that will be reported here is a multiple of
the number of edges that were present in the
chart when the first parse was found. Thus, if
the parameter is 1, the parser stops when the
first parse is found; if the parameter is 10, the
parser stops when the number of edges in the
chart is ten times the number that were in the
chart when the first parse was found.
This is the first stage of the parser. The
second stage takes all of the parses packed in
the chart that are above a certain probability
threshold given the PCFG, and assigns a score
using the full probability model. To evaluate
the probability of each parse, the evaluation
proceeds from the top down. Given a particu-
lar constituent, it first evaluates the probability
of the part-of-speech of the head of that con-
stituent, conditioned on a variety of contextual
information from the context. Next, it eval-
uates the probability of the head itself, given
the part-of-speech that was just predicted (plus
other information). Finally, it evaluates the
probability of the rule expansion, conditioned
on, among other things, the POS of the head
and the head. It then moves down the tree to
evaluate the newly predicted constituents. See
Charniak (2000) for more details on the specifics
of the parser.
Notice that the events are heterogeneous.
One of the key events in the model is the con-
stituent/head relation, which is not an edge.
Note also that this two-stage search strategy
means that many edges will be considered mul-
tiple times, once by the first stage and in every
complete parse within which they occur in the
second stage, and hence will be counted multi-
ple times by our metric.
The parse with the best score is returned for
evaluation in terms of precision and recall. Ta-
ble 1 shows accuracy and efficiency results when
the EC parser is run at various initial parameter
values, i.e. the number of times past the first
parse the first-stage of the parser continues.
</bodyText>
<subsectionHeader confidence="0.998545">
2.2 BR parser
</subsectionHeader>
<bodyText confidence="0.999979571428572">
The BR parser proceeds from left-to-right across
the string, building analyses top-down in a sin-
gle pass. While its accuracy is several points be-
low that of the EC parser, it is useful in circum-
stances requiring incremental processing, e.g.
on-line speech recognition, where a multi-stage
parser is not an option.
Very briefly, partial analyses are ranked by
a figure-of-merit that is the product of their
probability (using the full conditional probabil-
ity model) and a look-ahead probability, which
is a measure of the likelihood of the current
stack state of an analysis rewriting to the look-
ahead word at its left-corner. Partial analy-
ses are popped from the heap, expanded, and
pushed back onto the heap. When an analysis
is found that extends to the look-ahead word, it
is pushed onto a new heap, which collects these
&amp;quot;successful&amp;quot; analyses until there are &amp;quot;enough&amp;quot;,
at which point the look-ahead is moved to the
next word in the string, and all of the &amp;quot;unsuc-
</bodyText>
<page confidence="0.996185">
32
</page>
<table confidence="0.9947909">
section 23: 2416 sentences of length &lt; 100
Average length: 23.46 words/sentence
Base Beam Avg. Events Time in Pct. failed
Factor Prec/Rec Considered t secondst
10-12 85.9 265,509 7.6 1.3
10—&amp;quot; 85.7 164,127 4.3 1.7
10-10 85.3 100,439 2.7 2.2
10-8 84.3 36,861 0.9 3.8
10-6 81.8 13,512 0.4 7.1
per sentence
</table>
<tableCaption confidence="0.999791">
Table 2: Results from the BR parser at different initial parameter values
</tableCaption>
<bodyText confidence="0.999089375">
cessful&amp;quot; analyses are discarded. This is a beam-
search, and the criterion by which it is judged
that &amp;quot;enough&amp;quot; analyses have succeeded can be
either narrow (i.e. stopping early) or wide (i.e.
stopping late). The unpruned parse with the
highest probability that successfully covers the
entire input string is evaluated for accuracy.
The beam parameter in the trials that will be
reported here, is called the base beam factor,
and it works as follows. Let 0 be the base beam
factor, and let /3 be the probability of the high-
est ranked &amp;quot;successful&amp;quot; parse. Then any analy-
sis whose probability falls below a,(315, where a
is the cube of the number of successful analyses,
is discarded. The basic idea is that we want the
beam to be very wide if there are few analyses
that have extended to the current look-ahead
word, but relatively narrow if many such anal-
yses have been found. Thus, if 0 = 10-12, and
100 analyses have extended to the current look-
ahead word, then a candidate analysis must
have a probability above 10-6/5 to avoid being
pruned. After 1000 candidates, the beam has
narrowed to 10-3/3. Table 2 shows accuracy and
efficiency results when the BR parser is run at
various base beam factors. See Roark (2000) for
more details on the specifics of this parser.
The conditional probability model that is
used in the BR parser is constrained by the left-
to-right nature of the algorithm. Whereas the
conditional probability model used in the sec-
ond stage of the EC parser has access to the full
parse trees, and thus can condition the struc-
tures with information from either the left or
right context, any model used in the BR parser
can only use information from the left-context,
since that is all that has been built at the mo-
ment the probability of a structure is evaluated.
For example, a subject NP can be conditioned
on the head of the sentence (usually the main
verb) in the EC parser, but not in the BR parser,
since the head of sentence has yet to be encoun-
tered. This accounts for some of the accuracy
difference between the two parsers. Also, note
that the BR parser can and does fail to find a
parse in some percentage of cases, as a conse-
quence of the incremental beam-search. This
percentage is reported as well.
</bodyText>
<sectionHeader confidence="0.997352" genericHeader="introduction">
3 Discussion
</sectionHeader>
<bodyText confidence="0.9998302">
The number of ways in which these two parsers
differ is large, and many of these differences
make it difficult to compare their relative ef-
ficiency. A partial list of these complicating dif-
ferences is the following:
</bodyText>
<listItem confidence="0.939888916666667">
• Best-first vs. beam search pruning strat-
egy, which impacts the number of events
that must be retained
• Two-stage vs. single pass parsing
• Heterogeneous events, within and between
parsers
• Different conditional probability models,
with different numbers of conditioning
events, and slightly different methods of in-
terpolation
• EC parser written in C++; BR parser writ-
ten in C
</listItem>
<bodyText confidence="0.999792833333333">
In addition, for these runs, the EC parser par-
allelized the processing by sending each sentence
individually off to different processors on the
network, whereas the BR parser was run on a
single computing server. Since for the EC parser
we do not know which sentence went to which
</bodyText>
<page confidence="0.989766">
33
</page>
<figure confidence="0.994206448275862">
x 105
Events Considered per Sentence
5 x 104
BR parser runs
EC parser runs
0
1 2 3 4 5
Parser Run
0
0 0.5 1 1.5 2 2.5 3
Events Considered per Second
Seconds
20
15
10
5
4.5
4
3.5
3
2.5
2
1.5
1
0.5
30
25
BR parser runs
EC parser runs
Events Considered per Sentence
x 105
13
12
11
10
0 0.5 1 1.5 2 2.5 3
Events Considered per Sentence
100
x 105
95
0 0.5 1 1.5 2 2.5 3
BR parser runs
EC parser runs
Parser Error
16
15
14
BR parser runs
EC parser runs
99
Percent of Best Accuracy
98
97
96
20
19
18
17
</figure>
<sectionHeader confidence="0.737183" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999558940298507">
Black, E., S. Abney, D. Flickenger, C. Gdaniec,
R. Grishman, P. Harrison, D. Hindle, R. In-
gria, F. Jelinek, J. Klavans, M. Liberman,
M. Marcus, S. Roukos, B. Santorini, and
T. Strzalkowski. 1991. A procedure for quan-
titatively comparing the syntactic coverage
of english grammars. In DARPA Speech and
Natural Language Workshop, pages 306-311.
Blaheta, D. and E. Charniak. 1999. Automatic
compensation for parser figure-of-merit flaws.
In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguis-
tics, pages 513-518.
Caraballo, S. and E. Charniak. 1998. New
figures of merit for best-first probabilistic
chart parsing. Computational Linguistics,
24(2):275-298.
Charniak, E. 1997. Statistical parsing with a
context-free grammar and word statistics. In
Proceedings of the Fourteenth National Con-
ference on Artificial Intelligence, Menlo Park.
AAAI Press/MIT Press.
Charniak, E. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st
Conference of the North American Chapter of
the Association for Computational Linguis-
tics.
Charniak, E., S. Goldwater, and M. Johnson.
1998. Edge-based best-first chart parsing. In
Proceedings of the Sixth Workshop on Very
Large Corpora, pages 127-133.
Collins, M.J. 1997. Three generative, lexi-
calised models for statistical parsing. In The
Proceedings of the 35th Annual Meeting of
the Association for Computational Linguis-
tics, pages 16-23.
Collins, M.J. 2000. Discriminative reranking
for natural language parsing. In The Proceed-
ings of the 17th International Conference on
Machine Learning.
Jelinek, F. and R.L. Mercer. 1980. Interpo-
lated estimation of markov source parame-
ters from sparse data. In Proceedings of the
Workshop on Pattern Recognition in Prac-
tice, pages 381-397.
Johnson, M. 1998. PCFG models of linguistic
tree representations. Computational Linguis-
tics, 24(4):617-636.
Marcus, M.P., B. Santorini, and M.A.
Marcinkiewicz. 1993. Building a large anno-
tated corpus of English: The Penn Treebank.
Computational Linguistics, 19(2):313-330.
Moore, R. 2000a. Improved left-corner chart
parsing for large context-free grammars. In
Proceedings of the Sixth International Work-
shop on Parsing Technologies, pages 171-182.
Moore, R. 2000b. Time as a measure of parsing
efficiency. In Proceedings of the COLING-00
Workshop on Efficiency in Large-scale pars-
ing systems.
Ratnaparkhi, A. 1997. A linear observed time
statistical parser based on maximum entropy
models. In Proceedings of the Second Confer-
ence on Empirical Methods in Natural Lan-
guage Processing, pages 1-10.
Roark, B. 2000. Probabilistic top-down parsing
and language modeling. Submitted.
</reference>
<page confidence="0.998933">
36
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.808294">
<title confidence="0.9959255">Measuring efficiency in high-accuracy, broad-coverage statistical parsing*</title>
<author confidence="0.999672">Brian Roark Eugene Charniak</author>
<affiliation confidence="0.984346">Brown Laboratory for Linguistic Information Processing (BLLIP), and Cognitive and Linguistic Sciences Computer Science</affiliation>
<address confidence="0.99683">Box 1978 Box 1910</address>
<affiliation confidence="0.99917">Brown University Brown University</affiliation>
<address confidence="0.916375">RI 02912 02912</address>
<email confidence="0.968213">brian-roarkObrown.eduec@cs.brown.edu</email>
<abstract confidence="0.99667325">little attention has been paid to comparison of efficiency between high accuracy statistical parsers. This paper proposes one machine-independent metric that is general enough to allow comparisons across very different parsing architectures. This metric, which we call &amp;quot;events considered&amp;quot;, measures the numof however they are defined for a particular parser, for which a probability must be calculated, in order to find the parse. It is applicable to single-pass or multi-stage parsers. We discuss the advantages of the metric, and demonstrate its usefulness by using it to compare two parsers which differ in several fundamental ways.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Black</author>
<author>S Abney</author>
<author>D Flickenger</author>
<author>C Gdaniec</author>
<author>R Grishman</author>
<author>P Harrison</author>
<author>D Hindle</author>
<author>R Ingria</author>
<author>F Jelinek</author>
<author>J Klavans</author>
<author>M Liberman</author>
<author>M Marcus</author>
<author>S Roukos</author>
<author>B Santorini</author>
<author>T Strzalkowski</author>
</authors>
<title>A procedure for quantitatively comparing the syntactic coverage of english grammars.</title>
<date>1991</date>
<booktitle>In DARPA Speech and Natural Language Workshop,</booktitle>
<pages>306--311</pages>
<marker>Black, Abney, Flickenger, Gdaniec, Grishman, Harrison, Hindle, Ingria, Jelinek, Klavans, Liberman, Marcus, Roukos, Santorini, Strzalkowski, 1991</marker>
<rawString>Black, E., S. Abney, D. Flickenger, C. Gdaniec, R. Grishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek, J. Klavans, M. Liberman, M. Marcus, S. Roukos, B. Santorini, and T. Strzalkowski. 1991. A procedure for quantitatively comparing the syntactic coverage of english grammars. In DARPA Speech and Natural Language Workshop, pages 306-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Blaheta</author>
<author>E Charniak</author>
</authors>
<title>Automatic compensation for parser figure-of-merit flaws.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>513--518</pages>
<contexts>
<context position="4529" citStr="Blaheta and Charniak, 1999" startWordPosition="736" endWordPosition="739">ons, which can be improved through optimization techniques that do not change the search space. This is not to say that these techniques do not have a great deal of value; simply that, for comparisons between approaches to statistical parsing, the implementations of which may or may not have carried out the same optimizations, they are less informative than the metric we have proposed. Some recent papers on efficiency in statistical parsing have looked at the number of pops from a heap as the relevant measure of efficiency (Caraballo and Charniak, 1998; Charniak, Goldwater, and Johnson, 1998; Blaheta and Charniak, 1999), and have demonstrated techniques for improving the scoring function so that this number is dramatically reduced. This is also a score that cannot be &amp;quot;artificially&amp;quot; reduced through optimization. It may very well be, however, that some significant part of a parser&apos;s function is not an operation on a heap. For example, a parser could run a part-of-speech (POS) tagger over the string as a first stage. What is relevant for this first stage are the number of (POS,word) pairs that must be considered by the tagger. Each of these pairs would have a score calculated for them, and would hence be an eve</context>
</contexts>
<marker>Blaheta, Charniak, 1999</marker>
<rawString>Blaheta, D. and E. Charniak. 1999. Automatic compensation for parser figure-of-merit flaws. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 513-518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Caraballo</author>
<author>E Charniak</author>
</authors>
<title>New figures of merit for best-first probabilistic chart parsing.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<pages>24--2</pages>
<contexts>
<context position="4460" citStr="Caraballo and Charniak, 1998" startWordPosition="726" endWordPosition="729">rser below. 29 competitor measures, such as time or total heap operations, which can be improved through optimization techniques that do not change the search space. This is not to say that these techniques do not have a great deal of value; simply that, for comparisons between approaches to statistical parsing, the implementations of which may or may not have carried out the same optimizations, they are less informative than the metric we have proposed. Some recent papers on efficiency in statistical parsing have looked at the number of pops from a heap as the relevant measure of efficiency (Caraballo and Charniak, 1998; Charniak, Goldwater, and Johnson, 1998; Blaheta and Charniak, 1999), and have demonstrated techniques for improving the scoring function so that this number is dramatically reduced. This is also a score that cannot be &amp;quot;artificially&amp;quot; reduced through optimization. It may very well be, however, that some significant part of a parser&apos;s function is not an operation on a heap. For example, a parser could run a part-of-speech (POS) tagger over the string as a first stage. What is relevant for this first stage are the number of (POS,word) pairs that must be considered by the tagger. Each of these pa</context>
</contexts>
<marker>Caraballo, Charniak, 1998</marker>
<rawString>Caraballo, S. and E. Charniak. 1998. New figures of merit for best-first probabilistic chart parsing. Computational Linguistics, 24(2):275-298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Statistical parsing with a context-free grammar and word statistics.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fourteenth National Conference on Artificial Intelligence, Menlo Park.</booktitle>
<publisher>AAAI Press/MIT Press.</publisher>
<contexts>
<context position="1245" citStr="Charniak (1997" startWordPosition="182" endWordPosition="183">which we call &amp;quot;events considered&amp;quot;, measures the number of &amp;quot;events&amp;quot;, however they are defined for a particular parser, for which a probability must be calculated, in order to find the parse. It is applicable to single-pass or multi-stage parsers. We discuss the advantages of the metric, and demonstrate its usefulness by using it to compare two parsers which differ in several fundamental ways. 1 Introduction The past five years have seen enormous improvements in broad-coverage parsing accuracy, through the use of statistical techniques. The parsers that perform at the highest level of accuracy (Charniak (1997; 2000); Collins (1997; 2000); Ratnaparkhi, 1997) use probabilistic models with a very large number of parameters, which can be costly to use in evaluating structures. Parsers that have been built for this level of accuracy have generally been compared only with respect to accuracy, not efficiency. This is understandable: their great selling point is the high level of accuracy they are able to achieve. In addition, these parsers are difficult to compare with respect to efficiency: the models are quite diverse, with very different kinds of parameters and different estimation * Thanks to everyon</context>
</contexts>
<marker>Charniak, 1997</marker>
<rawString>Charniak, E. 1997. Statistical parsing with a context-free grammar and word statistics. In Proceedings of the Fourteenth National Conference on Artificial Intelligence, Menlo Park. AAAI Press/MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>A maximum-entropyinspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="11362" citStr="Charniak, 2000" startWordPosition="1879" endWordPosition="1880">que, and as such should be explicitly evaluated as a competitor strategy. The basic point that we are making here is that our metric measures that which is central to statistical parsing techniques, and not something that can be incidentally improved. In the next section, we outline two quite different statistical parsers, and present some results using our new metric. 2 Comparing statistical parsers To illustrate the utility of this metric for comparing the efficiency of radically different approaches to broad-coverage parsing, we will contrast some results from a two-stage bestfirst parser (Charniak, 2000) with a single-pass left-to-right, incremental beam-search parser (Roark, 2000). Both of these parsers (which we will refer to, henceforth, as the EC and BR parsers, respectively) score between 85 and 90 percent average precision and recall; both condition the probabilities of events on a large number of contextual parameters in more-or-less the way outlined above; and both use boundary statistics to assign partial structures a figureof-merit, which is the product of the probability of the structure in its own right and a score for its likelihood of integrating with its surrounding context. Bo</context>
<context position="15360" citStr="Charniak (2000)" startWordPosition="2560" endWordPosition="2561">probability of each parse, the evaluation proceeds from the top down. Given a particular constituent, it first evaluates the probability of the part-of-speech of the head of that constituent, conditioned on a variety of contextual information from the context. Next, it evaluates the probability of the head itself, given the part-of-speech that was just predicted (plus other information). Finally, it evaluates the probability of the rule expansion, conditioned on, among other things, the POS of the head and the head. It then moves down the tree to evaluate the newly predicted constituents. See Charniak (2000) for more details on the specifics of the parser. Notice that the events are heterogeneous. One of the key events in the model is the constituent/head relation, which is not an edge. Note also that this two-stage search strategy means that many edges will be considered multiple times, once by the first stage and in every complete parse within which they occur in the second stage, and hence will be counted multiple times by our metric. The parse with the best score is returned for evaluation in terms of precision and recall. Table 1 shows accuracy and efficiency results when the EC parser is ru</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Charniak, E. 2000. A maximum-entropyinspired parser. In Proceedings of the 1st Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>S Goldwater</author>
<author>M Johnson</author>
</authors>
<title>Edge-based best-first chart parsing.</title>
<date>1998</date>
<booktitle>In Proceedings of the Sixth Workshop on Very Large Corpora,</booktitle>
<pages>127--133</pages>
<contexts>
<context position="4500" citStr="Charniak, Goldwater, and Johnson, 1998" startWordPosition="730" endWordPosition="735">ures, such as time or total heap operations, which can be improved through optimization techniques that do not change the search space. This is not to say that these techniques do not have a great deal of value; simply that, for comparisons between approaches to statistical parsing, the implementations of which may or may not have carried out the same optimizations, they are less informative than the metric we have proposed. Some recent papers on efficiency in statistical parsing have looked at the number of pops from a heap as the relevant measure of efficiency (Caraballo and Charniak, 1998; Charniak, Goldwater, and Johnson, 1998; Blaheta and Charniak, 1999), and have demonstrated techniques for improving the scoring function so that this number is dramatically reduced. This is also a score that cannot be &amp;quot;artificially&amp;quot; reduced through optimization. It may very well be, however, that some significant part of a parser&apos;s function is not an operation on a heap. For example, a parser could run a part-of-speech (POS) tagger over the string as a first stage. What is relevant for this first stage are the number of (POS,word) pairs that must be considered by the tagger. Each of these pairs would have a score calculated for th</context>
</contexts>
<marker>Charniak, Goldwater, Johnson, 1998</marker>
<rawString>Charniak, E., S. Goldwater, and M. Johnson. 1998. Edge-based best-first chart parsing. In Proceedings of the Sixth Workshop on Very Large Corpora, pages 127-133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In The Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>16--23</pages>
<contexts>
<context position="1267" citStr="Collins (1997" startWordPosition="185" endWordPosition="186">onsidered&amp;quot;, measures the number of &amp;quot;events&amp;quot;, however they are defined for a particular parser, for which a probability must be calculated, in order to find the parse. It is applicable to single-pass or multi-stage parsers. We discuss the advantages of the metric, and demonstrate its usefulness by using it to compare two parsers which differ in several fundamental ways. 1 Introduction The past five years have seen enormous improvements in broad-coverage parsing accuracy, through the use of statistical techniques. The parsers that perform at the highest level of accuracy (Charniak (1997; 2000); Collins (1997; 2000); Ratnaparkhi, 1997) use probabilistic models with a very large number of parameters, which can be costly to use in evaluating structures. Parsers that have been built for this level of accuracy have generally been compared only with respect to accuracy, not efficiency. This is understandable: their great selling point is the high level of accuracy they are able to achieve. In addition, these parsers are difficult to compare with respect to efficiency: the models are quite diverse, with very different kinds of parameters and different estimation * Thanks to everyone in the Brown Laborat</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Collins, M.J. 1997. Three generative, lexicalised models for statistical parsing. In The Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, pages 16-23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Collins</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2000</date>
<booktitle>In The Proceedings of the 17th International Conference on Machine Learning.</booktitle>
<marker>Collins, 2000</marker>
<rawString>Collins, M.J. 2000. Discriminative reranking for natural language parsing. In The Proceedings of the 17th International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
<author>R L Mercer</author>
</authors>
<title>Interpolated estimation of markov source parameters from sparse data.</title>
<date>1980</date>
<booktitle>In Proceedings of the Workshop on Pattern Recognition in Practice,</booktitle>
<pages>381--397</pages>
<contexts>
<context position="7951" citStr="Jelinek and Mercer, 1980" startWordPosition="1307" endWordPosition="1311">rse (Johnson, 1998). That is, instead of: P(RHSILHS,Puris) = P(LHS,Pmis) where Pm&apos;s is the parent above the lefthand side of the rule. This additional conditioning event allows us to capture the fact that the distribution of, say, S node expansions underneath VPs is quite different than that of S nodes at the root of the tree. The models that we will be discussing in this paper condition on many such events, somewhere between five and ten. This can lead to sparse data problems, necessitating some kind of smoothing - in these cases, deleted interpolation. The idea behind deleted interpolation (Jelinek and Mercer, 1980) is simple: mix the empirically observed probability using n conditioning events with lower order models. The mixing coefficients, AT, , are functions of the frequency of the joint occurrence of the conditioning events, estimated from a held out portion of the corpus. Let e0 be the event whose probability is P(LHS,RHS) P(RHS1LHS) = P(LHS) the probability of the rule instance is: P(LHS,RHS,PEris) 30 to be conditioned, ei ... en the n conditioning events used in the model, and P the empirically observed conditional probability. Then the following is a recursive definition of the interpolated pro</context>
</contexts>
<marker>Jelinek, Mercer, 1980</marker>
<rawString>Jelinek, F. and R.L. Mercer. 1980. Interpolated estimation of markov source parameters from sparse data. In Proceedings of the Workshop on Pattern Recognition in Practice, pages 381-397.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
</authors>
<title>PCFG models of linguistic tree representations.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<pages>24--4</pages>
<contexts>
<context position="4500" citStr="Johnson, 1998" startWordPosition="734" endWordPosition="735">al heap operations, which can be improved through optimization techniques that do not change the search space. This is not to say that these techniques do not have a great deal of value; simply that, for comparisons between approaches to statistical parsing, the implementations of which may or may not have carried out the same optimizations, they are less informative than the metric we have proposed. Some recent papers on efficiency in statistical parsing have looked at the number of pops from a heap as the relevant measure of efficiency (Caraballo and Charniak, 1998; Charniak, Goldwater, and Johnson, 1998; Blaheta and Charniak, 1999), and have demonstrated techniques for improving the scoring function so that this number is dramatically reduced. This is also a score that cannot be &amp;quot;artificially&amp;quot; reduced through optimization. It may very well be, however, that some significant part of a parser&apos;s function is not an operation on a heap. For example, a parser could run a part-of-speech (POS) tagger over the string as a first stage. What is relevant for this first stage are the number of (POS,word) pairs that must be considered by the tagger. Each of these pairs would have a score calculated for th</context>
<context position="7345" citStr="Johnson, 1998" startWordPosition="1208" endWordPosition="1209">n the lefthand side of the rule. These probabilities can be estimated via their relative frequency in a corpus of trees. For instance, we can assign a probability to the rule S —&gt; NP VP by counting the number of occurrences of this rule in the corpus, and dividing by the total number of S nodes in the corpus. We can improve the probability model if we add in more conditioning events beyond the lefthand side of the rule. For example, if we throw in the parent of the lefthand side in the tree within which it appears, we can immediately see a dramatic improvement in the maximum likelihood parse (Johnson, 1998). That is, instead of: P(RHSILHS,Puris) = P(LHS,Pmis) where Pm&apos;s is the parent above the lefthand side of the rule. This additional conditioning event allows us to capture the fact that the distribution of, say, S node expansions underneath VPs is quite different than that of S nodes at the root of the tree. The models that we will be discussing in this paper condition on many such events, somewhere between five and ten. This can lead to sparse data problems, necessitating some kind of smoothing - in these cases, deleted interpolation. The idea behind deleted interpolation (Jelinek and Mercer,</context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>Johnson, M. 1998. PCFG models of linguistic tree representations. Computational Linguistics, 24(4):617-636.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<pages>19--2</pages>
<contexts>
<context position="12550" citStr="Marcus, Santorini, and Marcinkiewicz, 1993" startWordPosition="2074" endWordPosition="2078">lihood of integrating with its surrounding context. Both of the parsers also use parameterized pruning strategies, which will be described when the parsers are outlined. Results will be presented for each parser at a range of parameter values, to give a sense of the behavior of the parser as more or fewer events are taken into consideration. From this data, we shall be able to see the degree to which the events considered score correlates with time, as well as the convergence in accuracy. The parsers were trained on sections 2-21 and tested on section 23 of the Penn Wall St. Journal Treebank (Marcus, Santorini, and Marcinkiewicz, 1993), which are the standards in the statistical parsing literature. Accuracy is reported in terms of average labelled precision and recall. Precision is the number of correct constituents divided by the number of 31 section 23: 2416 sentences of length &lt; 100 Average length: 23.46 words/sentence Times past Avg. Events Time in first parse Prec/Rec Considered f secondst 21 89.7 212,014 26.7 13 89.6 107,221 14.0 7.5 89.1 48,606 6.7 2.5 86.8 9,621 1.5 2 85.6 6,826 1.1 per sentence Table 1: Results from the EC parser at different initial parameter values constituents proposed by the parser. Recall is </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Marcus, M.P., B. Santorini, and M.A. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Moore</author>
</authors>
<title>Improved left-corner chart parsing for large context-free grammars.</title>
<date>2000</date>
<booktitle>In Proceedings of the Sixth International Workshop on Parsing Technologies,</booktitle>
<pages>171--182</pages>
<contexts>
<context position="5807" citStr="Moore, 2000" startWordPosition="956" endWordPosition="957">edges in the chart. A parser&apos;s efficiency score would be the total number of these considered events across all stages. The principle merits of this metric are that it is general enough to cover different search and pruning techniques (including exhaustive parsing); that it is machine-independent; and that it is, to a certain extent, implementationindependent. The last of these might be what recommends the metric most, insofar as it is not the case for other simple metrics. For example, using time as a metric is perfectly general, and there are ways to normalize for processor differences (see Moore, 2000b). However, unless one is comparing two implementations that are essentially identical in all incidental ways, it is not possible to normalize for certain specifics of the implementation. For example, how probabilities are accessed, upon which processing time is very dependent, can differ from implementation to implementation (see discussion below). Thus, while time may be ideal for highly controlled studies of relatively similar algorithms (as in Moore, 2000a), its applicability for comparing diverse parsers is problematic. Let us consider a specific example: calculating scores from highly c</context>
</contexts>
<marker>Moore, 2000</marker>
<rawString>Moore, R. 2000a. Improved left-corner chart parsing for large context-free grammars. In Proceedings of the Sixth International Workshop on Parsing Technologies, pages 171-182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Moore</author>
</authors>
<title>Time as a measure of parsing efficiency.</title>
<date>2000</date>
<booktitle>In Proceedings of the COLING-00 Workshop on Efficiency in Large-scale parsing systems.</booktitle>
<contexts>
<context position="5807" citStr="Moore, 2000" startWordPosition="956" endWordPosition="957">edges in the chart. A parser&apos;s efficiency score would be the total number of these considered events across all stages. The principle merits of this metric are that it is general enough to cover different search and pruning techniques (including exhaustive parsing); that it is machine-independent; and that it is, to a certain extent, implementationindependent. The last of these might be what recommends the metric most, insofar as it is not the case for other simple metrics. For example, using time as a metric is perfectly general, and there are ways to normalize for processor differences (see Moore, 2000b). However, unless one is comparing two implementations that are essentially identical in all incidental ways, it is not possible to normalize for certain specifics of the implementation. For example, how probabilities are accessed, upon which processing time is very dependent, can differ from implementation to implementation (see discussion below). Thus, while time may be ideal for highly controlled studies of relatively similar algorithms (as in Moore, 2000a), its applicability for comparing diverse parsers is problematic. Let us consider a specific example: calculating scores from highly c</context>
</contexts>
<marker>Moore, 2000</marker>
<rawString>Moore, R. 2000b. Time as a measure of parsing efficiency. In Proceedings of the COLING-00 Workshop on Efficiency in Large-scale parsing systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A linear observed time statistical parser based on maximum entropy models.</title>
<date>1997</date>
<booktitle>In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--10</pages>
<contexts>
<context position="1294" citStr="Ratnaparkhi, 1997" startWordPosition="188" endWordPosition="189">he number of &amp;quot;events&amp;quot;, however they are defined for a particular parser, for which a probability must be calculated, in order to find the parse. It is applicable to single-pass or multi-stage parsers. We discuss the advantages of the metric, and demonstrate its usefulness by using it to compare two parsers which differ in several fundamental ways. 1 Introduction The past five years have seen enormous improvements in broad-coverage parsing accuracy, through the use of statistical techniques. The parsers that perform at the highest level of accuracy (Charniak (1997; 2000); Collins (1997; 2000); Ratnaparkhi, 1997) use probabilistic models with a very large number of parameters, which can be costly to use in evaluating structures. Parsers that have been built for this level of accuracy have generally been compared only with respect to accuracy, not efficiency. This is understandable: their great selling point is the high level of accuracy they are able to achieve. In addition, these parsers are difficult to compare with respect to efficiency: the models are quite diverse, with very different kinds of parameters and different estimation * Thanks to everyone in the Brown Laboratory for Linguistic Informat</context>
</contexts>
<marker>Ratnaparkhi, 1997</marker>
<rawString>Ratnaparkhi, A. 1997. A linear observed time statistical parser based on maximum entropy models. In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, pages 1-10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Roark</author>
</authors>
<title>Probabilistic top-down parsing and language modeling.</title>
<date>2000</date>
<note>Submitted.</note>
<contexts>
<context position="11441" citStr="Roark, 2000" startWordPosition="1888" endWordPosition="1889">c point that we are making here is that our metric measures that which is central to statistical parsing techniques, and not something that can be incidentally improved. In the next section, we outline two quite different statistical parsers, and present some results using our new metric. 2 Comparing statistical parsers To illustrate the utility of this metric for comparing the efficiency of radically different approaches to broad-coverage parsing, we will contrast some results from a two-stage bestfirst parser (Charniak, 2000) with a single-pass left-to-right, incremental beam-search parser (Roark, 2000). Both of these parsers (which we will refer to, henceforth, as the EC and BR parsers, respectively) score between 85 and 90 percent average precision and recall; both condition the probabilities of events on a large number of contextual parameters in more-or-less the way outlined above; and both use boundary statistics to assign partial structures a figureof-merit, which is the product of the probability of the structure in its own right and a score for its likelihood of integrating with its surrounding context. Both of the parsers also use parameterized pruning strategies, which will be desc</context>
<context position="18659" citStr="Roark (2000)" startWordPosition="3133" endWordPosition="3134">15, where a is the cube of the number of successful analyses, is discarded. The basic idea is that we want the beam to be very wide if there are few analyses that have extended to the current look-ahead word, but relatively narrow if many such analyses have been found. Thus, if 0 = 10-12, and 100 analyses have extended to the current lookahead word, then a candidate analysis must have a probability above 10-6/5 to avoid being pruned. After 1000 candidates, the beam has narrowed to 10-3/3. Table 2 shows accuracy and efficiency results when the BR parser is run at various base beam factors. See Roark (2000) for more details on the specifics of this parser. The conditional probability model that is used in the BR parser is constrained by the leftto-right nature of the algorithm. Whereas the conditional probability model used in the second stage of the EC parser has access to the full parse trees, and thus can condition the structures with information from either the left or right context, any model used in the BR parser can only use information from the left-context, since that is all that has been built at the moment the probability of a structure is evaluated. For example, a subject NP can be c</context>
</contexts>
<marker>Roark, 2000</marker>
<rawString>Roark, B. 2000. Probabilistic top-down parsing and language modeling. Submitted.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>