<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009852">
<title confidence="0.974143">
Large Scale Relation Detection*
</title>
<author confidence="0.997699">
Chris Welty and James Fan and David Gondek and Andrew Schlaikjer
</author>
<affiliation confidence="0.99823">
IBM Watson Research Center · 19 Skyline Drive · Hawthorne, NY 10532, USA
</affiliation>
<email confidence="0.982596">
{welty, fanj, dgondek, ahschlai}@us.ibm.com
</email>
<sectionHeader confidence="0.995377" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999985153846154">
We present a technique for reading sentences
and producing sets of hypothetical relations
that the sentence may be expressing. The
technique uses large amounts of instance-level
background knowledge about the relations in
order to gather statistics on the various ways
the relation may be expressed in language, and
was inspired by the observation that half of the
linguistic forms used to express relations oc-
cur very infrequently and are simply not con-
sidered by systems that use too few seed ex-
amples. Some very early experiments are pre-
sented that show promising results.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999886866666667">
We are building a system that learns to read in a new
domain by applying a novel combination of natural
language processing, machine learning, knowledge
representation and reasoning, information retrieval,
data mining, etc. techniques in an integrated way.
Central to our approach is the view that all parts of
the system should be able to interact during any level
of processing, rather than a pipeline view in which
certain parts of the system only take as input the re-
sults of other parts, and thus cannot influence those
results. In this paper we discuss a particular case
of that idea, using large knowledge bases hand in
hand with natural language processing to improve
the quality of relation detection. Ultimately we de-
fine reading as representing natural language text in
</bodyText>
<footnote confidence="0.8668805">
* Research supported in part by DARPA MRP Grant
FA8750-09-C0172
</footnote>
<page confidence="0.995619">
24
</page>
<bodyText confidence="0.998052166666667">
a way that integrates background knowledge and in-
ference, and thus are doing the relation detection
to better integrate text with pre-existing knowledge,
however that should not (and does not) prevent us
from using what knowledge we have to influence
that integration along the way.
</bodyText>
<sectionHeader confidence="0.981103" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999996166666667">
The most obvious points of interaction between NLP
and KR systems are named entity tagging and other
forms of type instance extraction. The second ma-
jor point of interaction is relation extraction, and
while there are many kinds of relations that may
be detected (e.g. syntactic relations such as modi-
fiers and verb subject/object, equivalence relations
like coreference or nicknames, event frame relations
such as participants, etc.), the kind of relations that
reading systems need to extract to support domain-
specific reasoning tasks are relations that are known
to be expressed in supporting knowledge-bases. We
call these relations semantic relations in this paper.
Compared to entity and type detection, extraction
of semantic relations is significantly harder. In our
work on bridging the NLP-KR gap, we have ob-
served several aspects of what makes this task dif-
ficult, which we discuss below.
</bodyText>
<subsectionHeader confidence="0.974918">
2.1 Keep reading
</subsectionHeader>
<bodyText confidence="0.999003">
Humans do not read and understand text by first rec-
ognizing named entities, giving them types, and then
finding a small fixed set of relations between them.
Rather, humans start with the first sentence and build
up a representation of what they read that expands
and is refined during reading. Furthermore, humans
</bodyText>
<note confidence="0.9845375">
Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 24–33,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999884827586207">
do not “populate databases” by reading; knowledge
is not only a product of reading, it is an integral part
of it. We require knowledge during reading in order
to understand what we read.
One of the central tenets of our machine reading
system is the notion that reading is not performed on
sentences in isolation. Often, problems in NLP can
be resolved by simply waiting for the next sentence,
or remembering the results from the previous, and
incorporating background or domain specific knowl-
edge. This includes parse ambiguity, coreference,
typing of named entities, etc. We call this the Keep
Reading principle.
Keep reading applies to relation extraction as
well. Most relation extraction systems are imple-
mented such that a single interpretation is forced
on a sentence, based only on features of the sen-
tence itself. In fact, this has been a shortcoming
of many NLP systems in the past. However, when
you apply the Keep Reading principle, multiple hy-
potheses from different parts of the NLP pipeline are
maintained, and decisions are deferred until there is
enough evidence to make a high confidence choice
between competing hypotheses. Knowledge, such
as those entities already known to participate in a
relation and how that relation was expressed, can
and should be part of that evidence. We will present
many examples of the principle in subsequent sec-
tions.
</bodyText>
<subsectionHeader confidence="0.999842">
2.2 Expressing relations in language
</subsectionHeader>
<bodyText confidence="0.999942954545455">
Due to the flexibility and expressive power of nat-
ural language, a specific type of semantic relation
can usually be expressed in language in a myriad
of ways. In addition, semantic relations are of-
ten implied by the expression of other relations.
For example, all of the following sentences more
or less express the same relation between an actor
and a movie: (1) “Elijah wood starred in Lord of
the Rings: The Fellowship of the Ring”, (2) “Lord
of the Rings: The Fellowship of the Ring’s Elijah
Wood, ...”, and(3) “Elijah Wood’s coming of age
was clearly his portrayal of the dedicated and noble
hobbit that led the eponymous fellowship from the
first episode of the Lord of the Rings trilogy.” No
human reader would have any trouble recognizing
the relation, but clearly this variability of expression
presents a major problem for machine reading sys-
tems.
To get an empirical sense of the variability of nat-
ural language used to express a relation, we stud-
ied a few semantic relations and found sentences
that expressed that relation, extracted simple pat-
terns to account for how the relation is expressed
between two arguments, mainly by removing the re-
lation arguments (e.g. “Elijah Wood” and “Lord of
the Rings: The Fellowship of the Ring” above) and
replacing them with variables. We then counted the
number of times each pattern was used to express
the relation, producing a recognizable very long tail
shown in Figure 1 for the top 50 patterns expressing
the acted-in-movie relation in 17k sentences. More
sophisticated pattern generalization (as discussed in
later sections) would significantly fatten the head,
bringing it closer to the traditional 50% of the area
under the curve, but no amount of generalization
will eliminate the tail. The patterns become increas-
ingly esoteric, such as “The movie Death Becomes
Her features a brief sequence in which Bruce Willis
and Goldie Hawn’s characters plan Meryl Streep’s
character’s death by sending her car off of a cliff
on Mulholland Drive,” or “The best known Hawk-
sian woman is probably Lauren Bacall, who iconi-
cally played the type opposite Humphrey Bogart in
To Have and Have Not and The Big Sleep.”
</bodyText>
<subsectionHeader confidence="0.997322">
2.3 What relations matter
</subsectionHeader>
<bodyText confidence="0.999989157894737">
We do not consider relation extraction to be an end
in and of itself, but rather as a component in larger
systems that perform some task requiring interoper-
ation between language- and knowledge-based com-
ponents. Such larger tasks can include question
answering, medical diagnosis, intelligence analysis,
museum curation, etc. These tasks have evaluation
criteria that go beyond measuring relation extraction
results. The first step in applying relation detection
to these larger tasks is analysis to determine what
relations matter for the task and domain.
There are a number of manual and semi-automatic
ways to perform such analysis. Repeating the
theme of this paper, which is to use pre-existing
knowledge-bases as resources, we performed this
analysis using freebase and a set of 20k question-
answer pairs representing our task domain. For each
question, we formed tuples of each entity name in
the question (QNE) with the answer, and found all
</bodyText>
<page confidence="0.989529">
25
</page>
<figure confidence="0.99860905">
1000
900
800
700
600
500
400
300
200
100
0
80
70
60
50
40
30
20
10
0
</figure>
<figureCaption confidence="0.9994755">
Figure 2: Relative frequency for top 50 relations in 20K question-answer pairs.
Figure 1: Pattern frequency for acted-in-movie relation for 17k sentences.
</figureCaption>
<bodyText confidence="0.999867045454546">
the relations in the KB connecting the entities. We
kept a count for each relation of how often it con-
nected a QNE to an answer. Of course we don’t ac-
tually know for sure that the relation is the one being
asked, but the intuition is that if the amount of data
is big enough, you will have at least a ranked list of
which relations are the most frequent.
Figure 2 shows the ranking for the top 50 rela-
tions. Note that, even when restricted to the top 50
relations, the graph has no head, it is basically all
tail; The top 50 relations cover about 15% of the do-
main. In smaller, manual attempts to determine the
most frequent relations in our domain, we had a sim-
ilar result. What this means is that supporting even
the top 50 relations with perfect recall covers about
15% of the questions. It is possible, of course, to
narrow the domain and restrict the relations that can
be queried–this is what database systems do. For
reading, however, the results are the same. A read-
ing system requires the ability to recognize hundreds
of relations to have any significant impact on under-
standing.
</bodyText>
<subsectionHeader confidence="0.998393">
2.4 Multi-relation learning on many seeds
</subsectionHeader>
<bodyText confidence="0.99997775">
The results shown in Figure 1 and Figure 2 con-
firmed much of the analysis and experiences we’d
had in the past trying to apply relation extraction in
the traditional way to natural language problems like
</bodyText>
<page confidence="0.763049">
26
</page>
<bodyText confidence="0.953094923913044">
question answering, building concept graphs from Large Scale Relation Detection (LSRD), differs in
intelligence reports, semantic search, etc. Either by three important ways:
training machine learning algorithms on manually
annotated data or by manually crafting finite-state
transducers, relation detection is faced by this two-
fold problem: the per-relation extraction hits a wall
around 50% recall, and each relation itself occurs
infrequently in the data.
This apparent futility of relation extraction led us
to rethink our approach. First of all, the very long
tail for relation patterns led us to consider how to
pick up the tail. We concluded that to do so would
require many more examples of the relation, but
where can we get them? In the world of linked-data,
huge instance-centered knowledge-bases are rapidly
growing and spreading on the semantic web1. Re-
sources like DBPedia, Freebase, IMDB, Geonames,
the Gene Ontology, etc., are making available RDF-
based data about a number of domains. These
sources of structured knowledge can provide a large
number of seed tuples for many different relations.
This is discussed further below.
Furthermore, the all-tail nature of relation cover-
age led us to consider performing relation extraction
on multiple relations at once. Some promising re-
sults on multi-relation learning have already been re-
ported in (Carlson et al., 2009), and the data sources
mentioned above give us many more than just the
handful of seed instances used in those experiments.
The idea of learning multiple relations at once also
fits with our keep reading principle - multiple rela-
tion hypotheses may be annotated between the same
arguments, with further evidence helping to disam-
biguate them.
3 Approach
One common approach to relation extraction is to
start with seed tuples and find sentences that con-
tain mentions of both elements of the tuple. From
each such sentence a pattern is generated using at
minimum universal generalization (replace the tuple
elements with variables), though adding any form of
generalization here can significantly improve recall.
Finally, evaluate the patterns by applying them to
text and evaluating the precision and recall of the tu-
ples extracted by the patterns. Our approach, called
1. We start with a knowledge-base containing a
large number (thousands to millions) of tuples
encoding relation instances of various types.
Our hypothesis is that only a large number of
examples can possibly account for the long tail.
2. We do not learn one relation at a time, but
rather, associate a pattern with a set of relations
whose tuples appear in that pattern. Thus, when
a pattern is matched to a sentence during read-
ing, each relation in its set of associated rela-
tions is posited as a hypothetical interpretation
of the sentence, to be supported or refuted by
further reading.
3. We use the knowledge-base as an oracle to de-
termine negative examples of a relation. As
a result the technique is semi-supervised; it
requires no human intervention but does re-
quire reliable knowledge-bases as input–these
knowledge-bases are readily available today.
Many relation extraction techniques depend on a
prior step of named entity recognition (NER) and
typing, in order to identify potential arguments.
However, this limits recall to the recall of the NER
step. In our approach patterns can match on any
noun phrase, and typing of these NPs is simply an-
other form of evidence.
All this means our approach is not relation extrac-
tion per se, it typically does not make conclusions
about a relation in a sentence, but extracts hypothe-
ses to be resolved by other parts of our reading sys-
tem.
In the following sections, we elaborate on the
technique and some details of the current implemen-
tation.
3.1 Basic pipeline
The two principle inputs are a corpus and a
knowledge-base (KB). For the experiments below,
we used the English Gigaword corpus2 extended
with Wikipedia and other news sources, and IMDB,
DBPedia, and Freebase KBs, as shown. The intent is
2http://www.ldc.upenn.edu/Catalog/
CatalogEntry.jsp?catalogId=LDC2003T05
1http://linkeddata.org/
27
to run against a web-scale corpus and larger linked- appeared together at the premier.
data sets. The proper names “Tom Cruise” and “Nicole Kid-
Input documents are sentence delimited, tok- man” are recognized and looked up in the KB. We
enized and parsed. The technique can benefit dra- find instances in the KB with those names, and the
matically from coreference resolution, however in following relations: coStar(Tom Cruise,
the experiments shown, this was not present. For Nicole Kidman); marriedTo(Tom
each pair of proper names in a sentence, the names Cruise, Nicole Kidman). We extract a
are looked up in the KB, and if they are related, pattern p1: ?x and co-star ?y appeared
a pattern is extracted from the sentence. At min- together at the premier in which all the
imum, pattern extraction should replace the names names have been replace by variables, and the
with variables. Depending on how patterns are ex- associations &lt;p1, costar, 1, 0&gt; and &lt;p1,
tracted, one pattern may be extracted per sentence, marriedTo, 1, 0&gt; with positive counts and
or one pattern may be extracted per pair of proper zero negative counts. Over the entire corpus, we’d
names in the sentence. Each pattern is associated expect the pattern to appear a few times and end
with all the relations known in the KB between the up with final positive counts like &lt;p1, coStar,
two proper names. If the pattern has been extracted 14, 0&gt; and &lt;p1, marriedTo, 2, 0&gt;, in-
before, the two are merged by incrementing the as- dicating the pattern p1 appeared 14 times in the
sociated relation counts. This phase, called pattern corpus between names known to participate in the
induction, is repeated for the entire corpus, resulting coStar relation, and twice between names known
in a large set of patterns, each pattern associated with to participate in the marriedTo relation. During
relations. For each ¡pattern, relation¿ pair, there is a training, the following sentence is encountered that
count of the number of times that pattern appeared matches p1:
in the corpus with names that are in the relation ac- Tom Hanks and co-star Daryl Hannah ap-
cording to the KB. peared together at the premier.
The pattern induction phase results in positive The names “Tom Hanks” and “Daryl Hannah”
counts, i.e. the number of times a pattern appeared are looked up in the KB and in this case only
in the corpus with named entities known to be re- the relation coStar is found between them, so the
lated in the KB. However, the induction phase does marriedTo association is updated with a negative
not exhaustively count the number of times each pat- count: &lt;p1, marriedTo, 2, -1&gt;. Over the
tern appears in the corpus, as a pattern may appear entire corpus, we’d expect the counts to be some-
with entities that are not known in the KB, or are not thing like &lt;p1, costar, 14, -6&gt; and &lt;p1,
known to be related. The second phase, called pat- marriedTo, 2, -18&gt;.
tern training, goes through the entire corpus again, This is a very simple example and it is difficult to
trying to match induced patterns to sentences, bind- see the value of the pattern training phase, as it may
ing any noun phrase to the pattern variables. Some appear the negative counts could be collected during
attempt is made to resolve the noun phrase to some- the induction phase. There are several reasons why
thing (most obviously, a name) that can be looked this is not so. First of all, since the first phase only
up in the KB, and for each relation associated with induces patterns between proper names that appear
the pattern, if the two names are not in the relation and are related within the KB, a sentence in the cor-
according to the KB, the negative count for that re- pus matching the pattern would be missed if it did
lation in the matched pattern is incremented. The not meet that criteria but was encountered before the
result of the pattern training phase is an updated set pattern was induced. Secondly, for reasons that are
of ¡pattern, relation¿ pairs with negative counts. beyond the scope of this paper, having to do with
The following example illustrates the basic pro- our Keep Reading principle, the second phase does
cessing. During induction, this sentence is encoun- slightly more general matching: note that it matches
tered: noun phrases instead of proper nouns.
Tom Cruise and co-star Nicole Kidman
28
3.2 Candidate-instance matching themselves are different. The problems are severe
An obvious part of the process in both phases is enough that the candidate-instance matching prob-
taking strings from text and matching them against lem contributes the most, of all components in this
names or labels in the KB. We refer to the strings in pipeline, to precision and recall failures. We have
the sentences as candidate arguments or simply can- observed recall drops of as much as 15% and preci-
didates, and refer to instances in the KB as entities sion drops of 10% due to candidate-instance match-
with associated attributes. For simplicity of discus- ing.
sion we will assume all KBs are in RDF, and thus This problem has been studied somewhat in the
all KB instances are nodes in a graph with unique literature, especially in the area of database record
identifiers (URIs) and arcs connecting them to other matching and coreference resolution (Michelson and
instances or primitive values (strings, numbers, etc.). Knoblock, 2007), but the experiments presented be-
A set of specially designated arcs, called labels, con- low use rudimentary solutions and would benefit
nect instances to strings that are understood to name significantly from improvements; it is important to
the instances. The reverse lookup of entity identi- acknowledge that the problem exists and is not as
fiers via names referred to in the previous section trivial as it appears at first glance.
requires searching for the labels that match a string 3.3 Pattern representation
found in a sentence and returning the instance iden- The basic approach accommodates any pattern rep-
tifier. resentation, and in fact we can accommodate non
This step is so obvious it belies the difficultly of pattern-based learning approaches, such as CRFs, as
the matching process and is often overlooked, how- the primary hypothesis is principally concerned with
ever in our experiments we have found candidate- the number of seed examples (scaling up initial set
instance matching to be a significant source of error. of examples is important). Thus far we have only
Problems include having many instances with the experimented with two pattern representations: sim-
same or lexically similar names, slight variations in ple lexical patterns in which the known arguments
spelling especially with non-English names, inflex- are replaced in the sentence by variables (as shown
ibility or inefficiency in string matching in KB im- in the example above), and patterns based on the
plementations, etc. In some of our sources, names spanning tree between the two arguments in a de-
are also encoded as URLs. In the case of movie pendency parse, again with the known arguments re-
and book titles-two of the domains we experimented placed by variables. In our initial design we down-
with-the titles seem almost as if they were designed played the importance of the pattern representation
specifically to befuddle attempts to automatically and especially generalization, with the belief that
recognize them. Just about every English word is a very large scale would remove the need to general-
book or movie title, including “It”, “Them”, “And”, ize. However, our initial experiments suggest that
etc., many years are titles, and just about every num- good pattern generalization would have a signifi-
ber under 1000. Longer titles are difficult as well, cant impact on recall, without negative impact on
since simple lexical variations can prevent matching precision, which agrees with findings in the litera-
from succeeding, e.g. the Shakespeare play, A Mid- ture (Pantel and Pennacchiotti, 2006). Thus, these
summer Night’s Dream appears often as Midsummer early results only employ rudimentary pattern gen-
Night’s Dream, A Midsummer Night Dream, and oc- eralization techniques, though this is an area we in-
casionally, in context, just Dream. When titles are tend to improve. We discuss some more details of
not distinguished or delimited somehow, they can the lack of generalization below.
confuse parsing which may fail to recognize them as 4 Experiment
noun phrases. We eventually had to build dictionar- In this section we present a set of very early proof of
ies of multi-word titles to help parsing, but of course concept experiments performed using drastic simpli-
that was imperfect as well. fications of the LSRD design. We began, in fact, by
The problems go beyond the analogous ones in
coreference resolution as the sources and technology
</bodyText>
<table confidence="0.9096325">
29
Relation Prec Rec F1 Tuples Seeds
imdb:actedIn 46.3 45.8 0.46 9M 30K
frb:authorOf 23.4 27.5 0.25 2M 2M
imdb:directorOf 22.8 22.4 0.22 700K 700K
frb:parentOf 68.2 8.6 0.16 10K 10K
</table>
<tableCaption confidence="0.981536">
Table 1: Precision and recall vs. number of tuples used
for 4 freebase relations.
</tableCaption>
<bodyText confidence="0.999790785714286">
using single-relation experiments, despite the cen-
trality of multiple hypotheses to our reading system,
in order to facilitate evaluation and understanding of
the technique. Our main focus was to gather data
to support (or refute) the hypothesis that more re-
lation examples would matter during pattern induc-
tion, and that using the KB as an oracle for training
would work. Clearly, no KB is complete to begin
with, and candidate-instance matching errors drop
apparent coverage further, so we intended to explore
the degree to which the KB’s coverage of the relation
impacted performance. To accomplish this, we ex-
amined four relations with different coverage char-
acteristics in the KB.
</bodyText>
<subsectionHeader confidence="0.998">
4.1 Setup and results
</subsectionHeader>
<bodyText confidence="0.999990327272728">
The first relation we tried was the acted-in-show
relation from IMDB; for convenience we refer to
it as imdb:actedIn. An IMDB show is a movie,
TV episode, or series. This relation has over 9M
&lt;actor, show&gt; tuples, and its coverage was
complete as far as we were able to determine. How-
ever, the version we used did not have a lot of name
variations for actors. The second relation was the
author-of relation from Freebase (frb:authorOf),
with roughly 2M &lt;author, written-work&gt;
tuples. The third relation was the director-of-
movie relation from IMDB (imdb:directorOf), with
700k &lt;director,movie&gt; tuples. The fourth
relation was the parent-of relation from Free-
base (frb:parentOf), with roughly 10K &lt;parent,
child&gt; tuples (mostly biblical and entertainment).
Results are shown in Table 1.
The imdb:actedIn experiment was performed on
the first version of the system that ran on 1 CPU and,
due to resource constraints, was not able to use more
than 30K seed tuples for the rule induction phase.
However, the full KB (9M relation instances) was
available for the training phase. With some man-
ual effort, we selected tuples (actor-movie pairs) of
popular actors and movies that we expected to ap-
pear most frequently in the corpus. In the other ex-
periments, the full tuple set was available for both
phases, but 2M tuples was the limit for the size of
the KB in the implementation. With these promising
preliminary results, we expect a full implementation
to accommodate up to 1B tuples or more.
The evaluation was performed in decreasing de-
grees of rigor. The imdb:actedIn experiment was run
against 20K sentences with roughly 1000 actor in
movie relations and checked by hand. For the other
three, the same sentences were used, but the ground
truth was generated in a semi-automatic way by re-
using the LSRD assumption that a sentence con-
taining tuples in the relation expresses the relation,
and then spot-checked manually. Thus the evalua-
tion for these three experiments favors the LSRD ap-
proach, though spot checking revealed it is the pre-
cision and not the recall that benefits most from this,
and all the recall problems in the ground truth (i.e.
sentences that did express the relation but were not
in the ground truth) were due to candidate-instance
matching problems. An additional idiosyncrasy in
the evaluation is that the sentences in the ground
truth were actually questions, in which one of the
arguments to the relation was the answer. Since
the patterns were induced and trained on statements,
there is a mismatch in style which also significantly
impacts recall. Thus the precision and recall num-
bers should not be taken as general performance, but
are useful only relative to each other.
</bodyText>
<subsectionHeader confidence="0.906248">
4.2 Discussion
</subsectionHeader>
<bodyText confidence="0.999990571428572">
The results are promising, and we are continuing the
work with a scalable implementation. Overall, the
results seem to show a clear correlation between the
number of seed tuples and relation extraction recall.
However, the results do not as clearly support the
many examples hypothesis as it may seem. When
an actor and a film that actor starred in are men-
tioned in a sentence, it is very often the case that the
sentence expresses that relation. However, this was
less likely in the case of the parent-of relation, and
as we considered other relations, we found a wide
degree of variation. The borders relation between
two countries, for example, is on the other extreme
from actor-in-movie. Bordering nations often wage
</bodyText>
<page confidence="0.995824">
30
</page>
<bodyText confidence="0.999961109589041">
war, trade, suspend relations, deport refugees, sup-
port, oppose, etc. each other, so finding the two na-
tions in a sentence together is not highly indicative
of one relation or another. The director-of-movie re-
lation was closer to acted-in-movie in this regard,
and author-of a bit below that. The obvious next step
to gather more data on the many examples hypoth-
esis is to run the experiments with one relation, in-
creasing the number of tuples with each experiment
and observing the change in precision and recall.
The recall results do not seem particularly strik-
ing, though these experiments do not include pat-
tern generalization (other than what a dependency
parse provides) or coreference, use a small corpus,
and poor candidate-instance matching. Further, as
noted above there were other idiosyncrasies in the
evaluation that make them only useful for relative
comparison, not as general results.
Many of the patterns induced, especially for
the acted-in-movie relation, were highly lexical,
using e.g. parenthesis or other punctuation to
signal the relation. For example, a common
pattern was actor-name (movie-name), or
movie-name: actor-name, e.g. “Leonardo
DiCaprio (Titanic) was considering accepting the
role as Anakin Skywalker,” or “Titanic: Leonardo
DiCaprio and Kate Blanchett steam up the silver
screen against the backdrop of the infamous disas-
ter.” Clearly patterns like this rely heavily on the
context and typing to work. In general the pattern
?x (?y) is not reliable for the actor-in-movie re-
lation unless you know ?x is an actor and ?y is a
movie. However, some patterns, like ?x appears
in the screen epic ?y is highly indicative
of the relation without the types at all - in fact it is
so high precision it could be used to infer the types
of ?x and ?y if they were not known. This seems
to fit extremely well in our larger reading system,
in which the pattern itself provides one form of evi-
dence to be combined with others, but was not a part
of our evaluation.
One of the most important things to general-
ize in the patterns we observed was dates. If
patterns like, actor-name appears in the
1994 screen epic movie-name could have
been generalized to actor-name appears in
the date screen epic movie-name, re-
call would have been boosted significantly. As it
stood in these experiments, everything but the argu-
ments had to match. Similarly, many relations often
appear in lists, and our patterns were not able to gen-
eralize that away. For example the sentence, “Mark
Hamill appeared in Star Wars, Star Wars: The Em-
pire Strikes Back, and Star Wars: The Return of the
Jedi,” causes three patterns to be induced; in each,
one of the movies is replaced by a variable in the
pattern and the other two are required to be present.
Then of course all this needs to be combined, so that
the sentence, “Indiana Jones and the Last Crusade is
a 1989 adventure film directed by Steven Spielberg
and starring Harrison Ford, Sean Connery, Denholm
Elliott and Julian Glover,” would generate a pattern
that would get the right arguments out of “Titanic
is a 1997 epic film directed by James Cameron and
starring Leonardo DiCaprio, Kate Winslett, Kathy
Bates and Bill Paxon.” At the moment the former
sentence generates four patterns that require the di-
rector and dates to be exactly the same.
Some articles in the corpus were biographies
which were rich with relation content but also with
pervasive anaphora, name abbreviations, and other
coreference manifestations that severely hampered
induction and evaluation.
</bodyText>
<sectionHeader confidence="0.999804" genericHeader="related work">
5 Related work
</sectionHeader>
<bodyText confidence="0.99981605">
Early work in semi-supervised learning techniques
such as co-training and multi-view learning (Blum
and Mitchell, 1998) laid much of the ground work
for subsequent experiments in bootstrapped learn-
ing for various NLP tasks, including named entity
detection (Craven et al., 2000; Etzioni et al., 2005)
and document classification (Nigam et al., 2006).
This work’s pattern induction technique also repre-
sents a semi-supervised approach, here applied to
relation learning, and at face value is similar in mo-
tivation to many of the other reported experiments
in large scale relation learning (Banko and Etzioni,
2008; Yates and Etzioni, 2009; Carlson et al., 2009;
Carlson et al., 2010). However, previous techniques
generally rely on a small set of example relation in-
stances and/or patterns, whereas here we explicitly
require a larger source of relation instances for pat-
tern induction and training. This allows us to better
evaluate the precision of all learned patterns across
multiple relation types, as well as improve coverage
</bodyText>
<page confidence="0.999582">
31
</page>
<bodyText confidence="0.999903056603773">
of the pattern space for any given relation.
Another fundamental aspect of our approach lies
in the fact that we attempt to learn many relations
simultaneously. Previously, (Whitelaw et al., 2008)
found that such a joint learning approach was use-
ful for large-scale named entity detection, and we
expect to see this result carry over to the relation ex-
traction task. (Carlson et al., 2010) also describes
relation learning in a multi-task learning framework,
and attempts to optimize various constraints posited
across all relation classes.
Examples of the use of negative evidence
for learning the strength of associations between
learned patterns and relation classes as proposed
here has not been reported in prior work to our
knowledge. A number of multi-class learning tech-
niques require negative examples in order to prop-
erly learn discriminative features of positive class
instances. To address this requirement, a number of
approaches have been suggested in the literature for
selection or generation of negative class instances.
For example, sampling from the positive instances
of other classes, randomly perturbing known pos-
itive instances, or breaking known semantic con-
straints of the positive class (e.g. positing multiple
state capitols for the same state). With this work,
we treat our existing RDF store as an oracle, and as-
sume it is sufficiently comprehensive that it allows
estimation of negative evidence for all target relation
classes simultaneously.
The first (induction) phase of LSRD is very simi-
lar to PORE (Wang et al., 2007) (Dolby et al., 2009;
Gabrilovich and Markovitch, 2007) and (Nguyen
et al., 2007), in which positive examples were ex-
tracted from Wikipedia infoboxes. These also bear
striking similarity to (Agichtein and Gravano, 2000),
and all suffer from a significantly smaller number of
seed examples. Indeed, its not using a database of
specific tuples that distinguishes LSRD, but that it
uses so many; the scale of the induction in LSRD
is designed to capture far less frequent patterns by
using significantly more seeds
In (Ramakrishnan et al., 2006) the same intu-
ition is captured that knowledge of the structure of
a database should be employed when trying to inter-
pret text, though again the three basic hypotheses of
LSRD are not supported.
In (Huang et al., 2004), a similar phenomenon to
what we observed with the acted-in-movie relation
was reported in which the chances of a protein in-
teraction relation being expressed in a sentence are
already quite high if two proteins are mentioned in
that sentence.
</bodyText>
<sectionHeader confidence="0.999173" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999994175">
We have presented an approach for Large Scale Re-
lation Detection (LSRD) that is intended to be used
within a machine reading system as a source of hy-
pothetical interpretations of input sentences in natu-
ral language. The interpretations produced are se-
mantic relations between named arguments in the
sentences, and they are produced by using a large
knowledge source to generate many possible pat-
terns for expressing the relations known by that
source.
We have specifically targeted the technique at the
problem that the frequency of patterns occurring in
text that express a particular relation has a very long
tail (see Figure 1), and without enough seed exam-
ples the extremely infrequent expressions of the re-
lation will never be found and learned. Further, we
do not commit to any learning strategy at this stage
of processing, rather we simply produce counts, for
each relation, of how often a particular pattern pro-
duces tuples that are in that relation, and how of-
ten it doesn’t. These counts are simply used as ev-
idence for different possible interpretations, which
can be supported or refuted by other components in
the reading system, such as type detection.
We presented some very early results which while
promising are not conclusive. There were many
idiosyncrasies in the evaluation that made the re-
sults meaningful only with respect to other experi-
ments that were evaluated the same way. In addi-
tion, the evaluation was done at a component level,
as if the technique were a traditional relation extrac-
tion component, which ignores one of its primary
differentiators–that it produces sets of hypothetical
interpretations. Instead, the evaluation was done
only on the top hypothesis independent of other evi-
dence.
Despite these problems, the intuitions behind
LSRD still seem to us valid, and we are investing in a
truly large scale implementation that will overcome
the problems discussed here and can provide more
</bodyText>
<page confidence="0.997421">
32
</page>
<bodyText confidence="0.620263">
valid evidence to support or refute the hypotheses
LSRD is based on:
</bodyText>
<listItem confidence="0.606369444444445">
1. A large number of examples can account for the
long tail in relation expression;
2. Producing sets of hypothetical interpretations
of the sentence, to be supported or refuted by
further reading, works better than producing
one;
3. Using existing, large, linked-data knowledge-
bases as oracles can be effective in relation de-
tection.
</listItem>
<sectionHeader confidence="0.978702" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999926523255813">
[Agichtein and Gravano2000] E. Agichtein and L. Gra-
vano. 2000. Snowball: extracting relations from large
plain-text collections. In Proceedings of the 5th ACM
Conference on Digital Libraries, pages 85–94, San
Antonio, Texas, United States, June. ACM.
[Banko and Etzioni2008] Michele Banko and Oren Et-
zioni. 2008. The tradeoffs between open and tradi-
tional relation extraction. In Proceedings of the 46th
Annual Meeting of the Association for Computational
Linguistics.
[Blum and Mitchell1998] A. Blum and T. Mitchell. 1998.
Combining labeled and unlabeled data with co-
training. In Proceedings of the 1998 Conference on
Computational Learning Theory.
[Carlson et al.2009] A. Carlson, J. Betteridge, E. R. Hr-
uschka Jr., and T. M. Mitchell. 2009. Coupling semi-
supervised learning of categories and relations. In
Proceedings of the NAACL HLT 2009 Workshop on
Semi-supervised Learning for Natural Language Pro-
cessing.
[Carlson et al.2010] A. Carlson, J. Betteridge, R. C.
Wang, E. R. Hruschka Jr., and T. M. Mitchell. 2010.
Coupled semi-supervised learning for information ex-
traction. In Proceedings of the 3rd ACM International
Conference on Web Search and Data Mining.
[Craven et al.2000] Mark Craven, Dan DiPasquo, Dayne
Freitag, Andrew McCallum, Tom Mitchell, Kamal
Nigam, and Sean Slattery. 2000. Learning to construct
knowledge bases from the World Wide Web. Artificial
Intelligence, 118(1–2):69–113.
[Dolby et al.2009] Julian Dolby, Achille Fokoue, Aditya
Kalyanpur, Edith Schonberg, and Kavitha Srinivas.
2009. Extracting enterprise vocabularies using linked
open data. In Proceedings of the 8th International Se-
mantic Web Conference.
[Etzioni et al.2005] Oren Etzioni, Michael Cafarella,
Doug Downey, Ana-Maria Popescu, Tal Shaked,
Stephen Soderland, Daniel S. Weld, and Alexander
Yates. 2005. Unsupervised named-entity extraction
from the web: An experimental study. Artificial Intel-
ligence, 165(1):91–134, June.
[Gabrilovich and Markovitch2007] Evgeniy Gabrilovich
and Shaul Markovitch. 2007. Computing seman-
tic relatedness using wikipedia-based explicit seman-
tic analysis. In IJCAI.
[Huang et al.2004] Minlie Huang, Xiaoyan Zhu, Yu Hao,
Donald G. Payan, Kunbin Qu, and Ming Li. 2004.
Discovering patterns to extract protein-protein interac-
tions from full texts. Bioinformatics, 20(18).
[Michelson and Knoblock2007] Matthew Michelson and
Craig A. Knoblock. 2007. Mining heterogeneous
transformations for record linkage. In Proceedings of
the 6th International Workshop on Information Inte-
gration on the Web, pages 68–73.
[Nguyen et al.2007] Dat P. Nguyen, Yutaka Matsuo, ,
and Mitsuru Ishizuka. 2007. Exploiting syntactic
and semantic information for relation extraction from
wikipedia. In IJCAI.
[Nigam et al.2006] K. Nigam, A. McCallum, , and
T. Mitchell, 2006. Semi-Supervised Learning, chapter
Semi-Supervised Text Classification Using EM. MIT
Press.
[Pantel and Pennacchiotti2006] Patrick Pantel and Marco
Pennacchiotti. 2006. Espresso: Leveraging generic
patterns for automatically harvesting semantic rela-
tions. In Proceedings of the 21st international Confer-
ence on Computational Linguistics and the 44th An-
nual Meeting of the Association For Computational
Linguistics, Sydney, Australia, July.
[Ramakrishnan et al.2006] Cartic Ramakrishnan, Krys J.
Kochut, and Amit P. Sheth. 2006. A framework for
schema-driven relationship discovery from unstruc-
tured text. In ISWC.
[Wang et al.2007] Gang Wang, Yong Yu, and Haiping
Zhu. 2007. PORE: Positive-only relation extraction
from wikipedia text. In ISWC.
[Whitelaw et al.2008] C. Whitelaw, A. Kehlenbeck,
N. Petrovic, , and L. Ungar. 2008. Web-scale named
entity recognition. In Proceeding of the 17th ACM
Conference on information and Knowledge Manage-
ment, pages 123–132, Napa Valley, California, USA,
October. ACM.
[Yates and Etzioni2009] Alexander Yates and Oren Et-
zioni. 2009. Unsupervised methods for determining
object and relation synonyms on the web. Artificial
Intelligence, 34:255–296.
</reference>
<page confidence="0.999379">
33
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.787980">
<title confidence="0.999793">Scale Relation</title>
<author confidence="0.981513">Welty Fan Gondek Schlaikjer</author>
<affiliation confidence="0.842068">Watson Research Center Skyline Drive NY 10532,</affiliation>
<email confidence="0.958505">fanj,dgondek,</email>
<abstract confidence="0.998784">We present a technique for reading sentences and producing sets of hypothetical relations that the sentence may be expressing. The technique uses large amounts of instance-level background knowledge about the relations in order to gather statistics on the various ways the relation may be expressed in language, and was inspired by the observation that half of the linguistic forms used to express relations occur very infrequently and are simply not considered by systems that use too few seed examples. Some very early experiments are presented that show promising results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agichtein</author>
<author>L Gravano</author>
</authors>
<title>Snowball: extracting relations from large plain-text collections.</title>
<date>2000</date>
<booktitle>In Proceedings of the 5th ACM Conference on Digital Libraries,</booktitle>
<pages>85--94</pages>
<publisher>ACM.</publisher>
<location>San Antonio, Texas, United States,</location>
<marker>[Agichtein and Gravano2000]</marker>
<rawString>E. Agichtein and L. Gravano. 2000. Snowball: extracting relations from large plain-text collections. In Proceedings of the 5th ACM Conference on Digital Libraries, pages 85–94, San Antonio, Texas, United States, June. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michele Banko</author>
<author>Oren Etzioni</author>
</authors>
<title>The tradeoffs between open and traditional relation extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<marker>[Banko and Etzioni2008]</marker>
<rawString>Michele Banko and Oren Etzioni. 2008. The tradeoffs between open and traditional relation extraction. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Blum</author>
<author>T Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with cotraining.</title>
<date>1998</date>
<booktitle>In Proceedings of the 1998 Conference on Computational Learning Theory.</booktitle>
<marker>[Blum and Mitchell1998]</marker>
<rawString>A. Blum and T. Mitchell. 1998. Combining labeled and unlabeled data with cotraining. In Proceedings of the 1998 Conference on Computational Learning Theory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Carlson</author>
<author>J Betteridge</author>
<author>E R Hruschka Jr</author>
<author>T M Mitchell</author>
</authors>
<title>Coupling semisupervised learning of categories and relations.</title>
<date>2009</date>
<booktitle>In Proceedings of the NAACL HLT 2009 Workshop on Semi-supervised Learning for Natural Language Processing.</booktitle>
<marker>[Carlson et al.2009]</marker>
<rawString>A. Carlson, J. Betteridge, E. R. Hruschka Jr., and T. M. Mitchell. 2009. Coupling semisupervised learning of categories and relations. In Proceedings of the NAACL HLT 2009 Workshop on Semi-supervised Learning for Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Carlson</author>
<author>J Betteridge</author>
<author>R C Wang</author>
<author>E R Hruschka Jr</author>
<author>T M Mitchell</author>
</authors>
<title>Coupled semi-supervised learning for information extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 3rd ACM International Conference on Web Search and Data Mining.</booktitle>
<marker>[Carlson et al.2010]</marker>
<rawString>A. Carlson, J. Betteridge, R. C. Wang, E. R. Hruschka Jr., and T. M. Mitchell. 2010. Coupled semi-supervised learning for information extraction. In Proceedings of the 3rd ACM International Conference on Web Search and Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Craven</author>
<author>Dan DiPasquo</author>
<author>Dayne Freitag</author>
<author>Andrew McCallum</author>
<author>Tom Mitchell</author>
<author>Kamal Nigam</author>
<author>Sean Slattery</author>
</authors>
<title>Learning to construct knowledge bases from the World Wide Web.</title>
<date>2000</date>
<journal>Artificial Intelligence,</journal>
<pages>118--1</pages>
<marker>[Craven et al.2000]</marker>
<rawString>Mark Craven, Dan DiPasquo, Dayne Freitag, Andrew McCallum, Tom Mitchell, Kamal Nigam, and Sean Slattery. 2000. Learning to construct knowledge bases from the World Wide Web. Artificial Intelligence, 118(1–2):69–113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Dolby</author>
</authors>
<title>Achille Fokoue, Aditya Kalyanpur, Edith Schonberg, and Kavitha Srinivas.</title>
<date>2009</date>
<booktitle>In Proceedings of the 8th International Semantic Web Conference.</booktitle>
<marker>[Dolby et al.2009]</marker>
<rawString>Julian Dolby, Achille Fokoue, Aditya Kalyanpur, Edith Schonberg, and Kavitha Srinivas. 2009. Extracting enterprise vocabularies using linked open data. In Proceedings of the 8th International Semantic Web Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Michael Cafarella</author>
<author>Doug Downey</author>
<author>Ana-Maria Popescu</author>
<author>Tal Shaked</author>
<author>Stephen Soderland</author>
<author>Daniel S Weld</author>
<author>Alexander Yates</author>
</authors>
<title>Unsupervised named-entity extraction from the web: An experimental study.</title>
<date>2005</date>
<journal>Artificial Intelligence,</journal>
<volume>165</volume>
<issue>1</issue>
<marker>[Etzioni et al.2005]</marker>
<rawString>Oren Etzioni, Michael Cafarella, Doug Downey, Ana-Maria Popescu, Tal Shaked, Stephen Soderland, Daniel S. Weld, and Alexander Yates. 2005. Unsupervised named-entity extraction from the web: An experimental study. Artificial Intelligence, 165(1):91–134, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Computing semantic relatedness using wikipedia-based explicit semantic analysis.</title>
<date>2007</date>
<booktitle>In IJCAI.</booktitle>
<marker>[Gabrilovich and Markovitch2007]</marker>
<rawString>Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing semantic relatedness using wikipedia-based explicit semantic analysis. In IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minlie Huang</author>
<author>Xiaoyan Zhu</author>
<author>Yu Hao</author>
<author>Donald G Payan</author>
<author>Kunbin Qu</author>
<author>Ming Li</author>
</authors>
<title>Discovering patterns to extract protein-protein interactions from full texts. Bioinformatics,</title>
<date>2004</date>
<marker>[Huang et al.2004]</marker>
<rawString>Minlie Huang, Xiaoyan Zhu, Yu Hao, Donald G. Payan, Kunbin Qu, and Ming Li. 2004. Discovering patterns to extract protein-protein interactions from full texts. Bioinformatics, 20(18).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Michelson</author>
<author>Craig A Knoblock</author>
</authors>
<title>Mining heterogeneous transformations for record linkage.</title>
<date>2007</date>
<booktitle>In Proceedings of the 6th International Workshop on Information Integration on the Web,</booktitle>
<pages>68--73</pages>
<marker>[Michelson and Knoblock2007]</marker>
<rawString>Matthew Michelson and Craig A. Knoblock. 2007. Mining heterogeneous transformations for record linkage. In Proceedings of the 6th International Workshop on Information Integration on the Web, pages 68–73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dat P Nguyen</author>
<author>Yutaka Matsuo</author>
</authors>
<title>Exploiting syntactic and semantic information for relation extraction from wikipedia.</title>
<date>2007</date>
<booktitle>In IJCAI.</booktitle>
<marker>[Nguyen et al.2007]</marker>
<rawString>Dat P. Nguyen, Yutaka Matsuo, , and Mitsuru Ishizuka. 2007. Exploiting syntactic and semantic information for relation extraction from wikipedia. In IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Nigam</author>
<author>A McCallum</author>
</authors>
<title>Semi-Supervised Learning, chapter Semi-Supervised Text Classification Using EM.</title>
<date>2006</date>
<publisher>MIT Press.</publisher>
<marker>[Nigam et al.2006]</marker>
<rawString>K. Nigam, A. McCallum, , and T. Mitchell, 2006. Semi-Supervised Learning, chapter Semi-Supervised Text Classification Using EM. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Marco Pennacchiotti</author>
</authors>
<title>Espresso: Leveraging generic patterns for automatically harvesting semantic relations.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st international Conference on Computational Linguistics and the 44th Annual Meeting of the Association For Computational Linguistics,</booktitle>
<location>Sydney, Australia,</location>
<marker>[Pantel and Pennacchiotti2006]</marker>
<rawString>Patrick Pantel and Marco Pennacchiotti. 2006. Espresso: Leveraging generic patterns for automatically harvesting semantic relations. In Proceedings of the 21st international Conference on Computational Linguistics and the 44th Annual Meeting of the Association For Computational Linguistics, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cartic Ramakrishnan</author>
<author>Krys J Kochut</author>
<author>Amit P Sheth</author>
</authors>
<title>A framework for schema-driven relationship discovery from unstructured text.</title>
<date>2006</date>
<booktitle>In ISWC.</booktitle>
<marker>[Ramakrishnan et al.2006]</marker>
<rawString>Cartic Ramakrishnan, Krys J. Kochut, and Amit P. Sheth. 2006. A framework for schema-driven relationship discovery from unstructured text. In ISWC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gang Wang</author>
<author>Yong Yu</author>
<author>Haiping Zhu</author>
</authors>
<title>PORE: Positive-only relation extraction from wikipedia text.</title>
<date>2007</date>
<booktitle>In ISWC.</booktitle>
<marker>[Wang et al.2007]</marker>
<rawString>Gang Wang, Yong Yu, and Haiping Zhu. 2007. PORE: Positive-only relation extraction from wikipedia text. In ISWC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Whitelaw</author>
<author>A Kehlenbeck</author>
<author>N Petrovic</author>
</authors>
<title>Web-scale named entity recognition.</title>
<date>2008</date>
<booktitle>In Proceeding of the 17th ACM Conference on information and Knowledge Management,</booktitle>
<pages>123--132</pages>
<publisher>ACM.</publisher>
<location>Napa Valley, California, USA,</location>
<marker>[Whitelaw et al.2008]</marker>
<rawString>C. Whitelaw, A. Kehlenbeck, N. Petrovic, , and L. Ungar. 2008. Web-scale named entity recognition. In Proceeding of the 17th ACM Conference on information and Knowledge Management, pages 123–132, Napa Valley, California, USA, October. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yates</author>
<author>Oren Etzioni</author>
</authors>
<title>Unsupervised methods for determining object and relation synonyms on the web.</title>
<date>2009</date>
<journal>Artificial Intelligence,</journal>
<pages>34--255</pages>
<marker>[Yates and Etzioni2009]</marker>
<rawString>Alexander Yates and Oren Etzioni. 2009. Unsupervised methods for determining object and relation synonyms on the web. Artificial Intelligence, 34:255–296.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>