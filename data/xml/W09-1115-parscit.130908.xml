<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.048758">
<title confidence="0.9987315">
Investigating Automatic Alignment Methods for Slide Generation from
Academic Papers
</title>
<author confidence="0.994923">
Brandon Beamer and Roxana Girju
</author>
<affiliation confidence="0.999027">
Department of Linguistics
University of Illinois
</affiliation>
<address confidence="0.647021">
Urbana, IL
</address>
<email confidence="0.999513">
{bbeamer,girju}@illinos.edu
</email>
<sectionHeader confidence="0.997497" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.970037266666667">
In this paper we investigate the task of auto-
matic generation of slide presentations from
academic papers, focusing initially on slide
to paper alignment. We compare and eval-
uate four different alignment systems which
utilize various combinations of methods used
widely in other alignment and question an-
swering approaches, such as TF-IDF term
weighting and query expansion. Our best
aligner achieves an accuracy of 75% and our
findings show that for this application, av-
erage TF-IDF scoring performs more poorly
than a simpler method based on the number of
matched terms, and query expansion degrades
aligner performance.
</bodyText>
<sectionHeader confidence="0.999501" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999699961538461">
Automatic generation of slide presentations is a task
the Computational Linguistics community has not
yet pursued in much depth. A robust system capable
of generating slide presentations from papers would
save the author much tedium when organizing her
presentations. In this paper we investigate this task
from a novel perspective. While others have devel-
oped interesting approaches to slide generation from
documents by modeling the problem in a unique
way (Utiyama and Hasida, 1999; Shibata and Kuro-
hashi, 2005), the aim of the research this paper initi-
ates is to discover how humans create slide presen-
tations, focusing more specifically on academic pa-
pers. Thus we take a corpus-based approach to the
problem, and as a first step focus on the task of au-
tomatically aligning slide presentations to academic
papers.
We built a corpus of 296 slide-paper pairs and im-
plemented four slide to paper aligners which utilize
popular information retrieval methods such as TF-
IDF term weighting and query expansion. In this
paper we show that, in this application, TF-IDF term
weighting is inferior to a simpler scoring mecha-
nism based only on the number of matched terms
and query expansion degrades aligner performance.
Our best aligner achieves an accuracy of 75%.
</bodyText>
<sectionHeader confidence="0.999925" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999673476190476">
Automatic slide generation from documents is a thus
far under-investigated topic. Utiyama and Hasida
(1999) generate slides from GDA1 (global document
annotation) tagged documents. They detect topics
within the documents by analyzing GDA corefer-
ence links, modeled each slide as a topic and item-
ized elaborations (which were also tagged with the
GDA tag set). Shibata and Kurohashi (2005) convert
Japanese documents to slide representation by pars-
ing their discourse structures and representing the
resulting tree in an outline format. While (Utiyama
and Hasida, 1999) and (Shibata and Kurohashi,
2005) generate slides from documents by modeling
the task in creative ways, we aim to learn something
deeper regarding how humans actually go about the
task. Creating a corpus of slide/paper pairs will en-
able us to study the intricacies involved in how real
humans approach this task.
Our current focus is slide to paper (region) align-
ment, which can be categorized best as align-
ment between monolingual comparable corpora, but
</bodyText>
<footnote confidence="0.999875666666667">
1The GDA tag set is designed to allow machines to auto-
matically infer the underlying structure of documents. More
information is available at http://i-content.org/gda.
</footnote>
<page confidence="0.94082">
111
</page>
<note confidence="0.9901955">
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 111–119,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999711566666667">
could also be easily construed as document passage
retrieval, which is a well-researched topic in the In-
formation Retrieval community. Barzilay and El-
hadad (2003) incorporate context to facilitate align-
ment between monolingual comparable corpora by
first learning paragraph matching rules in a super-
vised way, and then refining the alignment at the sen-
tence level within paragraphs. Nelken and Shieber
(2008) used TF-IDF term weighting with logistic
regression to align sentences from pericopes in the
gospels of the new testament. Callan (1994) ana-
lyzed various ways to define document passages and
identified three main passage types, discourse (based
on physical structure of the document), semantic
(based on topic boundaries), and window (based on
token distance) and suggests that while discourse
passages may be an attractive way to define and re-
trieve document passages, due to reasons related to
sloppy writing, visual aids, or other factors, para-
graph boundaries may not be the best indicators of
content boundaries. Our alignment task differs from
that of (Barzilay and Elhadad, 2003) and (Nelken
and Shieber, 2008) in two ways. First, Barzilay
and Elhadad (2003) and Nelken and Shieber (2008)
align like-chunks between the two documents. That
is, they are either aligning sentences to sentences or
paragraphs to paragraphs. In our task we are align-
ing slide regions which are usually bullets spanning
at most a couple lines, to paper regions which can
be a whole paragraph long. Second, Barzilay and
Elhadad (2003) and (Nelken and Shieber, 2008) are
working with comparable corpora in which the same
information is assumed to be present in each docu-
ment, but expressed in a different way. We are not
able to necessarily make this assumption, in fact we
show in this paper that as much as half of the infor-
mation in slide presentations may not be present in
the corresponding paper.
The concept of query expansion that we im-
plement in some of our aligners is also not new.
Voorhees (1994) suggests that query expansion
tends to help performance with short, incomplete
queries but degrades performance with longer, more
complete queries. van der Plas and Tiedemann
(2008) investigated several types of lexico-semantic
information for query expansion in their question
answering system. They found that expansions that
bridge the terminology gap (synonyms, etc.) did not
result in improvement but expansions that bridge the
knowledge gap (words belonging to the same subject
field) did. In this paper, to get an idea of the base-
line performance of query expansion with regard to
our unique task, we implement a more rudimentary
form of query expansion which only expands syn-
onyms of terms. Since our slide regions don’t vary
much in length, it’s hard to say how our results relate
to the findings of Voorhees (1994). Our results par-
tially support (van der Plas and Tiedemann, 2008) in
that our implementation only bridges the terminol-
ogy gap, and isn’t very successful.
</bodyText>
<sectionHeader confidence="0.997637" genericHeader="method">
3 The Corpus
</sectionHeader>
<bodyText confidence="0.999963181818182">
The first step to understanding how humans generate
slides from papers is to collect real-world examples
of academic papers and corresponding slide presen-
tations. To build our corpus, we searched the in-
ternet for web pages containing workshop proceed-
ings from various fields using generic queries such
as ‘workshop slide paper’. The collected papers and
presentations come from a variety of fields but tend
to be focused generally on science and technology.
Workshop proceedings are an ideal source for our
data because they often provide the papers and slide
presentations side-by-side. Using this strategy, we
manually extracted 296 slide-paper pairs. The pa-
pers were downloaded in PDF format and the slides
were a mixture of PDF and Powerpoint formats. Be-
fore working with these files, we converted them to a
custom XML format which represents relevant parts
of the original data as logical regions. In the case of
slides, regions include bullets, headings, and other
text spans. In the case of papers, regions include re-
gions (or passages) which correspond to paragraphs,
section headings, and list items.
To work with PDF data, we convert it to a cus-
tom XML format which represents logical chunks
or regions of the paper. In our approach we delimit
regions by orthographic boundaries. Orthographic
boundaries delimit the physical structure of a paper
and describe the paper in a physical fashion in terms
of paragraphs, headings, bullets, etc. We do recog-
nize that there are other ways to define paper regions
though. As Callan (1994) observes, academic papers
could also be represented via semantic boundaries
which delimit the topical structure of papers and de-
</bodyText>
<page confidence="0.997369">
112
</page>
<bodyText confidence="0.999838363636363">
scribe them in terms of where new topics are intro-
duced and where old ones are no longer discussed.
We prefer using orthographic boundaries in our ap-
proach for two reasons. First, detecting orthographic
boundaries can be accomplished with simple heuris-
tics while topic boundary detection requires more
sophisticated methods2, thus implementation is eas-
ier. Second, because orthographic boundaries are far
less subjective than topic boundaries, it’s easier to
verify the validity of orthographic boundaries than
semantic ones.
Preprocessing Powerpoint files is significantly
simpler than PDF files. To convert the Powerpoint
data to our custom XML, we first convert the Pow-
erpoint file to an OpenOffice.org3 ODP file via the
document converter tool that comes standard with
OpenOffice. ODP files are already encoded with a
rich XML which already describes physical regions
such as list items, bullets, and other text, so region
identification is unnecessary. We only needed to im-
plement a filter that translates the available data to
the custom XML format.
</bodyText>
<sectionHeader confidence="0.999605" genericHeader="method">
4 Alignment Methods
</sectionHeader>
<bodyText confidence="0.9999572">
Discovering how humans generate slide presenta-
tions from papers starts with observing where slide
regions originate from. We make the general as-
sumption that a slide region either a) is a summa-
rization (excerpt or abstract) from the associated pa-
per, or b) comes from other sources including but
not limited to the author’s personal (world and/or
specific) knowledge. A complete alignment module
would thus need to be able to discern if the informa-
tion in a region comes from the target paper or if it
does not. When it does, the task of the aligner is then
to choose the region in the paper that is summarized
or from which the excerpt is taken. Our original hy-
pothesis was that the vast majority of the data in a
given slide presentation would come from the target
paper and concluded that a reasonable first attempt
at building an aligner could be made under this as-
sumption.
We approach the task of aligning slide regions to
paper regions with methods popular in information
</bodyText>
<footnote confidence="0.99731375">
2Reynar (1998) provides a detailed overview of the basic
topic detection and segmentation methods
3OpenOffice.org is a freely available office suite available at
http://www.openoffice.org.
</footnote>
<table confidence="0.9998504">
Aligner Scoring Query Expansion
A Method 1 No
B Method 1 Yes
C Method 2 No
D Method 2 Yes
</table>
<tableCaption confidence="0.999718">
Table 1: Features implemented by each aligner.
</tableCaption>
<bodyText confidence="0.999503307692308">
retrieval. When aligning a slide region to a paper
region, we treat the slide region as a search query
and the target regions as documents in the informa-
tion retrieval sense. We compare two TF-IDF based
scoring methods and the effect of query expansion
by building four different aligners, each of which
corresponds to one combination of scoring type and
usage of query expansion. Table 1 shows a diagram
indicating which aligners have which features.
To prepare both the slide region and paper for
alignment, certain preprocessing tasks are executed
by all our aligners. The general procedure all our
aligners follow is outlined below:
</bodyText>
<listItem confidence="0.948700642857143">
1. For each token in each region in the paper, the to-
ken’s TF-IDF score is calculated, where the token’s
term frequency is the frequency of the token’s stem
in the region and the term’s document frequency is
the number of regions containing the token’s stem.
2. The slide region is tokenized and part-of-speech
tagged with the SNoW tagger (Roth, 1998) and non-
content words are removed. We consider content
words to be any token which is either a noun, adjec-
tive, verb, adverb, or cardinal number.
3. Each token in the slide region is stemmed and, in
the case of aligners B and D, query expansion is
performed.
4. A score is calculated for each region in the target pa-
</listItem>
<bodyText confidence="0.8519145">
per according to the scoring function implemented
by the aligner–method 1 for aligners A and B and
method 2 for aligners C and D.
These methods are presented in detail below.
</bodyText>
<subsectionHeader confidence="0.931441">
4.1 Scoring Methods
</subsectionHeader>
<bodyText confidence="0.999241888888889">
In this paper we investigate two scoring methods,
which we’ll refer to as scoring method 1 and scor-
ing method 2. Scoring method 1 is implemented by
aligners A and B and is equivalent to the average TF-
IDF score of the search terms relative to the target
region. I.e. to calculate the score for a slide region
relative to a target paper region with method 1, the
TF-IDF scores of all the search terms are added and
the sum is divided by the number of terms, and the
</bodyText>
<page confidence="0.996516">
113
</page>
<bodyText confidence="0.999977933333333">
target region with the highest average score wins.
Scoring method 2 is implemented by aligners C and
D and is based on the quantity of matched terms, re-
verting to scoring method 1 only in the case of a tie.
Thus, to calculate the score for a slide region rela-
tive to a target paper region with method 2, the num-
ber of search terms with non-zero TF-IDF scores for
the paper region is counted and the region with the
largest number of such search terms wins. In the
case of a tie, the average score is calculated as it is
in method 1 and the region with the highest average
score wins the tie.
With either scoring method, a zero score results
in the system predicting that the slide region is not
derived from any paper region.
</bodyText>
<subsectionHeader confidence="0.967037">
4.2 Query Expansion
</subsectionHeader>
<bodyText confidence="0.999144625">
One common problem with rudimentary TF-IDF
based information retrieval systems is that match-
ing tokens must have a form identical to the search
terms. Hence, synonyms and other semantically-
related words that probably should match do not.
Query expansion is one way to consider terms which
are semantically near, but orthographically differ-
ent from the search terms. The general principle of
query expansion is that, via an external knowledge
base, semantic neighbors of search terms are added
to the search query before the score is calculated.
Our implementation of query expansion is utilized
by aligners B and D and uses Wordnet (Fellbaum,
1998) to extract synonyms of search terms. When a
slide region undergoes query expansion our aligner
executes the following steps:
</bodyText>
<listItem confidence="0.995006583333334">
1. The search terms are part-of-speech tagged using
the SNoW part-of-speech tagger (Roth, 1998) and
lemmatized with a morphological analyzer4.
2. The resulting lemmas and parts of speech are used
to query Wordnet for matching synsets.
3. Synonyms for all retrieved synsets are recorded.
4. When scoring occurs, the TF-IDF score of a search
term changes from the score of the stem to the maxi-
mum score among the stem and all its synonyms. In
the case of scoring method 2, a search term matches
if it stem is found in the target region or if any of its
synonyms’ stems are found.
</listItem>
<footnote confidence="0.980276">
4The morphological analyzer we use is called mor-
pha and is freely available and can be downloaded at
http://www.informatics.susx.ac.uk/research/groups/nlp/carroll
/morph.html
</footnote>
<sectionHeader confidence="0.997568" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999976657894737">
To evaluate our aligners, we manually checked the
alignment of each on four randomly chosen slide
presentation-paper pairs. We refer to these presen-
tations here as P1, P2, P3, and P4. Collectively,
these four presentations with their respective papers
amount to 587 alignment decisions which were eval-
uated according to the following guidelines. If the
slide region is either an excerpt from the chosen pa-
per region or if the slide region is an abstract of
the chosen paper region, the alignment is judged as
good. In cases where the matching excerpt or ab-
stract text spans more than one paper region, the
alignment is judged as good if the aligner selected
any of the involved regions. Otherwise, the align-
ment is judged as bad and an error code is recorded.
The three error codes we utilize are BR, NR, and ER.
BR is short for “better region” and indicates that the
alignment is bad because the chosen paper region is
not the paper region from which the slide region is
extracted or generated, but such a region does in-
deed exist. NR is short for “no region” and indi-
cates that the alignment is bad because there is no
region in the paper to which the slide region should
be aligned. ER is short for “existing region” and in-
dicates that the alignment is bad because the aligner
decided there was no paper region to which the slide
region should be aligned, but in fact there was. Also,
the type of each slide region was recorded as either
frontmatter (which covers text spans such as titles,
authors, dates, and addresses), outline, heading, bul-
let, or diagram. Table 2 illustrates the composition
of the four presentations insofar as slide region type
is concerned.
The distribution of slide region types is not sur-
prising. Table 2 shows that two of our presentations
included diagrams and the other two did not, and
that bullets not surprisingly account for more slide
regions than any other region type.
</bodyText>
<subsectionHeader confidence="0.999358">
5.1 Alignability of Slide Regions
</subsectionHeader>
<bodyText confidence="0.9997785">
Table 3 shows the percentage of slide regions which
have a target paper region (i.e. the percentage of
alignable slide regions). One surprising observation
is that only about half (57%) of the slide bullets were
alignable. This goes against our initial hypothesis
that the vast majority of slide regions would come
</bodyText>
<page confidence="0.996818">
114
</page>
<table confidence="0.999449">
Presentation Frontmatter Outline Heading Bullet Diagram
P1 3/174 (1.7%) 0/174 (0.0%) 5/174 (2.9%) 74/174 (42.5%) 92/174 (52.9%)
P2 9/181 (5.0%) 9/181 (5.0%) 34/181 (18.8%) 129/181 (71.3%) 0/181 (0.0%)
P3 5/114 (4.4%) 1/114 (0.9%) 52/114 (45.6%) 55/114 (48.2%) 0/114 (0.0%)
P4 5/118 (4.2%) 1/118 (0.8%) 13/118 (11.0%) 47/118 (39.8%) 52/118 (44.0%)
Total 22/587 (3.7%) 11/587 (1.9%) 104/587 (17.7%) 305/587 (52.0%) 144/587 (24.5%)
</table>
<tableCaption confidence="0.996552">
Table 2: Breakdown of slide text spans by type. Columns correspond to slide text span types. Percentages in each column measure the fraction
of text spans which are of the given type.
</tableCaption>
<bodyText confidence="0.811965142857143">
from the associated paper, and not from the author’s
knowledge.
Another important observation from the data in
table 3 is that the fraction of slide regions which are
alignable for any given presentation can vary wildly.
82% of P4’s regions were alignable while 60% of
P3’s and only 14% of P1’s regions were alignable.
</bodyText>
<subsectionHeader confidence="0.997518">
5.2 Aligner Accuracy
</subsectionHeader>
<bodyText confidence="0.99999095">
Tables 4 and 5 show the raw accuracy and alignable
accuracy of the four aligners respectively. Raw
accuracy is the number of slide regions correctly
aligned out of the total number of slide regions.
Alignable accuracy is the percentage of alignable
slide regions which were aligned correctly.
Given the surprising results that a large percent-
age of slide regions need not come from the paper,
any fully fledged slide to paper aligner would need
a module which first filters out the unalignable slide
regions. Because such a module is not implemented
in our aligners, as our aligners make the assumption
that each slide region has a corresponding paper re-
gion, we limit most of our accuracy evaluation to
alignable accuracy rather than raw accuracy.
From tables 4 and 5 we can easily see the im-
portance of such a filtering module. As our best
aligner, which achieves an average alignable accu-
racy of 75%, only achieves an average raw accuracy
of 50%.
</bodyText>
<subsectionHeader confidence="0.945723">
5.3 Error Analysis
</subsectionHeader>
<bodyText confidence="0.999969470588235">
Tables 6 and 7 show what percentage of an aligner’s
errors correspond to which error types. Because our
aligners are based on term matching, the only way
for them to predict no alignment is for the average
TF-IDF score of the terms to be zero (no matching
terms anywhere). Because this is a very rare event,
ER-type errors are also extremely rare, and are ex-
cluded from our error analysis.
We can see from tables 6 and 7 that our poorer
aligners (A and B) have a fairly even split between
BR-type and NR-type errors, while our better align-
ers (C and D) have a far greater percentage of NR-
type errors, indicating that the features we are in-
vestigating can only reduce BR-type errors. This
verifies the importance of the proposed alignability
module which first filters out unalignable slide re-
gions.
</bodyText>
<subsectionHeader confidence="0.625878">
5.4 Error Reduction
</subsectionHeader>
<bodyText confidence="0.999969909090909">
Tables 8 and 9 analyze how well query expansion
and scoring method 2 reduce errors by measuring
the percentage of errors made by one aligner, which
were not made by another. Four pairings of align-
ers are considered: A and B, A and C, B and D, and
C and D. By comparing aligner A to B and C to D,
we have one measure of the error reduction achieved
by adding query expansion to an aligner. If the ad-
dition of query expansion enables an aligner to cor-
rectly align slide regions which its query expansion-
less counterpart could not, then we should see large
percentages of errors being corrected when compar-
ing aligner A to B and C to D. By comparing aligner
A to C and B to D, we have a measure of the error
reduction achieved by implementing scoring method
2 instead of method 1.
Tables 8 and 9 show that aligner D significantly
reduced aligner B’s errors and aligner C significantly
reduced aligner A’s errors, but aligner B did not im-
prove much on A, nor did D on C. In other words,
adding query expansion did not significantly reduce
errors, but using scoring method 2 instead of 1 did.
</bodyText>
<sectionHeader confidence="0.999946" genericHeader="evaluation">
6 Discussion
</sectionHeader>
<subsectionHeader confidence="0.999811">
6.1 On Alignability
</subsectionHeader>
<bodyText confidence="0.998192333333333">
Before mentioning alignment performance, it is im-
portant to notice from our data that there is great va-
riety among slide presentations. For example, ta-
</bodyText>
<page confidence="0.996301">
115
</page>
<table confidence="0.994820333333334">
Presentation Frontmatter Outline Heading Bullet Diagram Overall
P1 3/3 (100.0%) 0/0 0/5 (0.0%) 21/74 (28.4%) 0/92 (0.0%) 24/174 (13.8%)
P2 9/9 (100.0%) 8/9 (88.9%) 24/34 (70.6%) 104/129 (80.6%) 0/0 145/181 (80.1%)
P3 5/5 (100.0%) 0/1 (0.0%) 48/52 (92.3%) 15/55 (27.3%) 0/0 68/114 (59.5%)
P4 4/5 (80.0%) 0/1 (0.0%) 11/13 (74.6%) 33/47 (70.2%) 49/52 (94.2%) 97/118 (82.2%)
Total 21/22 (95.5%) 8/11 (72.7%) 83/104 (79.8%) 173/305 (56.7%) 49/144 (34.0%) 334/587 (56.9%)
</table>
<tableCaption confidence="0.996225">
Table 3: Breakdown of alignable slide text spans by type. Columns correspond to slide text span types. Percentages in each column measure the
fraction of text spans of that type which are alignable. E.g. of the 129 bullets in presentation P2, 104 are alignable. The “Overall” column measures
the fraction of all text spans which are alignable. E.g. of the 181 text spans in presentation P2, 145 are alignable.
</tableCaption>
<table confidence="0.9999295">
Presentation Aligner A Aligner B Aligner C Aligner D
P1 34/174 (19.5%) 129/174 (16.7%) 37/174 (21.3%) 35/174 (20.1%)
P2 71/181 (39.2%) 64/181 (35.4%) 101/181 (55.8%) 97/181 (53.6%)
P3 66/114 (57.9%) 64/114 (56.1%) 77/114 (67.5%) 77/114 (67.5%)
P4 50/118 (42.4%) 48/118 (40.7%) 78/118 (66.1%) 77/118 (65.3%)
Total 221/587 (37.6%) 205/587 (34.9%) 293/587 (49.9%) 286/587 (48.7%)
</table>
<tableCaption confidence="0.9883285">
Table 4: Raw accuracy. Each column corresponds to one of the four aligners evaluated. Percentages measure the fraction of text spans which
were aligned correctly.
</tableCaption>
<table confidence="0.9999035">
Presentation Aligner A Aligner B Aligner C Aligner D
P1 12/24 (50.0%) 9/24 (37.5%) 15/24 (62.5%) 15/24 (62.5%)
P2 63/145 (43.4%) 56/145 (38.6%) 93/145 (64.1%) 90/145 (62.1%)
P3 55/68 (80.9%) 54/68 (79.4%) 66/68 (97.1%) 67/68 (98.5%)
P4 49/97 (50.5%) 47/97 (48.5%) 77/97 (79.4%) 76/97 (78.4%)
Total 179/334 (53.6%) 166/334 (49.7%) 251/334 (75.1%) 248/334 (74.3%)
</table>
<tableCaption confidence="0.991578">
Table 5: Alignable accuracy. Each column corresponds to one of the four aligners evaluated. Percentages measure the fraction of alignable text
spans which were aligned correctly.
</tableCaption>
<table confidence="0.999795">
Aligner A Aligner B
Presentation BR NR BR NR
P1 11/140 (7.9%) 128/140 (91.4%) 14/145 (9.7%) 130/145 (89.7%)
P2 82/110 (74.5%) 28/110 (25.5%) 89/117 (76.1%) 28/117 (23.9%)
P3 13/48 (27.1%) 35/48 (72.9%) 14/50 (28.0%) 36/50 (72.0%)
P4 48/68 (70.6%) 20/68 (29.4%) 50/70 (71.4%) 20/70 (28.6%)
Total 154/366 (42.1%) 211/366 (57.7%) 167/382 (43.7%) 214/382 (56.0%)
</table>
<tableCaption confidence="0.963671666666667">
Table 6: Error type breakdown for aligners A and B. Columns correspond to specific types of alignment errors. “BR” is short for “better region”
and “NR” is short for “no region”. An error of type “BR” means that the aligner choose an incorrect region in the paper, and a better region existed.
An error of type “NR” means the aligner choose an incorrect region, and there was no correct region.
</tableCaption>
<table confidence="0.999833571428571">
Presentation Aligner C Aligner D
BR NR BR NR
P1 8/137 (5.8%) 128/137 (93.4%) 8/139 (5.8%) 130/139 (93.5%)
P2 52/80 (65.0%) 28/80 (35.0%) 55/84 (65.5%) 29/84 (34.5%)
P3 2/37 (5.4%) 35/37 (94.6%) 1/37 (2.7%) 36/37 (97.3%)
P4 20/40 (50.0%) 20/40 (50.0%) 21/41 (51.2%) 20/41 (48.8%)
Total 82/294 (27.9%) 211/294 (71.8%) 85/301 (28.2%) 215/301 (71.4%)
</table>
<tableCaption confidence="0.748841333333333">
Table 7: Error type breakdown for aligners C and D. Columns correspond to specific types of alignment errors. “BR” is short for “better region”
and “NR” is short for “no region”. An error of type “BR” means that the aligner choose an incorrect region in the paper, and a better region existed.
An error of type “NR” means the aligner choose an incorrect region, and there was no correct region.
</tableCaption>
<page confidence="0.972527">
116
</page>
<table confidence="0.999621">
Aligner A → B Aligner A → C
Presentation BR NR Overall BR NR Overall
P1 0/11 (0.0%) 0/128 (0.0%) 0/140 (0.0%) 4/11 (36.4%) 0/128 (0.0%) 4/140 (2.9%)
P2 0/82 (0.0%) 0/28 (0.0%) 0/110 (0.0%) 38/82 (46.3%) 0/28 (0.0%) 38/110 (34.5%)
P3 0/13 (0.0%) 0/35 (0.0%) 0/48 (0.0%) 11/13 (84.6%) 0/35 (0.0%) 11/48 (22.9%)
P4 0/48 (0.0%) 0/20 (0.0%) 0/68 (0.0%) 31/48 (64.6%) 0/20 (0.0%) 31/68 (45.6%)
Total 0/154 (0.0%) 0/211 (0.0%) 0/366 (0.0%) 84/154 (54.5%) 0/211 (0.0%) 84/366 (23.0%)
</table>
<tableCaption confidence="0.9978366">
Table 8: Error reduction between aligners A and B, and between aligners A and C. Major columns correspond to aligner pairs and minor columns
correspond to error types. A pair denoted by X → Y indicates that the corresponding percentages are measuring the fraction of slide text spans
aligned incorrectly by aligner X, which were aligned correctly by aligner Y . E.g. from this table you can see that in presentation P1, aligner A
incorrectly aligned 140 text spans. 11 of them were BR-type errors and 128 of them were NR-type errors. Four of aligner A’s BR-type errors were
aligned correctly by aligner C.
</tableCaption>
<table confidence="0.999837571428571">
Aligner B → D Aligner C → D
Presentation BR NR Overall BR NR Overall
P1 7/14 (50.0%) 0/130 (0.0%) 7/145 (4.8%) 0/8 (0.0%) 0/128 (0.0%) 0/137 (0.0%)
P2 42/89 (47.2%) 0/28 (0.0%) 42/117 (35.9%) 1/52 (1.9%) 0/28 (0.0%) 1/80 (1.2%)
P3 13/14 (92.9%) 0/36 (0.0%) 13/50 (26.0%) 1/2 (50.0%) 0/35 (0.0%) 1/37 (2.7%)
P4 32/50 (64.0%) 0/20 (0.0%) 32/70 (45.7%) 1/20 (5.0%) 0/20 (0.0%) 1/40 (2.5%)
Total 94/167 (56.3%) 0/214 (0.0%) 94/382 (24.6%) 3/82 (3.7%) 0/211 (0.0%) 3/294 (1.0%)
</table>
<tableCaption confidence="0.9024816">
Table 9: Error reduction between aligners B and D, and between aligners C and D. Major columns correspond to aligner pairs and minor columns
correspond to error types. A pair denoted by X → Y indicates that the corresponding percentages are measuring the fraction of slide text spans
aligned incorrectly by aligner X, which were aligned correctly by aligner Y . E.g. from this table you can see that in presentation P1, aligner B
incorrectly aligned 145 text spans. 14 of them were BR-type errors and 130 of them were NR-type errors. 7 of aligners B’s BR-type errors were
correctly aligned by aligner D.
</tableCaption>
<bodyText confidence="0.999959888888889">
ble 3 shows that 28% of P1’s bullets were alignable,
while 81% of P2’s were alignable. P1 and P4 both
contained diagrams, but only P4’s diagram existed
in the paper. Our initial hypothesis was that the vast
majority of slide regions would either be excerpts or
abstracts from/of the paper regions. Table 3 shows
that a nontrivial amount of slide regions does not
map to the paper at all. Also, tables 6 and 7 show
that as a result, NR-type errors make up the majority
of the errors made by the better aligners. Thus, the
data indicates that the task of slide-presentation gen-
eration is highly dependent on the end purpose the
presentation will serve, as well as the target audience
and other factors. We will focus more on identify-
ing these factors in future research. Once identified,
these factors should be quantified and controlled in
future corpora of presentation-paper pairs used for
this task.
</bodyText>
<subsectionHeader confidence="0.998649">
6.2 On Scoring Methods and Query Expansion
</subsectionHeader>
<bodyText confidence="0.9999555">
Our results clearly show that, for this task, query
expansion has little or negative impact on aligners
and that scoring method 2 is indeed superior to scor-
ing method 1. Tables 4 and 5 show that aligner
C consistently outperforms aligner A and aligner D
consistently outperforms aligner B, especially when
limited to alignable slide regions. Hence, scoring
method 2 is better than method 1. We can also
see from tables 4 and 5 that aligner B consistently
under-performs A and aligner D consistently under-
performs C, which shows that query expansion does
not improve performance and in fact, it degrades it.
Tables 8 and 9 show the same results from a differ-
ent perspective: aligner C correctly aligned 55% of
the aligner A’s erroneous alignable slide regions and
aligner D correctly aligned 56% of aligner B’s erro-
neous alignable slide regions. But aligner B did not
catch any of aligner A’s errors and aligner D only
caught 4% of aligner C’s errors – but ended up mak-
ing more in the end anyway.
With regard to query expansion, there are two
possibilities. Query expansion was not very help-
ful here because either (a) slide authors tend to use
wording identical to that in the paper, or (b) using
synonyms from Wordnet is not aggressive enough
and we should consider expanding our query expan-
sion approach to include hypernyms, immediate hy-
ponyms, and other semantically related terms. We
think the data suggests that (a) is more the case than
(b). If (b) were the case, including synonyms in our
search should have improved the performance, just
not by a lot. In actually, aligner B performed worse
</bodyText>
<page confidence="0.99276">
117
</page>
<bodyText confidence="0.999794575">
on average than aligner A, and likewise with aligner
D when compared to C. Synonyms are semanti-
cally closer to the original term than hypernyms,
hyponyms, or other semantically related terms, and
our results show that introducing this small amount
of semantic distance is (a little bit) detrimental. By
adding hypernyms and other relations, only a wider,
less focused group of terms will be introduced which
will probably just result in more false positives.
One possible criticism against our argument for
(a) could be that our implementation of query expan-
sion performed poorly because we don’t word sense
disambiguate, and thus we introduce synonyms from
incorrect senses of each term. This probably isn’t
the case because the search terms are not in isolation,
but are part of a larger query. For an incorrect paper
region to be select based on an error of this type,
it would have to contain many of the terms in the
query as well as the semantically inaccurate sense of
the one in question. This situation is unlikely due to
one of the most basic assumptions made when sense
disambiguating: that context restricts the possible
senses of any word. So, if a paper region contains
many of the terms in a slide region, it is unlikely
that it will also contain the off-topic, semantically
awkward term pertaining to a bad sense of one of
them.
With regard to scoring methods. Average TF-IDF
scoring is probably ineffective in this application be-
cause of the nature of paper regions. When retriev-
ing whole documents given a search query, one doc-
ument’s contents are probably independent of any
other, so terms related to the document’s topic are
stated explicitly. Paper regions, however, are in the
context of each other. The topic of one can be very
similar to another, only because it’s nearby, not be-
cause of the terms explicitly mentioned in the re-
gion. Add to this the fact that paper regions are ex-
tremely non-uniform in length and TF-IDF scores
end up skewed.
</bodyText>
<subsectionHeader confidence="0.986441">
6.3 On Improvement
</subsectionHeader>
<bodyText confidence="0.999991472222222">
There is a lot of room for improvement on slide
to paper alignment. As mentioned previously in
section 6.1, unalignable slide regions account for a
much larger portion of the slide presentations than
our initial hypothesis predicted; around 70% of the
errors made by our better aligners (C and D) were
NR-type errors, meaning the alignment was bad be-
cause the system selected a paper region when in fact
there was no correct paper region. A robust slide to
paper aligner would need to have a module capable
of filtering out unalignable slide regions. If this task
were solved and implemented on our better aligners,
raw accuracy would raise from 50% to about 75%
on average which is nearing the level of robustness
necessary for real-world applications.
We also suggest that, in regard to alignable slide
regions, performance would be significantly boosted
by taking context into account, both on the slide and
paper side. We noticed during evaluation that many
of the BR-type errors occurred when the slide region
in question lacked the necessary terms, but the terms
existed in nearby slide regions. Examples of this in-
clude when for instance, the title is broken across
two lines and the second line only has a word or
two in it, or when a heading is rather non-descriptive
but the sub-bullets beneath it contain many relevant
terms to the topic. Incorporating terms of nearby
slide regions (perhaps in query-expansion fashion),
rather than just treating each one as an independent
search query will certainly boost performance.
Likewise on the paper end, it is reasonable to as-
sume that in most cases, the topic of one region is
similar to the topics of adjacent regions. And just as
terms from nearby slide regions could supplement
term-poor slide regions, terms from nearby paper re-
gions could supplement term-poor paper regions.
</bodyText>
<sectionHeader confidence="0.999503" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999492266666667">
In this paper we investigated the task of automatic
slide to paper alignment. We built a corpus of
slide-paper pairs and used four presentations from
it to evaluate four aligners which utilize methods
such as TF-IDF term weighting and query expan-
sion. We showed that query expansion does not im-
prove performance in our application and that TF-
IDF term weighting is inferior to a much simpler
scoring mechanism based on the number of matched
terms. For future improvements, we suggest that a
module capable of robustly filtering out unalignable
slide regions is necessary. We also suggest that per-
formance can be improved by taking context into ac-
count and using terms in nearby regions to supple-
ment both slide regions and paper regions.
</bodyText>
<page confidence="0.997733">
118
</page>
<sectionHeader confidence="0.998303" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.993799829268292">
Regina Barzilay and Noemie Elhadad. 2003. Sen-
tence alignment for monolingual comparable corpora.
In Proceedings of the 2003 Conference on Empirical
Methods in Natural Language Processing (EMNLP).
James P. Callan. 1994. Passage-level evidence in docu-
ment retrieval. In Proceedings of the 17th Annual In-
ternational ACM SIGIR Conference on Research and
Development in Information Retrieval.
Christiane Fellbaum. 1998. WordNet - An Electronic
Lexical Database. Cambridge MA: MIT Press.
Rani Nelken and Stuart M. Shieber. 2008. Towards ro-
bust context-sensitive sentence alignment for monolin-
gua corpora. In Proceedings of the 11th Conference of
the European Chapter of the Association for Compu-
tational Linguistics.
Jeffrey C. Reynar. 1998. Topic Segmentation: Algo-
rithms and Applications. Ph.D. thesis, University of
Pennsylvania.
Dan Roth. 1998. Learning to resolve natural lanuage
ambiguities: A unified approach. In Proceedings of
the 15th Conference of the American Association for
Artificial Intelligence (AAAI).
Tomohide Shibata and Sadao Kurohashi. 2005. Au-
tomatic slide generation based on discourse structure
analysis. In Proceedings of the second international
joint conference on natural language processing (IJC-
NLP).
Masao Utiyama and Koiti Hasida. 1999. Automatic slide
presentation from semantically annotated documents.
In Proceedings of the workshop held in conjunction
with the 37th annual meeting of the Association for
Computational Linguistics (ACL).
Lonneke van der Plas and J¨org Tiedemann. 2008. Us-
ing lexico-semantic information for query expansion
in passage retrieval for question answering. In Pro-
ceedings of the 9th SIGdial Workshop on Discourse
and Dialogue.
Ellen M. Voorhees. 1994. Query expansion using
lexical-semantic relations. In Proceedings of the 17th
Annual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval.
</reference>
<page confidence="0.999087">
119
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.812967">
<title confidence="0.998669">Investigating Automatic Alignment Methods for Slide Generation Academic Papers</title>
<author confidence="0.998723">Brandon Beamer</author>
<author confidence="0.998723">Roxana</author>
<affiliation confidence="0.9995545">Department of University of</affiliation>
<address confidence="0.830502">Urbana,</address>
<abstract confidence="0.9989513125">In this paper we investigate the task of automatic generation of slide presentations from academic papers, focusing initially on slide to paper alignment. We compare and evaluate four different alignment systems which utilize various combinations of methods used widely in other alignment and question answering approaches, such as TF-IDF term weighting and query expansion. Our best aligner achieves an accuracy of 75% and our findings show that for this application, average TF-IDF scoring performs more poorly than a simpler method based on the number of matched terms, and query expansion degrades aligner performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Noemie Elhadad</author>
</authors>
<title>Sentence alignment for monolingual comparable corpora.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="3673" citStr="Barzilay and Elhadad (2003)" startWordPosition="558" endWordPosition="562">region) alignment, which can be categorized best as alignment between monolingual comparable corpora, but 1The GDA tag set is designed to allow machines to automatically infer the underlying structure of documents. More information is available at http://i-content.org/gda. 111 Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 111–119, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics could also be easily construed as document passage retrieval, which is a well-researched topic in the Information Retrieval community. Barzilay and Elhadad (2003) incorporate context to facilitate alignment between monolingual comparable corpora by first learning paragraph matching rules in a supervised way, and then refining the alignment at the sentence level within paragraphs. Nelken and Shieber (2008) used TF-IDF term weighting with logistic regression to align sentences from pericopes in the gospels of the new testament. Callan (1994) analyzed various ways to define document passages and identified three main passage types, discourse (based on physical structure of the document), semantic (based on topic boundaries), and window (based on token dis</context>
<context position="5037" citStr="Barzilay and Elhadad (2003)" startWordPosition="774" endWordPosition="777">ed to sloppy writing, visual aids, or other factors, paragraph boundaries may not be the best indicators of content boundaries. Our alignment task differs from that of (Barzilay and Elhadad, 2003) and (Nelken and Shieber, 2008) in two ways. First, Barzilay and Elhadad (2003) and Nelken and Shieber (2008) align like-chunks between the two documents. That is, they are either aligning sentences to sentences or paragraphs to paragraphs. In our task we are aligning slide regions which are usually bullets spanning at most a couple lines, to paper regions which can be a whole paragraph long. Second, Barzilay and Elhadad (2003) and (Nelken and Shieber, 2008) are working with comparable corpora in which the same information is assumed to be present in each document, but expressed in a different way. We are not able to necessarily make this assumption, in fact we show in this paper that as much as half of the information in slide presentations may not be present in the corresponding paper. The concept of query expansion that we implement in some of our aligners is also not new. Voorhees (1994) suggests that query expansion tends to help performance with short, incomplete queries but degrades performance with longer, m</context>
</contexts>
<marker>Barzilay, Elhadad, 2003</marker>
<rawString>Regina Barzilay and Noemie Elhadad. 2003. Sentence alignment for monolingual comparable corpora. In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>James P Callan</author>
</authors>
<title>Passage-level evidence in document retrieval.</title>
<date>1994</date>
<booktitle>In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.</booktitle>
<contexts>
<context position="4056" citStr="Callan (1994)" startWordPosition="619" endWordPosition="620">lorado, June 2009. c�2009 Association for Computational Linguistics could also be easily construed as document passage retrieval, which is a well-researched topic in the Information Retrieval community. Barzilay and Elhadad (2003) incorporate context to facilitate alignment between monolingual comparable corpora by first learning paragraph matching rules in a supervised way, and then refining the alignment at the sentence level within paragraphs. Nelken and Shieber (2008) used TF-IDF term weighting with logistic regression to align sentences from pericopes in the gospels of the new testament. Callan (1994) analyzed various ways to define document passages and identified three main passage types, discourse (based on physical structure of the document), semantic (based on topic boundaries), and window (based on token distance) and suggests that while discourse passages may be an attractive way to define and retrieve document passages, due to reasons related to sloppy writing, visual aids, or other factors, paragraph boundaries may not be the best indicators of content boundaries. Our alignment task differs from that of (Barzilay and Elhadad, 2003) and (Nelken and Shieber, 2008) in two ways. First</context>
<context position="8042" citStr="Callan (1994)" startWordPosition="1271" endWordPosition="1272">regions include bullets, headings, and other text spans. In the case of papers, regions include regions (or passages) which correspond to paragraphs, section headings, and list items. To work with PDF data, we convert it to a custom XML format which represents logical chunks or regions of the paper. In our approach we delimit regions by orthographic boundaries. Orthographic boundaries delimit the physical structure of a paper and describe the paper in a physical fashion in terms of paragraphs, headings, bullets, etc. We do recognize that there are other ways to define paper regions though. As Callan (1994) observes, academic papers could also be represented via semantic boundaries which delimit the topical structure of papers and de112 scribe them in terms of where new topics are introduced and where old ones are no longer discussed. We prefer using orthographic boundaries in our approach for two reasons. First, detecting orthographic boundaries can be accomplished with simple heuristics while topic boundary detection requires more sophisticated methods2, thus implementation is easier. Second, because orthographic boundaries are far less subjective than topic boundaries, it’s easier to verify t</context>
</contexts>
<marker>Callan, 1994</marker>
<rawString>James P. Callan. 1994. Passage-level evidence in document retrieval. In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet - An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<location>Cambridge MA:</location>
<contexts>
<context position="13917" citStr="Fellbaum, 1998" startWordPosition="2273" endWordPosition="2274">ry TF-IDF based information retrieval systems is that matching tokens must have a form identical to the search terms. Hence, synonyms and other semanticallyrelated words that probably should match do not. Query expansion is one way to consider terms which are semantically near, but orthographically different from the search terms. The general principle of query expansion is that, via an external knowledge base, semantic neighbors of search terms are added to the search query before the score is calculated. Our implementation of query expansion is utilized by aligners B and D and uses Wordnet (Fellbaum, 1998) to extract synonyms of search terms. When a slide region undergoes query expansion our aligner executes the following steps: 1. The search terms are part-of-speech tagged using the SNoW part-of-speech tagger (Roth, 1998) and lemmatized with a morphological analyzer4. 2. The resulting lemmas and parts of speech are used to query Wordnet for matching synsets. 3. Synonyms for all retrieved synsets are recorded. 4. When scoring occurs, the TF-IDF score of a search term changes from the score of the stem to the maximum score among the stem and all its synonyms. In the case of scoring method 2, a s</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet - An Electronic Lexical Database. Cambridge MA: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rani Nelken</author>
<author>Stuart M Shieber</author>
</authors>
<title>Towards robust context-sensitive sentence alignment for monolingua corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3919" citStr="Nelken and Shieber (2008)" startWordPosition="596" endWordPosition="599">http://i-content.org/gda. 111 Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 111–119, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics could also be easily construed as document passage retrieval, which is a well-researched topic in the Information Retrieval community. Barzilay and Elhadad (2003) incorporate context to facilitate alignment between monolingual comparable corpora by first learning paragraph matching rules in a supervised way, and then refining the alignment at the sentence level within paragraphs. Nelken and Shieber (2008) used TF-IDF term weighting with logistic regression to align sentences from pericopes in the gospels of the new testament. Callan (1994) analyzed various ways to define document passages and identified three main passage types, discourse (based on physical structure of the document), semantic (based on topic boundaries), and window (based on token distance) and suggests that while discourse passages may be an attractive way to define and retrieve document passages, due to reasons related to sloppy writing, visual aids, or other factors, paragraph boundaries may not be the best indicators of c</context>
</contexts>
<marker>Nelken, Shieber, 2008</marker>
<rawString>Rani Nelken and Stuart M. Shieber. 2008. Towards robust context-sensitive sentence alignment for monolingua corpora. In Proceedings of the 11th Conference of the European Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey C Reynar</author>
</authors>
<title>Topic Segmentation: Algorithms and Applications.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="10244" citStr="Reynar (1998)" startWordPosition="1631" endWordPosition="1632">lignment module would thus need to be able to discern if the information in a region comes from the target paper or if it does not. When it does, the task of the aligner is then to choose the region in the paper that is summarized or from which the excerpt is taken. Our original hypothesis was that the vast majority of the data in a given slide presentation would come from the target paper and concluded that a reasonable first attempt at building an aligner could be made under this assumption. We approach the task of aligning slide regions to paper regions with methods popular in information 2Reynar (1998) provides a detailed overview of the basic topic detection and segmentation methods 3OpenOffice.org is a freely available office suite available at http://www.openoffice.org. Aligner Scoring Query Expansion A Method 1 No B Method 1 Yes C Method 2 No D Method 2 Yes Table 1: Features implemented by each aligner. retrieval. When aligning a slide region to a paper region, we treat the slide region as a search query and the target regions as documents in the information retrieval sense. We compare two TF-IDF based scoring methods and the effect of query expansion by building four different aligners</context>
</contexts>
<marker>Reynar, 1998</marker>
<rawString>Jeffrey C. Reynar. 1998. Topic Segmentation: Algorithms and Applications. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
</authors>
<title>Learning to resolve natural lanuage ambiguities: A unified approach.</title>
<date>1998</date>
<booktitle>In Proceedings of the 15th Conference of the American Association for Artificial Intelligence (AAAI).</booktitle>
<contexts>
<context position="11544" citStr="Roth, 1998" startWordPosition="1845" endWordPosition="1846">on. Table 1 shows a diagram indicating which aligners have which features. To prepare both the slide region and paper for alignment, certain preprocessing tasks are executed by all our aligners. The general procedure all our aligners follow is outlined below: 1. For each token in each region in the paper, the token’s TF-IDF score is calculated, where the token’s term frequency is the frequency of the token’s stem in the region and the term’s document frequency is the number of regions containing the token’s stem. 2. The slide region is tokenized and part-of-speech tagged with the SNoW tagger (Roth, 1998) and noncontent words are removed. We consider content words to be any token which is either a noun, adjective, verb, adverb, or cardinal number. 3. Each token in the slide region is stemmed and, in the case of aligners B and D, query expansion is performed. 4. A score is calculated for each region in the target paper according to the scoring function implemented by the aligner–method 1 for aligners A and B and method 2 for aligners C and D. These methods are presented in detail below. 4.1 Scoring Methods In this paper we investigate two scoring methods, which we’ll refer to as scoring method </context>
<context position="14138" citStr="Roth, 1998" startWordPosition="2306" endWordPosition="2307">ne way to consider terms which are semantically near, but orthographically different from the search terms. The general principle of query expansion is that, via an external knowledge base, semantic neighbors of search terms are added to the search query before the score is calculated. Our implementation of query expansion is utilized by aligners B and D and uses Wordnet (Fellbaum, 1998) to extract synonyms of search terms. When a slide region undergoes query expansion our aligner executes the following steps: 1. The search terms are part-of-speech tagged using the SNoW part-of-speech tagger (Roth, 1998) and lemmatized with a morphological analyzer4. 2. The resulting lemmas and parts of speech are used to query Wordnet for matching synsets. 3. Synonyms for all retrieved synsets are recorded. 4. When scoring occurs, the TF-IDF score of a search term changes from the score of the stem to the maximum score among the stem and all its synonyms. In the case of scoring method 2, a search term matches if it stem is found in the target region or if any of its synonyms’ stems are found. 4The morphological analyzer we use is called morpha and is freely available and can be downloaded at http://www.infor</context>
</contexts>
<marker>Roth, 1998</marker>
<rawString>Dan Roth. 1998. Learning to resolve natural lanuage ambiguities: A unified approach. In Proceedings of the 15th Conference of the American Association for Artificial Intelligence (AAAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomohide Shibata</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Automatic slide generation based on discourse structure analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the second international joint conference on natural language processing (IJCNLP).</booktitle>
<contexts>
<context position="1365" citStr="Shibata and Kurohashi, 2005" startWordPosition="198" endWordPosition="202">ler method based on the number of matched terms, and query expansion degrades aligner performance. 1 Introduction Automatic generation of slide presentations is a task the Computational Linguistics community has not yet pursued in much depth. A robust system capable of generating slide presentations from papers would save the author much tedium when organizing her presentations. In this paper we investigate this task from a novel perspective. While others have developed interesting approaches to slide generation from documents by modeling the problem in a unique way (Utiyama and Hasida, 1999; Shibata and Kurohashi, 2005), the aim of the research this paper initiates is to discover how humans create slide presentations, focusing more specifically on academic papers. Thus we take a corpus-based approach to the problem, and as a first step focus on the task of automatically aligning slide presentations to academic papers. We built a corpus of 296 slide-paper pairs and implemented four slide to paper aligners which utilize popular information retrieval methods such as TFIDF term weighting and query expansion. In this paper we show that, in this application, TF-IDF term weighting is inferior to a simpler scoring m</context>
<context position="2729" citStr="Shibata and Kurohashi, 2005" startWordPosition="417" endWordPosition="420">cy of 75%. 2 Related Work Automatic slide generation from documents is a thus far under-investigated topic. Utiyama and Hasida (1999) generate slides from GDA1 (global document annotation) tagged documents. They detect topics within the documents by analyzing GDA coreference links, modeled each slide as a topic and itemized elaborations (which were also tagged with the GDA tag set). Shibata and Kurohashi (2005) convert Japanese documents to slide representation by parsing their discourse structures and representing the resulting tree in an outline format. While (Utiyama and Hasida, 1999) and (Shibata and Kurohashi, 2005) generate slides from documents by modeling the task in creative ways, we aim to learn something deeper regarding how humans actually go about the task. Creating a corpus of slide/paper pairs will enable us to study the intricacies involved in how real humans approach this task. Our current focus is slide to paper (region) alignment, which can be categorized best as alignment between monolingual comparable corpora, but 1The GDA tag set is designed to allow machines to automatically infer the underlying structure of documents. More information is available at http://i-content.org/gda. 111 Proce</context>
</contexts>
<marker>Shibata, Kurohashi, 2005</marker>
<rawString>Tomohide Shibata and Sadao Kurohashi. 2005. Automatic slide generation based on discourse structure analysis. In Proceedings of the second international joint conference on natural language processing (IJCNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masao Utiyama</author>
<author>Koiti Hasida</author>
</authors>
<title>Automatic slide presentation from semantically annotated documents.</title>
<date>1999</date>
<booktitle>In Proceedings of the workshop held in conjunction with the 37th annual meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="1335" citStr="Utiyama and Hasida, 1999" startWordPosition="194" endWordPosition="197">ms more poorly than a simpler method based on the number of matched terms, and query expansion degrades aligner performance. 1 Introduction Automatic generation of slide presentations is a task the Computational Linguistics community has not yet pursued in much depth. A robust system capable of generating slide presentations from papers would save the author much tedium when organizing her presentations. In this paper we investigate this task from a novel perspective. While others have developed interesting approaches to slide generation from documents by modeling the problem in a unique way (Utiyama and Hasida, 1999; Shibata and Kurohashi, 2005), the aim of the research this paper initiates is to discover how humans create slide presentations, focusing more specifically on academic papers. Thus we take a corpus-based approach to the problem, and as a first step focus on the task of automatically aligning slide presentations to academic papers. We built a corpus of 296 slide-paper pairs and implemented four slide to paper aligners which utilize popular information retrieval methods such as TFIDF term weighting and query expansion. In this paper we show that, in this application, TF-IDF term weighting is i</context>
<context position="2695" citStr="Utiyama and Hasida, 1999" startWordPosition="412" endWordPosition="415">best aligner achieves an accuracy of 75%. 2 Related Work Automatic slide generation from documents is a thus far under-investigated topic. Utiyama and Hasida (1999) generate slides from GDA1 (global document annotation) tagged documents. They detect topics within the documents by analyzing GDA coreference links, modeled each slide as a topic and itemized elaborations (which were also tagged with the GDA tag set). Shibata and Kurohashi (2005) convert Japanese documents to slide representation by parsing their discourse structures and representing the resulting tree in an outline format. While (Utiyama and Hasida, 1999) and (Shibata and Kurohashi, 2005) generate slides from documents by modeling the task in creative ways, we aim to learn something deeper regarding how humans actually go about the task. Creating a corpus of slide/paper pairs will enable us to study the intricacies involved in how real humans approach this task. Our current focus is slide to paper (region) alignment, which can be categorized best as alignment between monolingual comparable corpora, but 1The GDA tag set is designed to allow machines to automatically infer the underlying structure of documents. More information is available at h</context>
</contexts>
<marker>Utiyama, Hasida, 1999</marker>
<rawString>Masao Utiyama and Koiti Hasida. 1999. Automatic slide presentation from semantically annotated documents. In Proceedings of the workshop held in conjunction with the 37th annual meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lonneke van der Plas</author>
<author>J¨org Tiedemann</author>
</authors>
<title>Using lexico-semantic information for query expansion in passage retrieval for question answering.</title>
<date>2008</date>
<booktitle>In Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue.</booktitle>
<marker>van der Plas, Tiedemann, 2008</marker>
<rawString>Lonneke van der Plas and J¨org Tiedemann. 2008. Using lexico-semantic information for query expansion in passage retrieval for question answering. In Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen M Voorhees</author>
</authors>
<title>Query expansion using lexical-semantic relations.</title>
<date>1994</date>
<booktitle>In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.</booktitle>
<contexts>
<context position="5510" citStr="Voorhees (1994)" startWordPosition="862" endWordPosition="863">are usually bullets spanning at most a couple lines, to paper regions which can be a whole paragraph long. Second, Barzilay and Elhadad (2003) and (Nelken and Shieber, 2008) are working with comparable corpora in which the same information is assumed to be present in each document, but expressed in a different way. We are not able to necessarily make this assumption, in fact we show in this paper that as much as half of the information in slide presentations may not be present in the corresponding paper. The concept of query expansion that we implement in some of our aligners is also not new. Voorhees (1994) suggests that query expansion tends to help performance with short, incomplete queries but degrades performance with longer, more complete queries. van der Plas and Tiedemann (2008) investigated several types of lexico-semantic information for query expansion in their question answering system. They found that expansions that bridge the terminology gap (synonyms, etc.) did not result in improvement but expansions that bridge the knowledge gap (words belonging to the same subject field) did. In this paper, to get an idea of the baseline performance of query expansion with regard to our unique </context>
</contexts>
<marker>Voorhees, 1994</marker>
<rawString>Ellen M. Voorhees. 1994. Query expansion using lexical-semantic relations. In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>