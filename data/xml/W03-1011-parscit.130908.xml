<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004329">
<title confidence="0.993813">
A General Framework for Distributional Similarity
</title>
<author confidence="0.975908">
Julie Weeds and David Weir
</author>
<affiliation confidence="0.9934425">
School of Cognitive and Computing Sciences
University of Sussex
</affiliation>
<address confidence="0.979169">
Brighton, BN1 9QH, UK
</address>
<email confidence="0.643105">
{juliewe, davidw}Acogs.susx.ac.uk
</email>
<sectionHeader confidence="0.989519" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999795">
We present a general framework for
distributional similarity based on the
concepts of precision and recall. Dif-
ferent parameter settings within this
framework approximate different ex-
isting similarity measures as well as
many more which have, until now,
been unexplored. We show that op-
timal parameter settings outperform
two existing state-of-the-art similarity
measures on two evaluation tasks for
high and low frequency nouns.
</bodyText>
<sectionHeader confidence="0.998524" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999779057971015">
There are many potential applications of sets of
distributionally similar words. In the syntactic
domain, language models, which can be used to
evaluate alternative interpretations of text and
speech, require probabilistic information about
words and their co-occurrences which is often
not available due to the sparse data problem.
In order to overcome this problem, researchers
(e.g. Pereira et al. (1993)) have proposed es-
timating probabilities based on sets of words
which are known to be distributionally similar.
In the semantic domain, the hypothesis that
words which mean similar things behave in sim-
ilar ways (Levin, 1993), has led researchers (e.g.
Lin (1998)) to propose that distributional sim-
ilarity might be used as a predictor of seman-
tic similarity. Accordingly, we might automat-
ically build thesauruses which could be used in
tasks such as malapropism correction (Budan-
itsky and Hirst, 2001) and text summarization
(Silber and McCoy, 2002).
However, the loose definition of distributional
similarity that two words are distributionally
similar if they appear in similar contexts has
led to many distributional similarity measures
being proposed; for example, the L1 Norm, the
Euclidean Distance, the Cosine Metric (Salton
and McGill, 1983), Jaccard&apos;s Coefficient (Frakes
and Baeza-Yates, 1992), the Dice Coefficient
(Frakes and Baeza-Yates, 1992), the Kullback-
Leibler Divergence (Cover and Thomas, 1991),
the Jenson-Shannon Divergence (Rao, 1983),
the a-skew Divergence (Lee, 1999), the Con-
fusion Probability (Essen and Steinbiss, 1992),
Hindle&apos;s Mutual Information(MI)-Based Mea-
sure (Hindle, 1990) and Lin&apos;s MI-Based Mea-
sure (Lin, 1998).
Further, there is no clear way of deciding
which is the best measure. Application-based
evaluation tasks have been proposed, yet it
is not clear (Weeds and Weir, 2003) whether
there is or should be one distributional similar-
ity measure which outperforms all other distri-
butional similarity measures on all tasks and for
all words.
We take a generic approach that does not di-
rectly reduce distributional similarity to a single
dimension. The way dimensions are combined
together will depend on parameters tuned to the
demands of a given application. Further, differ-
ent parameter settings will approximate differ-
ent existing similarity measures as well as many
more which have, until now, been unexplored.
The contributions of this paper are four-fold.
First, we propose a general framework for distri-
butional similarity based on the concepts of pre-
cision and recall (Section 2). Second, we evalu-
ate the framework at its optimal parameter set-
tings for two different applications (Section 3),
showing that it outperforms existing state-of-
the-art similarity measures for both high and
low frequency nouns. Third, we begin to inves-
tigate to what extent existing similarity mea-
sures might be characterised in terms of param-
eter settings within the framework (Section 4).
Fourth, we provide an understanding of why
a single existing measure cannot achieve opti-
mal results in every application of distributional
similarity measures.
</bodyText>
<sectionHeader confidence="0.989899" genericHeader="method">
2 The Framework
</sectionHeader>
<bodyText confidence="0.999887">
In this section, we introduce the relevance of
the Information Retrieval (IR) concepts of pre-
cision and recall in the context of word similar-
ity. We provide combinatorial, probabilistic and
mutual-information based models for precision
and recall and discuss combining precision and
recall to provide a single number in the context
of a particular application.
</bodyText>
<subsectionHeader confidence="0.9895">
2.1 Precision and Recall
</subsectionHeader>
<bodyText confidence="0.999992714285714">
The similarity&apos; of two nouns can be viewed as a
measure of how appropriate it is to use one noun
(or its distribution) in place of the other. If we
are using the distribution of one noun in place of
the distribution the other noun, we can consider
the precision and recall of the prediction made.
Precision tells us how much of what has been
predicted is correct whilst recall tells us how
much of what is required has been predicted.
In order to calculate precision and recall, we
first need to consider for each noun n which verb
co-occurrences will be predicted by it and, con-
versely, required in a description of it. We will
refer to these verbs as the features of n, F (n):
</bodyText>
<equation confidence="0.997242">
F(n) = {v : D(n,v) &gt; 0}
</equation>
<bodyText confidence="0.999938125">
where D(n,v), is the degree of association be-
tween noun n and verb v. Possible association
functions will be defined in the context of each
model described below.
If we are considering the ability of noun A
to predict noun B then it follows that the set
of True Positives is TP = F (A) n F(B) and
precision and recall can be defined as:
</bodyText>
<equation confidence="0.7484065">
P(A,B)
TZ(A, B)
</equation>
<footnote confidence="0.9845795">
1-We will consider, for simplicity, similarity between
nouns based on the the verbs they co-occur with in the
direct object relation but, of course, it would be possible
to consider other parts of speech and other relations.
</footnote>
<bodyText confidence="0.999493857142857">
Precision and recall both lie in the range [0,1]
and are both equal to one when each noun has
exactly the same features. It should also be
noted that RA, B) = P(B, A).
We will now consider some different possibil-
ities for measuring the degree of association be-
tween a noun n and a verb v.
</bodyText>
<subsectionHeader confidence="0.983954">
2.2 Combinatorial Model
</subsectionHeader>
<bodyText confidence="0.992465666666667">
In the combinatorial model, we simply consider
whether a verb has ever been seen to co-occur
with the noun. In other words, the degree of
association (D) between a noun n and a verb
v is 1 if they have co-occurred together and 0
otherwise.
</bodyText>
<equation confidence="0.7764145">
f 1 if P(v In) &gt;0
D(n,v) = 0 otherwise
</equation>
<bodyText confidence="0.990586">
In this case, it should be noted that the defi-
nitions of precision and recall can be simplified
as follows:
</bodyText>
<subsectionHeader confidence="0.988263">
2.3 Probabilistic Model
</subsectionHeader>
<bodyText confidence="0.9998574">
In the probabilistic model, more probable (or
more frequent) co-occurrences are considered
more significant. The degree of association be-
tween a noun n and verb v is defined in the
probabilistic model as:
</bodyText>
<equation confidence="0.984057">
Dp(n,v) = P(vin)
</equation>
<bodyText confidence="0.9999604">
The definitions for feature set membership,
TP, precision and recall all remain the same ex-
cept for the use of the new association function.
Using the probabilistic model, the precision
of A&apos;s prediction of B is the probability that a
verb picked at random from those co-occurring
with A will also co-occur with B; and the recall
of A&apos;s prediction of B is the probability that
a verb picked at random from those those co-
occurring with B will also co-occur with A.
</bodyText>
<subsectionHeader confidence="0.532805">
Mutual Information Based Model
</subsectionHeader>
<bodyText confidence="0.999038">
Mutual information (MI) allows us to capture
the idea that a co-occurrence of low probability
</bodyText>
<figure confidence="0.91946325">
D (A, v)
EF(A) D (A, V)
Erp D(B ,v)
EF(B) D(B ,v)
ET P D CO, V) IT PI
IF(A)1
R(A, B) = ETp De(B, v) ITPI
e
IF (B)i
Pe(A, B) =
,i3 -y Special Case
- 1 harmonic mean
- 0 weighted arithmetic mean
1 0 precision
0 0 recall
0.5 0 unweighted arithmetic mean
</figure>
<bodyText confidence="0.999349285714286">
events is more informative than a co-occurrence
of high probability events.
In this model, as before, we retain the defi-
nitions for feature set membership, TP, preci-
sion and recall but again change the association
function. Here, the degree of association be-
tween a noun n and a verb v is their MI.
</bodyText>
<tableCaption confidence="0.988573">
Table 1: Table of Special Values of and -y
</tableCaption>
<equation confidence="0.9854215">
P(v,n)
Dmj(n, v) = (n,v) = log P (v)P (n)
</equation>
<bodyText confidence="0.99908675">
Accordingly, verb v will be considered to be a
feature of noun n if the probability of their co-
occurrence is greater than would be expected if
verbs and nouns occurred independently.
</bodyText>
<subsectionHeader confidence="0.999635">
2.4 Combining Precision and Recall
</subsectionHeader>
<bodyText confidence="0.9999949375">
Although we have defined a pair of numbers for
similarity, in applications it will still be neces-
sary to compute a single number in order to de-
termine neighbourhood or cluster membership.
There are two obvious ways to optimise a pair
of numbers such as precision and recall. The
first is to use an arithmetic mean, which opti-
mises the sum of the numbers, and the second is
to use a harmonic mean2, which optimises the
product of the numbers.
In an attempt to retain generality, we can al-
low both alternatives by computing an arith-
metic mean of the harmonic mean and the arith-
metic mean, noting that the relative importance
of each term in an arithmetic mean is controlled
by weights (which sum to 1):
</bodyText>
<equation confidence="0.9887828">
2.P(A,B).7?,(A,B)
mh (A, B) =
P(A,B) + 7?,(A,B))
ma(A, B) = )3.P(A, B) + (1 — )3).7?,(A, B)
sim(A, B) = -y.mh(A, B) + (1 — -y).ma(A, B)
</equation>
<bodyText confidence="0.9999357">
where both and -y lie in the range [0,1]. The
resulting similarity sim(A,B) will also lie in the
range [0,1] where 0 represents complete lack of
similarity and 1 represents equivalence. This
formula can be used in combination with any
of the models for precision and recall outlined
above. Further, the generality allows us to in-
vestigate empirically the relative significance of
the different terms and thus whether one (or
more) might be omitted in future work.
</bodyText>
<footnote confidence="0.9867325">
2This is the standard IR measure known as the F-
score or F-measure
</footnote>
<bodyText confidence="0.9922462">
Precision and recall can be computed once for
every pair of words whereas similarity is some-
thing which will be computed for a specific task
and will depend on the values of )3 and -y. Table
1 summarizes some special parameter settings.
</bodyText>
<sectionHeader confidence="0.998703" genericHeader="method">
3 Empirical Evaluation
</sectionHeader>
<bodyText confidence="0.999788045454545">
In this section, we evaluate the performance
of the framework, using the combinatorial and
MI-based models of precision and recall, at
two application based tasks against Lin&apos;s MI-
based Measure (simun) and the a-skew Diver-
gence Measure (simasd). The formulae for these
measures are given in Figure 1. For the a-
skew divergence measure we set a = 0.99 since
this most closely approximates the Kullback-
Leibler divergence measure. The two evaluation
tasks used pseudo-disambiguation and Word-
Net (Fellbaum, 1998) prediction are fairly
standard for distributional similarity measures.
However, in the future we wish to extend our
evaluation to other tasks such as malapropism
correction (Budanitsky and Hirst, 2001) and
PP-attachment ambiguity resolution (Resnik,
1993) and also to the probabilistic model.
Since we use the same data and methodology
as in earlier work, some detail is omitted in the
subsequent discussion but full details and ratio-
nale can be found in Weeds and Weir (2003).
</bodyText>
<subsectionHeader confidence="0.993712">
3.1 Pseudo-Disambiguation Task
</subsectionHeader>
<bodyText confidence="0.999981333333333">
Pseudo-disambiguation tasks (e.g. Lee, 1999)
have become a standard evaluation technique
and, in the current context, we may use a
word&apos;s neighbours to decide which of two co-
occurrences is the most likely.
Although pseudo-disambiguation itself is an
artificial task, it has relevance in at least two
real application areas. First, by replacing occur-
rences of a particular word in a test suite with
a pair or set of words from which a technique
must choose, we recreate a simplified version
of the word sense disambiguation task; that is,
</bodyText>
<equation confidence="0.996931125">
simun(ni, n2)
simasd (n2, ni)
simKL(q, r)
simpice(ni, n2)
sim,(ni, n2)
ET(ni)nT(n2)(/(ni, v) ± /(n2, v))
= , where T (n) = {v : _1 - (n, v &gt; 0}
2_, T(ni) -1-(nl, V) ± ET(n2) i(n2, v)
simi(L(gliax ± (1— ce).q) where q(v) = P(v Ini), r(v) = P(vin2)
= q(v) x log q(v)
r(v)
= 2.IF(ni) n F(n2)1
IF(ni)I + IF(n2) I where F(n) = {v : P(v In) &gt; 0}
(maxcesuper(consuper(c2) iog(p(cd) ± iog(P(c2)) )
2 log P(c)
rnax ciesyn(ni)Aczesyn(n2)
</equation>
<figureCaption confidence="0.998738">
Figure 1: Definitions for similarity measures used throughout this paper
</figureCaption>
<bodyText confidence="0.999781142857143">
choosing between a fixed number of homonyms
based on local context. The second is in lan-
guage modelling where we wish to estimate the
probability of co-occurrences of events but, due
to the sparse data problem, it is often the case
that a possible co-occurrence has not been seen
in the training data.
</bodyText>
<sectionHeader confidence="0.505089" genericHeader="method">
3.1.1 Methodology
</sectionHeader>
<bodyText confidence="0.999902291666667">
As is common in this field (e.g. Lee, 1999), we
study similarity between nouns based on their
co-occurrences with verbs in the direct object
relation. We study similarity between high and
low frequency nouns since we want to investi-
gate any associations between word frequency
and quality of neighbours found by the mea-
sures but it is impractical to evaluate a large
number of similarity measures over all nouns.
2,852,300 lemmatised (noun-verb) direct-
object pairs were extracted from the BNC us-
ing a shallow parser (Briscoe and Carroll, 1995;
Carroll and Briscoe, 1996). From those nouns
also occurring in WordNet, we selected the 1000
most frequent3 nouns and a set of 1000 low fre-
quency4 nouns.
For each noun, 80% of the available data
was randomly selected as training data and the
other 20% set aside as test data. Precision and
recall were computed for each pair of nouns us-
ing the combinatorial and MI models. This data
is then available to the application task which
will first have to compute the similarity for each
pair of nouns based on current parameter set-
</bodyText>
<footnote confidence="0.993735666666667">
3This corresponds to a frequency range of [576,20561].
4We used frequency ranks 3001 to 4000 which corre-
spond to a frequency range of [70,120].
</footnote>
<bodyText confidence="0.989798">
tings and select nearest neighbours accordingly.
We converted each noun-verb pair (n, vi) in
the set-aside test data into a noun-verb-verb
triple (n, vi, v2) where P(vi) is approximately
equal to P(v2) over all the training data and
(n, v2) has not been seen in the test or training
data. A high frequency noun test set and a low
frequency noun test set, each containing 10,000
test instances, were then constructed by select-
ing ten test instances for each noun in a two step
process of 1) whilst more than ten triples re-
mained, discarding duplicate triples and 2) ran-
domly selecting ten triples from those remaining
after step 1. Each set of test triples was split
into five disjoint subsets, containing two triples
for each noun, so that average performance and
standard error could be computed. Addition-
ally, three of the five subsets were used as a
development set to optimise parameters (k, ,i3
and -y) and the remaining two used as a test set
to find error rates at these optimal settings.
The task is then for the nearest neighbours of
noun n to decide which of (n, vi) and (n, v2) was
the original co-occurrence. Each of n&apos;s neigh-
bours, m, is given a vote which is equal to the
difference in frequencies of the co-occurrences
(m, vi) and (m, v2) and which it casts to the co-
occurrence in which it appears most frequently.
The votes for each co-occurrence are summed
over all of the k nearest neighbours of n and the
co-occurrence with the most votes wins. Perfor-
mance is measured as error rate.
# of ties ,
error = T-1(# of incorrect choices + )
2
where T is the number of test instances.
</bodyText>
<figure confidence="0.818681909090909">
0 0.2 0.4β
0.6 0.8 1
Optimal Error Rate
0.35
0.25
0.3
0.2
high freq, simc
low freq, simc
high freq, simmi
low freq, simmi
</figure>
<bodyText confidence="0.9998565">
is that the hyponymy relation in WordNet is a
gold standard for semantic similarity which is,
of course, not true. However, we believe that
a distributional similarity measure which more
closely predicts WordNet, is more likely to be a
good predictor of semantic similarity.
</bodyText>
<subsectionHeader confidence="0.930614">
3.2.1 Methodology
</subsectionHeader>
<bodyText confidence="0.99985682051282">
We will first explain the WordNet-based dis-
tance measure (Lin, 1997) and then explain how
we determine the similarity between neighbour
sets generated using different measures.
The similarity of two nouns in WordNet is de-
fined as the similarity of their maximally sim-
ilar senses. The commonality of two concepts
is defined as the maximally specific superclass
of those concepts. So, if syn(n) is the set of
senses of the noun n in WordNet, sup(c) is the
set of (possibly indirect) superclasses of concept
c in WordNet and P(c) is the probability that a
randomly selected noun refers to an instance of
c, then the similarity between ni and n2 can be
calculated using the formula for simwn in Figure
1.
The probabilities P(c) are estimated by the
frequencies of concepts in SemCor (Miller et
al., 1994), a sense-tagged subset of the Brown
corpus, noting that the occurrence of a concept
refers to instances of all the superclasses of that
concept (i.e. P(root of tree6) = 1).
The k nearest neighbours7 of each noun, com-
puted using each distributional similarity mea-
sure at each parameter setting, are then com-
pared with the k nearest neighbours of the noun
according to the WordNet based measure. In or-
der to compute the similarity of two neighbour
sets, we transform each neighbour set so that
each neighbour is given a rank score of k — rank.
We do not use the similarity scores directly since
these require normalization if different similar-
ity measures (using different scales) are to be
compared. Having performed this transforma-
tion, the neighbour sets for the same word w
may be represented by two ordered sets of words
[wk, w1] and [w, wl]. The similarity be-
tween such sets is computed using the same cal-
culation as used by Lin (1998) except for sim-
</bodyText>
<footnote confidence="0.99931725">
6The root of the WordNet hyponymy relation is taken
to be an imagined superclass of all concepts in WordNet.
7As in previous work (Lin, 1998; Weeds and Weir,
2003), we use k = 200.
</footnote>
<table confidence="0.999542222222222">
Measure Noun Frequency
high low
params sim params sim
sim e -y = 0.25 0.299 -y = 0.5 0.260
)3 = 0.5 )3 = 0.4
simmt 7 = 0.25 0.317 7 = 0.25 0.274
)3 = 0.3 )3 = 0.3
simun - 0.307 - 0.210
SiMasd - 0.290 - 0.270
</table>
<tableCaption confidence="0.991128333333333">
Table 3: Optimal Mean Similarities and Cor-
responding Parameter Settings Between The-
saurus Entries for WordNet Prediction Task
</tableCaption>
<bodyText confidence="0.930807">
plifications due to the use of ranks:
</bodyText>
<equation confidence="0.9913475">
E
EiiF i2
</equation>
<bodyText confidence="0.9979585">
where i and j are the rank scores of the words
within each neighbour set.
</bodyText>
<sectionHeader confidence="0.905839" genericHeader="evaluation">
3.2.2 Results
</sectionHeader>
<bodyText confidence="0.999803076923077">
Table 3 summarizes the optimal mean simi-
larities and parameter settings for the general
framework using both the combinatorial (sim)
and the MI-based (simmt) models. Results for
Lin&apos;s MI-based measure (simun) and the a-skew
divergence measure (simasd) are also given and
results are divided into those for high frequency
nouns and those for low frequency nouns. Stan-
dard errors in the optimal mean similarities are
not given but were of the order of 0.1.
Our first observation is that the general
framework using the MI-based model for pre-
cision and recall outperforms all of the other
distributional similarity measures.
We also observe that lower values of -y pro-
duce better results, particularly for low fre-
quency nouns. For example, when -y = 1, sim-
ilarity for low frequency nouns drops to 0.147
using the combinatorial model and 0.177 using
the MI-based model.
Third, from Figure 3, it appears that this
WordNet prediction task favours measures
which select high recall neighbours. Although
optimum similarity for the combinatorial model
occurs at ,8=0.5, similarity is always higher for
lower values of than for higher values of )3.
</bodyText>
<figure confidence="0.992381230769231">
0.35
Optimal Mean Similarity
0.3
0.25
0.2
.15
high freq, simc
low freq, simc
high freq, simmi
low freq, simmi
0.1
0 0.2 0.4 0.6 0.8 1
β
</figure>
<bodyText confidence="0.999473714285714">
ing the a-skew divergence measure and those
found using the MI-Based model. Optimal sim-
ilarity (0.760 and 0.725 respectively) was found
at -y = 0.0 and ,i3 = 0.0 for high frequency nouns
and at -y = 0.25 and )3 = 0.0 for low frequency
nouns. Further, similarity between the mea-
sures drops rapidly once ,i3 rises above 0.3.
</bodyText>
<sectionHeader confidence="0.990538" genericHeader="conclusions">
5 Conclusions and Further Work
</sectionHeader>
<bodyText confidence="0.99998409375">
Using the MI-based model for precision and re-
call and with a parameter setting of -y = 1.0,
the general framework for distributional similar-
ity proposed herein closely approximates Lin&apos;s
(1998) Measure. However, we have shown that
using a much lower value of -y so that the
combination of precision and recall is closer to
a weighted arithmetic mean than a harmonic
mean yields better results in the two applica-
tion tasks considered here. This is because the
relative importance of precision and recall can
be tuned to the task at hand.
Further, we have shown that pseudo-
disambiguation is a task which requires high
precision neighbours whereas WordNet predic-
tion is a task which requires high recall neigh-
bours. Accordingly, it is not clear how a sin-
gle (unparameterised) similarity measure could
give optimum results on both tasks.
In the future, we intend to extend the work
to the characterisation of other tasks and other
existing similarity measures. As well as their,
usually implicit, use of precision and recall, the
main difference between existing similarity mea-
sures will be the models in which precision and
recall are defined. We have explored two such
models here - a combinatorial model and a MI-
based model - and have shown that the MI-
based model achieves significantly improved re-
sults over the combinatorial model. We propose
to investigate other models such as the proba-
bilistic one given in Section 2.3.
</bodyText>
<sectionHeader confidence="0.999061" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999945">
We would like to thank John Carroll for the use
of his parser, Adam Kilgarriff and Bill Keller for
valuable discussions and the UK EPSRC for its
studentship to the first author.
</bodyText>
<sectionHeader confidence="0.997458" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996131052631579">
E. Briscoe and J. Carroll. 1995. Developing
and evaluating a probabilistic lr parser of
part-of-speech and punctuation labels. In 4th
ACL/SIGDAT International Workshop on Pars-
ing Technologies, pages 48-58.
A. Budanitsky and G. Hirst. 2001. Semantic dis-
tance in WordNet: An experimental, application-
oriented evaluation of five measures. In NAACL-
01.
J. Carroll and E. Briscoe. 1996. Apportioning devel-
opment effort in a probabilistic lr parsing system
through evaluation. In ACL/SIGDAT Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 92-100.
T.M. Cover and J.A. Thomas. 1991. Elements of
Information Theory. Wiley, New York.
U. Essen and V. Steinbiss. 1992. Cooccurrence
smoothing for stochastic language modelling.
ICA SSP 92, 1:161-164.
C. Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
W.B. Frakes and R. Baeza-Yates, editors. 1992. In-
formation Retrieval, Data Structures and Algo-
rithms. Prentice Hall.
D. Hindle. 1990. Noun classification from predicate-
argument structures. In ACL-90, pages 268-275.
L. Lee. 1999. Measures of distributional similarity.
In ACL-99.
B. Levin. 1993. Towards a Lexical Organization of
English Verbs. Chicago University Press.
D. Lin. 1997. Using syntactic dependency as local
context to resolve word sense ambiguity. In Pro-
ceedings of ACL/EACL-97, pages 64-71.
D. Lin. 1998. Automatic retrieval and clustering of
similar words. In COLING-ACL &apos;98.
G. Miller, M. Chodorow, S. Landes, C. Leacock, and
R. Thomas. 1994. Using a semantic concordance
for sense identification. In ARPA Human Lan-
guage Technology Workshop.
F. Pereira, N. Tishby, and L. Lee. 1993. Distribu-
tional clustering of similar words. In ACL&apos;93.
C. Radhakrishna Rao. 1983. Diversity: Its measure-
ment, decomposition, apportionment and analy-
sis. Sankyha: The Indian Journal of Statistics,
44(A):1-22.
P. Resnik. 1993. Selection and Information: A
Class-Based Approach to Lexical Relationships.
Ph.D. thesis, University of Pennsylvania.
G. Salton and M.J. McGill. 1983. Introduction to
Modern Information Retrieval. McGraw-Hill.
H. Silber and K. McCoy. 2002. Efficiently com-
puted lexical chains as an intermediate represen-
tation for automatic text summarization. Com-
putational Linguistics, 28(4).
J. Weeds and D. Weir. 2003. Finding and evaluating
sets of nearest neighbours. In Proceedings of 2nd
Conference on Corpus Linguistics.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.942711">
<title confidence="0.994363">A General Framework for Distributional Similarity</title>
<author confidence="0.994595">Julie Weeds</author>
<author confidence="0.994595">David</author>
<affiliation confidence="0.999522">School of Cognitive and Computing University of</affiliation>
<address confidence="0.970917">Brighton, BN1 9QH,</address>
<email confidence="0.996225">julieweAcogs.susx.ac.uk</email>
<email confidence="0.996225">davidwAcogs.susx.ac.uk</email>
<abstract confidence="0.998851384615384">We present a general framework for distributional similarity based on the concepts of precision and recall. Different parameter settings within this framework approximate different existing similarity measures as well as many more which have, until now, been unexplored. We show that optimal parameter settings outperform two existing state-of-the-art similarity measures on two evaluation tasks for high and low frequency nouns.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Briscoe</author>
<author>J Carroll</author>
</authors>
<title>Developing and evaluating a probabilistic lr parser of part-of-speech and punctuation labels.</title>
<date>1995</date>
<booktitle>In 4th ACL/SIGDAT International Workshop on Parsing Technologies,</booktitle>
<pages>48--58</pages>
<contexts>
<context position="12347" citStr="Briscoe and Carroll, 1995" startWordPosition="2073" endWordPosition="2076">a possible co-occurrence has not been seen in the training data. 3.1.1 Methodology As is common in this field (e.g. Lee, 1999), we study similarity between nouns based on their co-occurrences with verbs in the direct object relation. We study similarity between high and low frequency nouns since we want to investigate any associations between word frequency and quality of neighbours found by the measures but it is impractical to evaluate a large number of similarity measures over all nouns. 2,852,300 lemmatised (noun-verb) directobject pairs were extracted from the BNC using a shallow parser (Briscoe and Carroll, 1995; Carroll and Briscoe, 1996). From those nouns also occurring in WordNet, we selected the 1000 most frequent3 nouns and a set of 1000 low frequency4 nouns. For each noun, 80% of the available data was randomly selected as training data and the other 20% set aside as test data. Precision and recall were computed for each pair of nouns using the combinatorial and MI models. This data is then available to the application task which will first have to compute the similarity for each pair of nouns based on current parameter set3This corresponds to a frequency range of [576,20561]. 4We used frequenc</context>
</contexts>
<marker>Briscoe, Carroll, 1995</marker>
<rawString>E. Briscoe and J. Carroll. 1995. Developing and evaluating a probabilistic lr parser of part-of-speech and punctuation labels. In 4th ACL/SIGDAT International Workshop on Parsing Technologies, pages 48-58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Budanitsky</author>
<author>G Hirst</author>
</authors>
<title>Semantic distance in WordNet: An experimental, applicationoriented evaluation of five measures.</title>
<date>2001</date>
<booktitle>In NAACL01.</booktitle>
<contexts>
<context position="1559" citStr="Budanitsky and Hirst, 2001" startWordPosition="227" endWordPosition="231">often not available due to the sparse data problem. In order to overcome this problem, researchers (e.g. Pereira et al. (1993)) have proposed estimating probabilities based on sets of words which are known to be distributionally similar. In the semantic domain, the hypothesis that words which mean similar things behave in similar ways (Levin, 1993), has led researchers (e.g. Lin (1998)) to propose that distributional similarity might be used as a predictor of semantic similarity. Accordingly, we might automatically build thesauruses which could be used in tasks such as malapropism correction (Budanitsky and Hirst, 2001) and text summarization (Silber and McCoy, 2002). However, the loose definition of distributional similarity that two words are distributionally similar if they appear in similar contexts has led to many distributional similarity measures being proposed; for example, the L1 Norm, the Euclidean Distance, the Cosine Metric (Salton and McGill, 1983), Jaccard&apos;s Coefficient (Frakes and Baeza-Yates, 1992), the Dice Coefficient (Frakes and Baeza-Yates, 1992), the KullbackLeibler Divergence (Cover and Thomas, 1991), the Jenson-Shannon Divergence (Rao, 1983), the a-skew Divergence (Lee, 1999), the Conf</context>
<context position="10138" citStr="Budanitsky and Hirst, 2001" startWordPosition="1698" endWordPosition="1701">orial and MI-based models of precision and recall, at two application based tasks against Lin&apos;s MIbased Measure (simun) and the a-skew Divergence Measure (simasd). The formulae for these measures are given in Figure 1. For the askew divergence measure we set a = 0.99 since this most closely approximates the KullbackLeibler divergence measure. The two evaluation tasks used pseudo-disambiguation and WordNet (Fellbaum, 1998) prediction are fairly standard for distributional similarity measures. However, in the future we wish to extend our evaluation to other tasks such as malapropism correction (Budanitsky and Hirst, 2001) and PP-attachment ambiguity resolution (Resnik, 1993) and also to the probabilistic model. Since we use the same data and methodology as in earlier work, some detail is omitted in the subsequent discussion but full details and rationale can be found in Weeds and Weir (2003). 3.1 Pseudo-Disambiguation Task Pseudo-disambiguation tasks (e.g. Lee, 1999) have become a standard evaluation technique and, in the current context, we may use a word&apos;s neighbours to decide which of two cooccurrences is the most likely. Although pseudo-disambiguation itself is an artificial task, it has relevance in at le</context>
</contexts>
<marker>Budanitsky, Hirst, 2001</marker>
<rawString>A. Budanitsky and G. Hirst. 2001. Semantic distance in WordNet: An experimental, applicationoriented evaluation of five measures. In NAACL01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
<author>E Briscoe</author>
</authors>
<title>Apportioning development effort in a probabilistic lr parsing system through evaluation.</title>
<date>1996</date>
<booktitle>In ACL/SIGDAT Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>92--100</pages>
<contexts>
<context position="12375" citStr="Carroll and Briscoe, 1996" startWordPosition="2077" endWordPosition="2080">s not been seen in the training data. 3.1.1 Methodology As is common in this field (e.g. Lee, 1999), we study similarity between nouns based on their co-occurrences with verbs in the direct object relation. We study similarity between high and low frequency nouns since we want to investigate any associations between word frequency and quality of neighbours found by the measures but it is impractical to evaluate a large number of similarity measures over all nouns. 2,852,300 lemmatised (noun-verb) directobject pairs were extracted from the BNC using a shallow parser (Briscoe and Carroll, 1995; Carroll and Briscoe, 1996). From those nouns also occurring in WordNet, we selected the 1000 most frequent3 nouns and a set of 1000 low frequency4 nouns. For each noun, 80% of the available data was randomly selected as training data and the other 20% set aside as test data. Precision and recall were computed for each pair of nouns using the combinatorial and MI models. This data is then available to the application task which will first have to compute the similarity for each pair of nouns based on current parameter set3This corresponds to a frequency range of [576,20561]. 4We used frequency ranks 3001 to 4000 which c</context>
</contexts>
<marker>Carroll, Briscoe, 1996</marker>
<rawString>J. Carroll and E. Briscoe. 1996. Apportioning development effort in a probabilistic lr parsing system through evaluation. In ACL/SIGDAT Conference on Empirical Methods in Natural Language Processing, pages 92-100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T M Cover</author>
<author>J A Thomas</author>
</authors>
<title>Elements of Information Theory.</title>
<date>1991</date>
<publisher>Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="2071" citStr="Cover and Thomas, 1991" startWordPosition="299" endWordPosition="302">ally build thesauruses which could be used in tasks such as malapropism correction (Budanitsky and Hirst, 2001) and text summarization (Silber and McCoy, 2002). However, the loose definition of distributional similarity that two words are distributionally similar if they appear in similar contexts has led to many distributional similarity measures being proposed; for example, the L1 Norm, the Euclidean Distance, the Cosine Metric (Salton and McGill, 1983), Jaccard&apos;s Coefficient (Frakes and Baeza-Yates, 1992), the Dice Coefficient (Frakes and Baeza-Yates, 1992), the KullbackLeibler Divergence (Cover and Thomas, 1991), the Jenson-Shannon Divergence (Rao, 1983), the a-skew Divergence (Lee, 1999), the Confusion Probability (Essen and Steinbiss, 1992), Hindle&apos;s Mutual Information(MI)-Based Measure (Hindle, 1990) and Lin&apos;s MI-Based Measure (Lin, 1998). Further, there is no clear way of deciding which is the best measure. Application-based evaluation tasks have been proposed, yet it is not clear (Weeds and Weir, 2003) whether there is or should be one distributional similarity measure which outperforms all other distributional similarity measures on all tasks and for all words. We take a generic approach that d</context>
</contexts>
<marker>Cover, Thomas, 1991</marker>
<rawString>T.M. Cover and J.A. Thomas. 1991. Elements of Information Theory. Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Essen</author>
<author>V Steinbiss</author>
</authors>
<title>Cooccurrence smoothing for stochastic language modelling.</title>
<date>1992</date>
<journal>ICA SSP</journal>
<volume>92</volume>
<pages>1--161</pages>
<contexts>
<context position="2204" citStr="Essen and Steinbiss, 1992" startWordPosition="317" endWordPosition="320">tion (Silber and McCoy, 2002). However, the loose definition of distributional similarity that two words are distributionally similar if they appear in similar contexts has led to many distributional similarity measures being proposed; for example, the L1 Norm, the Euclidean Distance, the Cosine Metric (Salton and McGill, 1983), Jaccard&apos;s Coefficient (Frakes and Baeza-Yates, 1992), the Dice Coefficient (Frakes and Baeza-Yates, 1992), the KullbackLeibler Divergence (Cover and Thomas, 1991), the Jenson-Shannon Divergence (Rao, 1983), the a-skew Divergence (Lee, 1999), the Confusion Probability (Essen and Steinbiss, 1992), Hindle&apos;s Mutual Information(MI)-Based Measure (Hindle, 1990) and Lin&apos;s MI-Based Measure (Lin, 1998). Further, there is no clear way of deciding which is the best measure. Application-based evaluation tasks have been proposed, yet it is not clear (Weeds and Weir, 2003) whether there is or should be one distributional similarity measure which outperforms all other distributional similarity measures on all tasks and for all words. We take a generic approach that does not directly reduce distributional similarity to a single dimension. The way dimensions are combined together will depend on para</context>
</contexts>
<marker>Essen, Steinbiss, 1992</marker>
<rawString>U. Essen and V. Steinbiss. 1992. Cooccurrence smoothing for stochastic language modelling. ICA SSP 92, 1:161-164.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>C. Fellbaum, editor.</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1320" citStr="(1998)" startWordPosition="193" endWordPosition="193">ly similar words. In the syntactic domain, language models, which can be used to evaluate alternative interpretations of text and speech, require probabilistic information about words and their co-occurrences which is often not available due to the sparse data problem. In order to overcome this problem, researchers (e.g. Pereira et al. (1993)) have proposed estimating probabilities based on sets of words which are known to be distributionally similar. In the semantic domain, the hypothesis that words which mean similar things behave in similar ways (Levin, 1993), has led researchers (e.g. Lin (1998)) to propose that distributional similarity might be used as a predictor of semantic similarity. Accordingly, we might automatically build thesauruses which could be used in tasks such as malapropism correction (Budanitsky and Hirst, 2001) and text summarization (Silber and McCoy, 2002). However, the loose definition of distributional similarity that two words are distributionally similar if they appear in similar contexts has led to many distributional similarity measures being proposed; for example, the L1 Norm, the Euclidean Distance, the Cosine Metric (Salton and McGill, 1983), Jaccard&apos;s C</context>
<context position="16771" citStr="(1998)" startWordPosition="2857" endWordPosition="2857">ighbours of the noun according to the WordNet based measure. In order to compute the similarity of two neighbour sets, we transform each neighbour set so that each neighbour is given a rank score of k — rank. We do not use the similarity scores directly since these require normalization if different similarity measures (using different scales) are to be compared. Having performed this transformation, the neighbour sets for the same word w may be represented by two ordered sets of words [wk, w1] and [w, wl]. The similarity between such sets is computed using the same calculation as used by Lin (1998) except for sim6The root of the WordNet hyponymy relation is taken to be an imagined superclass of all concepts in WordNet. 7As in previous work (Lin, 1998; Weeds and Weir, 2003), we use k = 200. Measure Noun Frequency high low params sim params sim sim e -y = 0.25 0.299 -y = 0.5 0.260 )3 = 0.5 )3 = 0.4 simmt 7 = 0.25 0.317 7 = 0.25 0.274 )3 = 0.3 )3 = 0.3 simun - 0.307 - 0.210 SiMasd - 0.290 - 0.270 Table 3: Optimal Mean Similarities and Corresponding Parameter Settings Between Thesaurus Entries for WordNet Prediction Task plifications due to the use of ranks: E EiiF i2 where i and j are the </context>
<context position="19259" citStr="(1998)" startWordPosition="3296" endWordPosition="3296">high freq, simmi low freq, simmi 0.1 0 0.2 0.4 0.6 0.8 1 β ing the a-skew divergence measure and those found using the MI-Based model. Optimal similarity (0.760 and 0.725 respectively) was found at -y = 0.0 and ,i3 = 0.0 for high frequency nouns and at -y = 0.25 and )3 = 0.0 for low frequency nouns. Further, similarity between the measures drops rapidly once ,i3 rises above 0.3. 5 Conclusions and Further Work Using the MI-based model for precision and recall and with a parameter setting of -y = 1.0, the general framework for distributional similarity proposed herein closely approximates Lin&apos;s (1998) Measure. However, we have shown that using a much lower value of -y so that the combination of precision and recall is closer to a weighted arithmetic mean than a harmonic mean yields better results in the two application tasks considered here. This is because the relative importance of precision and recall can be tuned to the task at hand. Further, we have shown that pseudodisambiguation is a task which requires high precision neighbours whereas WordNet prediction is a task which requires high recall neighbours. Accordingly, it is not clear how a single (unparameterised) similarity measure c</context>
</contexts>
<marker>1998</marker>
<rawString>C. Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<title>Information Retrieval, Data Structures and Algorithms.</title>
<date>1992</date>
<editor>W.B. Frakes and R. Baeza-Yates, editors.</editor>
<publisher>Prentice Hall.</publisher>
<marker>1992</marker>
<rawString>W.B. Frakes and R. Baeza-Yates, editors. 1992. Information Retrieval, Data Structures and Algorithms. Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hindle</author>
</authors>
<title>Noun classification from predicateargument structures.</title>
<date>1990</date>
<booktitle>In ACL-90,</booktitle>
<pages>268--275</pages>
<contexts>
<context position="2266" citStr="Hindle, 1990" startWordPosition="326" endWordPosition="327">nal similarity that two words are distributionally similar if they appear in similar contexts has led to many distributional similarity measures being proposed; for example, the L1 Norm, the Euclidean Distance, the Cosine Metric (Salton and McGill, 1983), Jaccard&apos;s Coefficient (Frakes and Baeza-Yates, 1992), the Dice Coefficient (Frakes and Baeza-Yates, 1992), the KullbackLeibler Divergence (Cover and Thomas, 1991), the Jenson-Shannon Divergence (Rao, 1983), the a-skew Divergence (Lee, 1999), the Confusion Probability (Essen and Steinbiss, 1992), Hindle&apos;s Mutual Information(MI)-Based Measure (Hindle, 1990) and Lin&apos;s MI-Based Measure (Lin, 1998). Further, there is no clear way of deciding which is the best measure. Application-based evaluation tasks have been proposed, yet it is not clear (Weeds and Weir, 2003) whether there is or should be one distributional similarity measure which outperforms all other distributional similarity measures on all tasks and for all words. We take a generic approach that does not directly reduce distributional similarity to a single dimension. The way dimensions are combined together will depend on parameters tuned to the demands of a given application. Further, d</context>
</contexts>
<marker>Hindle, 1990</marker>
<rawString>D. Hindle. 1990. Noun classification from predicateargument structures. In ACL-90, pages 268-275. L. Lee. 1999. Measures of distributional similarity. In ACL-99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Levin</author>
</authors>
<title>Towards a Lexical Organization of English Verbs.</title>
<date>1993</date>
<publisher>Chicago University Press.</publisher>
<contexts>
<context position="1282" citStr="Levin, 1993" startWordPosition="186" endWordPosition="187">ntial applications of sets of distributionally similar words. In the syntactic domain, language models, which can be used to evaluate alternative interpretations of text and speech, require probabilistic information about words and their co-occurrences which is often not available due to the sparse data problem. In order to overcome this problem, researchers (e.g. Pereira et al. (1993)) have proposed estimating probabilities based on sets of words which are known to be distributionally similar. In the semantic domain, the hypothesis that words which mean similar things behave in similar ways (Levin, 1993), has led researchers (e.g. Lin (1998)) to propose that distributional similarity might be used as a predictor of semantic similarity. Accordingly, we might automatically build thesauruses which could be used in tasks such as malapropism correction (Budanitsky and Hirst, 2001) and text summarization (Silber and McCoy, 2002). However, the loose definition of distributional similarity that two words are distributionally similar if they appear in similar contexts has led to many distributional similarity measures being proposed; for example, the L1 Norm, the Euclidean Distance, the Cosine Metric </context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>B. Levin. 1993. Towards a Lexical Organization of English Verbs. Chicago University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Using syntactic dependency as local context to resolve word sense ambiguity.</title>
<date>1997</date>
<booktitle>In Proceedings of ACL/EACL-97,</booktitle>
<pages>64--71</pages>
<contexts>
<context position="15099" citStr="Lin, 1997" startWordPosition="2565" endWordPosition="2566">mance is measured as error rate. # of ties , error = T-1(# of incorrect choices + ) 2 where T is the number of test instances. 0 0.2 0.4β 0.6 0.8 1 Optimal Error Rate 0.35 0.25 0.3 0.2 high freq, simc low freq, simc high freq, simmi low freq, simmi is that the hyponymy relation in WordNet is a gold standard for semantic similarity which is, of course, not true. However, we believe that a distributional similarity measure which more closely predicts WordNet, is more likely to be a good predictor of semantic similarity. 3.2.1 Methodology We will first explain the WordNet-based distance measure (Lin, 1997) and then explain how we determine the similarity between neighbour sets generated using different measures. The similarity of two nouns in WordNet is defined as the similarity of their maximally similar senses. The commonality of two concepts is defined as the maximally specific superclass of those concepts. So, if syn(n) is the set of senses of the noun n in WordNet, sup(c) is the set of (possibly indirect) superclasses of concept c in WordNet and P(c) is the probability that a randomly selected noun refers to an instance of c, then the similarity between ni and n2 can be calculated using th</context>
</contexts>
<marker>Lin, 1997</marker>
<rawString>D. Lin. 1997. Using syntactic dependency as local context to resolve word sense ambiguity. In Proceedings of ACL/EACL-97, pages 64-71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In COLING-ACL &apos;98.</booktitle>
<contexts>
<context position="1320" citStr="Lin (1998)" startWordPosition="192" endWordPosition="193">onally similar words. In the syntactic domain, language models, which can be used to evaluate alternative interpretations of text and speech, require probabilistic information about words and their co-occurrences which is often not available due to the sparse data problem. In order to overcome this problem, researchers (e.g. Pereira et al. (1993)) have proposed estimating probabilities based on sets of words which are known to be distributionally similar. In the semantic domain, the hypothesis that words which mean similar things behave in similar ways (Levin, 1993), has led researchers (e.g. Lin (1998)) to propose that distributional similarity might be used as a predictor of semantic similarity. Accordingly, we might automatically build thesauruses which could be used in tasks such as malapropism correction (Budanitsky and Hirst, 2001) and text summarization (Silber and McCoy, 2002). However, the loose definition of distributional similarity that two words are distributionally similar if they appear in similar contexts has led to many distributional similarity measures being proposed; for example, the L1 Norm, the Euclidean Distance, the Cosine Metric (Salton and McGill, 1983), Jaccard&apos;s C</context>
<context position="16771" citStr="Lin (1998)" startWordPosition="2856" endWordPosition="2857">t neighbours of the noun according to the WordNet based measure. In order to compute the similarity of two neighbour sets, we transform each neighbour set so that each neighbour is given a rank score of k — rank. We do not use the similarity scores directly since these require normalization if different similarity measures (using different scales) are to be compared. Having performed this transformation, the neighbour sets for the same word w may be represented by two ordered sets of words [wk, w1] and [w, wl]. The similarity between such sets is computed using the same calculation as used by Lin (1998) except for sim6The root of the WordNet hyponymy relation is taken to be an imagined superclass of all concepts in WordNet. 7As in previous work (Lin, 1998; Weeds and Weir, 2003), we use k = 200. Measure Noun Frequency high low params sim params sim sim e -y = 0.25 0.299 -y = 0.5 0.260 )3 = 0.5 )3 = 0.4 simmt 7 = 0.25 0.317 7 = 0.25 0.274 )3 = 0.3 )3 = 0.3 simun - 0.307 - 0.210 SiMasd - 0.290 - 0.270 Table 3: Optimal Mean Similarities and Corresponding Parameter Settings Between Thesaurus Entries for WordNet Prediction Task plifications due to the use of ranks: E EiiF i2 where i and j are the </context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998. Automatic retrieval and clustering of similar words. In COLING-ACL &apos;98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
<author>M Chodorow</author>
<author>S Landes</author>
<author>C Leacock</author>
<author>R Thomas</author>
</authors>
<title>Using a semantic concordance for sense identification.</title>
<date>1994</date>
<booktitle>In ARPA Human Language Technology Workshop.</booktitle>
<contexts>
<context position="15831" citStr="Miller et al., 1994" startWordPosition="2689" endWordPosition="2692"> similarity of two nouns in WordNet is defined as the similarity of their maximally similar senses. The commonality of two concepts is defined as the maximally specific superclass of those concepts. So, if syn(n) is the set of senses of the noun n in WordNet, sup(c) is the set of (possibly indirect) superclasses of concept c in WordNet and P(c) is the probability that a randomly selected noun refers to an instance of c, then the similarity between ni and n2 can be calculated using the formula for simwn in Figure 1. The probabilities P(c) are estimated by the frequencies of concepts in SemCor (Miller et al., 1994), a sense-tagged subset of the Brown corpus, noting that the occurrence of a concept refers to instances of all the superclasses of that concept (i.e. P(root of tree6) = 1). The k nearest neighbours7 of each noun, computed using each distributional similarity measure at each parameter setting, are then compared with the k nearest neighbours of the noun according to the WordNet based measure. In order to compute the similarity of two neighbour sets, we transform each neighbour set so that each neighbour is given a rank score of k — rank. We do not use the similarity scores directly since these </context>
</contexts>
<marker>Miller, Chodorow, Landes, Leacock, Thomas, 1994</marker>
<rawString>G. Miller, M. Chodorow, S. Landes, C. Leacock, and R. Thomas. 1994. Using a semantic concordance for sense identification. In ARPA Human Language Technology Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
<author>N Tishby</author>
<author>L Lee</author>
</authors>
<title>Distributional clustering of similar words.</title>
<date>1993</date>
<booktitle>In ACL&apos;93.</booktitle>
<contexts>
<context position="1058" citStr="Pereira et al. (1993)" startWordPosition="148" endWordPosition="151">which have, until now, been unexplored. We show that optimal parameter settings outperform two existing state-of-the-art similarity measures on two evaluation tasks for high and low frequency nouns. 1 Introduction There are many potential applications of sets of distributionally similar words. In the syntactic domain, language models, which can be used to evaluate alternative interpretations of text and speech, require probabilistic information about words and their co-occurrences which is often not available due to the sparse data problem. In order to overcome this problem, researchers (e.g. Pereira et al. (1993)) have proposed estimating probabilities based on sets of words which are known to be distributionally similar. In the semantic domain, the hypothesis that words which mean similar things behave in similar ways (Levin, 1993), has led researchers (e.g. Lin (1998)) to propose that distributional similarity might be used as a predictor of semantic similarity. Accordingly, we might automatically build thesauruses which could be used in tasks such as malapropism correction (Budanitsky and Hirst, 2001) and text summarization (Silber and McCoy, 2002). However, the loose definition of distributional s</context>
</contexts>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>F. Pereira, N. Tishby, and L. Lee. 1993. Distributional clustering of similar words. In ACL&apos;93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Radhakrishna Rao</author>
</authors>
<title>Diversity: Its measurement, decomposition, apportionment and analysis.</title>
<date>1983</date>
<journal>Sankyha: The Indian Journal of Statistics,</journal>
<pages>44--1</pages>
<contexts>
<context position="2114" citStr="Rao, 1983" startWordPosition="306" endWordPosition="307"> as malapropism correction (Budanitsky and Hirst, 2001) and text summarization (Silber and McCoy, 2002). However, the loose definition of distributional similarity that two words are distributionally similar if they appear in similar contexts has led to many distributional similarity measures being proposed; for example, the L1 Norm, the Euclidean Distance, the Cosine Metric (Salton and McGill, 1983), Jaccard&apos;s Coefficient (Frakes and Baeza-Yates, 1992), the Dice Coefficient (Frakes and Baeza-Yates, 1992), the KullbackLeibler Divergence (Cover and Thomas, 1991), the Jenson-Shannon Divergence (Rao, 1983), the a-skew Divergence (Lee, 1999), the Confusion Probability (Essen and Steinbiss, 1992), Hindle&apos;s Mutual Information(MI)-Based Measure (Hindle, 1990) and Lin&apos;s MI-Based Measure (Lin, 1998). Further, there is no clear way of deciding which is the best measure. Application-based evaluation tasks have been proposed, yet it is not clear (Weeds and Weir, 2003) whether there is or should be one distributional similarity measure which outperforms all other distributional similarity measures on all tasks and for all words. We take a generic approach that does not directly reduce distributional simi</context>
</contexts>
<marker>Rao, 1983</marker>
<rawString>C. Radhakrishna Rao. 1983. Diversity: Its measurement, decomposition, apportionment and analysis. Sankyha: The Indian Journal of Statistics, 44(A):1-22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Selection and Information: A Class-Based Approach to Lexical Relationships.</title>
<date>1993</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="10192" citStr="Resnik, 1993" startWordPosition="1706" endWordPosition="1707">n based tasks against Lin&apos;s MIbased Measure (simun) and the a-skew Divergence Measure (simasd). The formulae for these measures are given in Figure 1. For the askew divergence measure we set a = 0.99 since this most closely approximates the KullbackLeibler divergence measure. The two evaluation tasks used pseudo-disambiguation and WordNet (Fellbaum, 1998) prediction are fairly standard for distributional similarity measures. However, in the future we wish to extend our evaluation to other tasks such as malapropism correction (Budanitsky and Hirst, 2001) and PP-attachment ambiguity resolution (Resnik, 1993) and also to the probabilistic model. Since we use the same data and methodology as in earlier work, some detail is omitted in the subsequent discussion but full details and rationale can be found in Weeds and Weir (2003). 3.1 Pseudo-Disambiguation Task Pseudo-disambiguation tasks (e.g. Lee, 1999) have become a standard evaluation technique and, in the current context, we may use a word&apos;s neighbours to decide which of two cooccurrences is the most likely. Although pseudo-disambiguation itself is an artificial task, it has relevance in at least two real application areas. First, by replacing oc</context>
</contexts>
<marker>Resnik, 1993</marker>
<rawString>P. Resnik. 1993. Selection and Information: A Class-Based Approach to Lexical Relationships. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>M J McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval.</title>
<date>1983</date>
<publisher>McGraw-Hill.</publisher>
<contexts>
<context position="1907" citStr="Salton and McGill, 1983" startWordPosition="278" endWordPosition="281"> has led researchers (e.g. Lin (1998)) to propose that distributional similarity might be used as a predictor of semantic similarity. Accordingly, we might automatically build thesauruses which could be used in tasks such as malapropism correction (Budanitsky and Hirst, 2001) and text summarization (Silber and McCoy, 2002). However, the loose definition of distributional similarity that two words are distributionally similar if they appear in similar contexts has led to many distributional similarity measures being proposed; for example, the L1 Norm, the Euclidean Distance, the Cosine Metric (Salton and McGill, 1983), Jaccard&apos;s Coefficient (Frakes and Baeza-Yates, 1992), the Dice Coefficient (Frakes and Baeza-Yates, 1992), the KullbackLeibler Divergence (Cover and Thomas, 1991), the Jenson-Shannon Divergence (Rao, 1983), the a-skew Divergence (Lee, 1999), the Confusion Probability (Essen and Steinbiss, 1992), Hindle&apos;s Mutual Information(MI)-Based Measure (Hindle, 1990) and Lin&apos;s MI-Based Measure (Lin, 1998). Further, there is no clear way of deciding which is the best measure. Application-based evaluation tasks have been proposed, yet it is not clear (Weeds and Weir, 2003) whether there is or should be on</context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>G. Salton and M.J. McGill. 1983. Introduction to Modern Information Retrieval. McGraw-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Silber</author>
<author>K McCoy</author>
</authors>
<title>Efficiently computed lexical chains as an intermediate representation for automatic text summarization.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>4</issue>
<contexts>
<context position="1607" citStr="Silber and McCoy, 2002" startWordPosition="235" endWordPosition="238">In order to overcome this problem, researchers (e.g. Pereira et al. (1993)) have proposed estimating probabilities based on sets of words which are known to be distributionally similar. In the semantic domain, the hypothesis that words which mean similar things behave in similar ways (Levin, 1993), has led researchers (e.g. Lin (1998)) to propose that distributional similarity might be used as a predictor of semantic similarity. Accordingly, we might automatically build thesauruses which could be used in tasks such as malapropism correction (Budanitsky and Hirst, 2001) and text summarization (Silber and McCoy, 2002). However, the loose definition of distributional similarity that two words are distributionally similar if they appear in similar contexts has led to many distributional similarity measures being proposed; for example, the L1 Norm, the Euclidean Distance, the Cosine Metric (Salton and McGill, 1983), Jaccard&apos;s Coefficient (Frakes and Baeza-Yates, 1992), the Dice Coefficient (Frakes and Baeza-Yates, 1992), the KullbackLeibler Divergence (Cover and Thomas, 1991), the Jenson-Shannon Divergence (Rao, 1983), the a-skew Divergence (Lee, 1999), the Confusion Probability (Essen and Steinbiss, 1992), H</context>
</contexts>
<marker>Silber, McCoy, 2002</marker>
<rawString>H. Silber and K. McCoy. 2002. Efficiently computed lexical chains as an intermediate representation for automatic text summarization. Computational Linguistics, 28(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Weeds</author>
<author>D Weir</author>
</authors>
<title>Finding and evaluating sets of nearest neighbours.</title>
<date>2003</date>
<booktitle>In Proceedings of 2nd Conference on Corpus Linguistics.</booktitle>
<contexts>
<context position="2474" citStr="Weeds and Weir, 2003" startWordPosition="359" endWordPosition="362"> Distance, the Cosine Metric (Salton and McGill, 1983), Jaccard&apos;s Coefficient (Frakes and Baeza-Yates, 1992), the Dice Coefficient (Frakes and Baeza-Yates, 1992), the KullbackLeibler Divergence (Cover and Thomas, 1991), the Jenson-Shannon Divergence (Rao, 1983), the a-skew Divergence (Lee, 1999), the Confusion Probability (Essen and Steinbiss, 1992), Hindle&apos;s Mutual Information(MI)-Based Measure (Hindle, 1990) and Lin&apos;s MI-Based Measure (Lin, 1998). Further, there is no clear way of deciding which is the best measure. Application-based evaluation tasks have been proposed, yet it is not clear (Weeds and Weir, 2003) whether there is or should be one distributional similarity measure which outperforms all other distributional similarity measures on all tasks and for all words. We take a generic approach that does not directly reduce distributional similarity to a single dimension. The way dimensions are combined together will depend on parameters tuned to the demands of a given application. Further, different parameter settings will approximate different existing similarity measures as well as many more which have, until now, been unexplored. The contributions of this paper are four-fold. First, we propos</context>
<context position="10413" citStr="Weeds and Weir (2003)" startWordPosition="1744" endWordPosition="1747">st closely approximates the KullbackLeibler divergence measure. The two evaluation tasks used pseudo-disambiguation and WordNet (Fellbaum, 1998) prediction are fairly standard for distributional similarity measures. However, in the future we wish to extend our evaluation to other tasks such as malapropism correction (Budanitsky and Hirst, 2001) and PP-attachment ambiguity resolution (Resnik, 1993) and also to the probabilistic model. Since we use the same data and methodology as in earlier work, some detail is omitted in the subsequent discussion but full details and rationale can be found in Weeds and Weir (2003). 3.1 Pseudo-Disambiguation Task Pseudo-disambiguation tasks (e.g. Lee, 1999) have become a standard evaluation technique and, in the current context, we may use a word&apos;s neighbours to decide which of two cooccurrences is the most likely. Although pseudo-disambiguation itself is an artificial task, it has relevance in at least two real application areas. First, by replacing occurrences of a particular word in a test suite with a pair or set of words from which a technique must choose, we recreate a simplified version of the word sense disambiguation task; that is, simun(ni, n2) simasd (n2, ni)</context>
<context position="16949" citStr="Weeds and Weir, 2003" startWordPosition="2886" endWordPosition="2889"> neighbour is given a rank score of k — rank. We do not use the similarity scores directly since these require normalization if different similarity measures (using different scales) are to be compared. Having performed this transformation, the neighbour sets for the same word w may be represented by two ordered sets of words [wk, w1] and [w, wl]. The similarity between such sets is computed using the same calculation as used by Lin (1998) except for sim6The root of the WordNet hyponymy relation is taken to be an imagined superclass of all concepts in WordNet. 7As in previous work (Lin, 1998; Weeds and Weir, 2003), we use k = 200. Measure Noun Frequency high low params sim params sim sim e -y = 0.25 0.299 -y = 0.5 0.260 )3 = 0.5 )3 = 0.4 simmt 7 = 0.25 0.317 7 = 0.25 0.274 )3 = 0.3 )3 = 0.3 simun - 0.307 - 0.210 SiMasd - 0.290 - 0.270 Table 3: Optimal Mean Similarities and Corresponding Parameter Settings Between Thesaurus Entries for WordNet Prediction Task plifications due to the use of ranks: E EiiF i2 where i and j are the rank scores of the words within each neighbour set. 3.2.2 Results Table 3 summarizes the optimal mean similarities and parameter settings for the general framework using both the</context>
</contexts>
<marker>Weeds, Weir, 2003</marker>
<rawString>J. Weeds and D. Weir. 2003. Finding and evaluating sets of nearest neighbours. In Proceedings of 2nd Conference on Corpus Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>