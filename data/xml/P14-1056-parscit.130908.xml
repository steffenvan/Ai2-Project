<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004252">
<title confidence="0.996556">
Learning Soft Linear Constraints with Application to Citation Field
Extraction
</title>
<author confidence="0.998494">
Sam Anzaroot Alexandre Passos David Belanger Andrew McCallum
</author>
<affiliation confidence="0.9988975">
Department of Computer Science
University of Massachusetts, Amherst
</affiliation>
<email confidence="0.989066">
{anzaroot, apassos, belanger, mccallum}@cs.umass.edu
</email>
<sectionHeader confidence="0.993753" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999872043478261">
Accurately segmenting a citation string
into fields for authors, titles, etc. is a chal-
lenging task because the output typically
obeys various global constraints. Previous
work has shown that modeling soft con-
straints, where the model is encouraged,
but not require to obey the constraints, can
substantially improve segmentation per-
formance. On the other hand, for impos-
ing hard constraints, dual decomposition
is a popular technique for efficient predic-
tion given existing algorithms for uncon-
strained inference. We extend dual decom-
position to perform prediction subject to
soft constraints. Moreover, with a tech-
nique for performing inference given soft
constraints, it is easy to automatically gen-
erate large families of constraints and learn
their costs with a simple convex optimiza-
tion problem during training. This allows
us to obtain substantial gains in accuracy
on a new, challenging citation extraction
dataset.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999954107142857">
Citation field extraction, an instance of informa-
tion extraction, is the task of segmenting and la-
beling research paper citation strings into their
constituent parts, including authors, editors, year,
journal, volume, conference venue, etc. This task
is important because citation data is often pro-
vided only in plain text; however, having an ac-
curate structured database of bibliographic infor-
mation is necessary for many scientometric tasks,
such as mapping scientific sub-communities, dis-
covering research trends, and analyzing networks
of researchers. Automated citation field extrac-
tion needs further research because it has not yet
reached a level of accuracy at which it can be prac-
tically deployed in real-world systems.
Hidden Markov models and linear-chain condi-
tional random fields (CRFs) have previously been
applied to citation extraction (Hetzner, 2008; Peng
and McCallum, 2004) . These models support ef-
ficient dynamic-programming inference, but only
model local dependencies in the output label se-
quence. However citations have strong global reg-
ularities not captured by these models. For exam-
ple many book citations contain both an author
section and an editor section, but none have two
disjoint author sections. Since linear-chain mod-
els are unable to capture more than Markov depen-
dencies, the models sometimes mislabel the editor
as a second author. If we could enforce the global
constraint that there should be only one author
section, accuracy could be improved.
One framework for adding such global con-
straints into tractable models is constrained infer-
ence, in which at inference time the original model
is augmented with restrictions on the outputs such
that they obey certain global regularities. When
hard constraints can be encoded as linear equa-
tions on the output variables, and the underlying
model’s inference task can be posed as linear opti-
mization, one can formulate this constrained infer-
ence problem as an integer linear program (ILP)
(Roth and Yih, 2004). Alternatively, one can em-
ploy dual decomposition (Rush et al., 2010). Dual
decompositions’s advantage over ILP is is that it
can leverage existing inference algorithms for the
original model as a black box. Such a modular
algorithm is easy to implement, and works quite
well in practice, providing certificates of optimal-
ity for most examples.
The above two approaches have previously been
applied to impose hard constraints on a model’s
output. On the other hand, recent work has demon-
strated improvements in citation field extraction
by imposing soft constraints (Chang et al., 2012).
Here, the model is not required obey the global
constraints, but merely pays a penalty for their vi-
</bodyText>
<page confidence="0.985159">
593
</page>
<note confidence="0.962227">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 593–602,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
4 .ref-marker [ J. first D. middle Monk ,last person ]authors [ Cardinal Functions on Boolean Algebra , ]title [ Lectures in Mathematics
, ETH Zurich , series Birkhause Verlag , publisher Basel , Boston , Berlin , address 1990 . year date ]venue
</note>
<figureCaption confidence="0.999519">
Figure 1: Example labeled citation
</figureCaption>
<bodyText confidence="0.999414033333333">
olation.
This paper introduces a novel method for im-
posing soft constraints via dual decomposition.
We also propose a method for learning the penal-
ties the prediction problem incurs for violating
these soft constraints. Because our learning
method drives many penalties to zero, it allows
practitioners to perform ‘constraint selection,’ in
which a large number of automatically-generated
candidate global constraints can be considered and
automatically culled to a smaller set of useful con-
straints, which can be run quickly at test time.
Using our new method, we are able to incor-
porate not only all the soft global constraints of
Chang et al. (2012), but also far more com-
plex data-driven constraints, while also provid-
ing stronger optimality certificates than their beam
search technique. On a new, more broadly rep-
resentative, and challenging citation field extrac-
tion data set, we show that our methods achieve a
17.9% reduction in error versus a linear-chain con-
ditional random field. Furthermore, we demon-
strate that our inference technique can use and
benefit from the constraints of Chang et al. (2012),
but that including our data-driven constraints on
top of these is beneficial. While this paper fo-
cusses on an application to citation field extrac-
tion, the novel methods introduced here would
easily generalize to many problems with global
output regularities.
</bodyText>
<sectionHeader confidence="0.995333" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.999406">
2.1 Structured Linear Models
</subsectionHeader>
<bodyText confidence="0.99987275">
The overall modeling technique we employ is to
add soft constraints to a simple model for which
we have an existing efficient prediction algorithm.
For this underlying model, we employ a chain-
structured conditional random field (CRF), since
CRFs have been shown to perform better than
other simple unconstrained models like hidden
markov models for citation extraction (Peng and
McCallum, 2004). We produce a prediction by
performing MAP inference (Koller and Friedman,
2009).
The MAP inference task in a CRF be can ex-
pressed as an optimization problem with a lin-
ear objective (Sontag, 2010; Sontag et al., 2011).
Here, we define a binary indicator variable for
each candidate setting of each factor in the graph-
ical model. Each of these indicator variables is
associated with the score that the factor takes on
when it has the indictor variable’s corresponding
value. Since the log probability of some y in the
CRF is proportional to sum of the scores of all the
factors, we can concatenate the indicator variables
as a vector y and the scores as a vector w and write
the MAP problem as
</bodyText>
<equation confidence="0.992212">
max. (w, y) (1)
</equation>
<bodyText confidence="0.999827642857143">
where the set U represents the set of valid config-
urations of the indicator variables. Here, the con-
straints are that all neighboring factors agree on
the components of y in their overlap.
Structured Linear Models are the general fam-
ily of models where prediction requires solving a
problem of the form (1), and they do not always
correspond to a probabilistic model. The algo-
rithms we present in later sections for handling
soft global constraints and for learning the penal-
ties of these constraints can be applied to gen-
eral structured linear models, not just CRFs, pro-
vided we have an available algorithm for perform-
ing MAP inference.
</bodyText>
<subsectionHeader confidence="0.988351">
2.2 Dual Decomposition for Global
Constraints
</subsectionHeader>
<bodyText confidence="0.9989385">
In order to perform prediction subject to various
global constraints, we may need to augment the
problem (1) with additional constraints. Dual De-
composition is a popular method for performing
MAP inference in this scenario, since it lever-
ages known algorithms for MAP in the base prob-
lem where these extra constraints have not been
added (Komodakis et al., 2007; Sontag et al.,
2011; Rush and Collins, 2012). In this case, the
MAP problem can be formulated as a structured
linear model similar to equation (1), for which we
have a MAP algorithm, but where we have im-
posed some additional constraints Ay &lt; b that
no longer allow us to use the algorithm. In other
</bodyText>
<equation confidence="0.950764">
s.t. y E U,
</equation>
<page confidence="0.989454">
594
</page>
<bodyText confidence="0.7910565">
Algorithm 1 DD: projected subgradient for dual
decomposition with hard constraints
</bodyText>
<listItem confidence="0.582008">
1: while has not converged do
</listItem>
<equation confidence="0.990439375">
(w + AT λ, y)
2: y(t) = argmaxy∈U
� ]
3: λ(t) = Π0≤· λ(t−1) _ η(t)(Ay _ b)
words, we consider the problem
max. (w, y)
s.t. y E U (2)
Ay &lt; b,
</equation>
<bodyText confidence="0.9533995">
for an arbitrary matrix A and vector b. We can
write the Lagrangian of this problem as
</bodyText>
<equation confidence="0.937021">
L(y, A) = (w, y) + AT (Ay − b). (3)
</equation>
<bodyText confidence="0.9984035">
Regrouping terms and maximizing over the primal
variables, we have the dual problem
</bodyText>
<equation confidence="0.998138">
min.λD(A) = max (w + AT A, y) − AT b. (4)
yEU
</equation>
<bodyText confidence="0.999297090909091">
For any A, we can evaluate the dual objective
D(A), since the maximization in (4) is of the same
form as the original problem (1), and we assumed
we had a method for performing MAP in this. Fur-
thermore, a subgradient of D(A) is Ay* −b, for an
y* which maximizes this inner optimization prob-
lem. Therefore, we can minimize D(A) with the
projected subgradient method (Boyd and Vanden-
berghe, 2004), and the optimal y can be obtained
when evaluating D(A*). Note that the subgradient
of D(A) is the amount by which each constraint is
violated by A when maximizing over y.
Algorithm 1 depicts the basic projected subgra-
dient descent algorithm for dual decomposition.
The projection operator H consists of truncating
all negative coordinates of A to 0. This is neces-
sary because A is a vector of dual variables for in-
equality constraints. The algorithm has converged
when each constraint is either satisfied by y(t) with
equality or its corresponding component of A is 0,
due to complimentary slackness (Boyd and Van-
denberghe, 2004).
</bodyText>
<sectionHeader confidence="0.9821475" genericHeader="method">
3 Soft Constraints in Dual
Decomposition
</sectionHeader>
<bodyText confidence="0.999643071428571">
We now introduce an extension of Algorithm 1
to handle soft constraints. In our formulation, a
soft-constrained model imposes a penalty for each
unsatisfied constraint, proportional to the amount
by which it is violated. Therefore, our derivation
parallels how soft-margin SVMs are derived from
hard-margin SVMs by introducing auxiliary slack
variables (Cortes and Vapnik, 1995). Note that
when performing MAP subject to soft constraints,
optimal solutions might not satisfy some con-
straints, since doing so would reduce the model’s
score by too much.
Consider the optimization problems of the
form:
</bodyText>
<equation confidence="0.8972245">
max. (w, y) − (c, z)
s.t. y E U
Ay − b &lt; z (5)
−z &lt; 0,
</equation>
<bodyText confidence="0.998856272727273">
For positive ci, it is clear that an optimal zi will
be equal to the degree to which aTi y &lt; bi is vio-
lated. Therefore, we pay a cost ci times the degree
to which the ith constraint is violated, which mir-
rors how slack variables are used to represent the
hinge loss for SVMs. Note that ci has to be pos-
itive, otherwise this linear program is unbounded
and an optimal value can be obtained by setting zi
to infinity.
Using a similar construction as in section 2.2 we
write the Lagrangian as:
</bodyText>
<equation confidence="0.9403165">
L(y, z, A, µ) = (w, y) − (c, z)
+ AT (Ay − b − z) + µT(−z). (6)
</equation>
<bodyText confidence="0.833989">
The optimality constraints with respect to z tell us
that −c − A − µ = 0, hence µ = −c − A. Substi-
tuting, we have
</bodyText>
<equation confidence="0.792785">
L(y, A) = (w, y) + AT (Ay − b), (7)
</equation>
<bodyText confidence="0.998805842105263">
except the constraint that µ = −c − A implies that
for µ to be positive A &lt; c.
Since this Lagrangian has the same form as
equation (3), we can also derive a dual problem,
which is the same as in equation (4), with the ad-
ditional constraint that each Ai can not be bigger
than its cost ci. In other words, the dual problem
can not penalize the violation of a constraint more
than the soft constraint model in the primal would
penalize you if you violated it.
This optimization problem can still be solved
with projected subgradient descent and is depicted
in Algorithm 2. The only modifications to Al-
gorithm 1 are replacing the coordinate-wise pro-
jection H0&lt;· with H0&lt;·&lt;c and how we check for
convergence. Now, we check for the KKT con-
ditions of (5), where for every constraint i, either
the constraint is satisfied with equality, Ai = 0, or
Ai = ci.
</bodyText>
<page confidence="0.992817">
595
</page>
<bodyText confidence="0.8847065">
Algorithm 2 Soft-DD: projected subgradient for
dual decomposition with soft constraints
</bodyText>
<listItem confidence="0.617352">
1: while has not converged do
</listItem>
<equation confidence="0.98750825">
(w + AT λ, y)
2: y(t) = argmaxy∈U
� Jll
3: λ(t) = Π0≤·≤c λ(t−1) − η(t)(Ay − b)
</equation>
<bodyText confidence="0.999862833333333">
Therefore, implementing soft-constrained dual
decomposition is as easy as implementing hard-
constrained dual decomposition, and the per-
iteration complexity is the same. We encourage
further applications of soft-constraint dual decom-
position to existing and new NLP problems.
</bodyText>
<subsectionHeader confidence="0.999872">
3.1 Learning Penalties
</subsectionHeader>
<bodyText confidence="0.999962294117647">
One consideration when using soft v.s. hard con-
straints is that soft constraints present a new train-
ing problem, since we need to choose the vector
c, the penalties for violating the constraints. An
important property of problem (5) in the previous
section is that it corresponds to a structured lin-
ear model over y and z. Therefore, we can apply
known training algorithms for estimating the pa-
rameters of structured linear models to choose c.
All we need to employ the structured perceptron
algorithm (Collins, 2002) or the structured SVM
algorithm (Tsochantaridis et al., 2004) is a black-
box procedure for performing MAP inference in
the structured linear model given an arbitrary cost
vector. Fortunately, the MAP problem for (5) can
be solved using Soft-DD, in Algorithm 2.
Each penalty ci has to be non-negative; other-
wise, the optimization problem in equation (5) is
ill-defined. This can be ensured by simple mod-
ifications of the perceptron and subgradient de-
scent optimization of the structured SVM objec-
tive simply by truncating c coordinate-wise to be
non-negative at every learning iteration.
Intuitively, the perceptron update increases the
penalty for a constraint if it is satisfied in the
ground truth and not in an inferred prediction, and
decreases the penalty if the constraint is satisfied
in the prediction and not the ground truth. Since
we truncate penalties at 0, this suggests that we
will learn a penalty of 0 for constraints in three cat-
egories: constraints that do not hold in the ground
truth, constraints that hold in the ground truth but
are satisfied in practice by performing inference
in the base CRF model, and constraints that are
satisfied in practice as a side-effect of imposing
non-zero penalties on some other constraints . A
similar analysis holds for the structured SVM ap-
proach.
Therefore, we can view learning the values of
the penalties not just as parameter tuning, but as a
means to perform ‘constraint selection,’ since con-
straints that have a penalty of 0 can be ignored.
This property allows us to consider large families
of constraints, from which the useful ones are au-
tomatically identified.
We found it beneficial, though it is not theoreti-
cally necessary, to learn the constraints on a held-
out development set, separately from the other
model parameters, as during training most con-
straints are satisfied due to overfitting, which leads
to an underestimation of the relevant penalties.
</bodyText>
<sectionHeader confidence="0.97498" genericHeader="method">
4 Citation Extraction Data
</sectionHeader>
<bodyText confidence="0.998304034482759">
We consider the UMass citation dataset, first intro-
duced in Anzaroot and McCallum (2013). It has
over 1800 citation from many academic fields, ex-
tracted from the arXiv. This dataset contains both
coarse-grained and fine-grained labels; for exam-
ple it contains labels for the segment of all authors,
segments for each individual author, and for the
first and last name of each author. There are 660
citations in the development set and 367 citation
in the test set.
The labels in the UMass dataset are a con-
catenation of labels from a hierarchically-defined
schema. For example, a first name of an author is
tagged as: authors/person/irst. In addition, indi-
vidual tokens are labeled using a BIO label schema
for each level in the hierarchy. BIO is a commonly
used labeling schema for information extraction
tasks. BIO labeling allows individual labels on
tokens to label segmentation information as well
as labels for the segments. In this schema, labels
that begin segments are prepended with a B, la-
bels that continue a segment are prepended with
an I, and tokens that don’t have a labeling in this
schema are given an O label. For example, in a hi-
erarchical BIO label schema the first token in the
first name for the second author may be labeled as:
I-authors/B-person/B-irst.
An example labeled citation in this dataset can
be viewed in figure 1.
</bodyText>
<sectionHeader confidence="0.9990885" genericHeader="method">
5 Global Constraints for Citation
Extraction
</sectionHeader>
<subsectionHeader confidence="0.996926">
5.1 Constraint Templates
</subsectionHeader>
<bodyText confidence="0.9953685">
We now describe the families of global constraints
we consider for citation extraction. Note these
</bodyText>
<page confidence="0.993652">
596
</page>
<bodyText confidence="0.999860375">
constraints are all linear, since they depend only
on the counts of each possible conditional ran-
dom field label. Moreover, since our labels are
BIO-encoded, it is possible, by counting B tags,
to count how often each citation tag itself appears
in a sentence. The first two families of constraints
that we describe are general to any sequence la-
beling task while the last is specific to hierarchical
labeling such as available in the UMass dataset.
Our sequence output is denoted as y and an ele-
ment of this sequence is yk.
We denote [[yk = i]] as the function that outputs
1 if yk has a 1 at index i and 0 otherwise. Here, yk
represents an output tag of the CRF, so if [[yk = i]]
= 1, then we have that yk was given a label with
index i.
</bodyText>
<subsectionHeader confidence="0.996526">
5.2 Singleton Constraints
</subsectionHeader>
<bodyText confidence="0.999926833333333">
Singleton constraints ensure that each label can
appear at most once in a citation. These are same
global constraints that were used for citation field
extraction in Chang et al. (2012). We define s(i)
to be the number of times the label with index i is
predicted in a citation, formally:
</bodyText>
<equation confidence="0.985218">
s(i) = X [[yk = i]]
ykEy
</equation>
<bodyText confidence="0.998053">
The constraint that each label can appear at
most once takes the form:
</bodyText>
<equation confidence="0.545448">
s(i) &lt;= 1
</equation>
<subsectionHeader confidence="0.982488">
5.3 Pairwise Constraints
</subsectionHeader>
<bodyText confidence="0.999712">
Pairwise constraints are constraints on the counts
of two labels in a citation. We define z1(i, j) to be
</bodyText>
<equation confidence="0.973053">
[[yk = i]] + X [[yk = j]]
ykEy
and z2(i, j) to be
z2(i,j) = X [[yk = i]] − X [[yk = j]]
ykEy ykEy
</equation>
<bodyText confidence="0.96297225">
We consider all constraints of the forms:
z(i, j) &lt; 0, 1, 2, 3 and z(i, j) &gt; 0, 1, 2, 3.
Note that some pairs of these constraints are re-
dundant or logically incompatible. However, we
are using them as soft constraints, so these con-
straints will not necessarily be satisfied by the out-
put of the model, which eliminates concern over
enforcing logically impossible outputs. Further-
more, in section 3.1 we described how our proce-
dure for learning penalties will drive some penal-
ties to 0, which effectively removes them from our
set of constraints we consider. It can be shown, for
example, that we will never learn non-zero penal-
ties for certain pairs of logically incompatible con-
straints using the perceptron-style algorithm de-
scribed in section 3.1 .
</bodyText>
<subsectionHeader confidence="0.992891">
5.4 Hierarchical Equality Constraints
</subsectionHeader>
<bodyText confidence="0.9999565">
The labels in the citation dataset are hierarchical
labels. This means that the labels are the concate-
nation of all the levels in the hierarchy. We can
create constraints that are dependent on only one
or couple of elements in the hierarchy.
We define C(x, i) as the function that returns 1
if the output x contains the label i in the hierarchy
and 0 otherwise. We define e(i, j) to be
</bodyText>
<equation confidence="0.9818258">
e(i, j) = X [[C(yk, i)]] − X [[C(yk,j)]]
ykEy ykEy
Hierarchical equality constraints take the forms:
e(i, j) &gt; 0 (8)
e(i, j) &lt; 0 (9)
</equation>
<subsectionHeader confidence="0.963045">
5.5 Local constraints
</subsectionHeader>
<bodyText confidence="0.999934266666667">
We constrain the output labeling of the chain-
structured CRF to be a valid BIO encoding.
This both improves performance of the underly-
ing model when used without global constraints,
as well as ensures the validity of the global con-
straints we impose, since they operate only on
B labels. The constraint that the labeling is
valid BIO can be expressed as a collection of
pairwise constraints on adjacent labels in the se-
quence. Rather than enforcing these constraints
using dual decomposition, they can be enforced
directly when performing MAP inference in the
CRF by modifying the dynamic program of the
Viterbi algorithm to only allow valid pairs of adja-
cent labels.
</bodyText>
<subsectionHeader confidence="0.980667">
5.6 Constraint Pruning
</subsectionHeader>
<bodyText confidence="0.9996888">
While the techniques from section 3.1 can easily
cope with a large numbers of constraints at train-
ing time, this can be computationally costly, spe-
cially if one is considering very large constraint
families. This is problematic because the size
</bodyText>
<equation confidence="0.983482">
X
z1(i,j) =
ykEy
</equation>
<page confidence="0.974445">
597
</page>
<table confidence="0.9940672">
Unconstrained [17]ref-marker [ D.first Sivia ,last person J.first Skilling ,last person ]authors [ Data Analysis : A Bayesian Tutorial
,booktitle Oxford University Press , publisher 2006 year date ]venue
Constrained [17]ref-marker [ D.first Sivia ,last person J.first Skilling ,last person ]authors Data Analysis : A Bayesian Tutorial
,title [ Oxford University Press , publisher 2006 year date ]venue
Unconstrained [ Sobol’ ,last I.first M.middle person ]authors [ (1990) .year ]date [On sensitivity estimation for nonlinear mathe-
matical models .]title [ Matematicheskoe Modelirovanie ,journal 2volume (1) :number 112–118 .pages ( In Russian
) . status ]venue
Constrained [ Sobol’ ,last I.first M.middle person ]authors [ (1990) .year ]date [On sensitivity estimation for nonlinear mathe-
matical models .]title [ Matematicheskoe Modelirovanie ,journal 2volume (1) :number 112–118 .pages ( In Russian
) . language ]venue
</table>
<figureCaption confidence="0.906377333333333">
Figure 2: Two examples where imposing soft global constraints improves field extraction errors. Soft-
DD converged in 1 iteration on the first example, and 7 iterations on the second. When a reference is
citing a book and not a section of the book, the correct labeling of the name of the book is title. In
the first example, the baseline CRF incorrectly outputs booktitle, but this is fixed by Soft-DD, which
penalizes outputs based on the constraint that booktitle should co-occur with an address label. In the
second example, the unconstrained CRF output violates the constraint that title and status labels should
not co-occur. The ground truth labeling also violates a constraint that title and language labels should
not co-occur. At convergence of the Soft-DD algorithm, the correct labeling of language is predicted,
which is possible because of the use of soft constraints.
</figureCaption>
<table confidence="0.999223571428571">
Constraints F1 score Sparsity # of cons
Baseline 94.44
Only-one 94.62 0% 3
Hierarchical 94.55 56.25% 16
Pairwise 95.23 43.19% 609
All 95.39 32.96% 628
All DD 94.60 0% 628
</table>
<tableCaption confidence="0.998435">
Table 1: Set of constraints learned and F1 scores.
</tableCaption>
<bodyText confidence="0.992472638888889">
The last row depicts the result of inference using
all constraints as hard constraints.
of some constraint families we consider grows
quadratically with the number of candidate labels,
and there are about 100 in the UMass dataset.
Such a family consists of constraints that the sum
of the counts of two different label types has to
be bounded (a useful example is that there can’t
be more than one out of “phd thesis” and “jour-
nal”). Therefore, quickly pruning bad constraints
can save a substantial amount of training time, and
can lead to better generalization.
To do so, we calculate a score that estimates
how useful each constraint is expected to be. Our
score compares how often the constraint is vio-
lated in the ground truth examples versus our pre-
dictions. Here, prediction is done with respect to
the base chain-structured CRF tagger and does not
include global constraints. Note that it may make
sense to consider a constraint that is sometimes vi-
olated in the ground truth, as the penalty learning
algorithm can learn a small penalty for it, which
will allow it to be violated some of the time. Our
importance score is defined as, for each constraint
c on labeled set D,
where [[y]]c is 1 if the constraint is violated on out-
put y and 0 otherwise. Here, yd denotes the ground
truth labeling and wd is the vector of scores for the
CRF tagger.
We prune constraints by picking a cutoff value
for imp(c). A value of imp(c) above 1 implies
that the constraint is more violated on the pre-
dicted examples than on the ground truth, and
hence that we might want to keep it.
We also find that the constraints that have the
largest imp values are semantically interesting.
</bodyText>
<sectionHeader confidence="0.999959" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.9993298">
There are multiple previous examples of augment-
ing chain-structured sequence models with terms
capturing global relationships by expanding the
chain to a more complex graphical model with
non-local dependencies between the outputs. In-
ference in these models can be performed, for
example, with loopy belief propagation (Bunescu
and Mooney, 2004; Sutton and McCallum, 2004)
or Gibbs sampling (Finkel et al., 2005). Be-
lief propagation is prohibitively expensive in our
</bodyText>
<equation confidence="0.99471175">
Ed∈D[[maxywTd y]]c
imp(c) =
Ed∈D[[yd]] (10)
C
</equation>
<page confidence="0.983337">
598
</page>
<bodyText confidence="0.999984762711864">
model due to the high cardinalities of the out-
put variables and of the global factors, which in-
volve all output variables simultaneously. There
are various methods for exploiting the combi-
natorial structure of these factors, but perfor-
mance would still have higher complexity than our
method. While Gibbs sampling has been shown
to work well tasks such as named entity recogni-
tion (Finkel et al., 2005), our previous experiments
show that it does not work well for citation extrac-
tion, where it found only low-quality solutions in
practice because the sampling did not mix well,
even on a simple chain-structured CRF.
Recently, dual decomposition has become a
popular method for solving complex structured
prediction problems in NLP (Koo et al., 2010;
Rush et al., 2010; Rush and Collins, 2012; Paul
and Eisner, 2012; Chieu and Teow, 2012). Soft
constraints can be implemented inefficiently using
hard constraints and dual decomposition— by in-
troducing copies of output variables and an aux-
iliary graphical model, as in Rush et al. (2012).
However, at every iteration of dual decomposition,
MAP must be run in this auxiliary model. Further-
more the copying of variables doubles the num-
ber of iterations needed for information to flow
between output variables, and thus slows conver-
gence. On the other hand, our approach to soft
constraints has identical per-iteration complexity
as for hard constraints, and is a very easy modifi-
cation to existing hard constraint code.
Initial work in machine learning for citation ex-
traction used Markov models with no global con-
straints. Hidden Markov models (HMMs), were
originally employed for automatically extracting
information from research papers on the CORA
dataset (Seymore et al., 1999; Hetzner, 2008).
Later, CRFs were shown to perform better on
CORA, improving the results from the Hmm’s
token-level F1 of 86.6 to 91.5 with a CRF(Peng
and McCallum, 2004).
Recent work on globally-constrained inference
in citation extraction used an HMMCCM, which is
an HMM with the addition of global features that
are restricted to have positive weights (Chang et
al., 2012). Approximate inference is performed
using beam search. This method increased the
HMM token-level accuracy from 86.69 to 93.92
on a test set of 100 citations from the CORA
dataset. The global constraints added into the
model are simply that each label only occurs
once per citation. This approach is limited in its
use of an HMM as an underlying model, as it
has been shown that CRFs perform significantly
better, achieving 95.37 token-level accuracy on
CORA (Peng and McCallum, 2004). In our ex-
periments, we demonstrate that the specific global
constraints used by Chang et al. (2012) help on the
UMass dataset as well.
</bodyText>
<sectionHeader confidence="0.979912" genericHeader="evaluation">
7 Experimental Results
</sectionHeader>
<bodyText confidence="0.999908487804878">
Our baseline is the one used in Anzaroot and
McCallum (2013), with some labeling errors re-
moved. This is a chain-structured CRF trained
to maximize the conditional likelihood using L-
BFGS with L2 regularization.
We use the same features as Anzaroot and Mc-
Callum (2013), which include word type, capital-
ization, binned location in citation, regular expres-
sion matches, and matches into lexicons. In addi-
tion, we use a rule-based segmenter that segments
the citation string based on punctuation as well as
probable start or end segment words (e.g. ‘in’ and
‘volume’). We add a binary feature to tokens that
correspond to the start of a segment in the output
of this simple segmenter. This final feature im-
proves the F1 score on the cleaned test set from
94.0 F1 to 94.44 F1, which we use as a baseline
score.
We then use the development set to learn the
penalties for the soft constraints, using the percep-
tron algorithm described in section 3.1. MAP in-
ference in the model with soft constraints is per-
formed using Soft-DD, shown in Algorithm 2.
We instantiate constraints from each template in
section 5.1, iterating over all possible labels that
contain a B prefix at any level in the hierarchy and
pruning all constraints with imp(c) &lt; 2.75 cal-
culated on the development set. We asses perfor-
mance in terms of field-level F1 score, which is
the harmonic mean of precision and recall for pre-
dicted segments.
Table 1 shows how each type of constraint fam-
ily improved the F1 score on the dataset. Learning
all the constraints jointly provides the largest im-
provement in F1 at 95.39. This improvement in F1
over the baseline CRF as well as the improvement
in F1 over using only-one constraints was shown
to be statistically significant using the Wilcoxon
signed rank test with p-values &lt; 0.05. In the
all-constraints settings, 32.96% of the constraints
have a learned parameter of 0, and therefore only
</bodyText>
<page confidence="0.99591">
599
</page>
<table confidence="0.9983974">
Stop F1 score Convergence Avg Iterations
1 94.44 76.29% 1.0
2 95.07 83.38% 1.24
5 95.12 95.91% 1.61
10 95.39 99.18% 1.73
</table>
<tableCaption confidence="0.992402">
Table 2: Performance from terminating Soft-DD
</tableCaption>
<bodyText confidence="0.997879360655738">
early. Column 1 is the number of iterations we
allow each example. Column 3 is the % of test
examples that converged. Column 4 is the aver-
age number of necessary iterations, a surrogate for
the slowdown over performing unconstrained in-
ference.
421 constraints are active. Soft-DD converges,
and thus solves the constrained inference prob-
lem exactly, for all test set examples after at most
41 iterations. Running Soft-DD to convergence
requires 1.83 iterations on average per example.
Since performing inference in the CRF is by far
the most computationally intensive step in the iter-
ative algorithm, this means our procedure requires
approximately twice as much work as running the
baseline CRF on the dataset. On examples where
unconstrained inference does not satisfy the con-
straints, Soft-DD converges after 4.52 iterations
on average. For 11.99% of the examples, the
Soft-DD algorithm satisfies constraints that were
not satisfied during unconstrained inference, while
in the remaining 11.72% Soft-DD converges with
some constraints left unsatisfied, which is possible
since we are imposing them as soft constraints.
We could have enforced these constraints as
hard constraints rather than soft ones. This exper-
iment is shown in the last row of Table 1, where
F1 only improves to 94.6. In addition, running
the DD algorithm with these constraints takes 5.21
iterations on average per example, which is 2.8
times slower than Soft-DD with learned penalties.
In Figure 2, we analyze the performance of
Soft-DD when we don’t necessarily run it to con-
vergence, but stop after a fixed number of itera-
tions on each test set example. We find that a large
portion of our gain in accuracy can be obtained
when we allow ourselves as few as 2 dual decom-
position iterations. However, this only amounts to
1.24 times as much work as running the baseline
CRF on the dataset, since the constraints are satis-
fied immediately for many examples.
In Figure 2 we consider two applications of our
Soft-DD algorithm, and provide analysis in the
caption.
We train and evaluate on the UMass dataset in-
stead of CORA, because it is significantly larger,
has a useful finer-grained labeling schema, and its
annotation is more consistent. We were able to ob-
tain better performance on CORA using our base-
line CRF than the HMMCCM results presented
in Chang et al. (2012), which include soft con-
straints. Given this high performance of our base
model on CORA, we did not apply our Soft-DD
algorithm to the dataset. Furthermore, since the
dataset is so small, learning the penalties for our
large collection of constraints is difficult, and test
set results are unreliable. Rather than compare our
work to Chang et al. (2012) via results on CORA,
we apply their constraints on the UMass data us-
ing Soft-DD and demonstrate accuracy gains, as
discussed above.
</bodyText>
<subsectionHeader confidence="0.99748">
7.1 Examples of learned constraints
</subsectionHeader>
<bodyText confidence="0.999966696969697">
We now describe a number of the useful con-
straints that receive non-zero learned penalties
and have high importance scores, defined in Sec-
tion 5.6. The importance score of a constraint pro-
vides information about how often it is violated
by the CRF, but holds in the ground truth, and a
non-zero penalty implies we enforce it as a soft
constraint at test time.
The two singleton constraints with highest im-
portance score are that there should only be at
most one title segment in a citation and that there
should be at most one author segment in a cita-
tion. The only one author constraint is particu-
larly useful for correctly labeling editor segments
in cases where unconstrained inference mislabels
them as author segments. As can be seen in Table
3, editor fields are among the most improved with
our new method, largely due to this constraint.
The two hierarchical constraints with the high-
est importance scores with non-zero learned
penalties constrain the output such that number
of person segments does not exceed the number
of first segments and vice-versa. Together, these
constraints penalize outputs in which the number
of person segments do not equal the number of
first segments, i.e., every author should have a first
name.
One important pairwise constraint penalizes
outputs in which thesis segments don’t co-occur
with school segments. School segments label the
name of the university that the thesis was submit-
ted to. The application of this constraint increases
the performance of the model on school segments
</bodyText>
<page confidence="0.964405">
600
</page>
<figure confidence="0.591558785714286">
U C +
35.29 66.67 31.37
66.67 94.74 28.07
40.00 66.67 26.67
75.00 94.74 19.74
77.78 90.00 12.22
81.82 91.67 9.85
Label
venue/series
venue/editor/person/first
venue/school
venue/editor/person/last
venue/editor
venue/editor/person/middle
</figure>
<tableCaption confidence="0.938572666666667">
Table 3: Labels with highest improvement in F1.
U is in unconstrained inference. C is the results of
constrained inference. + is the improvement in F1.
</tableCaption>
<bodyText confidence="0.999531">
dramatically, as can be seen in table 3.
An interesting form of pairwise constraints pe-
nalize outputs in which some labels do not co-
occur with other labels. Some examples of con-
straints in this form enforce that journal segments
should co-occur with pages segments and that
booktitle segments should co-occur with address
segments. An example of the latter constraint be-
ing employed during inference is the first example
in Figure 2. Here, the constrained inference pe-
nalizes output which contains a booktitle segment
but no address segment. This penalization leads
allows the constrained inference to correctly label
the booktitle segment as a title segment.
The above example constraints are almost al-
ways satisfied on the ground truth, and would be
useful to enforce as hard constraints. However,
there are a number of learned constraints that are
often violated on the ground truth but are still use-
ful as soft constraints. Take, for example, the con-
straint that the number of number segments does
not exceed the number of booktitle segments, as
well as the constraint that it does not exceed the
number of journal segments. These constraints
are moderately violated on ground truth examples,
however. For example, when booktitle segments
co-occur with number segments but not with jour-
nal segments, the second constraint is violated. It
is still useful to impose these soft constraints, as
strong evidence from the CRF allows us to violate
them, and they can guide the model to good pre-
dictions when the CRF is unconfident.
</bodyText>
<sectionHeader confidence="0.998306" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999980291666667">
We introduce a novel modification to the stan-
dard projected subgradient dual decomposition al-
gorithm for performing MAP inference subject to
hard constraints to one for performing MAP in the
presence of soft constraints. In addition, we offer
an easy-to-implement procedure for learning the
penalties on soft constraints. This method drives
many penalties to zero, which allows users to auto-
matically discover discriminative constraints from
large families of candidates.
We show via experiments on a recent substantial
dataset that using soft constraints, and selecting
which constraints to use with our penalty-learning
procedure, can lead to significant gains in accu-
racy. We achieve a 17% gain in accuracy over
a chain-structured CRF model, while only need-
ing to run MAP in the CRF an average of less
than 2 times per example. This minor incremen-
tal cost over Viterbi, plus the fact that we obtain
certificates of optimality on 100% of our test ex-
amples in practice, suggests the usefulness of our
algorithm for large-scale applications. We encour-
age further use of our Soft-DD procedure for other
structured prediction problems.
</bodyText>
<sectionHeader confidence="0.99856" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999605">
This work was supported in part by the Cen-
ter for Intelligent Information Retrieval, in part
by DARPA under agreement number FA8750-13-
2-0020, in part by NSF grant #CNS-0958392,
and in part by IARPA via DoI/NBC contract
#D11PC20152. The U.S. Government is autho-
rized to reproduce and distribute reprint for Gov-
ernmental purposes notwithstanding any copy-
right annotation thereon. Any opinions, findings
and conclusions or recommendations expressed in
this material are those of the authors and do not
necessarily reflect those of the sponsor.
</bodyText>
<sectionHeader confidence="0.999216" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999104368421053">
Sam Anzaroot and Andrew McCallum. 2013. A new
dataset for fine-grained citation field extraction. In
ICML Workshop on Peer Reviewing and Publishing
Models.
Stephen Poythress Boyd and Lieven Vandenberghe.
2004. Convex optimization. Cambridge university
press.
Razvan Bunescu and Raymond J Mooney. 2004.
Collective information extraction with relational
markov networks. In Proceedings of the 42nd An-
nual Meeting on Association for Computational Lin-
guistics, page 438. Association for Computational
Linguistics.
M. Chang, L. Ratinov, and D. Roth. 2012. Structured
learning with constrained conditional models. Ma-
chine Learning, 88(3):399–431, 6.
Hai Leong Chieu and Loo-Nin Teow. 2012. Com-
bining local and non-local information with dual de-
composition for named entity recognition from text.
</reference>
<page confidence="0.978112">
601
</page>
<reference confidence="0.999276115789473">
In Information Fusion (FUSION), 2012 15th Inter-
national Conference on, pages 231–238. IEEE.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing-Volume 10, pages 1–8.
Association for Computational Linguistics.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine learning, 20(3):273–297.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 363–370. Association for Computational Lin-
guistics.
Erik Hetzner. 2008. A simple method for citation
metadata extraction using hidden markov models. In
Proceedings of the 8th ACM/IEEE-CS joint confer-
ence on Digital libraries, pages 280–284. ACM.
Daphne Koller and Nir Friedman. 2009. Probabilistic
graphical models: principles and techniques. The
MIT Press.
Nikos Komodakis, Nikos Paragios, and Georgios Tzir-
itas. 2007. Mrf optimization via dual decomposi-
tion: Message-passing revisited. In Computer Vi-
sion, 2007. ICCV 2007. IEEE 11th International
Conference on, pages 1–8. IEEE.
Terry Koo, Alexander M Rush, Michael Collins,
Tommi Jaakkola, and David Sontag. 2010. Dual
decomposition for parsing with non-projective head
automata. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1288–1298. Association for Compu-
tational Linguistics.
Michael J Paul and Jason Eisner. 2012. Implicitly in-
tersecting weighted automata using dual decompo-
sition. In Proceedings of the 2012 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 232–242. Association for Computa-
tional Linguistics.
Fuchun Peng and Andrew McCallum. 2004. Accu-
rate information extraction from research papers us-
ing conditional random fields. In Daniel Marcu Su-
san Dumais and Salim Roukos, editors, HLT-NAACL
2004: Main Proceedings, pages 329–336, Boston,
Massachusetts, USA, May 2 - May 7. Association
for Computational Linguistics.
Dan Roth and Wen-tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. Defense Technical Information Center.
Alexander M. Rush and Michael Collins. 2012. A tu-
torial on dual decomposition and lagrangian relax-
ation for inference in natural language processing.
J. Artif. Intell. Res. (JAIR), 45:305–362.
Alexander M Rush, David Sontag, Michael Collins,
and Tommi Jaakkola. 2010. On dual decomposition
and linear programming relaxations for natural lan-
guage processing. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1–11. Association for Computa-
tional Linguistics.
Alexander M Rush, Roi Reichart, Michael Collins, and
Amir Globerson. 2012. Improved parsing and pos
tagging using inter-sentence consistency constraints.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 1434–1444.
Kristie Seymore, Andrew McCallum, Roni Rosenfeld,
et al. 1999. Learning hidden markov model struc-
ture for information extraction. In AAAI-99 Work-
shop on Machine Learning for Information Extrac-
tion, pages 37–42.
David Sontag, Amir Globerson, and Tommi Jaakkola.
2011. Introduction to dual decomposition for in-
ference. In Suvrit Sra, Sebastian Nowozin, and
Stephen J. Wright, editors, Optimization for Ma-
chine Learning. MIT Press.
David Sontag. 2010. Approximate Inference in Graph-
ical Models using LP Relaxations. Ph.D. thesis,
Massachusetts Institute of Technology, Department
of Electrical Engineering and Computer Science.
Charles Sutton and Andrew McCallum. 2004. Col-
lective segmentation and labeling of distant entities
in information extraction. Technical report, DTIC
Document.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vector
machine learning for interdependent and structured
output spaces. In Proceedings of the twenty-first in-
ternational conference on Machine learning, page
104. ACM.
</reference>
<page confidence="0.998145">
602
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.695263">
<title confidence="0.998062">Learning Soft Linear Constraints with Application to Citation Field Extraction</title>
<author confidence="0.999868">Sam Anzaroot Alexandre Passos David Belanger Andrew</author>
<affiliation confidence="0.999954">Department of Computer University of Massachusetts,</affiliation>
<email confidence="0.807294">apassos,belanger,</email>
<abstract confidence="0.994332541666667">Accurately segmenting a citation string into fields for authors, titles, etc. is a challenging task because the output typically obeys various global constraints. Previous has shown that modeling conwhere the model is encouraged, but not require to obey the constraints, can substantially improve segmentation performance. On the other hand, for imposdual decomposition is a popular technique for efficient prediction given existing algorithms for unconstrained inference. We extend dual decomposition to perform prediction subject to soft constraints. Moreover, with a technique for performing inference given soft constraints, it is easy to automatically generate large families of constraints and learn their costs with a simple convex optimization problem during training. This allows us to obtain substantial gains in accuracy on a new, challenging citation extraction dataset.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sam Anzaroot</author>
<author>Andrew McCallum</author>
</authors>
<title>A new dataset for fine-grained citation field extraction.</title>
<date>2013</date>
<booktitle>In ICML Workshop on Peer Reviewing and Publishing Models.</booktitle>
<contexts>
<context position="15094" citStr="Anzaroot and McCallum (2013)" startWordPosition="2519" endWordPosition="2522">o perform ‘constraint selection,’ since constraints that have a penalty of 0 can be ignored. This property allows us to consider large families of constraints, from which the useful ones are automatically identified. We found it beneficial, though it is not theoretically necessary, to learn the constraints on a heldout development set, separately from the other model parameters, as during training most constraints are satisfied due to overfitting, which leads to an underestimation of the relevant penalties. 4 Citation Extraction Data We consider the UMass citation dataset, first introduced in Anzaroot and McCallum (2013). It has over 1800 citation from many academic fields, extracted from the arXiv. This dataset contains both coarse-grained and fine-grained labels; for example it contains labels for the segment of all authors, segments for each individual author, and for the first and last name of each author. There are 660 citations in the development set and 367 citation in the test set. The labels in the UMass dataset are a concatenation of labels from a hierarchically-defined schema. For example, a first name of an author is tagged as: authors/person/irst. In addition, individual tokens are labeled using </context>
<context position="27244" citStr="Anzaroot and McCallum (2013)" startWordPosition="4570" endWordPosition="4573">d the HMM token-level accuracy from 86.69 to 93.92 on a test set of 100 citations from the CORA dataset. The global constraints added into the model are simply that each label only occurs once per citation. This approach is limited in its use of an HMM as an underlying model, as it has been shown that CRFs perform significantly better, achieving 95.37 token-level accuracy on CORA (Peng and McCallum, 2004). In our experiments, we demonstrate that the specific global constraints used by Chang et al. (2012) help on the UMass dataset as well. 7 Experimental Results Our baseline is the one used in Anzaroot and McCallum (2013), with some labeling errors removed. This is a chain-structured CRF trained to maximize the conditional likelihood using LBFGS with L2 regularization. We use the same features as Anzaroot and McCallum (2013), which include word type, capitalization, binned location in citation, regular expression matches, and matches into lexicons. In addition, we use a rule-based segmenter that segments the citation string based on punctuation as well as probable start or end segment words (e.g. ‘in’ and ‘volume’). We add a binary feature to tokens that correspond to the start of a segment in the output of th</context>
</contexts>
<marker>Anzaroot, McCallum, 2013</marker>
<rawString>Sam Anzaroot and Andrew McCallum. 2013. A new dataset for fine-grained citation field extraction. In ICML Workshop on Peer Reviewing and Publishing Models.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Poythress Boyd</author>
<author>Lieven Vandenberghe</author>
</authors>
<title>Convex optimization. Cambridge university press.</title>
<date>2004</date>
<contexts>
<context position="9156" citStr="Boyd and Vandenberghe, 2004" startWordPosition="1489" endWordPosition="1493">trix A and vector b. We can write the Lagrangian of this problem as L(y, A) = (w, y) + AT (Ay − b). (3) Regrouping terms and maximizing over the primal variables, we have the dual problem min.λD(A) = max (w + AT A, y) − AT b. (4) yEU For any A, we can evaluate the dual objective D(A), since the maximization in (4) is of the same form as the original problem (1), and we assumed we had a method for performing MAP in this. Furthermore, a subgradient of D(A) is Ay* −b, for an y* which maximizes this inner optimization problem. Therefore, we can minimize D(A) with the projected subgradient method (Boyd and Vandenberghe, 2004), and the optimal y can be obtained when evaluating D(A*). Note that the subgradient of D(A) is the amount by which each constraint is violated by A when maximizing over y. Algorithm 1 depicts the basic projected subgradient descent algorithm for dual decomposition. The projection operator H consists of truncating all negative coordinates of A to 0. This is necessary because A is a vector of dual variables for inequality constraints. The algorithm has converged when each constraint is either satisfied by y(t) with equality or its corresponding component of A is 0, due to complimentary slacknes</context>
</contexts>
<marker>Boyd, Vandenberghe, 2004</marker>
<rawString>Stephen Poythress Boyd and Lieven Vandenberghe. 2004. Convex optimization. Cambridge university press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Raymond J Mooney</author>
</authors>
<title>Collective information extraction with relational markov networks.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>438</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="24261" citStr="Bunescu and Mooney, 2004" startWordPosition="4087" endWordPosition="4090">c). A value of imp(c) above 1 implies that the constraint is more violated on the predicted examples than on the ground truth, and hence that we might want to keep it. We also find that the constraints that have the largest imp values are semantically interesting. 6 Related Work There are multiple previous examples of augmenting chain-structured sequence models with terms capturing global relationships by expanding the chain to a more complex graphical model with non-local dependencies between the outputs. Inference in these models can be performed, for example, with loopy belief propagation (Bunescu and Mooney, 2004; Sutton and McCallum, 2004) or Gibbs sampling (Finkel et al., 2005). Belief propagation is prohibitively expensive in our Ed∈D[[maxywTd y]]c imp(c) = Ed∈D[[yd]] (10) C 598 model due to the high cardinalities of the output variables and of the global factors, which involve all output variables simultaneously. There are various methods for exploiting the combinatorial structure of these factors, but performance would still have higher complexity than our method. While Gibbs sampling has been shown to work well tasks such as named entity recognition (Finkel et al., 2005), our previous experiment</context>
</contexts>
<marker>Bunescu, Mooney, 2004</marker>
<rawString>Razvan Bunescu and Raymond J Mooney. 2004. Collective information extraction with relational markov networks. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 438. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Chang</author>
<author>L Ratinov</author>
<author>D Roth</author>
</authors>
<title>Structured learning with constrained conditional models.</title>
<date>2012</date>
<booktitle>Machine Learning,</booktitle>
<volume>88</volume>
<issue>3</issue>
<pages>6</pages>
<contexts>
<context position="3791" citStr="Chang et al., 2012" startWordPosition="571" endWordPosition="574">ogram (ILP) (Roth and Yih, 2004). Alternatively, one can employ dual decomposition (Rush et al., 2010). Dual decompositions’s advantage over ILP is is that it can leverage existing inference algorithms for the original model as a black box. Such a modular algorithm is easy to implement, and works quite well in practice, providing certificates of optimality for most examples. The above two approaches have previously been applied to impose hard constraints on a model’s output. On the other hand, recent work has demonstrated improvements in citation field extraction by imposing soft constraints (Chang et al., 2012). Here, the model is not required obey the global constraints, but merely pays a penalty for their vi593 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 593–602, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics 4 .ref-marker [ J. first D. middle Monk ,last person ]authors [ Cardinal Functions on Boolean Algebra , ]title [ Lectures in Mathematics , ETH Zurich , series Birkhause Verlag , publisher Basel , Boston , Berlin , address 1990 . year date ]venue Figure 1: Example labeled citation olation. This pape</context>
<context position="5025" citStr="Chang et al. (2012)" startWordPosition="768" endWordPosition="771">novel method for imposing soft constraints via dual decomposition. We also propose a method for learning the penalties the prediction problem incurs for violating these soft constraints. Because our learning method drives many penalties to zero, it allows practitioners to perform ‘constraint selection,’ in which a large number of automatically-generated candidate global constraints can be considered and automatically culled to a smaller set of useful constraints, which can be run quickly at test time. Using our new method, we are able to incorporate not only all the soft global constraints of Chang et al. (2012), but also far more complex data-driven constraints, while also providing stronger optimality certificates than their beam search technique. On a new, more broadly representative, and challenging citation field extraction data set, we show that our methods achieve a 17.9% reduction in error versus a linear-chain conditional random field. Furthermore, we demonstrate that our inference technique can use and benefit from the constraints of Chang et al. (2012), but that including our data-driven constraints on top of these is beneficial. While this paper focusses on an application to citation fiel</context>
<context position="17480" citStr="Chang et al. (2012)" startWordPosition="2935" endWordPosition="2938">ny sequence labeling task while the last is specific to hierarchical labeling such as available in the UMass dataset. Our sequence output is denoted as y and an element of this sequence is yk. We denote [[yk = i]] as the function that outputs 1 if yk has a 1 at index i and 0 otherwise. Here, yk represents an output tag of the CRF, so if [[yk = i]] = 1, then we have that yk was given a label with index i. 5.2 Singleton Constraints Singleton constraints ensure that each label can appear at most once in a citation. These are same global constraints that were used for citation field extraction in Chang et al. (2012). We define s(i) to be the number of times the label with index i is predicted in a citation, formally: s(i) = X [[yk = i]] ykEy The constraint that each label can appear at most once takes the form: s(i) &lt;= 1 5.3 Pairwise Constraints Pairwise constraints are constraints on the counts of two labels in a citation. We define z1(i, j) to be [[yk = i]] + X [[yk = j]] ykEy and z2(i, j) to be z2(i,j) = X [[yk = i]] − X [[yk = j]] ykEy ykEy We consider all constraints of the forms: z(i, j) &lt; 0, 1, 2, 3 and z(i, j) &gt; 0, 1, 2, 3. Note that some pairs of these constraints are redundant or logically inco</context>
<context position="26540" citStr="Chang et al., 2012" startWordPosition="4452" endWordPosition="4455">chine learning for citation extraction used Markov models with no global constraints. Hidden Markov models (HMMs), were originally employed for automatically extracting information from research papers on the CORA dataset (Seymore et al., 1999; Hetzner, 2008). Later, CRFs were shown to perform better on CORA, improving the results from the Hmm’s token-level F1 of 86.6 to 91.5 with a CRF(Peng and McCallum, 2004). Recent work on globally-constrained inference in citation extraction used an HMMCCM, which is an HMM with the addition of global features that are restricted to have positive weights (Chang et al., 2012). Approximate inference is performed using beam search. This method increased the HMM token-level accuracy from 86.69 to 93.92 on a test set of 100 citations from the CORA dataset. The global constraints added into the model are simply that each label only occurs once per citation. This approach is limited in its use of an HMM as an underlying model, as it has been shown that CRFs perform significantly better, achieving 95.37 token-level accuracy on CORA (Peng and McCallum, 2004). In our experiments, we demonstrate that the specific global constraints used by Chang et al. (2012) help on the UM</context>
<context position="31597" citStr="Chang et al. (2012)" startWordPosition="5301" endWordPosition="5304">s few as 2 dual decomposition iterations. However, this only amounts to 1.24 times as much work as running the baseline CRF on the dataset, since the constraints are satisfied immediately for many examples. In Figure 2 we consider two applications of our Soft-DD algorithm, and provide analysis in the caption. We train and evaluate on the UMass dataset instead of CORA, because it is significantly larger, has a useful finer-grained labeling schema, and its annotation is more consistent. We were able to obtain better performance on CORA using our baseline CRF than the HMMCCM results presented in Chang et al. (2012), which include soft constraints. Given this high performance of our base model on CORA, we did not apply our Soft-DD algorithm to the dataset. Furthermore, since the dataset is so small, learning the penalties for our large collection of constraints is difficult, and test set results are unreliable. Rather than compare our work to Chang et al. (2012) via results on CORA, we apply their constraints on the UMass data using Soft-DD and demonstrate accuracy gains, as discussed above. 7.1 Examples of learned constraints We now describe a number of the useful constraints that receive non-zero learn</context>
</contexts>
<marker>Chang, Ratinov, Roth, 2012</marker>
<rawString>M. Chang, L. Ratinov, and D. Roth. 2012. Structured learning with constrained conditional models. Machine Learning, 88(3):399–431, 6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Leong Chieu</author>
<author>Loo-Nin Teow</author>
</authors>
<title>Combining local and non-local information with dual decomposition for named entity recognition from text.</title>
<date>2012</date>
<contexts>
<context position="25273" citStr="Chieu and Teow, 2012" startWordPosition="4252" endWordPosition="4255">ctors, but performance would still have higher complexity than our method. While Gibbs sampling has been shown to work well tasks such as named entity recognition (Finkel et al., 2005), our previous experiments show that it does not work well for citation extraction, where it found only low-quality solutions in practice because the sampling did not mix well, even on a simple chain-structured CRF. Recently, dual decomposition has become a popular method for solving complex structured prediction problems in NLP (Koo et al., 2010; Rush et al., 2010; Rush and Collins, 2012; Paul and Eisner, 2012; Chieu and Teow, 2012). Soft constraints can be implemented inefficiently using hard constraints and dual decomposition— by introducing copies of output variables and an auxiliary graphical model, as in Rush et al. (2012). However, at every iteration of dual decomposition, MAP must be run in this auxiliary model. Furthermore the copying of variables doubles the number of iterations needed for information to flow between output variables, and thus slows convergence. On the other hand, our approach to soft constraints has identical per-iteration complexity as for hard constraints, and is a very easy modification to e</context>
</contexts>
<marker>Chieu, Teow, 2012</marker>
<rawString>Hai Leong Chieu and Loo-Nin Teow. 2012. Combining local and non-local information with dual decomposition for named entity recognition from text.</rawString>
</citation>
<citation valid="false">
<booktitle>In Information Fusion (FUSION), 2012 15th International Conference on,</booktitle>
<pages>231--238</pages>
<publisher>IEEE.</publisher>
<marker></marker>
<rawString>In Information Fusion (FUSION), 2012 15th International Conference on, pages 231–238. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="13055" citStr="Collins, 2002" startWordPosition="2188" endWordPosition="2189">s of soft-constraint dual decomposition to existing and new NLP problems. 3.1 Learning Penalties One consideration when using soft v.s. hard constraints is that soft constraints present a new training problem, since we need to choose the vector c, the penalties for violating the constraints. An important property of problem (5) in the previous section is that it corresponds to a structured linear model over y and z. Therefore, we can apply known training algorithms for estimating the parameters of structured linear models to choose c. All we need to employ the structured perceptron algorithm (Collins, 2002) or the structured SVM algorithm (Tsochantaridis et al., 2004) is a blackbox procedure for performing MAP inference in the structured linear model given an arbitrary cost vector. Fortunately, the MAP problem for (5) can be solved using Soft-DD, in Algorithm 2. Each penalty ci has to be non-negative; otherwise, the optimization problem in equation (5) is ill-defined. This can be ensured by simple modifications of the perceptron and subgradient descent optimization of the structured SVM objective simply by truncating c coordinate-wise to be non-negative at every learning iteration. Intuitively, </context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10, pages 1–8. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Corinna Cortes</author>
<author>Vladimir Vapnik</author>
</authors>
<title>Supportvector networks.</title>
<date>1995</date>
<booktitle>Machine learning,</booktitle>
<pages>20--3</pages>
<contexts>
<context position="10208" citStr="Cortes and Vapnik, 1995" startWordPosition="1656" endWordPosition="1659">straints. The algorithm has converged when each constraint is either satisfied by y(t) with equality or its corresponding component of A is 0, due to complimentary slackness (Boyd and Vandenberghe, 2004). 3 Soft Constraints in Dual Decomposition We now introduce an extension of Algorithm 1 to handle soft constraints. In our formulation, a soft-constrained model imposes a penalty for each unsatisfied constraint, proportional to the amount by which it is violated. Therefore, our derivation parallels how soft-margin SVMs are derived from hard-margin SVMs by introducing auxiliary slack variables (Cortes and Vapnik, 1995). Note that when performing MAP subject to soft constraints, optimal solutions might not satisfy some constraints, since doing so would reduce the model’s score by too much. Consider the optimization problems of the form: max. (w, y) − (c, z) s.t. y E U Ay − b &lt; z (5) −z &lt; 0, For positive ci, it is clear that an optimal zi will be equal to the degree to which aTi y &lt; bi is violated. Therefore, we pay a cost ci times the degree to which the ith constraint is violated, which mirrors how slack variables are used to represent the hinge loss for SVMs. Note that ci has to be positive, otherwise this</context>
</contexts>
<marker>Cortes, Vapnik, 1995</marker>
<rawString>Corinna Cortes and Vladimir Vapnik. 1995. Supportvector networks. Machine learning, 20(3):273–297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>363--370</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="24329" citStr="Finkel et al., 2005" startWordPosition="4098" endWordPosition="4101">ed on the predicted examples than on the ground truth, and hence that we might want to keep it. We also find that the constraints that have the largest imp values are semantically interesting. 6 Related Work There are multiple previous examples of augmenting chain-structured sequence models with terms capturing global relationships by expanding the chain to a more complex graphical model with non-local dependencies between the outputs. Inference in these models can be performed, for example, with loopy belief propagation (Bunescu and Mooney, 2004; Sutton and McCallum, 2004) or Gibbs sampling (Finkel et al., 2005). Belief propagation is prohibitively expensive in our Ed∈D[[maxywTd y]]c imp(c) = Ed∈D[[yd]] (10) C 598 model due to the high cardinalities of the output variables and of the global factors, which involve all output variables simultaneously. There are various methods for exploiting the combinatorial structure of these factors, but performance would still have higher complexity than our method. While Gibbs sampling has been shown to work well tasks such as named entity recognition (Finkel et al., 2005), our previous experiments show that it does not work well for citation extraction, where it </context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 363–370. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik Hetzner</author>
</authors>
<title>A simple method for citation metadata extraction using hidden markov models.</title>
<date>2008</date>
<booktitle>In Proceedings of the 8th ACM/IEEE-CS joint conference on Digital libraries,</booktitle>
<pages>280--284</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2076" citStr="Hetzner, 2008" startWordPosition="299" endWordPosition="300"> important because citation data is often provided only in plain text; however, having an accurate structured database of bibliographic information is necessary for many scientometric tasks, such as mapping scientific sub-communities, discovering research trends, and analyzing networks of researchers. Automated citation field extraction needs further research because it has not yet reached a level of accuracy at which it can be practically deployed in real-world systems. Hidden Markov models and linear-chain conditional random fields (CRFs) have previously been applied to citation extraction (Hetzner, 2008; Peng and McCallum, 2004) . These models support efficient dynamic-programming inference, but only model local dependencies in the output label sequence. However citations have strong global regularities not captured by these models. For example many book citations contain both an author section and an editor section, but none have two disjoint author sections. Since linear-chain models are unable to capture more than Markov dependencies, the models sometimes mislabel the editor as a second author. If we could enforce the global constraint that there should be only one author section, accurac</context>
<context position="26180" citStr="Hetzner, 2008" startWordPosition="4395" endWordPosition="4396">Furthermore the copying of variables doubles the number of iterations needed for information to flow between output variables, and thus slows convergence. On the other hand, our approach to soft constraints has identical per-iteration complexity as for hard constraints, and is a very easy modification to existing hard constraint code. Initial work in machine learning for citation extraction used Markov models with no global constraints. Hidden Markov models (HMMs), were originally employed for automatically extracting information from research papers on the CORA dataset (Seymore et al., 1999; Hetzner, 2008). Later, CRFs were shown to perform better on CORA, improving the results from the Hmm’s token-level F1 of 86.6 to 91.5 with a CRF(Peng and McCallum, 2004). Recent work on globally-constrained inference in citation extraction used an HMMCCM, which is an HMM with the addition of global features that are restricted to have positive weights (Chang et al., 2012). Approximate inference is performed using beam search. This method increased the HMM token-level accuracy from 86.69 to 93.92 on a test set of 100 citations from the CORA dataset. The global constraints added into the model are simply that</context>
</contexts>
<marker>Hetzner, 2008</marker>
<rawString>Erik Hetzner. 2008. A simple method for citation metadata extraction using hidden markov models. In Proceedings of the 8th ACM/IEEE-CS joint conference on Digital libraries, pages 280–284. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daphne Koller</author>
<author>Nir Friedman</author>
</authors>
<title>Probabilistic graphical models: principles and techniques.</title>
<date>2009</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="6264" citStr="Koller and Friedman, 2009" startWordPosition="960" endWordPosition="963"> the novel methods introduced here would easily generalize to many problems with global output regularities. 2 Background 2.1 Structured Linear Models The overall modeling technique we employ is to add soft constraints to a simple model for which we have an existing efficient prediction algorithm. For this underlying model, we employ a chainstructured conditional random field (CRF), since CRFs have been shown to perform better than other simple unconstrained models like hidden markov models for citation extraction (Peng and McCallum, 2004). We produce a prediction by performing MAP inference (Koller and Friedman, 2009). The MAP inference task in a CRF be can expressed as an optimization problem with a linear objective (Sontag, 2010; Sontag et al., 2011). Here, we define a binary indicator variable for each candidate setting of each factor in the graphical model. Each of these indicator variables is associated with the score that the factor takes on when it has the indictor variable’s corresponding value. Since the log probability of some y in the CRF is proportional to sum of the scores of all the factors, we can concatenate the indicator variables as a vector y and the scores as a vector w and write the MA</context>
</contexts>
<marker>Koller, Friedman, 2009</marker>
<rawString>Daphne Koller and Nir Friedman. 2009. Probabilistic graphical models: principles and techniques. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikos Komodakis</author>
<author>Nikos Paragios</author>
<author>Georgios Tziritas</author>
</authors>
<title>Mrf optimization via dual decomposition: Message-passing revisited.</title>
<date>2007</date>
<booktitle>In Computer Vision,</booktitle>
<pages>1--8</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="7938" citStr="Komodakis et al., 2007" startWordPosition="1251" endWordPosition="1254">g soft global constraints and for learning the penalties of these constraints can be applied to general structured linear models, not just CRFs, provided we have an available algorithm for performing MAP inference. 2.2 Dual Decomposition for Global Constraints In order to perform prediction subject to various global constraints, we may need to augment the problem (1) with additional constraints. Dual Decomposition is a popular method for performing MAP inference in this scenario, since it leverages known algorithms for MAP in the base problem where these extra constraints have not been added (Komodakis et al., 2007; Sontag et al., 2011; Rush and Collins, 2012). In this case, the MAP problem can be formulated as a structured linear model similar to equation (1), for which we have a MAP algorithm, but where we have imposed some additional constraints Ay &lt; b that no longer allow us to use the algorithm. In other s.t. y E U, 594 Algorithm 1 DD: projected subgradient for dual decomposition with hard constraints 1: while has not converged do (w + AT λ, y) 2: y(t) = argmaxy∈U � ] 3: λ(t) = Π0≤· λ(t−1) _ η(t)(Ay _ b) words, we consider the problem max. (w, y) s.t. y E U (2) Ay &lt; b, for an arbitrary matrix A and</context>
</contexts>
<marker>Komodakis, Paragios, Tziritas, 2007</marker>
<rawString>Nikos Komodakis, Nikos Paragios, and Georgios Tziritas. 2007. Mrf optimization via dual decomposition: Message-passing revisited. In Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on, pages 1–8. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Alexander M Rush</author>
<author>Michael Collins</author>
<author>Tommi Jaakkola</author>
<author>David Sontag</author>
</authors>
<title>Dual decomposition for parsing with non-projective head automata.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1288--1298</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="25184" citStr="Koo et al., 2010" startWordPosition="4236" endWordPosition="4239">ly. There are various methods for exploiting the combinatorial structure of these factors, but performance would still have higher complexity than our method. While Gibbs sampling has been shown to work well tasks such as named entity recognition (Finkel et al., 2005), our previous experiments show that it does not work well for citation extraction, where it found only low-quality solutions in practice because the sampling did not mix well, even on a simple chain-structured CRF. Recently, dual decomposition has become a popular method for solving complex structured prediction problems in NLP (Koo et al., 2010; Rush et al., 2010; Rush and Collins, 2012; Paul and Eisner, 2012; Chieu and Teow, 2012). Soft constraints can be implemented inefficiently using hard constraints and dual decomposition— by introducing copies of output variables and an auxiliary graphical model, as in Rush et al. (2012). However, at every iteration of dual decomposition, MAP must be run in this auxiliary model. Furthermore the copying of variables doubles the number of iterations needed for information to flow between output variables, and thus slows convergence. On the other hand, our approach to soft constraints has identic</context>
</contexts>
<marker>Koo, Rush, Collins, Jaakkola, Sontag, 2010</marker>
<rawString>Terry Koo, Alexander M Rush, Michael Collins, Tommi Jaakkola, and David Sontag. 2010. Dual decomposition for parsing with non-projective head automata. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1288–1298. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael J Paul</author>
<author>Jason Eisner</author>
</authors>
<title>Implicitly intersecting weighted automata using dual decomposition.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>232--242</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="25250" citStr="Paul and Eisner, 2012" startWordPosition="4248" endWordPosition="4251">l structure of these factors, but performance would still have higher complexity than our method. While Gibbs sampling has been shown to work well tasks such as named entity recognition (Finkel et al., 2005), our previous experiments show that it does not work well for citation extraction, where it found only low-quality solutions in practice because the sampling did not mix well, even on a simple chain-structured CRF. Recently, dual decomposition has become a popular method for solving complex structured prediction problems in NLP (Koo et al., 2010; Rush et al., 2010; Rush and Collins, 2012; Paul and Eisner, 2012; Chieu and Teow, 2012). Soft constraints can be implemented inefficiently using hard constraints and dual decomposition— by introducing copies of output variables and an auxiliary graphical model, as in Rush et al. (2012). However, at every iteration of dual decomposition, MAP must be run in this auxiliary model. Furthermore the copying of variables doubles the number of iterations needed for information to flow between output variables, and thus slows convergence. On the other hand, our approach to soft constraints has identical per-iteration complexity as for hard constraints, and is a very</context>
</contexts>
<marker>Paul, Eisner, 2012</marker>
<rawString>Michael J Paul and Jason Eisner. 2012. Implicitly intersecting weighted automata using dual decomposition. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 232–242. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fuchun Peng</author>
<author>Andrew McCallum</author>
</authors>
<title>Accurate information extraction from research papers using conditional random fields.</title>
<date>2004</date>
<booktitle>In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Main Proceedings,</booktitle>
<volume>2</volume>
<pages>329--336</pages>
<location>Boston, Massachusetts, USA,</location>
<contexts>
<context position="2102" citStr="Peng and McCallum, 2004" startWordPosition="301" endWordPosition="304">use citation data is often provided only in plain text; however, having an accurate structured database of bibliographic information is necessary for many scientometric tasks, such as mapping scientific sub-communities, discovering research trends, and analyzing networks of researchers. Automated citation field extraction needs further research because it has not yet reached a level of accuracy at which it can be practically deployed in real-world systems. Hidden Markov models and linear-chain conditional random fields (CRFs) have previously been applied to citation extraction (Hetzner, 2008; Peng and McCallum, 2004) . These models support efficient dynamic-programming inference, but only model local dependencies in the output label sequence. However citations have strong global regularities not captured by these models. For example many book citations contain both an author section and an editor section, but none have two disjoint author sections. Since linear-chain models are unable to capture more than Markov dependencies, the models sometimes mislabel the editor as a second author. If we could enforce the global constraint that there should be only one author section, accuracy could be improved. One f</context>
<context position="6183" citStr="Peng and McCallum, 2004" startWordPosition="948" endWordPosition="951">cial. While this paper focusses on an application to citation field extraction, the novel methods introduced here would easily generalize to many problems with global output regularities. 2 Background 2.1 Structured Linear Models The overall modeling technique we employ is to add soft constraints to a simple model for which we have an existing efficient prediction algorithm. For this underlying model, we employ a chainstructured conditional random field (CRF), since CRFs have been shown to perform better than other simple unconstrained models like hidden markov models for citation extraction (Peng and McCallum, 2004). We produce a prediction by performing MAP inference (Koller and Friedman, 2009). The MAP inference task in a CRF be can expressed as an optimization problem with a linear objective (Sontag, 2010; Sontag et al., 2011). Here, we define a binary indicator variable for each candidate setting of each factor in the graphical model. Each of these indicator variables is associated with the score that the factor takes on when it has the indictor variable’s corresponding value. Since the log probability of some y in the CRF is proportional to sum of the scores of all the factors, we can concatenate th</context>
<context position="26335" citStr="Peng and McCallum, 2004" startWordPosition="4420" endWordPosition="4423">vergence. On the other hand, our approach to soft constraints has identical per-iteration complexity as for hard constraints, and is a very easy modification to existing hard constraint code. Initial work in machine learning for citation extraction used Markov models with no global constraints. Hidden Markov models (HMMs), were originally employed for automatically extracting information from research papers on the CORA dataset (Seymore et al., 1999; Hetzner, 2008). Later, CRFs were shown to perform better on CORA, improving the results from the Hmm’s token-level F1 of 86.6 to 91.5 with a CRF(Peng and McCallum, 2004). Recent work on globally-constrained inference in citation extraction used an HMMCCM, which is an HMM with the addition of global features that are restricted to have positive weights (Chang et al., 2012). Approximate inference is performed using beam search. This method increased the HMM token-level accuracy from 86.69 to 93.92 on a test set of 100 citations from the CORA dataset. The global constraints added into the model are simply that each label only occurs once per citation. This approach is limited in its use of an HMM as an underlying model, as it has been shown that CRFs perform sig</context>
</contexts>
<marker>Peng, McCallum, 2004</marker>
<rawString>Fuchun Peng and Andrew McCallum. 2004. Accurate information extraction from research papers using conditional random fields. In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLT-NAACL 2004: Main Proceedings, pages 329–336, Boston, Massachusetts, USA, May 2 - May 7. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
</authors>
<title>A linear programming formulation for global inference in natural language tasks. Defense Technical Information Center.</title>
<date>2004</date>
<contexts>
<context position="3204" citStr="Roth and Yih, 2004" startWordPosition="478" endWordPosition="481">e could enforce the global constraint that there should be only one author section, accuracy could be improved. One framework for adding such global constraints into tractable models is constrained inference, in which at inference time the original model is augmented with restrictions on the outputs such that they obey certain global regularities. When hard constraints can be encoded as linear equations on the output variables, and the underlying model’s inference task can be posed as linear optimization, one can formulate this constrained inference problem as an integer linear program (ILP) (Roth and Yih, 2004). Alternatively, one can employ dual decomposition (Rush et al., 2010). Dual decompositions’s advantage over ILP is is that it can leverage existing inference algorithms for the original model as a black box. Such a modular algorithm is easy to implement, and works quite well in practice, providing certificates of optimality for most examples. The above two approaches have previously been applied to impose hard constraints on a model’s output. On the other hand, recent work has demonstrated improvements in citation field extraction by imposing soft constraints (Chang et al., 2012). Here, the m</context>
</contexts>
<marker>Roth, Yih, 2004</marker>
<rawString>Dan Roth and Wen-tau Yih. 2004. A linear programming formulation for global inference in natural language tasks. Defense Technical Information Center.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander M Rush</author>
<author>Michael Collins</author>
</authors>
<title>A tutorial on dual decomposition and lagrangian relaxation for inference in natural language processing.</title>
<date>2012</date>
<journal>J. Artif. Intell. Res. (JAIR),</journal>
<pages>45--305</pages>
<contexts>
<context position="7984" citStr="Rush and Collins, 2012" startWordPosition="1259" endWordPosition="1262">e penalties of these constraints can be applied to general structured linear models, not just CRFs, provided we have an available algorithm for performing MAP inference. 2.2 Dual Decomposition for Global Constraints In order to perform prediction subject to various global constraints, we may need to augment the problem (1) with additional constraints. Dual Decomposition is a popular method for performing MAP inference in this scenario, since it leverages known algorithms for MAP in the base problem where these extra constraints have not been added (Komodakis et al., 2007; Sontag et al., 2011; Rush and Collins, 2012). In this case, the MAP problem can be formulated as a structured linear model similar to equation (1), for which we have a MAP algorithm, but where we have imposed some additional constraints Ay &lt; b that no longer allow us to use the algorithm. In other s.t. y E U, 594 Algorithm 1 DD: projected subgradient for dual decomposition with hard constraints 1: while has not converged do (w + AT λ, y) 2: y(t) = argmaxy∈U � ] 3: λ(t) = Π0≤· λ(t−1) _ η(t)(Ay _ b) words, we consider the problem max. (w, y) s.t. y E U (2) Ay &lt; b, for an arbitrary matrix A and vector b. We can write the Lagrangian of this</context>
<context position="25227" citStr="Rush and Collins, 2012" startWordPosition="4244" endWordPosition="4247">loiting the combinatorial structure of these factors, but performance would still have higher complexity than our method. While Gibbs sampling has been shown to work well tasks such as named entity recognition (Finkel et al., 2005), our previous experiments show that it does not work well for citation extraction, where it found only low-quality solutions in practice because the sampling did not mix well, even on a simple chain-structured CRF. Recently, dual decomposition has become a popular method for solving complex structured prediction problems in NLP (Koo et al., 2010; Rush et al., 2010; Rush and Collins, 2012; Paul and Eisner, 2012; Chieu and Teow, 2012). Soft constraints can be implemented inefficiently using hard constraints and dual decomposition— by introducing copies of output variables and an auxiliary graphical model, as in Rush et al. (2012). However, at every iteration of dual decomposition, MAP must be run in this auxiliary model. Furthermore the copying of variables doubles the number of iterations needed for information to flow between output variables, and thus slows convergence. On the other hand, our approach to soft constraints has identical per-iteration complexity as for hard con</context>
</contexts>
<marker>Rush, Collins, 2012</marker>
<rawString>Alexander M. Rush and Michael Collins. 2012. A tutorial on dual decomposition and lagrangian relaxation for inference in natural language processing. J. Artif. Intell. Res. (JAIR), 45:305–362.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander M Rush</author>
<author>David Sontag</author>
<author>Michael Collins</author>
<author>Tommi Jaakkola</author>
</authors>
<title>On dual decomposition and linear programming relaxations for natural language processing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--11</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3274" citStr="Rush et al., 2010" startWordPosition="489" endWordPosition="492">hor section, accuracy could be improved. One framework for adding such global constraints into tractable models is constrained inference, in which at inference time the original model is augmented with restrictions on the outputs such that they obey certain global regularities. When hard constraints can be encoded as linear equations on the output variables, and the underlying model’s inference task can be posed as linear optimization, one can formulate this constrained inference problem as an integer linear program (ILP) (Roth and Yih, 2004). Alternatively, one can employ dual decomposition (Rush et al., 2010). Dual decompositions’s advantage over ILP is is that it can leverage existing inference algorithms for the original model as a black box. Such a modular algorithm is easy to implement, and works quite well in practice, providing certificates of optimality for most examples. The above two approaches have previously been applied to impose hard constraints on a model’s output. On the other hand, recent work has demonstrated improvements in citation field extraction by imposing soft constraints (Chang et al., 2012). Here, the model is not required obey the global constraints, but merely pays a pe</context>
<context position="25203" citStr="Rush et al., 2010" startWordPosition="4240" endWordPosition="4243">ous methods for exploiting the combinatorial structure of these factors, but performance would still have higher complexity than our method. While Gibbs sampling has been shown to work well tasks such as named entity recognition (Finkel et al., 2005), our previous experiments show that it does not work well for citation extraction, where it found only low-quality solutions in practice because the sampling did not mix well, even on a simple chain-structured CRF. Recently, dual decomposition has become a popular method for solving complex structured prediction problems in NLP (Koo et al., 2010; Rush et al., 2010; Rush and Collins, 2012; Paul and Eisner, 2012; Chieu and Teow, 2012). Soft constraints can be implemented inefficiently using hard constraints and dual decomposition— by introducing copies of output variables and an auxiliary graphical model, as in Rush et al. (2012). However, at every iteration of dual decomposition, MAP must be run in this auxiliary model. Furthermore the copying of variables doubles the number of iterations needed for information to flow between output variables, and thus slows convergence. On the other hand, our approach to soft constraints has identical per-iteration co</context>
</contexts>
<marker>Rush, Sontag, Collins, Jaakkola, 2010</marker>
<rawString>Alexander M Rush, David Sontag, Michael Collins, and Tommi Jaakkola. 2010. On dual decomposition and linear programming relaxations for natural language processing. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1–11. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander M Rush</author>
<author>Roi Reichart</author>
<author>Michael Collins</author>
<author>Amir Globerson</author>
</authors>
<title>Improved parsing and pos tagging using inter-sentence consistency constraints.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1434--1444</pages>
<contexts>
<context position="25472" citStr="Rush et al. (2012)" startWordPosition="4283" endWordPosition="4286">eriments show that it does not work well for citation extraction, where it found only low-quality solutions in practice because the sampling did not mix well, even on a simple chain-structured CRF. Recently, dual decomposition has become a popular method for solving complex structured prediction problems in NLP (Koo et al., 2010; Rush et al., 2010; Rush and Collins, 2012; Paul and Eisner, 2012; Chieu and Teow, 2012). Soft constraints can be implemented inefficiently using hard constraints and dual decomposition— by introducing copies of output variables and an auxiliary graphical model, as in Rush et al. (2012). However, at every iteration of dual decomposition, MAP must be run in this auxiliary model. Furthermore the copying of variables doubles the number of iterations needed for information to flow between output variables, and thus slows convergence. On the other hand, our approach to soft constraints has identical per-iteration complexity as for hard constraints, and is a very easy modification to existing hard constraint code. Initial work in machine learning for citation extraction used Markov models with no global constraints. Hidden Markov models (HMMs), were originally employed for automat</context>
</contexts>
<marker>Rush, Reichart, Collins, Globerson, 2012</marker>
<rawString>Alexander M Rush, Roi Reichart, Michael Collins, and Amir Globerson. 2012. Improved parsing and pos tagging using inter-sentence consistency constraints. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1434–1444.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristie Seymore</author>
<author>Andrew McCallum</author>
<author>Roni Rosenfeld</author>
</authors>
<title>Learning hidden markov model structure for information extraction.</title>
<date>1999</date>
<booktitle>In AAAI-99 Workshop on Machine Learning for Information Extraction,</booktitle>
<pages>37--42</pages>
<contexts>
<context position="26164" citStr="Seymore et al., 1999" startWordPosition="4391" endWordPosition="4394">this auxiliary model. Furthermore the copying of variables doubles the number of iterations needed for information to flow between output variables, and thus slows convergence. On the other hand, our approach to soft constraints has identical per-iteration complexity as for hard constraints, and is a very easy modification to existing hard constraint code. Initial work in machine learning for citation extraction used Markov models with no global constraints. Hidden Markov models (HMMs), were originally employed for automatically extracting information from research papers on the CORA dataset (Seymore et al., 1999; Hetzner, 2008). Later, CRFs were shown to perform better on CORA, improving the results from the Hmm’s token-level F1 of 86.6 to 91.5 with a CRF(Peng and McCallum, 2004). Recent work on globally-constrained inference in citation extraction used an HMMCCM, which is an HMM with the addition of global features that are restricted to have positive weights (Chang et al., 2012). Approximate inference is performed using beam search. This method increased the HMM token-level accuracy from 86.69 to 93.92 on a test set of 100 citations from the CORA dataset. The global constraints added into the model</context>
</contexts>
<marker>Seymore, McCallum, Rosenfeld, 1999</marker>
<rawString>Kristie Seymore, Andrew McCallum, Roni Rosenfeld, et al. 1999. Learning hidden markov model structure for information extraction. In AAAI-99 Workshop on Machine Learning for Information Extraction, pages 37–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Sontag</author>
<author>Amir Globerson</author>
<author>Tommi Jaakkola</author>
</authors>
<title>Introduction to dual decomposition for inference.</title>
<date>2011</date>
<booktitle>Optimization for Machine Learning.</booktitle>
<editor>In Suvrit Sra, Sebastian Nowozin, and Stephen J. Wright, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="6401" citStr="Sontag et al., 2011" startWordPosition="986" endWordPosition="989"> Models The overall modeling technique we employ is to add soft constraints to a simple model for which we have an existing efficient prediction algorithm. For this underlying model, we employ a chainstructured conditional random field (CRF), since CRFs have been shown to perform better than other simple unconstrained models like hidden markov models for citation extraction (Peng and McCallum, 2004). We produce a prediction by performing MAP inference (Koller and Friedman, 2009). The MAP inference task in a CRF be can expressed as an optimization problem with a linear objective (Sontag, 2010; Sontag et al., 2011). Here, we define a binary indicator variable for each candidate setting of each factor in the graphical model. Each of these indicator variables is associated with the score that the factor takes on when it has the indictor variable’s corresponding value. Since the log probability of some y in the CRF is proportional to sum of the scores of all the factors, we can concatenate the indicator variables as a vector y and the scores as a vector w and write the MAP problem as max. (w, y) (1) where the set U represents the set of valid configurations of the indicator variables. Here, the constraints</context>
<context position="7959" citStr="Sontag et al., 2011" startWordPosition="1255" endWordPosition="1258">s and for learning the penalties of these constraints can be applied to general structured linear models, not just CRFs, provided we have an available algorithm for performing MAP inference. 2.2 Dual Decomposition for Global Constraints In order to perform prediction subject to various global constraints, we may need to augment the problem (1) with additional constraints. Dual Decomposition is a popular method for performing MAP inference in this scenario, since it leverages known algorithms for MAP in the base problem where these extra constraints have not been added (Komodakis et al., 2007; Sontag et al., 2011; Rush and Collins, 2012). In this case, the MAP problem can be formulated as a structured linear model similar to equation (1), for which we have a MAP algorithm, but where we have imposed some additional constraints Ay &lt; b that no longer allow us to use the algorithm. In other s.t. y E U, 594 Algorithm 1 DD: projected subgradient for dual decomposition with hard constraints 1: while has not converged do (w + AT λ, y) 2: y(t) = argmaxy∈U � ] 3: λ(t) = Π0≤· λ(t−1) _ η(t)(Ay _ b) words, we consider the problem max. (w, y) s.t. y E U (2) Ay &lt; b, for an arbitrary matrix A and vector b. We can wri</context>
</contexts>
<marker>Sontag, Globerson, Jaakkola, 2011</marker>
<rawString>David Sontag, Amir Globerson, and Tommi Jaakkola. 2011. Introduction to dual decomposition for inference. In Suvrit Sra, Sebastian Nowozin, and Stephen J. Wright, editors, Optimization for Machine Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Sontag</author>
</authors>
<title>Approximate Inference in Graphical Models using LP Relaxations.</title>
<date>2010</date>
<tech>Ph.D. thesis,</tech>
<institution>Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science.</institution>
<contexts>
<context position="6379" citStr="Sontag, 2010" startWordPosition="984" endWordPosition="985">uctured Linear Models The overall modeling technique we employ is to add soft constraints to a simple model for which we have an existing efficient prediction algorithm. For this underlying model, we employ a chainstructured conditional random field (CRF), since CRFs have been shown to perform better than other simple unconstrained models like hidden markov models for citation extraction (Peng and McCallum, 2004). We produce a prediction by performing MAP inference (Koller and Friedman, 2009). The MAP inference task in a CRF be can expressed as an optimization problem with a linear objective (Sontag, 2010; Sontag et al., 2011). Here, we define a binary indicator variable for each candidate setting of each factor in the graphical model. Each of these indicator variables is associated with the score that the factor takes on when it has the indictor variable’s corresponding value. Since the log probability of some y in the CRF is proportional to sum of the scores of all the factors, we can concatenate the indicator variables as a vector y and the scores as a vector w and write the MAP problem as max. (w, y) (1) where the set U represents the set of valid configurations of the indicator variables.</context>
</contexts>
<marker>Sontag, 2010</marker>
<rawString>David Sontag. 2010. Approximate Inference in Graphical Models using LP Relaxations. Ph.D. thesis, Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Sutton</author>
<author>Andrew McCallum</author>
</authors>
<title>Collective segmentation and labeling of distant entities in information extraction.</title>
<date>2004</date>
<tech>Technical report, DTIC Document.</tech>
<contexts>
<context position="24289" citStr="Sutton and McCallum, 2004" startWordPosition="4091" endWordPosition="4094">e 1 implies that the constraint is more violated on the predicted examples than on the ground truth, and hence that we might want to keep it. We also find that the constraints that have the largest imp values are semantically interesting. 6 Related Work There are multiple previous examples of augmenting chain-structured sequence models with terms capturing global relationships by expanding the chain to a more complex graphical model with non-local dependencies between the outputs. Inference in these models can be performed, for example, with loopy belief propagation (Bunescu and Mooney, 2004; Sutton and McCallum, 2004) or Gibbs sampling (Finkel et al., 2005). Belief propagation is prohibitively expensive in our Ed∈D[[maxywTd y]]c imp(c) = Ed∈D[[yd]] (10) C 598 model due to the high cardinalities of the output variables and of the global factors, which involve all output variables simultaneously. There are various methods for exploiting the combinatorial structure of these factors, but performance would still have higher complexity than our method. While Gibbs sampling has been shown to work well tasks such as named entity recognition (Finkel et al., 2005), our previous experiments show that it does not work</context>
</contexts>
<marker>Sutton, McCallum, 2004</marker>
<rawString>Charles Sutton and Andrew McCallum. 2004. Collective segmentation and labeling of distant entities in information extraction. Technical report, DTIC Document.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis Tsochantaridis</author>
<author>Thomas Hofmann</author>
<author>Thorsten Joachims</author>
<author>Yasemin Altun</author>
</authors>
<title>Support vector machine learning for interdependent and structured output spaces.</title>
<date>2004</date>
<booktitle>In Proceedings of the twenty-first international conference on Machine learning,</booktitle>
<pages>104</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="13117" citStr="Tsochantaridis et al., 2004" startWordPosition="2195" endWordPosition="2198">ing and new NLP problems. 3.1 Learning Penalties One consideration when using soft v.s. hard constraints is that soft constraints present a new training problem, since we need to choose the vector c, the penalties for violating the constraints. An important property of problem (5) in the previous section is that it corresponds to a structured linear model over y and z. Therefore, we can apply known training algorithms for estimating the parameters of structured linear models to choose c. All we need to employ the structured perceptron algorithm (Collins, 2002) or the structured SVM algorithm (Tsochantaridis et al., 2004) is a blackbox procedure for performing MAP inference in the structured linear model given an arbitrary cost vector. Fortunately, the MAP problem for (5) can be solved using Soft-DD, in Algorithm 2. Each penalty ci has to be non-negative; otherwise, the optimization problem in equation (5) is ill-defined. This can be ensured by simple modifications of the perceptron and subgradient descent optimization of the structured SVM objective simply by truncating c coordinate-wise to be non-negative at every learning iteration. Intuitively, the perceptron update increases the penalty for a constraint i</context>
</contexts>
<marker>Tsochantaridis, Hofmann, Joachims, Altun, 2004</marker>
<rawString>Ioannis Tsochantaridis, Thomas Hofmann, Thorsten Joachims, and Yasemin Altun. 2004. Support vector machine learning for interdependent and structured output spaces. In Proceedings of the twenty-first international conference on Machine learning, page 104. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>