<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.324783">
<title confidence="0.942088">
Tools for hierarchical annotation of typed dialogue
</title>
<author confidence="0.988348">
Myroslava O. Dzikovska, Charles Callaway, Elaine Farrow
</author>
<affiliation confidence="0.9065245">
Human Communication Research Centre, University of Edinburgh
2 Buccleuch Place, Edinburgh, EH8 9LW, United Kingdom,
</affiliation>
<email confidence="0.999038">
{mdzikovs,ccallawa,efarrow}@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.999862" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999894333333333">
We discuss a set of tools for annotating a complex
hierarchical and linguistic structure of tutorial di-
alogue based on the NITE XML Toolkit (NXT)
(Carletta et al., 2003). The NXT API supports
multi-layered stand-off data annotation and syn-
chronisation with timed and speech data. Using
NXT, we built a set of extensible tools for de-
tailed structure annotation of typed tutorial dia-
logue, collected from a tutor and student typing
via a chat interface. There are several corpora of
tutoring done with such chat-style communication
techniques (Shah et al., 2002; Jordan and Siler,
2002), however, our annotation presents a special
problem because of its detailed hierarchical struc-
ture. We applied our annotation methodology to
annotating corpora in two different tutoring do-
mains: basic electricity and electronics, and sym-
bolic differentiation.
</bodyText>
<sectionHeader confidence="0.986557" genericHeader="method">
2 Data Structures
</sectionHeader>
<bodyText confidence="0.9993293">
Our corpus has two sources of overlapping anno-
tations: the turn structure of the corpus and situ-
ational factors annotation. The data are naturally
split into turns whenever a participant presses their
“submit” button. Timing information is associated
with individual turns, representing the time when
the entire message was sent to the other partici-
pant, rather than with individual words and sounds
as it would be in spoken corpora.
However, turns are too large to be used as units
in the annotation for dialogue phenomena. For
example, the single turn “Well done. Let’s try a
harder one.” consists of two utterances making
different dialogue contributions: positive tutorial
feedback for the previous student utterance and a
statement of a new tutorial goal. Thus, turns must
be segmented into smaller units which can serve
as a basis for dialogue annotation. We call these
utterances by analogy with spoken language, be-
cause they are often fragments such as “well done”
rather than complete sentences.
Thus, the corpus has two inherently overlap-
ping layers: the turn segmentation layer, grouping
utterances into turns, and the dialogue structure
layer built up over individual utterances. The NXT
toolkit supports such overlapping annotations, and
we built two individual tools to support corpus an-
notation: an utterance segmentation tool and a tu-
torial annotation tool.
Additionally, the corpus contains annotation
done by the tutor herself at collection time which
we call “situational factors”. The tutors were
asked to submit a set of these factors after each
turn describing the progress and state of the stu-
dent, such as answer correctness, confidence and
engagement. The factors were submitted sepa-
rately from dialogue contributions and provide an-
other layer of dialogue annotation which has to
be coordinated with other annotations. The fac-
tors are typically related to the preceding student’s
utterance, but the link is implicit in the submis-
sion time.1 Currently we include the factors in the
tool’s transcript display based on the submission
time, so they are displayed after the appropriate
turn in the transcript allowing the annotators to vi-
sually synchronise them with the dialogue. We
also provide an option to annotators for making
them visible or not. In the future we plan to make
factors a separate layer of the annotation linked by
pointers with the preceding student and tutor turns.
</bodyText>
<footnote confidence="0.996630333333333">
1The factor interface was designed to be quick to use and
minimally impact the dialogue flow, so the submission tim-
ings are generally reliable.
</footnote>
<page confidence="0.999465">
57
</page>
<sectionHeader confidence="0.99097" genericHeader="method">
3 Utterance Segmentation
</sectionHeader>
<bodyText confidence="0.9999948">
We process the raw data with an automatic seg-
menter/tokenizer which subdivides turns into indi-
vidual utterances, and utterances into tokens, pro-
viding an initial segmentation for the annotation.
However, perfect automatic segmentation is not
possible, because punctuation is often either in-
consistent or missing in typed dialogue and this
task therefore requires human judgement. The
output of our automatic segmentation algorithm
was verified and corrected by a human annotator.
A screen-shot of the interface we developed for
segmentation verification is displayed in Figure 1.
With the aid of this tool, it took 6 person-hours
to check and correct the automatically segmented
utterances for the 18 dialogues in our corpus.
</bodyText>
<sectionHeader confidence="0.997406" genericHeader="method">
4 Tutorial Annotation
</sectionHeader>
<bodyText confidence="0.999964508474577">
To provide a detailed analysis of tutorial dialogue
and remediation strategies, we employ a hierarchi-
cal annotation scheme which encodes the recur-
sive dialogue structure. Each tutorial session con-
sists of a sequence of tasks, which may be either
teaching specific domain concepts or doing indi-
vidual exercises. Each task’s structure includes
one or more of the following: giving definitions,
formulating a question, obtaining the student an-
swer and remediation by the tutor.
Generally speaking, the structure of tutorial di-
alogue is governed by the task structure just as in
task-oriented dialogue (Grosz and Sidner, 1986).
However, the specific annotation structure differs
depending on the tutoring method. In our basic
electricity and electronics domain, a tutorial ses-
sion consists of a set of “teach” segments, and
within each segment a number of “task” segments.
Task segments usually contain exercises in which
the student is asked a question requiring a simple
(one- or two-line) answer, which may be followed
by a long remediation segment to address the con-
ceptual problems revealed by the answer.
In contrast, in our calculus domain the students
have to do multi-step procedures to differentiate
complex math expressions, but most of the reme-
diations are very short, fixing the immediate prob-
lem and letting the student continue on with the
procedure. Thus even though the dialogue is hier-
archically structured in both cases, the annotation
schemes differ depending on the domain. We de-
veloped a generic tool for annotating hierarchical
dialogue structure which can be configured with
the specific annotation scheme.
The tool interface (Figure 2) consists of a tran-
script of a session and a linked tree representation.
Individual utterances displayed in the transcript
are leaves of the tree. It is not possible to display
them as tree leaves directly as would be done in
syntactic trees, because they are too large to fit in
graphical tree display. Instead, a segment is high-
lighted in a transcript whenever it is selected in the
tutorial structure, and a hotkey is provided to ex-
pand the tree to see all annotations of a particular
utterance in the transcript.
The hierarchical tree structure is supported by a
schema which describes the annotations possible
on each hierarchical tree level. Since the multi-
layered annotation scheme is quite complex, the
tool uses the annotation schema to limit the num-
ber of codes presented to the annotator to be only
those consistent with the tree level. For exam-
ple, in our basic electricity domain annotation de-
scribed above, there are about 20 codes at different
level, but an annotator will only have “teach” as an
option for assigning a code to a top tree level, and
only “task” and “test” (with appropriate subtypes)
for assigning codes immediately below the teach
level, based on the schema defined for the domain.
</bodyText>
<sectionHeader confidence="0.995384" genericHeader="method">
5 Transcript Segmentation
</sectionHeader>
<bodyText confidence="0.999975722222222">
We had to conduct several simpler data analy-
ses where the utterances in the transcript are seg-
mented according to their purpose. For exam-
ple, in tutorial differentiation the dialogue con-
centrates on 4 main purposes: general discussion,
introducing problems, performing differentiation
proper, or doing algebraic transformations to sim-
plify the resulting expressions. In another analysis
we needed to mark the segments where the student
was making errors and the nature of those errors.
We developed a generic annotation tool to sup-
port such segmentation annotation over the utter-
ance layer. The tool is configured with the name
of the segment tag and colours indicating different
segment types. The annotator can enter a segment
type, and use a freetext field for other information.
A screenshot of the annotation tool with utterance
purposes marked is given in Figure 3.
</bodyText>
<sectionHeader confidence="0.994302" genericHeader="method">
6 Data Analysis
</sectionHeader>
<bodyText confidence="0.999503333333333">
The NITE query language (NQL) enables us to ac-
cess the data as a directed acyclic graph to cor-
relate simple annotations, such as finding out the
</bodyText>
<page confidence="0.994972">
58
</page>
<table confidence="0.996335833333334">
BEE Utterance Splitter: pc-059-1 D
File Search Edit Utterance Options
Transcript Display ro 1.7&apos; i Utterance Display iiiii,......., rt
- -.
token: of
&apos;Search for ID Replay] E factors ii, studentactions E slides I Join m Split II Edit I Join Tokens II Split Token II Edit Token I
utt229: tutor: Perfect! what? token: Happily [original: Happliy 1
utt230a: will punc: -
permutation of this exorcise- token: I
token: think
token: is
t
token: permutation
t t
token: exercise
:
ipunc -
token: this
token: he
token: last
oken: his
utt230ba: tutor: I think that you&apos;ll get to solve the
Christmas tree light problem shortly
utt230bb: tutor: and then the lesson is over
sm87: 19:2945 student:
utt231: student: ok
sm88: 19:3022 student:
utt232: student: voltage is the difference in
electrical states?
tm81: 19 30:34 tutor:
utt233a: tutor: half way-
utt233b: tutor: difference in electrical states of
tm82: 1930:45 tutor:
utt235: tutor: Got it!
sm89: 19:3045 student:
utt234: student: two terminals
sm90: 19:32:39 student:
utt236: student: Yes,
utt237: student: because the gap in the circuit
cause it to have a difference in voltage
sm91: 19:33:17 student:
utt238a: student: I am not really sure though,
!._ ... .• _ • . • . _ • , . _ •
Editable notes for nn80
Previous
Nest
Clear
Figure 1: Utterance Segmentation Tool.
BEE Tutorial Annotator gc-rtil-1 X
File Search Edit Options
Ifl SegmentS o&apos;ial 9 Transcript Display
Search for ID Replay e&apos; factors v studentactions i., slides
_Edit Make Super Okla, h Rdryd Duni. ate : Delete ,lReplay
Li ..111.1111......11.,114) ..114. I Id, U LI. i_unibieie ....
? il subplan(map_concept, tsar&apos; Initiate end: Complete
,:.-ligIve(map_concept, ) start: Initiate end Complete
a- il irrelevant( ) start: Initiate end: Complete
0-11 elirtl(map_concept, )Sar&apos; Initiate end: Complete
a- il review(nut s115]) slag: Initiate end Conbolete
a- il review(nut s116]) slag: Initiate end Complete
a- il give(map_concepti, answer(s116)D start Initiate end Complete
o il remediation(express_confusion, ) start: Initiate end: Complete
se Il give(expression_affectioncertablv]) start: Initiate end: Complete
irrelevant(noll) start: Initiate end Complete
o il giveCiustricationi, utt2L ) start Initiate end: Complete
D assert(rephraseL s116]) start: Initiate end: Complete
.- il give(map_conceptl, answerts116)1) start Initiate end Complete
.- il irrelevant(noll) start: Initiate end Complete
.- il remediation(reprhase_questionidiftwordsL )oar&apos; Initiate end Complet
a- il remediation(reprhase_questioniadd_ohoices], ) start: Initiate end: Comp!!
.- il subplan(predict, ) start Initiate end Abandon
.- il subplan(verify, ) start
dict, ) Initiate end Dend Comp efer
subplan(pre start Reinitiate lete
se Il subplan(veriN )0tart Reinitiate end: Complete
o- il subplan(discuss) start: Initiate end: Complete
D unannetated(s157) start: Initiate end: Complete
c,-Ilelic11(verification) start: Initiate end: Complete
.- il give(verification) start Inate end Complete
0- ligIve(feetlback_local[correct Door&apos; Initiate end: Complete
.- il give(expression_affectisatisfaction]) start Initiate end: Complete —
oi I IL. 11_7.‘F.J..l., 7C-C.FUEITL
utt2: student: 1 2 3
• • TI ll:&apos; 47:2 fl,t.or:
utt3:
asa7: 18:45:31 tutor:
si6: 18:45:31 tutor:
fac34 Factor: apt de 3 (not_changed)
: itu
:
fac35 Factor: some_error 0 (changed)
fac36: Factor: needtomoveon 1 (not_changed)
fac37: Factor: engagement 4 (not_changed)
fac38: Factor: missing_part 0 (changed)
fac39: Factor: confidence 4 (changed)
fac40: Factor: tutor_comment (not_changed)
fac41: Factor: correctness other (changed)
slm19: 18:45:39 slide:
s119: Loaded slide: lesson1middle/img14 of type PREDICT
sm3: 18:4552 student:
utt4: student: it asks me to choose
which ones are closed
5m4: 18:45:54 student:
Utt5: student: 1 2 4
SM5: 1845:56 student:
utt6: student: i mean
sm6: 1845:58 student:
utt7: student: 1 2 3
slm20: 18:46:08 slide:
s120: Loaded slide: lesson1middle/img15 of type PREDICT
tm3: 18:46:09 tutor:
gc-m1-1.tutor.tokenization.1: tutor: not exactly-
gc-m1-1.tutor.tokenization.2: tutor: it asked what components
were in a closed path in each diagram
iii I I 01
Ennoble notes for: ge-m1-1.tutorlal_strategles.693
Previous
N ext
Clear
J
</table>
<figureCaption confidence="0.942367">
Figure 2: Tutorial Strategy Annotation Tool.
</figureCaption>
<page confidence="0.989944">
59
</page>
<figureCaption confidence="0.999689">
Figure 3: Segmentation tool. The segment labels are shown on the left.
</figureCaption>
<bodyText confidence="0.999927470588235">
number of turns which contain only mathematical
expressions but no words. We use the NITE query
interface for simpler analysis tasks such as finding
all instances of specific tags and tag combinations.
However, we found the query language less use-
ful for coordinating the situational factors anno-
tated by tutors with other annotation. Each set of
factors submitted is normally associated with the
first student turn which precedes it, but the factors
were not linked to student utterances explicitly.
NQL does not have a “direct precedence” opera-
tor.2 Thus it is easier derive this information using
the JAVA API. To make the data analysis simpler,
we are planning to add a pointer layer, generated
automatically based on timing information, which
will use explicit pointers between the factor sub-
missions and preceding tutor and student turns.
</bodyText>
<sectionHeader confidence="0.999584" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.826828666666667">
We presented a set of tools for hierarchically an-
notating dialogue structure, suitable for annotating
typed dialogue. The turns in these dialogues are
complex and overlap with dialogue structure, and
our toolset supports segmenting turns into smaller
2It’s possible to express the query in NQL us-
ing its precedence operator “«” as “($f factor)
($u utterance) (forall $u1 utterance) :
(($f « $u) &amp;&amp; ($f « u1)) --+ (u « u1)”.
However, this is very inefficient since it must check all
utterance pairs in the corpus to determine direct precedence,
especially if it needs to be included as part of a bigger query.
utterance units and annotating hierarchical dia-
logue structure over the utterances, as well as pro-
viding simpler segmentation annotation.
</bodyText>
<sectionHeader confidence="0.998191" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.996282">
This material is based upon work supported by a
grant from The Office of Naval Research num-
ber N000149910165 and European Union 6th
framework programme grant EC-FP6-2002-IST-
1-507826 (LeActiveMath).
</bodyText>
<sectionHeader confidence="0.999425" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999526944444444">
Jean Carletta, J. Kilgour, T. O’Donnell, S. Evert, and
H. Voormann. 2003. The NITE object model li-
brary for handling structured linguistic annotation
on multimodal data sets. In Proceedings of the
EACL Workshop on Language Technology and the
Semantic Web.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, intentions, and the structure of discourse. Com-
put. Linguist., 12(3):175–204.
Pamela Jordan and Stephanie Siler. 2002. Student
initiative and questioning strategies in computer-
mediated human tutoring dialogues. In Proceedings
ofITS 2002 Workshop on Empirical Methods for Tu-
torial Dialogue Systems.
Farhana Shah, Martha W. Evens, Joel Michael, and
Allen Rovick. 2002. Classifying student initiatives
and tutor responses in human keyboard-to-keyboard
tutoring sessions. Discourse Processes, 33(1).
</reference>
<page confidence="0.998405">
60
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.200625">
<title confidence="0.877627">Tools for hierarchical annotation of typed dialogue</title>
<author confidence="0.850379">Myroslava O Dzikovska</author>
<author confidence="0.850379">Charles Callaway</author>
<author confidence="0.850379">Elaine</author>
<affiliation confidence="0.529881">Human Communication Research Centre, University of</affiliation>
<intro confidence="0.302752">2 Buccleuch Place, Edinburgh, EH8 9LW, United</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
<author>J Kilgour</author>
<author>T O’Donnell</author>
<author>S Evert</author>
<author>H Voormann</author>
</authors>
<title>The NITE object model library for handling structured linguistic annotation on multimodal data sets.</title>
<date>2003</date>
<booktitle>In Proceedings of the EACL Workshop on Language Technology and the Semantic Web.</booktitle>
<marker>Carletta, Kilgour, O’Donnell, Evert, Voormann, 2003</marker>
<rawString>Jean Carletta, J. Kilgour, T. O’Donnell, S. Evert, and H. Voormann. 2003. The NITE object model library for handling structured linguistic annotation on multimodal data sets. In Proceedings of the EACL Workshop on Language Technology and the Semantic Web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Candace L Sidner</author>
</authors>
<title>Attention, intentions, and the structure of discourse.</title>
<date>1986</date>
<journal>Comput. Linguist.,</journal>
<volume>12</volume>
<issue>3</issue>
<contexts>
<context position="5094" citStr="Grosz and Sidner, 1986" startWordPosition="783" endWordPosition="786"> To provide a detailed analysis of tutorial dialogue and remediation strategies, we employ a hierarchical annotation scheme which encodes the recursive dialogue structure. Each tutorial session consists of a sequence of tasks, which may be either teaching specific domain concepts or doing individual exercises. Each task’s structure includes one or more of the following: giving definitions, formulating a question, obtaining the student answer and remediation by the tutor. Generally speaking, the structure of tutorial dialogue is governed by the task structure just as in task-oriented dialogue (Grosz and Sidner, 1986). However, the specific annotation structure differs depending on the tutoring method. In our basic electricity and electronics domain, a tutorial session consists of a set of “teach” segments, and within each segment a number of “task” segments. Task segments usually contain exercises in which the student is asked a question requiring a simple (one- or two-line) answer, which may be followed by a long remediation segment to address the conceptual problems revealed by the answer. In contrast, in our calculus domain the students have to do multi-step procedures to differentiate complex math exp</context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>Barbara J. Grosz and Candace L. Sidner. 1986. Attention, intentions, and the structure of discourse. Comput. Linguist., 12(3):175–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pamela Jordan</author>
<author>Stephanie Siler</author>
</authors>
<title>Student initiative and questioning strategies in computermediated human tutoring dialogues.</title>
<date>2002</date>
<booktitle>In Proceedings ofITS 2002 Workshop on Empirical Methods for Tutorial Dialogue Systems.</booktitle>
<contexts>
<context position="864" citStr="Jordan and Siler, 2002" startWordPosition="123" endWordPosition="126">a,efarrow}@inf.ed.ac.uk 1 Introduction We discuss a set of tools for annotating a complex hierarchical and linguistic structure of tutorial dialogue based on the NITE XML Toolkit (NXT) (Carletta et al., 2003). The NXT API supports multi-layered stand-off data annotation and synchronisation with timed and speech data. Using NXT, we built a set of extensible tools for detailed structure annotation of typed tutorial dialogue, collected from a tutor and student typing via a chat interface. There are several corpora of tutoring done with such chat-style communication techniques (Shah et al., 2002; Jordan and Siler, 2002), however, our annotation presents a special problem because of its detailed hierarchical structure. We applied our annotation methodology to annotating corpora in two different tutoring domains: basic electricity and electronics, and symbolic differentiation. 2 Data Structures Our corpus has two sources of overlapping annotations: the turn structure of the corpus and situational factors annotation. The data are naturally split into turns whenever a participant presses their “submit” button. Timing information is associated with individual turns, representing the time when the entire message w</context>
</contexts>
<marker>Jordan, Siler, 2002</marker>
<rawString>Pamela Jordan and Stephanie Siler. 2002. Student initiative and questioning strategies in computermediated human tutoring dialogues. In Proceedings ofITS 2002 Workshop on Empirical Methods for Tutorial Dialogue Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Farhana Shah</author>
<author>Martha W Evens</author>
<author>Joel Michael</author>
<author>Allen Rovick</author>
</authors>
<title>Classifying student initiatives and tutor responses in human keyboard-to-keyboard tutoring sessions.</title>
<date>2002</date>
<booktitle>Discourse Processes,</booktitle>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="839" citStr="Shah et al., 2002" startWordPosition="119" endWordPosition="122">, {mdzikovs,ccallawa,efarrow}@inf.ed.ac.uk 1 Introduction We discuss a set of tools for annotating a complex hierarchical and linguistic structure of tutorial dialogue based on the NITE XML Toolkit (NXT) (Carletta et al., 2003). The NXT API supports multi-layered stand-off data annotation and synchronisation with timed and speech data. Using NXT, we built a set of extensible tools for detailed structure annotation of typed tutorial dialogue, collected from a tutor and student typing via a chat interface. There are several corpora of tutoring done with such chat-style communication techniques (Shah et al., 2002; Jordan and Siler, 2002), however, our annotation presents a special problem because of its detailed hierarchical structure. We applied our annotation methodology to annotating corpora in two different tutoring domains: basic electricity and electronics, and symbolic differentiation. 2 Data Structures Our corpus has two sources of overlapping annotations: the turn structure of the corpus and situational factors annotation. The data are naturally split into turns whenever a participant presses their “submit” button. Timing information is associated with individual turns, representing the time </context>
</contexts>
<marker>Shah, Evens, Michael, Rovick, 2002</marker>
<rawString>Farhana Shah, Martha W. Evens, Joel Michael, and Allen Rovick. 2002. Classifying student initiatives and tutor responses in human keyboard-to-keyboard tutoring sessions. Discourse Processes, 33(1).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>