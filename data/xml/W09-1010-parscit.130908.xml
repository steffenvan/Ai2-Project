<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000030">
<title confidence="0.9994115">
Comparing learners for Boolean partitions:
implications for morphological paradigms *
</title>
<author confidence="0.99615">
Katya Pertsova
</author>
<affiliation confidence="0.999354">
University of North Carolina,
</affiliation>
<address confidence="0.684332">
Chapel Hill
</address>
<email confidence="0.998769">
pertsova@email.unc.edu
</email>
<sectionHeader confidence="0.993898" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999837285714286">
In this paper, I show that a problem of
learning a morphological paradigm is sim-
ilar to a problem of learning a partition
of the space of Boolean functions. I de-
scribe several learners that solve this prob-
lem in different ways, and compare their
basic properties.
</bodyText>
<sectionHeader confidence="0.999267" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999831361702128">
Lately, there has been a lot of work on acquir-
ing paradigms as part of the word-segmentation
problem (Zeman, 2007; Goldsmith, 2001; Snover
et al., 2002). However, the problem of learning
the distribution of affixes within paradigms as a
function of their semantic (or syntactic) features is
much less explored to my knowledge. This prob-
lem can be described as follows: suppose that the
segmentation has already been established. Can
we now predict what affixes should appear in
what contexts, where by a ‘context’ I mean some-
thing quite general: some specification of seman-
tic (and/or syntactic) features of the utterance. For
example, one might say that the nominal suffix -
z in English (as in apple-z) occurs in contexts that
involve plural or possesive nouns whose stems end
in a voiced segment.
In this paper, I show that the problem of learn-
ing the distribution of morphemes in contexts
specified over some finite number of features
is roughly equivalent to the problem of learn-
ing Boolean partitions of DNF formulas. Given
this insight, one can easily extend standard DNF-
learners to morphological paradigm learners. I
show how this can be done on an example of
the classical k-DNF learner (Valiant, 1984). This
insight also allows us to bridge the paradigm-
learning problem with other similar problems in
This paper ows a great deal to the input from Ed Stabler.
As usual, all the errors and shortcomings are entirely mine.
the domain of cognitive science for which DNF’s
have been used, e.g., concept learning. I also de-
scribe two other learners proposed specifically for
learning morphological paradigms. The first of
these learners, proposed by me, was designed to
capture certain empirical facts about syncretism
and free variation in typological data (Pertsova,
2007). The second learner, proposed by David
Adger, was designed as a possible explanation of
another empirical fact - uneven frequencies of free
variants in paradigms (Adger, 2006).
In the last section, I compare the learners on
some simple examples and comment on their mer-
its and the key differences among the algorithms.
I also draw connections to other work, and discuss
directions for further empirical tests of these pro-
posals.
</bodyText>
<sectionHeader confidence="0.930181" genericHeader="method">
2 The problem
</sectionHeader>
<bodyText confidence="0.994477764705882">
Consider a problem of learning the distribution
of inflectional morphemes as a function of some
set of features. Using featural representations, we
can represent morpheme distributions in terms of
a formula. The DNF formulas are commonly used
for such algebraic representation. For instance,
given the nominal suffix -z mentioned in the in-
troduction, we can assign to it the following rep-
resentation: [(noun; +voiced]stem; +plural) V
(noun; +voiced]stem; +possesive)]. Presum-
ably, features like [plural] or [+voiced] or ]stem
(end of the stem) are accessible to the learners’
cognitive system, and can be exploited during
the learning process for the purpose of “ground-
ing” the distribution of morphemes.1 This way
of looking at things is similar to how some re-
searchers conceive of concept-learning or word-
</bodyText>
<footnote confidence="0.790133166666667">
1Assuming an a priori given universal feature set, the
problem of feature discovery is a subproblem of learning
morpheme distributions. This is because learning what fea-
ture condition the distribution is the same as learning what
features (from the universal set) are relevant and should be
paid attention to.
</footnote>
<note confidence="0.951943">
Proceedings of the EACL 2009 Workshop on Computational Linguistic Aspects of Grammatical Inference, pages 66–74,
Athens, Greece, 30 March 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.990555">
66
</page>
<bodyText confidence="0.984582625">
learning (Siskind, 1996; Feldman, 2000; Nosofsky
et al., 1994).
However, one prominent distinction that sets
inflectional morphemes apart from words is that
they occur in paradigms, semantic spaces defin-
ing a relatively small set of possible distinctions.
In the absence of free variation, one can say that
the affixes define a partition of this semantic space
into disjoint blocks, in which each block is asso-
ciated with a unique form. Consider for instance
a present tense paradigm of the verb “to be” in
standard English represented below as a partition
of the set of environments over the following fea-
tures: class (with values masc, fem, both (masc
&amp; fem),inanim,), number (with values +sg and
−sg), and person (with values 1st, 2nd, 3rd).2
</bodyText>
<listItem confidence="0.983302176470588">
am 1st. person; fem; +sg.
1st. person; masc; +sg.
are 2nd. person; fem; +sg.
2nd. person; masc; +sg.
2nd. person; fem; −sg.
2nd. person; masc; −sg.
2nd. person; both; −sg.
1st. person; fem; −sg.
1st. person; masc; −sg.
1st. person; both; −sg.
3rd. person; masc; −sg
3rd. person; fem; −sg
3rd. person; both; −sg
3rd. person; inanim; −sg
is 3rd person; masc; +sg
3rd person; fem; +sg
3rd person; inanim; +sg
</listItem>
<bodyText confidence="0.960360272727273">
Each block in the above partition can be rep-
resented as a mapping between the phonological
form of the morpheme (a morph) and a DNF for-
mula. A single morph will be typically mapped to
a DNF containing a single conjunction of features
(called a monomial). When a morph is mapped
to a disjunction of monomials (as the morph [-z]
discussed above), we think of such a morph as
a homonym (having more than one “meaning”).
Thus, one way of defining the learning problem is
in terms of learning a partition of a set of DNF’s.
</bodyText>
<footnote confidence="0.988978666666667">
2These particular features and their values are chosen just
for illustration. There might be a much better way to repre-
sent the distinctions encoded by the pronouns. Also notice
that the feature values are not fully independent: some com-
binations are logically ruled out (e.g. speakers and listeners
are usually animate entities).
</footnote>
<bodyText confidence="0.997657357142857">
Alternatively, we could say that the learner has
to learn a partition of Boolean functions associated
with each morph (a Boolean function for a morph
m maps the contexts in which m occurs to true,
and all other contexts to false).
However, when paradigms contain free varia-
tion, the divisions created by the morphs no longer
define a partition since a single context may be as-
sociated with more than one morph. (Free vari-
ation is attested in world’s languages, although
it is rather marginal (Kroch, 1994).) In case a
paradigm contains free variation, it is still possible
to represent it as a partition by doing the follow-
ing:
</bodyText>
<listItem confidence="0.621501">
(1) Take a singleton partition of morph-
</listItem>
<bodyText confidence="0.965353888888889">
meaning pairs (m, r) and merge any cells
that have the same meaning r. Then merge
those blocks that are associated with the
same set of morphs.
Below is an example of how we can use this trick
to partition a paradigm with free-variation. The
data comes from the past tense forms of “to be” in
Buckie English.
was 1st. person; fem; +sg.
</bodyText>
<listItem confidence="0.99109425">
1st. person; masc; +sg.
3rd person; masc; +sg
3rd person; fem; +sg
3rd person; inanim; +sg
was/were 2nd. person; fem; +sg.
2nd. person; masc; +sg.
2nd. person; fem; −sg.
2nd. person; masc; −sg.
2nd. person; both; −sg.
1st. person; fem; −sg.
1st. person; masc; −sg.
1st. person; both; −sg.
were 3rd. person; masc; −sg
3rd. person; fem; −sg
3rd. person; both; −sg
3rd. person; inanim; −sg
</listItem>
<bodyText confidence="0.999659">
In general, then, the problem of learning the
distribution of morphs within a single inflectional
paradigm is equivalent to learning a Boolean par-
tition.
In what follows, I consider and compare several
learners for learning Boolean partitions. Some of
these learners are extensions of learners proposed
in the literature for learning DNFs. Other learners
</bodyText>
<page confidence="0.998985">
67
</page>
<bodyText confidence="0.99939175">
were explicitly proposed for learning morphologi-
cal paradigms.
We should keep in mind that all these learners
are idealizations and are not realistic if only be-
cause they are batch-learners. However, because
they are relatively simple to state and to under-
stand, they allow a deeper understanding of what
properties of the data drive generalization.
</bodyText>
<subsectionHeader confidence="0.99611">
2.1 Some definitions
</subsectionHeader>
<bodyText confidence="0.999958583333334">
Assume a finite set of morphs, E, and a finite set
of features F. It would be convenient to think of
morphs as chunks of phonological material cor-
responding to the pronounced morphemes.3 Ev-
ery feature f E F is associated with some set
of values Vf that includes a value [*], unspec-
ified. Let 5 be the space of all possible com-
plete assignments over F (an assignment is a set
{fi —* VfJVfi E F}). We will call those assign-
ments that do not include any unspecified features
environments. Let the set 5&apos; C_ 5 correspond to
the set of environments.
It should be easy to see that the set 5 forms a
Boolean lattice with the following relation among
the assignments, GR: for any two assignments a1
and a2, a1 GR a2 iff the value of every feature fi
in a1 is identical to the value of fi in a2, unless fi
is unspecified in a2. The top element of the lattice
is an assignment in which all features are unspec-
ified, and the bottom is the contradiction. Every
element of the lattice is a monomial corresponding
to the conjunction of the specified feature values.
An example lattice for two binary features is given
in Figure 1.
</bodyText>
<figureCaption confidence="0.999441">
Figure 1: A lattice for 2 binary features
</figureCaption>
<bodyText confidence="0.999955346153846">
A language L consists of pairs from E x 5&apos;.
That is, the learner is exposed to morphs in differ-
ent environments.
3However, we could also conceive of morphs as functions
specifying what transformations apply to the stem without
much change to the formalism.
One way of stating the learning problem is to
say that the learner has to learn a grammar for the
target language L (we would then have to spec-
ify what this grammar should look like). Another
way is to say that the learner has to learn the lan-
guage mapping itself. We can do the latter by us-
ing Boolean functions to represent the mapping of
each morph to a set of environments. Depending
on how we state the learning problem, we might
get different results. For instance, it’s known that
some subsets of DNF’s are not learnable, while
the Boolean functions corresponding to them are
learnable (Valiant, 1984). Since I will use Boolean
functions for some of the learners below, I intro-
duce the following notation. Let B be the set of
Boolean functions mapping elements of 5&apos; to true
or false. For convenience, we say that bm corre-
sponds to a Boolean function that maps a set of en-
vironments to true when they are associated with
m in L, and to false otherwise.
</bodyText>
<sectionHeader confidence="0.993596" genericHeader="method">
3 Learning Algorithms
</sectionHeader>
<subsectionHeader confidence="0.891988">
3.1 Learner 1: an extension of the Valiant
k-DNF learner
</subsectionHeader>
<bodyText confidence="0.99999128">
An observation that a morphological paradigm can
be represented as a partition of environments in
which each block corresponds to a mapping be-
tween a morph and a DNF, allows us to easily con-
vert standard DNF learning algorithms that rely
on positive and negative examples into paradigm-
learning algorithms that rely on positive examples
only. We can do that by iteratively applying any
DNF learning algorithm treating instances of in-
put pairs like (m, e) as positive examples for m
and as negative examples for all other morphs.
Below, I show how this can be done by ex-
tending a k-DNF4 learner of (Valiant, 1984) to a
paradigm-learner. To handle cases of free varia-
tion we need to keep track of what morphs occur
in exactly the same environments. We can do this
by defining the partition H on the input following
the recipe in (1) (substituting environments for the
variable r).
The original learner learns from negative exam-
ples alone. It initializes the hypothesis to the dis-
junction of all possible conjunctions of length at
most k, and subtracts from this hypothesis mono-
mials that are consistent with the negative ex-
amples. We will do the same thing for each
</bodyText>
<footnote confidence="0.5934225">
4k-DNF formula is a formula with at most k feature val-
ues in each conjunct.
</footnote>
<page confidence="0.998237">
68
</page>
<bodyText confidence="0.9683092">
morph using positive examples only (as described
above), and forgoing subtraction in a cases of free-
variation. The modified learner is given below.
The following additional notation is used: Lex is
the lexicon or a hypothesis. The formula D is a
disjunction of all possible conjunctions of length
at most k. We say that two assignments are con-
sistent with each other if they agree on all specified
features. Following standard notation, we assume
that the learner is exposed to some text T that con-
sists of an infinite sequence of (possibly) repeating
elements from L. tj is a finite subsequence of the
first j elements from T. L(tj) is the set of ele-
ments in tj.
Learner 1 (input: tj)
</bodyText>
<listItem confidence="0.9409566">
1. set Lex {(m, D) ](m, e) E
L(tj)1
2. For each (m, e) E L(tj), for each
m&apos; s.t. -,] block bl E II of L(tj),
(m, e) E bl and (m&apos;, e) E bl:
</listItem>
<bodyText confidence="0.9910793">
replace (m&apos;, f) in Lex by (m&apos;, f&apos;)
where f&apos; is the result of removing
every monomial consistent with e.
This learner initially assumes that every morph
can be used everywhere. Then, when it hears one
morph in a given environment, it assumes that no
other morph can be heard in exactly that environ-
ment unless it already knows that this environment
permits free variation (this is established in the
partition II).
</bodyText>
<sectionHeader confidence="0.997156" genericHeader="method">
4 Learner 2:
</sectionHeader>
<bodyText confidence="0.983944303571429">
The next learner is an elaboration on the previous
learner. It differs from it in only one respect: in-
stead of initializing lexical representations of ev-
ery morph to be a disjunction of all possible mono-
mials of length at most k, we initialize it to be the
disjunction of all and only those monomials that
are consistent with some environment paired with
the morph in the language. This learner is simi-
lar to the DNF learners that do something on both
positive and negative examples (see (Kushilevitz
and Roth, 1996; Blum, 1992)).
So, for every morph m used in the language, we
define a disjunction of monomials Dm that can be
derived as follows. (i) Let Em be the enumeration
of all environments in which m occurs in L (ii)
let Mi correspond to a set of all subsets of feature
values in ei, ei E E (iii) let Dm be V M, where a
set s E M iff s E Mi, for some i.
Learner 2 can now be stated as a learner that
is identical to Learner 1 except for the initial set-
ting of Lex. Now, Lex will be set to Lex ��
{(m,Dm)I ](m,e) E L(ti)1.
Because this learner does not require enumer-
ation of all possible monomials, but just those
that are consistent with the positive data, it can
handle “polynomially explainable” subclass of
DNF’s (for more on this see (Kushilevitz and Roth,
1996)).
5 Learner 3: a learner biased towards
monomial and elsewhere distributions
Next, I present a batch version of a learner I pro-
posed based on certain typological observations
and linguists’ insights about blocking. The typo-
logical observations come from a sample of verbal
agreement paradigms (Pertsova, 2007) and per-
sonal pronoun paradigms (Cysouw, 2003) show-
ing that majority of paradigms have either “mono-
mial” or “elsewhere” distribution (defined below).
Roughly speaking, a morph has a monomial dis-
tribution if it can be described with a single mono-
mial. A morph has an elsewhere distribution if
this distribution can be viewed as a complement
of distributions of other monomial or elsewhere-
morphs. To define these terms more precisely I
need to introduce some additional notation. Let
n ex be the intersection of all environments in
which morph x occurs (i.e., these are the invariant
features of x). This set corresponds to a least up-
per bound of the environments associated with x in
the lattice (5, &lt;R), call it lubx. Then, let the min-
imal monomial function for a morph x, denoted
mmx, be a Boolean function that maps an envi-
ronment to true if it is consistent with lubx and
to false otherwise. As usual, an extension of a
Boolean function, ext(b) is the set of all assign-
ments that b maps to true.
</bodyText>
<listItem confidence="0.579092">
(2) Monomial distribution
</listItem>
<bodyText confidence="0.982513">
A morph x has a monomial distribution iff
bx - mmx.
The above definition states that a morph has a
monomial distribution if its invariant features pick
out just those environments that are associated
with this morph in the language. More concretely,
if a monomial morph always co-occurs with the
feature +singular, it will appear in all singular en-
</bodyText>
<page confidence="0.996704">
69
</page>
<bodyText confidence="0.977768">
vironments in the language.
</bodyText>
<listItem confidence="0.79976">
(3) Elsewhere distribution
</listItem>
<bodyText confidence="0.997276545454545">
A morph x has an elsewhere distribution
iff bx - mmx − (mmx, V mmx2 V ... V
(mmxj) for all xi =� x in Σ.
The definition above amounts to saying that a
morph has an elsewhere distribution if the envi-
ronments in which it occurs are in the extension
of its minimal monomial function minus the min-
imal monomial functions of all other morphs. An
example of a lexical item with an elsewhere distri-
bution is the present tense form are of the verb “to
be”, shown below.
</bodyText>
<tableCaption confidence="0.944822">
Table 1: The present tense of “to be” in English
sg. pl
</tableCaption>
<bodyText confidence="0.988781341463415">
Elsewhere morphemes are often described in
linguistic accounts by appealing to the notion of
blocking. For instance, the lexical representation
of are is said to be unspecified for both person
and number, and is said to be “blocked” by two
other forms: am and is. My hypothesis is that
the reason why such non-monotonic analyses ap-
pear so natural to linguists is the same reason for
why monomial and elsewhere distributions are ty-
pologically common: namely, the learners (and,
apparently, the analysts) are prone to generalize
the distribution of morphs to minimal monomi-
als first, and later correct any overgeneralizations
that might arise by using default reasoning, i.e. by
positing exceptions that override the general rule.
Of course, the above strategy alone is not sufficient
to capture distributions that are neither monomial,
nor elsewhere (I call such distributions “overlap-
ping”, cf. the suffixes -en and -t in the German
paradigm in Table 2), which might also explain
why such paradigms are typologically rare.
Table 2: Present tense of some regular verbs in
German
The original learner I proposed is an incre-
mental learner that calculates grammars similar
to those proposed by linguists, namely grammars
consisting of a lexicon and a filtering “blocking”
component. The version presented here is a sim-
pler batch learner that learns a partition of Boolean
functions instead.5 Nevertheless, the main proper-
ties of the original learner are preserved: specifi-
cally, a bias towards monomial and elsewhere dis-
tributions.
To determine what kind of distribution a morph
has, I define a relation C. A morph m stands in a
relation C to another morph m&apos; if I(m, e) E L,
such that lubm0 is consistent with e. In other
words, mCm&apos; if m occurs in any environment
consistent with the invariant features of m&apos;. Let
C+ be a transitive closure of C.
Learner 3 (input: tj)
</bodyText>
<listItem confidence="0.996808058823529">
1. Let 5(tj) be the set of pairs in tj containing
monomial- or elsewhere-distribution morphs.
That is, (m, e) E 5(tj) iff -,Im&apos; such that
mC+m&apos; and m&apos;C+m.
2. Let O(tj) = tj − 5(tj) (the set of all other
pairs).
3. A pair (m, e) E 5 is a least element of 5
iff -,I(m&apos;, e&apos;) E (5 − {(m, e)1) such that
m&apos;C+m.
4. Given a hypothesis Lex, and for any expres-
sion (m, e) E Lex: let rem((m, e), Lex) =
(m, (mmm − IbI(m&apos;, b) E Lex1))6
1. set 5 := 5(tj) and Lex := 0
2. While 5 =� 0: remove a least x
from 5 and set Lex := Lex U
rem(x, Lex)
3. Set Lex := Lex U O(tj).
</listItem>
<bodyText confidence="0.994984">
This learner initially assumes that the lexicon is
empty. Then it proceeds adding Boolean functions
corresponding to minimal monomials for morphs
that are in the set 5(tj) (i.e., morphs that have ei-
ther monomial or elsewhere distributions). This
</bodyText>
<figure confidence="0.847612">
1p. am are
2p. are are
3p.
</figure>
<footnote confidence="0.8567277">
is are
sg. pl 5I thank Ed Stabler for relating this batch learner to me
(p.c.).
6For any two Boolean functions b, b0: b−b0 is the function
that maps e to 1 iff e E ext(b) and e E� ext(b0). Similarly,
b + b0 is the function that maps e to 1 iff e E ext(b) and
e E ext(b0).
1p. -e -en
2p. -st -t
3p. -t -en
</footnote>
<page confidence="0.996583">
70
</page>
<bodyText confidence="0.964046125">
is done in a particular order, namely in the or-
der in which the morphs can be said to block
each other. The remaining text is learned by rote-
memorization. Although this learner is more com-
plex than the previous two learners, it generalizes
fast when applied to paradigms with monomial
and elsewhere distributions.
5.1 Learner 4: a learner biased towards
shorter formulas
Next, I discuss a learner for morphological
paradigms, proposed by another linguist, David
Adger. Adger describes his learner informally
showing how it would work on a few examples.
Below, I formalize his proposal in terms of learn-
ing Boolean partitions. The general strategy of
this learner is to consider simplest monomials first
(those with the fewer number of specified features)
and see how much data they can unambiguously
and non-redundantly account for. If a monomial
is consistent with several morphs in the text - it is
discarded unless the morphs in question are in free
variation. This simple strategy is reiterated for the
next set of most simple monomials, etc.
Learner 4 (input tj)
</bodyText>
<listItem confidence="0.994026941176471">
1. Let Mi be the set of all monomials over F
with i specified features.
2. Let Bi be the set of Boolean functions from
environments to truth values corresponding
to Mi in the following way: for each mono-
mial mn E Mi the corresponding Boolean
function b is such that b(e) = 1 if e is an
environment consistent with mn; otherwise
b(e) = 0.
3. Uniqueness check:
For a Boolean function b, morph m, and text
tj let unique(b, m, tj) = 1 iff ext(bm) C_
ext(b) and -,](m&apos;, e) E L(tj), s.t. e E
ext(b) and e E� ext(bm).
1. set Lex := E x 0 and i := 0;
2. while Lex does not correspond to
L(tj) AND i &lt; IFI do:
</listItem>
<equation confidence="0.6020405">
for each b E Bi, for each m, s.t.
I(m,e) E L(tj):
</equation>
<listItem confidence="0.99372">
• if unique(b, m, tj) = 1 then
replace (m, f) with (m, f + b)
in Lex
</listItem>
<equation confidence="0.763195">
i i i + 1
</equation>
<bodyText confidence="0.9998873">
This learner considers all monomials in the or-
der of their simplicity (determined by the num-
ber of specified features), and if the monomial in
question is consistent with environments associ-
ated with a unique morph then these environments
are added to the extension of the Boolean function
for that morph. As a result, this learner will con-
verge faster on paradigms in which morphs can be
described with disjunctions of shorter monomials
since such monomials are considered first.
</bodyText>
<sectionHeader confidence="0.985292" genericHeader="evaluation">
6 Comparison
</sectionHeader>
<subsectionHeader confidence="0.999595">
6.1 Basic properties
</subsectionHeader>
<bodyText confidence="0.999986888888889">
First, consider some of the basic properties of the
learners presented here. For this purpose, we will
assume that we can apply these learners in an iter-
ative fashion to larger and larger batches of data.
We say that a learner is consistent if and only if,
given a text tj, it always converges on the gram-
mar generating all the data seen in tj (Osherson
et al., 1986). A learner is monotonic if and only
if for every text t and every point j &lt; k, the hy-
pothesis the learner converges on at tj is a subset
of the hypothesis at tk (or for learners that learn
by elimination: the hypothesis at tj is a superset
of the hypothesis at tk). And, finally, a learner is
generalizing if and only if for some tj it converges
on a hypothesis that makes a prediction beyond the
elements of tj.
The table below classifies the four learners ac-
cording to the above properties.
</bodyText>
<table confidence="0.7826752">
Learner consist. monoton. generalizing
Learner 1 yes yes yes
Learner 2 yes yes yes
Learner 3 yes no yes
Learner 4 yes yes yes
</table>
<bodyText confidence="0.998143090909091">
All learners considered here are generalizing
and consistent, but they differ with respect to
monotonicity. Learner 3 is non-monotonic while
the remaining learners are monotonic. While
monotonicity is a nice computational property,
some aspects of human language acquisition are
suggestive of a non-monotonic learning strategy,
e.g. the presence of overgeneralization errors and
their subsequent corrections by children(Marcus et
al., 1992). Thus, the fact that Learner 3 is non-
monotonic might speak in its favor.
</bodyText>
<page confidence="0.996222">
71
</page>
<subsectionHeader confidence="0.995383">
6.2 Illustration
</subsectionHeader>
<bodyText confidence="0.9985785">
To demonstrate how the learners work, consider
this simple example. Suppose we are learning the
following distribution of morphs A and B over 2
binary features.
</bodyText>
<figure confidence="0.4412268">
(4) Example 1
Suppose further that the text t3 is:
A +f1;+f2
B −f1;+f2
B +f1;−f2
</figure>
<bodyText confidence="0.9975014">
Learner 1 generalizes right away by assuming
that every morph can appear in every environment
which leads to massive overgeneralizations. These
overgeneralizations are eventually eliminated as
more data is discovered. For instance, after pro-
cessing the first pair in the text above, the learner
“learns” that B does not occur in any environ-
ment consistent with (+f1; +f2) since it has just
seen A in that environment. After processing t3,
Learner 1 has the following hypothesis:
</bodyText>
<equation confidence="0.983472">
A (+f1;+f2) V (−f1;−f2)
B (−f1) V (−f2)
</equation>
<bodyText confidence="0.999612472222222">
That is, after seeing t3, Learner 2 correctly pre-
dicts the distribution of morphs in environments
that it has seen, but it still predicts that both A
and B should occur in the not-yet-observed en-
vironment, (−f1; −f2). This learner can some-
times converge before seeing all data-points, es-
pecially if the input includes a lot of free varia-
tion. If fact, if in the above example A and B were
in free variation in all environments, Learner 1
would have converged right away on its initial set-
ting of the lexicon. However, in paradigms with no
free variation convergence is typically slow since
the learner follows a very conservative strategy of
learning by elimination.
Unlike Learner 1, Learner 2 will converge after
seeing t3. This is because this learner’s initial hy-
pothesis is more restricted. Namely, the initial hy-
pothesis for A includes disjunction of only those
monomials that are consistent with (+f1; +f2).
Hence, A is never overgeneralized to (−f1; −f2).
Like Learner 1, Learner 2 also learns by elimina-
tion, however, on top of that it also restricts its ini-
tial hypothesis which leads to faster convergence.
Let’s now consider the behavior of learner 3 on
example 1. Recall that this learner first computes
minimal monomials of all morphs, and checks
in they have monomial or elsewhere distributions
(this is done via the relation C+). In this case, A
has a monomial distribution, and B has an else-
where distribution. Therefore, the learner first
computes the Boolean function for A whose exten-
sion is simply (+f1; +f2); and then the Boolean
function for B, whose extension includes environ-
ments consistent with (*;*) minus those consistent
with (+f1; +f2), which yields the following hy-
pothesis:
</bodyText>
<equation confidence="0.994826">
ext(bA) [+f1; +f2]
ext(bB) [−f1;+f2][+f1;−f2][−f1;−f2]
</equation>
<bodyText confidence="0.998722533333333">
That is, Learner 3 generalizes and converges on
the right language after seeing text t3.
Learner 4 also converges at this point. This
learner first considers how much data can be un-
ambiguously accounted for with the most minimal
monomial (*;*). Since both A and B occur in en-
vironments consistent with this monomial, noth-
ing is added to the lexicon. On the next round,
it considers all monomials with one specified fea-
ture. 2 such monomials, (−f1) and (−f2), are
consistent only with B, and so we predict B to ap-
pear in the not-yet-seen environment (−f1; −f2).
Thus, the hypothesis that Learner 4 arrives at is the
same as the hypothesis Learners 3 arrives at after
seeing t3.
</bodyText>
<subsectionHeader confidence="0.980028">
6.3 Differences
</subsectionHeader>
<bodyText confidence="0.99999325">
While the last three learners perform similarly on
the simple example above, there are significant
differences between them. These differences be-
come apparent when we consider larger paradigms
with homonymy and free variation.
First, let’s look at an example that involves a
more elaborate homonymy than example 1. Con-
sider, for instance, the following text.
</bodyText>
<figure confidence="0.998340888888889">
(5) Example 2
A [+f1;+f2;+f3]
A [+f1;−f2;−f3]
A [+f1;+f2;−f3]
A [−f1;+f2;+f3]
B [−f1;−f2;−f3]
+f1 −f1
+f2 A B
−f2 B B
</figure>
<page confidence="0.989032">
72
</page>
<bodyText confidence="0.999986444444445">
Given this text, all three learners will differ in
their predictions with respect to the environ-
ment (−f1; +f2; −f3). Learner 2 will pre-
dict both A and B to occur in this environment
since not enough monomials will be removed
from representations of A or B to rule out ei-
ther morph from occurring in (−f1; +f2; −f3).
Learner 3 will predict A to appear in all envi-
ronments that haven’t been seen yet, including
(−f1; +f2; −f3). This is because in the cur-
rent text the minimal monomial for A is (*; *; *)
and A has an elsewhere distribution. On the
other hand, Learner 4 predicts B to occur in
(−f1; +f2; −f3). This is because the exten-
sion of the Boolean function for B includes
any environments consistent with (−f1; −f3) or
(−f1; −f2) since these are the simplest monomi-
als that uniquely pick out B.
Thus, the three learners follow very different
generalization routes. Overall, Learner 2 is more
cautious and slower to generalize. It predicts free
variation in all environments for which not enough
data has been seen to converge on a single morph.
Learner 3 is unique in preferring monomial and
elsewhere distributions. For instance, in the above
example it treats A as a ‘default’ morph. Learner
4 is unique in its preference for morphs describ-
able with disjunction of simpler monomials. Be-
cause of this preference, it will sometimes gener-
alize even after seeing just one instance of a morph
(since several simple monomials can be consistent
with this instance alone).
One way to test what the human learners do
in a situation like the one above is to use artifi-
cial grammar learning experiments. Such experi-
ments have been used for learning individual con-
cepts over features like shape, color, texture, etc.
Some work on concept learning suggests that it is
subjectively easier to learn concepts describable
with shorter formulas (Feldman, 2000; Feldman,
2004). Other recent work challenges this idea (La-
fond et al., 2007), showing that people don’t al-
ways converge on the most minimal representa-
tion, but instead go for the more simple and gen-
eral representation and learn exceptions to it (this
approach is more in line with Learner 3).
Some initial results from my pilot experiments
on learning partitions of concept spaces (using ab-
stract shapes, rather than language stimuli) also
suggest that people find paradigms with else-
where distributions easier to learn than the ones
with overlapping distributions (like the German
paradigms in 2). However, I also found a bias to-
ward paradigms with the fewer number of relevant
features. This bias is consistent with Learner 4
since this learner tries to assume the smallest num-
ber of relevant features possible. Thus, both learn-
ers have their merits.
Another area in which the considered learn-
ers make somewhat different predictions has to
do with free variation. While I can’t discuss
this at length due to space constraints, let me
comment that any batch learner can easily de-
tect free-variation before generalizing, which is
exactly what most of the above learners do (ex-
cept Learner 3, but it can also be changed to do
the same thing). However, since free variation
is rather marginal in morphological paradigms,
it is possible that it would be rather problem-
atic. In fact, free variation is more problematic if
we switch from the batch learners to incremental
learners.
</bodyText>
<sectionHeader confidence="0.998646" genericHeader="conclusions">
7 Directions for further research
</sectionHeader>
<bodyText confidence="0.999996956521739">
There are of course many other learners one could
consider for learning paradigms, including ap-
proaches quite different in spirit from the ones
considered here. In particular, some recently pop-
ular approaches conceive of learning as matching
probabilities of the observed data (e.g., Bayesian
learning). Comparing such approaches with the
algorithmic ones is difficult since the criteria for
success are defined so differently, but it would
still be interesting to see whether the kinds of
prior assumptions needed for a Bayesian model
to match human performance would have some-
thing in common with properties that the learn-
ers considered here relied on. These properties
include the disjoint nature of paradigm cells, the
prevalence of monomial and elsewhere morphs,
and the economy considerations. Other empirical
work that might help to differentiate Boolean par-
tition learners (besides typological and experimen-
tal work already mentioned) includes finding rele-
vant language acquisition data, and examining (or
modeling) language change (assuming that learn-
ing biases influence language change).
</bodyText>
<sectionHeader confidence="0.99926" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9986455">
David Adger. 2006. Combinatorial variation. Journal
of Linguistics, 42:503–530.
</reference>
<page confidence="0.997621">
73
</page>
<bodyText confidence="0.853174">
Leslie G. Valiant. 1984. A theory of the learnable.
CACM, 17(11):1134–1142.
Avrim Blum. 1992. Learning Boolean functions in an
infinite attribute space. Machine Learning, 9:373–
386.
</bodyText>
<reference confidence="0.999756944444445">
Michael Cysouw. 2003. The Paradigmatic Structure
of Person Marking. Oxford University Press, NY.
Jacob Feldman. 2000. Minimization of complexity in
human concept learning. Nature, 407:630–633.
Jacob Feldman. 2004. How surprising is a simple pat-
tern? Quantifying ‘Eureka!’. Cognition, 93:199–
224.
John Goldsmith. 2001. Unsupervised learning of a
morphology of a natural language. Computational
Linguistics, 27:153–198.
Anthony Kroch. 1994. Morphosyntactic variation. In
Katharine Beals et al., editor, Papers from the 30th
regional meeting of the Chicago Linguistics Soci-
ety: Parasession on variation and linguistic theory.
Chicago Linguistics Society, Chicago.
Eyal Kushilevitz and Dan Roth. 1996. On learning vi-
sual concepts and DNF formulae. Machine Learn-
ing, 24:65–85.
Daniel Lafond, Yves Lacouture, and Guy Mineau.
2007. Complexity minimization in rule-based cat-
egory learning: revising the catalog of boolean con-
cepts and evidence for non-minimal rules. Journal
of Mathematical Psychology, 51:57–74.
Gary Marcus, Steven Pinker, Michael Ullman,
Michelle Hollander, T. John Rosen, and Fei Xu.
1992. Overregularization in language acquisition.
Monographs of the Society for Research in Child
Development, 57(4). Includes commentary by
Harold Clahsen.
Robert M. Nosofsky, Thomas J. Palmeri, and S.C.
McKinley. 1994. Rule-plus-exception model
of classification learning. Psychological Review,
101:53–79.
Daniel Osherson, Scott Weinstein, and Michael Stob.
1986. Systems that Learn. MIT Press, Cambridge,
Massachusetts.
Katya Pertsova. 2007. Learning Form-Meaning Map-
pings in the Presence of Homonymy. Ph.D. thesis,
University of California, Los Angeles.
Jeffrey Mark Siskind. 1996. A computational study
of cross-situational techniques for learning word-to-
meaning mappings. Cognition, 61(1-2):1–38, Oct-
Nov.
Matthew G. Snover, Gaja E. Jarosz, and Michael R.
Brent. 2002. Unsupervised learning of morphology
using a novel directed search algorithm: taking the
first step. In Proceedings of the ACL-02 workshop
on Morphological and phonological learning, pages
11–20, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Daniel Zeman. 2007. Unsupervised acquiring of mor-
phological paradigms from tokenized text. In Work-
ing Notes for the Cross Language Evaluation Forum,
Budapest. Madarsko. Workshop.
</reference>
<page confidence="0.999135">
74
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.621333">
<title confidence="0.9946125">Comparing learners for Boolean partitions: for morphological paradigms</title>
<author confidence="0.994993">Katya</author>
<affiliation confidence="0.8167705">University of North Chapel</affiliation>
<email confidence="0.998569">pertsova@email.unc.edu</email>
<abstract confidence="0.999258375">In this paper, I show that a problem of learning a morphological paradigm is similar to a problem of learning a partition of the space of Boolean functions. I describe several learners that solve this problem in different ways, and compare their basic properties.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Adger</author>
</authors>
<title>Combinatorial variation.</title>
<date>2006</date>
<journal>Journal of Linguistics,</journal>
<pages>42--503</pages>
<contexts>
<context position="2408" citStr="Adger, 2006" startWordPosition="386" endWordPosition="387">the input from Ed Stabler. As usual, all the errors and shortcomings are entirely mine. the domain of cognitive science for which DNF’s have been used, e.g., concept learning. I also describe two other learners proposed specifically for learning morphological paradigms. The first of these learners, proposed by me, was designed to capture certain empirical facts about syncretism and free variation in typological data (Pertsova, 2007). The second learner, proposed by David Adger, was designed as a possible explanation of another empirical fact - uneven frequencies of free variants in paradigms (Adger, 2006). In the last section, I compare the learners on some simple examples and comment on their merits and the key differences among the algorithms. I also draw connections to other work, and discuss directions for further empirical tests of these proposals. 2 The problem Consider a problem of learning the distribution of inflectional morphemes as a function of some set of features. Using featural representations, we can represent morpheme distributions in terms of a formula. The DNF formulas are commonly used for such algebraic representation. For instance, given the nominal suffix -z mentioned in</context>
</contexts>
<marker>Adger, 2006</marker>
<rawString>David Adger. 2006. Combinatorial variation. Journal of Linguistics, 42:503–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Cysouw</author>
</authors>
<title>The Paradigmatic Structure of Person Marking.</title>
<date>2003</date>
<publisher>Oxford University Press, NY.</publisher>
<contexts>
<context position="14690" citStr="Cysouw, 2003" startWordPosition="2528" endWordPosition="2529">) E L(ti)1. Because this learner does not require enumeration of all possible monomials, but just those that are consistent with the positive data, it can handle “polynomially explainable” subclass of DNF’s (for more on this see (Kushilevitz and Roth, 1996)). 5 Learner 3: a learner biased towards monomial and elsewhere distributions Next, I present a batch version of a learner I proposed based on certain typological observations and linguists’ insights about blocking. The typological observations come from a sample of verbal agreement paradigms (Pertsova, 2007) and personal pronoun paradigms (Cysouw, 2003) showing that majority of paradigms have either “monomial” or “elsewhere” distribution (defined below). Roughly speaking, a morph has a monomial distribution if it can be described with a single monomial. A morph has an elsewhere distribution if this distribution can be viewed as a complement of distributions of other monomial or elsewheremorphs. To define these terms more precisely I need to introduce some additional notation. Let n ex be the intersection of all environments in which morph x occurs (i.e., these are the invariant features of x). This set corresponds to a least upper bound of t</context>
</contexts>
<marker>Cysouw, 2003</marker>
<rawString>Michael Cysouw. 2003. The Paradigmatic Structure of Person Marking. Oxford University Press, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Feldman</author>
</authors>
<title>Minimization of complexity in human concept learning.</title>
<date>2000</date>
<journal>Nature,</journal>
<pages>407--630</pages>
<contexts>
<context position="4027" citStr="Feldman, 2000" startWordPosition="637" endWordPosition="638"> similar to how some researchers conceive of concept-learning or word1Assuming an a priori given universal feature set, the problem of feature discovery is a subproblem of learning morpheme distributions. This is because learning what feature condition the distribution is the same as learning what features (from the universal set) are relevant and should be paid attention to. Proceedings of the EACL 2009 Workshop on Computational Linguistic Aspects of Grammatical Inference, pages 66–74, Athens, Greece, 30 March 2009. c�2009 Association for Computational Linguistics 66 learning (Siskind, 1996; Feldman, 2000; Nosofsky et al., 1994). However, one prominent distinction that sets inflectional morphemes apart from words is that they occur in paradigms, semantic spaces defining a relatively small set of possible distinctions. In the absence of free variation, one can say that the affixes define a partition of this semantic space into disjoint blocks, in which each block is associated with a unique form. Consider for instance a present tense paradigm of the verb “to be” in standard English represented below as a partition of the set of environments over the following features: class (with values masc, </context>
<context position="28977" citStr="Feldman, 2000" startWordPosition="5044" endWordPosition="5045">rphs describable with disjunction of simpler monomials. Because of this preference, it will sometimes generalize even after seeing just one instance of a morph (since several simple monomials can be consistent with this instance alone). One way to test what the human learners do in a situation like the one above is to use artificial grammar learning experiments. Such experiments have been used for learning individual concepts over features like shape, color, texture, etc. Some work on concept learning suggests that it is subjectively easier to learn concepts describable with shorter formulas (Feldman, 2000; Feldman, 2004). Other recent work challenges this idea (Lafond et al., 2007), showing that people don’t always converge on the most minimal representation, but instead go for the more simple and general representation and learn exceptions to it (this approach is more in line with Learner 3). Some initial results from my pilot experiments on learning partitions of concept spaces (using abstract shapes, rather than language stimuli) also suggest that people find paradigms with elsewhere distributions easier to learn than the ones with overlapping distributions (like the German paradigms in 2).</context>
</contexts>
<marker>Feldman, 2000</marker>
<rawString>Jacob Feldman. 2000. Minimization of complexity in human concept learning. Nature, 407:630–633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Feldman</author>
</authors>
<title>How surprising is a simple pattern? Quantifying ‘Eureka!’.</title>
<date>2004</date>
<journal>Cognition,</journal>
<volume>93</volume>
<pages>224</pages>
<contexts>
<context position="28993" citStr="Feldman, 2004" startWordPosition="5046" endWordPosition="5047">e with disjunction of simpler monomials. Because of this preference, it will sometimes generalize even after seeing just one instance of a morph (since several simple monomials can be consistent with this instance alone). One way to test what the human learners do in a situation like the one above is to use artificial grammar learning experiments. Such experiments have been used for learning individual concepts over features like shape, color, texture, etc. Some work on concept learning suggests that it is subjectively easier to learn concepts describable with shorter formulas (Feldman, 2000; Feldman, 2004). Other recent work challenges this idea (Lafond et al., 2007), showing that people don’t always converge on the most minimal representation, but instead go for the more simple and general representation and learn exceptions to it (this approach is more in line with Learner 3). Some initial results from my pilot experiments on learning partitions of concept spaces (using abstract shapes, rather than language stimuli) also suggest that people find paradigms with elsewhere distributions easier to learn than the ones with overlapping distributions (like the German paradigms in 2). However, I also</context>
</contexts>
<marker>Feldman, 2004</marker>
<rawString>Jacob Feldman. 2004. How surprising is a simple pattern? Quantifying ‘Eureka!’. Cognition, 93:199– 224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Goldsmith</author>
</authors>
<title>Unsupervised learning of a morphology of a natural language.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<pages>27--153</pages>
<marker>Goldsmith, 2001</marker>
<rawString>John Goldsmith. 2001. Unsupervised learning of a morphology of a natural language. Computational Linguistics, 27:153–198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Kroch</author>
</authors>
<title>Morphosyntactic variation.</title>
<date>1994</date>
<booktitle>Papers from the 30th regional meeting of the Chicago Linguistics Society: Parasession on variation and linguistic theory. Chicago Linguistics Society,</booktitle>
<editor>In Katharine Beals et al., editor,</editor>
<location>Chicago.</location>
<contexts>
<context position="6496" citStr="Kroch, 1994" startWordPosition="1060" endWordPosition="1061">ent: some combinations are logically ruled out (e.g. speakers and listeners are usually animate entities). Alternatively, we could say that the learner has to learn a partition of Boolean functions associated with each morph (a Boolean function for a morph m maps the contexts in which m occurs to true, and all other contexts to false). However, when paradigms contain free variation, the divisions created by the morphs no longer define a partition since a single context may be associated with more than one morph. (Free variation is attested in world’s languages, although it is rather marginal (Kroch, 1994).) In case a paradigm contains free variation, it is still possible to represent it as a partition by doing the following: (1) Take a singleton partition of morphmeaning pairs (m, r) and merge any cells that have the same meaning r. Then merge those blocks that are associated with the same set of morphs. Below is an example of how we can use this trick to partition a paradigm with free-variation. The data comes from the past tense forms of “to be” in Buckie English. was 1st. person; fem; +sg. 1st. person; masc; +sg. 3rd person; masc; +sg 3rd person; fem; +sg 3rd person; inanim; +sg was/were 2n</context>
</contexts>
<marker>Kroch, 1994</marker>
<rawString>Anthony Kroch. 1994. Morphosyntactic variation. In Katharine Beals et al., editor, Papers from the 30th regional meeting of the Chicago Linguistics Society: Parasession on variation and linguistic theory. Chicago Linguistics Society, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eyal Kushilevitz</author>
<author>Dan Roth</author>
</authors>
<title>On learning visual concepts and DNF formulae.</title>
<date>1996</date>
<booktitle>Machine Learning,</booktitle>
<pages>24--65</pages>
<contexts>
<context position="13574" citStr="Kushilevitz and Roth, 1996" startWordPosition="2322" endWordPosition="2325">ady knows that this environment permits free variation (this is established in the partition II). 4 Learner 2: The next learner is an elaboration on the previous learner. It differs from it in only one respect: instead of initializing lexical representations of every morph to be a disjunction of all possible monomials of length at most k, we initialize it to be the disjunction of all and only those monomials that are consistent with some environment paired with the morph in the language. This learner is similar to the DNF learners that do something on both positive and negative examples (see (Kushilevitz and Roth, 1996; Blum, 1992)). So, for every morph m used in the language, we define a disjunction of monomials Dm that can be derived as follows. (i) Let Em be the enumeration of all environments in which m occurs in L (ii) let Mi correspond to a set of all subsets of feature values in ei, ei E E (iii) let Dm be V M, where a set s E M iff s E Mi, for some i. Learner 2 can now be stated as a learner that is identical to Learner 1 except for the initial setting of Lex. Now, Lex will be set to Lex �� {(m,Dm)I ](m,e) E L(ti)1. Because this learner does not require enumeration of all possible monomials, but just</context>
</contexts>
<marker>Kushilevitz, Roth, 1996</marker>
<rawString>Eyal Kushilevitz and Dan Roth. 1996. On learning visual concepts and DNF formulae. Machine Learning, 24:65–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Lafond</author>
<author>Yves Lacouture</author>
<author>Guy Mineau</author>
</authors>
<title>Complexity minimization in rule-based category learning: revising the catalog of boolean concepts and evidence for non-minimal rules.</title>
<date>2007</date>
<journal>Journal of Mathematical Psychology,</journal>
<pages>51--57</pages>
<contexts>
<context position="29055" citStr="Lafond et al., 2007" startWordPosition="5054" endWordPosition="5058"> preference, it will sometimes generalize even after seeing just one instance of a morph (since several simple monomials can be consistent with this instance alone). One way to test what the human learners do in a situation like the one above is to use artificial grammar learning experiments. Such experiments have been used for learning individual concepts over features like shape, color, texture, etc. Some work on concept learning suggests that it is subjectively easier to learn concepts describable with shorter formulas (Feldman, 2000; Feldman, 2004). Other recent work challenges this idea (Lafond et al., 2007), showing that people don’t always converge on the most minimal representation, but instead go for the more simple and general representation and learn exceptions to it (this approach is more in line with Learner 3). Some initial results from my pilot experiments on learning partitions of concept spaces (using abstract shapes, rather than language stimuli) also suggest that people find paradigms with elsewhere distributions easier to learn than the ones with overlapping distributions (like the German paradigms in 2). However, I also found a bias toward paradigms with the fewer number of releva</context>
</contexts>
<marker>Lafond, Lacouture, Mineau, 2007</marker>
<rawString>Daniel Lafond, Yves Lacouture, and Guy Mineau. 2007. Complexity minimization in rule-based category learning: revising the catalog of boolean concepts and evidence for non-minimal rules. Journal of Mathematical Psychology, 51:57–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gary Marcus</author>
<author>Steven Pinker</author>
<author>Michael Ullman</author>
<author>Michelle Hollander</author>
<author>T John Rosen</author>
<author>Fei Xu</author>
</authors>
<title>Overregularization in language acquisition.</title>
<date>1992</date>
<journal>Monographs of the Society for Research in Child Development,</journal>
<volume>57</volume>
<issue>4</issue>
<contexts>
<context position="23338" citStr="Marcus et al., 1992" startWordPosition="4087" endWordPosition="4090"> the four learners according to the above properties. Learner consist. monoton. generalizing Learner 1 yes yes yes Learner 2 yes yes yes Learner 3 yes no yes Learner 4 yes yes yes All learners considered here are generalizing and consistent, but they differ with respect to monotonicity. Learner 3 is non-monotonic while the remaining learners are monotonic. While monotonicity is a nice computational property, some aspects of human language acquisition are suggestive of a non-monotonic learning strategy, e.g. the presence of overgeneralization errors and their subsequent corrections by children(Marcus et al., 1992). Thus, the fact that Learner 3 is nonmonotonic might speak in its favor. 71 6.2 Illustration To demonstrate how the learners work, consider this simple example. Suppose we are learning the following distribution of morphs A and B over 2 binary features. (4) Example 1 Suppose further that the text t3 is: A +f1;+f2 B −f1;+f2 B +f1;−f2 Learner 1 generalizes right away by assuming that every morph can appear in every environment which leads to massive overgeneralizations. These overgeneralizations are eventually eliminated as more data is discovered. For instance, after processing the first pair </context>
</contexts>
<marker>Marcus, Pinker, Ullman, Hollander, Rosen, Xu, 1992</marker>
<rawString>Gary Marcus, Steven Pinker, Michael Ullman, Michelle Hollander, T. John Rosen, and Fei Xu. 1992. Overregularization in language acquisition. Monographs of the Society for Research in Child Development, 57(4). Includes commentary by Harold Clahsen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert M Nosofsky</author>
<author>Thomas J Palmeri</author>
<author>S C McKinley</author>
</authors>
<title>Rule-plus-exception model of classification learning.</title>
<date>1994</date>
<journal>Psychological Review,</journal>
<pages>101--53</pages>
<contexts>
<context position="4051" citStr="Nosofsky et al., 1994" startWordPosition="639" endWordPosition="642"> some researchers conceive of concept-learning or word1Assuming an a priori given universal feature set, the problem of feature discovery is a subproblem of learning morpheme distributions. This is because learning what feature condition the distribution is the same as learning what features (from the universal set) are relevant and should be paid attention to. Proceedings of the EACL 2009 Workshop on Computational Linguistic Aspects of Grammatical Inference, pages 66–74, Athens, Greece, 30 March 2009. c�2009 Association for Computational Linguistics 66 learning (Siskind, 1996; Feldman, 2000; Nosofsky et al., 1994). However, one prominent distinction that sets inflectional morphemes apart from words is that they occur in paradigms, semantic spaces defining a relatively small set of possible distinctions. In the absence of free variation, one can say that the affixes define a partition of this semantic space into disjoint blocks, in which each block is associated with a unique form. Consider for instance a present tense paradigm of the verb “to be” in standard English represented below as a partition of the set of environments over the following features: class (with values masc, fem, both (masc &amp; fem),i</context>
</contexts>
<marker>Nosofsky, Palmeri, McKinley, 1994</marker>
<rawString>Robert M. Nosofsky, Thomas J. Palmeri, and S.C. McKinley. 1994. Rule-plus-exception model of classification learning. Psychological Review, 101:53–79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Osherson</author>
<author>Scott Weinstein</author>
<author>Michael Stob</author>
</authors>
<title>Systems that Learn.</title>
<date>1986</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="22278" citStr="Osherson et al., 1986" startWordPosition="3909" endWordPosition="3912">nsion of the Boolean function for that morph. As a result, this learner will converge faster on paradigms in which morphs can be described with disjunctions of shorter monomials since such monomials are considered first. 6 Comparison 6.1 Basic properties First, consider some of the basic properties of the learners presented here. For this purpose, we will assume that we can apply these learners in an iterative fashion to larger and larger batches of data. We say that a learner is consistent if and only if, given a text tj, it always converges on the grammar generating all the data seen in tj (Osherson et al., 1986). A learner is monotonic if and only if for every text t and every point j &lt; k, the hypothesis the learner converges on at tj is a subset of the hypothesis at tk (or for learners that learn by elimination: the hypothesis at tj is a superset of the hypothesis at tk). And, finally, a learner is generalizing if and only if for some tj it converges on a hypothesis that makes a prediction beyond the elements of tj. The table below classifies the four learners according to the above properties. Learner consist. monoton. generalizing Learner 1 yes yes yes Learner 2 yes yes yes Learner 3 yes no yes Le</context>
</contexts>
<marker>Osherson, Weinstein, Stob, 1986</marker>
<rawString>Daniel Osherson, Scott Weinstein, and Michael Stob. 1986. Systems that Learn. MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katya Pertsova</author>
</authors>
<title>Learning Form-Meaning Mappings in the Presence of Homonymy.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>University of California,</institution>
<location>Los Angeles.</location>
<contexts>
<context position="2232" citStr="Pertsova, 2007" startWordPosition="359" endWordPosition="360">e of the classical k-DNF learner (Valiant, 1984). This insight also allows us to bridge the paradigmlearning problem with other similar problems in This paper ows a great deal to the input from Ed Stabler. As usual, all the errors and shortcomings are entirely mine. the domain of cognitive science for which DNF’s have been used, e.g., concept learning. I also describe two other learners proposed specifically for learning morphological paradigms. The first of these learners, proposed by me, was designed to capture certain empirical facts about syncretism and free variation in typological data (Pertsova, 2007). The second learner, proposed by David Adger, was designed as a possible explanation of another empirical fact - uneven frequencies of free variants in paradigms (Adger, 2006). In the last section, I compare the learners on some simple examples and comment on their merits and the key differences among the algorithms. I also draw connections to other work, and discuss directions for further empirical tests of these proposals. 2 The problem Consider a problem of learning the distribution of inflectional morphemes as a function of some set of features. Using featural representations, we can repr</context>
<context position="14644" citStr="Pertsova, 2007" startWordPosition="2521" endWordPosition="2522">x. Now, Lex will be set to Lex �� {(m,Dm)I ](m,e) E L(ti)1. Because this learner does not require enumeration of all possible monomials, but just those that are consistent with the positive data, it can handle “polynomially explainable” subclass of DNF’s (for more on this see (Kushilevitz and Roth, 1996)). 5 Learner 3: a learner biased towards monomial and elsewhere distributions Next, I present a batch version of a learner I proposed based on certain typological observations and linguists’ insights about blocking. The typological observations come from a sample of verbal agreement paradigms (Pertsova, 2007) and personal pronoun paradigms (Cysouw, 2003) showing that majority of paradigms have either “monomial” or “elsewhere” distribution (defined below). Roughly speaking, a morph has a monomial distribution if it can be described with a single monomial. A morph has an elsewhere distribution if this distribution can be viewed as a complement of distributions of other monomial or elsewheremorphs. To define these terms more precisely I need to introduce some additional notation. Let n ex be the intersection of all environments in which morph x occurs (i.e., these are the invariant features of x). Th</context>
</contexts>
<marker>Pertsova, 2007</marker>
<rawString>Katya Pertsova. 2007. Learning Form-Meaning Mappings in the Presence of Homonymy. Ph.D. thesis, University of California, Los Angeles.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Mark Siskind</author>
</authors>
<title>A computational study of cross-situational techniques for learning word-tomeaning mappings.</title>
<date>1996</date>
<journal>Cognition,</journal>
<pages>61--1</pages>
<contexts>
<context position="4012" citStr="Siskind, 1996" startWordPosition="635" endWordPosition="636">ng at things is similar to how some researchers conceive of concept-learning or word1Assuming an a priori given universal feature set, the problem of feature discovery is a subproblem of learning morpheme distributions. This is because learning what feature condition the distribution is the same as learning what features (from the universal set) are relevant and should be paid attention to. Proceedings of the EACL 2009 Workshop on Computational Linguistic Aspects of Grammatical Inference, pages 66–74, Athens, Greece, 30 March 2009. c�2009 Association for Computational Linguistics 66 learning (Siskind, 1996; Feldman, 2000; Nosofsky et al., 1994). However, one prominent distinction that sets inflectional morphemes apart from words is that they occur in paradigms, semantic spaces defining a relatively small set of possible distinctions. In the absence of free variation, one can say that the affixes define a partition of this semantic space into disjoint blocks, in which each block is associated with a unique form. Consider for instance a present tense paradigm of the verb “to be” in standard English represented below as a partition of the set of environments over the following features: class (wit</context>
</contexts>
<marker>Siskind, 1996</marker>
<rawString>Jeffrey Mark Siskind. 1996. A computational study of cross-situational techniques for learning word-tomeaning mappings. Cognition, 61(1-2):1–38, OctNov.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew G Snover</author>
<author>Gaja E Jarosz</author>
<author>Michael R Brent</author>
</authors>
<title>Unsupervised learning of morphology using a novel directed search algorithm: taking the first step.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 workshop on Morphological and phonological learning,</booktitle>
<pages>11--20</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<marker>Snover, Jarosz, Brent, 2002</marker>
<rawString>Matthew G. Snover, Gaja E. Jarosz, and Michael R. Brent. 2002. Unsupervised learning of morphology using a novel directed search algorithm: taking the first step. In Proceedings of the ACL-02 workshop on Morphological and phonological learning, pages 11–20, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Zeman</author>
</authors>
<title>Unsupervised acquiring of morphological paradigms from tokenized text. In Working Notes for the Cross Language Evaluation Forum,</title>
<date>2007</date>
<location>Budapest. Madarsko. Workshop.</location>
<marker>Zeman, 2007</marker>
<rawString>Daniel Zeman. 2007. Unsupervised acquiring of morphological paradigms from tokenized text. In Working Notes for the Cross Language Evaluation Forum, Budapest. Madarsko. Workshop.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>