<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.565327">
Parsing Algorithms based on Tree Automata
</note>
<author confidence="0.867737">
Andreas Maletti Giorgio Satta
</author>
<affiliation confidence="0.828417">
Departament de Filologies Rom`aniques Department of Information Engineering
</affiliation>
<address confidence="0.567751">
Universitat Rovira i Virgili, Tarragona, Spain University of Padua, Italy
</address>
<email confidence="0.977314">
andreas.maletti@urv.cat satta@dei.unipd.it
</email>
<sectionHeader confidence="0.994193" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999910538461538">
We investigate several algorithms related
to the parsing problem for weighted au-
tomata, under the assumption that the in-
put is a string rather than a tree. This
assumption is motivated by several natu-
ral language processing applications. We
provide algorithms for the computation of
parse-forests, best tree probability, inside
probability (called partition function), and
prefix probability. Our algorithms are ob-
tained by extending to weighted tree au-
tomata the Bar-Hillel technique, as defined
for context-free grammars.
</bodyText>
<sectionHeader confidence="0.998429" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999981757575758">
Tree automata are finite-state devices that recog-
nize tree languages, that is, sets of trees. There
is a growing interest nowadays in the natural
language parsing community, and especially in
the area of syntax-based machine translation, for
probabilistic tree automata (PTA) viewed as suit-
able representations of grammar models. In fact,
probabilistic tree automata are generatively more
powerful than probabilistic context-free gram-
mars (PCFGs), when we consider the latter as de-
vices that generate tree languages. This difference
can be intuitively understood if we consider that a
computation by a PTA uses hidden states, drawn
from a finite set, that can be used to transfer infor-
mation within the tree structure being recognized.
As an example, in written English we can em-
pirically observe different distributions in the ex-
pansion of so-called noun phrase (NP) nodes, in
the contexts of subject and direct-object positions,
respectively. This can be easily captured using
some states of a PTA that keep a record of the dif-
ferent contexts. In contrast, PCFGs are unable to
model these effects, because NP node expansion
should be independent of the context in the deriva-
tion. This problem for PCFGs is usually solved by
resorting to so-called parental annotations (John-
son, 1998), but this, of course, results in a different
tree language, since these annotations will appear
in the derived tree.
Most of the theoretical work on parsing and es-
timation based on PTA has assumed that the in-
put is a tree (Graehl et al., 2008), in accordance
with the very definition of these devices. How-
ever, both in parsing as well as in machine transla-
tion, the input is most often represented as a string
rather than a tree. When the input is a string, some
trick is applied to map the problem back to the
case of an input tree. As an example in the con-
text of machine translation, assume a probabilistic
tree transducer T as a translation model, and an
input string w to be translated. One can then inter-
mediately construct a tree automaton Mw that rec-
ognizes the set of all possible trees that have w as
yield, with internal nodes from the input alphabet
of T. This automaton Mw is further transformed
into a tree transducer implementing a partial iden-
tity translation, and such a transducer is composed
with T (relation composition). This is usually
called the ‘cascaded’ approach. Such an approach
can be easily applied also to parsing problems.
In contrast with the cascaded approach above,
which may be rather inefficient, in this paper we
investigate a more direct technique for parsing
strings based on weighted and probabilistic tree
automata. We do this by extending to weighted
tree automata the well-known Bar-Hillel construc-
tion defined for context-free grammars (Bar-Hillel
et al., 1964) and for weighted context-free gram-
mars (Nederhof and Satta, 2003). This provides
an abstract framework under which several pars-
ing algorithms can be directly derived, based on
weighted tree automata. We discuss several appli-
cations of our results, including algorithms for the
computation of parse-forests, best tree probability,
inside probability (called partition function), and
prefix probability.
</bodyText>
<page confidence="0.822919">
1
</page>
<note confidence="0.9595865">
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 1–12,
Paris, October 2009. c�2009 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.918347" genericHeader="method">
2 Preliminary definitions
</sectionHeader>
<bodyText confidence="0.998996">
Let S be a nonempty set and · be an associative
binary operation on S. If S contains an element 1
such that 1 · s = s = s · 1 for every s ∈ S, then
(S, ·,1) is a monoid. A monoid (S, ·,1) is com-
mutative if the equation s1 · s2 = s2 · s1 holds
for every s1, s2 ∈ S. A commutative semiring
(S, +, ·, 0,1) is a nonempty set S on which a bi-
nary addition + and a binary multiplication · have
been defined such that the following conditions are
satisfied:
</bodyText>
<listItem confidence="0.99986725">
• (S, +, 0) and (S, ·,1) are commutative
monoids,
• · distributes over + from both sides, and
• s · 0 = 0 = 0 · s for every s ∈ S.
</listItem>
<bodyText confidence="0.941001666666667">
A weighted string automaton, abbreviated WSA,
(Sch¨utzenberger, 1961; Eilenberg, 1974) is a sys-
tem M = (Q, E, S, I, v, F) where
</bodyText>
<listItem confidence="0.999868571428572">
• Q is a finite alphabet of states,
• E is a finite alphabet of input symbols,
• S = (S, +, ·, 0, 1) is a semiring,
• I : Q → S assigns initial weights,
• v : Q × E × Q → S assigns a weight to each
transition, and
• F : Q → S assigns final weights.
</listItem>
<bodyText confidence="0.995484125">
We now proceed with the semantics of M. Let
w ∈ E* be an input string of length n. For each
integer i with 1 ≤ i ≤ n, we write w(i) to denote
the i-th character of w. The set Pos(w) of posi-
tions of w is {i  |0 ≤ i ≤ n}. A run of M on w
is a mapping r: Pos(w) → Q. We denote the set
of all such runs by RunM(w). The weight of a
run r ∈ RunM(w) is
</bodyText>
<equation confidence="0.981584">
n
wtM(r) = v(r(i − 1), w(i), r(i)) .
z=1
</equation>
<bodyText confidence="0.999769">
We assume the right-hand side of the above equa-
tion evaluates to 1 in case n = 0. The WSA M
recognizes the mapping M : E* → S, which is
defined for every w ∈ E* of length n by1
</bodyText>
<equation confidence="0.9867075">
M(w) = � I(r(0)) · wtM(r) · F(r(n)) .
rERunyl (w)
</equation>
<bodyText confidence="0.9999265">
In order to define weighted tree automata (Bers-
tel and Reutenauer, 1982; ´Esik and Kuich, 2003;
Borchardt, 2005), we need to introduce some addi-
tional notation. Let E be a ranked alphabet, that
</bodyText>
<footnote confidence="0.976286">
1We overload the symbol M to denote both an automaton
and its recognized mapping. However, the intended meaning
will always be clear from the context.
</footnote>
<bodyText confidence="0.995893333333333">
is, an alphabet whose symbols have an associated
arity. We write Ek to denote the set of all k-ary
symbols in E. We use a special symbol e ∈ E0
to syntactically represent the empty string E. The
set of E-trees, denoted by TE, is the smallest set
satisfying both of the following conditions
</bodyText>
<listItem confidence="0.861050375">
• for every α ∈ E0, the single node labeled α,
written α(), is a tree of TE,
• for every a ∈ Ek with k ≥ 1 and for every
t1, ... , tk ∈ TE, the tree with a root node la-
beled a and trees t1, ... , tk as its k children,
written a(t1, ... , tk), belongs to TE.
As a convention, throughout this paper we assume
that a(t1, ... , tk) denotes a() if k = 0. The size
</listItem>
<bodyText confidence="0.93091475">
of the tree t ∈ TE, written |t|, is defined as the
number of occurrences of symbols from E in t.
Let t = a(t1, ... , tk). The yield of t is recur-
sively defined by
</bodyText>
<equation confidence="0.983012666666667">
{ a if a ∈ E0 \ {e}
E ifa=e
yd(t1) · · · yd(tk) otherwise.
</equation>
<bodyText confidence="0.997888">
The set of positions of t, denoted by Pos(t), is
recursively defined by
</bodyText>
<equation confidence="0.99056">
Pos(a(t1, ... , tk)) =
{E} ∪ {iw  |1 ≤ i ≤ k, w ∈ Pos(tz)} .
</equation>
<bodyText confidence="0.98218">
Note that |t |= |Pos(t) |and, according to our con-
vention, when k = 0 the above definition provides
Pos(a()) = {E}. We denote the symbol of t at
position w by t(w) and its rank by rkt(w).
A weighted tree automaton (WTA) is a system
M = (Q, E, S, µ, F) where
</bodyText>
<listItem confidence="0.9988825">
• Q is a finite alphabet of states,
• E is a finite ranked alphabet of input symbols,
• S = (S, +, ·, 0,1) is a semiring,
• µ is an indexed family (µk )kE]N of mappings
µk : Ek → SQ&amp;quot;Qk, and
•
</listItem>
<equation confidence="0.869786">
F : Q → S assigns final weights.
</equation>
<bodyText confidence="0.999016181818182">
In the above definition, Qk is the set of all strings
over Q having length k, with Q0 = {E}. Fur-
ther note that SQ&amp;quot;Qk is the set of all matrices
with elements in S, row index set Q, and column
index set Qk. Correspondingly, we will use the
common matrix notation and write instances of µ
in the form µk(a)q0,q1···qk. Finally, we assume
q1 ··· qk = E if k = 0.
We define the semantics also in terms of runs.
Let t ∈ TE. A run of M on t is a mapping
r: Pos(t) → Q. We denote the set of all such runs
</bodyText>
<equation confidence="0.991161">
yd(t) =
</equation>
<page confidence="0.808044">
2
</page>
<figure confidence="0.788259428571428">
σ
@
α
α
γ δ
α @
β @
σ
@
β α
σ
γ δ
α β σ
β α
</figure>
<bodyText confidence="0.362732">
by Runm(t). The weight of a run r ∈ Runm(t)
is
</bodyText>
<equation confidence="0.951994555555555">
wtm(r) = � µk(t(w))r(w),r(w1)···r(wk) .
wEPos(t)
rkt(w)=k
Note that, according to our convention, the string
r(w1) · · · r(wk) denotes ε when k = 0. The
WTA M recognizes the mapping M : TΣ → S,
which is defined by
M(t) = � wtm(r) · F(r(ε))
rERunM (t)
</equation>
<bodyText confidence="0.995722384615385">
for every t ∈ TΣ. We say that t is recognized
by M if M(t) =6 0.
In our complexity analyses, we use the follow-
ing measures. The size of a transition (p, α, q) in
(the domain of ν in) a WSA is |pαq |= 3. The size
of a transition in a WTA, viewed as an instance
(σ, q0, q1 · · · qk) of some mapping µk, is defined
as |σq0 · · · qk|, that is, the rank of the input symbol
occurring in the transition plus two. Finally, the
size |M |of an automaton M (WSA or WTA) is
defined as the sum of the sizes of its nonzero tran-
sitions. Note that this does not take into account
the size of the representation of the weights.
</bodyText>
<sectionHeader confidence="0.987311" genericHeader="method">
3 Binarization
</sectionHeader>
<bodyText confidence="0.998370130434783">
We introduce in this section a specific transfor-
mation of WTA, called binarization, that reduces
the transitions of the automaton to some normal
form in which no more than three states are in-
volved. This transformation maps the set of rec-
ognized trees into a special binary form, in such a
way that the yields of corresponding trees and their
weights are both preserved. We use this transfor-
mation in the next section in order to guarantee
the computational efficiency of the parsing algo-
rithm we develop. The standard ‘first-child, next-
sibling’ binary encoding for trees (Knuth, 1997)
would eventually result in a transformed WTA of
quadratic size. To obtain instead a linear size
transformation, we introduce a slightly modified
encoding (H¨ogberg et al., 2009, Section 4), which
is inspired by (Carme et al., 2004) and the classical
currying operation.
Let E be a ranked alphabet and assume a
fresh symbol @ ∈/ E (corresponding to the ba-
sic list concatenation operator). Moreover, let
A = A2 ∪ A1 ∪ A0 be the ranked alphabet such
that A2 = {@}, A1 = Uk&gt;1 Ek, and A0 = E0. In
</bodyText>
<figureCaption confidence="0.998865">
Figure 1: Input tree t and encoded tree enc(t).
</figureCaption>
<bodyText confidence="0.9983426">
words, all the original non-nullary symbols from
E are now unary, @ is binary, and the original
nullary symbols from E have their rank preserved.
We encode each tree of TΣ as a tree of TΔ as fol-
lows:
</bodyText>
<listItem confidence="0.9996325">
• enc(α) = α() for every α ∈ E0,
• enc(γ(t)) = γ(enc(t)) for every γ ∈ E1 and
t ∈ TΣ, and
• for k ≥ 2, σ ∈ Ek, and t1, ... , tk ∈ TΣ
</listItem>
<equation confidence="0.9949885">
enc(σ(t1, ... , tk)) =
σ(@(enc(t1), ... @(enc(tk−1), enc(tk)) ··· )).
</equation>
<bodyText confidence="0.999844428571429">
An example of the above encoding is illustrated
in Figure 1. Note that |enc(t) |∈ O(|t|) for every
t ∈ TΣ. Furthermore, t can be easily reconstructed
from enc(t) in linear time.
Definition 1 Let M = (Q, E, S, µ, F) be a WTA.
The encoded WTA enc(M) is (P, A, S, µ&apos;, F&apos;)
where
</bodyText>
<equation confidence="0.918483888888889">
P = {[q]|q ∈ Q} ∪
∪ {[w]  |µk(σ)q,uw =6 0,u ∈ Q*, w ∈ Q+},
F&apos;([q]) = F(q) for every q ∈ Q, and the transi-
tions are constructed as follows:
(i) µ&apos;0(α)[q],ε = µ0(α)q,ε for every α ∈ E0,
(ii) µ&apos;1(σ)[q],[w] = µk(σ)q,w for every σ ∈ Ek,
k ≥ 1, q ∈ Q, and w ∈ Qk, and
(iii) µ&apos;2(@)[qw],[q][w] = 1 for every [qw] ∈ P with
|w |≥ 1 and q ∈ Q.
</equation>
<bodyText confidence="0.98569">
All remaining entries in F&apos; and µ&apos; are 0. ✷
Notice that each transition of enc(M) involves no
more than three states from P. Furthermore, we
have |enc(M) |∈ O(|M|). The following result is
rather intuitive (H¨ogberg et al., 2009, Lemma 4.2);
its proof is therefore omitted.
</bodyText>
<page confidence="0.997066">
3
</page>
<bodyText confidence="0.994486666666667">
Theorem 1 Let M = (Q, E, S, µ, F) be a WTA,
and let M0 = enc(M). Then M(t) = M0(enc(t))
for every t E TE. ❑
</bodyText>
<sectionHeader confidence="0.986286" genericHeader="method">
4 Bar-Hillel construction
</sectionHeader>
<bodyText confidence="0.985279388888889">
The so-called Bar-Hillel construction was pro-
posed in (Bar-Hillel et al., 1964) to show that
the intersection of a context-free language and
a regular language is still a context-free lan-
guage. The proof of the result consisted in an
effective construction of a context-free grammar
Prod(G, N) from a context-free grammar G and
a finite automaton N, such that Prod(G, N) gen-
erates the intersection of the languages generated
by G and N.
It was later recognized that the Bar-Hillel con-
struction constitutes one of the foundations of the
theory of tabular parsing based on context-free
grammars. More precisely, by taking the finite
automaton N to be of some special kind, accept-
ing only a single string, the Bar-Hillel construction
provides a framework under which several well-
known tabular parsing algorithms can easily be de-
rived, that were proposed much later in the litera-
ture.
In this section we extend the Bar-Hillel con-
struction to WTA, with a similar purpose of es-
tablishing an abstract framework under which one
could easily derive parsing algorithms based on
these devices. In order to guarantee computational
efficiency, we avoid here stating the Bar-Hillel
construction for WTA with alphabets of arbitrary
rank. The next result therefore refers to WTA with
alphabet symbols of rank at most 2. These may,
but need not, be automata obtained through the bi-
nary encoding discussed in Section 3.
Definition 2 Let M = (Q, E, S, µ, F) be a WTA
such that the maximum rank of a symbol in E is 2,
and let N = (P, E0 \ 1e}, S, I, ν, G) be a WSA
over the same semiring. We construct the WTA
Prod(M, N) = (P x Q x P, E, S, µ0, F0)
</bodyText>
<listItem confidence="0.873153666666667">
as follows:
(i) For every σ E E2, states p0, p1, p2 E P, and
states q0, q1, q2 E Q let
</listItem>
<equation confidence="0.897735">
µ02(σ)(p0,q0,p2),(p0,q1,p1)(p1,q2,p2) = µ2(σ)q0,q1q2 .
</equation>
<bodyText confidence="0.8121155">
(ii) For every symbol γ E E1, states p0, p1 E P,
and states q0, q1 E Q let
</bodyText>
<equation confidence="0.970528">
µ01(γ)(p0,q0,p1),(p0,q1,p1) = µ1(γ)q0,q1 .
</equation>
<figureCaption confidence="0.972366666666667">
Figure 2: Information transport in the first and
third components of the states in our Bar-Hillel
construction.
</figureCaption>
<bodyText confidence="0.7779915">
(iii) For every symbol α E E0, states p0, p1 E P,
and q E Q let
</bodyText>
<equation confidence="0.995356333333333">
µ00(α)(p0,q,p1),ε = µ0(α)q,ε · s
(iv) F0(p0, q, p1) = I(p0)·F(q)·G(p1) for every
p0, p1 E P and q E Q.
</equation>
<bodyText confidence="0.979935555555556">
All remaining entries in µ0 are 0. ❑
Theorem 2 Let M and N be as in Definition 2,
and let M0 = Prod(M, N). If S is commutative,
then M0(t) = M(t)·N(yd(t)) for every t E TE.❑
PROOF For a state q E P x Q x P, we write qz
to denote its i-th component with i E 11, 2, 3}.
Let t E TE and r E RunM,(t) be a run of M0
on t. We call the run r well-formed if for every
w E Pos(t):
</bodyText>
<listItem confidence="0.884466625">
(i) if t(w) = e, then r(w)1 = r(w)3,
(ii) if t(w) E/ E0, then:
(a) r(w)1 = r(w1)1,
(b) r(wrkt(w))3 = r(w)3, and
(c) if rkt(w) = 2, then r(w1)3 = r(w2)1.
Note that no conditions are placed on the second
components of the states in r. We try to illustrate
the conditions in Figure 2.
</listItem>
<bodyText confidence="0.994263">
A standard proof shows that wtM,(r) = 0 for
all runs r E RunM,(t) that are not well-formed.
We now need to map runs of M0 back into ‘cor-
responding’ runs for M and N. Let us fix some
t E TE and some well-formed run r E RunM,(t).
</bodyText>
<equation confidence="0.9313346">
p0 α p1
p0 e p0
σ
p0 p2
γ
p0 p1
=
=
= =
=
p0 p1
p0 p1 p1 p2
ν(p0,α,p1)
=
where
�
ν(p0, α, p1) if α =� e
s =
α
1 if
</equation>
<bodyText confidence="0.536721">
= e and p0 = p1 .
</bodyText>
<page confidence="0.957328">
4
</page>
<bodyText confidence="0.999573">
We define the run πM(r) E RunM(t) by letting
</bodyText>
<equation confidence="0.710932222222222">
πM(r)(w) = r(w)2,
for every w E Pos(t). Let {w1, ... , wn} =
{w0  |w0 E Pos(t), t(w0) E E0 \ {e}}, with
w1 &lt; ··· &lt; wn according to the lexico-
graphic order on Pos(t). We also define the run
πN(r) E RunN(yd(t)) by letting
πN(r)(i − 1) = r(wi)1,
for every 1 &lt; i &lt; n, and
πN(r)(n) = r(wn)3 .
</equation>
<bodyText confidence="0.999767">
Note that conversely every run of M on t and ev-
ery run of N on yd(t) yield a unique run of M0
on t.
</bodyText>
<equation confidence="0.836694">
Now, we claim that
wtM0(r) = wtM(πM(r)) · wtN(πN(r))
</equation>
<bodyText confidence="0.99722375">
for every well-formed run r E RunM0(t). To
prove the claim, let t = σ(t1, ... , tk) for some
σ E Ek, k &lt; 2, and t1, ... , tk E TE. Moreover,
for every 1 &lt; i &lt; k let ri(w) = r(iw) for every
w E Pos(ti). Note that ri E RunM0(ti) and that
ri is well-formed for every 1 &lt; i &lt; k.
For the induction base, let σ E E0; we can write
In the induction step (i.e., k &gt; 0) we have
</bodyText>
<equation confidence="0.998997333333333">
wtM0(r)
Y= µ0n(t(w))r(w),r(w1)···r(wn)
w∈Pos(t)
rkt(w)=n
= µ0k(σ)r(ε),r(1)···r(k) · Yk wtM0(ri) .
i=1
</equation>
<bodyText confidence="0.9980745">
Using the fact that r is well-formed, commutativ-
ity, and the induction hypothesis, we obtain
</bodyText>
<equation confidence="0.59507">
= wtM(π2(r)) · wtN(πN(r)) ,
</equation>
<bodyText confidence="0.98818875">
where in the last step we have again used the fact
that r is well-formed. Using the auxiliary state-
ment wtM0(r) = wtM(πM(r)) · wtN(πN(r)), the
main proof now is easy.
</bodyText>
<equation confidence="0.998533230769231">
M0(t)
X= wtM0(r) · F0(r(ε))
r∈Run 0(t)
= X wtM(πM(r)) · wtN(πN(r)) ·
r∈Run 0(t)
r well-formed
· I(r(ε)1) · F(r(ε)2) · G(r(ε)3)
~ X �wtM(r) · F (r(ε)) ·
r∈Run (t)
· � X I(r(0)) · wtN(r) · G(r(|w|)) �
w=yd(t)
r∈RunN(w)
= M(t) · N(yd(t)) 0
</equation>
<bodyText confidence="0.9976766875">
Let us analyze now the computational complex-
ity of a possible implementation of the construc-
tion in Definition 2. In step (i), we could restrict
the computation by considering only those transi-
tions in M satisfying µ2(σ)q0,q1q2 =� 0, which pro-
vides a number of choices in O(|M|). Combined
with the choices for the states p0, p1, p2 of N,
this provides O(|M|· |P |3) non-zero transitions
in Prod(M, N). This is also a bound on the over-
all running time of step (i). Since we additionally
assume that weights can be multiplied in constant
time, it is not difficult to see that all of the remain-
ing steps can be accommodated within such a time
bound. We thus conclude that the construction in
Definition 2 can be implemented to run in time and
space O(|M |· |P|3).
</bodyText>
<sectionHeader confidence="0.955972" genericHeader="method">
5 Parsing applications
</sectionHeader>
<bodyText confidence="0.999713333333333">
In this section we discuss several applications of
the construction presented in Definition 2 that are
relevant for parsing based on WTA models.
</bodyText>
<subsectionHeader confidence="0.955549">
5.1 Parse forest
</subsectionHeader>
<bodyText confidence="0.999913666666667">
Parsing is usually defined as the problem of con-
structing a suitable representation for the set of all
possible parse trees that are assigned to a given in-
put string w by some grammar model. The set of
all such parse trees is called parse forest. The ex-
tension of the Bar-Hillel construction that we have
</bodyText>
<equation confidence="0.997265615384615">
wtM0(r)
= µ00(σ)r(ε),ε
(
µ0(σ)r(ε)2,ε · ν(r(ε)1, σ, r(ε)3) if σ =� e
µ0(σ)r(ε)2,ε if σ = e
= wtM(πM(r)) · wtN(πN(r)) .
=
= µk(σ)r(ε)2,r(1)2···r(k)2·
Yk
·
i=1
� �
wtM(πM(ri)) · wtN(πN(ri))
</equation>
<page confidence="0.865431">
5
</page>
<bodyText confidence="0.99992">
presented in Section 4 can be easily adapted to ob-
tain a parsing algorithm for WTA models. This is
described in what follows.
First, we should represent the input string w in
a WSA that recognizes the language {w}. Such
an automaton has a state set P = {p0, ... , p|w|}
and transition weights v(pi_1, w(i), pi) = 1 for
each i with 1 &lt; i &lt; |w|. We also set I(p0) = 1
and F(p|w|) = 1. Setting all the weights to 1 for
a WSA N amounts to ignoring the weights, i.e.,
those weights will not contribute in any way when
applying the Bar-Hillel construction.
Assume now that M is our grammar model,
represented as a WTA. The WTA Prod(M, N)
constructed as in Definition 2 is not necessarily
trim, meaning that it might contain transitions
with non-zero weight that are never used in the
recognition. Techniques for eliminating such use-
less transitions are well-known, see for instance
(G´ecseg and Steinby, 1984, Section II.6), and can
be easily implemented to run in linear time. Once
Prod(M, N) is trim, we have a device that rec-
ognizes all and only those trees that are assigned
by M to the input string w, and the weights of
those trees are preserved, as seen in Theorem 2.
The WTA Prod(M, N) can then be seen as a rep-
resentation of a parse forest for the input string w,
and we conclude that the construction in Defini-
tion 2, combined with some WTA reduction al-
gorithm, represents a parsing algorithm for WTA
models working in cubic time on the length of the
input string and in linear time on the size of the
grammar model.
More interestingly, from the framework devel-
oped in Section 4, one can also design more effi-
cient parsing algorithms based on WTA. Borrow-
ing from standard ideas developed in the litera-
ture for parsing based on context-free grammars,
one can specialize the construction in Definition 2
in such a way that the number of useless transi-
tions generated for Prod(M, N) is considerably
reduced, resulting in a more efficient construction.
This can be done by adopting some search strat-
egy that guides the construction of Prod(M, N)
using knowledge of the input string w as well as
knowledge about the source model M.
As an example, we can apply step (i) only on de-
mand, that is, we process a transition µ2(σ)q0,q1q2
in Prod(M, N) only if we have already computed
non-zero transitions of the form µk1(σ1)q1,w1 and
µk2(σ2)q2,w2, for some σ1 E Ek1, w1 E Qk1 and
σ2 E Ek2, w2 E Qk2 where Q is the state set
of Prod(M, N). The above amounts to a bottom-
up strategy that is also used in the Cocke-Kasami-
Younger recognition algorithm for context-free
grammars (Younger, 1967).
More sophisticated strategies are also possible.
For instance, one could adopt the Earley strategy
developed for context-free grammar parsing (Ear-
ley, 1970). In this case, parsing is carried out in
a top-down left-to-right fashion, and the binariza-
tion construction of Section 3 is carried out on the
flight. This has the additional advantage that it
would be possible to use WTA models that are not
restricted to the special normal form of Section 3,
still maintaining the cubic time complexity in the
length of the input string. We do not pursue this
idea any further in this paper, since our main goal
here is to outline an abstract framework for pars-
ing based on WTA models.
</bodyText>
<subsectionHeader confidence="0.996962">
5.2 Probabilistic tree automata
</subsectionHeader>
<bodyText confidence="0.999155833333333">
Let us now look into specific semirings that are
relevant for statistical natural language process-
ing. The semiring of non-negative real numbers
is 1R,&gt;0 = (1R,&gt;0, +, ·, 0, 1). For the remainder of
the section, let M = (Q, E, 1R,&gt;0, µ, F) be a WTA
over 1R,&gt;0. M is convergent if
</bodyText>
<equation confidence="0.6918285">
� M(t) &lt; 00.
tETΣ
</equation>
<bodyText confidence="0.9854521">
We say that M is a probabilistic tree automa-
ton (Ellis, 1971; Magidor and Moran, 1970),
or PTA for short, if µk(σ)q,q1···qk E [0, 1]
and F(q) E [0, 1], for every σ E Ek and
q, q1, ... , qk E Q. In other words, in a PTA all
weights are in the range [0, 1] and can be inter-
preted as probabilities. For a PTA M we therefore
write pM(r) = wt(r) and pM(t) = M(t), for
each t E TΣ and r E RunM(t).
A PTA is proper if EqEQ F(q) = 1 and
</bodyText>
<equation confidence="0.8496325">
� µk(σ)q,w = 1
σEΣk,k&gt;0,wEQk
</equation>
<bodyText confidence="0.999447">
for every q E Q. Since the set of symbols is finite,
we could have only required that the sum over all
weights as shown with w E Qk equals 1 for every
q E Q and σ E Ek. A simple rescaling would then
be sufficient to arrive at our notion. Furthermore, a
PTA is consistent if EtETΣ pM(t) = 1. If a PTA
is consistent, then pM is a probability distribution
over the set TΣ.
</bodyText>
<page confidence="0.995852">
6
</page>
<bodyText confidence="0.999077285714286">
The WTA M is unambiguous if for every input
tree t ∈ TΣ, there exists at most one r ∈ RunM(t)
such that r(ε) ∈ F and wtM(r) =6 0. In other
words, in an unambiguous WTA, there exists at
most one successful run for each input tree. Fi-
nally, M is in final-state normal form if there ex-
ists a state qS ∈ Q such that
</bodyText>
<listItem confidence="0.998737">
• F(qS) = 1,
• F(q) = 0 for every q ∈ Q \ {qS}, and
• µk(σ)q,w = 0 if w(i) = qS for some
1 ≤ i ≤ k.
</listItem>
<bodyText confidence="0.996837571428571">
We commonly denote the unique final state by qS.
For the following result we refer the reader
to (Droste et al., 2005, Lemma 4.8) and (Bozapa-
lidis, 1999, Lemma 22). The additional properties
mentioned in the items of it are easily seen.
Theorem 3 For every WTA M there exists an
equivalent WTA M&apos; in final-state normal form.
</bodyText>
<listItem confidence="0.9785402">
• If M is convergent (respectively, proper, con-
sistent), then M&apos; is such, too.
• If M is unambiguous, then M&apos; is
also unambiguous and for every
t ∈ TΣ and r ∈ RunM(t) we have
</listItem>
<equation confidence="0.869637666666667">
wtM,(r&apos;) = wtM(r) · F(r(ε)) where
r&apos;(ε) = qS and r&apos;(w) = r(w) for every
w ∈ Pos(t) \ {ε}. ✷
</equation>
<bodyText confidence="0.999922294117647">
It is not difficult to see that a proper PTA in
final-state normal form is always convergent.
In statistical parsing applications we use gram-
mar models that induce a probability distribution
on the set of parse trees. In these applications,
there is often the need to visit a parse tree with
highest probability, among those in the parse for-
est obtained from the input sentence. This imple-
ments a form of disambiguation, where the most
likely tree under the given model is selected, pre-
tending that it provides the most likely syntactic
analysis of the input string. In our setting, the
above approach reduces to the problem of ‘unfold-
ing’ a tree from a PTA Prod(M, N), that is as-
signed the highest probability.
In order to find efficient solutions for the above
problem, we make the following two assumptions.
</bodyText>
<listItem confidence="0.909095">
• M is in final-state normal form. By Theo-
rem 3 this can be achieved without loss of
generality.
• M is unambiguous. This restrictive assump-
</listItem>
<bodyText confidence="0.98171575">
tion avoids the so-called ‘spurious’ ambigu-
ity, that would result in several computations
in the model for an individual parse tree.
It is not difficult to see that PTA satisfying these
</bodyText>
<listItem confidence="0.735446230769231">
1: Function BESTPARSE(M)
2: E ← ∅
3: repeat
4: A ← {q  |µk(σ)q,q1···qk &gt; 0, q ∈/ E,
q1,...,qk ∈ E}
5: for all q ∈ Ado
6: δ(q) ← max
σEΣk,k&gt;0
q1,...,qkE£
7: E ← E ∪ {argmax
qEA
8: until qS ∈ E
9: return δ(qS)
</listItem>
<figureCaption confidence="0.979481333333333">
Figure 3: Search algorithm for the most probable
parse in an unambiguous PTA M in final-state nor-
mal form.
</figureCaption>
<bodyText confidence="0.994438529411765">
two properties are still more powerful than the
probabilistic context-free grammar models that are
commonly used in statistical natural language pro-
cessing.
Once more, we borrow from the literature on
parsing for context-free grammars, and adapt a
search algorithm developed by Knuth (1977); see
also (Nederhof, 2003). The basic idea here is
to generalize Dijkstra’s algorithm to compute the
shortest path in a weighted graph. The search al-
gorithm is presented in Figure 3.
The algorithm takes as input a trim PTA M that
recognizes at least one parse tree. We do not im-
pose any bound on the rank of the alphabet sym-
bols for M. Furthermore, M needs not be a proper
PTA. In order to simplify the presentation, we pro-
vide the algorithm in a form that returns the largest
probability assigned to some tree by M.
The algorithm records into the δ(q) variables
the largest probability found so far for a run that
brings M into state q, and stores these states into
an agenda A. States for which δ(q) becomes opti-
mal are popped from A and stored into a set E.
Choices are made on a greedy base. Note that
when a run has been found leading to an optimal
probability δ(q), from our assumption we know
that the associated tree has only one run that ends
up in state q.
Since E is initially empty (line 2), only weights
satisfying µ0(σ)q,ε &gt; 0 are considered when line 4
is executed for the first time. Later on (line 7)
the largest probability is selected among all those
that can be computed at this time, and the set E is
populated. As a consequence, more states become
</bodyText>
<equation confidence="0.9698162">
µk(σ)q,q1···qk ·
δ(q)}
k
i=1
δ(qi)
</equation>
<page confidence="0.98658">
7
</page>
<bodyText confidence="0.997349214285714">
available in the agenda in the next iteration, and
new transitions can now be considered. The algo-
rithm ends when the largest probability has been
calculated for the unique final state qS.
We now analyze the computational complexity
of the algorithm in Figure 3. The ‘repeat-until’
loop runs at most |Q |times. Entirely reprocess-
ing set A at each iteration would be too expensive.
We instead implement A as a priority heap and
maintain a clock for each weight µk(σ)q,q1···qk,
initially set to k. Whenever a new optimal proba-
bility δ(q) becomes available through £, we decre-
ment the clock associated with each µk(σ)q,q1···qk
by d, in case d &gt; 0 occurrences of q are found
in the string q1 · · · qk. In this way, at each it-
eration of the ‘repeat-until’ loop, we can con-
sider only those weights µk(σ)q,q1···qk with asso-
ciated clock of zero, compute new values δ(q),
and update the heap. For each µk(σ)q,q1···qk &gt; 0,
all clock updates and the computation of quan-
tity µk(σ)q,q1···qk ·Hki=1 δ(qi) (when the associ-
ated clock becomes zero) both take an amount of
time proportional to the length of the transition
itself. The overall time to execute these opera-
tions is therefore linear in |M|. Accounting for
the heap, the algorithm has overall running time
in O(|M |+ |Q |log|Q|).
The algorithm can be easily adapted to return a
tree having probability δ(qS), if we keep a record
of all transitions selected in the computation along
with links from a selected transition and all of the
previously selected transitions that have caused its
selection. If we drop the unambiguity assump-
tion for the PTA, then the problem of comput-
ing the best parse tree becomes NP-hard, through
a reduction from similar problems for finite au-
tomata (Casacuberta and de la Higuera, 2000). In
contrast, the problem of computing the probability
of all parse trees of a string, also called the inside
probability, can be solved in polynomial time in
most practical cases and will be addressed in Sub-
section 5.4.
</bodyText>
<subsectionHeader confidence="0.972957">
5.3 Normalization
</subsectionHeader>
<bodyText confidence="0.999001454545455">
Consider the WTA Prod(M, N) obtained as in
Definition 2. If N is a WSA encoding an in-
put string w as in Subsection 5.1 and if M is a
proper and consistent PTA, then Prod(M, N) is
a PTA as well. However, in general Prod(M, N)
will not be proper, nor consistent. Properness and
consistency of Prod(M, N) are convenient in all
those applications where a statistical parsing mod-
ule needs to be coupled with other statistical mod-
ules, in such a way that the composition of the
probability spaces still induces a probability dis-
tribution. In this subsection we deal with the more
general problem of how to transform a WTA that
is convergent into a PTA that is proper and con-
sistent. This process is called normalization. The
normalization technique we propose here has been
previously explored, in the context of probabilis-
tic context-free grammars, in (Abney et al., 1999;
Chi, 1999; Nederhof and Satta, 2003).
We start by introducing some new notions. Let
us assume that M is a convergent WTA. For every
q E Q, we define
</bodyText>
<equation confidence="0.982529333333333">
wtM(q) = � wtM(r) .
t∈TΣ,r∈RunM (t)
r(E)=q
</equation>
<bodyText confidence="0.998224647058824">
Note that quantity wtM(q) equals the sum of the
weights of all trees in TE that would be recognized
by M if we set F(q) = 1 and F(p) = 0 for each
p E Q \ {q}, that is, if q is the unique final state
of M. It is not difficult to show that, since M is
convergent, the sum in the definition of wtM(q)
converges for each q E Q. We will show in Sub-
section 5.4 that the quantities wtM(q) can be ap-
proximated to any desired precision.
To simplify the presentation, and without any
loss of generality, throughout this subsection we
assume that our WTA are in final-state normal
form. We can now introduce the normalization
technique.
Definition 3 Let M = (Q, E, &gt;R,≥0, µ, F) be a
convergent WTA in final-state normal form. We
construct the WTA
</bodyText>
<equation confidence="0.552394">
Norm(M) =(Q, E, &gt;R,≥0, µ0, F) ,
</equation>
<bodyText confidence="0.997105">
where for every σ E Ek, k &gt; 0, and
q,q1, ... ,qk E Q
</bodyText>
<equation confidence="0.99779925">
µ0k(σ)q,q1···qk = µk(σ)q,q1···qk ·
wtM(q1) · ... · wtM(qk)
· .❑
wtM(q)
</equation>
<bodyText confidence="0.956468">
We now show the claimed property for our
transformation.
Theorem 4 Let M be as in Definition 3, and let
M0 = Norm(M). Then M0 is a proper and
consistent PTA, and for every t E TE we have
</bodyText>
<equation confidence="0.8892575">
M0(t) = M(t). ❑
wtM(qS)
8
wtM(r) M(t)
wtM(qS) =
wtM(qS)
</equation>
<bodyText confidence="0.584776333333333">
and
PROOF Clearly, M0 is again in final-state normal
form. An easy derivation shows that
</bodyText>
<equation confidence="0.99298275">
k
wtM(q) = X µk(σ)q,q1···qk · Y wtM(qi)
σ∈Ek i=1
q1,...,qk∈Q
</equation>
<bodyText confidence="0.99225">
for every q E Q. Using the previous remark, we
obtain
</bodyText>
<equation confidence="0.983784307692308">
X=
r∈RunM (t)
r(E)=qS
X M0(t) = X
t∈TΣ t∈TΣ,r∈RunM0(t)
r(E)=qS
wtM0(r)
wtM(r)
wtM(qS)
wtM(qS)
=
wtM(qS)
= 1 ,
</equation>
<bodyText confidence="0.99971">
which prove the main statement and the consis-
tency of M0, respectively.
</bodyText>
<subsectionHeader confidence="0.987522">
5.4 Probability mass of a state
</subsectionHeader>
<bodyText confidence="0.999957592592593">
Assume M is a convergent WTA. We have defined
quantities wtM(q) for each q E Q. Note that when
M is a proper PTA in final-state normal form, then
wtM(q) can be seen as the probability mass that
‘rests’ on state q. When dealing with such PTA,
we use the notation ZM(q) in place of wtM(q),
and call ZM the partition function of M. This
terminology is borrowed from the literature on ex-
ponential or Gibbs probabilistic models.
In the context of probabilistic context-free
grammars, the computation of the partition func-
tion has several applications, including the elim-
ination of epsilon rules (Abney et al., 1999) and
the computation of probabilistic distances between
probability distributions realized by these for-
malisms (Nederhof and Satta, 2008). Besides
what we have seen in Subsection 5.3, we will pro-
vide one more application of partition functions
for the computations of so-called prefix probabil-
ities in Subsection 5.5 We also add that, when
computed on the Bar-Hillel automata of Section 4,
the partition function provides the so-called inside
probabilities of (Graehl et al., 2008) for the given
states and substrings.
Let |Q |= n and let us assume an arbitrary or-
dering q1, ... , qn for the states in Q. We can then
rewrite the definition of wtM(q) as
</bodyText>
<equation confidence="0.9933465">
k
wtM(q) = X µk(σ)q,qi1···qik · Y wtM(qij)
σ∈Ek,k≥0 j=1
qi1,...,qik ∈Q
</equation>
<bodyText confidence="0.9943855">
(see proof of Theorem 4). We rename wtM(qi)
with the unknown Xqi, 1 &lt; i &lt; n, and derive a
</bodyText>
<equation confidence="0.998711952380952">
= X
t∈TΣ,r∈RunM(t)
r(E)=qS
X 0(
σ∈Ek,q1,...,qk∈Q µklσ)q,q1···qk
X= µk(σ)q,q1···qk ·
σ∈Ek,q1,...,qk∈Q
wtM(q1) · . . . · wtM(qk0)
·
wtM(q)
k
X µk(σ)q,q1···qk · Y wtM(qi)
σ∈Ek, i=1
q1,...,qk∈Q
=
k
X µk(σ)q,p1···pk · Y wtM(pi)
σ∈Ek, i=1
p1,...,pk∈Q
wtM0(r) = ((ε))
wtMr
</equation>
<bodyText confidence="0.665598666666667">
for every r E RunM(t) = RunM0(t). For ev-
ery 1 &lt; i &lt; k, let ri E RunM(ti) be such that
ri(w) = r(iw) for every w E Pos(ti). Then
</bodyText>
<equation confidence="0.919618222222222">
YwtM0(r) = µ0n(t(w))r(w),r(w1)···r(wn)
w∈Pos(t)
rkt(w)=n
k
= µ0k(σ)r(E),r(1)···r(k) · Y wtM0(ri)
i=1
Yk
= µ0k(σ)r(E),r1(E)···rk(E) ·
i=1
wtM(r1) ·· ·· · wtM(rk)
= µk(σ)r(E),r(1)···r(k) · wtM(r(ε))
wtM(r)
= 1 ,
which proves that M0 is a proper PTA.
Next, we prove an auxiliary statement. Let
t = σ(t1, ... , tk) for some σ E Ek, k &gt; 0, and
t1, ... , tk E TE. We claim that
wtM(r)
wtM(ri)
wtM(ri(ε))
=
wtM(r(ε)) . wtM0(r)
Consequently,
X
M0(t) =
r∈RunM0(t)
r(E)=qS
</equation>
<page confidence="0.951059">
9
</page>
<bodyText confidence="0.968881">
system of n nonlinear polynomial equations of the
form
</bodyText>
<equation confidence="0.959505">
�Xqi = µk(σ)q,qi1···qik · Xqi1 · ... · Xqik
σ∈Ek,k≥0
qi1,...,qik ∈Q
= fqi(Xq1, ... ,Xqn) , (1)
for each i with 1 ≤ i ≤ n.
</equation>
<bodyText confidence="0.999172333333333">
Throughout this subsection, we will consider
solutions of the above system in the extended non-
negative real number semiring
</bodyText>
<equation confidence="0.992189">
R∞≥0 = (R≥0 ∪ {∞}, +, ·, 0,1)
</equation>
<bodyText confidence="0.999732">
with the usual operations extended to ∞. We
can write the system in (1) in the compact form
X = F(X), where we represent the unknowns
as a vector X = (Xq1, ... , Xqn) and F is a map-
ping of type (R∞≥0)n → (R∞≥0)n consisting of the
polynomials fqi(X).
We denote the vector (0, ... , 0) ∈ (R∞≥0)n as
X0. Let X, X0 ∈ (R∞≥0)n. We write X ≤ X0
if Xqi ≤ X0qi for every 1 ≤ i ≤ n. Since
each polynomial fqi(X) has coefficients repre-
sented by positive real numbers, it is not difficult
to see that, for each X, X0 ∈ (R∞≥0)n, we have
F(X) ≤ F(X0) whenever X0 ≤ X ≤ X0. This
means that F is an order preserving, or monotone,
mapping.
We observe that ((R∞≥0)n, ≤) is a complete
lattice with least element X0 and greatest el-
ement (∞, ... , ∞). Since F is monotone on
a complete lattice, by the Knaster-Tarski theo-
rem (Knaster, 1928; Tarski, 1955) there exists a
least and a greatest fixed-point of F that are solu-
tions of X = F (X).
The Kleene theorem states that the least fixed-
point solution of X = F(X) can be obtained
by iterating F starting with the least element X0.
In other words, the sequence Xk = F(Xk−1),
k = 1, 2, ... converges to the least fixed-point so-
lution. Notice that each Xk provides an approxi-
mation for the partition function of M where only
trees of depth not larger than k are considered.
This means that limk→∞ Xk converges to the par-
tition function of M, and the least fixed-point so-
lution is also the sought solution. Thus, we can
approximate wtm(q) with q ∈ Q to any degree by
iterating F a sufficiently large number of times.
The fixed-point iteration method discussed
above is also well-known in the numerical calcu-
lus literature, and is frequently applied to systems
of nonlinear equations in general, because it can
be easily implemented. When a number of stan-
dard conditions are met, each iteration of the algo-
rithm (corresponding to the value of k above) adds
a fixed number of bits to the precision of the ap-
proximated solution; see (Kelley, 1995) for further
discussion.
Systems of the form X = F(X) where all
fqi(X) are polynomials with nonnegative real co-
efficients are called monotone system of poly-
nomials. Monotone systems of polynomials as-
sociated with proper PTA have been specifically
investigated in (Etessami and Yannakakis, 2005)
and (Kiefer et al., 2007), where worst case results
on exponential rate of convergence are reported
for the fixed-point method.
</bodyText>
<subsectionHeader confidence="0.974159">
5.5 Prefix probability
</subsectionHeader>
<bodyText confidence="0.99993996969697">
In this subsection we deal with one more applica-
tion of the Bar-Hillel technique presented in Sec-
tion 4. We show how to compute the so-called
prefix probabilities, that is, the probability that a
tree recognized by a PTA generates a string start-
ing with a given prefix. Such probabilities have
several applications in language modeling. As an
example, prefix probabilities can be used to com-
pute the probability distribution on the terminal
symbol that follows a given prefix (under the given
model).
For probabilistic context-free grammars, the
problem of the computation of prefix probabili-
ties has been solved in (Jelinek et al., 1992); see
also (Persoon and Fu, 1975). The approach we
propose here, originally formulated for probabilis-
tic context-free grammars in (Nederhof and Satta,
2003; Nederhof and Satta, 2009), is more abstract
than the previous ones, since it entirely rests on
properties of the Bar-Hillel construction that we
have already proved in Section 4.
Let M = (Q, E, R≥0, µ, F) be a proper
and consistent PTA in final-state normal form,
A = E0 \ {e}, and let u ∈ A+ be some string.
We assume here that M is in the binary form
discussed in Section 3. In addition, we assume
that M has been preprocessed in order to remove
from its recognized trees all of the unary branches
as well as those branches that generate the null
string E. Although we do not discuss this con-
struction at length in this paper, the result follows
from a transformation casting weighted context-
free grammars into Chomsky Normal Form (Fu
</bodyText>
<page confidence="0.994374">
10
</page>
<bodyText confidence="0.9987515">
and Huang, 1972; Abney et al., 1999).
We define
</bodyText>
<equation confidence="0.997329">
Pref(M, u) = {t  |t ∈ TE, M(t) &gt; 0,
yd(t) = uv, v ∈ A*} .
</equation>
<bodyText confidence="0.958279125">
The prefix probability of u under M is defined as
� pM(t) .
tEPref(M,u)
Let |u |= n. We define a WSA Nu with state
set P = {p0, ... , pn} and transition weights
ν(pi−1, u(i), pi) = 1 for each i with 1 ≤ i ≤ n,
and ν(pn, σ, pn) = 1 for each σ ∈ A. We also
set I(p0) = 1 and F(pn) = 1. It is easy to see
that Nu recognizes the language {uv  |v ∈ A*}.
Furthermore, the PTA Mp = Prod(M, Nu) spec-
ified as in Definition 2 recognizes the desired tree
set Pref(M, u), and it preserves the weights of
those trees with respect to M. We therefore con-
clude that ZM,(qS) is the prefix probability of u
under M. Prefix probabilities can then be approx-
imated using the fixed-point iteration method of
Subsection 5.4. Rather than using an approxima-
tion method, we discuss in what follows how the
prefix probabilities can be exactly computed.
Let us consider more closely the product au-
tomaton Mp, assuming that it is trim. Each state
of Mp has the form 7r = (pi, q, pj), pi, pj ∈ P and
q ∈ Q, with i ≤ j. We distinguish three, mutually
exclusive cases.
(i) j &lt; n: From our assumption that M (and
thus Mp) does not have unary or E branches,
it is not difficult to see that all ZM,(7r) can be
exactly computed in time O((j − i)3).
(ii) i = j = n: We have 7r = (pn, q, pn).
Then the equations for ZM,(7r) exactly
mirror the equations for ZM(q), and
ZM,(7r) = ZM,(q). Because M is proper
and consistent, this means that ZM,(7r) = 1.
(iii) i &lt; j = n: A close inspection of Definition 2
reveals that in this case the equations (1) are
all linear, assuming that we have already re-
placed the solutions from (i) and (ii) above
into the system. This is because any weight
A2(σ)π0,π1π &gt; 0 in Mp with 7r = (pi, q, pn)
and i &lt; n must have (7r1)3 &lt; n. Quanti-
ties ZM,(7r) can then be exactly computed as
the solution of a linear system of equations in
time O(n3).
Putting together all of the observations above,
we obtain that for a proper and consistent PTA that
has been preprocessed, the prefix probability of u
can be computed in cubic time in the length of the
prefix itself.
</bodyText>
<sectionHeader confidence="0.972704" genericHeader="method">
6 Concluding remarks
</sectionHeader>
<bodyText confidence="0.999845407407407">
In this paper we have extended the Bar-Hillel con-
struction to WTA, closely following the method-
ology proposed in (Nederhof and Satta, 2003) for
weighted context-free grammars. Based on the ob-
tained framework, we have derived several parsing
algorithms for WTA, under the assumption that the
input is a string rather than a tree.
As already remarked in the introduction, WTA
are richer models than weighted context-free
grammar, since the formers use hidden states in
the recognition of trees. This feature makes it
possible to define a product automaton in Defini-
tion 2 that generates exactly those trees of interest
for the input string. In contrast, in the context-
free grammar case the Bar-Hillel technique pro-
vides trees that must be mapped to the tree of in-
terest using some homomorphism. For the same
reason, one cannot directly convert WTA into
weighted context-free grammars and then apply
existing parsing algorithms for the latter formal-
ism, unless the alphabet of nonterminal symbols
is changed. Finally, our main motivation in de-
veloping a framework specifically based on WTA
is that this can be extended to classes of weighted
tree transducers, in order to deal with computa-
tional problems that arise in machine translation
applications. We leave this for future work.
</bodyText>
<sectionHeader confidence="0.998436" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998718">
The first author has been supported by the Minis-
terio de Educaci´on y Ciencia (MEC) under grant
JDCI-2007-760. The second author has been par-
tially supported by MIUR under project PRIN No.
2007TJNZRE 002.
</bodyText>
<sectionHeader confidence="0.998948" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999170909090909">
S. Abney, D. McAllester, and F. Pereira. 1999. Relat-
ing probabilistic grammars and automata. In 37th
Annual Meeting of the Association for Computa-
tional Linguistics, Proceedings of the Conference,
pages 542–549, Maryland, USA, June.
Y. Bar-Hillel, M. Perles, and E. Shamir. 1964. On
formal properties of simple phrase structure gram-
mars. In Y. Bar-Hillel, editor, Language and Infor-
mation: Selected Essays on their Theory and Appli-
cation, chapter 9, pages 116–150. Addison-Wesley,
Reading, Massachusetts.
</reference>
<page confidence="0.989841">
11
</page>
<reference confidence="0.999908153846154">
J. Berstel and C. Reutenauer. 1982. Recognizable for-
mal power series on trees. Theoret. Comput. Sci.,
18(2):115–148.
B. Borchardt. 2005. The Theory of Recognizable Tree
Series. Ph.D. thesis, Technische Universit¨at Dres-
den.
S. Bozapalidis. 1999. Equational elements in additive
algebras. Theory Comput. Systems, 32(1):1–33.
J. Carme, J. Niehren, and M. Tommasi. 2004. Query-
ing unranked trees with stepwise tree automata. In
Proc. RTA, volume 3091 of LNCS, pages 105–118.
Springer.
F. Casacuberta and C. de la Higuera. 2000. Com-
putational complexity of problems on probabilis-
tic grammars and transducers. In L. Oliveira, edi-
tor, Grammatical Inference: Algorithms and Appli-
cations; 5th International Colloquium, ICGI 2000,
pages 15–24. Springer.
Z. Chi. 1999. Statistical properties of probabilistic
context-free grammars. Computational Linguistics,
25(1):131–160.
M. Droste, C. Pech, and H. Vogler. 2005. A Kleene
theorem for weighted tree automata. Theory Com-
put. Systems, 38(1):1–38.
J. Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the ACM, 13(2):94–102,
February.
S. Eilenberg. 1974. Automata, Languages, and Ma-
chines, volume 59 of Pure and Applied Math. Aca-
demic Press.
C. A. Ellis. 1971. Probabilistic tree automata. Infor-
mation and Control, 19(5):401–416.
Z. ´Esik and W. Kuich. 2003. Formal tree series. J.
Autom. Lang. Combin., 8(2):219–285.
K. Etessami and M. Yannakakis. 2005. Recursive
Markov chains, stochastic grammars, and monotone
systems of nonlinear equations. In 22nd Interna-
tional Symposium on Theoretical Aspects of Com-
puter Science, volume 3404 of Lecture Notes in
Computer Science, pages 340–352, Stuttgart, Ger-
many. Springer-Verlag.
K.S. Fu and T. Huang. 1972. Stochastic grammars and
languages. International Journal of Computer and
Information Sciences, 1(2):135–170.
F. G´ecseg and M. Steinby. 1984. Tree Automata.
Akad´emiai Kiad´o, Budapest.
J. Graehl, K. Knight, and J. May. 2008. Training tree
transducers. Comput. Linguist., 34(3):391–427.
J. H¨ogberg, A. Maletti, and H. Vogler. 2009. Bisim-
ulation minimisation of weighted automata on un-
ranked trees. Fundam. Inform. to appear.
F. Jelinek, J.D. Lafferty, and R.L. Mercer. 1992. Basic
methods of probabilistic context free grammars. In
P. Laface and R. De Mori, editors, Speech Recogni-
tion and Understanding — Recent Advances, Trends
and Applications, pages 345–360. Springer-Verlag.
M. Johnson. 1998. PCFG models of linguistic
tree representations. Computational Linguistics,
24(4):613–632.
C. T. Kelley. 1995. Iterative Methods for Linear and
Nonlinear Equations. Society for Industrial and Ap-
plied Mathematics, Philadelphia, PA.
S. Kiefer, M. Luttenberger, and J. Esparza. 2007. On
the convergence of Newton’s method for monotone
systems of polynomial equations. In Proceedings of
the 39th ACM Symposium on Theory of Computing,
pages 217–266.
B. Knaster. 1928. Un th´eor`eme sur les fonctions
d’ensembles. Ann. Soc. Polon. Math., 6:133–134.
D. E. Knuth. 1977. A generalization of Dijkstra’s al-
gorithm. Information Processing Letters, 6(1):1–5,
February.
D. E. Knuth. 1997. Fundamental Algorithms. The Art
of Computer Programming. Addison Wesley, 3rd
edition.
M. Magidor and G. Moran. 1970. Probabilistic tree
automata and context free languages. Israel Journal
of Mathematics, 8(4):340–348.
M.-J. Nederhof and G. Satta. 2003. Probabilistic pars-
ing as intersection. In 8th International Workshop
on Parsing Technologies, pages 137–148, LORIA,
Nancy, France, April.
M.-J. Nederhof and G. Satta. 2008. Computation of
distances for regular and context-free probabilistic
languages. Theoretical Computer Science, 395(2-
3):235–254.
M.-J. Nederhof and G. Satta. 2009. Computing parti-
tion functions of PCFGs. Research on Language &amp;
Computation, 6(2):139–162.
M.-J. Nederhof. 2003. Weighted deductive parsing
and Knuth’s algorithm. Computational Linguistics,
29(1):135–143.
E. Persoon and K.S. Fu. 1975. Sequential classi-
fication of strings generated by SCFG’s. Interna-
tional Journal of Computer and Information Sci-
ences, 4(3):205–217.
M. P. Sch¨utzenberger. 1961. On the definition of a
family of automata. Information and Control, 4(2–
3):245–270.
A. Tarski. 1955. A lattice-theoretical fixpoint theorem
and its applications. Pacific J. Math., 5(2):285–309.
D. H. Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information and
Control, 10:189–208.
</reference>
<page confidence="0.998461">
12
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.667458">
<title confidence="0.999925">Parsing Algorithms based on Tree Automata</title>
<author confidence="0.99989">Andreas Maletti Giorgio Satta</author>
<affiliation confidence="0.999528">Departament de Filologies Rom`aniques Department of Information Engineering</affiliation>
<address confidence="0.864632">Universitat Rovira i Virgili, Tarragona, Spain University of Padua, Italy</address>
<email confidence="0.929864">andreas.maletti@urv.catsatta@dei.unipd.it</email>
<abstract confidence="0.986520928571428">We investigate several algorithms related to the parsing problem for weighted automata, under the assumption that the input is a string rather than a tree. This assumption is motivated by several natural language processing applications. We provide algorithms for the computation of parse-forests, best tree probability, inside probability (called partition function), and prefix probability. Our algorithms are obtained by extending to weighted tree automata the Bar-Hillel technique, as defined for context-free grammars.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abney</author>
<author>D McAllester</author>
<author>F Pereira</author>
</authors>
<title>Relating probabilistic grammars and automata.</title>
<date>1999</date>
<booktitle>In 37th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference,</booktitle>
<pages>542--549</pages>
<location>Maryland, USA,</location>
<contexts>
<context position="29069" citStr="Abney et al., 1999" startWordPosition="5524" endWordPosition="5527"> consistent. Properness and consistency of Prod(M, N) are convenient in all those applications where a statistical parsing module needs to be coupled with other statistical modules, in such a way that the composition of the probability spaces still induces a probability distribution. In this subsection we deal with the more general problem of how to transform a WTA that is convergent into a PTA that is proper and consistent. This process is called normalization. The normalization technique we propose here has been previously explored, in the context of probabilistic context-free grammars, in (Abney et al., 1999; Chi, 1999; Nederhof and Satta, 2003). We start by introducing some new notions. Let us assume that M is a convergent WTA. For every q E Q, we define wtM(q) = � wtM(r) . t∈TΣ,r∈RunM (t) r(E)=q Note that quantity wtM(q) equals the sum of the weights of all trees in TE that would be recognized by M if we set F(q) = 1 and F(p) = 0 for each p E Q \ {q}, that is, if q is the unique final state of M. It is not difficult to show that, since M is convergent, the sum in the definition of wtM(q) converges for each q E Q. We will show in Subsection 5.4 that the quantities wtM(q) can be approximated to a</context>
<context position="31426" citStr="Abney et al., 1999" startWordPosition="5970" endWordPosition="5973"> a state Assume M is a convergent WTA. We have defined quantities wtM(q) for each q E Q. Note that when M is a proper PTA in final-state normal form, then wtM(q) can be seen as the probability mass that ‘rests’ on state q. When dealing with such PTA, we use the notation ZM(q) in place of wtM(q), and call ZM the partition function of M. This terminology is borrowed from the literature on exponential or Gibbs probabilistic models. In the context of probabilistic context-free grammars, the computation of the partition function has several applications, including the elimination of epsilon rules (Abney et al., 1999) and the computation of probabilistic distances between probability distributions realized by these formalisms (Nederhof and Satta, 2008). Besides what we have seen in Subsection 5.3, we will provide one more application of partition functions for the computations of so-called prefix probabilities in Subsection 5.5 We also add that, when computed on the Bar-Hillel automata of Section 4, the partition function provides the so-called inside probabilities of (Graehl et al., 2008) for the given states and substrings. Let |Q |= n and let us assume an arbitrary ordering q1, ... , qn for the states i</context>
<context position="37416" citStr="Abney et al., 1999" startWordPosition="7046" endWordPosition="7049"> proved in Section 4. Let M = (Q, E, R≥0, µ, F) be a proper and consistent PTA in final-state normal form, A = E0 \ {e}, and let u ∈ A+ be some string. We assume here that M is in the binary form discussed in Section 3. In addition, we assume that M has been preprocessed in order to remove from its recognized trees all of the unary branches as well as those branches that generate the null string E. Although we do not discuss this construction at length in this paper, the result follows from a transformation casting weighted contextfree grammars into Chomsky Normal Form (Fu 10 and Huang, 1972; Abney et al., 1999). We define Pref(M, u) = {t |t ∈ TE, M(t) &gt; 0, yd(t) = uv, v ∈ A*} . The prefix probability of u under M is defined as � pM(t) . tEPref(M,u) Let |u |= n. We define a WSA Nu with state set P = {p0, ... , pn} and transition weights ν(pi−1, u(i), pi) = 1 for each i with 1 ≤ i ≤ n, and ν(pn, σ, pn) = 1 for each σ ∈ A. We also set I(p0) = 1 and F(pn) = 1. It is easy to see that Nu recognizes the language {uv |v ∈ A*}. Furthermore, the PTA Mp = Prod(M, Nu) specified as in Definition 2 recognizes the desired tree set Pref(M, u), and it preserves the weights of those trees with respect to M. We theref</context>
</contexts>
<marker>Abney, McAllester, Pereira, 1999</marker>
<rawString>S. Abney, D. McAllester, and F. Pereira. 1999. Relating probabilistic grammars and automata. In 37th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference, pages 542–549, Maryland, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bar-Hillel</author>
<author>M Perles</author>
<author>E Shamir</author>
</authors>
<title>On formal properties of simple phrase structure grammars.</title>
<date>1964</date>
<booktitle>Language and Information: Selected Essays on their Theory and Application, chapter 9,</booktitle>
<pages>116--150</pages>
<editor>In Y. Bar-Hillel, editor,</editor>
<publisher>Addison-Wesley,</publisher>
<location>Reading, Massachusetts.</location>
<contexts>
<context position="3605" citStr="Bar-Hillel et al., 1964" startWordPosition="569" endWordPosition="572"> Mw is further transformed into a tree transducer implementing a partial identity translation, and such a transducer is composed with T (relation composition). This is usually called the ‘cascaded’ approach. Such an approach can be easily applied also to parsing problems. In contrast with the cascaded approach above, which may be rather inefficient, in this paper we investigate a more direct technique for parsing strings based on weighted and probabilistic tree automata. We do this by extending to weighted tree automata the well-known Bar-Hillel construction defined for context-free grammars (Bar-Hillel et al., 1964) and for weighted context-free grammars (Nederhof and Satta, 2003). This provides an abstract framework under which several parsing algorithms can be directly derived, based on weighted tree automata. We discuss several applications of our results, including algorithms for the computation of parse-forests, best tree probability, inside probability (called partition function), and prefix probability. 1 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 1–12, Paris, October 2009. c�2009 Association for Computational Linguistics 2 Preliminary definitions Let S </context>
<context position="11702" citStr="Bar-Hillel et al., 1964" startWordPosition="2242" endWordPosition="2245">,[w] = µk(σ)q,w for every σ ∈ Ek, k ≥ 1, q ∈ Q, and w ∈ Qk, and (iii) µ&apos;2(@)[qw],[q][w] = 1 for every [qw] ∈ P with |w |≥ 1 and q ∈ Q. All remaining entries in F&apos; and µ&apos; are 0. ✷ Notice that each transition of enc(M) involves no more than three states from P. Furthermore, we have |enc(M) |∈ O(|M|). The following result is rather intuitive (H¨ogberg et al., 2009, Lemma 4.2); its proof is therefore omitted. 3 Theorem 1 Let M = (Q, E, S, µ, F) be a WTA, and let M0 = enc(M). Then M(t) = M0(enc(t)) for every t E TE. ❑ 4 Bar-Hillel construction The so-called Bar-Hillel construction was proposed in (Bar-Hillel et al., 1964) to show that the intersection of a context-free language and a regular language is still a context-free language. The proof of the result consisted in an effective construction of a context-free grammar Prod(G, N) from a context-free grammar G and a finite automaton N, such that Prod(G, N) generates the intersection of the languages generated by G and N. It was later recognized that the Bar-Hillel construction constitutes one of the foundations of the theory of tabular parsing based on context-free grammars. More precisely, by taking the finite automaton N to be of some special kind, acceptin</context>
</contexts>
<marker>Bar-Hillel, Perles, Shamir, 1964</marker>
<rawString>Y. Bar-Hillel, M. Perles, and E. Shamir. 1964. On formal properties of simple phrase structure grammars. In Y. Bar-Hillel, editor, Language and Information: Selected Essays on their Theory and Application, chapter 9, pages 116–150. Addison-Wesley, Reading, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Berstel</author>
<author>C Reutenauer</author>
</authors>
<title>Recognizable formal power series on trees.</title>
<date>1982</date>
<journal>Theoret. Comput. Sci.,</journal>
<volume>18</volume>
<issue>2</issue>
<contexts>
<context position="5841" citStr="Berstel and Reutenauer, 1982" startWordPosition="1029" endWordPosition="1033"> input string of length n. For each integer i with 1 ≤ i ≤ n, we write w(i) to denote the i-th character of w. The set Pos(w) of positions of w is {i |0 ≤ i ≤ n}. A run of M on w is a mapping r: Pos(w) → Q. We denote the set of all such runs by RunM(w). The weight of a run r ∈ RunM(w) is n wtM(r) = v(r(i − 1), w(i), r(i)) . z=1 We assume the right-hand side of the above equation evaluates to 1 in case n = 0. The WSA M recognizes the mapping M : E* → S, which is defined for every w ∈ E* of length n by1 M(w) = � I(r(0)) · wtM(r) · F(r(n)) . rERunyl (w) In order to define weighted tree automata (Berstel and Reutenauer, 1982; ´Esik and Kuich, 2003; Borchardt, 2005), we need to introduce some additional notation. Let E be a ranked alphabet, that 1We overload the symbol M to denote both an automaton and its recognized mapping. However, the intended meaning will always be clear from the context. is, an alphabet whose symbols have an associated arity. We write Ek to denote the set of all k-ary symbols in E. We use a special symbol e ∈ E0 to syntactically represent the empty string E. The set of E-trees, denoted by TE, is the smallest set satisfying both of the following conditions • for every α ∈ E0, the single node </context>
</contexts>
<marker>Berstel, Reutenauer, 1982</marker>
<rawString>J. Berstel and C. Reutenauer. 1982. Recognizable formal power series on trees. Theoret. Comput. Sci., 18(2):115–148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Borchardt</author>
</authors>
<title>The Theory of Recognizable Tree Series.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>Technische Universit¨at Dresden.</institution>
<contexts>
<context position="5882" citStr="Borchardt, 2005" startWordPosition="1038" endWordPosition="1039">≤ i ≤ n, we write w(i) to denote the i-th character of w. The set Pos(w) of positions of w is {i |0 ≤ i ≤ n}. A run of M on w is a mapping r: Pos(w) → Q. We denote the set of all such runs by RunM(w). The weight of a run r ∈ RunM(w) is n wtM(r) = v(r(i − 1), w(i), r(i)) . z=1 We assume the right-hand side of the above equation evaluates to 1 in case n = 0. The WSA M recognizes the mapping M : E* → S, which is defined for every w ∈ E* of length n by1 M(w) = � I(r(0)) · wtM(r) · F(r(n)) . rERunyl (w) In order to define weighted tree automata (Berstel and Reutenauer, 1982; ´Esik and Kuich, 2003; Borchardt, 2005), we need to introduce some additional notation. Let E be a ranked alphabet, that 1We overload the symbol M to denote both an automaton and its recognized mapping. However, the intended meaning will always be clear from the context. is, an alphabet whose symbols have an associated arity. We write Ek to denote the set of all k-ary symbols in E. We use a special symbol e ∈ E0 to syntactically represent the empty string E. The set of E-trees, denoted by TE, is the smallest set satisfying both of the following conditions • for every α ∈ E0, the single node labeled α, written α(), is a tree of TE, </context>
</contexts>
<marker>Borchardt, 2005</marker>
<rawString>B. Borchardt. 2005. The Theory of Recognizable Tree Series. Ph.D. thesis, Technische Universit¨at Dresden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bozapalidis</author>
</authors>
<title>Equational elements in additive algebras.</title>
<date>1999</date>
<journal>Theory Comput. Systems,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="22714" citStr="Bozapalidis, 1999" startWordPosition="4386" endWordPosition="4388"> pM is a probability distribution over the set TΣ. 6 The WTA M is unambiguous if for every input tree t ∈ TΣ, there exists at most one r ∈ RunM(t) such that r(ε) ∈ F and wtM(r) =6 0. In other words, in an unambiguous WTA, there exists at most one successful run for each input tree. Finally, M is in final-state normal form if there exists a state qS ∈ Q such that • F(qS) = 1, • F(q) = 0 for every q ∈ Q \ {qS}, and • µk(σ)q,w = 0 if w(i) = qS for some 1 ≤ i ≤ k. We commonly denote the unique final state by qS. For the following result we refer the reader to (Droste et al., 2005, Lemma 4.8) and (Bozapalidis, 1999, Lemma 22). The additional properties mentioned in the items of it are easily seen. Theorem 3 For every WTA M there exists an equivalent WTA M&apos; in final-state normal form. • If M is convergent (respectively, proper, consistent), then M&apos; is such, too. • If M is unambiguous, then M&apos; is also unambiguous and for every t ∈ TΣ and r ∈ RunM(t) we have wtM,(r&apos;) = wtM(r) · F(r(ε)) where r&apos;(ε) = qS and r&apos;(w) = r(w) for every w ∈ Pos(t) \ {ε}. ✷ It is not difficult to see that a proper PTA in final-state normal form is always convergent. In statistical parsing applications we use grammar models that ind</context>
</contexts>
<marker>Bozapalidis, 1999</marker>
<rawString>S. Bozapalidis. 1999. Equational elements in additive algebras. Theory Comput. Systems, 32(1):1–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carme</author>
<author>J Niehren</author>
<author>M Tommasi</author>
</authors>
<title>Querying unranked trees with stepwise tree automata.</title>
<date>2004</date>
<booktitle>In Proc. RTA,</booktitle>
<volume>3091</volume>
<pages>105--118</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="9894" citStr="Carme et al., 2004" startWordPosition="1866" endWordPosition="1869"> This transformation maps the set of recognized trees into a special binary form, in such a way that the yields of corresponding trees and their weights are both preserved. We use this transformation in the next section in order to guarantee the computational efficiency of the parsing algorithm we develop. The standard ‘first-child, nextsibling’ binary encoding for trees (Knuth, 1997) would eventually result in a transformed WTA of quadratic size. To obtain instead a linear size transformation, we introduce a slightly modified encoding (H¨ogberg et al., 2009, Section 4), which is inspired by (Carme et al., 2004) and the classical currying operation. Let E be a ranked alphabet and assume a fresh symbol @ ∈/ E (corresponding to the basic list concatenation operator). Moreover, let A = A2 ∪ A1 ∪ A0 be the ranked alphabet such that A2 = {@}, A1 = Uk&gt;1 Ek, and A0 = E0. In Figure 1: Input tree t and encoded tree enc(t). words, all the original non-nullary symbols from E are now unary, @ is binary, and the original nullary symbols from E have their rank preserved. We encode each tree of TΣ as a tree of TΔ as follows: • enc(α) = α() for every α ∈ E0, • enc(γ(t)) = γ(enc(t)) for every γ ∈ E1 and t ∈ TΣ, and •</context>
</contexts>
<marker>Carme, Niehren, Tommasi, 2004</marker>
<rawString>J. Carme, J. Niehren, and M. Tommasi. 2004. Querying unranked trees with stepwise tree automata. In Proc. RTA, volume 3091 of LNCS, pages 105–118. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Casacuberta</author>
<author>C de la Higuera</author>
</authors>
<title>Computational complexity of problems on probabilistic grammars and transducers.</title>
<date>2000</date>
<booktitle>Grammatical Inference: Algorithms and Applications; 5th International Colloquium, ICGI 2000,</booktitle>
<pages>15--24</pages>
<editor>In L. Oliveira, editor,</editor>
<publisher>Springer.</publisher>
<marker>Casacuberta, Higuera, 2000</marker>
<rawString>F. Casacuberta and C. de la Higuera. 2000. Computational complexity of problems on probabilistic grammars and transducers. In L. Oliveira, editor, Grammatical Inference: Algorithms and Applications; 5th International Colloquium, ICGI 2000, pages 15–24. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Chi</author>
</authors>
<title>Statistical properties of probabilistic context-free grammars.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>1</issue>
<contexts>
<context position="29080" citStr="Chi, 1999" startWordPosition="5528" endWordPosition="5529">ess and consistency of Prod(M, N) are convenient in all those applications where a statistical parsing module needs to be coupled with other statistical modules, in such a way that the composition of the probability spaces still induces a probability distribution. In this subsection we deal with the more general problem of how to transform a WTA that is convergent into a PTA that is proper and consistent. This process is called normalization. The normalization technique we propose here has been previously explored, in the context of probabilistic context-free grammars, in (Abney et al., 1999; Chi, 1999; Nederhof and Satta, 2003). We start by introducing some new notions. Let us assume that M is a convergent WTA. For every q E Q, we define wtM(q) = � wtM(r) . t∈TΣ,r∈RunM (t) r(E)=q Note that quantity wtM(q) equals the sum of the weights of all trees in TE that would be recognized by M if we set F(q) = 1 and F(p) = 0 for each p E Q \ {q}, that is, if q is the unique final state of M. It is not difficult to show that, since M is convergent, the sum in the definition of wtM(q) converges for each q E Q. We will show in Subsection 5.4 that the quantities wtM(q) can be approximated to any desired </context>
</contexts>
<marker>Chi, 1999</marker>
<rawString>Z. Chi. 1999. Statistical properties of probabilistic context-free grammars. Computational Linguistics, 25(1):131–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Droste</author>
<author>C Pech</author>
<author>H Vogler</author>
</authors>
<title>A Kleene theorem for weighted tree automata.</title>
<date>2005</date>
<journal>Theory Comput. Systems,</journal>
<volume>38</volume>
<issue>1</issue>
<contexts>
<context position="22679" citStr="Droste et al., 2005" startWordPosition="4379" endWordPosition="4382">(t) = 1. If a PTA is consistent, then pM is a probability distribution over the set TΣ. 6 The WTA M is unambiguous if for every input tree t ∈ TΣ, there exists at most one r ∈ RunM(t) such that r(ε) ∈ F and wtM(r) =6 0. In other words, in an unambiguous WTA, there exists at most one successful run for each input tree. Finally, M is in final-state normal form if there exists a state qS ∈ Q such that • F(qS) = 1, • F(q) = 0 for every q ∈ Q \ {qS}, and • µk(σ)q,w = 0 if w(i) = qS for some 1 ≤ i ≤ k. We commonly denote the unique final state by qS. For the following result we refer the reader to (Droste et al., 2005, Lemma 4.8) and (Bozapalidis, 1999, Lemma 22). The additional properties mentioned in the items of it are easily seen. Theorem 3 For every WTA M there exists an equivalent WTA M&apos; in final-state normal form. • If M is convergent (respectively, proper, consistent), then M&apos; is such, too. • If M is unambiguous, then M&apos; is also unambiguous and for every t ∈ TΣ and r ∈ RunM(t) we have wtM,(r&apos;) = wtM(r) · F(r(ε)) where r&apos;(ε) = qS and r&apos;(w) = r(w) for every w ∈ Pos(t) \ {ε}. ✷ It is not difficult to see that a proper PTA in final-state normal form is always convergent. In statistical parsing applicat</context>
</contexts>
<marker>Droste, Pech, Vogler, 2005</marker>
<rawString>M. Droste, C. Pech, and H. Vogler. 2005. A Kleene theorem for weighted tree automata. Theory Comput. Systems, 38(1):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1970</date>
<journal>Communications of the ACM,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="20468" citStr="Earley, 1970" startWordPosition="3924" endWordPosition="3926">l M. As an example, we can apply step (i) only on demand, that is, we process a transition µ2(σ)q0,q1q2 in Prod(M, N) only if we have already computed non-zero transitions of the form µk1(σ1)q1,w1 and µk2(σ2)q2,w2, for some σ1 E Ek1, w1 E Qk1 and σ2 E Ek2, w2 E Qk2 where Q is the state set of Prod(M, N). The above amounts to a bottomup strategy that is also used in the Cocke-KasamiYounger recognition algorithm for context-free grammars (Younger, 1967). More sophisticated strategies are also possible. For instance, one could adopt the Earley strategy developed for context-free grammar parsing (Earley, 1970). In this case, parsing is carried out in a top-down left-to-right fashion, and the binarization construction of Section 3 is carried out on the flight. This has the additional advantage that it would be possible to use WTA models that are not restricted to the special normal form of Section 3, still maintaining the cubic time complexity in the length of the input string. We do not pursue this idea any further in this paper, since our main goal here is to outline an abstract framework for parsing based on WTA models. 5.2 Probabilistic tree automata Let us now look into specific semirings that </context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>J. Earley. 1970. An efficient context-free parsing algorithm. Communications of the ACM, 13(2):94–102, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Eilenberg</author>
</authors>
<date>1974</date>
<journal>of Pure and Applied</journal>
<booktitle>Automata, Languages, and Machines,</booktitle>
<volume>59</volume>
<publisher>Academic Press.</publisher>
<contexts>
<context position="4866" citStr="Eilenberg, 1974" startWordPosition="796" endWordPosition="797">ary operation on S. If S contains an element 1 such that 1 · s = s = s · 1 for every s ∈ S, then (S, ·,1) is a monoid. A monoid (S, ·,1) is commutative if the equation s1 · s2 = s2 · s1 holds for every s1, s2 ∈ S. A commutative semiring (S, +, ·, 0,1) is a nonempty set S on which a binary addition + and a binary multiplication · have been defined such that the following conditions are satisfied: • (S, +, 0) and (S, ·,1) are commutative monoids, • · distributes over + from both sides, and • s · 0 = 0 = 0 · s for every s ∈ S. A weighted string automaton, abbreviated WSA, (Sch¨utzenberger, 1961; Eilenberg, 1974) is a system M = (Q, E, S, I, v, F) where • Q is a finite alphabet of states, • E is a finite alphabet of input symbols, • S = (S, +, ·, 0, 1) is a semiring, • I : Q → S assigns initial weights, • v : Q × E × Q → S assigns a weight to each transition, and • F : Q → S assigns final weights. We now proceed with the semantics of M. Let w ∈ E* be an input string of length n. For each integer i with 1 ≤ i ≤ n, we write w(i) to denote the i-th character of w. The set Pos(w) of positions of w is {i |0 ≤ i ≤ n}. A run of M on w is a mapping r: Pos(w) → Q. We denote the set of all such runs by RunM(w).</context>
</contexts>
<marker>Eilenberg, 1974</marker>
<rawString>S. Eilenberg. 1974. Automata, Languages, and Machines, volume 59 of Pure and Applied Math. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C A Ellis</author>
</authors>
<title>Probabilistic tree automata.</title>
<date>1971</date>
<journal>Information and Control,</journal>
<volume>19</volume>
<issue>5</issue>
<contexts>
<context position="21380" citStr="Ellis, 1971" startWordPosition="4093" endWordPosition="4094"> maintaining the cubic time complexity in the length of the input string. We do not pursue this idea any further in this paper, since our main goal here is to outline an abstract framework for parsing based on WTA models. 5.2 Probabilistic tree automata Let us now look into specific semirings that are relevant for statistical natural language processing. The semiring of non-negative real numbers is 1R,&gt;0 = (1R,&gt;0, +, ·, 0, 1). For the remainder of the section, let M = (Q, E, 1R,&gt;0, µ, F) be a WTA over 1R,&gt;0. M is convergent if � M(t) &lt; 00. tETΣ We say that M is a probabilistic tree automaton (Ellis, 1971; Magidor and Moran, 1970), or PTA for short, if µk(σ)q,q1···qk E [0, 1] and F(q) E [0, 1], for every σ E Ek and q, q1, ... , qk E Q. In other words, in a PTA all weights are in the range [0, 1] and can be interpreted as probabilities. For a PTA M we therefore write pM(r) = wt(r) and pM(t) = M(t), for each t E TΣ and r E RunM(t). A PTA is proper if EqEQ F(q) = 1 and � µk(σ)q,w = 1 σEΣk,k&gt;0,wEQk for every q E Q. Since the set of symbols is finite, we could have only required that the sum over all weights as shown with w E Qk equals 1 for every q E Q and σ E Ek. A simple rescaling would then be </context>
</contexts>
<marker>Ellis, 1971</marker>
<rawString>C. A. Ellis. 1971. Probabilistic tree automata. Information and Control, 19(5):401–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z ´Esik</author>
<author>W Kuich</author>
</authors>
<title>Formal tree series.</title>
<date>2003</date>
<journal>J. Autom. Lang. Combin.,</journal>
<volume>8</volume>
<issue>2</issue>
<marker>´Esik, Kuich, 2003</marker>
<rawString>Z. ´Esik and W. Kuich. 2003. Formal tree series. J. Autom. Lang. Combin., 8(2):219–285.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Etessami</author>
<author>M Yannakakis</author>
</authors>
<title>Recursive Markov chains, stochastic grammars, and monotone systems of nonlinear equations.</title>
<date>2005</date>
<booktitle>In 22nd International Symposium on Theoretical Aspects of Computer Science,</booktitle>
<volume>3404</volume>
<pages>340--352</pages>
<publisher>Springer-Verlag.</publisher>
<location>Stuttgart, Germany.</location>
<contexts>
<context position="35693" citStr="Etessami and Yannakakis, 2005" startWordPosition="6752" endWordPosition="6755">us literature, and is frequently applied to systems of nonlinear equations in general, because it can be easily implemented. When a number of standard conditions are met, each iteration of the algorithm (corresponding to the value of k above) adds a fixed number of bits to the precision of the approximated solution; see (Kelley, 1995) for further discussion. Systems of the form X = F(X) where all fqi(X) are polynomials with nonnegative real coefficients are called monotone system of polynomials. Monotone systems of polynomials associated with proper PTA have been specifically investigated in (Etessami and Yannakakis, 2005) and (Kiefer et al., 2007), where worst case results on exponential rate of convergence are reported for the fixed-point method. 5.5 Prefix probability In this subsection we deal with one more application of the Bar-Hillel technique presented in Section 4. We show how to compute the so-called prefix probabilities, that is, the probability that a tree recognized by a PTA generates a string starting with a given prefix. Such probabilities have several applications in language modeling. As an example, prefix probabilities can be used to compute the probability distribution on the terminal symbol </context>
</contexts>
<marker>Etessami, Yannakakis, 2005</marker>
<rawString>K. Etessami and M. Yannakakis. 2005. Recursive Markov chains, stochastic grammars, and monotone systems of nonlinear equations. In 22nd International Symposium on Theoretical Aspects of Computer Science, volume 3404 of Lecture Notes in Computer Science, pages 340–352, Stuttgart, Germany. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K S Fu</author>
<author>T Huang</author>
</authors>
<title>Stochastic grammars and languages.</title>
<date>1972</date>
<journal>International Journal of Computer and Information Sciences,</journal>
<volume>1</volume>
<issue>2</issue>
<marker>Fu, Huang, 1972</marker>
<rawString>K.S. Fu and T. Huang. 1972. Stochastic grammars and languages. International Journal of Computer and Information Sciences, 1(2):135–170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F G´ecseg</author>
<author>M Steinby</author>
</authors>
<title>Tree Automata. Akad´emiai Kiad´o,</title>
<date>1984</date>
<location>Budapest.</location>
<marker>G´ecseg, Steinby, 1984</marker>
<rawString>F. G´ecseg and M. Steinby. 1984. Tree Automata. Akad´emiai Kiad´o, Budapest.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Graehl</author>
<author>K Knight</author>
<author>J May</author>
</authors>
<title>Training tree transducers.</title>
<date>2008</date>
<journal>Comput. Linguist.,</journal>
<volume>34</volume>
<issue>3</issue>
<contexts>
<context position="2342" citStr="Graehl et al., 2008" startWordPosition="355" endWordPosition="358">ect-object positions, respectively. This can be easily captured using some states of a PTA that keep a record of the different contexts. In contrast, PCFGs are unable to model these effects, because NP node expansion should be independent of the context in the derivation. This problem for PCFGs is usually solved by resorting to so-called parental annotations (Johnson, 1998), but this, of course, results in a different tree language, since these annotations will appear in the derived tree. Most of the theoretical work on parsing and estimation based on PTA has assumed that the input is a tree (Graehl et al., 2008), in accordance with the very definition of these devices. However, both in parsing as well as in machine translation, the input is most often represented as a string rather than a tree. When the input is a string, some trick is applied to map the problem back to the case of an input tree. As an example in the context of machine translation, assume a probabilistic tree transducer T as a translation model, and an input string w to be translated. One can then intermediately construct a tree automaton Mw that recognizes the set of all possible trees that have w as yield, with internal nodes from </context>
<context position="31907" citStr="Graehl et al., 2008" startWordPosition="6043" endWordPosition="6046">ammars, the computation of the partition function has several applications, including the elimination of epsilon rules (Abney et al., 1999) and the computation of probabilistic distances between probability distributions realized by these formalisms (Nederhof and Satta, 2008). Besides what we have seen in Subsection 5.3, we will provide one more application of partition functions for the computations of so-called prefix probabilities in Subsection 5.5 We also add that, when computed on the Bar-Hillel automata of Section 4, the partition function provides the so-called inside probabilities of (Graehl et al., 2008) for the given states and substrings. Let |Q |= n and let us assume an arbitrary ordering q1, ... , qn for the states in Q. We can then rewrite the definition of wtM(q) as k wtM(q) = X µk(σ)q,qi1···qik · Y wtM(qij) σ∈Ek,k≥0 j=1 qi1,...,qik ∈Q (see proof of Theorem 4). We rename wtM(qi) with the unknown Xqi, 1 &lt; i &lt; n, and derive a = X t∈TΣ,r∈RunM(t) r(E)=qS X 0( σ∈Ek,q1,...,qk∈Q µklσ)q,q1···qk X= µk(σ)q,q1···qk · σ∈Ek,q1,...,qk∈Q wtM(q1) · . . . · wtM(qk0) · wtM(q) k X µk(σ)q,q1···qk · Y wtM(qi) σ∈Ek, i=1 q1,...,qk∈Q = k X µk(σ)q,p1···pk · Y wtM(pi) σ∈Ek, i=1 p1,...,pk∈Q wtM0(r) = ((ε)) wtMr f</context>
</contexts>
<marker>Graehl, Knight, May, 2008</marker>
<rawString>J. Graehl, K. Knight, and J. May. 2008. Training tree transducers. Comput. Linguist., 34(3):391–427.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H¨ogberg</author>
<author>A Maletti</author>
<author>H Vogler</author>
</authors>
<title>Bisimulation minimisation of weighted automata on unranked trees.</title>
<date>2009</date>
<journal>Fundam. Inform.</journal>
<note>to appear.</note>
<marker>H¨ogberg, Maletti, Vogler, 2009</marker>
<rawString>J. H¨ogberg, A. Maletti, and H. Vogler. 2009. Bisimulation minimisation of weighted automata on unranked trees. Fundam. Inform. to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
<author>J D Lafferty</author>
<author>R L Mercer</author>
</authors>
<title>Basic methods of probabilistic context free grammars.</title>
<date>1992</date>
<booktitle>Speech Recognition and Understanding — Recent Advances, Trends and Applications,</booktitle>
<pages>345--360</pages>
<editor>In P. Laface and R. De Mori, editors,</editor>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="36483" citStr="Jelinek et al., 1992" startWordPosition="6879" endWordPosition="6882">n we deal with one more application of the Bar-Hillel technique presented in Section 4. We show how to compute the so-called prefix probabilities, that is, the probability that a tree recognized by a PTA generates a string starting with a given prefix. Such probabilities have several applications in language modeling. As an example, prefix probabilities can be used to compute the probability distribution on the terminal symbol that follows a given prefix (under the given model). For probabilistic context-free grammars, the problem of the computation of prefix probabilities has been solved in (Jelinek et al., 1992); see also (Persoon and Fu, 1975). The approach we propose here, originally formulated for probabilistic context-free grammars in (Nederhof and Satta, 2003; Nederhof and Satta, 2009), is more abstract than the previous ones, since it entirely rests on properties of the Bar-Hillel construction that we have already proved in Section 4. Let M = (Q, E, R≥0, µ, F) be a proper and consistent PTA in final-state normal form, A = E0 \ {e}, and let u ∈ A+ be some string. We assume here that M is in the binary form discussed in Section 3. In addition, we assume that M has been preprocessed in order to re</context>
</contexts>
<marker>Jelinek, Lafferty, Mercer, 1992</marker>
<rawString>F. Jelinek, J.D. Lafferty, and R.L. Mercer. 1992. Basic methods of probabilistic context free grammars. In P. Laface and R. De Mori, editors, Speech Recognition and Understanding — Recent Advances, Trends and Applications, pages 345–360. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
</authors>
<title>PCFG models of linguistic tree representations.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="2098" citStr="Johnson, 1998" startWordPosition="311" endWordPosition="313">transfer information within the tree structure being recognized. As an example, in written English we can empirically observe different distributions in the expansion of so-called noun phrase (NP) nodes, in the contexts of subject and direct-object positions, respectively. This can be easily captured using some states of a PTA that keep a record of the different contexts. In contrast, PCFGs are unable to model these effects, because NP node expansion should be independent of the context in the derivation. This problem for PCFGs is usually solved by resorting to so-called parental annotations (Johnson, 1998), but this, of course, results in a different tree language, since these annotations will appear in the derived tree. Most of the theoretical work on parsing and estimation based on PTA has assumed that the input is a tree (Graehl et al., 2008), in accordance with the very definition of these devices. However, both in parsing as well as in machine translation, the input is most often represented as a string rather than a tree. When the input is a string, some trick is applied to map the problem back to the case of an input tree. As an example in the context of machine translation, assume a pro</context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>M. Johnson. 1998. PCFG models of linguistic tree representations. Computational Linguistics, 24(4):613–632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C T Kelley</author>
</authors>
<title>Iterative Methods for Linear and Nonlinear Equations. Society for Industrial and Applied Mathematics,</title>
<date>1995</date>
<location>Philadelphia, PA.</location>
<contexts>
<context position="35399" citStr="Kelley, 1995" startWordPosition="6709" endWordPosition="6710">ction of M, and the least fixed-point solution is also the sought solution. Thus, we can approximate wtm(q) with q ∈ Q to any degree by iterating F a sufficiently large number of times. The fixed-point iteration method discussed above is also well-known in the numerical calculus literature, and is frequently applied to systems of nonlinear equations in general, because it can be easily implemented. When a number of standard conditions are met, each iteration of the algorithm (corresponding to the value of k above) adds a fixed number of bits to the precision of the approximated solution; see (Kelley, 1995) for further discussion. Systems of the form X = F(X) where all fqi(X) are polynomials with nonnegative real coefficients are called monotone system of polynomials. Monotone systems of polynomials associated with proper PTA have been specifically investigated in (Etessami and Yannakakis, 2005) and (Kiefer et al., 2007), where worst case results on exponential rate of convergence are reported for the fixed-point method. 5.5 Prefix probability In this subsection we deal with one more application of the Bar-Hillel technique presented in Section 4. We show how to compute the so-called prefix proba</context>
</contexts>
<marker>Kelley, 1995</marker>
<rawString>C. T. Kelley. 1995. Iterative Methods for Linear and Nonlinear Equations. Society for Industrial and Applied Mathematics, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kiefer</author>
<author>M Luttenberger</author>
<author>J Esparza</author>
</authors>
<title>On the convergence of Newton’s method for monotone systems of polynomial equations.</title>
<date>2007</date>
<booktitle>In Proceedings of the 39th ACM Symposium on Theory of Computing,</booktitle>
<pages>217--266</pages>
<contexts>
<context position="35719" citStr="Kiefer et al., 2007" startWordPosition="6757" endWordPosition="6760">lied to systems of nonlinear equations in general, because it can be easily implemented. When a number of standard conditions are met, each iteration of the algorithm (corresponding to the value of k above) adds a fixed number of bits to the precision of the approximated solution; see (Kelley, 1995) for further discussion. Systems of the form X = F(X) where all fqi(X) are polynomials with nonnegative real coefficients are called monotone system of polynomials. Monotone systems of polynomials associated with proper PTA have been specifically investigated in (Etessami and Yannakakis, 2005) and (Kiefer et al., 2007), where worst case results on exponential rate of convergence are reported for the fixed-point method. 5.5 Prefix probability In this subsection we deal with one more application of the Bar-Hillel technique presented in Section 4. We show how to compute the so-called prefix probabilities, that is, the probability that a tree recognized by a PTA generates a string starting with a given prefix. Such probabilities have several applications in language modeling. As an example, prefix probabilities can be used to compute the probability distribution on the terminal symbol that follows a given prefi</context>
</contexts>
<marker>Kiefer, Luttenberger, Esparza, 2007</marker>
<rawString>S. Kiefer, M. Luttenberger, and J. Esparza. 2007. On the convergence of Newton’s method for monotone systems of polynomial equations. In Proceedings of the 39th ACM Symposium on Theory of Computing, pages 217–266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Knaster</author>
</authors>
<title>Un th´eor`eme sur les fonctions d’ensembles.</title>
<date>1928</date>
<journal>Ann. Soc. Polon. Math.,</journal>
<pages>6--133</pages>
<contexts>
<context position="34248" citStr="Knaster, 1928" startWordPosition="6501" endWordPosition="6502">R∞≥0)n consisting of the polynomials fqi(X). We denote the vector (0, ... , 0) ∈ (R∞≥0)n as X0. Let X, X0 ∈ (R∞≥0)n. We write X ≤ X0 if Xqi ≤ X0qi for every 1 ≤ i ≤ n. Since each polynomial fqi(X) has coefficients represented by positive real numbers, it is not difficult to see that, for each X, X0 ∈ (R∞≥0)n, we have F(X) ≤ F(X0) whenever X0 ≤ X ≤ X0. This means that F is an order preserving, or monotone, mapping. We observe that ((R∞≥0)n, ≤) is a complete lattice with least element X0 and greatest element (∞, ... , ∞). Since F is monotone on a complete lattice, by the Knaster-Tarski theorem (Knaster, 1928; Tarski, 1955) there exists a least and a greatest fixed-point of F that are solutions of X = F (X). The Kleene theorem states that the least fixedpoint solution of X = F(X) can be obtained by iterating F starting with the least element X0. In other words, the sequence Xk = F(Xk−1), k = 1, 2, ... converges to the least fixed-point solution. Notice that each Xk provides an approximation for the partition function of M where only trees of depth not larger than k are considered. This means that limk→∞ Xk converges to the partition function of M, and the least fixed-point solution is also the sou</context>
</contexts>
<marker>Knaster, 1928</marker>
<rawString>B. Knaster. 1928. Un th´eor`eme sur les fonctions d’ensembles. Ann. Soc. Polon. Math., 6:133–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Knuth</author>
</authors>
<title>A generalization of Dijkstra’s algorithm.</title>
<date>1977</date>
<journal>Information Processing Letters,</journal>
<volume>6</volume>
<issue>1</issue>
<contexts>
<context position="24895" citStr="Knuth (1977)" startWordPosition="4782" endWordPosition="4783">g these 1: Function BESTPARSE(M) 2: E ← ∅ 3: repeat 4: A ← {q |µk(σ)q,q1···qk &gt; 0, q ∈/ E, q1,...,qk ∈ E} 5: for all q ∈ Ado 6: δ(q) ← max σEΣk,k&gt;0 q1,...,qkE£ 7: E ← E ∪ {argmax qEA 8: until qS ∈ E 9: return δ(qS) Figure 3: Search algorithm for the most probable parse in an unambiguous PTA M in final-state normal form. two properties are still more powerful than the probabilistic context-free grammar models that are commonly used in statistical natural language processing. Once more, we borrow from the literature on parsing for context-free grammars, and adapt a search algorithm developed by Knuth (1977); see also (Nederhof, 2003). The basic idea here is to generalize Dijkstra’s algorithm to compute the shortest path in a weighted graph. The search algorithm is presented in Figure 3. The algorithm takes as input a trim PTA M that recognizes at least one parse tree. We do not impose any bound on the rank of the alphabet symbols for M. Furthermore, M needs not be a proper PTA. In order to simplify the presentation, we provide the algorithm in a form that returns the largest probability assigned to some tree by M. The algorithm records into the δ(q) variables the largest probability found so far</context>
</contexts>
<marker>Knuth, 1977</marker>
<rawString>D. E. Knuth. 1977. A generalization of Dijkstra’s algorithm. Information Processing Letters, 6(1):1–5, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Knuth</author>
</authors>
<title>Fundamental Algorithms. The Art of Computer Programming.</title>
<date>1997</date>
<publisher>Addison Wesley,</publisher>
<note>3rd edition.</note>
<contexts>
<context position="9662" citStr="Knuth, 1997" startWordPosition="1831" endWordPosition="1832">he weights. 3 Binarization We introduce in this section a specific transformation of WTA, called binarization, that reduces the transitions of the automaton to some normal form in which no more than three states are involved. This transformation maps the set of recognized trees into a special binary form, in such a way that the yields of corresponding trees and their weights are both preserved. We use this transformation in the next section in order to guarantee the computational efficiency of the parsing algorithm we develop. The standard ‘first-child, nextsibling’ binary encoding for trees (Knuth, 1997) would eventually result in a transformed WTA of quadratic size. To obtain instead a linear size transformation, we introduce a slightly modified encoding (H¨ogberg et al., 2009, Section 4), which is inspired by (Carme et al., 2004) and the classical currying operation. Let E be a ranked alphabet and assume a fresh symbol @ ∈/ E (corresponding to the basic list concatenation operator). Moreover, let A = A2 ∪ A1 ∪ A0 be the ranked alphabet such that A2 = {@}, A1 = Uk&gt;1 Ek, and A0 = E0. In Figure 1: Input tree t and encoded tree enc(t). words, all the original non-nullary symbols from E are now </context>
</contexts>
<marker>Knuth, 1997</marker>
<rawString>D. E. Knuth. 1997. Fundamental Algorithms. The Art of Computer Programming. Addison Wesley, 3rd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Magidor</author>
<author>G Moran</author>
</authors>
<title>Probabilistic tree automata and context free languages.</title>
<date>1970</date>
<journal>Israel Journal of Mathematics,</journal>
<volume>8</volume>
<issue>4</issue>
<contexts>
<context position="21406" citStr="Magidor and Moran, 1970" startWordPosition="4095" endWordPosition="4098">the cubic time complexity in the length of the input string. We do not pursue this idea any further in this paper, since our main goal here is to outline an abstract framework for parsing based on WTA models. 5.2 Probabilistic tree automata Let us now look into specific semirings that are relevant for statistical natural language processing. The semiring of non-negative real numbers is 1R,&gt;0 = (1R,&gt;0, +, ·, 0, 1). For the remainder of the section, let M = (Q, E, 1R,&gt;0, µ, F) be a WTA over 1R,&gt;0. M is convergent if � M(t) &lt; 00. tETΣ We say that M is a probabilistic tree automaton (Ellis, 1971; Magidor and Moran, 1970), or PTA for short, if µk(σ)q,q1···qk E [0, 1] and F(q) E [0, 1], for every σ E Ek and q, q1, ... , qk E Q. In other words, in a PTA all weights are in the range [0, 1] and can be interpreted as probabilities. For a PTA M we therefore write pM(r) = wt(r) and pM(t) = M(t), for each t E TΣ and r E RunM(t). A PTA is proper if EqEQ F(q) = 1 and � µk(σ)q,w = 1 σEΣk,k&gt;0,wEQk for every q E Q. Since the set of symbols is finite, we could have only required that the sum over all weights as shown with w E Qk equals 1 for every q E Q and σ E Ek. A simple rescaling would then be sufficient to arrive at ou</context>
</contexts>
<marker>Magidor, Moran, 1970</marker>
<rawString>M. Magidor and G. Moran. 1970. Probabilistic tree automata and context free languages. Israel Journal of Mathematics, 8(4):340–348.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-J Nederhof</author>
<author>G Satta</author>
</authors>
<title>Probabilistic parsing as intersection.</title>
<date>2003</date>
<booktitle>In 8th International Workshop on Parsing Technologies,</booktitle>
<pages>137--148</pages>
<location>LORIA, Nancy, France,</location>
<contexts>
<context position="3671" citStr="Nederhof and Satta, 2003" startWordPosition="579" endWordPosition="582">partial identity translation, and such a transducer is composed with T (relation composition). This is usually called the ‘cascaded’ approach. Such an approach can be easily applied also to parsing problems. In contrast with the cascaded approach above, which may be rather inefficient, in this paper we investigate a more direct technique for parsing strings based on weighted and probabilistic tree automata. We do this by extending to weighted tree automata the well-known Bar-Hillel construction defined for context-free grammars (Bar-Hillel et al., 1964) and for weighted context-free grammars (Nederhof and Satta, 2003). This provides an abstract framework under which several parsing algorithms can be directly derived, based on weighted tree automata. We discuss several applications of our results, including algorithms for the computation of parse-forests, best tree probability, inside probability (called partition function), and prefix probability. 1 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 1–12, Paris, October 2009. c�2009 Association for Computational Linguistics 2 Preliminary definitions Let S be a nonempty set and · be an associative binary operation on S. I</context>
<context position="29107" citStr="Nederhof and Satta, 2003" startWordPosition="5530" endWordPosition="5533">sistency of Prod(M, N) are convenient in all those applications where a statistical parsing module needs to be coupled with other statistical modules, in such a way that the composition of the probability spaces still induces a probability distribution. In this subsection we deal with the more general problem of how to transform a WTA that is convergent into a PTA that is proper and consistent. This process is called normalization. The normalization technique we propose here has been previously explored, in the context of probabilistic context-free grammars, in (Abney et al., 1999; Chi, 1999; Nederhof and Satta, 2003). We start by introducing some new notions. Let us assume that M is a convergent WTA. For every q E Q, we define wtM(q) = � wtM(r) . t∈TΣ,r∈RunM (t) r(E)=q Note that quantity wtM(q) equals the sum of the weights of all trees in TE that would be recognized by M if we set F(q) = 1 and F(p) = 0 for each p E Q \ {q}, that is, if q is the unique final state of M. It is not difficult to show that, since M is convergent, the sum in the definition of wtM(q) converges for each q E Q. We will show in Subsection 5.4 that the quantities wtM(q) can be approximated to any desired precision. To simplify the </context>
<context position="36638" citStr="Nederhof and Satta, 2003" startWordPosition="6902" endWordPosition="6905">is, the probability that a tree recognized by a PTA generates a string starting with a given prefix. Such probabilities have several applications in language modeling. As an example, prefix probabilities can be used to compute the probability distribution on the terminal symbol that follows a given prefix (under the given model). For probabilistic context-free grammars, the problem of the computation of prefix probabilities has been solved in (Jelinek et al., 1992); see also (Persoon and Fu, 1975). The approach we propose here, originally formulated for probabilistic context-free grammars in (Nederhof and Satta, 2003; Nederhof and Satta, 2009), is more abstract than the previous ones, since it entirely rests on properties of the Bar-Hillel construction that we have already proved in Section 4. Let M = (Q, E, R≥0, µ, F) be a proper and consistent PTA in final-state normal form, A = E0 \ {e}, and let u ∈ A+ be some string. We assume here that M is in the binary form discussed in Section 3. In addition, we assume that M has been preprocessed in order to remove from its recognized trees all of the unary branches as well as those branches that generate the null string E. Although we do not discuss this constru</context>
<context position="39694" citStr="Nederhof and Satta, 2003" startWordPosition="7501" endWordPosition="7504">) and (ii) above into the system. This is because any weight A2(σ)π0,π1π &gt; 0 in Mp with 7r = (pi, q, pn) and i &lt; n must have (7r1)3 &lt; n. Quantities ZM,(7r) can then be exactly computed as the solution of a linear system of equations in time O(n3). Putting together all of the observations above, we obtain that for a proper and consistent PTA that has been preprocessed, the prefix probability of u can be computed in cubic time in the length of the prefix itself. 6 Concluding remarks In this paper we have extended the Bar-Hillel construction to WTA, closely following the methodology proposed in (Nederhof and Satta, 2003) for weighted context-free grammars. Based on the obtained framework, we have derived several parsing algorithms for WTA, under the assumption that the input is a string rather than a tree. As already remarked in the introduction, WTA are richer models than weighted context-free grammar, since the formers use hidden states in the recognition of trees. This feature makes it possible to define a product automaton in Definition 2 that generates exactly those trees of interest for the input string. In contrast, in the contextfree grammar case the Bar-Hillel technique provides trees that must be ma</context>
</contexts>
<marker>Nederhof, Satta, 2003</marker>
<rawString>M.-J. Nederhof and G. Satta. 2003. Probabilistic parsing as intersection. In 8th International Workshop on Parsing Technologies, pages 137–148, LORIA, Nancy, France, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-J Nederhof</author>
<author>G Satta</author>
</authors>
<title>Computation of distances for regular and context-free probabilistic languages.</title>
<date>2008</date>
<journal>Theoretical Computer Science,</journal>
<pages>395--2</pages>
<contexts>
<context position="31563" citStr="Nederhof and Satta, 2008" startWordPosition="5988" endWordPosition="5991">state normal form, then wtM(q) can be seen as the probability mass that ‘rests’ on state q. When dealing with such PTA, we use the notation ZM(q) in place of wtM(q), and call ZM the partition function of M. This terminology is borrowed from the literature on exponential or Gibbs probabilistic models. In the context of probabilistic context-free grammars, the computation of the partition function has several applications, including the elimination of epsilon rules (Abney et al., 1999) and the computation of probabilistic distances between probability distributions realized by these formalisms (Nederhof and Satta, 2008). Besides what we have seen in Subsection 5.3, we will provide one more application of partition functions for the computations of so-called prefix probabilities in Subsection 5.5 We also add that, when computed on the Bar-Hillel automata of Section 4, the partition function provides the so-called inside probabilities of (Graehl et al., 2008) for the given states and substrings. Let |Q |= n and let us assume an arbitrary ordering q1, ... , qn for the states in Q. We can then rewrite the definition of wtM(q) as k wtM(q) = X µk(σ)q,qi1···qik · Y wtM(qij) σ∈Ek,k≥0 j=1 qi1,...,qik ∈Q (see proof of</context>
</contexts>
<marker>Nederhof, Satta, 2008</marker>
<rawString>M.-J. Nederhof and G. Satta. 2008. Computation of distances for regular and context-free probabilistic languages. Theoretical Computer Science, 395(2-3):235–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-J Nederhof</author>
<author>G Satta</author>
</authors>
<title>Computing partition functions of PCFGs.</title>
<date>2009</date>
<journal>Research on Language &amp; Computation,</journal>
<volume>6</volume>
<issue>2</issue>
<contexts>
<context position="36665" citStr="Nederhof and Satta, 2009" startWordPosition="6906" endWordPosition="6909"> tree recognized by a PTA generates a string starting with a given prefix. Such probabilities have several applications in language modeling. As an example, prefix probabilities can be used to compute the probability distribution on the terminal symbol that follows a given prefix (under the given model). For probabilistic context-free grammars, the problem of the computation of prefix probabilities has been solved in (Jelinek et al., 1992); see also (Persoon and Fu, 1975). The approach we propose here, originally formulated for probabilistic context-free grammars in (Nederhof and Satta, 2003; Nederhof and Satta, 2009), is more abstract than the previous ones, since it entirely rests on properties of the Bar-Hillel construction that we have already proved in Section 4. Let M = (Q, E, R≥0, µ, F) be a proper and consistent PTA in final-state normal form, A = E0 \ {e}, and let u ∈ A+ be some string. We assume here that M is in the binary form discussed in Section 3. In addition, we assume that M has been preprocessed in order to remove from its recognized trees all of the unary branches as well as those branches that generate the null string E. Although we do not discuss this construction at length in this pap</context>
</contexts>
<marker>Nederhof, Satta, 2009</marker>
<rawString>M.-J. Nederhof and G. Satta. 2009. Computing partition functions of PCFGs. Research on Language &amp; Computation, 6(2):139–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-J Nederhof</author>
</authors>
<title>Weighted deductive parsing and Knuth’s algorithm.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="24922" citStr="Nederhof, 2003" startWordPosition="4786" endWordPosition="4787">PARSE(M) 2: E ← ∅ 3: repeat 4: A ← {q |µk(σ)q,q1···qk &gt; 0, q ∈/ E, q1,...,qk ∈ E} 5: for all q ∈ Ado 6: δ(q) ← max σEΣk,k&gt;0 q1,...,qkE£ 7: E ← E ∪ {argmax qEA 8: until qS ∈ E 9: return δ(qS) Figure 3: Search algorithm for the most probable parse in an unambiguous PTA M in final-state normal form. two properties are still more powerful than the probabilistic context-free grammar models that are commonly used in statistical natural language processing. Once more, we borrow from the literature on parsing for context-free grammars, and adapt a search algorithm developed by Knuth (1977); see also (Nederhof, 2003). The basic idea here is to generalize Dijkstra’s algorithm to compute the shortest path in a weighted graph. The search algorithm is presented in Figure 3. The algorithm takes as input a trim PTA M that recognizes at least one parse tree. We do not impose any bound on the rank of the alphabet symbols for M. Furthermore, M needs not be a proper PTA. In order to simplify the presentation, we provide the algorithm in a form that returns the largest probability assigned to some tree by M. The algorithm records into the δ(q) variables the largest probability found so far for a run that brings M in</context>
</contexts>
<marker>Nederhof, 2003</marker>
<rawString>M.-J. Nederhof. 2003. Weighted deductive parsing and Knuth’s algorithm. Computational Linguistics, 29(1):135–143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Persoon</author>
<author>K S Fu</author>
</authors>
<title>Sequential classification of strings generated by SCFG’s.</title>
<date>1975</date>
<journal>International Journal of Computer and Information Sciences,</journal>
<volume>4</volume>
<issue>3</issue>
<contexts>
<context position="36516" citStr="Persoon and Fu, 1975" startWordPosition="6885" endWordPosition="6888">on of the Bar-Hillel technique presented in Section 4. We show how to compute the so-called prefix probabilities, that is, the probability that a tree recognized by a PTA generates a string starting with a given prefix. Such probabilities have several applications in language modeling. As an example, prefix probabilities can be used to compute the probability distribution on the terminal symbol that follows a given prefix (under the given model). For probabilistic context-free grammars, the problem of the computation of prefix probabilities has been solved in (Jelinek et al., 1992); see also (Persoon and Fu, 1975). The approach we propose here, originally formulated for probabilistic context-free grammars in (Nederhof and Satta, 2003; Nederhof and Satta, 2009), is more abstract than the previous ones, since it entirely rests on properties of the Bar-Hillel construction that we have already proved in Section 4. Let M = (Q, E, R≥0, µ, F) be a proper and consistent PTA in final-state normal form, A = E0 \ {e}, and let u ∈ A+ be some string. We assume here that M is in the binary form discussed in Section 3. In addition, we assume that M has been preprocessed in order to remove from its recognized trees al</context>
</contexts>
<marker>Persoon, Fu, 1975</marker>
<rawString>E. Persoon and K.S. Fu. 1975. Sequential classification of strings generated by SCFG’s. International Journal of Computer and Information Sciences, 4(3):205–217.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Sch¨utzenberger</author>
</authors>
<title>On the definition of a family of automata.</title>
<date>1961</date>
<journal>Information and Control,</journal>
<volume>4</volume>
<issue>2</issue>
<pages>3--245</pages>
<marker>Sch¨utzenberger, 1961</marker>
<rawString>M. P. Sch¨utzenberger. 1961. On the definition of a family of automata. Information and Control, 4(2– 3):245–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Tarski</author>
</authors>
<title>A lattice-theoretical fixpoint theorem and its applications.</title>
<date>1955</date>
<journal>Pacific J. Math.,</journal>
<volume>5</volume>
<issue>2</issue>
<contexts>
<context position="34263" citStr="Tarski, 1955" startWordPosition="6503" endWordPosition="6504">ng of the polynomials fqi(X). We denote the vector (0, ... , 0) ∈ (R∞≥0)n as X0. Let X, X0 ∈ (R∞≥0)n. We write X ≤ X0 if Xqi ≤ X0qi for every 1 ≤ i ≤ n. Since each polynomial fqi(X) has coefficients represented by positive real numbers, it is not difficult to see that, for each X, X0 ∈ (R∞≥0)n, we have F(X) ≤ F(X0) whenever X0 ≤ X ≤ X0. This means that F is an order preserving, or monotone, mapping. We observe that ((R∞≥0)n, ≤) is a complete lattice with least element X0 and greatest element (∞, ... , ∞). Since F is monotone on a complete lattice, by the Knaster-Tarski theorem (Knaster, 1928; Tarski, 1955) there exists a least and a greatest fixed-point of F that are solutions of X = F (X). The Kleene theorem states that the least fixedpoint solution of X = F(X) can be obtained by iterating F starting with the least element X0. In other words, the sequence Xk = F(Xk−1), k = 1, 2, ... converges to the least fixed-point solution. Notice that each Xk provides an approximation for the partition function of M where only trees of depth not larger than k are considered. This means that limk→∞ Xk converges to the partition function of M, and the least fixed-point solution is also the sought solution. T</context>
</contexts>
<marker>Tarski, 1955</marker>
<rawString>A. Tarski. 1955. A lattice-theoretical fixpoint theorem and its applications. Pacific J. Math., 5(2):285–309.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D H Younger</author>
</authors>
<title>Recognition and parsing of context-free languages</title>
<date>1967</date>
<booktitle>in time n3. Information and Control,</booktitle>
<pages>10--189</pages>
<contexts>
<context position="20310" citStr="Younger, 1967" startWordPosition="3903" endWordPosition="3904">ne by adopting some search strategy that guides the construction of Prod(M, N) using knowledge of the input string w as well as knowledge about the source model M. As an example, we can apply step (i) only on demand, that is, we process a transition µ2(σ)q0,q1q2 in Prod(M, N) only if we have already computed non-zero transitions of the form µk1(σ1)q1,w1 and µk2(σ2)q2,w2, for some σ1 E Ek1, w1 E Qk1 and σ2 E Ek2, w2 E Qk2 where Q is the state set of Prod(M, N). The above amounts to a bottomup strategy that is also used in the Cocke-KasamiYounger recognition algorithm for context-free grammars (Younger, 1967). More sophisticated strategies are also possible. For instance, one could adopt the Earley strategy developed for context-free grammar parsing (Earley, 1970). In this case, parsing is carried out in a top-down left-to-right fashion, and the binarization construction of Section 3 is carried out on the flight. This has the additional advantage that it would be possible to use WTA models that are not restricted to the special normal form of Section 3, still maintaining the cubic time complexity in the length of the input string. We do not pursue this idea any further in this paper, since our mai</context>
</contexts>
<marker>Younger, 1967</marker>
<rawString>D. H. Younger. 1967. Recognition and parsing of context-free languages in time n3. Information and Control, 10:189–208.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>