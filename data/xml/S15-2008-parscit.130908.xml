<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.068752">
<title confidence="0.998084">
AMRITA¯CEN@SemEval-2015: Paraphrase Detection for Twitter using
Unsupervised Feature Learning with Recursive Autoencoders
</title>
<author confidence="0.860264">
Mahalakshmi Shanmuga Sundaram, Anand Kumar Madasamy and Soman Kotti Padannayil
</author>
<affiliation confidence="0.595187666666667">
Center for Excellence in Computational Engineering and Networking
Amrita Vishwa Vidyapeetham
Coimbatore, India
</affiliation>
<email confidence="0.99155">
mahalakshmisklu@gmail.com
m¯anandkumar@cb.amrita.edu
kp¯soman@amrita.edu
</email>
<sectionHeader confidence="0.995612" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999802">
We explore using recursive autoencoders for
SemEval 2015 Task 1: Paraphrase and Seman-
tic Similarity in Twitter. Our paraphrase de-
tection system makes use of phrase-structure
parse tree embeddings that are then provided
as input to a conventional supervised classi-
fication model. We achieve an F1 score of
0.45 on paraphrase identification and a Pear-
son correlation of 0.303 on computing seman-
tic similarity.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999981172413793">
The process of rewriting text with a different choice
of words or using a different sentence structure
while preserving meaning is called paraphrasing.
Identifying paraphrases can be a difficult task owing
to the fact that evaluating surface level similarity is
often not enough, but rather systems must take into
account the underlying semantics of the content be-
ing assessed.
Paraphrasing and paraphrase detection are impor-
tant and challenging tasks, which find their applica-
tion in various subfields of Natural Language Pro-
cessing (NLP) such as information retrieval, ques-
tion answering (Erwin and Emiel, 2005), plagiarism
detection (Paul Clough et al., 2002), text summa-
rization and evaluation of machine translation (Chris
Callison Burch, 2008).
We explore using recursive autoencoders for para-
phrase detection and similarity scoring as a part of
SemEval 2015 Task 1: Paraphrase and Semantic
Similarity in Twitter. Twitter is an online social net-
working service with millions of users who casually
converse about diverse topics in a continuous and
contemporaneous manner (Wei Xu et al., 2014; Wei
Xu et al., 2015). Table 1 gives an example of real
tweets, some of which are paraphrases of each other.
The very casual style of the Twitter corpus makes it
more challenging to work with for many NLP tools.
We use vector space embeddings, in part, since they
are relatively good at dealing with noisy data.
</bodyText>
<sectionHeader confidence="0.999691" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999954272727273">
Socher et al. (2011) explored using recursive au-
toencoders (RAEs) and dynamic pooling for para-
phrase detection. They parse each sentence within a
pair, compute embeddings for each node in the parse
trees, and then construct a similarity matrix compar-
ing the embedding vectors for all nodes within the
two parse trees. Using dynamic pooling, they con-
vert the variable size similarity matrix for each sen-
tence pair to a matrix of fixed size. The resulting
fixed size matrix is then given to a softmax classifier
to detect whether the sentences are paraphrases.
</bodyText>
<sectionHeader confidence="0.991275" genericHeader="method">
3 A Deep Learning System
</sectionHeader>
<bodyText confidence="0.999864181818182">
The architecture of our system is depicted in Figure
1. The raw Twitter corpus is preprocessed using a
phrase-structure parser. The resulting parse trees are
then used to train an unfolding RAE model. This
model provides us with embedding vectors that are
then used to compute the similarity between every
node in the parse trees associated with a sentence
pair. A similarity matrix is populated with the node-
to-node similarity scores as measured by the Eu-
clidean distance beween the node embedding vec-
tors. The size of the similarity matrix depends on
</bodyText>
<page confidence="0.99818">
45
</page>
<note confidence="0.679699">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 45–50,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
</note>
<table confidence="0.972647333333333">
Sentence 1 Sentence 2 Paraphrase or Not
AAP is in the Adidas commercial AAP in that Adidas Commercial lol Paraphrase
That amber alert was getting annoying Why do I get amber alerts tho Not paraphrase
I am so watching Cinderella right now Im so watching Cinderella right now Paraphrase
That shot counted by Bayless Bayless just RAN for it Not Paraphrase
Damon EJ 1st Qb off the board if EJ is the 1st QB off the board Paraphrase
</table>
<tableCaption confidence="0.999743">
Table 1: Sample tweets from SemEval 2015 Twitter Paraphrase Corpus.
</tableCaption>
<figureCaption confidence="0.9439112">
Figure 1: System architecture: The unfolding recursive autoencoder computes phrase embedding vectors for each
node in a parse tree. For a pair of sentences being evaluated, the distances between all the nodes in the paired parse
trees are computed and fill a variable sized similarity matrix. Dynamic pooling is used to convert the variable size
similarity matrix to fixed size matrix. The fixed size similarity matrix is given to a softmax classifier to detect both
whether the paired sentences are paraphrases and for paraphrase similarity scoring.
</figureCaption>
<page confidence="0.99624">
46
</page>
<figureCaption confidence="0.998555428571429">
Figure 2: Dynamic pooling: The original variable sized
matrix is partitioned into an np x np grid of blocks of ap-
proximately equivalent size. We use min-pooling as the
aggregation operation, whereby the values of the cells in
the fixed size np x np matrix are assigned to the mini-
mum value of the corresponding partition in the original
matrix.
</figureCaption>
<bodyText confidence="0.999721947368421">
the number of nodes in the parse trees being com-
pared. This variable size similarity matrix is con-
verted to a fixed size matrix using Dynamic Pooling
(Socher et.al, 2011). Dynamic pooling partitions the
rows and columns of similarity matrix into np ap-
proximately equivalent segments which creates an
np x np grid. As depicted in Figure 2, the individ-
ual cells in the fixed size np x np matrix are assign
to the minimum values of their corresponding par-
titions in the original matrix. The resulting fixed
size matrix is then used to train a softmax classifier
to perform the actual paraphrase detection and pair-
wise similarity scoring tasks. To classify a pair of
new sentences, the sentences are first parsed. Using
the parse trees, the embedding vectors for each sen-
tence are constructed and used to populate a node-
to-node similarity matrix. This matrix is converted
to a fixed size using dynamic pooling and passed to
the softmax classification model.
</bodyText>
<subsectionHeader confidence="0.999938">
3.1 Unfolding Recursive Autoencoders (RAEs)
</subsectionHeader>
<bodyText confidence="0.999952666666667">
The architecture of our unfolding RAEs is illustrated
in Figure 2. The main difference between standard
RAEs and unfolding RAEs is that standard RAEs
are only directly trained to have each node recon-
struct its immediate children. Unfolding RAEs dif-
fer in that the training objective assess not only how
</bodyText>
<figureCaption confidence="0.9870795">
Figure 3: Architecture of unfolding RAEs. Using unfold-
ing RAEs, the embedding vector associated with each
node in a parse tree is trained to reconstruct the whole
parse tree fragment rooted at the current node.
</figureCaption>
<bodyText confidence="0.99929725">
well the representation of each node reconstructs it’s
immediate children, but rather how well the node’s
representation reconstructs the entire parse tree frag-
ment rooted at the current node.
</bodyText>
<sectionHeader confidence="0.995065" genericHeader="method">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.9999901">
We use a general domain parsing model distributed
with the Stanford Parser, englishPCFG v1.6.9 (Klein
and Manning, 2003). Prior to training the RAE vec-
tors, we pre-trained word embedding vectors for use
as the word level representations (Ronan and Jason,
2008). The hyperparameter values used for our sys-
tem are as follows: (1) the size of the pooling matrix
np = 13; (2) the regularization for the softmax clas-
sifier c = 0.05; (3) Both the RAE and word embed-
dings are 100-dimensional vectors.
</bodyText>
<subsectionHeader confidence="0.993647">
4.1 Data Set Details
</subsectionHeader>
<bodyText confidence="0.999685">
Our SemEval task provided the PIT-2015 Twitter
Paraphrase corpus for training and system develop-
ment (Wei Xu, 2014; Wei Xu et al., 2014; Wei Xu
et al., 2015). The corpus contains a training set with
13,063 sentence pairs, a development set with 4,727
sentence pairs, and a test set with 972 sentence pairs.
Table 2 shows the label distribution statistics for this
corpus. This data set is distinct from the data used
</bodyText>
<page confidence="0.992468">
47
</page>
<table confidence="0.9999456">
Category Paraphrase Non-Paraphrase Debatable Total
Sentence pair Sentence pair Sentence pair
Training 3,996 7,534 1,533 13,063
Development 1,470 2,672 585 4,727
Testing 175 663 134 972
</table>
<tableCaption confidence="0.942373">
Table 2: Statistics of PIT-2015 Twitter Paraphrase Corpus.
</tableCaption>
<table confidence="0.9999298">
Twitter Training Testing/ Precision Recall F1
Corpus Development Measure
50,000 13,063 4,727 0.51 0.48 0.49
80,000 13,063 4,727 0.65 0.37 0.51
95,000 13,063 4,727 0.77 0.35 0.56
</table>
<tableCaption confidence="0.999833">
Table 3: PIT-2015 dev set performance using varying amounts of training data.
</tableCaption>
<bodyText confidence="0.849801307692308">
in other work on paraphrasing in the following ways:
(1) it contains sentences that are colloquial and opin-
ionated; (2) it contains paraphrases that are lexically
diverse; and (3) it contains many sentences that are
lexically similar but semantically dissimilar (Wei Xu
et al., 2015).
The training and development data was jointly
collected from 500+ trending topics and then ran-
domly split into the final training and development
sets. The test data was drawn from 20 randomly
sampled Twitter trending topics. Labels were col-
lected by having each sentence pair annotated by 5
different crowdsourced workers.
</bodyText>
<subsectionHeader confidence="0.977539">
4.2 Evaluation and Discussion
</subsectionHeader>
<bodyText confidence="0.9998084">
For the unsupervised unfolding RAE training, we
experimented with using subsets of different sized
Twitter corpora of 50,000, 80,000 and 95,000 sen-
tences to evaluate the proposed system. Using PIT-
2015, we trained using tweets from the training set
and evaluated the resulting series of systems on the
dev set (Wei Xu et al., 2015). For supervised train-
ing, we used the training set from PIT-2015. For
training the unsupervised unfolding RAE vectors,
we collected additional data using the Twitter De-
veloper API. As shown in Table 3, we found that
increasing the size of the data set used to train the
RAE embeddings leads to strong gains in system
performance.1 Notice that as the amount of data
used to train the RAE vectors increases, the preci-
</bodyText>
<footnote confidence="0.986136">
1Due to time constraints we did not explore using more than
95,000 sentences to train our embedding model.
</footnote>
<table confidence="0.9963362">
Metrics Type Accuracy
maxF1 0.457
mPrecision 0.543
mRecall 0.394
Pearson 0.303
</table>
<tableCaption confidence="0.999752">
Table 4: Results from the SemEval-2015.
</tableCaption>
<bodyText confidence="0.9999635">
sion value for paraphrase detection increases signif-
icantly while the recall value is actually falling.
The official evaluation metrics for SemEval-2015
Task 1 are F1-score for paraphrase identification
and Pearson correlation for the semantic similarity
scores. The performance of our system on the shared
task evaluation data using these metrics is presented
in Table 4.
</bodyText>
<sectionHeader confidence="0.980038" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999982785714286">
We participated in SemEval 2015 Task 1: Para-
phrase and Semantic Similarity in Twitter using
a system architecture motivated by the success of
prior work on using RAE for paraphrase detection
(Socher et al. 2011). We find that the performance
of the system receives a sizable boost with the ad-
dition of a moderate amount of unsupervised RAE
training data.
In future work, we plan to try to improve perfor-
mance by first normalizing the Twitter data prior to
parsing. Given the mismatch between general do-
main English data and tweets, parse accuracy would
have likely been improved by performing a pre-
processing step that normalized the tweets prior to
</bodyText>
<page confidence="0.998125">
48
</page>
<bodyText confidence="0.99840725">
giving them to the parser (Juri Ganitkevitch et al.,
2013; Brendan O Connor et al., 2010). This could
lead to improved downstream paraphrase detection
and similarity scoring. We would also like to ex-
plore using new learning algorithms for the final
paraphrase classification as well as alternative mech-
anisms of constructing the sentence level embedding
vectors.
</bodyText>
<sectionHeader confidence="0.983947" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999846222222222">
In this work, we would like to convey our sincere
gratitude and special thanks towards Wei Xu, orga-
nizer of SemEval PIT 2015, who helped us in the
training and development data set and to evaluate
our system results. We would like again to convey
our sincere gratitude towards Daniel Cer, who en-
couraged and motivated us throughout the final sub-
mission. And we would convey our sincere thanks
to all the organizers of SemEval 2015.
</bodyText>
<sectionHeader confidence="0.979739" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.993373098765432">
Bill Dolan., Chris Quirk and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
Exploiting massively parallel news sources. Proceed-
ings of the 20th international conference on Computa-
tional Linguistics (pp. 350).
Brendan O Connor., Michel Krieger and David Ahn.
2010. TweetMotif: Exploratory Search and Topic
Summarization for Twitter.
Chris Callison Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (pp. 196-205).
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. Proceedings of the 41st
Annual Meeting on Association for Computational
Linguistics-Volume 1, pp. 423-430.
Duyu Tang., Furu Wei., Bing Qin., Ting Liu and Ming
Zhou. 2014. Coooolll: A Deep Learning System for
Twitter Sentiment Classification. Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval-2014). (pp. 208-212).
Eric Huang 2011. Paraphrase Detection Using Recur-
sive Autoencoder.
Erwin Marsi and Emiel Krahmer 2005. Explorations in
sentence fusion. Proceedings of the European Work-
shop on Natural Language Generation (pp. 109-117).
Fabio Massimo Zanzotto., Marco Pennacchiotti and
Kostas Tsioutsiouliklis. 2011. Linguistic redundancy
in twitter. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing (pp.
659-669).
Juri Ganitkevitch., Benjamin Van Durme and Chris
Callison-Burch. 2013. PPDB: The Paraphrase
Database. In HLT-NAACL (pp. 758-764).
Leon Derczynski., Alan Ritter., Sam Clark and Kalina
Bontcheva. 2013. Twitter Part-of-Speech Tagging for
All: Overcoming Sparse and Noisy Data. In RANLP
(pp. 198-206).
Microsoft research paraphrase corpus. Accessed on
September-2014. http://research.microsoft.com/en-
us/.
Nitin Madnan and Bonnie J. Dorr. 2010. Generating
phrasal and sentential paraphrases: A survey of data-
driven methods. Computational Linguistics, 36(3),
(pp. 341-387).
Paul Clough., Robert Gaizauskas., Scott SL Piao and
Yorick Wilks. 2002. Meter: Measuring text reuse.
Proceedings of the 40th Annual Meeting on Associa-
tion for Computational Linguistics (pp. 152-159).
Qayyum Ul Zia and Altaf Wasif. 2012. Paraphrase
Identification using Semantic Heuristic Features. Re-
search Journal of Applied Sciences, Engineering and
Technology 4(22): 4894-4904.
Richard Socher., Eric H. Huang., Jeffrey Pennin.,
Christopher D. Manning and Andrew Y. Ng. 2011.
Dynamic pooling and unfolding recursive autoen-
coders for paraphrase detection. Advances in Neural
Information Processing Systems (pp. 801-809).
Ronan Collobert and Jason Weston. 2008. A unified ar-
chitecture for natural language processing: Deep neu-
ral networks with multitask learning. Proceedings of
the 25th international conference on Machine learning
(pp. 160-167).
Samuel Fernando and Mark Stevenson. 2008. A seman-
tic similarity approach to paraphrase detection. Com-
putational Linguistics UK (CLUK 2008) 11th Annual
Research Colloquium (pp. 45-52).
Sasa Petrovic., Miles Osborne and Victor Lavrenko.
2012. Using paraphrases for improving first story de-
tection in news and Twitter. Proceedings of the 2012
Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies (pp. 338-346).
Wang Ling., Chris Dyer., Alan W. Black and Isabel Tran-
coso. 2013. Paraphrasing 4 Microblog Normaliza-
tion. In EMNLP (pp. 73-84).
Wei Wu., Yun-Cheng Ju., Xiao Li and Ye-Yi Wang.
2010. Paraphrase detection on SMS messages in au-
tomobiles. In Acoustics Speech and Signal Process-
ing (ICASSP), 2010 IEEE International Conference on
(pp. 5326-5329).
</reference>
<page confidence="0.988091">
49
</page>
<reference confidence="0.999168833333333">
Wei Xu., Alan Ritter., Bill Dolan., Ralph Grishman and
Colin Cherry. 2012. Paraphrasing for Style. Proceed-
ings of COLING 2012:Technical paper, pages 2899-
2914, Coling 2012, Mumbai, December 2012.
Wei Xu., Alan Ritter and Ralph Grishmann. 2013. Gath-
ering and generating paraphrases from twitter with
application to normalization. Proceedings of the Sixth
Workshop on Building and Using Comparable Cor-
pora (pp. 121-128).
Wei Xu., Ralph Grishman., Adam Meyers and Alan Rit-
ter. 2013. A Preliminary Study of Tweet Summariza-
tion using Information Extraction.
Wei Xu 2014. Data-driven approaches for paraphras-
ing across language variations (Doctoral dissertation,
New York University).
Wei Xu., Alan Ritter., Chris Callison Burch., William
B. Dolan and Yangfeng Ji. 2014. Extracting Lex-
ically Divergent Paraphrases from Twitter. Transac-
tions Of The Association For Computational Linguis-
tics, 2, 435-448.
Wei Xu., Chris Callison Burch and William B. Dolan.
2015. Paraphrase and Semantic Similarity in Twitter
(PIT2015). International Workshop on Semantic Eval-
uation (SemEval 2015).
</reference>
<page confidence="0.997683">
50
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.205600">
<title confidence="0.999786">Paraphrase Detection for Twitter Unsupervised Feature Learning with Recursive Autoencoders</title>
<author confidence="0.999514">Mahalakshmi Shanmuga Sundaram</author>
<author confidence="0.999514">Anand Kumar Madasamy</author>
<author confidence="0.999514">Soman Kotti</author>
<affiliation confidence="0.997782">Center for Excellence in Computational Engineering and</affiliation>
<note confidence="0.260604">Amrita Vishwa Coimbatore,</note>
<abstract confidence="0.996251181818182">We explore using recursive autoencoders for SemEval 2015 Task 1: Paraphrase and Semantic Similarity in Twitter. Our paraphrase detection system makes use of phrase-structure parse tree embeddings that are then provided as input to a conventional supervised classification model. We achieve an F1 score of 0.45 on paraphrase identification and a Pearson correlation of 0.303 on computing semantic similarity.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Bill Dolan</author>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
</authors>
<title>Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources.</title>
<date>2004</date>
<booktitle>Proceedings of the 20th international conference on Computational Linguistics</booktitle>
<pages>350</pages>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>Bill Dolan., Chris Quirk and Chris Brockett. 2004. Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources. Proceedings of the 20th international conference on Computational Linguistics (pp. 350).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brendan O Connor</author>
<author>Michel Krieger</author>
<author>David Ahn</author>
</authors>
<title>TweetMotif: Exploratory Search and Topic Summarization for Twitter.</title>
<date>2010</date>
<contexts>
<context position="10875" citStr="Connor et al., 2010" startWordPosition="1745" endWordPosition="1748">d by the success of prior work on using RAE for paraphrase detection (Socher et al. 2011). We find that the performance of the system receives a sizable boost with the addition of a moderate amount of unsupervised RAE training data. In future work, we plan to try to improve performance by first normalizing the Twitter data prior to parsing. Given the mismatch between general domain English data and tweets, parse accuracy would have likely been improved by performing a preprocessing step that normalized the tweets prior to 48 giving them to the parser (Juri Ganitkevitch et al., 2013; Brendan O Connor et al., 2010). This could lead to improved downstream paraphrase detection and similarity scoring. We would also like to explore using new learning algorithms for the final paraphrase classification as well as alternative mechanisms of constructing the sentence level embedding vectors. Acknowledgments In this work, we would like to convey our sincere gratitude and special thanks towards Wei Xu, organizer of SemEval PIT 2015, who helped us in the training and development data set and to evaluate our system results. We would like again to convey our sincere gratitude towards Daniel Cer, who encouraged and mo</context>
</contexts>
<marker>Connor, Krieger, Ahn, 2010</marker>
<rawString>Brendan O Connor., Michel Krieger and David Ahn. 2010. TweetMotif: Exploratory Search and Topic Summarization for Twitter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison Burch</author>
</authors>
<title>Syntactic constraints on paraphrases extracted from parallel corpora.</title>
<date>2008</date>
<booktitle>Proceedings of the Conference on Empirical Methods in Natural Language Processing</booktitle>
<pages>196--205</pages>
<contexts>
<context position="1566" citStr="Burch, 2008" startWordPosition="216" endWordPosition="217"> paraphrasing. Identifying paraphrases can be a difficult task owing to the fact that evaluating surface level similarity is often not enough, but rather systems must take into account the underlying semantics of the content being assessed. Paraphrasing and paraphrase detection are important and challenging tasks, which find their application in various subfields of Natural Language Processing (NLP) such as information retrieval, question answering (Erwin and Emiel, 2005), plagiarism detection (Paul Clough et al., 2002), text summarization and evaluation of machine translation (Chris Callison Burch, 2008). We explore using recursive autoencoders for paraphrase detection and similarity scoring as a part of SemEval 2015 Task 1: Paraphrase and Semantic Similarity in Twitter. Twitter is an online social networking service with millions of users who casually converse about diverse topics in a continuous and contemporaneous manner (Wei Xu et al., 2014; Wei Xu et al., 2015). Table 1 gives an example of real tweets, some of which are paraphrases of each other. The very casual style of the Twitter corpus makes it more challenging to work with for many NLP tools. We use vector space embeddings, in part,</context>
</contexts>
<marker>Burch, 2008</marker>
<rawString>Chris Callison Burch. 2008. Syntactic constraints on paraphrases extracted from parallel corpora. Proceedings of the Conference on Empirical Methods in Natural Language Processing (pp. 196-205).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="6805" citStr="Klein and Manning, 2003" startWordPosition="1080" endWordPosition="1083">en. Unfolding RAEs differ in that the training objective assess not only how Figure 3: Architecture of unfolding RAEs. Using unfolding RAEs, the embedding vector associated with each node in a parse tree is trained to reconstruct the whole parse tree fragment rooted at the current node. well the representation of each node reconstructs it’s immediate children, but rather how well the node’s representation reconstructs the entire parse tree fragment rooted at the current node. 4 Experimental Results We use a general domain parsing model distributed with the Stanford Parser, englishPCFG v1.6.9 (Klein and Manning, 2003). Prior to training the RAE vectors, we pre-trained word embedding vectors for use as the word level representations (Ronan and Jason, 2008). The hyperparameter values used for our system are as follows: (1) the size of the pooling matrix np = 13; (2) the regularization for the softmax classifier c = 0.05; (3) Both the RAE and word embeddings are 100-dimensional vectors. 4.1 Data Set Details Our SemEval task provided the PIT-2015 Twitter Paraphrase corpus for training and system development (Wei Xu, 2014; Wei Xu et al., 2014; Wei Xu et al., 2015). The corpus contains a training set with 13,063</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pp. 423-430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Duyu Tang</author>
<author>Furu Wei</author>
<author>Bing Qin</author>
<author>Ting Liu</author>
<author>Ming Zhou</author>
</authors>
<title>Coooolll: A Deep Learning System for Twitter Sentiment Classification.</title>
<date>2014</date>
<booktitle>Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014).</booktitle>
<pages>208--212</pages>
<marker>Tang, Wei, Qin, Liu, Zhou, 2014</marker>
<rawString>Duyu Tang., Furu Wei., Bing Qin., Ting Liu and Ming Zhou. 2014. Coooolll: A Deep Learning System for Twitter Sentiment Classification. Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014). (pp. 208-212).</rawString>
</citation>
<citation valid="false">
<title>Eric Huang 2011. Paraphrase Detection Using Recursive Autoencoder.</title>
<marker></marker>
<rawString>Eric Huang 2011. Paraphrase Detection Using Recursive Autoencoder.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erwin Marsi</author>
<author>Emiel Krahmer</author>
</authors>
<title>Explorations in sentence fusion.</title>
<date>2005</date>
<booktitle>Proceedings of the European Workshop on Natural Language Generation</booktitle>
<pages>109--117</pages>
<marker>Marsi, Krahmer, 2005</marker>
<rawString>Erwin Marsi and Emiel Krahmer 2005. Explorations in sentence fusion. Proceedings of the European Workshop on Natural Language Generation (pp. 109-117).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Massimo Zanzotto</author>
</authors>
<title>Marco Pennacchiotti and Kostas Tsioutsiouliklis.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing</booktitle>
<pages>659--669</pages>
<marker>Zanzotto, 2011</marker>
<rawString>Fabio Massimo Zanzotto., Marco Pennacchiotti and Kostas Tsioutsiouliklis. 2011. Linguistic redundancy in twitter. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (pp. 659-669).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juri Ganitkevitch</author>
<author>Benjamin Van Durme</author>
<author>Chris Callison-Burch</author>
</authors>
<title>PPDB: The Paraphrase Database.</title>
<date>2013</date>
<booktitle>In HLT-NAACL</booktitle>
<pages>758--764</pages>
<marker>Ganitkevitch, Van Durme, Callison-Burch, 2013</marker>
<rawString>Juri Ganitkevitch., Benjamin Van Durme and Chris Callison-Burch. 2013. PPDB: The Paraphrase Database. In HLT-NAACL (pp. 758-764).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leon Derczynski</author>
<author>Alan Ritter</author>
<author>Sam Clark</author>
<author>Kalina Bontcheva</author>
</authors>
<title>Twitter Part-of-Speech Tagging for All: Overcoming Sparse and Noisy Data.</title>
<date>2013</date>
<booktitle>In RANLP</booktitle>
<pages>198--206</pages>
<marker>Derczynski, Ritter, Clark, Bontcheva, 2013</marker>
<rawString>Leon Derczynski., Alan Ritter., Sam Clark and Kalina Bontcheva. 2013. Twitter Part-of-Speech Tagging for All: Overcoming Sparse and Noisy Data. In RANLP (pp. 198-206).</rawString>
</citation>
<citation valid="false">
<title>Microsoft research paraphrase corpus.</title>
<note>Accessed on September-2014. http://research.microsoft.com/enus/.</note>
<marker></marker>
<rawString>Microsoft research paraphrase corpus. Accessed on September-2014. http://research.microsoft.com/enus/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnan</author>
<author>Bonnie J Dorr</author>
</authors>
<title>Generating phrasal and sentential paraphrases: A survey of datadriven methods.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>3</issue>
<pages>341--387</pages>
<marker>Madnan, Dorr, 2010</marker>
<rawString>Nitin Madnan and Bonnie J. Dorr. 2010. Generating phrasal and sentential paraphrases: A survey of datadriven methods. Computational Linguistics, 36(3), (pp. 341-387).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Clough</author>
<author>Robert Gaizauskas</author>
<author>Scott SL Piao</author>
<author>Yorick Wilks</author>
</authors>
<title>Meter: Measuring text reuse.</title>
<date>2002</date>
<journal>Research Journal of Applied Sciences, Engineering and Technology</journal>
<booktitle>Proceedings of the 40th Annual Meeting on Association for Computational Linguistics</booktitle>
<volume>4</volume>
<issue>22</issue>
<pages>152--159</pages>
<institution>Qayyum Ul Zia and Altaf Wasif.</institution>
<contexts>
<context position="1479" citStr="Clough et al., 2002" startWordPosition="202" endWordPosition="205">rent choice of words or using a different sentence structure while preserving meaning is called paraphrasing. Identifying paraphrases can be a difficult task owing to the fact that evaluating surface level similarity is often not enough, but rather systems must take into account the underlying semantics of the content being assessed. Paraphrasing and paraphrase detection are important and challenging tasks, which find their application in various subfields of Natural Language Processing (NLP) such as information retrieval, question answering (Erwin and Emiel, 2005), plagiarism detection (Paul Clough et al., 2002), text summarization and evaluation of machine translation (Chris Callison Burch, 2008). We explore using recursive autoencoders for paraphrase detection and similarity scoring as a part of SemEval 2015 Task 1: Paraphrase and Semantic Similarity in Twitter. Twitter is an online social networking service with millions of users who casually converse about diverse topics in a continuous and contemporaneous manner (Wei Xu et al., 2014; Wei Xu et al., 2015). Table 1 gives an example of real tweets, some of which are paraphrases of each other. The very casual style of the Twitter corpus makes it mor</context>
</contexts>
<marker>Clough, Gaizauskas, Piao, Wilks, 2002</marker>
<rawString>Paul Clough., Robert Gaizauskas., Scott SL Piao and Yorick Wilks. 2002. Meter: Measuring text reuse. Proceedings of the 40th Annual Meeting on Association for Computational Linguistics (pp. 152-159). Qayyum Ul Zia and Altaf Wasif. 2012. Paraphrase Identification using Semantic Heuristic Features. Research Journal of Applied Sciences, Engineering and Technology 4(22): 4894-4904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Eric H Huang</author>
<author>Jeffrey Pennin</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Dynamic pooling and unfolding recursive autoencoders for paraphrase detection.</title>
<date>2011</date>
<booktitle>Advances in Neural Information Processing Systems</booktitle>
<pages>801--809</pages>
<contexts>
<context position="2261" citStr="Socher et al. (2011)" startWordPosition="333" endWordPosition="336">larity scoring as a part of SemEval 2015 Task 1: Paraphrase and Semantic Similarity in Twitter. Twitter is an online social networking service with millions of users who casually converse about diverse topics in a continuous and contemporaneous manner (Wei Xu et al., 2014; Wei Xu et al., 2015). Table 1 gives an example of real tweets, some of which are paraphrases of each other. The very casual style of the Twitter corpus makes it more challenging to work with for many NLP tools. We use vector space embeddings, in part, since they are relatively good at dealing with noisy data. 2 Related Work Socher et al. (2011) explored using recursive autoencoders (RAEs) and dynamic pooling for paraphrase detection. They parse each sentence within a pair, compute embeddings for each node in the parse trees, and then construct a similarity matrix comparing the embedding vectors for all nodes within the two parse trees. Using dynamic pooling, they convert the variable size similarity matrix for each sentence pair to a matrix of fixed size. The resulting fixed size matrix is then given to a softmax classifier to detect whether the sentences are paraphrases. 3 A Deep Learning System The architecture of our system is de</context>
<context position="10344" citStr="Socher et al. 2011" startWordPosition="1653" endWordPosition="1656">2015. sion value for paraphrase detection increases significantly while the recall value is actually falling. The official evaluation metrics for SemEval-2015 Task 1 are F1-score for paraphrase identification and Pearson correlation for the semantic similarity scores. The performance of our system on the shared task evaluation data using these metrics is presented in Table 4. 5 Conclusion and Future Work We participated in SemEval 2015 Task 1: Paraphrase and Semantic Similarity in Twitter using a system architecture motivated by the success of prior work on using RAE for paraphrase detection (Socher et al. 2011). We find that the performance of the system receives a sizable boost with the addition of a moderate amount of unsupervised RAE training data. In future work, we plan to try to improve performance by first normalizing the Twitter data prior to parsing. Given the mismatch between general domain English data and tweets, parse accuracy would have likely been improved by performing a preprocessing step that normalized the tweets prior to 48 giving them to the parser (Juri Ganitkevitch et al., 2013; Brendan O Connor et al., 2010). This could lead to improved downstream paraphrase detection and sim</context>
</contexts>
<marker>Socher, Huang, Pennin, Manning, Ng, 2011</marker>
<rawString>Richard Socher., Eric H. Huang., Jeffrey Pennin., Christopher D. Manning and Andrew Y. Ng. 2011. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. Advances in Neural Information Processing Systems (pp. 801-809).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>Proceedings of the 25th international conference on Machine learning</booktitle>
<pages>160--167</pages>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. Proceedings of the 25th international conference on Machine learning (pp. 160-167).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Fernando</author>
<author>Mark Stevenson</author>
</authors>
<title>A semantic similarity approach to paraphrase detection.</title>
<date>2008</date>
<booktitle>Computational Linguistics UK (CLUK 2008) 11th Annual Research Colloquium</booktitle>
<pages>45--52</pages>
<marker>Fernando, Stevenson, 2008</marker>
<rawString>Samuel Fernando and Mark Stevenson. 2008. A semantic similarity approach to paraphrase detection. Computational Linguistics UK (CLUK 2008) 11th Annual Research Colloquium (pp. 45-52).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sasa Petrovic</author>
<author>Miles Osborne</author>
<author>Victor Lavrenko</author>
</authors>
<title>Using paraphrases for improving first story detection in news and Twitter.</title>
<date>2012</date>
<booktitle>Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</booktitle>
<pages>338--346</pages>
<marker>Petrovic, Osborne, Lavrenko, 2012</marker>
<rawString>Sasa Petrovic., Miles Osborne and Victor Lavrenko. 2012. Using paraphrases for improving first story detection in news and Twitter. Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (pp. 338-346).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wang Ling</author>
<author>Chris Dyer</author>
<author>Alan W Black</author>
<author>Isabel Trancoso</author>
</authors>
<title>Paraphrasing 4 Microblog Normalization.</title>
<date>2013</date>
<booktitle>In EMNLP</booktitle>
<pages>73--84</pages>
<marker>Ling, Dyer, Black, Trancoso, 2013</marker>
<rawString>Wang Ling., Chris Dyer., Alan W. Black and Isabel Trancoso. 2013. Paraphrasing 4 Microblog Normalization. In EMNLP (pp. 73-84).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Wu</author>
<author>Yun-Cheng Ju</author>
<author>Xiao Li</author>
<author>Ye-Yi Wang</author>
</authors>
<title>Paraphrase detection on SMS messages in automobiles.</title>
<date>2010</date>
<booktitle>In Acoustics Speech and Signal Processing (ICASSP), 2010 IEEE International Conference on</booktitle>
<pages>5326--5329</pages>
<marker>Wu, Ju, Li, Wang, 2010</marker>
<rawString>Wei Wu., Yun-Cheng Ju., Xiao Li and Ye-Yi Wang. 2010. Paraphrase detection on SMS messages in automobiles. In Acoustics Speech and Signal Processing (ICASSP), 2010 IEEE International Conference on (pp. 5326-5329).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Xu</author>
<author>Alan Ritter</author>
<author>Bill Dolan</author>
<author>Ralph Grishman</author>
<author>Colin Cherry</author>
</authors>
<title>Paraphrasing for Style.</title>
<date>2012</date>
<booktitle>Proceedings of COLING 2012:Technical paper,</booktitle>
<pages>2899--2914</pages>
<location>Mumbai,</location>
<marker>Xu, Ritter, Dolan, Grishman, Cherry, 2012</marker>
<rawString>Wei Xu., Alan Ritter., Bill Dolan., Ralph Grishman and Colin Cherry. 2012. Paraphrasing for Style. Proceedings of COLING 2012:Technical paper, pages 2899-2914, Coling 2012, Mumbai, December 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Xu</author>
<author>Alan Ritter</author>
<author>Ralph Grishmann</author>
</authors>
<title>Gathering and generating paraphrases from twitter with application to normalization.</title>
<date>2013</date>
<booktitle>Proceedings of the Sixth Workshop on Building and Using Comparable Corpora</booktitle>
<pages>121--128</pages>
<marker>Xu, Ritter, Grishmann, 2013</marker>
<rawString>Wei Xu., Alan Ritter and Ralph Grishmann. 2013. Gathering and generating paraphrases from twitter with application to normalization. Proceedings of the Sixth Workshop on Building and Using Comparable Corpora (pp. 121-128).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Xu</author>
<author>Ralph Grishman</author>
<author>Adam Meyers</author>
<author>Alan Ritter</author>
</authors>
<title>A Preliminary Study of Tweet Summarization using Information Extraction.</title>
<date>2013</date>
<marker>Xu, Grishman, Meyers, Ritter, 2013</marker>
<rawString>Wei Xu., Ralph Grishman., Adam Meyers and Alan Ritter. 2013. A Preliminary Study of Tweet Summarization using Information Extraction.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Xu</author>
</authors>
<title>Data-driven approaches for paraphrasing across language variations (Doctoral dissertation,</title>
<date>2014</date>
<location>New York University).</location>
<contexts>
<context position="7314" citStr="Xu, 2014" startWordPosition="1170" endWordPosition="1171">in parsing model distributed with the Stanford Parser, englishPCFG v1.6.9 (Klein and Manning, 2003). Prior to training the RAE vectors, we pre-trained word embedding vectors for use as the word level representations (Ronan and Jason, 2008). The hyperparameter values used for our system are as follows: (1) the size of the pooling matrix np = 13; (2) the regularization for the softmax classifier c = 0.05; (3) Both the RAE and word embeddings are 100-dimensional vectors. 4.1 Data Set Details Our SemEval task provided the PIT-2015 Twitter Paraphrase corpus for training and system development (Wei Xu, 2014; Wei Xu et al., 2014; Wei Xu et al., 2015). The corpus contains a training set with 13,063 sentence pairs, a development set with 4,727 sentence pairs, and a test set with 972 sentence pairs. Table 2 shows the label distribution statistics for this corpus. This data set is distinct from the data used 47 Category Paraphrase Non-Paraphrase Debatable Total Sentence pair Sentence pair Sentence pair Training 3,996 7,534 1,533 13,063 Development 1,470 2,672 585 4,727 Testing 175 663 134 972 Table 2: Statistics of PIT-2015 Twitter Paraphrase Corpus. Twitter Training Testing/ Precision Recall F1 Corp</context>
</contexts>
<marker>Xu, 2014</marker>
<rawString>Wei Xu 2014. Data-driven approaches for paraphrasing across language variations (Doctoral dissertation, New York University).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Xu</author>
<author>Alan Ritter</author>
<author>Chris Callison Burch</author>
<author>William B Dolan</author>
<author>Yangfeng Ji</author>
</authors>
<title>Extracting Lexically Divergent Paraphrases from Twitter.</title>
<date>2014</date>
<journal>Transactions Of The Association For Computational Linguistics,</journal>
<volume>2</volume>
<pages>435--448</pages>
<contexts>
<context position="1913" citStr="Xu et al., 2014" startWordPosition="269" endWordPosition="272"> in various subfields of Natural Language Processing (NLP) such as information retrieval, question answering (Erwin and Emiel, 2005), plagiarism detection (Paul Clough et al., 2002), text summarization and evaluation of machine translation (Chris Callison Burch, 2008). We explore using recursive autoencoders for paraphrase detection and similarity scoring as a part of SemEval 2015 Task 1: Paraphrase and Semantic Similarity in Twitter. Twitter is an online social networking service with millions of users who casually converse about diverse topics in a continuous and contemporaneous manner (Wei Xu et al., 2014; Wei Xu et al., 2015). Table 1 gives an example of real tweets, some of which are paraphrases of each other. The very casual style of the Twitter corpus makes it more challenging to work with for many NLP tools. We use vector space embeddings, in part, since they are relatively good at dealing with noisy data. 2 Related Work Socher et al. (2011) explored using recursive autoencoders (RAEs) and dynamic pooling for paraphrase detection. They parse each sentence within a pair, compute embeddings for each node in the parse trees, and then construct a similarity matrix comparing the embedding vect</context>
<context position="7335" citStr="Xu et al., 2014" startWordPosition="1173" endWordPosition="1176">el distributed with the Stanford Parser, englishPCFG v1.6.9 (Klein and Manning, 2003). Prior to training the RAE vectors, we pre-trained word embedding vectors for use as the word level representations (Ronan and Jason, 2008). The hyperparameter values used for our system are as follows: (1) the size of the pooling matrix np = 13; (2) the regularization for the softmax classifier c = 0.05; (3) Both the RAE and word embeddings are 100-dimensional vectors. 4.1 Data Set Details Our SemEval task provided the PIT-2015 Twitter Paraphrase corpus for training and system development (Wei Xu, 2014; Wei Xu et al., 2014; Wei Xu et al., 2015). The corpus contains a training set with 13,063 sentence pairs, a development set with 4,727 sentence pairs, and a test set with 972 sentence pairs. Table 2 shows the label distribution statistics for this corpus. This data set is distinct from the data used 47 Category Paraphrase Non-Paraphrase Debatable Total Sentence pair Sentence pair Sentence pair Training 3,996 7,534 1,533 13,063 Development 1,470 2,672 585 4,727 Testing 175 663 134 972 Table 2: Statistics of PIT-2015 Twitter Paraphrase Corpus. Twitter Training Testing/ Precision Recall F1 Corpus Development Measur</context>
</contexts>
<marker>Xu, Ritter, Burch, Dolan, Ji, 2014</marker>
<rawString>Wei Xu., Alan Ritter., Chris Callison Burch., William B. Dolan and Yangfeng Ji. 2014. Extracting Lexically Divergent Paraphrases from Twitter. Transactions Of The Association For Computational Linguistics, 2, 435-448.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Xu</author>
<author>Chris Callison Burch</author>
<author>William B Dolan</author>
</authors>
<date>2015</date>
<booktitle>Paraphrase and Semantic Similarity in Twitter (PIT2015). International Workshop on Semantic Evaluation (SemEval</booktitle>
<contexts>
<context position="1935" citStr="Xu et al., 2015" startWordPosition="274" endWordPosition="277"> of Natural Language Processing (NLP) such as information retrieval, question answering (Erwin and Emiel, 2005), plagiarism detection (Paul Clough et al., 2002), text summarization and evaluation of machine translation (Chris Callison Burch, 2008). We explore using recursive autoencoders for paraphrase detection and similarity scoring as a part of SemEval 2015 Task 1: Paraphrase and Semantic Similarity in Twitter. Twitter is an online social networking service with millions of users who casually converse about diverse topics in a continuous and contemporaneous manner (Wei Xu et al., 2014; Wei Xu et al., 2015). Table 1 gives an example of real tweets, some of which are paraphrases of each other. The very casual style of the Twitter corpus makes it more challenging to work with for many NLP tools. We use vector space embeddings, in part, since they are relatively good at dealing with noisy data. 2 Related Work Socher et al. (2011) explored using recursive autoencoders (RAEs) and dynamic pooling for paraphrase detection. They parse each sentence within a pair, compute embeddings for each node in the parse trees, and then construct a similarity matrix comparing the embedding vectors for all nodes with</context>
<context position="7357" citStr="Xu et al., 2015" startWordPosition="1178" endWordPosition="1181">he Stanford Parser, englishPCFG v1.6.9 (Klein and Manning, 2003). Prior to training the RAE vectors, we pre-trained word embedding vectors for use as the word level representations (Ronan and Jason, 2008). The hyperparameter values used for our system are as follows: (1) the size of the pooling matrix np = 13; (2) the regularization for the softmax classifier c = 0.05; (3) Both the RAE and word embeddings are 100-dimensional vectors. 4.1 Data Set Details Our SemEval task provided the PIT-2015 Twitter Paraphrase corpus for training and system development (Wei Xu, 2014; Wei Xu et al., 2014; Wei Xu et al., 2015). The corpus contains a training set with 13,063 sentence pairs, a development set with 4,727 sentence pairs, and a test set with 972 sentence pairs. Table 2 shows the label distribution statistics for this corpus. This data set is distinct from the data used 47 Category Paraphrase Non-Paraphrase Debatable Total Sentence pair Sentence pair Sentence pair Training 3,996 7,534 1,533 13,063 Development 1,470 2,672 585 4,727 Testing 175 663 134 972 Table 2: Statistics of PIT-2015 Twitter Paraphrase Corpus. Twitter Training Testing/ Precision Recall F1 Corpus Development Measure 50,000 13,063 4,727 </context>
<context position="9089" citStr="Xu et al., 2015" startWordPosition="1451" endWordPosition="1454">ending topics and then randomly split into the final training and development sets. The test data was drawn from 20 randomly sampled Twitter trending topics. Labels were collected by having each sentence pair annotated by 5 different crowdsourced workers. 4.2 Evaluation and Discussion For the unsupervised unfolding RAE training, we experimented with using subsets of different sized Twitter corpora of 50,000, 80,000 and 95,000 sentences to evaluate the proposed system. Using PIT2015, we trained using tweets from the training set and evaluated the resulting series of systems on the dev set (Wei Xu et al., 2015). For supervised training, we used the training set from PIT-2015. For training the unsupervised unfolding RAE vectors, we collected additional data using the Twitter Developer API. As shown in Table 3, we found that increasing the size of the data set used to train the RAE embeddings leads to strong gains in system performance.1 Notice that as the amount of data used to train the RAE vectors increases, the preci1Due to time constraints we did not explore using more than 95,000 sentences to train our embedding model. Metrics Type Accuracy maxF1 0.457 mPrecision 0.543 mRecall 0.394 Pearson 0.30</context>
</contexts>
<marker>Xu, Burch, Dolan, 2015</marker>
<rawString>Wei Xu., Chris Callison Burch and William B. Dolan. 2015. Paraphrase and Semantic Similarity in Twitter (PIT2015). International Workshop on Semantic Evaluation (SemEval 2015).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>