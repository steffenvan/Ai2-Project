<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.023374">
<title confidence="0.939763">
HIT: Web based Scoring Method for English Lexical Substitution
</title>
<author confidence="0.997961">
Shiqi Zhao, Lin Zhao, Yu Zhang, Ting Liu, Sheng Li
</author>
<affiliation confidence="0.988138">
Information Retrieval Laboratory, School of Computer Science and Technology,
</affiliation>
<address confidence="0.963813">
Box 321, Harbin Institute of Technology
Harbin, P.R. China, 150001
</address>
<email confidence="0.990589">
{ zhaosq, lzhao, zhangyu, tliu, lisheng }@ir.hit.edu.cn
</email>
<sectionHeader confidence="0.99545" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999709214285714">
This paper describes the HIT system and its
participation in SemEval-2007 English
Lexical Substitution Task. Two main steps
are included in our method: candidate sub-
stitute extraction and candidate scoring. In
the first step, candidate substitutes for each
target word in a given sentence are ex-
tracted from WordNet. In the second step,
the extracted candidates are scored and
ranked using a web-based scoring method.
The substitute ranked first is selected as the
best substitute. For the multiword subtask,
a simple WordNet-based approach is em-
ployed.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99993106122449">
Lexical substitution aims to find alternative words
that can occur in given contexts. It is important in
many applications, such as query reformulation in
question answering, sentence generation, and
paraphrasing. There are two key problems in the
lexical substitution task, the first of which is
candidate substitute extraction. Generally speaking,
synonyms can be regarded as candidate substitutes
of words. However, some looser lexical
relationships can also be considered, such as
Hypernyms and Hyponyms defined in WordNet
(Fellbaum, 1998). In addition, since lexical
substitution is context dependent, some words
which do not have similar meanings in general
may also be substituted in some certain contexts
(Zhao et al., 2007). As a result, finding a lexical
knowledge base for substitute extraction is a
challenging task.
The other problem is candidate scoring and
ranking according to given contexts. In the lexical
substitution task of SemEval-2007, context is con-
strained as a sentence. The system therefore has to
score the candidate substitutes of each target word
using the given sentence. The following questions
should be considered here: (1) What words in the
given sentence are “useful” context? (2) How to
combine the context words and use them in rank-
ing candidate substitutes? For the first question, we
can use all words of the sentence, words in a win-
dow, or words having syntactic relations with the
target word. For the second question, we can re-
gard the context words as “bag of words”, n-grams,
or syntactic structures.
In HIT, we extract candidate substitutes from
WordNet, in which both synonyms and hypernyms
are investigated (Section 3.1). After that, we score
the candidates using a web-based scoring method
(Section 3.2). In this method, we first select frag-
ments containing the target word from the given
sentence. Then we construct queries by replacing
the target word in the fragments with the candidate
substitute. Finally, we search Google using the
constructed queries and score each candidate based
on the counts of retrieved snippets.
The rest of this paper is organized as follows:
Section 2 reviews some related work on lexical
substitution. Section 3 describes our system, espe-
cially the web-based scoring method. Section 4
presents the results and analysis.
</bodyText>
<sectionHeader confidence="0.999522" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.941361">
Synonyms defined in WordNet have been widely
used in lexical substitution and expansion (Smea-
ton et al., 1994; Langkilde and Knight, 1998; Bol-
</bodyText>
<page confidence="0.985251">
173
</page>
<bodyText confidence="0.990872666666667">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 173–176,
Prague, June 2007. c�2007 Association for Computational Linguistics
shakov and Gelbukh, 2004). In addition, a lot of
methods have been proposed to automatically con-
struct thesauri of synonyms. For example, Lin
(1998) clustered words with similar meanings by
calculating the dependency similarity. Barzilay and
McKeown (2001) extracted paraphrases using mul-
tiple translations of literature works. Wu and Zhou
(2003) extracted synonyms with multiple resources,
including a monolingual dictionary, a bilingual
corpus, and a monolingual corpus. Besides the
handcrafted and automatic synonym resources, the
web has been exploited as a resource for lexical
substitute extraction (Zhao et al., 2007).
As for substitute scoring, various methods have
been investigated, among which the classification
method is the most widely used (Dagan et al., 2006;
Kauchak and Barzilay, 2006). In detail, a binary
classifier is trained for each candidate substitute,
using the contexts of the substitute as features.
Then a new contextual sentence containing the tar-
get word can be classified as 1 (the candidate is a
correct substitute in the given sentence) or 0 (oth-
erwise). The features used in the classification are
usually similar with that in word sense disam-
biguation (WSD), including bag of word lemmas
in the sentence, n-grams and parts of speech (POS)
in a window, etc. There are other models presented
for candidate substitute scoring. Glickman et al.
(2006) proposed a Bayesian model and a Neural
Network model, which estimate the probability of
a word may occur in a given context.
</bodyText>
<sectionHeader confidence="0.998244" genericHeader="method">
3 HIT System
</sectionHeader>
<subsectionHeader confidence="0.999777">
3.1 Candidate Substitute Extraction
</subsectionHeader>
<bodyText confidence="0.999940833333333">
In HIT, candidate substitutes are extracted from
WordNet. Both synonyms and hypernyms defined
in WordNet are investigated. Let w be a target
word, pos the specified POS of w. n the number of
w’s synsets defined in WordNet. Then the system
extracts w’s candidate substitutes as follows:
</bodyText>
<listItem confidence="0.989019">
• Extracts all the synonyms in each synset
under pos1 as candidate substitutes.
• If w has no synonym for the i-th synset
(1&lt;i&lt;n), then extracts the synonyms of its
nearest hypernym.
• If pos is r (or a), and no candidate substi-
tute can be extracted as described above,
1 In this task, four kinds of POS are specified: n - noun, v -
verb, a - adjective, r - adverb.
</listItem>
<bodyText confidence="0.648262">
then extracts candidate substitutes under the
POS a (or r).
</bodyText>
<subsectionHeader confidence="0.998678">
3.2 Candidate Substitute Scoring
</subsectionHeader>
<bodyText confidence="0.9938825">
As mentioned above, all words in the given sen-
tence can be used as contextual information in the
scoring of candidate substitutes. However, it is ob-
vious that not all context words are really useful
when determining a word’s substitutes. An exam-
ple can be seen from Figure 1.
She turns eyes &lt;head&gt;bright&lt;/head&gt; with
excitement towards Fiona , still tugging on the
string of the minitiature airship-cum-dance
card she has just received at the door .
</bodyText>
<figureCaption confidence="0.993585">
Figure 1. An example of a context sentence.
</figureCaption>
<bodyText confidence="0.999931636363636">
In the example above, words turns, eyes, with,
and excitement are useful context words, while the
others are not. The useless contexts may even be
noise if they are used in the scoring. As a result, it
is important to select context words carefully.
In HIT, we select context words based on the
following assumption: useful context words for
lexical substitute are those near the target word in
the given sentence. In other words, the words that
are far from the target word are not taken into con-
sideration. Obviously, this assumption is not al-
ways true. However, considering only the
neighboring words can reduce the risk of bringing
in noise. Besides, Edmonds (1997) has also dem-
onstrated in his paper that short-distance colloca-
tions with neighboring words are more useful in
lexical choice than long ones.
Let w be the target word, t a candidate substitute,
S the context sentence. Our basic idea is that: One
can substitute w in S with t, which generates a new
sentence S’. If S’ can be found on the web, then the
substitute is admissible. The more times S’ occurs
on the web, the more probable the substitute is. In
practice, however, it is difficult to find a whole
sentence S’ on the web due to sparseness. Instead,
we use fragments of S’ which contains t and sev-
eral neighboring context words (based on the as-
sumption above). Then the question is how to ob-
tain one (or more) fragment of S’.
A window with fixed size can be used here. Su-
ppose p is the position of t in S’, for instance, we
can construct a fragment using words from posi-
tion p-r to p+r, where r is the radius of window.
</bodyText>
<page confidence="0.992564">
174
</page>
<bodyText confidence="0.9996632">
However, a fixed r is difficult to set, since it may
be too large for some sentences, which makes the
fragments too specific, while too small for some
other sentences, which makes the fragments too
loose. An example can be seen in Table 1.
</bodyText>
<listItem confidence="0.730192333333333">
1(a) But when Daniel turned &lt;head&gt;blue&lt;/head&gt;
one time and he totally stopped breathing.
1(b) Daniel turned t one time
2(a) We recommend that you &lt;head&gt;check&lt;/head&gt;
with us beforehand.
2(b) that you t with us
</listItem>
<tableCaption confidence="0.990398">
Table 1. Examples of fragments with fixed size.
</tableCaption>
<bodyText confidence="0.937359151515152">
In Table1, 1(a) and 2(a) are two sentences from
the test data of SemEval-2007Task10. 1(b) and 2(b)
are fragments constructed according to 1(a) and
2(a), where the window radius is 2 and t denotes
any candidate substitute of the target word. It is
obvious that 1(b) is a rather strict fragment, which
makes it difficult to find sentences containing it on
the web, while 2(b) is quite loose, which can
hardly constrain the semantics of t.
Having considered the problem above, we pro-
pose a rule-based method that constructs fragments
with varied lengths. Let Ft be a fragment contain-
ing t, the construction rules are as follows:
Rule-1: Ft must contain at least two words be-
sides t, at least one of which is non-stop word.
Rule-2: Ft does not cross sub-sentence boundary
(“,”).
Rule-3: Ft should be the shortest fragment that
satisfies Rule-1 and Rule-2.
According to the rules above, we construct at
most three fragments for each S’: (1) t occurs at the
beginning of Ft, (2) t occurs in the middle of Ft,
and (3) t occurs at the end of Ft. Here we have an-
other constraint: if one constructed fragment F1 is
the substring of F2, then F2 is removed. Please
note that the morphology is not taken into account
when we construct queries.
For the sentence 1(a) and 2(a) in Table 1, the
constructed fragments are as follows:
For 1(a): Daniel turned t; t one time; turned t
one
For 2(a): recommend that you t; t with us be-
forehand
</bodyText>
<tableCaption confidence="0.757871">
Table 2. Examples of the constructed fragments
</tableCaption>
<bodyText confidence="0.9998824">
To score a candidate substitute, we replace “t” in
the fragments with each candidate substitute and
use them as queries, which are then fed to Google.
The score of t is computed according to the counts
of retrieved snippets:
</bodyText>
<equation confidence="0.9958255">
n
1
(t)= ∑count(Snippet(Fi
n i=1
</equation>
<bodyText confidence="0.999946153846154">
where n is the number of constructed fragments,
Fti is the i-th fragment (query) corresponding to t,
and count(Snippet(Fti)) is the count of snippets
retrieved by Fti.
All candidate substitutes with scores larger than
0 are ranked and the first 10 substitutes are re-
tained for the oot subtask. If the number of candi-
dates whose scores are larger than 0 is less than 10,
the system ranks the rest of the candidates by their
frequencies using a word frequency list. The spare
capacity is filled with those candidates with largest
frequencies. For the best subtask, we simply output
the substitute that ranks first in oot.
</bodyText>
<subsectionHeader confidence="0.999943">
3.3 Detection of Multiwords
</subsectionHeader>
<bodyText confidence="0.999979">
The method used to detect multiword in the HIT
system is quite similar to that employed in the
baseline system. We also use WordNet to detect if
a multiword that includes the target word occurs
within a window of 2 words before and 2 words
after the target word.
A difference from the baseline system lies in
that our system looks up WordNet using longer
multiword candidates first. If a longer one is found
in WordNet, then its substrings will be ignored.
For example, if we find “get along with” in Word-
Net, we will output it as a multiword and will not
check “get along” any more.
</bodyText>
<sectionHeader confidence="0.99987" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999672692307692">
Our system is the only one that participates all the
three subtasks of Task10, i.e., best, oot, and mw.
The evaluation results of our system can be found
in Table 3 to Table 5. Our system ranks the fourth
in the best subtask and seventh in the oot subtask.
We have analyzed the results from two aspects,
i.e., the ability of the system to extract candidate
substitutes and the ability to rank the correct sub-
stitutes in front. There are a total of 6,873 manual
substitutes for all the 1,710 items in the gold stan-
dard, only 2,168 (31.54%) of which have been ex-
tracted as candidate substitutes by our system. This
result suggests that WordNet is not an appropriate
</bodyText>
<equation confidence="0.6682155">
ScoreWebMining
)) (1)
</equation>
<page confidence="0.989987">
175
</page>
<bodyText confidence="0.999971555555556">
source for lexical substitute extraction. In the fu-
ture work, we will try some other lexical resources,
such as the Oxford American Writer Thesaurus
and Encarta. In addition, we will also try the
method that automatically constructs lexical re-
sources, such as the automatic clustering method.
Further analysis shows that, 1,388 (64.02%) out
of the 2,168 extracted correct candidates are
ranked in the first 10 in the oot output of our sys-
tem. This suggests that there is a big space for our
system to improve the candidate scoring method.
In the future work, we will consider more and
richer features, such as the syntactic features, in
candidate substitute scoring. Furthermore, A dis-
advantage of this method is that the web mining
process is quite inefficient. Therefore, we will try
to use the Web 1T 5-gram Version 1 from Google
(LDC2006T13) in the future.
</bodyText>
<table confidence="0.9992443">
P R ModeP ModeR
OVERALL 11.35 11.35 18.86 18.86
Further Analysis
NMWT 11.97 11.97 19.81 19.81
NMWS 12.55 12.38 19.93 19.65
RAND 11.81 11.81 20.03 20.03
MAN 10.81 10.81 17.53 17.53
Baselines
WORDNET 9.95 9.95 15.58 15.58
LIN 8.84 8.53 14.69 14.23
</table>
<tableCaption confidence="0.950378">
Table 3. best results.
</tableCaption>
<table confidence="0.9999206">
P R ModeP ModeR
OVERALL 33.88 33.88 46.91 46.91
Further Analysis
NMWT 35.60 35.60 48.48 48.48
NMWS 36.63 36.63 49.33 49.33
RAND 33.95 33.95 47.25 47.25
MAN 33.81 33.81 46.53 46.53
Baselines
WORDNET 29.70 29.35 40.57 40.57
LIN 27.70 26.72 40.47 39.19
</table>
<tableCaption confidence="0.998768">
Table 4. oot results.
</tableCaption>
<table confidence="0.99960775">
Our System WordNet BL
P R P R
detection 45.34 56.15 43.64 36.92
identification 41.61 51.54 40.00 33.85
</table>
<tableCaption confidence="0.99874">
Table 5. mw results.
</tableCaption>
<sectionHeader confidence="0.992926" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.928570666666667">
This research was supported by National Natural
Science Foundation of China (60575042,
60503072, 60675034).
</bodyText>
<sectionHeader confidence="0.981117" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999777743589744">
Barzilay Regina and McKeown Kathleen R. 2001. Ex-
tracting paraphrases from a Parallel Corpus. In Pro-
ceedings of ACL/EACL.
Bolshakov Igor A. and Gelbukh Alexander. 2004. Syn-
onymous Paraphrasing Using WordNet and Internet.
In Proceedings of NLDB.
Dagan Ido, Glickman Oren, Gliozzo Alfio, Marmor-
shtein Efrat, Strapparava Carlo. 2006. Direct Word
Sense Matching for Lexical Substitution. In Proceed-
ings of ACL.
Edmonds Philip. 1997. Choosing the Word Most Typi-
cal in Context Using a Lexical Co-occurrence Net-
work. In Proceedings of ACL.
Fellbaum Christiane. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, MA.
Glickman Oren, Dagan Ido, Keller Mikaela, Bengio
Samy. 2006. Investigating Lexical Substitution Scor-
ing for Subtitle Generation. In Proceedings of
CoNLL.
Kauchak David and Barzilay Regina. 2006. Paraphras-
ing for Automatic Evaluation. In Proceedings of
HLT-NAACL.
Langkilde I. and Knight K. 1998. Generation that Ex-
ploits Corpus-based Statistical Knowledge. In Pro-
ceedings of the COLING-ACL.
Lin Dekang. 1998. Automatic Retrieval and Clustering
of Similar Words. In Proceedings of COLING-ACL.
Smeaton Alan F., Kelledy Fergus, and O’Donell Ruari.
1994. TREC-4 Experiments at Dublin City Univer-
sity: Thresholding Posting Lists, Query Expansion
with WordNet and POS Tagging of Spanish. In Pro-
ceedings of TREC-4.
Wu Hua and Zhou Ming. 2003. Optimizing Synonym
Extraction Using Monolingual and Bilingual Re-
sources. In Proceedings of IWP.
Zhao Shiqi, Liu Ting, Yuan Xincheng, Li Sheng, and
Zhang Yu. 2007. Automatic Acquisition of Context-
Specific Lexical Paraphrases. In Proceedings of
IJCAI-07.
</reference>
<page confidence="0.998741">
176
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.423230">
<title confidence="0.999821">HIT: Web based Scoring Method for English Lexical Substitution</title>
<author confidence="0.998829">Shiqi Zhao</author>
<author confidence="0.998829">Lin Zhao</author>
<author confidence="0.998829">Yu Zhang</author>
<author confidence="0.998829">Ting Liu</author>
<author confidence="0.998829">Sheng Li</author>
<affiliation confidence="0.999319">Information Retrieval Laboratory, School of Computer Science and Technology,</affiliation>
<address confidence="0.985727">Box 321, Harbin Institute of Technology Harbin, P.R. China, 150001</address>
<email confidence="0.620185">zhaosq@ir.hit.edu.cn</email>
<email confidence="0.620185">lzhao@ir.hit.edu.cn</email>
<email confidence="0.620185">zhangyu@ir.hit.edu.cn</email>
<email confidence="0.620185">tliu@ir.hit.edu.cn</email>
<email confidence="0.620185">lisheng@ir.hit.edu.cn</email>
<abstract confidence="0.979809733333333">This paper describes the HIT system and its participation in SemEval-2007 English Lexical Substitution Task. Two main steps are included in our method: candidate substitute extraction and candidate scoring. In the first step, candidate substitutes for each target word in a given sentence are extracted from WordNet. In the second step, the extracted candidates are scored and ranked using a web-based scoring method. The substitute ranked first is selected as the best substitute. For the multiword subtask, a simple WordNet-based approach is employed.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Barzilay Regina</author>
<author>McKeown Kathleen R</author>
</authors>
<title>Extracting paraphrases from a Parallel Corpus.</title>
<date>2001</date>
<booktitle>In Proceedings of ACL/EACL.</booktitle>
<marker>Regina, R, 2001</marker>
<rawString>Barzilay Regina and McKeown Kathleen R. 2001. Extracting paraphrases from a Parallel Corpus. In Proceedings of ACL/EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bolshakov Igor A</author>
<author>Gelbukh Alexander</author>
</authors>
<title>Synonymous Paraphrasing Using WordNet and Internet.</title>
<date>2004</date>
<booktitle>In Proceedings of NLDB.</booktitle>
<marker>A, Alexander, 2004</marker>
<rawString>Bolshakov Igor A. and Gelbukh Alexander. 2004. Synonymous Paraphrasing Using WordNet and Internet. In Proceedings of NLDB.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dagan Ido</author>
<author>Glickman Oren</author>
</authors>
<title>Gliozzo Alfio, Marmorshtein Efrat, Strapparava Carlo.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Ido, Oren, 2006</marker>
<rawString>Dagan Ido, Glickman Oren, Gliozzo Alfio, Marmorshtein Efrat, Strapparava Carlo. 2006. Direct Word Sense Matching for Lexical Substitution. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edmonds Philip</author>
</authors>
<title>Choosing the Word Most Typical in Context Using a Lexical Co-occurrence Network.</title>
<date>1997</date>
<booktitle>In Proceedings of ACL.</booktitle>
<marker>Philip, 1997</marker>
<rawString>Edmonds Philip. 1997. Choosing the Word Most Typical in Context Using a Lexical Co-occurrence Network. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fellbaum Christiane</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>Christiane, 1998</marker>
<rawString>Fellbaum Christiane. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Glickman Oren</author>
<author>Dagan Ido</author>
<author>Keller Mikaela</author>
<author>Bengio Samy</author>
</authors>
<title>Investigating Lexical Substitution Scoring for Subtitle Generation.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<marker>Oren, Ido, Mikaela, Samy, 2006</marker>
<rawString>Glickman Oren, Dagan Ido, Keller Mikaela, Bengio Samy. 2006. Investigating Lexical Substitution Scoring for Subtitle Generation. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kauchak David</author>
<author>Barzilay Regina</author>
</authors>
<title>Paraphrasing for Automatic Evaluation.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<marker>David, Regina, 2006</marker>
<rawString>Kauchak David and Barzilay Regina. 2006. Paraphrasing for Automatic Evaluation. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Langkilde</author>
<author>K Knight</author>
</authors>
<title>Generation that Exploits Corpus-based Statistical Knowledge.</title>
<date>1998</date>
<booktitle>In Proceedings of the COLING-ACL.</booktitle>
<contexts>
<context position="3346" citStr="Langkilde and Knight, 1998" startWordPosition="516" endWordPosition="519">the given sentence. Then we construct queries by replacing the target word in the fragments with the candidate substitute. Finally, we search Google using the constructed queries and score each candidate based on the counts of retrieved snippets. The rest of this paper is organized as follows: Section 2 reviews some related work on lexical substitution. Section 3 describes our system, especially the web-based scoring method. Section 4 presents the results and analysis. 2 Related Work Synonyms defined in WordNet have been widely used in lexical substitution and expansion (Smeaton et al., 1994; Langkilde and Knight, 1998; Bol173 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 173–176, Prague, June 2007. c�2007 Association for Computational Linguistics shakov and Gelbukh, 2004). In addition, a lot of methods have been proposed to automatically construct thesauri of synonyms. For example, Lin (1998) clustered words with similar meanings by calculating the dependency similarity. Barzilay and McKeown (2001) extracted paraphrases using multiple translations of literature works. Wu and Zhou (2003) extracted synonyms with multiple resources, including a monolingual diction</context>
</contexts>
<marker>Langkilde, Knight, 1998</marker>
<rawString>Langkilde I. and Knight K. 1998. Generation that Exploits Corpus-based Statistical Knowledge. In Proceedings of the COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lin Dekang</author>
</authors>
<title>Automatic Retrieval and Clustering of Similar Words.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL.</booktitle>
<marker>Dekang, 1998</marker>
<rawString>Lin Dekang. 1998. Automatic Retrieval and Clustering of Similar Words. In Proceedings of COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Smeaton Alan F</author>
<author>Kelledy Fergus</author>
<author>O’Donell Ruari</author>
</authors>
<title>TREC-4 Experiments at Dublin City University: Thresholding Posting Lists, Query Expansion with WordNet and POS Tagging of Spanish.</title>
<date>1994</date>
<booktitle>In Proceedings of TREC-4.</booktitle>
<marker>F, Fergus, Ruari, 1994</marker>
<rawString>Smeaton Alan F., Kelledy Fergus, and O’Donell Ruari. 1994. TREC-4 Experiments at Dublin City University: Thresholding Posting Lists, Query Expansion with WordNet and POS Tagging of Spanish. In Proceedings of TREC-4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wu Hua</author>
<author>Zhou Ming</author>
</authors>
<title>Optimizing Synonym Extraction Using Monolingual and Bilingual Resources.</title>
<date>2003</date>
<booktitle>In Proceedings of IWP.</booktitle>
<marker>Hua, Ming, 2003</marker>
<rawString>Wu Hua and Zhou Ming. 2003. Optimizing Synonym Extraction Using Monolingual and Bilingual Resources. In Proceedings of IWP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhao Shiqi</author>
<author>Liu Ting</author>
<author>Yuan Xincheng</author>
<author>Li Sheng</author>
<author>Zhang Yu</author>
</authors>
<title>Automatic Acquisition of ContextSpecific Lexical Paraphrases.</title>
<date>2007</date>
<booktitle>In Proceedings of IJCAI-07.</booktitle>
<marker>Shiqi, Ting, Xincheng, Sheng, Yu, 2007</marker>
<rawString>Zhao Shiqi, Liu Ting, Yuan Xincheng, Li Sheng, and Zhang Yu. 2007. Automatic Acquisition of ContextSpecific Lexical Paraphrases. In Proceedings of IJCAI-07.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>