<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.016429">
<title confidence="0.9952215">
UVAVU: WordNet Similarity and Lexical Patterns
for Semantic Relation Classification
</title>
<author confidence="0.415635666666667">
Willem Robert van Hage
TNO Science &amp; Industry
Stieltjesweg 1, 2628CK Delft
</author>
<affiliation confidence="0.344204">
the Netherlands
</affiliation>
<email confidence="0.988645">
wrvhage@few.vu.nl
</email>
<author confidence="0.839917">
Sophia Katrenko
</author>
<affiliation confidence="0.869417">
HCSL, University of Amsterdam
</affiliation>
<address confidence="0.7607245">
Kruislaan 419, 1098VA Amsterdam
the Netherlands
</address>
<email confidence="0.997628">
katrenko@science.uva.nl
</email>
<sectionHeader confidence="0.99859" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999405">
The system we propose to learning seman-
tic relations consists of two parallel com-
ponents. For our final submission we used
components based on the similarity mea-
sures defined over WordNet and the patterns
extracted from the Web and WMTS. Other
components using syntactic structures were
explored but not used for the final run.
</bodyText>
<sectionHeader confidence="0.998704" genericHeader="keywords">
1 Experimental Set-up
</sectionHeader>
<bodyText confidence="0.9984806">
The system we used to classify the semantic rela-
tions consists of two parallel binary classifiers. We
ran this system for each of the seven semantic re-
lations separately. Each classifier predicts for each
instance of the relation whether it holds or not. The
predictions of all the classifiers are aggregated for
each instance by disjunction. That is to say, each in-
stance is predicted to be false by default unless any
of the classifiers gives evidence against this.
To generate the submitted predictions we used
two parallel classifiers: (1) a classifier that com-
bines eleven WordNet-based similarity measures,
see Sec. 2.1, and (2) a classifier that learns lexical
patterns from Google and the Waterloo Multi-Text
System (WMTS)(Turney, 2004) snippets and ap-
plies these on the same corpora, see Sec. 2.2.
Three other classifiers we experimented with, but
that were not used to generate the submitted predic-
tions: (3) a classifier that uses string kernel methods
on the dependency paths of the training sentences,
see Sec. 3.1, (4) a classifier that uses string kernels
on the local context of the subject and object nom-
inals in the training sentences, see Sec. 3.2 and (5)
a classifier that uses hand-made lexical patterns on
Google and WMTS, see Sec. 3.3.
</bodyText>
<sectionHeader confidence="0.997141" genericHeader="introduction">
2 Submitted Run
</sectionHeader>
<subsectionHeader confidence="0.999841">
2.1 WordNet-based Similarity Measures
</subsectionHeader>
<bodyText confidence="0.999971571428572">
WordNet 3.0 (Fellbaum, 1998) is the most fre-
quently used lexical database of English. As this re-
source consists of lexical and semantic relations, its
use constitutes an appealing option to learning rela-
tions. In particular, we believe that given two men-
tions of the same semantic relation, their arguments
should also be similar. Or, in analogy learning terms,
if R1(X1,Y1) and R2(X2,Y2) are relation mentions of
the same type, then X1 :: Y1 as X2 :: Y2. Our prelim-
inary experiments with WordNet suggested that few
arguments of each relation are connected by imme-
diate hyperonymy or meronymy relations. As a re-
sult, we decided to use similarity measures defined
over WordNet (Pedersen et al., 2004). The Word-
Net::Similarity package (Pedersen et al., 2004) in-
cludes 11 different measures, which mostly use ei-
ther the WordNet glosses (lesk or vector measures)
or the paths between a pair of concepts (lch; wup) to
determine their relatedness.
To be able to use WordNet::Similarity, we
mapped all WordNet sense keys from the training
and test sets to the earlier WordNet version (2.1).
Given a relation R(X,Y), we computed the related-
ness scores for each pair of arguments X and Y. The
scores together with the sense keys of arguments
were further used as features for the machine learn-
ing method. As there is no a priori knowledge on
what measures are the most important for each rela-
</bodyText>
<page confidence="0.987466">
472
</page>
<bodyText confidence="0.9693783">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 472–475,
Prague, June 2007. c�2007 Association for Computational Linguistics
tion, all of them were used and no feature selection
step has been taken.
We experimented with a number of machine
learning methods such as k-nearest neighbour al-
gorithm, logistic regression, bayesian networks and
others. For each relation a method performing best
on the training set was selected (using 5-fold cross-
validation).
</bodyText>
<subsectionHeader confidence="0.999873">
2.2 Learnt Lexical Patterns
</subsectionHeader>
<bodyText confidence="0.999940619047619">
This classifier models the intuition that when a pair
of nominals is used in similar phrases as another pair
they share at least one relation, and when no such
phrases can be found they do not share any relation.
Applied to the semantic relation classification prob-
lem this means that when a pair in the test set can be
found in the same patterns as pairs from the training
set, the classification for the pair will be true.
To find the patterns we followed step 1 to 6 de-
scribed in (Turney, 2006), with the exception that
we used both Google and the WMTS to compute
pattern frequency.
First we extracted the pairs of nominals (X,Y)
from the training sentences and created one Google
query and a set of WMTS queries for each pair.
The Google queries were of the form &amp;quot;X * Y&amp;quot;
OR &amp;quot;Y * X&amp;quot;. Currently, Google performs mor-
phological normalization on every query, so we
did not make separate queries for various endings
of the nominals. For the WMTS we did make
separate queries for various morphological varia-
tions. We used the following set of suffixes: ‘-
tion(s|al)’, ‘-ly’, ‘-ist’, ‘-ical’, ‘-y’, ‘-ing’, ‘-ed’,
‘-ies’, and ‘-s’. For this we used Peter Turney’s
pairs Perl package. The WMTS queries looked
like [n]&gt;([5]..&amp;quot;X&amp;quot;..[i]..&amp;quot;Y&amp;quot;..[5]) and
[n]&gt;([5]..&amp;quot;Y&amp;quot;..[i]..&amp;quot;X&amp;quot;..[5]) for i =
1,2,3 and n = i+12, and for each variation of X and
Y. Then we extracted sentences from the Google
snippets and cut out a context of size 5, so that
we were left with similar text segments as those
returned by the WMTS queries. We merged the
lists of text segments and counted all n-grams that
contained both nominals for n = 1 to 6. We sub-
stituted the nominals by variables in the n-grams
with a count greater than 10 and used these as pat-
terns for the classifier. An example of such a pat-
tern for the Cause-Effect relation is &amp;quot;generation
of Y by X&amp;quot;. After this we followed step 3 to
6 of (Turney, 2006), which left us with a matrix
for each of the seven semantic relations, where each
row represented a pair of nominals and each column
represented the frequency of a pattern, and where
each pair was classified as either true or false. The
straightforward way to find pattern frequencies for
the pairs in the test set would be to fill in these pat-
terns with the pairs of nominals from the test set.
This was not feasible given the time limitation on
the task. So instead, for each pair of nominals in
the test set we gathered the top-1000 snippets and
computed pattern frequencies by counting how of-
ten the nominals occur in every pattern on this set
text segments. We constructed a matrix from these
frequencies in the same way as for the training set,
but without classifications for the pairs. We experi-
mented with various machine learning algorithms to
predict the classes of the pairs. We chose to use k-
nearest neighbors, because it was the only algorithm
that gave more subtle predictions than true for every
pair or false for every pair. For each semantic rela-
tion we used the value of k that produced the highest
Fl score on 5-fold cross validation on the training
data.
</bodyText>
<sectionHeader confidence="0.999303" genericHeader="method">
3 Additional Runs
</sectionHeader>
<subsectionHeader confidence="0.999962">
3.1 String Kernels on Dependency Paths
</subsectionHeader>
<bodyText confidence="0.999987473684211">
It has been a long tradition to use syntactic structures
for relation extraction task. Some of the methods
as in (Katrenko and Adriaans, 2004) have used in-
formation extracted from the dependency trees. We
followed similar approach by considering the paths
between each pair of arguments X and Y. Ideally, if
each syntactic structure is a tree, there is only one
path from one node to the other. After we have ex-
tracted paths, we used them as input for the string
kernel methods (Hal Daumé III, 2004). The advan-
tage of using string kernels is that they can handle
sequences of different lengths and already proved to
be efficient for a number of tasks.
All sentences in the training data were parsed
using MIVIPAR (Lin, 1998). From each depen-
dency tree we extracted a dependency path (if any)
between the arguments by collecting all lemmas
(nodes) and syntactic functions (edges). The se-
quences we obtained were fed into string kernel.
</bodyText>
<page confidence="0.997109">
473
</page>
<bodyText confidence="0.999746714285714">
To assess the results, we carried out 5-fold cross-
validation. Even by optimizing the parameters of
the kernel (such as the length of subsequences) for
each relation, the highest accuracy we obtained was
equal 61,54% (on Origin-Entity relation) and the
lowest was accuracy for the Instrument-Agency re-
lation (50,48%).
</bodyText>
<subsectionHeader confidence="0.999879">
3.2 String Kernels on Local Context
</subsectionHeader>
<bodyText confidence="0.999974583333333">
Alternatively to syntactic information, we also ex-
tracted the snippets of the fixed length from each
sentence. For each relation mention of R(X,Y), all
tokens between the relation arguments X and Y were
collected along with at most three tokens to the left
and to the right. Unfortunately, the results we ob-
tained on the training set were comparable to those
obtained by string kernels on dependency paths and
less accurate than the results provided by WordNet
similarity measures or patterns extracted from the
Web and WMTS. As a consequence, string kernel
methods were not used for the final submission.
</bodyText>
<subsectionHeader confidence="0.999375">
3.3 Manually-created Lexical Patterns
</subsectionHeader>
<bodyText confidence="0.985526888888889">
The results of the method described in Sec. 2.2 are
quite far below what we expected given earlier re-
sults in the literature (Turney, 2006; van Hage, Ka-
trenko, and Schreiber, 2005; van Hage, Kolb, and
Schreiber, 2006; Berland and Charniak, 2006; Et-
zioni et al., 2004). We think this is caused by
the fact that many pairs in the training set are non-
stereotypical examples. So often the most com-
monly described relation of such a pair is not the re-
lation we try to classify with the pair. For example,
common associations with the pair (body,parents)
are that it is the parents’ body, or that the parents
are member of some organizing body, while it is a
positive example for the Product-Producer relation.
We wanted to see if this could be the case by testing
whether more intuitive patterns give better results on
the test set. The patterns we manually created for
each relation are shown in Table 1. If a pair gives
any results for these patterns on Google or WMTS,
we classify the pair as true, otherwise we classify
it as false. The results are shown in Table 2. We
did not use these results for the submitted run, be-
cause only automatic runs were permitted. The man-
ual patterns did not yield many useful results at all.
Apparently intuitive patterns do not capture what is
required to classify the relations in the test set. The
patterns we used for the Part-Whole (6) relation had
an average Precision of .50, which is much lower
than the average Precision found in (van Hage, Kolb,
and Schreiber, 2006), which was around 0.88. We
conclude that both the sets of training and test ex-
amples capture different semantics of the relations
than the intuitive ones, which causes common sense
background knowledge, such as Google to produce
bad results.
rel. patterns
</bodyText>
<listItem confidence="0.881113125">
1. X causes Y, X caused by Y, X * cause Y
2. X used Y, X uses Y, X * with a Y
3. X made by Y, X produced by Y, Y makes X,
Y produces X
4. Y comes from X, X * source of Y, Y * from * X
5. Y * to * X, Y * for * X, used Y for * X
6. X in Y, Y contains X, X from Y
7. Y contains X, X in Y, X containing Y, X into Y
</listItem>
<tableCaption confidence="0.999809">
Table 1: Hand-written patterns.
</tableCaption>
<table confidence="0.999875">
relation N Prec. Recall Fl Acc.
1. Cause-Effect 6 1 0.15 0.25 0.56
2.Instr.-Agency 2 1 0.05 0.10 0.54
3. Prod.-Prod. 4 0.75 0.05 0.09 0.35
4. Origin-Ent. 6 0.33 0.05 0.09 0.35
5. Theme-Tool 2 0 0 0 0.56
6. Part-Whole 16 0.50 0.31 0.38 0.64
7. Cont.-Cont. 11 0.54 0.16 0.24 0.50
</table>
<tableCaption confidence="0.98146">
Table 2: Results for hand-written lexical patterns on
Google and WMTS.
</tableCaption>
<sectionHeader confidence="0.999782" genericHeader="evaluation">
4 Results
</sectionHeader>
<subsectionHeader confidence="0.999977">
4.1 WordNet-based Similarity Measures
</subsectionHeader>
<bodyText confidence="0.9995502">
Table 3 shows the results of the WordNet-based sim-
ilarity measure method. In the ‘methods’ column,
the abbreviation LR stands for logistic regression,
K-NN stands for k-nearest neighbour, and DT stands
for decision trees.
</bodyText>
<listItem confidence="0.72886">
relation method Prec. Recall Fl Acc.
1. Cause-Effect LR 0.48 0.51 0.49 0.45
2.Instr.-Agency DT 0.65 0.63 0.64 0.62
3. Prod.-Prod. DT 0.67 0.50 0.57 0.46
4. Origin-Ent. LR 0.50 0.47 0.49 0.49
5. Theme-Tool LR 0.54 0.52 0.53 0.62
6. Part-Whole DT 0.54 0.73 0.62 0.67
7. Cont.-Cont. 2-NN 0.66 0.55 0.60 0.62
</listItem>
<tableCaption confidence="0.994963">
Table 3: Results for similarity-measure methods.
</tableCaption>
<page confidence="0.99765">
474
</page>
<subsectionHeader confidence="0.974712">
4.2 Learnt Lexical Patterns
</subsectionHeader>
<bodyText confidence="0.781685666666667">
Table 4 shows the results of the learnt lexical pat-
terns method. For all relations we used the k-nearest
neighbour method.
</bodyText>
<table confidence="0.999174375">
relation method Prec. Recall Fl Acc.
1. Cause-Effect 3-NN 0.53 0.76 0.63 0.54
2. Instr.-Agency 2-NN 0.47 0.89 0.62 0.46
3. Prod.-Prod. 2-NN 0 0 0 0.33
4. Origin-Ent. 2-NN 0.47 0.22 0.30 0.54
5. Theme-Tool 3-NN 0.39 0.93 0.55 0.38
6. Part-Whole 2-NN 0.36 1 0.53 0.36
7. Cont.-Cont. 2-NN 0.51 0.97 0.67 0.51
</table>
<tableCaption confidence="0.9416285">
Table 4: Results for learnt lexical patterns on Google
and WMTS.
</tableCaption>
<bodyText confidence="0.991756538461538">
range restrictions: Product-Producer #142 “And, of
course, everyone wants to prove the truth of their be-
liefs through experience, but the &lt;e1&gt;belief&lt;/e1&gt;
begets the &lt;e2&gt;experience&lt;/e2&gt;.” The common-
sense domain and range restriction of the Product-
Producer relation are respectively something like
‘Entity’ and ‘Agent’. However, ‘belief’ is generally
not considered to be an entity, and ‘experience’ not
an agent. The definition of Product-Producer rela-
tion used for the Challenge is more flexible and al-
lows therefore many examples which are difficult to
find by such common-sense resources as Google or
WordNet.
</bodyText>
<sectionHeader confidence="0.999772" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.99996825">
Our methods had the most difficulty with classify-
ing relation 1, 3 and 4. We wanted to see if hu-
man assessors perform less consistent for those re-
lations. If so, then those relations would simply be
harder to classify. Otherwise, our system performed
worse for those relations. We manually assessed ten
sample sentences from the test set, five of which
were positive examples and five were false exam-
ples. The result of a comparison with the test set is
shown in Table 5. The numbers listed there repre-
sent the fraction of examples on which we agreed
with the judges of the test set. There was quite a
</bodyText>
<table confidence="0.992438333333333">
inter-judge agreement
relation judge 1 judge 2
1. Cause-Effect 0.93 0.93
2. Instrument-Agency 0.77 0.77
3. Product-Producer 0.87 0.80
4. Origin-Entity 0.80 0.77
5. Theme-Tool 0.80 0.77
6. Part-Whole 0.97 1.00
7. Content-Container 0.77 0.77
</table>
<tableCaption confidence="0.999774">
Table 5: Inter-judge agreement.
</tableCaption>
<bodyText confidence="0.999947">
large variation in the inter-judge agreement, but for
relation 1 and 3 the consensus was high. We con-
clude that the reason for our low performance on
those relations are not caused by the difficulty of
the sentences, but due to other reasons. Our intu-
ition is that the sentences, especially those of rela-
tion 1 and 3, are easily decidable by humans, but
that they are non-stereotypical examples of the re-
lation, and thus hard to learn. The following ex-
ample sentence breaks common-sense domain and
</bodyText>
<sectionHeader confidence="0.999439" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998791352941177">
Matthew Berland and Eugene Charniak. 1999. Finding
Parts in Very Large Corpora. In Proceedings of ACL
1999.
Christiane Fellbaum (ed.). 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Hal Daumé III. 2004. SVMsequel Tutorial Manual.
Available at http://www.cs.utah.edu/
-hal/SVMsequel/svmsequel.pdf
Oren Etzioni et al. 2004. Methods for Domain-
Independent Information Extraction from the Web: An
Experimental Comparison. In Proceedings of AAAI
2004.
Willem Robert van Hage, Sophia Katrenko, and Guus
Schreiber. 2005. A Method to Combine Linguis-
tic Ontology-Mapping Techniques. In Proceedings of
ISWC 2005.
Willem Robert van Hage, Hap Kolb, and Guus Schreiber.
2006. A Method for Learning Part-Whole Relations.
In Proceedings of ISWC 2006.
Sophia Katrenko and Pieter Adriaans. 2007. Learn-
ing Relations from Biomedical Corpora Using Depen-
dency Trees. In KDECB, LNBI, vol. 4366.
Dekang Lin. 1998. Dependency-based Evaluation of
MINIPAR. In Workshop on the Evaluation of Parsing
Systems, Granada, Spain.
Ted Pedersen, Patwardhan, and Michelizzi. 2004. Word-
Net::Similarity - Measuring the Relatedness of Con-
cepts. In the Proceedings of AAAI-04, San Jose, CA.
Peter Turney. 2006. Expressing Implicit Semantic
Relations without Supervision. In Proceedings of
COLING-ACL 2006.
Peter Turney. 2004. The MultiText Project Home Page,
University of Waterloo, School of Computer Science,
http://www.multitext.uwaterloo.ca
</reference>
<page confidence="0.999048">
475
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000059">
<title confidence="0.9997225">UVAVU: WordNet Similarity and Lexical Patterns for Semantic Relation Classification</title>
<author confidence="0.999953">Willem Robert van_Hage</author>
<affiliation confidence="0.995737">TNO Science &amp; Industry</affiliation>
<address confidence="0.8535865">Stieltjesweg 1, 2628CK Delft the Netherlands</address>
<email confidence="0.974686">wrvhage@few.vu.nl</email>
<author confidence="0.991794">Sophia Katrenko</author>
<affiliation confidence="0.999764">HCSL, University of Amsterdam</affiliation>
<address confidence="0.963701">Kruislaan 419, 1098VA Amsterdam the Netherlands</address>
<email confidence="0.995718">katrenko@science.uva.nl</email>
<abstract confidence="0.990335841409692">The system we propose to learning semantic relations consists of two parallel components. For our final submission we used components based on the similarity measures defined over WordNet and the patterns extracted from the Web and WMTS. Other components using syntactic structures were explored but not used for the final run. 1 Experimental Set-up The system we used to classify the semantic relations consists of two parallel binary classifiers. We ran this system for each of the seven semantic relations separately. Each classifier predicts for each instance of the relation whether it holds or not. The predictions of all the classifiers are aggregated for each instance by disjunction. That is to say, each instance is predicted to be false by default unless any of the classifiers gives evidence against this. To generate the submitted predictions we used two parallel classifiers: (1) a classifier that combines eleven WordNet-based similarity measures, see Sec. 2.1, and (2) a classifier that learns lexical patterns from Google and the Waterloo Multi-Text System (WMTS)(Turney, 2004) snippets and applies these on the same corpora, see Sec. 2.2. Three other classifiers we experimented with, but that were not used to generate the submitted predictions: (3) a classifier that uses string kernel methods on the dependency paths of the training sentences, see Sec. 3.1, (4) a classifier that uses string kernels on the local context of the subject and object nominals in the training sentences, see Sec. 3.2 and (5) a classifier that uses hand-made lexical patterns on Google and WMTS, see Sec. 3.3. 2 Submitted Run 2.1 WordNet-based Similarity Measures WordNet 3.0 (Fellbaum, 1998) is the most frequently used lexical database of English. As this resource consists of lexical and semantic relations, its use constitutes an appealing option to learning relations. In particular, we believe that given two mentions of the same semantic relation, their arguments should also be similar. Or, in analogy learning terms, and are relation mentions of same type, then Our preliminary experiments with WordNet suggested that few arguments of each relation are connected by immediate hyperonymy or meronymy relations. As a result, we decided to use similarity measures defined over WordNet (Pedersen et al., 2004). The Word- Net::Similarity package (Pedersen et al., 2004) includes 11 different measures, which mostly use eithe WordNet glosses the paths between a pair of concepts to determine their relatedness. To be able to use WordNet::Similarity, we mapped all WordNet sense keys from the training and test sets to the earlier WordNet version (2.1). a relation we computed the relatedscores for each pair of arguments The scores together with the sense keys of arguments were further used as features for the machine learning method. As there is no a priori knowledge on measures are the most important for each rela- 472 of the 4th International Workshop on Semantic Evaluations pages 472–475, June 2007. Association for Computational Linguistics tion, all of them were used and no feature selection step has been taken. We experimented with a number of machine methods such as neighbour algorithm, logistic regression, bayesian networks and others. For each relation a method performing best on the training set was selected (using 5-fold crossvalidation). 2.2 Learnt Lexical Patterns This classifier models the intuition that when a pair of nominals is used in similar phrases as another pair they share at least one relation, and when no such phrases can be found they do not share any relation. Applied to the semantic relation classification problem this means that when a pair in the test set can be found in the same patterns as pairs from the training set, the classification for the pair will be true. To find the patterns we followed step 1 to 6 described in (Turney, 2006), with the exception that we used both Google and the WMTS to compute pattern frequency. we extracted the pairs of nominals from the training sentences and created one Google query and a set of WMTS queries for each pair. Google queries were of the form Currently, Google performs morphological normalization on every query, so we did not make separate queries for various endings of the nominals. For the WMTS we did make separate queries for various morphological variations. We used the following set of suffixes: ‘tion(s|al)’, ‘-ly’, ‘-ist’, ‘-ical’, ‘-y’, ‘-ing’, ‘-ed’, ‘-ies’, and ‘-s’. For this we used Peter Turney’s package. The WMTS queries looked and and for each variation of Then we extracted sentences from the Google snippets and cut out a context of size 5, so that we were left with similar text segments as those returned by the WMTS queries. We merged the of text segments and counted all that both nominals for to 6. We subthe nominals by variables in the with a count greater than 10 and used these as patterns for the classifier. An example of such a patfor the is After this we followed step 3 to 6 of (Turney, 2006), which left us with a matrix for each of the seven semantic relations, where each row represented a pair of nominals and each column represented the frequency of a pattern, and where each pair was classified as either true or false. The straightforward way to find pattern frequencies for the pairs in the test set would be to fill in these patterns with the pairs of nominals from the test set. This was not feasible given the time limitation on the task. So instead, for each pair of nominals in the test set we gathered the top-1000 snippets and computed pattern frequencies by counting how often the nominals occur in every pattern on this set text segments. We constructed a matrix from these frequencies in the same way as for the training set, but without classifications for the pairs. We experimented with various machine learning algorithms to the classes of the pairs. We chose to use nearest neighbors, because it was the only algorithm that gave more subtle predictions than true for every pair or false for every pair. For each semantic relawe used the value of produced the highest on 5-fold cross validation on the training data. 3 Additional Runs 3.1 String Kernels on Dependency Paths It has been a long tradition to use syntactic structures for relation extraction task. Some of the methods as in (Katrenko and Adriaans, 2004) have used information extracted from the dependency trees. We followed similar approach by considering the paths each pair of arguments Ideally, if each syntactic structure is a tree, there is only one path from one node to the other. After we have extracted paths, we used them as input for the string kernel methods (Hal Daumé III, 2004). The advantage of using string kernels is that they can handle sequences of different lengths and already proved to be efficient for a number of tasks. All sentences in the training data were parsed using MIVIPAR (Lin, 1998). From each dependency tree we extracted a dependency path (if any) between the arguments by collecting all lemmas (nodes) and syntactic functions (edges). The sequences we obtained were fed into string kernel. 473 To assess the results, we carried out 5-fold crossvalidation. Even by optimizing the parameters of the kernel (such as the length of subsequences) for each relation, the highest accuracy we obtained was equal 61,54% (on Origin-Entity relation) and the lowest was accuracy for the Instrument-Agency relation (50,48%). 3.2 String Kernels on Local Context Alternatively to syntactic information, we also extracted the snippets of the fixed length from each For each relation mention of all between the relation arguments collected along with at most three tokens to the left and to the right. Unfortunately, the results we obtained on the training set were comparable to those obtained by string kernels on dependency paths and less accurate than the results provided by WordNet similarity measures or patterns extracted from the Web and WMTS. As a consequence, string kernel methods were not used for the final submission. 3.3 Manually-created Lexical Patterns The results of the method described in Sec. 2.2 are quite far below what we expected given earlier results in the literature (Turney, 2006; van Hage, Katrenko, and Schreiber, 2005; van Hage, Kolb, and Schreiber, 2006; Berland and Charniak, 2006; Etzioni et al., 2004). We think this is caused by the fact that many pairs in the training set are nonstereotypical examples. So often the most commonly described relation of such a pair is not the relation we try to classify with the pair. For example, associations with the pair are that it is the parents’ body, or that the parents are member of some organizing body, while it is a example for the We wanted to see if this could be the case by testing whether more intuitive patterns give better results on the test set. The patterns we manually created for each relation are shown in Table 1. If a pair gives any results for these patterns on Google or WMTS, we classify the pair as true, otherwise we classify it as false. The results are shown in Table 2. We did not use these results for the submitted run, because only automatic runs were permitted. The manual patterns did not yield many useful results at all. Apparently intuitive patterns do not capture what is required to classify the relations in the test set. The we used for the relation had an average Precision of .50, which is much lower than the average Precision found in (van Hage, Kolb, and Schreiber, 2006), which was around 0.88. We conclude that both the sets of training and test examples capture different semantics of the relations than the intuitive ones, which causes common sense background knowledge, such as Google to produce bad results. rel. patterns 1. X causes Y, X caused by Y, X * cause Y 2. X used Y, X uses Y, X * with a Y 3. X made by Y, X produced by Y, Y makes X, Y produces X 4. Y comes from X, X * source of Y, Y * from * X 5. Y * to * X, Y * for * X, used Y for * X 6. X in Y, Y contains X, X from Y 7. Y contains X, X in Y, X containing Y, X into Y Table 1: Hand-written patterns. relation N Prec. Recall Acc.</abstract>
<note confidence="0.6368178">6 1 0.15 0.25 0.56 2 1 0.05 0.10 0.54 3. Prod.-Prod. 4 0.75 0.05 0.09 0.35 4. Origin-Ent. 6 0.33 0.05 0.09 0.35 5. Theme-Tool 2 0 0 0 0.56 6. Part-Whole 16 0.50 0.31 0.38 0.64 7. Cont.-Cont. 11 0.54 0.16 0.24 0.50 Table 2: Results for hand-written lexical patterns on Google and WMTS. 4 Results</note>
<abstract confidence="0.988342144736842">4.1 WordNet-based Similarity Measures Table 3 shows the results of the WordNet-based similarity measure method. In the ‘methods’ column, the abbreviation LR stands for logistic regression, stands for neighbour, and DT stands for decision trees. relation method Prec. Recall Acc. LR 0.48 0.51 0.49 0.45 DT 0.65 0.63 0.64 0.62 3. Prod.-Prod. DT 0.67 0.50 0.57 0.46 4. Origin-Ent. LR 0.50 0.47 0.49 0.49 5. Theme-Tool LR 0.54 0.52 0.53 0.62 6. Part-Whole DT 0.54 0.73 0.62 0.67 7. Cont.-Cont. 2-NN 0.66 0.55 0.60 0.62 Table 3: Results for similarity-measure methods. 474 4.2 Learnt Lexical Patterns Table 4 shows the results of the learnt lexical patmethod. For all relations we used the neighbour method. relation method Prec. Recall Acc. 1. Cause-Effect 3-NN 0.53 0.76 0.63 0.54 2. Instr.-Agency 2-NN 0.47 0.89 0.62 0.46 3. Prod.-Prod. 2-NN 0 0 0 0.33 4. Origin-Ent. 2-NN 0.47 0.22 0.30 0.54 5. Theme-Tool 3-NN 0.39 0.93 0.55 0.38 6. Part-Whole 2-NN 0.36 1 0.53 0.36 7. Cont.-Cont. 2-NN 0.51 0.97 0.67 0.51 Table 4: Results for learnt lexical patterns on Google and WMTS. restrictions: of course, everyone wants to prove the truth of their beliefs through experience, but the &lt;e1&gt;belief&lt;/e1&gt; the &lt;e2&gt;experience&lt;/e2&gt;.” commondomain and range restriction of the Productare respectively something like ‘Entity’ and ‘Agent’. However, ‘belief’ is generally not considered to be an entity, and ‘experience’ not agent. The definition of relation used for the Challenge is more flexible and allows therefore many examples which are difficult to find by such common-sense resources as Google or WordNet. 5 Discussion Our methods had the most difficulty with classifying relation 1, 3 and 4. We wanted to see if human assessors perform less consistent for those relations. If so, then those relations would simply be harder to classify. Otherwise, our system performed worse for those relations. We manually assessed ten sample sentences from the test set, five of which were positive examples and five were false examples. The result of a comparison with the test set is shown in Table 5. The numbers listed there represent the fraction of examples on which we agreed with the judges of the test set. There was quite a inter-judge agreement relation judge 1 judge 2 1. Cause-Effect 0.93 0.93 2. Instrument-Agency 0.77 0.77 3. Product-Producer 0.87 0.80 4. Origin-Entity 0.80 0.77 5. Theme-Tool 0.80 0.77 6. Part-Whole 0.97 1.00 7. Content-Container 0.77 0.77 Table 5: Inter-judge agreement. large variation in the inter-judge agreement, but for relation 1 and 3 the consensus was high. We conclude that the reason for our low performance on those relations are not caused by the difficulty of the sentences, but due to other reasons. Our intuition is that the sentences, especially those of relation 1 and 3, are easily decidable by humans, but that they are non-stereotypical examples of the relation, and thus hard to learn. The following example sentence breaks common-sense domain and</abstract>
<title confidence="0.75352">References</title>
<author confidence="0.784377">Finding</author>
<note confidence="0.507730333333333">in Very Large Corpora. Proceedings of ACL Christiane Fellbaum (ed.). 1998. WordNet: An Elec- Lexical Database. Hal Daumé III. 2004. SVMsequel Tutorial Manual. at http://www.cs.utah.edu/ Oren Etzioni et al. 2004. Methods for Domain- Independent Information Extraction from the Web: An Comparison. Proceedings of AAAI Willem Robert van Hage, Sophia Katrenko, and Guus Schreiber. 2005. A Method to Combine Linguis- Ontology-Mapping Techniques. Proceedings of Willem Robert van Hage, Hap Kolb, and Guus Schreiber. 2006. A Method for Learning Part-Whole Relations. Proceedings of ISWC Sophia Katrenko and Pieter Adriaans. 2007. Learn-</note>
<title confidence="0.682254">ing Relations from Biomedical Corpora Using Depen-</title>
<note confidence="0.863060875">Trees. KDECB, LNBI, vol. Dekang Lin. 1998. Dependency-based Evaluation of Workshop on the Evaluation of Parsing Granada, Ted Pedersen, Patwardhan, and Michelizzi. 2004. Word- Net::Similarity - Measuring the Relatedness of Conthe Proceedings of AAAI-04, San Jose, Peter Turney. 2006. Expressing Implicit Semantic</note>
<title confidence="0.66545">without Supervision. Proceedings of</title>
<author confidence="0.985185">The MultiText Project Home Page</author>
<affiliation confidence="0.999407">University of Waterloo, School of Computer Science,</affiliation>
<web confidence="0.835575">http://www.multitext.uwaterloo.ca</web>
<intro confidence="0.344553">475</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Matthew Berland</author>
<author>Eugene Charniak</author>
</authors>
<title>Finding Parts in Very Large Corpora.</title>
<date>1999</date>
<booktitle>In Proceedings of ACL</booktitle>
<marker>Berland, Charniak, 1999</marker>
<rawString>Matthew Berland and Eugene Charniak. 1999. Finding Parts in Very Large Corpora. In Proceedings of ACL 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="2011" citStr="Fellbaum, 1998" startWordPosition="313" endWordPosition="314">MTS)(Turney, 2004) snippets and applies these on the same corpora, see Sec. 2.2. Three other classifiers we experimented with, but that were not used to generate the submitted predictions: (3) a classifier that uses string kernel methods on the dependency paths of the training sentences, see Sec. 3.1, (4) a classifier that uses string kernels on the local context of the subject and object nominals in the training sentences, see Sec. 3.2 and (5) a classifier that uses hand-made lexical patterns on Google and WMTS, see Sec. 3.3. 2 Submitted Run 2.1 WordNet-based Similarity Measures WordNet 3.0 (Fellbaum, 1998) is the most frequently used lexical database of English. As this resource consists of lexical and semantic relations, its use constitutes an appealing option to learning relations. In particular, we believe that given two mentions of the same semantic relation, their arguments should also be similar. Or, in analogy learning terms, if R1(X1,Y1) and R2(X2,Y2) are relation mentions of the same type, then X1 :: Y1 as X2 :: Y2. Our preliminary experiments with WordNet suggested that few arguments of each relation are connected by immediate hyperonymy or meronymy relations. As a result, we decided </context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum (ed.). 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daumé</author>
</authors>
<date>2004</date>
<note>SVMsequel Tutorial Manual. Available at http://www.cs.utah.edu/ -hal/SVMsequel/svmsequel.pdf</note>
<marker>Daumé, 2004</marker>
<rawString>Hal Daumé III. 2004. SVMsequel Tutorial Manual. Available at http://www.cs.utah.edu/ -hal/SVMsequel/svmsequel.pdf</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
</authors>
<title>Methods for DomainIndependent Information Extraction from the Web: An Experimental Comparison.</title>
<date>2004</date>
<booktitle>In Proceedings of AAAI</booktitle>
<marker>Etzioni, 2004</marker>
<rawString>Oren Etzioni et al. 2004. Methods for DomainIndependent Information Extraction from the Web: An Experimental Comparison. In Proceedings of AAAI 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Willem Robert van Hage</author>
<author>Sophia Katrenko</author>
<author>Guus Schreiber</author>
</authors>
<title>A Method to Combine Linguistic Ontology-Mapping Techniques.</title>
<date>2005</date>
<booktitle>In Proceedings of ISWC</booktitle>
<marker>van Hage, Katrenko, Schreiber, 2005</marker>
<rawString>Willem Robert van Hage, Sophia Katrenko, and Guus Schreiber. 2005. A Method to Combine Linguistic Ontology-Mapping Techniques. In Proceedings of ISWC 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Willem Robert van Hage</author>
<author>Hap Kolb</author>
<author>Guus Schreiber</author>
</authors>
<title>A Method for Learning Part-Whole Relations.</title>
<date>2006</date>
<booktitle>In Proceedings of ISWC</booktitle>
<marker>van Hage, Kolb, Schreiber, 2006</marker>
<rawString>Willem Robert van Hage, Hap Kolb, and Guus Schreiber. 2006. A Method for Learning Part-Whole Relations. In Proceedings of ISWC 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sophia Katrenko</author>
<author>Pieter Adriaans</author>
</authors>
<title>Learning Relations from Biomedical Corpora Using Dependency Trees.</title>
<date>2007</date>
<booktitle>In KDECB, LNBI,</booktitle>
<volume>vol.</volume>
<pages>4366</pages>
<marker>Katrenko, Adriaans, 2007</marker>
<rawString>Sophia Katrenko and Pieter Adriaans. 2007. Learning Relations from Biomedical Corpora Using Dependency Trees. In KDECB, LNBI, vol. 4366.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Dependency-based Evaluation of MINIPAR.</title>
<date>1998</date>
<booktitle>In Workshop on the Evaluation of Parsing Systems,</booktitle>
<location>Granada,</location>
<contexts>
<context position="7741" citStr="Lin, 1998" startWordPosition="1306" endWordPosition="1307">Katrenko and Adriaans, 2004) have used information extracted from the dependency trees. We followed similar approach by considering the paths between each pair of arguments X and Y. Ideally, if each syntactic structure is a tree, there is only one path from one node to the other. After we have extracted paths, we used them as input for the string kernel methods (Hal Daumé III, 2004). The advantage of using string kernels is that they can handle sequences of different lengths and already proved to be efficient for a number of tasks. All sentences in the training data were parsed using MIVIPAR (Lin, 1998). From each dependency tree we extracted a dependency path (if any) between the arguments by collecting all lemmas (nodes) and syntactic functions (edges). The sequences we obtained were fed into string kernel. 473 To assess the results, we carried out 5-fold crossvalidation. Even by optimizing the parameters of the kernel (such as the length of subsequences) for each relation, the highest accuracy we obtained was equal 61,54% (on Origin-Entity relation) and the lowest was accuracy for the Instrument-Agency relation (50,48%). 3.2 String Kernels on Local Context Alternatively to syntactic infor</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Dependency-based Evaluation of MINIPAR. In Workshop on the Evaluation of Parsing Systems, Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Patwardhan</author>
<author>Michelizzi</author>
</authors>
<title>WordNet::Similarity - Measuring the Relatedness of Concepts.</title>
<date>2004</date>
<booktitle>In the Proceedings of AAAI-04,</booktitle>
<location>San Jose, CA.</location>
<contexts>
<context position="2682" citStr="Pedersen et al., 2004" startWordPosition="424" endWordPosition="427">English. As this resource consists of lexical and semantic relations, its use constitutes an appealing option to learning relations. In particular, we believe that given two mentions of the same semantic relation, their arguments should also be similar. Or, in analogy learning terms, if R1(X1,Y1) and R2(X2,Y2) are relation mentions of the same type, then X1 :: Y1 as X2 :: Y2. Our preliminary experiments with WordNet suggested that few arguments of each relation are connected by immediate hyperonymy or meronymy relations. As a result, we decided to use similarity measures defined over WordNet (Pedersen et al., 2004). The WordNet::Similarity package (Pedersen et al., 2004) includes 11 different measures, which mostly use either the WordNet glosses (lesk or vector measures) or the paths between a pair of concepts (lch; wup) to determine their relatedness. To be able to use WordNet::Similarity, we mapped all WordNet sense keys from the training and test sets to the earlier WordNet version (2.1). Given a relation R(X,Y), we computed the relatedness scores for each pair of arguments X and Y. The scores together with the sense keys of arguments were further used as features for the machine learning method. As </context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Ted Pedersen, Patwardhan, and Michelizzi. 2004. WordNet::Similarity - Measuring the Relatedness of Concepts. In the Proceedings of AAAI-04, San Jose, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Expressing Implicit Semantic Relations without Supervision.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL</booktitle>
<contexts>
<context position="4394" citStr="Turney, 2006" startWordPosition="714" endWordPosition="715">method performing best on the training set was selected (using 5-fold crossvalidation). 2.2 Learnt Lexical Patterns This classifier models the intuition that when a pair of nominals is used in similar phrases as another pair they share at least one relation, and when no such phrases can be found they do not share any relation. Applied to the semantic relation classification problem this means that when a pair in the test set can be found in the same patterns as pairs from the training set, the classification for the pair will be true. To find the patterns we followed step 1 to 6 described in (Turney, 2006), with the exception that we used both Google and the WMTS to compute pattern frequency. First we extracted the pairs of nominals (X,Y) from the training sentences and created one Google query and a set of WMTS queries for each pair. The Google queries were of the form &amp;quot;X * Y&amp;quot; OR &amp;quot;Y * X&amp;quot;. Currently, Google performs morphological normalization on every query, so we did not make separate queries for various endings of the nominals. For the WMTS we did make separate queries for various morphological variations. We used the following set of suffixes: ‘- tion(s|al)’, ‘-ly’, ‘-ist’, ‘-ical’, ‘-y’, ‘</context>
<context position="5783" citStr="Turney, 2006" startWordPosition="961" endWordPosition="962"> for i = 1,2,3 and n = i+12, and for each variation of X and Y. Then we extracted sentences from the Google snippets and cut out a context of size 5, so that we were left with similar text segments as those returned by the WMTS queries. We merged the lists of text segments and counted all n-grams that contained both nominals for n = 1 to 6. We substituted the nominals by variables in the n-grams with a count greater than 10 and used these as patterns for the classifier. An example of such a pattern for the Cause-Effect relation is &amp;quot;generation of Y by X&amp;quot;. After this we followed step 3 to 6 of (Turney, 2006), which left us with a matrix for each of the seven semantic relations, where each row represented a pair of nominals and each column represented the frequency of a pattern, and where each pair was classified as either true or false. The straightforward way to find pattern frequencies for the pairs in the test set would be to fill in these patterns with the pairs of nominals from the test set. This was not feasible given the time limitation on the task. So instead, for each pair of nominals in the test set we gathered the top-1000 snippets and computed pattern frequencies by counting how often</context>
<context position="9091" citStr="Turney, 2006" startWordPosition="1525" endWordPosition="1526"> relation arguments X and Y were collected along with at most three tokens to the left and to the right. Unfortunately, the results we obtained on the training set were comparable to those obtained by string kernels on dependency paths and less accurate than the results provided by WordNet similarity measures or patterns extracted from the Web and WMTS. As a consequence, string kernel methods were not used for the final submission. 3.3 Manually-created Lexical Patterns The results of the method described in Sec. 2.2 are quite far below what we expected given earlier results in the literature (Turney, 2006; van Hage, Katrenko, and Schreiber, 2005; van Hage, Kolb, and Schreiber, 2006; Berland and Charniak, 2006; Etzioni et al., 2004). We think this is caused by the fact that many pairs in the training set are nonstereotypical examples. So often the most commonly described relation of such a pair is not the relation we try to classify with the pair. For example, common associations with the pair (body,parents) are that it is the parents’ body, or that the parents are member of some organizing body, while it is a positive example for the Product-Producer relation. We wanted to see if this could be</context>
</contexts>
<marker>Turney, 2006</marker>
<rawString>Peter Turney. 2006. Expressing Implicit Semantic Relations without Supervision. In Proceedings of COLING-ACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>The MultiText Project Home</title>
<date>2004</date>
<institution>Page, University of Waterloo, School of Computer Science,</institution>
<location>http://www.multitext.uwaterloo.ca</location>
<contexts>
<context position="1414" citStr="Turney, 2004" startWordPosition="213" endWordPosition="214">he seven semantic relations separately. Each classifier predicts for each instance of the relation whether it holds or not. The predictions of all the classifiers are aggregated for each instance by disjunction. That is to say, each instance is predicted to be false by default unless any of the classifiers gives evidence against this. To generate the submitted predictions we used two parallel classifiers: (1) a classifier that combines eleven WordNet-based similarity measures, see Sec. 2.1, and (2) a classifier that learns lexical patterns from Google and the Waterloo Multi-Text System (WMTS)(Turney, 2004) snippets and applies these on the same corpora, see Sec. 2.2. Three other classifiers we experimented with, but that were not used to generate the submitted predictions: (3) a classifier that uses string kernel methods on the dependency paths of the training sentences, see Sec. 3.1, (4) a classifier that uses string kernels on the local context of the subject and object nominals in the training sentences, see Sec. 3.2 and (5) a classifier that uses hand-made lexical patterns on Google and WMTS, see Sec. 3.3. 2 Submitted Run 2.1 WordNet-based Similarity Measures WordNet 3.0 (Fellbaum, 1998) is</context>
</contexts>
<marker>Turney, 2004</marker>
<rawString>Peter Turney. 2004. The MultiText Project Home Page, University of Waterloo, School of Computer Science, http://www.multitext.uwaterloo.ca</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>