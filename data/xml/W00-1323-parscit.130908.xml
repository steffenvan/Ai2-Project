<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.968617">
Combining Lexical and Formatting Cues for Named Entity
Acquisition from the Web
</title>
<note confidence="0.662212">
Christian Jacqueminl and Caroline Bushl&apos;2
1CNRS-LIMSI, BP 133, F-91403 ORSAY Cedex, FRANCE
2UMIST, Dept of Language Engineering, PO Box 88, Manchester M60 1QD, UK
</note>
<email confidence="0.984015">
{jacquemin,caroline}@1imsi.fr
</email>
<sectionHeader confidence="0.986639" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999207">
Because of their constant renewal, it is nec-
essary to acquire fresh named entities (NEs)
from recent text sources. We present a tool
for the acquisition and the typing of NEs from
the Web that associates a harvester and three
parallel shallow parsers dedicated to specific
structures (lists, enumerations, and anchors).
The parsers combine lexical indices such as
discourse markers with formatting instruc-
tions (HTML tags) for analyzing enumera-
tions and associated initializers.
</bodyText>
<sectionHeader confidence="0.96208" genericHeader="keywords">
1 Overview
</sectionHeader>
<bodyText confidence="0.999447729729729">
Lexical acquisition from large corpora has
long been considered as a means for enrich-
ing vocabularies (Boguraev and Pustejovsky,
1996). Depending on the studies, different is-
sues are considered: the acquisition of terms
(Daille, 1996), the acquisition of subcatego-
rization frames (Basili et al., 1993), the acqui-
sition of semantic links (Grefenstette, 1994),
etc. While traditional electronic corpora such
as journal articles or corpus resources (BNC,
SUSANNE, Brown corpus) are satisfactory for
classical lexical acquisition, Web corpora are
another source of knowledge (Crimmins et al.,
1999) that can be used to acquire NEs because
of the constant updating of online data.
The purpose of our work is to propose a
technique for the extraction of NEs from the
Web through the combination of a harvester
and shallow parsers. Our study also belongs
to corpus-based acquisition of semantic re-
lationships through the analysis of specific
lexico-syntactic contexts (Hearst, 1998) be-
cause hypernym relationships are acquired to-
gether with NEs. The unique contribution
of our technique is to offer an integrated ap-
proach to the analysis of HTML documents
that associates lexical cues with formatting
instructions in a single and cohesive frame-
work. The combination of structural informa-
tion and linguistic patterns is also found in
wrapper induction, an emerging topic of re-
search in artificial intelligence and machine
learning (Kushmerick et al., 1997).
Our work differs from the MUC-related NE
tagging task and its possible extension to
name indexing of web pages (Aone et al.,
1997) for the following reasons:
</bodyText>
<listItem confidence="0.86502665625">
• The purpose of our task is to build lists of
NEs, not to tag corpora. For this reason,
we only collect non-ambiguous context-
independent NEs; partial or incomplete
occurrences such as anaphora are consid-
ered as incorrect.
• The types of NEs collected here are much
more accurate than the four basic types
defined in MUC. The proposed tech-
nique could be extended to the collec-
tion of any non-MUC names which can
be grouped under a common hypernym:
botanic names, mechanical parts, book
titles, events...
• We emphasize the role of document struc-
ture in web-based collection.
2 Focusing on Definitory Contexts
Two issues are addressed in this paper:
1. While traditional electronic corpora can
be accessed directly and entirely through
large-scale filters such as shallow parsers,
access to Web pages is restricted to
the narrow and specialized medium of a
search engine. In order to spot and re-
trieve relevant text chunks, we must fo-
cus on linguistic cues that can be used to
access pages containing typed NEs with
high precision.
2. While Web pages are full of NEs, only a
small proportion of them are relevant for
the acquisition of public, fresh and well-
known NEs (the name of someone&apos;s cat
</listItem>
<page confidence="0.988576">
181
</page>
<figure confidence="0.990378941176471">
WWW
Harvester
Search Engine
111
HTML corpus
Queries
Pe P1
Pa Parsers
Enumeration
Parser
Candidate NEs
List and tables Anchor
Parser Parser
t
Initializers and Candidate
Candidate NEs Index pages
Typed NEs
</figure>
<figureCaption confidence="0.999964">
Figure 1: Architecture
</figureCaption>
<bodyText confidence="0.99998955">
is not relevant to our purpose). So that
automatically acquired NEs can be used
in a NE recognition task, they are asso-
ciated with types such as actor (PER-
SON), lake (LOCATION), or university
(ORGANIZATION).
The need for selective linguistic cues (wrt to
the current facilities offered by search engines)
and for informative and typifying contexts has
led us to focus on collections, a specific type of
definitory contexts (Pery-Woodley, 1998). Be-
cause they contain specific linguistic triggers
such as following or such as, definitory con-
texts can be accessed through phrase queries
to a search engine. In addition, these contexts
use the classical scheme genus/differentia to
define NEs, and thus provide, through the
genus, a hypernym of the NEs they define.
Our study extends (Hearst, 1998) to Web-
based and spatially formatted corpora.
</bodyText>
<sectionHeader confidence="0.788932" genericHeader="introduction">
3 Architecture and Principles
</sectionHeader>
<bodyText confidence="0.972146333333333">
To acquire NEs from the Web, we have devel-
oped a system that consists of three sequential
modules (see Figure 1):
</bodyText>
<listItem confidence="0.9969144">
1. A harvester that downloads the pages re-
trieved by a search engine from the four
following query strings
(1.a) following (NE) (1.c) (NE) such as
(1.b) list of (NE) (1.d) such (NE) as
</listItem>
<bodyText confidence="0.961688666666667">
in which (NE) stands for a typifying
hypernym of NEs such as Universities,
politicians, or car makers (see list in 4).
</bodyText>
<listItem confidence="0.958090875">
2. Three parallel shallow parsers Pe, P1 and
Pa which extract candidate NEs respec-
tively from enumerations, lists and ta-
bles, and anchors.
3. A post-filtering module that cleans up the
candidate NEs from leading determiners
or trailing unrelated words and splits co-
ordinated NEs into unitary items.
</listItem>
<subsectionHeader confidence="0.993874">
Corpus Harvesting
</subsectionHeader>
<bodyText confidence="0.9987915">
The four strings (1.a-d) given above are used
to query a search engine. They consist of an
hypernym and a discourse marker. They are
expected to be followed by a collection of NEs.
Figure 2 shows five prototypical examples
of collections encountered in HTML pages re-
trieved through one of the strings (1.a-d).1
The first collection is an enumeration and
consists of a coordination of three NEs. The
second collection is a list organized into two
sublists. Each sublist is introduced by a hy-
pernym. The third structure is a list marked
by bullets. Such lists can be constructed
through an HTML table (this example), or
by using enumeration marks (&lt;u/&gt; or &lt;o/&gt;).
The fourth example is also a list built by us-
ing a table structure but displays a more com-
plex spatial organization and does not em-
ploy graphical bullets. The fifth example is
an anchor to a collection not provided to the
reader within the document, but which can be
reached by following an hyperlink instead.
The corpus of HTML pages is collected
through two search engines with different ca-
pabilities: AltaVista (AV) and Northern Light
(NL).2 AV offers the facility of double-quoting
the query strings in order to search for exact
strings (also called phrases in IR). NL does
not support phrase search. However, in AV,
the number of retrievable documents is limited
to the 200 highest ranked documents while it
is potentially unlimited in NL. For NL, the
</bodyText>
<footnote confidence="0.955173625">
&apos;(NE) is mternational organtzations, here. The ty-
pographical mark-up of the query string in the figure
is ours. The hypemym is in bold italics and the dis-
course marker is in bold.
2The harvester that retrieves Web pages through a
search engine is a combination of wget available from
ftp: //sunsite. auc . dk/pub/infosysteinslvget/ and
Perl scripts.
</footnote>
<page confidence="0.992078">
182
</page>
<table confidence="0.651061125">
It&apos;s development is due to the support given by the Ministry of Public Health, aided by
international organizations such as the Pan American Health Organization (PAHO), the
United Nations Development program, and the Caribbean and Latin American Medical Science
Information Center.
7. The session was also attended by observers from the following international organizations:
(a) United Nations organs
International Bank for Reconstruction and Development (World Bank)
(b) Intergovernmental organizations
Asian-African Legal Consultative Committee (AALCC)
Inter-American Development Bank
International Institute for the Unification of Pnvate Law (UNIDROIT)
International Organizations
The following international organizations are collaborating on the Project:
0 International Commission on Non-Ionizing Radiation Protection (ICNIRP)
0 International Agency for Research on Cancer (IARC)
0 United Nations Environment Programme (UNEP)
</table>
<bodyText confidence="0.961295823529412">
Below is the list of international organizations that we distribute:
EU (European Union)
Books, documentation, periodicals on European legislation,
economy, agriculture, industry, education, norms, social
politics, law. For more information on publications. COM
TT documents and to subscribe to the Official Journal please
contact Dunya 1nfotel.
UN (United Nations)
Peace and security, economics, statistics, energy, natural
resources, environment, international law, human rights,
political affairs and disarmament, social questions. 1997
•: -&apos;4 periodicals include: Development Business, East-West
Investment News, Transnational Corporations, Monthly
Bulletin of Statistics, etc.
An agency may detail or transfer an employee to any organization which the Office of
Personnel Management has designated as an international organization (see list of international
organizations).
</bodyText>
<figureCaption confidence="0.981658">
Figure 2: Five different types of formatting used for enumerating NEs.
</figureCaption>
<bodyText confidence="0.999883909090909">
number of retrieved documents was however
restricted to 2000 in order to limit process-
ing times. The choice of these two search en-
gines is intended to evaluate whether a poorer
query mode (bags of words in NL instead of
strings in AV) can be palliated by accessing
more documents (2000 max. for NL instead
of 200 max. for AV).
The corpus collected by the two search
engines and the four families of queries is
2,958Mb large (details are given in Section 4).
</bodyText>
<subsectionHeader confidence="0.786737">
Acquisition of Candidate NEs
</subsectionHeader>
<bodyText confidence="0.99886">
Three parallel shallow parsers Pe, P1 and Pa
are used to extract NEs from the corpora col-
lected by the harvester. The parsers rely on
the query string to detect the sentence intro-
ducing the collection of NEs (the initializer in
(Pery-Woodley, 1998)). The text and HTML
marks after the initializer are parsed jointly
in order to retrieve one of the following three
spatio-syntactic structures:
</bodyText>
<listItem confidence="0.882407">
1. a textual enumeration (parser Pe, top-
</listItem>
<page confidence="0.971686">
183
</page>
<bodyText confidence="0.583524">
most example in Figure 2),
</bodyText>
<listItem confidence="0.9966165">
2. a list or a table (parser P1, the next three
examples in Figure 2),
3. an anchor toward a page containing a list
(parser Pa, bottom example in Figure 2).
</listItem>
<bodyText confidence="0.999607733333334">
In brief, these parsers combine string
matching (the initial lexical cue), syntactic
analysis (enumerations in Pe), analysis of for-
matting instructions (lists and tables in P1),
and access to linked documents through an-
chors detected by Pa. The results presented in
this paper only concern the first two parsers.
Since anchors raise specific problems in lin-
guistic analysis (Amitay, 1999), they will be
analyzed in another publication. The result-
ing candidate NEs are cleaned up and filtered
by a post-filtering module that splits associa-
tions of NEs, suppresses initial determiners or
trailing modifiers and punctuations, and re-
jects incorrect NEs.
</bodyText>
<subsectionHeader confidence="0.69024">
The Enumeration Parser Pe
</subsectionHeader>
<bodyText confidence="0.997249666666667">
The enumerations are expected to occur in-
side the sentences containing the query string.
Pe uses a traditional approach to parsing
through conjunction splitting in which a NE
pattern NE is given by (3) and an enumera-
tion by (4).3
</bodyText>
<equation confidence="0.9978125">
NE = ([A-Z\Rz][a-zA-Z \-\1* )+ (3)
Enum = (NE, )* NE (,? ) (andlor) NE (4)
</equation>
<bodyText confidence="0.97897247826087">
The List Parser P1
The lists are expected to occur no further than
four lines after the sentence containing the
query string. The lists are extracted through
one of the following three patterns. They cor-
respond to three alternatives commonly used
by HTML authors in order to build a spa-
tial construction of aligned items (lists, line
breaks, or tables). They are expressed by
case-insensitive regular expressions in which
the selected string is the shortest acceptable
underlined pattern:
In addition, after the removal of the HTML
mark-up tags, only the longest subpart of the
string accepted by (3) is produced as output
to the final filter. These patterns do not cover
all the situations in which a formatted text de-
notes a list. Some specific cases of lists such as
pre-formatted text in a verbatim environment
(&lt;pre&gt;), or items marked by a paragraph tag
(&lt;p&gt;) are not considered here. They would
produce too inaccurate results because they
are not typical enough of lists.
</bodyText>
<subsectionHeader confidence="0.714595">
Post filt ering
</subsectionHeader>
<bodyText confidence="0.99979775">
The pre-candidate NEs produced by the shal-
low parsers are processed by filters before be-
ing proposed as candidate NEs. The roles of
the filters are (in this order):
</bodyText>
<listItem confidence="0.998580105263158">
• removal of trailing lower-case words,
• deletion of the determiner the and the co-
ordinating conjunctions and and or and
the words which follow them,
• rejection of pre-candidates that contain
the characters @, # , $, ! or?.
• suppression of item marks such as 1., —,
* or a),
• suppression of HTML markups,
• suppression of leading coordinating con-
junctions,
• suppression of appositive sequences after
a comma or a hyphen,
• transformation of upper-case words into
initial upper-case in non-organization
candidate NEs because only organization
names are expected to contain acronyms,
• rejection of NEs containing words in a
stop list such as Next, Top, Web, or Click.
</listItem>
<bodyText confidence="0.7940618">
Postfiltering is completed by discarding
single-word candidates, that are described as
common words in the CELEX4 database, and
multi-word candidates that contain more than
5 words.
</bodyText>
<figure confidence="0.9426126">
&lt;li&gt; L&apos; (&lt;11i&gt; I&lt;li&gt; 1&lt;101&gt; I&lt;/141&gt;X5) 4 Experiments and Evaluations
&lt;br&gt; • &lt;Ibr&gt; (6) Data Collection
(&lt;td&gt; I &lt;th&gt;) • (&lt;td&gt; I&lt;th&gt; 1&lt;ltd&gt; (7) The acquisition of NEs is performed on 34
I &lt; I th&gt; I&lt; &apos;table&gt;) types of NEs chosen arbitrarily among three
subtypes of the MUC typology:
</figure>
<footnote confidence="0.980933666666667">
3The patterns are slightly more complicated in or-
der to accept diacriticized letters, and possible abbre-
viations composed of a single letter followed by a dot.
4The CELEX database for the English language is
available from the Consortium for Lexical Resources at
inni.ldc.upenn.eduireadme_files/celex.readme.html.
</footnote>
<page confidence="0.988271">
184
</page>
<construct confidence="0.9287755">
ORGANIZATION (American companies,
international organizations, universities, po-
litical organizations, international agencies,
car makers, terrorist groups, financial insti-
tutions, museums, international companies,
holdings, sects, and realtors),
PERSON (politicians, VIPs, actors, man-
agers, celebrities, actresses, athletes, authors,
film directors, top models, musicians, singers,
and journalists), and
LOCATION (countries, regions, states,
lakes, cities, rivers, mountains, and islands).
</construct>
<bodyText confidence="0.99998075">
Each of these 34 types (a (NE) string)
is combined with the four discourse mark-
ers given in (1.a-d), yielding 136 queries for
the two search engines. Each of the 272 cor-
pora collected through the harvester is made
of the 200 documents downloadable through
AV for the phrase search (or less if less are
retrieved) and 2,000 documents though NL.
Each of these corpora is parsed by the enu-
meration and the list parsers.
Two aspects of the data are evaluated.
First, the size of the yield is measured in order
to compare the productivity of the 272 queries
according to the type of query (type of NE
and type of discourse marker) and the type
.of search engine (rich versus plain queries and
low versus high number of downloaded docu-
ments). Second, the quality of the candidate
NEs is measured through human inspection of
accessible Web pages containing each NE.
</bodyText>
<subsectionHeader confidence="0.93583">
Corpus Size
</subsectionHeader>
<bodyText confidence="0.999855434782609">
The 272 corpora are 2,958 Mb large: 368
Mb for the corpora collected through AV and
2,590 Mb for those obtained through NL. De-
tailed sizes of corpora are shown in Table 1.
The corpora collected through NL for the pat-
tern list of (NE) represent more than a half
of the NL collection (1,307 Mb). The most
productive pattern for AV is (NE) such as
through which 41% of the AV collection is
downloaded (150 Mb).
The sizes of the corpora also depends on
the type of NEs. For each search engine, the
total sizes are reported for each pattern (1.a-
d). In addition, the largest corpus for each
of the three types of NEs is indicated in the
last three lines. The variety of sizes and dis-
tribution among the types of NEs shows that
using search engines with different capabili-
ties yields different figures for the collections
of pages. Therefore, the subsequent process of
NE acquisition heavily depends on the means
used to collect the basic textual data from
which knowledge is acquired.
</bodyText>
<subsectionHeader confidence="0.984381">
Quantitative Evaluation of Acquisition
</subsectionHeader>
<bodyText confidence="0.998314392156862">
Table 2 presents, for each pattern and each
search engine, the number of candidates, the
productivity, the ratios of the number of enu-
merations to lists, and the rate of redundancy.
In all, 17,176 candidates are produced
through AV and 34,978 through NL. The low-
est accuracy of the NL query mode is well pal-
liated by a larger collection of pages.
Productivity. The productivity is the ra-
tio of the number of candidates to the size
of the collection. Using a unit of number of
candidates per Mb, the productivity of AV is
46.7 while it is 3.5 times lower for NL (13.5).
Thus, collecting NEs from a coarser search en-
gine, such as NL, requires downloading 3.5
times larger corpora for the same yield. A
finer search engine with phrase query facili-
ties, such as AV, is more economical with re-
spect to knowledge acquisition based on dis-
course markers.
As was the case for the size of the col-
lection, the productivity of the corpora also
depends on the types of NEs. Universi-
ties (28.1), celebrities (53.0) and countries
(36.5) are the most productive NEs in their
categories while international agencies (4.0),
film directors (4.4) and states (8.7) are the
less productive ones. These discrepancies
certainly depend on the number of existing
names in these categories. For instance, there
are many more names of celebrities than film
directors. In fact, the productivity of NL is
significantly lower than the productivity of AV
only for the pattern list of NE. Since this pat-
tern corresponds to the largest corpus (see Ta-
ble 1), its poor performance in acquisition has
a strong impact on the overall productivity
of NL. Avoiding this pattern would make NL
more suitable for acquisition with a produc-
tivity of 23.2 (only 2 times lower than AV).
Ratios enumerations/lists. The ratios
in the third lines of the tables correspond to
the quotient of the number of candidates ac-
quired by analyzing enumerations (Pe parser)
to the number of candidates obtained from
the analysis of lists (P1 parser). Following
NE mainly yields NEs through the analysis
of lists, probably because enumerations using
coordinations are better introduced by such
as. The outcome is more balanced for list
of NE. It could be expected that this pat-
</bodyText>
<page confidence="0.999557">
185
</page>
<tableCaption confidence="0.965318">
Table 1: Size of the corpora of HTML pages (in Mb) collected on the four patterns (1.a-d)
through AltaVista (AV) and Northern Light (NL).
</tableCaption>
<table confidence="0.9998601875">
AV engine following NE (AV) list of NE (AV) NE such as (AV) such NE as (AV)
Largest corpus 6.1 6.4 11.3 5.8
ORGANIZATIONS int. organizations universities int. organizations int. organizations
Largest corpus 5.8 4.3 7.3 2.8
PERSON managers journalists politicians musicians
Largest corpus 6.8 4.9 13.6 7.3
LOCATION countries countries states states
Total size 85.9 64.9 150.4 _ 66.3
NL engine following NE (NL) list of NE (NL) NE such as (NL) such NE as (NL)
Largest corpus 10.0 75.1 58.5 19.5
ORGANIZATIONS museums int. agencies holdings universities
Largest corpus 10.2 60.0 44.1 48.6
PERSON actors politicians actors authors
Largest corpus 23.0 61.2 34.4 118.3
LOCATION rivers islands rivers states
Total size 172.8 1,306.9 652.7 458.1
</table>
<tableCaption confidence="0.932904">
Table 2: Size of the number of candidate NEs acquired from the web-based corpora described
in Table 1.
</tableCaption>
<table confidence="0.999766538461538">
AV engine following NE (AV) list of NE (AV) NE such as (AV) such NE as (AV)
# candidates 4,747 3,112 5,738 3,579
Productivity 55.2 48.0 38.2 53.9
Ratio enum./list 0.28 0.83 12.5 43.74
Redundancy 2.12 2.15 1.77 1.69
NL engine following NE (NL) list of NE (NL) NE such as (NL) such NE as (NL)
# candidates 5,667 5,176 14,800 9,335
Productivity 32.8 4.0 22.7 20.4
Ratio enum./list 0.31 0.49 10.41 14.72
Redundancy 2.12 2.34 2.13 2.20
AV &amp; NL following NE list of NE NE such as such NE as Total
# candidates 8,673 7,380 18,005 10,566 44,624
Overlap 16.7% 11.0% 12.3% 18.2% 15.0%
</table>
<page confidence="0.998648">
186
</page>
<bodyText confidence="0.999980069767442">
tern tends to introduce only lists, but there
are only 1.66 times more NEs obtained from
lists than from enumerations through list of
NE. The large number of NEs produced from
enumerations after this pattern certainly re-
lies on the combination of linguistics and for-
matting cues in the construction of meaning.
The writer avoids using (the word) list when
the text is followed by a (physical) list. Lastly,
in all, 11 times more NEs are obtained from
enumerations than from lists after the pattern
NE such as, and 18 times more after such NE
as. This shows that the linguistic pattern such
as preferably introduces textual enumerations
through coordinations (Hearst, 1998).
Redundancy. There are two main causes
of redundancy in acquisition. A first cause is
that the same NE can be acquired from sev-
eral collections in the same corpus. Redun-
dancy in the fourth lines of the tables is the
ratio of duplicates among the yield of can-
didate NEs for each search engine and each
query. This value is relatively stable what-
ever the search engine or the query pattern.
On average, redundancy is 2.09: each candi-
date is acquired slightly more than two times.
Acquisition through NL is slightly more re-
.dundant (2.18) than through AV (1.92). This
difference is not significant since the number
of NEs acquired through NL is twice as large
as the number of NEs acquired through AV.
Overlap. Another cause of multiple acqui-
sition is due to the concurrent exploitation of
two search engines. If these engines were using
similar techniques to retrieve documents, the
overlap would be large. Since we have chosen
two radically different modes of query (phrase
vs. bag-of-word technique), the overlap—the
ratio of the number common candidates to
the number of total candidates—is low (15%).
The two search engines seem to be comple-
mentary rather than competitive because they
retrieve different sets of documents.
</bodyText>
<subsectionHeader confidence="0.970046">
Precision of Acquisition
</subsectionHeader>
<bodyText confidence="0.999914571428571">
In all, 31,759 candidates are produced by
postfiltering the acquisition from the corpora
retrieved by the two search engines. A set of
504 candidates is randomly chosen for the pur-
pose of evaluation. For each candidate, AV is
queried with a phrase containing the string of
the NE. The topmost 20 pages retrieved by
AV are downloaded and then used for manual
inspection in case of doubt about the actual
status of the candidate. We assume that if
a candidate is correct, an unambiguous refer-
ence with the expected type should be found
at least in one of the topmost 20 pages.
Two levels of precision are measured:
</bodyText>
<listItem confidence="0.997202571428571">
1. A NE is correct if its full name is re-
trieved and if its fine-grained type (the 34
types given at the beginning of this sec-
tion) is correct. The manual inspection
of the 504 candidates indicates a preci-
sion of 62.8%.
2. A NE is correct if its full name is retrieved
</listItem>
<bodyText confidence="0.934272564102564">
and if its MUC type (ORGANIZATION,
PERSON, or LOCATION) is correct. In
this case, the precision is 73.6%.
The errors can be classified into the follow-
ing categories:
Wrong type Many errors in NE typing are
due to an incorrect connection between a
query pattern and a collection in a doc-
ument. For instance, Ashley Judd is in-
correctly reported as an athlete (she is an
actress) from the occurrence
His clientele includes stars and
athletes such as Ashley Judd
(below) and Mats Sundin.
The error is due to a partial analysis of
the initializer (underlined above). Only
athletes is seen as the hypernym while
stars is also part of it. A correct anal-
ysis of the occurrence would have led to
a type ambiguity. In this context, there is
no clue for deciding whether Ashley Judd
is a star or an athlete.
Other wrong types are due to poly-
semy. For instance, HorseFlySwarm is
extracted from a list of actors in a page
describing the commands and procedures
for programming a video game. Here ac-
tors has the meaning of a virtual actor,
a procedure in a programming environ-
ment, and not a movie star.
Incomplete Partial extraction of candidates
is mainly due to parsing errors or to col-
lections containing partial names of enti-
ties.
As an illustration of the second case, the
author&apos;s name Goffman is drawn from
the occurrence
Readings are drawn from the
work of such authors as Laing,
</bodyText>
<page confidence="0.99479">
187
</page>
<bodyText confidence="0.964526419354839">
Szasz, Goffman, Sartre, Bate-
son, and Freud.
Since this enumeration does not contain
the first names of the authors, it is not
appropriate for an acquisition of unam-
biguous author&apos;s names.
Other names such as Lucero are ambigu-
ous even though they are completely ex-
tracted because they correspond to a first
name or to a name that is part of sev-
eral other ones. They are also counted
as errors since they will be responsible of
spurious identifications in a name tagging
task.
Over-complete Excessive extractions are
due to parsing errors or to collections that
contain words accompanying names that
are incorrectly collected together with
the name. For instance, Director Lewis
Burke Frumkes is extracted as an au-
thor&apos;s name from a list in which the ac-
tual name Lewis Burke Frumkes is pre-
ceded by the title Director.
Miscellaneous Other types of errors do not
show clear connection between the ex-
tracted sequence and a NE. They are
mainly due to errors in the analysis of
the web page.
These types of errors are distributed as fol-
lows: wrong type 25%, incomplete 24%, over-
complete 8% and miscellaneous 43%.
</bodyText>
<sectionHeader confidence="0.941575" genericHeader="method">
5 Refinement of the Types of NEs
</sectionHeader>
<bodyText confidence="0.999666">
So far, the type of the candidate NEs is pro-
vided by the NE hypernym given in (1.a-d).
However, the initializer preceding the collec-
tion of NEs to be extracted can contain more
information on the type of the following NEs.
In fact the initializer fulfills four distinct func-
tions:
</bodyText>
<listItem confidence="0.774510875">
1. introduces the presence and the proxim-
ity of the collection, e.g. Here is
2. describes the structure of the collection,
e.g. a list of
3. gives the type of each item of the collec-
tion, e.g. universities
4. specifies the particular characteristics of
each item. e.g. universities in Vietnam
</listItem>
<bodyText confidence="0.997932310344828">
The cues used by the harvester are elements
which either introduce the collection (e.g. the
following) or describe the structure (e.g. a
list of). In initializers in general, these first
2 functions need not be expressed explicitly
by lexical means, as the layout itself indi-
cates the presence and type of the collection.
Readers exploit the visual properties of writ-
ten text to aid the construction of meaning
(Pery-Woodley, 1998).
However it is necessary to be explicit when
defining the items of the collection as this
information is not available to the reader
via structural properties. Initializers gener-
ally contain additional characteristics of the
items which provide the differentia (under-
lined here):
This is a list of American companies
with business interests in Latvia.
This example is the most explicit form an ini-
tializer can take as it contains a lexical ele-
ment which corresponds to each of the four
functions outlined above. It is fairly simple to
extract the details of the items from initializ-
ers with this basic form, as the modification
of the hypernym takes the form of a relative
clause, a prepositional phrase or an adjectival
phrase. A detailed grammar of this form of
initializer is as shown in Figure 3.5
</bodyText>
<figure confidence="0.870676">
The following is NP
PP
NP
(adj) Npl (PP I rel.c1.)
I
list of universities in Indonesia:
</figure>
<figureCaption confidence="0.999972">
Figure 3: The structure of a basic initializer
</figureCaption>
<bodyText confidence="0.9764358">
We tag the collection by part of speech us-
ing the TreeTagger (Schmid, 1999). The el-
ements which express the differentia are ex-
tracted by means of pattern matching: they
are always the modifiers of the plural noun in
the string, which is the hypernym of the items
of the collection.
5PP = prepositional phrase, Ns = noun (singular),
No = noun (plural), VP = verb in present tense, rel.cl.
= relative clause.
</bodyText>
<figure confidence="0.993807333333333">
NP
(det) (adj) N
Initializer
V
Vp
(det) (adj) Ns
</figure>
<page confidence="0.989753">
188
</page>
<bodyText confidence="0.988711068181819">
Initializers containing the search string such
as behave somewhat differently. They are
syntactically incomplete, and the missing con-
stituent is provided by each item of the col-
lection (Virbel, 1985). These phrases vary
considerably in structure and can require rela-
tively complex syntactic rearrangement to ex-
tract the properties of the hypernym. We will
not discuss these in more detail here.
One type of error in this system occurs
when a paragraph containing the search string
is followed by an unrelated list. For example
the harvester recognizes
Ask the long list of American com-
panies who have unsuccessfully mar-
keted products in Japan.
as an initializer when in fact it is not related to
any collection. If it happened to be followed
on the page by an collection of any kind the
system would mistakenly collect the items as
NEs of the type specified by the search string.
The cue list of is commonly used in dis-
cursive texts, so some filtering is required to
identify collections which are not employed as
initializers and to reduce the collection of er-
roneous items. Analyzing the syntactic forms
has allowed us to construct a set of regular
expressions which are used to eliminate non-
initializers and disregard any items collected
following them.
We have extracted 1813 potential initial-
izers from the corpus of HTML pages col-
lected via AV &amp; NL for the query string list
of NE. Using lexico-syntactic patterns in or-
der to identify correct initializers, we have de-
signed a shallow parser for filtering and ana-
lyzing the strings. This parser consists of 14
modules, 4 of which carry out pre-filtering to
prepare and tag the corpus, and 10 of which
carry out a fine-grained syntactic analysis, re-
moving collections that do not function as ini-
tializers. After filtering, the corpus contains
520 collections. The process has a precision
of 78% and a recall of 90%.
</bodyText>
<sectionHeader confidence="0.998424" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999936083333333">
This study is another application that demon-
strates the usability of the WWW as a re-
source for NLP (see, for instance, (Grefen-
stette, 1999) for an application of using
WWW frequencies in selecting translations).
It also confirms the interest of non-textual lin-
guistic features, such as formatting markups,
in.NLP for structured documents such as Web
pages. Further work on Web-based NE acqui-
sition could take advantage of machine learn-
ing techniques as used for wrapper induction
(Kushmerick et al., 1997).
</bodyText>
<sectionHeader confidence="0.992228" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999777886792453">
E. Amitay. 1999. Anchors in context: A corpus
analysis of web pages authoring conventions. In
L. Pemberton and S. Shurville, editors, Words
on the Web - Computer Mediated Communica-
tion, page 192. Intellect Books, UK.
C. Aone, N. Charocopos, and J. Gorlinski.
1997. An intelligent multilingual information
browsing and retrieval system using Informa-
tion Extraction. In Proceedings, Fifth Confer-
ence on Applied Natural Language Processing
(ANLP&apos;97), pages 332-39, Washington, DC.
R. Basili, M.T. Pazienza, and P. Velardi. 1993.
Acquisition of selectional patterns in sublan-
guages. Machine Translation, 8:175-201.
B. Boguraev and J. Pustejovsky, editors. 1996.
Corpus Processing for Lexical Acquisition. MIT
Press, Cambridge, MA.
F. Crimmins, A.F. Smeaton, T. Dkaki, and
J Mothe. 1999. Tetrafusion: Information dis-
covery on the internet. IEEE Intelligent Sys-
tems and Their Applications, 14(4):55-62.
B. Daille. 1996. Study and implementation of
combined techniques for automatic extraction
of terminology. In J.L. Klavaxis and P. Resnik,
editors, The Balancing Act, pages 49-66. MIT
Press, Cambridge, MA.
G. Grefenstette. 1994. Explorations in Automatic
Thesaurus Discovery. Kluwer Academic Pub-
lisher, Boston, MA.
G. Grefenstette. 1999. The WWW as a resource
for example-based MT tasks. In Proc., ASLIB
Translating and the Computer 21 Conference,
London.
M.A. Hearst. 1998. Automated discovery- of
WordNet relations. In C. Fellbaum, editor,
WordNet: An Electronic Lexical Database. MIT
Press, Cambridge, MA.
N. Kushmerick, D.S. Weld, and R. Doorenbos.
1997. Wrapper induction for information ex-
traction. In Proc., L1CAF97, pages 729-735,
Nagoya.
M.-P. Pery-Woodley. 1998. Signalling in written
text: a corpus based approach. In Workshop on
Discourse Relations and Discourse Markers at
COLING-ALC&apos;98, pages 79-85.
H. Schmid. 1999. Improvements in part-of-
speech tagging with an application to german.
In S. Armstrong, K.W. Church, P. Isabelle,
S. Manzi, E. Tzoukermann, and D. Yarowsld,
editors, Natural Language Processing Using
Very Large Corpora. Kluwer, Dordrecht.
J. Virbel. 1985. Mise en forme des documents.
Cahiers de Grammaire, 17.
</reference>
<page confidence="0.998915">
189
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9382805">Combining Lexical and Formatting Cues for Named Acquisition from the Web</title>
<author confidence="0.600605">Caroline</author>
<affiliation confidence="0.5428185">BP 133, F-91403 ORSAY Cedex, Dept of Language Engineering, PO Box 88, Manchester M60 1QD,</affiliation>
<email confidence="0.758572">jacquemin@1imsi.fr</email>
<email confidence="0.758572">caroline@1imsi.fr</email>
<abstract confidence="0.994010674698795">Because of their constant renewal, it is necessary to acquire fresh named entities (NEs) from recent text sources. We present a tool for the acquisition and the typing of NEs from the Web that associates a harvester and three parallel shallow parsers dedicated to specific structures (lists, enumerations, and anchors). The parsers combine lexical indices such as discourse markers with formatting instructags) for enumerations and associated initializers. 1 Overview Lexical acquisition from large corpora has long been considered as a means for enriching vocabularies (Boguraev and Pustejovsky, 1996). Depending on the studies, different issues are considered: the acquisition of terms (Daille, 1996), the acquisition of subcategorization frames (Basili et al., 1993), the acquisition of semantic links (Grefenstette, 1994), etc. While traditional electronic corpora such as journal articles or corpus resources (BNC, SUSANNE, Brown corpus) are satisfactory for classical lexical acquisition, Web corpora are another source of knowledge (Crimmins et al., 1999) that can be used to acquire NEs because of the constant updating of online data. The purpose of our work is to propose a technique for the extraction of NEs from the Web through the combination of a harvester and shallow parsers. Our study also belongs to corpus-based acquisition of semantic relationships through the analysis of specific lexico-syntactic contexts (Hearst, 1998) because hypernym relationships are acquired together with NEs. The unique contribution of our technique is to offer an integrated approach to the analysis of HTML documents that associates lexical cues with formatting in a single and cohesive framecombination of structural information and linguistic patterns is also found in induction, emerging topic of research in artificial intelligence and machine learning (Kushmerick et al., 1997). Our work differs from the MUC-related NE tagging task and its possible extension to name indexing of web pages (Aone et al., 1997) for the following reasons: • The purpose of our task is to build lists of NEs, not to tag corpora. For this reason, we only collect non-ambiguous contextindependent NEs; partial or incomplete occurrences such as anaphora are considered as incorrect. • The types of NEs collected here are much more accurate than the four basic types defined in MUC. The proposed technique could be extended to the collection of any non-MUC names which can be grouped under a common hypernym: botanic names, mechanical parts, book titles, events... • We emphasize the role of document structure in web-based collection. 2 Focusing on Definitory Contexts issues are addressed this paper: 1. While traditional electronic corpora can be accessed directly and entirely through large-scale filters such as shallow parsers, access to Web pages is restricted to the narrow and specialized medium of a search engine. In order to spot and retrieve relevant text chunks, we must focus on linguistic cues that can be used to access pages containing typed NEs with high precision. 2. While Web pages are full of NEs, only a small proportion of them are relevant for the acquisition of public, fresh and wellknown NEs (the name of someone&apos;s cat 181</abstract>
<title confidence="0.891852125">WWW Harvester Search Engine Queries Enumeration Parser List and tables Anchor Parser Parser</title>
<abstract confidence="0.986316">t Candidate Candidate NEs Index pages Typed NEs Figure 1: Architecture is not relevant to our purpose). So that automatically acquired NEs can be used in a NE recognition task, they are assowith types such as (PERor (ORGANIZATION). The need for selective linguistic cues (wrt to the current facilities offered by search engines) and for informative and typifying contexts has led us to focus on collections, a specific type of definitory contexts (Pery-Woodley, 1998). Because they contain specific linguistic triggers as as, contexts can be accessed through phrase queries to a search engine. In addition, these contexts the classical scheme define NEs, and thus provide, through the hypernym of the NEs they define. Our study extends (Hearst, 1998) to Webbased and spatially formatted corpora. 3 Architecture and Principles To acquire NEs from the Web, we have developed a system that consists of three sequential modules (see Figure 1): 1. A harvester that downloads the pages retrieved by a search engine from the four following query strings following (1.c) (NE) as list of (1.d) in which (NE) stands for a typifying of NEs such as car list in 4). Three parallel shallow parsers P1 extract candidate NEs respectively from enumerations, lists and tables, and anchors. 3. A post-filtering module that cleans up the candidate NEs from leading determiners or trailing unrelated words and splits coordinated NEs into unitary items. Corpus Harvesting The four strings (1.a-d) given above are used to query a search engine. They consist of an hypernym and a discourse marker. They are expected to be followed by a collection of NEs. Figure 2 shows five prototypical examples of collections encountered in HTML pages rethrough one of the strings first collection is an consists of a coordination of three NEs. The collection is a into two sublists. Each sublist is introduced by a hy- The third structure is a by bullets. Such lists can be constructed through an HTML table (this example), or using enumeration marks (&lt;u/&gt; or fourth example is also a by using a table structure but displays a more complex spatial organization and does not employ graphical bullets. The fifth example is a collection not provided to the reader within the document, but which can be reached by following an hyperlink instead. The corpus of HTML pages is collected through two search engines with different capabilities: AltaVista (AV) and Northern Light AV offers the facility of double-quoting the query strings in order to search for exact strings (also called phrases in IR). NL does not support phrase search. However, in AV, the number of retrievable documents is limited to the 200 highest ranked documents while it is potentially unlimited in NL. For NL, the is here. The typographical mark-up of the query string in the figure is ours. The hypemym is in bold italics and the discourse marker is in bold. harvester that retrieves Web pages through a search engine is a combination of wget available from //sunsite. auc . and Perl scripts.</abstract>
<note confidence="0.6972678">182 It&apos;s development is due to the support given by the Ministry of Public Health, aided by organizations as Pan American Health Organization (PAHO), the United Nations Development program, and the Caribbean and Latin American Medical Science Information Center. The session was also attended by observers from the organizations: (a) United Nations organs International Bank for Reconstruction and Development (World Bank)</note>
<title confidence="0.722539">(b) Intergovernmental organizations</title>
<author confidence="0.768006">Asian-African Legal Consultative Committee Inter-American Development Bank</author>
<affiliation confidence="0.681013">International Institute for the Unification of Pnvate Law (UNIDROIT)</affiliation>
<note confidence="0.951933083333333">International Organizations organizations collaborating on the Project: 0 International Commission on Non-Ionizing Radiation Protection (ICNIRP) 0 International Agency for Research on Cancer (IARC) 0United Nations Environment Programme (UNEP) is the of organizations we distribute: EU (European Union) Books, documentation, periodicals on European legislation, economy, agriculture, industry, education, norms, social politics, law. For more information on publications. COM and to subscribe to the Official Journal please contact Dunya 1nfotel. UN (United Nations)</note>
<date confidence="0.400125">Peace and security, economics, statistics, energy, natural resources, environment, international law, human rights, political affairs and disarmament, social questions. 1997</date>
<affiliation confidence="0.385508">periodicals include: Development Business,</affiliation>
<address confidence="0.324233">Investment News, Transnational Corporations, Monthly</address>
<abstract confidence="0.990167240847785">Bulletin of Statistics, etc. agency may employee to any organization which the Office of Management has designated as an international organization (see of organizations). Figure 2: Five different types of formatting used for enumerating NEs. number of retrieved documents was however restricted to 2000 in order to limit processing times. The choice of these two search engines is intended to evaluate whether a poorer query mode (bags of words in NL instead of strings in AV) can be palliated by accessing more documents (2000 max. for NL instead of 200 max. for AV). The corpus collected by the two search engines and the four families of queries is 2,958Mb large (details are given in Section 4). Acquisition of Candidate NEs parallel shallow parsers P1 are used to extract NEs from the corpora collected by the harvester. The parsers rely on the query string to detect the sentence introthe collection of NEs (the 1998)). The text and marks after the initializer are parsed jointly in order to retrieve one of the following three spatio-syntactic structures: a textual enumeration (parser top- 183 most example in Figure 2), a list or a table (parser next three examples in Figure 2), 3. an anchor toward a page containing a list (parser Pa, bottom example in Figure 2). In brief, these parsers combine string matching (the initial lexical cue), syntactic analysis (enumerations in Pe), analysis of formatting instructions (lists and tables in P1), and access to linked documents through anchors detected by Pa. The results presented in this paper only concern the first two parsers. Since anchors raise specific problems in linguistic analysis (Amitay, 1999), they will be analyzed in another publication. The resulting candidate NEs are cleaned up and filtered by a post-filtering module that splits associations of NEs, suppresses initial determiners or trailing modifiers and punctuations, and rejects incorrect NEs. The Enumeration Parser Pe The enumerations are expected to occur inside the sentences containing the query string. Pe uses a traditional approach to parsing through conjunction splitting in which a NE given by (3) and an enumeraby = ([A-Z\Rz][a-zA-Z \-\1* )+ = (NE, )* NE (,? ) (andlor) NE List Parser lists are expected occur no further than lines the sentence the query string. The lists are extracted through one of the following three patterns. They correspond to three alternatives commonly used by HTML authors in order to build a spatial construction of aligned items (lists, line breaks, or tables). They are expressed by case-insensitive regular expressions in which selected string is the underlined pattern: In addition, after the removal of the HTML tags, only the of the string accepted by (3) is produced as output to the final filter. These patterns do not cover all the situations in which a formatted text denotes a list. Some specific cases of lists such as pre-formatted text in a verbatim environment items marked by a paragraph tag not considered here. They would produce too inaccurate results because they are not typical enough of lists. Post filt ering The pre-candidate NEs produced by the shallow parsers are processed by filters before being proposed as candidate NEs. The roles of the filters are (in this order): • removal of trailing lower-case words, deletion of the determiner the coconjunctions or and the words which follow them, • rejection of pre-candidates that contain the characters @, # , $, ! or?. • suppression of item marks such as 1., —, * or a), • suppression of HTML markups, • suppression of leading coordinating conjunctions, • suppression of appositive sequences after a comma or a hyphen, • transformation of upper-case words into initial upper-case in non-organization candidate NEs because only organization names are expected to contain acronyms, • rejection of NEs containing words in a list such as Top, Web, Postfiltering is completed by discarding single-word candidates, that are described as words in the database, and multi-word candidates that contain more than 5 words. &lt;li&gt; &lt;br&gt; L&apos; (&lt;11i&gt; I&lt;li&gt; 1&lt;101&gt; I&lt;/141&gt;X5) 4 Experiments and Evaluations Data Collection (&lt;td&gt; I &lt;th&gt;) &lt;Ibr&gt; acquisition of NEs is performed on types of NEs chosen arbitrarily among three subtypes of the MUC typology: (&lt;td&gt; I&lt;th&gt; 1&lt;ltd&gt; I &lt; I th&gt; I&lt; &apos;table&gt;) patterns are slightly more complicated in order to accept diacriticized letters, and possible abbreviations composed of a single letter followed by a dot. CELEX database for the English language is available from the Consortium for Lexical Resources at inni.ldc.upenn.eduireadme_files/celex.readme.html. 184 companies, international organizations, universities, political organizations, international agencies, car makers, terrorist groups, financial institutions, museums, international companies, sects, VIPs, actors, managers, celebrities, actresses, athletes, authors, film directors, top models, musicians, singers, regions, states, cities, rivers, mountains, Each of these 34 types (a (NE) string) is combined with the four discourse markers given in (1.a-d), yielding 136 queries for the two search engines. Each of the 272 corpora collected through the harvester is made of the 200 documents downloadable through AV for the phrase search (or less if less are retrieved) and 2,000 documents though NL. Each of these corpora is parsed by the enumeration and the list parsers. Two aspects of the data are evaluated. First, the size of the yield is measured in order to compare the productivity of the 272 queries according to the type of query (type of NE and type of discourse marker) and the type .of search engine (rich versus plain queries and low versus high number of downloaded documents). Second, the quality of the candidate NEs is measured through human inspection of accessible Web pages containing each NE. Corpus Size corpora are 2,958 Mb large: 368 Mb for the corpora collected through AV and 2,590 Mb for those obtained through NL. Detailed sizes of corpora are shown in Table 1. The corpora collected through NL for the patof represent more than a half of the NL collection (1,307 Mb). The most pattern for AV is (NE) as through which 41% of the AV collection is downloaded (150 Mb). The sizes of the corpora also depends on the type of NEs. For each search engine, the total sizes are reported for each pattern (1.ad). In addition, the largest corpus for each of the three types of NEs is indicated in the last three lines. The variety of sizes and distribution among the types of NEs shows that using search engines with different capabilities yields different figures for the collections of pages. Therefore, the subsequent process of NE acquisition heavily depends on the means used to collect the basic textual data from which knowledge is acquired. Quantitative Evaluation of Acquisition presents, for each pattern and each search engine, the number of candidates, the of the number of enuto lists, the rate of In all, 17,176 candidates are produced through AV and 34,978 through NL. The lowest accuracy of the NL query mode is well palliated by a larger collection of pages. productivity is the ratio of the number of candidates to the size of the collection. Using a unit of number of candidates per Mb, the productivity of AV is 46.7 while it is 3.5 times lower for NL (13.5). Thus, collecting NEs from a coarser search engine, such as NL, requires downloading 3.5 times larger corpora for the same yield. A finer search engine with phrase query facilities, such as AV, is more economical with respect to knowledge acquisition based on discourse markers. As was the case for the size of the collection, the productivity of the corpora also on the types of NEs. Universiand (36.5) are the most productive NEs in their while agencies (4.0), directors and are the less productive ones. These discrepancies certainly depend on the number of existing names in these categories. For instance, there many more names of fact, the productivity of NL is significantly lower than the productivity of AV for the pattern of Since this pattern corresponds to the largest corpus (see Table 1), its poor performance in acquisition has a strong impact on the overall productivity of NL. Avoiding this pattern would make NL more suitable for acquisition with a productivity of 23.2 (only 2 times lower than AV). enumerations/lists. ratios in the third lines of the tables correspond to the quotient of the number of candidates acby analyzing enumerations to the number of candidates obtained from analysis of lists NE mainly yields NEs through the analysis of lists, probably because enumerations using are better introduced by outcome is more balanced for It could be expected that this pat- 185 Table 1: Size of the corpora of HTML pages (in Mb) collected on the four patterns (1.a-d) through AltaVista (AV) and Northern Light (NL). AV engine (AV) of (AV) as Largest corpus ORGANIZATIONS 6.1 6.4 11.3 5.8 int. organizations universities int. organizations int. organizations Largest corpus PERSON 5.8 4.3 7.3 2.8 managers journalists politicians musicians Largest corpus LOCATION 6.8 4.9 13.6 7.3 countries countries states states Total size 85.9 64.9 150.4 _ 66.3 (NL) of (NL) as as (NL) Largest corpus ORGANIZATIONS 10.0 75.1 58.5 19.5 museums int. agencies holdings universities Largest corpus PERSON 10.2 60.0 44.1 48.6 actors politicians actors authors Largest corpus LOCATION 23.0 61.2 34.4 118.3 rivers islands rivers states Total size 172.8 1,306.9 652.7 458.1 Table 2: Size of the number of candidate NEs acquired from the web-based corpora described in Table 1. (AV) (AV) as NE (AV) Productivity 55.2 48.0 38.2 53.9 Ratio enum./list 0.28 0.83 12.5 43.74 Redundancy 2.12 2.15 1.77 1.69 (NL) of (NL) (NL) Productivity 32.8 4.0 22.7 20.4 Ratio enum./list 0.31 0.49 10.41 14.72 Redundancy 2.12 2.34 2.13 2.20 AV &amp; NL of as as Total 8,673 7,380 18,005 10,566 44,624 Overlap 16.7% 11.0% 12.3% 18.2% 15.0% 186 tern tends to introduce only lists, but there are only 1.66 times more NEs obtained from than from enumerations through of NE. The large number of NEs produced from enumerations after this pattern certainly relies on the combination of linguistics and formatting cues in the construction of meaning. writer avoids using (the word) the text is followed by a (physical) list. Lastly, in all, 11 times more NEs are obtained from enumerations than from lists after the pattern as, 18 times more after shows that the linguistic pattern introduces textual enumerations through coordinations (Hearst, 1998). are two main causes of redundancy in acquisition. A first cause is that the same NE can be acquired from several collections in the same corpus. Redundancy in the fourth lines of the tables is the ratio of duplicates among the yield of candidate NEs for each search engine and each query. This value is relatively stable whatever the search engine or the query pattern. On average, redundancy is 2.09: each candidate is acquired slightly more than two times. Acquisition through NL is slightly more re- .dundant (2.18) than through AV (1.92). This difference is not significant since the number of NEs acquired through NL is twice as large as the number of NEs acquired through AV. cause of multiple acquisition is due to the concurrent exploitation of two search engines. If these engines were using similar techniques to retrieve documents, the overlap would be large. Since we have chosen two radically different modes of query (phrase vs. bag-of-word technique), the overlap—the ratio of the number common candidates to the number of total candidates—is low (15%). The two search engines seem to be complementary rather than competitive because they retrieve different sets of documents. Precision of Acquisition In all, 31,759 candidates are produced by postfiltering the acquisition from the corpora retrieved by the two search engines. A set of 504 candidates is randomly chosen for the purpose of evaluation. For each candidate, AV is queried with a phrase containing the string of the NE. The topmost 20 pages retrieved by AV are downloaded and then used for manual inspection in case of doubt about the actual status of the candidate. We assume that if a candidate is correct, an unambiguous reference with the expected type should be found at least in one of the topmost 20 pages. Two levels of precision are measured: 1. A NE is correct if its full name is retrieved and if its fine-grained type (the 34 types given at the beginning of this section) is correct. The manual inspection of the 504 candidates indicates a precision of 62.8%. 2. A NE is correct if its full name is retrieved and if its MUC type (ORGANIZATION, PERSON, or LOCATION) is correct. In this case, the precision is 73.6%. The errors can be classified into the following categories: type errors in NE typing are due to an incorrect connection between a query pattern and a collection in a doc- For instance, Judd incorrectly reported as an athlete (she is an actress) from the occurrence His clientele includes stars and such asAshley Judd (below) and Mats Sundin. The error is due to a partial analysis of the initializer (underlined above). Only seen as the hypernym while is part of it. A correct analysis of the occurrence would have led to a type ambiguity. In this context, there is clue for deciding whether Judd is a star or an athlete. Other wrong types are due to poly- For instance, is extracted from a list of actors in a page describing the commands and procedures programming a video game. Here acthe meaning of a virtual actor, a procedure in a programming environment, and not a movie star. extraction of candidates is mainly due to parsing errors or to collections containing partial names of entities. As an illustration of the second case, the name drawn from the occurrence Readings are drawn from work of such authors as Laing, 187 Szasz, Goffman, Sartre, Bateson, and Freud. Since this enumeration does not contain the first names of the authors, it is not appropriate for an acquisition of unambiguous author&apos;s names. names such as ambiguous even though they are completely extracted because they correspond to a first name or to a name that is part of several other ones. They are also counted as errors since they will be responsible of spurious identifications in a name tagging task. extractions are due to parsing errors or to collections that contain words accompanying names that are incorrectly collected together with name. For instance, Lewis Frumkes extracted as an author&apos;s name from a list in which the acname Burke Frumkes preby the title types of errors do not show clear connection between the extracted sequence and a NE. They are mainly due to errors in the analysis of the web page. These types of errors are distributed as follows: wrong type 25%, incomplete 24%, overcomplete 8% and miscellaneous 43%. 5 Refinement of the Types of NEs So far, the type of the candidate NEs is provided by the NE hypernym given in (1.a-d). However, the initializer preceding the collection of NEs to be extracted can contain more information on the type of the following NEs. In fact the initializer fulfills four distinct functions: 1. introduces the presence and the proximof the collection, Here is 2. describes the structure of the collection, e.g. a list of 3. gives the type of each item of the collecuniversities 4. specifies the particular characteristics of item. universities in Vietnam The cues used by the harvester are elements either introduce the collection (e.g. describe the structure (e.g. of). initializers in general, these first 2 functions need not be expressed explicitly by lexical means, as the layout itself indicates the presence and type of the collection. Readers exploit the visual properties of written text to aid the construction of meaning (Pery-Woodley, 1998). However it is necessary to be explicit when defining the items of the collection as this information is not available to the reader via structural properties. Initializers generally contain additional characteristics of the items which provide the differentia (underlined here): This is a list of American companies with business interests in Latvia. This example is the most explicit form an initializer can take as it contains a lexical element which corresponds to each of the four functions outlined above. It is fairly simple to extract the details of the items from initializers with this basic form, as the modification of the hypernym takes the form of a relative clause, a prepositional phrase or an adjectival phrase. A detailed grammar of this form of is as shown in Figure following is PP NP Npl rel.c1.) I list of universities in Indonesia: Figure 3: The structure of a basic initializer We tag the collection by part of speech using the TreeTagger (Schmid, 1999). The elements which express the differentia are extracted by means of pattern matching: they are always the modifiers of the plural noun in the string, which is the hypernym of the items of the collection. = prepositional phrase, = (singular), = noun (plural), = verb in present tense, rel.cl. = relative clause. NP (det) (adj) N Initializer V Vp (det) (adj) Ns 188 containing the search string somewhat differently. They are syntactically incomplete, and the missing constituent is provided by each item of the collection (Virbel, 1985). These phrases vary considerably in structure and can require relatively complex syntactic rearrangement to extract the properties of the hypernym. We will not discuss these in more detail here. One type of error in this system occurs when a paragraph containing the search string is followed by an unrelated list. For example the harvester recognizes Ask the long list of American companies who have unsuccessfully marketed products in Japan. as an initializer when in fact it is not related to any collection. If it happened to be followed on the page by an collection of any kind the system would mistakenly collect the items as NEs of the type specified by the search string. cue of commonly used in discursive texts, so some filtering is required to identify collections which are not employed as initializers and to reduce the collection of erroneous items. Analyzing the syntactic forms has allowed us to construct a set of regular which are used to eliminate nondisregard any collected following them. We have extracted 1813 potential initializers from the corpus of HTML pages colvia AV &amp; NL for the query string NE. lexico-syntactic patterns in order to identify correct initializers, we have designed a shallow parser for filtering and analyzing the strings. This parser consists of 14 modules, 4 of which carry out pre-filtering to prepare and tag the corpus, and 10 of which carry out a fine-grained syntactic analysis, removing collections that do not function as initializers. After filtering, the corpus contains 520 collections. The process has a precision of 78% and a recall of 90%. 6 Conclusion This study is another application that demonstrates the usability of the WWW as a resource for NLP (see, for instance, (Grefenstette, 1999) for an application of using WWW frequencies in selecting translations). It also confirms the interest of non-textual linguistic features, such as formatting markups, for structured documents such as Web pages. Further work on Web-based NE acquisition could take advantage of machine learning techniques as used for wrapper induction</abstract>
<note confidence="0.884425333333333">(Kushmerick et al., 1997). References E. Amitay. 1999. Anchors in context: A corpus analysis of web pages authoring conventions. In Pemberton and S. Shurville, editors, on the Web - Computer Mediated Communica- 192. Intellect Books, UK. C. Aone, N. Charocopos, and J. Gorlinski. 1997. An intelligent multilingual information</note>
<title confidence="0.899107">browsing and retrieval system using Informa-</title>
<author confidence="0.593221">In Fifth Confer-</author>
<affiliation confidence="0.368568">ence on Applied Natural Language Processing</affiliation>
<address confidence="0.773098">332-39, Washington, DC.</address>
<note confidence="0.88026803030303">R. Basili, M.T. Pazienza, and P. Velardi. 1993. Acquisition of selectional patterns in sublan- Translation, B. Boguraev and J. Pustejovsky, editors. 1996. Processing for Lexical Acquisition. Press, Cambridge, MA. F. Crimmins, A.F. Smeaton, T. Dkaki, and J Mothe. 1999. Tetrafusion: Information dison the internet. Intelligent Sysand Their Applications, B. Daille. 1996. Study and implementation of combined techniques for automatic extraction of terminology. In J.L. Klavaxis and P. Resnik, Balancing Act, 49-66. MIT Press, Cambridge, MA. Grefenstette. 1994. in Automatic Discovery. Academic Publisher, Boston, MA. G. Grefenstette. 1999. The WWW as a resource example-based MT tasks. In ASLIB Translating and the Computer 21 Conference, London. M.A. Hearst. 1998. Automated discoveryof WordNet relations. In C. Fellbaum, editor, An Electronic Lexical Database. Press, Cambridge, MA. N. Kushmerick, D.S. Weld, and R. Doorenbos. 1997. Wrapper induction for information ex- In L1CAF97, 729-735, Nagoya. M.-P. Pery-Woodley. 1998. Signalling in written a corpus based approach. In on Discourse Relations and Discourse Markers at</note>
<abstract confidence="0.929453">H. Schmid. 1999. Improvements in part-ofspeech tagging with an application to german.</abstract>
<note confidence="0.5016796">In S. Armstrong, K.W. Church, P. Isabelle, Manzi, E. Tzoukermann, and Language Processing Using Kluwer, Dordrecht. J. Virbel. 1985. Mise en forme des documents.</note>
<affiliation confidence="0.710289">de Grammaire,</affiliation>
<address confidence="0.703261">189</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Amitay</author>
</authors>
<title>Anchors in context: A corpus analysis of web pages authoring conventions.</title>
<date>1999</date>
<booktitle>Words on the Web - Computer Mediated Communication,</booktitle>
<pages>192</pages>
<editor>In L. Pemberton and S. Shurville, editors,</editor>
<publisher>Intellect Books, UK.</publisher>
<contexts>
<context position="10561" citStr="Amitay, 1999" startWordPosition="1666" endWordPosition="1667">a textual enumeration (parser Pe, top183 most example in Figure 2), 2. a list or a table (parser P1, the next three examples in Figure 2), 3. an anchor toward a page containing a list (parser Pa, bottom example in Figure 2). In brief, these parsers combine string matching (the initial lexical cue), syntactic analysis (enumerations in Pe), analysis of formatting instructions (lists and tables in P1), and access to linked documents through anchors detected by Pa. The results presented in this paper only concern the first two parsers. Since anchors raise specific problems in linguistic analysis (Amitay, 1999), they will be analyzed in another publication. The resulting candidate NEs are cleaned up and filtered by a post-filtering module that splits associations of NEs, suppresses initial determiners or trailing modifiers and punctuations, and rejects incorrect NEs. The Enumeration Parser Pe The enumerations are expected to occur inside the sentences containing the query string. Pe uses a traditional approach to parsing through conjunction splitting in which a NE pattern NE is given by (3) and an enumeration by (4).3 NE = ([A-Z\Rz][a-zA-Z \-\1* )+ (3) Enum = (NE, )* NE (,? ) (andlor) NE (4) The Lis</context>
</contexts>
<marker>Amitay, 1999</marker>
<rawString>E. Amitay. 1999. Anchors in context: A corpus analysis of web pages authoring conventions. In L. Pemberton and S. Shurville, editors, Words on the Web - Computer Mediated Communication, page 192. Intellect Books, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Aone</author>
<author>N Charocopos</author>
<author>J Gorlinski</author>
</authors>
<title>An intelligent multilingual information browsing and retrieval system using Information Extraction.</title>
<date>1997</date>
<booktitle>In Proceedings, Fifth Conference on Applied Natural Language Processing (ANLP&apos;97),</booktitle>
<pages>332--39</pages>
<location>Washington, DC.</location>
<contexts>
<context position="2352" citStr="Aone et al., 1997" startWordPosition="354" endWordPosition="357">Hearst, 1998) because hypernym relationships are acquired together with NEs. The unique contribution of our technique is to offer an integrated approach to the analysis of HTML documents that associates lexical cues with formatting instructions in a single and cohesive framework. The combination of structural information and linguistic patterns is also found in wrapper induction, an emerging topic of research in artificial intelligence and machine learning (Kushmerick et al., 1997). Our work differs from the MUC-related NE tagging task and its possible extension to name indexing of web pages (Aone et al., 1997) for the following reasons: • The purpose of our task is to build lists of NEs, not to tag corpora. For this reason, we only collect non-ambiguous contextindependent NEs; partial or incomplete occurrences such as anaphora are considered as incorrect. • The types of NEs collected here are much more accurate than the four basic types defined in MUC. The proposed technique could be extended to the collection of any non-MUC names which can be grouped under a common hypernym: botanic names, mechanical parts, book titles, events... • We emphasize the role of document structure in web-based collectio</context>
</contexts>
<marker>Aone, Charocopos, Gorlinski, 1997</marker>
<rawString>C. Aone, N. Charocopos, and J. Gorlinski. 1997. An intelligent multilingual information browsing and retrieval system using Information Extraction. In Proceedings, Fifth Conference on Applied Natural Language Processing (ANLP&apos;97), pages 332-39, Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Basili</author>
<author>M T Pazienza</author>
<author>P Velardi</author>
</authors>
<title>Acquisition of selectional patterns in sublanguages.</title>
<date>1993</date>
<booktitle>Machine Translation,</booktitle>
<pages>8--175</pages>
<contexts>
<context position="1073" citStr="Basili et al., 1993" startWordPosition="154" endWordPosition="157">NEs from the Web that associates a harvester and three parallel shallow parsers dedicated to specific structures (lists, enumerations, and anchors). The parsers combine lexical indices such as discourse markers with formatting instructions (HTML tags) for analyzing enumerations and associated initializers. 1 Overview Lexical acquisition from large corpora has long been considered as a means for enriching vocabularies (Boguraev and Pustejovsky, 1996). Depending on the studies, different issues are considered: the acquisition of terms (Daille, 1996), the acquisition of subcategorization frames (Basili et al., 1993), the acquisition of semantic links (Grefenstette, 1994), etc. While traditional electronic corpora such as journal articles or corpus resources (BNC, SUSANNE, Brown corpus) are satisfactory for classical lexical acquisition, Web corpora are another source of knowledge (Crimmins et al., 1999) that can be used to acquire NEs because of the constant updating of online data. The purpose of our work is to propose a technique for the extraction of NEs from the Web through the combination of a harvester and shallow parsers. Our study also belongs to corpus-based acquisition of semantic relationships</context>
</contexts>
<marker>Basili, Pazienza, Velardi, 1993</marker>
<rawString>R. Basili, M.T. Pazienza, and P. Velardi. 1993. Acquisition of selectional patterns in sublanguages. Machine Translation, 8:175-201.</rawString>
</citation>
<citation valid="true">
<title>Corpus Processing for Lexical Acquisition.</title>
<date>1996</date>
<editor>B. Boguraev and J. Pustejovsky, editors.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>1996</marker>
<rawString>B. Boguraev and J. Pustejovsky, editors. 1996. Corpus Processing for Lexical Acquisition. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Crimmins</author>
<author>A F Smeaton</author>
<author>T Dkaki</author>
<author>J Mothe</author>
</authors>
<title>Tetrafusion: Information discovery on the internet.</title>
<date>1999</date>
<booktitle>IEEE Intelligent Systems and Their Applications,</booktitle>
<pages>14--4</pages>
<contexts>
<context position="1366" citStr="Crimmins et al., 1999" startWordPosition="195" endWordPosition="198">ed initializers. 1 Overview Lexical acquisition from large corpora has long been considered as a means for enriching vocabularies (Boguraev and Pustejovsky, 1996). Depending on the studies, different issues are considered: the acquisition of terms (Daille, 1996), the acquisition of subcategorization frames (Basili et al., 1993), the acquisition of semantic links (Grefenstette, 1994), etc. While traditional electronic corpora such as journal articles or corpus resources (BNC, SUSANNE, Brown corpus) are satisfactory for classical lexical acquisition, Web corpora are another source of knowledge (Crimmins et al., 1999) that can be used to acquire NEs because of the constant updating of online data. The purpose of our work is to propose a technique for the extraction of NEs from the Web through the combination of a harvester and shallow parsers. Our study also belongs to corpus-based acquisition of semantic relationships through the analysis of specific lexico-syntactic contexts (Hearst, 1998) because hypernym relationships are acquired together with NEs. The unique contribution of our technique is to offer an integrated approach to the analysis of HTML documents that associates lexical cues with formatting </context>
</contexts>
<marker>Crimmins, Smeaton, Dkaki, Mothe, 1999</marker>
<rawString>F. Crimmins, A.F. Smeaton, T. Dkaki, and J Mothe. 1999. Tetrafusion: Information discovery on the internet. IEEE Intelligent Systems and Their Applications, 14(4):55-62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Daille</author>
</authors>
<title>Study and implementation of combined techniques for automatic extraction of terminology.</title>
<date>1996</date>
<booktitle>The Balancing Act,</booktitle>
<pages>49--66</pages>
<editor>In J.L. Klavaxis and P. Resnik, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1006" citStr="Daille, 1996" startWordPosition="146" endWordPosition="147">es. We present a tool for the acquisition and the typing of NEs from the Web that associates a harvester and three parallel shallow parsers dedicated to specific structures (lists, enumerations, and anchors). The parsers combine lexical indices such as discourse markers with formatting instructions (HTML tags) for analyzing enumerations and associated initializers. 1 Overview Lexical acquisition from large corpora has long been considered as a means for enriching vocabularies (Boguraev and Pustejovsky, 1996). Depending on the studies, different issues are considered: the acquisition of terms (Daille, 1996), the acquisition of subcategorization frames (Basili et al., 1993), the acquisition of semantic links (Grefenstette, 1994), etc. While traditional electronic corpora such as journal articles or corpus resources (BNC, SUSANNE, Brown corpus) are satisfactory for classical lexical acquisition, Web corpora are another source of knowledge (Crimmins et al., 1999) that can be used to acquire NEs because of the constant updating of online data. The purpose of our work is to propose a technique for the extraction of NEs from the Web through the combination of a harvester and shallow parsers. Our study</context>
</contexts>
<marker>Daille, 1996</marker>
<rawString>B. Daille. 1996. Study and implementation of combined techniques for automatic extraction of terminology. In J.L. Klavaxis and P. Resnik, editors, The Balancing Act, pages 49-66. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Grefenstette</author>
</authors>
<title>Explorations in Automatic Thesaurus Discovery.</title>
<date>1994</date>
<publisher>Kluwer Academic Publisher,</publisher>
<location>Boston, MA.</location>
<contexts>
<context position="1129" citStr="Grefenstette, 1994" startWordPosition="164" endWordPosition="165">rallel shallow parsers dedicated to specific structures (lists, enumerations, and anchors). The parsers combine lexical indices such as discourse markers with formatting instructions (HTML tags) for analyzing enumerations and associated initializers. 1 Overview Lexical acquisition from large corpora has long been considered as a means for enriching vocabularies (Boguraev and Pustejovsky, 1996). Depending on the studies, different issues are considered: the acquisition of terms (Daille, 1996), the acquisition of subcategorization frames (Basili et al., 1993), the acquisition of semantic links (Grefenstette, 1994), etc. While traditional electronic corpora such as journal articles or corpus resources (BNC, SUSANNE, Brown corpus) are satisfactory for classical lexical acquisition, Web corpora are another source of knowledge (Crimmins et al., 1999) that can be used to acquire NEs because of the constant updating of online data. The purpose of our work is to propose a technique for the extraction of NEs from the Web through the combination of a harvester and shallow parsers. Our study also belongs to corpus-based acquisition of semantic relationships through the analysis of specific lexico-syntactic conte</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>G. Grefenstette. 1994. Explorations in Automatic Thesaurus Discovery. Kluwer Academic Publisher, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Grefenstette</author>
</authors>
<title>The WWW as a resource for example-based MT tasks.</title>
<date>1999</date>
<booktitle>In Proc., ASLIB Translating and the Computer 21 Conference,</booktitle>
<location>London.</location>
<marker>Grefenstette, 1999</marker>
<rawString>G. Grefenstette. 1999. The WWW as a resource for example-based MT tasks. In Proc., ASLIB Translating and the Computer 21 Conference, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Hearst</author>
</authors>
<title>Automated discovery- of WordNet relations.</title>
<date>1998</date>
<booktitle>WordNet: An Electronic Lexical Database.</booktitle>
<editor>In C. Fellbaum, editor,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="1747" citStr="Hearst, 1998" startWordPosition="259" endWordPosition="260">. While traditional electronic corpora such as journal articles or corpus resources (BNC, SUSANNE, Brown corpus) are satisfactory for classical lexical acquisition, Web corpora are another source of knowledge (Crimmins et al., 1999) that can be used to acquire NEs because of the constant updating of online data. The purpose of our work is to propose a technique for the extraction of NEs from the Web through the combination of a harvester and shallow parsers. Our study also belongs to corpus-based acquisition of semantic relationships through the analysis of specific lexico-syntactic contexts (Hearst, 1998) because hypernym relationships are acquired together with NEs. The unique contribution of our technique is to offer an integrated approach to the analysis of HTML documents that associates lexical cues with formatting instructions in a single and cohesive framework. The combination of structural information and linguistic patterns is also found in wrapper induction, an emerging topic of research in artificial intelligence and machine learning (Kushmerick et al., 1997). Our work differs from the MUC-related NE tagging task and its possible extension to name indexing of web pages (Aone et al., </context>
<context position="4599" citStr="Hearst, 1998" startWordPosition="729" endWordPosition="730">university (ORGANIZATION). The need for selective linguistic cues (wrt to the current facilities offered by search engines) and for informative and typifying contexts has led us to focus on collections, a specific type of definitory contexts (Pery-Woodley, 1998). Because they contain specific linguistic triggers such as following or such as, definitory contexts can be accessed through phrase queries to a search engine. In addition, these contexts use the classical scheme genus/differentia to define NEs, and thus provide, through the genus, a hypernym of the NEs they define. Our study extends (Hearst, 1998) to Webbased and spatially formatted corpora. 3 Architecture and Principles To acquire NEs from the Web, we have developed a system that consists of three sequential modules (see Figure 1): 1. A harvester that downloads the pages retrieved by a search engine from the four following query strings (1.a) following (NE) (1.c) (NE) such as (1.b) list of (NE) (1.d) such (NE) as in which (NE) stands for a typifying hypernym of NEs such as Universities, politicians, or car makers (see list in 4). 2. Three parallel shallow parsers Pe, P1 and Pa which extract candidate NEs respectively from enumerations</context>
<context position="20591" citStr="Hearst, 1998" startWordPosition="3340" endWordPosition="3341"> times more NEs obtained from lists than from enumerations through list of NE. The large number of NEs produced from enumerations after this pattern certainly relies on the combination of linguistics and formatting cues in the construction of meaning. The writer avoids using (the word) list when the text is followed by a (physical) list. Lastly, in all, 11 times more NEs are obtained from enumerations than from lists after the pattern NE such as, and 18 times more after such NE as. This shows that the linguistic pattern such as preferably introduces textual enumerations through coordinations (Hearst, 1998). Redundancy. There are two main causes of redundancy in acquisition. A first cause is that the same NE can be acquired from several collections in the same corpus. Redundancy in the fourth lines of the tables is the ratio of duplicates among the yield of candidate NEs for each search engine and each query. This value is relatively stable whatever the search engine or the query pattern. On average, redundancy is 2.09: each candidate is acquired slightly more than two times. Acquisition through NL is slightly more re.dundant (2.18) than through AV (1.92). This difference is not significant sinc</context>
</contexts>
<marker>Hearst, 1998</marker>
<rawString>M.A. Hearst. 1998. Automated discovery- of WordNet relations. In C. Fellbaum, editor, WordNet: An Electronic Lexical Database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Kushmerick</author>
<author>D S Weld</author>
<author>R Doorenbos</author>
</authors>
<title>Wrapper induction for information extraction.</title>
<date>1997</date>
<booktitle>In Proc., L1CAF97,</booktitle>
<pages>729--735</pages>
<location>Nagoya.</location>
<contexts>
<context position="2220" citStr="Kushmerick et al., 1997" startWordPosition="331" endWordPosition="334"> Our study also belongs to corpus-based acquisition of semantic relationships through the analysis of specific lexico-syntactic contexts (Hearst, 1998) because hypernym relationships are acquired together with NEs. The unique contribution of our technique is to offer an integrated approach to the analysis of HTML documents that associates lexical cues with formatting instructions in a single and cohesive framework. The combination of structural information and linguistic patterns is also found in wrapper induction, an emerging topic of research in artificial intelligence and machine learning (Kushmerick et al., 1997). Our work differs from the MUC-related NE tagging task and its possible extension to name indexing of web pages (Aone et al., 1997) for the following reasons: • The purpose of our task is to build lists of NEs, not to tag corpora. For this reason, we only collect non-ambiguous contextindependent NEs; partial or incomplete occurrences such as anaphora are considered as incorrect. • The types of NEs collected here are much more accurate than the four basic types defined in MUC. The proposed technique could be extended to the collection of any non-MUC names which can be grouped under a common hy</context>
</contexts>
<marker>Kushmerick, Weld, Doorenbos, 1997</marker>
<rawString>N. Kushmerick, D.S. Weld, and R. Doorenbos. 1997. Wrapper induction for information extraction. In Proc., L1CAF97, pages 729-735, Nagoya.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-P Pery-Woodley</author>
</authors>
<title>Signalling in written text: a corpus based approach.</title>
<date>1998</date>
<booktitle>In Workshop on Discourse Relations and Discourse Markers at COLING-ALC&apos;98,</booktitle>
<pages>79--85</pages>
<contexts>
<context position="4248" citStr="Pery-Woodley, 1998" startWordPosition="673" endWordPosition="674"> Pe P1 Pa Parsers Enumeration Parser Candidate NEs List and tables Anchor Parser Parser t Initializers and Candidate Candidate NEs Index pages Typed NEs Figure 1: Architecture is not relevant to our purpose). So that automatically acquired NEs can be used in a NE recognition task, they are associated with types such as actor (PERSON), lake (LOCATION), or university (ORGANIZATION). The need for selective linguistic cues (wrt to the current facilities offered by search engines) and for informative and typifying contexts has led us to focus on collections, a specific type of definitory contexts (Pery-Woodley, 1998). Because they contain specific linguistic triggers such as following or such as, definitory contexts can be accessed through phrase queries to a search engine. In addition, these contexts use the classical scheme genus/differentia to define NEs, and thus provide, through the genus, a hypernym of the NEs they define. Our study extends (Hearst, 1998) to Webbased and spatially formatted corpora. 3 Architecture and Principles To acquire NEs from the Web, we have developed a system that consists of three sequential modules (see Figure 1): 1. A harvester that downloads the pages retrieved by a sear</context>
<context position="9800" citStr="Pery-Woodley, 1998" startWordPosition="1540" endWordPosition="1541"> two search engines is intended to evaluate whether a poorer query mode (bags of words in NL instead of strings in AV) can be palliated by accessing more documents (2000 max. for NL instead of 200 max. for AV). The corpus collected by the two search engines and the four families of queries is 2,958Mb large (details are given in Section 4). Acquisition of Candidate NEs Three parallel shallow parsers Pe, P1 and Pa are used to extract NEs from the corpora collected by the harvester. The parsers rely on the query string to detect the sentence introducing the collection of NEs (the initializer in (Pery-Woodley, 1998)). The text and HTML marks after the initializer are parsed jointly in order to retrieve one of the following three spatio-syntactic structures: 1. a textual enumeration (parser Pe, top183 most example in Figure 2), 2. a list or a table (parser P1, the next three examples in Figure 2), 3. an anchor toward a page containing a list (parser Pa, bottom example in Figure 2). In brief, these parsers combine string matching (the initial lexical cue), syntactic analysis (enumerations in Pe), analysis of formatting instructions (lists and tables in P1), and access to linked documents through anchors de</context>
<context position="26248" citStr="Pery-Woodley, 1998" startWordPosition="4323" endWordPosition="4324">f the collection, e.g. a list of 3. gives the type of each item of the collection, e.g. universities 4. specifies the particular characteristics of each item. e.g. universities in Vietnam The cues used by the harvester are elements which either introduce the collection (e.g. the following) or describe the structure (e.g. a list of). In initializers in general, these first 2 functions need not be expressed explicitly by lexical means, as the layout itself indicates the presence and type of the collection. Readers exploit the visual properties of written text to aid the construction of meaning (Pery-Woodley, 1998). However it is necessary to be explicit when defining the items of the collection as this information is not available to the reader via structural properties. Initializers generally contain additional characteristics of the items which provide the differentia (underlined here): This is a list of American companies with business interests in Latvia. This example is the most explicit form an initializer can take as it contains a lexical element which corresponds to each of the four functions outlined above. It is fairly simple to extract the details of the items from initializers with this bas</context>
</contexts>
<marker>Pery-Woodley, 1998</marker>
<rawString>M.-P. Pery-Woodley. 1998. Signalling in written text: a corpus based approach. In Workshop on Discourse Relations and Discourse Markers at COLING-ALC&apos;98, pages 79-85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmid</author>
</authors>
<title>Improvements in part-ofspeech tagging with an application to german.</title>
<date>1999</date>
<booktitle>Natural Language Processing Using Very Large Corpora.</booktitle>
<editor>In S. Armstrong, K.W. Church, P. Isabelle, S. Manzi, E. Tzoukermann, and D. Yarowsld, editors,</editor>
<publisher>Kluwer,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="27261" citStr="Schmid, 1999" startWordPosition="4497" endWordPosition="4498">alizer can take as it contains a lexical element which corresponds to each of the four functions outlined above. It is fairly simple to extract the details of the items from initializers with this basic form, as the modification of the hypernym takes the form of a relative clause, a prepositional phrase or an adjectival phrase. A detailed grammar of this form of initializer is as shown in Figure 3.5 The following is NP PP NP (adj) Npl (PP I rel.c1.) I list of universities in Indonesia: Figure 3: The structure of a basic initializer We tag the collection by part of speech using the TreeTagger (Schmid, 1999). The elements which express the differentia are extracted by means of pattern matching: they are always the modifiers of the plural noun in the string, which is the hypernym of the items of the collection. 5PP = prepositional phrase, Ns = noun (singular), No = noun (plural), VP = verb in present tense, rel.cl. = relative clause. NP (det) (adj) N Initializer V Vp (det) (adj) Ns 188 Initializers containing the search string such as behave somewhat differently. They are syntactically incomplete, and the missing constituent is provided by each item of the collection (Virbel, 1985). These phrases </context>
</contexts>
<marker>Schmid, 1999</marker>
<rawString>H. Schmid. 1999. Improvements in part-ofspeech tagging with an application to german. In S. Armstrong, K.W. Church, P. Isabelle, S. Manzi, E. Tzoukermann, and D. Yarowsld, editors, Natural Language Processing Using Very Large Corpora. Kluwer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Virbel</author>
</authors>
<title>Mise en forme des documents.</title>
<date>1985</date>
<journal>Cahiers de Grammaire,</journal>
<volume>17</volume>
<contexts>
<context position="27845" citStr="Virbel, 1985" startWordPosition="4597" endWordPosition="4598">the TreeTagger (Schmid, 1999). The elements which express the differentia are extracted by means of pattern matching: they are always the modifiers of the plural noun in the string, which is the hypernym of the items of the collection. 5PP = prepositional phrase, Ns = noun (singular), No = noun (plural), VP = verb in present tense, rel.cl. = relative clause. NP (det) (adj) N Initializer V Vp (det) (adj) Ns 188 Initializers containing the search string such as behave somewhat differently. They are syntactically incomplete, and the missing constituent is provided by each item of the collection (Virbel, 1985). These phrases vary considerably in structure and can require relatively complex syntactic rearrangement to extract the properties of the hypernym. We will not discuss these in more detail here. One type of error in this system occurs when a paragraph containing the search string is followed by an unrelated list. For example the harvester recognizes Ask the long list of American companies who have unsuccessfully marketed products in Japan. as an initializer when in fact it is not related to any collection. If it happened to be followed on the page by an collection of any kind the system would</context>
</contexts>
<marker>Virbel, 1985</marker>
<rawString>J. Virbel. 1985. Mise en forme des documents. Cahiers de Grammaire, 17.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>