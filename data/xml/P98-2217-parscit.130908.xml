<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.959265">
A tabular interpretation of a class of 2-Stack Automata
</title>
<note confidence="0.536441666666667">
Eric Villemonte de la Clergerie
INRIA - Rocquencourt - B.P. 105
78153 Le Chesnay Cedex, FRANCE
</note>
<email confidence="0.794232">
Eric.De_La_Clergerie@inria.fr
</email>
<note confidence="0.64407125">
Miguel Alonso Pardo
Universidad de La Coruna
Campus de Elviiia s/n
15071 La Coruiia, SPAIN
</note>
<email confidence="0.627499">
alonsoOdc.fi.udc.es
</email>
<sectionHeader confidence="0.985986" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99986225">
The paper presents a tabular interpretation for a
kind of 2-Stack Automata. These automata may be
used to describe various parsing strategies, ranging
from purely top-down to purely bottom-up, for LIGs
and TAGs. The tabular interpretation ensures, for
all strategies, a time complexity in 0(n6) and space
complexity in 0(n5) where n is the length of the
input string.
</bodyText>
<sectionHeader confidence="0.958177" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.999959404761905">
2-Stack automata [2SA] have been identified as pos-
sible operational devices to describe parsing strate-
gies for Linear Indexed Grammars [LIG] or Tree Ad-
joining Grammars [TAG] (mirroring the traditional
use of Push-Down Automata [PDA] for Context-
Free Grammars [CFG]). Different variants of 2SA
(or not so distant Embedded Push-Down Automata)
have been proposed, some to describe top-down
strategies (Vijay-Shanker, 1988; Becker, 1994), some
to describe bottom-up strategies (Rambow, 1994;
Nederhof, 1998; Alonso Pardo et al., 1997), but none
(that we know) that are able to describe both kinds
of strategies.
The same dichotomy also exists in the different
tabular algorithms that has been proposed for spe-
cific parsing strategies with complexity ranging from
0(70) for bottom-up strategies to 0(n9) for prefix-
valid top-down strategies (with the exception of a
0(n6) tabular interpretation of a prefix-valid hybrid
strategy (Nederhof, 1997)). It must also be noted
that the different tabular algorithms may be diffi-
cult to understand and it is often unclear to know if
the algorithms still hold for different strategies.
This paper overcomes these problems by (a) in-
troducing strongly-driven SA [SD-2SAI that may
be used to describe parsing strategies for TAGs
and LIGs, ranging from purely top-down to purely
bottom-up, and (b) presenting a tabular interpre-
tation of these automata in time complexity 0(n6)
and space complexity 0(0).
The tabular interpretation follows the principles
of Dynamic Programming: the derivations are bro-
ken into elementary sub-derivations that may (a) be
combined in different contexts to retrieve all possi-
ble derivations and (b) be represented in a compact
way by items, allowing tabulation.
The strongly-driven 2SA are introduced and moti-
vated in Section 1. We illustrate in Sections 2 and 3
their power by describing several parsing strategies
for LIGs and TAGs. Items are presented in Sec-
tion 4. Section 5 lists the rules to combine items and
transitions and establishes correctness theorems.
</bodyText>
<sectionHeader confidence="0.89393" genericHeader="method">
1 Strongly-driven 2-Stack Automata
</sectionHeader>
<bodyText confidence="0.991931375">
2SA are natural extensions of Push-Down Automata
working on a pair of stacks. However, it is known
that unrestricted 2SA have the power of a Turing
Machine. The remedy is to consider asymmetric
stacks, one being the Master Stack MS where most
of the work is done and the other being the Auxiliary
Stack AS mainly used for restricted &amp;quot;bookkeeping&amp;quot;.
The following remarks are intended to give an idea
of the restrictions we want to enforce. The first ones
are rather standard and may be found under differ-
ent forms in the literature. The last one justifies the
qualification of &amp;quot;strongly-driven&amp;quot; for our automata.
[Session] AS should actually be seen as a stack of
session stacks, each one being associated to a
session. Only the topmost session stack may
be consulted or modified. This idea is closely
related to the notion of Embedded Push-Down
Automata (Rambow, 1994, 96-102).
[Linearity] A session starts in mode write w and
switches at some point in mode erase e. In
mode w (resp. e), no element can be popped
from (resp. pushed to) the master stack MS.
Switching back from e to w is not allowed. This
requirement is related to linearity because it
means that a same session stack is never used
twice by &amp;quot;descendants&amp;quot; of an element in MS.
[Soft Session Exit] Exiting a session is only possi-
ble when reaching back, with an empty session
stack and in mode erase, the MS element that
initiated the session.
[Driving] Each pushing on MS done in write mode
leaves some mark in MS about the action that
</bodyText>
<page confidence="0.970416">
1333
</page>
<figure confidence="0.999296">
Write Mode
I I I I I I I I I I I -
Master stack
Auxiliary stack
</figure>
<figureCaption confidence="0.999997">
Figure 1: Representation of transitions and derivations
</figureCaption>
<bodyText confidence="0.999086578947369">
took place on the session stack. The popping
of this mark (in erase mode) will guide which
action should take place on the session stack.
In other words, we want the erasing actions to
faithfully retrace the writing actions.
Formally, a SD-2SA A is specified by a tuple
(E, M, X,$0,Sf, 0) where E denotes the finite set of
terminals, M the finite set of master stack elements
and X the finite set of auxiliary stack elements. The
mit symbol $0 and final symbol $f are distinguished
elements of M. 0 is a finite set of transitions.
The master stack MS is a word in (DM)* where
D denotes the set {/, \„ -4, of action marks
used to remember which action (w.r.t. the auxiliary
stack AS) takes place when pushing the next master
stack element. The empty master stack is noted c
and a non-empty master stack 61A1 where
An denotes the topmost element.
The meaning of the action marks is:
</bodyText>
<listItem confidence="0.87903075">
/ Pushing of an element on AS.
\ Popping of the topmost element of AS.
No action on AS.
• Creation of a new session (with a new empty
</listItem>
<bodyText confidence="0.983637043478261">
session stack on AS).
The auxiliary stack AS is a word of (/CX*)* where
= v1=e}is a set of two elements used to
delimit session stacks in AS. Delimiter Wv (resp.
e) is used to start a new session from a session
which is in writing (resp. erasing) mode. The empty
auxiliary stack is noted e.
Given some input string x1 ... x1 E E*, a configu-
ration of A is a tuple (m, u, 6) where m E {w, e}
denotes a mode (writing or erasing), u a string posi-
tion in [0, f], E the master stack and e the auxiliary
stack. Modes are ordered by w e to capture the
fact that no switching from e to w is possible. The
initial configuration of A is (w, 0, I&apos;v) and the
final one (e, f,
A transition is given as a pair (p, E, 6) (q, 0,9)
where p,q are modes (or, with some abuse, variables
ranging over modes), z in E*, E. and 0 suffixes of
master stacks in M(VM)*, and ,9 suffixes of aux-
iliary stacks in X*(iCX*)* = (Xu/C)*. Such a transi-
tion applies on any configuration (p, u, W,//)6) such
that x,a+i xv = z and returns (q,v , ,
We restrict the kind of allowed transitions:
</bodyText>
<equation confidence="0.835399666666667">
SWAP (p, A, 6) (q, B, 6) with p q and either
6 E IC (&amp;quot;session bottom check&amp;quot;) or 6 = E (&amp;quot;no
AS consultation&amp;quot;) .
</equation>
<construct confidence="0.75520025">
/-WRITE (w, A, e) 1-2-4 (w,A/B,b)
/-ERASE (e, A/ B , a) (e, D,E)
-0-WRITE (w, A, e) (w, A--+B ,E)
-0-ERASE (e, A- e) 1-14 (e, C, e)
(m, A, e) (w, AB
-ERASE (e, 12-+ (m,C,E)
\-WRITE (w, A, a) 1--z-■ (w , A\ B , E)
\-ERASE (e, A\ B ,E) 1-L■ (e,C,c)
</construct>
<bodyText confidence="0.991955863636364">
Figure 1 graphically outlines the different kinds
of transitions using a 2D representation where the
X-axis (Y-axis) is related to the master (resp. aux-
iliary) stack. Figure 1 also shows the two forms of
derivations we encounter (during a same session).
2 Using 2SA to parse LIGs
Indexed Grammars (Aho, 1968) are an extension of
Context-free Grammars in which a stack of indices
is associated with each non-terminal symbol. Linear
Indexed Grammars (Gazdar, 1987) are a restricted
form of Indexed Grammars in which the index stack
of at most one body non-terminal (the child) is re-
lated with the stack of the head non-terminal (the
father). The other stacks of the production must
have a bounded stack size.
Formally, a LIG G is a 5-tuple (VT ,VN P)
where VT is a finite set of terminals, VN is a finite
set of non-terminals, S E VN is the start symbol,
V1 is a finite set of indices and P is a finite set of
productions. Following (Gazdar, 1987) we consider
productions in which at most one element can be
pushed on or popped from a stack of indices:
</bodyText>
<page confidence="0.980638">
1334
</page>
<figure confidence="0.96833775">
[Terminal] Ak,o[] ak where ak E VT U {e},
[POP] Ak,o[00] Ak ,i[l • • • Ak,d[00-Y] • • • Ak,nA.[]
[PUSH] Ak,0[00-y] Ak,1[] • • • Ak,d[00]. • • Ak,n([1
[H011] Ak,o [00] Ak,1H . • • Ak,d[°01 • • • Ak,ni.1]
</figure>
<bodyText confidence="0.99020775">
To each production k of type PUSH, POP or
HOR, we associate a characteristic tuple t(k) =
(d, (5, a, )3) where d is the position of the child and
the other arguments given by the following table:
</bodyText>
<table confidence="0.555034">
Type 5 a 0
PUSH / E -y
POP \ 7 c
HOR
</table>
<bodyText confidence="0.876181">
We introduce symbols Vrk,., as a shortcut for dotted
</bodyText>
<subsubsectionHeader confidence="0.932751">
productions [Ak,o—)Ak,i Ak,t • Ak,t+i • .. Ak,n&amp;L
</subsubsectionHeader>
<bodyText confidence="0.984174142857143">
In order to design a broad class of parsing strate-
gies ranging from pure top-down to pure bottom-up,
we parameterize the automaton to be presented by
a call projection from V to V&amp;quot;11 and a return
projection from V to Vret where V = VN U
and v&amp;quot;11 and Vret are two sets of elements. We re-
quire Vali n V&amp;quot;&apos;= 0 and (&amp;quot;, &apos;) to be invertible,
</bodyText>
<equation confidence="0.992515">
i.eVX,YE V, ) = (r, x Y
</equation>
<bodyText confidence="0.999623833333333">
The projections extend to sequences by taking
X?, and -e = e (similarly for &amp;quot;).
Given a LIG G and a choice of projections, we
define the 2SA A(G , ,&apos;) = (VT, M, X ,+:57 , 0)
where .A4 = {VkAUVTUV7-, X = V;U, and whose
transitions are built using the following rules.
</bodyText>
<listItem confidence="0.837015692307692">
• Call/Return of a non child
CALL: (in, Vk,i, f) (WI Vk,iHAk,i+11
RET: (e, 7 771) (m, f)
• Call/Return of a child for t(k) = (i+ 1,6,a,8).
CALL(5) : (w, Vk,„ (w, Vk,t6Ak,i+i,
RET(5) : (e, Vk,ibAk,i+1,4/(7) (e, +a-)
• Production Selection
SEL : (w, Ak,O) f) 1&amp;quot;-- (W7 Vk,0 7 f)
• Production Publishing
Ak,07
PUB : (e, Vk,nk e) (e, e)
• Scanning (for terminal productions)
SCAN: (w, Ak,01 Hm), Ak,07
</listItem>
<bodyText confidence="0.998708375">
The reader may easily check that A(G, &apos;)
recognizes L(G). The choice of the call and return
elements for the MS and &apos;AZ) and the AS
(7 y4 and &apos;71) defines a parsing strategy, by controlling
how information flow between the phases of predic-
tion and propagation. The following table lists the
choices corresponding to the main parsing strategies
(but others are definable).
</bodyText>
<table confidence="0.99805725">
Strategy 74+ A
Top-Down A 1 -y 1
Bottom-Up 1 A&apos; 1 7
Earley A A&apos; -y
</table>
<bodyText confidence="0.90352775">
It is also worth to note that the descrip-
tion of A(G,&apos;,&apos;) could be simplified. In-
deed, for every configuration (Tri, u,E,) deriv-
able with A(G,&apos;,&apos;), we can show that E =
Hvki,ei 81 • • • Vk,,,t„ 871X , and that öt only depends
on Vki,„ . That means that we could use a master
stack without action marks, these marks being im-
plicitly given by the elements 77k,t•
</bodyText>
<sectionHeader confidence="0.856138" genericHeader="method">
3 Using 2SA to parse TAGs
</sectionHeader>
<bodyText confidence="0.9997722">
Tree Adjoining Grammars are a extension of CFG
introduced by Joshi in (Joshi, 1987) that use
trees instead of productions as primary represent-
ing structure. Formally, a TAG is a 5-tuple g
(VN,VT, .5, I, A), where VN is a finite set of non-
terminal symbols, VT a finite set of terminal sym-
bols, S the axiom of the grammar, I a finite set of
initial trees and A a finite set of auxiliary trees. I U A
is the set of elementary trees. Internal nodes are la-
beled by non-terminals and leaf nodes by terminals
or E, except for exactly one leaf per auxiliary tree
(the foot) which is labeled by the same non-terminal
used as label of its root node.
New trees are derived by adjoining: let be a a
tree containing a node v labeled by A and let be
an auxiliary tree whose root and foot nodes are
also labeled by A. Then, the adjoining of ,3 at the
adjunction node v is obtained by excising the subtree
av of a with root v, attaching f3 to v and attaching
the excised subtree to the foot of l3 (See Fig. 2).
</bodyText>
<figureCaption confidence="0.996097">
Figure 2: Traversal of an adjunction
</figureCaption>
<bodyText confidence="0.993483333333333">
An elementary tree a may be represented by a
set P(a) of context free productions, each one being
either of the form
</bodyText>
<listItem confidence="0.9331725">
• vk,0 vk,i , where vkoa denotes some
non-leaf node k of a and vk,, the ith son of k.
</listItem>
<page confidence="0.725744">
1335
</page>
<listItem confidence="0.969115">
• vk,o ak, where vk,o denotes some leaf node k
of a with terminal label ak •
</listItem>
<bodyText confidence="0.973449142857143">
As done for LIGs, we introduce symbols Vo
to denote dotted productions and consider pro-
jections and to define the narameterized
2SA A(G,&apos;,&apos;) = (VT, .M, .A4, vo,o, vo,o, 0) where
M = {V} U {vZi} U .Z01. The transitions are
given by the following rules (and illustrated in Fig-
ure 2).
</bodyText>
<listItem confidence="0.858746862068965">
• Call / Return for a node not on a spine. The
call starts a new session, exited at return.
CALL: (771, E) (w,
RET: (e, Wn) (M, Vk,i+17 6)
• Call / Return for a node vk,i+i on a spine.
The adjunction stack is propagated un-modified
along the spine.
SCALL: (w, E) (W, E)
SRET : (e, Vk,i-41/k,i+17 e) (e, f)
• Call / Return for an adjunction on node Vk,O•
The computation is diverted to parse some ac-
ceptable auxiliary tree 0 (with root node ro),
and a continuation point is stored on the auxil-
iary stack.
ACALL : (w, vk,o, E) 1-0 (w, vk,0 /773 Vk,o)
ARET : (e, VIc,0/7 f31 (e,vk,o,e)
• Call / Return for a foot node fp. The continu-
ation stored by the adjunction is used to parse
the excised subtree.
FCALL : (w, (w, 7;\A,c)
FRET: (e, J\ A, c) (e, b, A)
Note: These two transitions use a variable A
over M. This is a slight extension of 2SA that
preserves correctness and complexity.
• Production Selection
SEL:
• Production Publishing
PUB: (M7 Vk,nk E) e, Vk,0 f)
• Scanning
</listItem>
<bodyText confidence="0.9721358">
SCAN: (w, fli)F(14-&amp;quot;■(e, vi+-707), Wn)
Different parsing strategies can be obtained by
choosing the call (vk,i) and return () elements:
S t r at e gy v 4-
v
prefix-valid Top-Down v 1
Bottom-Up 1 v&apos;
prefix-valid Earley v v&apos;
Non prefix-valid variants of the top-down and
Earley-like strategies can also be defined, by tak-
ing77&apos;3 = J._ and = ro for every root node rp of
an auxiliary tree 0 (the projections being unmodi-
fied on the other elements). In other words, we get
a full prediction on the context-free backbone of G
but no prediction on the adjunctions.
</bodyText>
<sectionHeader confidence="0.993406" genericHeader="method">
4 Items
</sectionHeader>
<bodyText confidence="0.999846789473684">
We identify two kinds of elementary deriva-
tions, namely Context-Free [CF] and escaped
Context-Free [xCF] derivations, respectively rep-
resented by CF and xCF items. An item keeps the
pertinent information relative to a derivation, which
allows to apply the sequence of transitions associ-
ated with the derivation in different contexts.
Before presenting these items, we introduce the
following classification about derivations.
A derivation (p,u,A,e)1--17 (q,v,0,0) is said
rightward if no element of E is accessed (even for
consultation) during the derivation and if A is only
consulted. Then EA is a prefix of 0.
Similarly, a derivation (p,u,E, e)ji- (q, v, 0,0) is
said upward if no element of e is accessed (even for
consultation). Then e is a prefix of 0.
We also note w[q/p] the prefix substitution of p by
q for all words w, p, q on some vocabulary such that
p is prefix of w.
</bodyText>
<subsectionHeader confidence="0.966163">
4.1 Context-Free Derivations
</subsectionHeader>
<bodyText confidence="0.999934">
A Context-Free [CF] derivation only depends on
the topmost element A of the initial stack MS. That
means that no element of the initial AS and no ele-
ment of MS below element A is needed:
</bodyText>
<equation confidence="0.635153">
(o, u, EA, e)l-- (w, v, OB, 09)1 (m, w, ()BSC, ec)
d2
</equation>
<bodyText confidence="0.995762">
where
</bodyText>
<listItem confidence="0.99879225">
• d1 and di d2 are both rightward and upward.
• d2 is rightward.
• either (6 o = w, and c E X) or
(6 = and c =
</listItem>
<bodyText confidence="0.898842">
For such a derivation, we have:
</bodyText>
<equation confidence="0.926953666666667">
Proposition 4.1 For all prefix stacks e ,
(o, u, E&apos;A, e) 1* (w, v,O&apos;B4O&apos;)
(m, w, O&apos;BSC, Cc)
</equation>
<bodyText confidence="0.900036333333333">
where 0&apos; = 0[E,&apos;/E] and 0&apos; = g].
The proposition suggests representing the CF
derivation by a CF item of the form
AB6Cm
where A = (u, A) and B = (v, B) are micro config-
urations and C = (w,C,c) a mini configuration.
</bodyText>
<equation confidence="0.916646">
(lAr; e) (w7 Vk,0 e)
</equation>
<page confidence="0.985342">
1336
</page>
<figureCaption confidence="0.99716">
Figure 3: Items Shapes
</figureCaption>
<figure confidence="0.997599583333333">
•C
A
A
CF(/) or CF() Item
CF(\) Item
CF(—o) Item
efTb
A
xCF(—) Item
I I I I I 1 I 1 I
xCF(/) ItemE xCF(\) Item
1 1 1 1 1 1 1 1 1 I 1 1
</figure>
<subsectionHeader confidence="0.570776">
4.2 Escaped Context-Free Derivations
</subsectionHeader>
<bodyText confidence="0.770666375">
An escaped Context-Free [xCF] derivation is al-
most a CF derivation, except for an escape sub-
derivation that accesses deep elements of AS.
(w,v,OB4O)
(w,s,(DD,d)
(e, t, D\E, 0)
(e, w, OB6C , 0c)
where
</bodyText>
<listItem confidence="0.99755875">
• d1 and d1d2 are both rightward and upward.
• d2 and dx are rightward.
• d3 is upward.
• 6 0 and d,c E X.
</listItem>
<construct confidence="0.7230465">
Proposition 4.2 For all prefix stacks E&apos; and e&apos; ,
stack 4)&apos;, and rightward derivation
</construct>
<equation confidence="0.919363714285714">
(w, s, D, e d)j-- (e,t,V D\E, 0&apos;)
de
where = 4.[E&apos;/EI, we have
(w, v, ▪ 14B , 0[e I e])
(w,s, 4&gt; [E.&apos; /E]D,ed)
(e,t,(1)[:€11,E-..]D\E, 0&apos;)
(e, w, 0 ▪ 74B6C, O&apos;c)
</equation>
<bodyText confidence="0.9522559">
The proposition suggests representing the xCF
derivation by an xCF item of the form
ABS[DE]be
where A = (u, A), B (v, B), D = (s, D, d), E =
(t,E) and C = (w, C, c).
In order to homogenize notations, we also use
the alternate notation ABSP&gt;oiCm, to represent CF
item AB8Cm, introducing a dummy symbol o.
The specific forms taken by CF and xCF items for
the different actions 6 are outlined in Figure 3.
</bodyText>
<sectionHeader confidence="0.73946" genericHeader="method">
5 Combining items and transitions
</sectionHeader>
<bodyText confidence="0.8911765">
We provide the rules to combine items and transi-
tions in order to retrieve all possible 25A derivations.
These rules do not explicit the scanning con-
straints and suppose that the string z may be read
between positions w and k of the input string. They
use holes * to denote slots that not need be con-
sulted. For any mini configuration A = (u, A, a), we
note A° = (u, A) its micro projection.
[—p—WRITE] r = (w, C, e) 1-2-4 (w, E)
A**[oo]Cw AC°--qoorPw (1)
where C = (w, C, c), and P = (k,F,c).
[/—WRITE] r = (w, C,c)i-Lo (w,C /F, f)
</bodyText>
<equation confidence="0.479056">
A,1/4-k[oo]Cw = °C°/[oc]Fw (2)
where C = (w, C, c), and F = (k,F, f).
[—WRITE] T = (171, C e) 12—* (w,CF,Wn)
Ald,[oo]Crn C°C° [.0&lt;&gt;1PW
</equation>
<bodyText confidence="0.6153305">
where C = (w, C, c), and P = (k,F, Hm)
[\—WRITE] r = (NV C C) 1-3—■ (w,C\F,c)
</bodyText>
<figure confidence="0.5853265">
A° **[&amp;quot;]Cw Mb° \[*°}Fw
M*-k[oolAw
</figure>
<bodyText confidence="0.6563955">
where C = (w,C,c), A = (u, A, a), and P =
(k, F, a).
</bodyText>
<equation confidence="0.7017345">
[—i—ERASE] r = (e, B--+C, c) (e,F,c)
A° B° --[.DE]Cel
A MA[DElPe (5)
A°MA[ooli3w
</equation>
<bodyText confidence="0.9896125">
where C = (w, C, c), f3 = (v, B,b), F = (k,F,c),
and (when D o) D = (s,D,b).
</bodyText>
<table confidence="0.2493115">
(w, u, EA, 4&amp;quot;)
(w, u, &apos;A,
</table>
<page confidence="0.426159">
1337
</page>
<figure confidence="0.790495421052632">
[ —ERASE] T = (e, B\C, c) 12-4 (e, F, f)
A°A[}M =4 M°011,[LIC°]Fe (6)
Op,[ocif3w
where C = (w, C, c), B = (v, B ,b), M =
(1, M,m), P = (k,F, f), and (when D o)
=
[—ERASE] r = (e, (m, F, e)
13° B[oo]Ce
MNA[DE]Bm} MNA[D.E]Fm (7)
where C = (w, C, km), B = (v, B ,b), and P =
(k, F, a)
[/—ERASE] T = (e,B/C,c) (e, F, e)
B° /[oo]Ce M N A[00] Fe (8)
MNA[oojBw
where C = (w, C, c), B = (v, B , b), and P
(k,F,b)
13° B° /[D.E1Ce
M 1 V A[oo]Bw MNA[011Fe
MD°\[0.11Ee
</figure>
<bodyText confidence="0.977281333333333">
where -6&apos; = (w, C, c), B = (v, B ,b), P = (k,F,b),
and (when 0 o) 0 = (1, 0,b).
[SWAP] T = (p, C,) (q, F,
AB(5[D.E]Cm ABS[13E]Pm (10)
where C = (w, C, c), F = (k,F,c), and either
c = W&apos; or = c.
The best way to apprehend these rules is to vi-
sualize them graphically as done for the two most
complex ones (Rules 6 and 9) in Figures 4 and 5.
</bodyText>
<figureCaption confidence="0.999997">
Figure 4: Application of Rule 6
Figure 5: Application of Rule 9
</figureCaption>
<subsectionHeader confidence="0.996416">
5.1 Reducing the complexity
</subsectionHeader>
<bodyText confidence="0.9999354">
An analysis of the time complexity to apply each rule
gives us polynomial complexities 0(nu) with u &lt; 6
except for Rule 9 where u = 8. However, by adapt-
ing an idea from (Nederhof, 1997), we replace Rule 9
by the alternate and equivalent Rule 11.
</bodyText>
<figure confidence="0.929015">
.13°*/[DET-Ce
*.1-3°\[-OP]Ee
MNA[OP]Fe (11)
MNA[oo]Bw
M*\[-OP]ire
</figure>
<bodyText confidence="0.9914906">
where C = (w, C, c), B = (v, B , b), F = (k,F,b),
and (when 0 0 o) 0 = (1,0,b).
Rule 11 has same complexity than Rule 9, but may
actually be split into two rules of lesser complex-
ity 0(n6), introducing an intermediary pseudo-item
BB /[[0.11Ce (intuitively assimilable to a &amp;quot;deeply
escaped&amp;quot; CF derivation).
Rule 12 collects these pseudo-items (indepen-
dently from any transition) while Rule 13 combines
them with items (given a /—ERASE transition r).
</bodyText>
<equation confidence="0.5197965">
BB/[Dr]Ce}BB/rOPliCe (12)
*D°\[OP]Ee
MNA[oo]Bw MNA[011Fe (13)
Mir\[0.11,ke
</equation>
<bodyText confidence="0.990117333333333">
where C= (w, C, c), B = (v,B,b), F = (k,F,b),
and (when 0 o) 0 =
Theorem 5.1 The worst time complexity of the ap-
plication rules (1,2,3,4,5,6,7,8,10,12,13) is 0(n6)
where n is the length of the input string. The worst
space complexity is 0(n5).
</bodyText>
<subsectionHeader confidence="0.999369">
5.2 Correctness results
</subsectionHeader>
<bodyText confidence="0.6933104">
Two main theorems establish the correctness of
derivable items w.r.t. derivable configurations.
A derivable item is either the initial item Or
an item resulting from the application of a combi-
nation rules on derivable items. The initial item
(0, e)(0, c)H[00] (0, so, Wv)w stands for the virtual
derivation step (w, 0,i, C)I- (NV) 03 $r1 WV)*
Theorem 5.2 (Soundness) For every derivable
item I = AB6[DE]Cm, there exists a derivation
on configurations
V
such that 14It V is a CF or xCF derivation repre-
sentable by I.
Proof: By induction on the item derivation length
and by case analysis. II
</bodyText>
<figure confidence="0.550499">
(9)
</figure>
<page confidence="0.954373">
1338
</page>
<construct confidence="0.821648">
Theorem 5.3 (Completeness) For all derivable
configuration (m,w,EC ,Cc), there exists a derivable
item A.1361.13E1Cm such that C = (w,C,c).
</construct>
<bodyText confidence="0.87384675">
Proof: By induction on the configuration deriva-
tion length and by case analysis of the different ap-
plication rules. We also need the following &amp;quot;Extrac-
tion Lemma&amp;quot;. g
</bodyText>
<construct confidence="0.48281675">
Proposition 5.1 From any derivation
(0, 01—* (m, w ,EC , Cc)
may be extracted a suffix CF or xCF sub-derivation
tir± (m,w,E---C,Cc) for some configuration U.
</construct>
<subsectionHeader confidence="0.9036">
5.3 Illustration
</subsectionHeader>
<bodyText confidence="0.995724888888889">
In the context of TAG parsing (Sect. 3), we can
provide some intuition of the items that are built
with A(G,&apos;,&apos;), using some characteristic points
encountered during the traversal of an adjunction
(Fig. 6).
after CALL before RET
on ADJ il1A1/[oo]Riw A1it1/[F1A4]R2e
on SPINE Ai Si -4[oolFi w Ai Si -[Fi A4]F2e
on FOOT Bi Pi \ [0.01A3w Bi Fi \,[Gi B4]Aie
</bodyText>
<figureCaption confidence="0.997256">
Figure 6: Adjunction and Items
</figureCaption>
<sectionHeader confidence="0.999473" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999969739130435">
This paper unifies different results about TAGs and
LIGs in an uniform setting and illustrates the ad-
vantages of a clear distinction between the use of
an operational device and the evaluation of this de-
vice. The operational device (here SD-2SA) helps us
to focus on the description of parsing strategies (for
LIGs and TAGs), while, independently, we design an
efficient evaluation mechanism for this device (here
tabular interpretation with complexity 0(n6)).
Besides illustrating a methodology, we believe our
approach also opens new axes of research.
For instance, even if the tabular interpretation
we have presented has (we believe) the best possi-
ble complexity, it is still possible (using techniques
outside the scope of this paper, (Barthelemy and
Villemonte de la Clergerie, 1996)) to improve its ef-
ficiency by refining what information should be kept
in each kind of items (hence increasing computation
sharing and reducing the number of items).
To handle TAGs or LIGs with attributes, we also
plan to extend SD-2SA to deal with first-order terms
(rather than just symbols) using unification to apply
transitions and subsumption to check items.
</bodyText>
<sectionHeader confidence="0.999434" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999886975609756">
Alfred V. Aho. 1968. Indexed grammars — an ex-
tension of context-free grammars. Journal of the
ACM, 15(4):647-671, October.
Miguel Angel Alonso Pardo, Eric de la Clergerie,
and Manuel Vilares Ferro. 1997. Automata-based
parsing in dynamic programming for Linear In-
dexed Grammars. In A. S. Narin&apos;yani, editor,
Proc. of DIALOGUE&apos;97 Computational Linguis-
tics and its Applications International Workshop,
pages 22-27, Moscow, Russia, June.
F. P. Barthelemy and E. Villemonte de la Clergerie.
1996. Information flow in tabular interpretations
for generalized push-down automata. To appear
in journal of TCS.
Tilman Becker. 1994. A new automaton model
for TAGs: 2-SA. Computational Intelligence,
10(4):422-430.
Gerald Gazdar. 1987. Applicability of indexed
grammars to natural languages. In U. Reyle and
C. Rohrer, editors, Natural Language Parsing and
Linguistic Theories, pages 69-94. D. Reidel Pub-
lishing Company.
Aravind K. Joshi. 1987. An introduction to tree
adjoining grammars. In Alexis Manaster-Ramer,
editor, Mathematics of Language, pages 87-
115. John Benjamins Publishing Co., Amster-
dam/Philadelphia.
Mark-Jan Nederhof. 1997. Solving the correct-
prefix property for TAGs. In T. Becker and H.-V.
Krieger, editors, Proc. of MOL &apos;97, pages 124-130,
Schloss Dagstuhl, Germany, August.
Mark-Jan Nederhof. 1998. Linear indexed automata
and tabulation of TAG parsing. In Proc. of First
Workshop on Tabulation in Parsing and Deduc-
tion (TAPD&apos;98), pages 1-9, Paris, France, April.
Owen Rambow. 1994. Formal and Computational
Aspects of Natural Language Syntax. Ph.D. thesis,
University of Pennsylvania.
K. Vijay-Shanker. 1988. A Study of Tree Adjoining
Grammars. Ph.D. thesis, University of Pennsyl-
vania, January.
</reference>
<page confidence="0.995874">
1339
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.274271">
<title confidence="0.972512">A tabular interpretation of a class of 2-Stack Automata</title>
<author confidence="0.967909">Eric Villemonte de_la Clergerie</author>
<affiliation confidence="0.528056">105</affiliation>
<address confidence="0.892346">Chesnay Cedex, FRANCE</address>
<email confidence="0.925353">Eric.De_La_Clergerie@inria.fr</email>
<author confidence="0.999361">Miguel Alonso Pardo</author>
<affiliation confidence="0.992123">Universidad de La Coruna</affiliation>
<address confidence="0.796539">Campus de Elviiia s/n 15071 La Coruiia, SPAIN</address>
<email confidence="0.984279">alonsoOdc.fi.udc.es</email>
<abstract confidence="0.999316">The paper presents a tabular interpretation for a kind of 2-Stack Automata. These automata may be used to describe various parsing strategies, ranging from purely top-down to purely bottom-up, for LIGs and TAGs. The tabular interpretation ensures, for strategies, a time complexity in space in where n is the length of the input string.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
</authors>
<title>Indexed grammars — an extension of context-free grammars.</title>
<date>1968</date>
<journal>Journal of the ACM,</journal>
<pages>15--4</pages>
<contexts>
<context position="7101" citStr="Aho, 1968" startWordPosition="1246" endWordPosition="1247">ssion bottom check&amp;quot;) or 6 = E (&amp;quot;no AS consultation&amp;quot;) . /-WRITE (w, A, e) 1-2-4 (w,A/B,b) /-ERASE (e, A/ B , a) (e, D,E) -0-WRITE (w, A, e) (w, A--+B ,E) -0-ERASE (e, A- e) 1-14 (e, C, e) (m, A, e) (w, AB -ERASE (e, 12-+ (m,C,E) \-WRITE (w, A, a) 1--z-■ (w , A\ B , E) \-ERASE (e, A\ B ,E) 1-L■ (e,C,c) Figure 1 graphically outlines the different kinds of transitions using a 2D representation where the X-axis (Y-axis) is related to the master (resp. auxiliary) stack. Figure 1 also shows the two forms of derivations we encounter (during a same session). 2 Using 2SA to parse LIGs Indexed Grammars (Aho, 1968) are an extension of Context-free Grammars in which a stack of indices is associated with each non-terminal symbol. Linear Indexed Grammars (Gazdar, 1987) are a restricted form of Indexed Grammars in which the index stack of at most one body non-terminal (the child) is related with the stack of the head non-terminal (the father). The other stacks of the production must have a bounded stack size. Formally, a LIG G is a 5-tuple (VT ,VN P) where VT is a finite set of terminals, VN is a finite set of non-terminals, S E VN is the start symbol, V1 is a finite set of indices and P is a finite set of </context>
</contexts>
<marker>Aho, 1968</marker>
<rawString>Alfred V. Aho. 1968. Indexed grammars — an extension of context-free grammars. Journal of the ACM, 15(4):647-671, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miguel Angel Alonso Pardo</author>
</authors>
<title>Eric de la Clergerie, and Manuel Vilares Ferro.</title>
<date>1997</date>
<booktitle>Proc. of DIALOGUE&apos;97 Computational Linguistics and its Applications International Workshop,</booktitle>
<pages>22--27</pages>
<editor>In A. S. Narin&apos;yani, editor,</editor>
<location>Moscow, Russia,</location>
<marker>Pardo, 1997</marker>
<rawString>Miguel Angel Alonso Pardo, Eric de la Clergerie, and Manuel Vilares Ferro. 1997. Automata-based parsing in dynamic programming for Linear Indexed Grammars. In A. S. Narin&apos;yani, editor, Proc. of DIALOGUE&apos;97 Computational Linguistics and its Applications International Workshop, pages 22-27, Moscow, Russia, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F P Barthelemy</author>
<author>E Villemonte de la Clergerie</author>
</authors>
<title>Information flow in tabular interpretations for generalized push-down automata.</title>
<date>1996</date>
<note>To appear in journal of TCS.</note>
<marker>Barthelemy, Clergerie, 1996</marker>
<rawString>F. P. Barthelemy and E. Villemonte de la Clergerie. 1996. Information flow in tabular interpretations for generalized push-down automata. To appear in journal of TCS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tilman Becker</author>
</authors>
<title>A new automaton model for TAGs: 2-SA.</title>
<date>1994</date>
<journal>Computational Intelligence,</journal>
<pages>10--4</pages>
<contexts>
<context position="1114" citStr="Becker, 1994" startWordPosition="164" endWordPosition="165">s and TAGs. The tabular interpretation ensures, for all strategies, a time complexity in 0(n6) and space complexity in 0(n5) where n is the length of the input string. Introduction 2-Stack automata [2SA] have been identified as possible operational devices to describe parsing strategies for Linear Indexed Grammars [LIG] or Tree Adjoining Grammars [TAG] (mirroring the traditional use of Push-Down Automata [PDA] for ContextFree Grammars [CFG]). Different variants of 2SA (or not so distant Embedded Push-Down Automata) have been proposed, some to describe top-down strategies (Vijay-Shanker, 1988; Becker, 1994), some to describe bottom-up strategies (Rambow, 1994; Nederhof, 1998; Alonso Pardo et al., 1997), but none (that we know) that are able to describe both kinds of strategies. The same dichotomy also exists in the different tabular algorithms that has been proposed for specific parsing strategies with complexity ranging from 0(70) for bottom-up strategies to 0(n9) for prefixvalid top-down strategies (with the exception of a 0(n6) tabular interpretation of a prefix-valid hybrid strategy (Nederhof, 1997)). It must also be noted that the different tabular algorithms may be difficult to understand </context>
</contexts>
<marker>Becker, 1994</marker>
<rawString>Tilman Becker. 1994. A new automaton model for TAGs: 2-SA. Computational Intelligence, 10(4):422-430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Gazdar</author>
</authors>
<title>Applicability of indexed grammars to natural languages.</title>
<date>1987</date>
<booktitle>Natural Language Parsing and Linguistic Theories,</booktitle>
<pages>69--94</pages>
<editor>In U. Reyle and C. Rohrer, editors,</editor>
<publisher>Publishing Company.</publisher>
<contexts>
<context position="7255" citStr="Gazdar, 1987" startWordPosition="1269" endWordPosition="1270">) -0-ERASE (e, A- e) 1-14 (e, C, e) (m, A, e) (w, AB -ERASE (e, 12-+ (m,C,E) \-WRITE (w, A, a) 1--z-■ (w , A\ B , E) \-ERASE (e, A\ B ,E) 1-L■ (e,C,c) Figure 1 graphically outlines the different kinds of transitions using a 2D representation where the X-axis (Y-axis) is related to the master (resp. auxiliary) stack. Figure 1 also shows the two forms of derivations we encounter (during a same session). 2 Using 2SA to parse LIGs Indexed Grammars (Aho, 1968) are an extension of Context-free Grammars in which a stack of indices is associated with each non-terminal symbol. Linear Indexed Grammars (Gazdar, 1987) are a restricted form of Indexed Grammars in which the index stack of at most one body non-terminal (the child) is related with the stack of the head non-terminal (the father). The other stacks of the production must have a bounded stack size. Formally, a LIG G is a 5-tuple (VT ,VN P) where VT is a finite set of terminals, VN is a finite set of non-terminals, S E VN is the start symbol, V1 is a finite set of indices and P is a finite set of productions. Following (Gazdar, 1987) we consider productions in which at most one element can be pushed on or popped from a stack of indices: 1334 [Termi</context>
</contexts>
<marker>Gazdar, 1987</marker>
<rawString>Gerald Gazdar. 1987. Applicability of indexed grammars to natural languages. In U. Reyle and C. Rohrer, editors, Natural Language Parsing and Linguistic Theories, pages 69-94. D. Reidel Publishing Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
</authors>
<title>An introduction to tree adjoining grammars.</title>
<date>1987</date>
<booktitle>Mathematics of Language,</booktitle>
<pages>87--115</pages>
<editor>In Alexis Manaster-Ramer, editor,</editor>
<publisher>John Benjamins Publishing Co., Amsterdam/Philadelphia.</publisher>
<contexts>
<context position="10320" citStr="Joshi, 1987" startWordPosition="1868" endWordPosition="1869">s corresponding to the main parsing strategies (but others are definable). Strategy 74+ A Top-Down A 1 -y 1 Bottom-Up 1 A&apos; 1 7 Earley A A&apos; -y It is also worth to note that the description of A(G,&apos;,&apos;) could be simplified. Indeed, for every configuration (Tri, u,E,) derivable with A(G,&apos;,&apos;), we can show that E = Hvki,ei 81 • • • Vk,,,t„ 871X , and that öt only depends on Vki,„ . That means that we could use a master stack without action marks, these marks being implicitly given by the elements 77k,t• 3 Using 2SA to parse TAGs Tree Adjoining Grammars are a extension of CFG introduced by Joshi in (Joshi, 1987) that use trees instead of productions as primary representing structure. Formally, a TAG is a 5-tuple g (VN,VT, .5, I, A), where VN is a finite set of nonterminal symbols, VT a finite set of terminal symbols, S the axiom of the grammar, I a finite set of initial trees and A a finite set of auxiliary trees. I U A is the set of elementary trees. Internal nodes are labeled by non-terminals and leaf nodes by terminals or E, except for exactly one leaf per auxiliary tree (the foot) which is labeled by the same non-terminal used as label of its root node. New trees are derived by adjoining: let be </context>
</contexts>
<marker>Joshi, 1987</marker>
<rawString>Aravind K. Joshi. 1987. An introduction to tree adjoining grammars. In Alexis Manaster-Ramer, editor, Mathematics of Language, pages 87-115. John Benjamins Publishing Co., Amsterdam/Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark-Jan Nederhof</author>
</authors>
<title>Solving the correctprefix property for TAGs.</title>
<date>1997</date>
<booktitle>Proc. of MOL &apos;97,</booktitle>
<pages>124--130</pages>
<editor>In T. Becker and H.-V. Krieger, editors,</editor>
<location>Schloss Dagstuhl, Germany,</location>
<contexts>
<context position="1620" citStr="Nederhof, 1997" startWordPosition="241" endWordPosition="242">h-Down Automata) have been proposed, some to describe top-down strategies (Vijay-Shanker, 1988; Becker, 1994), some to describe bottom-up strategies (Rambow, 1994; Nederhof, 1998; Alonso Pardo et al., 1997), but none (that we know) that are able to describe both kinds of strategies. The same dichotomy also exists in the different tabular algorithms that has been proposed for specific parsing strategies with complexity ranging from 0(70) for bottom-up strategies to 0(n9) for prefixvalid top-down strategies (with the exception of a 0(n6) tabular interpretation of a prefix-valid hybrid strategy (Nederhof, 1997)). It must also be noted that the different tabular algorithms may be difficult to understand and it is often unclear to know if the algorithms still hold for different strategies. This paper overcomes these problems by (a) introducing strongly-driven SA [SD-2SAI that may be used to describe parsing strategies for TAGs and LIGs, ranging from purely top-down to purely bottom-up, and (b) presenting a tabular interpretation of these automata in time complexity 0(n6) and space complexity 0(0). The tabular interpretation follows the principles of Dynamic Programming: the derivations are broken into</context>
<context position="18293" citStr="Nederhof, 1997" startWordPosition="3458" endWordPosition="3459">here -6&apos; = (w, C, c), B = (v, B ,b), P = (k,F,b), and (when 0 o) 0 = (1, 0,b). [SWAP] T = (p, C,) (q, F, AB(5[D.E]Cm ABS[13E]Pm (10) where C = (w, C, c), F = (k,F,c), and either c = W&apos; or = c. The best way to apprehend these rules is to visualize them graphically as done for the two most complex ones (Rules 6 and 9) in Figures 4 and 5. Figure 4: Application of Rule 6 Figure 5: Application of Rule 9 5.1 Reducing the complexity An analysis of the time complexity to apply each rule gives us polynomial complexities 0(nu) with u &lt; 6 except for Rule 9 where u = 8. However, by adapting an idea from (Nederhof, 1997), we replace Rule 9 by the alternate and equivalent Rule 11. .13°*/[DET-Ce *.1-3°\[-OP]Ee MNA[OP]Fe (11) MNA[oo]Bw M*\[-OP]ire where C = (w, C, c), B = (v, B , b), F = (k,F,b), and (when 0 0 o) 0 = (1,0,b). Rule 11 has same complexity than Rule 9, but may actually be split into two rules of lesser complexity 0(n6), introducing an intermediary pseudo-item BB /[[0.11Ce (intuitively assimilable to a &amp;quot;deeply escaped&amp;quot; CF derivation). Rule 12 collects these pseudo-items (independently from any transition) while Rule 13 combines them with items (given a /—ERASE transition r). BB/[Dr]Ce}BB/rOPliCe (12</context>
</contexts>
<marker>Nederhof, 1997</marker>
<rawString>Mark-Jan Nederhof. 1997. Solving the correctprefix property for TAGs. In T. Becker and H.-V. Krieger, editors, Proc. of MOL &apos;97, pages 124-130, Schloss Dagstuhl, Germany, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark-Jan Nederhof</author>
</authors>
<title>Linear indexed automata and tabulation of TAG parsing.</title>
<date>1998</date>
<booktitle>In Proc. of First Workshop on Tabulation in Parsing and Deduction (TAPD&apos;98),</booktitle>
<pages>1--9</pages>
<location>Paris, France,</location>
<contexts>
<context position="1183" citStr="Nederhof, 1998" startWordPosition="173" endWordPosition="174">a time complexity in 0(n6) and space complexity in 0(n5) where n is the length of the input string. Introduction 2-Stack automata [2SA] have been identified as possible operational devices to describe parsing strategies for Linear Indexed Grammars [LIG] or Tree Adjoining Grammars [TAG] (mirroring the traditional use of Push-Down Automata [PDA] for ContextFree Grammars [CFG]). Different variants of 2SA (or not so distant Embedded Push-Down Automata) have been proposed, some to describe top-down strategies (Vijay-Shanker, 1988; Becker, 1994), some to describe bottom-up strategies (Rambow, 1994; Nederhof, 1998; Alonso Pardo et al., 1997), but none (that we know) that are able to describe both kinds of strategies. The same dichotomy also exists in the different tabular algorithms that has been proposed for specific parsing strategies with complexity ranging from 0(70) for bottom-up strategies to 0(n9) for prefixvalid top-down strategies (with the exception of a 0(n6) tabular interpretation of a prefix-valid hybrid strategy (Nederhof, 1997)). It must also be noted that the different tabular algorithms may be difficult to understand and it is often unclear to know if the algorithms still hold for diff</context>
</contexts>
<marker>Nederhof, 1998</marker>
<rawString>Mark-Jan Nederhof. 1998. Linear indexed automata and tabulation of TAG parsing. In Proc. of First Workshop on Tabulation in Parsing and Deduction (TAPD&apos;98), pages 1-9, Paris, France, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Owen Rambow</author>
</authors>
<title>Formal and Computational Aspects of Natural Language Syntax.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="1167" citStr="Rambow, 1994" startWordPosition="171" endWordPosition="172">l strategies, a time complexity in 0(n6) and space complexity in 0(n5) where n is the length of the input string. Introduction 2-Stack automata [2SA] have been identified as possible operational devices to describe parsing strategies for Linear Indexed Grammars [LIG] or Tree Adjoining Grammars [TAG] (mirroring the traditional use of Push-Down Automata [PDA] for ContextFree Grammars [CFG]). Different variants of 2SA (or not so distant Embedded Push-Down Automata) have been proposed, some to describe top-down strategies (Vijay-Shanker, 1988; Becker, 1994), some to describe bottom-up strategies (Rambow, 1994; Nederhof, 1998; Alonso Pardo et al., 1997), but none (that we know) that are able to describe both kinds of strategies. The same dichotomy also exists in the different tabular algorithms that has been proposed for specific parsing strategies with complexity ranging from 0(70) for bottom-up strategies to 0(n9) for prefixvalid top-down strategies (with the exception of a 0(n6) tabular interpretation of a prefix-valid hybrid strategy (Nederhof, 1997)). It must also be noted that the different tabular algorithms may be difficult to understand and it is often unclear to know if the algorithms sti</context>
<context position="3609" citStr="Rambow, 1994" startWordPosition="565" endWordPosition="566">is done and the other being the Auxiliary Stack AS mainly used for restricted &amp;quot;bookkeeping&amp;quot;. The following remarks are intended to give an idea of the restrictions we want to enforce. The first ones are rather standard and may be found under different forms in the literature. The last one justifies the qualification of &amp;quot;strongly-driven&amp;quot; for our automata. [Session] AS should actually be seen as a stack of session stacks, each one being associated to a session. Only the topmost session stack may be consulted or modified. This idea is closely related to the notion of Embedded Push-Down Automata (Rambow, 1994, 96-102). [Linearity] A session starts in mode write w and switches at some point in mode erase e. In mode w (resp. e), no element can be popped from (resp. pushed to) the master stack MS. Switching back from e to w is not allowed. This requirement is related to linearity because it means that a same session stack is never used twice by &amp;quot;descendants&amp;quot; of an element in MS. [Soft Session Exit] Exiting a session is only possible when reaching back, with an empty session stack and in mode erase, the MS element that initiated the session. [Driving] Each pushing on MS done in write mode leaves some </context>
</contexts>
<marker>Rambow, 1994</marker>
<rawString>Owen Rambow. 1994. Formal and Computational Aspects of Natural Language Syntax. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
</authors>
<title>A Study of Tree Adjoining Grammars.</title>
<date>1988</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania,</institution>
<contexts>
<context position="1099" citStr="Vijay-Shanker, 1988" startWordPosition="162" endWordPosition="163">ly bottom-up, for LIGs and TAGs. The tabular interpretation ensures, for all strategies, a time complexity in 0(n6) and space complexity in 0(n5) where n is the length of the input string. Introduction 2-Stack automata [2SA] have been identified as possible operational devices to describe parsing strategies for Linear Indexed Grammars [LIG] or Tree Adjoining Grammars [TAG] (mirroring the traditional use of Push-Down Automata [PDA] for ContextFree Grammars [CFG]). Different variants of 2SA (or not so distant Embedded Push-Down Automata) have been proposed, some to describe top-down strategies (Vijay-Shanker, 1988; Becker, 1994), some to describe bottom-up strategies (Rambow, 1994; Nederhof, 1998; Alonso Pardo et al., 1997), but none (that we know) that are able to describe both kinds of strategies. The same dichotomy also exists in the different tabular algorithms that has been proposed for specific parsing strategies with complexity ranging from 0(70) for bottom-up strategies to 0(n9) for prefixvalid top-down strategies (with the exception of a 0(n6) tabular interpretation of a prefix-valid hybrid strategy (Nederhof, 1997)). It must also be noted that the different tabular algorithms may be difficult</context>
</contexts>
<marker>Vijay-Shanker, 1988</marker>
<rawString>K. Vijay-Shanker. 1988. A Study of Tree Adjoining Grammars. Ph.D. thesis, University of Pennsylvania, January.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>