<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004821">
<title confidence="0.991399">
Machine Reading at the University of Washington
</title>
<author confidence="0.987653333333333">
Hoifung Poon, Janara Christensen, Pedro Domingos, Oren Etzioni, Raphael Hoffmann,
Chloe Kiddon, Thomas Lin, Xiao Ling, Mausam, Alan Ritter, Stefan Schoenmackers,
Stephen Soderland, Dan Weld, Fei Wu, Congle Zhang
</author>
<affiliation confidence="0.997824">
Department of Computer Science &amp; Engineering
University of Washington
</affiliation>
<address confidence="0.943563">
Seattle, WA 98195
</address>
<email confidence="0.8448695">
{hoifung,janara,pedrod,etzioni,raphaelh,chloe,tlin,xiaoling,mausam,
aritter,stef,soderland,weld,wufei,clzhang}@cs.washington.edu
</email>
<sectionHeader confidence="0.995578" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999635">
Machine reading is a long-standing goal of AI
and NLP. In recent years, tremendous progress
has been made in developing machine learning
approaches for many of its subtasks such as
parsing, information extraction, and question
answering. However, existing end-to-end so-
lutions typically require substantial amount of
human efforts (e.g., labeled data and/or man-
ual engineering), and are not well poised for
Web-scale knowledge acquisition. In this pa-
per, we propose a unifying approach for ma-
chine reading by bootstrapping from the easi-
est extractable knowledge and conquering the
long tail via a self-supervised learning pro-
cess. This self-supervision is powered by joint
inference based on Markov logic, and is made
scalable by leveraging hierarchical structures
and coarse-to-fine inference. Researchers at
the University of Washington have taken the
first steps in this direction. Our existing work
explores the wide spectrum of this vision and
shows its promise.
</bodyText>
<sectionHeader confidence="0.99913" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.990684355555556">
Machine reading, or learning by reading, aims to
extract knowledge automatically from unstructured
text and apply the extracted knowledge to end tasks
such as decision making and question answering. It
has been a major goal of AI and NLP since their
early days. With the advent of the Web, the billions
of online text documents contain virtually unlimited
amount of knowledge to extract, further increasing
the importance and urgency of machine reading.
In the past, there has been a lot of progress in
automating many subtasks of machine reading by
87
machine learning approaches (e.g., components in
the traditional NLP pipeline such as POS tagging
and syntactic parsing). However, end-to-end solu-
tions are still rare, and existing systems typically re-
quire substantial amount of human effort in manual
engineering and/or labeling examples. As a result,
they often target restricted domains and only extract
limited types of knowledge (e.g., a pre-specified re-
lation). Moreover, many machine reading systems
train their knowledge extractors once and do not
leverage further learning opportunities such as ad-
ditional text and interaction with end users.
Ideally, a machine reading system should strive to
satisfy the following desiderata:
End-to-end: the system should input raw text, ex-
tract knowledge, and be able to answer ques-
tions and support other end tasks;
High quality: the system should extract knowledge
with high accuracy;
Large-scale: the system should acquire knowledge
at Web-scale and be open to arbitrary domains,
genres, and languages;
Maximally autonomous: the system should incur
minimal human effort;
Continuous learning from experience: the
system should constantly integrate new infor-
mation sources (e.g., new text documents) and
learn from user questions and feedback (e.g.,
via performing end tasks) to continuously
improve its performance.
These desiderata raise many intriguing and chal-
lenging research questions. Machine reading re-
search at the University of Washington has explored
</bodyText>
<note confidence="0.984587">
Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 87–95,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999116833333333">
a wide spectrum of solutions to these challenges and
has produced a large number of initial systems that
demonstrated promising performance. During this
expedition, an underlying unifying vision starts to
emerge. It becomes apparent that the key to solving
machine reading is to:
</bodyText>
<listItem confidence="0.955005625">
1. Conquer the long tail of textual knowledge via
a self-supervised learning process that lever-
ages data redundancy to bootstrap from the
head and propagates information down the long
tail by joint inference;
2. Scale this process to billions of Web documents
by identifying and leveraging ubiquitous struc-
tures that lead to sparsity.
</listItem>
<bodyText confidence="0.998470625">
In Section 2, we present this vision in detail, iden-
tify the major dimensions these initial systems have
explored, and propose a unifying approach that sat-
isfies all five desiderata. In Section 3, we reivew
machine reading research at the University of Wash-
ington and show how they form synergistic effort
towards solving the machine reading problem. We
conclude in Section 4.
</bodyText>
<sectionHeader confidence="0.961532" genericHeader="method">
2 A Unifying Approach for Machine
Reading
</sectionHeader>
<bodyText confidence="0.9999335">
The core challenges to machine reading stem from
the massive scale of the Web and the long-tailed dis-
tribution of textual knowledge. The heterogeneous
Web contains texts that vary substantially in subject
matters (e.g., finance vs. biology) and writing styles
(e.g., blog posts vs. scientific papers). In addition,
natural languages are famous for their myraid vari-
ations in expressing the same meaning. A fact may
be stated in a straightforward way such as “kale con-
tains calcium”. More often though, it may be stated
in a syntactically and/or lexically different way than
as phrased in an end task (e.g., “calcium is found in
kale”). Finally, many facts are not even stated ex-
plicitly, and must be inferred from other facts (e.g.,
“kale prevents osteoporosis” may not be stated ex-
plicitly but can be inferred by combining facts such
as “kale contains calcium” and “calcium helps pre-
vent osteoporosis”). As a result, machine reading
must not rely on explicit supervision such as manual
rules and labeled examples, which will incur pro-
hibitive cost in the Web scale. Instead, it must be
able to learn from indirect supervision.
</bodyText>
<figure confidence="0.588971">
Knowledge
</figure>
<figureCaption confidence="0.996215">
Figure 1: A unifying vision for machine reading: boot-
</figureCaption>
<bodyText confidence="0.943853939393939">
strap from the head regime of the power-law distribu-
tion of textual knowledge, and conquer the long tail in
a self-supervised learning process that raises certainty on
sparse extractions by propagating information via joint
inference from frequent extractions.
A key source of indirect supervision is meta
knowledge about the domains. For example, the
TextRunner system (Banko et al., 2007) hinges on
the observation that there exist general, relation-
independent patterns for information extraction. An-
other key source of indirect supervision is data re-
dundancy. While a rare extracted fact or inference
pattern may arise by chance of error, it is much less
likely so for the ones with many repetitions (Downey
et al., 2010). Such highly-redundant knowledge can
be extracted easily and with high confidence, and
can be leveraged for bootstrapping. For knowledge
that resides in the long tail, explicit forms of redun-
dancy (e.g., identical expressions) are rare, but this
can be circumvented by joint inference. For exam-
ple, expressions that are composed with or by sim-
ilar expressions probably have the same meaning;
the fact that kale prevents osteoporosis can be de-
rived by combining the facts that kale contains cal-
cium and that calcium helps prevent osteoporosis via
a transitivity-through inference pattern. In general,
joint inference can take various forms, ranging from
simple voting to shrinkage in a probabilistic ontol-
ogy to sophisticated probabilistic reasoning based
on a joint model. Simple ones tend to scale bet-
ter, but their capability in propagating information
is limited. More sophisticated methods can uncover
implicit redundancy and propagate much more in-
</bodyText>
<figure confidence="0.99180925">
Certainty
Self-Supervision
�
Joint Inference
</figure>
<page confidence="0.998159">
88
</page>
<bodyText confidence="0.999847476190477">
formation with higher quality, yet the challenge is
how to make them scale as well as simple ones.
To do machine reading, a self-supervised learning
process, informed by meta knowledege, stipulates
what form of joint inference to use and how. Effec-
tively, it increases certainty on sparse extractions by
propagating information from more frequent ones.
Figure 1 illustrates this unifying vision.
In the past, machine reading research at the Uni-
versity of Washington has explored a variety of so-
lutions that span the key dimensions of this uni-
fying vision: knowledge representation, bootstrap-
ping, self-supervised learning, large-scale joint in-
ference, ontology induction, continuous learning.
See Section 3 for more details. Based on this ex-
perience, one direction seems particularly promising
that we would propose here as our unifying approach
for end-to-end machine reading:
Markov logic is used as the unifying framework for
knowledge representation and joint inference;
Self-supervised learning is governed by a joint
probabilistic model that incorporates a small
amount of heuristic knowledge and large-scale
relational structures to maximize the amount
and quality of information to propagate;
Joint inference is made scalable to the Web by
coarse-to-fine inference.
Probabilistic ontologies are induced from text to
guarantee tractability in coarse-to-fine infer-
ence. This ontology induction and popula-
tion are incorporated into the joint probabilistic
model for self-supervision;
Continuous learning is accomplished by combin-
ing bootstrapping and crowdsourced content
creation to synergistically improve the reading
system from user interaction and feedback.
A distinctive feature of this approach is its empha-
sis on using sophisticated joint inference. Recently,
joint inference has received increasing interest in
AI, machine learning, and NLP, with Markov logic
(Domingos and Lowd, 2009) being one of the lead-
ing unifying frameworks. Past work has shown that
it can substantially improve predictive accuracy in
supervised learning (e.g., (Getoor and Taskar, 2007;
Bakir et al., 2007)). We propose to build on these ad-
vances, but apply joint inference beyond supervised
learning, with labeled examples supplanted by indi-
rect supervision.
Another distinctive feature is that we propose
to use coarse-to-fine inference (Felzenszwalb and
McAllester, 2007; Petrov, 2009) as a unifying
framework to scale inference to the Web. Es-
sentially, coarse-to-fine inference leverages the
sparsity imposed by hierarchical structures that
are ubiquitous in human knowledge (e.g., tax-
onomies/ontologies). At coarse levels (top levels in
a hierarchy), ambiguities are rare (there are few ob-
jects and relations), and inference can be conducted
very efficiently. The result is then used to prune un-
promising refinements at the next level. This process
continues down the hierarchy until decision can be
made. In this way, inference can potentially be sped
up exponentially, analogous to binary tree search.
Finally, we propose a novel form of continuous
learning by leveraging the interaction between the
system and end users to constantly improve the per-
formance. This is straightforward to do in our ap-
proach given the self-supervision process and the
availability of powerful joint inference. Essentially,
when the system output is applied to an end task
(e.g., answering questions), the feedback from user
is collected and incorporated back into the system
as a bootstrap source. The feedback can take the
form of explicit supervision (e.g., via community
content creation or active learning) or indirect sig-
nals (e.g., click data and query logs). In this way,
we can bootstrap an online community by an initial
machine reading system that provides imperfect but
valuable service in end tasks, and continuously im-
prove the quality of system output, which attracts
more users with higher degree of participation, thus
creating a positive feedback loop and raising the ma-
chine reading performance to a high level that is dif-
ficult to attain otherwise.
</bodyText>
<sectionHeader confidence="0.716193" genericHeader="method">
3 Summary of Progress to Date
</sectionHeader>
<bodyText confidence="0.998602571428571">
The University of Washington has been one of the
leading places for machine reading research and has
produced many cutting-edge systems, e.g., WIEN
(first wrapper induction system for information ex-
traction), Mulder (first fully automatic Web-scale
question answering system), KnowItAll/TextRunner
(first systems to do open-domain information extrac-
</bodyText>
<page confidence="0.998998">
89
</page>
<bodyText confidence="0.99994675862069">
tion from the Web corpus at large scale), Kylin (first
self-supervised system for Wikipedia-based infor-
mation extraction), UCR (first unsupervised corefer-
ence resolution system that rivals the performance of
supervised systems), Holmes (first Web-scale joint
inference system), USP (first unsupervised system
for semantic parsing).
Figure 2 shows the evolution of the major sys-
tems; dashed lines signify influence in key ideas
(e.g., Mulder inspires KnowItAll), and solid lines
signify dataflow (e.g., Holmes inputs TextRunner tu-
ples). These systems span a wide spectrum in scal-
ability (assessed by speed and quantity in extrac-
tion) and comprehension (assessed by unit yield of
knowledge at a fixed precision level). At one ex-
treme, the TextRunner system is highly scalable, ca-
pable of extracting billions of facts, but it focuses on
shallow extractions from simple sentences. At the
other extreme, the USP and LOFT systems achieve
much higher level of comprehension (e.g., in a task
of extracting knowledge from biomedical papers and
answering questions, USP obtains more than three
times as many correct answers as TextRunner, and
LOFT obtains more than six times as many correct
answers as TextRunner), but are much less scalable
than TextRunner.
In the remainder of the section, we review the
progress made to date and identify key directions for
future work.
</bodyText>
<subsectionHeader confidence="0.9546135">
3.1 Knowledge Representation and Joint
Inference
</subsectionHeader>
<bodyText confidence="0.999981">
Knowledge representations used in these systems
vary widely in expressiveness, ranging from sim-
ple ones like relation triples (&lt;subject, relation,
object&gt;; e.g., in KnowItAll and TextRunner), to
clusters of relation triples or triple components (e.g.,
in SNE, RESOLVER), to arbitrary logical formulas
and their clusters (e.g., in USP, LOFT). Similarly,
a variety forms of joint inference have been used,
ranging from simple voting to heuristic rules to so-
phisticated probabilistic models. All these can be
compactly encoded in Markov logic (Domingos and
Lowd, 2009), which provides a unifying framework
for knowledge representation and joint inference.
Past work at Washington has shown that in su-
pervised learning, joint inference can substantially
improve predictive performance on tasks related to
machine reading (e.g., citation information extrac-
tion (Poon and Domingos, 2007), ontology induc-
tion (Wu and Weld, 2008), temporal information
extraction (Ling and Weld, 2010)). In addition, it
has demonstrated that sophisticated joint inference
can enable effective learning without any labeled
information (UCR, USP, LOFT), and that joint in-
ference can scale to millions of Web documents by
leveraging sparsity in naturally occurring relations
(Holmes, Sherlock), showing the promise of our uni-
fying approach.
Simpler representations limit the expressiveness
in representing knowledge and the degree of sophis-
tication in joint inference, but they currently scale
much better than more expressive ones. A key direc-
tion for future work is to evaluate this tradeoff more
thoroughly, e.g., for each class of end tasks, to what
degree do simple representations limit the effective-
ness in performing the end tasks? Can we automate
the choice of representations to strike the best trade-
off for a specific end task? Can we advance joint
inference algorithms to such a degree that sophisti-
cated inference scales as well as simple ones?
</bodyText>
<subsectionHeader confidence="0.999168">
3.2 Bootstrapping
</subsectionHeader>
<bodyText confidence="0.999925133333333">
Past work at Washington has identified and lever-
aged a wide range of sources for bootstrapping. Ex-
amples include Wikipedia (Kylin, KOG, IIA, WOE,
WPE), Web lists (KnowItAll, WPE), Web tables
(WebTables), Hearst patterns (KnowItAll), heuristic
rules (TextRunner), semantic role labels (SRL-IE),
etc.
In general, potential bootstrap sources can be
broadly divided into domain knowledge (e.g., pat-
terns and rules) and crowdsourced contents (e.g., lin-
guistic resources, Wikipedia, Amazon Mechanical
Turk, the ESP game).
A key direction for future work is to combine
bootstrapping with crowdsourced content creation
for continuous learning. (Also see Subsection 3.6.)
</bodyText>
<subsectionHeader confidence="0.99444">
3.3 Self-Supervised Learning
</subsectionHeader>
<bodyText confidence="0.999463333333333">
Although the ways past systems conduct self-
supervision vary widely in detail, they can be di-
vided into two broad categories. One uses heuristic
rules that exploit existing semi-structured resources
to generate noisy training examples for use by su-
pervised learning methods and with cotraining (e.g.,
</bodyText>
<page confidence="0.993405">
90
</page>
<note confidence="0.807126">
1997 2001 2004 2005 2007 2008 2009 2010
</note>
<figureCaption confidence="0.8667075">
Figure 2: The evolution of major machine reading systems at the University of Washington. Dashed lines signify
influence and solid lines signify dataflow. At the top are the years of publications. ShopBot learns comparison-
</figureCaption>
<bodyText confidence="0.950044625">
shopping agents via self-supervision using heuristic knowledge (Doorenbos et al., 1997); WIEN induces wrappers
for information extraction via self-supervision using joint inference to combine simple atomic extractors (Kushmerick
et al., 1997); Mulder answers factoid questions by leveraging redundancy to rank candidate answers extracted from
multiple search query results (Kwok et al., 2001); KnowItAll conducts open-domain information extraction via self-
supervision bootstrapping from Hearst patterns (Etzioni et al., 2005); Opine builds on KnowItAll and mines product
reviews via self-supervision using joint inference over neighborhood features (Popescu and Etzioni, 2005); Kylin pop-
ulates Wikipedia infoboxes via self-supervision bootstrapping from existing infoboxes (Wu and Weld, 2007); LEX
conducts Web-scale name entity recognition by leveraging collocation statistics (Downey et al., 2007a); REALM
improves sparse open-domain information extraction via relational clustering and language modeling (Downey et al.,
2007b); RESOLVER performs entity and relation resolution via relational clustering (Yates and Etzioni, 2007); Tex-
tRunner conducts open-domain information extraction via self-supervision bootstrapping from heuristic rules (Banko
et al., 2007); AuContraire automatically identifies contradictory statements in a large web corpus using functional re-
lations (Ritter et al., 2008); HOLMES infers new facts from TextRunner output using Markov logic (Schoenmackers
et al., 2008); KOG learns a rich ontology by combining Wikipedia infoboxes with WordNet via joint inference using
Markov Logic Networks (Wu and Weld, 2008), shrinkage over this ontology vastly improves the recall of Kylin’s
extractors; UCR performs state-of-the-art unsupervised coreference resolution by incorporating a small amount of
domain knowledge and conducting joint inference among entity mentions with Markov logic (Poon and Domingos,
2008b); SNE constructs a semantic network over TextRunner output via relational clustering with Markov logic (Kok
and Domingos, 2008); WebTables conducts Web-scale information extraction by leveraging HTML table structures
(Cafarella et al., 2008); IIA learns from infoboxes to filter open-domain information extraction toward assertions that
are interesting to people (Lin et al., 2009); USP jointly learns a semantic parser and extracts knowledge via recursive
relational clustering with Markov logic (Poon and Domingos, 2009); LDA-SP automatically infers a compact repre-
sentation describing the plausible arguments for a relation using an LDA-Style model and Bayesian Inference (Ritter
et al., 2010); LOFT builds on USP and jointly performs ontology induction, population, and knowledge extraction via
joint recursive relational clustering and shrinkage with Markov logic (Poon and Domingos, 2010); OLPI improves the
efficiency of lifted probabilistic inference and learning via coarse-to-fine inference based on type hierarchies (Kiddon
and Domingos, 2010). SHERLOCK induces new inference rules via relational learning (Schoenmackers et al., 2010);
SRL-IE conducts open-domain information extraction by bootstrapping from semantic role labels, PrecHybrid is a
hybrid version between SRL-IE and TextRunner, which given a budget of computation time does better than either
system (Christensen et al., 2010); WOE builds on Kylin and conducts open-domain information extraction (Wu and
Weld, 2010); WPE learns 5000 relational extractors by bootstrapping from Wikipedia and using Web lists to generate
dynamic, relation-specific lexicon features (Hoffmann et al., 2010).
</bodyText>
<figure confidence="0.999454454545455">
Kylin
WOE
OLPI
WebTables
Opine LEX
Holmes Sherlock
SRL-IE
RESOLVER
CR
PrecHybrid
SP LOFT
IIA
KnowItAll
TextRunner
SNE
LDA-SP
REALM
AuContraire
KOG WPE
ShopBot
WIEN
Mulder
</figure>
<page confidence="0.995699">
91
</page>
<bodyText confidence="0.999916956521739">
TextRunner, Kylin, KOG, WOE, WPE). Another
uses unsupervised learning and often takes a partic-
ular form of relational clustering (e.g., objects asso-
ciated with similar relations tend to be the same and
vice versa, as in REALM, RESOLVER, SNE, UCR,
USP, LDA-SP, LOFT, etc.).
Some distinctive types of self-supervision in-
clude shrinkage based on an ontology (KOG,
LOFT, OLPI), probabilistic inference via hand-
crafted or learned inference patterns (Holmes, Sher-
lock), and cotraining using relation-specific and
relation-independent (open) extraction to reinforce
semantic coherence (Wu et al., 2008).
A key direction for future work is to develop a
unifying framework for self-supervised learning by
combining the strengths of existing methods and
overcoming their limitations. This will likely take
the form of a new learning paradigm that combines
existing paradigms such as supervised learning, rela-
tional clustering, semi-supervised learning, and ac-
tive learning into a unifying learning framework that
synergistically leverages diverse forms of supervi-
sion and information sources.
</bodyText>
<subsectionHeader confidence="0.998682">
3.4 Large-Scale Joint Inference
</subsectionHeader>
<bodyText confidence="0.9999880625">
To apply sophisticated joint inference in machine
reading, the major challenge is to make it scal-
able to billions of text documents. A general solu-
tion is to identify and leverage ubiquitous problem
structures that lead to sparsity. For example, order
of magnitude reduction in both memory and infer-
ence time can be achieved for relational inference
by leveraging the fact that most relational atoms are
false, which trivially satisfy most relational formulas
(Singla and Domingos, 2006; Poon and Domingos,
2008a); joint inference with naturally occurring tex-
tual relations can scale to millions of Web pages by
leveraging the fact that such relations are approxi-
mately functional (Schoenmackers et al., 2008).
More generally, sparsity arises from hierarchical
structures (e.g., ontologies) that are naturally exhib-
ited in human knowledge, and can be leveraged to
do coarse-to-fine inference (OLPI).
The success of coarse-to-fine inference hinges on
the availability and quality of hierarchical structures.
Therefore, a key direction for future work is to auto-
matically induce such hierarchies. (Also see next
subsection.) Moreover, given the desideratum of
continuous learning from experience, and the speedy
evolution of the Web (new contents, formats, etc.),
it is important that we develop online methods for
self-supervision and joint inference. For example,
when a new text document arrives, the reading sys-
tem should not relearn from scratch, but should iden-
tify only the relevant pieces of knowledge and con-
duct limited-scoped inference and learning accord-
ingly.
</bodyText>
<subsectionHeader confidence="0.943342">
3.5 Ontology Induction
</subsectionHeader>
<bodyText confidence="0.999986657142857">
As mentioned in previous subsections, ontologies
play an important role in both self-supervision
(shrinkage) and large-scale inference (coarse-to-fine
inference). A distinctive feature in our unifying ap-
proach is to induce probabilistic ontologies, which
can be learned from noisy text and support joint
inference. Past systems have explored two differ-
ent approaches to probabilistic ontology induction.
One approach is to bootstrap from existing onto-
logical structures and apply self-supervision to cor-
rect the erroneous nodes and fill in the missing ones
(KOG). Another approach is to integrate ontology
induction with hierarchical smoothing, and jointly
pursue unsupervised ontology induction, population
and knowledge extraction (LOFT).
A key direction for future work is to combine
these two paradigms. As case studies in ontology
integration, prior research has devised probabilistic
schema mappings and corpus-based matching algo-
rithms (Doan, 2002; Madhavan, 2005; Dong et al.,
2007), and has automatically constructed mappings
between the Wikipedia infobox “ontology” and the
Freebase ontology. This latter endeavor illustrated
the complexity of the necessary mappings: a simple
attribute in one ontology may correspond to a com-
plex relational view in the other, comprising three
join operations; searching for such matches yields
a search space with billions of possible correspon-
dences for just a single attribute.
Another key direction is to develop general meth-
ods for inducing multi-facet, multi-inheritance on-
tologies. Although single-inheritance, tree-like hier-
archies are easier to induce and reason with, natu-
rally occurring ontologies generally take the form of
a lattice rather than a tree.
</bodyText>
<page confidence="0.977253">
92
</page>
<subsectionHeader confidence="0.51593">
3.6 Continuous Learning FA8750-07-D-0185, HR0011-06-C-0025, HR0011-
</subsectionHeader>
<bodyText confidence="0.986694625">
Early work at Washington proposed to construct 07-C-0060, NBCH-D030010 and FA8750-09-C-
knowledge bases by mass collaboration (Richard- 0179, NSF grants IIS-0803481 and IIS-0534881,
son and Domingos, 2003). A key challenge is to and ONR grant N00014-08-1-0670 and N00014-08-
combine inconsistent knowledge sources of varying 1-0431, the WRF / TJ Cable Professorship, a gift
quality, which motivated the subsequent develop- from Google, and carried out at the University of
ment of Markov logic. While this work did not do Washington’s Turing Center. The views and con-
machine reading, its emphasis on lifelong learning clusions contained in this document are those of the
from user feedback resonates with our approach on authors and should not be interpreted as necessarily
continuous learning. representing the official policies, either expressed or
Past work at Washington has demonstrated the implied, of ARO, DARPA, NSF, ONR, or the United
promise of our approach. For example, (Banko and States Government.
Etzioni, 2007) automated theory formation based on References
TextRunner extractions via a lifelong-learning pro- G. Bakir, T. Hofmann, B. B. Sch¨olkopf, A. Smola,
cess; (Hoffmann et al., 2009) show that the pairing B. Taskar, S. Vishwanathan, and (eds.). 2007. Pre-
of Kylin and community content creation benefits dicting Structured Data. MIT Press, Cambridge, MA.
both by sharing Wikipedia edits; (Soderland et al., M. Banko and O. Etzioni. 2007. Strategies for lifelong
2010) successfully adapted the TextRunner open- knowledge extraction from the web. In Proceedings of
domain information extraction system to specific do- the Sixteenth International Conference on Knowledge
mains via active learning. Capture, British Columbia, Canada.
Our approach also resonates with the never- Michele Banko, Michael J. Cafarella, Stephen Soderland,
ending learning paradigm for “Reading the Web” Matt Broadhead, and Oren Etzioni. 2007. Open in-
(Carlson et al., 2010). In future work, we intend to formation extraction from the web. In Proceedings of
combine our approach with related ones to enable the Twentieth International Joint Conference on Artifi-
more effective continuous learning from experience. cial Intelligence, pages 2670–2676, Hyderabad, India.
4 Conclusion AAAI Press.
This paper proposes a unifying approach to ma- Michael J. Cafarella, Jayant Madhavan, and Alon Halevy.
chine reading that is end-to-end, large-scale, maxi- 2008. Web-scale extraction of structured data. SIG-
mally autonomous, and capable of continuous learn- MOD Record, 37(4):55–61.
ing from experience. At the core of this approach Andrew Carlson, Justin Betteridge, Richard C. Wang, Es-
is a self-supervised learning process that conquers tevam R. Hruschka Jr., and Tom M. Mitchell. 2010.
the long tail of textual knowledge by propagating in- Coupled semi-supervised learning for information ex-
formation via joint inference. Markov logic is used traction. In Proceedings of the Third ACM Interna-
as the unifying framework for knowledge represen- tional Conference on Web Search and Data Mining.
tation and joint inference. Sophisticated joint in- Janara Christensen, Mausam, Stephen Soderland, and
ference is made scalable by coarse-to-fine inference Oren Etzioni. 2010. Semantic role labeling for open
based on induced probabilistic ontologies. This uni- information extraction. In Proceedings of the First In-
fying approach builds on the prolific experience in ternational Workshop on Formalisms and Methodol-
cutting-edge machine reading research at the Uni- ogy for Learning by Reading.
versity of Washington. Past results demonstrate its Anhai Doan. 2002. Learning to Map between Structured
promise and reveal key directions for future work. Representations of Data. Ph.D. thesis, University of
</bodyText>
<table confidence="0.979266444444444">
5 Acknowledgement Washington.
This research was partly funded by ARO grant Pedro Domingos and Daniel Lowd. 2009. Markov
W911NF-08-1-0242, AFRL contract FA8750-09- Logic: An Interface Layer for Artificial Intelligence.
C-0181, DARPA contracts FA8750-05-2-0283, Morgan &amp; Claypool, San Rafael, CA.
93 Xin Dong, Alon Halevy, and Cong Yu. 2007. Data inte-
gration with uncertainty. VLDB Journal.
Robert B. Doorenbos, Oren Etzioni, and Daniel S. Weld.
1997. A scalable comparison-shopping agent for the
world-wide web. In Proceedings of AGENTS-97.
</table>
<reference confidence="0.997404719626169">
Doug Downey, Matthew Broadhead, and Oren Etzioni.
2007a. Locating complex named entities in web text.
In Proceedings of the Twentieth International Joint
Conference on Artificial Intelligence.
Doug Downey, Stefan Schoenmackers, and Oren Etzioni.
2007b. Sparse information extraction: Unsupervised
language models to the rescue. In Proceedings of
the Forty Fifth Annual Meeting of the Association for
Computational Linguistics.
Doug Downey, Oren Etzioni, and Stephen Soderland.
2010. Analysis of a probabilistic model of redundancy
in unsupervised information extraction. In Artificial
Intelligence.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsuper-
vised named-entity extraction from the web: An exper-
imental study. Artificial Intelligence, 165(1):91–134.
Pedro F. Felzenszwalb and David McAllester. 2007. The
generalized A* architecture. Journal of Artificial In-
telligence Research, 29.
Lise Getoor and Ben Taskar, editors. 2007. Introduction
to Statistical Relational Learning. MIT Press, Cam-
bridge, MA.
Raphael Hoffmann, Saleema Amershi, Kayur Patel, Fei
Wu, James Fogarty, and Daniel S. Weld. 2009.
Amplifying community content creation using mixed-
initiative information extraction. In Proceedings of
CHI-09.
Raphael Hoffmann, Congle Zhang, and Daniel S. Weld.
2010. Learning 5000 relational extractors. In submis-
sion.
Chloe Kiddon and Pedro Domingos. 2010. Ontological
lifted probabilistic inference. In submission.
Stanley Kok and Pedro Domingos. 2008. Extracting se-
mantic networks from text via relational clustering. In
Proceedings of the Nineteenth European Conference
on Machine Learning, pages 624–639, Antwerp, Bel-
gium. Springer.
Nicholas Kushmerick, Daniel S. Weld, and Robert
Doorenbos. 1997. Wrapper induction for information
extraction. In Proceedings of the Fifteenth Interna-
tional Joint Conference on Artificial Intelligence.
Cody Kwok, Oren Etzioni, and Daniel S. Weld. 2001.
Scaling question answering to the web. In Proceed-
ings of the Tenth International Conference on World
Wide Web.
Thomas Lin, Oren Etzioni, and James Fogarty. 2009.
Identifying interesting assertions from the web. In
Proceedings of the Eighteenth Conference on Informa-
tion and Knowledge Management.
Xiao Ling and Daniel S. Weld. 2010. Temporal infor-
mation extraction. In Proceedings of the Twenty Fifth
National Conference on Artificial Intelligence.
Jayant Madhavan. 2005. Using known schemas and
mappings to construct new semantic mappings. Ph.D.
thesis, University of Washington.
Slav Petrov. 2009. Coarse-to-Fine Natural Language
Processing. Ph.D. thesis, Department of Computer
Science, University of Berkeley.
Hoifung Poon and Pedro Domingos. 2007. Joint infer-
ence in information extraction. In Proceedings of the
Twenty Second National Conference on Artificial In-
telligence, pages 913–918, Vancouver, Canada. AAAI
Press.
Hoifung Poon and Pedro Domingos. 2008a. A general
method for reducing the complexity of relational in-
ference and its application to mcmc. In Proceedings
of the Twenty Third National Conference on Artificial
Intelligence, pages 1075–1080, Chicago, IL. AAAI
Press.
Hoifung Poon and Pedro Domingos. 2008b. Joint un-
supervised coreference resolution with Markov logic.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages 649–
658, Honolulu, HI. ACL.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1–10, Singapore. ACL.
Hoifung Poon and Pedro Domingos. 2010. Unsuper-
vised ontological induction from text. In submission.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
Proceedings of the Joint Conference on Human Lan-
guage Technology and Empirical Methods in Natural
Language Processing.
Matt Richardson and Pedro Domingos. 2003. Building
large knowledge bases by mass collaboration. In Pro-
ceedings of the Second International Conference on
Knowledge Capture, pages 129–137, Sanibel Island,
FL. ACM Press.
Alan Ritter, Doug Downey, Stephen Soderland, and Oren
Etzioni. 2008. It’s a contradiction – no, it’s not: A
case study using functional relations. In Proceedings
of the 2008 Conference on Empirical Methods in Nat-
ural Language Processing.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A latent
dirichlet allocation method for selectional preferences.
In submission.
Stefan Schoenmackers, Oren Etzioni, and Daniel S.
Weld. 2008. Scaling textual inference to the web.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing.
Stefan Schoenmackers, Jesse Davis, Oren Etzioni, and
Daniel S. Weld. 2010. Learning first-order horn
clauses from web text. In submission.
</reference>
<page confidence="0.990698">
94
</page>
<reference confidence="0.99914036">
Parag Singla and Pedro Domingos. 2006. Memory-
efficient inference in relational domains. In Proceed-
ings of the Twenty First National Conference on Arti-
ficial Intelligence.
Stephen Soderland, Brendan Roof, Bo Qin, Mausam, and
Oren Etzioni. 2010. Adapting open information ex-
traction to domain-specific relations. In submission.
Fei Wu and Daniel S. Weld. 2007. Automatically se-
mantifying wikipedia. In Proceedings of the Sixteenth
Conference on Information and Knowledge Manage-
ment, Lisbon, Portugal.
Fei Wu and Daniel S. Weld. 2008. Automatically refin-
ing the wikipedia infobox ontology. In Proceedings
of the Seventeenth International Conference on World
Wide Web, Beijing, China.
Fei Wu and Daniel S. Weld. 2010. Open information
extraction using wikipedia. In submission.
Fei Wu, Raphael Hoffmann, and Daniel S. Weld. 2008.
Information extraction from Wikipedia: Moving down
the long tail. In Proceedings of the Fourteenth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, Las Vegas, NV.
Alexander Yates and Oren Etzioni. 2007. Unsupervised
resolution of objects and relations on the web. In Pro-
ceedings of Human Language Technology (NAACL).
</reference>
<page confidence="0.999076">
95
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.829648">
<title confidence="0.995709">Machine Reading at the University of Washington</title>
<author confidence="0.945776">Hoifung Poon</author>
<author confidence="0.945776">Janara Christensen</author>
<author confidence="0.945776">Pedro Domingos</author>
<author confidence="0.945776">Oren Etzioni</author>
<author confidence="0.945776">Raphael Chloe Kiddon</author>
<author confidence="0.945776">Thomas Lin</author>
<author confidence="0.945776">Xiao Ling</author>
<author confidence="0.945776">Alan Ritter Mausam</author>
<author confidence="0.945776">Stefan Stephen Soderland</author>
<author confidence="0.945776">Dan Weld</author>
<author confidence="0.945776">Fei Wu</author>
<author confidence="0.945776">Congle</author>
<affiliation confidence="0.9998445">Department of Computer Science &amp; University of</affiliation>
<address confidence="0.999884">Seattle, WA 98195</address>
<abstract confidence="0.999252">Machine reading is a long-standing goal of AI and NLP. In recent years, tremendous progress has been made in developing machine learning approaches for many of its subtasks such as parsing, information extraction, and question answering. However, existing end-to-end solutions typically require substantial amount of human efforts (e.g., labeled data and/or manual engineering), and are not well poised for Web-scale knowledge acquisition. In this paper, we propose a unifying approach for machine reading by bootstrapping from the easiest extractable knowledge and conquering the long tail via a self-supervised learning process. This self-supervision is powered by joint inference based on Markov logic, and is made scalable by leveraging hierarchical structures and coarse-to-fine inference. Researchers at the University of Washington have taken the first steps in this direction. Our existing work explores the wide spectrum of this vision and shows its promise.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Doug Downey</author>
<author>Matthew Broadhead</author>
<author>Oren Etzioni</author>
</authors>
<title>Locating complex named entities in web text.</title>
<date>2007</date>
<booktitle>In Proceedings of the Twentieth International Joint Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="17590" citStr="Downey et al., 2007" startWordPosition="2634" endWordPosition="2637"> leveraging redundancy to rank candidate answers extracted from multiple search query results (Kwok et al., 2001); KnowItAll conducts open-domain information extraction via selfsupervision bootstrapping from Hearst patterns (Etzioni et al., 2005); Opine builds on KnowItAll and mines product reviews via self-supervision using joint inference over neighborhood features (Popescu and Etzioni, 2005); Kylin populates Wikipedia infoboxes via self-supervision bootstrapping from existing infoboxes (Wu and Weld, 2007); LEX conducts Web-scale name entity recognition by leveraging collocation statistics (Downey et al., 2007a); REALM improves sparse open-domain information extraction via relational clustering and language modeling (Downey et al., 2007b); RESOLVER performs entity and relation resolution via relational clustering (Yates and Etzioni, 2007); TextRunner conducts open-domain information extraction via self-supervision bootstrapping from heuristic rules (Banko et al., 2007); AuContraire automatically identifies contradictory statements in a large web corpus using functional relations (Ritter et al., 2008); HOLMES infers new facts from TextRunner output using Markov logic (Schoenmackers et al., 2008); KO</context>
</contexts>
<marker>Downey, Broadhead, Etzioni, 2007</marker>
<rawString>Doug Downey, Matthew Broadhead, and Oren Etzioni. 2007a. Locating complex named entities in web text. In Proceedings of the Twentieth International Joint Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Downey</author>
<author>Stefan Schoenmackers</author>
<author>Oren Etzioni</author>
</authors>
<title>Sparse information extraction: Unsupervised language models to the rescue.</title>
<date>2007</date>
<booktitle>In Proceedings of the Forty Fifth Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="17590" citStr="Downey et al., 2007" startWordPosition="2634" endWordPosition="2637"> leveraging redundancy to rank candidate answers extracted from multiple search query results (Kwok et al., 2001); KnowItAll conducts open-domain information extraction via selfsupervision bootstrapping from Hearst patterns (Etzioni et al., 2005); Opine builds on KnowItAll and mines product reviews via self-supervision using joint inference over neighborhood features (Popescu and Etzioni, 2005); Kylin populates Wikipedia infoboxes via self-supervision bootstrapping from existing infoboxes (Wu and Weld, 2007); LEX conducts Web-scale name entity recognition by leveraging collocation statistics (Downey et al., 2007a); REALM improves sparse open-domain information extraction via relational clustering and language modeling (Downey et al., 2007b); RESOLVER performs entity and relation resolution via relational clustering (Yates and Etzioni, 2007); TextRunner conducts open-domain information extraction via self-supervision bootstrapping from heuristic rules (Banko et al., 2007); AuContraire automatically identifies contradictory statements in a large web corpus using functional relations (Ritter et al., 2008); HOLMES infers new facts from TextRunner output using Markov logic (Schoenmackers et al., 2008); KO</context>
</contexts>
<marker>Downey, Schoenmackers, Etzioni, 2007</marker>
<rawString>Doug Downey, Stefan Schoenmackers, and Oren Etzioni. 2007b. Sparse information extraction: Unsupervised language models to the rescue. In Proceedings of the Forty Fifth Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Downey</author>
<author>Oren Etzioni</author>
<author>Stephen Soderland</author>
</authors>
<title>Analysis of a probabilistic model of redundancy in unsupervised information extraction.</title>
<date>2010</date>
<journal>In Artificial Intelligence.</journal>
<contexts>
<context position="6646" citStr="Downey et al., 2010" startWordPosition="1003" endWordPosition="1006"> in a self-supervised learning process that raises certainty on sparse extractions by propagating information via joint inference from frequent extractions. A key source of indirect supervision is meta knowledge about the domains. For example, the TextRunner system (Banko et al., 2007) hinges on the observation that there exist general, relationindependent patterns for information extraction. Another key source of indirect supervision is data redundancy. While a rare extracted fact or inference pattern may arise by chance of error, it is much less likely so for the ones with many repetitions (Downey et al., 2010). Such highly-redundant knowledge can be extracted easily and with high confidence, and can be leveraged for bootstrapping. For knowledge that resides in the long tail, explicit forms of redundancy (e.g., identical expressions) are rare, but this can be circumvented by joint inference. For example, expressions that are composed with or by similar expressions probably have the same meaning; the fact that kale prevents osteoporosis can be derived by combining the facts that kale contains calcium and that calcium helps prevent osteoporosis via a transitivity-through inference pattern. In general,</context>
</contexts>
<marker>Downey, Etzioni, Soderland, 2010</marker>
<rawString>Doug Downey, Oren Etzioni, and Stephen Soderland. 2010. Analysis of a probabilistic model of redundancy in unsupervised information extraction. In Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Michael Cafarella</author>
<author>Doug Downey</author>
<author>AnaMaria Popescu</author>
<author>Tal Shaked</author>
<author>Stephen Soderland</author>
<author>Daniel S Weld</author>
<author>Alexander Yates</author>
</authors>
<title>Unsupervised named-entity extraction from the web: An experimental study.</title>
<date>2005</date>
<journal>Artificial Intelligence,</journal>
<volume>165</volume>
<issue>1</issue>
<contexts>
<context position="17217" citStr="Etzioni et al., 2005" startWordPosition="2585" endWordPosition="2588">olid lines signify dataflow. At the top are the years of publications. ShopBot learns comparisonshopping agents via self-supervision using heuristic knowledge (Doorenbos et al., 1997); WIEN induces wrappers for information extraction via self-supervision using joint inference to combine simple atomic extractors (Kushmerick et al., 1997); Mulder answers factoid questions by leveraging redundancy to rank candidate answers extracted from multiple search query results (Kwok et al., 2001); KnowItAll conducts open-domain information extraction via selfsupervision bootstrapping from Hearst patterns (Etzioni et al., 2005); Opine builds on KnowItAll and mines product reviews via self-supervision using joint inference over neighborhood features (Popescu and Etzioni, 2005); Kylin populates Wikipedia infoboxes via self-supervision bootstrapping from existing infoboxes (Wu and Weld, 2007); LEX conducts Web-scale name entity recognition by leveraging collocation statistics (Downey et al., 2007a); REALM improves sparse open-domain information extraction via relational clustering and language modeling (Downey et al., 2007b); RESOLVER performs entity and relation resolution via relational clustering (Yates and Etzioni,</context>
</contexts>
<marker>Etzioni, Cafarella, Downey, Popescu, Shaked, Soderland, Weld, Yates, 2005</marker>
<rawString>Oren Etzioni, Michael Cafarella, Doug Downey, AnaMaria Popescu, Tal Shaked, Stephen Soderland, Daniel S. Weld, and Alexander Yates. 2005. Unsupervised named-entity extraction from the web: An experimental study. Artificial Intelligence, 165(1):91–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pedro F Felzenszwalb</author>
<author>David McAllester</author>
</authors>
<title>The generalized A* architecture.</title>
<date>2007</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>29</volume>
<contexts>
<context position="10009" citStr="Felzenszwalb and McAllester, 2007" startWordPosition="1499" endWordPosition="1502">ng sophisticated joint inference. Recently, joint inference has received increasing interest in AI, machine learning, and NLP, with Markov logic (Domingos and Lowd, 2009) being one of the leading unifying frameworks. Past work has shown that it can substantially improve predictive accuracy in supervised learning (e.g., (Getoor and Taskar, 2007; Bakir et al., 2007)). We propose to build on these advances, but apply joint inference beyond supervised learning, with labeled examples supplanted by indirect supervision. Another distinctive feature is that we propose to use coarse-to-fine inference (Felzenszwalb and McAllester, 2007; Petrov, 2009) as a unifying framework to scale inference to the Web. Essentially, coarse-to-fine inference leverages the sparsity imposed by hierarchical structures that are ubiquitous in human knowledge (e.g., taxonomies/ontologies). At coarse levels (top levels in a hierarchy), ambiguities are rare (there are few objects and relations), and inference can be conducted very efficiently. The result is then used to prune unpromising refinements at the next level. This process continues down the hierarchy until decision can be made. In this way, inference can potentially be sped up exponentiall</context>
</contexts>
<marker>Felzenszwalb, McAllester, 2007</marker>
<rawString>Pedro F. Felzenszwalb and David McAllester. 2007. The generalized A* architecture. Journal of Artificial Intelligence Research, 29.</rawString>
</citation>
<citation valid="true">
<title>Introduction to Statistical Relational Learning.</title>
<date>2007</date>
<editor>Lise Getoor and Ben Taskar, editors.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>2007</marker>
<rawString>Lise Getoor and Ben Taskar, editors. 2007. Introduction to Statistical Relational Learning. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Hoffmann</author>
<author>Saleema Amershi</author>
<author>Kayur Patel</author>
<author>Fei Wu</author>
<author>James Fogarty</author>
<author>Daniel S Weld</author>
</authors>
<title>Amplifying community content creation using mixedinitiative information extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of CHI-09.</booktitle>
<contexts>
<context position="26155" citStr="Hoffmann et al., 2009" startWordPosition="3863" endWordPosition="3866"> its emphasis on lifelong learning clusions contained in this document are those of the from user feedback resonates with our approach on authors and should not be interpreted as necessarily continuous learning. representing the official policies, either expressed or Past work at Washington has demonstrated the implied, of ARO, DARPA, NSF, ONR, or the United promise of our approach. For example, (Banko and States Government. Etzioni, 2007) automated theory formation based on References TextRunner extractions via a lifelong-learning pro- G. Bakir, T. Hofmann, B. B. Sch¨olkopf, A. Smola, cess; (Hoffmann et al., 2009) show that the pairing B. Taskar, S. Vishwanathan, and (eds.). 2007. Preof Kylin and community content creation benefits dicting Structured Data. MIT Press, Cambridge, MA. both by sharing Wikipedia edits; (Soderland et al., M. Banko and O. Etzioni. 2007. Strategies for lifelong 2010) successfully adapted the TextRunner open- knowledge extraction from the web. In Proceedings of domain information extraction system to specific do- the Sixteenth International Conference on Knowledge mains via active learning. Capture, British Columbia, Canada. Our approach also resonates with the never- Michele B</context>
</contexts>
<marker>Hoffmann, Amershi, Patel, Wu, Fogarty, Weld, 2009</marker>
<rawString>Raphael Hoffmann, Saleema Amershi, Kayur Patel, Fei Wu, James Fogarty, and Daniel S. Weld. 2009. Amplifying community content creation using mixedinitiative information extraction. In Proceedings of CHI-09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Hoffmann</author>
<author>Congle Zhang</author>
<author>Daniel S Weld</author>
</authors>
<title>Learning 5000 relational extractors. In submission.</title>
<date>2010</date>
<contexts>
<context position="20286" citStr="Hoffmann et al., 2010" startWordPosition="3001" endWordPosition="3004">omingos, 2010). SHERLOCK induces new inference rules via relational learning (Schoenmackers et al., 2010); SRL-IE conducts open-domain information extraction by bootstrapping from semantic role labels, PrecHybrid is a hybrid version between SRL-IE and TextRunner, which given a budget of computation time does better than either system (Christensen et al., 2010); WOE builds on Kylin and conducts open-domain information extraction (Wu and Weld, 2010); WPE learns 5000 relational extractors by bootstrapping from Wikipedia and using Web lists to generate dynamic, relation-specific lexicon features (Hoffmann et al., 2010). Kylin WOE OLPI WebTables Opine LEX Holmes Sherlock SRL-IE RESOLVER CR PrecHybrid SP LOFT IIA KnowItAll TextRunner SNE LDA-SP REALM AuContraire KOG WPE ShopBot WIEN Mulder 91 TextRunner, Kylin, KOG, WOE, WPE). Another uses unsupervised learning and often takes a particular form of relational clustering (e.g., objects associated with similar relations tend to be the same and vice versa, as in REALM, RESOLVER, SNE, UCR, USP, LDA-SP, LOFT, etc.). Some distinctive types of self-supervision include shrinkage based on an ontology (KOG, LOFT, OLPI), probabilistic inference via handcrafted or learned</context>
</contexts>
<marker>Hoffmann, Zhang, Weld, 2010</marker>
<rawString>Raphael Hoffmann, Congle Zhang, and Daniel S. Weld. 2010. Learning 5000 relational extractors. In submission.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chloe Kiddon</author>
<author>Pedro Domingos</author>
</authors>
<title>Ontological lifted probabilistic inference. In submission.</title>
<date>2010</date>
<contexts>
<context position="19678" citStr="Kiddon and Domingos, 2010" startWordPosition="2917" endWordPosition="2920">acts knowledge via recursive relational clustering with Markov logic (Poon and Domingos, 2009); LDA-SP automatically infers a compact representation describing the plausible arguments for a relation using an LDA-Style model and Bayesian Inference (Ritter et al., 2010); LOFT builds on USP and jointly performs ontology induction, population, and knowledge extraction via joint recursive relational clustering and shrinkage with Markov logic (Poon and Domingos, 2010); OLPI improves the efficiency of lifted probabilistic inference and learning via coarse-to-fine inference based on type hierarchies (Kiddon and Domingos, 2010). SHERLOCK induces new inference rules via relational learning (Schoenmackers et al., 2010); SRL-IE conducts open-domain information extraction by bootstrapping from semantic role labels, PrecHybrid is a hybrid version between SRL-IE and TextRunner, which given a budget of computation time does better than either system (Christensen et al., 2010); WOE builds on Kylin and conducts open-domain information extraction (Wu and Weld, 2010); WPE learns 5000 relational extractors by bootstrapping from Wikipedia and using Web lists to generate dynamic, relation-specific lexicon features (Hoffmann et al</context>
</contexts>
<marker>Kiddon, Domingos, 2010</marker>
<rawString>Chloe Kiddon and Pedro Domingos. 2010. Ontological lifted probabilistic inference. In submission.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley Kok</author>
<author>Pedro Domingos</author>
</authors>
<title>Extracting semantic networks from text via relational clustering.</title>
<date>2008</date>
<booktitle>In Proceedings of the Nineteenth European Conference on Machine Learning,</booktitle>
<pages>624--639</pages>
<publisher>Springer.</publisher>
<location>Antwerp, Belgium.</location>
<contexts>
<context position="18751" citStr="Kok and Domingos, 2008" startWordPosition="2790" endWordPosition="2793">ner output using Markov logic (Schoenmackers et al., 2008); KOG learns a rich ontology by combining Wikipedia infoboxes with WordNet via joint inference using Markov Logic Networks (Wu and Weld, 2008), shrinkage over this ontology vastly improves the recall of Kylin’s extractors; UCR performs state-of-the-art unsupervised coreference resolution by incorporating a small amount of domain knowledge and conducting joint inference among entity mentions with Markov logic (Poon and Domingos, 2008b); SNE constructs a semantic network over TextRunner output via relational clustering with Markov logic (Kok and Domingos, 2008); WebTables conducts Web-scale information extraction by leveraging HTML table structures (Cafarella et al., 2008); IIA learns from infoboxes to filter open-domain information extraction toward assertions that are interesting to people (Lin et al., 2009); USP jointly learns a semantic parser and extracts knowledge via recursive relational clustering with Markov logic (Poon and Domingos, 2009); LDA-SP automatically infers a compact representation describing the plausible arguments for a relation using an LDA-Style model and Bayesian Inference (Ritter et al., 2010); LOFT builds on USP and jointl</context>
</contexts>
<marker>Kok, Domingos, 2008</marker>
<rawString>Stanley Kok and Pedro Domingos. 2008. Extracting semantic networks from text via relational clustering. In Proceedings of the Nineteenth European Conference on Machine Learning, pages 624–639, Antwerp, Belgium. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicholas Kushmerick</author>
<author>Daniel S Weld</author>
<author>Robert Doorenbos</author>
</authors>
<title>Wrapper induction for information extraction.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="16934" citStr="Kushmerick et al., 1997" startWordPosition="2548" endWordPosition="2551">ured resources to generate noisy training examples for use by supervised learning methods and with cotraining (e.g., 90 1997 2001 2004 2005 2007 2008 2009 2010 Figure 2: The evolution of major machine reading systems at the University of Washington. Dashed lines signify influence and solid lines signify dataflow. At the top are the years of publications. ShopBot learns comparisonshopping agents via self-supervision using heuristic knowledge (Doorenbos et al., 1997); WIEN induces wrappers for information extraction via self-supervision using joint inference to combine simple atomic extractors (Kushmerick et al., 1997); Mulder answers factoid questions by leveraging redundancy to rank candidate answers extracted from multiple search query results (Kwok et al., 2001); KnowItAll conducts open-domain information extraction via selfsupervision bootstrapping from Hearst patterns (Etzioni et al., 2005); Opine builds on KnowItAll and mines product reviews via self-supervision using joint inference over neighborhood features (Popescu and Etzioni, 2005); Kylin populates Wikipedia infoboxes via self-supervision bootstrapping from existing infoboxes (Wu and Weld, 2007); LEX conducts Web-scale name entity recognition b</context>
</contexts>
<marker>Kushmerick, Weld, Doorenbos, 1997</marker>
<rawString>Nicholas Kushmerick, Daniel S. Weld, and Robert Doorenbos. 1997. Wrapper induction for information extraction. In Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cody Kwok</author>
<author>Oren Etzioni</author>
<author>Daniel S Weld</author>
</authors>
<title>Scaling question answering to the web.</title>
<date>2001</date>
<booktitle>In Proceedings of the Tenth International Conference on World Wide Web.</booktitle>
<contexts>
<context position="17084" citStr="Kwok et al., 2001" startWordPosition="2569" endWordPosition="2572">010 Figure 2: The evolution of major machine reading systems at the University of Washington. Dashed lines signify influence and solid lines signify dataflow. At the top are the years of publications. ShopBot learns comparisonshopping agents via self-supervision using heuristic knowledge (Doorenbos et al., 1997); WIEN induces wrappers for information extraction via self-supervision using joint inference to combine simple atomic extractors (Kushmerick et al., 1997); Mulder answers factoid questions by leveraging redundancy to rank candidate answers extracted from multiple search query results (Kwok et al., 2001); KnowItAll conducts open-domain information extraction via selfsupervision bootstrapping from Hearst patterns (Etzioni et al., 2005); Opine builds on KnowItAll and mines product reviews via self-supervision using joint inference over neighborhood features (Popescu and Etzioni, 2005); Kylin populates Wikipedia infoboxes via self-supervision bootstrapping from existing infoboxes (Wu and Weld, 2007); LEX conducts Web-scale name entity recognition by leveraging collocation statistics (Downey et al., 2007a); REALM improves sparse open-domain information extraction via relational clustering and lan</context>
</contexts>
<marker>Kwok, Etzioni, Weld, 2001</marker>
<rawString>Cody Kwok, Oren Etzioni, and Daniel S. Weld. 2001. Scaling question answering to the web. In Proceedings of the Tenth International Conference on World Wide Web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Lin</author>
<author>Oren Etzioni</author>
<author>James Fogarty</author>
</authors>
<title>Identifying interesting assertions from the web.</title>
<date>2009</date>
<booktitle>In Proceedings of the Eighteenth Conference on Information and Knowledge Management.</booktitle>
<contexts>
<context position="19005" citStr="Lin et al., 2009" startWordPosition="2824" endWordPosition="2827"> Kylin’s extractors; UCR performs state-of-the-art unsupervised coreference resolution by incorporating a small amount of domain knowledge and conducting joint inference among entity mentions with Markov logic (Poon and Domingos, 2008b); SNE constructs a semantic network over TextRunner output via relational clustering with Markov logic (Kok and Domingos, 2008); WebTables conducts Web-scale information extraction by leveraging HTML table structures (Cafarella et al., 2008); IIA learns from infoboxes to filter open-domain information extraction toward assertions that are interesting to people (Lin et al., 2009); USP jointly learns a semantic parser and extracts knowledge via recursive relational clustering with Markov logic (Poon and Domingos, 2009); LDA-SP automatically infers a compact representation describing the plausible arguments for a relation using an LDA-Style model and Bayesian Inference (Ritter et al., 2010); LOFT builds on USP and jointly performs ontology induction, population, and knowledge extraction via joint recursive relational clustering and shrinkage with Markov logic (Poon and Domingos, 2010); OLPI improves the efficiency of lifted probabilistic inference and learning via coars</context>
</contexts>
<marker>Lin, Etzioni, Fogarty, 2009</marker>
<rawString>Thomas Lin, Oren Etzioni, and James Fogarty. 2009. Identifying interesting assertions from the web. In Proceedings of the Eighteenth Conference on Information and Knowledge Management.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiao Ling</author>
<author>Daniel S Weld</author>
</authors>
<title>Temporal information extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of the Twenty Fifth National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="14463" citStr="Ling and Weld, 2010" startWordPosition="2178" endWordPosition="2181">ariety forms of joint inference have been used, ranging from simple voting to heuristic rules to sophisticated probabilistic models. All these can be compactly encoded in Markov logic (Domingos and Lowd, 2009), which provides a unifying framework for knowledge representation and joint inference. Past work at Washington has shown that in supervised learning, joint inference can substantially improve predictive performance on tasks related to machine reading (e.g., citation information extraction (Poon and Domingos, 2007), ontology induction (Wu and Weld, 2008), temporal information extraction (Ling and Weld, 2010)). In addition, it has demonstrated that sophisticated joint inference can enable effective learning without any labeled information (UCR, USP, LOFT), and that joint inference can scale to millions of Web documents by leveraging sparsity in naturally occurring relations (Holmes, Sherlock), showing the promise of our unifying approach. Simpler representations limit the expressiveness in representing knowledge and the degree of sophistication in joint inference, but they currently scale much better than more expressive ones. A key direction for future work is to evaluate this tradeoff more thoro</context>
</contexts>
<marker>Ling, Weld, 2010</marker>
<rawString>Xiao Ling and Daniel S. Weld. 2010. Temporal information extraction. In Proceedings of the Twenty Fifth National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jayant Madhavan</author>
</authors>
<title>Using known schemas and mappings to construct new semantic mappings.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Washington.</institution>
<contexts>
<context position="24141" citStr="Madhavan, 2005" startWordPosition="3568" endWordPosition="3569">approaches to probabilistic ontology induction. One approach is to bootstrap from existing ontological structures and apply self-supervision to correct the erroneous nodes and fill in the missing ones (KOG). Another approach is to integrate ontology induction with hierarchical smoothing, and jointly pursue unsupervised ontology induction, population and knowledge extraction (LOFT). A key direction for future work is to combine these two paradigms. As case studies in ontology integration, prior research has devised probabilistic schema mappings and corpus-based matching algorithms (Doan, 2002; Madhavan, 2005; Dong et al., 2007), and has automatically constructed mappings between the Wikipedia infobox “ontology” and the Freebase ontology. This latter endeavor illustrated the complexity of the necessary mappings: a simple attribute in one ontology may correspond to a complex relational view in the other, comprising three join operations; searching for such matches yields a search space with billions of possible correspondences for just a single attribute. Another key direction is to develop general methods for inducing multi-facet, multi-inheritance ontologies. Although single-inheritance, tree-lik</context>
</contexts>
<marker>Madhavan, 2005</marker>
<rawString>Jayant Madhavan. 2005. Using known schemas and mappings to construct new semantic mappings. Ph.D. thesis, University of Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
</authors>
<title>Coarse-to-Fine Natural Language Processing.</title>
<date>2009</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Science, University of Berkeley.</institution>
<contexts>
<context position="10024" citStr="Petrov, 2009" startWordPosition="1503" endWordPosition="1504">ecently, joint inference has received increasing interest in AI, machine learning, and NLP, with Markov logic (Domingos and Lowd, 2009) being one of the leading unifying frameworks. Past work has shown that it can substantially improve predictive accuracy in supervised learning (e.g., (Getoor and Taskar, 2007; Bakir et al., 2007)). We propose to build on these advances, but apply joint inference beyond supervised learning, with labeled examples supplanted by indirect supervision. Another distinctive feature is that we propose to use coarse-to-fine inference (Felzenszwalb and McAllester, 2007; Petrov, 2009) as a unifying framework to scale inference to the Web. Essentially, coarse-to-fine inference leverages the sparsity imposed by hierarchical structures that are ubiquitous in human knowledge (e.g., taxonomies/ontologies). At coarse levels (top levels in a hierarchy), ambiguities are rare (there are few objects and relations), and inference can be conducted very efficiently. The result is then used to prune unpromising refinements at the next level. This process continues down the hierarchy until decision can be made. In this way, inference can potentially be sped up exponentially, analogous to</context>
</contexts>
<marker>Petrov, 2009</marker>
<rawString>Slav Petrov. 2009. Coarse-to-Fine Natural Language Processing. Ph.D. thesis, Department of Computer Science, University of Berkeley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Joint inference in information extraction.</title>
<date>2007</date>
<booktitle>In Proceedings of the Twenty Second National Conference on Artificial Intelligence,</booktitle>
<pages>913--918</pages>
<publisher>AAAI Press.</publisher>
<location>Vancouver, Canada.</location>
<contexts>
<context position="14368" citStr="Poon and Domingos, 2007" startWordPosition="2164" endWordPosition="2167">E, RESOLVER), to arbitrary logical formulas and their clusters (e.g., in USP, LOFT). Similarly, a variety forms of joint inference have been used, ranging from simple voting to heuristic rules to sophisticated probabilistic models. All these can be compactly encoded in Markov logic (Domingos and Lowd, 2009), which provides a unifying framework for knowledge representation and joint inference. Past work at Washington has shown that in supervised learning, joint inference can substantially improve predictive performance on tasks related to machine reading (e.g., citation information extraction (Poon and Domingos, 2007), ontology induction (Wu and Weld, 2008), temporal information extraction (Ling and Weld, 2010)). In addition, it has demonstrated that sophisticated joint inference can enable effective learning without any labeled information (UCR, USP, LOFT), and that joint inference can scale to millions of Web documents by leveraging sparsity in naturally occurring relations (Holmes, Sherlock), showing the promise of our unifying approach. Simpler representations limit the expressiveness in representing knowledge and the degree of sophistication in joint inference, but they currently scale much better tha</context>
</contexts>
<marker>Poon, Domingos, 2007</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2007. Joint inference in information extraction. In Proceedings of the Twenty Second National Conference on Artificial Intelligence, pages 913–918, Vancouver, Canada. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>A general method for reducing the complexity of relational inference and its application to mcmc.</title>
<date>2008</date>
<booktitle>In Proceedings of the Twenty Third National Conference on Artificial Intelligence,</booktitle>
<pages>1075--1080</pages>
<publisher>AAAI Press.</publisher>
<location>Chicago, IL.</location>
<contexts>
<context position="18622" citStr="Poon and Domingos, 2008" startWordPosition="2772" endWordPosition="2775">radictory statements in a large web corpus using functional relations (Ritter et al., 2008); HOLMES infers new facts from TextRunner output using Markov logic (Schoenmackers et al., 2008); KOG learns a rich ontology by combining Wikipedia infoboxes with WordNet via joint inference using Markov Logic Networks (Wu and Weld, 2008), shrinkage over this ontology vastly improves the recall of Kylin’s extractors; UCR performs state-of-the-art unsupervised coreference resolution by incorporating a small amount of domain knowledge and conducting joint inference among entity mentions with Markov logic (Poon and Domingos, 2008b); SNE constructs a semantic network over TextRunner output via relational clustering with Markov logic (Kok and Domingos, 2008); WebTables conducts Web-scale information extraction by leveraging HTML table structures (Cafarella et al., 2008); IIA learns from infoboxes to filter open-domain information extraction toward assertions that are interesting to people (Lin et al., 2009); USP jointly learns a semantic parser and extracts knowledge via recursive relational clustering with Markov logic (Poon and Domingos, 2009); LDA-SP automatically infers a compact representation describing the plausi</context>
<context position="22088" citStr="Poon and Domingos, 2008" startWordPosition="3270" endWordPosition="3273">synergistically leverages diverse forms of supervision and information sources. 3.4 Large-Scale Joint Inference To apply sophisticated joint inference in machine reading, the major challenge is to make it scalable to billions of text documents. A general solution is to identify and leverage ubiquitous problem structures that lead to sparsity. For example, order of magnitude reduction in both memory and inference time can be achieved for relational inference by leveraging the fact that most relational atoms are false, which trivially satisfy most relational formulas (Singla and Domingos, 2006; Poon and Domingos, 2008a); joint inference with naturally occurring textual relations can scale to millions of Web pages by leveraging the fact that such relations are approximately functional (Schoenmackers et al., 2008). More generally, sparsity arises from hierarchical structures (e.g., ontologies) that are naturally exhibited in human knowledge, and can be leveraged to do coarse-to-fine inference (OLPI). The success of coarse-to-fine inference hinges on the availability and quality of hierarchical structures. Therefore, a key direction for future work is to automatically induce such hierarchies. (Also see next s</context>
</contexts>
<marker>Poon, Domingos, 2008</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2008a. A general method for reducing the complexity of relational inference and its application to mcmc. In Proceedings of the Twenty Third National Conference on Artificial Intelligence, pages 1075–1080, Chicago, IL. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Joint unsupervised coreference resolution with Markov logic.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>649--658</pages>
<publisher>ACL.</publisher>
<location>Honolulu, HI.</location>
<contexts>
<context position="18622" citStr="Poon and Domingos, 2008" startWordPosition="2772" endWordPosition="2775">radictory statements in a large web corpus using functional relations (Ritter et al., 2008); HOLMES infers new facts from TextRunner output using Markov logic (Schoenmackers et al., 2008); KOG learns a rich ontology by combining Wikipedia infoboxes with WordNet via joint inference using Markov Logic Networks (Wu and Weld, 2008), shrinkage over this ontology vastly improves the recall of Kylin’s extractors; UCR performs state-of-the-art unsupervised coreference resolution by incorporating a small amount of domain knowledge and conducting joint inference among entity mentions with Markov logic (Poon and Domingos, 2008b); SNE constructs a semantic network over TextRunner output via relational clustering with Markov logic (Kok and Domingos, 2008); WebTables conducts Web-scale information extraction by leveraging HTML table structures (Cafarella et al., 2008); IIA learns from infoboxes to filter open-domain information extraction toward assertions that are interesting to people (Lin et al., 2009); USP jointly learns a semantic parser and extracts knowledge via recursive relational clustering with Markov logic (Poon and Domingos, 2009); LDA-SP automatically infers a compact representation describing the plausi</context>
<context position="22088" citStr="Poon and Domingos, 2008" startWordPosition="3270" endWordPosition="3273">synergistically leverages diverse forms of supervision and information sources. 3.4 Large-Scale Joint Inference To apply sophisticated joint inference in machine reading, the major challenge is to make it scalable to billions of text documents. A general solution is to identify and leverage ubiquitous problem structures that lead to sparsity. For example, order of magnitude reduction in both memory and inference time can be achieved for relational inference by leveraging the fact that most relational atoms are false, which trivially satisfy most relational formulas (Singla and Domingos, 2006; Poon and Domingos, 2008a); joint inference with naturally occurring textual relations can scale to millions of Web pages by leveraging the fact that such relations are approximately functional (Schoenmackers et al., 2008). More generally, sparsity arises from hierarchical structures (e.g., ontologies) that are naturally exhibited in human knowledge, and can be leveraged to do coarse-to-fine inference (OLPI). The success of coarse-to-fine inference hinges on the availability and quality of hierarchical structures. Therefore, a key direction for future work is to automatically induce such hierarchies. (Also see next s</context>
</contexts>
<marker>Poon, Domingos, 2008</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2008b. Joint unsupervised coreference resolution with Markov logic. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 649– 658, Honolulu, HI. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Unsupervised semantic parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--10</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="19146" citStr="Poon and Domingos, 2009" startWordPosition="2844" endWordPosition="2847">edge and conducting joint inference among entity mentions with Markov logic (Poon and Domingos, 2008b); SNE constructs a semantic network over TextRunner output via relational clustering with Markov logic (Kok and Domingos, 2008); WebTables conducts Web-scale information extraction by leveraging HTML table structures (Cafarella et al., 2008); IIA learns from infoboxes to filter open-domain information extraction toward assertions that are interesting to people (Lin et al., 2009); USP jointly learns a semantic parser and extracts knowledge via recursive relational clustering with Markov logic (Poon and Domingos, 2009); LDA-SP automatically infers a compact representation describing the plausible arguments for a relation using an LDA-Style model and Bayesian Inference (Ritter et al., 2010); LOFT builds on USP and jointly performs ontology induction, population, and knowledge extraction via joint recursive relational clustering and shrinkage with Markov logic (Poon and Domingos, 2010); OLPI improves the efficiency of lifted probabilistic inference and learning via coarse-to-fine inference based on type hierarchies (Kiddon and Domingos, 2010). SHERLOCK induces new inference rules via relational learning (Scho</context>
</contexts>
<marker>Poon, Domingos, 2009</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2009. Unsupervised semantic parsing. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1–10, Singapore. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Unsupervised ontological induction from text.</title>
<date>2010</date>
<booktitle>In submission.</booktitle>
<contexts>
<context position="19518" citStr="Poon and Domingos, 2010" startWordPosition="2896" endWordPosition="2899">o filter open-domain information extraction toward assertions that are interesting to people (Lin et al., 2009); USP jointly learns a semantic parser and extracts knowledge via recursive relational clustering with Markov logic (Poon and Domingos, 2009); LDA-SP automatically infers a compact representation describing the plausible arguments for a relation using an LDA-Style model and Bayesian Inference (Ritter et al., 2010); LOFT builds on USP and jointly performs ontology induction, population, and knowledge extraction via joint recursive relational clustering and shrinkage with Markov logic (Poon and Domingos, 2010); OLPI improves the efficiency of lifted probabilistic inference and learning via coarse-to-fine inference based on type hierarchies (Kiddon and Domingos, 2010). SHERLOCK induces new inference rules via relational learning (Schoenmackers et al., 2010); SRL-IE conducts open-domain information extraction by bootstrapping from semantic role labels, PrecHybrid is a hybrid version between SRL-IE and TextRunner, which given a budget of computation time does better than either system (Christensen et al., 2010); WOE builds on Kylin and conducts open-domain information extraction (Wu and Weld, 2010); W</context>
</contexts>
<marker>Poon, Domingos, 2010</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2010. Unsupervised ontological induction from text. In submission.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana-Maria Popescu</author>
<author>Oren Etzioni</author>
</authors>
<title>Extracting product features and opinions from reviews.</title>
<date>2005</date>
<booktitle>In Proceedings of the Joint Conference on Human Language Technology and Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="17368" citStr="Popescu and Etzioni, 2005" startWordPosition="2605" endWordPosition="2608">ic knowledge (Doorenbos et al., 1997); WIEN induces wrappers for information extraction via self-supervision using joint inference to combine simple atomic extractors (Kushmerick et al., 1997); Mulder answers factoid questions by leveraging redundancy to rank candidate answers extracted from multiple search query results (Kwok et al., 2001); KnowItAll conducts open-domain information extraction via selfsupervision bootstrapping from Hearst patterns (Etzioni et al., 2005); Opine builds on KnowItAll and mines product reviews via self-supervision using joint inference over neighborhood features (Popescu and Etzioni, 2005); Kylin populates Wikipedia infoboxes via self-supervision bootstrapping from existing infoboxes (Wu and Weld, 2007); LEX conducts Web-scale name entity recognition by leveraging collocation statistics (Downey et al., 2007a); REALM improves sparse open-domain information extraction via relational clustering and language modeling (Downey et al., 2007b); RESOLVER performs entity and relation resolution via relational clustering (Yates and Etzioni, 2007); TextRunner conducts open-domain information extraction via self-supervision bootstrapping from heuristic rules (Banko et al., 2007); AuContrair</context>
</contexts>
<marker>Popescu, Etzioni, 2005</marker>
<rawString>Ana-Maria Popescu and Oren Etzioni. 2005. Extracting product features and opinions from reviews. In Proceedings of the Joint Conference on Human Language Technology and Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Richardson</author>
<author>Pedro Domingos</author>
</authors>
<title>Building large knowledge bases by mass collaboration.</title>
<date>2003</date>
<booktitle>In Proceedings of the Second International Conference on Knowledge Capture,</booktitle>
<pages>129--137</pages>
<publisher>ACM Press.</publisher>
<location>Sanibel Island, FL.</location>
<marker>Richardson, Domingos, 2003</marker>
<rawString>Matt Richardson and Pedro Domingos. 2003. Building large knowledge bases by mass collaboration. In Proceedings of the Second International Conference on Knowledge Capture, pages 129–137, Sanibel Island, FL. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Doug Downey</author>
<author>Stephen Soderland</author>
<author>Oren Etzioni</author>
</authors>
<title>It’s a contradiction – no, it’s not: A case study using functional relations.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="18090" citStr="Ritter et al., 2008" startWordPosition="2697" endWordPosition="2700">nd Weld, 2007); LEX conducts Web-scale name entity recognition by leveraging collocation statistics (Downey et al., 2007a); REALM improves sparse open-domain information extraction via relational clustering and language modeling (Downey et al., 2007b); RESOLVER performs entity and relation resolution via relational clustering (Yates and Etzioni, 2007); TextRunner conducts open-domain information extraction via self-supervision bootstrapping from heuristic rules (Banko et al., 2007); AuContraire automatically identifies contradictory statements in a large web corpus using functional relations (Ritter et al., 2008); HOLMES infers new facts from TextRunner output using Markov logic (Schoenmackers et al., 2008); KOG learns a rich ontology by combining Wikipedia infoboxes with WordNet via joint inference using Markov Logic Networks (Wu and Weld, 2008), shrinkage over this ontology vastly improves the recall of Kylin’s extractors; UCR performs state-of-the-art unsupervised coreference resolution by incorporating a small amount of domain knowledge and conducting joint inference among entity mentions with Markov logic (Poon and Domingos, 2008b); SNE constructs a semantic network over TextRunner output via rel</context>
</contexts>
<marker>Ritter, Downey, Soderland, Etzioni, 2008</marker>
<rawString>Alan Ritter, Doug Downey, Stephen Soderland, and Oren Etzioni. 2008. It’s a contradiction – no, it’s not: A case study using functional relations. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>A latent dirichlet allocation method for selectional preferences. In submission.</title>
<date>2010</date>
<contexts>
<context position="19320" citStr="Ritter et al., 2010" startWordPosition="2869" endWordPosition="2872">ustering with Markov logic (Kok and Domingos, 2008); WebTables conducts Web-scale information extraction by leveraging HTML table structures (Cafarella et al., 2008); IIA learns from infoboxes to filter open-domain information extraction toward assertions that are interesting to people (Lin et al., 2009); USP jointly learns a semantic parser and extracts knowledge via recursive relational clustering with Markov logic (Poon and Domingos, 2009); LDA-SP automatically infers a compact representation describing the plausible arguments for a relation using an LDA-Style model and Bayesian Inference (Ritter et al., 2010); LOFT builds on USP and jointly performs ontology induction, population, and knowledge extraction via joint recursive relational clustering and shrinkage with Markov logic (Poon and Domingos, 2010); OLPI improves the efficiency of lifted probabilistic inference and learning via coarse-to-fine inference based on type hierarchies (Kiddon and Domingos, 2010). SHERLOCK induces new inference rules via relational learning (Schoenmackers et al., 2010); SRL-IE conducts open-domain information extraction by bootstrapping from semantic role labels, PrecHybrid is a hybrid version between SRL-IE and Text</context>
</contexts>
<marker>Ritter, Mausam, Etzioni, 2010</marker>
<rawString>Alan Ritter, Mausam, and Oren Etzioni. 2010. A latent dirichlet allocation method for selectional preferences. In submission.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Schoenmackers</author>
<author>Oren Etzioni</author>
<author>Daniel S Weld</author>
</authors>
<title>Scaling textual inference to the web.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="18186" citStr="Schoenmackers et al., 2008" startWordPosition="2711" endWordPosition="2714">statistics (Downey et al., 2007a); REALM improves sparse open-domain information extraction via relational clustering and language modeling (Downey et al., 2007b); RESOLVER performs entity and relation resolution via relational clustering (Yates and Etzioni, 2007); TextRunner conducts open-domain information extraction via self-supervision bootstrapping from heuristic rules (Banko et al., 2007); AuContraire automatically identifies contradictory statements in a large web corpus using functional relations (Ritter et al., 2008); HOLMES infers new facts from TextRunner output using Markov logic (Schoenmackers et al., 2008); KOG learns a rich ontology by combining Wikipedia infoboxes with WordNet via joint inference using Markov Logic Networks (Wu and Weld, 2008), shrinkage over this ontology vastly improves the recall of Kylin’s extractors; UCR performs state-of-the-art unsupervised coreference resolution by incorporating a small amount of domain knowledge and conducting joint inference among entity mentions with Markov logic (Poon and Domingos, 2008b); SNE constructs a semantic network over TextRunner output via relational clustering with Markov logic (Kok and Domingos, 2008); WebTables conducts Web-scale info</context>
<context position="22286" citStr="Schoenmackers et al., 2008" startWordPosition="3300" endWordPosition="3303">to make it scalable to billions of text documents. A general solution is to identify and leverage ubiquitous problem structures that lead to sparsity. For example, order of magnitude reduction in both memory and inference time can be achieved for relational inference by leveraging the fact that most relational atoms are false, which trivially satisfy most relational formulas (Singla and Domingos, 2006; Poon and Domingos, 2008a); joint inference with naturally occurring textual relations can scale to millions of Web pages by leveraging the fact that such relations are approximately functional (Schoenmackers et al., 2008). More generally, sparsity arises from hierarchical structures (e.g., ontologies) that are naturally exhibited in human knowledge, and can be leveraged to do coarse-to-fine inference (OLPI). The success of coarse-to-fine inference hinges on the availability and quality of hierarchical structures. Therefore, a key direction for future work is to automatically induce such hierarchies. (Also see next subsection.) Moreover, given the desideratum of continuous learning from experience, and the speedy evolution of the Web (new contents, formats, etc.), it is important that we develop online methods </context>
</contexts>
<marker>Schoenmackers, Etzioni, Weld, 2008</marker>
<rawString>Stefan Schoenmackers, Oren Etzioni, and Daniel S. Weld. 2008. Scaling textual inference to the web. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Schoenmackers</author>
<author>Jesse Davis</author>
<author>Oren Etzioni</author>
<author>Daniel S Weld</author>
</authors>
<title>Learning first-order horn clauses from web text.</title>
<date>2010</date>
<booktitle>In submission.</booktitle>
<contexts>
<context position="19769" citStr="Schoenmackers et al., 2010" startWordPosition="2929" endWordPosition="2932">009); LDA-SP automatically infers a compact representation describing the plausible arguments for a relation using an LDA-Style model and Bayesian Inference (Ritter et al., 2010); LOFT builds on USP and jointly performs ontology induction, population, and knowledge extraction via joint recursive relational clustering and shrinkage with Markov logic (Poon and Domingos, 2010); OLPI improves the efficiency of lifted probabilistic inference and learning via coarse-to-fine inference based on type hierarchies (Kiddon and Domingos, 2010). SHERLOCK induces new inference rules via relational learning (Schoenmackers et al., 2010); SRL-IE conducts open-domain information extraction by bootstrapping from semantic role labels, PrecHybrid is a hybrid version between SRL-IE and TextRunner, which given a budget of computation time does better than either system (Christensen et al., 2010); WOE builds on Kylin and conducts open-domain information extraction (Wu and Weld, 2010); WPE learns 5000 relational extractors by bootstrapping from Wikipedia and using Web lists to generate dynamic, relation-specific lexicon features (Hoffmann et al., 2010). Kylin WOE OLPI WebTables Opine LEX Holmes Sherlock SRL-IE RESOLVER CR PrecHybrid </context>
</contexts>
<marker>Schoenmackers, Davis, Etzioni, Weld, 2010</marker>
<rawString>Stefan Schoenmackers, Jesse Davis, Oren Etzioni, and Daniel S. Weld. 2010. Learning first-order horn clauses from web text. In submission.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Parag Singla</author>
<author>Pedro Domingos</author>
</authors>
<title>Memoryefficient inference in relational domains.</title>
<date>2006</date>
<booktitle>In Proceedings of the Twenty First National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="22063" citStr="Singla and Domingos, 2006" startWordPosition="3266" endWordPosition="3269">ng learning framework that synergistically leverages diverse forms of supervision and information sources. 3.4 Large-Scale Joint Inference To apply sophisticated joint inference in machine reading, the major challenge is to make it scalable to billions of text documents. A general solution is to identify and leverage ubiquitous problem structures that lead to sparsity. For example, order of magnitude reduction in both memory and inference time can be achieved for relational inference by leveraging the fact that most relational atoms are false, which trivially satisfy most relational formulas (Singla and Domingos, 2006; Poon and Domingos, 2008a); joint inference with naturally occurring textual relations can scale to millions of Web pages by leveraging the fact that such relations are approximately functional (Schoenmackers et al., 2008). More generally, sparsity arises from hierarchical structures (e.g., ontologies) that are naturally exhibited in human knowledge, and can be leveraged to do coarse-to-fine inference (OLPI). The success of coarse-to-fine inference hinges on the availability and quality of hierarchical structures. Therefore, a key direction for future work is to automatically induce such hier</context>
</contexts>
<marker>Singla, Domingos, 2006</marker>
<rawString>Parag Singla and Pedro Domingos. 2006. Memoryefficient inference in relational domains. In Proceedings of the Twenty First National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Soderland</author>
<author>Brendan Roof</author>
<author>Bo Qin</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>Adapting open information extraction to domain-specific relations.</title>
<date>2010</date>
<booktitle>In submission.</booktitle>
<marker>Soderland, Roof, Qin, Mausam, Etzioni, 2010</marker>
<rawString>Stephen Soderland, Brendan Roof, Bo Qin, Mausam, and Oren Etzioni. 2010. Adapting open information extraction to domain-specific relations. In submission.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Wu</author>
<author>Daniel S Weld</author>
</authors>
<title>Automatically semantifying wikipedia.</title>
<date>2007</date>
<booktitle>In Proceedings of the Sixteenth Conference on Information and Knowledge Management,</booktitle>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="17484" citStr="Wu and Weld, 2007" startWordPosition="2620" endWordPosition="2623">erence to combine simple atomic extractors (Kushmerick et al., 1997); Mulder answers factoid questions by leveraging redundancy to rank candidate answers extracted from multiple search query results (Kwok et al., 2001); KnowItAll conducts open-domain information extraction via selfsupervision bootstrapping from Hearst patterns (Etzioni et al., 2005); Opine builds on KnowItAll and mines product reviews via self-supervision using joint inference over neighborhood features (Popescu and Etzioni, 2005); Kylin populates Wikipedia infoboxes via self-supervision bootstrapping from existing infoboxes (Wu and Weld, 2007); LEX conducts Web-scale name entity recognition by leveraging collocation statistics (Downey et al., 2007a); REALM improves sparse open-domain information extraction via relational clustering and language modeling (Downey et al., 2007b); RESOLVER performs entity and relation resolution via relational clustering (Yates and Etzioni, 2007); TextRunner conducts open-domain information extraction via self-supervision bootstrapping from heuristic rules (Banko et al., 2007); AuContraire automatically identifies contradictory statements in a large web corpus using functional relations (Ritter et al.,</context>
</contexts>
<marker>Wu, Weld, 2007</marker>
<rawString>Fei Wu and Daniel S. Weld. 2007. Automatically semantifying wikipedia. In Proceedings of the Sixteenth Conference on Information and Knowledge Management, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Wu</author>
<author>Daniel S Weld</author>
</authors>
<title>Automatically refining the wikipedia infobox ontology.</title>
<date>2008</date>
<booktitle>In Proceedings of the Seventeenth International Conference on World Wide Web,</booktitle>
<location>Beijing, China.</location>
<contexts>
<context position="14408" citStr="Wu and Weld, 2008" startWordPosition="2171" endWordPosition="2174">d their clusters (e.g., in USP, LOFT). Similarly, a variety forms of joint inference have been used, ranging from simple voting to heuristic rules to sophisticated probabilistic models. All these can be compactly encoded in Markov logic (Domingos and Lowd, 2009), which provides a unifying framework for knowledge representation and joint inference. Past work at Washington has shown that in supervised learning, joint inference can substantially improve predictive performance on tasks related to machine reading (e.g., citation information extraction (Poon and Domingos, 2007), ontology induction (Wu and Weld, 2008), temporal information extraction (Ling and Weld, 2010)). In addition, it has demonstrated that sophisticated joint inference can enable effective learning without any labeled information (UCR, USP, LOFT), and that joint inference can scale to millions of Web documents by leveraging sparsity in naturally occurring relations (Holmes, Sherlock), showing the promise of our unifying approach. Simpler representations limit the expressiveness in representing knowledge and the degree of sophistication in joint inference, but they currently scale much better than more expressive ones. A key direction </context>
<context position="18328" citStr="Wu and Weld, 2008" startWordPosition="2733" endWordPosition="2736">al., 2007b); RESOLVER performs entity and relation resolution via relational clustering (Yates and Etzioni, 2007); TextRunner conducts open-domain information extraction via self-supervision bootstrapping from heuristic rules (Banko et al., 2007); AuContraire automatically identifies contradictory statements in a large web corpus using functional relations (Ritter et al., 2008); HOLMES infers new facts from TextRunner output using Markov logic (Schoenmackers et al., 2008); KOG learns a rich ontology by combining Wikipedia infoboxes with WordNet via joint inference using Markov Logic Networks (Wu and Weld, 2008), shrinkage over this ontology vastly improves the recall of Kylin’s extractors; UCR performs state-of-the-art unsupervised coreference resolution by incorporating a small amount of domain knowledge and conducting joint inference among entity mentions with Markov logic (Poon and Domingos, 2008b); SNE constructs a semantic network over TextRunner output via relational clustering with Markov logic (Kok and Domingos, 2008); WebTables conducts Web-scale information extraction by leveraging HTML table structures (Cafarella et al., 2008); IIA learns from infoboxes to filter open-domain information e</context>
</contexts>
<marker>Wu, Weld, 2008</marker>
<rawString>Fei Wu and Daniel S. Weld. 2008. Automatically refining the wikipedia infobox ontology. In Proceedings of the Seventeenth International Conference on World Wide Web, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Wu</author>
<author>Daniel S Weld</author>
</authors>
<title>Open information extraction using wikipedia. In submission.</title>
<date>2010</date>
<contexts>
<context position="20115" citStr="Wu and Weld, 2010" startWordPosition="2978" endWordPosition="2981">n and Domingos, 2010); OLPI improves the efficiency of lifted probabilistic inference and learning via coarse-to-fine inference based on type hierarchies (Kiddon and Domingos, 2010). SHERLOCK induces new inference rules via relational learning (Schoenmackers et al., 2010); SRL-IE conducts open-domain information extraction by bootstrapping from semantic role labels, PrecHybrid is a hybrid version between SRL-IE and TextRunner, which given a budget of computation time does better than either system (Christensen et al., 2010); WOE builds on Kylin and conducts open-domain information extraction (Wu and Weld, 2010); WPE learns 5000 relational extractors by bootstrapping from Wikipedia and using Web lists to generate dynamic, relation-specific lexicon features (Hoffmann et al., 2010). Kylin WOE OLPI WebTables Opine LEX Holmes Sherlock SRL-IE RESOLVER CR PrecHybrid SP LOFT IIA KnowItAll TextRunner SNE LDA-SP REALM AuContraire KOG WPE ShopBot WIEN Mulder 91 TextRunner, Kylin, KOG, WOE, WPE). Another uses unsupervised learning and often takes a particular form of relational clustering (e.g., objects associated with similar relations tend to be the same and vice versa, as in REALM, RESOLVER, SNE, UCR, USP, L</context>
</contexts>
<marker>Wu, Weld, 2010</marker>
<rawString>Fei Wu and Daniel S. Weld. 2010. Open information extraction using wikipedia. In submission.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Wu</author>
<author>Raphael Hoffmann</author>
<author>Daniel S Weld</author>
</authors>
<title>Information extraction from Wikipedia: Moving down the long tail.</title>
<date>2008</date>
<booktitle>In Proceedings of the Fourteenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<location>Las Vegas, NV.</location>
<contexts>
<context position="21057" citStr="Wu et al., 2008" startWordPosition="3115" endWordPosition="3118">opBot WIEN Mulder 91 TextRunner, Kylin, KOG, WOE, WPE). Another uses unsupervised learning and often takes a particular form of relational clustering (e.g., objects associated with similar relations tend to be the same and vice versa, as in REALM, RESOLVER, SNE, UCR, USP, LDA-SP, LOFT, etc.). Some distinctive types of self-supervision include shrinkage based on an ontology (KOG, LOFT, OLPI), probabilistic inference via handcrafted or learned inference patterns (Holmes, Sherlock), and cotraining using relation-specific and relation-independent (open) extraction to reinforce semantic coherence (Wu et al., 2008). A key direction for future work is to develop a unifying framework for self-supervised learning by combining the strengths of existing methods and overcoming their limitations. This will likely take the form of a new learning paradigm that combines existing paradigms such as supervised learning, relational clustering, semi-supervised learning, and active learning into a unifying learning framework that synergistically leverages diverse forms of supervision and information sources. 3.4 Large-Scale Joint Inference To apply sophisticated joint inference in machine reading, the major challenge i</context>
</contexts>
<marker>Wu, Hoffmann, Weld, 2008</marker>
<rawString>Fei Wu, Raphael Hoffmann, and Daniel S. Weld. 2008. Information extraction from Wikipedia: Moving down the long tail. In Proceedings of the Fourteenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Las Vegas, NV.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yates</author>
<author>Oren Etzioni</author>
</authors>
<title>Unsupervised resolution of objects and relations on the web.</title>
<date>2007</date>
<booktitle>In Proceedings of Human Language Technology (NAACL).</booktitle>
<contexts>
<context position="17823" citStr="Yates and Etzioni, 2007" startWordPosition="2663" endWordPosition="2666">ioni et al., 2005); Opine builds on KnowItAll and mines product reviews via self-supervision using joint inference over neighborhood features (Popescu and Etzioni, 2005); Kylin populates Wikipedia infoboxes via self-supervision bootstrapping from existing infoboxes (Wu and Weld, 2007); LEX conducts Web-scale name entity recognition by leveraging collocation statistics (Downey et al., 2007a); REALM improves sparse open-domain information extraction via relational clustering and language modeling (Downey et al., 2007b); RESOLVER performs entity and relation resolution via relational clustering (Yates and Etzioni, 2007); TextRunner conducts open-domain information extraction via self-supervision bootstrapping from heuristic rules (Banko et al., 2007); AuContraire automatically identifies contradictory statements in a large web corpus using functional relations (Ritter et al., 2008); HOLMES infers new facts from TextRunner output using Markov logic (Schoenmackers et al., 2008); KOG learns a rich ontology by combining Wikipedia infoboxes with WordNet via joint inference using Markov Logic Networks (Wu and Weld, 2008), shrinkage over this ontology vastly improves the recall of Kylin’s extractors; UCR performs s</context>
</contexts>
<marker>Yates, Etzioni, 2007</marker>
<rawString>Alexander Yates and Oren Etzioni. 2007. Unsupervised resolution of objects and relations on the web. In Proceedings of Human Language Technology (NAACL).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>