<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001484">
<title confidence="0.960265">
Refining the most frequent sense baseline
</title>
<author confidence="0.991969">
Judita Preiss
</author>
<affiliation confidence="0.9940225">
Department of Linguistics
The Ohio State University
</affiliation>
<email confidence="0.991974">
judita@ling.ohio-state.edu
</email>
<author confidence="0.997218">
Josh King
</author>
<affiliation confidence="0.9961245">
Computer Science and Engineering
The Ohio State University
</affiliation>
<email confidence="0.99847">
kingjo@cse.ohio-state.edu
</email>
<sectionHeader confidence="0.995643" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999033">
We refine the most frequent sense baseline
for word sense disambiguation using a num-
ber of novel word sense disambiguation tech-
niques. Evaluating on the SENSEVAL-3 English
all words task, our combined system focuses
on improving every stage of word sense dis-
ambiguation: starting with the lemmatization
and part of speech tags used, through the ac-
curacy of the most frequent sense baseline, to
highly targeted individual systems. Our super-
vised systems include a ranking algorithm and
a Wikipedia similarity measure.
</bodyText>
<sectionHeader confidence="0.99895" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998907210526316">
The difficulty of outperforming the most frequent
sense baseline, the assignment of the sense which
appears most often in a given annotated corpus, in
word sense disambiguation (WSD) has been brought
to light by the recent SENSEVAL WSD system evalu-
ation exercises. In this work, we present a combi-
nation system, which, rather than designing a single
approach to all words, enriches the most frequent
sense baseline when there is high confidence for an
alternative sense to be chosen.
WSD, the task of assigning a sense to a given
word from a sense inventory is clearly necessary
for other natural language processing tasks. For ex-
ample, when performing machine translation, it is
necessary to distinguish between word senses in the
original language if the different senses have differ-
ent possible translations in the target language (Yn-
gve, 1955). A number of different approaches to
WSD have been explored in recent years, with two
</bodyText>
<author confidence="0.808955">
Jon Dehdari
</author>
<affiliation confidence="0.9435425">
Department of Linguistics
The Ohio State University
</affiliation>
<email confidence="0.968551">
jonsafari@ling.ohio-state.edu
</email>
<author confidence="0.950808">
Dennis Mehay
</author>
<affiliation confidence="0.99663">
Department of Linguistics
The Ohio State University
</affiliation>
<email confidence="0.99307">
mehay@ling.ohio-state.edu
</email>
<bodyText confidence="0.999912096774193">
distinct approaches: techniques which require anno-
tated training data (supervised techniques) and tech-
niques which do not (unsupervised methods).
It has long been believed that supervised systems,
which can be tuned to a word’s context, greatly out-
perform unsupervised systems. This theory was sup-
ported in the SENSEVAL WSD system evaluation exer-
cises, where the performance gap between the best
supervised system and the best unsupervised sys-
tem is large. Unsupervised systems were found to
never outperform the most frequent sense (MFS)
baseline (a sense assignment made on the basis of
the most frequent sense in an annotated corpus),
while supervised systems occasionally perform bet-
ter than the MFS baseline, though rarely by more
than 5%. However, recent work by McCarthy et al.
(2007) shows that acquiring a predominant sense
from an unannotated corpus can outperform many
supervised systems, and under certain conditions
will also outperform the MFS baseline.
Rather than proposing a new algorithm which will
tackle all words, we focus on improving upon the
MFS baseline system when an alternative system
proposes a high confidence answer. An MFS refin-
ing system can therefore benefit from answers sug-
gested by a very low recall (but high precision) WSD
system. We propose a number of novel approaches
to WSD, but also demonstrate the importance of a
highly accurate lemmatizer and part of speech tag-
ger to the English all words task of SENSEVAL-3.1
We present our enriched most frequent sense
</bodyText>
<footnote confidence="0.93033">
1Unless specified otherwise, we use WordNet 1.7.1 (Miller
et al., 1990) and the associated sense annotated SemCor cor-
pus (Miller et al., 1993) (translated to WordNet 1.7.1 by Rada
Mihalcea).
</footnote>
<page confidence="0.97025">
10
</page>
<note confidence="0.992211">
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 10–18,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999095625">
baseline in Section 2, which motivates the lemma-
tizer and part of speech tagger refinements presented
in Section 3. Our novel high precision WSD al-
gorithms include a reranking algorithm (Section 4),
and a Wikipedia-based similarity measure (Sec-
tion 5). The individual systems are combined in
Section 6, and we close with our conclusions in Sec-
tion 7.
</bodyText>
<sectionHeader confidence="0.893822" genericHeader="introduction">
2 Most frequent sense baseline
</sectionHeader>
<bodyText confidence="0.95910514893617">
The most frequent sense (MFS) baseline assumes
a sense annotated corpus from which the frequen-
cies of individual senses are learnt. For each tar-
get word, a part of speech tagger is used to deter-
mine the word’s part of speech, and the MFS for
that part of speech is selected. Although this is a
fairly naive baseline, it has been shown to be diffi-
cult to beat, with only 5 systems of the 26 submitted
to the SENSEvnr.-3 English all words task outperform-
ing the reported 62.5% MFS baseline. The success
of the MFS baseline is mainly due to the frequency
distribution of senses, with the shape of the sense
rank versus frequency graph being a Zipfian curve
(i.e., the top-ranked sense being much more likely
than any other sense).
However, two different MFS baseline perfor-
mance results are reported in Snyder and Palmer
(2004), with further implementations being differ-
ent still. The differences in performance of the MFS
baseline can be attributed to a number of factors:
the English all words task is run on natural text and
therefore performance greatly depends on the accu-
racy of the lemmatizer and the part of speech tag-
ger employed.2 If the lemmatizer incorrectly iden-
tifies the stem of the word, the MFS will be looked
up for the wrong word and the resulting sense as-
signment will be incorrect. The performance of the
MFS given the correct lemma and part of speech
information is 66%, while the performance of the
MFS with a Port Stemmer without any POS infor-
mation is 32%. With a TreeTagger (Schmidt, 1994),
and a sophisticated lemma back-off strategy, the per-
formance increases to 56%. It is this difference in
2Other possible factors include: 1) The sense distribution in
the corpus which the MFS baseline is drawn from, 2) If SemCor
is used as the underlying sense annotated corpus, the accuracy
of the mapping from WordNet 1.6 (with which SemCor was
initially annotated) to WordNet 1.7.1 could also have an effect
on the performance).
performance which motivates refining the most fre-
quent sense baseline, and our work on improving
the underlying lemmatizer and part of speech tagger
presented in Section 3.
Our initial investigation refines the SemCor based
MFS baseline using the automatic method of de-
termining the predominant sense presented in Mc-
Carthy et al. (2007).
</bodyText>
<listItem confidence="0.974702428571429">
1. For nouns and adjectives which appear in Sem-
Cor fewer than 5 times, we employ the auto-
matically determined predominant sense.
2. For verbs which appear in SemCor fewer than 5
times, we employ subcategorization frame sim-
ilarity rather than Lesk similarity to give us a
verb’s predominant sense.
</listItem>
<subsectionHeader confidence="0.99736">
2.1 Predominant sense
</subsectionHeader>
<bodyText confidence="0.999992">
McCarthy et al. (2007) demonstrate that it is possi-
ble to acquire the predominant sense for a word in
a corpus without having access to annotated data.
They employ an automatically created thesaurus
(Lin, 1998), and a sense–word similarity metric to
assign to each sense si of a word w a score corre-
sponding to
where dss(w, nj) reflects the distributional simi-
larity of word w to nj, w’s thesaural neighbour, and
sss(si, nj) = maxsxEsenses(nj) sss&apos;(si, sx) is the max-
imum similarity3 between w’s sense si and a sense
sx of w’s thesaural neighbour nj. The authors show
that although this method does not always outper-
form the MFS baseline based on SemCor, it does
outperform it when the word’s SemCor frequency is
below 5. We therefore switch our MFS baseline to
this value for such words. This result is represented
as ’McCarthy’ in Table 1, which contains the results
of the techniques presented in this Section evaluated
on the SENSEvnr.-3 English all words task.
</bodyText>
<subsectionHeader confidence="0.99937">
2.2 Verb predominant sense
</subsectionHeader>
<bodyText confidence="0.9929645">
McCarthy et al. (2007) observe that their predom-
inant sense method is not performing as well for
</bodyText>
<footnote confidence="0.8161975">
3We use the Lesk (overlap) similarity as implemented by the
WordNet::similarity package (Pedersen et al., 2004).
</footnote>
<equation confidence="0.543679666666667">
z dss(w, nj) * sss(si, nj)
njENw
Es&apos;iEsenses(w) sss(s&apos;i, nj)
</equation>
<page confidence="0.877499">
11
</page>
<table confidence="0.9998466">
System Precision Recall F-measure
MFS 58.4% 58.4% 58.4%
McCarthy 58.5% 58.5% 58.5%
Verbs 58.5% 58.5% 58.5%
All 58.6% 58.6% 58.6%
</table>
<tableCaption confidence="0.972768">
Table 1: Refining the MFS baseline with predominant
sense
</tableCaption>
<bodyText confidence="0.990941285714286">
verbs as it does for nouns and adjectives. We hy-
pothesize that this is due to the thesaural neighbours
obtained from Lin’s thesaurus, and we group verbs
according to the subcategorization frame (SCF) dis-
tributions they present in the vnr.Ex (Korhonen et al.,
2006) lexicon. A word w1 is grouped with word w2
if the Bhattacharyya coefficient
</bodyText>
<equation confidence="0.9887315">
�BC(w1, w2) = � p(x)q(x)
x∈X
</equation>
<bodyText confidence="0.999933">
where p(x) and q(x) represent the probability val-
ues for subcategorization class x, is above a cer-
tain threshold. The BC coefficient then replaces the
dss value in the original formula and the predomi-
nant senses are obtained. Again, this system is only
used for words with frequency lower than 5 in Sem-
Cor. The great advantage of the Bhattacharyya co-
efficient over various entropy based similarity mea-
sures which are usually used to compare SCF distri-
butions (Korhonen and Krymolowski, 2002), is that
it is guaranteed to lie between 0 and 1, unlike the
entropy based measures which are not easily com-
parable between different word pairs. This result is
represented by ’Verbs’ in Table 1.
Table 1 displays the results for the MFS, the MFS
combined with the two approaches described above,
and the MFS combining MFS with verbs and Mc-
Carthy.
</bodyText>
<sectionHeader confidence="0.7546435" genericHeader="method">
3 Lemmatization and Part of Speech
Tagging
</sectionHeader>
<bodyText confidence="0.999841333333333">
We made use of several lemmatizers and part-of-
speech taggers, in order to give the other WSD com-
ponents the best starting point possible.
</bodyText>
<subsectionHeader confidence="0.998515">
3.1 Lemmatization
</subsectionHeader>
<bodyText confidence="0.99998780952381">
Lemmatization, the process of obtaining the canon-
ical form of a word, was the first step for us to
ultimately identify the correct WordNet sense of
a given word in the English all words task. We
found that without any lemmatizing of the test input,
the maximum f-score possible was in the mid-50’s.
Conversely, we found that a basic most-frequent-
sense system that had a perfectly-lemmatized input
achieved an f-score in the mid-60’s. This large dif-
ference in the ceiling of a non-lemmatized system
and the floor of a perfectly-lemmatized system mo-
tivated us to focus on this task.
We looked at three different lemmatizers: the lem-
matizing backend of the XTAG project (XTAG Re-
search Group, 2001)4, Celex (Baayen et al., 1995),
and the lemmatizing component of an enhanced
TBL tagger (Brill, 1992).5 We then employed a vot-
ing system on these three components, taking the
lemma from the most individual lemmatizers. If all
three differ, we take the lemma from the most accu-
rate individual system, namely the TBL tagger.
</bodyText>
<subsectionHeader confidence="0.620549">
3.1.1 Lemmatizer Evaluation
</subsectionHeader>
<bodyText confidence="0.9999966">
We evaluated the lemmatizers against the lem-
mas found in the SENsEvnr.-3 gold standard.6 Even
the lowest performing system improved accuracy
by 31.74% over the baseline, which baseline sim-
ply equates the given token with the lemma. Ta-
ble 2 shows the results of evaluating the lemmatizers
against the EAW key.
While the simple voting system performed bet-
ter than any of the individual lemmatizers, hyphen-
ated words proved problematic for all of the sys-
tems. Some hyphenated words in the test set re-
mained hyphenated in the gold standard, and some
others were separated. However, evaluation results
show that splitting hyphenated words increases lem-
matizing accuracy by 0.9%.
</bodyText>
<subsectionHeader confidence="0.999984">
3.2 Part of Speech Tagging
</subsectionHeader>
<bodyText confidence="0.9988524">
We also investigated the contribution of part of
speech taggers to the task of word sense disam-
biguation. We considered three taggers: the El-
worthy bigram tagger (Elworthy, 1994) within the
RASP parser (Briscoe et al., 2006), an enhanced
</bodyText>
<footnote confidence="0.968728714285714">
4http://www.cis.upenn.edu/˜xtag
5http://gposttl.sourceforge.net
6We removed those lines from both the test input and the
gold standard which were marked U (= unknown, 34 lines), and
we removed the 40 lines from the test input that were missing
from the gold standard. This gave us 2007 words in both the
test set and the gold standard.
</footnote>
<page confidence="0.98482">
12
</page>
<table confidence="0.999165142857143">
Lemmatizer Accuracy
Baseline 57.50%
XTAG 89.24%
Celex 91.58%
TBL 92.38%
Voting {XTAG,Celex,TBL} 93.77%
Voting, no hyphen {XTAG,Celex,TBL} 94.67%
</table>
<tableCaption confidence="0.995408">
Table 2: Accuracy of several lemmatizers on &lt;head&gt;
words of EAW task.
</tableCaption>
<bodyText confidence="0.999942666666667">
TBL tagger (Brill, 1992)7, and a TnT-style trigram
tagger (Hal´acsy et al., 2007).8 The baseline was a
unigram tagger which selects the most frequently-
occurring tag of singletons when dealing with un-
seen words.
All three of the main taggers performed compa-
rably, although only the Elworthy tagger provides
probabilities associated with tags, rather than get-
ting a single tag as output. This additional infor-
mation can be useful, since we can employ differ-
ent strategies for a word with one single tag with a
probability of 1, versus a word with multiple tags,
the most probable of which might only have a prob-
ability of 0.3 for example. For comparative pur-
poses, we mapped the various instantiations of tags
for nouns, verbs, adjectives, and adverbs to these
four basic tags, and evaluated the taggers’ results
against the EAW key. Table 3 shows the results of
this evaluation.
The performance of these taggers on the EAW
&lt;head&gt;-words is lower than results reported on
other datasets. This can explained by the lack of
frequently-occurring function words, which are easy
to tag and raise overall accuracy. Also, the words
in the test set are often highly ambiguous not only
with respect to their word sense, but also their part
of speech.
</bodyText>
<sectionHeader confidence="0.97072" genericHeader="method">
4 Supervised Learning of Sparse Category
Indices for WSD
</sectionHeader>
<bodyText confidence="0.9997692">
In this component of our refinement of the base-
line, we train a supervised system that performs
higher-precision classification, only returning an an-
swer when a predictive feature that strongly pre-
dicts a particular sense is observed. To achieve this,
</bodyText>
<footnote confidence="0.999725">
7http://gposttl.sourceforge.net
8http://code.google.com/p/hunpos
</footnote>
<table confidence="0.998860833333333">
POS Tagger Accuracy
Baseline 84.10%
TBL 90.48%
Elworthy 90.58%
TnT 91.13%
Voting {TBL,Elw.,TnT} 91.88%
</table>
<tableCaption confidence="0.997096">
Table 3: Accuracy of several POS taggers on &lt;head&gt;
words of EAW task.
</tableCaption>
<bodyText confidence="0.9978592">
we implemented a “feature focus” classifier (sparse
weighted index) as described in (Madani and Con-
nor, 2008, henceforth, MC08). MC08’s methods
for restricting and pruning the number of feature-to-
class associations are useful for finding and retain-
ing only strong predictive features. Moreover, this
allowed us to use a rich feature set (more than 1.6
million features) without an unwieldy explosion in
the number of parameters, as feature-class associa-
tions that are not strong enough are simply dropped.
</bodyText>
<subsectionHeader confidence="0.997707">
4.1 Sparse Category Indices
</subsectionHeader>
<bodyText confidence="0.999982333333334">
MC08 describe a space and time efficient method
for learning discriminative classifiers that rank large
numbers of output classes using potentially millions
of features for many instances in potentially tera-
scale data sets. The authors describe a method for
learning ‘category indices’ — i.e., weighted bipar-
tite graphs G ⊆ F x W xC, where F is the set of fea-
tures, C is the set of output classes and all weights
(or ‘associations’) w E W between features and the
output classes they predict are real-valued and in
[0.0, 1.0]. The space and time efficiency of MC08’s
approach stems chiefly from three (parameterisable)
restrictions on category indices and how they are up-
dated. First, at any time in the learning process, only
those edges (fi, wj, ck) E G whose associations wj
are a large enough proportion of the sum of all class
associations for fi are retained: that is, only retain
wj s.t. wj ≥ wmin.9 Second, by setting an upper
bound dmax on the number of associations that a
feature fi is allowed to have, only the largest fea-
ture associations are retained. Setting dmax to a low
number (≤ 25) makes each feature a high-precision,
low-recall predictor of output classes. Further, the
dmax and wmin restrictions on parameter reten-
</bodyText>
<footnote confidence="0.9590015">
9Recall that wj E W are all between 0.0 and 1.0 and sum to
1.0.
</footnote>
<page confidence="0.998834">
13
</page>
<bodyText confidence="0.999977052631579">
tion allow efficient retrieval and update of feature
weights, as only a small number of feature weights
need be consulted for predicting output classes or
learning from prediction mistakes in an online learn-
ing setting.10 Finally, in the online learning algo-
rithm,11 in addition to the small number of features
that need be consulted or updated, an error margin
marg can be set so that parameter update only oc-
curs when the score(c) − score(c*) &lt; marg, where c
is the correct output class and c* # c is the most con-
fident incorrect prediction of the classifier. Setting
marg = 0.0 leads to purely error-driven learning,
while marg = 1.0 always updates on every learning
instance. Values of marg E (0.0, 1.0) will bias the
category index learner to update at different levels
of separation of the correct class from the most con-
fident incorrect class, ranging from almost always
error driven (near 0.0) to almost error-insensitive
learning (near 1.0).
</bodyText>
<subsectionHeader confidence="0.873881">
4.2 Integration into the WSD Task
</subsectionHeader>
<bodyText confidence="0.960050513513514">
Using both the Semcor-3 and English Lexical Sam-
ple training data sets (a total of X45,000 sentences,
each with one or more labeled instances), we trained
a sparse category index classifier as in MC08 with
the following features: using words, lemmas and
parts of speech (POSs) as tokens, we define fea-
tures for (1) preceding and following unigrams and
bigrams over tokens, as well as (2) the conjunc-
tion of the preceding unigrams (i.e., a 3-word win-
dow minus the current token) and (3) the conjunc-
tion of the preceding and following bigrams (5-
word window minus the current token). Finally
all surrounding lemmas in the sentence are treated
as left- or right-oriented slot-independent features
with an exponentially decaying level of activation
( )
act(li) = 0.5 · exp 0.5 · − dist(li , targ wd)
— where dist(li, targ wd) is simply the word dis-
tance from the target word to the contextual lemma
li.12 Although WSD is not a many-class, large-
10dmax bounds the number of feature-class associations (pa-
rameters) must be consulted in prediction and updating, but,
because of the wmin restriction, MC08 found that, on aver-
age, many fewer feature associations — &lt; 16 — were ever
touched per training or testing instance in their classification
experiments. See Madani and Connor (2008) for more details.
11Again, see Madani and Connor (2008) for more details.
12The value 0.5 is also a parameter that we have fixed, but it
could in principle be tuned to a particular data set. In the interest
of simplicity, we have not done this.
scale classification task,13 we nevertheless found
MC08’s pruning mechanisms useful for removing
weak feature-word associations. Due to the ag-
gressive pruning of feature-class associations, our
model only has 7z�1.9M parameters out of a potential
1, 600, 000 x 200, 000 = 320 billion (the number of
features times the number of WordNet 3.0 senses).
</bodyText>
<subsectionHeader confidence="0.997184">
4.3 Individual System Results
</subsectionHeader>
<bodyText confidence="0.999957380952381">
To integrate the predictions of the classifier into the
EAW task, we looked up all senses for each lemma-
POS pairing, backing off to looking up the words
themselves by the same POS, and finally resorting
to splitting hyphenated words and rejoining multi-
word units (as marked up in the EAW test set). Be-
ing high precision, the classifier does not return a
valid answer for every lemma, so we report results
with and without backing off to the most frequent
sense baseline to fill in these gaps.
Individual system scores are listed in Table 4. The
classifier on its own returns very few answers (with a
coverage — as distinct from recall — of only 10.4%
of the test set items). Although the classifier-only
performance does not have broad enough coverage
for stand-alone use, its predictions are nonetheless
useful in combination with the baseline. Further, we
expect coverage to grow when trained over a larger
corpus (such as the very large web-extracted corpus
of Agirre et al. (2004), which this learning method
is well suited for).
</bodyText>
<sectionHeader confidence="0.997417" genericHeader="method">
5 Wikipedia for Word Sense
Disambiguation
</sectionHeader>
<bodyText confidence="0.999825333333333">
Wikipedia, an online, user-created encyclopedia,
can be considered a collection of articles which link
to each other. While much information exists within
the textual content of Wikipedia that may assist in
WSD, the approach presented here instead uses the
article names and link structure within Wikipedia to
find articles which are most related to a WordNet
sense or context. We use the Green method to find a
relatedness metric for articles from Wikipedia14 (Ol-
</bodyText>
<footnote confidence="0.6468564">
13Large-scale data sets are available, but this does not change
the level of polysemy in WordNet, which is not in the thousands
for any given lemma.
14Computations were performed using a January 3rd 2008
download of the English Wikipedia.
</footnote>
<page confidence="0.994866">
14
</page>
<table confidence="0.999652333333333">
Back-off Precision Recall Prec. (n-best) Rec. (n-best)
Yes 0.592 0.589 0.594 0.589
No 0.622 0.065 0.694 0.070
</table>
<tableCaption confidence="0.989448">
Table 4: Precision and recall of sparse category index classifier — both “soft” scores of standard Senseval script and
scores where any correct answer in list returned by the classifier is counted as a correct answer (‘n-best’). ‘Back-off’
signals whether the system backs off to the most frequent sense baseline.
</tableCaption>
<bodyText confidence="0.992708823529412">
livier and Senellart, 2007) based on each sense or
context of interest.
Advantages of this method over alternative meth-
ods that attempt to incorporate Wikipedia into WSD
is that our system is unsupervised and that no man-
ual mapping needs to take place between WordNet
and Wikipedia. Mihalcea (2007) demonstrates that
manual mappings can be created for a small num-
ber of words with relative ease, but for a very large
number of words the effort involved in mapping
would approach presented involves no be consider-
able. The approach presented here involves no map-
ping between WordNet and Wikipedia but human ef-
fort in mapping between WordNet and Wikipedia,
but instead initializes the Green method with a vec-
tor based only on the article names (as described in
Section 5.2).
</bodyText>
<subsectionHeader confidence="0.984898">
5.1 Green Method
</subsectionHeader>
<bodyText confidence="0.999997736842105">
The Green method (Ollivier and Senellart, 2007) is
used to determine the importance of one node in a
directed graph with respect to other nodes.15 In the
context of Wikipedia the method finds the articles
which are most likely to be frequented if a random
walk were used to traverse the articles, starting with
a specific article and returning to that article if the
random walk either strays too far off topic or to an
article which is generally popular even without the
context of the initial article. One of the features of
the Green method is that it does not simply repro-
duce the global PageRank (Brin and Page, 1998),
instead determining the related pages nearby due to
relevance to the initial node.
The probability that the random walker of
Wikipedia will transfer to an article is defined as a
uniform distribution over the outlinks of the page
where the random walker is currently located. As
an approximation to the method described by Ol-
</bodyText>
<footnote confidence="0.707659">
15In subsequent sections we give a high-level description of
using the Green method with Wikipedia, however see Ollivier
and Senellart (2007) for a much more detailed explanation.
</footnote>
<bodyText confidence="0.99961375">
livier and Senellart (2007), we create a subgraph of
Wikipedia for every computation, comprised of the
articles within a distance of 2 outlink traversals from
the initial articles. Since Wikipedia is very highly
connected, this constructed subgraph still contains
a large number of articles and performance of the
Green method on this subgraph is similar to that on
the whole connectivity graph.
</bodyText>
<subsectionHeader confidence="0.997728">
5.2 Green Method for Contexts
</subsectionHeader>
<bodyText confidence="0.999995">
To use the Green method to find Wikipedia arti-
cles which correspond to a given word to be dis-
ambiguated, articles which may discuss that word
and the context surrounding that word are found in
Wikipedia as an initial set of locations for the ran-
dom walker to start. This is done by looking for the
word itself as the name of an article. If there is not
an article whose name corresponds to the word in
question, then articles with the word as a substring
of the article name are found.
Since the goal of WSD is to choose the best word
sense within the context of other words, we use a
given word’s context to select a set of Wikipedia ar-
ticles which may discuss the content of the word in
question. The expectation is that the context words
will aid in disambiguation and that the context words
will together be associated with an appropriate sense
of the word being disambiguated. For this method
we defined a word’s context as the word itself, the
content words in the sentence the word occurs in,
and those occurring in the sentences before and af-
ter that sentence.
</bodyText>
<subsectionHeader confidence="0.996458">
5.3 Green Method for Senses
</subsectionHeader>
<bodyText confidence="0.99987">
Every sense of a word to be disambiguated also
needs to be represented as corresponding articles
in Wikipedia before using the Green method. The
words that we search for in the titles of Wikipedia
articles include the word itself, and, for every sense,
the content words of the sense’s WordNet gloss, as
well as the content of the sense’s hypernym gloss
</bodyText>
<page confidence="0.994911">
15
</page>
<bodyText confidence="0.99620175">
and the synonyms of the hypernym. Exploring this
particular aspect of this module — which informa-
tion about a sense to extract before using the Green
Method — is a point for further exploration.
</bodyText>
<subsectionHeader confidence="0.993133">
5.4 Interpreting Projections
</subsectionHeader>
<bodyText confidence="0.999991421052632">
The Green method as described by Ollivier and
Senellart (2007) uses, as the initial set of articles,
the vector containing only one article: that article
for which related articles are being searched. We
use as the initial set of articles the collection of ar-
ticles in Wikipedia corresponding to either the con-
text for the word to be disambiguated or the sense of
a word. The random walker is modeled as starting
in any of the articles in this set with uniform proba-
bility. Within the context of the Green method, this
means that this initial set of articles corresponds to
what would be linked to from a new Wikipedia arti-
cle about the sense or context. Each of the content
words in this new article (which is not in Wikipedia)
would link to one of the articles in the set found by
the methods described above. In this way the results
of the Green method computation can be interpreted
as a relatedness metric for the sense or context itself
and the articles which are in Wikipedia.
</bodyText>
<subsectionHeader confidence="0.996791">
5.5 Analysis
</subsectionHeader>
<bodyText confidence="0.999968818181818">
The process of finding the sense of a word to be dis-
ambiguated is as follows: the vector output from the
Green method (a relatedness measure between the
initial seed and each article in Wikipedia) for the
context of the word is compared against the vector
output from using the Green method on each sense
that the word could have. The comparison is done
using the cosine of the angle between the two vec-
tors.
To determine for which instances in SENSEVnr. this
method may perform well, an analysis was per-
formed on a small development set (15 sentences)
from SemCor. A simple heuristic was formulated,
selecting the sense with the nearest Green method
output to the sentence’s Green method output when
the ratio between the first and second highest ranked
senses’ cosine angle scores was above a threshold.
Applying this heuristic to the EAW task yielded
an expectedly low recall of 11% but a precision of
81% on all the words that this heuristic could apply,
but only a precision of 25% (recall 0.5%) for non-
monosemous words (which were the desired targets
</bodyText>
<table confidence="0.995233">
MFS Rerank Wiki
MFS – 94% 97%
Rerank 23% – 99%
Wiki 45% 98% –
</table>
<tableCaption confidence="0.999576">
Table 5: Complementarity between modules
</tableCaption>
<bodyText confidence="0.992495">
of the method). Of 37 instances where this method
differs from the MFS baseline in the EAW task, 8 in-
stances are correctly disambiguated by this module.
</bodyText>
<sectionHeader confidence="0.99997" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.987023028571429">
Although the individual systems have fairly low re-
call, we can calculate pairwise complementarity be-
tween systems si and sj by evaluating
)
|wrong in si and sj|
|wrong in si|
The results, presented in Table 5, indicate that the
systems complement each other well, and suggest
that a combination system could have a higher per-
formance than the individual systems.
We investigate a number of techniques to combine
the results – while the integration of the lemma/ part
of speech refinement is done by all modules as a pre-
processing step, the method of combination of the
resulting modules is less clear. As shown in Florian
et al. (2002), a simple voting mechanism achieves
comparable performance to a stacking mechanism.
We present our results in Table 6, DT gives the re-
sult of a 10-fold cross-validation of WEKA stacked
decision trees and nearest neighbours built from the
individual system results (Witten and Frank, 2000).
Very few decisions are changed with the voting
method of combination, and the overall result does
not outperform the best MFS baseline (presented in
the table as “All MFS”). This combination method
may be more useful with a greater number of sys-
tems being combined – our system only combines
three systems (thus only one non-MFS system has to
suggest the MFS for this to be selected), and backs
off to the MFS sense in case all three disagree. The
degree of complementarity between the Wiki system
and the MFS system indicates that these will over-
ride the Rerank system in many cases.
Better results are seen with the simple stacking
result: in this case, systems are ordered and thus
</bodyText>
<equation confidence="0.443648">
1 −
</equation>
<page confidence="0.937843">
16
</page>
<table confidence="0.9998056">
System Precision Recall F-measure
All MFS 58.6% 58.6% 58.6%
Voting 58.6% 58.6% 58.6%
Stacking 58.9% 58.9% 58.9%
Stacked DT/NN 58.7% 58.7% 58.7%
</table>
<tableCaption confidence="0.998696">
Table 6: Resulting refined system (forced-choice)
</tableCaption>
<bodyText confidence="0.803365">
are not being subjected to overriding by other MFS
skewed systems.
</bodyText>
<sectionHeader confidence="0.997631" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999994388888889">
We have presented a refinement of the most fre-
quent sense baseline system, which incorporates a
number of novel approaches to word sense disam-
biguation methods. We demonstrate the need for
accurate lemmatization and part of speech tagging,
showing that that is probably the area where the
biggest boost in performance can currently be ob-
tained. We would also argue that examining the ab-
solute performance in a task where the baseline is so
exceedingly variable (ourselves, we have found the
baseline to be as low as 56% with restricted lemma
backoff, 58.4% with a fairly sophisticated lemma /
PoS module, against published baselines of 61.5%
in McCarthy et al., 62.5% reported in Snyder, or the
upper bound baseline of 66% using correct lemmas
and parts of speech), the performance difference be-
tween the baseline used and the resulting system is
interesting in itself.
</bodyText>
<sectionHeader confidence="0.997639" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999638">
We would like to thank DJ Hovermale for his input
throughout this project.
</bodyText>
<sectionHeader confidence="0.998018" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998104283018868">
Agirre, E., , and de Lacalle Lekuona, O. L. (2004).
Publicly Available Topic Signatures for all Word-
Net Nominal Senses. In Proceedings of the 4th In-
ternational Conference on Languages Resources
and Evaluations (LREC), Lisbon, Portugal.
Baayen, H., Piepenbrock, R., and Gulikers, L.
(1995). The CELEX lexical database (release 2).
CD-ROM. Centre for Lexical Information, Max
Planck Institute for Psycholinguistics, Nijmegen;
Linguistic Data Consortium, University of Penn-
sylvania.
Brill, E. (1992). A simple rule-based part of speech
tagger. In Proceedings of the Third Conference
on Applied Natural Language Processing, pages
152–155, Trento, Italy.
Brin, S. and Page, L. (1998). The anatomy of a large-
scale hypertextual web search engine. In Com-
puter Networks and ISDN Systems, pages 107–
117.
Briscoe, E., Carroll, J., and Watson, R. (2006). The
second release of the RASP system. In Proceed-
ings of the COLING/ACL 2006 Interactive Pre-
sentation Sessions, Sydney, Australia.
Elworthy, D. (1994). Does Baum-Welch re-
estimation help taggers? In Proceedings of the
4th ACL Conference on Applied NLP, pages 53–
58, Stuttgart, Germany.
Florian, R., Cucerzan, S., Schafer, C., and
Yarowsky, D. (2002). Combining classifiers for
word sense disambiguation. Journal of Natural
Language Engineering, 8(4):327–342.
Hal´acsy, P., Kornai, A., and Oravecz, C. (2007).
HunPos – an open source trigram tagger. In Pro-
ceedings of the 45th Annual Meeting of the Asso-
ciation for Computational Linguistics Companion
Volume Proceedings of the Demo and Poster Ses-
sions, pages 209–212, Prague, Czech Republic.
Association for Computational Linguistics.
Korhonen, A., Krymolovski, Y., and Briscoe, T.
(2006). A large subcategorization lexicon for
natural language processing applications. In
Proceedings of the 5th international conference
on Language Resources and Evaluation, pages
1015–1020.
Korhonen, A. and Krymolowski, Y. (2002). On the
robustness of entropy-based similarity measures
in evaluation of subcategorization acquisition sys-
tems. In Proceedings of the 6th Conference on
Natural Language Learning, pages 91–97.
Lin, D. (1998). Automatic retrieval and clustering
of similar words. In Proceedings of the COLING-
ACL’98, pages 768–773.
Madani, O. and Connor, M. (2008). Large-Scale
</reference>
<page confidence="0.987951">
17
</page>
<reference confidence="0.999736924528302">
Many-Class Learning. In Proceedins of the SIAM
Conference on Data Mining (SDM-08).
McCarthy, D., Koeling, R., Weeds, J., and Carroll,
J. (2007). Unsupervised acquisition of predom-
inant word senses. Computational Linguistics,
33(4):553–590.
Mihalcea, R. (2007). Using Wikipedia for automatic
word sense disambiguation. In Human Language
Technologies 2007: The Conferece of the North
Americ an Chapter of the Association for Compu-
tational Linguistics, Rochester, New York.
Miller, G., Beckwith, R., Felbaum, C., Gross, D.,
and Miller, K. (1990). Introduction to WordNet:
An on-line lexical database. Journal of Lexicog-
raphy, 3(4):235–244.
Miller, G., Leacock, C., Ranee, T., and Bunker, R.
(1993). A semantic concordance. In Proceedings
of the 3rd DARPA Workshop on Human Language
Technology, pages 232–235.
Ollivier, Y. and Senellart, P. (2007). Finding related
pages using Green measures: An illustration with
Wikipedia. In Association for the Advancement
of Artificial Intelligence Conference on Artificial
Intelligence (AAAI 2007).
Pedersen, T., Patwardhan, S., and Michelizzi, J.
(2004). Wordnet::similarity - measuring the re-
latedness of concepts. In Proceedings of Fifth
Annual Meeting of the North American Chapter
of the Association for Computational Linguistics,
pages 38–41.
Schmidt, H. (1994). Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
the International Conference on New Methods in
Language Processing, pages 44–49.
Snyder, B. and Palmer, M. (2004). The english all-
words task. In Mihalcea, R. and Chklowski, T.,
editors, Proceedings of SENSEVAL-3: Third In-
ternational Workshop on Evaluating Word Sense
Disambiguating Systems, pages 41–43.
Witten, I. H. and Frank, E. (2000). Data min-
ing: Practical Machine Learning Tools and Tech-
niques with Java Implementations, chapter 8.
Morgan Kaufmann Publishers.
XTAG Research Group (2001). A lexicalized tree
adjoining grammar for English. Technical Report
IRCS-01-03, IRCS, University of Pennsylvania.
Yarowsky, D. (1993). One Sense Per Collocation. In
Proceedings of the Human Language Technology
Conference, Princeton, NJ, USA.
Yngve, V. H. (1955). Syntax and the problem of
multiple meaning. In Locke, W. N. and Booth,
A. D., editors, Machine translation of languages,
pages 208–226. John Wiley and Sons, New York.
</reference>
<page confidence="0.999293">
18
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.629290">
<title confidence="0.998605">Refining the most frequent sense baseline</title>
<author confidence="0.950303">Judita</author>
<affiliation confidence="0.922954">Department of The Ohio State</affiliation>
<email confidence="0.99883">judita@ling.ohio-state.edu</email>
<author confidence="0.927347">Josh</author>
<affiliation confidence="0.917684">Computer Science and The Ohio State</affiliation>
<email confidence="0.999245">kingjo@cse.ohio-state.edu</email>
<abstract confidence="0.997129">We refine the most frequent sense baseline for word sense disambiguation using a number of novel word sense disambiguation tech- Evaluating on the English all words task, our combined system focuses on improving every stage of word sense disambiguation: starting with the lemmatization and part of speech tags used, through the accuracy of the most frequent sense baseline, to highly targeted individual systems. Our supervised systems include a ranking algorithm and a Wikipedia similarity measure.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>de Lacalle Lekuona</author>
<author>O L</author>
</authors>
<title>Publicly Available Topic Signatures for all WordNet Nominal Senses.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th International Conference on Languages Resources and Evaluations (LREC),</booktitle>
<location>Lisbon, Portugal.</location>
<marker>Lekuona, L, 2004</marker>
<rawString>Agirre, E., , and de Lacalle Lekuona, O. L. (2004). Publicly Available Topic Signatures for all WordNet Nominal Senses. In Proceedings of the 4th International Conference on Languages Resources and Evaluations (LREC), Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Baayen</author>
<author>R Piepenbrock</author>
<author>L Gulikers</author>
</authors>
<title>The CELEX lexical database (release 2).</title>
<date>1995</date>
<institution>CD-ROM. Centre for Lexical Information, Max Planck Institute for Psycholinguistics, Nijmegen; Linguistic Data Consortium, University of Pennsylvania.</institution>
<contexts>
<context position="10354" citStr="Baayen et al., 1995" startWordPosition="1689" endWordPosition="1692">ately identify the correct WordNet sense of a given word in the English all words task. We found that without any lemmatizing of the test input, the maximum f-score possible was in the mid-50’s. Conversely, we found that a basic most-frequentsense system that had a perfectly-lemmatized input achieved an f-score in the mid-60’s. This large difference in the ceiling of a non-lemmatized system and the floor of a perfectly-lemmatized system motivated us to focus on this task. We looked at three different lemmatizers: the lemmatizing backend of the XTAG project (XTAG Research Group, 2001)4, Celex (Baayen et al., 1995), and the lemmatizing component of an enhanced TBL tagger (Brill, 1992).5 We then employed a voting system on these three components, taking the lemma from the most individual lemmatizers. If all three differ, we take the lemma from the most accurate individual system, namely the TBL tagger. 3.1.1 Lemmatizer Evaluation We evaluated the lemmatizers against the lemmas found in the SENsEvnr.-3 gold standard.6 Even the lowest performing system improved accuracy by 31.74% over the baseline, which baseline simply equates the given token with the lemma. Table 2 shows the results of evaluating the lem</context>
</contexts>
<marker>Baayen, Piepenbrock, Gulikers, 1995</marker>
<rawString>Baayen, H., Piepenbrock, R., and Gulikers, L. (1995). The CELEX lexical database (release 2). CD-ROM. Centre for Lexical Information, Max Planck Institute for Psycholinguistics, Nijmegen; Linguistic Data Consortium, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>A simple rule-based part of speech tagger.</title>
<date>1992</date>
<booktitle>In Proceedings of the Third Conference on Applied Natural Language Processing,</booktitle>
<pages>152--155</pages>
<location>Trento, Italy.</location>
<contexts>
<context position="10425" citStr="Brill, 1992" startWordPosition="1702" endWordPosition="1703">ds task. We found that without any lemmatizing of the test input, the maximum f-score possible was in the mid-50’s. Conversely, we found that a basic most-frequentsense system that had a perfectly-lemmatized input achieved an f-score in the mid-60’s. This large difference in the ceiling of a non-lemmatized system and the floor of a perfectly-lemmatized system motivated us to focus on this task. We looked at three different lemmatizers: the lemmatizing backend of the XTAG project (XTAG Research Group, 2001)4, Celex (Baayen et al., 1995), and the lemmatizing component of an enhanced TBL tagger (Brill, 1992).5 We then employed a voting system on these three components, taking the lemma from the most individual lemmatizers. If all three differ, we take the lemma from the most accurate individual system, namely the TBL tagger. 3.1.1 Lemmatizer Evaluation We evaluated the lemmatizers against the lemmas found in the SENsEvnr.-3 gold standard.6 Even the lowest performing system improved accuracy by 31.74% over the baseline, which baseline simply equates the given token with the lemma. Table 2 shows the results of evaluating the lemmatizers against the EAW key. While the simple voting system performed </context>
<context position="12192" citStr="Brill, 1992" startWordPosition="1985" endWordPosition="1986">riscoe et al., 2006), an enhanced 4http://www.cis.upenn.edu/˜xtag 5http://gposttl.sourceforge.net 6We removed those lines from both the test input and the gold standard which were marked U (= unknown, 34 lines), and we removed the 40 lines from the test input that were missing from the gold standard. This gave us 2007 words in both the test set and the gold standard. 12 Lemmatizer Accuracy Baseline 57.50% XTAG 89.24% Celex 91.58% TBL 92.38% Voting {XTAG,Celex,TBL} 93.77% Voting, no hyphen {XTAG,Celex,TBL} 94.67% Table 2: Accuracy of several lemmatizers on &lt;head&gt; words of EAW task. TBL tagger (Brill, 1992)7, and a TnT-style trigram tagger (Hal´acsy et al., 2007).8 The baseline was a unigram tagger which selects the most frequentlyoccurring tag of singletons when dealing with unseen words. All three of the main taggers performed comparably, although only the Elworthy tagger provides probabilities associated with tags, rather than getting a single tag as output. This additional information can be useful, since we can employ different strategies for a word with one single tag with a probability of 1, versus a word with multiple tags, the most probable of which might only have a probability of 0.3 </context>
</contexts>
<marker>Brill, 1992</marker>
<rawString>Brill, E. (1992). A simple rule-based part of speech tagger. In Proceedings of the Third Conference on Applied Natural Language Processing, pages 152–155, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brin</author>
<author>L Page</author>
</authors>
<title>The anatomy of a largescale hypertextual web search engine.</title>
<date>1998</date>
<booktitle>In Computer Networks and ISDN Systems,</booktitle>
<pages>107--117</pages>
<contexts>
<context position="22298" citStr="Brin and Page, 1998" startWordPosition="3671" endWordPosition="3674">e Green method (Ollivier and Senellart, 2007) is used to determine the importance of one node in a directed graph with respect to other nodes.15 In the context of Wikipedia the method finds the articles which are most likely to be frequented if a random walk were used to traverse the articles, starting with a specific article and returning to that article if the random walk either strays too far off topic or to an article which is generally popular even without the context of the initial article. One of the features of the Green method is that it does not simply reproduce the global PageRank (Brin and Page, 1998), instead determining the related pages nearby due to relevance to the initial node. The probability that the random walker of Wikipedia will transfer to an article is defined as a uniform distribution over the outlinks of the page where the random walker is currently located. As an approximation to the method described by Ol15In subsequent sections we give a high-level description of using the Green method with Wikipedia, however see Ollivier and Senellart (2007) for a much more detailed explanation. livier and Senellart (2007), we create a subgraph of Wikipedia for every computation, compris</context>
</contexts>
<marker>Brin, Page, 1998</marker>
<rawString>Brin, S. and Page, L. (1998). The anatomy of a largescale hypertextual web search engine. In Computer Networks and ISDN Systems, pages 107– 117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Briscoe</author>
<author>J Carroll</author>
<author>R Watson</author>
</authors>
<title>The second release of the RASP system.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="11600" citStr="Briscoe et al., 2006" startWordPosition="1892" endWordPosition="1895">key. While the simple voting system performed better than any of the individual lemmatizers, hyphenated words proved problematic for all of the systems. Some hyphenated words in the test set remained hyphenated in the gold standard, and some others were separated. However, evaluation results show that splitting hyphenated words increases lemmatizing accuracy by 0.9%. 3.2 Part of Speech Tagging We also investigated the contribution of part of speech taggers to the task of word sense disambiguation. We considered three taggers: the Elworthy bigram tagger (Elworthy, 1994) within the RASP parser (Briscoe et al., 2006), an enhanced 4http://www.cis.upenn.edu/˜xtag 5http://gposttl.sourceforge.net 6We removed those lines from both the test input and the gold standard which were marked U (= unknown, 34 lines), and we removed the 40 lines from the test input that were missing from the gold standard. This gave us 2007 words in both the test set and the gold standard. 12 Lemmatizer Accuracy Baseline 57.50% XTAG 89.24% Celex 91.58% TBL 92.38% Voting {XTAG,Celex,TBL} 93.77% Voting, no hyphen {XTAG,Celex,TBL} 94.67% Table 2: Accuracy of several lemmatizers on &lt;head&gt; words of EAW task. TBL tagger (Brill, 1992)7, and a</context>
</contexts>
<marker>Briscoe, Carroll, Watson, 2006</marker>
<rawString>Briscoe, E., Carroll, J., and Watson, R. (2006). The second release of the RASP system. In Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Elworthy</author>
</authors>
<title>Does Baum-Welch reestimation help taggers?</title>
<date>1994</date>
<booktitle>In Proceedings of the 4th ACL Conference on Applied NLP,</booktitle>
<pages>53--58</pages>
<location>Stuttgart, Germany.</location>
<contexts>
<context position="11554" citStr="Elworthy, 1994" startWordPosition="1886" endWordPosition="1887">luating the lemmatizers against the EAW key. While the simple voting system performed better than any of the individual lemmatizers, hyphenated words proved problematic for all of the systems. Some hyphenated words in the test set remained hyphenated in the gold standard, and some others were separated. However, evaluation results show that splitting hyphenated words increases lemmatizing accuracy by 0.9%. 3.2 Part of Speech Tagging We also investigated the contribution of part of speech taggers to the task of word sense disambiguation. We considered three taggers: the Elworthy bigram tagger (Elworthy, 1994) within the RASP parser (Briscoe et al., 2006), an enhanced 4http://www.cis.upenn.edu/˜xtag 5http://gposttl.sourceforge.net 6We removed those lines from both the test input and the gold standard which were marked U (= unknown, 34 lines), and we removed the 40 lines from the test input that were missing from the gold standard. This gave us 2007 words in both the test set and the gold standard. 12 Lemmatizer Accuracy Baseline 57.50% XTAG 89.24% Celex 91.58% TBL 92.38% Voting {XTAG,Celex,TBL} 93.77% Voting, no hyphen {XTAG,Celex,TBL} 94.67% Table 2: Accuracy of several lemmatizers on &lt;head&gt; words</context>
</contexts>
<marker>Elworthy, 1994</marker>
<rawString>Elworthy, D. (1994). Does Baum-Welch reestimation help taggers? In Proceedings of the 4th ACL Conference on Applied NLP, pages 53– 58, Stuttgart, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Florian</author>
<author>S Cucerzan</author>
<author>C Schafer</author>
<author>D Yarowsky</author>
</authors>
<title>Combining classifiers for word sense disambiguation.</title>
<date>2002</date>
<journal>Journal of Natural Language Engineering,</journal>
<volume>8</volume>
<issue>4</issue>
<contexts>
<context position="27864" citStr="Florian et al. (2002)" startWordPosition="4642" endWordPosition="4645">ndividual systems have fairly low recall, we can calculate pairwise complementarity between systems si and sj by evaluating ) |wrong in si and sj| |wrong in si| The results, presented in Table 5, indicate that the systems complement each other well, and suggest that a combination system could have a higher performance than the individual systems. We investigate a number of techniques to combine the results – while the integration of the lemma/ part of speech refinement is done by all modules as a preprocessing step, the method of combination of the resulting modules is less clear. As shown in Florian et al. (2002), a simple voting mechanism achieves comparable performance to a stacking mechanism. We present our results in Table 6, DT gives the result of a 10-fold cross-validation of WEKA stacked decision trees and nearest neighbours built from the individual system results (Witten and Frank, 2000). Very few decisions are changed with the voting method of combination, and the overall result does not outperform the best MFS baseline (presented in the table as “All MFS”). This combination method may be more useful with a greater number of systems being combined – our system only combines three systems (th</context>
</contexts>
<marker>Florian, Cucerzan, Schafer, Yarowsky, 2002</marker>
<rawString>Florian, R., Cucerzan, S., Schafer, C., and Yarowsky, D. (2002). Combining classifiers for word sense disambiguation. Journal of Natural Language Engineering, 8(4):327–342.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Hal´acsy</author>
<author>A Kornai</author>
<author>C Oravecz</author>
</authors>
<title>HunPos – an open source trigram tagger.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,</booktitle>
<pages>209--212</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic.</location>
<marker>Hal´acsy, Kornai, Oravecz, 2007</marker>
<rawString>Hal´acsy, P., Kornai, A., and Oravecz, C. (2007). HunPos – an open source trigram tagger. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 209–212, Prague, Czech Republic. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Korhonen</author>
<author>Y Krymolovski</author>
<author>T Briscoe</author>
</authors>
<title>A large subcategorization lexicon for natural language processing applications.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th international conference on Language Resources and Evaluation,</booktitle>
<pages>1015--1020</pages>
<contexts>
<context position="8486" citStr="Korhonen et al., 2006" startWordPosition="1370" endWordPosition="1373">e the Lesk (overlap) similarity as implemented by the WordNet::similarity package (Pedersen et al., 2004). z dss(w, nj) * sss(si, nj) njENw Es&apos;iEsenses(w) sss(s&apos;i, nj) 11 System Precision Recall F-measure MFS 58.4% 58.4% 58.4% McCarthy 58.5% 58.5% 58.5% Verbs 58.5% 58.5% 58.5% All 58.6% 58.6% 58.6% Table 1: Refining the MFS baseline with predominant sense verbs as it does for nouns and adjectives. We hypothesize that this is due to the thesaural neighbours obtained from Lin’s thesaurus, and we group verbs according to the subcategorization frame (SCF) distributions they present in the vnr.Ex (Korhonen et al., 2006) lexicon. A word w1 is grouped with word w2 if the Bhattacharyya coefficient �BC(w1, w2) = � p(x)q(x) x∈X where p(x) and q(x) represent the probability values for subcategorization class x, is above a certain threshold. The BC coefficient then replaces the dss value in the original formula and the predominant senses are obtained. Again, this system is only used for words with frequency lower than 5 in SemCor. The great advantage of the Bhattacharyya coefficient over various entropy based similarity measures which are usually used to compare SCF distributions (Korhonen and Krymolowski, 2002), i</context>
</contexts>
<marker>Korhonen, Krymolovski, Briscoe, 2006</marker>
<rawString>Korhonen, A., Krymolovski, Y., and Briscoe, T. (2006). A large subcategorization lexicon for natural language processing applications. In Proceedings of the 5th international conference on Language Resources and Evaluation, pages 1015–1020.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Korhonen</author>
<author>Y Krymolowski</author>
</authors>
<title>On the robustness of entropy-based similarity measures in evaluation of subcategorization acquisition systems.</title>
<date>2002</date>
<booktitle>In Proceedings of the 6th Conference on Natural Language Learning,</booktitle>
<pages>91--97</pages>
<contexts>
<context position="9083" citStr="Korhonen and Krymolowski, 2002" startWordPosition="1471" endWordPosition="1474">the vnr.Ex (Korhonen et al., 2006) lexicon. A word w1 is grouped with word w2 if the Bhattacharyya coefficient �BC(w1, w2) = � p(x)q(x) x∈X where p(x) and q(x) represent the probability values for subcategorization class x, is above a certain threshold. The BC coefficient then replaces the dss value in the original formula and the predominant senses are obtained. Again, this system is only used for words with frequency lower than 5 in SemCor. The great advantage of the Bhattacharyya coefficient over various entropy based similarity measures which are usually used to compare SCF distributions (Korhonen and Krymolowski, 2002), is that it is guaranteed to lie between 0 and 1, unlike the entropy based measures which are not easily comparable between different word pairs. This result is represented by ’Verbs’ in Table 1. Table 1 displays the results for the MFS, the MFS combined with the two approaches described above, and the MFS combining MFS with verbs and McCarthy. 3 Lemmatization and Part of Speech Tagging We made use of several lemmatizers and part-ofspeech taggers, in order to give the other WSD components the best starting point possible. 3.1 Lemmatization Lemmatization, the process of obtaining the canonical</context>
</contexts>
<marker>Korhonen, Krymolowski, 2002</marker>
<rawString>Korhonen, A. and Krymolowski, Y. (2002). On the robustness of entropy-based similarity measures in evaluation of subcategorization acquisition systems. In Proceedings of the 6th Conference on Natural Language Learning, pages 91–97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of the COLINGACL’98,</booktitle>
<pages>768--773</pages>
<contexts>
<context position="6978" citStr="Lin, 1998" startWordPosition="1120" endWordPosition="1121">method of determining the predominant sense presented in McCarthy et al. (2007). 1. For nouns and adjectives which appear in SemCor fewer than 5 times, we employ the automatically determined predominant sense. 2. For verbs which appear in SemCor fewer than 5 times, we employ subcategorization frame similarity rather than Lesk similarity to give us a verb’s predominant sense. 2.1 Predominant sense McCarthy et al. (2007) demonstrate that it is possible to acquire the predominant sense for a word in a corpus without having access to annotated data. They employ an automatically created thesaurus (Lin, 1998), and a sense–word similarity metric to assign to each sense si of a word w a score corresponding to where dss(w, nj) reflects the distributional similarity of word w to nj, w’s thesaural neighbour, and sss(si, nj) = maxsxEsenses(nj) sss&apos;(si, sx) is the maximum similarity3 between w’s sense si and a sense sx of w’s thesaural neighbour nj. The authors show that although this method does not always outperform the MFS baseline based on SemCor, it does outperform it when the word’s SemCor frequency is below 5. We therefore switch our MFS baseline to this value for such words. This result is repres</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Lin, D. (1998). Automatic retrieval and clustering of similar words. In Proceedings of the COLINGACL’98, pages 768–773.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Madani</author>
<author>M Connor</author>
</authors>
<title>Large-Scale Many-Class Learning.</title>
<date>2008</date>
<booktitle>In Proceedins of the SIAM Conference on Data Mining (SDM-08).</booktitle>
<contexts>
<context position="14062" citStr="Madani and Connor, 2008" startWordPosition="2282" endWordPosition="2286">Category Indices for WSD In this component of our refinement of the baseline, we train a supervised system that performs higher-precision classification, only returning an answer when a predictive feature that strongly predicts a particular sense is observed. To achieve this, 7http://gposttl.sourceforge.net 8http://code.google.com/p/hunpos POS Tagger Accuracy Baseline 84.10% TBL 90.48% Elworthy 90.58% TnT 91.13% Voting {TBL,Elw.,TnT} 91.88% Table 3: Accuracy of several POS taggers on &lt;head&gt; words of EAW task. we implemented a “feature focus” classifier (sparse weighted index) as described in (Madani and Connor, 2008, henceforth, MC08). MC08’s methods for restricting and pruning the number of feature-toclass associations are useful for finding and retaining only strong predictive features. Moreover, this allowed us to use a rich feature set (more than 1.6 million features) without an unwieldy explosion in the number of parameters, as feature-class associations that are not strong enough are simply dropped. 4.1 Sparse Category Indices MC08 describe a space and time efficient method for learning discriminative classifiers that rank large numbers of output classes using potentially millions of features for m</context>
<context position="18055" citStr="Madani and Connor (2008)" startWordPosition="2960" endWordPosition="2963">ight-oriented slot-independent features with an exponentially decaying level of activation ( ) act(li) = 0.5 · exp 0.5 · − dist(li , targ wd) — where dist(li, targ wd) is simply the word distance from the target word to the contextual lemma li.12 Although WSD is not a many-class, large10dmax bounds the number of feature-class associations (parameters) must be consulted in prediction and updating, but, because of the wmin restriction, MC08 found that, on average, many fewer feature associations — &lt; 16 — were ever touched per training or testing instance in their classification experiments. See Madani and Connor (2008) for more details. 11Again, see Madani and Connor (2008) for more details. 12The value 0.5 is also a parameter that we have fixed, but it could in principle be tuned to a particular data set. In the interest of simplicity, we have not done this. scale classification task,13 we nevertheless found MC08’s pruning mechanisms useful for removing weak feature-word associations. Due to the aggressive pruning of feature-class associations, our model only has 7z�1.9M parameters out of a potential 1, 600, 000 x 200, 000 = 320 billion (the number of features times the number of WordNet 3.0 senses). 4.3 I</context>
</contexts>
<marker>Madani, Connor, 2008</marker>
<rawString>Madani, O. and Connor, M. (2008). Large-Scale Many-Class Learning. In Proceedins of the SIAM Conference on Data Mining (SDM-08).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McCarthy</author>
<author>R Koeling</author>
<author>J Weeds</author>
<author>J Carroll</author>
</authors>
<title>Unsupervised acquisition of predominant word senses.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="2681" citStr="McCarthy et al. (2007)" startWordPosition="404" endWordPosition="407">ed that supervised systems, which can be tuned to a word’s context, greatly outperform unsupervised systems. This theory was supported in the SENSEVAL WSD system evaluation exercises, where the performance gap between the best supervised system and the best unsupervised system is large. Unsupervised systems were found to never outperform the most frequent sense (MFS) baseline (a sense assignment made on the basis of the most frequent sense in an annotated corpus), while supervised systems occasionally perform better than the MFS baseline, though rarely by more than 5%. However, recent work by McCarthy et al. (2007) shows that acquiring a predominant sense from an unannotated corpus can outperform many supervised systems, and under certain conditions will also outperform the MFS baseline. Rather than proposing a new algorithm which will tackle all words, we focus on improving upon the MFS baseline system when an alternative system proposes a high confidence answer. An MFS refining system can therefore benefit from answers suggested by a very low recall (but high precision) WSD system. We propose a number of novel approaches to WSD, but also demonstrate the importance of a highly accurate lemmatizer and p</context>
<context position="6447" citStr="McCarthy et al. (2007)" startWordPosition="1029" endWordPosition="1033">ense distribution in the corpus which the MFS baseline is drawn from, 2) If SemCor is used as the underlying sense annotated corpus, the accuracy of the mapping from WordNet 1.6 (with which SemCor was initially annotated) to WordNet 1.7.1 could also have an effect on the performance). performance which motivates refining the most frequent sense baseline, and our work on improving the underlying lemmatizer and part of speech tagger presented in Section 3. Our initial investigation refines the SemCor based MFS baseline using the automatic method of determining the predominant sense presented in McCarthy et al. (2007). 1. For nouns and adjectives which appear in SemCor fewer than 5 times, we employ the automatically determined predominant sense. 2. For verbs which appear in SemCor fewer than 5 times, we employ subcategorization frame similarity rather than Lesk similarity to give us a verb’s predominant sense. 2.1 Predominant sense McCarthy et al. (2007) demonstrate that it is possible to acquire the predominant sense for a word in a corpus without having access to annotated data. They employ an automatically created thesaurus (Lin, 1998), and a sense–word similarity metric to assign to each sense si of a </context>
<context position="7783" citStr="McCarthy et al. (2007)" startWordPosition="1257" endWordPosition="1260">ural neighbour, and sss(si, nj) = maxsxEsenses(nj) sss&apos;(si, sx) is the maximum similarity3 between w’s sense si and a sense sx of w’s thesaural neighbour nj. The authors show that although this method does not always outperform the MFS baseline based on SemCor, it does outperform it when the word’s SemCor frequency is below 5. We therefore switch our MFS baseline to this value for such words. This result is represented as ’McCarthy’ in Table 1, which contains the results of the techniques presented in this Section evaluated on the SENSEvnr.-3 English all words task. 2.2 Verb predominant sense McCarthy et al. (2007) observe that their predominant sense method is not performing as well for 3We use the Lesk (overlap) similarity as implemented by the WordNet::similarity package (Pedersen et al., 2004). z dss(w, nj) * sss(si, nj) njENw Es&apos;iEsenses(w) sss(s&apos;i, nj) 11 System Precision Recall F-measure MFS 58.4% 58.4% 58.4% McCarthy 58.5% 58.5% 58.5% Verbs 58.5% 58.5% 58.5% All 58.6% 58.6% 58.6% Table 1: Refining the MFS baseline with predominant sense verbs as it does for nouns and adjectives. We hypothesize that this is due to the thesaural neighbours obtained from Lin’s thesaurus, and we group verbs accordin</context>
</contexts>
<marker>McCarthy, Koeling, Weeds, Carroll, 2007</marker>
<rawString>McCarthy, D., Koeling, R., Weeds, J., and Carroll, J. (2007). Unsupervised acquisition of predominant word senses. Computational Linguistics, 33(4):553–590.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
</authors>
<title>Using Wikipedia for automatic word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conferece of the North Americ an Chapter of the Association for Computational Linguistics,</booktitle>
<location>Rochester, New York.</location>
<contexts>
<context position="21184" citStr="Mihalcea (2007)" startWordPosition="3478" endWordPosition="3479">70 Table 4: Precision and recall of sparse category index classifier — both “soft” scores of standard Senseval script and scores where any correct answer in list returned by the classifier is counted as a correct answer (‘n-best’). ‘Back-off’ signals whether the system backs off to the most frequent sense baseline. livier and Senellart, 2007) based on each sense or context of interest. Advantages of this method over alternative methods that attempt to incorporate Wikipedia into WSD is that our system is unsupervised and that no manual mapping needs to take place between WordNet and Wikipedia. Mihalcea (2007) demonstrates that manual mappings can be created for a small number of words with relative ease, but for a very large number of words the effort involved in mapping would approach presented involves no be considerable. The approach presented here involves no mapping between WordNet and Wikipedia but human effort in mapping between WordNet and Wikipedia, but instead initializes the Green method with a vector based only on the article names (as described in Section 5.2). 5.1 Green Method The Green method (Ollivier and Senellart, 2007) is used to determine the importance of one node in a directe</context>
</contexts>
<marker>Mihalcea, 2007</marker>
<rawString>Mihalcea, R. (2007). Using Wikipedia for automatic word sense disambiguation. In Human Language Technologies 2007: The Conferece of the North Americ an Chapter of the Association for Computational Linguistics, Rochester, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
<author>R Beckwith</author>
<author>C Felbaum</author>
<author>D Gross</author>
<author>K Miller</author>
</authors>
<title>Introduction to WordNet: An on-line lexical database.</title>
<date>1990</date>
<journal>Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="3463" citStr="Miller et al., 1990" startWordPosition="531" endWordPosition="534"> MFS baseline. Rather than proposing a new algorithm which will tackle all words, we focus on improving upon the MFS baseline system when an alternative system proposes a high confidence answer. An MFS refining system can therefore benefit from answers suggested by a very low recall (but high precision) WSD system. We propose a number of novel approaches to WSD, but also demonstrate the importance of a highly accurate lemmatizer and part of speech tagger to the English all words task of SENSEVAL-3.1 We present our enriched most frequent sense 1Unless specified otherwise, we use WordNet 1.7.1 (Miller et al., 1990) and the associated sense annotated SemCor corpus (Miller et al., 1993) (translated to WordNet 1.7.1 by Rada Mihalcea). 10 Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 10–18, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics baseline in Section 2, which motivates the lemmatizer and part of speech tagger refinements presented in Section 3. Our novel high precision WSD algorithms include a reranking algorithm (Section 4), and a Wikipedia-based similarity measure (Section 5). The individual systems are</context>
</contexts>
<marker>Miller, Beckwith, Felbaum, Gross, Miller, 1990</marker>
<rawString>Miller, G., Beckwith, R., Felbaum, C., Gross, D., and Miller, K. (1990). Introduction to WordNet: An on-line lexical database. Journal of Lexicography, 3(4):235–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
<author>C Leacock</author>
<author>T Ranee</author>
<author>R Bunker</author>
</authors>
<title>A semantic concordance.</title>
<date>1993</date>
<booktitle>In Proceedings of the 3rd DARPA Workshop on Human Language Technology,</booktitle>
<pages>232--235</pages>
<contexts>
<context position="3534" citStr="Miller et al., 1993" startWordPosition="543" endWordPosition="546">all words, we focus on improving upon the MFS baseline system when an alternative system proposes a high confidence answer. An MFS refining system can therefore benefit from answers suggested by a very low recall (but high precision) WSD system. We propose a number of novel approaches to WSD, but also demonstrate the importance of a highly accurate lemmatizer and part of speech tagger to the English all words task of SENSEVAL-3.1 We present our enriched most frequent sense 1Unless specified otherwise, we use WordNet 1.7.1 (Miller et al., 1990) and the associated sense annotated SemCor corpus (Miller et al., 1993) (translated to WordNet 1.7.1 by Rada Mihalcea). 10 Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 10–18, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics baseline in Section 2, which motivates the lemmatizer and part of speech tagger refinements presented in Section 3. Our novel high precision WSD algorithms include a reranking algorithm (Section 4), and a Wikipedia-based similarity measure (Section 5). The individual systems are combined in Section 6, and we close with our conclusions in Section 7.</context>
</contexts>
<marker>Miller, Leacock, Ranee, Bunker, 1993</marker>
<rawString>Miller, G., Leacock, C., Ranee, T., and Bunker, R. (1993). A semantic concordance. In Proceedings of the 3rd DARPA Workshop on Human Language Technology, pages 232–235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Ollivier</author>
<author>P Senellart</author>
</authors>
<title>Finding related pages using Green measures: An illustration with Wikipedia.</title>
<date>2007</date>
<booktitle>In Association for the Advancement of Artificial Intelligence Conference on Artificial Intelligence (AAAI</booktitle>
<contexts>
<context position="21723" citStr="Ollivier and Senellart, 2007" startWordPosition="3567" endWordPosition="3570"> that no manual mapping needs to take place between WordNet and Wikipedia. Mihalcea (2007) demonstrates that manual mappings can be created for a small number of words with relative ease, but for a very large number of words the effort involved in mapping would approach presented involves no be considerable. The approach presented here involves no mapping between WordNet and Wikipedia but human effort in mapping between WordNet and Wikipedia, but instead initializes the Green method with a vector based only on the article names (as described in Section 5.2). 5.1 Green Method The Green method (Ollivier and Senellart, 2007) is used to determine the importance of one node in a directed graph with respect to other nodes.15 In the context of Wikipedia the method finds the articles which are most likely to be frequented if a random walk were used to traverse the articles, starting with a specific article and returning to that article if the random walk either strays too far off topic or to an article which is generally popular even without the context of the initial article. One of the features of the Green method is that it does not simply reproduce the global PageRank (Brin and Page, 1998), instead determining the</context>
<context position="24971" citStr="Ollivier and Senellart (2007)" startWordPosition="4130" endWordPosition="4133"> a word to be disambiguated also needs to be represented as corresponding articles in Wikipedia before using the Green method. The words that we search for in the titles of Wikipedia articles include the word itself, and, for every sense, the content words of the sense’s WordNet gloss, as well as the content of the sense’s hypernym gloss 15 and the synonyms of the hypernym. Exploring this particular aspect of this module — which information about a sense to extract before using the Green Method — is a point for further exploration. 5.4 Interpreting Projections The Green method as described by Ollivier and Senellart (2007) uses, as the initial set of articles, the vector containing only one article: that article for which related articles are being searched. We use as the initial set of articles the collection of articles in Wikipedia corresponding to either the context for the word to be disambiguated or the sense of a word. The random walker is modeled as starting in any of the articles in this set with uniform probability. Within the context of the Green method, this means that this initial set of articles corresponds to what would be linked to from a new Wikipedia article about the sense or context. Each of</context>
</contexts>
<marker>Ollivier, Senellart, 2007</marker>
<rawString>Ollivier, Y. and Senellart, P. (2007). Finding related pages using Green measures: An illustration with Wikipedia. In Association for the Advancement of Artificial Intelligence Conference on Artificial Intelligence (AAAI 2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
<author>S Patwardhan</author>
<author>J Michelizzi</author>
</authors>
<title>Wordnet::similarity - measuring the relatedness of concepts.</title>
<date>2004</date>
<booktitle>In Proceedings of Fifth Annual Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>38--41</pages>
<contexts>
<context position="7969" citStr="Pedersen et al., 2004" startWordPosition="1286" endWordPosition="1289">gh this method does not always outperform the MFS baseline based on SemCor, it does outperform it when the word’s SemCor frequency is below 5. We therefore switch our MFS baseline to this value for such words. This result is represented as ’McCarthy’ in Table 1, which contains the results of the techniques presented in this Section evaluated on the SENSEvnr.-3 English all words task. 2.2 Verb predominant sense McCarthy et al. (2007) observe that their predominant sense method is not performing as well for 3We use the Lesk (overlap) similarity as implemented by the WordNet::similarity package (Pedersen et al., 2004). z dss(w, nj) * sss(si, nj) njENw Es&apos;iEsenses(w) sss(s&apos;i, nj) 11 System Precision Recall F-measure MFS 58.4% 58.4% 58.4% McCarthy 58.5% 58.5% 58.5% Verbs 58.5% 58.5% 58.5% All 58.6% 58.6% 58.6% Table 1: Refining the MFS baseline with predominant sense verbs as it does for nouns and adjectives. We hypothesize that this is due to the thesaural neighbours obtained from Lin’s thesaurus, and we group verbs according to the subcategorization frame (SCF) distributions they present in the vnr.Ex (Korhonen et al., 2006) lexicon. A word w1 is grouped with word w2 if the Bhattacharyya coefficient �BC(w1</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Pedersen, T., Patwardhan, S., and Michelizzi, J. (2004). Wordnet::similarity - measuring the relatedness of concepts. In Proceedings of Fifth Annual Meeting of the North American Chapter of the Association for Computational Linguistics, pages 38–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmidt</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on New Methods in Language Processing,</booktitle>
<pages>44--49</pages>
<contexts>
<context position="5678" citStr="Schmidt, 1994" startWordPosition="908" endWordPosition="909">erformance of the MFS baseline can be attributed to a number of factors: the English all words task is run on natural text and therefore performance greatly depends on the accuracy of the lemmatizer and the part of speech tagger employed.2 If the lemmatizer incorrectly identifies the stem of the word, the MFS will be looked up for the wrong word and the resulting sense assignment will be incorrect. The performance of the MFS given the correct lemma and part of speech information is 66%, while the performance of the MFS with a Port Stemmer without any POS information is 32%. With a TreeTagger (Schmidt, 1994), and a sophisticated lemma back-off strategy, the performance increases to 56%. It is this difference in 2Other possible factors include: 1) The sense distribution in the corpus which the MFS baseline is drawn from, 2) If SemCor is used as the underlying sense annotated corpus, the accuracy of the mapping from WordNet 1.6 (with which SemCor was initially annotated) to WordNet 1.7.1 could also have an effect on the performance). performance which motivates refining the most frequent sense baseline, and our work on improving the underlying lemmatizer and part of speech tagger presented in Secti</context>
</contexts>
<marker>Schmidt, 1994</marker>
<rawString>Schmidt, H. (1994). Probabilistic part-of-speech tagging using decision trees. In Proceedings of the International Conference on New Methods in Language Processing, pages 44–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Snyder</author>
<author>M Palmer</author>
</authors>
<title>The english allwords task.</title>
<date>2004</date>
<booktitle>Proceedings of SENSEVAL-3: Third International Workshop on Evaluating Word Sense Disambiguating Systems,</booktitle>
<pages>41--43</pages>
<editor>In Mihalcea, R. and Chklowski, T., editors,</editor>
<contexts>
<context position="4990" citStr="Snyder and Palmer (2004)" startWordPosition="785" endWordPosition="788">he word’s part of speech, and the MFS for that part of speech is selected. Although this is a fairly naive baseline, it has been shown to be difficult to beat, with only 5 systems of the 26 submitted to the SENSEvnr.-3 English all words task outperforming the reported 62.5% MFS baseline. The success of the MFS baseline is mainly due to the frequency distribution of senses, with the shape of the sense rank versus frequency graph being a Zipfian curve (i.e., the top-ranked sense being much more likely than any other sense). However, two different MFS baseline performance results are reported in Snyder and Palmer (2004), with further implementations being different still. The differences in performance of the MFS baseline can be attributed to a number of factors: the English all words task is run on natural text and therefore performance greatly depends on the accuracy of the lemmatizer and the part of speech tagger employed.2 If the lemmatizer incorrectly identifies the stem of the word, the MFS will be looked up for the wrong word and the resulting sense assignment will be incorrect. The performance of the MFS given the correct lemma and part of speech information is 66%, while the performance of the MFS w</context>
</contexts>
<marker>Snyder, Palmer, 2004</marker>
<rawString>Snyder, B. and Palmer, M. (2004). The english allwords task. In Mihalcea, R. and Chklowski, T., editors, Proceedings of SENSEVAL-3: Third International Workshop on Evaluating Word Sense Disambiguating Systems, pages 41–43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I H Witten</author>
<author>E Frank</author>
</authors>
<date>2000</date>
<booktitle>Data mining: Practical Machine Learning Tools and Techniques with Java Implementations, chapter 8.</booktitle>
<publisher>Morgan Kaufmann Publishers.</publisher>
<contexts>
<context position="28153" citStr="Witten and Frank, 2000" startWordPosition="4687" endWordPosition="4690"> could have a higher performance than the individual systems. We investigate a number of techniques to combine the results – while the integration of the lemma/ part of speech refinement is done by all modules as a preprocessing step, the method of combination of the resulting modules is less clear. As shown in Florian et al. (2002), a simple voting mechanism achieves comparable performance to a stacking mechanism. We present our results in Table 6, DT gives the result of a 10-fold cross-validation of WEKA stacked decision trees and nearest neighbours built from the individual system results (Witten and Frank, 2000). Very few decisions are changed with the voting method of combination, and the overall result does not outperform the best MFS baseline (presented in the table as “All MFS”). This combination method may be more useful with a greater number of systems being combined – our system only combines three systems (thus only one non-MFS system has to suggest the MFS for this to be selected), and backs off to the MFS sense in case all three disagree. The degree of complementarity between the Wiki system and the MFS system indicates that these will override the Rerank system in many cases. Better result</context>
</contexts>
<marker>Witten, Frank, 2000</marker>
<rawString>Witten, I. H. and Frank, E. (2000). Data mining: Practical Machine Learning Tools and Techniques with Java Implementations, chapter 8. Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<title>A lexicalized tree adjoining grammar for English.</title>
<date>2001</date>
<tech>Technical Report IRCS-01-03,</tech>
<institution>XTAG Research Group</institution>
<marker>2001</marker>
<rawString>XTAG Research Group (2001). A lexicalized tree adjoining grammar for English. Technical Report IRCS-01-03, IRCS, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>One Sense Per Collocation.</title>
<date>1993</date>
<booktitle>In Proceedings of the Human Language Technology Conference,</booktitle>
<location>Princeton, NJ, USA.</location>
<marker>Yarowsky, 1993</marker>
<rawString>Yarowsky, D. (1993). One Sense Per Collocation. In Proceedings of the Human Language Technology Conference, Princeton, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V H Yngve</author>
</authors>
<title>Syntax and the problem of multiple meaning.</title>
<date>1955</date>
<booktitle>Machine translation of languages,</booktitle>
<pages>208--226</pages>
<editor>In Locke, W. N. and Booth, A. D., editors,</editor>
<publisher>John Wiley and Sons,</publisher>
<location>New York.</location>
<contexts>
<context position="1618" citStr="Yngve, 1955" startWordPosition="246" endWordPosition="248"> system evaluation exercises. In this work, we present a combination system, which, rather than designing a single approach to all words, enriches the most frequent sense baseline when there is high confidence for an alternative sense to be chosen. WSD, the task of assigning a sense to a given word from a sense inventory is clearly necessary for other natural language processing tasks. For example, when performing machine translation, it is necessary to distinguish between word senses in the original language if the different senses have different possible translations in the target language (Yngve, 1955). A number of different approaches to WSD have been explored in recent years, with two Jon Dehdari Department of Linguistics The Ohio State University jonsafari@ling.ohio-state.edu Dennis Mehay Department of Linguistics The Ohio State University mehay@ling.ohio-state.edu distinct approaches: techniques which require annotated training data (supervised techniques) and techniques which do not (unsupervised methods). It has long been believed that supervised systems, which can be tuned to a word’s context, greatly outperform unsupervised systems. This theory was supported in the SENSEVAL WSD syst</context>
</contexts>
<marker>Yngve, 1955</marker>
<rawString>Yngve, V. H. (1955). Syntax and the problem of multiple meaning. In Locke, W. N. and Booth, A. D., editors, Machine translation of languages, pages 208–226. John Wiley and Sons, New York.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>