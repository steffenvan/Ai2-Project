<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005750">
<title confidence="0.967432">
Combining MDL Transliteration Training with Discriminative Modeling
</title>
<author confidence="0.842917">
Dmitry Zelenko
</author>
<affiliation confidence="0.445138">
4300 Fair Lakes Ct.
Fairfax, VA 22033, USA
</affiliation>
<email confidence="0.964957">
dmitry zelenko@sra.com
</email>
<sectionHeader confidence="0.992986" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999934">
We present a transliteration system that
introduces minimum description length
training for transliteration and combines
it with discriminative modeling. We ap-
ply the proposed approach to translitera-
tion from English to 8 non-Latin scripts,
with promising results.
</bodyText>
<sectionHeader confidence="0.999254" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998979">
Recent research in transliteration and translation
showed utility of increasing the n-gram size in
transliteration models and phrase tables (Koehn
et al., 2003). Yet most learning algorithms for
training n-gram transliteration models place re-
strictions on the size of n-gram due to tractability
and overfitting issues, and, in the case of machine
translation, construct the phrase table after train-
ing the model, in an ad-hoc manner. In this paper,
we present a minimum description length (MDL)
approach (Grunwald, 2007) for learning transliter-
ation models comprising n-grams of unrestricted
size. Given a bilingual dictionary of transliterated
data we seek to derive a transliteration model so
that the combined size of the data and the model is
minimized.
Use of discriminative modeling for transliter-
ation and translation is another promising direc-
tion allowing incorporation of arbitrary features
in the transliteration process (Zelenko and Aone,
2006; Goldwasser and Roth, 2008). Here we pro-
pose to use the transliteration model derived via
MDL training as a starting point and learn the
model weights in the discriminative manner. The
discriminative approach also provides a natural
way to integrate the language modeling compo-
nent into the transliteration decoding process.
We experimentally evaluate the proposed ap-
proach on the standard datasets for the task of
transliterating from English to 8 non-Latin scripts
</bodyText>
<sectionHeader confidence="0.938477" genericHeader="method">
2 MDL Training for Transliteration
</sectionHeader>
<bodyText confidence="0.99978525">
In our transliteration setting, we are given a string
e written in an alphabet V1 (e.g., Latin), which is
to be transliterated into a string f written in an al-
phabet V2 (e.g., Chinese). We consider a transliter-
ation process that is conducted by a transliteration
model T, which represents a function mapping a
pair of strings (ei, fi) into a score T(ei, fi) E R.
For an alignment 1 A = {(ei, fi)} of e and f, we
define the alignment score T (A) = Ei T (ei, fi).
For a string e and a model T, the decoding process
seeks the optimal transliteration T(e) with respect
to the model T:
</bodyText>
<equation confidence="0.89345">
T(e) = arg max{ T(A)  |�A = {(ei, fi)} }
f
</equation>
<bodyText confidence="0.99551525">
Different assumptions for transliteration mod-
els lead to different estimation algorithms. A
popular approach is to assume a joint gener-
ative model for pairs (e, f), so that given an
alignment A = {(ei, fi)}, a probability P(e, f)
is defined to be Hi p(ei, fi). The probabili-
ties p(ei, fi) are estimated using the EM algo-
rithm, and the corresponding transliteration model
is T(ei, fi) = log(p(ei, fi)). We can alterna-
tively model the conditional probability directly:
P(f|e) = Hi p(fi|ei), where we again estimate
the conditional probabilities p(fi|ei) via the EM
algorithm, and define the transliteration model ac-
cordingly: T(ei, fi) = log(p(fi|ei)). We can also
combine joint estimation with conditional decod-
ing, observing that p(fi|ei) = p(ei,fi) and us-
</bodyText>
<subsectionHeader confidence="0.537287">
Ef p(ei,fi)
</subsectionHeader>
<bodyText confidence="0.999567428571429">
ing the conditional transliteration model after esti-
mating a joint generative model.
Increasing the maximum n-gram size in prob-
abilistic modeling approaches, at some point, de-
grades model accuracy due to overfitting. There-
fore, probabilistic approaches typically use a small
n-gram size, and perform additional modeling post
</bodyText>
<footnote confidence="0.993238">
1Here we consider only monotone alignments.
</footnote>
<page confidence="0.980039">
116
</page>
<note confidence="0.9838775">
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 116–119,
Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.9991662">
factum: examples include joint n-gram modeling
and phrase table construction in machine transla-
tion.
We propose to apply the MDL principle to
transliteration modeling by seeking the model that
compresses the transliteration data so that the
combined size of the compressed data and the
model is minimized. If T corresponds to a joint
probabilistic model P = {p(ei, fi)}, then we can
use the model to encode the data D = {(e, f)} in
</bodyText>
<equation confidence="0.889193">
CD(P) = − X log P(e, f)
(e,f)
X= − XmaxA log p(ei, fi)
(e,f) i
bits, where A = {(ei, fi)} is an alignment of e and
f.
</equation>
<bodyText confidence="0.975420833333333">
We can encode each symbol of an alphabet V
using log |V  |bits so encoding a string s of length
|s |from alphabet V takes CV (s) = log |V |(|s |+
1) bits (we add an extra string termination sym-
bol for separability). Therefore, we encode each
transliteration model in
</bodyText>
<equation confidence="0.983824">
CT (P) = X CT (ei, fi)
(ei,fi)
</equation>
<bodyText confidence="0.9716558">
bits, where CT (ei, fi) = CV1(ei) + CV2(fi) −
log p(ei, fi) is the number of bits used to encode
both the pair (ei, fi) and its code according to P.
Thus, we seek a probability distribution P that
minimizes C(P) = CD(P) + CT(P).
Let P be an initial joint probability distribution
for a transliteration model T such that a string pair
(ei, fi) appeared n(ei, fi) times, and p(ei, fi) =
n(ei, fi)/N, where N = P(ei,fi) n(ei, fi).
here we distribute the model size component to
Then, encoding a pair (ei, fi) takes on aver-
age C(ei, fi) = CT (ei,fi)
n(ei,fi) − log p(ei, fi) bits -
all occurrences of (ei, fi) in the data. Notice
that the combined data and model size C(P) =
</bodyText>
<listItem confidence="0.935599727272727">
P(ei,fi) n(ei,fi)C(ei,fi). It is this quantity
C(ei, fi) that we propose to use when conducting
the MDL training algorithm below.
1. Pick an initial P. Compute C(ei, fi) =
CT (ei,fi) − log p(ei, fi). Set combined size
n(ei,fi)
C(P) = P(ei,fi) n(ei, fi)C(ei, fi).
2. Iterate: during each iteration, for each
(e, f) ∈ D, find the minimum codesize
P
alignment A = arg minA i C(ei, fi) of
</listItem>
<bodyText confidence="0.999236666666667">
(e, f). Use the alignments to re-estimate P
and re-compute C. Exit when there is no im-
provement in the combined model and data
size.
Experimentally, we observed fast convergence of
the above algorithm just after a few iterations,
though we cannot present a convergence proof as
yet. We picked the initial model by computing
co-occurrence counts of n-gram pairs in D, that
is, n(ei, fi) = P(e,f) min(ne(ei), nf(fi)), where
ne(ei) (nf(fi)) is the number of times the n-gram
ei (fi) appeared in the string e (f).
Note that a Bayesian interpretation of the pro-
posed approach is not straightforward due to
the use of empirical component −log p(ei, fi) in
model encoding. Changing the model encoding to
use, for example, a code for n(ei, fi) would allow
for a direct Bayesian interpretation of the proposed
code, and we plan to pursue this direction in the
future.
The output of the MDL training algorithm is
the joint probability model P that we use to de-
fine the transliteration model weights as the loga-
rithm of corresponding conditional probabilities:
</bodyText>
<equation confidence="0.822834">
T (ei, fi) = logP p(ei,fi)
f p(ei,f). During the decod-
</equation>
<bodyText confidence="0.999725166666667">
ing process of inferring f from e via an align-
ment A, we integrate the language model proba-
bility p(f) via a linear combination: TGEN(e) =
arg maxf {T (A) + p log p(f)/|f|}, where p is
a combination parameter estimated via cross-
validation.
</bodyText>
<sectionHeader confidence="0.997369" genericHeader="method">
3 Discriminative Training
</sectionHeader>
<bodyText confidence="0.999993166666667">
We use the MDL-trained transliteration model
T as a starting point for discriminative train-
ing: we consider all n-gram pairs (ei, fi) with
nonzero probabilities p(ei, fi) as features of a lin-
ear discriminative model TDISCR. We also in-
tegrate the normalized language modeling prob-
</bodyText>
<equation confidence="0.828346">
1
</equation>
<bodyText confidence="0.965910454545455">
ability p0(f) = p(f) If in the discriminative
model as one of the features: TDISCR(e) =
arg maxf {T (A) + T0p0(f)}. We learn the
weights T(ei, fi) and T0 of the discriminative
model using the average perceptron algorithm of
(Collins, 2002). Since both the transliteration
model and the language model are required to be
learned from the same data, and the language mod-
eling probability is integrated into our decoding
process, we remove the string e from the language
model before processing the example (f, e) during
</bodyText>
<page confidence="0.990529">
117
</page>
<bodyText confidence="0.9999868">
training; we re-incorporate the string e in the lan-
guage model after the example (f, e) is processed
by the averaged perceptron algorithm. We use the
discriminatively trained model as the ”standard”
system in our experiments.
</bodyText>
<sectionHeader confidence="0.999207" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999967333333333">
We use the standard data for transliterating
from English into 8 non-Latin scripts: Chinese
(Haizhou et al., 2004); Korean, Japanese (Kanji),
and Japanese (Katakana) (CJK Institute, 2009);
Hindi, Tamil, Kannada, and Russian (Kumaran
and Kellner, 2007). The data is provided as part
of the Named Entities Workshop 2009 Machine
Transliteration Shared Task (Li et al., 2009).
For all 8 datasets, we report scores on the stan-
dard tests sets provided as part of the evaluation.
Details of the evaluation methodology are pre-
sented in (Li et al., 2009).
</bodyText>
<subsectionHeader confidence="0.985662">
4.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999984714285714">
We perform the same uniform processing of data:
names are considered sequences of Unicode char-
acters in their standard decomposed form (NFD).
In particular, Korean Hangul characters are de-
composed into Jamo syllabary. Since the evalu-
ation data are provided in the re-composed form,
we re-compose output of the transliteration sys-
tem.
We split multi-word names (in Hindi, Tamil,
and Kannada datasets) in single words and con-
ducted training and evaluation on the single word
level. We assume no word order change for multi-
word names and ignore name pairs with different
numbers of words.
</bodyText>
<subsectionHeader confidence="0.998937">
4.2 System Parameters and Tuning
</subsectionHeader>
<bodyText confidence="0.9840255">
We apply pre-set system parameters with very lit-
tle tuning. In particular, we utilize a 5-gram lan-
guage model with Good-Turing discounting. The
MDL training algorithm requires only the cardi-
nalities of the corresponding alphabets as parame-
ters, and we use the following approximate vocab-
ulary sizes typically rounded to the closest power
of 2 (except for Chinese and Japanese): for En-
glish, Russian, Tamil, and Kannada, we set |V  |=
32; for Katakana and Hindi, |V  |= 64; for Korean
Jamo, |V  |= 128; for Chinese and Japanese Kanji,
|V  |= 1024.
We perform 10 iterations of the average per-
ceptron algorithm for discriminative training. For
</bodyText>
<table confidence="0.999700363636364">
Init Comp Ratio Dict
Chinese 333 Kb 158 Kb 0.48 5780
Hindi 159 Kb 72 Kb 0.45 1956
Japanese 170 Kb 82 Kb 0.48 4394
(Kanji)
Kannada 131 Kb 62 Kb 0.48 2010
Japanese 289 Kb 136 Kb 0.47 3383
(Katakana)
Korean 69 Kb 31 Kb 0.45 1181
Russian 78 Kb 37 Kb 0.48 865
Tamil 134 Kb 62 Kb 0.46 1827
</table>
<tableCaption confidence="0.89676625">
Table 1: MDL Data and Model Compression
showing initial data size, final combined data and
model size, the compression ratio, and the number
of n-gram pairs in the final model.
</tableCaption>
<table confidence="0.999115818181818">
T1(Acc) T2(Acc) T2(F) T2(MRR)
Chinese 0.522 0.619 0.847 0.711
Hindi 0.312 0.409 0.864 0.527
Japanese 0.484 0.509 0.675 0.6
(Kanji)
Kannada 0.227 0.345 0.854 0.462
Japanese 0.318 0.420 0.807 0.541
(Katakana)
Korean 0.339 0.413 0.702 0.524
Russian 0.488 0.566 0.919 0.662
Tamil 0.267 0.374 0.880 0.512
</table>
<tableCaption confidence="0.986115">
Table 2: Experimental results for transliteration
</tableCaption>
<bodyText confidence="0.9359392">
from English to 8 non-Latin scripts comparing
performance of generative (T1) and corresponding
discriminative (T2) models.
both alignment and decoding, we use a beam
search decoder, with the beam size set to 100.
</bodyText>
<subsectionHeader confidence="0.949194">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.999949384615385">
Our first set of experiments illustrates compres-
sion achieved by MDL training. Table 1 shows for
each for the training datasets, the original size of
the data, compressed size of the data including the
model size, the compression ratio, and the number
of n-gram pairs in the final model.
We see very similar compression for all lan-
guages. The number of n-gram pairs for the final
model is also relatively small. In general, MDL
training with discriminative modeling allows us to
discover a flexible small set of features (n-gram
pairs) without placing any restriction on n-gram
size. We can interpret MDL training as search-
</bodyText>
<page confidence="0.996669">
118
</page>
<bodyText confidence="0.999966703703704">
ing implicitly for the best bound on the n-gram
size together with searching for appropriate fea-
tures. Our preliminary experiments also indicate
that performance of models produced by the MDL
approach roughly corresponds to performance of
models trained with the optimal bound on the size
of n-gram features.
Table 2 demonstrates that discriminative model-
ing significantly improves performance of the cor-
responding generative models. In this setting, the
MDL training step is effectively used for feature
construction: its goal is to automatically hone in
on a small set of features whose weights are later
learned by discriminative methods.
From a broader perspective, it is an open
question whether seeking a compact representa-
tion of sequential data leads to robust and best-
performing models, especially in noisy environ-
ments. For example, state-of-the-art phrase trans-
lation models eschew succinct representations,
and instead employ broad redundant sets of fea-
tures (Koehn et al., 2003). On the other hand,
recent research show that small translation mod-
els lead to superior alignment (Bodrumlu et al.,
2009). Therefore, investigation of the trade-off
between robust redundant and succinct representa-
tion present an interesting area for future research.
</bodyText>
<sectionHeader confidence="0.999961" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999966769230769">
There is plethora of work on transliteration cov-
ering both generative and discriminative models:
(Knight and Graehl, 1997; Al-onaizan and Knight,
2002; Huang et al., 2004; Haizhou et al., 2004; Ze-
lenko and Aone, 2006; Sherif and Kondrak, 2007;
Goldwasser and Roth, 2008). Application of the
minimum description length principle (Grunwald,
2007) in natural language processing has been
heretofore mostly limited to morphological analy-
sis (Goldsmith, 2001; Argamon et al., 2004). (Bo-
drumlu et al., 2009) present a related approach on
optimizing the alignment dictionary size in ma-
chine translation.
</bodyText>
<sectionHeader confidence="0.999667" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9990985">
We introduced a minimum description length ap-
proach for training transliteration models that al-
lows to avoid overfitting without putting apriori
constraints of the size of n-grams in transliteration
models. We plan to apply the same paradigm to
other sequence modeling tasks such as sequence
classification and segmentation, in both super-
vised and unsupervised settings.
</bodyText>
<sectionHeader confidence="0.990165" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9999178125">
Y. Al-onaizan and K. Knight. 2002. Machine translit-
eration of names in arabic text. In ACL Workshop
on Comp. Approaches to Semitic Languages, pages
34–46.
S. Argamon, N. Akiva, A. Amir, and O. Kapah. 2004.
Efficient unsupervised recursive word segmentation
using minimum description length. In Proceedings
of COLING.
T. Bodrumlu, K. Knight, and S. Ravi. 2009. A new ob-
jective function for word alignment. In Proceedings
NAACL Workshop on Integer Linear Programming
for NLP.
CJK Institute. 2009. http://www.cjk.org.
M. Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
ofEMNLP.
J. Goldsmith. 2001. Unsupervised learning of the mor-
phology of a natural language. Computational Lin-
guistics, pages 153–198.
D. Goldwasser and D. Roth. 2008. Translitera-
tion as constrained optimization. In Proceedings of
EMNLP.
P. Grunwald. 2007. The Minimum Description Length
principle. MIT Press.
L. Haizhou, Z. Min, and S. Jian. 2004. A joint source-
channel model for machine transliteration. In Pro-
ceedings ofACL.
F. Huang, S. Vogel,, and A. Waibel. 2004. Improving
named entity translation combining phonetic and se-
mantic similarities. In Proceedings ofHLT/NAACL.
K. Knight and J. Graehl. 1997. Machine translitera-
tion. Computational Linguistics, pages 128–135.
P. Koehn, F. Och, and D. Marcu. 2003. Statis-
tical phrase-based translation. In Proceedings of
NLT/NAACL.
A. Kumaran and T. Kellner. 2007. A generic frame-
work for machine transliteration. In Proceedings of
SIGIR.
Haizhou Li, A. Kumaran, Min Zhang, and V. Pervou-
chine. 2009. Whitepaper of news 2009 machine
transliteration shared task. In Proceedings of ACL-
IJCNLP 2009 Named Entities Workshop (NEWS
2009).
T. Sherif and G. Kondrak. 2007. Substring-based
transliteration. In Proceedings ofACL.
D. Zelenko and C. Aone. 2006. Discriminative meth-
ods for transliteration. In Proceedings of EMNLP.
</reference>
<page confidence="0.999093">
119
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.298318">
<title confidence="0.6500565">Combining MDL Transliteration Training with Discriminative Modeling Dmitry</title>
<address confidence="0.9902635">4300 Fair Lakes Fairfax, VA 22033,</address>
<email confidence="0.994641">dmitryzelenko@sra.com</email>
<abstract confidence="0.998337375">We present a transliteration system that introduces minimum description length training for transliteration and combines it with discriminative modeling. We apply the proposed approach to transliteration from English to 8 non-Latin scripts, with promising results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Al-onaizan</author>
<author>K Knight</author>
</authors>
<title>Machine transliteration of names in arabic text.</title>
<date>2002</date>
<booktitle>In ACL Workshop on Comp. Approaches to Semitic Languages,</booktitle>
<pages>34--46</pages>
<contexts>
<context position="13020" citStr="Al-onaizan and Knight, 2002" startWordPosition="2164" endWordPosition="2167">specially in noisy environments. For example, state-of-the-art phrase translation models eschew succinct representations, and instead employ broad redundant sets of features (Koehn et al., 2003). On the other hand, recent research show that small translation models lead to superior alignment (Bodrumlu et al., 2009). Therefore, investigation of the trade-off between robust redundant and succinct representation present an interesting area for future research. 5 Related Work There is plethora of work on transliteration covering both generative and discriminative models: (Knight and Graehl, 1997; Al-onaizan and Knight, 2002; Huang et al., 2004; Haizhou et al., 2004; Zelenko and Aone, 2006; Sherif and Kondrak, 2007; Goldwasser and Roth, 2008). Application of the minimum description length principle (Grunwald, 2007) in natural language processing has been heretofore mostly limited to morphological analysis (Goldsmith, 2001; Argamon et al., 2004). (Bodrumlu et al., 2009) present a related approach on optimizing the alignment dictionary size in machine translation. 6 Conclusions We introduced a minimum description length approach for training transliteration models that allows to avoid overfitting without putting ap</context>
</contexts>
<marker>Al-onaizan, Knight, 2002</marker>
<rawString>Y. Al-onaizan and K. Knight. 2002. Machine transliteration of names in arabic text. In ACL Workshop on Comp. Approaches to Semitic Languages, pages 34–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Argamon</author>
<author>N Akiva</author>
<author>A Amir</author>
<author>O Kapah</author>
</authors>
<title>Efficient unsupervised recursive word segmentation using minimum description length.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING.</booktitle>
<marker>Argamon, Akiva, Amir, Kapah, 2004</marker>
<rawString>S. Argamon, N. Akiva, A. Amir, and O. Kapah. 2004. Efficient unsupervised recursive word segmentation using minimum description length. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Bodrumlu</author>
<author>K Knight</author>
<author>S Ravi</author>
</authors>
<title>A new objective function for word alignment.</title>
<date>2009</date>
<booktitle>In Proceedings NAACL Workshop on Integer Linear Programming for NLP.</booktitle>
<contexts>
<context position="12709" citStr="Bodrumlu et al., 2009" startWordPosition="2120" endWordPosition="2123">d for feature construction: its goal is to automatically hone in on a small set of features whose weights are later learned by discriminative methods. From a broader perspective, it is an open question whether seeking a compact representation of sequential data leads to robust and bestperforming models, especially in noisy environments. For example, state-of-the-art phrase translation models eschew succinct representations, and instead employ broad redundant sets of features (Koehn et al., 2003). On the other hand, recent research show that small translation models lead to superior alignment (Bodrumlu et al., 2009). Therefore, investigation of the trade-off between robust redundant and succinct representation present an interesting area for future research. 5 Related Work There is plethora of work on transliteration covering both generative and discriminative models: (Knight and Graehl, 1997; Al-onaizan and Knight, 2002; Huang et al., 2004; Haizhou et al., 2004; Zelenko and Aone, 2006; Sherif and Kondrak, 2007; Goldwasser and Roth, 2008). Application of the minimum description length principle (Grunwald, 2007) in natural language processing has been heretofore mostly limited to morphological analysis (G</context>
</contexts>
<marker>Bodrumlu, Knight, Ravi, 2009</marker>
<rawString>T. Bodrumlu, K. Knight, and S. Ravi. 2009. A new objective function for word alignment. In Proceedings NAACL Workshop on Integer Linear Programming for NLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>CJK Institute</author>
</authors>
<date>2009</date>
<note>http://www.cjk.org.</note>
<contexts>
<context position="8299" citStr="Institute, 2009" startWordPosition="1397" endWordPosition="1398">the same data, and the language modeling probability is integrated into our decoding process, we remove the string e from the language model before processing the example (f, e) during 117 training; we re-incorporate the string e in the language model after the example (f, e) is processed by the averaged perceptron algorithm. We use the discriminatively trained model as the ”standard” system in our experiments. 4 Experiments We use the standard data for transliterating from English into 8 non-Latin scripts: Chinese (Haizhou et al., 2004); Korean, Japanese (Kanji), and Japanese (Katakana) (CJK Institute, 2009); Hindi, Tamil, Kannada, and Russian (Kumaran and Kellner, 2007). The data is provided as part of the Named Entities Workshop 2009 Machine Transliteration Shared Task (Li et al., 2009). For all 8 datasets, we report scores on the standard tests sets provided as part of the evaluation. Details of the evaluation methodology are presented in (Li et al., 2009). 4.1 Preprocessing We perform the same uniform processing of data: names are considered sequences of Unicode characters in their standard decomposed form (NFD). In particular, Korean Hangul characters are decomposed into Jamo syllabary. Sinc</context>
</contexts>
<marker>Institute, 2009</marker>
<rawString>CJK Institute. 2009. http://www.cjk.org.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings ofEMNLP.</booktitle>
<contexts>
<context position="7589" citStr="Collins, 2002" startWordPosition="1285" endWordPosition="1286">, where p is a combination parameter estimated via crossvalidation. 3 Discriminative Training We use the MDL-trained transliteration model T as a starting point for discriminative training: we consider all n-gram pairs (ei, fi) with nonzero probabilities p(ei, fi) as features of a linear discriminative model TDISCR. We also integrate the normalized language modeling prob1 ability p0(f) = p(f) If in the discriminative model as one of the features: TDISCR(e) = arg maxf {T (A) + T0p0(f)}. We learn the weights T(ei, fi) and T0 of the discriminative model using the average perceptron algorithm of (Collins, 2002). Since both the transliteration model and the language model are required to be learned from the same data, and the language modeling probability is integrated into our decoding process, we remove the string e from the language model before processing the example (f, e) during 117 training; we re-incorporate the string e in the language model after the example (f, e) is processed by the averaged perceptron algorithm. We use the discriminatively trained model as the ”standard” system in our experiments. 4 Experiments We use the standard data for transliterating from English into 8 non-Latin sc</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M. Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings ofEMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goldsmith</author>
</authors>
<title>Unsupervised learning of the morphology of a natural language. Computational Linguistics,</title>
<date>2001</date>
<pages>153--198</pages>
<marker>Goldsmith, 2001</marker>
<rawString>J. Goldsmith. 2001. Unsupervised learning of the morphology of a natural language. Computational Linguistics, pages 153–198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Goldwasser</author>
<author>D Roth</author>
</authors>
<title>Transliteration as constrained optimization.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1421" citStr="Goldwasser and Roth, 2008" startWordPosition="202" endWordPosition="205">n, construct the phrase table after training the model, in an ad-hoc manner. In this paper, we present a minimum description length (MDL) approach (Grunwald, 2007) for learning transliteration models comprising n-grams of unrestricted size. Given a bilingual dictionary of transliterated data we seek to derive a transliteration model so that the combined size of the data and the model is minimized. Use of discriminative modeling for transliteration and translation is another promising direction allowing incorporation of arbitrary features in the transliteration process (Zelenko and Aone, 2006; Goldwasser and Roth, 2008). Here we propose to use the transliteration model derived via MDL training as a starting point and learn the model weights in the discriminative manner. The discriminative approach also provides a natural way to integrate the language modeling component into the transliteration decoding process. We experimentally evaluate the proposed approach on the standard datasets for the task of transliterating from English to 8 non-Latin scripts 2 MDL Training for Transliteration In our transliteration setting, we are given a string e written in an alphabet V1 (e.g., Latin), which is to be transliterate</context>
<context position="13140" citStr="Goldwasser and Roth, 2008" startWordPosition="2185" endWordPosition="2188"> and instead employ broad redundant sets of features (Koehn et al., 2003). On the other hand, recent research show that small translation models lead to superior alignment (Bodrumlu et al., 2009). Therefore, investigation of the trade-off between robust redundant and succinct representation present an interesting area for future research. 5 Related Work There is plethora of work on transliteration covering both generative and discriminative models: (Knight and Graehl, 1997; Al-onaizan and Knight, 2002; Huang et al., 2004; Haizhou et al., 2004; Zelenko and Aone, 2006; Sherif and Kondrak, 2007; Goldwasser and Roth, 2008). Application of the minimum description length principle (Grunwald, 2007) in natural language processing has been heretofore mostly limited to morphological analysis (Goldsmith, 2001; Argamon et al., 2004). (Bodrumlu et al., 2009) present a related approach on optimizing the alignment dictionary size in machine translation. 6 Conclusions We introduced a minimum description length approach for training transliteration models that allows to avoid overfitting without putting apriori constraints of the size of n-grams in transliteration models. We plan to apply the same paradigm to other sequence</context>
</contexts>
<marker>Goldwasser, Roth, 2008</marker>
<rawString>D. Goldwasser and D. Roth. 2008. Transliteration as constrained optimization. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Grunwald</author>
</authors>
<title>The Minimum Description Length principle.</title>
<date>2007</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="958" citStr="Grunwald, 2007" startWordPosition="136" endWordPosition="137">to transliteration from English to 8 non-Latin scripts, with promising results. 1 Introduction Recent research in transliteration and translation showed utility of increasing the n-gram size in transliteration models and phrase tables (Koehn et al., 2003). Yet most learning algorithms for training n-gram transliteration models place restrictions on the size of n-gram due to tractability and overfitting issues, and, in the case of machine translation, construct the phrase table after training the model, in an ad-hoc manner. In this paper, we present a minimum description length (MDL) approach (Grunwald, 2007) for learning transliteration models comprising n-grams of unrestricted size. Given a bilingual dictionary of transliterated data we seek to derive a transliteration model so that the combined size of the data and the model is minimized. Use of discriminative modeling for transliteration and translation is another promising direction allowing incorporation of arbitrary features in the transliteration process (Zelenko and Aone, 2006; Goldwasser and Roth, 2008). Here we propose to use the transliteration model derived via MDL training as a starting point and learn the model weights in the discri</context>
<context position="13214" citStr="Grunwald, 2007" startWordPosition="2196" endWordPosition="2197">er hand, recent research show that small translation models lead to superior alignment (Bodrumlu et al., 2009). Therefore, investigation of the trade-off between robust redundant and succinct representation present an interesting area for future research. 5 Related Work There is plethora of work on transliteration covering both generative and discriminative models: (Knight and Graehl, 1997; Al-onaizan and Knight, 2002; Huang et al., 2004; Haizhou et al., 2004; Zelenko and Aone, 2006; Sherif and Kondrak, 2007; Goldwasser and Roth, 2008). Application of the minimum description length principle (Grunwald, 2007) in natural language processing has been heretofore mostly limited to morphological analysis (Goldsmith, 2001; Argamon et al., 2004). (Bodrumlu et al., 2009) present a related approach on optimizing the alignment dictionary size in machine translation. 6 Conclusions We introduced a minimum description length approach for training transliteration models that allows to avoid overfitting without putting apriori constraints of the size of n-grams in transliteration models. We plan to apply the same paradigm to other sequence modeling tasks such as sequence classification and segmentation, in both </context>
</contexts>
<marker>Grunwald, 2007</marker>
<rawString>P. Grunwald. 2007. The Minimum Description Length principle. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Haizhou</author>
<author>Z Min</author>
<author>S Jian</author>
</authors>
<title>A joint sourcechannel model for machine transliteration.</title>
<date>2004</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="8226" citStr="Haizhou et al., 2004" startWordPosition="1386" endWordPosition="1389"> transliteration model and the language model are required to be learned from the same data, and the language modeling probability is integrated into our decoding process, we remove the string e from the language model before processing the example (f, e) during 117 training; we re-incorporate the string e in the language model after the example (f, e) is processed by the averaged perceptron algorithm. We use the discriminatively trained model as the ”standard” system in our experiments. 4 Experiments We use the standard data for transliterating from English into 8 non-Latin scripts: Chinese (Haizhou et al., 2004); Korean, Japanese (Kanji), and Japanese (Katakana) (CJK Institute, 2009); Hindi, Tamil, Kannada, and Russian (Kumaran and Kellner, 2007). The data is provided as part of the Named Entities Workshop 2009 Machine Transliteration Shared Task (Li et al., 2009). For all 8 datasets, we report scores on the standard tests sets provided as part of the evaluation. Details of the evaluation methodology are presented in (Li et al., 2009). 4.1 Preprocessing We perform the same uniform processing of data: names are considered sequences of Unicode characters in their standard decomposed form (NFD). In part</context>
<context position="13062" citStr="Haizhou et al., 2004" startWordPosition="2172" endWordPosition="2175">te-of-the-art phrase translation models eschew succinct representations, and instead employ broad redundant sets of features (Koehn et al., 2003). On the other hand, recent research show that small translation models lead to superior alignment (Bodrumlu et al., 2009). Therefore, investigation of the trade-off between robust redundant and succinct representation present an interesting area for future research. 5 Related Work There is plethora of work on transliteration covering both generative and discriminative models: (Knight and Graehl, 1997; Al-onaizan and Knight, 2002; Huang et al., 2004; Haizhou et al., 2004; Zelenko and Aone, 2006; Sherif and Kondrak, 2007; Goldwasser and Roth, 2008). Application of the minimum description length principle (Grunwald, 2007) in natural language processing has been heretofore mostly limited to morphological analysis (Goldsmith, 2001; Argamon et al., 2004). (Bodrumlu et al., 2009) present a related approach on optimizing the alignment dictionary size in machine translation. 6 Conclusions We introduced a minimum description length approach for training transliteration models that allows to avoid overfitting without putting apriori constraints of the size of n-grams i</context>
</contexts>
<marker>Haizhou, Min, Jian, 2004</marker>
<rawString>L. Haizhou, Z. Min, and S. Jian. 2004. A joint sourcechannel model for machine transliteration. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Huang</author>
<author>S Vogel</author>
</authors>
<title>Improving named entity translation combining phonetic and semantic similarities.</title>
<date>2004</date>
<booktitle>In Proceedings ofHLT/NAACL.</booktitle>
<marker>Huang, Vogel, 2004</marker>
<rawString>F. Huang, S. Vogel,, and A. Waibel. 2004. Improving named entity translation combining phonetic and semantic similarities. In Proceedings ofHLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>J Graehl</author>
</authors>
<date>1997</date>
<booktitle>Machine transliteration. Computational Linguistics,</booktitle>
<pages>128--135</pages>
<contexts>
<context position="12991" citStr="Knight and Graehl, 1997" startWordPosition="2160" endWordPosition="2163"> bestperforming models, especially in noisy environments. For example, state-of-the-art phrase translation models eschew succinct representations, and instead employ broad redundant sets of features (Koehn et al., 2003). On the other hand, recent research show that small translation models lead to superior alignment (Bodrumlu et al., 2009). Therefore, investigation of the trade-off between robust redundant and succinct representation present an interesting area for future research. 5 Related Work There is plethora of work on transliteration covering both generative and discriminative models: (Knight and Graehl, 1997; Al-onaizan and Knight, 2002; Huang et al., 2004; Haizhou et al., 2004; Zelenko and Aone, 2006; Sherif and Kondrak, 2007; Goldwasser and Roth, 2008). Application of the minimum description length principle (Grunwald, 2007) in natural language processing has been heretofore mostly limited to morphological analysis (Goldsmith, 2001; Argamon et al., 2004). (Bodrumlu et al., 2009) present a related approach on optimizing the alignment dictionary size in machine translation. 6 Conclusions We introduced a minimum description length approach for training transliteration models that allows to avoid o</context>
</contexts>
<marker>Knight, Graehl, 1997</marker>
<rawString>K. Knight and J. Graehl. 1997. Machine transliteration. Computational Linguistics, pages 128–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of NLT/NAACL.</booktitle>
<contexts>
<context position="12587" citStr="Koehn et al., 2003" startWordPosition="2100" endWordPosition="2103"> improves performance of the corresponding generative models. In this setting, the MDL training step is effectively used for feature construction: its goal is to automatically hone in on a small set of features whose weights are later learned by discriminative methods. From a broader perspective, it is an open question whether seeking a compact representation of sequential data leads to robust and bestperforming models, especially in noisy environments. For example, state-of-the-art phrase translation models eschew succinct representations, and instead employ broad redundant sets of features (Koehn et al., 2003). On the other hand, recent research show that small translation models lead to superior alignment (Bodrumlu et al., 2009). Therefore, investigation of the trade-off between robust redundant and succinct representation present an interesting area for future research. 5 Related Work There is plethora of work on transliteration covering both generative and discriminative models: (Knight and Graehl, 1997; Al-onaizan and Knight, 2002; Huang et al., 2004; Haizhou et al., 2004; Zelenko and Aone, 2006; Sherif and Kondrak, 2007; Goldwasser and Roth, 2008). Application of the minimum description length</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>P. Koehn, F. Och, and D. Marcu. 2003. Statistical phrase-based translation. In Proceedings of NLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kumaran</author>
<author>T Kellner</author>
</authors>
<title>A generic framework for machine transliteration.</title>
<date>2007</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<contexts>
<context position="8363" citStr="Kumaran and Kellner, 2007" startWordPosition="1404" endWordPosition="1407">s integrated into our decoding process, we remove the string e from the language model before processing the example (f, e) during 117 training; we re-incorporate the string e in the language model after the example (f, e) is processed by the averaged perceptron algorithm. We use the discriminatively trained model as the ”standard” system in our experiments. 4 Experiments We use the standard data for transliterating from English into 8 non-Latin scripts: Chinese (Haizhou et al., 2004); Korean, Japanese (Kanji), and Japanese (Katakana) (CJK Institute, 2009); Hindi, Tamil, Kannada, and Russian (Kumaran and Kellner, 2007). The data is provided as part of the Named Entities Workshop 2009 Machine Transliteration Shared Task (Li et al., 2009). For all 8 datasets, we report scores on the standard tests sets provided as part of the evaluation. Details of the evaluation methodology are presented in (Li et al., 2009). 4.1 Preprocessing We perform the same uniform processing of data: names are considered sequences of Unicode characters in their standard decomposed form (NFD). In particular, Korean Hangul characters are decomposed into Jamo syllabary. Since the evaluation data are provided in the re-composed form, we r</context>
</contexts>
<marker>Kumaran, Kellner, 2007</marker>
<rawString>A. Kumaran and T. Kellner. 2007. A generic framework for machine transliteration. In Proceedings of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
<author>A Kumaran</author>
<author>Min Zhang</author>
<author>V Pervouchine</author>
</authors>
<title>Whitepaper of news 2009 machine transliteration shared task.</title>
<date>2009</date>
<booktitle>In Proceedings of ACLIJCNLP 2009 Named Entities Workshop (NEWS</booktitle>
<contexts>
<context position="8483" citStr="Li et al., 2009" startWordPosition="1424" endWordPosition="1427"> 117 training; we re-incorporate the string e in the language model after the example (f, e) is processed by the averaged perceptron algorithm. We use the discriminatively trained model as the ”standard” system in our experiments. 4 Experiments We use the standard data for transliterating from English into 8 non-Latin scripts: Chinese (Haizhou et al., 2004); Korean, Japanese (Kanji), and Japanese (Katakana) (CJK Institute, 2009); Hindi, Tamil, Kannada, and Russian (Kumaran and Kellner, 2007). The data is provided as part of the Named Entities Workshop 2009 Machine Transliteration Shared Task (Li et al., 2009). For all 8 datasets, we report scores on the standard tests sets provided as part of the evaluation. Details of the evaluation methodology are presented in (Li et al., 2009). 4.1 Preprocessing We perform the same uniform processing of data: names are considered sequences of Unicode characters in their standard decomposed form (NFD). In particular, Korean Hangul characters are decomposed into Jamo syllabary. Since the evaluation data are provided in the re-composed form, we re-compose output of the transliteration system. We split multi-word names (in Hindi, Tamil, and Kannada datasets) in sin</context>
</contexts>
<marker>Li, Kumaran, Zhang, Pervouchine, 2009</marker>
<rawString>Haizhou Li, A. Kumaran, Min Zhang, and V. Pervouchine. 2009. Whitepaper of news 2009 machine transliteration shared task. In Proceedings of ACLIJCNLP 2009 Named Entities Workshop (NEWS 2009).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Sherif</author>
<author>G Kondrak</author>
</authors>
<title>Substring-based transliteration.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="13112" citStr="Sherif and Kondrak, 2007" startWordPosition="2181" endWordPosition="2184"> succinct representations, and instead employ broad redundant sets of features (Koehn et al., 2003). On the other hand, recent research show that small translation models lead to superior alignment (Bodrumlu et al., 2009). Therefore, investigation of the trade-off between robust redundant and succinct representation present an interesting area for future research. 5 Related Work There is plethora of work on transliteration covering both generative and discriminative models: (Knight and Graehl, 1997; Al-onaizan and Knight, 2002; Huang et al., 2004; Haizhou et al., 2004; Zelenko and Aone, 2006; Sherif and Kondrak, 2007; Goldwasser and Roth, 2008). Application of the minimum description length principle (Grunwald, 2007) in natural language processing has been heretofore mostly limited to morphological analysis (Goldsmith, 2001; Argamon et al., 2004). (Bodrumlu et al., 2009) present a related approach on optimizing the alignment dictionary size in machine translation. 6 Conclusions We introduced a minimum description length approach for training transliteration models that allows to avoid overfitting without putting apriori constraints of the size of n-grams in transliteration models. We plan to apply the sam</context>
</contexts>
<marker>Sherif, Kondrak, 2007</marker>
<rawString>T. Sherif and G. Kondrak. 2007. Substring-based transliteration. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Zelenko</author>
<author>C Aone</author>
</authors>
<title>Discriminative methods for transliteration.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="1393" citStr="Zelenko and Aone, 2006" startWordPosition="198" endWordPosition="201">se of machine translation, construct the phrase table after training the model, in an ad-hoc manner. In this paper, we present a minimum description length (MDL) approach (Grunwald, 2007) for learning transliteration models comprising n-grams of unrestricted size. Given a bilingual dictionary of transliterated data we seek to derive a transliteration model so that the combined size of the data and the model is minimized. Use of discriminative modeling for transliteration and translation is another promising direction allowing incorporation of arbitrary features in the transliteration process (Zelenko and Aone, 2006; Goldwasser and Roth, 2008). Here we propose to use the transliteration model derived via MDL training as a starting point and learn the model weights in the discriminative manner. The discriminative approach also provides a natural way to integrate the language modeling component into the transliteration decoding process. We experimentally evaluate the proposed approach on the standard datasets for the task of transliterating from English to 8 non-Latin scripts 2 MDL Training for Transliteration In our transliteration setting, we are given a string e written in an alphabet V1 (e.g., Latin), </context>
<context position="13086" citStr="Zelenko and Aone, 2006" startWordPosition="2176" endWordPosition="2180">ranslation models eschew succinct representations, and instead employ broad redundant sets of features (Koehn et al., 2003). On the other hand, recent research show that small translation models lead to superior alignment (Bodrumlu et al., 2009). Therefore, investigation of the trade-off between robust redundant and succinct representation present an interesting area for future research. 5 Related Work There is plethora of work on transliteration covering both generative and discriminative models: (Knight and Graehl, 1997; Al-onaizan and Knight, 2002; Huang et al., 2004; Haizhou et al., 2004; Zelenko and Aone, 2006; Sherif and Kondrak, 2007; Goldwasser and Roth, 2008). Application of the minimum description length principle (Grunwald, 2007) in natural language processing has been heretofore mostly limited to morphological analysis (Goldsmith, 2001; Argamon et al., 2004). (Bodrumlu et al., 2009) present a related approach on optimizing the alignment dictionary size in machine translation. 6 Conclusions We introduced a minimum description length approach for training transliteration models that allows to avoid overfitting without putting apriori constraints of the size of n-grams in transliteration models</context>
</contexts>
<marker>Zelenko, Aone, 2006</marker>
<rawString>D. Zelenko and C. Aone. 2006. Discriminative methods for transliteration. In Proceedings of EMNLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>