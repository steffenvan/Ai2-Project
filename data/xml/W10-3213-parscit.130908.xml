<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.980835">
Dzongkha Word Segmentation
</title>
<author confidence="0.957205">
Sithar Norbu, Pema Choejey, Tenzin Dendup
</author>
<affiliation confidence="0.904876333333333">
Research Division
Department of Information Technology &amp;
Telecom
</affiliation>
<email confidence="0.963257">
fsnorbu, pchoejay, tdendup}@dit.gov.bt
</email>
<sectionHeader confidence="0.994748" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999991851851852">
Dzongkha, the national language of
Bhutan, is continuous in written form
and it fails to mark the word boundary.
Dzongkha word segmentation is one of
the fundamental problems and a
prerequisite that needs to be solved
before more advanced Dzongkha text
processing and other natural language
processing tools can be developed.
This paper presents our initial attempt
at segmenting Dzongkha sentences into
words. The paper describes the
implementation of Maximal Matching
(Dictionary based Approach) followed
by bigram techniques (Non-dictionary
based Approach) in segmenting the
Dzongkha scripts. Although the used
techniques are basic and naive, it
provides a baseline of the Dzongkha
word segmentation task. Preliminary
experimental results show percentage
of segmentation accuracy. However,
the segmentation accuracy is dependent
on the type of document domain and
size and quality of the lexicon and the
corpus. Some of the related issues for
future directions are also discussed.
</bodyText>
<keyword confidence="0.992589666666667">
Keywords: Dzongkha script, word
segmentation, maximal matching, bigram
technique, smoothing technique.
</keyword>
<sectionHeader confidence="0.999616" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9128915">
Segmentation of a sentence into word is one of
the necessary preprocessing tasks and is
</bodyText>
<author confidence="0.456253">
Sarmad Hussain, Ahmed Mauz
</author>
<affiliation confidence="0.8548145">
Center for Research in Urdu Language Processing
National University of Computer &amp; Emerging
</affiliation>
<address confidence="0.687679">
Sciences
</address>
<email confidence="0.973943">
fsarmad.hussain, ahmed.mauz}@nu.edu.pk
</email>
<bodyText confidence="0.999896368421053">
essential in the analysis of natural language
processing. This is because word is both
syntactically and semantically, the
fundamental unit for analyzing language
structure. Like in any other language
processing task, Dzongkha word segmentation
is also viewed as one of the fundamental and
foremost steps in Dzongkha related language
processing tasks.
The most challenging features of Dzongkha
script is the lack of word boundary separation
between the words1. So, in order to do the
further linguistic and natural language
processing tasks, the scripts should be
transformed into a chain of words. Therefore,
segmenting a word is an essential role in
Natural Language Processing. Like Chinese,
Japanese and Korean (CJK) languages,
Dzongkha script being written continuously
without any word delimiter causes a major
problem in natural language processing tasks.
But, in case of CJK, Thai, and Vietnamese
languages, many solutions have been
published before. For Dzongkha, this is the
first ever word segmentation solution to be
documented.
In this paper, we describe the Dzongkha
word segmentation, which is performed firstly
using the Dictionary based approach where the
principle of maximal matching algorithm is
applied to the input text. Here, given the
collection of lexicon, the maximal matching
algorithm selects the segmentation that yields
the minimum number of words token from all
possible segmentations of the input sentence.
Then, it uses non-dictionary based approach
where bigram technique is applied. The
probabilistic model of a word sequence is
</bodyText>
<footnote confidence="0.993353">
1http://www.learntibetan.net/grammar/sentence.htm
</footnote>
<page confidence="0.982142">
95
</page>
<note confidence="0.876705">
Proceedings of the 8th Workshop on Asian Language Resources, pages 95–102,
Beijing, China, 21-22 August 2010. c�2010 Asian Federation for Natural Language Processing
</note>
<bodyText confidence="0.9997515">
studied using the Maximum Likelihood
Estimation (MLE). The approach using the
MLE has an obvious disadvantage because of
the unavoidably limited size of the training
corpora (Nuges, 2006). To this problem of data
sparseness, the idea of Katz back-off model
with Good-Turing smoothing technique is
applied.
</bodyText>
<sectionHeader confidence="0.967066" genericHeader="method">
2 Dzongkha Script
</sectionHeader>
<bodyText confidence="0.9998521">
Dzongkha language is the official and national
language of Bhutan. It is spoken as the first
language by approximately 130,000 people
and as the second language by about 470,000
people (Van Driem and Tshering, 1998).
Dzongkha is very much related to Sino-
Tibetan language which is a member of
Tibeto-Burmese language family. It is an
alphabetic language, with phonetic
characteristics that mirror those of Sanskrit.
Like many of the alphabets of India and South
East Asia, the Bhutanese script called
Dzongkha script is also a syllabic2. A syllable
can contain as little as one character or as
many as six characters. And a word can be of
one syllable, two syllable or multiple syllables.
In the written form, Dzongkha script contains a
dot, called Tsheg ( ་ ) that serve as syllable and
phrase delimiter, but words are not separated at
all.
</bodyText>
<table confidence="0.9442858">
For example,
Dzongkha Transliteration English Syllables
དམརཔོ dmarpo red Single-
syllabled
སོབ་དཔོན slop-pon Teacher Two-
syllabled
འཇམ་ཏོག་ཏོ hjam-tog-to easy Three-
syllabled
འར་རི་འར་རི har-ri-hur-ri crowdedness Four-
/confusion syllabled
</table>
<tableCaption confidence="0.999927">
Table 1: Different syllabled Dzongkha scripts.
</tableCaption>
<bodyText confidence="0.934804">
The sentence is terminated with a vertical
stroke called Shad ( ། ). This Shad acts as a
full_stop. The frequent appearance of
</bodyText>
<footnote confidence="0.785439">
2http://omniglot.com/writing/tibetan.htm
</footnote>
<bodyText confidence="0.999802285714286">
whitespace in the Dzongkha sentence serves as
a phrase boundary or comma, and is a faithful
representation of speech: after all in speech,
we pause not between words, but either after
certain phrases or at the end of sentence.
The sample dzongkha sentence reads as
follows:
</bodyText>
<equation confidence="0.997746571428571">
རོང་ཁ་གོང་འཕེལ་ལན་ཚོགས་འདི་ འབག་རལ་ཁབ་ནང་ གཞང་གི་ཁ་
ཐག་ལས་ འབག་གི་རལ་ཡོངས་སད་ཡིག་ རོང་ཁའི་སིད་བས་རམ་མི་
དང་ རོང་ཁའི་མཐར་ཐག་གི་དབང་འཛིན་པ་ རང་དབང་རང་སོང་གི་
འདས་ཚོགས་ མཐོ་ཤོས་ཅིག་ཨིན། འདས་ཚོགས་འདི་ འབག་རལ་
བཞི་པ་མི་དབང་མངའ་བདག་རིན་པོ་ཆེ་ དཔལ་འཇིགས་མེད་སེང་གེ་
དབང་ཕག་མཆོག་གི་ཐགས་དགོངས་དང་འཁིལ་ཏེ་ སི་ལོ་ ༡༩༨༦ ལ་
གཞི་བཙགས་གནང་གནངམ་ཨིན།
</equation>
<bodyText confidence="0.99761175">
(English Translation of example text)
[The Dzongkha Development Commission is
the leading institute in the country for the
advancement of Dzongkha, the national
language of Bhutan. It is an independent
organization established by the Fourth King of
Bhutan, His Majesty the King Jigme Singye
Wangchuck, in 1986.]
</bodyText>
<sectionHeader confidence="0.990564" genericHeader="method">
3 Materials and Methods
</sectionHeader>
<bodyText confidence="0.999907590909091">
Since, our language has no word boundary
delimiter, the major resource for Dzongkha
word segmentation is a collection of lexicon
(dictionary). For such languages, dictionaries
are needed to segment the running texts.
Therefore, the coverage of a dictionary plays a
significant role in the accuracy of word
segmentation (Pong and Robert, 1994).
The dictionary that we used contains 23,333
word lists/lexicons. The lexicons were
collected from “Dzongkha Dictionary”, 2nd
Edition, Published by Dzongkha Development
Authority, Ministry of Education, 2005,
(ddc@druknet.bt). The manually segmented
text corpus containing 41,739 tokens are also
used for the method. The text corpora were
collected from different sources like
newspaper articles, dictionaries, printed books,
etc. and belong to domains such as World
Affairs, Social Sciences, Arts, Literatures,
Adventures, Culture and History. Some texts
like poetry and songs were added manually.
</bodyText>
<page confidence="0.946827">
96
</page>
<bodyText confidence="0.9932425">
Table below gives the glimpse of textual
domains contained in the text corpora used for
the method (Chungku et al., 2010).
(Huor et al., 2007) generated by the maximal
matching algorithm. These mechanisms are
described in the following
</bodyText>
<table confidence="0.99984675">
Domain Sub domain (%)
World Affairs Bilateral relations 12%
Social Science Political Science 2%
Arts Poetry/Songs/Ballad 9%
Literatures Essays/Letters/Dictionary 72%
Adventures Travel Adventures 1%
Culture Culture Heritage/Tradition 2%
History Myths/Architecture 2%
</table>
<tableCaption confidence="0.998049">
Table 2: Textual domain contained in Corpus
</tableCaption>
<figureCaption confidence="0.9875295">
Figure 1: Dzongkha Word Segmentation
Process.
</figureCaption>
<bodyText confidence="0.999830230769231">
Dzongkha word segmentation implements a
principle of maximal matching algorithm
followed by statistical (bigram) method. It uses
a word list/lexicon at first to segment the raw
input sentences. It then uses MLE principles to
estimate the bigram probabilities for each
segmented words. All possible segmentation
of an input sentence by Maximal Matching are
then re-ranked and picked the mostly likely
segmentation from the set of possible
segmentations using a statistical approach
(bigram technique). This is to decide the best
possible segmentation among all the words
</bodyText>
<subsectionHeader confidence="0.999664">
3.1 Maximal Matching Algorithm
</subsectionHeader>
<bodyText confidence="0.999502125">
The basic idea of Maximal matching algorithm
is, it first generates all possible segmentations
for an input sentence and then selects the
segmentation that contains the minimum
number of word tokens. It uses dictionary
lookup.
We used the following steps to segment the
given input sentence.
</bodyText>
<listItem confidence="0.986797789473684">
1. Read the input of string text. If an
input line contains more than one
sentence, a sentence separator is
applied to break the line into
individual sentences.
2. Split input string of text by Tsheg( &apos; )
into syllables.
3. Taking the next syllables, generate all
possible strings
4. If the number of string is greater than
n for some value n3
• Look up the series of string in the
dictionary to find matches, and assign
some weight-age4 accordingly.
• Sort the string on the given weight-age
• Delete (number of strings – n) low
count strings.
5. Repeat from Step 2 until all syllables
are processed.
</listItem>
<bodyText confidence="0.899398785714286">
The above mentioned steps produced all
possible segmented words from the given input
sentence based on the provided lexicon. Thus,
the overall accuracy and performance depends
on the coverage of lexicon (Pong and Robert,
1994).
3The greater the value of n, the better the chances of
selecting the sentence with the fewest words from
the possible segmentation.
4If the possible string is found in the dictionary
entries, the number of syllable in the string is
counted. Then, the weight-age for the string is
calculated as (number of syllable)2 else it carries the
weight-age 0
</bodyText>
<figureCaption confidence="0.820482">
Figure 1 below shows the Dzongkha Word
Segmentation Process.
</figureCaption>
<page confidence="0.982419">
97
</page>
<subsectionHeader confidence="0.822197">
3.2 Bigram Method
</subsectionHeader>
<bodyText confidence="0.958572571428571">
(a) Maximum Likelihood Estimation5
In the bigram method, we make the
approximation that the probability of a word
depends on identifying the immediately
preceding word. That is, we calculate the
probability of next word given the previous
word, as follows:
</bodyText>
<equation confidence="0.547262">
Pw1n=Πi=1
n P wi/wi−1
where
count wi−1wi
 Pwi/wi−1= count wi−1 
</equation>
<bodyText confidence="0.970202285714286">
where
 count  wi−1wi  is a total occurrence
of a word sequence wi−1wi in the
corpus, and
 count  wi−1 is a total occurrence of a
word wi−1 in the corpus.
To make P wi/wi−1 meaningful for i=1 ,
we use the distinguished token &lt;s&gt; at the
beginning of the sentence; that is, we pretend
w0 = &lt;s&gt;. In addition, to make the sum of the
probabilities of all strings equal 1, it is
necessary to place a distinguished token &lt;/s&gt;
at the end of the sentence.
One of the key problems with the MLE is
insufficient data. That is, because of the
unavoidably limited size of the training corpus,
vast majority of the word are uncommon and
some of the bigrams may not occur at all in the
corpus, leading to zero probabilities.
Therefore, following smoothing techniques
were used to count the probabilities of unseen
bigram.
probability distribution implied by a language
model so that all reasonable word sequences
can occur with some probability. This often
involves adjusting zero probabilities upward
and high probabilities downwards. This way,
smoothing technique not only helps prevent
zero probabilities but the overall accuracy of
the model are also significantly improved
(Chen and Goodman, 1998).
In Dzongkha word segmentation, Katz back-
off model based on Good-Turing smoothing
principle is applied to handle the issue of data
sparseness. The basic idea of Katz back-off
model is to use the frequency of n-grams and if
no n-grams are available, to back off to (n-1)
grams, and then to (n-2) grams and so on
(Chen and Goodman, 1998).
The summarized procedure of Katz
smoothing technique is given by the following
algorithm:6
</bodyText>
<equation confidence="0.970212666666667">
C wi−1/wi ifr&gt;k
Pkatz wi∣wi−1 ={ drC wi−1/wi ifk≥r&gt;0
α wi−1 P wi ifr=0
</equation>
<bodyText confidence="0.971584">
where
 r is the frequency of bigram counts
 k is taken for some value in the
range of 5 to 10, other counts are
not re-estimated.
</bodyText>
<equation confidence="0.943818444444444">
−k+1 nK+1
n1
k+1nk+1
1−
n1
r
r
 dr =

</equation>
<subsectionHeader confidence="0.833801">
(b) Smoothing Bigram Probabilistic
</subsectionHeader>
<bodyText confidence="0.999889666666667">
The above problem of data sparseness
underestimates the probability of some of the
sentences that are in the test set. The
smoothing technique helps to prevent errors by
making the probabilities more uniform.
Smoothing is the process of flattening a
</bodyText>
<figureCaption confidence="0.5608254">
5P.M, Nugues. An Introduction to Language
Processing with Perl and Prolog: An Outline of
Theories, Implementation, and Application with
Special Consideration of English, French, and
German (Cognitive Technologies) (95 – 104).
</figureCaption>
<equation confidence="0.98490625">
1− ∑ PKatz wi∣wi−1
wi:r&gt;0
1− ∑
wi:r&gt; 0 PKatz wi
</equation>
<bodyText confidence="0.997993">
With the above equations, bigrams with non-
zero count r are discounted according to the
</bodyText>
<footnote confidence="0.96241475">
6X. Huang, A. Acero, H.-W.Hon, Spoken Language
Processing: A Guide to Theory, Algorithm and
System Development, (Prentice-Hall Inc., New
Jersey 07458, 2001), 559 - 561.
</footnote>
<equation confidence="0.751989166666667">
α wi−1 =
98
ཐོ
r�
discount ratio dr=
r
</equation>
<bodyText confidence="0.99985125">
subtracted from the non-zero count are
redistributed among the zero count bigrams
according to the next lower-order distribution,
the unigram model.
</bodyText>
<sectionHeader confidence="0.990799" genericHeader="method">
4 Evaluations and Results
</sectionHeader>
<bodyText confidence="0.991210714285714">
Subjective evaluation has been performed by
comparing the experimental results with the
manually segmented tokens. The method was
evaluated using different sets of test
documents from various domains consisting of
714 manually segmented words. Table 3
summarizes the evaluation results.
</bodyText>
<table confidence="0.998525846153846">
Document text Correct Detect Accuracy
(Correctly segmented
tokens / total no. of
words)
Astrology.txt 102/116 87.9%
dzo_linux.txt 85/93 91.4%
movie_awards.txt 76/84 90.5%
News.txt 78/85 91.8%
Notice.txt 83/92 90.2%
Religious.txt 63/73 89.0%
Song.txt 57/60 95.0%
Tradition.txt 109/111 98.2%
Total 653/714 91.5%
</table>
<bodyText confidence="0.933157833333333">
manually segmented corpus containing 41,739
tokens are used for the method.
In the sample comparison below, the symbol
( ་ ) does not make the segmentation unit&apos;s
mark, but ( ། ) takes the segmentation unit&apos;s
mark, despite its actual mark for comma or
full_stop. The whitespace in the sentence are
phrase boundary or comma, and is a faithful
representation of speech where we pause not
between words, but either after certain phrases
or at the end of sentence.
Consider the sample input sentence:
</bodyText>
<equation confidence="0.96623175">
་ནགསི་འདི་ རོང་ཁ་གོག་རིག་ནང་བཙགས་ནིའི་དོན་ལ་ རབ་
སོར་ཧིལ་བ་གཅིག་ཁར་བསོམ་མི་ རང་དབང་ལི་ནགསི་ བཀོལ་སོད་
རིམ་ལགས་འདི་གི་ ཉེ་གནས་སེ་མཐན་འགར་བཟོ་ཡོད་པའི་ཐོན་རིམ་
ཅིག་ཨིན། དེ་གིས་ ཆ་ཚང་སེ་སད་བསར་འབད་ཡོད་པའི་ལག་ལེན་པའི་
ངོས་འད་བ་ཚ་སོནམ་ཨིན།
Manually segmented sentence of the sample
input sentence:
རོང་ཁ།ལི་ནགསི།འདི ། རོང་ཁ།གོག་རིག།ནང།བཙགས།ནིའི།དོན་ལ།
ག།ཁར།བསོམ།མི། རང་དབང།ལི་ནགསི། བཀོལ་
སོད།རིམ་ལགས།འདི།གི། ཉེ་གནས།སེ།མཐན་འགར།བཟོ།ཡོད།པའི།
ན།རིམ།ཅིག།ཨིན། དེ་གིས། ཆ་ཚང།སེ།སད་བསར།འབད།ཡོད།པའི།
ལག་ལེན།པའི།ངོས།འད་བ།ཚ།སོནམ།ཨིན།
Using maximal matching algorithm:
i.e., the count
རོང་ཁ་ལི
རབ་སོར། ཧིལ་བ།གཅི
Table 3: Evaluation Results
རོང་ཁ། ལི། ནགསི། འདི། རོང་ཁ། གོག་རིག། ནང། བཙགས།
ནིའི། དོན། ལ། རབ་སོར། ཧིལ། བ། གཅིག། ཁར། བསོམ།
མི། རང་དབང། ལི། ནགསི། བཀ
Accuracy in %age are measured as:
ོལ་སོ
ད། རིམ་ལགས། འདི་གི།
Accuracy(%) = NT ∗100
</equation>
<bodyText confidence="0.989107">
where
</bodyText>
<listItem confidence="0.994164">
• N is the number of correctly
segmented tokens
• T is the total number of manually
segmented tokens/ Total number of
words.
</listItem>
<bodyText confidence="0.970682">
We have taken the extract of different test data
hoping it may contain fair amount of general
terms, technical terms and common nouns. The
</bodyText>
<equation confidence="0.994071666666667">
ཉེ་གནས། སེ། མཐན་འགར། བཟོ། ཡོད། པའི། ཐོན། རིམ།
ཅིག་ཨིན། དེ། གིས། ཆ་ཚང། སེ། སད་བསར། འབད། ཡོད།
པའི། ལག་ལེན། པའི། ངོས། འད། བ། ཚ། སོནམ། ཨིན།
</equation>
<bodyText confidence="0.993100333333333">
System segmented version of the sample input
sentence: Underlined text shows the incorrect
segmentation.
</bodyText>
<equation confidence="0.975018">
རོང་ཁ།ལི་ནགསི་འདི ། རོང་ཁ།གོག་རིག།ནང།བཙགས།ནིའི་དོན་ལ།
ག།ཁར།བསོམ།མི། རང་དབང།ལི་ནགསི།བཀོལ་
སོད།རིམ་ལགས།འདི།གི། ཉེ་གནས།སེ།མཐན་འགར།བཟོ།ཡོད།པའི།
རབ་སོར། ཧིལ་བ།གཅི
</equation>
<page confidence="0.923595">
99
</page>
<figure confidence="0.761461666666667">
ཐོན།རིམ།ཅིག།ཨིན། དེ།གིས།ཆ་ཚང།སེ།སད་བསར།འབད།ཡོད།པའི། ི །རོང་ཁ།གི།ཞིབ།འཚོལ།ཡིག་ཆ།ཨིན
2.འ
ལག་ལེན།པའི།ངོས།འད་བ།ཚ།སོནམ། ཨིན།
</figure>
<sectionHeader confidence="0.801454" genericHeader="evaluation">
5 Discussions
</sectionHeader>
<bodyText confidence="0.995942571428571">
During the process of word segmentation, it is
understood that the maximal matching
algorithm is simply effective and can produce
accurate segmentation only if all the words are
present in the lexicon. But since not all the
word entry can be found in lexicon database in
real application, the performance of word
segmentation degrades when it encounters
words that are not in the lexicon (Chiang et al.,
1992).
Following are the significant problems with
the dictionary-based maximal matching
method because of the coverage of lexicon
(Emerson, 2000):
</bodyText>
<listItem confidence="0.9998325">
• incomplete and inconsistency of the
lexicon database
• absence of technical domains in the
lexicon
• transliterated foreign names
• some of the common nouns not
included in the lexicon
• lexicon/word lists do not contains
</listItem>
<bodyText confidence="0.891469823529412">
genitive endings པའི (expresses the
genitive relationship as a quality or
characteristic of the second element,
for example, དབལ་པའི་བ &apos;son of a
pauper&apos;) and འི (first singular
possessive, for example, ངེའི
which actually is ངི་གི་བམོ &apos;my
daughter&apos;) that indicates possession
or a part-to-whole relationship, like
English &apos;of&apos;.
A Dzongkha sentence like:
འདི་རོང་ཁ་གི་ ཞིབ་འཚོལ་ཡིག་ཆ་ ཨིན།
may include the following ambiguous possible
segmentation based on simple dictionary
lookup:
this  |Dzongkha  |of  |arrange together  |search/
expose  |written document  |is
</bodyText>
<equation confidence="0.763096">
།ཞིབ་འཚོལ།ཡིག་ཆ།ཨིན
this  |fortress  |mouth/surface  |of  |research |
written document  |is
</equation>
<bodyText confidence="0.999934444444445">
These problems of ambiguous word
divisions, unknown proper names, are lessened
and solved partially when it is re-ranked using
the bigram techniques. Still the solution to the
following issues needs to be discussed in the
future. Although the texts were collected from
widest range of domains possible, the lack of
available electronic resources of informative
text adds to the following issues:
</bodyText>
<listItem confidence="0.9511575">
• small number of corpus were not
very impressive for the method
• ambiguity and inconsistent of
manual segmentation of a token in
</listItem>
<bodyText confidence="0.985593818181818">
the corpus resulting in
incompatibility and sometimes in
conflict.
Ambiguity and inconsistency occurs
because of difficulties in identifying a word.
Since the manual segmentation of corpus entry
was carried out by humans rather than
computer, such humans have to be well skilled
in identifying or understanding what a word is.
The problem with the Dzongkha scripts that
also hampers the accuracy of dzongkha word
segmentation includes the issues such as
ambiguous use of Tsheg ( ་ ) in different
documents. There are two different types of
Tsheg: Unicode 0F0B ( ་ ) called Tibetan mark
inter syllabic tsheg is a normal tsheg that
provides a break opportunity. Unicode 0F0C
( ༌ ) called Tibetan Mark Delimiter Tsheg Bstar
is a non-breaking tsheg and it inhibits line
breaking.
For example,
input sentence with Tsheg 0F0B:
</bodyText>
<figure confidence="0.98348225">
སངས་རས་དང་ཚེ་རིང་གཉིས་ བར་དོན་དང་འཕལ་རིག་ནང་ ལ་འབད་
དོ་ཡོདཔ་ཨིན་པས།
ོ
་བམ
3.འདི
།རོང།ཁ།གི
ི །རོང་ཁ།གི །ཞིབ་འཚོལ།ཡིག་ཆ།ཨིན achieves 100% segmentation as follow:
1.འ
</figure>
<footnote confidence="0.3648555">
this  |Dzongkha  |of  |research  |written
document  |is
</footnote>
<page confidence="0.502119">
100
</page>
<equation confidence="0.5922925">
སངས་རས། དང། ཚེ་རིང། གཉིས། བར་དོན། དང། འཕལ། རིག། ནང།
ལ། འབད། དོ། ཡོདཔ། ཨིན། པས།
</equation>
<bodyText confidence="0.873244375">
whereas the same input sentence with Tsheg
0F0C is incorrectly segmented as follows:
སངས༌རས༌དང༌ཚེ༌རིང༌གཉིས༌། བར༌དོན༌དང༌འཕལ༌རིག༌ནང༌།
ལ༌འབད༌དོ༌ཡོདཔ༌ཨིན༌པས།
There are also cases like shortening of words,
removing of inflectional words and
abbreviating of words for the convenience of
the writer. But this is not so reflected in the
dictionaries, thus affecting the accuracy of the
segmentation.
Following words has a special abbreviated way
of writing a letter or sequence of letters at the
end of a syllable as
རོ་རེ as རོེ
ཡེ་ཤེས as ཡེས
etc..
</bodyText>
<sectionHeader confidence="0.994034" genericHeader="conclusions">
6 Conclusion and Future works
</sectionHeader>
<bodyText confidence="0.99999084">
This paper describes the initial effort in
segmenting the Dzongkha scripts. In this
preliminary analysis of Dzongkha word
segmentation, the preprocessing and
normalizations are not dealt with. Numberings,
special symbols and characters are also not
included. These issues will have to be studied
in the future. A lot of discussions and works
also have to be done to improve the
performance of word segmentation. Although
the study was a success, there are still some
obvious limitations, such as its dependency on
dictionaries/lexicon, and the current Dzongkha
lexicon is not comprehensive. Also, there is
absence of large corpus collection from
various domains. Future work may include
overall improvement of the method for better
efficiency, effectiveness and functionality, by
exploring different algorithms. Furthermore,
the inclusion of POS Tag sets applied on n-
gram techniques which is proven to be helpful
in handling the unknown word problems might
enhance the performance and accuracy.
Increasing corpus size might also help to
improve the results.
</bodyText>
<sectionHeader confidence="0.908323" genericHeader="acknowledgments">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.996386888888889">
This research work was carried out as a part of
PAN Localization Project
(http://www.PANL10n.net) with the aid of a
grant from the International Development
Research Centre (IDRC), Ottawa, Canada,
administered through the Center of Research in
Urdu Language Processing (CRULP), National
University of Computer and Emerging
Sciences (NUCES), Pakistan. The research
team would also like to express the gratitude to
all the PAN Localization Project members of
Bhutanese team based at Department of
Information Technology and Telecom, for
their efforts in collecting, preparing and
providing with the lexicon, corpus, useful
training and testing materials and finally for
the their valuable support and contribution that
made this research successful.
</bodyText>
<sectionHeader confidence="0.99921" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999754892857143">
Chen, Stanley F., Joshua Goodman, 1998. An
Empirical Study of Smoothing Techniques for
Language Modeling, Computer Science Group,
Harvard University, Cambridge, Massachusetts
Chiang, T-Hui., J-Shin Chang,, M-Yu Lin, K-Yih
Su, 2007. Statistical models for word
segmentation and unknown word resolution.
Department of Electrical Engineering , National
Tsing Hua University, Hsinchu, Taiwan.
Chungku., Jurmey Rabgay, Gertrud Faaβ, 2010.
NLP Resources for Dzongkha. Department of
Information Technology &amp; Telecom, Ministry of
Information &amp; Communications, Thimphu,
Bhutan.
Durrani, Nadir and Sarmad Hussain, 2010. Urdu
Word Segmentation. Human Language
Technologies: 11th Annual Conference of the
North American Chapter of the Association for
Computational Linguistics, Los Angeles, June
2010.
Emerson, Thomas. 2000. Segmenting Chinese in
Unicode. 16th International Unicode conference,
Amsterdam, The Netherlands, March 2000
Haizhou, Li and Yuan Baosheng, 1998. Chinese
Word Segmentation. Language, Information and
Computation (PACLIC12), 1998.
Haruechaiyasak, C., S Kongyoung, M.N. Dailey,
2008. A Comparative Study on Thai Word
</reference>
<page confidence="0.975751">
101
</page>
<reference confidence="0.999745083333333">
Segmentation Approaches. In Proceedings of
ECTI-CON, 2008.
Huang, X., A. Acero, H.-W. Hon, 2001. Spoken
Language Processing: A Guide to Theory,
Algorithm and System Development (pp. 539 –
578). Prentice-Hall Inc., New Jersey 07458.
Huor, C.S., T. Rithy, R.P. Hemy, V. Navy, C.
Chanthirith, C. Tola, 2007. Word Bigram Vs
Orthographic Syllable Bigram in Khmer Word
Segmentation. PAN Localization Working
Papers 2004 - 2007. PAN Localization Project,
National University of Computer and Emerging
Sciences, Lahore, Pakistan.
Jurafsky, D., A. Acero, H.-W. Hon, 1999. Speech
and Language Processing: An Introduction to
Natural Language Processing, Computational
Linguistics and Speech Recognition (pp. 189 –
230). Prentice-Hall Inc., New Jersey 07458.
Nugues, P.M. 2006. An Introduction to Language
Processing with Perl and Prolog: An Outline of
Theories, Implementation, and Application with
Special Consideration of English, French, and
German (Cognitive Technologies) (pp. 87 –
104). Springer-Verlag Berlin Heidelberg
Pong, L.W. and Robert. 1994. Chinese word
segmentation based on maximal matching and
bigram techniques. Retrieved from The
Association for Computational Linguistics and
Chinese Language Processing. On-line:
http://www.aclclp.org.tw/rocling/1994/P04.pdf
Sunthi, Thepchai. 2007. Word Segmentation and
POS tagging. ADD-2 Workshop, SIIT,
NECTEC, Thailand.
Van Driem, George. and Karma Tshering, (Collab),
“Languages of Greater Himalayan Region”,
1998.
</reference>
<page confidence="0.998604">
102
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.454592">
<title confidence="0.999017">Dzongkha Word Segmentation</title>
<author confidence="0.933979">Sithar Norbu</author>
<author confidence="0.933979">Pema Choejey</author>
<author confidence="0.933979">Tenzin</author>
<affiliation confidence="0.8471015">Research Department of Information Technology</affiliation>
<email confidence="0.8734355">Telecomfsnorbu,pchoejay,tdendup}@dit.gov.bt</email>
<abstract confidence="0.992210935483871">Dzongkha, the national language of Bhutan, is continuous in written form and it fails to mark the word boundary. Dzongkha word segmentation is one of the fundamental problems and a prerequisite that needs to be solved before more advanced Dzongkha text processing and other natural language processing tools can be developed. This paper presents our initial attempt at segmenting Dzongkha sentences into words. The paper describes the implementation of Maximal Matching (Dictionary based Approach) followed by bigram techniques (Non-dictionary based Approach) in segmenting the Dzongkha scripts. Although the used techniques are basic and naive, it provides a baseline of the Dzongkha word segmentation task. Preliminary experimental results show percentage of segmentation accuracy. However, the segmentation accuracy is dependent on the type of document domain and size and quality of the lexicon and the corpus. Some of the related issues for future directions are also discussed. script, word segmentation, maximal matching, bigram technique, smoothing technique.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An Empirical Study of Smoothing Techniques</title>
<date>1998</date>
<institution>for Language Modeling, Computer Science Group, Harvard University,</institution>
<location>Cambridge, Massachusetts</location>
<contexts>
<context position="11095" citStr="Chen and Goodman, 1998" startWordPosition="1659" endWordPosition="1662">raining corpus, vast majority of the word are uncommon and some of the bigrams may not occur at all in the corpus, leading to zero probabilities. Therefore, following smoothing techniques were used to count the probabilities of unseen bigram. probability distribution implied by a language model so that all reasonable word sequences can occur with some probability. This often involves adjusting zero probabilities upward and high probabilities downwards. This way, smoothing technique not only helps prevent zero probabilities but the overall accuracy of the model are also significantly improved (Chen and Goodman, 1998). In Dzongkha word segmentation, Katz backoff model based on Good-Turing smoothing principle is applied to handle the issue of data sparseness. The basic idea of Katz back-off model is to use the frequency of n-grams and if no n-grams are available, to back off to (n-1) grams, and then to (n-2) grams and so on (Chen and Goodman, 1998). The summarized procedure of Katz smoothing technique is given by the following algorithm:6 C wi−1/wi ifr&gt;k Pkatz wi∣wi−1 ={ drC wi−1/wi ifk≥r&gt;0 α wi−1 P wi ifr=0 where  r is the frequency of bigram counts  k is taken for some value in the range of 5 </context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Chen, Stanley F., Joshua Goodman, 1998. An Empirical Study of Smoothing Techniques for Language Modeling, Computer Science Group, Harvard University, Cambridge, Massachusetts</rawString>
</citation>
<citation valid="true">
<authors>
<author>T-Hui Chiang</author>
<author>J-Shin Chang</author>
</authors>
<title>Statistical models for word segmentation and unknown word resolution.</title>
<date>2007</date>
<institution>Department of Electrical Engineering , National Tsing Hua University,</institution>
<location>Hsinchu, Taiwan.</location>
<marker>Chiang, Chang, 2007</marker>
<rawString>Chiang, T-Hui., J-Shin Chang,, M-Yu Lin, K-Yih Su, 2007. Statistical models for word segmentation and unknown word resolution. Department of Electrical Engineering , National Tsing Hua University, Hsinchu, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jurmey Rabgay Chungku</author>
<author>Gertrud Faaβ</author>
</authors>
<title>NLP Resources for Dzongkha.</title>
<date>2010</date>
<journal>Department of Information Technology &amp; Telecom, Ministry of Information &amp; Communications,</journal>
<location>Thimphu, Bhutan.</location>
<marker>Chungku, Faaβ, 2010</marker>
<rawString>Chungku., Jurmey Rabgay, Gertrud Faaβ, 2010. NLP Resources for Dzongkha. Department of Information Technology &amp; Telecom, Ministry of Information &amp; Communications, Thimphu, Bhutan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadir Durrani</author>
<author>Sarmad Hussain</author>
</authors>
<title>Urdu Word Segmentation. Human Language Technologies:</title>
<date>2010</date>
<booktitle>11th Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<location>Los Angeles,</location>
<marker>Durrani, Hussain, 2010</marker>
<rawString>Durrani, Nadir and Sarmad Hussain, 2010. Urdu Word Segmentation. Human Language Technologies: 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics, Los Angeles, June 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Emerson</author>
</authors>
<title>Segmenting Chinese in Unicode.</title>
<date>2000</date>
<booktitle>16th International Unicode conference,</booktitle>
<location>Amsterdam, The Netherlands,</location>
<contexts>
<context position="16089" citStr="Emerson, 2000" startWordPosition="2388" endWordPosition="2389">ོལ།ཡིག་ཆ།ཨིན 2.འ ལག་ལེན།པའི།ངོས།འད་བ།ཚ།སོནམ། ཨིན། 5 Discussions During the process of word segmentation, it is understood that the maximal matching algorithm is simply effective and can produce accurate segmentation only if all the words are present in the lexicon. But since not all the word entry can be found in lexicon database in real application, the performance of word segmentation degrades when it encounters words that are not in the lexicon (Chiang et al., 1992). Following are the significant problems with the dictionary-based maximal matching method because of the coverage of lexicon (Emerson, 2000): • incomplete and inconsistency of the lexicon database • absence of technical domains in the lexicon • transliterated foreign names • some of the common nouns not included in the lexicon • lexicon/word lists do not contains genitive endings པའི (expresses the genitive relationship as a quality or characteristic of the second element, for example, དབལ་པའི་བ &apos;son of a pauper&apos;) and འི (first singular possessive, for example, ངེའི which actually is ངི་གི་བམོ &apos;my daughter&apos;) that indicates possession or a part-to-whole relationship, like English &apos;of&apos;. A Dzongkha sentence like: འདི་རོང་ཁ་གི་ ཞིབ་འཚ</context>
</contexts>
<marker>Emerson, 2000</marker>
<rawString>Emerson, Thomas. 2000. Segmenting Chinese in Unicode. 16th International Unicode conference, Amsterdam, The Netherlands, March 2000</rawString>
</citation>
<citation valid="true">
<authors>
<author>Li Haizhou</author>
<author>Yuan Baosheng</author>
</authors>
<date>1998</date>
<booktitle>Chinese Word Segmentation. Language, Information and Computation (PACLIC12),</booktitle>
<marker>Haizhou, Baosheng, 1998</marker>
<rawString>Haizhou, Li and Yuan Baosheng, 1998. Chinese Word Segmentation. Language, Information and Computation (PACLIC12), 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Haruechaiyasak</author>
<author>S Kongyoung</author>
<author>M N Dailey</author>
</authors>
<title>A Comparative Study on Thai Word Segmentation Approaches.</title>
<date>2008</date>
<booktitle>In Proceedings of ECTI-CON,</booktitle>
<marker>Haruechaiyasak, Kongyoung, Dailey, 2008</marker>
<rawString>Haruechaiyasak, C., S Kongyoung, M.N. Dailey, 2008. A Comparative Study on Thai Word Segmentation Approaches. In Proceedings of ECTI-CON, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Huang</author>
<author>A Acero</author>
<author>H-W Hon</author>
</authors>
<date>2001</date>
<booktitle>Spoken Language Processing: A Guide to Theory, Algorithm and System Development (pp. 539 – 578). Prentice-Hall Inc.,</booktitle>
<location>New Jersey</location>
<marker>Huang, Acero, Hon, 2001</marker>
<rawString>Huang, X., A. Acero, H.-W. Hon, 2001. Spoken Language Processing: A Guide to Theory, Algorithm and System Development (pp. 539 – 578). Prentice-Hall Inc., New Jersey 07458.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C S Huor</author>
<author>T Rithy</author>
<author>R P Hemy</author>
<author>V Navy</author>
<author>C Chanthirith</author>
<author>C Tola</author>
</authors>
<title>Word Bigram Vs Orthographic Syllable Bigram in Khmer Word Segmentation. PAN Localization Working Papers</title>
<date>2007</date>
<institution>PAN Localization Project, National University of Computer and Emerging Sciences,</institution>
<location>Lahore, Pakistan.</location>
<contexts>
<context position="6966" citStr="Huor et al., 2007" startWordPosition="995" endWordPosition="998">Edition, Published by Dzongkha Development Authority, Ministry of Education, 2005, (ddc@druknet.bt). The manually segmented text corpus containing 41,739 tokens are also used for the method. The text corpora were collected from different sources like newspaper articles, dictionaries, printed books, etc. and belong to domains such as World Affairs, Social Sciences, Arts, Literatures, Adventures, Culture and History. Some texts like poetry and songs were added manually. 96 Table below gives the glimpse of textual domains contained in the text corpora used for the method (Chungku et al., 2010). (Huor et al., 2007) generated by the maximal matching algorithm. These mechanisms are described in the following Domain Sub domain (%) World Affairs Bilateral relations 12% Social Science Political Science 2% Arts Poetry/Songs/Ballad 9% Literatures Essays/Letters/Dictionary 72% Adventures Travel Adventures 1% Culture Culture Heritage/Tradition 2% History Myths/Architecture 2% Table 2: Textual domain contained in Corpus Figure 1: Dzongkha Word Segmentation Process. Dzongkha word segmentation implements a principle of maximal matching algorithm followed by statistical (bigram) method. It uses a word list/lexicon a</context>
</contexts>
<marker>Huor, Rithy, Hemy, Navy, Chanthirith, Tola, 2007</marker>
<rawString>Huor, C.S., T. Rithy, R.P. Hemy, V. Navy, C. Chanthirith, C. Tola, 2007. Word Bigram Vs Orthographic Syllable Bigram in Khmer Word Segmentation. PAN Localization Working Papers 2004 - 2007. PAN Localization Project, National University of Computer and Emerging Sciences, Lahore, Pakistan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurafsky</author>
<author>A Acero</author>
<author>H-W Hon</author>
</authors>
<title>Speech and Language Processing: An Introduction to Natural Language</title>
<date>1999</date>
<booktitle>Processing, Computational Linguistics and Speech Recognition (pp. 189 – 230). Prentice-Hall Inc.,</booktitle>
<location>New Jersey</location>
<marker>Jurafsky, Acero, Hon, 1999</marker>
<rawString>Jurafsky, D., A. Acero, H.-W. Hon, 1999. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics and Speech Recognition (pp. 189 – 230). Prentice-Hall Inc., New Jersey 07458.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P M Nugues</author>
</authors>
<title>An Introduction to Language Processing with Perl and Prolog: An Outline of Theories, Implementation,</title>
<date>2006</date>
<journal>and Application with Special Consideration of English, French, and German (Cognitive Technologies) (pp. 87 –</journal>
<volume>104</volume>
<publisher>Springer-Verlag</publisher>
<location>Berlin Heidelberg</location>
<marker>Nugues, 2006</marker>
<rawString>Nugues, P.M. 2006. An Introduction to Language Processing with Perl and Prolog: An Outline of Theories, Implementation, and Application with Special Consideration of English, French, and German (Cognitive Technologies) (pp. 87 – 104). Springer-Verlag Berlin Heidelberg</rawString>
</citation>
<citation valid="true">
<authors>
<author>L W Pong</author>
<author>Robert</author>
</authors>
<title>Chinese word segmentation based on maximal matching and bigram techniques.</title>
<date>1994</date>
<booktitle>Retrieved from The Association for Computational Linguistics and Chinese Language Processing. On-line: http://www.aclclp.org.tw/rocling/1994/P04.pdf</booktitle>
<contexts>
<context position="6221" citStr="Pong and Robert, 1994" startWordPosition="887" endWordPosition="890">lopment Commission is the leading institute in the country for the advancement of Dzongkha, the national language of Bhutan. It is an independent organization established by the Fourth King of Bhutan, His Majesty the King Jigme Singye Wangchuck, in 1986.] 3 Materials and Methods Since, our language has no word boundary delimiter, the major resource for Dzongkha word segmentation is a collection of lexicon (dictionary). For such languages, dictionaries are needed to segment the running texts. Therefore, the coverage of a dictionary plays a significant role in the accuracy of word segmentation (Pong and Robert, 1994). The dictionary that we used contains 23,333 word lists/lexicons. The lexicons were collected from “Dzongkha Dictionary”, 2nd Edition, Published by Dzongkha Development Authority, Ministry of Education, 2005, (ddc@druknet.bt). The manually segmented text corpus containing 41,739 tokens are also used for the method. The text corpora were collected from different sources like newspaper articles, dictionaries, printed books, etc. and belong to domains such as World Affairs, Social Sciences, Arts, Literatures, Adventures, Culture and History. Some texts like poetry and songs were added manually. </context>
<context position="9137" citStr="Pong and Robert, 1994" startWordPosition="1331" endWordPosition="1334">nto syllables. 3. Taking the next syllables, generate all possible strings 4. If the number of string is greater than n for some value n3 • Look up the series of string in the dictionary to find matches, and assign some weight-age4 accordingly. • Sort the string on the given weight-age • Delete (number of strings – n) low count strings. 5. Repeat from Step 2 until all syllables are processed. The above mentioned steps produced all possible segmented words from the given input sentence based on the provided lexicon. Thus, the overall accuracy and performance depends on the coverage of lexicon (Pong and Robert, 1994). 3The greater the value of n, the better the chances of selecting the sentence with the fewest words from the possible segmentation. 4If the possible string is found in the dictionary entries, the number of syllable in the string is counted. Then, the weight-age for the string is calculated as (number of syllable)2 else it carries the weight-age 0 Figure 1 below shows the Dzongkha Word Segmentation Process. 97 3.2 Bigram Method (a) Maximum Likelihood Estimation5 In the bigram method, we make the approximation that the probability of a word depends on identifying the immediately preceding word</context>
</contexts>
<marker>Pong, Robert, 1994</marker>
<rawString>Pong, L.W. and Robert. 1994. Chinese word segmentation based on maximal matching and bigram techniques. Retrieved from The Association for Computational Linguistics and Chinese Language Processing. On-line: http://www.aclclp.org.tw/rocling/1994/P04.pdf</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thepchai Sunthi</author>
</authors>
<date>2007</date>
<booktitle>Word Segmentation and POS tagging. ADD-2 Workshop, SIIT, NECTEC,</booktitle>
<marker>Sunthi, 2007</marker>
<rawString>Sunthi, Thepchai. 2007. Word Segmentation and POS tagging. ADD-2 Workshop, SIIT, NECTEC, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karma Tshering</author>
</authors>
<title>(Collab), “Languages of Greater Himalayan Region”,</title>
<date>1998</date>
<contexts>
<context position="3860" citStr="Tshering, 1998" startWordPosition="556" endWordPosition="557"> 21-22 August 2010. c�2010 Asian Federation for Natural Language Processing studied using the Maximum Likelihood Estimation (MLE). The approach using the MLE has an obvious disadvantage because of the unavoidably limited size of the training corpora (Nuges, 2006). To this problem of data sparseness, the idea of Katz back-off model with Good-Turing smoothing technique is applied. 2 Dzongkha Script Dzongkha language is the official and national language of Bhutan. It is spoken as the first language by approximately 130,000 people and as the second language by about 470,000 people (Van Driem and Tshering, 1998). Dzongkha is very much related to SinoTibetan language which is a member of Tibeto-Burmese language family. It is an alphabetic language, with phonetic characteristics that mirror those of Sanskrit. Like many of the alphabets of India and South East Asia, the Bhutanese script called Dzongkha script is also a syllabic2. A syllable can contain as little as one character or as many as six characters. And a word can be of one syllable, two syllable or multiple syllables. In the written form, Dzongkha script contains a dot, called Tsheg ( ་ ) that serve as syllable and phrase delimiter, but words </context>
</contexts>
<marker>Tshering, 1998</marker>
<rawString>Van Driem, George. and Karma Tshering, (Collab), “Languages of Greater Himalayan Region”, 1998.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>