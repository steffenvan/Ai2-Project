<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.018404">
<title confidence="0.9683235">
DepecheMood:
a Lexicon for Emotion Analysis from Crowd-Annotated News
</title>
<author confidence="0.999151">
Jacopo Staiano Marco Guerini
</author>
<affiliation confidence="0.772543">
University of Trento Trento RISE
Trento - Italy Trento - Italy
</affiliation>
<email confidence="0.995819">
staiano@disi.unitn.it marco.guerini@trentorise.eu
</email>
<sectionHeader confidence="0.993776" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99987125">
While many lexica annotated with words
polarity are available for sentiment anal-
ysis, very few tackle the harder task of
emotion analysis and are usually quite
limited in coverage. In this paper, we
present a novel approach for extracting
– in a totally automated way – a high-
coverage and high-precision lexicon of
roughly 37 thousand terms annotated with
emotion scores, called DepecheMood.
Our approach exploits in an original way
‘crowd-sourced’ affective annotation im-
plicitly provided by readers of news ar-
ticles from rappler.com. By provid-
ing new state-of-the-art performances in
unsupervised settings for regression and
classification tasks, even using a naive ap-
proach, our experiments show the benefi-
cial impact of harvesting social media data
for affective lexicon building.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999502206896552">
Sentiment analysis has proved useful in several ap-
plication scenarios, for instance in buzz monitor-
ing – the marketing technique for keeping track
of consumer responses to services and products –
where identifying positive and negative customer
experiences helps to assess product and service de-
mand, tackle crisis management, etc.
On the other hand, the use of finer-grained mod-
els, accounting for the role of individual emotions,
is still in its infancy. The simple division in ‘pos-
itive’ vs. ‘negative’ comments may not suffice, as
in these examples: ‘I’m so miserable, I dropped
my IPhone in the water and now it’s not working
anymore’ (SADNESS) vs. ‘I am very upset, my new
IPhone keeps not working!’ (ANGER). While both
texts express a negative sentiment, the latter, con-
nected to anger, is more relevant for buzz monitor-
ing. Thus, emotion analysis represents a natural
evolution of sentiment analysis.
Many approaches to sentiment analysis make
use of lexical resources – i.e. lists of positive and
negative words – often deployed as baselines or as
features for other methods, usually machine learn-
ing based (Liu and Zhang, 2012). In these lexica,
words are associated with their prior polarity, i.e.
whether such word out of context evokes some-
thing positive or something negative. For exam-
ple, wonderful has a positive connotation – prior
polarity – while horrible has a negative one.
The quest for a high precision and high cov-
erage lexicon, where words are associated with
either sentiment or emotion scores, has several
reasons. First, it is fundamental for tasks such
as affective modification of existing texts, where
words’ polarity together with their score are nec-
essary for creating multiple graded variations of
the original text (Inkpen et al., 2006; Guerini et
al., 2008; Whitehead and Cavedon, 2010).
Second, considering word order makes a differ-
ence in sentiment analysis. This calls for a role of
compositionality, where the score of a sentence is
computed by composing the scores of the words
up in the syntactic tree. Works worth mention-
ing in this connection are: Socher et al. (2013),
which uses recursive neural networks to learn
compositional rules for sentiment analysis, and
(Neviarouskaya et al., 2009; Neviarouskaya et al.,
2011) which exploit hand-coded rules to compose
the emotions expressed by words in a sentence. In
this respect, compositional approaches represent a
new promising trend, since all other approaches,
either using semantic similarity or Bag-of-Words
(BOW) based machine-learning, cannot handle,
for example, cases of texts with same wording
but different words order: “The dangerous killer
escaped one month ago, but recently he was ar-
rested” (RELIEF, HAPPYNESS) vs. “The danger-
ous killer was arrested one month ago, but re-
</bodyText>
<page confidence="0.979246">
427
</page>
<bodyText confidence="0.990787128205128">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 427–433,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
cently he escaped” (FEAR). The work in (Wang
and Manning, 2012) partially accounts for this
problem and argues that using word bigram fea-
tures allows improving over BOW based meth-
ods, where words are taken as features in isola-
tion. This way it is possible to capture simple
compositional phenomena like polarity reversing
in “killing cancer”.
Finally, tasks such as copywriting, where evoca-
tive names are a key element to a successful prod-
uct (Ozbal and Strapparava, 2012; Ozbal et al.,
2012) require exhaustive lists of emotion related
words. In such cases no context is given and the
brand name alone, with its perceived prior polar-
ity, is responsible for stating the area of compe-
tition and evoking semantic associations. For ex-
ample Mitsubishi changed the name of one of its
SUVs for the Spanish market, since the original
name Pajero had a very negative prior polarity, as
it means ‘wanker’ in Spanish (Piller, 2003). Evok-
ing emotions is also fundamental for a successful
name: consider names of a perfume like Obses-
sion, or technological products like MacBook air.
In this work, we aim at automatically producing
a high coverage and high precision emotion lex-
icon using distributional semantics, with numer-
ical scores associated with each emotion, like it
has already been done for sentiment analysis. To
this end, we take advantage in an original way of
massive crowd-sourced affective annotations as-
sociated with news articles, obtained by crawl-
ing the rappler.com social news network. We
also evaluate our lexicon by integrating it in unsu-
pervised classification and regression settings for
emotion recognition. Results indicate that the use
of our resource, even if automatically acquired, is
highly beneficial in affective text recognition.
</bodyText>
<sectionHeader confidence="0.999766" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999955380952381">
Within the broad field of sentiment analysis, we
hereby provide a short review of research efforts
put towards building sentiment and emotion lex-
ica, regardless of the approach in which such lists
are then used (machine learning, rule based or
deep learning). A general overview can be found
in (Pang and Lee, 2008; Liu and Zhang, 2012;
Wilson et al., 2004; Paltoglou et al., 2010).
Sentiment Lexica. In recent years there has
been an increasing focus on producing lists of
words (lexica) with prior polarities, to be used in
sentiment analysis. When building such lists, a
trade-off between coverage of the resource and its
precision is to be found.
One of the most well-known resources is Senti-
WordNet (SWN) (Esuli and Sebastiani, 2006; Bac-
cianella et al., 2010), in which each entry is as-
sociated with the numerical scores Pos(s) and
Neg(s), ranging from 0 to 1. These scores –
automatically assigned starting from a bunch of
seed terms – represent the positive and negative
valence (or posterior polarity) of each entry, that
takes the form lemma#pos#sense-number.
Starting from SWN, several prior polarities for
words (SWN-prior), in the form lemma#PoS,
can be computed (e.g. considering only the first-
sense, averaging on all the senses, etc.). These ap-
proaches, detailed in (Guerini et al., 2013), pro-
duce a list of 155k words, where the lower pre-
cision given by the automatic scoring of SWN is
compensated by the high coverage.
Another widely used resource is ANEW
(Bradley and Lang, 1999), providing valence
scores for 1k words, which were manually as-
signed by several annotators. This resource has
a low coverage, but the precision is maximized.
Similarly, the SO-CAL entries (Taboada et al.,
2011) were manually tagged by a small num-
ber of annotators with a multi-class label (from
very negative to very positive). These
ratings were further validated through crowd-
sourcing, ending up with a list of roughly 4k
words. More recently, a resource that repli-
cated ANEW annotation approach using crowd-
sourcing, was released (Warriner et al., 2013), pro-
viding sentiment scores for 14k words. Interest-
ingly, this resource annotates the most frequent
words in English, so, even if lexicon coverage is
still far lower than SWN-prior, it grants a high cov-
erage, with human precision, of language use.
Finally, the General Inquirer lexicon (Stone
et al., 1966) provides a binary classifica-
tion (positive/negative) of 4k sentiment-
bearing words, while the resource in (Wilson et al.,
2005) expands the General Inquirer to 6k words.
Emotion Lexica. Compared to sentiment
lexica, far less emotion lexica have been pro-
duced, and all have lower coverage. One of the
most used resources is WordNetAffect (Strappa-
rava and Valitutti, 2004) which contains manu-
ally assigned affective labels to WordNet synsets
(ANGER, JOY, FEAR, etc.). It currently provides
900 annotated synsets and 1.6k words in the form
</bodyText>
<page confidence="0.999293">
428
</page>
<tableCaption confidence="0.999573">
Table 1: An excerpt of the Document-by-Emotion Matrix - MDE
</tableCaption>
<figure confidence="0.624599818181818">
AFRAID AMUSED ANGRY ANNOYED DONT CARE HAPPY INSPIRED SAD
0.75 0.00 0.00 0.00 0.00 0.00 0.25 0.00
0.00 0.50 0.00 0.16 0.17 0.17 0.00 0.00
0.52 0.02 0.03 0.02 0.02 0.06 0.02 0.31
0.40 0.00 0.00 0.20 0.00 0.20 0.20 0.00
0.00 0.30 0.08 0.00 0.00 0.23 0.31 0.08
doc 10002
doc 10003
doc 10004
doc 10011
doc 10028
</figure>
<bodyText confidence="0.994530166666667">
lemma#PoS#sense, corresponding to roughly
1 thousand lemma#PoS.
AffectNet, part of the SenticNet project (Cam-
bria and Hussain, 2012), contains 10k words (out
of 23k entries) taken from ConceptNet and aligned
with WordNetAffect. This resource extends Word-
NetAffect labels to concepts like ‘have breakfast’.
Fuzzy Affect Lexicon (Subasic and Huettner, 2001)
contains roughly 4k lemma#PoS manually an-
notated by one linguist using 80 emotion labels.
EmoLex (Mohammad and Turney, 2013) contains
almost 10k lemmas annotated with an intensity la-
bel for each emotion using Mechanical Turk. Fi-
nally Affect database is an extension of SentiFul
(Neviarouskaya et al., 2007) and contains 2.5K
words in the form lemma#PoS. The latter is the
only lexicon providing words annotated also with
emotion scores rather than only with labels.
</bodyText>
<sectionHeader confidence="0.995072" genericHeader="method">
3 Dataset Collection
</sectionHeader>
<bodyText confidence="0.999991681818182">
To build our emotion lexicon we harvested all the
news articles from rappler.com, as of June
3rd 2013: the final dataset consists of 13.5 M
words over 25.3 K documents, with an average
of 530 words per document. For each document,
along with the text we also harvested the informa-
tion displayed by Rappler’s Mood Meter, a small
interface offering the readers the opportunity to
click on the emotion that a given Rappler story
made them feel. The idea behind the Mood Me-
ter is actually “getting people to crowdsource the
mood for the day”&apos;, and returning the percentage
of votes for each emotion label for a given story.
This way, hundreds of thousands votes have been
collected since the launch of the service. In our
novel approach to ‘crowdsourcing’, as compared
to other NLP tasks that rely on tools like Ama-
zon’s Mechanical Turk (Snow et al., 2008), the
subjects are aware of the ‘implicit annotation task’
but they are not paid. From this data, we built a
document-by-emotion matrix MDE, providing the
voting percentages for each document in the eight
</bodyText>
<footnote confidence="0.847815">
1http://nie.mn/QuD17Z
</footnote>
<bodyText confidence="0.980209653846154">
affective dimensions available in Rappler. An ex-
cerpt is provided in Table 1.
The idea of using documents annotated with
emotions is not new (Strapparava and Mihalcea,
2008; Mishne, 2005; Bellegarda, 2010), but these
works had the limitation of providing a single
emotion label per document, rather than a score for
each emotion, and, moreover, the annotation was
performed by the author of the document alone.
Table 2 reports the average percentage of votes
for each emotion on the whole corpus: HAPPI-
NESS has a far higher percentage of votes (at least
three times). There are several possible explana-
tions, out of the scope of the present paper, for this
bias: (i) it is due to cultural characteristics of the
audience (ii) the bias is in the dataset itself, being
formed mainly by ‘positive’ news; (iii) it is a psy-
chological phenomenon due to the fact that peo-
ple tend to express more positive moods on social
networks (Quercia et al., 2011; Vittengl and Holt,
1998; De Choudhury et al., 2012). In any case, the
predominance of happy mood has been found in
other datasets, for instance LiveJournal.com
posts (Strapparava and Mihalcea, 2008). In the
following section we will discuss how we handled
this problem.
</bodyText>
<table confidence="0.9991646">
EMOTION Votes,, EMOTION Votes,
AFRAID 0.04 DONT CARE 0.05
AMUSED 0.10 HAPPY 0.32
ANGRY 0.10 INSPIRED 0.10
ANNOYED 0.06 SAD 0.11
</table>
<tableCaption confidence="0.997136">
Table 2: Average percentages of votes.
</tableCaption>
<sectionHeader confidence="0.984682" genericHeader="method">
4 Emotion Lexicon Creation
</sectionHeader>
<bodyText confidence="0.999969777777778">
As a next step we built a word-by-emotion matrix
starting from MDE using an approach based on
compositional semantics. To do so, we first lem-
matized and PoS tagged all the documents (where
PoS can be adj., nouns, verbs, adv.) and kept
only those lemma#PoS present also in Word-
Net, similar to SWN-prior and WordNetAffect re-
sources, to which we want to align. We then com-
puted the term-by-document matrices using raw
</bodyText>
<page confidence="0.992387">
429
</page>
<table confidence="0.999721692307692">
Word AFRAID AMUSED ANGRY ANNOYED DONT CARE HAPPY INSPIRED SAD
awe#n 0.08 0.12 0.04 0.11 0.07 0.15 0.38 0.05
comical#a 0.02 0.51 0.04 0.05 0.12 0.17 0.03 0.06
crime#n 0.11 0.10 0.23 0.15 0.07 0.09 0.09 0.15
criminal#a 0.12 0.10 0.25 0.14 0.10 0.11 0.07 0.11
dead#a 0.17 0.07 0.17 0.07 0.07 0.05 0.05 0.35
funny#a 0.04 0.29 0.04 0.11 0.16 0.13 0.15 0.08
future#n 0.09 0.12 0.09 0.12 0.13 0.13 0.21 0.10
game#n 0.06 0.15 0.06 0.08 0.15 0.23 0.15 0.12
kill#v 0.23 0.06 0.21 0.07 0.05 0.06 0.05 0.27
rapist#n 0.02 0.07 0.46 0.07 0.08 0.16 0.03 0.12
sad#a 0.06 0.12 0.09 0.14 0.13 0.07 0.15 0.24
warning#n 0.44 0.06 0.09 0.09 0.06 0.06 0.04 0.16
</table>
<tableCaption confidence="0.999729">
Table 3: An excerpt of the Word-by-Emotion Matrix (MWE) using normalized frequencies (nf). Emo-
</tableCaption>
<bodyText confidence="0.982703">
tions weighting more than 20% in a word are highlighted for readability purposes.
frequencies, normalized frequencies, and tf-idf
(MWD,f, MWD,nf and MWD,tfidf respectively),
so to test which of the three weights is better. Af-
ter that, we applied matrix multiplication between
the document-by-emotion and word-by-document
matrices (MDE · MWD) to obtain a (raw) word-
by-emotion matrix MWE. This method allows us
to ‘merge’ words with emotions by summing the
products of the weight of a word with the weight
of the emotions in each document.
Finally, we transformed MWE by first apply-
ing normalization column-wise (so to eliminate
the over representation for happiness as discussed
in Section 3) and then scaling the data row-wise so
to sum up to one. An excerpt of the final Matrix
MWE is presented in Table 3, and it can be in-
terpreted as a list of words with scores that repre-
sent how much weight a given word has in the af-
fective dimensions we consider. So, for example,
awe#n has a predominant weight in INSPIRED
(0.38), comical#a has a predominant weight in
AMUSED (0.51), while kill#v has a predomi-
nant weight in AFRAID, ANGRY and SAD (0.23,
0.21 and 0.27 respectively). This matrix, that we
call DepecheMood2, represents our emotion lex-
icon, it contains 37k entries and is freely available
for research purposes at http://git.io/MqyoIg.
</bodyText>
<sectionHeader confidence="0.999609" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999646285714286">
To evaluate the performance we can obtain with
our lexicon, we use the public dataset provided for
the SemEval 2007 task on ‘Affective Text’ (Strap-
parava and Mihalcea, 2007). The task was focused
on emotion recognition in one thousand news
headlines, both in regression and classification
settings. Headlines typically consist of a few
</bodyText>
<footnote confidence="0.868031">
2In French, ‘depeche’ means dispatch/news.
</footnote>
<bodyText confidence="0.999834857142857">
words and are often written with the intention to
‘provoke’ emotions so to attract the readers’ atten-
tion. An example of headline from the dataset is
the following: “Iraq car bombings kill 22 People,
wound more than 60”. For the regression task
the values provided are: &lt;anger (0.32),
disgust (0.27), fear (0.84), joy
(0.0), sadness (0.95), surprise
(0.20)&gt; while for the classification task the
labels provided are {FEAR, SADNESS}.
This dataset is of interest to us since the ‘com-
positional’ problem is less prominent given the
simplified syntax of news headlines, containing,
for example, fewer adverbs (like negations or in-
tensifiers) than normal sentences (Turchi et al.,
2012). Furthermore, this is to our knowledge the
only dataset available providing numerical scores
for emotions. Finally, this dataset was meant for
unsupervised approaches (just a small trial sample
was provided), so to avoid simple text categoriza-
tion approaches.
As the affective dimensions present in the test
set – based on the six basic emotions model (Ek-
man and Friesen, 1971) – do not exactly match
with the ones provided by Rappler’s Mood Meter,
we first define a mapping between the two when
possible, see Table 4. Then, we proceed to trans-
form the test headlines to the lemma#PoS format.
</bodyText>
<table confidence="0.9975976">
SemEval Rappler SemEval Rappler
FEAR AFRAID SURPRISE INSPIRED
ANGER ANGRY - ANNOYED
JOY HAPPY - AMUSED
SADNESS SAD - DON’T CARE
</table>
<tableCaption confidence="0.7527515">
Table 4: Mapping of Rappler labels on Se-
meval2007. In bold, cases of suboptimal mapping.
Only one test headline contained exclusively
words not present in DepecheMood, further indi-
</tableCaption>
<page confidence="0.996792">
430
</page>
<bodyText confidence="0.999873375">
cating the high-coverage nature of our resource. In
Table 5 we report the coverage of some Sentiment
and Emotion Lexica of different sizes on the same
dataset. Similar to Warriner et al. (2013), we ob-
serve that even if the number of entries of our lex-
icon is far lower than SWN-prior approaches, the
fact that we extracted and annotated words from
documents grants a high coverage of language use.
</bodyText>
<table confidence="0.9998166">
ANEW 1k entries 0.10
Sentiment Warriner et. al 13k entries 0.51
Lexica SWN-prior 155k entries 0.67
Emotion WNAffect 1k entries 0.12
Lexica DepecheMood 37k entries 0.64
</table>
<tableCaption confidence="0.999839">
Table 5: Statistics on words coverage per headline.
</tableCaption>
<bodyText confidence="0.999780875">
Since our primary goal is to assess the quality of
DepecheMood we first focus on the regression
task. We do so by using a very naive approach,
similar to “WordNetAffect presence” discussed in
(Strapparava and Mihalcea, 2008): for each head-
line, we simply compute a value, for any affective
dimension, by averaging the corresponding affec-
tive scores –obtained from DepecheMood- of all
lemma#PoS present in the headline.
In Table 6 we report the results obtained using
the three versions of our resource (Pearson corre-
lation), along with the best performance on each
emotion of other systems3 (bests,); the last col-
umn contains the upper bound of inter-annotator
agreement. For all the 5 emotions we improve
over the best performing systems (DISGUST has
no alignment with our labels and was discarded).
Interestingly, even using a sub-optimal align-
ment for SURPRISE we still manage to outper-
form other systems. Considering the naive ap-
proach we used, we can reasonably conclude that
the quality and coverage of our resource are the
reason of such results, and that adopting more
complex approaches (i.e. compositionality) can
possibly further improve performances in text-
based emotion recognition.
As a final test, we evaluate our resource in the
classification task. The naive approach used in
this case consists in mapping the average of the
scores of all words in the headline to a binary de-
cision with fixed threshold at 0.5 for each emotion
(after min-max normalization on all test headlines
</bodyText>
<footnote confidence="0.9887166">
3Systems participating in the ‘Affective Text’ task plus the
approaches in (Strapparava and Mihalcea, 2008). Other su-
pervised approaches in the classification task (Mohammad,
2012; Bellegarda, 2010; Chaffar and Inkpen, 2011), report-
ing only overall performances, are not considered.
</footnote>
<table confidence="0.997539428571429">
DepecheMood best., upper
f of tfidf
FEAR 0.56 0.54 0.53 0.45 0.64
ANGER 0.36 0.38 0.36 0.32 0.50
SURPRISE* 0.25 0.21 0.24 0.16 0.36
JOY 0.39 0.40 0.39 0.26 0.60
SADNESS 0.48 0.47 0.46 0.41 0.68
</table>
<tableCaption confidence="0.994447">
Table 6: Regression results – Pearson’s correlation
</tableCaption>
<bodyText confidence="0.996752181818182">
scores). In Table 7 we report the results (F1 mea-
sure) of our approach along with the best perfor-
mance of other systems on each emotion (bests,),
as in the previous case. For 3 emotions out of
5 we improve over the best performing systems,
for one emotion we obtain the same results, and
for one emotion we do not outperform other sys-
tems. In this case the difference in performances
among the various ways of representing the word-
by-document matrix is more prominent: normal-
ized frequencies (nf) provide the best results.
</bodyText>
<table confidence="0.998895285714286">
DepecheMood best.,
f of tfidf
FEAR 0.25 0.32 0.31 0.23
ANGER 0.00 0.00 0.00 0.17
SURPRISE* 0.13 0.16 0.09 0.15
JOY 0.22 0.30 0.32 0.32
SADNESS 0.36 0.40 0.38 0.30
</table>
<tableCaption confidence="0.994048">
Table 7: Classification results – F1 measures
</tableCaption>
<sectionHeader confidence="0.994222" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9999729">
We presented DepecheMood, an emotion lexi-
con built in a novel and totally automated way
by harvesting crowd-sourced affective annota-
tion from a social news network. Our experi-
mental results indicate high-coverage and high-
precision of the lexicon, showing significant im-
provements over state-of-the-art unsupervised ap-
proaches even when using the resource with very
naive classification and regression strategies. We
believe that the wealth of information provided by
social media can be harnessed to build models and
resources for emotion recognition from text, going
a step beyond sentiment analysis. Our future work
will include testing Singular Value Decomposi-
tion on the word-by-document matrices, allowing
to propagate emotions values for a document to
similar words non present in the document itself,
and the study of perceived mood effects on viral-
ity indices and readers engagement by exploiting
tweets, likes, reshares and comments.
</bodyText>
<footnote confidence="0.889638">
This work has been partially supported by the Trento
RISE PerTe project.
</footnote>
<page confidence="0.996808">
431
</page>
<sectionHeader confidence="0.880836" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.991429355140187">
S. Baccianella, A. Esuli, and F. Sebastiani. 2010. Sen-
tiWordNet 3.0: An enhanced lexical resource for
sentiment analysis and opinion mining. In Proceed-
ings of the Conference on International Language
Resources and Evaluation (LREC), pages 2200–
2204, Valletta, Malta.
J. R. Bellegarda. 2010. Emotion analysis using latent
affective folding and embedding. In Proceedings of
the NAACL HLT 2010 workshop on computational
approaches to analysis and generation of emotion in
text, pages 1–9. Association for Computational Lin-
guistics.
M. Bradley and P. Lang. 1999. Affective norms for
english words (ANEW): Instruction manual and af-
fective ratings. Technical Report C-1, University of
Florida.
E. Cambria and A. Hussain. 2012. Sentic computing.
Springer.
S. Chaffar and D. Inkpen. 2011. Using a hetero-
geneous dataset for emotion analysis in text. In
Advances in Artificial Intelligence, pages 62–67.
Springer.
M. De Choudhury, S. Counts, and M. Gamon. 2012.
Not all moods are created equal! exploring human
emotional states in social media. In Proceedings of
the International Conference on Weblogs and Social
Media (ICWSM).
P. Ekman and W. V. Friesen. 1971. Constants across
cultures in the face and emotion. Journal of Person-
ality and Social Psychology, 17:124–129.
A. Esuli and F. Sebastiani. 2006. SentiWordNet: A
publicly available lexical resource for opinion min-
ing. In Proceedings of the Conference on Interna-
tional Language Resources and Evaluation (LREC),
pages 417–422, Genova, IT.
M. Guerini, O. Stock, and C. Strapparava. 2008.
Valentino: A tool for valence shifting of natural lan-
guage texts. In Proceedings of the Conference on
International Language Resources and Evaluation
(LREC), Marrakech, Morocco.
M. Guerini, L. Gatti, and M. Turchi. 2013. Senti-
ment analysis: How to derive prior polarities from
sentiwordnet. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 1259–1269.
D. Z. Inkpen, O. Feiguina, and G. Hirst. 2006. Gener-
ating more-positive and more-negative text. In Com-
puting Attitude and Affect in Text: Theory andAppli-
cations, pages 187–198. Springer.
B. Liu and L. Zhang. 2012. A survey of opinion min-
ing and sentiment analysis. Mining Text Data, pages
415–463.
G. Mishne. 2005. Experiments with mood classifica-
tion in blog posts. In Proceedings of ACM SIGIR
2005 Workshop on Stylistic Analysis of Text for In-
formation Access, volume 19.
S. M. Mohammad and P. D. Turney. 2013. Crowd-
sourcing a word–emotion association lexicon. Com-
putational Intelligence, 29(3):436–465.
S. M. Mohammad. 2012. # Emotional tweets. In Pro-
ceedings of the First Joint Conference on Lexical
and Computational Semantics (*Sem), pages 246–
255. Association for Computational Linguistics.
A. Neviarouskaya, H. Prendinger, and M. Ishizuka.
2007. Textual affect sensing for sociable and
expressive online communication. In A. Paiva,
R. Prada, and R. Picard, editors, Affective Com-
puting and Intelligent Interaction, volume 4738 of
Lecture Notes in Computer Science, pages 218–229.
Springer Berlin Heidelberg.
A. Neviarouskaya, H. Prendinger, and M. Ishizuka.
2009. Compositionality principle in recognition of
fine-grained emotions from text. In Proceedings of
the International Conference on Weblogs and Social
Media (ICWSM).
A. Neviarouskaya, H. Prendinger, and M. Ishizuka.
2011. Affect analysis model: novel rule-based ap-
proach to affect sensing from text. Natural Lan-
guage Engineering, 17(1):95.
G. Ozbal and C. Strapparava. 2012. A computational
approach to the automation of creative naming. Pro-
ceedings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL).
G. Ozbal, C. Strapparava, and M. Guerini. 2012.
Brand pitt: A corpus to explore the art of naming.
In Proceedings of the Conference on International
Language Resources and Evaluation (LREC).
G. Paltoglou, M. Thelwall, and K. Buckley. 2010. On-
line textual communications annotated with grades
of emotion strength. In Proceedings of the 3rd In-
ternational Workshop of Emotion: Corpora for re-
search on Emotion and Affect, pages 25–31.
B. Pang and L. Lee. 2008. Opinion mining and senti-
ment analysis. Foundations and Trends in Informa-
tion Retrieval, 2(1-2):1–135.
I. Piller. 2003. 10. advertising as a site of lan-
guage contact. Annual Review of Applied Linguis-
tics, 23:170–183.
D. Quercia, J. Ellis, L. Capra, and J. Crowcroft. 2011.
In the mood for being influential on twitter. Pro-
ceedings of IEEE SocialCom’11.
R. Snow, B. O’Connor, D. Jurafsky, and A. Ng. 2008.
Cheap and fast—but is it good?: evaluating non-
expert annotations for natural language tasks. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP),
pages 254–263.
</reference>
<page confidence="0.981831">
432
</page>
<reference confidence="0.99979653968254">
R. Socher, A. Perelygin, J. Y. Wu, J. Chuang, C. D.
Manning, A. Y. Ng, and C. Potts. 2013. Recur-
sive deep models for semantic compositionality over
a sentiment treebank. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1631–1642.
P. Stone, D. Dunphy, and M. Smith. 1966. The Gen-
eral Inquirer: A Computer Approach to Content
Analysis. MIT press.
C. Strapparava and R. Mihalcea. 2007. Semeval-
2007 task 14: Affective text. In Proceedings of
the 4th International Workshop on Semantic Evalu-
ations, pages 70–74. Association for Computational
Linguistics.
C. Strapparava and R. Mihalcea. 2008. Learning to
identify emotions in text. In Proceedings of the
2008 ACM symposium on Applied computing, pages
1556–1560. ACM.
C. Strapparava and A. Valitutti. 2004. WordNet-
Affect: an affective extension of WordNet. In Pro-
ceedings of the Conference on International Lan-
guage Resources and Evaluation (LREC), pages
1083 – 1086, Lisbon, May.
P. Subasic and A. Huettner. 2001. Affect analysis of
text using fuzzy semantic typing. Fuzzy Systems,
IEEE Transactions on, 9(4):483–496.
M. Taboada, J. Brooke, M. Tofiloski, K. Voll, and
M. Stede. 2011. Lexicon-based methods for
sentiment analysis. Computational linguistics,
37(2):267–307.
M. Turchi, M. Atkinson, A. Wilcox, B. Crawley,
S. Bucci, R. Steinberger, and E. Van der Goot. 2012.
Onts: optima news translation system. In Proceed-
ings of the Demonstrations at the 13th Conference of
the European Chapter of the Association for Com-
putational Linguistics, pages 25–30. Association for
Computational Linguistics.
J. R. Vittengl and C. S. Holt. 1998. A time-series diary
study of mood and social interaction. Motivation
and Emotion, 22(3):255–275.
S. Wang and C. Manning. 2012. Baselines and bi-
grams: Simple, good sentiment and topic classifica-
tion. Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics (ACL).
A. B. Warriner, V. Kuperman, and M. Brysbaert.
2013. Norms of valence, arousal, and dominance for
13,915 english lemmas. Behavior research methods,
45(4):1191–1207.
S. Whitehead and L. Cavedon. 2010. Generating
shifting sentiment for a conversational agent. In
Proceedings of the NAACL HLT 2010 Workshop on
Computational Approaches to Analysis and Genera-
tion of Emotion in Text, pages 89–97, Los Angeles,
CA, June. Association for Computational Linguis-
tics.
T. Wilson, J. Wiebe, and R. Hwa. 2004. Just how mad
are you? finding strong and weak opinion clauses.
In Proceedings of AAAI, pages 761–769.
T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recog-
nizing contextual polarity in phrase-level sentiment
analysis. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, pages 347–354.
</reference>
<page confidence="0.999437">
433
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.455004">
<title confidence="0.99568">DepecheMood: a Lexicon for Emotion Analysis from Crowd-Annotated News</title>
<author confidence="0.999941">Jacopo Staiano Marco Guerini</author>
<affiliation confidence="0.999902">University of Trento Trento RISE</affiliation>
<address confidence="0.477264">Trento - Italy Trento - Italy</address>
<email confidence="0.979652">staiano@disi.unitn.itmarco.guerini@trentorise.eu</email>
<abstract confidence="0.998625666666667">While many lexica annotated with words polarity are available for sentiment analysis, very few tackle the harder task of emotion analysis and are usually quite limited in coverage. In this paper, we present a novel approach for extracting – in a totally automated way – a highcoverage and high-precision lexicon of roughly 37 thousand terms annotated with scores, called Our approach exploits in an original way ‘crowd-sourced’ affective annotation implicitly provided by readers of news arfrom By providing new state-of-the-art performances in unsupervised settings for regression and classification tasks, even using a naive approach, our experiments show the beneficial impact of harvesting social media data for affective lexicon building.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Baccianella</author>
<author>A Esuli</author>
<author>F Sebastiani</author>
</authors>
<title>SentiWordNet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on International Language Resources and Evaluation (LREC),</booktitle>
<pages>2200--2204</pages>
<location>Valletta,</location>
<contexts>
<context position="6546" citStr="Baccianella et al., 2010" startWordPosition="1029" endWordPosition="1033">on lexica, regardless of the approach in which such lists are then used (machine learning, rule based or deep learning). A general overview can be found in (Pang and Lee, 2008; Liu and Zhang, 2012; Wilson et al., 2004; Paltoglou et al., 2010). Sentiment Lexica. In recent years there has been an increasing focus on producing lists of words (lexica) with prior polarities, to be used in sentiment analysis. When building such lists, a trade-off between coverage of the resource and its precision is to be found. One of the most well-known resources is SentiWordNet (SWN) (Esuli and Sebastiani, 2006; Baccianella et al., 2010), in which each entry is associated with the numerical scores Pos(s) and Neg(s), ranging from 0 to 1. These scores – automatically assigned starting from a bunch of seed terms – represent the positive and negative valence (or posterior polarity) of each entry, that takes the form lemma#pos#sense-number. Starting from SWN, several prior polarities for words (SWN-prior), in the form lemma#PoS, can be computed (e.g. considering only the firstsense, averaging on all the senses, etc.). These approaches, detailed in (Guerini et al., 2013), produce a list of 155k words, where the lower precision give</context>
</contexts>
<marker>Baccianella, Esuli, Sebastiani, 2010</marker>
<rawString>S. Baccianella, A. Esuli, and F. Sebastiani. 2010. SentiWordNet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining. In Proceedings of the Conference on International Language Resources and Evaluation (LREC), pages 2200– 2204, Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Bellegarda</author>
</authors>
<title>Emotion analysis using latent affective folding and embedding.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT</booktitle>
<pages>1--9</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11176" citStr="Bellegarda, 2010" startWordPosition="1788" endWordPosition="1789"> collected since the launch of the service. In our novel approach to ‘crowdsourcing’, as compared to other NLP tasks that rely on tools like Amazon’s Mechanical Turk (Snow et al., 2008), the subjects are aware of the ‘implicit annotation task’ but they are not paid. From this data, we built a document-by-emotion matrix MDE, providing the voting percentages for each document in the eight 1http://nie.mn/QuD17Z affective dimensions available in Rappler. An excerpt is provided in Table 1. The idea of using documents annotated with emotions is not new (Strapparava and Mihalcea, 2008; Mishne, 2005; Bellegarda, 2010), but these works had the limitation of providing a single emotion label per document, rather than a score for each emotion, and, moreover, the annotation was performed by the author of the document alone. Table 2 reports the average percentage of votes for each emotion on the whole corpus: HAPPINESS has a far higher percentage of votes (at least three times). There are several possible explanations, out of the scope of the present paper, for this bias: (i) it is due to cultural characteristics of the audience (ii) the bias is in the dataset itself, being formed mainly by ‘positive’ news; (iii</context>
<context position="19165" citStr="Bellegarda, 2010" startWordPosition="3110" endWordPosition="3111"> complex approaches (i.e. compositionality) can possibly further improve performances in textbased emotion recognition. As a final test, we evaluate our resource in the classification task. The naive approach used in this case consists in mapping the average of the scores of all words in the headline to a binary decision with fixed threshold at 0.5 for each emotion (after min-max normalization on all test headlines 3Systems participating in the ‘Affective Text’ task plus the approaches in (Strapparava and Mihalcea, 2008). Other supervised approaches in the classification task (Mohammad, 2012; Bellegarda, 2010; Chaffar and Inkpen, 2011), reporting only overall performances, are not considered. DepecheMood best., upper f of tfidf FEAR 0.56 0.54 0.53 0.45 0.64 ANGER 0.36 0.38 0.36 0.32 0.50 SURPRISE* 0.25 0.21 0.24 0.16 0.36 JOY 0.39 0.40 0.39 0.26 0.60 SADNESS 0.48 0.47 0.46 0.41 0.68 Table 6: Regression results – Pearson’s correlation scores). In Table 7 we report the results (F1 measure) of our approach along with the best performance of other systems on each emotion (bests,), as in the previous case. For 3 emotions out of 5 we improve over the best performing systems, for one emotion we obtain th</context>
</contexts>
<marker>Bellegarda, 2010</marker>
<rawString>J. R. Bellegarda. 2010. Emotion analysis using latent affective folding and embedding. In Proceedings of the NAACL HLT 2010 workshop on computational approaches to analysis and generation of emotion in text, pages 1–9. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Bradley</author>
<author>P Lang</author>
</authors>
<title>Affective norms for english words (ANEW): Instruction manual and affective ratings.</title>
<date>1999</date>
<tech>Technical Report C-1,</tech>
<institution>University of Florida.</institution>
<contexts>
<context position="7278" citStr="Bradley and Lang, 1999" startWordPosition="1151" endWordPosition="1154">cores – automatically assigned starting from a bunch of seed terms – represent the positive and negative valence (or posterior polarity) of each entry, that takes the form lemma#pos#sense-number. Starting from SWN, several prior polarities for words (SWN-prior), in the form lemma#PoS, can be computed (e.g. considering only the firstsense, averaging on all the senses, etc.). These approaches, detailed in (Guerini et al., 2013), produce a list of 155k words, where the lower precision given by the automatic scoring of SWN is compensated by the high coverage. Another widely used resource is ANEW (Bradley and Lang, 1999), providing valence scores for 1k words, which were manually assigned by several annotators. This resource has a low coverage, but the precision is maximized. Similarly, the SO-CAL entries (Taboada et al., 2011) were manually tagged by a small number of annotators with a multi-class label (from very negative to very positive). These ratings were further validated through crowdsourcing, ending up with a list of roughly 4k words. More recently, a resource that replicated ANEW annotation approach using crowdsourcing, was released (Warriner et al., 2013), providing sentiment scores for 14k words. </context>
</contexts>
<marker>Bradley, Lang, 1999</marker>
<rawString>M. Bradley and P. Lang. 1999. Affective norms for english words (ANEW): Instruction manual and affective ratings. Technical Report C-1, University of Florida.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Cambria</author>
<author>A Hussain</author>
</authors>
<title>Sentic computing.</title>
<date>2012</date>
<publisher>Springer.</publisher>
<contexts>
<context position="9181" citStr="Cambria and Hussain, 2012" startWordPosition="1461" endWordPosition="1465">labels to WordNet synsets (ANGER, JOY, FEAR, etc.). It currently provides 900 annotated synsets and 1.6k words in the form 428 Table 1: An excerpt of the Document-by-Emotion Matrix - MDE AFRAID AMUSED ANGRY ANNOYED DONT CARE HAPPY INSPIRED SAD 0.75 0.00 0.00 0.00 0.00 0.00 0.25 0.00 0.00 0.50 0.00 0.16 0.17 0.17 0.00 0.00 0.52 0.02 0.03 0.02 0.02 0.06 0.02 0.31 0.40 0.00 0.00 0.20 0.00 0.20 0.20 0.00 0.00 0.30 0.08 0.00 0.00 0.23 0.31 0.08 doc 10002 doc 10003 doc 10004 doc 10011 doc 10028 lemma#PoS#sense, corresponding to roughly 1 thousand lemma#PoS. AffectNet, part of the SenticNet project (Cambria and Hussain, 2012), contains 10k words (out of 23k entries) taken from ConceptNet and aligned with WordNetAffect. This resource extends WordNetAffect labels to concepts like ‘have breakfast’. Fuzzy Affect Lexicon (Subasic and Huettner, 2001) contains roughly 4k lemma#PoS manually annotated by one linguist using 80 emotion labels. EmoLex (Mohammad and Turney, 2013) contains almost 10k lemmas annotated with an intensity label for each emotion using Mechanical Turk. Finally Affect database is an extension of SentiFul (Neviarouskaya et al., 2007) and contains 2.5K words in the form lemma#PoS. The latter is the only</context>
</contexts>
<marker>Cambria, Hussain, 2012</marker>
<rawString>E. Cambria and A. Hussain. 2012. Sentic computing. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Chaffar</author>
<author>D Inkpen</author>
</authors>
<title>Using a heterogeneous dataset for emotion analysis in text.</title>
<date>2011</date>
<booktitle>In Advances in Artificial Intelligence,</booktitle>
<pages>62--67</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="19192" citStr="Chaffar and Inkpen, 2011" startWordPosition="3112" endWordPosition="3115">s (i.e. compositionality) can possibly further improve performances in textbased emotion recognition. As a final test, we evaluate our resource in the classification task. The naive approach used in this case consists in mapping the average of the scores of all words in the headline to a binary decision with fixed threshold at 0.5 for each emotion (after min-max normalization on all test headlines 3Systems participating in the ‘Affective Text’ task plus the approaches in (Strapparava and Mihalcea, 2008). Other supervised approaches in the classification task (Mohammad, 2012; Bellegarda, 2010; Chaffar and Inkpen, 2011), reporting only overall performances, are not considered. DepecheMood best., upper f of tfidf FEAR 0.56 0.54 0.53 0.45 0.64 ANGER 0.36 0.38 0.36 0.32 0.50 SURPRISE* 0.25 0.21 0.24 0.16 0.36 JOY 0.39 0.40 0.39 0.26 0.60 SADNESS 0.48 0.47 0.46 0.41 0.68 Table 6: Regression results – Pearson’s correlation scores). In Table 7 we report the results (F1 measure) of our approach along with the best performance of other systems on each emotion (bests,), as in the previous case. For 3 emotions out of 5 we improve over the best performing systems, for one emotion we obtain the same results, and for one</context>
</contexts>
<marker>Chaffar, Inkpen, 2011</marker>
<rawString>S. Chaffar and D. Inkpen. 2011. Using a heterogeneous dataset for emotion analysis in text. In Advances in Artificial Intelligence, pages 62–67. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M De Choudhury</author>
<author>S Counts</author>
<author>M Gamon</author>
</authors>
<title>Not all moods are created equal! exploring human emotional states in social media.</title>
<date>2012</date>
<booktitle>In Proceedings of the International Conference on Weblogs and Social Media (ICWSM).</booktitle>
<marker>De Choudhury, Counts, Gamon, 2012</marker>
<rawString>M. De Choudhury, S. Counts, and M. Gamon. 2012. Not all moods are created equal! exploring human emotional states in social media. In Proceedings of the International Conference on Weblogs and Social Media (ICWSM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Ekman</author>
<author>W V Friesen</author>
</authors>
<title>Constants across cultures in the face and emotion.</title>
<date>1971</date>
<journal>Journal of Personality and Social Psychology,</journal>
<pages>17--124</pages>
<contexts>
<context position="16327" citStr="Ekman and Friesen, 1971" startWordPosition="2644" endWordPosition="2648">is dataset is of interest to us since the ‘compositional’ problem is less prominent given the simplified syntax of news headlines, containing, for example, fewer adverbs (like negations or intensifiers) than normal sentences (Turchi et al., 2012). Furthermore, this is to our knowledge the only dataset available providing numerical scores for emotions. Finally, this dataset was meant for unsupervised approaches (just a small trial sample was provided), so to avoid simple text categorization approaches. As the affective dimensions present in the test set – based on the six basic emotions model (Ekman and Friesen, 1971) – do not exactly match with the ones provided by Rappler’s Mood Meter, we first define a mapping between the two when possible, see Table 4. Then, we proceed to transform the test headlines to the lemma#PoS format. SemEval Rappler SemEval Rappler FEAR AFRAID SURPRISE INSPIRED ANGER ANGRY - ANNOYED JOY HAPPY - AMUSED SADNESS SAD - DON’T CARE Table 4: Mapping of Rappler labels on Semeval2007. In bold, cases of suboptimal mapping. Only one test headline contained exclusively words not present in DepecheMood, further indi430 cating the high-coverage nature of our resource. In Table 5 we report th</context>
</contexts>
<marker>Ekman, Friesen, 1971</marker>
<rawString>P. Ekman and W. V. Friesen. 1971. Constants across cultures in the face and emotion. Journal of Personality and Social Psychology, 17:124–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Esuli</author>
<author>F Sebastiani</author>
</authors>
<title>SentiWordNet: A publicly available lexical resource for opinion mining.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference on International Language Resources and Evaluation (LREC),</booktitle>
<pages>417--422</pages>
<location>Genova, IT.</location>
<contexts>
<context position="6519" citStr="Esuli and Sebastiani, 2006" startWordPosition="1025" endWordPosition="1028">building sentiment and emotion lexica, regardless of the approach in which such lists are then used (machine learning, rule based or deep learning). A general overview can be found in (Pang and Lee, 2008; Liu and Zhang, 2012; Wilson et al., 2004; Paltoglou et al., 2010). Sentiment Lexica. In recent years there has been an increasing focus on producing lists of words (lexica) with prior polarities, to be used in sentiment analysis. When building such lists, a trade-off between coverage of the resource and its precision is to be found. One of the most well-known resources is SentiWordNet (SWN) (Esuli and Sebastiani, 2006; Baccianella et al., 2010), in which each entry is associated with the numerical scores Pos(s) and Neg(s), ranging from 0 to 1. These scores – automatically assigned starting from a bunch of seed terms – represent the positive and negative valence (or posterior polarity) of each entry, that takes the form lemma#pos#sense-number. Starting from SWN, several prior polarities for words (SWN-prior), in the form lemma#PoS, can be computed (e.g. considering only the firstsense, averaging on all the senses, etc.). These approaches, detailed in (Guerini et al., 2013), produce a list of 155k words, whe</context>
</contexts>
<marker>Esuli, Sebastiani, 2006</marker>
<rawString>A. Esuli and F. Sebastiani. 2006. SentiWordNet: A publicly available lexical resource for opinion mining. In Proceedings of the Conference on International Language Resources and Evaluation (LREC), pages 417–422, Genova, IT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Guerini</author>
<author>O Stock</author>
<author>C Strapparava</author>
</authors>
<title>Valentino: A tool for valence shifting of natural language texts.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on International Language Resources and Evaluation (LREC),</booktitle>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="2813" citStr="Guerini et al., 2008" startWordPosition="436" endWordPosition="439">ated with their prior polarity, i.e. whether such word out of context evokes something positive or something negative. For example, wonderful has a positive connotation – prior polarity – while horrible has a negative one. The quest for a high precision and high coverage lexicon, where words are associated with either sentiment or emotion scores, has several reasons. First, it is fundamental for tasks such as affective modification of existing texts, where words’ polarity together with their score are necessary for creating multiple graded variations of the original text (Inkpen et al., 2006; Guerini et al., 2008; Whitehead and Cavedon, 2010). Second, considering word order makes a difference in sentiment analysis. This calls for a role of compositionality, where the score of a sentence is computed by composing the scores of the words up in the syntactic tree. Works worth mentioning in this connection are: Socher et al. (2013), which uses recursive neural networks to learn compositional rules for sentiment analysis, and (Neviarouskaya et al., 2009; Neviarouskaya et al., 2011) which exploit hand-coded rules to compose the emotions expressed by words in a sentence. In this respect, compositional approac</context>
</contexts>
<marker>Guerini, Stock, Strapparava, 2008</marker>
<rawString>M. Guerini, O. Stock, and C. Strapparava. 2008. Valentino: A tool for valence shifting of natural language texts. In Proceedings of the Conference on International Language Resources and Evaluation (LREC), Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Guerini</author>
<author>L Gatti</author>
<author>M Turchi</author>
</authors>
<title>Sentiment analysis: How to derive prior polarities from sentiwordnet.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1259--1269</pages>
<contexts>
<context position="7084" citStr="Guerini et al., 2013" startWordPosition="1116" endWordPosition="1119">ources is SentiWordNet (SWN) (Esuli and Sebastiani, 2006; Baccianella et al., 2010), in which each entry is associated with the numerical scores Pos(s) and Neg(s), ranging from 0 to 1. These scores – automatically assigned starting from a bunch of seed terms – represent the positive and negative valence (or posterior polarity) of each entry, that takes the form lemma#pos#sense-number. Starting from SWN, several prior polarities for words (SWN-prior), in the form lemma#PoS, can be computed (e.g. considering only the firstsense, averaging on all the senses, etc.). These approaches, detailed in (Guerini et al., 2013), produce a list of 155k words, where the lower precision given by the automatic scoring of SWN is compensated by the high coverage. Another widely used resource is ANEW (Bradley and Lang, 1999), providing valence scores for 1k words, which were manually assigned by several annotators. This resource has a low coverage, but the precision is maximized. Similarly, the SO-CAL entries (Taboada et al., 2011) were manually tagged by a small number of annotators with a multi-class label (from very negative to very positive). These ratings were further validated through crowdsourcing, ending up with a </context>
</contexts>
<marker>Guerini, Gatti, Turchi, 2013</marker>
<rawString>M. Guerini, L. Gatti, and M. Turchi. 2013. Sentiment analysis: How to derive prior polarities from sentiwordnet. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1259–1269.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Z Inkpen</author>
<author>O Feiguina</author>
<author>G Hirst</author>
</authors>
<title>Generating more-positive and more-negative text.</title>
<date>2006</date>
<booktitle>In Computing Attitude and Affect in Text: Theory andApplications,</booktitle>
<pages>187--198</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="2791" citStr="Inkpen et al., 2006" startWordPosition="432" endWordPosition="435">ica, words are associated with their prior polarity, i.e. whether such word out of context evokes something positive or something negative. For example, wonderful has a positive connotation – prior polarity – while horrible has a negative one. The quest for a high precision and high coverage lexicon, where words are associated with either sentiment or emotion scores, has several reasons. First, it is fundamental for tasks such as affective modification of existing texts, where words’ polarity together with their score are necessary for creating multiple graded variations of the original text (Inkpen et al., 2006; Guerini et al., 2008; Whitehead and Cavedon, 2010). Second, considering word order makes a difference in sentiment analysis. This calls for a role of compositionality, where the score of a sentence is computed by composing the scores of the words up in the syntactic tree. Works worth mentioning in this connection are: Socher et al. (2013), which uses recursive neural networks to learn compositional rules for sentiment analysis, and (Neviarouskaya et al., 2009; Neviarouskaya et al., 2011) which exploit hand-coded rules to compose the emotions expressed by words in a sentence. In this respect,</context>
</contexts>
<marker>Inkpen, Feiguina, Hirst, 2006</marker>
<rawString>D. Z. Inkpen, O. Feiguina, and G. Hirst. 2006. Generating more-positive and more-negative text. In Computing Attitude and Affect in Text: Theory andApplications, pages 187–198. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Liu</author>
<author>L Zhang</author>
</authors>
<title>A survey of opinion mining and sentiment analysis. Mining Text Data,</title>
<date>2012</date>
<pages>415--463</pages>
<contexts>
<context position="2158" citStr="Liu and Zhang, 2012" startWordPosition="330" endWordPosition="333">y not suffice, as in these examples: ‘I’m so miserable, I dropped my IPhone in the water and now it’s not working anymore’ (SADNESS) vs. ‘I am very upset, my new IPhone keeps not working!’ (ANGER). While both texts express a negative sentiment, the latter, connected to anger, is more relevant for buzz monitoring. Thus, emotion analysis represents a natural evolution of sentiment analysis. Many approaches to sentiment analysis make use of lexical resources – i.e. lists of positive and negative words – often deployed as baselines or as features for other methods, usually machine learning based (Liu and Zhang, 2012). In these lexica, words are associated with their prior polarity, i.e. whether such word out of context evokes something positive or something negative. For example, wonderful has a positive connotation – prior polarity – while horrible has a negative one. The quest for a high precision and high coverage lexicon, where words are associated with either sentiment or emotion scores, has several reasons. First, it is fundamental for tasks such as affective modification of existing texts, where words’ polarity together with their score are necessary for creating multiple graded variations of the o</context>
<context position="6117" citStr="Liu and Zhang, 2012" startWordPosition="959" endWordPosition="962">s network. We also evaluate our lexicon by integrating it in unsupervised classification and regression settings for emotion recognition. Results indicate that the use of our resource, even if automatically acquired, is highly beneficial in affective text recognition. 2 Related Work Within the broad field of sentiment analysis, we hereby provide a short review of research efforts put towards building sentiment and emotion lexica, regardless of the approach in which such lists are then used (machine learning, rule based or deep learning). A general overview can be found in (Pang and Lee, 2008; Liu and Zhang, 2012; Wilson et al., 2004; Paltoglou et al., 2010). Sentiment Lexica. In recent years there has been an increasing focus on producing lists of words (lexica) with prior polarities, to be used in sentiment analysis. When building such lists, a trade-off between coverage of the resource and its precision is to be found. One of the most well-known resources is SentiWordNet (SWN) (Esuli and Sebastiani, 2006; Baccianella et al., 2010), in which each entry is associated with the numerical scores Pos(s) and Neg(s), ranging from 0 to 1. These scores – automatically assigned starting from a bunch of seed t</context>
</contexts>
<marker>Liu, Zhang, 2012</marker>
<rawString>B. Liu and L. Zhang. 2012. A survey of opinion mining and sentiment analysis. Mining Text Data, pages 415–463.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Mishne</author>
</authors>
<title>Experiments with mood classification in blog posts.</title>
<date>2005</date>
<booktitle>In Proceedings of ACM SIGIR 2005 Workshop on Stylistic Analysis of Text for Information Access,</booktitle>
<volume>19</volume>
<contexts>
<context position="11157" citStr="Mishne, 2005" startWordPosition="1786" endWordPosition="1787">otes have been collected since the launch of the service. In our novel approach to ‘crowdsourcing’, as compared to other NLP tasks that rely on tools like Amazon’s Mechanical Turk (Snow et al., 2008), the subjects are aware of the ‘implicit annotation task’ but they are not paid. From this data, we built a document-by-emotion matrix MDE, providing the voting percentages for each document in the eight 1http://nie.mn/QuD17Z affective dimensions available in Rappler. An excerpt is provided in Table 1. The idea of using documents annotated with emotions is not new (Strapparava and Mihalcea, 2008; Mishne, 2005; Bellegarda, 2010), but these works had the limitation of providing a single emotion label per document, rather than a score for each emotion, and, moreover, the annotation was performed by the author of the document alone. Table 2 reports the average percentage of votes for each emotion on the whole corpus: HAPPINESS has a far higher percentage of votes (at least three times). There are several possible explanations, out of the scope of the present paper, for this bias: (i) it is due to cultural characteristics of the audience (ii) the bias is in the dataset itself, being formed mainly by ‘p</context>
</contexts>
<marker>Mishne, 2005</marker>
<rawString>G. Mishne. 2005. Experiments with mood classification in blog posts. In Proceedings of ACM SIGIR 2005 Workshop on Stylistic Analysis of Text for Information Access, volume 19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Mohammad</author>
<author>P D Turney</author>
</authors>
<title>Crowdsourcing a word–emotion association lexicon.</title>
<date>2013</date>
<journal>Computational Intelligence,</journal>
<volume>29</volume>
<issue>3</issue>
<contexts>
<context position="9529" citStr="Mohammad and Turney, 2013" startWordPosition="1513" endWordPosition="1516"> 0.06 0.02 0.31 0.40 0.00 0.00 0.20 0.00 0.20 0.20 0.00 0.00 0.30 0.08 0.00 0.00 0.23 0.31 0.08 doc 10002 doc 10003 doc 10004 doc 10011 doc 10028 lemma#PoS#sense, corresponding to roughly 1 thousand lemma#PoS. AffectNet, part of the SenticNet project (Cambria and Hussain, 2012), contains 10k words (out of 23k entries) taken from ConceptNet and aligned with WordNetAffect. This resource extends WordNetAffect labels to concepts like ‘have breakfast’. Fuzzy Affect Lexicon (Subasic and Huettner, 2001) contains roughly 4k lemma#PoS manually annotated by one linguist using 80 emotion labels. EmoLex (Mohammad and Turney, 2013) contains almost 10k lemmas annotated with an intensity label for each emotion using Mechanical Turk. Finally Affect database is an extension of SentiFul (Neviarouskaya et al., 2007) and contains 2.5K words in the form lemma#PoS. The latter is the only lexicon providing words annotated also with emotion scores rather than only with labels. 3 Dataset Collection To build our emotion lexicon we harvested all the news articles from rappler.com, as of June 3rd 2013: the final dataset consists of 13.5 M words over 25.3 K documents, with an average of 530 words per document. For each document, along </context>
</contexts>
<marker>Mohammad, Turney, 2013</marker>
<rawString>S. M. Mohammad and P. D. Turney. 2013. Crowdsourcing a word–emotion association lexicon. Computational Intelligence, 29(3):436–465.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Mohammad</author>
</authors>
<title>Emotional tweets.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics (*Sem),</booktitle>
<pages>246--255</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="19147" citStr="Mohammad, 2012" startWordPosition="3108" endWordPosition="3109">at adopting more complex approaches (i.e. compositionality) can possibly further improve performances in textbased emotion recognition. As a final test, we evaluate our resource in the classification task. The naive approach used in this case consists in mapping the average of the scores of all words in the headline to a binary decision with fixed threshold at 0.5 for each emotion (after min-max normalization on all test headlines 3Systems participating in the ‘Affective Text’ task plus the approaches in (Strapparava and Mihalcea, 2008). Other supervised approaches in the classification task (Mohammad, 2012; Bellegarda, 2010; Chaffar and Inkpen, 2011), reporting only overall performances, are not considered. DepecheMood best., upper f of tfidf FEAR 0.56 0.54 0.53 0.45 0.64 ANGER 0.36 0.38 0.36 0.32 0.50 SURPRISE* 0.25 0.21 0.24 0.16 0.36 JOY 0.39 0.40 0.39 0.26 0.60 SADNESS 0.48 0.47 0.46 0.41 0.68 Table 6: Regression results – Pearson’s correlation scores). In Table 7 we report the results (F1 measure) of our approach along with the best performance of other systems on each emotion (bests,), as in the previous case. For 3 emotions out of 5 we improve over the best performing systems, for one em</context>
</contexts>
<marker>Mohammad, 2012</marker>
<rawString>S. M. Mohammad. 2012. # Emotional tweets. In Proceedings of the First Joint Conference on Lexical and Computational Semantics (*Sem), pages 246– 255. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Neviarouskaya</author>
<author>H Prendinger</author>
<author>M Ishizuka</author>
</authors>
<title>Textual affect sensing for sociable and expressive online communication.</title>
<date>2007</date>
<booktitle>Affective Computing and Intelligent Interaction,</booktitle>
<volume>4738</volume>
<pages>218--229</pages>
<editor>In A. Paiva, R. Prada, and R. Picard, editors,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="9711" citStr="Neviarouskaya et al., 2007" startWordPosition="1542" endWordPosition="1545">oughly 1 thousand lemma#PoS. AffectNet, part of the SenticNet project (Cambria and Hussain, 2012), contains 10k words (out of 23k entries) taken from ConceptNet and aligned with WordNetAffect. This resource extends WordNetAffect labels to concepts like ‘have breakfast’. Fuzzy Affect Lexicon (Subasic and Huettner, 2001) contains roughly 4k lemma#PoS manually annotated by one linguist using 80 emotion labels. EmoLex (Mohammad and Turney, 2013) contains almost 10k lemmas annotated with an intensity label for each emotion using Mechanical Turk. Finally Affect database is an extension of SentiFul (Neviarouskaya et al., 2007) and contains 2.5K words in the form lemma#PoS. The latter is the only lexicon providing words annotated also with emotion scores rather than only with labels. 3 Dataset Collection To build our emotion lexicon we harvested all the news articles from rappler.com, as of June 3rd 2013: the final dataset consists of 13.5 M words over 25.3 K documents, with an average of 530 words per document. For each document, along with the text we also harvested the information displayed by Rappler’s Mood Meter, a small interface offering the readers the opportunity to click on the emotion that a given Rappler</context>
</contexts>
<marker>Neviarouskaya, Prendinger, Ishizuka, 2007</marker>
<rawString>A. Neviarouskaya, H. Prendinger, and M. Ishizuka. 2007. Textual affect sensing for sociable and expressive online communication. In A. Paiva, R. Prada, and R. Picard, editors, Affective Computing and Intelligent Interaction, volume 4738 of Lecture Notes in Computer Science, pages 218–229. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Neviarouskaya</author>
<author>H Prendinger</author>
<author>M Ishizuka</author>
</authors>
<title>Compositionality principle in recognition of fine-grained emotions from text.</title>
<date>2009</date>
<booktitle>In Proceedings of the International Conference on Weblogs and Social Media (ICWSM).</booktitle>
<contexts>
<context position="3256" citStr="Neviarouskaya et al., 2009" startWordPosition="507" endWordPosition="510">f existing texts, where words’ polarity together with their score are necessary for creating multiple graded variations of the original text (Inkpen et al., 2006; Guerini et al., 2008; Whitehead and Cavedon, 2010). Second, considering word order makes a difference in sentiment analysis. This calls for a role of compositionality, where the score of a sentence is computed by composing the scores of the words up in the syntactic tree. Works worth mentioning in this connection are: Socher et al. (2013), which uses recursive neural networks to learn compositional rules for sentiment analysis, and (Neviarouskaya et al., 2009; Neviarouskaya et al., 2011) which exploit hand-coded rules to compose the emotions expressed by words in a sentence. In this respect, compositional approaches represent a new promising trend, since all other approaches, either using semantic similarity or Bag-of-Words (BOW) based machine-learning, cannot handle, for example, cases of texts with same wording but different words order: “The dangerous killer escaped one month ago, but recently he was arrested” (RELIEF, HAPPYNESS) vs. “The dangerous killer was arrested one month ago, but re427 Proceedings of the 52nd Annual Meeting of the Associ</context>
</contexts>
<marker>Neviarouskaya, Prendinger, Ishizuka, 2009</marker>
<rawString>A. Neviarouskaya, H. Prendinger, and M. Ishizuka. 2009. Compositionality principle in recognition of fine-grained emotions from text. In Proceedings of the International Conference on Weblogs and Social Media (ICWSM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Neviarouskaya</author>
<author>H Prendinger</author>
<author>M Ishizuka</author>
</authors>
<title>Affect analysis model: novel rule-based approach to affect sensing from text.</title>
<date>2011</date>
<journal>Natural Language Engineering,</journal>
<volume>17</volume>
<issue>1</issue>
<contexts>
<context position="3285" citStr="Neviarouskaya et al., 2011" startWordPosition="511" endWordPosition="514">s’ polarity together with their score are necessary for creating multiple graded variations of the original text (Inkpen et al., 2006; Guerini et al., 2008; Whitehead and Cavedon, 2010). Second, considering word order makes a difference in sentiment analysis. This calls for a role of compositionality, where the score of a sentence is computed by composing the scores of the words up in the syntactic tree. Works worth mentioning in this connection are: Socher et al. (2013), which uses recursive neural networks to learn compositional rules for sentiment analysis, and (Neviarouskaya et al., 2009; Neviarouskaya et al., 2011) which exploit hand-coded rules to compose the emotions expressed by words in a sentence. In this respect, compositional approaches represent a new promising trend, since all other approaches, either using semantic similarity or Bag-of-Words (BOW) based machine-learning, cannot handle, for example, cases of texts with same wording but different words order: “The dangerous killer escaped one month ago, but recently he was arrested” (RELIEF, HAPPYNESS) vs. “The dangerous killer was arrested one month ago, but re427 Proceedings of the 52nd Annual Meeting of the Association for Computational Lingu</context>
</contexts>
<marker>Neviarouskaya, Prendinger, Ishizuka, 2011</marker>
<rawString>A. Neviarouskaya, H. Prendinger, and M. Ishizuka. 2011. Affect analysis model: novel rule-based approach to affect sensing from text. Natural Language Engineering, 17(1):95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Ozbal</author>
<author>C Strapparava</author>
</authors>
<title>A computational approach to the automation of creative naming.</title>
<date>2012</date>
<booktitle>Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="4486" citStr="Ozbal and Strapparava, 2012" startWordPosition="695" endWordPosition="698">on for Computational Linguistics (Short Papers), pages 427–433, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics cently he escaped” (FEAR). The work in (Wang and Manning, 2012) partially accounts for this problem and argues that using word bigram features allows improving over BOW based methods, where words are taken as features in isolation. This way it is possible to capture simple compositional phenomena like polarity reversing in “killing cancer”. Finally, tasks such as copywriting, where evocative names are a key element to a successful product (Ozbal and Strapparava, 2012; Ozbal et al., 2012) require exhaustive lists of emotion related words. In such cases no context is given and the brand name alone, with its perceived prior polarity, is responsible for stating the area of competition and evoking semantic associations. For example Mitsubishi changed the name of one of its SUVs for the Spanish market, since the original name Pajero had a very negative prior polarity, as it means ‘wanker’ in Spanish (Piller, 2003). Evoking emotions is also fundamental for a successful name: consider names of a perfume like Obsession, or technological products like MacBook air. </context>
</contexts>
<marker>Ozbal, Strapparava, 2012</marker>
<rawString>G. Ozbal and C. Strapparava. 2012. A computational approach to the automation of creative naming. Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Ozbal</author>
<author>C Strapparava</author>
<author>M Guerini</author>
</authors>
<title>Brand pitt: A corpus to explore the art of naming.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference on International Language Resources and Evaluation (LREC).</booktitle>
<contexts>
<context position="4507" citStr="Ozbal et al., 2012" startWordPosition="699" endWordPosition="702">ics (Short Papers), pages 427–433, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics cently he escaped” (FEAR). The work in (Wang and Manning, 2012) partially accounts for this problem and argues that using word bigram features allows improving over BOW based methods, where words are taken as features in isolation. This way it is possible to capture simple compositional phenomena like polarity reversing in “killing cancer”. Finally, tasks such as copywriting, where evocative names are a key element to a successful product (Ozbal and Strapparava, 2012; Ozbal et al., 2012) require exhaustive lists of emotion related words. In such cases no context is given and the brand name alone, with its perceived prior polarity, is responsible for stating the area of competition and evoking semantic associations. For example Mitsubishi changed the name of one of its SUVs for the Spanish market, since the original name Pajero had a very negative prior polarity, as it means ‘wanker’ in Spanish (Piller, 2003). Evoking emotions is also fundamental for a successful name: consider names of a perfume like Obsession, or technological products like MacBook air. In this work, we aim </context>
</contexts>
<marker>Ozbal, Strapparava, Guerini, 2012</marker>
<rawString>G. Ozbal, C. Strapparava, and M. Guerini. 2012. Brand pitt: A corpus to explore the art of naming. In Proceedings of the Conference on International Language Resources and Evaluation (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Paltoglou</author>
<author>M Thelwall</author>
<author>K Buckley</author>
</authors>
<title>Online textual communications annotated with grades of emotion strength.</title>
<date>2010</date>
<booktitle>In Proceedings of the 3rd International Workshop of Emotion: Corpora for research on Emotion and Affect,</booktitle>
<pages>25--31</pages>
<contexts>
<context position="6163" citStr="Paltoglou et al., 2010" startWordPosition="967" endWordPosition="970"> integrating it in unsupervised classification and regression settings for emotion recognition. Results indicate that the use of our resource, even if automatically acquired, is highly beneficial in affective text recognition. 2 Related Work Within the broad field of sentiment analysis, we hereby provide a short review of research efforts put towards building sentiment and emotion lexica, regardless of the approach in which such lists are then used (machine learning, rule based or deep learning). A general overview can be found in (Pang and Lee, 2008; Liu and Zhang, 2012; Wilson et al., 2004; Paltoglou et al., 2010). Sentiment Lexica. In recent years there has been an increasing focus on producing lists of words (lexica) with prior polarities, to be used in sentiment analysis. When building such lists, a trade-off between coverage of the resource and its precision is to be found. One of the most well-known resources is SentiWordNet (SWN) (Esuli and Sebastiani, 2006; Baccianella et al., 2010), in which each entry is associated with the numerical scores Pos(s) and Neg(s), ranging from 0 to 1. These scores – automatically assigned starting from a bunch of seed terms – represent the positive and negative val</context>
</contexts>
<marker>Paltoglou, Thelwall, Buckley, 2010</marker>
<rawString>G. Paltoglou, M. Thelwall, and K. Buckley. 2010. Online textual communications annotated with grades of emotion strength. In Proceedings of the 3rd International Workshop of Emotion: Corpora for research on Emotion and Affect, pages 25–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<booktitle>Foundations and Trends in Information Retrieval,</booktitle>
<pages>2--1</pages>
<contexts>
<context position="6096" citStr="Pang and Lee, 2008" startWordPosition="955" endWordPosition="958">ppler.com social news network. We also evaluate our lexicon by integrating it in unsupervised classification and regression settings for emotion recognition. Results indicate that the use of our resource, even if automatically acquired, is highly beneficial in affective text recognition. 2 Related Work Within the broad field of sentiment analysis, we hereby provide a short review of research efforts put towards building sentiment and emotion lexica, regardless of the approach in which such lists are then used (machine learning, rule based or deep learning). A general overview can be found in (Pang and Lee, 2008; Liu and Zhang, 2012; Wilson et al., 2004; Paltoglou et al., 2010). Sentiment Lexica. In recent years there has been an increasing focus on producing lists of words (lexica) with prior polarities, to be used in sentiment analysis. When building such lists, a trade-off between coverage of the resource and its precision is to be found. One of the most well-known resources is SentiWordNet (SWN) (Esuli and Sebastiani, 2006; Baccianella et al., 2010), in which each entry is associated with the numerical scores Pos(s) and Neg(s), ranging from 0 to 1. These scores – automatically assigned starting f</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>B. Pang and L. Lee. 2008. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval, 2(1-2):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Piller</author>
</authors>
<title>10. advertising as a site of language contact.</title>
<date>2003</date>
<journal>Annual Review of Applied Linguistics,</journal>
<pages>23--170</pages>
<contexts>
<context position="4936" citStr="Piller, 2003" startWordPosition="774" endWordPosition="775">ity reversing in “killing cancer”. Finally, tasks such as copywriting, where evocative names are a key element to a successful product (Ozbal and Strapparava, 2012; Ozbal et al., 2012) require exhaustive lists of emotion related words. In such cases no context is given and the brand name alone, with its perceived prior polarity, is responsible for stating the area of competition and evoking semantic associations. For example Mitsubishi changed the name of one of its SUVs for the Spanish market, since the original name Pajero had a very negative prior polarity, as it means ‘wanker’ in Spanish (Piller, 2003). Evoking emotions is also fundamental for a successful name: consider names of a perfume like Obsession, or technological products like MacBook air. In this work, we aim at automatically producing a high coverage and high precision emotion lexicon using distributional semantics, with numerical scores associated with each emotion, like it has already been done for sentiment analysis. To this end, we take advantage in an original way of massive crowd-sourced affective annotations associated with news articles, obtained by crawling the rappler.com social news network. We also evaluate our lexico</context>
</contexts>
<marker>Piller, 2003</marker>
<rawString>I. Piller. 2003. 10. advertising as a site of language contact. Annual Review of Applied Linguistics, 23:170–183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Quercia</author>
<author>J Ellis</author>
<author>L Capra</author>
<author>J Crowcroft</author>
</authors>
<title>In the mood for being influential on twitter.</title>
<date>2011</date>
<booktitle>Proceedings of IEEE SocialCom’11.</booktitle>
<contexts>
<context position="11915" citStr="Quercia et al., 2011" startWordPosition="1916" endWordPosition="1919">tion, and, moreover, the annotation was performed by the author of the document alone. Table 2 reports the average percentage of votes for each emotion on the whole corpus: HAPPINESS has a far higher percentage of votes (at least three times). There are several possible explanations, out of the scope of the present paper, for this bias: (i) it is due to cultural characteristics of the audience (ii) the bias is in the dataset itself, being formed mainly by ‘positive’ news; (iii) it is a psychological phenomenon due to the fact that people tend to express more positive moods on social networks (Quercia et al., 2011; Vittengl and Holt, 1998; De Choudhury et al., 2012). In any case, the predominance of happy mood has been found in other datasets, for instance LiveJournal.com posts (Strapparava and Mihalcea, 2008). In the following section we will discuss how we handled this problem. EMOTION Votes,, EMOTION Votes, AFRAID 0.04 DONT CARE 0.05 AMUSED 0.10 HAPPY 0.32 ANGRY 0.10 INSPIRED 0.10 ANNOYED 0.06 SAD 0.11 Table 2: Average percentages of votes. 4 Emotion Lexicon Creation As a next step we built a word-by-emotion matrix starting from MDE using an approach based on compositional semantics. To do so, we fi</context>
</contexts>
<marker>Quercia, Ellis, Capra, Crowcroft, 2011</marker>
<rawString>D. Quercia, J. Ellis, L. Capra, and J. Crowcroft. 2011. In the mood for being influential on twitter. Proceedings of IEEE SocialCom’11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Snow</author>
<author>B O’Connor</author>
<author>D Jurafsky</author>
<author>A Ng</author>
</authors>
<title>Cheap and fast—but is it good?: evaluating nonexpert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>254--263</pages>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>R. Snow, B. O’Connor, D. Jurafsky, and A. Ng. 2008. Cheap and fast—but is it good?: evaluating nonexpert annotations for natural language tasks. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 254–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Socher</author>
<author>A Perelygin</author>
<author>J Y Wu</author>
<author>J Chuang</author>
<author>C D Manning</author>
<author>A Y Ng</author>
<author>C Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1631--1642</pages>
<contexts>
<context position="3133" citStr="Socher et al. (2013)" startWordPosition="490" endWordPosition="493">sentiment or emotion scores, has several reasons. First, it is fundamental for tasks such as affective modification of existing texts, where words’ polarity together with their score are necessary for creating multiple graded variations of the original text (Inkpen et al., 2006; Guerini et al., 2008; Whitehead and Cavedon, 2010). Second, considering word order makes a difference in sentiment analysis. This calls for a role of compositionality, where the score of a sentence is computed by composing the scores of the words up in the syntactic tree. Works worth mentioning in this connection are: Socher et al. (2013), which uses recursive neural networks to learn compositional rules for sentiment analysis, and (Neviarouskaya et al., 2009; Neviarouskaya et al., 2011) which exploit hand-coded rules to compose the emotions expressed by words in a sentence. In this respect, compositional approaches represent a new promising trend, since all other approaches, either using semantic similarity or Bag-of-Words (BOW) based machine-learning, cannot handle, for example, cases of texts with same wording but different words order: “The dangerous killer escaped one month ago, but recently he was arrested” (RELIEF, HAPP</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>R. Socher, A. Perelygin, J. Y. Wu, J. Chuang, C. D. Manning, A. Y. Ng, and C. Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1631–1642.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Stone</author>
<author>D Dunphy</author>
<author>M Smith</author>
</authors>
<title>The General Inquirer: A Computer Approach to Content Analysis.</title>
<date>1966</date>
<publisher>MIT press.</publisher>
<contexts>
<context position="8141" citStr="Stone et al., 1966" startWordPosition="1290" endWordPosition="1293">number of annotators with a multi-class label (from very negative to very positive). These ratings were further validated through crowdsourcing, ending up with a list of roughly 4k words. More recently, a resource that replicated ANEW annotation approach using crowdsourcing, was released (Warriner et al., 2013), providing sentiment scores for 14k words. Interestingly, this resource annotates the most frequent words in English, so, even if lexicon coverage is still far lower than SWN-prior, it grants a high coverage, with human precision, of language use. Finally, the General Inquirer lexicon (Stone et al., 1966) provides a binary classification (positive/negative) of 4k sentimentbearing words, while the resource in (Wilson et al., 2005) expands the General Inquirer to 6k words. Emotion Lexica. Compared to sentiment lexica, far less emotion lexica have been produced, and all have lower coverage. One of the most used resources is WordNetAffect (Strapparava and Valitutti, 2004) which contains manually assigned affective labels to WordNet synsets (ANGER, JOY, FEAR, etc.). It currently provides 900 annotated synsets and 1.6k words in the form 428 Table 1: An excerpt of the Document-by-Emotion Matrix - MDE</context>
</contexts>
<marker>Stone, Dunphy, Smith, 1966</marker>
<rawString>P. Stone, D. Dunphy, and M. Smith. 1966. The General Inquirer: A Computer Approach to Content Analysis. MIT press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Strapparava</author>
<author>R Mihalcea</author>
</authors>
<title>Semeval2007 task 14: Affective text.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations,</booktitle>
<pages>70--74</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="15062" citStr="Strapparava and Mihalcea, 2007" startWordPosition="2447" endWordPosition="2451"> given word has in the affective dimensions we consider. So, for example, awe#n has a predominant weight in INSPIRED (0.38), comical#a has a predominant weight in AMUSED (0.51), while kill#v has a predominant weight in AFRAID, ANGRY and SAD (0.23, 0.21 and 0.27 respectively). This matrix, that we call DepecheMood2, represents our emotion lexicon, it contains 37k entries and is freely available for research purposes at http://git.io/MqyoIg. 5 Experiments To evaluate the performance we can obtain with our lexicon, we use the public dataset provided for the SemEval 2007 task on ‘Affective Text’ (Strapparava and Mihalcea, 2007). The task was focused on emotion recognition in one thousand news headlines, both in regression and classification settings. Headlines typically consist of a few 2In French, ‘depeche’ means dispatch/news. words and are often written with the intention to ‘provoke’ emotions so to attract the readers’ attention. An example of headline from the dataset is the following: “Iraq car bombings kill 22 People, wound more than 60”. For the regression task the values provided are: &lt;anger (0.32), disgust (0.27), fear (0.84), joy (0.0), sadness (0.95), surprise (0.20)&gt; while for the classification task th</context>
</contexts>
<marker>Strapparava, Mihalcea, 2007</marker>
<rawString>C. Strapparava and R. Mihalcea. 2007. Semeval2007 task 14: Affective text. In Proceedings of the 4th International Workshop on Semantic Evaluations, pages 70–74. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Strapparava</author>
<author>R Mihalcea</author>
</authors>
<title>Learning to identify emotions in text.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 ACM symposium on Applied computing,</booktitle>
<pages>1556--1560</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="11143" citStr="Strapparava and Mihalcea, 2008" startWordPosition="1782" endWordPosition="1785">his way, hundreds of thousands votes have been collected since the launch of the service. In our novel approach to ‘crowdsourcing’, as compared to other NLP tasks that rely on tools like Amazon’s Mechanical Turk (Snow et al., 2008), the subjects are aware of the ‘implicit annotation task’ but they are not paid. From this data, we built a document-by-emotion matrix MDE, providing the voting percentages for each document in the eight 1http://nie.mn/QuD17Z affective dimensions available in Rappler. An excerpt is provided in Table 1. The idea of using documents annotated with emotions is not new (Strapparava and Mihalcea, 2008; Mishne, 2005; Bellegarda, 2010), but these works had the limitation of providing a single emotion label per document, rather than a score for each emotion, and, moreover, the annotation was performed by the author of the document alone. Table 2 reports the average percentage of votes for each emotion on the whole corpus: HAPPINESS has a far higher percentage of votes (at least three times). There are several possible explanations, out of the scope of the present paper, for this bias: (i) it is due to cultural characteristics of the audience (ii) the bias is in the dataset itself, being forme</context>
<context position="17697" citStr="Strapparava and Mihalcea, 2008" startWordPosition="2875" endWordPosition="2878">even if the number of entries of our lexicon is far lower than SWN-prior approaches, the fact that we extracted and annotated words from documents grants a high coverage of language use. ANEW 1k entries 0.10 Sentiment Warriner et. al 13k entries 0.51 Lexica SWN-prior 155k entries 0.67 Emotion WNAffect 1k entries 0.12 Lexica DepecheMood 37k entries 0.64 Table 5: Statistics on words coverage per headline. Since our primary goal is to assess the quality of DepecheMood we first focus on the regression task. We do so by using a very naive approach, similar to “WordNetAffect presence” discussed in (Strapparava and Mihalcea, 2008): for each headline, we simply compute a value, for any affective dimension, by averaging the corresponding affective scores –obtained from DepecheMood- of all lemma#PoS present in the headline. In Table 6 we report the results obtained using the three versions of our resource (Pearson correlation), along with the best performance on each emotion of other systems3 (bests,); the last column contains the upper bound of inter-annotator agreement. For all the 5 emotions we improve over the best performing systems (DISGUST has no alignment with our labels and was discarded). Interestingly, even usi</context>
<context position="19075" citStr="Strapparava and Mihalcea, 2008" startWordPosition="3096" endWordPosition="3099">lude that the quality and coverage of our resource are the reason of such results, and that adopting more complex approaches (i.e. compositionality) can possibly further improve performances in textbased emotion recognition. As a final test, we evaluate our resource in the classification task. The naive approach used in this case consists in mapping the average of the scores of all words in the headline to a binary decision with fixed threshold at 0.5 for each emotion (after min-max normalization on all test headlines 3Systems participating in the ‘Affective Text’ task plus the approaches in (Strapparava and Mihalcea, 2008). Other supervised approaches in the classification task (Mohammad, 2012; Bellegarda, 2010; Chaffar and Inkpen, 2011), reporting only overall performances, are not considered. DepecheMood best., upper f of tfidf FEAR 0.56 0.54 0.53 0.45 0.64 ANGER 0.36 0.38 0.36 0.32 0.50 SURPRISE* 0.25 0.21 0.24 0.16 0.36 JOY 0.39 0.40 0.39 0.26 0.60 SADNESS 0.48 0.47 0.46 0.41 0.68 Table 6: Regression results – Pearson’s correlation scores). In Table 7 we report the results (F1 measure) of our approach along with the best performance of other systems on each emotion (bests,), as in the previous case. For 3 e</context>
</contexts>
<marker>Strapparava, Mihalcea, 2008</marker>
<rawString>C. Strapparava and R. Mihalcea. 2008. Learning to identify emotions in text. In Proceedings of the 2008 ACM symposium on Applied computing, pages 1556–1560. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Strapparava</author>
<author>A Valitutti</author>
</authors>
<title>WordNetAffect: an affective extension of WordNet.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on International Language Resources and Evaluation (LREC),</booktitle>
<pages>1083--1086</pages>
<location>Lisbon,</location>
<contexts>
<context position="8511" citStr="Strapparava and Valitutti, 2004" startWordPosition="1347" endWordPosition="1351">nterestingly, this resource annotates the most frequent words in English, so, even if lexicon coverage is still far lower than SWN-prior, it grants a high coverage, with human precision, of language use. Finally, the General Inquirer lexicon (Stone et al., 1966) provides a binary classification (positive/negative) of 4k sentimentbearing words, while the resource in (Wilson et al., 2005) expands the General Inquirer to 6k words. Emotion Lexica. Compared to sentiment lexica, far less emotion lexica have been produced, and all have lower coverage. One of the most used resources is WordNetAffect (Strapparava and Valitutti, 2004) which contains manually assigned affective labels to WordNet synsets (ANGER, JOY, FEAR, etc.). It currently provides 900 annotated synsets and 1.6k words in the form 428 Table 1: An excerpt of the Document-by-Emotion Matrix - MDE AFRAID AMUSED ANGRY ANNOYED DONT CARE HAPPY INSPIRED SAD 0.75 0.00 0.00 0.00 0.00 0.00 0.25 0.00 0.00 0.50 0.00 0.16 0.17 0.17 0.00 0.00 0.52 0.02 0.03 0.02 0.02 0.06 0.02 0.31 0.40 0.00 0.00 0.20 0.00 0.20 0.20 0.00 0.00 0.30 0.08 0.00 0.00 0.23 0.31 0.08 doc 10002 doc 10003 doc 10004 doc 10011 doc 10028 lemma#PoS#sense, corresponding to roughly 1 thousand lemma#PoS</context>
</contexts>
<marker>Strapparava, Valitutti, 2004</marker>
<rawString>C. Strapparava and A. Valitutti. 2004. WordNetAffect: an affective extension of WordNet. In Proceedings of the Conference on International Language Resources and Evaluation (LREC), pages 1083 – 1086, Lisbon, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Subasic</author>
<author>A Huettner</author>
</authors>
<title>Affect analysis of text using fuzzy semantic typing. Fuzzy Systems,</title>
<date>2001</date>
<journal>IEEE Transactions on,</journal>
<volume>9</volume>
<issue>4</issue>
<contexts>
<context position="9404" citStr="Subasic and Huettner, 2001" startWordPosition="1494" endWordPosition="1497">RE HAPPY INSPIRED SAD 0.75 0.00 0.00 0.00 0.00 0.00 0.25 0.00 0.00 0.50 0.00 0.16 0.17 0.17 0.00 0.00 0.52 0.02 0.03 0.02 0.02 0.06 0.02 0.31 0.40 0.00 0.00 0.20 0.00 0.20 0.20 0.00 0.00 0.30 0.08 0.00 0.00 0.23 0.31 0.08 doc 10002 doc 10003 doc 10004 doc 10011 doc 10028 lemma#PoS#sense, corresponding to roughly 1 thousand lemma#PoS. AffectNet, part of the SenticNet project (Cambria and Hussain, 2012), contains 10k words (out of 23k entries) taken from ConceptNet and aligned with WordNetAffect. This resource extends WordNetAffect labels to concepts like ‘have breakfast’. Fuzzy Affect Lexicon (Subasic and Huettner, 2001) contains roughly 4k lemma#PoS manually annotated by one linguist using 80 emotion labels. EmoLex (Mohammad and Turney, 2013) contains almost 10k lemmas annotated with an intensity label for each emotion using Mechanical Turk. Finally Affect database is an extension of SentiFul (Neviarouskaya et al., 2007) and contains 2.5K words in the form lemma#PoS. The latter is the only lexicon providing words annotated also with emotion scores rather than only with labels. 3 Dataset Collection To build our emotion lexicon we harvested all the news articles from rappler.com, as of June 3rd 2013: the final</context>
</contexts>
<marker>Subasic, Huettner, 2001</marker>
<rawString>P. Subasic and A. Huettner. 2001. Affect analysis of text using fuzzy semantic typing. Fuzzy Systems, IEEE Transactions on, 9(4):483–496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Taboada</author>
<author>J Brooke</author>
<author>M Tofiloski</author>
<author>K Voll</author>
<author>M Stede</author>
</authors>
<title>Lexicon-based methods for sentiment analysis.</title>
<date>2011</date>
<journal>Computational linguistics,</journal>
<pages>37--2</pages>
<contexts>
<context position="7489" citStr="Taboada et al., 2011" startWordPosition="1184" endWordPosition="1187">, several prior polarities for words (SWN-prior), in the form lemma#PoS, can be computed (e.g. considering only the firstsense, averaging on all the senses, etc.). These approaches, detailed in (Guerini et al., 2013), produce a list of 155k words, where the lower precision given by the automatic scoring of SWN is compensated by the high coverage. Another widely used resource is ANEW (Bradley and Lang, 1999), providing valence scores for 1k words, which were manually assigned by several annotators. This resource has a low coverage, but the precision is maximized. Similarly, the SO-CAL entries (Taboada et al., 2011) were manually tagged by a small number of annotators with a multi-class label (from very negative to very positive). These ratings were further validated through crowdsourcing, ending up with a list of roughly 4k words. More recently, a resource that replicated ANEW annotation approach using crowdsourcing, was released (Warriner et al., 2013), providing sentiment scores for 14k words. Interestingly, this resource annotates the most frequent words in English, so, even if lexicon coverage is still far lower than SWN-prior, it grants a high coverage, with human precision, of language use. Finall</context>
</contexts>
<marker>Taboada, Brooke, Tofiloski, Voll, Stede, 2011</marker>
<rawString>M. Taboada, J. Brooke, M. Tofiloski, K. Voll, and M. Stede. 2011. Lexicon-based methods for sentiment analysis. Computational linguistics, 37(2):267–307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Turchi</author>
<author>M Atkinson</author>
<author>A Wilcox</author>
<author>B Crawley</author>
<author>S Bucci</author>
<author>R Steinberger</author>
<author>E Van der Goot</author>
</authors>
<title>Onts: optima news translation system.</title>
<date>2012</date>
<booktitle>In Proceedings of the Demonstrations at the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>25--30</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Turchi, Atkinson, Wilcox, Crawley, Bucci, Steinberger, Van der Goot, 2012</marker>
<rawString>M. Turchi, M. Atkinson, A. Wilcox, B. Crawley, S. Bucci, R. Steinberger, and E. Van der Goot. 2012. Onts: optima news translation system. In Proceedings of the Demonstrations at the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 25–30. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Vittengl</author>
<author>C S Holt</author>
</authors>
<title>A time-series diary study of mood and social interaction. Motivation and Emotion,</title>
<date>1998</date>
<contexts>
<context position="11940" citStr="Vittengl and Holt, 1998" startWordPosition="1920" endWordPosition="1923">he annotation was performed by the author of the document alone. Table 2 reports the average percentage of votes for each emotion on the whole corpus: HAPPINESS has a far higher percentage of votes (at least three times). There are several possible explanations, out of the scope of the present paper, for this bias: (i) it is due to cultural characteristics of the audience (ii) the bias is in the dataset itself, being formed mainly by ‘positive’ news; (iii) it is a psychological phenomenon due to the fact that people tend to express more positive moods on social networks (Quercia et al., 2011; Vittengl and Holt, 1998; De Choudhury et al., 2012). In any case, the predominance of happy mood has been found in other datasets, for instance LiveJournal.com posts (Strapparava and Mihalcea, 2008). In the following section we will discuss how we handled this problem. EMOTION Votes,, EMOTION Votes, AFRAID 0.04 DONT CARE 0.05 AMUSED 0.10 HAPPY 0.32 ANGRY 0.10 INSPIRED 0.10 ANNOYED 0.06 SAD 0.11 Table 2: Average percentages of votes. 4 Emotion Lexicon Creation As a next step we built a word-by-emotion matrix starting from MDE using an approach based on compositional semantics. To do so, we first lemmatized and PoS ta</context>
</contexts>
<marker>Vittengl, Holt, 1998</marker>
<rawString>J. R. Vittengl and C. S. Holt. 1998. A time-series diary study of mood and social interaction. Motivation and Emotion, 22(3):255–275.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Wang</author>
<author>C Manning</author>
</authors>
<title>Baselines and bigrams: Simple, good sentiment and topic classification.</title>
<date>2012</date>
<booktitle>Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="4078" citStr="Wang and Manning, 2012" startWordPosition="628" endWordPosition="631"> all other approaches, either using semantic similarity or Bag-of-Words (BOW) based machine-learning, cannot handle, for example, cases of texts with same wording but different words order: “The dangerous killer escaped one month ago, but recently he was arrested” (RELIEF, HAPPYNESS) vs. “The dangerous killer was arrested one month ago, but re427 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 427–433, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics cently he escaped” (FEAR). The work in (Wang and Manning, 2012) partially accounts for this problem and argues that using word bigram features allows improving over BOW based methods, where words are taken as features in isolation. This way it is possible to capture simple compositional phenomena like polarity reversing in “killing cancer”. Finally, tasks such as copywriting, where evocative names are a key element to a successful product (Ozbal and Strapparava, 2012; Ozbal et al., 2012) require exhaustive lists of emotion related words. In such cases no context is given and the brand name alone, with its perceived prior polarity, is responsible for stati</context>
</contexts>
<marker>Wang, Manning, 2012</marker>
<rawString>S. Wang and C. Manning. 2012. Baselines and bigrams: Simple, good sentiment and topic classification. Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A B Warriner</author>
<author>V Kuperman</author>
<author>M Brysbaert</author>
</authors>
<title>Norms of valence, arousal, and dominance for 13,915 english lemmas. Behavior research methods,</title>
<date>2013</date>
<pages>45--4</pages>
<contexts>
<context position="7834" citStr="Warriner et al., 2013" startWordPosition="1240" endWordPosition="1243">age. Another widely used resource is ANEW (Bradley and Lang, 1999), providing valence scores for 1k words, which were manually assigned by several annotators. This resource has a low coverage, but the precision is maximized. Similarly, the SO-CAL entries (Taboada et al., 2011) were manually tagged by a small number of annotators with a multi-class label (from very negative to very positive). These ratings were further validated through crowdsourcing, ending up with a list of roughly 4k words. More recently, a resource that replicated ANEW annotation approach using crowdsourcing, was released (Warriner et al., 2013), providing sentiment scores for 14k words. Interestingly, this resource annotates the most frequent words in English, so, even if lexicon coverage is still far lower than SWN-prior, it grants a high coverage, with human precision, of language use. Finally, the General Inquirer lexicon (Stone et al., 1966) provides a binary classification (positive/negative) of 4k sentimentbearing words, while the resource in (Wilson et al., 2005) expands the General Inquirer to 6k words. Emotion Lexica. Compared to sentiment lexica, far less emotion lexica have been produced, and all have lower coverage. One </context>
<context position="17048" citStr="Warriner et al. (2013)" startWordPosition="2767" endWordPosition="2770">etween the two when possible, see Table 4. Then, we proceed to transform the test headlines to the lemma#PoS format. SemEval Rappler SemEval Rappler FEAR AFRAID SURPRISE INSPIRED ANGER ANGRY - ANNOYED JOY HAPPY - AMUSED SADNESS SAD - DON’T CARE Table 4: Mapping of Rappler labels on Semeval2007. In bold, cases of suboptimal mapping. Only one test headline contained exclusively words not present in DepecheMood, further indi430 cating the high-coverage nature of our resource. In Table 5 we report the coverage of some Sentiment and Emotion Lexica of different sizes on the same dataset. Similar to Warriner et al. (2013), we observe that even if the number of entries of our lexicon is far lower than SWN-prior approaches, the fact that we extracted and annotated words from documents grants a high coverage of language use. ANEW 1k entries 0.10 Sentiment Warriner et. al 13k entries 0.51 Lexica SWN-prior 155k entries 0.67 Emotion WNAffect 1k entries 0.12 Lexica DepecheMood 37k entries 0.64 Table 5: Statistics on words coverage per headline. Since our primary goal is to assess the quality of DepecheMood we first focus on the regression task. We do so by using a very naive approach, similar to “WordNetAffect presen</context>
</contexts>
<marker>Warriner, Kuperman, Brysbaert, 2013</marker>
<rawString>A. B. Warriner, V. Kuperman, and M. Brysbaert. 2013. Norms of valence, arousal, and dominance for 13,915 english lemmas. Behavior research methods, 45(4):1191–1207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Whitehead</author>
<author>L Cavedon</author>
</authors>
<title>Generating shifting sentiment for a conversational agent.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text,</booktitle>
<pages>89--97</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, CA,</location>
<contexts>
<context position="2843" citStr="Whitehead and Cavedon, 2010" startWordPosition="440" endWordPosition="443">polarity, i.e. whether such word out of context evokes something positive or something negative. For example, wonderful has a positive connotation – prior polarity – while horrible has a negative one. The quest for a high precision and high coverage lexicon, where words are associated with either sentiment or emotion scores, has several reasons. First, it is fundamental for tasks such as affective modification of existing texts, where words’ polarity together with their score are necessary for creating multiple graded variations of the original text (Inkpen et al., 2006; Guerini et al., 2008; Whitehead and Cavedon, 2010). Second, considering word order makes a difference in sentiment analysis. This calls for a role of compositionality, where the score of a sentence is computed by composing the scores of the words up in the syntactic tree. Works worth mentioning in this connection are: Socher et al. (2013), which uses recursive neural networks to learn compositional rules for sentiment analysis, and (Neviarouskaya et al., 2009; Neviarouskaya et al., 2011) which exploit hand-coded rules to compose the emotions expressed by words in a sentence. In this respect, compositional approaches represent a new promising </context>
</contexts>
<marker>Whitehead, Cavedon, 2010</marker>
<rawString>S. Whitehead and L. Cavedon. 2010. Generating shifting sentiment for a conversational agent. In Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 89–97, Los Angeles, CA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wilson</author>
<author>J Wiebe</author>
<author>R Hwa</author>
</authors>
<title>Just how mad are you? finding strong and weak opinion clauses.</title>
<date>2004</date>
<booktitle>In Proceedings of AAAI,</booktitle>
<pages>761--769</pages>
<contexts>
<context position="6138" citStr="Wilson et al., 2004" startWordPosition="963" endWordPosition="966">aluate our lexicon by integrating it in unsupervised classification and regression settings for emotion recognition. Results indicate that the use of our resource, even if automatically acquired, is highly beneficial in affective text recognition. 2 Related Work Within the broad field of sentiment analysis, we hereby provide a short review of research efforts put towards building sentiment and emotion lexica, regardless of the approach in which such lists are then used (machine learning, rule based or deep learning). A general overview can be found in (Pang and Lee, 2008; Liu and Zhang, 2012; Wilson et al., 2004; Paltoglou et al., 2010). Sentiment Lexica. In recent years there has been an increasing focus on producing lists of words (lexica) with prior polarities, to be used in sentiment analysis. When building such lists, a trade-off between coverage of the resource and its precision is to be found. One of the most well-known resources is SentiWordNet (SWN) (Esuli and Sebastiani, 2006; Baccianella et al., 2010), in which each entry is associated with the numerical scores Pos(s) and Neg(s), ranging from 0 to 1. These scores – automatically assigned starting from a bunch of seed terms – represent the </context>
</contexts>
<marker>Wilson, Wiebe, Hwa, 2004</marker>
<rawString>T. Wilson, J. Wiebe, and R. Hwa. 2004. Just how mad are you? finding strong and weak opinion clauses. In Proceedings of AAAI, pages 761–769.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Wilson</author>
<author>J Wiebe</author>
<author>P Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>347--354</pages>
<contexts>
<context position="8268" citStr="Wilson et al., 2005" startWordPosition="1309" endWordPosition="1312">ugh crowdsourcing, ending up with a list of roughly 4k words. More recently, a resource that replicated ANEW annotation approach using crowdsourcing, was released (Warriner et al., 2013), providing sentiment scores for 14k words. Interestingly, this resource annotates the most frequent words in English, so, even if lexicon coverage is still far lower than SWN-prior, it grants a high coverage, with human precision, of language use. Finally, the General Inquirer lexicon (Stone et al., 1966) provides a binary classification (positive/negative) of 4k sentimentbearing words, while the resource in (Wilson et al., 2005) expands the General Inquirer to 6k words. Emotion Lexica. Compared to sentiment lexica, far less emotion lexica have been produced, and all have lower coverage. One of the most used resources is WordNetAffect (Strapparava and Valitutti, 2004) which contains manually assigned affective labels to WordNet synsets (ANGER, JOY, FEAR, etc.). It currently provides 900 annotated synsets and 1.6k words in the form 428 Table 1: An excerpt of the Document-by-Emotion Matrix - MDE AFRAID AMUSED ANGRY ANNOYED DONT CARE HAPPY INSPIRED SAD 0.75 0.00 0.00 0.00 0.00 0.00 0.25 0.00 0.00 0.50 0.00 0.16 0.17 0.17</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>T. Wilson, J. Wiebe, and P. Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 347–354.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>