<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.149561">
<title confidence="0.985636">
Applying morphological decomposition to statistical machine translation
</title>
<author confidence="0.992342">
Sami Virpioja and Jaakko V¨ayrynen and Andr´e Mansikkaniemi and Mikko Kurimo
</author>
<affiliation confidence="0.9957825">
Aalto University School of Science and Technology
Department of Information and Computer Science
</affiliation>
<address confidence="0.932742">
PO BOX 15400, 00076 Aalto, Finland
</address>
<email confidence="0.999368">
{svirpioj,jjvayryn,ammansik,mikkok}@cis.hut.fi
</email>
<sectionHeader confidence="0.997396" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99985015">
This paper describes the Aalto submission
for the German-to-English and the Czech-
to-English translation tasks of the ACL
2010 Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR.
Statistical machine translation has focused
on using words, and longer phrases con-
structed from words, as tokens in the sys-
tem. In contrast, we apply different mor-
phological decompositions of words using
the unsupervised Morfessor algorithms.
While translation models trained using the
morphological decompositions did not im-
prove the BLEU scores, we show that the
Minimum Bayes Risk combination with
a word-based translation model produces
significant improvements for the German-
to-English translation. However, we did
not see improvements for the Czech-to-
English translations.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999937432432432">
The effect of morphological variation in languages
can be alleviated by using word analysis schemes,
which may include morpheme discovery, part-of-
speech tagging, or other linguistic information.
Words are very convenient and even efficient rep-
resentation in statistical natural language process-
ing, especially with English, but morphologically
rich languages can benefit from more fine-grained
information. For instance, statistical morphs dis-
covered with unsupervised methods result in bet-
ter performance in automatic speech recognition
for highly-inflecting and agglutinative languages
(Hirsim¨aki et al., 2006; Kurimo et al., 2006).
Virpioja et al. (2007) applied morph-based
models in statistical machine translation (SMT)
between several language pairs without gaining
improvement in BLEU score, but obtaining re-
ductions in out-of-vocabulary rates. They uti-
lized morphs both in the source and in the tar-
get language. Later, de Gispert et al. (2009)
showed that Minimum Bayes Risk (MBR) com-
bination of word-based and morph-based trans-
lation models improves translation with Arabic-
to-English and Finnish-to-English language pairs,
where only the source language utilized morph-
based models. Similar results have been shown for
Finnish-to-English and Finnish-to-German in per-
formance evaluation of various unsupervised mor-
pheme analysis algorithms in Morpho Challenge
2009 competition (Kurimo et al., 2009).
We continue the research described above and
examine how the level of decomposition affects
both the individual morph-based systems and
MBR combinations with the baseline word-based
model. Experiments are conducted with the
WMT10 shared task data for German-to-English
and Czech-to-English language pairs.
</bodyText>
<sectionHeader confidence="0.99616" genericHeader="introduction">
2 Methods
</sectionHeader>
<bodyText confidence="0.9999835">
In this work, morphological analyses are con-
ducted on the source language data, and each dif-
ferent analysis is applied to create a unique seg-
mentation of words into morphemes. Translation
systems are trained with the Moses toolkit (Koehn
et al., 2007) from each differently segmented ver-
sion of the same source language to the target lan-
guage. Evaluation with BLEU is performed on
both the individual systems and system combina-
tions, using different levels of decomposition.
</bodyText>
<subsectionHeader confidence="0.99337">
2.1 Morphological models for words
</subsectionHeader>
<bodyText confidence="0.999911142857143">
Morfessor (Creutz and Lagus, 2002; Creutz and
Lagus, 2007, etc.) is a family of methods for
unsupervised morphological segmentation. Mor-
fessor does not limit the number of morphemes
for each word, making it suitable for agglutina-
tive and compounding languages. An analysis of a
single word is a list of non-overlapping segments,
</bodyText>
<page confidence="0.986056">
195
</page>
<note confidence="0.453511">
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 195–200,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999915">
morphs, stored in the model lexicon. We use both
the Morfessor Baseline (Creutz and Lagus, 2005b)
and the Morfessor Categories-MAP (Creutz and
Lagus, 2005a) algorithms.1 Both are formulated
in a maximum a posteriori (MAP) framework, i.e.,
the learning algorithm tries to optimize the prod-
uct of the model prior and the data likelihood.
The generative model applied by Morfessor
Baseline assumes that the morphs are independent.
The resulting segmentation can be influenced by
using explicit priors for the morph lengths and
frequencies, but their effect is usually minimal.
The training data has a larger effect on the re-
sults: A larger data set allows a larger lexicon,
and thus longer morphs and less morphs per word
(Creutz and Lagus, 2007). Moreover, the model
can be trained with or without taking into account
the word frequencies. If the frequencies are in-
cluded, the more frequent words are usually un-
dersegmented compared to a linguistic analysis,
whereas the rare words are oversegmented (Creutz
and Lagus, 2005b). An easy way to control the
amount of segmentation is to weight the training
data likelihood by a positive factor α. If α &gt; 1,
the increased likelihood results in longer morphs.
If α &lt; 1, the morphs will be shorter and the words
more segmented.
Words that are not present in the training data
can be segmented using an algorithm similar to
Viterbi. The algorithm can be modified to allow
new morphs types to be used by using an approx-
imative cost of adding them into the lexicon (Vir-
pioja and Kohonen, 2009). The modification pre-
vents oversegmentation of unseen word forms. In
machine translation, this is important especially
for proper nouns, for which there is usually no
need for translation.
The Morfessor Categories-MAP algorithm ex-
tends the model by imposing morph categories of
stems, prefixes and suffixes, as well as transition
probabilities between them. In addition, it applies
a hierarchical segmentation model that allows it to
construct new stems from smaller pieces of “non-
morphemes” (Creutz and Lagus, 2007). Due to
these features, it can provide reasonable segmen-
tations also for those words that contain new mor-
phemes. The drawback of the more sophisticated
model is the slower and more complex training al-
gorithm. In addition, the amount of the segmenta-
</bodyText>
<footnote confidence="0.884832">
1The respective software is available at http://www.
cis.hut.fi/projects/morpho/
</footnote>
<bodyText confidence="0.999029833333334">
tion is harder to control.
Morfessor Categories-MAP was applied to sta-
tistical machine translation by Virpioja et al.
(2007) and de Gispert et al. (2009). However,
Kurimo et al. (2009) report that Morfessor Base-
line outperformed Categories-MAP in Finnish-to-
English and German-to-English tasks both with
and without MBR combination, although the dif-
ferences were not statistically significant. In all
the previous cases, the models were trained on
word types, i.e., without using their frequencies.
Here, we also test models trained on word tokens.
</bodyText>
<subsectionHeader confidence="0.999762">
2.2 Statistical machine translation
</subsectionHeader>
<bodyText confidence="0.999996">
We utilize the Moses toolkit (Koehn et al., 2007)
for statistical machine translation. The default pa-
rameter values are used except with the segmented
source language, where the maximum sentence
length is increased from 80 to 100 tokens to com-
pensate for the larger number of tokens in text.
</bodyText>
<subsectionHeader confidence="0.999133">
2.3 Morphological model combination
</subsectionHeader>
<bodyText confidence="0.999989875">
For combining individual models, we apply Min-
imum Bayes Risk (MBR) system combination
(Sim et al., 2007). N-best lists from multiple
SMT systems trained with different morpholog-
ical analysis methods are merged; the posterior
distributions over the individual lists are interpo-
lated to form a new distribution over the merged
list. MBR hypotheses selection is then performed
using sentence-level BLEU score (Kumar and
Byrne, 2004).
In this work, the focus of the system combina-
tion is not to combine different translation systems
(e.g., Moses and Systran), but to combine systems
trained with the same translation algorithm using
the same source language data with with different
morphological decompositions.
</bodyText>
<sectionHeader confidence="0.999399" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999959454545455">
The German-to-English and Czech-to-English
parts of the ACL WMT10 shared task data were
investigated. Vanilla SMT models were trained
with Moses using word tokens for MBR combi-
nation and comparison purposes. Several different
morphological segmentation models for German
and Czech were trained with Morfessor. Each seg-
mentation model corresponds to a morph-based
SMT model trained with Moses. The word-based
vanilla Moses model is compared to each morph-
based model as well as to several MBR com-
</bodyText>
<page confidence="0.994926">
196
</page>
<bodyText confidence="0.99854925">
binations between word-based translation models
and morph-based translation models. Quantitative
evaluation is carried out using the BLEU score
with re-cased and re-tokenized translations.
</bodyText>
<sectionHeader confidence="0.995922" genericHeader="method">
4 Data
</sectionHeader>
<bodyText confidence="0.999954434782609">
The data used in the experiments consisted
of Czech-to-English (CZ-EN) and German-to-
English (DE-EN) parallel language data from
ACL WMT10. The data was divided into distinct
training, development, and evaluation sets. Statis-
tics and details are shown in Table 1.
Aligned data from Europarl v5 and News
Commentary corpora were included in training
German-to-English SMT models. The English
part from the same data sets was used for train-
ing a 5-gram language model, which was used in
all translation tasks. The Czech-to-English trans-
lation model was trained with CzEng v0.9 (train-
ing section 0) and News Commentary data. The
monolingual German and Czech parts of the train-
ing data sets were used for training the morph seg-
mentation models with Morfessor.
The data sets news-test2009, news-
syscomb2009 and news-syscombtune2010
from the ACL WMT 2009 and WMT 2010,
were used for development. The news-test2008,
news-test2010, and news-syscombtest2010 data
sets were used for evaluation.
</bodyText>
<subsectionHeader confidence="0.993732">
4.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999895571428571">
All data sets were preprocessed before use. XML-
tags were removed, text was tokenized and char-
acters were lowercased for every training, devel-
opment and evaluation set.
Morphological models for German and Czech
were trained using a corpus that was a combina-
tion of the respective training sets. Then the mod-
els were used for segmenting all the data sets, in-
cluding development and evaluation sets, with the
Viterbi algorithm discussed in Section 2.1. The
modification of allowing new morph types for out-
of-vocabulary words was not applied.
The Moses cleaning script performed additional
filtering on the parallel language training data.
Specifically, sentences with over 80 words were
removed from the vanilla Moses word-based mod-
els. For morph-based models the limit was set
to 100 morphs, which is the maximum limit of
the Giza++ alignment tool. After filtering with a
threshold of 100 tokens, the different morph seg-
mentations for DE-EN training data from com-
bined Europarl and News Commentary data sets
ranged from 1613 556 to 1624 070 sentences.
Similarly, segmented CZ-EN training data ranged
from 896163 to 897 744 sentences. The vanilla
words-based model was trained with 1609 998
sentences for DE-EN and 897 497 sentences for
CZ-EN.
</bodyText>
<sectionHeader confidence="0.999965" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.9999582">
The details of the ACL WMT10 submissions are
shown in Table 2. The results of experiments with
different morphological decompositions and MBR
system combinations are shown in Table 3. The
significances of the differences in BLEU scores
between the word-based model (Words) and mod-
els with different morphological decompositions
was measured by dividing each evaluation data set
into 49 subsets of 41–51 sentences, and using the
one-sided Wilcoxon signed rank test (p &lt; 0.05).
</bodyText>
<subsectionHeader confidence="0.989435">
5.1 Segmentation
</subsectionHeader>
<bodyText confidence="0.999882304347826">
We created several word segmentations with Mor-
fessor baseline and Morfessor Categories-MAP
(CatMAP). Statistics for the different segmenta-
tions are given in Table 3. The amount of seg-
mentation was measured as the average number of
morphs per word (m/w) and as the percentage of
segmented words (s-%) in the training data. In-
creasing the data likelihood weight α in Morfes-
sor Baseline increases the amount of segmentation
for both languages. However, it had little effect
on the proportion of segmented words in the three
evaluation data sets: The proportion of segmented
word tokens was 10–11 % for German and 8–9 %
for Czech, whereas the out-of-vocabulary rate was
7.5–7.8 % for German and 4.8–5.6 % for Czech.
Disregarding the word frequency information
in Morfessor Baseline (nofreq) produced more
morphs per word type and segmented nearly
all words in the training data. The Morfessor
CatMAP algorithm created segmentations with the
largest number of morphs per word, but did not
segment as many words as the Morfessor Baseline
without the frequencies.
</bodyText>
<subsectionHeader confidence="0.999269">
5.2 Morph-based translation systems
</subsectionHeader>
<bodyText confidence="0.9993485">
The models with segmented source language per-
formed worse individually than the word-based
models. The change in the BLEU score was statis-
tically significant in almost all segmentations and
</bodyText>
<page confidence="0.996891">
197
</page>
<table confidence="0.99952775">
Data set Statistics Training Development Evaluation
Sentences Words per sentence SM LM TM
DE CZ EN DE CZ EN DE-EN CZ-EN {DE,CZ}-EN {DE,CZ}-EN
Europarl v5 1540 549 23.2 25.2 x x x
News Commentary 100269 21.9 18.9 21.5 x x x x x
CzEng v0.9 (training section 0) 803 286 8.3 9.9 x x
news-test2009 2525 21.7 18.8 23.2 x
news-syscomb2009 502 19.7 17.2 21.1 x
news-syscombtune2010 455 20.2 17.3 21.0 x
news-test2008 2051 20.3 17.8 21.7 x
news-test2010 2489 21.7 18.4 22.3 x
news-syscombtest2010 2034 22.0 18.6 22.6 x
</table>
<tableCaption confidence="0.9056398">
Table 1: Data sets for the Czech-to-English and German-to-English SMT experiments, including the
number of aligned sentences and the average number of words per sentence in each language. The data
sets used for model training, development and evaluation are marked. Training is divided into German
(DE) and Czech (CZ) segmentation model (SM) training, English (EN) language model (LM) training
and German-to-English (DE-EN) and Czech-to-English (CZ-EN) translation model (TM) training.
</tableCaption>
<table confidence="0.999881333333333">
Submission Segmentation model for source language BLEU-cased
(news-test2010)
aalto DE-EN WMT10 Morfessor Baseline (α = 0.5) 17.0
aalto DE-EN WMT10 CatMAP Morfessor Categories-MAP 16.5
aalto CZ-EN WMT10 Morfessor Baseline (α = 0.5) 16.2
aalto CZ-EN WMT10 CatMAP Morfessor Categories-MAP 15.9
</table>
<tableCaption confidence="0.970156">
Table 2: Our submissions for the ACL WMT10 shared task in translation. The translation models are
trained from the segmented source language into unsegmented target language with Moses.
</tableCaption>
<bodyText confidence="0.998636875">
all evaluation sets. Morfessor Baseline (α = 0.5)
was the best individual segmented model for both
German and Czech in the sense that it had the
lowest number of significant decreases the BLEU
score compared to the word-based model. Remov-
ing word frequency information with Morfessor
Baseline and using Morfessor CatMAP gave the
lowest BLEU scores with both source languages.
</bodyText>
<subsectionHeader confidence="0.995166">
5.3 Translation system combination
</subsectionHeader>
<bodyText confidence="0.9999877">
For the DE-EN language pair, all MBR system
combinations between each segmented model and
the word-based model had slightly higher BLUE
scores than the individual word-based model.
Nearly all improvements were statistically signifi-
cant.
The BLEU scores for the MBR combinations
in the CZ-EN language pair were mostly not sig-
nificantly different from the individual word-based
model. Two scores were significantly lower.
</bodyText>
<sectionHeader confidence="0.999151" genericHeader="conclusions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999982888888889">
We have applied concatenative morphological
analysis, in which each original word token is seg-
mented into one or more non-overlapping morph
tokens. Our results with different levels of seg-
mentation with Morfessor suggest that the optimal
level of segmentation is language pair dependent
in machine translation.
Our approach for handling rich morphology has
not been able to directly improve the translation
quality. We assume that improvements might still
be possible by carefully tuning the amount of seg-
mentation. The experiments in this paper with
different values of the α parameter for Morfes-
sor Baseline were conducted with the word fre-
quencies. The parameter had little effect on the
proportion of segmented words in the evaluation
data sets, as frequent words were not segmented
at all, and out-of-vocabulary words were likely to
be oversegmented by the Viterbi algorithm. Fu-
ture work includes testing a larger range of val-
ues for α, also for models trained without the
word frequencies, and using the modification of
the Viterbi algorithm proposed in Virpioja and Ko-
honen (2009).
It might also be helpful to only segment selected
words, where the selection would be based on the
potential benefit in the translation process. In gen-
eral, the direct segmentation of words into morphs
is problematic because it increases the number
of tokens in the text and directly increases both
model training and decoding complexity. How-
ever, an efficient segmentation decreases the num-
ber of types and the out-of-vocabulary rate (Virpi-
oja et al., 2007).
We have replicated here the result that an MBR
combination of a morph-based MT system with
</bodyText>
<page confidence="0.995917">
198
</page>
<table confidence="0.9999817">
Segmentation (DE) Statistics (DE) BLEU-cased (DE-EN)
m/w s-% news-test2008 news-test2010 news-syscombtest2010
No MBR MBR with No MBR No MBR MBR with
Words Words
Words 1.00 0.0% 16.37 - 17.28 13.22 -
Morfessor Baseline (α = 0.5) 1.82 72.4% 15.19− 16.47+ 17.04° 13.28° 13.70+
Morfessor Baseline (α = 1.0) 1.65 61.0% 15.14− 16.54+ 16.87− 11.95− 13.66+
Morfessor Baseline (α = 5.0) 1.24 23.7% 15.04− 16.44° 16.63− 11.78− 13.43+
Morfessor CatMAP 2.25 67.5% 14.21− 16.42° 16.53− 11.15− 13.61+
Morfessor Baseline nofreq 2.24 91.6% 13.98− 16.47+ 16.36− 10.66− 13.58+
Segmentation (CZ) Statistics (CZ) BLEU-cased (CZ-EN)
m/w s-% news-test2008 news-test2010 news-syscombtest2010
No MBR MBR with No MBR No MBR MBR with
Words Words
Words 1.00 0.0% 14.91 - 16.73 12.75 -
Morfessor Baseline (α = 0.5) 1.19 17.7% 13.22− 14.87° 16.01− 12.60° 12.53−
Morfessor Baseline (α = 1.0) 1.09 8.1% 13.33− 14.88° 16.10− 11.29− 12.84°
Morfessor Baseline (α = 5.0) 1.03 2.9% 13.53− 14.83° 15.92− 11.17− 12.85°
Morfessor CatMAP 2.29 71.9% 11.93− 14.86° 15.79− 10.12− 10.79−
Morfessor Baseline nofreq 2.18 90.3% 12.43− 14.96° 15.82− 10.13− 12.89°
</table>
<tableCaption confidence="0.987992">
Table 3: Results for German-to-English (DE-EN) and Czech-to-English (CZ-EN) translation models.
</tableCaption>
<bodyText confidence="0.974207046511628">
The source language is segmented with the shown algorithms. The amount of segmentation in the train-
ing data is measured with the average number of morphs per word (m/w) and as proportion of segmented
words (s-%) against the word-based model (Words). The trained translation systems are evaluated in-
dependently (No MBR) and in Minimum Bayes Risk system combination of word-based translation
systems (MBR). Unchanged (°), significantly higher (+) and lower (−) BLEU scores compared to the
word-based translation model (Words) are marked. The best morph-based model for each column is
emphasized.
a word-based MT system can produce a BLEU
score that is higher than from either of the indi-
vidual systems (de Gispert et al., 2009; Kurimo
et al., 2009). With the DE-EN language pair, the
improvement was statistically significant with all
tested segmentation models. However, the im-
provements were not as large as those obtained
before and the results for the CZ-EN language
pair were not significantly different in most cases.
Whether this is due to the different languages,
training data sets, the domain of the evaluation
data sets, or some problems in the model training,
is currently uncertain.
One very different approach for applying dif-
ferent levels of linguistic analysis is factor mod-
els for SMT (Koehn and Hoang, 2007), where
pre-determined factors (e.g., surface form, lemma
and part-of-speech) are stored as vectors for each
word. This provides better integration of mor-
phosyntactic information and more control of the
process, but the translation models are more com-
plex and the number and factor types in each word
must be fixed.
Our submissions to the ACL WMT10 shared
task utilize unsupervised morphological decompo-
sition models in a straightforward manner. The
individual morph-based models trained with the
source language words segmented into morphs
did not improve the vanilla word-based models
trained with the unsegmented source language.
We have replicated the result for the German-
to-English language pair that an MBR combina-
tion of a word-based and a segmented morph-
based model gives significant improvements to the
BLEU score. However, we did not see improve-
ments for the Czech-to-English translations.
</bodyText>
<sectionHeader confidence="0.99863" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999566">
This work was supported by the Academy of
Finland in the project Adaptive Informatics, the
Finnish graduate school in Language Technology,
and the IST Programme of the European Commu-
nity, under the FP7 project EMIME (213845).
</bodyText>
<sectionHeader confidence="0.998944" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.972512444444445">
Mathias Creutz and Krista Lagus. 2002. Unsuper-
vised discovery of morphemes. In Proceedings of
the Workshop on Morphological and Phonological
Learning of ACL’02, pages 21–30, Philadelphia,
Pennsylvania, USA.
Mathias Creutz and Krista Lagus. 2005a. Inducing the
morphological lexicon of a natural language from
unannotated text. In Proceedings of the AKRR’05,
Espoo, Finland.
</reference>
<page confidence="0.994297">
199
</page>
<reference confidence="0.997659545454545">
Mathias Creutz and Krista Lagus. 2005b. Unsu-
pervised morpheme segmentation and morphology
induction from text corpora using Morfessor 1.0.
Technical Report A81, Publications in Computer
and Information Science, Helsinki University of
Technology.
Mathias Creutz and Krista Lagus. 2007. Unsuper-
vised models for morpheme segmentation and mor-
phology learning. ACM Transactions on Speech and
Language Processing, 4(1), January.
Adri`a de Gispert, Sami Virpioja, Mikko Kurimo, and
William Byrne. 2009. Minimum Bayes risk com-
bination of translation hypotheses from alternative
morphological decompositions. In Proceedings of
Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, Com-
panion Volume: Short Papers, pages 73–76, Boul-
der, USA, June. Association for Computational Lin-
guistics.
Teemu Hirsim¨aki, Mathias Creutz, Vesa Siivola, Mikko
Kurimo, Sami Virpioja, and Janne Pylkk¨onen.
2006. Unlimited vocabulary speech recognition
with morph language models applied to Finnish.
Computer Speech and Language, 20(4):515–541.
Philipp Koehn and Hieu Hoang. 2007. Factored trans-
lation models. In Proceedings of the EMNLP 2007,
pages 868–876, Prague, Czech Republic, June.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Annual Meeting of ACL, demonstration ses-
sion, pages 177–180, Czech Republic, June.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In Proceedings of the HLT-NAACL 2004, pages
169–176.
Mikko Kurimo, Antti Puurula, Ebru Arisoy, Vesa Si-
ivola, Teemu Hirsim¨aki, Janne Pylkk¨onen, Tanel
Alum¨ae, and Murat Saraclar. 2006. Unlimited vo-
cabulary speech recognition for agglutinative lan-
guages. In Proceedings of the HLT-NAACL 2006,
pages 487–494, New York, USA.
Mikko Kurimo, Sami Virpioja, Ville T. Turunen,
Graeme W. Blackwood, and William Byrne. 2009.
Overview and results of Morpho Challenge 2009. In
Working Notes for the CLEF 2009 Workshop, Corfu,
Greece, September.
K. C. Sim, W. J. Byrne, M. J. F. Gales, H. Sahbi, and
P. C. Woodl. 2007. Consensus network decoding
for statistical machine translation system combina-
tion. In IEEE Int. Conf. on Acoustics, Speech, and
Signal Processing.
Sami Virpioja and Oskar Kohonen. 2009. Unsuper-
vised morpheme analysis with Allomorfessor. In
Working notes for the CLEF 2009 Workshop, Corfu,
Greece.
Sami Virpioja, Jaakko J. V¨ayrynen, Mathias Creutz,
and Markus Sadeniemi. 2007. Morphology-aware
statistical machine translation based on morphs in-
duced in an unsupervised manner. In Proceedings
of the Machine Translation Summit XI, pages 491–
498, Copenhagen, Denmark, September.
</reference>
<page confidence="0.996627">
200
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.205426">
<title confidence="0.999412">Applying morphological decomposition to statistical machine translation</title>
<author confidence="0.995468">Virpioja V¨ayrynen Mansikkaniemi</author>
<affiliation confidence="0.9513975">Aalto University School of Science and Department of Information and Computer</affiliation>
<author confidence="0.355375">PO BOX</author>
<abstract confidence="0.983687333333333">This paper describes the Aalto submission for the German-to-English and the Czechto-English translation tasks of the ACL 2010 Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR. Statistical machine translation has focused on using words, and longer phrases constructed from words, as tokens in the system. In contrast, we apply different morphological decompositions of words using the unsupervised Morfessor algorithms. While translation models trained using the morphological decompositions did not improve the BLEU scores, we show that the Minimum Bayes Risk combination with a word-based translation model produces significant improvements for the Germanto-English translation. However, we did not see improvements for the Czech-to- English translations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mathias Creutz</author>
<author>Krista Lagus</author>
</authors>
<title>Unsupervised discovery of morphemes.</title>
<date>2002</date>
<booktitle>In Proceedings of the Workshop on Morphological and Phonological Learning of ACL’02,</booktitle>
<pages>21--30</pages>
<location>Philadelphia, Pennsylvania, USA.</location>
<contexts>
<context position="3398" citStr="Creutz and Lagus, 2002" startWordPosition="476" endWordPosition="479">task data for German-to-English and Czech-to-English language pairs. 2 Methods In this work, morphological analyses are conducted on the source language data, and each different analysis is applied to create a unique segmentation of words into morphemes. Translation systems are trained with the Moses toolkit (Koehn et al., 2007) from each differently segmented version of the same source language to the target language. Evaluation with BLEU is performed on both the individual systems and system combinations, using different levels of decomposition. 2.1 Morphological models for words Morfessor (Creutz and Lagus, 2002; Creutz and Lagus, 2007, etc.) is a family of methods for unsupervised morphological segmentation. Morfessor does not limit the number of morphemes for each word, making it suitable for agglutinative and compounding languages. An analysis of a single word is a list of non-overlapping segments, 195 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 195–200, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics morphs, stored in the model lexicon. We use both the Morfessor Baseline (Creutz and Lagus, 2005b) and the Morfes</context>
</contexts>
<marker>Creutz, Lagus, 2002</marker>
<rawString>Mathias Creutz and Krista Lagus. 2002. Unsupervised discovery of morphemes. In Proceedings of the Workshop on Morphological and Phonological Learning of ACL’02, pages 21–30, Philadelphia, Pennsylvania, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Creutz</author>
<author>Krista Lagus</author>
</authors>
<title>Inducing the morphological lexicon of a natural language from unannotated text.</title>
<date>2005</date>
<booktitle>In Proceedings of the AKRR’05,</booktitle>
<location>Espoo, Finland.</location>
<contexts>
<context position="3981" citStr="Creutz and Lagus, 2005" startWordPosition="564" endWordPosition="567">words Morfessor (Creutz and Lagus, 2002; Creutz and Lagus, 2007, etc.) is a family of methods for unsupervised morphological segmentation. Morfessor does not limit the number of morphemes for each word, making it suitable for agglutinative and compounding languages. An analysis of a single word is a list of non-overlapping segments, 195 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 195–200, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics morphs, stored in the model lexicon. We use both the Morfessor Baseline (Creutz and Lagus, 2005b) and the Morfessor Categories-MAP (Creutz and Lagus, 2005a) algorithms.1 Both are formulated in a maximum a posteriori (MAP) framework, i.e., the learning algorithm tries to optimize the product of the model prior and the data likelihood. The generative model applied by Morfessor Baseline assumes that the morphs are independent. The resulting segmentation can be influenced by using explicit priors for the morph lengths and frequencies, but their effect is usually minimal. The training data has a larger effect on the results: A larger data set allows a larger lexicon, and thus longer morphs a</context>
</contexts>
<marker>Creutz, Lagus, 2005</marker>
<rawString>Mathias Creutz and Krista Lagus. 2005a. Inducing the morphological lexicon of a natural language from unannotated text. In Proceedings of the AKRR’05, Espoo, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Creutz</author>
<author>Krista Lagus</author>
</authors>
<title>Unsupervised morpheme segmentation and morphology induction from text corpora using Morfessor 1.0.</title>
<date>2005</date>
<tech>Technical Report A81,</tech>
<institution>Publications in Computer and Information Science, Helsinki University of Technology.</institution>
<contexts>
<context position="3981" citStr="Creutz and Lagus, 2005" startWordPosition="564" endWordPosition="567">words Morfessor (Creutz and Lagus, 2002; Creutz and Lagus, 2007, etc.) is a family of methods for unsupervised morphological segmentation. Morfessor does not limit the number of morphemes for each word, making it suitable for agglutinative and compounding languages. An analysis of a single word is a list of non-overlapping segments, 195 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 195–200, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics morphs, stored in the model lexicon. We use both the Morfessor Baseline (Creutz and Lagus, 2005b) and the Morfessor Categories-MAP (Creutz and Lagus, 2005a) algorithms.1 Both are formulated in a maximum a posteriori (MAP) framework, i.e., the learning algorithm tries to optimize the product of the model prior and the data likelihood. The generative model applied by Morfessor Baseline assumes that the morphs are independent. The resulting segmentation can be influenced by using explicit priors for the morph lengths and frequencies, but their effect is usually minimal. The training data has a larger effect on the results: A larger data set allows a larger lexicon, and thus longer morphs a</context>
</contexts>
<marker>Creutz, Lagus, 2005</marker>
<rawString>Mathias Creutz and Krista Lagus. 2005b. Unsupervised morpheme segmentation and morphology induction from text corpora using Morfessor 1.0. Technical Report A81, Publications in Computer and Information Science, Helsinki University of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Creutz</author>
<author>Krista Lagus</author>
</authors>
<title>Unsupervised models for morpheme segmentation and morphology learning.</title>
<date>2007</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="3422" citStr="Creutz and Lagus, 2007" startWordPosition="480" endWordPosition="483">English and Czech-to-English language pairs. 2 Methods In this work, morphological analyses are conducted on the source language data, and each different analysis is applied to create a unique segmentation of words into morphemes. Translation systems are trained with the Moses toolkit (Koehn et al., 2007) from each differently segmented version of the same source language to the target language. Evaluation with BLEU is performed on both the individual systems and system combinations, using different levels of decomposition. 2.1 Morphological models for words Morfessor (Creutz and Lagus, 2002; Creutz and Lagus, 2007, etc.) is a family of methods for unsupervised morphological segmentation. Morfessor does not limit the number of morphemes for each word, making it suitable for agglutinative and compounding languages. An analysis of a single word is a list of non-overlapping segments, 195 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 195–200, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics morphs, stored in the model lexicon. We use both the Morfessor Baseline (Creutz and Lagus, 2005b) and the Morfessor Categories-MAP (Creu</context>
<context position="5934" citStr="Creutz and Lagus, 2007" startWordPosition="883" endWordPosition="886">pes to be used by using an approximative cost of adding them into the lexicon (Virpioja and Kohonen, 2009). The modification prevents oversegmentation of unseen word forms. In machine translation, this is important especially for proper nouns, for which there is usually no need for translation. The Morfessor Categories-MAP algorithm extends the model by imposing morph categories of stems, prefixes and suffixes, as well as transition probabilities between them. In addition, it applies a hierarchical segmentation model that allows it to construct new stems from smaller pieces of “nonmorphemes” (Creutz and Lagus, 2007). Due to these features, it can provide reasonable segmentations also for those words that contain new morphemes. The drawback of the more sophisticated model is the slower and more complex training algorithm. In addition, the amount of the segmenta1The respective software is available at http://www. cis.hut.fi/projects/morpho/ tion is harder to control. Morfessor Categories-MAP was applied to statistical machine translation by Virpioja et al. (2007) and de Gispert et al. (2009). However, Kurimo et al. (2009) report that Morfessor Baseline outperformed Categories-MAP in Finnish-toEnglish and G</context>
</contexts>
<marker>Creutz, Lagus, 2007</marker>
<rawString>Mathias Creutz and Krista Lagus. 2007. Unsupervised models for morpheme segmentation and morphology learning. ACM Transactions on Speech and Language Processing, 4(1), January.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Adri`a de Gispert</author>
<author>Sami Virpioja</author>
<author>Mikko Kurimo</author>
<author>William Byrne</author>
</authors>
<title>Minimum Bayes risk combination of translation hypotheses from alternative morphological decompositions.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers,</booktitle>
<pages>73--76</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, USA,</location>
<marker>de Gispert, Virpioja, Kurimo, Byrne, 2009</marker>
<rawString>Adri`a de Gispert, Sami Virpioja, Mikko Kurimo, and William Byrne. 2009. Minimum Bayes risk combination of translation hypotheses from alternative morphological decompositions. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers, pages 73–76, Boulder, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Teemu Hirsim¨aki</author>
<author>Mathias Creutz</author>
<author>Vesa Siivola</author>
<author>Mikko Kurimo</author>
<author>Sami Virpioja</author>
<author>Janne Pylkk¨onen</author>
</authors>
<title>Unlimited vocabulary speech recognition with morph language models applied to Finnish. Computer Speech and Language,</title>
<date>2006</date>
<marker>Hirsim¨aki, Creutz, Siivola, Kurimo, Virpioja, Pylkk¨onen, 2006</marker>
<rawString>Teemu Hirsim¨aki, Mathias Creutz, Vesa Siivola, Mikko Kurimo, Sami Virpioja, and Janne Pylkk¨onen. 2006. Unlimited vocabulary speech recognition with morph language models applied to Finnish. Computer Speech and Language, 20(4):515–541.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
</authors>
<title>Factored translation models.</title>
<date>2007</date>
<booktitle>In Proceedings of the EMNLP</booktitle>
<pages>868--876</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="19120" citStr="Koehn and Hoang, 2007" startWordPosition="2935" endWordPosition="2938">de Gispert et al., 2009; Kurimo et al., 2009). With the DE-EN language pair, the improvement was statistically significant with all tested segmentation models. However, the improvements were not as large as those obtained before and the results for the CZ-EN language pair were not significantly different in most cases. Whether this is due to the different languages, training data sets, the domain of the evaluation data sets, or some problems in the model training, is currently uncertain. One very different approach for applying different levels of linguistic analysis is factor models for SMT (Koehn and Hoang, 2007), where pre-determined factors (e.g., surface form, lemma and part-of-speech) are stored as vectors for each word. This provides better integration of morphosyntactic information and more control of the process, but the translation models are more complex and the number and factor types in each word must be fixed. Our submissions to the ACL WMT10 shared task utilize unsupervised morphological decomposition models in a straightforward manner. The individual morph-based models trained with the source language words segmented into morphs did not improve the vanilla word-based models trained with </context>
</contexts>
<marker>Koehn, Hoang, 2007</marker>
<rawString>Philipp Koehn and Hieu Hoang. 2007. Factored translation models. In Proceedings of the EMNLP 2007, pages 868–876, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Annual Meeting of ACL, demonstration session,</booktitle>
<pages>177--180</pages>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="3106" citStr="Koehn et al., 2007" startWordPosition="431" endWordPosition="434">o Challenge 2009 competition (Kurimo et al., 2009). We continue the research described above and examine how the level of decomposition affects both the individual morph-based systems and MBR combinations with the baseline word-based model. Experiments are conducted with the WMT10 shared task data for German-to-English and Czech-to-English language pairs. 2 Methods In this work, morphological analyses are conducted on the source language data, and each different analysis is applied to create a unique segmentation of words into morphemes. Translation systems are trained with the Moses toolkit (Koehn et al., 2007) from each differently segmented version of the same source language to the target language. Evaluation with BLEU is performed on both the individual systems and system combinations, using different levels of decomposition. 2.1 Morphological models for words Morfessor (Creutz and Lagus, 2002; Creutz and Lagus, 2007, etc.) is a family of methods for unsupervised morphological segmentation. Morfessor does not limit the number of morphemes for each word, making it suitable for agglutinative and compounding languages. An analysis of a single word is a list of non-overlapping segments, 195 Proceedi</context>
<context position="6897" citStr="Koehn et al., 2007" startWordPosition="1029" endWordPosition="1032"> to control. Morfessor Categories-MAP was applied to statistical machine translation by Virpioja et al. (2007) and de Gispert et al. (2009). However, Kurimo et al. (2009) report that Morfessor Baseline outperformed Categories-MAP in Finnish-toEnglish and German-to-English tasks both with and without MBR combination, although the differences were not statistically significant. In all the previous cases, the models were trained on word types, i.e., without using their frequencies. Here, we also test models trained on word tokens. 2.2 Statistical machine translation We utilize the Moses toolkit (Koehn et al., 2007) for statistical machine translation. The default parameter values are used except with the segmented source language, where the maximum sentence length is increased from 80 to 100 tokens to compensate for the larger number of tokens in text. 2.3 Morphological model combination For combining individual models, we apply Minimum Bayes Risk (MBR) system combination (Sim et al., 2007). N-best lists from multiple SMT systems trained with different morphological analysis methods are merged; the posterior distributions over the individual lists are interpolated to form a new distribution over the mer</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Annual Meeting of ACL, demonstration session, pages 177–180, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>William Byrne</author>
</authors>
<title>Minimum bayes-risk decoding for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the HLT-NAACL</booktitle>
<pages>169--176</pages>
<contexts>
<context position="7605" citStr="Kumar and Byrne, 2004" startWordPosition="1138" endWordPosition="1141">h the segmented source language, where the maximum sentence length is increased from 80 to 100 tokens to compensate for the larger number of tokens in text. 2.3 Morphological model combination For combining individual models, we apply Minimum Bayes Risk (MBR) system combination (Sim et al., 2007). N-best lists from multiple SMT systems trained with different morphological analysis methods are merged; the posterior distributions over the individual lists are interpolated to form a new distribution over the merged list. MBR hypotheses selection is then performed using sentence-level BLEU score (Kumar and Byrne, 2004). In this work, the focus of the system combination is not to combine different translation systems (e.g., Moses and Systran), but to combine systems trained with the same translation algorithm using the same source language data with with different morphological decompositions. 3 Experiments The German-to-English and Czech-to-English parts of the ACL WMT10 shared task data were investigated. Vanilla SMT models were trained with Moses using word tokens for MBR combination and comparison purposes. Several different morphological segmentation models for German and Czech were trained with Morfess</context>
</contexts>
<marker>Kumar, Byrne, 2004</marker>
<rawString>Shankar Kumar and William Byrne. 2004. Minimum bayes-risk decoding for statistical machine translation. In Proceedings of the HLT-NAACL 2004, pages 169–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikko Kurimo</author>
</authors>
<title>Antti Puurula, Ebru Arisoy, Vesa Siivola, Teemu Hirsim¨aki, Janne Pylkk¨onen, Tanel Alum¨ae, and Murat Saraclar.</title>
<date>2006</date>
<booktitle>In Proceedings of the HLT-NAACL</booktitle>
<pages>487--494</pages>
<location>New York, USA.</location>
<marker>Kurimo, 2006</marker>
<rawString>Mikko Kurimo, Antti Puurula, Ebru Arisoy, Vesa Siivola, Teemu Hirsim¨aki, Janne Pylkk¨onen, Tanel Alum¨ae, and Murat Saraclar. 2006. Unlimited vocabulary speech recognition for agglutinative languages. In Proceedings of the HLT-NAACL 2006, pages 487–494, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikko Kurimo</author>
<author>Sami Virpioja</author>
<author>Ville T Turunen</author>
<author>Graeme W Blackwood</author>
<author>William Byrne</author>
</authors>
<title>Overview and results of Morpho Challenge</title>
<date>2009</date>
<booktitle>In Working Notes for the CLEF 2009 Workshop,</booktitle>
<location>Corfu, Greece,</location>
<contexts>
<context position="2537" citStr="Kurimo et al., 2009" startWordPosition="345" endWordPosition="348">n BLEU score, but obtaining reductions in out-of-vocabulary rates. They utilized morphs both in the source and in the target language. Later, de Gispert et al. (2009) showed that Minimum Bayes Risk (MBR) combination of word-based and morph-based translation models improves translation with Arabicto-English and Finnish-to-English language pairs, where only the source language utilized morphbased models. Similar results have been shown for Finnish-to-English and Finnish-to-German in performance evaluation of various unsupervised morpheme analysis algorithms in Morpho Challenge 2009 competition (Kurimo et al., 2009). We continue the research described above and examine how the level of decomposition affects both the individual morph-based systems and MBR combinations with the baseline word-based model. Experiments are conducted with the WMT10 shared task data for German-to-English and Czech-to-English language pairs. 2 Methods In this work, morphological analyses are conducted on the source language data, and each different analysis is applied to create a unique segmentation of words into morphemes. Translation systems are trained with the Moses toolkit (Koehn et al., 2007) from each differently segmente</context>
<context position="6448" citStr="Kurimo et al. (2009)" startWordPosition="963" endWordPosition="966">model that allows it to construct new stems from smaller pieces of “nonmorphemes” (Creutz and Lagus, 2007). Due to these features, it can provide reasonable segmentations also for those words that contain new morphemes. The drawback of the more sophisticated model is the slower and more complex training algorithm. In addition, the amount of the segmenta1The respective software is available at http://www. cis.hut.fi/projects/morpho/ tion is harder to control. Morfessor Categories-MAP was applied to statistical machine translation by Virpioja et al. (2007) and de Gispert et al. (2009). However, Kurimo et al. (2009) report that Morfessor Baseline outperformed Categories-MAP in Finnish-toEnglish and German-to-English tasks both with and without MBR combination, although the differences were not statistically significant. In all the previous cases, the models were trained on word types, i.e., without using their frequencies. Here, we also test models trained on word tokens. 2.2 Statistical machine translation We utilize the Moses toolkit (Koehn et al., 2007) for statistical machine translation. The default parameter values are used except with the segmented source language, where the maximum sentence lengt</context>
<context position="18543" citStr="Kurimo et al., 2009" startWordPosition="2843" endWordPosition="2846">he average number of morphs per word (m/w) and as proportion of segmented words (s-%) against the word-based model (Words). The trained translation systems are evaluated independently (No MBR) and in Minimum Bayes Risk system combination of word-based translation systems (MBR). Unchanged (°), significantly higher (+) and lower (−) BLEU scores compared to the word-based translation model (Words) are marked. The best morph-based model for each column is emphasized. a word-based MT system can produce a BLEU score that is higher than from either of the individual systems (de Gispert et al., 2009; Kurimo et al., 2009). With the DE-EN language pair, the improvement was statistically significant with all tested segmentation models. However, the improvements were not as large as those obtained before and the results for the CZ-EN language pair were not significantly different in most cases. Whether this is due to the different languages, training data sets, the domain of the evaluation data sets, or some problems in the model training, is currently uncertain. One very different approach for applying different levels of linguistic analysis is factor models for SMT (Koehn and Hoang, 2007), where pre-determined </context>
</contexts>
<marker>Kurimo, Virpioja, Turunen, Blackwood, Byrne, 2009</marker>
<rawString>Mikko Kurimo, Sami Virpioja, Ville T. Turunen, Graeme W. Blackwood, and William Byrne. 2009. Overview and results of Morpho Challenge 2009. In Working Notes for the CLEF 2009 Workshop, Corfu, Greece, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K C Sim</author>
<author>W J Byrne</author>
<author>M J F Gales</author>
<author>H Sahbi</author>
<author>P C Woodl</author>
</authors>
<title>Consensus network decoding for statistical machine translation system combination.</title>
<date>2007</date>
<booktitle>In IEEE Int. Conf. on Acoustics, Speech, and Signal Processing.</booktitle>
<contexts>
<context position="7280" citStr="Sim et al., 2007" startWordPosition="1090" endWordPosition="1093"> the previous cases, the models were trained on word types, i.e., without using their frequencies. Here, we also test models trained on word tokens. 2.2 Statistical machine translation We utilize the Moses toolkit (Koehn et al., 2007) for statistical machine translation. The default parameter values are used except with the segmented source language, where the maximum sentence length is increased from 80 to 100 tokens to compensate for the larger number of tokens in text. 2.3 Morphological model combination For combining individual models, we apply Minimum Bayes Risk (MBR) system combination (Sim et al., 2007). N-best lists from multiple SMT systems trained with different morphological analysis methods are merged; the posterior distributions over the individual lists are interpolated to form a new distribution over the merged list. MBR hypotheses selection is then performed using sentence-level BLEU score (Kumar and Byrne, 2004). In this work, the focus of the system combination is not to combine different translation systems (e.g., Moses and Systran), but to combine systems trained with the same translation algorithm using the same source language data with with different morphological decompositi</context>
</contexts>
<marker>Sim, Byrne, Gales, Sahbi, Woodl, 2007</marker>
<rawString>K. C. Sim, W. J. Byrne, M. J. F. Gales, H. Sahbi, and P. C. Woodl. 2007. Consensus network decoding for statistical machine translation system combination. In IEEE Int. Conf. on Acoustics, Speech, and Signal Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sami Virpioja</author>
<author>Oskar Kohonen</author>
</authors>
<title>Unsupervised morpheme analysis with Allomorfessor.</title>
<date>2009</date>
<booktitle>In Working notes for the CLEF 2009 Workshop,</booktitle>
<location>Corfu, Greece.</location>
<contexts>
<context position="5417" citStr="Virpioja and Kohonen, 2009" startWordPosition="805" endWordPosition="809">ly undersegmented compared to a linguistic analysis, whereas the rare words are oversegmented (Creutz and Lagus, 2005b). An easy way to control the amount of segmentation is to weight the training data likelihood by a positive factor α. If α &gt; 1, the increased likelihood results in longer morphs. If α &lt; 1, the morphs will be shorter and the words more segmented. Words that are not present in the training data can be segmented using an algorithm similar to Viterbi. The algorithm can be modified to allow new morphs types to be used by using an approximative cost of adding them into the lexicon (Virpioja and Kohonen, 2009). The modification prevents oversegmentation of unseen word forms. In machine translation, this is important especially for proper nouns, for which there is usually no need for translation. The Morfessor Categories-MAP algorithm extends the model by imposing morph categories of stems, prefixes and suffixes, as well as transition probabilities between them. In addition, it applies a hierarchical segmentation model that allows it to construct new stems from smaller pieces of “nonmorphemes” (Creutz and Lagus, 2007). Due to these features, it can provide reasonable segmentations also for those wor</context>
<context position="16028" citStr="Virpioja and Kohonen (2009)" startWordPosition="2447" endWordPosition="2451">ossible by carefully tuning the amount of segmentation. The experiments in this paper with different values of the α parameter for Morfessor Baseline were conducted with the word frequencies. The parameter had little effect on the proportion of segmented words in the evaluation data sets, as frequent words were not segmented at all, and out-of-vocabulary words were likely to be oversegmented by the Viterbi algorithm. Future work includes testing a larger range of values for α, also for models trained without the word frequencies, and using the modification of the Viterbi algorithm proposed in Virpioja and Kohonen (2009). It might also be helpful to only segment selected words, where the selection would be based on the potential benefit in the translation process. In general, the direct segmentation of words into morphs is problematic because it increases the number of tokens in the text and directly increases both model training and decoding complexity. However, an efficient segmentation decreases the number of types and the out-of-vocabulary rate (Virpioja et al., 2007). We have replicated here the result that an MBR combination of a morph-based MT system with 198 Segmentation (DE) Statistics (DE) BLEU-case</context>
</contexts>
<marker>Virpioja, Kohonen, 2009</marker>
<rawString>Sami Virpioja and Oskar Kohonen. 2009. Unsupervised morpheme analysis with Allomorfessor. In Working notes for the CLEF 2009 Workshop, Corfu, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sami Virpioja</author>
<author>Jaakko J V¨ayrynen</author>
<author>Mathias Creutz</author>
<author>Markus Sadeniemi</author>
</authors>
<title>Morphology-aware statistical machine translation based on morphs induced in an unsupervised manner.</title>
<date>2007</date>
<booktitle>In Proceedings of the Machine Translation Summit XI,</booktitle>
<pages>491--498</pages>
<location>Copenhagen, Denmark,</location>
<marker>Virpioja, V¨ayrynen, Creutz, Sadeniemi, 2007</marker>
<rawString>Sami Virpioja, Jaakko J. V¨ayrynen, Mathias Creutz, and Markus Sadeniemi. 2007. Morphology-aware statistical machine translation based on morphs induced in an unsupervised manner. In Proceedings of the Machine Translation Summit XI, pages 491– 498, Copenhagen, Denmark, September.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>