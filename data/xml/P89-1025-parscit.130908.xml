<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.35905">
PLANNING TEXT FOR ADVISORY DIALOGUES*
Johanna D. Moore
UCLA Department of Computer Science
and
USC/Information Sciences Institute
4676 Admiralty Way
Marina del Rey, CA 90292-6695, USA
Cecile L. Paris
USC/Information Sciences Institute
4676 Admiralty Way
Marina del Rey, CA 90292-6695, USA
</note>
<sectionHeader confidence="0.633574" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.999961545454545">
Explanation is an interactive process re-
quiring a dialogue between advice-giver and
advice-seeker. In this paper, we argue that
in order to participate in a dialogue with its
users, a generation system must be capable of
reasoning about its own utterances and there-
fore must maintain a rich representation of
the responses it produces. We present a text
planner that constructs a detailed text plan,
containing the intentional, attentional, and
rhetorical structures of the text it generates.
</bodyText>
<sectionHeader confidence="0.997863" genericHeader="introduction">
INTRODUCTION
</sectionHeader>
<bodyText confidence="0.988828543859649">
Providing explanations in an advisory situa-
tion is a highly interactive process, requiring
a dialogue between advice-giver and advice-
seeker (Pollack et al., 1982). Participating in
a dialogue requires the ability to reason about
previous responses, e.g., to interpret the user&apos;s
follow-up questions in the context of the on-
going conversation and to determine how to
clarify a response when necessary. To pro-
vide these capabilities, an explanation facility
must understand what it was trying to convey
and how that information was conveyed, i.e.,
the intentional structure behind the explana-
tion, including the goal of the explanation as a
whole, the subgoal(s) of individual parts of the
explanation, and the rhetorical means used to
achieve them.
Researchers in natural language under-
standing have recognized the need for such
information. In their work on discourse anal-
ysis, Grosz and Sidner (1986) argue that it is
necessary to represent the intentional struc-
ture, the attentional structure (knowledge
about which aspects of a dialogue are in focus
at each point), and the linguistic structure of
The research described in this paper was sup-
ported by the Defense Advanced Research Projects
Agency (DARPA) under a NASA Ames cooperative
agreement number NCC 2-520. The authors would
like to thank William Swartout for comments on ear-
lier versions of this paper.
the discourse. In contrast, most text gener-
ation systems (with the notable exception of
KAMP (Appelt, 1985)) have used only rhetor-
ical and attentional information to produce
coherent text (McKeown, 1985, McCoy, 1985,
Paris, 1988b), omitting intentional informa-
tion, or conflating intentional and rhetorical
information (Hovy, 1988b). No text gener-
ation system records or reasons about the
rhetorical, the attentional, as well as the in-
tentional structures of the texts it produces.
In this paper, we argue that to success-
fully participate in an explanation dialogue,
a generation system must maintain the kinds
of information outlined by Grosz and Sidner
as well as an explicit representation of the
rhetorical structure of the texts it generates.
We present a text planner that builds a de-
tailed text plan, containing the intentional,
attentional, and rhetorical structures of the
responses it produces. The main focus of
this paper is the plan language and the plan
structure built by our system. Examples of
how this structure is used in answering follow-
up questions appear in (Moore and Swartout,
.1989).
</bodyText>
<sectionHeader confidence="0.868967" genericHeader="method">
WHY A DETAILED TEXT PLAN?
</sectionHeader>
<bodyText confidence="0.999968941176471">
In order to handle follow-up questions that
may arise if the user does not fully understand
a response given by the system, a generation
facility must be able to determine what por-
tion of the text failed to achieve its purpose. If
the generation system only knows the top-level
discourse goal that was being achieved by the
text (e.g., persuade the hearer to perform an
action), and not what effect the individual
parts of the text were intended to have on the
hearer and how they fit together to achieve
this top-level goal, its only recourse is to use a
different strategy to achieve the top-level goal.
It is not able to re-explain or clarify any part
of the explanation. There is thus a need for
a text plan to contain a specification of the
intended effect of individual parts of the text
</bodyText>
<page confidence="0.997982">
203
</page>
<bodyText confidence="0.999735">
on the hearer and how the parts relate to one
another. We have developed a text planner
that records the following information about
the responses it produces:
</bodyText>
<listItem confidence="0.999164333333333">
• the information that Grosz and Sidner
(1986) have presented as the basics of a
discourse structure:
</listItem>
<bodyText confidence="0.9637371875">
— intentional structure: a represen-
tation of the effect each part of
the text is intended to have on the
hearer and how the complete text
achieves the overall discourse pur-
pose (e.g., describe entity, persuade
hearer to perform an action).
— attentional structure: information
about which objects, properties and
events are salient at each point
in the discourse. User&apos;s follow-
up questions are often ambiguous.
Information about the attentional
state of the discourse can be used
to disambiguate them (cf. (Moore
and Swartout, 1989)).
</bodyText>
<listItem confidence="0.9389565">
• in addition, for generation we require the
following:
</listItem>
<bodyText confidence="0.980803083333333">
— rhetorical structure: an agent must
understand how each part of the
text relates rhetorically to the oth-
ers. This is necessary for linguis-
tic reasons (e.g., to generate the
appropriate clausal connectives in
multi-sentential responses) and for
responding to requests for elabora-
tion/clarification.
assumption information: advice-
giving systems must take knowl-
edge about their users into account.
However, since we cannot rely on
having complete user models, these
systems may have to make assump-
tions about the hearer in order to
use a particular explanation strat-
egy. Whenever such assumptions
are made, they must be recorded.
The next sections describe this new text plan-
ner and show how it records the information
needed to engage in a dialogue. Finally, a brief
comparison with other approaches to text gen-
eration is presented.
</bodyText>
<sectionHeader confidence="0.865541" genericHeader="method">
TEXT PLANNER
</sectionHeader>
<bodyText confidence="0.9999173125">
The text planner has been developed as part
of an explanation facility for an expert sys-
tern built using the Explainable Expert Sys-
tems (EES) framework (Swartout and Smo-
liar, 1987). The text planner has been used
in two applications. In this paper, we draw
our examples from one of them, the Program
Enhancement Advisor (PEA) (Neches et a/.,
1985). PEA is an advice-giving system in-
tended to aid users in improving their Com-
mon Lisp programs by recommending trans-
formations that enhance the user&apos;s code.&apos; The
user supplies PEA with a program and in-
dicates which characteristics of the program
should be enhanced (any combination of read-
ability, maintainability, and efficiency). PEA
then recommends transformations. After each
recommendation is made, the user is free to
ask questions about the recommendation.
We have implemented a top-down hier-
archical expansion planner ( a la Sacerdoti
(1975) ) that plans utterances to achieve dis-
course goals, building (and recording) the in-
tentional, attentional, and rhetorical struc-
ture of the generated text. In addition, since
the expert system explanation facility is in-
tended to be used by many different users,
the text planner takes knowledge about the
user into account. In our system, the user
model contains the user&apos;s domain goals and
the knowledge he is assumed to have about
the domain.
</bodyText>
<sectionHeader confidence="0.799927" genericHeader="method">
THE PLAN LANGUAGE
</sectionHeader>
<bodyText confidence="0.998312875">
In our plan language, intentional goals are
represented in terms of the effects the speaker
intends his utterance to have on the hearer.
Following Hovy (1988a), we use the terminol-
ogy for expressing beliefs developed by Cohen
and Levesque (1985) in their theory of ratio-
nal interaction, but have found the need to
extend the terminology to represent the types
of intentional goals necessary for the kinds
of responses desired in an advisory setting.
Although Cohen and Levesque have subse-
quently retracted some aspects of their theory
of rational interaction (Cohen and Levesque,
1987), the utility of their notation for our pur-
poses remains unaffected, as argued in (Hovy,
1989).2
</bodyText>
<footnote confidence="0.929232625">
&apos;PEA recommends transformations that improve
the &apos;style&apos; of the user&apos;s code. It does not attempt to
understand the content of the user&apos;s program.
2Space limitations prohibit an exposition of their
terminology in this paper. We provide English para-
phrases where necessary for clarity. (MB S H
should be read as &apos;the speaker believes the speaker
and hearer mutually believe x.&apos;
</footnote>
<page confidence="0.319983">
2 04
</page>
<figure confidence="0.439193571428571">
EFFECT: (PERSUADE S H (GOAL H Eventually(DONE H ?act)))
CONSTRAINTS: (AND (GOAL S ?domain-goal)
(STEP ?act ?domain-goal)
(BMB S H (GOAL H ?domain-goal)))
NUCLEUS: (FORALL ?domain-goal
(MOTIVATION ?act ?domain-goal))
SATELLITES: nil
</figure>
<figureCaption confidence="0.972993">
Figure 1: Plan Operator for Persuading the Hearer to Do An Act
</figureCaption>
<figure confidence="0.774216555555555">
EFFECT: (MOTIVATION ?act ?domain-goal)
CONSTRAINTS: (AND (GOAL S ?domain-goal)
(STEP ?act ?domain-goal)
(BMB S H (GOAL H ?domain-goal))
(ISA ?act REPLACE))
NUCLEUS: ((SETQ ?replacee (FILLER-OF OBJECT ?act))
(SETQ ?replacer (FILLER-OF GENERALIZED-MEANS ?act))
(BMB S H (DIFFERENCES ?replacee ?replacer ?domain-goal)))
SATELLITES: nil
</figure>
<figureCaption confidence="0.99066">
Figure 2: Plan Operator for Motivating a Replacement by Describing Differences between Replacer
and Replacee
</figureCaption>
<bodyText confidence="0.96716762962963">
Rhetorical structure is represented in
terms of the rhetorical relations defined in
Rhetorical Structure Theory (RST) (Mann
and Thompson, 1987), a descriptive theory
characterizing text structure in terms of the
relations that hold between parts of a text
(e.g., CONTRAST, MOTIVATION). The defini-
tion of each RST relation includes constraints
on the two entities being related as well as
constraints on their combination, and a spec-
ification of the effect which the speaker is
attempting to achieve on the hearer&apos;s be-
liefs. Although other researchers have cate-
gorized typical intersentential relations (e.g.,
(Grimes, 1975, Hobbs, 1978)), the set of rela-
tions proposed by RST is the most complete
and the theory sufficiently detailed to be eas-
ily adapted for use in generation.
In our plan language, each plan operator
consists of:
• an effect: a characterization of what
goal(s) this operator can be used to
achieve. An effect may be an in-
tentional goal, such as persuade the
hearer to do an action or a rhetorical
relation, such as provide motivation
for an action.
</bodyText>
<listItem confidence="0.999066076923077">
• a constraint list: a list of conditions that
must be true before the operator can be
applied. Constraints may refer to facts
in the system&apos;s knowledge base or in the
user model.
• a nucleus: the main topic to be ex-
pressed. The nucleus is either a prim-
itive operator (i.e., speech acts such as
inform, recommend and ask) or a goal
(intentional or rhetorical) which must be
further expanded. All operators must
contain a nucleus.
• satellites: subgoal(s) that express addi-
</listItem>
<bodyText confidence="0.995616375">
tional information which may be needed
to achieve the effect of the operator.
When present, satellites may be specified
as required or optional.
Examples of our plan operators are shown
in Figures• 1 and 2. The operator shown in
Figure 1 can be used if the speaker (S) intends
to persuade the hearer (H) to intend to do
some act. This plan operator states that if an
act is a step in achieving some domain goal(s)
that the hearer shares, one way to persuade
the hearer to do the act is to motivate the act
in terms of those domain goals. Note that this
plan operator takes into account not only the
system&apos;s knowledge of itself, but also the sys-
tem&apos;s knowledge about the user&apos;s goals, as em-
bodied in a user model. If any domain goals
that satisfy the constraints are found, this op-
erator will cause the planner to post one or
more MOTIVATION subgoals. This plan opera-
tor thus indicates that one way to achieve the
intentional goal of persuading the hearer to
perform an action is by using the rhetorical
means MOTIVATION.
</bodyText>
<page confidence="0.992081">
205
</page>
<figure confidence="0.723246">
EFFECT: (BMB S H ?x)
CONSTRAINTS: nil
NUCLEUS: (INFORM S H ?x)
SATELLITES: (((PERSUADE S H ?x) *optional*))
</figure>
<figureCaption confidence="0.833252">
Figure 3: Plan Operator for Achieving Mutual Belief of a Proposition
</figureCaption>
<figure confidence="0.393569">
SYSTEM What characteristics of the program would you like to enhance? [1]
USER Maintainability. [2]
SYSTEM You should replace (setq x 1) with (sett x 1). Sett can be used to assign a [3]
</figure>
<figureCaption confidence="0.529656">
value to any generalized-variable. Setq can only be used to assign a value to a
simple-variable. A generalized-variable is a storage location that can be named by
any accessor function.
</figureCaption>
<bodyText confidence="0.6608325">
USER What is a generalized variable? [4]
SYSTEM For example, the car and cdr of a cons are generalized-variables, named by the [51
accessor functions car and cdr. Other examples are an element of an array or a
component of a structure.
</bodyText>
<figureCaption confidence="0.994529">
Figure 4: Sample Dialogue
</figureCaption>
<bodyText confidence="0.999981725">
Plans that achieve intentional goals and
those that achieve rhetorical relations are dis-
tinguished for two reasons: (1) so that the
completed plan structure contains both the in-
tentional goals of the speaker and the rhetor-
ical means used to achieve them; (2) because
there are many different rhetorical strategies
for achieving any given intentional goal. For
example, the system has several plan opera-
tors for achieving the intentional goal of de-
scribing a concept. It may describe a concept
by stating its class membership and describ-
ing its attributes and its parts, by drawing
an analogy to a similar concept, or by giving
examples of the concept. There may also be
many different plan operators for achieving
a particular rhetorical strategy. (The plan-
ner employs selection heuristics for choosing
among applicable operators in a given situa-
tion (Moore and Swartout, 1989).)
Our plan language allows both general
and specific plans to be represented. For ex-
ample, Figure 2 shows a plan operator for
achieving the rhetorical relation MOTIVATION.
This is a very specific operator that can be
used only when the act to be motivated is a
replacement (e.g., replace setq with setf).
In this case, one strategy for motivating the
act is to compare the object being replaced
and the object that replaces it with respect
to the domain goal being achieved. On the
other hand, the operator shown in Figure 3
is general and can be used to achieve mu-
tual belief of any assertion by first inform-
ing the hearer of the assertion and then, op-
tionally, by persuading him of that fact. Be-
-cause we allow very general operators as well
as very specific ones, we can include both
domain-independent and domain-dependent
strategies.
</bodyText>
<sectionHeader confidence="0.959204" genericHeader="method">
A DETAILED EXAMPLE
</sectionHeader>
<bodyText confidence="0.999709045454546">
Consider the sample dialogue with our sys-
tem shown in Figure 4, in which the user in-
dicates that he wishes to enhance the main-
tainability of his program. While enhanc-
ing maintainability, the system recommends
that the user perform the act replace-1,
namely &apos;replace setq with sett&apos;, and thus
posts the intentional goal (BMB S H (GOAL
H Eventually(DONE H replace-1))). This
discourse goal says that the speaker would like
to achieve the state where the speaker believes
that the hearer and speaker mutually believe
that it is a goal of the hearer that the replace-
ment eventually be done by the hearer.
The planner then identifies all the opera-
tors whose effect field matches the discourse
goal to be achieved. For each operator found,
the planner checks to see if all of its con-
straints are satisfied. In doing so, the text
planner attempts to find variable bindings in
the expert system&apos;s knowledge base or the
user model that satisfy all the constraints in
</bodyText>
<page confidence="0.994161">
206
</page>
<table confidence="0.9696222">
EFFECT: (BMB S H (GOAL H Eventually(DONE H ?act)))
CONSTRAINTS: none
NUCLEUS: (RECOMMEND S H ?act)
SATELLITES: (((BMB S H (COMPETENT H (DONE H ?act))) *optional*)
((PERSUADE S H (GOAL H Eventually(DONE H ?act))) *option*))
</table>
<figureCaption confidence="0.897518">
Figure 5: High-level Plan Operator for Recommending an Act
</figureCaption>
<bodyText confidence="0.687550333333333">
apply -SETQ -to -SETF-transformation
apply -local -transformations -whose -rhs -use -is-more -general-than -lhs -use
apply -local -transformations -that -enhance -maintainability
apply -transformations -that-enhance -maintainability
enhance-maintainability
enhance-program
</bodyText>
<figureCaption confidence="0.995218">
Figure 6: System goals leading to replace setq with setf
</figureCaption>
<bodyText confidence="0.995902714285715">
the constraint list. Those operators whose
constraints are satisfied become candidates for
achieving the goal, and the planner chooses
one based on: the user model, the dialogue
history, the specificity of the plan operator,
and whether or not assumptions about the
user&apos;s beliefs must be made in order to satisfy
the operator&apos;s constraints.
Continuing the example, the current dis-
course goal is to achieve the state where
It is mutually believed by the speaker and
hearer that the hearer has the goal of even-
tually executing the replacement. This dis-
course goal can be achieved by the plan op-
erator in Figure 5. This operator has no
constraints. Assume it is chosen in this
case. The nucleus is expanded first,3 causing
(RECOMMEND S H replace-1) to be posted as
a subgoal. RECOMMEND is a primitive operator,
and so expansion of this branch of the plan is
complete.4
Next, the planner must expand the satel-
lites. Since both satellites are optional in this
case, the planner must decide which, if any,
are to be posted as subgoals. In this example,
the first satellite will not be expanded because
the user model indicates that the user is ca-
31n some cases, such as a satellite posting the
rhetorical relation background, the satellite is ex-
panded first.
4At this point, (RECOMMEND S H replace-1) must
be translated into a form appropriate as input•to the
realization component, the Penman system (Mann,
1983, Kasper, 1989). Based on the type of speech act,
its arguments, and the context in which it occurs, the
planner builds the appropriate structure. Bateman
and Paris (1989) have begun to investigate the prob-
lem of phrasing utterances for different types of users.
pable of performing replacement acts. The
second satellite is expanded,5 posting the in-
tentional subgoal to persuade the user to per-
form the replacement. A plan operator for
achieving this goal using the rhetorical rela-
tion MOTIVATION was shown in Figure 1.
When attempting to satisfy the con-
straints of the operator in Figure 1, the
system first checks the constraints (GOAL
S ?domain-goal) and (STEP replace-1
?domain-goal). These constraints state that,
in order to use this operator, the system must
find an expert system goal, ?domain-goal,
that replace-1 is a step in achieving.
This results in several possible bindings
for the variable ?domain-goal. In this case,
the applicable system goals, listed in order
from most specific to the top-level goal of the
system, are shown in Figure 6.
The last constraint of this plan opera-
tor, (3MH S H (GOAL H ?domain-goal)), is
a constraint on the user model stating that the
speaker and hearer should mutually believe
that ?domain-goal is a goal of the hearer.
Not all of the bindings found so fax will sat-
isfy this constraint. Those which do not will
not be rejected immediately, however, as we
do not assume that the user model is com-
plete. Instead, they will be noted as possible
bindings, and each will be marked to indicate
that, if this binding is used, an assumption
is being made, namely that the binding of
</bodyText>
<footnote confidence="0.898906666666667">
51n other situations, the system could choose not
to expand this satellite and await feedback from the
user instead (Moore and Swartout, 1989).
</footnote>
<page confidence="0.993498">
207
</page>
<figure confidence="0.95803475">
(BMB S H (GOAL H Eventually (DONE H replace-1)))
N
(MOTIVATION replace1 enhance-maintainability)
(RECOMMEND H replace-1) (PERSUADE S H (GOAL H Eventually (DONE H replace-1)))
N
(MOTIVATION replace-1 enhance-maintainability)
(MB S H (DIFFERENCES setq setf enhance-maintainability))
N
(BMB S H (DIFFERENCE setq setf use))
(INFORMS H (IDENTITY (VALUE-OF use setf)
assign-value-to-generalized-variable)) (BMB S H (KNOW H generalized-variable))
(CONTRAST (IDENTITY (VALUE-OF use setq)))
NI
(INFORM S H (IDENTITY (VALUE-OF use setq)
assign-value-to-simple-variable))
(INFORMS H (CLASS-ASCRIPTION
generalized-variable storage-location))
replacel replace SETO with sETF
N • Nucleus
S u Satellite
</figure>
<figureCaption confidence="0.999138">
Figure 7: Completed Text Plan for Recommending Replace SETQ with SETF
</figureCaption>
<equation confidence="0.644802857142857">
N
(ELABORATION generalized-variable)
(ELABORATION-OBJECT-ATTRIBUTE
generalized-variable named-by)
N
(INFORM S H (IDENTITY
(VALUE-OF named-by accessor-function )))
</equation>
<bodyText confidence="0.995738854545455">
?domain-goal is assumed to be a goal of the
user.
In this example, since the user is using
the system to enhance a program and has in-
dicated that he wishes to enhance the main-
tainability of the program, the system infers
the user shares the top-level goal of the system
(enhance-program), as well as the more spe-
cific goal enhance-maintainability. There-
fore, these are the two goals that satisfy the
constraints of the operator shown in Figure 1.
The text planner prefers choosing binding
environments that require no assumptions to
be made. In addition, in order to avoid ex-
plaining parts of the reasoning chain that the
user is familiar with, the most specific goal is
chosen. The plan operator is thus instanti-
ated with enhance-maintainability as the
binding for the variable ?domain-goal. The
selected plan operator is recorded as such, and
all other candidate operators are recorded as
untried alternatives.
The nucleus of the chosen plan op-
erator is now posted, resulting in the
subgoal (MOTIVATION replace-1 enhance-
maintainability). The plan operator cho-
sen for achieving this goal is the one that
was shown in Figure 2. This operator mo-
tivates the replacement by describing differ-
ences between the object being replaced and
the object replacing it. Although there are
many differences between setq and setf,
only the differences relevant to the domain
goal at hand (enhance-maintainability)
should be expressed. The relevant differ-
ences are determined in the following way.
From the expert system&apos;s problem-solving
knowledge, the planner determines what roles
setq and setf play in achieving the goal
enhance-maintainability. In this case, the
system is enhancing maintainability by ap-
plying transformations that replace a specific
construct with one that has a more general
usage. Setq has a more specific usage than
setf, and thus the comparison between setq
and sett should be based on the generality of
their usage.
Finally, since the term generalized-
variable has been introduced, and the
user model indicates that the user does
not know this term, an intentional goal
to define it is posted: (BMB S H (KNOW
H generalized-variable)). This goal is
achieved with a plan operator that describes
concepts by stating their class membership
</bodyText>
<page confidence="0.995917">
208
</page>
<bodyText confidence="0.999778">
and describing their attributes. Once com-
pleted, the text plan is recorded in the dia-
logue history. The completed text plan for
response (3) of the sample dialogue is shown
in Figure 7.
</bodyText>
<sectionHeader confidence="0.943902" genericHeader="method">
ADVANTAGES
</sectionHeader>
<bodyText confidence="0.999990287671233">
As illustrated in Figure 7, a text plan pro-
duced by our planner provides a detailed rep-
resentation of the text generated by the sys-
tem, indicating which purposes different parts
of the text serve, the rhetorical means used
to achieve them, and how parts of the plan
are related to each other. The text plan also
contains the assumptions that were made dur-
ing planning. This text plan thus contains
both the intentional structure and the rhetor-
ical structure of the generated text. From
this tree, the dominance and satisfaction-
precedence relationships as defined by Grosz
and Sidner can be inferred. Intentional goals
higher up in the tree dominate those lower
down and a left to right traversal of the
tree provides satisfaction-precedence ordering.
The attentional structure of the generated
text can also be derived from the text plan.
The text plan records the order in which top-
ics appear in the explanation. The global vari-
able *local-context* always points to the plan
node that is currently in focus, and previously
focused topics can be derived by an upward
traversal of the plan tree.
The information contained in the text
plan is necessary for a generation system to be
able to answer follow-up questions in context.
Follow-up questions are likely to refer to the
previously generated text, and, in addition,
they often refer to part of the generated text,
as opposed to the whole text. Without an ex-
plicit representation of the intentional struc-
ture of the text, a system cannot recognize
that a follow-up question refers to a portion of
the text already generated. Even if the system
realizes that the follow-up question refers back
to the original text, it cannot plan a text to
clarify a part of the text, as it no longer knows
what were the intentions behind various pieces
of the text.
Consider again the dialogue in Figure 4.
When the user asks &apos;What is a gener-
alized variable?&apos; (utterance (4) in Fig-
ure 4), the query analyzer interprets this ques-
tion and posts the goal: (BMB S H (KNOW H
generalized-variable)). At this point, the
explainer must recognize that this discourse
goal was attempted and not achieved by the
last sentence of the previous explanation.6
Failure to do so would lead to simply repeat-
ing the description of a generalized variable
that the user did not understand. By exam-
ining the text plan of the previous explanation
recorded in the dialogue history, the explainer
is able to determine whether the current goal
(resulting from the follow-up question) is a
goal that was attempted and failed, as it is
in this case. This time, when attempting to
achieve the goal, the planner must select an al-
ternative strategy. Moore (1989b) has devised
recovery heuristics for selecting an alternative
strategy when responding to such follow-up
questions. Providing an alternative explana-
tion would not be possible without the explicit
representation of the intentional structure of
the generated text. Note that it is important
to record the rhetorical structure as well, so
that the text planner can choose an alterna-
tive rhetorical strategy for achieving the goal.
In the example under consideration, the re-
covery heuristics indicate that the rhetorical
strategy of giving examples should be chosen.
</bodyText>
<sectionHeader confidence="0.995237" genericHeader="related work">
RELATED WORK
</sectionHeader>
<bodyText confidence="0.981173774193549">
Schemata (McKeown, 1985) encode standard
patterns of discourse structure, but do not in-
dude knowledge of how the various parts of
a schema relate to one another or what their
intended effect on the hearer is. A schema
can be viewed as a compiled version of one
of our text plans in which all of the non-
terminal nodes have been pruned out and only
the leaves (the speech acts) remain. While
schemata can produce the same initial behav-
ior as one of our text plans, all of the ratio-
nale for that behavior has been compiled out.
Thus schemata cannot be used to participate
in dialogues. If the user indicates that he has
not understood the explanation, the system
cannot know which part of the schema failed
to achieve its effect on the hearer or which
rhetorical strategy failed to achieve this ef-
fect. Planning a text using our approach is
essentially planning a schema from more fine-
grained plan operators. From a library of such
plan operators, many varied schemata can re-
sult, improving the flexibility of the system.
In an approach taken by Cohen and Ap-
pelt (1979) and Appelt (1985), text is planned
by reasoning about the beliefs of the hearer
and speaker and the effects of surface speech
&apos;We are also currently implementing another in-
terface which allows users to use a mouse to point at
the noun phrases or clauses in the text that were not
understood (Moore, 1989b).
</bodyText>
<page confidence="0.995907">
209
</page>
<bodyText confidence="0.999950692307692">
acts on these beliefs (i.e., the intentional ef-
fect). This approach does not include rhetori-
cal knowledge about how clausal units may be
combined into larger bodies of coherent text
to achieve a speaker&apos;s goals. It assumes that
appropriate axioms could be added to gen-
erate large (more than one- or two-sentence)
bodies of text and that the text produced will
be coherent as a by-product of the planning
process. However, this has not been demon-
strated.
Recently, Hovy (1988b) built a text struc-
turer which produces a coherent text when
given a set of inputs to express. Hovy uses
an opportunistic planning approach that or-
ders the inputs according to the constraints
on the rhetorical relations defined in Rhetori-
cal Structure Theory. His approach provides a
description of what can be said when, but does
not include information about why this infor-
mation can or should be included at a partic-
ular point. Hovy&apos;s approach conflates inten-
tional and rhetorical structure and, therefore,
a system using his approach could not later
reason about which rhetorical strategies were
used to achieve intentional goals.
</bodyText>
<sectionHeader confidence="0.994885" genericHeader="method">
STATUS AND FUTURE WORK
</sectionHeader>
<bodyText confidence="0.901157428571429">
The text planner presented is implemented
in Common Lisp and can produce the text
plans necessary to participate in the sample
dialogue described in this paper and several
others (see (Moore, 1989a, Paris, 1988a)). We
currently have over 60 plan operators and
the system can answer the following types of
(follow-up) questions:
- Why?
- Why conclusion?
- Why are you trying to achieve goal?
- Why are you using method to achieve goal?
- Why are you doing act?
- How do you achieve goal?
- How did you achieve goal (in this case)?
- What is a concept?
- What is the difference between conceptl
and concept2?
- Huh?
The text planning system described in this
paper is being incorporated into two expert
systems currently under development. These
systems will be installed and used in the field.
This will give us an opportunity to evaluate
the techniques proposed here.
We are currently studying how the atten-
tional structure inherent in our text plans can
be used to guide the realization process, for
example in the planning of referring expres-
sions and the use of cue phrases and pronouns.
We are also investigating criteria for the ex-
pansion and ordering of optional satellites in
our plan operators. Currently we use informa-
tion from the user model to dictate whether
or not optional satellites are expanded, and
their ordering is specified in each plan opera-
tor. We wish to extend our criteria for satel-
lite expansion to include other factors such as
pragmatic and stylistic goals (Hovy, 1988a)
(e.g., brevity) and the conversation that has
occurred so far. We are also investigating the
use of attentional information to control the
ordering of these satellites (McKeown, 1985).
We also believe that the detailed text plan
constructed by our planner will allow a system
to modify its strategies based on experience
(feedback from the user). In (Paris, 1988a),
we outline our preliminary ideas on this issue.
We have also begun to study how our planner
can be used to handle incremental generation
of texts. In (Moore, 1988), we argue that the
detailed representation provided by our text
plans is necessary for execution monitoring
and to indicate points in the planning process
where feedback from the user may be helpful
in incremental text planning.
</bodyText>
<sectionHeader confidence="0.996646" genericHeader="conclusions">
CONCLUSIONS
</sectionHeader>
<bodyText confidence="0.9997045">
In this paper, we have presented a text plan-
ner that builds a detailed text plan, contain-
ing the intentional, attentions&apos;, and rhetor-
ical structures of the responses it produces.
We argued that, in order to participate in a
dialogue with its users, a generation system
must be capable of reasoning about its past
utterances. The text plans built by our text
planner provide a generator with the infor-
mation needed to reason about its responses.
We illustrated these points with a sample di-
alogue.
</bodyText>
<sectionHeader confidence="0.99984" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.998958666666667">
Douglas E. Appelt. 1985. Planning Natu-
ral Language Utterances. Cambridge Univer-
sity Press, Cambridge, England.
John A. Bateman and Cecile L. Paris.
1989. Phrasing a text in terms the user can
understand. In Proceedings of the Eleventh
International Joint Conference on Artificial
Intelligence, Detroit, MI, August 20-25.
Philip R. Cohen and Hector J. Levesque.
1985. Speech Acts and Rationality. In Pro-
ceedings of the Twenty-Third Annual Meet-
ing of the Association for Computational Lin-
</reference>
<page confidence="0.981659">
210
</page>
<reference confidence="0.993289464646465">
guistics, pages 49-60, University of Chicago,
Chicago, Illinois, July 8-12.
Philip R. Cohen and Hector J. Levesque.
1987. Intention is Choice with Commitment,
November.
Philip R. Cohen and C. Raymond Per-
rault. 1979. Elements of a Plan-based Theory
of Speech Acts. Cognitive Science, 3:177-212.
Joseph E. Grimes. 1975. The Thread of
Discourse. Mouton, The Hague, Paris.
Barbara J. Grosz and Candace L. Sidner.
1986. Attention, Intention, and the Struc-
ture of Discourse. Computational Linguistics,
12(3):175-204.
Jerry Hobbs. 1978. Why is a Discourse
Coherent? Technical Report 176, SRI Inter-
national.
Eduard H. Hovy. 1988a. Generating Nat-
ural Language Under Pragmatic Constraints.
Lawrence Erlbaum, Hillsdale, New Jersey.
Eduard H. Hovy. 1988b. Planning Coher-
ent Multisentential Text. In Proceedings of
the Twenty-Sixth Annual Meeting of the As-
sociation for Computational Linguistics, State
University of New York, Buffalo, New York,
June 7-10.
Eduard H. Hovy. 1989. Unresolved Issues
in Paragraph Planning, April 6-8. Presented
at the Second European Workshop on Natural
Language Generation.
Robert Kasper. 1989. SPL: A Sentence
Plan Language for Text Generation. Technical
report, USC/ISI.
William C. Mann and Sandra A. Thomp-
son. 1987. Rhetorical Structure Theory:
A Theory of Text Organization. In Livia
Polanyi, Editor, The Structure of Discourse.
Ablex Publishing Corporation, Norwood, N.J.
William Mann. 1983. An Overview of the
Penman Text Generation System. Technical
report, USC/ISI.
Kathleen F. McCoy. 1985. Correcting
Object-Related Misconceptions. PhD thesis,
University of Pennsylvania, December. Pub-
lished by University of Pennsylvania as Tech-
nical Report MS-CIS-85-57.
Kathleen R McKeown. 1985. Text Gener-
ation: Using Discourse Strategies and Focus
Constraints to Generate Natural Language
Text. Cambridge University Press, Cam-
bridge, England.
Johanna D. Moore and William R.
Swartout. 1989. A Reactive Approach to Ex-
planation. In Proceedings of the Eleventh In-
ternational Joint Conference on Artificial In-
telligence, Detroit, MI, August 20-25.
Johanna D. Moore. 1988. Planning and
Reacting. In Proceedings of the AAAI Work-
shop on Text Planning and Generation, St
Paul, Minnesota, August 25.
Johanna D. Moore. 1989a. Responding
to &amp;quot;Huh?&amp;quot;: Answering Vaguely Articulated
Follow-up Questions. In Proceedings of the
Conference on Human Factors in Computing
Systems, Austin, Texas, April 30 - May 4.
Johanna D. Moore. 1989b. A Reactive Ap-
proach to Explanation in Expert and Advice-
Giving Systems. PhD thesis, University of
California, Los Angeles, forthcoming.
Robert Neches, William R. Swartout, and
Johanna D. Moore. 1985. Enhanced Main-
tenance and Explanation of Expert Systems
through Explicit Models of their Develop-
ment. IEEE Transactions on Software En-
gineering, SE-11(11), November.
Cecile L. Paris. 1988a. Generation and
Explanation: Building an Explanation Fa-
cility for the Explainable Expert Systems
Framework, July 17-21. Presented at the
Fourth International Workshop on Natural
Language Generation.
Cecile L. Paris. 1988b. Tailoring Object
Descriptions to the User&apos;s Level of Exper-
tise. Computational Linguistics Journal, 14
(3), September.
Martha E. Pollack, Julia Hirschberg, and
Bonnie Lynn Webber. 1982. User Participa-
tion in the Reasoning Processes of Expert Sys-
tems. In Proceedings of the Second National
Conference on Artificial Intelligence, Pitts-
burgh, Pennsylvania, August 18-20.
Earl D. Sacerdoti. 1975. A Structure for
Plans and Behavior. Technical Report TN-
109, SRI.
William R. Swartout and Stephen W.
Smoliar. 1987. On Making Expert Systems
more like Experts. Expert Systems, 4(3), Au-
gust.
2 1 1
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.951055">
<title confidence="0.998368">PLANNING TEXT FOR ADVISORY DIALOGUES*</title>
<author confidence="0.999886">Johanna D Moore</author>
<affiliation confidence="0.994474333333333">UCLA Department of Computer Science and USC/Information Sciences Institute</affiliation>
<address confidence="0.996783">4676 Admiralty Way Marina del Rey, CA 90292-6695, USA</address>
<author confidence="0.982994">Cecile L Paris</author>
<affiliation confidence="0.999774">USC/Information Sciences Institute</affiliation>
<address confidence="0.999259">4676 Admiralty Way Marina del Rey, CA 90292-6695, USA</address>
<abstract confidence="0.99941475">Explanation is an interactive process requiring a dialogue between advice-giver and advice-seeker. In this paper, we argue that in order to participate in a dialogue with its users, a generation system must be capable of reasoning about its own utterances and therefore must maintain a rich representation of the responses it produces. We present a text planner that constructs a detailed text plan, containing the intentional, attentional, and rhetorical structures of the text it generates.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Douglas E Appelt</author>
</authors>
<title>Planning Natural Language Utterances.</title>
<date>1985</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, England.</location>
<contexts>
<context position="2275" citStr="Appelt, 1985" startWordPosition="350" endWordPosition="351">discourse analysis, Grosz and Sidner (1986) argue that it is necessary to represent the intentional structure, the attentional structure (knowledge about which aspects of a dialogue are in focus at each point), and the linguistic structure of The research described in this paper was supported by the Defense Advanced Research Projects Agency (DARPA) under a NASA Ames cooperative agreement number NCC 2-520. The authors would like to thank William Swartout for comments on earlier versions of this paper. the discourse. In contrast, most text generation systems (with the notable exception of KAMP (Appelt, 1985)) have used only rhetorical and attentional information to produce coherent text (McKeown, 1985, McCoy, 1985, Paris, 1988b), omitting intentional information, or conflating intentional and rhetorical information (Hovy, 1988b). No text generation system records or reasons about the rhetorical, the attentional, as well as the intentional structures of the texts it produces. In this paper, we argue that to successfully participate in an explanation dialogue, a generation system must maintain the kinds of information outlined by Grosz and Sidner as well as an explicit representation of the rhetori</context>
<context position="26527" citStr="Appelt (1985)" startWordPosition="4329" endWordPosition="4330">he rationale for that behavior has been compiled out. Thus schemata cannot be used to participate in dialogues. If the user indicates that he has not understood the explanation, the system cannot know which part of the schema failed to achieve its effect on the hearer or which rhetorical strategy failed to achieve this effect. Planning a text using our approach is essentially planning a schema from more finegrained plan operators. From a library of such plan operators, many varied schemata can result, improving the flexibility of the system. In an approach taken by Cohen and Appelt (1979) and Appelt (1985), text is planned by reasoning about the beliefs of the hearer and speaker and the effects of surface speech &apos;We are also currently implementing another interface which allows users to use a mouse to point at the noun phrases or clauses in the text that were not understood (Moore, 1989b). 209 acts on these beliefs (i.e., the intentional effect). This approach does not include rhetorical knowledge about how clausal units may be combined into larger bodies of coherent text to achieve a speaker&apos;s goals. It assumes that appropriate axioms could be added to generate large (more than one- or two-sen</context>
</contexts>
<marker>Appelt, 1985</marker>
<rawString>Douglas E. Appelt. 1985. Planning Natural Language Utterances. Cambridge University Press, Cambridge, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John A Bateman</author>
<author>Cecile L Paris</author>
</authors>
<title>Phrasing a text in terms the user can understand.</title>
<date>1989</date>
<booktitle>In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence,</booktitle>
<location>Detroit, MI,</location>
<contexts>
<context position="17225" citStr="Bateman and Paris (1989)" startWordPosition="2807" endWordPosition="2810">s case, the planner must decide which, if any, are to be posted as subgoals. In this example, the first satellite will not be expanded because the user model indicates that the user is ca31n some cases, such as a satellite posting the rhetorical relation background, the satellite is expanded first. 4At this point, (RECOMMEND S H replace-1) must be translated into a form appropriate as input•to the realization component, the Penman system (Mann, 1983, Kasper, 1989). Based on the type of speech act, its arguments, and the context in which it occurs, the planner builds the appropriate structure. Bateman and Paris (1989) have begun to investigate the problem of phrasing utterances for different types of users. pable of performing replacement acts. The second satellite is expanded,5 posting the intentional subgoal to persuade the user to perform the replacement. A plan operator for achieving this goal using the rhetorical relation MOTIVATION was shown in Figure 1. When attempting to satisfy the constraints of the operator in Figure 1, the system first checks the constraints (GOAL S ?domain-goal) and (STEP replace-1 ?domain-goal). These constraints state that, in order to use this operator, the system must find</context>
</contexts>
<marker>Bateman, Paris, 1989</marker>
<rawString>John A. Bateman and Cecile L. Paris. 1989. Phrasing a text in terms the user can understand. In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, Detroit, MI, August 20-25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip R Cohen</author>
<author>Hector J Levesque</author>
</authors>
<title>Speech Acts and Rationality.</title>
<date>1985</date>
<booktitle>In Proceedings of the Twenty-Third Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>49--60</pages>
<institution>University of Chicago,</institution>
<location>Chicago, Illinois,</location>
<contexts>
<context position="7404" citStr="Cohen and Levesque (1985)" startWordPosition="1189" endWordPosition="1192">ntentional, attentional, and rhetorical structure of the generated text. In addition, since the expert system explanation facility is intended to be used by many different users, the text planner takes knowledge about the user into account. In our system, the user model contains the user&apos;s domain goals and the knowledge he is assumed to have about the domain. THE PLAN LANGUAGE In our plan language, intentional goals are represented in terms of the effects the speaker intends his utterance to have on the hearer. Following Hovy (1988a), we use the terminology for expressing beliefs developed by Cohen and Levesque (1985) in their theory of rational interaction, but have found the need to extend the terminology to represent the types of intentional goals necessary for the kinds of responses desired in an advisory setting. Although Cohen and Levesque have subsequently retracted some aspects of their theory of rational interaction (Cohen and Levesque, 1987), the utility of their notation for our purposes remains unaffected, as argued in (Hovy, 1989).2 &apos;PEA recommends transformations that improve the &apos;style&apos; of the user&apos;s code. It does not attempt to understand the content of the user&apos;s program. 2Space limitation</context>
</contexts>
<marker>Cohen, Levesque, 1985</marker>
<rawString>Philip R. Cohen and Hector J. Levesque. 1985. Speech Acts and Rationality. In Proceedings of the Twenty-Third Annual Meeting of the Association for Computational Linguistics, pages 49-60, University of Chicago, Chicago, Illinois, July 8-12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip R Cohen</author>
<author>Hector J Levesque</author>
</authors>
<date>1987</date>
<booktitle>Intention is Choice with Commitment,</booktitle>
<contexts>
<context position="7744" citStr="Cohen and Levesque, 1987" startWordPosition="1243" endWordPosition="1246">ave about the domain. THE PLAN LANGUAGE In our plan language, intentional goals are represented in terms of the effects the speaker intends his utterance to have on the hearer. Following Hovy (1988a), we use the terminology for expressing beliefs developed by Cohen and Levesque (1985) in their theory of rational interaction, but have found the need to extend the terminology to represent the types of intentional goals necessary for the kinds of responses desired in an advisory setting. Although Cohen and Levesque have subsequently retracted some aspects of their theory of rational interaction (Cohen and Levesque, 1987), the utility of their notation for our purposes remains unaffected, as argued in (Hovy, 1989).2 &apos;PEA recommends transformations that improve the &apos;style&apos; of the user&apos;s code. It does not attempt to understand the content of the user&apos;s program. 2Space limitations prohibit an exposition of their terminology in this paper. We provide English paraphrases where necessary for clarity. (MB S H should be read as &apos;the speaker believes the speaker and hearer mutually believe x.&apos; 2 04 EFFECT: (PERSUADE S H (GOAL H Eventually(DONE H ?act))) CONSTRAINTS: (AND (GOAL S ?domain-goal) (STEP ?act ?domain-goal) (</context>
</contexts>
<marker>Cohen, Levesque, 1987</marker>
<rawString>Philip R. Cohen and Hector J. Levesque. 1987. Intention is Choice with Commitment, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip R Cohen</author>
<author>C Raymond Perrault</author>
</authors>
<title>Elements of a Plan-based Theory of Speech Acts.</title>
<date>1979</date>
<journal>Cognitive Science,</journal>
<pages>3--177</pages>
<marker>Cohen, Perrault, 1979</marker>
<rawString>Philip R. Cohen and C. Raymond Perrault. 1979. Elements of a Plan-based Theory of Speech Acts. Cognitive Science, 3:177-212.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph E Grimes</author>
</authors>
<title>The Thread of Discourse.</title>
<date>1975</date>
<location>Mouton, The Hague, Paris.</location>
<contexts>
<context position="9581" citStr="Grimes, 1975" startWordPosition="1516" endWordPosition="1517"> structure is represented in terms of the rhetorical relations defined in Rhetorical Structure Theory (RST) (Mann and Thompson, 1987), a descriptive theory characterizing text structure in terms of the relations that hold between parts of a text (e.g., CONTRAST, MOTIVATION). The definition of each RST relation includes constraints on the two entities being related as well as constraints on their combination, and a specification of the effect which the speaker is attempting to achieve on the hearer&apos;s beliefs. Although other researchers have categorized typical intersentential relations (e.g., (Grimes, 1975, Hobbs, 1978)), the set of relations proposed by RST is the most complete and the theory sufficiently detailed to be easily adapted for use in generation. In our plan language, each plan operator consists of: • an effect: a characterization of what goal(s) this operator can be used to achieve. An effect may be an intentional goal, such as persuade the hearer to do an action or a rhetorical relation, such as provide motivation for an action. • a constraint list: a list of conditions that must be true before the operator can be applied. Constraints may refer to facts in the system&apos;s knowledge b</context>
</contexts>
<marker>Grimes, 1975</marker>
<rawString>Joseph E. Grimes. 1975. The Thread of Discourse. Mouton, The Hague, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Candace L Sidner</author>
</authors>
<date>1986</date>
<booktitle>Attention, Intention, and the Structure of Discourse. Computational Linguistics,</booktitle>
<pages>12--3</pages>
<contexts>
<context position="1705" citStr="Grosz and Sidner (1986)" startWordPosition="256" endWordPosition="259"> follow-up questions in the context of the ongoing conversation and to determine how to clarify a response when necessary. To provide these capabilities, an explanation facility must understand what it was trying to convey and how that information was conveyed, i.e., the intentional structure behind the explanation, including the goal of the explanation as a whole, the subgoal(s) of individual parts of the explanation, and the rhetorical means used to achieve them. Researchers in natural language understanding have recognized the need for such information. In their work on discourse analysis, Grosz and Sidner (1986) argue that it is necessary to represent the intentional structure, the attentional structure (knowledge about which aspects of a dialogue are in focus at each point), and the linguistic structure of The research described in this paper was supported by the Defense Advanced Research Projects Agency (DARPA) under a NASA Ames cooperative agreement number NCC 2-520. The authors would like to thank William Swartout for comments on earlier versions of this paper. the discourse. In contrast, most text generation systems (with the notable exception of KAMP (Appelt, 1985)) have used only rhetorical an</context>
<context position="4307" citStr="Grosz and Sidner (1986)" startWordPosition="690" endWordPosition="693">d not what effect the individual parts of the text were intended to have on the hearer and how they fit together to achieve this top-level goal, its only recourse is to use a different strategy to achieve the top-level goal. It is not able to re-explain or clarify any part of the explanation. There is thus a need for a text plan to contain a specification of the intended effect of individual parts of the text 203 on the hearer and how the parts relate to one another. We have developed a text planner that records the following information about the responses it produces: • the information that Grosz and Sidner (1986) have presented as the basics of a discourse structure: — intentional structure: a representation of the effect each part of the text is intended to have on the hearer and how the complete text achieves the overall discourse purpose (e.g., describe entity, persuade hearer to perform an action). — attentional structure: information about which objects, properties and events are salient at each point in the discourse. User&apos;s followup questions are often ambiguous. Information about the attentional state of the discourse can be used to disambiguate them (cf. (Moore and Swartout, 1989)). • in addi</context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>Barbara J. Grosz and Candace L. Sidner. 1986. Attention, Intention, and the Structure of Discourse. Computational Linguistics, 12(3):175-204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry Hobbs</author>
</authors>
<title>Why is a Discourse Coherent?</title>
<date>1978</date>
<tech>Technical Report 176,</tech>
<institution>SRI International.</institution>
<contexts>
<context position="9595" citStr="Hobbs, 1978" startWordPosition="1518" endWordPosition="1519">represented in terms of the rhetorical relations defined in Rhetorical Structure Theory (RST) (Mann and Thompson, 1987), a descriptive theory characterizing text structure in terms of the relations that hold between parts of a text (e.g., CONTRAST, MOTIVATION). The definition of each RST relation includes constraints on the two entities being related as well as constraints on their combination, and a specification of the effect which the speaker is attempting to achieve on the hearer&apos;s beliefs. Although other researchers have categorized typical intersentential relations (e.g., (Grimes, 1975, Hobbs, 1978)), the set of relations proposed by RST is the most complete and the theory sufficiently detailed to be easily adapted for use in generation. In our plan language, each plan operator consists of: • an effect: a characterization of what goal(s) this operator can be used to achieve. An effect may be an intentional goal, such as persuade the hearer to do an action or a rhetorical relation, such as provide motivation for an action. • a constraint list: a list of conditions that must be true before the operator can be applied. Constraints may refer to facts in the system&apos;s knowledge base or in the </context>
</contexts>
<marker>Hobbs, 1978</marker>
<rawString>Jerry Hobbs. 1978. Why is a Discourse Coherent? Technical Report 176, SRI International.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard H Hovy</author>
</authors>
<title>Generating Natural Language Under Pragmatic Constraints. Lawrence Erlbaum,</title>
<date>1988</date>
<location>Hillsdale, New Jersey.</location>
<contexts>
<context position="2498" citStr="Hovy, 1988" startWordPosition="380" endWordPosition="381">tic structure of The research described in this paper was supported by the Defense Advanced Research Projects Agency (DARPA) under a NASA Ames cooperative agreement number NCC 2-520. The authors would like to thank William Swartout for comments on earlier versions of this paper. the discourse. In contrast, most text generation systems (with the notable exception of KAMP (Appelt, 1985)) have used only rhetorical and attentional information to produce coherent text (McKeown, 1985, McCoy, 1985, Paris, 1988b), omitting intentional information, or conflating intentional and rhetorical information (Hovy, 1988b). No text generation system records or reasons about the rhetorical, the attentional, as well as the intentional structures of the texts it produces. In this paper, we argue that to successfully participate in an explanation dialogue, a generation system must maintain the kinds of information outlined by Grosz and Sidner as well as an explicit representation of the rhetorical structure of the texts it generates. We present a text planner that builds a detailed text plan, containing the intentional, attentional, and rhetorical structures of the responses it produces. The main focus of this pa</context>
<context position="7316" citStr="Hovy (1988" startWordPosition="1177" endWordPosition="1178">ans utterances to achieve discourse goals, building (and recording) the intentional, attentional, and rhetorical structure of the generated text. In addition, since the expert system explanation facility is intended to be used by many different users, the text planner takes knowledge about the user into account. In our system, the user model contains the user&apos;s domain goals and the knowledge he is assumed to have about the domain. THE PLAN LANGUAGE In our plan language, intentional goals are represented in terms of the effects the speaker intends his utterance to have on the hearer. Following Hovy (1988a), we use the terminology for expressing beliefs developed by Cohen and Levesque (1985) in their theory of rational interaction, but have found the need to extend the terminology to represent the types of intentional goals necessary for the kinds of responses desired in an advisory setting. Although Cohen and Levesque have subsequently retracted some aspects of their theory of rational interaction (Cohen and Levesque, 1987), the utility of their notation for our purposes remains unaffected, as argued in (Hovy, 1989).2 &apos;PEA recommends transformations that improve the &apos;style&apos; of the user&apos;s code</context>
<context position="27295" citStr="Hovy (1988" startWordPosition="4461" endWordPosition="4462">face which allows users to use a mouse to point at the noun phrases or clauses in the text that were not understood (Moore, 1989b). 209 acts on these beliefs (i.e., the intentional effect). This approach does not include rhetorical knowledge about how clausal units may be combined into larger bodies of coherent text to achieve a speaker&apos;s goals. It assumes that appropriate axioms could be added to generate large (more than one- or two-sentence) bodies of text and that the text produced will be coherent as a by-product of the planning process. However, this has not been demonstrated. Recently, Hovy (1988b) built a text structurer which produces a coherent text when given a set of inputs to express. Hovy uses an opportunistic planning approach that orders the inputs according to the constraints on the rhetorical relations defined in Rhetorical Structure Theory. His approach provides a description of what can be said when, but does not include information about why this information can or should be included at a particular point. Hovy&apos;s approach conflates intentional and rhetorical structure and, therefore, a system using his approach could not later reason about which rhetorical strategies wer</context>
<context position="29451" citStr="Hovy, 1988" startWordPosition="4826" endWordPosition="4827">tudying how the attentional structure inherent in our text plans can be used to guide the realization process, for example in the planning of referring expressions and the use of cue phrases and pronouns. We are also investigating criteria for the expansion and ordering of optional satellites in our plan operators. Currently we use information from the user model to dictate whether or not optional satellites are expanded, and their ordering is specified in each plan operator. We wish to extend our criteria for satellite expansion to include other factors such as pragmatic and stylistic goals (Hovy, 1988a) (e.g., brevity) and the conversation that has occurred so far. We are also investigating the use of attentional information to control the ordering of these satellites (McKeown, 1985). We also believe that the detailed text plan constructed by our planner will allow a system to modify its strategies based on experience (feedback from the user). In (Paris, 1988a), we outline our preliminary ideas on this issue. We have also begun to study how our planner can be used to handle incremental generation of texts. In (Moore, 1988), we argue that the detailed representation provided by our text pla</context>
</contexts>
<marker>Hovy, 1988</marker>
<rawString>Eduard H. Hovy. 1988a. Generating Natural Language Under Pragmatic Constraints. Lawrence Erlbaum, Hillsdale, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard H Hovy</author>
</authors>
<title>Planning Coherent Multisentential Text.</title>
<date>1988</date>
<booktitle>In Proceedings of the Twenty-Sixth Annual Meeting of the Association</booktitle>
<pages>7--10</pages>
<institution>for Computational Linguistics, State University of</institution>
<location>New York, Buffalo, New York,</location>
<contexts>
<context position="2498" citStr="Hovy, 1988" startWordPosition="380" endWordPosition="381">tic structure of The research described in this paper was supported by the Defense Advanced Research Projects Agency (DARPA) under a NASA Ames cooperative agreement number NCC 2-520. The authors would like to thank William Swartout for comments on earlier versions of this paper. the discourse. In contrast, most text generation systems (with the notable exception of KAMP (Appelt, 1985)) have used only rhetorical and attentional information to produce coherent text (McKeown, 1985, McCoy, 1985, Paris, 1988b), omitting intentional information, or conflating intentional and rhetorical information (Hovy, 1988b). No text generation system records or reasons about the rhetorical, the attentional, as well as the intentional structures of the texts it produces. In this paper, we argue that to successfully participate in an explanation dialogue, a generation system must maintain the kinds of information outlined by Grosz and Sidner as well as an explicit representation of the rhetorical structure of the texts it generates. We present a text planner that builds a detailed text plan, containing the intentional, attentional, and rhetorical structures of the responses it produces. The main focus of this pa</context>
<context position="7316" citStr="Hovy (1988" startWordPosition="1177" endWordPosition="1178">ans utterances to achieve discourse goals, building (and recording) the intentional, attentional, and rhetorical structure of the generated text. In addition, since the expert system explanation facility is intended to be used by many different users, the text planner takes knowledge about the user into account. In our system, the user model contains the user&apos;s domain goals and the knowledge he is assumed to have about the domain. THE PLAN LANGUAGE In our plan language, intentional goals are represented in terms of the effects the speaker intends his utterance to have on the hearer. Following Hovy (1988a), we use the terminology for expressing beliefs developed by Cohen and Levesque (1985) in their theory of rational interaction, but have found the need to extend the terminology to represent the types of intentional goals necessary for the kinds of responses desired in an advisory setting. Although Cohen and Levesque have subsequently retracted some aspects of their theory of rational interaction (Cohen and Levesque, 1987), the utility of their notation for our purposes remains unaffected, as argued in (Hovy, 1989).2 &apos;PEA recommends transformations that improve the &apos;style&apos; of the user&apos;s code</context>
<context position="27295" citStr="Hovy (1988" startWordPosition="4461" endWordPosition="4462">face which allows users to use a mouse to point at the noun phrases or clauses in the text that were not understood (Moore, 1989b). 209 acts on these beliefs (i.e., the intentional effect). This approach does not include rhetorical knowledge about how clausal units may be combined into larger bodies of coherent text to achieve a speaker&apos;s goals. It assumes that appropriate axioms could be added to generate large (more than one- or two-sentence) bodies of text and that the text produced will be coherent as a by-product of the planning process. However, this has not been demonstrated. Recently, Hovy (1988b) built a text structurer which produces a coherent text when given a set of inputs to express. Hovy uses an opportunistic planning approach that orders the inputs according to the constraints on the rhetorical relations defined in Rhetorical Structure Theory. His approach provides a description of what can be said when, but does not include information about why this information can or should be included at a particular point. Hovy&apos;s approach conflates intentional and rhetorical structure and, therefore, a system using his approach could not later reason about which rhetorical strategies wer</context>
<context position="29451" citStr="Hovy, 1988" startWordPosition="4826" endWordPosition="4827">tudying how the attentional structure inherent in our text plans can be used to guide the realization process, for example in the planning of referring expressions and the use of cue phrases and pronouns. We are also investigating criteria for the expansion and ordering of optional satellites in our plan operators. Currently we use information from the user model to dictate whether or not optional satellites are expanded, and their ordering is specified in each plan operator. We wish to extend our criteria for satellite expansion to include other factors such as pragmatic and stylistic goals (Hovy, 1988a) (e.g., brevity) and the conversation that has occurred so far. We are also investigating the use of attentional information to control the ordering of these satellites (McKeown, 1985). We also believe that the detailed text plan constructed by our planner will allow a system to modify its strategies based on experience (feedback from the user). In (Paris, 1988a), we outline our preliminary ideas on this issue. We have also begun to study how our planner can be used to handle incremental generation of texts. In (Moore, 1988), we argue that the detailed representation provided by our text pla</context>
</contexts>
<marker>Hovy, 1988</marker>
<rawString>Eduard H. Hovy. 1988b. Planning Coherent Multisentential Text. In Proceedings of the Twenty-Sixth Annual Meeting of the Association for Computational Linguistics, State University of New York, Buffalo, New York, June 7-10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard H Hovy</author>
</authors>
<title>Unresolved Issues in Paragraph Planning,</title>
<date>1989</date>
<booktitle>Presented at the Second European Workshop on Natural Language Generation.</booktitle>
<contexts>
<context position="7838" citStr="Hovy, 1989" startWordPosition="1261" endWordPosition="1262">the effects the speaker intends his utterance to have on the hearer. Following Hovy (1988a), we use the terminology for expressing beliefs developed by Cohen and Levesque (1985) in their theory of rational interaction, but have found the need to extend the terminology to represent the types of intentional goals necessary for the kinds of responses desired in an advisory setting. Although Cohen and Levesque have subsequently retracted some aspects of their theory of rational interaction (Cohen and Levesque, 1987), the utility of their notation for our purposes remains unaffected, as argued in (Hovy, 1989).2 &apos;PEA recommends transformations that improve the &apos;style&apos; of the user&apos;s code. It does not attempt to understand the content of the user&apos;s program. 2Space limitations prohibit an exposition of their terminology in this paper. We provide English paraphrases where necessary for clarity. (MB S H should be read as &apos;the speaker believes the speaker and hearer mutually believe x.&apos; 2 04 EFFECT: (PERSUADE S H (GOAL H Eventually(DONE H ?act))) CONSTRAINTS: (AND (GOAL S ?domain-goal) (STEP ?act ?domain-goal) (BMB S H (GOAL H ?domain-goal))) NUCLEUS: (FORALL ?domain-goal (MOTIVATION ?act ?domain-goal)) </context>
</contexts>
<marker>Hovy, 1989</marker>
<rawString>Eduard H. Hovy. 1989. Unresolved Issues in Paragraph Planning, April 6-8. Presented at the Second European Workshop on Natural Language Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Kasper</author>
</authors>
<title>SPL: A Sentence Plan Language for Text Generation.</title>
<date>1989</date>
<tech>Technical report, USC/ISI.</tech>
<contexts>
<context position="17069" citStr="Kasper, 1989" startWordPosition="2783" endWordPosition="2784">and so expansion of this branch of the plan is complete.4 Next, the planner must expand the satellites. Since both satellites are optional in this case, the planner must decide which, if any, are to be posted as subgoals. In this example, the first satellite will not be expanded because the user model indicates that the user is ca31n some cases, such as a satellite posting the rhetorical relation background, the satellite is expanded first. 4At this point, (RECOMMEND S H replace-1) must be translated into a form appropriate as input•to the realization component, the Penman system (Mann, 1983, Kasper, 1989). Based on the type of speech act, its arguments, and the context in which it occurs, the planner builds the appropriate structure. Bateman and Paris (1989) have begun to investigate the problem of phrasing utterances for different types of users. pable of performing replacement acts. The second satellite is expanded,5 posting the intentional subgoal to persuade the user to perform the replacement. A plan operator for achieving this goal using the rhetorical relation MOTIVATION was shown in Figure 1. When attempting to satisfy the constraints of the operator in Figure 1, the system first check</context>
</contexts>
<marker>Kasper, 1989</marker>
<rawString>Robert Kasper. 1989. SPL: A Sentence Plan Language for Text Generation. Technical report, USC/ISI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Rhetorical Structure Theory: A Theory of Text Organization.</title>
<date>1987</date>
<booktitle>In Livia Polanyi, Editor, The Structure of Discourse.</booktitle>
<publisher>Ablex Publishing Corporation,</publisher>
<location>Norwood, N.J.</location>
<contexts>
<context position="9102" citStr="Mann and Thompson, 1987" startWordPosition="1440" endWordPosition="1443">for Persuading the Hearer to Do An Act EFFECT: (MOTIVATION ?act ?domain-goal) CONSTRAINTS: (AND (GOAL S ?domain-goal) (STEP ?act ?domain-goal) (BMB S H (GOAL H ?domain-goal)) (ISA ?act REPLACE)) NUCLEUS: ((SETQ ?replacee (FILLER-OF OBJECT ?act)) (SETQ ?replacer (FILLER-OF GENERALIZED-MEANS ?act)) (BMB S H (DIFFERENCES ?replacee ?replacer ?domain-goal))) SATELLITES: nil Figure 2: Plan Operator for Motivating a Replacement by Describing Differences between Replacer and Replacee Rhetorical structure is represented in terms of the rhetorical relations defined in Rhetorical Structure Theory (RST) (Mann and Thompson, 1987), a descriptive theory characterizing text structure in terms of the relations that hold between parts of a text (e.g., CONTRAST, MOTIVATION). The definition of each RST relation includes constraints on the two entities being related as well as constraints on their combination, and a specification of the effect which the speaker is attempting to achieve on the hearer&apos;s beliefs. Although other researchers have categorized typical intersentential relations (e.g., (Grimes, 1975, Hobbs, 1978)), the set of relations proposed by RST is the most complete and the theory sufficiently detailed to be eas</context>
</contexts>
<marker>Mann, Thompson, 1987</marker>
<rawString>William C. Mann and Sandra A. Thompson. 1987. Rhetorical Structure Theory: A Theory of Text Organization. In Livia Polanyi, Editor, The Structure of Discourse. Ablex Publishing Corporation, Norwood, N.J.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Mann</author>
</authors>
<title>An Overview of the Penman Text Generation System.</title>
<date>1983</date>
<tech>Technical report, USC/ISI.</tech>
<contexts>
<context position="17054" citStr="Mann, 1983" startWordPosition="2781" endWordPosition="2782">e operator, and so expansion of this branch of the plan is complete.4 Next, the planner must expand the satellites. Since both satellites are optional in this case, the planner must decide which, if any, are to be posted as subgoals. In this example, the first satellite will not be expanded because the user model indicates that the user is ca31n some cases, such as a satellite posting the rhetorical relation background, the satellite is expanded first. 4At this point, (RECOMMEND S H replace-1) must be translated into a form appropriate as input•to the realization component, the Penman system (Mann, 1983, Kasper, 1989). Based on the type of speech act, its arguments, and the context in which it occurs, the planner builds the appropriate structure. Bateman and Paris (1989) have begun to investigate the problem of phrasing utterances for different types of users. pable of performing replacement acts. The second satellite is expanded,5 posting the intentional subgoal to persuade the user to perform the replacement. A plan operator for achieving this goal using the rhetorical relation MOTIVATION was shown in Figure 1. When attempting to satisfy the constraints of the operator in Figure 1, the sys</context>
</contexts>
<marker>Mann, 1983</marker>
<rawString>William Mann. 1983. An Overview of the Penman Text Generation System. Technical report, USC/ISI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen F McCoy</author>
</authors>
<title>Correcting Object-Related Misconceptions.</title>
<date>1985</date>
<tech>PhD thesis,</tech>
<institution>University of Pennsylvania, December. Published by University of Pennsylvania as</institution>
<contexts>
<context position="2383" citStr="McCoy, 1985" startWordPosition="366" endWordPosition="367"> the attentional structure (knowledge about which aspects of a dialogue are in focus at each point), and the linguistic structure of The research described in this paper was supported by the Defense Advanced Research Projects Agency (DARPA) under a NASA Ames cooperative agreement number NCC 2-520. The authors would like to thank William Swartout for comments on earlier versions of this paper. the discourse. In contrast, most text generation systems (with the notable exception of KAMP (Appelt, 1985)) have used only rhetorical and attentional information to produce coherent text (McKeown, 1985, McCoy, 1985, Paris, 1988b), omitting intentional information, or conflating intentional and rhetorical information (Hovy, 1988b). No text generation system records or reasons about the rhetorical, the attentional, as well as the intentional structures of the texts it produces. In this paper, we argue that to successfully participate in an explanation dialogue, a generation system must maintain the kinds of information outlined by Grosz and Sidner as well as an explicit representation of the rhetorical structure of the texts it generates. We present a text planner that builds a detailed text plan, contain</context>
</contexts>
<marker>McCoy, 1985</marker>
<rawString>Kathleen F. McCoy. 1985. Correcting Object-Related Misconceptions. PhD thesis, University of Pennsylvania, December. Published by University of Pennsylvania as Technical Report MS-CIS-85-57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen R McKeown</author>
</authors>
<title>Text Generation: Using Discourse Strategies and Focus Constraints to Generate Natural Language Text.</title>
<date>1985</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, England.</location>
<contexts>
<context position="2370" citStr="McKeown, 1985" startWordPosition="364" endWordPosition="365">onal structure, the attentional structure (knowledge about which aspects of a dialogue are in focus at each point), and the linguistic structure of The research described in this paper was supported by the Defense Advanced Research Projects Agency (DARPA) under a NASA Ames cooperative agreement number NCC 2-520. The authors would like to thank William Swartout for comments on earlier versions of this paper. the discourse. In contrast, most text generation systems (with the notable exception of KAMP (Appelt, 1985)) have used only rhetorical and attentional information to produce coherent text (McKeown, 1985, McCoy, 1985, Paris, 1988b), omitting intentional information, or conflating intentional and rhetorical information (Hovy, 1988b). No text generation system records or reasons about the rhetorical, the attentional, as well as the intentional structures of the texts it produces. In this paper, we argue that to successfully participate in an explanation dialogue, a generation system must maintain the kinds of information outlined by Grosz and Sidner as well as an explicit representation of the rhetorical structure of the texts it generates. We present a text planner that builds a detailed text </context>
<context position="25467" citStr="McKeown, 1985" startWordPosition="4138" endWordPosition="4139"> Moore (1989b) has devised recovery heuristics for selecting an alternative strategy when responding to such follow-up questions. Providing an alternative explanation would not be possible without the explicit representation of the intentional structure of the generated text. Note that it is important to record the rhetorical structure as well, so that the text planner can choose an alternative rhetorical strategy for achieving the goal. In the example under consideration, the recovery heuristics indicate that the rhetorical strategy of giving examples should be chosen. RELATED WORK Schemata (McKeown, 1985) encode standard patterns of discourse structure, but do not indude knowledge of how the various parts of a schema relate to one another or what their intended effect on the hearer is. A schema can be viewed as a compiled version of one of our text plans in which all of the nonterminal nodes have been pruned out and only the leaves (the speech acts) remain. While schemata can produce the same initial behavior as one of our text plans, all of the rationale for that behavior has been compiled out. Thus schemata cannot be used to participate in dialogues. If the user indicates that he has not und</context>
<context position="29637" citStr="McKeown, 1985" startWordPosition="4854" endWordPosition="4855"> phrases and pronouns. We are also investigating criteria for the expansion and ordering of optional satellites in our plan operators. Currently we use information from the user model to dictate whether or not optional satellites are expanded, and their ordering is specified in each plan operator. We wish to extend our criteria for satellite expansion to include other factors such as pragmatic and stylistic goals (Hovy, 1988a) (e.g., brevity) and the conversation that has occurred so far. We are also investigating the use of attentional information to control the ordering of these satellites (McKeown, 1985). We also believe that the detailed text plan constructed by our planner will allow a system to modify its strategies based on experience (feedback from the user). In (Paris, 1988a), we outline our preliminary ideas on this issue. We have also begun to study how our planner can be used to handle incremental generation of texts. In (Moore, 1988), we argue that the detailed representation provided by our text plans is necessary for execution monitoring and to indicate points in the planning process where feedback from the user may be helpful in incremental text planning. CONCLUSIONS In this pape</context>
</contexts>
<marker>McKeown, 1985</marker>
<rawString>Kathleen R McKeown. 1985. Text Generation: Using Discourse Strategies and Focus Constraints to Generate Natural Language Text. Cambridge University Press, Cambridge, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johanna D Moore</author>
<author>William R Swartout</author>
</authors>
<title>A Reactive Approach to Explanation.</title>
<date>1989</date>
<booktitle>In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence,</booktitle>
<location>Detroit, MI,</location>
<contexts>
<context position="4895" citStr="Moore and Swartout, 1989" startWordPosition="784" endWordPosition="787">ormation that Grosz and Sidner (1986) have presented as the basics of a discourse structure: — intentional structure: a representation of the effect each part of the text is intended to have on the hearer and how the complete text achieves the overall discourse purpose (e.g., describe entity, persuade hearer to perform an action). — attentional structure: information about which objects, properties and events are salient at each point in the discourse. User&apos;s followup questions are often ambiguous. Information about the attentional state of the discourse can be used to disambiguate them (cf. (Moore and Swartout, 1989)). • in addition, for generation we require the following: — rhetorical structure: an agent must understand how each part of the text relates rhetorically to the others. This is necessary for linguistic reasons (e.g., to generate the appropriate clausal connectives in multi-sentential responses) and for responding to requests for elaboration/clarification. assumption information: advicegiving systems must take knowledge about their users into account. However, since we cannot rely on having complete user models, these systems may have to make assumptions about the hearer in order to use a part</context>
<context position="13214" citStr="Moore and Swartout, 1989" startWordPosition="2145" endWordPosition="2148">m; (2) because there are many different rhetorical strategies for achieving any given intentional goal. For example, the system has several plan operators for achieving the intentional goal of describing a concept. It may describe a concept by stating its class membership and describing its attributes and its parts, by drawing an analogy to a similar concept, or by giving examples of the concept. There may also be many different plan operators for achieving a particular rhetorical strategy. (The planner employs selection heuristics for choosing among applicable operators in a given situation (Moore and Swartout, 1989).) Our plan language allows both general and specific plans to be represented. For example, Figure 2 shows a plan operator for achieving the rhetorical relation MOTIVATION. This is a very specific operator that can be used only when the act to be motivated is a replacement (e.g., replace setq with setf). In this case, one strategy for motivating the act is to compare the object being replaced and the object that replaces it with respect to the domain goal being achieved. On the other hand, the operator shown in Figure 3 is general and can be used to achieve mutual belief of any assertion by fi</context>
<context position="18823" citStr="Moore and Swartout, 1989" startWordPosition="3078" endWordPosition="3081">traint on the user model stating that the speaker and hearer should mutually believe that ?domain-goal is a goal of the hearer. Not all of the bindings found so fax will satisfy this constraint. Those which do not will not be rejected immediately, however, as we do not assume that the user model is complete. Instead, they will be noted as possible bindings, and each will be marked to indicate that, if this binding is used, an assumption is being made, namely that the binding of 51n other situations, the system could choose not to expand this satellite and await feedback from the user instead (Moore and Swartout, 1989). 207 (BMB S H (GOAL H Eventually (DONE H replace-1))) N (MOTIVATION replace1 enhance-maintainability) (RECOMMEND H replace-1) (PERSUADE S H (GOAL H Eventually (DONE H replace-1))) N (MOTIVATION replace-1 enhance-maintainability) (MB S H (DIFFERENCES setq setf enhance-maintainability)) N (BMB S H (DIFFERENCE setq setf use)) (INFORMS H (IDENTITY (VALUE-OF use setf) assign-value-to-generalized-variable)) (BMB S H (KNOW H generalized-variable)) (CONTRAST (IDENTITY (VALUE-OF use setq))) NI (INFORM S H (IDENTITY (VALUE-OF use setq) assign-value-to-simple-variable)) (INFORMS H (CLASS-ASCRIPTION gene</context>
</contexts>
<marker>Moore, Swartout, 1989</marker>
<rawString>Johanna D. Moore and William R. Swartout. 1989. A Reactive Approach to Explanation. In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, Detroit, MI, August 20-25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johanna D Moore</author>
</authors>
<title>Planning and Reacting.</title>
<date>1988</date>
<booktitle>In Proceedings of the AAAI Workshop on Text Planning and Generation, St Paul,</booktitle>
<location>Minnesota,</location>
<contexts>
<context position="29983" citStr="Moore, 1988" startWordPosition="4913" endWordPosition="4914">nsion to include other factors such as pragmatic and stylistic goals (Hovy, 1988a) (e.g., brevity) and the conversation that has occurred so far. We are also investigating the use of attentional information to control the ordering of these satellites (McKeown, 1985). We also believe that the detailed text plan constructed by our planner will allow a system to modify its strategies based on experience (feedback from the user). In (Paris, 1988a), we outline our preliminary ideas on this issue. We have also begun to study how our planner can be used to handle incremental generation of texts. In (Moore, 1988), we argue that the detailed representation provided by our text plans is necessary for execution monitoring and to indicate points in the planning process where feedback from the user may be helpful in incremental text planning. CONCLUSIONS In this paper, we have presented a text planner that builds a detailed text plan, containing the intentional, attentions&apos;, and rhetorical structures of the responses it produces. We argued that, in order to participate in a dialogue with its users, a generation system must be capable of reasoning about its past utterances. The text plans built by our text </context>
</contexts>
<marker>Moore, 1988</marker>
<rawString>Johanna D. Moore. 1988. Planning and Reacting. In Proceedings of the AAAI Workshop on Text Planning and Generation, St Paul, Minnesota, August 25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johanna D Moore</author>
</authors>
<title>Responding to &amp;quot;Huh?&amp;quot;: Answering Vaguely Articulated Follow-up Questions.</title>
<date>1989</date>
<booktitle>In Proceedings of the Conference on Human Factors in Computing Systems,</booktitle>
<location>Austin, Texas,</location>
<contexts>
<context position="24865" citStr="Moore (1989" startWordPosition="4049" endWordPosition="4050">ner must recognize that this discourse goal was attempted and not achieved by the last sentence of the previous explanation.6 Failure to do so would lead to simply repeating the description of a generalized variable that the user did not understand. By examining the text plan of the previous explanation recorded in the dialogue history, the explainer is able to determine whether the current goal (resulting from the follow-up question) is a goal that was attempted and failed, as it is in this case. This time, when attempting to achieve the goal, the planner must select an alternative strategy. Moore (1989b) has devised recovery heuristics for selecting an alternative strategy when responding to such follow-up questions. Providing an alternative explanation would not be possible without the explicit representation of the intentional structure of the generated text. Note that it is important to record the rhetorical structure as well, so that the text planner can choose an alternative rhetorical strategy for achieving the goal. In the example under consideration, the recovery heuristics indicate that the rhetorical strategy of giving examples should be chosen. RELATED WORK Schemata (McKeown, 198</context>
<context position="26813" citStr="Moore, 1989" startWordPosition="4380" endWordPosition="4381"> strategy failed to achieve this effect. Planning a text using our approach is essentially planning a schema from more finegrained plan operators. From a library of such plan operators, many varied schemata can result, improving the flexibility of the system. In an approach taken by Cohen and Appelt (1979) and Appelt (1985), text is planned by reasoning about the beliefs of the hearer and speaker and the effects of surface speech &apos;We are also currently implementing another interface which allows users to use a mouse to point at the noun phrases or clauses in the text that were not understood (Moore, 1989b). 209 acts on these beliefs (i.e., the intentional effect). This approach does not include rhetorical knowledge about how clausal units may be combined into larger bodies of coherent text to achieve a speaker&apos;s goals. It assumes that appropriate axioms could be added to generate large (more than one- or two-sentence) bodies of text and that the text produced will be coherent as a by-product of the planning process. However, this has not been demonstrated. Recently, Hovy (1988b) built a text structurer which produces a coherent text when given a set of inputs to express. Hovy uses an opportun</context>
<context position="28151" citStr="Moore, 1989" startWordPosition="4600" endWordPosition="4601">tructure Theory. His approach provides a description of what can be said when, but does not include information about why this information can or should be included at a particular point. Hovy&apos;s approach conflates intentional and rhetorical structure and, therefore, a system using his approach could not later reason about which rhetorical strategies were used to achieve intentional goals. STATUS AND FUTURE WORK The text planner presented is implemented in Common Lisp and can produce the text plans necessary to participate in the sample dialogue described in this paper and several others (see (Moore, 1989a, Paris, 1988a)). We currently have over 60 plan operators and the system can answer the following types of (follow-up) questions: - Why? - Why conclusion? - Why are you trying to achieve goal? - Why are you using method to achieve goal? - Why are you doing act? - How do you achieve goal? - How did you achieve goal (in this case)? - What is a concept? - What is the difference between conceptl and concept2? - Huh? The text planning system described in this paper is being incorporated into two expert systems currently under development. These systems will be installed and used in the field. Thi</context>
</contexts>
<marker>Moore, 1989</marker>
<rawString>Johanna D. Moore. 1989a. Responding to &amp;quot;Huh?&amp;quot;: Answering Vaguely Articulated Follow-up Questions. In Proceedings of the Conference on Human Factors in Computing Systems, Austin, Texas, April 30 - May 4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johanna D Moore</author>
</authors>
<title>A Reactive Approach to Explanation in Expert and AdviceGiving Systems.</title>
<date>1989</date>
<tech>PhD thesis,</tech>
<institution>University of California,</institution>
<location>Los Angeles, forthcoming.</location>
<contexts>
<context position="24865" citStr="Moore (1989" startWordPosition="4049" endWordPosition="4050">ner must recognize that this discourse goal was attempted and not achieved by the last sentence of the previous explanation.6 Failure to do so would lead to simply repeating the description of a generalized variable that the user did not understand. By examining the text plan of the previous explanation recorded in the dialogue history, the explainer is able to determine whether the current goal (resulting from the follow-up question) is a goal that was attempted and failed, as it is in this case. This time, when attempting to achieve the goal, the planner must select an alternative strategy. Moore (1989b) has devised recovery heuristics for selecting an alternative strategy when responding to such follow-up questions. Providing an alternative explanation would not be possible without the explicit representation of the intentional structure of the generated text. Note that it is important to record the rhetorical structure as well, so that the text planner can choose an alternative rhetorical strategy for achieving the goal. In the example under consideration, the recovery heuristics indicate that the rhetorical strategy of giving examples should be chosen. RELATED WORK Schemata (McKeown, 198</context>
<context position="26813" citStr="Moore, 1989" startWordPosition="4380" endWordPosition="4381"> strategy failed to achieve this effect. Planning a text using our approach is essentially planning a schema from more finegrained plan operators. From a library of such plan operators, many varied schemata can result, improving the flexibility of the system. In an approach taken by Cohen and Appelt (1979) and Appelt (1985), text is planned by reasoning about the beliefs of the hearer and speaker and the effects of surface speech &apos;We are also currently implementing another interface which allows users to use a mouse to point at the noun phrases or clauses in the text that were not understood (Moore, 1989b). 209 acts on these beliefs (i.e., the intentional effect). This approach does not include rhetorical knowledge about how clausal units may be combined into larger bodies of coherent text to achieve a speaker&apos;s goals. It assumes that appropriate axioms could be added to generate large (more than one- or two-sentence) bodies of text and that the text produced will be coherent as a by-product of the planning process. However, this has not been demonstrated. Recently, Hovy (1988b) built a text structurer which produces a coherent text when given a set of inputs to express. Hovy uses an opportun</context>
<context position="28151" citStr="Moore, 1989" startWordPosition="4600" endWordPosition="4601">tructure Theory. His approach provides a description of what can be said when, but does not include information about why this information can or should be included at a particular point. Hovy&apos;s approach conflates intentional and rhetorical structure and, therefore, a system using his approach could not later reason about which rhetorical strategies were used to achieve intentional goals. STATUS AND FUTURE WORK The text planner presented is implemented in Common Lisp and can produce the text plans necessary to participate in the sample dialogue described in this paper and several others (see (Moore, 1989a, Paris, 1988a)). We currently have over 60 plan operators and the system can answer the following types of (follow-up) questions: - Why? - Why conclusion? - Why are you trying to achieve goal? - Why are you using method to achieve goal? - Why are you doing act? - How do you achieve goal? - How did you achieve goal (in this case)? - What is a concept? - What is the difference between conceptl and concept2? - Huh? The text planning system described in this paper is being incorporated into two expert systems currently under development. These systems will be installed and used in the field. Thi</context>
</contexts>
<marker>Moore, 1989</marker>
<rawString>Johanna D. Moore. 1989b. A Reactive Approach to Explanation in Expert and AdviceGiving Systems. PhD thesis, University of California, Los Angeles, forthcoming.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Neches</author>
<author>William R Swartout</author>
<author>Johanna D Moore</author>
</authors>
<title>Enhanced Maintenance and Explanation of Expert Systems through Explicit Models of their Development.</title>
<date>1985</date>
<journal>IEEE Transactions on Software Engineering,</journal>
<volume>11</volume>
<issue>11</issue>
<marker>Neches, Swartout, Moore, 1985</marker>
<rawString>Robert Neches, William R. Swartout, and Johanna D. Moore. 1985. Enhanced Maintenance and Explanation of Expert Systems through Explicit Models of their Development. IEEE Transactions on Software Engineering, SE-11(11), November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cecile L Paris</author>
</authors>
<title>Generation and Explanation: Building an Explanation Facility for the Explainable Expert Systems Framework,</title>
<date>1988</date>
<booktitle>Presented at the Fourth International Workshop on Natural Language Generation.</booktitle>
<contexts>
<context position="2396" citStr="Paris, 1988" startWordPosition="368" endWordPosition="369">nal structure (knowledge about which aspects of a dialogue are in focus at each point), and the linguistic structure of The research described in this paper was supported by the Defense Advanced Research Projects Agency (DARPA) under a NASA Ames cooperative agreement number NCC 2-520. The authors would like to thank William Swartout for comments on earlier versions of this paper. the discourse. In contrast, most text generation systems (with the notable exception of KAMP (Appelt, 1985)) have used only rhetorical and attentional information to produce coherent text (McKeown, 1985, McCoy, 1985, Paris, 1988b), omitting intentional information, or conflating intentional and rhetorical information (Hovy, 1988b). No text generation system records or reasons about the rhetorical, the attentional, as well as the intentional structures of the texts it produces. In this paper, we argue that to successfully participate in an explanation dialogue, a generation system must maintain the kinds of information outlined by Grosz and Sidner as well as an explicit representation of the rhetorical structure of the texts it generates. We present a text planner that builds a detailed text plan, containing the inten</context>
<context position="28165" citStr="Paris, 1988" startWordPosition="4602" endWordPosition="4603">y. His approach provides a description of what can be said when, but does not include information about why this information can or should be included at a particular point. Hovy&apos;s approach conflates intentional and rhetorical structure and, therefore, a system using his approach could not later reason about which rhetorical strategies were used to achieve intentional goals. STATUS AND FUTURE WORK The text planner presented is implemented in Common Lisp and can produce the text plans necessary to participate in the sample dialogue described in this paper and several others (see (Moore, 1989a, Paris, 1988a)). We currently have over 60 plan operators and the system can answer the following types of (follow-up) questions: - Why? - Why conclusion? - Why are you trying to achieve goal? - Why are you using method to achieve goal? - Why are you doing act? - How do you achieve goal? - How did you achieve goal (in this case)? - What is a concept? - What is the difference between conceptl and concept2? - Huh? The text planning system described in this paper is being incorporated into two expert systems currently under development. These systems will be installed and used in the field. This will give us</context>
<context position="29816" citStr="Paris, 1988" startWordPosition="4884" endWordPosition="4885">l to dictate whether or not optional satellites are expanded, and their ordering is specified in each plan operator. We wish to extend our criteria for satellite expansion to include other factors such as pragmatic and stylistic goals (Hovy, 1988a) (e.g., brevity) and the conversation that has occurred so far. We are also investigating the use of attentional information to control the ordering of these satellites (McKeown, 1985). We also believe that the detailed text plan constructed by our planner will allow a system to modify its strategies based on experience (feedback from the user). In (Paris, 1988a), we outline our preliminary ideas on this issue. We have also begun to study how our planner can be used to handle incremental generation of texts. In (Moore, 1988), we argue that the detailed representation provided by our text plans is necessary for execution monitoring and to indicate points in the planning process where feedback from the user may be helpful in incremental text planning. CONCLUSIONS In this paper, we have presented a text planner that builds a detailed text plan, containing the intentional, attentions&apos;, and rhetorical structures of the responses it produces. We argued th</context>
</contexts>
<marker>Paris, 1988</marker>
<rawString>Cecile L. Paris. 1988a. Generation and Explanation: Building an Explanation Facility for the Explainable Expert Systems Framework, July 17-21. Presented at the Fourth International Workshop on Natural Language Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cecile L Paris</author>
</authors>
<title>Tailoring Object Descriptions to the User&apos;s Level of Expertise.</title>
<date>1988</date>
<journal>Computational Linguistics Journal,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="2396" citStr="Paris, 1988" startWordPosition="368" endWordPosition="369">nal structure (knowledge about which aspects of a dialogue are in focus at each point), and the linguistic structure of The research described in this paper was supported by the Defense Advanced Research Projects Agency (DARPA) under a NASA Ames cooperative agreement number NCC 2-520. The authors would like to thank William Swartout for comments on earlier versions of this paper. the discourse. In contrast, most text generation systems (with the notable exception of KAMP (Appelt, 1985)) have used only rhetorical and attentional information to produce coherent text (McKeown, 1985, McCoy, 1985, Paris, 1988b), omitting intentional information, or conflating intentional and rhetorical information (Hovy, 1988b). No text generation system records or reasons about the rhetorical, the attentional, as well as the intentional structures of the texts it produces. In this paper, we argue that to successfully participate in an explanation dialogue, a generation system must maintain the kinds of information outlined by Grosz and Sidner as well as an explicit representation of the rhetorical structure of the texts it generates. We present a text planner that builds a detailed text plan, containing the inten</context>
<context position="28165" citStr="Paris, 1988" startWordPosition="4602" endWordPosition="4603">y. His approach provides a description of what can be said when, but does not include information about why this information can or should be included at a particular point. Hovy&apos;s approach conflates intentional and rhetorical structure and, therefore, a system using his approach could not later reason about which rhetorical strategies were used to achieve intentional goals. STATUS AND FUTURE WORK The text planner presented is implemented in Common Lisp and can produce the text plans necessary to participate in the sample dialogue described in this paper and several others (see (Moore, 1989a, Paris, 1988a)). We currently have over 60 plan operators and the system can answer the following types of (follow-up) questions: - Why? - Why conclusion? - Why are you trying to achieve goal? - Why are you using method to achieve goal? - Why are you doing act? - How do you achieve goal? - How did you achieve goal (in this case)? - What is a concept? - What is the difference between conceptl and concept2? - Huh? The text planning system described in this paper is being incorporated into two expert systems currently under development. These systems will be installed and used in the field. This will give us</context>
<context position="29816" citStr="Paris, 1988" startWordPosition="4884" endWordPosition="4885">l to dictate whether or not optional satellites are expanded, and their ordering is specified in each plan operator. We wish to extend our criteria for satellite expansion to include other factors such as pragmatic and stylistic goals (Hovy, 1988a) (e.g., brevity) and the conversation that has occurred so far. We are also investigating the use of attentional information to control the ordering of these satellites (McKeown, 1985). We also believe that the detailed text plan constructed by our planner will allow a system to modify its strategies based on experience (feedback from the user). In (Paris, 1988a), we outline our preliminary ideas on this issue. We have also begun to study how our planner can be used to handle incremental generation of texts. In (Moore, 1988), we argue that the detailed representation provided by our text plans is necessary for execution monitoring and to indicate points in the planning process where feedback from the user may be helpful in incremental text planning. CONCLUSIONS In this paper, we have presented a text planner that builds a detailed text plan, containing the intentional, attentions&apos;, and rhetorical structures of the responses it produces. We argued th</context>
</contexts>
<marker>Paris, 1988</marker>
<rawString>Cecile L. Paris. 1988b. Tailoring Object Descriptions to the User&apos;s Level of Expertise. Computational Linguistics Journal, 14 (3), September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha E Pollack</author>
<author>Julia Hirschberg</author>
<author>Bonnie Lynn Webber</author>
</authors>
<title>User Participation in the Reasoning Processes of Expert Systems.</title>
<date>1982</date>
<booktitle>In Proceedings of the Second National Conference on Artificial Intelligence,</booktitle>
<location>Pittsburgh, Pennsylvania,</location>
<contexts>
<context position="966" citStr="Pollack et al., 1982" startWordPosition="140" endWordPosition="143"> a dialogue between advice-giver and advice-seeker. In this paper, we argue that in order to participate in a dialogue with its users, a generation system must be capable of reasoning about its own utterances and therefore must maintain a rich representation of the responses it produces. We present a text planner that constructs a detailed text plan, containing the intentional, attentional, and rhetorical structures of the text it generates. INTRODUCTION Providing explanations in an advisory situation is a highly interactive process, requiring a dialogue between advice-giver and adviceseeker (Pollack et al., 1982). Participating in a dialogue requires the ability to reason about previous responses, e.g., to interpret the user&apos;s follow-up questions in the context of the ongoing conversation and to determine how to clarify a response when necessary. To provide these capabilities, an explanation facility must understand what it was trying to convey and how that information was conveyed, i.e., the intentional structure behind the explanation, including the goal of the explanation as a whole, the subgoal(s) of individual parts of the explanation, and the rhetorical means used to achieve them. Researchers in</context>
</contexts>
<marker>Pollack, Hirschberg, Webber, 1982</marker>
<rawString>Martha E. Pollack, Julia Hirschberg, and Bonnie Lynn Webber. 1982. User Participation in the Reasoning Processes of Expert Systems. In Proceedings of the Second National Conference on Artificial Intelligence, Pittsburgh, Pennsylvania, August 18-20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Earl D Sacerdoti</author>
</authors>
<title>A Structure for Plans and Behavior.</title>
<date>1975</date>
<tech>Technical Report TN109, SRI.</tech>
<contexts>
<context position="6696" citStr="Sacerdoti (1975)" startWordPosition="1074" endWordPosition="1075">f them, the Program Enhancement Advisor (PEA) (Neches et a/., 1985). PEA is an advice-giving system intended to aid users in improving their Common Lisp programs by recommending transformations that enhance the user&apos;s code.&apos; The user supplies PEA with a program and indicates which characteristics of the program should be enhanced (any combination of readability, maintainability, and efficiency). PEA then recommends transformations. After each recommendation is made, the user is free to ask questions about the recommendation. We have implemented a top-down hierarchical expansion planner ( a la Sacerdoti (1975) ) that plans utterances to achieve discourse goals, building (and recording) the intentional, attentional, and rhetorical structure of the generated text. In addition, since the expert system explanation facility is intended to be used by many different users, the text planner takes knowledge about the user into account. In our system, the user model contains the user&apos;s domain goals and the knowledge he is assumed to have about the domain. THE PLAN LANGUAGE In our plan language, intentional goals are represented in terms of the effects the speaker intends his utterance to have on the hearer. </context>
</contexts>
<marker>Sacerdoti, 1975</marker>
<rawString>Earl D. Sacerdoti. 1975. A Structure for Plans and Behavior. Technical Report TN109, SRI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William R Swartout</author>
<author>Stephen W Smoliar</author>
</authors>
<title>On Making Expert Systems more like Experts.</title>
<date>1987</date>
<journal>Expert Systems,</journal>
<volume>4</volume>
<issue>3</issue>
<contexts>
<context position="5980" citStr="Swartout and Smoliar, 1987" startWordPosition="957" endWordPosition="961">wever, since we cannot rely on having complete user models, these systems may have to make assumptions about the hearer in order to use a particular explanation strategy. Whenever such assumptions are made, they must be recorded. The next sections describe this new text planner and show how it records the information needed to engage in a dialogue. Finally, a brief comparison with other approaches to text generation is presented. TEXT PLANNER The text planner has been developed as part of an explanation facility for an expert systern built using the Explainable Expert Systems (EES) framework (Swartout and Smoliar, 1987). The text planner has been used in two applications. In this paper, we draw our examples from one of them, the Program Enhancement Advisor (PEA) (Neches et a/., 1985). PEA is an advice-giving system intended to aid users in improving their Common Lisp programs by recommending transformations that enhance the user&apos;s code.&apos; The user supplies PEA with a program and indicates which characteristics of the program should be enhanced (any combination of readability, maintainability, and efficiency). PEA then recommends transformations. After each recommendation is made, the user is free to ask quest</context>
</contexts>
<marker>Swartout, Smoliar, 1987</marker>
<rawString>William R. Swartout and Stephen W. Smoliar. 1987. On Making Expert Systems more like Experts. Expert Systems, 4(3), August.</rawString>
</citation>
<citation valid="false">
<volume>2</volume>
<marker></marker>
<rawString>2 1 1</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>