<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000014">
<title confidence="0.994005">
Incrementality in Deterministic Dependency Parsing
</title>
<author confidence="0.992527">
Joakim Nivre
</author>
<affiliation confidence="0.9934345">
School of Mathematics and Systems Engineering
V¨axj¨o University
</affiliation>
<address confidence="0.873938">
SE-35195 V¨axj¨o
Sweden
</address>
<email confidence="0.996609">
joakim.nivre@msi.vxu.se
</email>
<sectionHeader confidence="0.993825" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999837875">
Deterministic dependency parsing is a robust
and efficient approach to syntactic parsing of
unrestricted natural language text. In this pa-
per, we analyze its potential for incremental
processing and conclude that strict incremen-
tality is not achievable within this framework.
However, we also show that it is possible to min-
imize the number of structures that require non-
incremental processing by choosing an optimal
parsing algorithm. This claim is substantiated
with experimental evidence showing that the al-
gorithm achieves incremental parsing for 68.9%
of the input when tested on a random sample
of Swedish text. When restricted to sentences
that are accepted by the parser, the degree of
incrementality increases to 87.9%.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999950278688525">
Incrementality in parsing has been advocated
for at least two different reasons. The first is
mainly practical and has to do with real-time
applications such as speech recognition, which
require a continually updated analysis of the in-
put received so far. The second reason is more
theoretical in that it connects parsing to cog-
nitive modeling, where there is psycholinguis-
tic evidence suggesting that human parsing is
largely incremental (Marslen-Wilson, 1973; Fra-
zier, 1987).
However, most state-of-the-art parsing meth-
ods today do not adhere to the principle of in-
crementality, for different reasons. Parsers that
attempt to disambiguate the input completely
— full parsing — typically first employ some
kind of dynamic programming algorithm to de-
rive a packed parse forest and then applies a
probabilistic top-down model in order to select
the most probable analysis (Collins, 1997; Char-
niak, 2000). Since the first step is essentially
nondeterministic, this seems to rule out incre-
mentality at least in a strict sense. By contrast,
parsers that only partially disambiguate the in-
put — partial parsing — are usually determin-
istic and construct the final analysis in one pass
over the input (Abney, 1991; Daelemans et al.,
1999). But since they normally output a se-
quence of unconnected phrases or chunks, they
fail to satisfy the constraint of incrementality
for a different reason.
Deterministic dependency parsing has re-
cently been proposed as a robust and effi-
cient method for syntactic parsing of unre-
stricted natural language text (Yamada and
Matsumoto, 2003; Nivre, 2003). In some ways,
this approach can be seen as a compromise be-
tween traditional full and partial parsing. Es-
sentially, it is a kind of full parsing in that the
goal is to build a complete syntactic analysis for
the input string, not just identify major con-
stituents. But it resembles partial parsing in
being robust, efficient and deterministic. Taken
together, these properties seem to make de-
pendency parsing suitable for incremental pro-
cessing, although existing implementations nor-
mally do not satisfy this constraint. For exam-
ple, Yamada and Matsumoto (2003) use a multi-
pass bottom-up algorithm, combined with sup-
port vector machines, in a way that does not
result in incremental processing.
In this paper, we analyze the constraints
on incrementality in deterministic dependency
parsing and argue that strict incrementality is
not achievable. We then analyze the algorithm
proposed in Nivre (2003) and show that, given
the previous result, this algorithm is optimal
from the point of view of incrementality. Fi-
nally, we evaluate experimentally the degree of
incrementality achieved with the algorithm in
practical parsing.
</bodyText>
<sectionHeader confidence="0.989431" genericHeader="introduction">
2 Dependency Parsing
</sectionHeader>
<bodyText confidence="0.999737">
In a dependency structure, every word token
is dependent on at most one other word to-
ken, usually called its head or regent, which
</bodyText>
<equation confidence="0.952896956521739">
ADV OBJ ATT
✞ ☎
✞ ☎
✞ ☎
PR
✞ ☎
❄
SUB
✞ ☎
❄
ATT
✞
❄
SUB
✞ ☎
❄
✞ OBJ
.&apos;N
.&apos;N
❄
ID
✞ ☎
❄
</equation>
<figure confidence="0.957752444444444">
❄
❄
❄
PP NN
P˚a 60-talet
(In the-60’s
VB PN
m˚alade han
painted he
JJ
dj¨arva
bold
NN
tavlor
HP
som
which
VB
retade
annoyed
PM
Nikita
Nikita
PM
Chrusjtjov.
Chrustjev.)
pictures
</figure>
<figureCaption confidence="0.999963">
Figure 1: Dependency graph for Swedish sentence
</figureCaption>
<bodyText confidence="0.999878625">
means that the structure can be represented as
a directed graph, with nodes representing word
tokens and arcs representing dependency rela-
tions. In addition, arcs may be labeled with
specific dependency types. Figure 1 shows a
labeled dependency graph for a simple Swedish
sentence, where each word of the sentence is la-
beled with its part of speech and each arc la-
beled with a grammatical function.
In the following, we will restrict our atten-
tion to unlabeled dependency graphs, i.e. graphs
without labeled arcs, but the results will ap-
ply to labeled dependency graphs as well. We
will also restrict ourselves to projective depen-
dency graphs (Mel’cuk, 1988). Formally, we de-
fine these structures in the following way:
</bodyText>
<listItem confidence="0.998401">
1. A dependency graph for a string of words
W = w1· · ·wn is a labeled directed graph
D = (W, A), where
(a) W is the set of nodes, i.e. word tokens
in the input string,
(b) A is a set of arcs (wz, wj) (wz, wj E W).
</listItem>
<bodyText confidence="0.983998352941176">
We write wz &lt; wj to express that wz pre-
cedes wj in the string W (i.e., i &lt; j); we
write wz —* wj to say that there is an arc
from wz to wj; we use —** to denote the re-
flexive and transitive closure of the arc re-
lation; and we use H and H* for the corre-
sponding undirected relations, i.e. wz H wj
iff wz —* wj or wj —* wz.
2. A dependency graph D = (W, A) is well-
formed iff the five conditions given in Fig-
ure 2 are satisfied.
The task of mapping a string W = w1· · ·wn
to a dependency graph satisfying these condi-
tions is what we call dependency parsing. For a
more detailed discussion of dependency graphs
and well-formedness conditions, the reader is re-
ferred to Nivre (2003).
</bodyText>
<sectionHeader confidence="0.774229" genericHeader="method">
3 Incrementality in Dependency
Parsing
</sectionHeader>
<bodyText confidence="0.9999656">
Having defined dependency graphs, we may
now consider to what extent it is possible to
construct these graphs incrementally. In the
strictest sense, we take incrementality to mean
that, at any point during the parsing process,
there is a single connected structure represent-
ing the analysis of the input consumed so far.
In terms of our dependency graphs, this would
mean that the graph being built during parsing
is connected at all times. We will try to make
this more precise in a minute, but first we want
to discuss the relation between incrementality
and determinism.
It seems that incrementality does not by itself
imply determinism, at least not in the sense of
never undoing previously made decisions. Thus,
a parsing method that involves backtracking can
be incremental, provided that the backtracking
is implemented in such a way that we can always
maintain a single structure representing the in-
put processed up to the point of backtracking.
In the context of dependency parsing, a case in
point is the parsing method proposed by Kro-
mann (Kromann, 2002), which combines heuris-
tic search with different repair mechanisms.
In this paper, we will nevertheless restrict our
attention to deterministic methods for depen-
dency parsing, because we think it is easier to
pinpoint the essential constraints within a more
restrictive framework. We will formalize deter-
ministic dependency parsing in a way which is
inspired by traditional shift-reduce parsing for
context-free grammars, using a buffer of input
tokens and a stack for storing previously pro-
cessed input. However, since there are no non-
terminal symbols involved in dependency pars-
ing, we also need to maintain a representation of
the dependency graph being constructed during
processing.
We will represent parser configurations by
</bodyText>
<figure confidence="0.860301666666667">
r r�
Unique label (wi —*wj n wi —*wj) r = r&apos;
Single head (wi —*wj n wk —*wj) wi = wk
Acyclic -(wi —*wj n wj —** wi)
Connected wi H* wj
Projective (wiHwk n wi&lt;wj&lt;wk) � (wi—**wj V wk—**wj)
</figure>
<figureCaption confidence="0.999455">
Figure 2: Well-formedness conditions on dependency graphs
</figureCaption>
<bodyText confidence="0.999151695652174">
triples (5, I, A), where 5 is the stack (repre-
sented as a list), I is the list of (remaining) input
tokens, and A is the (current) arc relation for
the dependency graph. (Since the nodes of the
dependency graph are given by the input string,
only the arc relation needs to be represented ex-
plicitly.) Given an input string W, the parser is
initialized to (nil, W, 0) and terminates when it
reaches a configuration (5, nil, A) (for any list
5 and set of arcs A). The input string W is
accepted if the dependency graph D = (W, A)
given at termination is well-formed; otherwise
W is rejected.
In order to understand the constraints on
incrementality in dependency parsing, we will
begin by considering the most straightforward
parsing strategy, i.e. left-to-right bottom-up
parsing, which in this case is essentially equiva-
lent to shift-reduce parsing with a context-free
grammar in Chomsky normal form. The parser
is defined in the form of a transition system,
represented in Figure 3 (where wi and wj are
arbitrary word tokens):
</bodyText>
<listItem confidence="0.9662968">
1. The transition Left-Reduce combines the
two topmost tokens on the stack, wi and
wj, by a left-directed arc wj —* wi and re-
duces them to the head wj.
2. The transition Right-Reduce combines
</listItem>
<bodyText confidence="0.94109172881356">
the two topmost tokens on the stack, wi
and wj, by a right-directed arc wi —* wj
and reduces them to the head wi.
3. The transition Shift pushes the next input
token wi onto the stack.
The transitions Left-Reduce and Right-
Reduce are subject to conditions that ensure
that the Single head condition is satisfied. For
Shift, the only condition is that the input list
is non-empty.
As it stands, this transition system is non-
deterministic, since several transitions can of-
ten be applied to the same configuration. Thus,
in order to get a deterministic parser, we need
to introduce a mechanism for resolving transi-
tion conflicts. Regardless of which mechanism
is used, the parser is guaranteed to terminate
after at most 2n transitions, given an input
string of length n. Moreover, the parser is guar-
anteed to produce a dependency graph that is
acyclic and projective (and satisfies the single-
head constraint). This means that the depen-
dency graph given at termination is well-formed
if and only if it is connected.
We can now define what it means for the pars-
ing to be incremental in this framework. Ide-
ally, we would like to require that the graph
(W —I, A) is connected at all times. How-
ever, given the definition of Left-Reduce and
Right-Reduce, it is impossible to connect a
new word without shifting it to the stack first,
so it seems that a more reasonable condition is
that the size of the stack should never exceed
2. In this way, we require every word to be at-
tached somewhere in the dependency graph as
soon as it has been shifted onto the stack.
We may now ask whether it is possible
to achieve incrementality with a left-to-right
bottom-up dependency parser, and the answer
turns out to be no in the general case. This can
be demonstrated by considering all the possible
projective dependency graphs containing only
three nodes and checking which of these can be
parsed incrementally. Figure 4 shows the rele-
vant structures, of which there are seven alto-
gether.
We begin by noting that trees (2–5) can all be
constructed incrementally by shifting the first
two tokens onto the stack, then reducing – with
Right-Reduce in (2–3) and Left-Reduce in
(4–5) – and then shifting and reducing again –
with Right-Reduce in (2) and (4) and Left-
Reduce in (3) and (5). By contrast, the three
remaining trees all require that three tokens are
Initialization hnil, W, ∅i
Termination hS, nil, Ai
Left-Reduce hwjwi|S, I, Ai → hwj|S, I, A ∪ {(wj, wi)}i ¬∃wk(wk, wi) ∈ A
Right-Reduce hwjwi|S, I, Ai → hwi|S, I, A ∪ {(wi, wj)}i ¬∃wk(wk, wj) ∈ A
Shift hS, wi|I, Ai → hwi|S, I, Ai
</bodyText>
<figureCaption confidence="0.9998875">
Figure 3: Left-to-right bottom-up dependency parsing
Figure 4: Projective three-node dependency structures
</figureCaption>
<figure confidence="0.998980242424243">
✞☎
✞☎
(2)
a b c
❄ ❄
✞☎
✞☎
(3) a b c
❄ ❄
✞☎
(4)
a b c
❄ ❄
✞ ☎
✞ ☎
✞ ☎
(1)
❄ ❄
a b c
✞ ☎
(5)
a b c
❄ ❄
✞ ☎
✞ ☎
(6)
a b c
❄ ❄
✞ ☎
✞ ☎
(7) a b c
❄ ❄
✞ ☎
</figure>
<bodyText confidence="0.999923164179105">
shifted onto the stack before the first reduction.
However, the reason why we cannot parse the
structure incrementally is different in (1) com-
pared to (6–7).
In (6–7) the problem is that the first two to-
kens are not connected by a single arc in the
final dependency graph. In (6) they are sisters,
both being dependents on the third token; in
(7) the first is the grandparent of the second.
And in pure dependency parsing without non-
terminal symbols, every reduction requires that
one of the tokens reduced is the head of the
other(s). This holds necessarily, regardless of
the algorithm used, and is the reason why it
is impossible to achieve strict incrementality in
dependency parsing as defined here. However,
it is worth noting that (2–3), which are the mir-
ror images of (6–7) can be parsed incrementally,
even though they contain adjacent tokens that
are not linked by a single arc. The reason is
that in (2–3) the reduction of the first two to-
kens makes the third token adjacent to the first.
Thus, the defining characteristic of the prob-
lematic structures is that precisely the leftmost
tokens are not linked directly.
The case of (1) is different in that here the
problem is caused by the strict bottom-up strat-
egy, which requires each token to have found
all its dependents before it is combined with its
head. For left-dependents this is not a problem,
as can be seen in (5), which can be processed
by alternating Shift and Left-Reduce. But in
(1) the sequence of reductions has to be per-
formed from right to left as it were, which rules
out strict incrementality. However, whereas the
structures exemplified in (6–7) can never be pro-
cessed incrementally within the present frame-
work, the structure in (1) can be handled by
modifying the parsing strategy, as we shall see
in the next section.
It is instructive at this point to make a com-
parison with incremental parsing based on ex-
tended categorial grammar, where the struc-
tures in (6–7) would normally be handled by
some kind of concatenation (or product), which
does not correspond to any real semantic com-
bination of the constituents (Steedman, 2000;
Morrill, 2000). By contrast, the structure in (1)
would typically be handled by function compo-
sition, which corresponds to a well-defined com-
positional semantic operation. Hence, it might
be argued that the treatment of (6–7) is only
pseudo-incremental even in other frameworks.
Before we leave the strict bottom-up ap-
proach, it can be noted that the algorithm de-
scribed in this section is essentially the algo-
rithm used by Yamada and Matsumoto (2003)
in combination with support vector machines,
except that they allow parsing to be performed
in multiple passes, where the graph produced in
one pass is given as input to the next pass.&apos; The
main motivation they give for parsing in multi-
ple passes is precisely the fact that the bottom-
up strategy requires each token to have found
all its dependents before it is combined with its
head, which is also what prevents the incremen-
tal parsing of structures like (1).
</bodyText>
<sectionHeader confidence="0.985157" genericHeader="method">
4 Arc-Eager Dependency Parsing
</sectionHeader>
<bodyText confidence="0.999120111111111">
In order to increase the incrementality of deter-
ministic dependency parsing, we need to com-
bine bottom-up and top-down processing. More
precisely, we need to process left-dependents
bottom-up and right-dependents top-down. In
this way, arcs will be added to the dependency
graph as soon as the respective head and depen-
dent are available, even if the dependent is not
complete with respect to its own dependents.
Following Abney and Johnson (1991), we will
call this arc-eager parsing, to distinguish it from
the standard bottom-up strategy discussed in
the previous section.
Using the same representation of parser con-
figurations as before, the arc-eager algorithm
can be defined by the transitions given in Fig-
ure 5, where wi and wj are arbitrary word to-
kens (Nivre, 2003):
</bodyText>
<listItem confidence="0.535471142857143">
1. The transition Left-Arc adds an arc
* wi from the next input token wj
r
to the token wi on top of the stack and
pops the stack.
2. The transition Right-Arc adds an arc
wi * wj from the token wi on top of
</listItem>
<equation confidence="0.502049">
r
</equation>
<bodyText confidence="0.96353">
the stack to the next input token wj, and
pushes wj onto the stack.
</bodyText>
<sectionHeader confidence="0.676167" genericHeader="method">
3. The transition Reduce pops the stack.
</sectionHeader>
<bodyText confidence="0.9289026">
4. The transition Shift (SH) pushes the next
input token wi onto the stack.
The transitions Left-Arc and Right-Arc, like
their counterparts Left-Reduce and Right-
Reduce, are subject to conditions that ensure
lA purely terminological, but potentially confusing,
difference is that Yamada and Matsumoto (2003) use the
term Right for what we call Left-Reduce and the term
Left for Right-Reduce (thus focusing on the position
of the head instead of the position of the dependent).
that the Single head constraint is satisfied,
while the Reduce transition can only be ap-
plied if the token on top of the stack already
has a head. The Shift transition is the same as
before and can be applied as long as the input
list is non-empty.
Comparing the two algorithms, we see that
the Left-Arc transition of the arc-eager algo-
rithm corresponds directly to the Left-Reduce
transition of the standard bottom-up algorithm.
The only difference is that, for reasons of sym-
metry, the former applies to the token on top
of the stack and the next input token instead
of the two topmost tokens on the stack. If we
compare Right-Arc to Right-Reduce, how-
ever, we see that the former performs no re-
duction but simply shifts the newly attached
right-dependent onto the stack, thus making
it possible for this dependent to have right-
dependents of its own. But in order to allow
multiple right-dependents, we must also have
a mechanism for popping right-dependents off
the stack, and this is the function of the Re-
duce transition. Thus, we can say that the
action performed by the Right-Reduce tran-
sition in the standard bottom-up algorithm is
performed by a Right-Arc transition in combi-
nation with a subsequent Reduce transition in
the arc-eager algorithm. And since the Right-
Arc and the Reduce can be separated by an
arbitrary number of transitions, this permits
the incremental parsing of arbitrary long right-
dependent chains.
Defining incrementality is less straightfor-
ward for the arc-eager algorithm than for the
standard bottom-up algorithm. Simply consid-
ering the size of the stack will not do anymore,
since the stack may now contain sequences of
tokens that form connected components of the
dependency graph. On the other hand, since it
is no longer necessary to shift both tokens to be
combined onto the stack, and since any tokens
that are popped off the stack are connected to
some token on the stack, we can require that
the graph (5, AS) should be connected at all
times, where AS is the restriction of A to 5, i.e.
AS = {(wi, wj) E A|wi, wj E 51.
Given this definition of incrementality, it is
easy to show that structures (2–5) in Figure 4
can be parsed incrementally with the arc-eager
algorithm as well as with the standard bottom-
up algorithm. However, with the new algorithm
we can also parse structure (1) incrementally, as
wj
Initialization hnil, W, ∅i
Termination hS, nil, Ai
Left-Arc hwi|S, wj|I, Ai → hS, wj|I, A ∪ {(wj, wi)}i ¬∃wk(wk, wi) ∈ A
Right-Arc hwi|S, wj|I, Ai → hwj|wi|S, I, A ∪ {(wi, wj)}i ¬∃wk(wk, wj) ∈ A
Reduce hwi|S, I, Ai → hS, I, Ai ∃wj(wj, wi) ∈ A
Shift hS, wi|I, Ai → hwi|S, I, Ai
</bodyText>
<figureCaption confidence="0.946417">
Figure 5: Left-to-right arc-eager dependency parsing
</figureCaption>
<bodyText confidence="0.956193547945205">
is shown by the following transition sequence:
hnil, abc, ∅i
↓ (Shift)
ha, bc, ∅i
↓ (Right-Arc)
hba, c, {(a, b)}i
↓ (Right-Arc)
hcba, nil, {(a, b), (b, c)}i
We conclude that the arc-eager algorithm is op-
timal with respect to incrementality in depen-
dency parsing, even though it still holds true
that the structures (6–7) in Figure 4 cannot be
parsed incrementally. This raises the question
how frequently these structures are found in
practical parsing, which is equivalent to asking
how often the arc-eager algorithm deviates from
strictly incremental processing. Although the
answer obviously depends on which language
and which theoretical framework we consider,
we will attempt to give at least a partial answer
to this question in the next section. Before that,
however, we want to relate our results to some
previous work on context-free parsing.
First of all, it should be observed that the
terms top-down and bottom-up take on a slightly
different meaning in the context of dependency
parsing, as compared to their standard use in
context-free parsing. Since there are no nonter-
minal nodes in a dependency graph, top-down
construction means that a head is attached to
a dependent before the dependent is attached
to (some of) its dependents, whereas bottom-
up construction means that a dependent is at-
tached to its head before the head is attached to
its head. However, top-down construction of de-
pendency graphs does not involve the prediction
of lower nodes from higher nodes, since all nodes
are given by the input string. Hence, in terms of
what drives the parsing process, all algorithms
discussed here correspond to bottom-up algo-
rithms in context-free parsing. It is interest-
ing to note that if we recast the problem of de-
pendency parsing as context-free parsing with a
CNF grammar, then the problematic structures
(1), (6–7) in Figure 4 all correspond to right-
branching structures, and it is well-known that
bottom-up parsers may require an unbounded
amount of memory in order to process right-
branching structure (Miller and Chomsky, 1963;
Abney and Johnson, 1991).
Moreover, if we analyze the two algorithms
discussed here in the framework of Abney and
Johnson (1991), they do not differ at all as to
the order in which nodes are enumerated, but
only with respect to the order in which arcs are
enumerated; the first algorithm is arc-standard
while the second is arc-eager. One of the obser-
vations made by Abney and Johnson (1991), is
that arc-eager strategies for context-free pars-
ing may sometimes require less space than arc-
standard strategies, although they may lead
to an increase in local ambiguities. It seems
that the advantage of the arc-eager strategy
for dependency parsing with respect to struc-
ture (1) in Figure 4 can be explained along the
same lines, although the lack of nonterminal
nodes in dependency graphs means that there
is no corresponding increase in local ambigui-
ties. Although a detailed discussion of the re-
lation between context-free parsing and depen-
dency parsing is beyond the scope of this paper,
we conjecture that this may be a genuine advan-
tage of dependency representations in parsing.
</bodyText>
<table confidence="0.991180214285714">
Connected Parser configurations
components Number Percent
0 1251 7.6
1 10148 61.3
2 2739 16.6
3 1471 8.9
4 587 3.5
5 222 1.3
6 98 0.6
7 26 0.2
8 3 0.0
&lt; 1 11399 68.9
&lt; 3 15609 94.3
&lt; 8 16545 100.0
</table>
<tableCaption confidence="0.999784">
Table 1: Number of connected components in (5, As) during parsing
</tableCaption>
<sectionHeader confidence="0.998304" genericHeader="evaluation">
5 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.999935234042553">
In order to measure the degree of incremental-
ity achieved in practical parsing, we have eval-
uated a parser that uses the arc-eager parsing
algorithm in combination with a memory-based
classifier for predicting the next transition. In
experiments reported in Nivre et al. (2004), a
parsing accuracy of 85.7% (unlabeled attach-
ment score) was achieved, using data from a
small treebank of Swedish (Einarsson, 1976), di-
vided into a training set of 5054 sentences and
a test set of 631 sentences. However, in the
present context, we are primarily interested in
the incrementality of the parser, which we mea-
sure by considering the number of connected
components in (5, As) at different stages dur-
ing the parsing of the test data.
The results can be found in Table 1, where
we see that out of 16545 configurations used in
parsing 613 sentences (with a mean length of
14.0 words), 68.9% have zero or one connected
component on the stack, which is what we re-
quire of a strictly incremental parser. We also
see that most violations of incrementality are
fairly mild, since more than 90% of all configu-
rations have no more than three connected com-
ponents on the stack.
Many violations of incrementality are caused
by sentences that cannot be parsed into a well-
formed dependency graph, i.e. a single projec-
tive dependency tree, but where the output of
the parser is a set of internally connected com-
ponents. In order to test the influence of incom-
plete parses on the statistics of incrementality,
we have performed a second experiment, where
we restrict the test data to those 444 sentences
(out of 613), for which the parser produces a
well-formed dependency graph. The results can
be seen in Table 2. In this case, 87.1% of all
configurations in fact satisfy the constraints of
incrementality, and the proportion of configu-
rations that have no more than three connected
components on the stack is as high as 99.5%.
It seems fair to conclude that, although strict
word-by-word incrementality is not possible in
deterministic dependency parsing, the arc-eager
algorithm can in practice be seen as a close ap-
proximation of incremental parsing.
</bodyText>
<sectionHeader confidence="0.999357" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9998925">
In this paper, we have analyzed the potential
for incremental processing in deterministic de-
pendency parsing. Our first result is negative,
since we have shown that strict incrementality
is not achievable within the restrictive parsing
framework considered here. However, we have
also shown that the arc-eager parsing algorithm
is optimal for incremental dependency parsing,
given the constraints imposed by the overall
framework. Moreover, we have shown that in
practical parsing, the algorithm performs in-
cremental processing for the majority of input
structures. If we consider all sentences in the
test data, the share is roughly two thirds, but if
we limit our attention to well-formed output, it
is almost 90%. Since deterministic dependency
parsing has previously been shown to be com-
petitive in terms of parsing accuracy (Yamada
and Matsumoto, 2003; Nivre et al., 2004), we
believe that this is a promising approach for sit-
uations that require parsing to be robust, effi-
cient and (almost) incremental.
</bodyText>
<figure confidence="0.943507916666667">
Connected Parser configurations
components Number Percent
0 928 9.2
1 7823 77.8
2 1000 10.0
3 248 2.5
4 41 0.4
5 8 0.1
6 1 0.0
&lt; 1 8751 87.1
&lt; 3 9999 99.5
&lt; 6 10049 100.0
</figure>
<tableCaption confidence="0.971609">
Table 2: Number of connected components in (S, As) for well-formed trees
</tableCaption>
<sectionHeader confidence="0.997687" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99994">
The work presented in this paper was sup-
ported by a grant from the Swedish Re-
search Council (621-2002-4207). The memory-
based classifiers used in the experiments were
constructed using the Tilburg Memory-Based
Learner (TiMBL) (Daelemans et al., 2003).
Thanks to three anonymous reviewers for con-
structive comments on the submitted paper.
</bodyText>
<sectionHeader confidence="0.998968" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999000723076923">
Steven Abney and Mark Johnson. 1991. Mem-
ory requirements and local ambiguities of
parsing strategies. Journal of Psycholinguis-
tic Research, 20:233–250.
Steven Abney. 1991. Parsing by chunks.
In Principle-Based Parsing, pages 257–278.
Kluwer.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings NAACL-
2000.
Michael Collins. 1997. Three generative, lexi-
calised models for statistical parsing. In Pro-
ceedings of the 35th Annatual Meeting of the
Association for Computational Linguistics,
pages 16–23, Madrid, Spain.
Walter Daelemans, Sabine Buchholz, and Jorn
Veenstra. 1999. Memory-based shallow pars-
ing. In Proceedings of the 3rd Conference
on Computational Natural Language Learn-
ing (CoNLL), pages 77–89.
Walter Daelemans, Jakub Zavrel, Ko van der
Sloot, and Antal van den Bosch. 2003.
Timbl: Tilburg memory based learner, ver-
sion 5.0, reference guide. Technical Report
ILK 03-10, Tilburg University, ILK.
Jan Einarsson. 1976. Talbankens skriftspr˚aks-
konkordans. Lund University.
Lyn Frazier. 1987. Syntactic processing: Ev-
idence from Dutch. Natural Language and
Linguistic Theory, 5:519–559.
Matthias Trautner Kromann. 2002. Optimality
parsing and local cost functions in Discontin-
uous Grammar. Electronic Notes of Theoret-
ical Computer Science, 52.
William Marslen-Wilson. 1973. Linguistic
structure and speech shadowing at very short
latencies. Nature, 244:522–533.
Igor Mel’cuk. 1988. Dependency Syntax: The-
ory and Practice. State University of New
York Press.
George A. Miller and Noam Chomsky. 1963.
Finitary models of language users. In R. D.
Luce, R. R. Bush, and E. Galanter, editors,
Handbook of Mathematical Psychology. Vol-
ume 2. Wiley.
Glyn Morrill. 2000. Inremental processing
and acceptability. Computational Linguis-
tics, 26:319–338.
Joakim Nivre, Johan Hall, and Jens Nils-
son. 2004. Memory-based dependency pars-
ing. In Proceedings of the 8th Conference
on Computational Natural Language Learn-
ing (CoNLL), pages 49–56.
Joakim Nivre. 2003. An efficient algorithm
for projective dependency parsing. In Pro-
ceedings of the 8th International Workshop
on Parsing Technologies (IWPT), pages 149–
160.
Mark Steedman. 2000. The Syntactic Process.
MIT Press.
Hiroyasu Yamada and Yuji Matsumoto. 2003.
Statistical dependency analysis with support
vector machines. In Proceedings of the 8th In-
ternational Workshop on Parsing Technolo-
gies (IWPT), pages 195–206.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.395480">
<title confidence="0.995802">Incrementality in Deterministic Dependency Parsing</title>
<author confidence="0.605889">Joakim</author>
<affiliation confidence="0.654449">School of Mathematics and Systems V¨axj¨o</affiliation>
<address confidence="0.589945">SE-35195</address>
<email confidence="0.979049">joakim.nivre@msi.vxu.se</email>
<abstract confidence="0.998029470588235">Deterministic dependency parsing is a robust and efficient approach to syntactic parsing of unrestricted natural language text. In this paper, we analyze its potential for incremental processing and conclude that strict incrementality is not achievable within this framework. However, we also show that it is possible to minimize the number of structures that require nonincremental processing by choosing an optimal parsing algorithm. This claim is substantiated with experimental evidence showing that the algorithm achieves incremental parsing for 68.9% of the input when tested on a random sample of Swedish text. When restricted to sentences that are accepted by the parser, the degree of incrementality increases to 87.9%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
<author>Mark Johnson</author>
</authors>
<title>Memory requirements and local ambiguities of parsing strategies.</title>
<date>1991</date>
<journal>Journal of Psycholinguistic Research,</journal>
<pages>20--233</pages>
<contexts>
<context position="15348" citStr="Abney and Johnson (1991)" startWordPosition="2652" endWordPosition="2655">to have found all its dependents before it is combined with its head, which is also what prevents the incremental parsing of structures like (1). 4 Arc-Eager Dependency Parsing In order to increase the incrementality of deterministic dependency parsing, we need to combine bottom-up and top-down processing. More precisely, we need to process left-dependents bottom-up and right-dependents top-down. In this way, arcs will be added to the dependency graph as soon as the respective head and dependent are available, even if the dependent is not complete with respect to its own dependents. Following Abney and Johnson (1991), we will call this arc-eager parsing, to distinguish it from the standard bottom-up strategy discussed in the previous section. Using the same representation of parser configurations as before, the arc-eager algorithm can be defined by the transitions given in Figure 5, where wi and wj are arbitrary word tokens (Nivre, 2003): 1. The transition Left-Arc adds an arc * wi from the next input token wj r to the token wi on top of the stack and pops the stack. 2. The transition Right-Arc adds an arc wi * wj from the token wi on top of r the stack to the next input token wj, and pushes wj onto the s</context>
<context position="21224" citStr="Abney and Johnson, 1991" startWordPosition="3660" endWordPosition="3663">lower nodes from higher nodes, since all nodes are given by the input string. Hence, in terms of what drives the parsing process, all algorithms discussed here correspond to bottom-up algorithms in context-free parsing. It is interesting to note that if we recast the problem of dependency parsing as context-free parsing with a CNF grammar, then the problematic structures (1), (6–7) in Figure 4 all correspond to rightbranching structures, and it is well-known that bottom-up parsers may require an unbounded amount of memory in order to process rightbranching structure (Miller and Chomsky, 1963; Abney and Johnson, 1991). Moreover, if we analyze the two algorithms discussed here in the framework of Abney and Johnson (1991), they do not differ at all as to the order in which nodes are enumerated, but only with respect to the order in which arcs are enumerated; the first algorithm is arc-standard while the second is arc-eager. One of the observations made by Abney and Johnson (1991), is that arc-eager strategies for context-free parsing may sometimes require less space than arcstandard strategies, although they may lead to an increase in local ambiguities. It seems that the advantage of the arc-eager strategy f</context>
</contexts>
<marker>Abney, Johnson, 1991</marker>
<rawString>Steven Abney and Mark Johnson. 1991. Memory requirements and local ambiguities of parsing strategies. Journal of Psycholinguistic Research, 20:233–250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<title>Parsing by chunks.</title>
<date>1991</date>
<booktitle>In Principle-Based Parsing,</booktitle>
<pages>257--278</pages>
<publisher>Kluwer.</publisher>
<contexts>
<context position="2136" citStr="Abney, 1991" startWordPosition="324" endWordPosition="325">reasons. Parsers that attempt to disambiguate the input completely — full parsing — typically first employ some kind of dynamic programming algorithm to derive a packed parse forest and then applies a probabilistic top-down model in order to select the most probable analysis (Collins, 1997; Charniak, 2000). Since the first step is essentially nondeterministic, this seems to rule out incrementality at least in a strict sense. By contrast, parsers that only partially disambiguate the input — partial parsing — are usually deterministic and construct the final analysis in one pass over the input (Abney, 1991; Daelemans et al., 1999). But since they normally output a sequence of unconnected phrases or chunks, they fail to satisfy the constraint of incrementality for a different reason. Deterministic dependency parsing has recently been proposed as a robust and efficient method for syntactic parsing of unrestricted natural language text (Yamada and Matsumoto, 2003; Nivre, 2003). In some ways, this approach can be seen as a compromise between traditional full and partial parsing. Essentially, it is a kind of full parsing in that the goal is to build a complete syntactic analysis for the input string</context>
</contexts>
<marker>Abney, 1991</marker>
<rawString>Steven Abney. 1991. Parsing by chunks. In Principle-Based Parsing, pages 257–278. Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropyinspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings NAACL2000.</booktitle>
<contexts>
<context position="1832" citStr="Charniak, 2000" startWordPosition="272" endWordPosition="274">etical in that it connects parsing to cognitive modeling, where there is psycholinguistic evidence suggesting that human parsing is largely incremental (Marslen-Wilson, 1973; Frazier, 1987). However, most state-of-the-art parsing methods today do not adhere to the principle of incrementality, for different reasons. Parsers that attempt to disambiguate the input completely — full parsing — typically first employ some kind of dynamic programming algorithm to derive a packed parse forest and then applies a probabilistic top-down model in order to select the most probable analysis (Collins, 1997; Charniak, 2000). Since the first step is essentially nondeterministic, this seems to rule out incrementality at least in a strict sense. By contrast, parsers that only partially disambiguate the input — partial parsing — are usually deterministic and construct the final analysis in one pass over the input (Abney, 1991; Daelemans et al., 1999). But since they normally output a sequence of unconnected phrases or chunks, they fail to satisfy the constraint of incrementality for a different reason. Deterministic dependency parsing has recently been proposed as a robust and efficient method for syntactic parsing </context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropyinspired parser. In Proceedings NAACL2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annatual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>16--23</pages>
<location>Madrid,</location>
<contexts>
<context position="1815" citStr="Collins, 1997" startWordPosition="270" endWordPosition="271">n is more theoretical in that it connects parsing to cognitive modeling, where there is psycholinguistic evidence suggesting that human parsing is largely incremental (Marslen-Wilson, 1973; Frazier, 1987). However, most state-of-the-art parsing methods today do not adhere to the principle of incrementality, for different reasons. Parsers that attempt to disambiguate the input completely — full parsing — typically first employ some kind of dynamic programming algorithm to derive a packed parse forest and then applies a probabilistic top-down model in order to select the most probable analysis (Collins, 1997; Charniak, 2000). Since the first step is essentially nondeterministic, this seems to rule out incrementality at least in a strict sense. By contrast, parsers that only partially disambiguate the input — partial parsing — are usually deterministic and construct the final analysis in one pass over the input (Abney, 1991; Daelemans et al., 1999). But since they normally output a sequence of unconnected phrases or chunks, they fail to satisfy the constraint of incrementality for a different reason. Deterministic dependency parsing has recently been proposed as a robust and efficient method for s</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of the 35th Annatual Meeting of the Association for Computational Linguistics, pages 16–23, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Sabine Buchholz</author>
<author>Jorn Veenstra</author>
</authors>
<title>Memory-based shallow parsing.</title>
<date>1999</date>
<booktitle>In Proceedings of the 3rd Conference on Computational Natural Language Learning (CoNLL),</booktitle>
<pages>77--89</pages>
<contexts>
<context position="2161" citStr="Daelemans et al., 1999" startWordPosition="326" endWordPosition="329">ers that attempt to disambiguate the input completely — full parsing — typically first employ some kind of dynamic programming algorithm to derive a packed parse forest and then applies a probabilistic top-down model in order to select the most probable analysis (Collins, 1997; Charniak, 2000). Since the first step is essentially nondeterministic, this seems to rule out incrementality at least in a strict sense. By contrast, parsers that only partially disambiguate the input — partial parsing — are usually deterministic and construct the final analysis in one pass over the input (Abney, 1991; Daelemans et al., 1999). But since they normally output a sequence of unconnected phrases or chunks, they fail to satisfy the constraint of incrementality for a different reason. Deterministic dependency parsing has recently been proposed as a robust and efficient method for syntactic parsing of unrestricted natural language text (Yamada and Matsumoto, 2003; Nivre, 2003). In some ways, this approach can be seen as a compromise between traditional full and partial parsing. Essentially, it is a kind of full parsing in that the goal is to build a complete syntactic analysis for the input string, not just identify major</context>
</contexts>
<marker>Daelemans, Buchholz, Veenstra, 1999</marker>
<rawString>Walter Daelemans, Sabine Buchholz, and Jorn Veenstra. 1999. Memory-based shallow parsing. In Proceedings of the 3rd Conference on Computational Natural Language Learning (CoNLL), pages 77–89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Jakub Zavrel</author>
<author>Ko van der Sloot</author>
<author>Antal van den Bosch</author>
</authors>
<title>Timbl: Tilburg memory based learner, version 5.0, reference guide.</title>
<date>2003</date>
<tech>Technical Report ILK 03-10,</tech>
<institution>Tilburg University, ILK.</institution>
<marker>Daelemans, Zavrel, van der Sloot, van den Bosch, 2003</marker>
<rawString>Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and Antal van den Bosch. 2003. Timbl: Tilburg memory based learner, version 5.0, reference guide. Technical Report ILK 03-10, Tilburg University, ILK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Einarsson</author>
</authors>
<title>Talbankens skriftspr˚akskonkordans.</title>
<date>1976</date>
<institution>Lund University.</institution>
<contexts>
<context position="22982" citStr="Einarsson, 1976" startWordPosition="3963" endWordPosition="3964">2739 16.6 3 1471 8.9 4 587 3.5 5 222 1.3 6 98 0.6 7 26 0.2 8 3 0.0 &lt; 1 11399 68.9 &lt; 3 15609 94.3 &lt; 8 16545 100.0 Table 1: Number of connected components in (5, As) during parsing 5 Experimental Evaluation In order to measure the degree of incrementality achieved in practical parsing, we have evaluated a parser that uses the arc-eager parsing algorithm in combination with a memory-based classifier for predicting the next transition. In experiments reported in Nivre et al. (2004), a parsing accuracy of 85.7% (unlabeled attachment score) was achieved, using data from a small treebank of Swedish (Einarsson, 1976), divided into a training set of 5054 sentences and a test set of 631 sentences. However, in the present context, we are primarily interested in the incrementality of the parser, which we measure by considering the number of connected components in (5, As) at different stages during the parsing of the test data. The results can be found in Table 1, where we see that out of 16545 configurations used in parsing 613 sentences (with a mean length of 14.0 words), 68.9% have zero or one connected component on the stack, which is what we require of a strictly incremental parser. We also see that most</context>
</contexts>
<marker>Einarsson, 1976</marker>
<rawString>Jan Einarsson. 1976. Talbankens skriftspr˚akskonkordans. Lund University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lyn Frazier</author>
</authors>
<title>Syntactic processing: Evidence from Dutch. Natural Language and Linguistic Theory,</title>
<date>1987</date>
<pages>5--519</pages>
<contexts>
<context position="1406" citStr="Frazier, 1987" startWordPosition="205" endWordPosition="207">edish text. When restricted to sentences that are accepted by the parser, the degree of incrementality increases to 87.9%. 1 Introduction Incrementality in parsing has been advocated for at least two different reasons. The first is mainly practical and has to do with real-time applications such as speech recognition, which require a continually updated analysis of the input received so far. The second reason is more theoretical in that it connects parsing to cognitive modeling, where there is psycholinguistic evidence suggesting that human parsing is largely incremental (Marslen-Wilson, 1973; Frazier, 1987). However, most state-of-the-art parsing methods today do not adhere to the principle of incrementality, for different reasons. Parsers that attempt to disambiguate the input completely — full parsing — typically first employ some kind of dynamic programming algorithm to derive a packed parse forest and then applies a probabilistic top-down model in order to select the most probable analysis (Collins, 1997; Charniak, 2000). Since the first step is essentially nondeterministic, this seems to rule out incrementality at least in a strict sense. By contrast, parsers that only partially disambiguat</context>
</contexts>
<marker>Frazier, 1987</marker>
<rawString>Lyn Frazier. 1987. Syntactic processing: Evidence from Dutch. Natural Language and Linguistic Theory, 5:519–559.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Trautner Kromann</author>
</authors>
<title>Optimality parsing and local cost functions in Discontinuous Grammar.</title>
<date>2002</date>
<journal>Electronic Notes of Theoretical Computer Science,</journal>
<volume>52</volume>
<contexts>
<context position="6819" citStr="Kromann, 2002" startWordPosition="1154" endWordPosition="1155">y to make this more precise in a minute, but first we want to discuss the relation between incrementality and determinism. It seems that incrementality does not by itself imply determinism, at least not in the sense of never undoing previously made decisions. Thus, a parsing method that involves backtracking can be incremental, provided that the backtracking is implemented in such a way that we can always maintain a single structure representing the input processed up to the point of backtracking. In the context of dependency parsing, a case in point is the parsing method proposed by Kromann (Kromann, 2002), which combines heuristic search with different repair mechanisms. In this paper, we will nevertheless restrict our attention to deterministic methods for dependency parsing, because we think it is easier to pinpoint the essential constraints within a more restrictive framework. We will formalize deterministic dependency parsing in a way which is inspired by traditional shift-reduce parsing for context-free grammars, using a buffer of input tokens and a stack for storing previously processed input. However, since there are no nonterminal symbols involved in dependency parsing, we also need to</context>
</contexts>
<marker>Kromann, 2002</marker>
<rawString>Matthias Trautner Kromann. 2002. Optimality parsing and local cost functions in Discontinuous Grammar. Electronic Notes of Theoretical Computer Science, 52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Marslen-Wilson</author>
</authors>
<title>Linguistic structure and speech shadowing at very short latencies.</title>
<date>1973</date>
<journal>Nature,</journal>
<pages>244--522</pages>
<contexts>
<context position="1390" citStr="Marslen-Wilson, 1973" startWordPosition="203" endWordPosition="204"> a random sample of Swedish text. When restricted to sentences that are accepted by the parser, the degree of incrementality increases to 87.9%. 1 Introduction Incrementality in parsing has been advocated for at least two different reasons. The first is mainly practical and has to do with real-time applications such as speech recognition, which require a continually updated analysis of the input received so far. The second reason is more theoretical in that it connects parsing to cognitive modeling, where there is psycholinguistic evidence suggesting that human parsing is largely incremental (Marslen-Wilson, 1973; Frazier, 1987). However, most state-of-the-art parsing methods today do not adhere to the principle of incrementality, for different reasons. Parsers that attempt to disambiguate the input completely — full parsing — typically first employ some kind of dynamic programming algorithm to derive a packed parse forest and then applies a probabilistic top-down model in order to select the most probable analysis (Collins, 1997; Charniak, 2000). Since the first step is essentially nondeterministic, this seems to rule out incrementality at least in a strict sense. By contrast, parsers that only parti</context>
</contexts>
<marker>Marslen-Wilson, 1973</marker>
<rawString>William Marslen-Wilson. 1973. Linguistic structure and speech shadowing at very short latencies. Nature, 244:522–533.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Mel’cuk</author>
</authors>
<title>Dependency Syntax: Theory and Practice.</title>
<date>1988</date>
<publisher>State University of New York Press.</publisher>
<marker>Mel’cuk, 1988</marker>
<rawString>Igor Mel’cuk. 1988. Dependency Syntax: Theory and Practice. State University of New York Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Noam Chomsky</author>
</authors>
<title>Finitary models of language users.</title>
<date>1963</date>
<booktitle>Handbook of Mathematical Psychology.</booktitle>
<volume>2</volume>
<editor>In R. D. Luce, R. R. Bush, and E. Galanter, editors,</editor>
<publisher>Wiley.</publisher>
<contexts>
<context position="21198" citStr="Miller and Chomsky, 1963" startWordPosition="3656" endWordPosition="3659">involve the prediction of lower nodes from higher nodes, since all nodes are given by the input string. Hence, in terms of what drives the parsing process, all algorithms discussed here correspond to bottom-up algorithms in context-free parsing. It is interesting to note that if we recast the problem of dependency parsing as context-free parsing with a CNF grammar, then the problematic structures (1), (6–7) in Figure 4 all correspond to rightbranching structures, and it is well-known that bottom-up parsers may require an unbounded amount of memory in order to process rightbranching structure (Miller and Chomsky, 1963; Abney and Johnson, 1991). Moreover, if we analyze the two algorithms discussed here in the framework of Abney and Johnson (1991), they do not differ at all as to the order in which nodes are enumerated, but only with respect to the order in which arcs are enumerated; the first algorithm is arc-standard while the second is arc-eager. One of the observations made by Abney and Johnson (1991), is that arc-eager strategies for context-free parsing may sometimes require less space than arcstandard strategies, although they may lead to an increase in local ambiguities. It seems that the advantage o</context>
</contexts>
<marker>Miller, Chomsky, 1963</marker>
<rawString>George A. Miller and Noam Chomsky. 1963. Finitary models of language users. In R. D. Luce, R. R. Bush, and E. Galanter, editors, Handbook of Mathematical Psychology. Volume 2. Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Glyn Morrill</author>
</authors>
<title>Inremental processing and acceptability.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<pages>26--319</pages>
<contexts>
<context position="13970" citStr="Morrill, 2000" startWordPosition="2427" endWordPosition="2428"> to left as it were, which rules out strict incrementality. However, whereas the structures exemplified in (6–7) can never be processed incrementally within the present framework, the structure in (1) can be handled by modifying the parsing strategy, as we shall see in the next section. It is instructive at this point to make a comparison with incremental parsing based on extended categorial grammar, where the structures in (6–7) would normally be handled by some kind of concatenation (or product), which does not correspond to any real semantic combination of the constituents (Steedman, 2000; Morrill, 2000). By contrast, the structure in (1) would typically be handled by function composition, which corresponds to a well-defined compositional semantic operation. Hence, it might be argued that the treatment of (6–7) is only pseudo-incremental even in other frameworks. Before we leave the strict bottom-up approach, it can be noted that the algorithm described in this section is essentially the algorithm used by Yamada and Matsumoto (2003) in combination with support vector machines, except that they allow parsing to be performed in multiple passes, where the graph produced in one pass is given as i</context>
</contexts>
<marker>Morrill, 2000</marker>
<rawString>Glyn Morrill. 2000. Inremental processing and acceptability. Computational Linguistics, 26:319–338.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Memory-based dependency parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the 8th Conference on Computational Natural Language Learning (CoNLL),</booktitle>
<pages>49--56</pages>
<contexts>
<context position="22848" citStr="Nivre et al. (2004)" startWordPosition="3940" endWordPosition="3943"> advantage of dependency representations in parsing. Connected Parser configurations components Number Percent 0 1251 7.6 1 10148 61.3 2 2739 16.6 3 1471 8.9 4 587 3.5 5 222 1.3 6 98 0.6 7 26 0.2 8 3 0.0 &lt; 1 11399 68.9 &lt; 3 15609 94.3 &lt; 8 16545 100.0 Table 1: Number of connected components in (5, As) during parsing 5 Experimental Evaluation In order to measure the degree of incrementality achieved in practical parsing, we have evaluated a parser that uses the arc-eager parsing algorithm in combination with a memory-based classifier for predicting the next transition. In experiments reported in Nivre et al. (2004), a parsing accuracy of 85.7% (unlabeled attachment score) was achieved, using data from a small treebank of Swedish (Einarsson, 1976), divided into a training set of 5054 sentences and a test set of 631 sentences. However, in the present context, we are primarily interested in the incrementality of the parser, which we measure by considering the number of connected components in (5, As) at different stages during the parsing of the test data. The results can be found in Table 1, where we see that out of 16545 configurations used in parsing 613 sentences (with a mean length of 14.0 words), 68.</context>
<context position="25593" citStr="Nivre et al., 2004" startWordPosition="4393" endWordPosition="4396">ere. However, we have also shown that the arc-eager parsing algorithm is optimal for incremental dependency parsing, given the constraints imposed by the overall framework. Moreover, we have shown that in practical parsing, the algorithm performs incremental processing for the majority of input structures. If we consider all sentences in the test data, the share is roughly two thirds, but if we limit our attention to well-formed output, it is almost 90%. Since deterministic dependency parsing has previously been shown to be competitive in terms of parsing accuracy (Yamada and Matsumoto, 2003; Nivre et al., 2004), we believe that this is a promising approach for situations that require parsing to be robust, efficient and (almost) incremental. Connected Parser configurations components Number Percent 0 928 9.2 1 7823 77.8 2 1000 10.0 3 248 2.5 4 41 0.4 5 8 0.1 6 1 0.0 &lt; 1 8751 87.1 &lt; 3 9999 99.5 &lt; 6 10049 100.0 Table 2: Number of connected components in (S, As) for well-formed trees Acknowledgements The work presented in this paper was supported by a grant from the Swedish Research Council (621-2002-4207). The memorybased classifiers used in the experiments were constructed using the Tilburg Memory-Bas</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2004</marker>
<rawString>Joakim Nivre, Johan Hall, and Jens Nilsson. 2004. Memory-based dependency parsing. In Proceedings of the 8th Conference on Computational Natural Language Learning (CoNLL), pages 49–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>An efficient algorithm for projective dependency parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT),</booktitle>
<pages>149--160</pages>
<contexts>
<context position="2511" citStr="Nivre, 2003" startWordPosition="383" endWordPosition="384">to rule out incrementality at least in a strict sense. By contrast, parsers that only partially disambiguate the input — partial parsing — are usually deterministic and construct the final analysis in one pass over the input (Abney, 1991; Daelemans et al., 1999). But since they normally output a sequence of unconnected phrases or chunks, they fail to satisfy the constraint of incrementality for a different reason. Deterministic dependency parsing has recently been proposed as a robust and efficient method for syntactic parsing of unrestricted natural language text (Yamada and Matsumoto, 2003; Nivre, 2003). In some ways, this approach can be seen as a compromise between traditional full and partial parsing. Essentially, it is a kind of full parsing in that the goal is to build a complete syntactic analysis for the input string, not just identify major constituents. But it resembles partial parsing in being robust, efficient and deterministic. Taken together, these properties seem to make dependency parsing suitable for incremental processing, although existing implementations normally do not satisfy this constraint. For example, Yamada and Matsumoto (2003) use a multipass bottom-up algorithm, c</context>
<context position="5713" citStr="Nivre (2003)" startWordPosition="972" endWordPosition="973">i.e., i &lt; j); we write wz —* wj to say that there is an arc from wz to wj; we use —** to denote the reflexive and transitive closure of the arc relation; and we use H and H* for the corresponding undirected relations, i.e. wz H wj iff wz —* wj or wj —* wz. 2. A dependency graph D = (W, A) is wellformed iff the five conditions given in Figure 2 are satisfied. The task of mapping a string W = w1· · ·wn to a dependency graph satisfying these conditions is what we call dependency parsing. For a more detailed discussion of dependency graphs and well-formedness conditions, the reader is referred to Nivre (2003). 3 Incrementality in Dependency Parsing Having defined dependency graphs, we may now consider to what extent it is possible to construct these graphs incrementally. In the strictest sense, we take incrementality to mean that, at any point during the parsing process, there is a single connected structure representing the analysis of the input consumed so far. In terms of our dependency graphs, this would mean that the graph being built during parsing is connected at all times. We will try to make this more precise in a minute, but first we want to discuss the relation between incrementality an</context>
<context position="15675" citStr="Nivre, 2003" startWordPosition="2708" endWordPosition="2709">ess left-dependents bottom-up and right-dependents top-down. In this way, arcs will be added to the dependency graph as soon as the respective head and dependent are available, even if the dependent is not complete with respect to its own dependents. Following Abney and Johnson (1991), we will call this arc-eager parsing, to distinguish it from the standard bottom-up strategy discussed in the previous section. Using the same representation of parser configurations as before, the arc-eager algorithm can be defined by the transitions given in Figure 5, where wi and wj are arbitrary word tokens (Nivre, 2003): 1. The transition Left-Arc adds an arc * wi from the next input token wj r to the token wi on top of the stack and pops the stack. 2. The transition Right-Arc adds an arc wi * wj from the token wi on top of r the stack to the next input token wj, and pushes wj onto the stack. 3. The transition Reduce pops the stack. 4. The transition Shift (SH) pushes the next input token wi onto the stack. The transitions Left-Arc and Right-Arc, like their counterparts Left-Reduce and RightReduce, are subject to conditions that ensure lA purely terminological, but potentially confusing, difference is that Y</context>
</contexts>
<marker>Nivre, 2003</marker>
<rawString>Joakim Nivre. 2003. An efficient algorithm for projective dependency parsing. In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT), pages 149– 160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="13954" citStr="Steedman, 2000" startWordPosition="2425" endWordPosition="2426">ormed from right to left as it were, which rules out strict incrementality. However, whereas the structures exemplified in (6–7) can never be processed incrementally within the present framework, the structure in (1) can be handled by modifying the parsing strategy, as we shall see in the next section. It is instructive at this point to make a comparison with incremental parsing based on extended categorial grammar, where the structures in (6–7) would normally be handled by some kind of concatenation (or product), which does not correspond to any real semantic combination of the constituents (Steedman, 2000; Morrill, 2000). By contrast, the structure in (1) would typically be handled by function composition, which corresponds to a well-defined compositional semantic operation. Hence, it might be argued that the treatment of (6–7) is only pseudo-incremental even in other frameworks. Before we leave the strict bottom-up approach, it can be noted that the algorithm described in this section is essentially the algorithm used by Yamada and Matsumoto (2003) in combination with support vector machines, except that they allow parsing to be performed in multiple passes, where the graph produced in one pa</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The Syntactic Process. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyasu Yamada</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT),</booktitle>
<pages>195--206</pages>
<contexts>
<context position="2497" citStr="Yamada and Matsumoto, 2003" startWordPosition="379" endWordPosition="382">ondeterministic, this seems to rule out incrementality at least in a strict sense. By contrast, parsers that only partially disambiguate the input — partial parsing — are usually deterministic and construct the final analysis in one pass over the input (Abney, 1991; Daelemans et al., 1999). But since they normally output a sequence of unconnected phrases or chunks, they fail to satisfy the constraint of incrementality for a different reason. Deterministic dependency parsing has recently been proposed as a robust and efficient method for syntactic parsing of unrestricted natural language text (Yamada and Matsumoto, 2003; Nivre, 2003). In some ways, this approach can be seen as a compromise between traditional full and partial parsing. Essentially, it is a kind of full parsing in that the goal is to build a complete syntactic analysis for the input string, not just identify major constituents. But it resembles partial parsing in being robust, efficient and deterministic. Taken together, these properties seem to make dependency parsing suitable for incremental processing, although existing implementations normally do not satisfy this constraint. For example, Yamada and Matsumoto (2003) use a multipass bottom-u</context>
<context position="14407" citStr="Yamada and Matsumoto (2003)" startWordPosition="2496" endWordPosition="2499"> in (6–7) would normally be handled by some kind of concatenation (or product), which does not correspond to any real semantic combination of the constituents (Steedman, 2000; Morrill, 2000). By contrast, the structure in (1) would typically be handled by function composition, which corresponds to a well-defined compositional semantic operation. Hence, it might be argued that the treatment of (6–7) is only pseudo-incremental even in other frameworks. Before we leave the strict bottom-up approach, it can be noted that the algorithm described in this section is essentially the algorithm used by Yamada and Matsumoto (2003) in combination with support vector machines, except that they allow parsing to be performed in multiple passes, where the graph produced in one pass is given as input to the next pass.&apos; The main motivation they give for parsing in multiple passes is precisely the fact that the bottomup strategy requires each token to have found all its dependents before it is combined with its head, which is also what prevents the incremental parsing of structures like (1). 4 Arc-Eager Dependency Parsing In order to increase the incrementality of deterministic dependency parsing, we need to combine bottom-up </context>
<context position="16301" citStr="Yamada and Matsumoto (2003)" startWordPosition="2819" endWordPosition="2822">): 1. The transition Left-Arc adds an arc * wi from the next input token wj r to the token wi on top of the stack and pops the stack. 2. The transition Right-Arc adds an arc wi * wj from the token wi on top of r the stack to the next input token wj, and pushes wj onto the stack. 3. The transition Reduce pops the stack. 4. The transition Shift (SH) pushes the next input token wi onto the stack. The transitions Left-Arc and Right-Arc, like their counterparts Left-Reduce and RightReduce, are subject to conditions that ensure lA purely terminological, but potentially confusing, difference is that Yamada and Matsumoto (2003) use the term Right for what we call Left-Reduce and the term Left for Right-Reduce (thus focusing on the position of the head instead of the position of the dependent). that the Single head constraint is satisfied, while the Reduce transition can only be applied if the token on top of the stack already has a head. The Shift transition is the same as before and can be applied as long as the input list is non-empty. Comparing the two algorithms, we see that the Left-Arc transition of the arc-eager algorithm corresponds directly to the Left-Reduce transition of the standard bottom-up algorithm. </context>
<context position="25572" citStr="Yamada and Matsumoto, 2003" startWordPosition="4389" endWordPosition="4392">rsing framework considered here. However, we have also shown that the arc-eager parsing algorithm is optimal for incremental dependency parsing, given the constraints imposed by the overall framework. Moreover, we have shown that in practical parsing, the algorithm performs incremental processing for the majority of input structures. If we consider all sentences in the test data, the share is roughly two thirds, but if we limit our attention to well-formed output, it is almost 90%. Since deterministic dependency parsing has previously been shown to be competitive in terms of parsing accuracy (Yamada and Matsumoto, 2003; Nivre et al., 2004), we believe that this is a promising approach for situations that require parsing to be robust, efficient and (almost) incremental. Connected Parser configurations components Number Percent 0 928 9.2 1 7823 77.8 2 1000 10.0 3 248 2.5 4 41 0.4 5 8 0.1 6 1 0.0 &lt; 1 8751 87.1 &lt; 3 9999 99.5 &lt; 6 10049 100.0 Table 2: Number of connected components in (S, As) for well-formed trees Acknowledgements The work presented in this paper was supported by a grant from the Swedish Research Council (621-2002-4207). The memorybased classifiers used in the experiments were constructed using t</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT), pages 195–206.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>