<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011146">
<title confidence="0.9989065">
Compositional-ly Derived Representations of
Morphologically Complex Words in Distributional Semantics
</title>
<author confidence="0.922724">
Angeliki Lazaridou and Marco Marelli and Roberto Zamparelli and Marco Baroni
</author>
<affiliation confidence="0.899875">
Center for Mind/Brain Sciences (University of Trento, Italy)
</affiliation>
<email confidence="0.977032">
first.last@unitn.it
</email>
<sectionHeader confidence="0.993266" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99993945">
Speakers of a language can construct an
unlimited number of new words through
morphological derivation. This is a major
cause of data sparseness for corpus-based
approaches to lexical semantics, such as
distributional semantic models of word
meaning. We adapt compositional meth-
ods originally developed for phrases to the
task of deriving the distributional meaning
of morphologically complex words from
their parts. Semantic representations con-
structed in this way beat a strong baseline
and can be of higher quality than represen-
tations directly constructed from corpus
data. Our results constitute a novel evalua-
tion of the proposed composition methods,
in which the full additive model achieves
the best performance, and demonstrate the
usefulness of a compositional morphology
component in distributional semantics.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.981937048387097">
Effective ways to represent word meaning are
needed in many branches of natural language pro-
cessing. In the last decades, corpus-based meth-
ods have achieved some degree of success in mod-
eling lexical semantics. Distributional semantic
models (DSMs) in particular represent the mean-
ing of a word by a vector, the dimensions of which
encode corpus-extracted co-occurrence statistics,
under the assumption that words that are semanti-
cally similar will occur in similar contexts (Turney
and Pantel, 2010). Reliable distributional vectors
can only be extracted for words that occur in many
contexts in the corpus. Not surprisingly, there is
a strong correlation between word frequency and
vector quality (Bullinaria and Levy, 2007), and
since most words occur only once even in very
large corpora (Baroni, 2009), DSMs suffer data
sparseness.
While word rarity has many sources, one of the
most common and systematic ones is the high pro-
ductivity of morphological derivation processes,
whereby an unlimited number of new words can
be constructed by adding affixes to existing stems
(Baayen, 2005; Bauer, 2001; Plag, 1999).1 For
example, in the multi-billion-word corpus we in-
troduce below, perfectly reasonable derived forms
such as lexicalizable or affixless never occur. Even
without considering the theoretically infinite num-
ber of possible derived nonce words, and restrict-
ing ourselves instead to words that are already
listed in dictionaries, complex forms cover a high
portion of the lexicon. For example, morphologi-
cally complex forms account for 55% of the lem-
mas in the CELEX English database (see Section
4.1 below). In most of these cases (80% according
to our corpus) the stem is more frequent than the
complex form (e.g., the stem build occurs 15 times
more often than the derived form rebuild, and the
latter is certainly not an unusual derived form).
DSMs ignore derivational morphology alto-
gether. Consequently, they cannot provide mean-
ing representations for new derived forms, nor can
they harness the systematic relation existing be-
tween stems and derivations (any English speaker
can infer that to rebuild is to build again, whether
they are familiar with the prefixed form or not)
in order to mitigate derived-form sparseness prob-
lems. A simple way to handle derivational mor-
1Morphological derivation constructs new words (in
the sense of lemmas) from existing lexical items (re-
source+ful→resourceful). In this work, we do not treat in-
flectional morphology, pertaining to affixes that encode gram-
matical features such as number or tense (dog+s). We use
morpheme for any component of a word (resource and -ful
are both morphemes). We use stem for the lexical item that
constitutes the base of derivation (resource) and affix (pre-
fix or suffix) for the element attached to the stem to derive
the new form (-ful). In English, stems are typically indepen-
dent words, affixes bound morphemes, i.e., they cannot stand
alone. Note that a stem can in turn be morphologically de-
rived, e.g., point+less in pointless+ly. Finally, we use mor-
phologically complex as synonymous with derived.
</bodyText>
<page confidence="0.940854">
1517
</page>
<note confidence="0.9136935">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1517–1526,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999983229166667">
phology would be to identify the stem of rare de-
rived words and use its distributional vector as a
proxy to derived-form meaning.2 The meaning of
rebuild is not that far from that of build, so the
latter might provide a reasonable surrogate. Still,
something is clearly lost (if the author of a text
felt the need to use the derived form, the stem was
not fully appropriate), and sometimes the jump in
meaning can be quite dramatic (resourceless and
resource mean very different things!).
In the past few years there has been much in-
terest in how DSMs can scale up to represent the
meaning of larger chunks of text such as phrases
or even sentences. Trying to represent the mean-
ing of arbitrarily long constructions by directly
collecting co-occurrence statistics is obviously in-
effective and thus methods have been developed
to derive the meaning of larger constructions as a
function of the meaning of their constituents (Ba-
roni and Zamparelli, 2010; Coecke et al., 2010;
Mitchell and Lapata, 2008; Mitchell and Lapata,
2010; Socher et al., 2012). Compositional distri-
butional semantic models (cDSMs) of word units
aim at handling, compositionally, the high produc-
tivity of phrases and consequent data sparseness.
It is natural to hypothesize that the same methods
can be applied to morphology to derive the mean-
ing of complex words from the meaning of their
parts: For example, instead of harvesting a rebuild
vector directly from the corpus, the latter could be
constructed from the distributional representations
of re- and build. Besides alleviating data sparse-
ness problems, a system of this sort, that automati-
cally induces the semantic contents of morpholog-
ical processes, would also be of tremendous theo-
retical interest, given that the semantics of deriva-
tion is a central and challenging topic in linguistic
morphology (Dowty, 1979; Lieber, 2004).
In this paper, we explore, for the first time (ex-
cept for the proof-of-concept study in Guevara
(2009)), the application of cDSMs to derivational
morphology. We adapt a number of composition
methods from the literature to the morphological
setting, and we show that some of these methods
can provide better distributional representations of
derived forms than either those directly harvested
from a large corpus, or those obtained by using
the stem as a proxy to derived-form meaning. Our
</bodyText>
<footnote confidence="0.981655">
2Of course, spotting and segmenting complex words is a
big research topic unto itself (Beesley and Karttunen, 2000;
Black et al., 1991; Sproat, 1992), and one we completely
sidestep here.
</footnote>
<bodyText confidence="0.999083">
results suggest that exploiting morphology could
improve the quality of DSMs in general, extend
the range of tasks that cDSMs can successfully
model and support the development of new ways
to test their performance.
</bodyText>
<sectionHeader confidence="0.9987" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999875386363636">
Morphological induction systems use corpus-
based methods to decide if two words are mor-
phologically related and/or to segment words into
morphemes (Dreyer and Eisner, 2011; Goldsmith,
2001; Goldwater and McClosky, 2005; Goldwater,
2006; Naradowsky and Goldwater, 2009; Wicen-
towski, 2004). Morphological induction has re-
cently received considerable attention since mor-
phological analysis can mitigate data sparseness in
domains such as parsing and machine translation
(Goldberg and Tsarfaty, 2008; Lee, 2004). Among
the cues that have been exploited there is distri-
butional similarity among morphologically related
words (Schone and Jurafsky, 2000; Yarowsky and
Wicentowski, 2000). Our work, however, dif-
fers substantially from this track of research. We
do not aim at segmenting morphological complex
words or identifying paradigms. Our goal is to
automatically construct, given distributional rep-
resentations of stems and affixes, semantic repre-
sentations for the derived words containing those
stems and affixes. A morphological induction sys-
tem, given rebuild, will segment it into re- and
build (possibly using distributional similarity be-
tween the words as a cue). Our system, given
re- and build, predicts the (distributional seman-
tic) meaning of rebuild.
Another emerging line of research uses distribu-
tional semantics to model human intuitions about
the semantic transparency of morphologically de-
rived or compound expressions and how these im-
pact various lexical processing tasks (Kuperman,
2009; Wang et al., 2012). Although these works
exploit vectors representing complex forms, they
do not attempt to generate them compositionally.
The only similar study we are aware of is that
of Guevara (2009). Guevara found a systematic
geometric relation between corpus-based vectors
of derived forms sharing an affix and their stems,
and used this finding to motivate the composition
method we term lexfunc below. However, unlike
us, he did not test alternative models, and he only
presented a qualitative analysis of the trajectories
triggered by composition with various affixes.
</bodyText>
<page confidence="0.99545">
1518
</page>
<sectionHeader confidence="0.999794" genericHeader="method">
3 Composition methods
</sectionHeader>
<bodyText confidence="0.998571775280899">
Distributional semantic models (DSMs), also
known as vector-space models, semantic spaces,
or by the names of famous incarnations such as
Latent Semantic Analysis or Topic Models, ap-
proximate the meaning of words with vectors that
record their patterns of co-occurrence with cor-
pus context features (often, other words). There
is an extensive literature on how to develop such
models and on their evaluation. Recent surveys
include Clark (2012), Erk (2012) and Turney and
Pantel (2010). We focus here on compositional
DSMs (cDSMs). Since the very inception of dis-
tributional semantics, there have been attempts to
compose meanings for sentences and larger pas-
sages (Landauer and Dumais, 1997), but inter-
est in compositional DSMs has skyrocketed in
the last few years, particularly since the influen-
tial work of Mitchell and Lapata (2008; 2009;
2010). For the current study, we have reimple-
mented and adapted to the morphological setting
all cDSMs we are aware of, excluding the tensor-
product-based models that Mitchell and Lapata
(2010) have shown to be empirically disappointing
and the models of Socher and colleagues (Socher
et al., 2011; Socher et al., 2012), that require com-
plex optimization procedures whose adaptation to
morphology we leave to future work.
Mitchell and Lapata proposed a set of simple
and effective models in which the composed vec-
tors are obtained through component-wise opera-
tions on the constituent vectors. Given input vec-
tors u and v, the multiplicative model (mult) re-
turns a composed vector c with: ci = uivi. In the
weighted additive model (wadd), the composed
vector is a weighted sum of the two input vectors:
c = αu + βv, where α and β are two scalars. In
the dilation model, the output vector is obtained
by first decomposing one of the input vectors, say
v, into a vector parallel to u and an orthogonal
vector. Following this, the parallel vector is dilated
by a factor A before re-combining. This results in:
c = (A − 1)(u, v)u + (u, u)v.
Guevara (2010) and Zanzotto et al. (2010) pro-
pose the full additive model (fulladd), where the
two vectors to be added are pre-multiplied by
weight matrices: c = Au + Bv
Since the Mitchell and Lapata and fulladd mod-
els were developed for phrase composition, the
two input vectors were taken to be, very straight-
forwardly, the vectors of the two words to be com-
posed into the phrase of interest. In morphological
derivation, at least one of the items to be composed
(the affix) is a bound morpheme. In our adapta-
tion of these composition models, we build bound
morpheme vectors by accumulating the contexts
in which a set of derived words containing the rel-
evant morphemes occur, e.g., the re- vector aggre-
gates co-occurrences of redo, remake, retry, etc.
Baroni and Zamparelli (2010) and Coecke et
al. (2010) take inspiration from formal semantics
to characterize composition in terms of function
application, where the distributional representa-
tion of one element in a composition (the func-
tor) is not a vector but a function. Given that
linear functions can be expressed by matrices and
their application by matrix-by-vector multiplica-
tion, in this lexical function (lexfunc) model, the
functor is represented by a matrix U to be multi-
plied with the argument vector v: c = Uv. In
the case of morphology, it is natural to treat bound
affixes as functions over stems, since affixes en-
code the systematic semantic patterns we intend
to capture. Unlike the other composition meth-
ods, lexfunc does not require the construction of
distributional vectors for affixes. A matrix repre-
sentation for every affix is instead induced directly
from examples of stems and the corresponding de-
rived forms, in line with the intuition that every af-
fix corresponds to a different pattern of change of
the stem meaning.
Finally, as already discussed in the Introduc-
tion, performing no composition at all but using
the stem vector as a surrogate of the derived form
is a reasonable strategy. We saw that morphologi-
cally derived words tend to appear less frequently
than their stems, and in many cases the meanings
are close. Consequently, we expect a stem-only
“composition” method to be a strong baseline in
the morphological setting.
</bodyText>
<sectionHeader confidence="0.993403" genericHeader="method">
4 Experimental setup
</sectionHeader>
<subsectionHeader confidence="0.99702">
4.1 Morphological data
</subsectionHeader>
<bodyText confidence="0.999989111111111">
We obtained a list of stem/derived-form pairs from
the CELEX English Lexical Database, a widely
used 100K-lemma lexicon containing, among
other things, information about the derivational
structure of words (Baayen et al., 1995). For each
derivational affix present in CELEX, we extracted
from the database the full list of stem/derived
pairs matching its most common part-of-speech
signature (e.g., for -er we only considered pairs
</bodyText>
<page confidence="0.97904">
1519
</page>
<table confidence="0.999404523809524">
Affix Stem/Der. Training HQ/Tot. Avg.
POS Items Test Items SDR
-able verb/adj 177 30/50 5.96
-al noun/adj 245 41/50 5.88
-er verb/noun 824 33/50 5.51
-ful noun/adj 53 42/50 6.11
-ic noun/adj 280 43/50 5.99
-ion verb/noun 637 38/50 6.22
-ist noun/noun 244 38/50 6.16
-ity adj/noun 372 33/50 6.19
-ize noun/verb 105 40/50 5.96
-less noun/adj 122 35/50 3.72
-ly adj/adv 1847 20/50 6.33
-ment verb/noun 165 38/50 6.06
-ness adj/noun 602 33/50 6.29
-ous noun/adj 157 35/50 5.94
-y noun/adj 404 27/50 5.25
in- adj/adj 101 34/50 3.39
re- verb/verb 86 27/50 5.28
un- adj/adj 128 36/50 3.23
tot */* 6549 623/900 5.52
</table>
<tableCaption confidence="0.999439">
Table 1: Derivational morphology dataset
</tableCaption>
<bodyText confidence="0.999608266666667">
having a verbal stem and nominal derived form).
Since CELEX was populated by semi-automated
morphological analysis, it includes forms that are
probably not synchronically related to their stems,
such as crypt+ic or re+form. However, we did not
manually intervene on the pairs, since we are in-
terested in training and testing our methods in re-
alistic, noisy conditions. In particular, the need to
pre-process corpora to determine which forms are
“opaque”, and should thus be bypassed by our sys-
tems, would greatly reduce their usefulness. Pairs
in which either word occurred less than 20 times
in our source corpus (described in Section 4.2 be-
low) were filtered out and, in our final dataset, we
only considered the 18 affixes (3 prefixes and 15
suffixes) with at least 100 pairs meeting this con-
dition. We randomly chose 50 stem/derived pairs
(900 in total) as test data. The remaining data were
used as training items to estimate the parameters
of the composition methods. Table 1 summarizes
various characteristics of the dataset3 (the last two
columns of the table are explained in the next para-
graphs).
Annotation of quality of test vectors The qual-
ity of the corpus-based vectors representing de-
rived test items was determined by collecting hu-
man semantic similarity judgments in a crowd-
sourcing survey. In particular, we use the similar-
ity of a vector to its nearest neighbors (NNs) as a
proxy measure of quality. The underlying assump-
</bodyText>
<footnote confidence="0.949118">
3Available from http://clic.cimec.unitn.it/
composes
</footnote>
<bodyText confidence="0.99944182">
tion is that a vector, in order to be a good represen-
tation of the meaning of the corresponding word,
should lie in a region of semantic space populated
by intuitively similar meanings, e.g., we are more
likely to have captured the meaning of car if the
NN of its vector is the automobile vector rather
than potato. Therefore, to measure the quality of
a given vector, we can look at the average simi-
larity score provided by humans when comparing
this very vector with its own NNs.
All 900 derived vectors from the test set were
matched with their three closest NNs in our se-
mantic space (see Section 4.2), thus producing a
set of 2, 700 word pairs. These pairs were admin-
istered to CrowdFlower users,4 who were asked
to judge the relatedness of the two meanings on a
7-point scale (higher for more related). In order
to ensure that participants were committed to the
task and exclude non-proficient English speakers,
we used 60 control pairs as gold standard, consist-
ing of either perfect synonyms or completely un-
related words. We obtained 30 judgments for each
derived form (10 judgments for each of 3 neighbor
comparisons), with mean participant agreement of
58%. These ratings were averaged item-wise, re-
sulting in a Gaussian distribution with a mean of
3.79 and a standard deviation of 1.31. Finally,
each test item was marked as high-quality (HQ)
if its derived form received an average score of at
least 3, as low-quality (LQ) otherwise. Table 1 re-
ports the proportion of HQ test items for each af-
fix, and Table 2 reports some examples of HQ and
LQ items with the corresponding NNs. It is worth
observing that the NNs of the LQ items, while not
as relevant as the HQ ones, are hardly random.
Annotation of similarity between stem and de-
rived forms Derived forms differ in terms of
how far their meaning is with respect to that of
their stem. Certain morphological processes have
systematically more impact than others on mean-
ing: For example, the adjectival prefix in- negates
the meaning of the stem, whereas -ly has the sole
function to convert an adjective into an adverb.
But the very same affix can affect different stems
in different ways. For example, remelt means lit-
tle more than to melt again, but rethink has subtler
implications of changing one’s way to look at a
problem, and while one of the senses of cycling is
present in recycle, it takes some effort to see their
relation.
</bodyText>
<footnote confidence="0.993785">
4http://www.crowdflower.com
</footnote>
<page confidence="0.815187">
1520
</page>
<table confidence="0.999957545454545">
Affix Type Derived form Neighbors
-ist HQ transcendentalist mythologist, futurist, theosophist
LQ florist Harrod, wholesaler, stockist
-ity HQ publicity publicise, press, publicize
LQ sparsity dissimilarity, contiguity, perceptibility
-ment HQ advertisement advert, promotional, advertising
LQ inducement litigant, contractually, voluntarily
in- HQ inaccurate misleading, incorrect, erroneous
LQ inoperable metastasis, colorectal, biopsy
re- HQ recapture retake, besiege, capture
LQ rename defunct, officially, merge
</table>
<tableCaption confidence="0.999466">
Table 2: Examples of HQ and LQ derived vectors with their NNs
</tableCaption>
<bodyText confidence="0.999967">
We conducted a separate crowdsourcing study
where participants were asked to rate the 900
test stem/derived pairs for the strength of their
semantic relationship on a 7-point scale. We
followed a procedure similar to the one de-
scribed for quality measurement; 7 judgments
were collected for each pair. Participants’ agree-
ment was at 60%. The last column of Ta-
ble 1 reports the average stem/derived related-
ness (SDR) for the various affixes. Note that
the affixes with systematically lower SDR are
those carrying a negative meaning (in-, un-, -less),
whereas those with highest SDR do little more
than changing the POS of the stem (-ion, -ly, -
ness). Among specific pairs with very low related-
ness we encounter hand/handy, bear/bearable and
active/activist, whereas compulsory/compulsorily,
shameless/shamelessness and chaos/chaotic have
high SDR. Since the distribution of the average
ratings was negatively skewed (mean rating: 5.52,
standard deviation: 1.26),5 we took 5 as the rating
threshold to classify items as having high (HR) or
low (LR) relatedness to their stems.
</bodyText>
<subsectionHeader confidence="0.985626">
4.2 Distributional semantic space6
</subsectionHeader>
<bodyText confidence="0.9999784">
We use as our source corpus the concatenation of
ukWaC, the English Wikipedia (2009 dump) and
the BNC,7 for a total of about 2.8 billion tokens.
We collect co-occurrence statistics for the top 20K
content words (adjectives, adverbs, nouns, verbs)
</bodyText>
<footnote confidence="0.983515777777778">
5The negative skew is not surprising, as derived forms
must have some relation to their stems!
6Most steps of the semantic space construction
and composition pipelines were implemented using
the DISSECT toolkit: https://github.com/
composes-toolkit/dissect.
7http://wacky.sslmit.unibo.it, http:
//en.wikipedia.org, http://www.natcorp.
ox.ac.uk
</footnote>
<bodyText confidence="0.999793714285714">
in lemma format, plus any item from the mor-
phological dataset described above that was below
this rank. The top 20K content words also con-
stitute our context elements. We use a standard
bag-of-words approach, counting collocates in a
narrow 2-word before-and-after window. We ap-
ply (non-negative) Pointwise Mutual Information
as weighting scheme and dimensionality reduc-
tion by Non-negative Matrix Factorization, setting
the number of reduced-space dimensions to 350.
These settings are chosen without tuning, and are
based on previous experiments where they pro-
duced high-quality semantic spaces (Boleda et al.,
2013; Bullinaria and Levy, 2007).
</bodyText>
<subsectionHeader confidence="0.999789">
4.3 Implementation of composition methods
</subsectionHeader>
<bodyText confidence="0.999965142857143">
All composition methods except mult and stem
have weights to be estimated (e.g., the λ parame-
ter of dilation or the affix matrices of lexfunc). We
adopt the estimation strategy proposed by Gue-
vara (2010) and Baroni and Zamparelli (2010),
namely we pick parameter values that optimize
the mapping between stem and derived vectors di-
rectly extracted from the corpus. To learn, say, a
lexfunc matrix representing the prefix re-, we ex-
tract vectors of V/reV pairs that occur with suffi-
cient frequency (visit/revisit, think/rethink... ). We
then use least-squares methods to find weights for
the re- matrix that minimize the distance between
each reV vector generated by the model given the
input V and the corresponding corpus-observed
derived vector (e.g., we try to make the model-
predicted re+visit vector as similar as possible
to the corpus-extracted one). This is a general
estimation approach that does not require task-
specific hand-labeled data, and for which simple
analytical solutions of the least-squares error prob-
</bodyText>
<page confidence="0.98317">
1521
</page>
<bodyText confidence="0.99997696">
lem exist for all our composition methods. We use
only the training items from Section 4.1 for esti-
mation. Note that, unlike the test items, these have
not been annotated for quality, so we are adopting
an unsupervised (no manual labeling) but noisy es-
timation method.8
For the lexfunc model, we use the training items
separately to obtain weight matrices represent-
ing each affix, whereas for the other models all
training data are used together to globally de-
rive single sets of affix and stem weights. For
the wadd model, the learning process results in
0.16xaffix+0.33xstem, i.e., the affix contributes
only half of its mass to the composition of the
derived form. For dilation, we stretch the stem
(i.e., v of the dilation equation is the stem vector),
since it should provide richer contents than the af-
fix to the derived meaning. We found that, on av-
erage across the training pairs, dilation weighted
the stem 20 times more heavily than the affix
(0.05xaffix+1xstem). We then expect that the di-
lation model will have similar performance to the
baseline stem model, as confirmed below.9
For all methods, vectors were normalized be-
fore composing both in training and in generation.
</bodyText>
<sectionHeader confidence="0.9154555" genericHeader="method">
5 Experiment 1: approximating
high-quality corpus-extracted vectors
</sectionHeader>
<bodyText confidence="0.9998740625">
The first experiment investigates to what extent
composition models can approximate high-quality
(HQ) corpus-extracted vectors representing de-
rived forms. Note that since the test items were
excluded from training, we are simulating a sce-
nario in which composition models must generate
representations for nonce derived forms.
Cosine similarity between model-generated and
corpus-extracted vectors were computed for all
models, including the stem baseline (i.e., co-
sine between stem and derived form). The first
row of Table 3 reports mean similarities. The
stem method sets the level of performance rel-
atively high, confirming its soundness. Indeed,
the parameter-free mult model performs below the
baseline.10 As expected, dilation performs simi-
</bodyText>
<footnote confidence="0.850746222222222">
8More accurately, we relied on semi-manual CELEX in-
formation to identify derived forms. A further step towards a
fully knowledge-free system would be to pre-process the cor-
pus with an unsupervised morphological induction system to
extract stem/derived pairs.
9The other models have thousands of weights to be es-
timated, so we cannot summarize the outcome of parameter
estimation here.
10This result does not necessarily contradict those of
</footnote>
<table confidence="0.99886975">
stem mult dil. wadd fulladd lexfunc
All 0.47 0.39 0.48 0.50 0.56 0.54
HR 0.52 0.43 0.53 0.55 0.61 0.58
LR 0.32 0.28 0.33 0.38 0.41 0.42
</table>
<tableCaption confidence="0.998747">
Table 3: Mean similarity of composed vectors to
</tableCaption>
<bodyText confidence="0.958301782608695">
high-quality corpus-extracted derived-form vec-
tors, for all as well as high- (HR) and low-
relatedness (LR) test items
larly to the baseline, while wadd outperforms it,
although the effect does not reach significance
(p=.06).11 Both fulladd and lexfunc perform sig-
nificantly better than stem (p &lt; .001). Lexfunc
provides a flexible way to account for affixation,
since it models it directly as a function mapping
from and onto word vectors, without requiring a
vector representation of bound affixes. The rea-
son at the base of its good performance is thus
quite straightforward. On the other hand, it is
surprising that a simple representation of bound
affixes (i.e., as vectors aggregating the contexts
of words containing them) can work so well, at
least when used in conjunction with the granular
dimension-by-dimension weights assigned by the
fulladd method. We hypothesize that these aggre-
gated contexts, by providing information about the
set of stems an affix combines with, capture the
shared semantic features that the affix operates on.
When the meaning of the derived form is far
from that of its stem, the stem baseline should no
longer constitute a suitable surrogate of derived-
form meaning. The LR cases (see Section 4.1
above) are thus crucial to understand how well
composition methods capture not only stem mean-
ing, but also affix-triggered semantics. The HR
and LR rows of Table 3 present the results for
the respective test subsets. As expected, the stem
approach undergoes a strong drop when perfor-
mance is measured on LR items. At the other ex-
treme, fulladd and lexfunc, while also finding the
LR cases more difficult, still clearly outperform
the baseline (p&lt;.001), confirming that they cap-
ture the meaning of derived forms beyond what
their stems contribute to it. The effect of wadd,
again, approaches significance when compared to
the baseline (p=.05). Very encouragingly, both
Mitchell and Lapata and others who found mult to be highly
competitive. Due to differences in co-occurrence weighting
schemes (we use a logarithmically scaled measure, they do
not), their multiplicative model is closer to our additive one.
11Significance assessed by means of Tukey Honestly Sig-
nificant Difference tests (Abdi and Williams, 2010)
</bodyText>
<page confidence="0.973738">
1522
</page>
<table confidence="0.9755905">
stem mult wadd dil. fulladd lexfunc
-less 0.22 0.23 0.30 0.24 0.38 0.44
in- 0.39 0.34 0.45 0.40 0.47 0.45
un- 0.33 0.33 0.41 0.34 0.44 0.46
</table>
<tableCaption confidence="0.973623">
Table 4: Mean similarity of composed vectors to
</tableCaption>
<bodyText confidence="0.984261851851852">
high-quality corpus-extracted derived-form vec-
tors with negative affixes
fulladd and lexfunc significantly outperform stem
also in the HR subset (p&lt;.001). That is, the mod-
els provide better approximations of derived forms
even when the stem itself should already be a good
surrogate. The difference between the two models
is not significant.
We noted in Section 4.1 that forms containing
the “negative” affixes -less, un- and in- received
on average low SDR scores, since negation im-
pacts meaning more drastically than other opera-
tions. Table 4 reports the performance of the mod-
els on these affixes. Indeed, the stem baseline per-
forms quite poorly, whereas fulladd, lexfunc and,
to a lesser extent, wadd are quite effective in this
condition as well, all performing greatly above the
baseline. These results are intriguing in light of
the fact that modeling negation is a challenging
task for DSMs (Mohammad et al., 2013) as well as
cDSMs (Preller and Sadrzadeh, 2011). To the ex-
tent that our best methods have captured the negat-
ing function of a prefix such as in-, they might be
applied to tasks such as recognizing lexical op-
posites, or even simple forms of syntactic nega-
tion (modeling inoperable is just a short step away
from modeling not operable compositionally).
</bodyText>
<sectionHeader confidence="0.538401333333333" genericHeader="method">
6 Experiment 2: Comparing the quality
of corpus-extracted and
compositionally generated words
</sectionHeader>
<bodyText confidence="0.999891642857143">
The first experiment simulated the scenario in
which derived forms are not in our corpus, so
that directly extracting their representation from
it is not an option. The second experiment tests
if compositionally-derived representations can be
better than those extracted directly from the corpus
when the latter is a possible strategy (i.e., the de-
rived forms are attested in the source corpus). To
this purpose, we focused on those 277 test items
that were judged as low-quality (LQ, see Section
4.1), which are presumably more challenging to
generate, and where the compositional route could
be most useful.
We evaluated the derived forms generated by
</bodyText>
<table confidence="0.995692">
corpus stem wadd fulladd lexfunc
All 2.28 3.26 4.12 3.99 3.09
HR 2.29 3.56 4.48 4.31 3.31
LR 2.22 2.48 3.14 3.12 2.52
</table>
<tableCaption confidence="0.975729">
Table 5: Average quality ratings of derived vectors
</tableCaption>
<table confidence="0.999531625">
Target Model Neighbors
florist wadd flora, fauna, ecosystem
fulladd flora, fauna, egologist
lexfunc ornithologist, naturalist, botanist
sparsity wadd sparse, sparsely, dense
fulladd sparse, sparseness, angularity
lexfunc fragility, angularity, smallness
inducement wadd induce, inhibit, inhibition
fulladd induce, inhibition, mediate
lexfunc impairment, cerebral, ocular
inoperable wadd operable, palliation, biopsy
fulladd operable, inoperative, ventilator
lexfunc inoperative, unavoidably, flaw
rename wadd name, later, namesake
fulladd name, namesake, later
lexfunc temporarily, reinstate, thereafter
</table>
<tableCaption confidence="0.976738">
Table 6: Examples of model-predicted neighbors
</tableCaption>
<bodyText confidence="0.9922972">
for words with LQ corpus-extracted vectors
the models that performed best in the first exper-
iment (fulladd, lexfunc and wadd), as well as the
stem baseline, by means of another crowdsourcing
study. We followed the same procedure used to
assess the quality of corpus-extracted vectors, that
is, we asked judges to rate the relatedness of the
target forms to their NNs (we obtained on average
29 responses per form).
The first line of Table 5 reports the average
quality (on a 7-point scale) of the representations
of the derived forms as produced by the models
and baseline, as well as of the corpus-harvested
ones (corpus column). All compositional models
produce representations that are of significantly
higher quality (p &lt; .001) than the corpus-based
ones. The effect is also evident in qualitative
terms. Table 6 presents the NNs predicted by the
three compositional methods for the same LQ test
items whose corpus-based NNs are presented in
Table 2. These results indicate that morpheme
composition is an effective solution when the qual-
ity of corpus-extracted derived forms is low (and
the previous experiment showed that, when their
quality is high, composition can at least approxi-
mate corpus-based vectors).
With respect to Experiment 1, we obtain a dif-
ferent ranking of the models, with lexfunc being
outperformed by both wadd and fulladd (p&lt;.001),
that are statistically indistinguishable. The wadd
</bodyText>
<page confidence="0.956827">
1523
</page>
<bodyText confidence="0.999979818181818">
composition is dominated by the stem, and by
looking at the examples in Table 6 we notice that
both this model and fulladd tend to feature the
stem as NN (100% of the cases for wadd, 73%
for fulladd in the complete test set). The question
thus arises as to whether the good performance of
these composition techniques is simply due to the
fact that they produce derived forms that are near
their stems, with no added semantic value from the
affix (a “stemploitation” strategy).
However, the stemploitation hypothesis is dis-
pelled by the observation that both models signifi-
cantly outperform the stem baseline (p&lt;.001), de-
spite the fact that the latter, again, has good per-
formance, significantly outperforming the corpus-
derived vectors (p &lt; .001). Thus, we confirm
that compositional models provide higher qual-
ity vectors that are capturing the meaning of de-
rived forms beyond the information provided by
the stem.
Indeed, if we focus on the third row of Ta-
ble 5, reporting performance on low stem-derived
relatedness (LR) items (annotated as described in
Section 4.1), fulladd and wadd still significantly
outperform the corpus representations (p&lt;.001),
whereas the quality of the stem representations of
LR items is not significantly different form that of
the corpus-derived ones. Interestingly, lexfunc dis-
plays the smallest drop in performance when re-
stricting evaluation to LR items; however, since it
does not significantly outperform the LQ corpus
representations, this is arguably due to a floor ef-
fect.
</bodyText>
<sectionHeader confidence="0.952239" genericHeader="conclusions">
7 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.999987684210527">
We investigated to what extent cDSMs can gener-
ate effective meaning representations of complex
words through morpheme composition. Several
state-of-the-art composition models were adapted
and evaluated on this novel task. Our results sug-
gest that morpheme composition can indeed pro-
vide high-quality vectors for complex forms, im-
proving both on vectors directly extracted from the
corpus and on a stem-backoff strategy. This re-
sult is of practical importance for distributional se-
mantics, as it paves the way to address one of the
main causes of data sparseness, and it confirms the
usefulness of the compositional approach in a new
domain. Overall, fulladd emerged as the best per-
forming model, with both lexfunc and the simple
wadd approach constituting strong rivals. The ef-
fectiveness of the best models extended also to the
challenging cases where the meaning of derived
forms is far from that of the stem, including nega-
tive affixes.
The fulladd method requires a vector represen-
tation for bound morphemes. A first direction for
future work will thus be to investigate which as-
pects of the meaning of bound morphemes are
captured by our current simple-minded approach
to populating their vectors, and to explore alterna-
tive ways to construct them, seeing if they further
improve fulladd performance.
A natural extension of our research is to ad-
dress morpheme composition and morphological
induction jointly, trying to model the intuition that
good candidate morphemes should have coherent
semantic representations. Relatedly, in the cur-
rent setting we generate complex forms from their
parts. We want to investigate the inverse route,
namely “de-composing” complex words to de-
rive representations of their stems, especially for
cases where the complex words are more frequent
(e.g. comfort/comfortable).
We would also like to apply composition to in-
flectional morphology (that currently lies outside
the scope of distributional semantics), to capture
the nuances of meaning that, for example, distin-
guish singular and plural nouns (consider, e.g., the
difference between the mass singular tea and the
plural teas, which coerces the noun into a count
interpretation (Katz and Zamparelli, 2012)).
Finally, in our current setup we focus on a single
composition step, e.g., we derive the meaning of
inoperable by composing the morphemes in- and
operable. But operable is in turn composed of op-
erate and -able. In the future, we will explore re-
cursive morpheme composition, especially since
we would like to apply these methods to more
complex morphological systems (e.g., agglutina-
tive languages) where multiple morphemes are the
norm.
</bodyText>
<sectionHeader confidence="0.998786" genericHeader="acknowledgments">
8 Acknowledgments
</sectionHeader>
<bodyText confidence="0.9997184">
We thank Georgiana Dinu and Nghia The Pham
for helping out with DISSECT-ion and the review-
ers for helpful feedback. This research was sup-
ported by the ERC 2011 Starting Independent Re-
search Grant n. 283554 (COMPOSES).
</bodyText>
<page confidence="0.994228">
1524
</page>
<sectionHeader confidence="0.989991" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999593455445545">
Herv´e Abdi and Lynne Williams. 2010. Newman-
Keuls and Tukey test. In Neil Salkind, Bruce Frey,
and Dondald Dougherty, editors, Encyclopedia of
Research Design, pages 897–904. Sage, Thousand
Oaks, CA.
Harald Baayen, Richard Piepenbrock, and Leon Gu-
likers. 1995. The CELEX lexical database (re-
lease 2). CD-ROM, Linguistic Data Consortium,
Philadelphia, PA.
Harald Baayen. 2005. Morphological productivity. In
Rajmund Piotrowski Reinhard K¨ohler, Gabriel Alt-
mann, editor, Quantitative Linguistics: An Inter-
national Handbook, pages 243–256. Mouton de
Gruyter, Berlin, Germany.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183–1193, Boston,
MA.
Marco Baroni. 2009. Distributions in text. In Anke
Lideling and Merja Kyt¨o, editors, Corpus Linguis-
tics: An International Handbook, volume 2, pages
803–821. Mouton de Gruyter, Berlin, Germany.
Laurie Bauer. 2001. Morphological Productivity.
Cambridge University Press, Cambridge, UK.
Kenneth Beesley and Lauri Karttunen. 2000. Finite-
State Morphology: Xerox Tools and Techniques.
Cambridge University Press, Cambridge, UK.
Alan Black, Stephen Pulman, Graeme Ritchie, and
Graham Russell. 1991. Computational Morphol-
ogy. MIT Press, Cambrdige, MA.
Gemma Boleda, Marco Baroni, Louise McNally, and
Nghia Pham. 2013. Intensionality was only alleged:
On adjective-noun composition in distributional se-
mantics. In Proceedings of IWCS, pages 35–46,
Potsdam, Germany.
John Bullinaria and Joseph Levy. 2007. Extracting
semantic representations from word co-occurrence
statistics: A computational study. Behavior Re-
search Methods, 39:510–526.
Stephen Clark. 2012. Vector space models of lexical
meaning. In Shalom Lappin and Chris Fox, editors,
Handbook of Contemporary Semantics, 2nd edition.
Blackwell, Malden, MA. In press.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical foundations for a com-
positional distributional model of meaning. Linguis-
tic Analysis, 36:345–384.
David Dowty. 1979. Word Meaning and Montague
Grammar. Springer, New York.
Markus Dreyer and Jason Eisner. 2011. Discover-
ing morphological paradigms from plain text using
a Dirichlet process mixture model. In Proceedings
of EMNLP, pages 616–627, Edinburgh, UK.
Katrin Erk. 2012. Vector space models of word mean-
ing and phrase meaning: A survey. Language and
Linguistics Compass, 6(10):635–653.
Yoav Goldberg and Reut Tsarfaty. 2008. A single gen-
erative model for joint morphological segmentation
and syntactic parsing. In Proceedings ofACL, pages
371–379, Columbus, OH.
John Goldsmith. 2001. Unsupervised learning of the
morphology of a natural language. Computational
Linguistics, 2(27):153–198.
Sharon Goldwater and David McClosky. 2005. Im-
proving statistical MT through morphological anal-
ysis. In Proceedings of EMNLP, pages 676–683,
Vancouver, Canada.
Sharon Goldwater. 2006. Nonparametric Bayesian
Models of Lexical Acquisition. Ph.D. thesis, Brown
University.
Emiliano Guevara. 2009. Compositionality in distribu-
tional semantics: Derivational affixes. In Proceed-
ings of the Words in Action Workshop, Pisa, Italy.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of GEMS, pages 33–37,
Uppsala, Sweden.
Graham Katz and Roberto Zamparelli. 2012. Quanti-
fying count/mass elasticity. In Proceedings of WC-
CFL, pages 371–379, Tucson, AR.
Victor Kuperman. 2009. Semantic transparency revis-
ited. Presentation at the 6th International Morpho-
logical Processing Conference.
Thomas Landauer and Susan Dumais. 1997. A solu-
tion to Plato’s problem: The latent semantic analysis
theory of acquisition, induction, and representation
of knowledge. Psychological Review, 104(2):211–
240.
Young-Suk Lee. 2004. Morphological analysis for sta-
tistical machine translation. In Proceedings of HLT-
NAACL, pages 57–60, Boston, MA.
Rochelle Lieber. 2004. Morphology and Lexical Se-
mantics. Cambridge University Press, Cambridge,
UK.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL, pages 236–244, Columbus, OH.
Jeff Mitchell and Mirella Lapata. 2009. Language
models based on semantic composition. In Proceed-
ings of EMNLP, pages 430–439, Singapore.
</reference>
<page confidence="0.995632">
1525
</page>
<bodyText confidence="0.914683">
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388–1429.
Fabio Zanzotto, Ioannis Korkontzelos, Francesca
Falucchi, and Suresh Manandhar. 2010. Estimat-
ing linear models for compositional distributional
semantics. In Proceedings of COLING, pages 1263–
1271, Beijing, China.
Saif Mohammad, Bonnie Dorr, Graeme Hirst, and Pe-
ter Turney. 2013. Computing lexical contrast. Com-
putational Linguistics. In press.
</bodyText>
<reference confidence="0.999453891304348">
Jason Naradowsky and Sharon Goldwater. 2009. Im-
proving morphology induction by learning spelling
rules. In Proceedings of IJCAI, pages 11–17,
Pasadena, CA.
Ingo Plag. 1999. Morphological Productivity: Struc-
tural Constraints in English Derivation. Mouton de
Gruyter, Berlin, Germany.
Anne Preller and Mehrnoosh Sadrzadeh. 2011. Bell
states and negative sentences in the distributed
model of meaning. Electr. Notes Theor. Comput.
Sci., 270(2):141–153.
Patrick Schone and Daniel Jurafsky. 2000.
Knowledge-free induction of morphology us-
ing latent semantic analysis. In Proceedings of the
ConLL workshop on learning language in logic,
pages 67–72, Lisbon, Portugal.
Richard Socher, Eric Huang, Jeffrey Pennin, Andrew
Ng, and Christopher Manning. 2011. Dynamic
pooling and unfolding recursive autoencoders for
paraphrase detection. In Proceedings of NIPS, pages
801–809, Granada, Spain.
Richard Socher, Brody Huval, Christopher Manning,
and Andrew Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of EMNLP, pages 1201–1211, Jeju Island, Ko-
rea.
Richard Sproat. 1992. Morphology and Computation.
MIT Press, Cambrdige, MA.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141–188.
Hsueh-Cheng Wang, Yi-Min Tien, Li-Chuan Hsu, and
Marc Pomplun. 2012. Estimating semantic trans-
parency of constituents of English compounds and
two-character Chinese words using Latent Semantic
Analysis. In Proceedings of CogSci, pages 2499–
2504, Sapporo, Japan.
Richard Wicentowski. 2004. Multilingual noise-
robust supervised morphological analysis using the
wordframe model. In Proceedings of SIGPHON,
pages 70–77, Barcelona, Spain.
David Yarowsky and Richard Wicentowski. 2000.
Minimally supervised morphological analysis by
multimodal alignment. In Proceedings of ACL,
pages 207–216, Hong Kong.
</reference>
<page confidence="0.992904">
1526
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.754689">
<title confidence="0.8951155">Compositional-ly Derived Representations Morphologically Complex Words in Distributional Semantics</title>
<author confidence="0.977752">Lazaridou Marelli Zamparelli</author>
<affiliation confidence="0.999728">Center for Mind/Brain Sciences (University of Trento,</affiliation>
<email confidence="0.999321">first.last@unitn.it</email>
<abstract confidence="0.99790819047619">Speakers of a language can construct an unlimited number of new words through morphological derivation. This is a major cause of data sparseness for corpus-based approaches to lexical semantics, such as distributional semantic models of word meaning. We adapt compositional methods originally developed for phrases to the task of deriving the distributional meaning of morphologically complex words from their parts. Semantic representations constructed in this way beat a strong baseline and can be of higher quality than representations directly constructed from corpus data. Our results constitute a novel evaluation of the proposed composition methods, which the additive achieves the best performance, and demonstrate the usefulness of a compositional morphology component in distributional semantics.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Herv´e Abdi</author>
<author>Lynne Williams</author>
</authors>
<title>NewmanKeuls and Tukey test.</title>
<date>2010</date>
<booktitle>Encyclopedia of Research Design,</booktitle>
<pages>897--904</pages>
<editor>In Neil Salkind, Bruce Frey, and Dondald Dougherty, editors,</editor>
<location>Sage, Thousand Oaks, CA.</location>
<contexts>
<context position="27390" citStr="Abdi and Williams, 2010" startWordPosition="4337" endWordPosition="4340">more difficult, still clearly outperform the baseline (p&lt;.001), confirming that they capture the meaning of derived forms beyond what their stems contribute to it. The effect of wadd, again, approaches significance when compared to the baseline (p=.05). Very encouragingly, both Mitchell and Lapata and others who found mult to be highly competitive. Due to differences in co-occurrence weighting schemes (we use a logarithmically scaled measure, they do not), their multiplicative model is closer to our additive one. 11Significance assessed by means of Tukey Honestly Significant Difference tests (Abdi and Williams, 2010) 1522 stem mult wadd dil. fulladd lexfunc -less 0.22 0.23 0.30 0.24 0.38 0.44 in- 0.39 0.34 0.45 0.40 0.47 0.45 un- 0.33 0.33 0.41 0.34 0.44 0.46 Table 4: Mean similarity of composed vectors to high-quality corpus-extracted derived-form vectors with negative affixes fulladd and lexfunc significantly outperform stem also in the HR subset (p&lt;.001). That is, the models provide better approximations of derived forms even when the stem itself should already be a good surrogate. The difference between the two models is not significant. We noted in Section 4.1 that forms containing the “negative” aff</context>
</contexts>
<marker>Abdi, Williams, 2010</marker>
<rawString>Herv´e Abdi and Lynne Williams. 2010. NewmanKeuls and Tukey test. In Neil Salkind, Bruce Frey, and Dondald Dougherty, editors, Encyclopedia of Research Design, pages 897–904. Sage, Thousand Oaks, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harald Baayen</author>
<author>Richard Piepenbrock</author>
<author>Leon Gulikers</author>
</authors>
<date>1995</date>
<booktitle>The CELEX lexical database (release 2). CD-ROM, Linguistic Data Consortium,</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="13737" citStr="Baayen et al., 1995" startWordPosition="2166" endWordPosition="2169">omposition at all but using the stem vector as a surrogate of the derived form is a reasonable strategy. We saw that morphologically derived words tend to appear less frequently than their stems, and in many cases the meanings are close. Consequently, we expect a stem-only “composition” method to be a strong baseline in the morphological setting. 4 Experimental setup 4.1 Morphological data We obtained a list of stem/derived-form pairs from the CELEX English Lexical Database, a widely used 100K-lemma lexicon containing, among other things, information about the derivational structure of words (Baayen et al., 1995). For each derivational affix present in CELEX, we extracted from the database the full list of stem/derived pairs matching its most common part-of-speech signature (e.g., for -er we only considered pairs 1519 Affix Stem/Der. Training HQ/Tot. Avg. POS Items Test Items SDR -able verb/adj 177 30/50 5.96 -al noun/adj 245 41/50 5.88 -er verb/noun 824 33/50 5.51 -ful noun/adj 53 42/50 6.11 -ic noun/adj 280 43/50 5.99 -ion verb/noun 637 38/50 6.22 -ist noun/noun 244 38/50 6.16 -ity adj/noun 372 33/50 6.19 -ize noun/verb 105 40/50 5.96 -less noun/adj 122 35/50 3.72 -ly adj/adv 1847 20/50 6.33 -ment v</context>
</contexts>
<marker>Baayen, Piepenbrock, Gulikers, 1995</marker>
<rawString>Harald Baayen, Richard Piepenbrock, and Leon Gulikers. 1995. The CELEX lexical database (release 2). CD-ROM, Linguistic Data Consortium, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harald Baayen</author>
</authors>
<title>Morphological productivity.</title>
<date>2005</date>
<booktitle>Quantitative Linguistics: An International Handbook,</booktitle>
<pages>243--256</pages>
<editor>In Rajmund Piotrowski Reinhard K¨ohler, Gabriel Altmann, editor,</editor>
<location>Berlin, Germany.</location>
<contexts>
<context position="2194" citStr="Baayen, 2005" startWordPosition="321" endWordPosition="322">urney and Pantel, 2010). Reliable distributional vectors can only be extracted for words that occur in many contexts in the corpus. Not surprisingly, there is a strong correlation between word frequency and vector quality (Bullinaria and Levy, 2007), and since most words occur only once even in very large corpora (Baroni, 2009), DSMs suffer data sparseness. While word rarity has many sources, one of the most common and systematic ones is the high productivity of morphological derivation processes, whereby an unlimited number of new words can be constructed by adding affixes to existing stems (Baayen, 2005; Bauer, 2001; Plag, 1999).1 For example, in the multi-billion-word corpus we introduce below, perfectly reasonable derived forms such as lexicalizable or affixless never occur. Even without considering the theoretically infinite number of possible derived nonce words, and restricting ourselves instead to words that are already listed in dictionaries, complex forms cover a high portion of the lexicon. For example, morphologically complex forms account for 55% of the lemmas in the CELEX English database (see Section 4.1 below). In most of these cases (80% according to our corpus) the stem is mo</context>
</contexts>
<marker>Baayen, 2005</marker>
<rawString>Harald Baayen. 2005. Morphological productivity. In Rajmund Piotrowski Reinhard K¨ohler, Gabriel Altmann, editor, Quantitative Linguistics: An International Handbook, pages 243–256. Mouton de Gruyter, Berlin, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1183--1193</pages>
<location>Boston, MA.</location>
<contexts>
<context position="5343" citStr="Baroni and Zamparelli, 2010" startWordPosition="827" endWordPosition="831"> derived form, the stem was not fully appropriate), and sometimes the jump in meaning can be quite dramatic (resourceless and resource mean very different things!). In the past few years there has been much interest in how DSMs can scale up to represent the meaning of larger chunks of text such as phrases or even sentences. Trying to represent the meaning of arbitrarily long constructions by directly collecting co-occurrence statistics is obviously ineffective and thus methods have been developed to derive the meaning of larger constructions as a function of the meaning of their constituents (Baroni and Zamparelli, 2010; Coecke et al., 2010; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Socher et al., 2012). Compositional distributional semantic models (cDSMs) of word units aim at handling, compositionally, the high productivity of phrases and consequent data sparseness. It is natural to hypothesize that the same methods can be applied to morphology to derive the meaning of complex words from the meaning of their parts: For example, instead of harvesting a rebuild vector directly from the corpus, the latter could be constructed from the distributional representations of re- and build. Besides allevia</context>
<context position="12033" citStr="Baroni and Zamparelli (2010)" startWordPosition="1892" endWordPosition="1895">s: c = Au + Bv Since the Mitchell and Lapata and fulladd models were developed for phrase composition, the two input vectors were taken to be, very straightforwardly, the vectors of the two words to be composed into the phrase of interest. In morphological derivation, at least one of the items to be composed (the affix) is a bound morpheme. In our adaptation of these composition models, we build bound morpheme vectors by accumulating the contexts in which a set of derived words containing the relevant morphemes occur, e.g., the re- vector aggregates co-occurrences of redo, remake, retry, etc. Baroni and Zamparelli (2010) and Coecke et al. (2010) take inspiration from formal semantics to characterize composition in terms of function application, where the distributional representation of one element in a composition (the functor) is not a vector but a function. Given that linear functions can be expressed by matrices and their application by matrix-by-vector multiplication, in this lexical function (lexfunc) model, the functor is represented by a matrix U to be multiplied with the argument vector v: c = Uv. In the case of morphology, it is natural to treat bound affixes as functions over stems, since affixes e</context>
<context position="21728" citStr="Baroni and Zamparelli (2010)" startWordPosition="3439" endWordPosition="3442">Pointwise Mutual Information as weighting scheme and dimensionality reduction by Non-negative Matrix Factorization, setting the number of reduced-space dimensions to 350. These settings are chosen without tuning, and are based on previous experiments where they produced high-quality semantic spaces (Boleda et al., 2013; Bullinaria and Levy, 2007). 4.3 Implementation of composition methods All composition methods except mult and stem have weights to be estimated (e.g., the λ parameter of dilation or the affix matrices of lexfunc). We adopt the estimation strategy proposed by Guevara (2010) and Baroni and Zamparelli (2010), namely we pick parameter values that optimize the mapping between stem and derived vectors directly extracted from the corpus. To learn, say, a lexfunc matrix representing the prefix re-, we extract vectors of V/reV pairs that occur with sufficient frequency (visit/revisit, think/rethink... ). We then use least-squares methods to find weights for the re- matrix that minimize the distance between each reV vector generated by the model given the input V and the corresponding corpus-observed derived vector (e.g., we try to make the modelpredicted re+visit vector as similar as possible to the co</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In Proceedings of EMNLP, pages 1183–1193, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
</authors>
<title>Distributions in text.</title>
<date>2009</date>
<booktitle>In Anke Lideling and Merja Kyt¨o, editors, Corpus Linguistics: An International Handbook,</booktitle>
<volume>2</volume>
<pages>803--821</pages>
<location>Berlin, Germany.</location>
<contexts>
<context position="1911" citStr="Baroni, 2009" startWordPosition="276" endWordPosition="277">exical semantics. Distributional semantic models (DSMs) in particular represent the meaning of a word by a vector, the dimensions of which encode corpus-extracted co-occurrence statistics, under the assumption that words that are semantically similar will occur in similar contexts (Turney and Pantel, 2010). Reliable distributional vectors can only be extracted for words that occur in many contexts in the corpus. Not surprisingly, there is a strong correlation between word frequency and vector quality (Bullinaria and Levy, 2007), and since most words occur only once even in very large corpora (Baroni, 2009), DSMs suffer data sparseness. While word rarity has many sources, one of the most common and systematic ones is the high productivity of morphological derivation processes, whereby an unlimited number of new words can be constructed by adding affixes to existing stems (Baayen, 2005; Bauer, 2001; Plag, 1999).1 For example, in the multi-billion-word corpus we introduce below, perfectly reasonable derived forms such as lexicalizable or affixless never occur. Even without considering the theoretically infinite number of possible derived nonce words, and restricting ourselves instead to words that</context>
</contexts>
<marker>Baroni, 2009</marker>
<rawString>Marco Baroni. 2009. Distributions in text. In Anke Lideling and Merja Kyt¨o, editors, Corpus Linguistics: An International Handbook, volume 2, pages 803–821. Mouton de Gruyter, Berlin, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurie Bauer</author>
</authors>
<title>Morphological Productivity.</title>
<date>2001</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="2207" citStr="Bauer, 2001" startWordPosition="323" endWordPosition="324">el, 2010). Reliable distributional vectors can only be extracted for words that occur in many contexts in the corpus. Not surprisingly, there is a strong correlation between word frequency and vector quality (Bullinaria and Levy, 2007), and since most words occur only once even in very large corpora (Baroni, 2009), DSMs suffer data sparseness. While word rarity has many sources, one of the most common and systematic ones is the high productivity of morphological derivation processes, whereby an unlimited number of new words can be constructed by adding affixes to existing stems (Baayen, 2005; Bauer, 2001; Plag, 1999).1 For example, in the multi-billion-word corpus we introduce below, perfectly reasonable derived forms such as lexicalizable or affixless never occur. Even without considering the theoretically infinite number of possible derived nonce words, and restricting ourselves instead to words that are already listed in dictionaries, complex forms cover a high portion of the lexicon. For example, morphologically complex forms account for 55% of the lemmas in the CELEX English database (see Section 4.1 below). In most of these cases (80% according to our corpus) the stem is more frequent t</context>
</contexts>
<marker>Bauer, 2001</marker>
<rawString>Laurie Bauer. 2001. Morphological Productivity. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Beesley</author>
<author>Lauri Karttunen</author>
</authors>
<title>FiniteState Morphology: Xerox Tools and Techniques.</title>
<date>2000</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="6855" citStr="Beesley and Karttunen, 2000" startWordPosition="1066" endWordPosition="1069">y, 1979; Lieber, 2004). In this paper, we explore, for the first time (except for the proof-of-concept study in Guevara (2009)), the application of cDSMs to derivational morphology. We adapt a number of composition methods from the literature to the morphological setting, and we show that some of these methods can provide better distributional representations of derived forms than either those directly harvested from a large corpus, or those obtained by using the stem as a proxy to derived-form meaning. Our 2Of course, spotting and segmenting complex words is a big research topic unto itself (Beesley and Karttunen, 2000; Black et al., 1991; Sproat, 1992), and one we completely sidestep here. results suggest that exploiting morphology could improve the quality of DSMs in general, extend the range of tasks that cDSMs can successfully model and support the development of new ways to test their performance. 2 Related work Morphological induction systems use corpusbased methods to decide if two words are morphologically related and/or to segment words into morphemes (Dreyer and Eisner, 2011; Goldsmith, 2001; Goldwater and McClosky, 2005; Goldwater, 2006; Naradowsky and Goldwater, 2009; Wicentowski, 2004). Morphol</context>
</contexts>
<marker>Beesley, Karttunen, 2000</marker>
<rawString>Kenneth Beesley and Lauri Karttunen. 2000. FiniteState Morphology: Xerox Tools and Techniques. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Black</author>
<author>Stephen Pulman</author>
<author>Graeme Ritchie</author>
<author>Graham Russell</author>
</authors>
<title>Computational Morphology.</title>
<date>1991</date>
<publisher>MIT Press,</publisher>
<location>Cambrdige, MA.</location>
<contexts>
<context position="6875" citStr="Black et al., 1991" startWordPosition="1070" endWordPosition="1073">is paper, we explore, for the first time (except for the proof-of-concept study in Guevara (2009)), the application of cDSMs to derivational morphology. We adapt a number of composition methods from the literature to the morphological setting, and we show that some of these methods can provide better distributional representations of derived forms than either those directly harvested from a large corpus, or those obtained by using the stem as a proxy to derived-form meaning. Our 2Of course, spotting and segmenting complex words is a big research topic unto itself (Beesley and Karttunen, 2000; Black et al., 1991; Sproat, 1992), and one we completely sidestep here. results suggest that exploiting morphology could improve the quality of DSMs in general, extend the range of tasks that cDSMs can successfully model and support the development of new ways to test their performance. 2 Related work Morphological induction systems use corpusbased methods to decide if two words are morphologically related and/or to segment words into morphemes (Dreyer and Eisner, 2011; Goldsmith, 2001; Goldwater and McClosky, 2005; Goldwater, 2006; Naradowsky and Goldwater, 2009; Wicentowski, 2004). Morphological induction has</context>
</contexts>
<marker>Black, Pulman, Ritchie, Russell, 1991</marker>
<rawString>Alan Black, Stephen Pulman, Graeme Ritchie, and Graham Russell. 1991. Computational Morphology. MIT Press, Cambrdige, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gemma Boleda</author>
<author>Marco Baroni</author>
<author>Louise McNally</author>
<author>Nghia Pham</author>
</authors>
<title>Intensionality was only alleged: On adjective-noun composition in distributional semantics.</title>
<date>2013</date>
<booktitle>In Proceedings of IWCS,</booktitle>
<pages>35--46</pages>
<location>Potsdam, Germany.</location>
<contexts>
<context position="21420" citStr="Boleda et al., 2013" startWordPosition="3390" endWordPosition="3393">k in lemma format, plus any item from the morphological dataset described above that was below this rank. The top 20K content words also constitute our context elements. We use a standard bag-of-words approach, counting collocates in a narrow 2-word before-and-after window. We apply (non-negative) Pointwise Mutual Information as weighting scheme and dimensionality reduction by Non-negative Matrix Factorization, setting the number of reduced-space dimensions to 350. These settings are chosen without tuning, and are based on previous experiments where they produced high-quality semantic spaces (Boleda et al., 2013; Bullinaria and Levy, 2007). 4.3 Implementation of composition methods All composition methods except mult and stem have weights to be estimated (e.g., the λ parameter of dilation or the affix matrices of lexfunc). We adopt the estimation strategy proposed by Guevara (2010) and Baroni and Zamparelli (2010), namely we pick parameter values that optimize the mapping between stem and derived vectors directly extracted from the corpus. To learn, say, a lexfunc matrix representing the prefix re-, we extract vectors of V/reV pairs that occur with sufficient frequency (visit/revisit, think/rethink..</context>
</contexts>
<marker>Boleda, Baroni, McNally, Pham, 2013</marker>
<rawString>Gemma Boleda, Marco Baroni, Louise McNally, and Nghia Pham. 2013. Intensionality was only alleged: On adjective-noun composition in distributional semantics. In Proceedings of IWCS, pages 35–46, Potsdam, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Bullinaria</author>
<author>Joseph Levy</author>
</authors>
<title>Extracting semantic representations from word co-occurrence statistics: A computational study.</title>
<date>2007</date>
<journal>Behavior Research Methods,</journal>
<pages>39--510</pages>
<contexts>
<context position="1831" citStr="Bullinaria and Levy, 2007" startWordPosition="260" endWordPosition="263"> In the last decades, corpus-based methods have achieved some degree of success in modeling lexical semantics. Distributional semantic models (DSMs) in particular represent the meaning of a word by a vector, the dimensions of which encode corpus-extracted co-occurrence statistics, under the assumption that words that are semantically similar will occur in similar contexts (Turney and Pantel, 2010). Reliable distributional vectors can only be extracted for words that occur in many contexts in the corpus. Not surprisingly, there is a strong correlation between word frequency and vector quality (Bullinaria and Levy, 2007), and since most words occur only once even in very large corpora (Baroni, 2009), DSMs suffer data sparseness. While word rarity has many sources, one of the most common and systematic ones is the high productivity of morphological derivation processes, whereby an unlimited number of new words can be constructed by adding affixes to existing stems (Baayen, 2005; Bauer, 2001; Plag, 1999).1 For example, in the multi-billion-word corpus we introduce below, perfectly reasonable derived forms such as lexicalizable or affixless never occur. Even without considering the theoretically infinite number </context>
<context position="21448" citStr="Bullinaria and Levy, 2007" startWordPosition="3394" endWordPosition="3397">us any item from the morphological dataset described above that was below this rank. The top 20K content words also constitute our context elements. We use a standard bag-of-words approach, counting collocates in a narrow 2-word before-and-after window. We apply (non-negative) Pointwise Mutual Information as weighting scheme and dimensionality reduction by Non-negative Matrix Factorization, setting the number of reduced-space dimensions to 350. These settings are chosen without tuning, and are based on previous experiments where they produced high-quality semantic spaces (Boleda et al., 2013; Bullinaria and Levy, 2007). 4.3 Implementation of composition methods All composition methods except mult and stem have weights to be estimated (e.g., the λ parameter of dilation or the affix matrices of lexfunc). We adopt the estimation strategy proposed by Guevara (2010) and Baroni and Zamparelli (2010), namely we pick parameter values that optimize the mapping between stem and derived vectors directly extracted from the corpus. To learn, say, a lexfunc matrix representing the prefix re-, we extract vectors of V/reV pairs that occur with sufficient frequency (visit/revisit, think/rethink... ). We then use least-squar</context>
</contexts>
<marker>Bullinaria, Levy, 2007</marker>
<rawString>John Bullinaria and Joseph Levy. 2007. Extracting semantic representations from word co-occurrence statistics: A computational study. Behavior Research Methods, 39:510–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
</authors>
<title>Vector space models of lexical meaning.</title>
<date>2012</date>
<booktitle>Handbook of Contemporary Semantics, 2nd edition.</booktitle>
<editor>In Shalom Lappin and Chris Fox, editors,</editor>
<publisher>In press.</publisher>
<location>Blackwell, Malden, MA.</location>
<contexts>
<context position="9715" citStr="Clark (2012)" startWordPosition="1496" endWordPosition="1497">not test alternative models, and he only presented a qualitative analysis of the trajectories triggered by composition with various affixes. 1518 3 Composition methods Distributional semantic models (DSMs), also known as vector-space models, semantic spaces, or by the names of famous incarnations such as Latent Semantic Analysis or Topic Models, approximate the meaning of words with vectors that record their patterns of co-occurrence with corpus context features (often, other words). There is an extensive literature on how to develop such models and on their evaluation. Recent surveys include Clark (2012), Erk (2012) and Turney and Pantel (2010). We focus here on compositional DSMs (cDSMs). Since the very inception of distributional semantics, there have been attempts to compose meanings for sentences and larger passages (Landauer and Dumais, 1997), but interest in compositional DSMs has skyrocketed in the last few years, particularly since the influential work of Mitchell and Lapata (2008; 2009; 2010). For the current study, we have reimplemented and adapted to the morphological setting all cDSMs we are aware of, excluding the tensorproduct-based models that Mitchell and Lapata (2010) have sh</context>
</contexts>
<marker>Clark, 2012</marker>
<rawString>Stephen Clark. 2012. Vector space models of lexical meaning. In Shalom Lappin and Chris Fox, editors, Handbook of Contemporary Semantics, 2nd edition. Blackwell, Malden, MA. In press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Coecke</author>
<author>Mehrnoosh Sadrzadeh</author>
<author>Stephen Clark</author>
</authors>
<title>Mathematical foundations for a compositional distributional model of meaning. Linguistic Analysis,</title>
<date>2010</date>
<pages>36--345</pages>
<contexts>
<context position="5364" citStr="Coecke et al., 2010" startWordPosition="832" endWordPosition="835">ot fully appropriate), and sometimes the jump in meaning can be quite dramatic (resourceless and resource mean very different things!). In the past few years there has been much interest in how DSMs can scale up to represent the meaning of larger chunks of text such as phrases or even sentences. Trying to represent the meaning of arbitrarily long constructions by directly collecting co-occurrence statistics is obviously ineffective and thus methods have been developed to derive the meaning of larger constructions as a function of the meaning of their constituents (Baroni and Zamparelli, 2010; Coecke et al., 2010; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Socher et al., 2012). Compositional distributional semantic models (cDSMs) of word units aim at handling, compositionally, the high productivity of phrases and consequent data sparseness. It is natural to hypothesize that the same methods can be applied to morphology to derive the meaning of complex words from the meaning of their parts: For example, instead of harvesting a rebuild vector directly from the corpus, the latter could be constructed from the distributional representations of re- and build. Besides alleviating data sparseness </context>
<context position="12058" citStr="Coecke et al. (2010)" startWordPosition="1897" endWordPosition="1900"> and Lapata and fulladd models were developed for phrase composition, the two input vectors were taken to be, very straightforwardly, the vectors of the two words to be composed into the phrase of interest. In morphological derivation, at least one of the items to be composed (the affix) is a bound morpheme. In our adaptation of these composition models, we build bound morpheme vectors by accumulating the contexts in which a set of derived words containing the relevant morphemes occur, e.g., the re- vector aggregates co-occurrences of redo, remake, retry, etc. Baroni and Zamparelli (2010) and Coecke et al. (2010) take inspiration from formal semantics to characterize composition in terms of function application, where the distributional representation of one element in a composition (the functor) is not a vector but a function. Given that linear functions can be expressed by matrices and their application by matrix-by-vector multiplication, in this lexical function (lexfunc) model, the functor is represented by a matrix U to be multiplied with the argument vector v: c = Uv. In the case of morphology, it is natural to treat bound affixes as functions over stems, since affixes encode the systematic sema</context>
</contexts>
<marker>Coecke, Sadrzadeh, Clark, 2010</marker>
<rawString>Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark. 2010. Mathematical foundations for a compositional distributional model of meaning. Linguistic Analysis, 36:345–384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Dowty</author>
</authors>
<title>Word Meaning and Montague Grammar.</title>
<date>1979</date>
<publisher>Springer,</publisher>
<location>New York.</location>
<contexts>
<context position="6235" citStr="Dowty, 1979" startWordPosition="970" endWordPosition="971"> hypothesize that the same methods can be applied to morphology to derive the meaning of complex words from the meaning of their parts: For example, instead of harvesting a rebuild vector directly from the corpus, the latter could be constructed from the distributional representations of re- and build. Besides alleviating data sparseness problems, a system of this sort, that automatically induces the semantic contents of morphological processes, would also be of tremendous theoretical interest, given that the semantics of derivation is a central and challenging topic in linguistic morphology (Dowty, 1979; Lieber, 2004). In this paper, we explore, for the first time (except for the proof-of-concept study in Guevara (2009)), the application of cDSMs to derivational morphology. We adapt a number of composition methods from the literature to the morphological setting, and we show that some of these methods can provide better distributional representations of derived forms than either those directly harvested from a large corpus, or those obtained by using the stem as a proxy to derived-form meaning. Our 2Of course, spotting and segmenting complex words is a big research topic unto itself (Beesley</context>
</contexts>
<marker>Dowty, 1979</marker>
<rawString>David Dowty. 1979. Word Meaning and Montague Grammar. Springer, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dreyer</author>
<author>Jason Eisner</author>
</authors>
<title>Discovering morphological paradigms from plain text using a Dirichlet process mixture model.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>616--627</pages>
<location>Edinburgh, UK.</location>
<contexts>
<context position="7330" citStr="Dreyer and Eisner, 2011" startWordPosition="1141" endWordPosition="1144">roxy to derived-form meaning. Our 2Of course, spotting and segmenting complex words is a big research topic unto itself (Beesley and Karttunen, 2000; Black et al., 1991; Sproat, 1992), and one we completely sidestep here. results suggest that exploiting morphology could improve the quality of DSMs in general, extend the range of tasks that cDSMs can successfully model and support the development of new ways to test their performance. 2 Related work Morphological induction systems use corpusbased methods to decide if two words are morphologically related and/or to segment words into morphemes (Dreyer and Eisner, 2011; Goldsmith, 2001; Goldwater and McClosky, 2005; Goldwater, 2006; Naradowsky and Goldwater, 2009; Wicentowski, 2004). Morphological induction has recently received considerable attention since morphological analysis can mitigate data sparseness in domains such as parsing and machine translation (Goldberg and Tsarfaty, 2008; Lee, 2004). Among the cues that have been exploited there is distributional similarity among morphologically related words (Schone and Jurafsky, 2000; Yarowsky and Wicentowski, 2000). Our work, however, differs substantially from this track of research. We do not aim at seg</context>
</contexts>
<marker>Dreyer, Eisner, 2011</marker>
<rawString>Markus Dreyer and Jason Eisner. 2011. Discovering morphological paradigms from plain text using a Dirichlet process mixture model. In Proceedings of EMNLP, pages 616–627, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
</authors>
<title>Vector space models of word meaning and phrase meaning: A survey.</title>
<date>2012</date>
<journal>Language and Linguistics Compass,</journal>
<volume>6</volume>
<issue>10</issue>
<contexts>
<context position="9727" citStr="Erk (2012)" startWordPosition="1498" endWordPosition="1499">native models, and he only presented a qualitative analysis of the trajectories triggered by composition with various affixes. 1518 3 Composition methods Distributional semantic models (DSMs), also known as vector-space models, semantic spaces, or by the names of famous incarnations such as Latent Semantic Analysis or Topic Models, approximate the meaning of words with vectors that record their patterns of co-occurrence with corpus context features (often, other words). There is an extensive literature on how to develop such models and on their evaluation. Recent surveys include Clark (2012), Erk (2012) and Turney and Pantel (2010). We focus here on compositional DSMs (cDSMs). Since the very inception of distributional semantics, there have been attempts to compose meanings for sentences and larger passages (Landauer and Dumais, 1997), but interest in compositional DSMs has skyrocketed in the last few years, particularly since the influential work of Mitchell and Lapata (2008; 2009; 2010). For the current study, we have reimplemented and adapted to the morphological setting all cDSMs we are aware of, excluding the tensorproduct-based models that Mitchell and Lapata (2010) have shown to be em</context>
</contexts>
<marker>Erk, 2012</marker>
<rawString>Katrin Erk. 2012. Vector space models of word meaning and phrase meaning: A survey. Language and Linguistics Compass, 6(10):635–653.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Reut Tsarfaty</author>
</authors>
<title>A single generative model for joint morphological segmentation and syntactic parsing.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>371--379</pages>
<location>Columbus, OH.</location>
<contexts>
<context position="7654" citStr="Goldberg and Tsarfaty, 2008" startWordPosition="1184" endWordPosition="1187">e range of tasks that cDSMs can successfully model and support the development of new ways to test their performance. 2 Related work Morphological induction systems use corpusbased methods to decide if two words are morphologically related and/or to segment words into morphemes (Dreyer and Eisner, 2011; Goldsmith, 2001; Goldwater and McClosky, 2005; Goldwater, 2006; Naradowsky and Goldwater, 2009; Wicentowski, 2004). Morphological induction has recently received considerable attention since morphological analysis can mitigate data sparseness in domains such as parsing and machine translation (Goldberg and Tsarfaty, 2008; Lee, 2004). Among the cues that have been exploited there is distributional similarity among morphologically related words (Schone and Jurafsky, 2000; Yarowsky and Wicentowski, 2000). Our work, however, differs substantially from this track of research. We do not aim at segmenting morphological complex words or identifying paradigms. Our goal is to automatically construct, given distributional representations of stems and affixes, semantic representations for the derived words containing those stems and affixes. A morphological induction system, given rebuild, will segment it into re- and bu</context>
</contexts>
<marker>Goldberg, Tsarfaty, 2008</marker>
<rawString>Yoav Goldberg and Reut Tsarfaty. 2008. A single generative model for joint morphological segmentation and syntactic parsing. In Proceedings ofACL, pages 371–379, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Goldsmith</author>
</authors>
<title>Unsupervised learning of the morphology of a natural language.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>2</volume>
<issue>27</issue>
<contexts>
<context position="7347" citStr="Goldsmith, 2001" startWordPosition="1145" endWordPosition="1146">ing. Our 2Of course, spotting and segmenting complex words is a big research topic unto itself (Beesley and Karttunen, 2000; Black et al., 1991; Sproat, 1992), and one we completely sidestep here. results suggest that exploiting morphology could improve the quality of DSMs in general, extend the range of tasks that cDSMs can successfully model and support the development of new ways to test their performance. 2 Related work Morphological induction systems use corpusbased methods to decide if two words are morphologically related and/or to segment words into morphemes (Dreyer and Eisner, 2011; Goldsmith, 2001; Goldwater and McClosky, 2005; Goldwater, 2006; Naradowsky and Goldwater, 2009; Wicentowski, 2004). Morphological induction has recently received considerable attention since morphological analysis can mitigate data sparseness in domains such as parsing and machine translation (Goldberg and Tsarfaty, 2008; Lee, 2004). Among the cues that have been exploited there is distributional similarity among morphologically related words (Schone and Jurafsky, 2000; Yarowsky and Wicentowski, 2000). Our work, however, differs substantially from this track of research. We do not aim at segmenting morpholog</context>
</contexts>
<marker>Goldsmith, 2001</marker>
<rawString>John Goldsmith. 2001. Unsupervised learning of the morphology of a natural language. Computational Linguistics, 2(27):153–198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>David McClosky</author>
</authors>
<title>Improving statistical MT through morphological analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>676--683</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="7377" citStr="Goldwater and McClosky, 2005" startWordPosition="1147" endWordPosition="1150">se, spotting and segmenting complex words is a big research topic unto itself (Beesley and Karttunen, 2000; Black et al., 1991; Sproat, 1992), and one we completely sidestep here. results suggest that exploiting morphology could improve the quality of DSMs in general, extend the range of tasks that cDSMs can successfully model and support the development of new ways to test their performance. 2 Related work Morphological induction systems use corpusbased methods to decide if two words are morphologically related and/or to segment words into morphemes (Dreyer and Eisner, 2011; Goldsmith, 2001; Goldwater and McClosky, 2005; Goldwater, 2006; Naradowsky and Goldwater, 2009; Wicentowski, 2004). Morphological induction has recently received considerable attention since morphological analysis can mitigate data sparseness in domains such as parsing and machine translation (Goldberg and Tsarfaty, 2008; Lee, 2004). Among the cues that have been exploited there is distributional similarity among morphologically related words (Schone and Jurafsky, 2000; Yarowsky and Wicentowski, 2000). Our work, however, differs substantially from this track of research. We do not aim at segmenting morphological complex words or identify</context>
</contexts>
<marker>Goldwater, McClosky, 2005</marker>
<rawString>Sharon Goldwater and David McClosky. 2005. Improving statistical MT through morphological analysis. In Proceedings of EMNLP, pages 676–683, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
</authors>
<title>Nonparametric Bayesian Models of Lexical Acquisition.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>Brown University.</institution>
<contexts>
<context position="7394" citStr="Goldwater, 2006" startWordPosition="1151" endWordPosition="1152">mplex words is a big research topic unto itself (Beesley and Karttunen, 2000; Black et al., 1991; Sproat, 1992), and one we completely sidestep here. results suggest that exploiting morphology could improve the quality of DSMs in general, extend the range of tasks that cDSMs can successfully model and support the development of new ways to test their performance. 2 Related work Morphological induction systems use corpusbased methods to decide if two words are morphologically related and/or to segment words into morphemes (Dreyer and Eisner, 2011; Goldsmith, 2001; Goldwater and McClosky, 2005; Goldwater, 2006; Naradowsky and Goldwater, 2009; Wicentowski, 2004). Morphological induction has recently received considerable attention since morphological analysis can mitigate data sparseness in domains such as parsing and machine translation (Goldberg and Tsarfaty, 2008; Lee, 2004). Among the cues that have been exploited there is distributional similarity among morphologically related words (Schone and Jurafsky, 2000; Yarowsky and Wicentowski, 2000). Our work, however, differs substantially from this track of research. We do not aim at segmenting morphological complex words or identifying paradigms. Ou</context>
</contexts>
<marker>Goldwater, 2006</marker>
<rawString>Sharon Goldwater. 2006. Nonparametric Bayesian Models of Lexical Acquisition. Ph.D. thesis, Brown University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiliano Guevara</author>
</authors>
<title>Compositionality in distributional semantics: Derivational affixes.</title>
<date>2009</date>
<booktitle>In Proceedings of the Words in Action Workshop,</booktitle>
<location>Pisa, Italy.</location>
<contexts>
<context position="6354" citStr="Guevara (2009)" startWordPosition="990" endWordPosition="991">ing of their parts: For example, instead of harvesting a rebuild vector directly from the corpus, the latter could be constructed from the distributional representations of re- and build. Besides alleviating data sparseness problems, a system of this sort, that automatically induces the semantic contents of morphological processes, would also be of tremendous theoretical interest, given that the semantics of derivation is a central and challenging topic in linguistic morphology (Dowty, 1979; Lieber, 2004). In this paper, we explore, for the first time (except for the proof-of-concept study in Guevara (2009)), the application of cDSMs to derivational morphology. We adapt a number of composition methods from the literature to the morphological setting, and we show that some of these methods can provide better distributional representations of derived forms than either those directly harvested from a large corpus, or those obtained by using the stem as a proxy to derived-form meaning. Our 2Of course, spotting and segmenting complex words is a big research topic unto itself (Beesley and Karttunen, 2000; Black et al., 1991; Sproat, 1992), and one we completely sidestep here. results suggest that expl</context>
<context position="8868" citStr="Guevara (2009)" startWordPosition="1368" endWordPosition="1369">ild (possibly using distributional similarity between the words as a cue). Our system, given re- and build, predicts the (distributional semantic) meaning of rebuild. Another emerging line of research uses distributional semantics to model human intuitions about the semantic transparency of morphologically derived or compound expressions and how these impact various lexical processing tasks (Kuperman, 2009; Wang et al., 2012). Although these works exploit vectors representing complex forms, they do not attempt to generate them compositionally. The only similar study we are aware of is that of Guevara (2009). Guevara found a systematic geometric relation between corpus-based vectors of derived forms sharing an affix and their stems, and used this finding to motivate the composition method we term lexfunc below. However, unlike us, he did not test alternative models, and he only presented a qualitative analysis of the trajectories triggered by composition with various affixes. 1518 3 Composition methods Distributional semantic models (DSMs), also known as vector-space models, semantic spaces, or by the names of famous incarnations such as Latent Semantic Analysis or Topic Models, approximate the m</context>
</contexts>
<marker>Guevara, 2009</marker>
<rawString>Emiliano Guevara. 2009. Compositionality in distributional semantics: Derivational affixes. In Proceedings of the Words in Action Workshop, Pisa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiliano Guevara</author>
</authors>
<title>A regression model of adjective-noun compositionality in distributional semantics.</title>
<date>2010</date>
<booktitle>In Proceedings of GEMS,</booktitle>
<pages>33--37</pages>
<location>Uppsala,</location>
<contexts>
<context position="11264" citStr="Guevara (2010)" startWordPosition="1761" endWordPosition="1762">gh component-wise operations on the constituent vectors. Given input vectors u and v, the multiplicative model (mult) returns a composed vector c with: ci = uivi. In the weighted additive model (wadd), the composed vector is a weighted sum of the two input vectors: c = αu + βv, where α and β are two scalars. In the dilation model, the output vector is obtained by first decomposing one of the input vectors, say v, into a vector parallel to u and an orthogonal vector. Following this, the parallel vector is dilated by a factor A before re-combining. This results in: c = (A − 1)(u, v)u + (u, u)v. Guevara (2010) and Zanzotto et al. (2010) propose the full additive model (fulladd), where the two vectors to be added are pre-multiplied by weight matrices: c = Au + Bv Since the Mitchell and Lapata and fulladd models were developed for phrase composition, the two input vectors were taken to be, very straightforwardly, the vectors of the two words to be composed into the phrase of interest. In morphological derivation, at least one of the items to be composed (the affix) is a bound morpheme. In our adaptation of these composition models, we build bound morpheme vectors by accumulating the contexts in which</context>
<context position="21695" citStr="Guevara (2010)" startWordPosition="3435" endWordPosition="3437">ply (non-negative) Pointwise Mutual Information as weighting scheme and dimensionality reduction by Non-negative Matrix Factorization, setting the number of reduced-space dimensions to 350. These settings are chosen without tuning, and are based on previous experiments where they produced high-quality semantic spaces (Boleda et al., 2013; Bullinaria and Levy, 2007). 4.3 Implementation of composition methods All composition methods except mult and stem have weights to be estimated (e.g., the λ parameter of dilation or the affix matrices of lexfunc). We adopt the estimation strategy proposed by Guevara (2010) and Baroni and Zamparelli (2010), namely we pick parameter values that optimize the mapping between stem and derived vectors directly extracted from the corpus. To learn, say, a lexfunc matrix representing the prefix re-, we extract vectors of V/reV pairs that occur with sufficient frequency (visit/revisit, think/rethink... ). We then use least-squares methods to find weights for the re- matrix that minimize the distance between each reV vector generated by the model given the input V and the corresponding corpus-observed derived vector (e.g., we try to make the modelpredicted re+visit vector</context>
</contexts>
<marker>Guevara, 2010</marker>
<rawString>Emiliano Guevara. 2010. A regression model of adjective-noun compositionality in distributional semantics. In Proceedings of GEMS, pages 33–37, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Katz</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Quantifying count/mass elasticity.</title>
<date>2012</date>
<booktitle>In Proceedings of WCCFL,</booktitle>
<pages>371--379</pages>
<location>Tucson, AR.</location>
<contexts>
<context position="35581" citStr="Katz and Zamparelli, 2012" startWordPosition="5621" endWordPosition="5624">ex forms from their parts. We want to investigate the inverse route, namely “de-composing” complex words to derive representations of their stems, especially for cases where the complex words are more frequent (e.g. comfort/comfortable). We would also like to apply composition to inflectional morphology (that currently lies outside the scope of distributional semantics), to capture the nuances of meaning that, for example, distinguish singular and plural nouns (consider, e.g., the difference between the mass singular tea and the plural teas, which coerces the noun into a count interpretation (Katz and Zamparelli, 2012)). Finally, in our current setup we focus on a single composition step, e.g., we derive the meaning of inoperable by composing the morphemes in- and operable. But operable is in turn composed of operate and -able. In the future, we will explore recursive morpheme composition, especially since we would like to apply these methods to more complex morphological systems (e.g., agglutinative languages) where multiple morphemes are the norm. 8 Acknowledgments We thank Georgiana Dinu and Nghia The Pham for helping out with DISSECT-ion and the reviewers for helpful feedback. This research was supporte</context>
</contexts>
<marker>Katz, Zamparelli, 2012</marker>
<rawString>Graham Katz and Roberto Zamparelli. 2012. Quantifying count/mass elasticity. In Proceedings of WCCFL, pages 371–379, Tucson, AR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Kuperman</author>
</authors>
<date>2009</date>
<booktitle>Semantic transparency revisited. Presentation at the 6th International Morphological Processing Conference.</booktitle>
<contexts>
<context position="8663" citStr="Kuperman, 2009" startWordPosition="1335" endWordPosition="1336">nal representations of stems and affixes, semantic representations for the derived words containing those stems and affixes. A morphological induction system, given rebuild, will segment it into re- and build (possibly using distributional similarity between the words as a cue). Our system, given re- and build, predicts the (distributional semantic) meaning of rebuild. Another emerging line of research uses distributional semantics to model human intuitions about the semantic transparency of morphologically derived or compound expressions and how these impact various lexical processing tasks (Kuperman, 2009; Wang et al., 2012). Although these works exploit vectors representing complex forms, they do not attempt to generate them compositionally. The only similar study we are aware of is that of Guevara (2009). Guevara found a systematic geometric relation between corpus-based vectors of derived forms sharing an affix and their stems, and used this finding to motivate the composition method we term lexfunc below. However, unlike us, he did not test alternative models, and he only presented a qualitative analysis of the trajectories triggered by composition with various affixes. 1518 3 Composition </context>
</contexts>
<marker>Kuperman, 2009</marker>
<rawString>Victor Kuperman. 2009. Semantic transparency revisited. Presentation at the 6th International Morphological Processing Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Landauer</author>
<author>Susan Dumais</author>
</authors>
<title>A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<pages>240</pages>
<contexts>
<context position="9963" citStr="Landauer and Dumais, 1997" startWordPosition="1533" endWordPosition="1536">models, semantic spaces, or by the names of famous incarnations such as Latent Semantic Analysis or Topic Models, approximate the meaning of words with vectors that record their patterns of co-occurrence with corpus context features (often, other words). There is an extensive literature on how to develop such models and on their evaluation. Recent surveys include Clark (2012), Erk (2012) and Turney and Pantel (2010). We focus here on compositional DSMs (cDSMs). Since the very inception of distributional semantics, there have been attempts to compose meanings for sentences and larger passages (Landauer and Dumais, 1997), but interest in compositional DSMs has skyrocketed in the last few years, particularly since the influential work of Mitchell and Lapata (2008; 2009; 2010). For the current study, we have reimplemented and adapted to the morphological setting all cDSMs we are aware of, excluding the tensorproduct-based models that Mitchell and Lapata (2010) have shown to be empirically disappointing and the models of Socher and colleagues (Socher et al., 2011; Socher et al., 2012), that require complex optimization procedures whose adaptation to morphology we leave to future work. Mitchell and Lapata propose</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>Thomas Landauer and Susan Dumais. 1997. A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological Review, 104(2):211– 240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young-Suk Lee</author>
</authors>
<title>Morphological analysis for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of HLTNAACL,</booktitle>
<pages>57--60</pages>
<location>Boston, MA.</location>
<contexts>
<context position="7666" citStr="Lee, 2004" startWordPosition="1188" endWordPosition="1189">an successfully model and support the development of new ways to test their performance. 2 Related work Morphological induction systems use corpusbased methods to decide if two words are morphologically related and/or to segment words into morphemes (Dreyer and Eisner, 2011; Goldsmith, 2001; Goldwater and McClosky, 2005; Goldwater, 2006; Naradowsky and Goldwater, 2009; Wicentowski, 2004). Morphological induction has recently received considerable attention since morphological analysis can mitigate data sparseness in domains such as parsing and machine translation (Goldberg and Tsarfaty, 2008; Lee, 2004). Among the cues that have been exploited there is distributional similarity among morphologically related words (Schone and Jurafsky, 2000; Yarowsky and Wicentowski, 2000). Our work, however, differs substantially from this track of research. We do not aim at segmenting morphological complex words or identifying paradigms. Our goal is to automatically construct, given distributional representations of stems and affixes, semantic representations for the derived words containing those stems and affixes. A morphological induction system, given rebuild, will segment it into re- and build (possibl</context>
</contexts>
<marker>Lee, 2004</marker>
<rawString>Young-Suk Lee. 2004. Morphological analysis for statistical machine translation. In Proceedings of HLTNAACL, pages 57–60, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rochelle Lieber</author>
</authors>
<title>Morphology and Lexical Semantics.</title>
<date>2004</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="6250" citStr="Lieber, 2004" startWordPosition="972" endWordPosition="973">that the same methods can be applied to morphology to derive the meaning of complex words from the meaning of their parts: For example, instead of harvesting a rebuild vector directly from the corpus, the latter could be constructed from the distributional representations of re- and build. Besides alleviating data sparseness problems, a system of this sort, that automatically induces the semantic contents of morphological processes, would also be of tremendous theoretical interest, given that the semantics of derivation is a central and challenging topic in linguistic morphology (Dowty, 1979; Lieber, 2004). In this paper, we explore, for the first time (except for the proof-of-concept study in Guevara (2009)), the application of cDSMs to derivational morphology. We adapt a number of composition methods from the literature to the morphological setting, and we show that some of these methods can provide better distributional representations of derived forms than either those directly harvested from a large corpus, or those obtained by using the stem as a proxy to derived-form meaning. Our 2Of course, spotting and segmenting complex words is a big research topic unto itself (Beesley and Karttunen,</context>
</contexts>
<marker>Lieber, 2004</marker>
<rawString>Rochelle Lieber. 2004. Morphology and Lexical Semantics. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>236--244</pages>
<location>Columbus, OH.</location>
<contexts>
<context position="5391" citStr="Mitchell and Lapata, 2008" startWordPosition="836" endWordPosition="839">, and sometimes the jump in meaning can be quite dramatic (resourceless and resource mean very different things!). In the past few years there has been much interest in how DSMs can scale up to represent the meaning of larger chunks of text such as phrases or even sentences. Trying to represent the meaning of arbitrarily long constructions by directly collecting co-occurrence statistics is obviously ineffective and thus methods have been developed to derive the meaning of larger constructions as a function of the meaning of their constituents (Baroni and Zamparelli, 2010; Coecke et al., 2010; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Socher et al., 2012). Compositional distributional semantic models (cDSMs) of word units aim at handling, compositionally, the high productivity of phrases and consequent data sparseness. It is natural to hypothesize that the same methods can be applied to morphology to derive the meaning of complex words from the meaning of their parts: For example, instead of harvesting a rebuild vector directly from the corpus, the latter could be constructed from the distributional representations of re- and build. Besides alleviating data sparseness problems, a system of this </context>
<context position="10107" citStr="Mitchell and Lapata (2008" startWordPosition="1557" endWordPosition="1560">s with vectors that record their patterns of co-occurrence with corpus context features (often, other words). There is an extensive literature on how to develop such models and on their evaluation. Recent surveys include Clark (2012), Erk (2012) and Turney and Pantel (2010). We focus here on compositional DSMs (cDSMs). Since the very inception of distributional semantics, there have been attempts to compose meanings for sentences and larger passages (Landauer and Dumais, 1997), but interest in compositional DSMs has skyrocketed in the last few years, particularly since the influential work of Mitchell and Lapata (2008; 2009; 2010). For the current study, we have reimplemented and adapted to the morphological setting all cDSMs we are aware of, excluding the tensorproduct-based models that Mitchell and Lapata (2010) have shown to be empirically disappointing and the models of Socher and colleagues (Socher et al., 2011; Socher et al., 2012), that require complex optimization procedures whose adaptation to morphology we leave to future work. Mitchell and Lapata proposed a set of simple and effective models in which the composed vectors are obtained through component-wise operations on the constituent vectors. </context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proceedings of ACL, pages 236–244, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Language models based on semantic composition.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>430--439</pages>
<marker>Mitchell, Lapata, 2009</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2009. Language models based on semantic composition. In Proceedings of EMNLP, pages 430–439, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Naradowsky</author>
<author>Sharon Goldwater</author>
</authors>
<title>Improving morphology induction by learning spelling rules.</title>
<date>2009</date>
<booktitle>In Proceedings of IJCAI,</booktitle>
<pages>11--17</pages>
<location>Pasadena, CA.</location>
<contexts>
<context position="7426" citStr="Naradowsky and Goldwater, 2009" startWordPosition="1153" endWordPosition="1156">big research topic unto itself (Beesley and Karttunen, 2000; Black et al., 1991; Sproat, 1992), and one we completely sidestep here. results suggest that exploiting morphology could improve the quality of DSMs in general, extend the range of tasks that cDSMs can successfully model and support the development of new ways to test their performance. 2 Related work Morphological induction systems use corpusbased methods to decide if two words are morphologically related and/or to segment words into morphemes (Dreyer and Eisner, 2011; Goldsmith, 2001; Goldwater and McClosky, 2005; Goldwater, 2006; Naradowsky and Goldwater, 2009; Wicentowski, 2004). Morphological induction has recently received considerable attention since morphological analysis can mitigate data sparseness in domains such as parsing and machine translation (Goldberg and Tsarfaty, 2008; Lee, 2004). Among the cues that have been exploited there is distributional similarity among morphologically related words (Schone and Jurafsky, 2000; Yarowsky and Wicentowski, 2000). Our work, however, differs substantially from this track of research. We do not aim at segmenting morphological complex words or identifying paradigms. Our goal is to automatically const</context>
</contexts>
<marker>Naradowsky, Goldwater, 2009</marker>
<rawString>Jason Naradowsky and Sharon Goldwater. 2009. Improving morphology induction by learning spelling rules. In Proceedings of IJCAI, pages 11–17, Pasadena, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ingo Plag</author>
</authors>
<title>Morphological Productivity: Structural Constraints in English Derivation. Mouton de Gruyter,</title>
<date>1999</date>
<location>Berlin, Germany.</location>
<contexts>
<context position="2220" citStr="Plag, 1999" startWordPosition="325" endWordPosition="326">liable distributional vectors can only be extracted for words that occur in many contexts in the corpus. Not surprisingly, there is a strong correlation between word frequency and vector quality (Bullinaria and Levy, 2007), and since most words occur only once even in very large corpora (Baroni, 2009), DSMs suffer data sparseness. While word rarity has many sources, one of the most common and systematic ones is the high productivity of morphological derivation processes, whereby an unlimited number of new words can be constructed by adding affixes to existing stems (Baayen, 2005; Bauer, 2001; Plag, 1999).1 For example, in the multi-billion-word corpus we introduce below, perfectly reasonable derived forms such as lexicalizable or affixless never occur. Even without considering the theoretically infinite number of possible derived nonce words, and restricting ourselves instead to words that are already listed in dictionaries, complex forms cover a high portion of the lexicon. For example, morphologically complex forms account for 55% of the lemmas in the CELEX English database (see Section 4.1 below). In most of these cases (80% according to our corpus) the stem is more frequent than the compl</context>
</contexts>
<marker>Plag, 1999</marker>
<rawString>Ingo Plag. 1999. Morphological Productivity: Structural Constraints in English Derivation. Mouton de Gruyter, Berlin, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anne Preller</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>Bell states and negative sentences in the distributed model of meaning.</title>
<date>2011</date>
<journal>Electr. Notes Theor. Comput. Sci.,</journal>
<volume>270</volume>
<issue>2</issue>
<contexts>
<context position="28553" citStr="Preller and Sadrzadeh, 2011" startWordPosition="4529" endWordPosition="4532"> We noted in Section 4.1 that forms containing the “negative” affixes -less, un- and in- received on average low SDR scores, since negation impacts meaning more drastically than other operations. Table 4 reports the performance of the models on these affixes. Indeed, the stem baseline performs quite poorly, whereas fulladd, lexfunc and, to a lesser extent, wadd are quite effective in this condition as well, all performing greatly above the baseline. These results are intriguing in light of the fact that modeling negation is a challenging task for DSMs (Mohammad et al., 2013) as well as cDSMs (Preller and Sadrzadeh, 2011). To the extent that our best methods have captured the negating function of a prefix such as in-, they might be applied to tasks such as recognizing lexical opposites, or even simple forms of syntactic negation (modeling inoperable is just a short step away from modeling not operable compositionally). 6 Experiment 2: Comparing the quality of corpus-extracted and compositionally generated words The first experiment simulated the scenario in which derived forms are not in our corpus, so that directly extracting their representation from it is not an option. The second experiment tests if compos</context>
</contexts>
<marker>Preller, Sadrzadeh, 2011</marker>
<rawString>Anne Preller and Mehrnoosh Sadrzadeh. 2011. Bell states and negative sentences in the distributed model of meaning. Electr. Notes Theor. Comput. Sci., 270(2):141–153.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Schone</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Knowledge-free induction of morphology using latent semantic analysis.</title>
<date>2000</date>
<booktitle>In Proceedings of the ConLL workshop on learning language in logic,</booktitle>
<pages>67--72</pages>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="7805" citStr="Schone and Jurafsky, 2000" startWordPosition="1206" endWordPosition="1209">ion systems use corpusbased methods to decide if two words are morphologically related and/or to segment words into morphemes (Dreyer and Eisner, 2011; Goldsmith, 2001; Goldwater and McClosky, 2005; Goldwater, 2006; Naradowsky and Goldwater, 2009; Wicentowski, 2004). Morphological induction has recently received considerable attention since morphological analysis can mitigate data sparseness in domains such as parsing and machine translation (Goldberg and Tsarfaty, 2008; Lee, 2004). Among the cues that have been exploited there is distributional similarity among morphologically related words (Schone and Jurafsky, 2000; Yarowsky and Wicentowski, 2000). Our work, however, differs substantially from this track of research. We do not aim at segmenting morphological complex words or identifying paradigms. Our goal is to automatically construct, given distributional representations of stems and affixes, semantic representations for the derived words containing those stems and affixes. A morphological induction system, given rebuild, will segment it into re- and build (possibly using distributional similarity between the words as a cue). Our system, given re- and build, predicts the (distributional semantic) mean</context>
</contexts>
<marker>Schone, Jurafsky, 2000</marker>
<rawString>Patrick Schone and Daniel Jurafsky. 2000. Knowledge-free induction of morphology using latent semantic analysis. In Proceedings of the ConLL workshop on learning language in logic, pages 67–72, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Eric Huang</author>
<author>Jeffrey Pennin</author>
<author>Andrew Ng</author>
<author>Christopher Manning</author>
</authors>
<title>Dynamic pooling and unfolding recursive autoencoders for paraphrase detection.</title>
<date>2011</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>801--809</pages>
<location>Granada,</location>
<contexts>
<context position="10411" citStr="Socher et al., 2011" startWordPosition="1606" endWordPosition="1609">s (cDSMs). Since the very inception of distributional semantics, there have been attempts to compose meanings for sentences and larger passages (Landauer and Dumais, 1997), but interest in compositional DSMs has skyrocketed in the last few years, particularly since the influential work of Mitchell and Lapata (2008; 2009; 2010). For the current study, we have reimplemented and adapted to the morphological setting all cDSMs we are aware of, excluding the tensorproduct-based models that Mitchell and Lapata (2010) have shown to be empirically disappointing and the models of Socher and colleagues (Socher et al., 2011; Socher et al., 2012), that require complex optimization procedures whose adaptation to morphology we leave to future work. Mitchell and Lapata proposed a set of simple and effective models in which the composed vectors are obtained through component-wise operations on the constituent vectors. Given input vectors u and v, the multiplicative model (mult) returns a composed vector c with: ci = uivi. In the weighted additive model (wadd), the composed vector is a weighted sum of the two input vectors: c = αu + βv, where α and β are two scalars. In the dilation model, the output vector is obtaine</context>
</contexts>
<marker>Socher, Huang, Pennin, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Eric Huang, Jeffrey Pennin, Andrew Ng, and Christopher Manning. 2011. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In Proceedings of NIPS, pages 801–809, Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher Manning</author>
<author>Andrew Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1201--1211</pages>
<location>Jeju Island,</location>
<contexts>
<context position="5440" citStr="Socher et al., 2012" startWordPosition="844" endWordPosition="847">tic (resourceless and resource mean very different things!). In the past few years there has been much interest in how DSMs can scale up to represent the meaning of larger chunks of text such as phrases or even sentences. Trying to represent the meaning of arbitrarily long constructions by directly collecting co-occurrence statistics is obviously ineffective and thus methods have been developed to derive the meaning of larger constructions as a function of the meaning of their constituents (Baroni and Zamparelli, 2010; Coecke et al., 2010; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Socher et al., 2012). Compositional distributional semantic models (cDSMs) of word units aim at handling, compositionally, the high productivity of phrases and consequent data sparseness. It is natural to hypothesize that the same methods can be applied to morphology to derive the meaning of complex words from the meaning of their parts: For example, instead of harvesting a rebuild vector directly from the corpus, the latter could be constructed from the distributional representations of re- and build. Besides alleviating data sparseness problems, a system of this sort, that automatically induces the semantic con</context>
<context position="10433" citStr="Socher et al., 2012" startWordPosition="1610" endWordPosition="1613">very inception of distributional semantics, there have been attempts to compose meanings for sentences and larger passages (Landauer and Dumais, 1997), but interest in compositional DSMs has skyrocketed in the last few years, particularly since the influential work of Mitchell and Lapata (2008; 2009; 2010). For the current study, we have reimplemented and adapted to the morphological setting all cDSMs we are aware of, excluding the tensorproduct-based models that Mitchell and Lapata (2010) have shown to be empirically disappointing and the models of Socher and colleagues (Socher et al., 2011; Socher et al., 2012), that require complex optimization procedures whose adaptation to morphology we leave to future work. Mitchell and Lapata proposed a set of simple and effective models in which the composed vectors are obtained through component-wise operations on the constituent vectors. Given input vectors u and v, the multiplicative model (mult) returns a composed vector c with: ci = uivi. In the weighted additive model (wadd), the composed vector is a weighted sum of the two input vectors: c = αu + βv, where α and β are two scalars. In the dilation model, the output vector is obtained by first decomposing</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher Manning, and Andrew Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of EMNLP, pages 1201–1211, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
</authors>
<title>Morphology and Computation.</title>
<date>1992</date>
<publisher>MIT Press,</publisher>
<location>Cambrdige, MA.</location>
<contexts>
<context position="6890" citStr="Sproat, 1992" startWordPosition="1074" endWordPosition="1075">, for the first time (except for the proof-of-concept study in Guevara (2009)), the application of cDSMs to derivational morphology. We adapt a number of composition methods from the literature to the morphological setting, and we show that some of these methods can provide better distributional representations of derived forms than either those directly harvested from a large corpus, or those obtained by using the stem as a proxy to derived-form meaning. Our 2Of course, spotting and segmenting complex words is a big research topic unto itself (Beesley and Karttunen, 2000; Black et al., 1991; Sproat, 1992), and one we completely sidestep here. results suggest that exploiting morphology could improve the quality of DSMs in general, extend the range of tasks that cDSMs can successfully model and support the development of new ways to test their performance. 2 Related work Morphological induction systems use corpusbased methods to decide if two words are morphologically related and/or to segment words into morphemes (Dreyer and Eisner, 2011; Goldsmith, 2001; Goldwater and McClosky, 2005; Goldwater, 2006; Naradowsky and Goldwater, 2009; Wicentowski, 2004). Morphological induction has recently recei</context>
</contexts>
<marker>Sproat, 1992</marker>
<rawString>Richard Sproat. 1992. Morphology and Computation. MIT Press, Cambrdige, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>37--141</pages>
<contexts>
<context position="1605" citStr="Turney and Pantel, 2010" startWordPosition="226" endWordPosition="229">ormance, and demonstrate the usefulness of a compositional morphology component in distributional semantics. 1 Introduction Effective ways to represent word meaning are needed in many branches of natural language processing. In the last decades, corpus-based methods have achieved some degree of success in modeling lexical semantics. Distributional semantic models (DSMs) in particular represent the meaning of a word by a vector, the dimensions of which encode corpus-extracted co-occurrence statistics, under the assumption that words that are semantically similar will occur in similar contexts (Turney and Pantel, 2010). Reliable distributional vectors can only be extracted for words that occur in many contexts in the corpus. Not surprisingly, there is a strong correlation between word frequency and vector quality (Bullinaria and Levy, 2007), and since most words occur only once even in very large corpora (Baroni, 2009), DSMs suffer data sparseness. While word rarity has many sources, one of the most common and systematic ones is the high productivity of morphological derivation processes, whereby an unlimited number of new words can be constructed by adding affixes to existing stems (Baayen, 2005; Bauer, 20</context>
<context position="9756" citStr="Turney and Pantel (2010)" startWordPosition="1501" endWordPosition="1504">and he only presented a qualitative analysis of the trajectories triggered by composition with various affixes. 1518 3 Composition methods Distributional semantic models (DSMs), also known as vector-space models, semantic spaces, or by the names of famous incarnations such as Latent Semantic Analysis or Topic Models, approximate the meaning of words with vectors that record their patterns of co-occurrence with corpus context features (often, other words). There is an extensive literature on how to develop such models and on their evaluation. Recent surveys include Clark (2012), Erk (2012) and Turney and Pantel (2010). We focus here on compositional DSMs (cDSMs). Since the very inception of distributional semantics, there have been attempts to compose meanings for sentences and larger passages (Landauer and Dumais, 1997), but interest in compositional DSMs has skyrocketed in the last few years, particularly since the influential work of Mitchell and Lapata (2008; 2009; 2010). For the current study, we have reimplemented and adapted to the morphological setting all cDSMs we are aware of, excluding the tensorproduct-based models that Mitchell and Lapata (2010) have shown to be empirically disappointing and t</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37:141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hsueh-Cheng Wang</author>
<author>Yi-Min Tien</author>
<author>Li-Chuan Hsu</author>
<author>Marc Pomplun</author>
</authors>
<title>Estimating semantic transparency of constituents of English compounds and two-character Chinese words using Latent Semantic Analysis.</title>
<date>2012</date>
<booktitle>In Proceedings of CogSci,</booktitle>
<pages>2499--2504</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="8683" citStr="Wang et al., 2012" startWordPosition="1337" endWordPosition="1340">ons of stems and affixes, semantic representations for the derived words containing those stems and affixes. A morphological induction system, given rebuild, will segment it into re- and build (possibly using distributional similarity between the words as a cue). Our system, given re- and build, predicts the (distributional semantic) meaning of rebuild. Another emerging line of research uses distributional semantics to model human intuitions about the semantic transparency of morphologically derived or compound expressions and how these impact various lexical processing tasks (Kuperman, 2009; Wang et al., 2012). Although these works exploit vectors representing complex forms, they do not attempt to generate them compositionally. The only similar study we are aware of is that of Guevara (2009). Guevara found a systematic geometric relation between corpus-based vectors of derived forms sharing an affix and their stems, and used this finding to motivate the composition method we term lexfunc below. However, unlike us, he did not test alternative models, and he only presented a qualitative analysis of the trajectories triggered by composition with various affixes. 1518 3 Composition methods Distribution</context>
</contexts>
<marker>Wang, Tien, Hsu, Pomplun, 2012</marker>
<rawString>Hsueh-Cheng Wang, Yi-Min Tien, Li-Chuan Hsu, and Marc Pomplun. 2012. Estimating semantic transparency of constituents of English compounds and two-character Chinese words using Latent Semantic Analysis. In Proceedings of CogSci, pages 2499– 2504, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Wicentowski</author>
</authors>
<title>Multilingual noiserobust supervised morphological analysis using the wordframe model.</title>
<date>2004</date>
<booktitle>In Proceedings of SIGPHON,</booktitle>
<pages>70--77</pages>
<location>Barcelona,</location>
<contexts>
<context position="7446" citStr="Wicentowski, 2004" startWordPosition="1157" endWordPosition="1159">Beesley and Karttunen, 2000; Black et al., 1991; Sproat, 1992), and one we completely sidestep here. results suggest that exploiting morphology could improve the quality of DSMs in general, extend the range of tasks that cDSMs can successfully model and support the development of new ways to test their performance. 2 Related work Morphological induction systems use corpusbased methods to decide if two words are morphologically related and/or to segment words into morphemes (Dreyer and Eisner, 2011; Goldsmith, 2001; Goldwater and McClosky, 2005; Goldwater, 2006; Naradowsky and Goldwater, 2009; Wicentowski, 2004). Morphological induction has recently received considerable attention since morphological analysis can mitigate data sparseness in domains such as parsing and machine translation (Goldberg and Tsarfaty, 2008; Lee, 2004). Among the cues that have been exploited there is distributional similarity among morphologically related words (Schone and Jurafsky, 2000; Yarowsky and Wicentowski, 2000). Our work, however, differs substantially from this track of research. We do not aim at segmenting morphological complex words or identifying paradigms. Our goal is to automatically construct, given distribu</context>
</contexts>
<marker>Wicentowski, 2004</marker>
<rawString>Richard Wicentowski. 2004. Multilingual noiserobust supervised morphological analysis using the wordframe model. In Proceedings of SIGPHON, pages 70–77, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Richard Wicentowski</author>
</authors>
<title>Minimally supervised morphological analysis by multimodal alignment.</title>
<date>2000</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>207--216</pages>
<location>Hong Kong.</location>
<contexts>
<context position="7838" citStr="Yarowsky and Wicentowski, 2000" startWordPosition="1210" endWordPosition="1213"> methods to decide if two words are morphologically related and/or to segment words into morphemes (Dreyer and Eisner, 2011; Goldsmith, 2001; Goldwater and McClosky, 2005; Goldwater, 2006; Naradowsky and Goldwater, 2009; Wicentowski, 2004). Morphological induction has recently received considerable attention since morphological analysis can mitigate data sparseness in domains such as parsing and machine translation (Goldberg and Tsarfaty, 2008; Lee, 2004). Among the cues that have been exploited there is distributional similarity among morphologically related words (Schone and Jurafsky, 2000; Yarowsky and Wicentowski, 2000). Our work, however, differs substantially from this track of research. We do not aim at segmenting morphological complex words or identifying paradigms. Our goal is to automatically construct, given distributional representations of stems and affixes, semantic representations for the derived words containing those stems and affixes. A morphological induction system, given rebuild, will segment it into re- and build (possibly using distributional similarity between the words as a cue). Our system, given re- and build, predicts the (distributional semantic) meaning of rebuild. Another emerging </context>
</contexts>
<marker>Yarowsky, Wicentowski, 2000</marker>
<rawString>David Yarowsky and Richard Wicentowski. 2000. Minimally supervised morphological analysis by multimodal alignment. In Proceedings of ACL, pages 207–216, Hong Kong.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>