<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000341">
<title confidence="0.9968475">
Automatic identification of semantic relations
in Italian complex nominals
</title>
<author confidence="0.995282">
Fabio Celli
</author>
<affiliation confidence="0.934828">
CLIC-CIMeC
University of Trento
</affiliation>
<email confidence="0.983714">
fabio.celli@email.unitn.it
</email>
<author confidence="0.823729">
Malvina Nissim
</author>
<affiliation confidence="0.8219055">
Dipartimento di Studi Linguistici e Orientali
University of Bologna
</affiliation>
<email confidence="0.972512">
malvina.nissim@unibo.it
</email>
<sectionHeader confidence="0.947187" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999870583333333">
This paper addresses the problem of the identification of the seman-
tic relations in Italian complex nominals (CNs) of the type N+P+N.
We exploit the fact that the semantic relation, which is underspeci-
fied in most cases, is partially made explicit by the preposition. We
develop an annotation framework around five different semantic rela-
tions, which we use to create a corpus of 1700 Italian CNs, obtaining an
inter-annotator agreement of K=.695. Exploiting this data, for each
preposition p we train a classifier to assign one of the five semantic
relations to any CN of the type N+p+N, by using both string and
supersense features. To obtain supersenses, we experiment with a se-
quential tagger as well as a plain lookup in MultiWordNet, and find
that using information obtained from the former yields better results.
</bodyText>
<sectionHeader confidence="0.997219" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997374">
Complex nominals are pervasive in language, and include noun-noun (N+N)
and adjective-noun (A+N) combinations (Levi, 1978), as in Ex. 1 and 2.
</bodyText>
<listItem confidence="0.995428">
(1) dessert fork
(2) medieval historian
</listItem>
<page confidence="0.989101">
45
</page>
<bodyText confidence="0.868881857142857">
Proceedings of the 8th International Conference on Computational Semantics, pages 45–60,
Tilburg, January 2009. c�2009 International Conference on Computational Semantics
A “dessert fork” is “a fork for eating dessert”, and a “medieval historian”
can be also described as ”a historian who studies medieval times”.&apos; In
both cases the relation is not overtly marked. Indeed, syntactically, there is
nothing that tells us that the semantic relation between “dessert” and “fork”
in Ex. 1 is different than the one binding “plastic” and “fork” in Ex. 3.
</bodyText>
<listItem confidence="0.902816">
(3) plastic fork
</listItem>
<bodyText confidence="0.999762">
However, it is well known that whereas English composes CNs of the type
N+N, Romance languages must glue the two nouns by means of a prepo-
sition, thus yielding CNs of the form N+P+N, thereby partially making
explicit the underlying semantic relation (Busa and Johnston, 1996). So, in
Ex. 4, the “purpose” relation between dessert and fork is (partially) made
explicit by the preposition “da”. In contrast, the “property” relation binding
plastic and fork (a fork made of plastic) is expressed using “di” (Ex. 5).
</bodyText>
<listItem confidence="0.993069">
(4) forchetta da dessert (en: dessert fork)
(5) forchetta di plastica (en: plastic fork)
</listItem>
<bodyText confidence="0.988517625">
Recently, Girju (2007) has exploited this observation including cross-language
information in a system for the automatic interpretation of NN compounds
in English. However, whereas it is true that the overt preposition restricts
the set of possible relations, it is also true that prepositions are still se-
mantically ambiguous, since there is no one-to-one correspondence between
prepositions and relations. So, “di”, used in a “property” relation above,
can also express a “part-whole” (Ex. 6), a “theme” (Ex. 7), and several other
relations.
</bodyText>
<listItem confidence="0.9355215">
(6) dorso della mano (the back of the hand)
(7) suonatore di chitarra (guitar player)
</listItem>
<bodyText confidence="0.999965">
In this work, we also exploit the presence of a preposition in Italian CNs as
an aid to detect the semantic relation. We extract and annotate CNs in a
corpus of written Italian, and develop a supervised system for determining
the semantics of the CN, comparing the contribution of plain nouns with
that of hypernym classes, and different ways in which such hypernyms can be
obtained. In the next section, we discuss previous work on the semantics of
complex nominals. In Section 3, we define a set of five semantic relations for
the annotation of Italian CNs and the details of the annotation framework,
and discuss the corpus distribution. In Section 4 we describe the experiments
for the automatic identification of semantic relations, and discuss the results.
We conclude with ideas for future work in Section 5.
</bodyText>
<footnote confidence="0.58234">
&apos;In this work we will only consider N+N CNs, thereby excluding A+N CNs.
</footnote>
<page confidence="0.999275">
46
</page>
<sectionHeader confidence="0.99039" genericHeader="method">
2 Previous work
</sectionHeader>
<bodyText confidence="0.999897095238096">
Given their underspecified nature, CNs, especially in English, have received
a large amount of attention in the linguistic and computational linguistic
literature (Downing, 1977; Levi, 1978; Warren, 1978; Lauer, 1995; John-
ston and Busa, 1996; Rosario and Hearst, 2001; Lapata, 2002; Girju, 2007,
among others). Current interest in NLP is also shown in the organisation of
a SemEval task especially dedicated to noun-noun compound interpretation
(Task 4, (Girju et al., 2007)). Indeed, NLP systems which aim at full text
understanding for higher NLP tasks, such as question answering, recognis-
ing textual entailment and machine translation, need to grasp the semantic
relation which noun compounds mostly leave underspecified.
One main issue in noun-noun compound interpretation is the lack of
general agreement on a well-defined set of semantic relations. Nastase and
Szpakowicz (2003), for instance, propose a two-level taxonomy, in which
fifteen fine-grained relations are subsumed into five general classes (causal,
participant, spatial, temporal, quality). An example of a causal relation
(with subtype “purpose”) is “concert hall”, and an example of a participant
relation (with subtype “beneficiary”) is “student discount”.
Girju et al. (2007) propose the smaller set reported in Table 1, which
was tested on English N+N complex nominals within the SemEval 2007 task.
They specifically spell out semantic relations as two-poles relationships: for
example an effect is an effect always with respect to a cause.
</bodyText>
<tableCaption confidence="0.999763">
Table 1: The set of 7 semantic relations from Girju et al. (2007)
</tableCaption>
<table confidence="0.902739">
Semantic relation Examples
Cause-Effect laugh (cause) wrinkles (effect)
Instrument-Agency laser (instrument) printer (agency)
Product-Producer honey (product) bee (producer)
Origin-Entity message (entity) from outer-space (origin)
Theme-Tool news (theme) conference(tool)
Part-Whole the door (part) of the car (whole)
Content-Container apples (content) in the basket (container)
</table>
<bodyText confidence="0.999841333333333">
As far as relation detection is concerned, Johnston and Busa (1996),
working specifically on Italian, have suggested using information included
in qualia structures (Pustejovsky, 1995) for deriving the compound’s inter-
pretation. The use of qualia structures for this task is appropriate and
semantically sound but absolutely not straightforward to implement, since
there does not exist an electronic repository of qualias, so that the structures
</bodyText>
<page confidence="0.998735">
47
</page>
<bodyText confidence="0.999976428571429">
would need to be constructed by hand, thereby involving a large amount of
manual work. Recent work has shown that the automatic acquisition of
qualias can be performed with reasonable success exploiting information
obtained using lexico-syntactic patterns over the Web (Cimiano and Wen-
deroth, 2005). For our purposes, though, if lexico-syntactic patterns can be
used successfully to induce qualia roles, we could directly use the informa-
tion we obtain from them, thus bypassing the qualia structure representa-
tion. We plan to include features based on such kinds of patterns in future
development of this work (see also (Nakov and Hearst, 2008)).
More purely computational approaches include both supervised (Lauer,
1995) as well as unsupervised models, such as (Lapata and Keller, 2005),
who use frequencies obtained over the Web. Some researchers also suggest
solutions to the data sparsness problem, which affects our approach as well,
by using lexical similarity (Turney, 2006) or clustering techniques (Pantel
and Pennacchiotti, 2006).
Finally, there exists specific work on compound nouns whose head is
derived from a verb (Lapata, 2002), and information about verbs deverbal
nouns are linked to has proved a useful feature in previous approaches (Girju,
2007). Whereas we have exploited this information in the annotation phase,
we have not included corresponding features yet in the statistical model we
use, but we plan to do so in future extensions.
</bodyText>
<sectionHeader confidence="0.942963" genericHeader="method">
3 Annotation Framework and Data
</sectionHeader>
<bodyText confidence="0.999608666666667">
For developing an annotation framework, we built on Italian grammars,
existing classifications (see Section 2), and a preliminary study of corpus
data.
</bodyText>
<subsectionHeader confidence="0.998763">
3.1 Annotation framework
</subsectionHeader>
<bodyText confidence="0.999602222222222">
In determining the set of relations to be annotated, following (Girju, 2007),
we also define two-pole relations between the involved nominals.
We assume that relations can be extracted and subsumed in general
classes starting from θ-roles, which are partially made explicit by the prepo-
sitional phrase. Since there is no general agreement on a complete list of
θ-roles we chose to work with types of complements, which are provided by
traditional Italian grammars and can be found in almost every Italian dic-
tionary. In (Zingarelli, 2008), we found 33 different types of prepositional
phrases (PPs), which we grouped into 21 classes (for instance, all of the
</bodyText>
<page confidence="0.998555">
48
</page>
<bodyText confidence="0.999635375">
location-related PPs were grouped under a single LOC class). This infor-
mation was included in the annotation scheme (Celli, 2008), although is not
used in the current relation identification model.
Following (Langacker, 1987), the nouns within each CNs were also re-
visited within a trajector (Tr) and landmark (Lm) approach mirroring the
two-pole interpretation of the semantic relations.
The set of five semantic relations we arrived at is given in Table 2. These
five relations are the target of our classification experiments (Section 4).
</bodyText>
<tableCaption confidence="0.967867">
Table 2: Relations for Italian prepositions.
</tableCaption>
<table confidence="0.880250833333333">
Relation(Tag) Description Examples
cause-effect (CE) tr. causes lm. deathLm by privationsTr
located-location (LL) lm. localizes tr. windowLm passageTr
owner-property (OP) tr. possess lm. stoneLm statueTr
included-set (IS) lm. includes tr. thousandsTr of menLm
bound-bounded (RR) lm. undergoes tr. cityLm destructionTr
</table>
<bodyText confidence="0.999934444444444">
In the cause-effect (CE) relation the trajector is the cause or the agent
and the landmark is the product or the effect produced by the agent/causer,
as in ”morte per stenti” (en: death by privations). In located-location (LL),
a trajector is located in space or time with respect to a landmark, as ”casa
in montagna” (en: mountain house). The owner-property (OP) relation as-
sociates a trajector (owner) with its property, part, or characteristic, which
is the landmark. Examples are ”statua di pietra” (en: stone statue) and
”cane da caccia” (en: hunting dog). In included-set (IS) the trajector is the
included object and the landmark is the set: in ”migliaia di uomini” (en:
thousands of men), ”migliaia” (en: thousands) is the subset and ”uomini”
(en: men) is the set. The bound-bounded (RR) relation is a direct rela-
tionship between an event, usually a deverbal (trajector), and its undergoer
(landmark), as ”distruzione della citt`a” (en: destruction of the city). Clas-
sic relations such as part-whole, producer-product, and is-a are covered in
this account by the owner-property, cause-effect and included-set relations,
respectively.
Annotation categories Each extracted CN (see Section 3.2) was anno-
tated with the following information:
</bodyText>
<listItem confidence="0.963504">
• the lemma (A, CON, DI, DA, IN, PER, SU, TRA)2
</listItem>
<footnote confidence="0.278813">
2The preposition ”tra” can also be written as ”fra”. They are semantically equivalent.
Occurrences of both variants were extracted, but we refer to them always as ”tra”.
</footnote>
<page confidence="0.97662">
49
</page>
<listItem confidence="0.9997744">
• the relation (CE, OP, LL, IS, RR)
• the type of prepositional phrase (21 tags)
• the semantic type of n1/n2 (natural, abstract, artifact, metaphorical usage)
• the position of trajector and landmark in the CN (TL, LT)
• the order of the head and the modifier in the CN (HM, MH)
</listItem>
<bodyText confidence="0.66424">
The following CN types were to be excluded from annotation:
</bodyText>
<listItem confidence="0.999775111111111">
• CNs including proper nouns, such as ”problema di Marco” (en: Mark’s prob-
lem);
• CNs involving complex prepositions, such as ”hotel nel mezzo del deserto”
(en: hotel in the middle of the desert);
• CNs involving n1 and/or n2 of categories other than noun, due to POS-
tagging errors;
• CNs containing bisyllabic prepositions, such as ”macchina senza benzina”
(en: car without fuel);3
• CNs used as adverbs, e.g. ”accordo di massima” (en: generally agreed with)
</listItem>
<subsectionHeader confidence="0.983775">
3.2 Data
</subsectionHeader>
<bodyText confidence="0.97746695">
Corpus Selection We used CORISsmall, a reduced version of CORIS,
a 100M-word, balanced corpus of written Italian (Rossini Favretti, 2000).
CORISsmall was sampled by randomly extracting sentences with a length
between 2 and 40 words. We discarded a few domain-specific subcorpora
which were likely to contain prepositions used in ways different from common
usage, as the legal subcorpus. The resulting corpus, henceforth CORISnom-
inals, contains 75,000 words. The corpus was then automatically tagged
with part-of-speech information, using TreeTagger (Schmid, 1994).
CN detection We chose to annotate monosyllabic prepositions only, namely
a (to), con (with), di (of), da (from), in (in),per (for), su (on) and tra
(within), because they are more frequent in CNs, more polysemous and
not occurring as any other grammatical category, differently from bisyllabic
prepositions which can be used adverbially. In any case, bisyllabic preposi-
tions occurr in less than 2% of all the extracted CNs (42 out of 2298).
Exploiting part-of-speech information, we extracted all the N+P+N com-
binations with a context window of 10 words left and right. The frequency
of the CNs found in CORISnominals is reported in Table 3.
3Prepositions which incorporate the determiner, such as ”della” (di+la, en: of the)
or ”sulla” (su+la, en: on the), although possible bisyllabic, are definite variants of their
corresponding monosyllabic prepositions, and are therefore included in the dataset.
</bodyText>
<page confidence="0.997587">
50
</page>
<tableCaption confidence="0.999653">
Table 3: Frequency of CN types in CORISnominals
</tableCaption>
<table confidence="0.9952182">
CNs extracted #inst example
N+P+N 1125 lampada a olio (oil lamp)
N+Pdet+N 1044 dorso della mano (back of the hand)
N+P+D+N 129 casa per le vacanze (holiday home)
total 2298
</table>
<bodyText confidence="0.9958181875">
Annotation procedure and evaluation The annotation was performed
by a native speaker of Italian, with experience in the semantic analysis of
complex nominals. After discarding some CNs according to the rules defined
in the annotation scheme, the final number of annotated instances is 1700.
In order to assess the difficulty of the relation assignment task, a randomly
extracted portion of the data (186 CNs) was further annotated by a second
native speaker of Italian. The second annotator marked them up following
specific guidelines and some training material composed of about 50 already
annotated CNs as examples. We calculated inter-annotator agreement using
Cohen’s kappa statistics (Cohen, 1960), obtaining a kappa of .695. While
this relatively not so high value can be considered satisfactory in the field
of semantic annotation (this score is also in the same ballpark as the 70.3%
agreement reported for the SemeEval Task 4 annotation (Girju et al., 2007)),
it still indicates that the phenomenon involves a good amount of ambiguity
thus making the classification task far from straightforward. Table 4 reports
the confusion matrix for the annotated subset.
</bodyText>
<tableCaption confidence="0.999288">
Table 4: Confusion matrix for annotator A and annotator B
</tableCaption>
<table confidence="0.999391857142857">
A/B CE IS LL OP RR total
CE 2 – – – – 2
IS 1 22 4 5 1 33
LL – – 12 4 1 17
OP 34 4 8 44 6 96
RR 5 – 1 3 29 38
total 42 26 25 56 37 186
</table>
<bodyText confidence="0.9993466">
The largest area of disagreement is in the opposition between CE and
OP: annotator B assigned the type CE to a large number of CNs which
annotator A had marked as OP. This might be due to the fact that CE
relations can be triggered by parts of objects (or features of concepts), which
are expressed by the OP relation. A prime example of such overlap is ”fumo
</bodyText>
<page confidence="0.993173">
51
</page>
<bodyText confidence="0.987147">
di sigaretta” (en: cigarette smoke), which can be seen both as a cause-effect
relation as well as a owner-property relation. Thus, future work will involve
a reassessment of these two categories and a revision of the guidelines.
Corpus Distribution Table 5 illustrates the distribution of semantic re-
lations across each preposition.
</bodyText>
<tableCaption confidence="0.809055">
Table 5: Distribution of relations across prepositions in CORISnominals
</tableCaption>
<bodyText confidence="0.988781375">
prep/rel CE IS LL OP RR total
a 0 8 29 34 28 99
con 0 5 0 10 14 29
di 62 262 69 646 289 1328
da 2 0 7 18 8 35
in 0 5 50 31 14 100
per 8 2 2 29 7 48
su 3 0 18 12 11 44
tra 0 0 4 3 10 17
total 75 282 179 783 381 1700
The most striking figure is the overwhelming predominance of ”di”,
which features in 78% of all CNs. This is in line with the extremely high
overall frequency of ”di” in Italian, which is ranked as the most frequent
word in CoLFIS (an Italian frequency lexicon based on a 3M word corpus,
Laudanna et al. (1995)), and also with Girju’s 2007 observation that 77.7%
of the English noun-noun compounds in her data can be rephrased as ”of”
phrases. We can also observe that some prepositions, namely ”a” and ”con”,
show more than one predominant relation usage in CNs. Overall, OP is by
far the most frequent relation, occurring in nearly half of the CNs.
As an additional observation, for each preposition we compared its fre-
quency of occurrence in CNs and in any other constructions. We found that
while ”di” and ”su” are particularly CN-oriented prepositions, both with
over 55% of their occurrences being in CNs, the others appear in CNs about
10% or less of their total occurrences.
</bodyText>
<sectionHeader confidence="0.93468" genericHeader="method">
4 Automatic identification of CN relations
</sectionHeader>
<bodyText confidence="0.995900666666667">
We can see the problem of semantic relations in CNs from at least two (con-
verging) points of view. From a more language understanding side, given a
CN (two nouns connected by a preposition), we might want to know what the
</bodyText>
<page confidence="0.998442">
52
</page>
<tableCaption confidence="0.96083">
Table 6: Accuracy for most frequent relation baseline and for basic system
</tableCaption>
<table confidence="0.998602333333333">
prep #inst most freq rel baseline basic system
a 99 OP (34) 34.34 47.47
con 29 RR (14) 48.28 48.28
da 35 OP (18) 51.43 51.43
di 1328 OP (646) 48.64 56.40
in 100 LL (50) 50.00 52.00
per 48 OP (29) 60.42 60.42
tra 17 RR (10) 58.82 58.82
su 44 LL (18) 40.91 50.00
</table>
<bodyText confidence="0.999885157894737">
underlying semantic relation is. From a more language generation perspec-
tive, though, we might want to be able to select the appropriate preposition,
given two nouns and a relation between the concepts they express.
This translates into two different classification tasks. One where the
target categories are relations, the other where they are prepositions. In
the work we describe in this paper we concentrate on the first task. For
each preposition we build a supervised model where the target categories
correspond to the annotation tags for the semantic relations: CE, IS, LL,
OP, RR. As evaluation measures, we report accuracy and coverage. Coverage
amounts to the portion of data for which supersenses could be found for both
n1 and n2, thus providing insights in assessing the contribution of different
supersense assignment methods (see Section 4.2 and Section 4.3).
For assessing the difficulty of the task, beside inter-annotator agreement,
we take a simple baseline where we assign to each CN the semantic relation
which is most frequently associated with the CN’s preposition (Table 6).
In the learning experiments, we use the Weka implementation (Wit-
ten and Frank, 2000) of the sequential minimal optimization algorithm for
training a support vector classifier, within a ten-fold cross-validation setting.
Girju (2007) has shown SVMs to be most efficient for this task.
</bodyText>
<subsectionHeader confidence="0.998385">
4.1 Basic system
</subsectionHeader>
<bodyText confidence="0.999956833333333">
The basic system uses as features only n1 and n2 as simple strings. Table 6
shows accuracy per preposition for the basic system and for the baseline.
The most evident limitation of this basic approach is data sparseness.
Out of 1700 CNs, 1662 involve a combination of n1 and n2 which occurs only
once, independently of the preposition used. The most frequent n1 (“parte”,
part) occurs 13 times with two different prepositions, and the most frequent
</bodyText>
<page confidence="0.996813">
53
</page>
<bodyText confidence="0.997741125">
n2 (“lavoro”, job/work) 16 across four different prepositions.
One intuitive way to alleviate the data sparseness problem without in-
creasing the corpus size, is to cluster instances. Following Girju (2007), who
uses hypernyms obtained from WordNet (Fellbaum, 1998) in place of strings,
we reduce each noun in our data set to its hypernym. In this supersense
assignment, we experimented with two procedures: a more sophisticated one
involving sequential sense tagging, thus dealing with sense disambiguation,
and a simpler one involving plain assignment of hypernyms.
</bodyText>
<subsectionHeader confidence="0.980502">
4.2 Hypernym selection via sense tagging
</subsectionHeader>
<bodyText confidence="0.999873448275862">
Two major problems related to finding a hypernym for a word are sense
ambiguity (one term can easily have more than one hypernym if it has
more than one sense) and coverage (even large ontologies/databases might
not include some of the encountered terms). A supersense tagger alleviates
such limitations by tagging words in context, thus tackling the ambiguity
issue, and by using a combination of features rather than just the lexical
entry, thereby being able to classify also words that are not included in the
dictionary. Picca et al. (2008) have developed such a tagger for Italian,
building on an existing version for English (Ciaramita and Altun, 2006),
retrained on MultiSemCor (Bentivogli and Pianta, 2005), a word-aligned
English-Italian corpus which contains the translation of the English texts
in SemCor. The set of 26 noun supersense labels come from MultiWordNet
(Pianta et al., 2002), a multilingual lexical database in which the Italian
WordNet is strictly aligned with Princeton WordNet 1.6, and which is linked
to MultiSemCor.
The average reported performance of the tagger is about 60% (Picca
et al., 2008). This relatively low accuracy introduces a large portion of errors
in the classification, thus reducing the advantage of dealing with supersenses
rather than words in the identification of semantic relations in CNs. Errors
can be of three types: (i) the assignment of a wrong noun class, (ii) the
assignment of a class of the wrong part-of-speech type (any non-noun tag),
and (iii) the non-assignment of any class (tag ”0”). Whereas errors of type
(i) can only be spotted via manual investigation, mistakes of type (ii) and
(iii) can be detected automatically and a backoff strategy can be deployed.
In 228 CNs out of 1700 both nouns have been assigned a ”0” tag. In a
further 751 CNs, one of the two nouns is tagged as ”0”. Out of these, there
are 33 cases where the other noun is assigned a non-noun tag (adj or verb).
A non-noun tag for n1 or n2 is also found in a further 57 cases.
As a backoff strategy for all cases that fall under (ii) and (iii), we searched
</bodyText>
<page confidence="0.99952">
54
</page>
<tableCaption confidence="0.949354">
Table 7: Results using supersenses obtained via tagging, in combination
with string features, and alone, and with and without backoff.
</tableCaption>
<table confidence="0.998809090909091">
no backoff backoff
prep #inst cov% acc% #inst cov% acc%
string nostring string nostring
a 32 32.32 68.75 75.00 99 100 45.46 44.44
con 11 37.93 72.73 63.64 29 100 62.07 58.62
da 14 40.00 64.29 57.14 35 100 65.71 65.71
di 526 39.61 58.55 51.71 1328 100 59.71 50.75
in 36 36.00 63.89 61.11 100 100 64.00 62.00
per 16 33.33 68.75 56.25 48 100 56.25 54.17
tra 10 58.82 70.00 70.00 17 100 64.71 64.71
su 20 45.45 45.00 50.00 44 100 54.54 47.73
</table>
<bodyText confidence="0.985666272727273">
hypernyms directly in MultiWordNet (MWN). (The set of possible hyper-
nyms is identical to the set of the 26 supersenses used by the tagger.) As a
first step, we lemmatised the string using Morph-it!, an existing lemmatiser
for Italian (Zanchetta and Baroni, 2005), since MWN contains lemmata but
not their morphological variants. Whenever we found more than one synset
associated to a term, a corresponding number of hypernyms was also found.
If one of the hypernyms was recurring more than the others, this was se-
lected. Otherwise, the hypernym associated to the first sense was selected.4
Whenever the lemmatised noun was not in MWN (106 cases), we assigned
the most frequent supersense in the dataset (”act” for both n1 and n2).
We then ran classification experiments using the obtained supersenses
for n1 and n2 as additional features, as well as on their own (thus ignoring
the original string—this is reported as ”nostring” in Tables 7–8), both with
and without the backoff strategy. In the latter case, we excluded all CNs
where at least one of the two nouns had been tagged as a non-noun or had no
supersense assignment. Under these settings coverage was seriously affected,
but accuracy was generally higher than when deploying the backoff strategy.
Table 7 reports results.
¢Optimally, we would select the hypernym for the most frequent sense (the one ranked
first in Princeton WordNet). However, synsets for a given term are not ordered by fre-
quency in MWN. One option would be to exploit frequencies from MultiSemCor, but the
corpus is rather small and might not be very reliable.
</bodyText>
<page confidence="0.999382">
55
</page>
<tableCaption confidence="0.9936165">
Table 8: Results using supersenses obtained via plain assignment, in com-
bination with string features, and alone, and with and without backoff.
</tableCaption>
<table confidence="0.999220363636363">
no backoff backoff
prep #inst cov% acc% #inst cov% acc%
string nostring string nostring
a 90 90.91 47.78 42.22 99 100 39.39 34.34
con 26 89.65 61.54 57.69 29 100 55.17 55.17
da 30 85.71 60.00 63.33 35 100 62.86 65.71
di 1178 88.70 61.88 52.63 1328 100 60.54 51.13
in 88 88.00 50.00 52.27 100 100 56.00 53.00
per 41 85.42 65.85 60.98 48 100 56.25 47.92
tra 14 82.35 42.86 42.86 17 100 47.06 35.29
su 36 81.82 52.78 52.78 44 100 59.09 40.91
</table>
<subsectionHeader confidence="0.907512">
4.3 Hypernym selection via plain assignment
</subsectionHeader>
<bodyText confidence="0.999973222222222">
Given the large number of cases where we had to resort to a backoff strategy
on the tagger’s output, we tried to obtain hypernyms from MWN directly,
thus bypassing the tagging stage. Whenever necessary, we employed the
backoff strategies described above: most frequent hypernym found for an
ambiguous term (or first sense’s hypernym in case of equal frequency), and
overall most frequent assigned hypernym in the corpus (”act” in this case as
well) for all those nouns that were not found in MWN. This direct lookup
approach should improve on coverage but suffer more from ambiguity-related
problems. Table 8 summarises the results.
</bodyText>
<subsectionHeader confidence="0.976459">
4.4 Discussion
</subsectionHeader>
<bodyText confidence="0.992252545454546">
Under the best settings, at full coverage, our average performance is around
59% (using tagger-assigned supersenses, backoff, the string feature), with
wide variation across prepositions. Given the currently limited set of fea-
tures, results are in general promising, especially if compared to the inter-
annotator agreement, and to previous work (see below).
When using supersenses obtained from the tagger, results are steadily
better than when using hypernyms directly looked up in MWN (both with
and without backoff) with the exception of “di” and “su”. The low coverage
but higher accuracy yielded when using the tagger’s senses without resorting
to a backoff strategy were both expected, as mentioned above.
Results suggest that the utility of a backoff strategy varies from one
</bodyText>
<page confidence="0.992544">
56
</page>
<bodyText confidence="0.999989725">
preposition to another. For instance, for “a”, ”con”, and ”per”, backoff
appears to lower performance, independently on how the supersenses were
obtained. These three prepositions had the three lowest coverage scores
when using the tagger, which suggests that if too large a proportion is left
to the approximation of backoff, the benefits of accurate sense tagging are
lost. This is not however true for the MWN lookup, where the coverage for
these three prepositions is rather high.
Additionally, we can observe that in most cases, in the back-off settings,
including the string as a feature helps improve the performance (both in the
tagging and in the plain assignment). This is likely due to the fact that the
approximation given by not having precise information about the supersense
and needing to resort to a backoff strategy is (partially) compensated by
taking into account the original noun. In contrast, using the string without
the backoff strategy on the tagger’s output yields a decrease in performance,
proving supersenses useful.
For a better assessment of the actual contribution of using hypernyms
for detecting the semantic relation without incurring in the noise introduced
by wrong hypernym assignments or the backoff strategy, we manually cor-
rected the tagger’s output in 60% of the data. This allowed us to evaluate
the tagger’s performance on supersense assignment for this 60% portion as
well as to compare on this subset, contaning 1024 CNs, an algorithm us-
ing ”gold” supersenses with that built on the tagger’s output (using the
backoff strategy, and including string features, see Section 4.2). We found
that supersenses were assigned by the tagger with an accuracy of 63.9%,
a result in line with previously reported performance (Picca et al., 2008).
We also observed that using the manually assigned hypernyms yielded an
average improvement of about seven percentage points over using the tag-
ger’s senses, although for some prepositions, instances in this smaller dataset
were just too few to draw any solid conclusion. Although more accurate,
the gold tags do not boost the performance as much as one might expect.
On the one hand, this might suggest that hypernyms can contribute only
to a certain extent to this task, and other more expressive features must be
found. On the other, it is also possible that the chosen set of 26 supersenses
is too large, especially for a dataset like ours which is rather small, thereby
not really overcoming the data sparseness problem.
Comparison to previous work in terms of performance is not straight-
forward, because of the language difference, the relation sets used, and the
evaluation settings. In the SemEval-2007 exercise, for example, for each
of the seven semantic relations used (see Table 1), a system must decide
whether a given instance expresses that relation or not within an ad hoc-
</bodyText>
<page confidence="0.994671">
57
</page>
<bodyText confidence="0.999911363636364">
built dataset, so that the overall semantic relation identification of the task
is actually split in seven different binary classification tasks, one per relation.
The highest reported average accuracy is 76.3% (Girju et al., 2007).
Girju (2007) classifies noun-noun compounds in 22 different semantic
relations. Best results on English are obtained when using a rich feature
set including cross-linguistic information. Reported figures differ slightly ac-
cording to the dataset used, with an average accuracy of 76.1%. When using
only language-internal supersense features, the average accuracy is 44.15%.
Girju (2007) also trains and tests another state-of-the-art supervised model
for English, namely Semantic Scattering (Moldovan and Badulescu, 2005),
reporting an average accuracy of 59.07%.
</bodyText>
<sectionHeader confidence="0.993495" genericHeader="conclusions">
5 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.9999846875">
We have presented a framework for the annotation of Italian complex nom-
inals in a very high data sparseness condition, and supervised models for
the identification of the underlying semantic relation for monosyllabic Ital-
ian prepositions. We exploited both string and supersense features, showing
that the importance of including string information varies from one prepo-
sition to another and from whether we are using backoff strategies or not.
We have also seen that for obtaining the supersenses, a sequential sense tag-
ging approach yields better overall results than a simple lookup in MWN,
although it dramatically cuts on coverage.
Future work will involve further classification experiments with addi-
tional features, including web counts obtained via lexico-syntactic patterns
(Lapata and Keller, 2005; Nakov and Hearst, 2008). We will exploit part of
the annotation which we have not considered in this study (see Section 3),
namely the type of prepositional phrase (see Appendix), a very general con-
ceptual clustering which also marks metaphorical usage, the position of tra-
jector and landmark in the CN, and the order of the head and the modifier.
</bodyText>
<sectionHeader confidence="0.986477" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.945470666666667">
Bentivogli, L. and E. Pianta (2005). Exploiting parallel texts in the cre-
ation of multilingual semantically annotated resources: the MultiSemCor
Corpus. Natural Language Engineering 11(3), 247–261.
Busa, F. and M. Johnston (1996). Cross-linguistic semantics for complex
nominals in the generative lexicon. In AISB Workshop on Multilinguality
in the Lexicon.
</bodyText>
<page confidence="0.997569">
58
</page>
<reference confidence="0.995412026315789">
Celli, F. (2008). La semantica delle preposizioni italiane nella combinazione
concettuale. Master thesis in Linguistics, Universit`a di Bologna.
Ciaramita, M. and Y. Altun (2006). Broad-coverage sense disambiguation
and information extraction with a supersense sequence tagger. In Pro-
ceedings of EMNLP 2006, pp. 594–602.
Cimiano, P. and J. Wenderoth (2005). Automatically learning qualia struc-
tures from the web. In Proceedings of the ACL-SIGLEX Workshop on
Deep Lexical Acquisition, Ann Arbor, Michigan, pp. 28–37.
Cohen, J. (1960). A coefficient of agreement for nominal scales. Educational
and Psychological Measurement 20, 37–46.
Downing, P. (1977). On the creation and use of English compound nouns.
Language 53, 810–842.
Fellbaum, C. (Ed.) (1998). WordNet: An Electronic Lexical Database. Cam-
bridge: MIT Press.
Girju, R. (2007). Improving the interpretation of noun phrases with cross-
linguistic information. In Proceedings of ACL’07, pp. 568–575.
Girju, R., P. Nakov, V. Nastase, S. Szpakowicz, P. Turney, and D. Yuret
(2007, June). SemEval-2007 Task 04: Classification of Semantic Relations
between Nominals. In Proceedings of SemEval-2007, pp. 13–18.
Johnston, M. and F. Busa (1996). Qualia structure and the compositional
interpretation of compounds. In Proceedings of the ACL Workshop on
breadth and depth of semantic lexicons.
Langacker, R.W. (1987). Foundations of cognitive grammar. Univ. Press.
Lapata, M. (2002). The disambiguation of nominalisations. Computational
Linguistics 28(3), 357–388.
Lapata, M. and F. Keller (2005). Web-based models for natural language
processing. ACM Transactions on Speech and Language Processing 2.
Laudanna, A., A. Thornton, G. Brown, C. Burani, and L. Marconi (1995).
Un corpus dell’italiano scritto contemporaneo dalla parte del ricevente. In
S. Bolasco, L. Lebart, and A. Salem (Eds.), III Giornate internazionali
di Analisi Statistica dei Dati Testuali. Volume I, pp. 103–109. Cisu.
Lauer, M. (1995). Corpus statistics meet the noun compound: some empir-
ical results. In Proceedings of ACL’95.
Levi, J. (1978). The Syntax and Semantics of Complex Nominals. Academic
Press.
Moldovan, D. and A. Badulescu (2005). A semantic scattering model for
the automatic interpretation of genitives. In Proceedings of HLT-EMNLP
2005, pp. 891–898.
</reference>
<page confidence="0.984434">
59
</page>
<reference confidence="0.976418576923077">
Nakov, P. and M. A. Hearst (2008). Solving relational similarity problems
using the web as a corpus. In Proceedings of ACL-08: HLT, Columbus,
Ohio, pp. 452–460. Association for Computational Linguistics.
Nastase, V. and S. Szpakowicz (2003). Exploring noun-modifier semantic
relations. In Proceedings of IWCS-5, pp. 285–301.
Pantel, P. and M. Pennacchiotti (2006). Espresso: Leveraging generic pat-
terns for automatically harvesting semantic relations. In Proceedings of
ACL’06, Sydney, Australia, pp. 113–120.
Pianta, E., L. Bentivogli, and C. Girardi (2002). MultiWordNet: developing
an aligned multilingual database. In Proceedings of the First International
Conference on Global WordNet, pp. 293–302.
Picca, D., A. M. Gliozzo, and M. Ciaramita (2008). Supersense Tagger for
Italian. In Proceedings of LREC 2008.
Pustejovsky, J. (1995). The Generative Lexicon. The MIT Press.
Rosario, B. and M. Hearst (2001). Classifying the semantic relations in
noun compounds via a domain-specific lexical hierarchy. In L. Lee and
D. Harman (Eds.), Proceedings of EMNLP 2001, pp. 82–90.
Rossini Favretti, R. (2000). Progettazione e costruzione di un corpus di ital-
iano scritto: CORIS/CODIS. In R. Rossini Favretti (Ed.), Linguistica e
informatica. Multimedialit`a, corpora e percorsi di apprendimento. Bulzoni.
Schmid, H. (1994). Probabilistic part-of-speech tagging using decision trees.
In Proc. of the Conference on New Methods in Language Processing, 44-49.
Turney, P. D. (2006). Expressing implicit semantic relations without super-
vision. In Proceedings of ACL’06, Sydney, Australia, pp. 313–320.
Warren, B. (1978). Semantic patterns of noun-noun compounds. Gothenburg
Studies in English 41.
</reference>
<bodyText confidence="0.801424666666667">
Witten, I. H. and E. Frank (2000). Data Mining: Practical Machine Learning
Tools and Techniques with Java Implementations. Morgan Kaufmann.
Zanchetta, E. and M. Baroni (2005). Morph-it! a free corpus-based morpho-
logical resource for the italian language. Corpus Linguistics 2005 1(1).
Zingarelli, N. (2008). Lo Zingarelli 2008. Vocabolario della Lingua Italiana.
Zanichelli.
</bodyText>
<page confidence="0.994817">
60
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.666362">
<title confidence="0.998203">Automatic identification of semantic in Italian complex nominals</title>
<author confidence="0.998601">Fabio</author>
<affiliation confidence="0.999147">University of</affiliation>
<email confidence="0.970937">fabio.celli@email.unitn.it</email>
<author confidence="0.794114">Malvina</author>
<affiliation confidence="0.957114">Dipartimento di Studi Linguistici e University of</affiliation>
<email confidence="0.96357">malvina.nissim@unibo.it</email>
<abstract confidence="0.998777">This paper addresses the problem of the identification of the semantic relations in Italian complex nominals (CNs) of the type N+P+N. We exploit the fact that the semantic relation, which is underspecified in most cases, is partially made explicit by the preposition. We develop an annotation framework around five different semantic relations, which we use to create a corpus of 1700 Italian CNs, obtaining an inter-annotator agreement of K=.695. Exploiting this data, for each train a classifier to assign one of the five semantic to any CN of the type by using both string and supersense features. To obtain supersenses, we experiment with a sequential tagger as well as a plain lookup in MultiWordNet, and find that using information obtained from the former yields better results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>F Celli</author>
</authors>
<title>La semantica delle preposizioni italiane nella combinazione concettuale. Master thesis in Linguistics, Universit`a di Bologna.</title>
<date>2008</date>
<contexts>
<context position="8858" citStr="Celli, 2008" startWordPosition="1357" endWordPosition="1358">acted and subsumed in general classes starting from θ-roles, which are partially made explicit by the prepositional phrase. Since there is no general agreement on a complete list of θ-roles we chose to work with types of complements, which are provided by traditional Italian grammars and can be found in almost every Italian dictionary. In (Zingarelli, 2008), we found 33 different types of prepositional phrases (PPs), which we grouped into 21 classes (for instance, all of the 48 location-related PPs were grouped under a single LOC class). This information was included in the annotation scheme (Celli, 2008), although is not used in the current relation identification model. Following (Langacker, 1987), the nouns within each CNs were also revisited within a trajector (Tr) and landmark (Lm) approach mirroring the two-pole interpretation of the semantic relations. The set of five semantic relations we arrived at is given in Table 2. These five relations are the target of our classification experiments (Section 4). Table 2: Relations for Italian prepositions. Relation(Tag) Description Examples cause-effect (CE) tr. causes lm. deathLm by privationsTr located-location (LL) lm. localizes tr. windowLm p</context>
</contexts>
<marker>Celli, 2008</marker>
<rawString>Celli, F. (2008). La semantica delle preposizioni italiane nella combinazione concettuale. Master thesis in Linguistics, Universit`a di Bologna.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ciaramita</author>
<author>Y Altun</author>
</authors>
<title>Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>594--602</pages>
<contexts>
<context position="20654" citStr="Ciaramita and Altun, 2006" startWordPosition="3317" endWordPosition="3320">ding a hypernym for a word are sense ambiguity (one term can easily have more than one hypernym if it has more than one sense) and coverage (even large ontologies/databases might not include some of the encountered terms). A supersense tagger alleviates such limitations by tagging words in context, thus tackling the ambiguity issue, and by using a combination of features rather than just the lexical entry, thereby being able to classify also words that are not included in the dictionary. Picca et al. (2008) have developed such a tagger for Italian, building on an existing version for English (Ciaramita and Altun, 2006), retrained on MultiSemCor (Bentivogli and Pianta, 2005), a word-aligned English-Italian corpus which contains the translation of the English texts in SemCor. The set of 26 noun supersense labels come from MultiWordNet (Pianta et al., 2002), a multilingual lexical database in which the Italian WordNet is strictly aligned with Princeton WordNet 1.6, and which is linked to MultiSemCor. The average reported performance of the tagger is about 60% (Picca et al., 2008). This relatively low accuracy introduces a large portion of errors in the classification, thus reducing the advantage of dealing wit</context>
</contexts>
<marker>Ciaramita, Altun, 2006</marker>
<rawString>Ciaramita, M. and Y. Altun (2006). Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger. In Proceedings of EMNLP 2006, pp. 594–602.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Cimiano</author>
<author>J Wenderoth</author>
</authors>
<title>Automatically learning qualia structures from the web.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL-SIGLEX Workshop on Deep Lexical Acquisition,</booktitle>
<pages>28--37</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="6696" citStr="Cimiano and Wenderoth, 2005" startWordPosition="1017" endWordPosition="1021">ing information included in qualia structures (Pustejovsky, 1995) for deriving the compound’s interpretation. The use of qualia structures for this task is appropriate and semantically sound but absolutely not straightforward to implement, since there does not exist an electronic repository of qualias, so that the structures 47 would need to be constructed by hand, thereby involving a large amount of manual work. Recent work has shown that the automatic acquisition of qualias can be performed with reasonable success exploiting information obtained using lexico-syntactic patterns over the Web (Cimiano and Wenderoth, 2005). For our purposes, though, if lexico-syntactic patterns can be used successfully to induce qualia roles, we could directly use the information we obtain from them, thus bypassing the qualia structure representation. We plan to include features based on such kinds of patterns in future development of this work (see also (Nakov and Hearst, 2008)). More purely computational approaches include both supervised (Lauer, 1995) as well as unsupervised models, such as (Lapata and Keller, 2005), who use frequencies obtained over the Web. Some researchers also suggest solutions to the data sparsness prob</context>
</contexts>
<marker>Cimiano, Wenderoth, 2005</marker>
<rawString>Cimiano, P. and J. Wenderoth (2005). Automatically learning qualia structures from the web. In Proceedings of the ACL-SIGLEX Workshop on Deep Lexical Acquisition, Ann Arbor, Michigan, pp. 28–37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<journal>Educational and Psychological Measurement</journal>
<volume>20</volume>
<pages>37--46</pages>
<contexts>
<context position="14305" citStr="Cohen, 1960" startWordPosition="2208" endWordPosition="2209">lian, with experience in the semantic analysis of complex nominals. After discarding some CNs according to the rules defined in the annotation scheme, the final number of annotated instances is 1700. In order to assess the difficulty of the relation assignment task, a randomly extracted portion of the data (186 CNs) was further annotated by a second native speaker of Italian. The second annotator marked them up following specific guidelines and some training material composed of about 50 already annotated CNs as examples. We calculated inter-annotator agreement using Cohen’s kappa statistics (Cohen, 1960), obtaining a kappa of .695. While this relatively not so high value can be considered satisfactory in the field of semantic annotation (this score is also in the same ballpark as the 70.3% agreement reported for the SemeEval Task 4 annotation (Girju et al., 2007)), it still indicates that the phenomenon involves a good amount of ambiguity thus making the classification task far from straightforward. Table 4 reports the confusion matrix for the annotated subset. Table 4: Confusion matrix for annotator A and annotator B A/B CE IS LL OP RR total CE 2 – – – – 2 IS 1 22 4 5 1 33 LL – – 12 4 1 17 O</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>Cohen, J. (1960). A coefficient of agreement for nominal scales. Educational and Psychological Measurement 20, 37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Downing</author>
</authors>
<title>On the creation and use of English compound nouns.</title>
<date>1977</date>
<journal>Language</journal>
<volume>53</volume>
<pages>810--842</pages>
<contexts>
<context position="4165" citStr="Downing, 1977" startWordPosition="653" endWordPosition="654">Section 3, we define a set of five semantic relations for the annotation of Italian CNs and the details of the annotation framework, and discuss the corpus distribution. In Section 4 we describe the experiments for the automatic identification of semantic relations, and discuss the results. We conclude with ideas for future work in Section 5. &apos;In this work we will only consider N+N CNs, thereby excluding A+N CNs. 46 2 Previous work Given their underspecified nature, CNs, especially in English, have received a large amount of attention in the linguistic and computational linguistic literature (Downing, 1977; Levi, 1978; Warren, 1978; Lauer, 1995; Johnston and Busa, 1996; Rosario and Hearst, 2001; Lapata, 2002; Girju, 2007, among others). Current interest in NLP is also shown in the organisation of a SemEval task especially dedicated to noun-noun compound interpretation (Task 4, (Girju et al., 2007)). Indeed, NLP systems which aim at full text understanding for higher NLP tasks, such as question answering, recognising textual entailment and machine translation, need to grasp the semantic relation which noun compounds mostly leave underspecified. One main issue in noun-noun compound interpretation</context>
</contexts>
<marker>Downing, 1977</marker>
<rawString>Downing, P. (1977). On the creation and use of English compound nouns. Language 53, 810–842.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<location>Cambridge:</location>
<contexts>
<context position="19651" citStr="Fellbaum, 1998" startWordPosition="3159" endWordPosition="3160">ition for the basic system and for the baseline. The most evident limitation of this basic approach is data sparseness. Out of 1700 CNs, 1662 involve a combination of n1 and n2 which occurs only once, independently of the preposition used. The most frequent n1 (“parte”, part) occurs 13 times with two different prepositions, and the most frequent 53 n2 (“lavoro”, job/work) 16 across four different prepositions. One intuitive way to alleviate the data sparseness problem without increasing the corpus size, is to cluster instances. Following Girju (2007), who uses hypernyms obtained from WordNet (Fellbaum, 1998) in place of strings, we reduce each noun in our data set to its hypernym. In this supersense assignment, we experimented with two procedures: a more sophisticated one involving sequential sense tagging, thus dealing with sense disambiguation, and a simpler one involving plain assignment of hypernyms. 4.2 Hypernym selection via sense tagging Two major problems related to finding a hypernym for a word are sense ambiguity (one term can easily have more than one hypernym if it has more than one sense) and coverage (even large ontologies/databases might not include some of the encountered terms). </context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Fellbaum, C. (Ed.) (1998). WordNet: An Electronic Lexical Database. Cambridge: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Girju</author>
</authors>
<title>Improving the interpretation of noun phrases with crosslinguistic information.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL’07,</booktitle>
<pages>568--575</pages>
<contexts>
<context position="2473" citStr="Girju (2007)" startWordPosition="382" endWordPosition="383">is well known that whereas English composes CNs of the type N+N, Romance languages must glue the two nouns by means of a preposition, thus yielding CNs of the form N+P+N, thereby partially making explicit the underlying semantic relation (Busa and Johnston, 1996). So, in Ex. 4, the “purpose” relation between dessert and fork is (partially) made explicit by the preposition “da”. In contrast, the “property” relation binding plastic and fork (a fork made of plastic) is expressed using “di” (Ex. 5). (4) forchetta da dessert (en: dessert fork) (5) forchetta di plastica (en: plastic fork) Recently, Girju (2007) has exploited this observation including cross-language information in a system for the automatic interpretation of NN compounds in English. However, whereas it is true that the overt preposition restricts the set of possible relations, it is also true that prepositions are still semantically ambiguous, since there is no one-to-one correspondence between prepositions and relations. So, “di”, used in a “property” relation above, can also express a “part-whole” (Ex. 6), a “theme” (Ex. 7), and several other relations. (6) dorso della mano (the back of the hand) (7) suonatore di chitarra (guitar </context>
<context position="4282" citStr="Girju, 2007" startWordPosition="672" endWordPosition="673">on framework, and discuss the corpus distribution. In Section 4 we describe the experiments for the automatic identification of semantic relations, and discuss the results. We conclude with ideas for future work in Section 5. &apos;In this work we will only consider N+N CNs, thereby excluding A+N CNs. 46 2 Previous work Given their underspecified nature, CNs, especially in English, have received a large amount of attention in the linguistic and computational linguistic literature (Downing, 1977; Levi, 1978; Warren, 1978; Lauer, 1995; Johnston and Busa, 1996; Rosario and Hearst, 2001; Lapata, 2002; Girju, 2007, among others). Current interest in NLP is also shown in the organisation of a SemEval task especially dedicated to noun-noun compound interpretation (Task 4, (Girju et al., 2007)). Indeed, NLP systems which aim at full text understanding for higher NLP tasks, such as question answering, recognising textual entailment and machine translation, need to grasp the semantic relation which noun compounds mostly leave underspecified. One main issue in noun-noun compound interpretation is the lack of general agreement on a well-defined set of semantic relations. Nastase and Szpakowicz (2003), for ins</context>
<context position="7664" citStr="Girju, 2007" startWordPosition="1168" endWordPosition="1169">rely computational approaches include both supervised (Lauer, 1995) as well as unsupervised models, such as (Lapata and Keller, 2005), who use frequencies obtained over the Web. Some researchers also suggest solutions to the data sparsness problem, which affects our approach as well, by using lexical similarity (Turney, 2006) or clustering techniques (Pantel and Pennacchiotti, 2006). Finally, there exists specific work on compound nouns whose head is derived from a verb (Lapata, 2002), and information about verbs deverbal nouns are linked to has proved a useful feature in previous approaches (Girju, 2007). Whereas we have exploited this information in the annotation phase, we have not included corresponding features yet in the statistical model we use, but we plan to do so in future extensions. 3 Annotation Framework and Data For developing an annotation framework, we built on Italian grammars, existing classifications (see Section 2), and a preliminary study of corpus data. 3.1 Annotation framework In determining the set of relations to be annotated, following (Girju, 2007), we also define two-pole relations between the involved nominals. We assume that relations can be extracted and subsumed</context>
<context position="18866" citStr="Girju (2007)" startWordPosition="3031" endWordPosition="3032">und for both n1 and n2, thus providing insights in assessing the contribution of different supersense assignment methods (see Section 4.2 and Section 4.3). For assessing the difficulty of the task, beside inter-annotator agreement, we take a simple baseline where we assign to each CN the semantic relation which is most frequently associated with the CN’s preposition (Table 6). In the learning experiments, we use the Weka implementation (Witten and Frank, 2000) of the sequential minimal optimization algorithm for training a support vector classifier, within a ten-fold cross-validation setting. Girju (2007) has shown SVMs to be most efficient for this task. 4.1 Basic system The basic system uses as features only n1 and n2 as simple strings. Table 6 shows accuracy per preposition for the basic system and for the baseline. The most evident limitation of this basic approach is data sparseness. Out of 1700 CNs, 1662 involve a combination of n1 and n2 which occurs only once, independently of the preposition used. The most frequent n1 (“parte”, part) occurs 13 times with two different prepositions, and the most frequent 53 n2 (“lavoro”, job/work) 16 across four different prepositions. One intuitive wa</context>
<context position="29434" citStr="Girju (2007)" startWordPosition="4778" endWordPosition="4779">. Comparison to previous work in terms of performance is not straightforward, because of the language difference, the relation sets used, and the evaluation settings. In the SemEval-2007 exercise, for example, for each of the seven semantic relations used (see Table 1), a system must decide whether a given instance expresses that relation or not within an ad hoc57 built dataset, so that the overall semantic relation identification of the task is actually split in seven different binary classification tasks, one per relation. The highest reported average accuracy is 76.3% (Girju et al., 2007). Girju (2007) classifies noun-noun compounds in 22 different semantic relations. Best results on English are obtained when using a rich feature set including cross-linguistic information. Reported figures differ slightly according to the dataset used, with an average accuracy of 76.1%. When using only language-internal supersense features, the average accuracy is 44.15%. Girju (2007) also trains and tests another state-of-the-art supervised model for English, namely Semantic Scattering (Moldovan and Badulescu, 2005), reporting an average accuracy of 59.07%. 5 Conclusions and future work We have presented a</context>
</contexts>
<marker>Girju, 2007</marker>
<rawString>Girju, R. (2007). Improving the interpretation of noun phrases with crosslinguistic information. In Proceedings of ACL’07, pp. 568–575.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Girju</author>
<author>P Nakov</author>
<author>V Nastase</author>
<author>S Szpakowicz</author>
<author>P Turney</author>
<author>D Yuret</author>
</authors>
<date>2007</date>
<booktitle>SemEval-2007 Task 04: Classification of Semantic Relations between Nominals. In Proceedings of SemEval-2007,</booktitle>
<pages>13--18</pages>
<contexts>
<context position="4462" citStr="Girju et al., 2007" startWordPosition="698" endWordPosition="701"> We conclude with ideas for future work in Section 5. &apos;In this work we will only consider N+N CNs, thereby excluding A+N CNs. 46 2 Previous work Given their underspecified nature, CNs, especially in English, have received a large amount of attention in the linguistic and computational linguistic literature (Downing, 1977; Levi, 1978; Warren, 1978; Lauer, 1995; Johnston and Busa, 1996; Rosario and Hearst, 2001; Lapata, 2002; Girju, 2007, among others). Current interest in NLP is also shown in the organisation of a SemEval task especially dedicated to noun-noun compound interpretation (Task 4, (Girju et al., 2007)). Indeed, NLP systems which aim at full text understanding for higher NLP tasks, such as question answering, recognising textual entailment and machine translation, need to grasp the semantic relation which noun compounds mostly leave underspecified. One main issue in noun-noun compound interpretation is the lack of general agreement on a well-defined set of semantic relations. Nastase and Szpakowicz (2003), for instance, propose a two-level taxonomy, in which fifteen fine-grained relations are subsumed into five general classes (causal, participant, spatial, temporal, quality). An example of</context>
<context position="14569" citStr="Girju et al., 2007" startWordPosition="2251" endWordPosition="2254">ent task, a randomly extracted portion of the data (186 CNs) was further annotated by a second native speaker of Italian. The second annotator marked them up following specific guidelines and some training material composed of about 50 already annotated CNs as examples. We calculated inter-annotator agreement using Cohen’s kappa statistics (Cohen, 1960), obtaining a kappa of .695. While this relatively not so high value can be considered satisfactory in the field of semantic annotation (this score is also in the same ballpark as the 70.3% agreement reported for the SemeEval Task 4 annotation (Girju et al., 2007)), it still indicates that the phenomenon involves a good amount of ambiguity thus making the classification task far from straightforward. Table 4 reports the confusion matrix for the annotated subset. Table 4: Confusion matrix for annotator A and annotator B A/B CE IS LL OP RR total CE 2 – – – – 2 IS 1 22 4 5 1 33 LL – – 12 4 1 17 OP 34 4 8 44 6 96 RR 5 – 1 3 29 38 total 42 26 25 56 37 186 The largest area of disagreement is in the opposition between CE and OP: annotator B assigned the type CE to a large number of CNs which annotator A had marked as OP. This might be due to the fact that CE </context>
<context position="29420" citStr="Girju et al., 2007" startWordPosition="4774" endWordPosition="4777">ta sparseness problem. Comparison to previous work in terms of performance is not straightforward, because of the language difference, the relation sets used, and the evaluation settings. In the SemEval-2007 exercise, for example, for each of the seven semantic relations used (see Table 1), a system must decide whether a given instance expresses that relation or not within an ad hoc57 built dataset, so that the overall semantic relation identification of the task is actually split in seven different binary classification tasks, one per relation. The highest reported average accuracy is 76.3% (Girju et al., 2007). Girju (2007) classifies noun-noun compounds in 22 different semantic relations. Best results on English are obtained when using a rich feature set including cross-linguistic information. Reported figures differ slightly according to the dataset used, with an average accuracy of 76.1%. When using only language-internal supersense features, the average accuracy is 44.15%. Girju (2007) also trains and tests another state-of-the-art supervised model for English, namely Semantic Scattering (Moldovan and Badulescu, 2005), reporting an average accuracy of 59.07%. 5 Conclusions and future work We ha</context>
</contexts>
<marker>Girju, Nakov, Nastase, Szpakowicz, Turney, Yuret, 2007</marker>
<rawString>Girju, R., P. Nakov, V. Nastase, S. Szpakowicz, P. Turney, and D. Yuret (2007, June). SemEval-2007 Task 04: Classification of Semantic Relations between Nominals. In Proceedings of SemEval-2007, pp. 13–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnston</author>
<author>F Busa</author>
</authors>
<title>Qualia structure and the compositional interpretation of compounds.</title>
<date>1996</date>
<booktitle>In Proceedings of the ACL Workshop on breadth and depth of semantic lexicons.</booktitle>
<contexts>
<context position="4229" citStr="Johnston and Busa, 1996" startWordPosition="661" endWordPosition="665">for the annotation of Italian CNs and the details of the annotation framework, and discuss the corpus distribution. In Section 4 we describe the experiments for the automatic identification of semantic relations, and discuss the results. We conclude with ideas for future work in Section 5. &apos;In this work we will only consider N+N CNs, thereby excluding A+N CNs. 46 2 Previous work Given their underspecified nature, CNs, especially in English, have received a large amount of attention in the linguistic and computational linguistic literature (Downing, 1977; Levi, 1978; Warren, 1978; Lauer, 1995; Johnston and Busa, 1996; Rosario and Hearst, 2001; Lapata, 2002; Girju, 2007, among others). Current interest in NLP is also shown in the organisation of a SemEval task especially dedicated to noun-noun compound interpretation (Task 4, (Girju et al., 2007)). Indeed, NLP systems which aim at full text understanding for higher NLP tasks, such as question answering, recognising textual entailment and machine translation, need to grasp the semantic relation which noun compounds mostly leave underspecified. One main issue in noun-noun compound interpretation is the lack of general agreement on a well-defined set of seman</context>
<context position="6016" citStr="Johnston and Busa (1996)" startWordPosition="919" endWordPosition="922">mantic relations as two-poles relationships: for example an effect is an effect always with respect to a cause. Table 1: The set of 7 semantic relations from Girju et al. (2007) Semantic relation Examples Cause-Effect laugh (cause) wrinkles (effect) Instrument-Agency laser (instrument) printer (agency) Product-Producer honey (product) bee (producer) Origin-Entity message (entity) from outer-space (origin) Theme-Tool news (theme) conference(tool) Part-Whole the door (part) of the car (whole) Content-Container apples (content) in the basket (container) As far as relation detection is concerned, Johnston and Busa (1996), working specifically on Italian, have suggested using information included in qualia structures (Pustejovsky, 1995) for deriving the compound’s interpretation. The use of qualia structures for this task is appropriate and semantically sound but absolutely not straightforward to implement, since there does not exist an electronic repository of qualias, so that the structures 47 would need to be constructed by hand, thereby involving a large amount of manual work. Recent work has shown that the automatic acquisition of qualias can be performed with reasonable success exploiting information obt</context>
</contexts>
<marker>Johnston, Busa, 1996</marker>
<rawString>Johnston, M. and F. Busa (1996). Qualia structure and the compositional interpretation of compounds. In Proceedings of the ACL Workshop on breadth and depth of semantic lexicons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R W Langacker</author>
</authors>
<title>Foundations of cognitive grammar.</title>
<date>1987</date>
<journal>Computational Linguistics</journal>
<volume>28</volume>
<issue>3</issue>
<pages>357--388</pages>
<publisher>Univ. Press.</publisher>
<location>Lapata, M.</location>
<contexts>
<context position="8954" citStr="Langacker, 1987" startWordPosition="1370" endWordPosition="1371">it by the prepositional phrase. Since there is no general agreement on a complete list of θ-roles we chose to work with types of complements, which are provided by traditional Italian grammars and can be found in almost every Italian dictionary. In (Zingarelli, 2008), we found 33 different types of prepositional phrases (PPs), which we grouped into 21 classes (for instance, all of the 48 location-related PPs were grouped under a single LOC class). This information was included in the annotation scheme (Celli, 2008), although is not used in the current relation identification model. Following (Langacker, 1987), the nouns within each CNs were also revisited within a trajector (Tr) and landmark (Lm) approach mirroring the two-pole interpretation of the semantic relations. The set of five semantic relations we arrived at is given in Table 2. These five relations are the target of our classification experiments (Section 4). Table 2: Relations for Italian prepositions. Relation(Tag) Description Examples cause-effect (CE) tr. causes lm. deathLm by privationsTr located-location (LL) lm. localizes tr. windowLm passageTr owner-property (OP) tr. possess lm. stoneLm statueTr included-set (IS) lm. includes tr.</context>
</contexts>
<marker>Langacker, 1987</marker>
<rawString>Langacker, R.W. (1987). Foundations of cognitive grammar. Univ. Press. Lapata, M. (2002). The disambiguation of nominalisations. Computational Linguistics 28(3), 357–388.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lapata</author>
<author>F Keller</author>
</authors>
<title>Web-based models for natural language processing.</title>
<date>2005</date>
<journal>ACM Transactions on Speech and Language Processing</journal>
<volume>2</volume>
<contexts>
<context position="7185" citStr="Lapata and Keller, 2005" startWordPosition="1094" endWordPosition="1097">rmed with reasonable success exploiting information obtained using lexico-syntactic patterns over the Web (Cimiano and Wenderoth, 2005). For our purposes, though, if lexico-syntactic patterns can be used successfully to induce qualia roles, we could directly use the information we obtain from them, thus bypassing the qualia structure representation. We plan to include features based on such kinds of patterns in future development of this work (see also (Nakov and Hearst, 2008)). More purely computational approaches include both supervised (Lauer, 1995) as well as unsupervised models, such as (Lapata and Keller, 2005), who use frequencies obtained over the Web. Some researchers also suggest solutions to the data sparsness problem, which affects our approach as well, by using lexical similarity (Turney, 2006) or clustering techniques (Pantel and Pennacchiotti, 2006). Finally, there exists specific work on compound nouns whose head is derived from a verb (Lapata, 2002), and information about verbs deverbal nouns are linked to has proved a useful feature in previous approaches (Girju, 2007). Whereas we have exploited this information in the annotation phase, we have not included corresponding features yet in </context>
<context position="30822" citStr="Lapata and Keller, 2005" startWordPosition="4977" endWordPosition="4980">mantic relation for monosyllabic Italian prepositions. We exploited both string and supersense features, showing that the importance of including string information varies from one preposition to another and from whether we are using backoff strategies or not. We have also seen that for obtaining the supersenses, a sequential sense tagging approach yields better overall results than a simple lookup in MWN, although it dramatically cuts on coverage. Future work will involve further classification experiments with additional features, including web counts obtained via lexico-syntactic patterns (Lapata and Keller, 2005; Nakov and Hearst, 2008). We will exploit part of the annotation which we have not considered in this study (see Section 3), namely the type of prepositional phrase (see Appendix), a very general conceptual clustering which also marks metaphorical usage, the position of trajector and landmark in the CN, and the order of the head and the modifier. References Bentivogli, L. and E. Pianta (2005). Exploiting parallel texts in the creation of multilingual semantically annotated resources: the MultiSemCor Corpus. Natural Language Engineering 11(3), 247–261. Busa, F. and M. Johnston (1996). Cross-li</context>
</contexts>
<marker>Lapata, Keller, 2005</marker>
<rawString>Lapata, M. and F. Keller (2005). Web-based models for natural language processing. ACM Transactions on Speech and Language Processing 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Laudanna</author>
<author>A Thornton</author>
<author>G Brown</author>
<author>C Burani</author>
<author>L Marconi</author>
</authors>
<title>Un corpus dell’italiano scritto contemporaneo dalla parte del ricevente. In</title>
<date>1995</date>
<booktitle>III Giornate internazionali di Analisi Statistica dei Dati Testuali. Volume I,</booktitle>
<pages>103--109</pages>
<contexts>
<context position="16257" citStr="Laudanna et al. (1995)" startWordPosition="2594" endWordPosition="2597">across each preposition. Table 5: Distribution of relations across prepositions in CORISnominals prep/rel CE IS LL OP RR total a 0 8 29 34 28 99 con 0 5 0 10 14 29 di 62 262 69 646 289 1328 da 2 0 7 18 8 35 in 0 5 50 31 14 100 per 8 2 2 29 7 48 su 3 0 18 12 11 44 tra 0 0 4 3 10 17 total 75 282 179 783 381 1700 The most striking figure is the overwhelming predominance of ”di”, which features in 78% of all CNs. This is in line with the extremely high overall frequency of ”di” in Italian, which is ranked as the most frequent word in CoLFIS (an Italian frequency lexicon based on a 3M word corpus, Laudanna et al. (1995)), and also with Girju’s 2007 observation that 77.7% of the English noun-noun compounds in her data can be rephrased as ”of” phrases. We can also observe that some prepositions, namely ”a” and ”con”, show more than one predominant relation usage in CNs. Overall, OP is by far the most frequent relation, occurring in nearly half of the CNs. As an additional observation, for each preposition we compared its frequency of occurrence in CNs and in any other constructions. We found that while ”di” and ”su” are particularly CN-oriented prepositions, both with over 55% of their occurrences being in CNs</context>
</contexts>
<marker>Laudanna, Thornton, Brown, Burani, Marconi, 1995</marker>
<rawString>Laudanna, A., A. Thornton, G. Brown, C. Burani, and L. Marconi (1995). Un corpus dell’italiano scritto contemporaneo dalla parte del ricevente. In S. Bolasco, L. Lebart, and A. Salem (Eds.), III Giornate internazionali di Analisi Statistica dei Dati Testuali. Volume I, pp. 103–109. Cisu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lauer</author>
</authors>
<title>Corpus statistics meet the noun compound: some empirical results.</title>
<date>1995</date>
<booktitle>In Proceedings of ACL’95.</booktitle>
<contexts>
<context position="4204" citStr="Lauer, 1995" startWordPosition="659" endWordPosition="660">ic relations for the annotation of Italian CNs and the details of the annotation framework, and discuss the corpus distribution. In Section 4 we describe the experiments for the automatic identification of semantic relations, and discuss the results. We conclude with ideas for future work in Section 5. &apos;In this work we will only consider N+N CNs, thereby excluding A+N CNs. 46 2 Previous work Given their underspecified nature, CNs, especially in English, have received a large amount of attention in the linguistic and computational linguistic literature (Downing, 1977; Levi, 1978; Warren, 1978; Lauer, 1995; Johnston and Busa, 1996; Rosario and Hearst, 2001; Lapata, 2002; Girju, 2007, among others). Current interest in NLP is also shown in the organisation of a SemEval task especially dedicated to noun-noun compound interpretation (Task 4, (Girju et al., 2007)). Indeed, NLP systems which aim at full text understanding for higher NLP tasks, such as question answering, recognising textual entailment and machine translation, need to grasp the semantic relation which noun compounds mostly leave underspecified. One main issue in noun-noun compound interpretation is the lack of general agreement on a </context>
<context position="7119" citStr="Lauer, 1995" startWordPosition="1085" endWordPosition="1086">that the automatic acquisition of qualias can be performed with reasonable success exploiting information obtained using lexico-syntactic patterns over the Web (Cimiano and Wenderoth, 2005). For our purposes, though, if lexico-syntactic patterns can be used successfully to induce qualia roles, we could directly use the information we obtain from them, thus bypassing the qualia structure representation. We plan to include features based on such kinds of patterns in future development of this work (see also (Nakov and Hearst, 2008)). More purely computational approaches include both supervised (Lauer, 1995) as well as unsupervised models, such as (Lapata and Keller, 2005), who use frequencies obtained over the Web. Some researchers also suggest solutions to the data sparsness problem, which affects our approach as well, by using lexical similarity (Turney, 2006) or clustering techniques (Pantel and Pennacchiotti, 2006). Finally, there exists specific work on compound nouns whose head is derived from a verb (Lapata, 2002), and information about verbs deverbal nouns are linked to has proved a useful feature in previous approaches (Girju, 2007). Whereas we have exploited this information in the ann</context>
</contexts>
<marker>Lauer, 1995</marker>
<rawString>Lauer, M. (1995). Corpus statistics meet the noun compound: some empirical results. In Proceedings of ACL’95.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Levi</author>
</authors>
<title>The Syntax and Semantics of Complex Nominals.</title>
<date>1978</date>
<publisher>Academic Press.</publisher>
<contexts>
<context position="1219" citStr="Levi, 1978" startWordPosition="180" endWordPosition="181">hich we use to create a corpus of 1700 Italian CNs, obtaining an inter-annotator agreement of K=.695. Exploiting this data, for each preposition p we train a classifier to assign one of the five semantic relations to any CN of the type N+p+N, by using both string and supersense features. To obtain supersenses, we experiment with a sequential tagger as well as a plain lookup in MultiWordNet, and find that using information obtained from the former yields better results. 1 Introduction Complex nominals are pervasive in language, and include noun-noun (N+N) and adjective-noun (A+N) combinations (Levi, 1978), as in Ex. 1 and 2. (1) dessert fork (2) medieval historian 45 Proceedings of the 8th International Conference on Computational Semantics, pages 45–60, Tilburg, January 2009. c�2009 International Conference on Computational Semantics A “dessert fork” is “a fork for eating dessert”, and a “medieval historian” can be also described as ”a historian who studies medieval times”.&apos; In both cases the relation is not overtly marked. Indeed, syntactically, there is nothing that tells us that the semantic relation between “dessert” and “fork” in Ex. 1 is different than the one binding “plastic” and “for</context>
<context position="4177" citStr="Levi, 1978" startWordPosition="655" endWordPosition="656">efine a set of five semantic relations for the annotation of Italian CNs and the details of the annotation framework, and discuss the corpus distribution. In Section 4 we describe the experiments for the automatic identification of semantic relations, and discuss the results. We conclude with ideas for future work in Section 5. &apos;In this work we will only consider N+N CNs, thereby excluding A+N CNs. 46 2 Previous work Given their underspecified nature, CNs, especially in English, have received a large amount of attention in the linguistic and computational linguistic literature (Downing, 1977; Levi, 1978; Warren, 1978; Lauer, 1995; Johnston and Busa, 1996; Rosario and Hearst, 2001; Lapata, 2002; Girju, 2007, among others). Current interest in NLP is also shown in the organisation of a SemEval task especially dedicated to noun-noun compound interpretation (Task 4, (Girju et al., 2007)). Indeed, NLP systems which aim at full text understanding for higher NLP tasks, such as question answering, recognising textual entailment and machine translation, need to grasp the semantic relation which noun compounds mostly leave underspecified. One main issue in noun-noun compound interpretation is the lack</context>
</contexts>
<marker>Levi, 1978</marker>
<rawString>Levi, J. (1978). The Syntax and Semantics of Complex Nominals. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Moldovan</author>
<author>A Badulescu</author>
</authors>
<title>A semantic scattering model for the automatic interpretation of genitives.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT-EMNLP</booktitle>
<pages>891--898</pages>
<contexts>
<context position="29942" citStr="Moldovan and Badulescu, 2005" startWordPosition="4845" endWordPosition="4848">classification tasks, one per relation. The highest reported average accuracy is 76.3% (Girju et al., 2007). Girju (2007) classifies noun-noun compounds in 22 different semantic relations. Best results on English are obtained when using a rich feature set including cross-linguistic information. Reported figures differ slightly according to the dataset used, with an average accuracy of 76.1%. When using only language-internal supersense features, the average accuracy is 44.15%. Girju (2007) also trains and tests another state-of-the-art supervised model for English, namely Semantic Scattering (Moldovan and Badulescu, 2005), reporting an average accuracy of 59.07%. 5 Conclusions and future work We have presented a framework for the annotation of Italian complex nominals in a very high data sparseness condition, and supervised models for the identification of the underlying semantic relation for monosyllabic Italian prepositions. We exploited both string and supersense features, showing that the importance of including string information varies from one preposition to another and from whether we are using backoff strategies or not. We have also seen that for obtaining the supersenses, a sequential sense tagging a</context>
</contexts>
<marker>Moldovan, Badulescu, 2005</marker>
<rawString>Moldovan, D. and A. Badulescu (2005). A semantic scattering model for the automatic interpretation of genitives. In Proceedings of HLT-EMNLP 2005, pp. 891–898.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Nakov</author>
<author>M A Hearst</author>
</authors>
<title>Solving relational similarity problems using the web as a corpus.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>452--460</pages>
<institution>Association for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="7042" citStr="Nakov and Hearst, 2008" startWordPosition="1074" endWordPosition="1077">tructed by hand, thereby involving a large amount of manual work. Recent work has shown that the automatic acquisition of qualias can be performed with reasonable success exploiting information obtained using lexico-syntactic patterns over the Web (Cimiano and Wenderoth, 2005). For our purposes, though, if lexico-syntactic patterns can be used successfully to induce qualia roles, we could directly use the information we obtain from them, thus bypassing the qualia structure representation. We plan to include features based on such kinds of patterns in future development of this work (see also (Nakov and Hearst, 2008)). More purely computational approaches include both supervised (Lauer, 1995) as well as unsupervised models, such as (Lapata and Keller, 2005), who use frequencies obtained over the Web. Some researchers also suggest solutions to the data sparsness problem, which affects our approach as well, by using lexical similarity (Turney, 2006) or clustering techniques (Pantel and Pennacchiotti, 2006). Finally, there exists specific work on compound nouns whose head is derived from a verb (Lapata, 2002), and information about verbs deverbal nouns are linked to has proved a useful feature in previous ap</context>
<context position="30847" citStr="Nakov and Hearst, 2008" startWordPosition="4981" endWordPosition="4984">yllabic Italian prepositions. We exploited both string and supersense features, showing that the importance of including string information varies from one preposition to another and from whether we are using backoff strategies or not. We have also seen that for obtaining the supersenses, a sequential sense tagging approach yields better overall results than a simple lookup in MWN, although it dramatically cuts on coverage. Future work will involve further classification experiments with additional features, including web counts obtained via lexico-syntactic patterns (Lapata and Keller, 2005; Nakov and Hearst, 2008). We will exploit part of the annotation which we have not considered in this study (see Section 3), namely the type of prepositional phrase (see Appendix), a very general conceptual clustering which also marks metaphorical usage, the position of trajector and landmark in the CN, and the order of the head and the modifier. References Bentivogli, L. and E. Pianta (2005). Exploiting parallel texts in the creation of multilingual semantically annotated resources: the MultiSemCor Corpus. Natural Language Engineering 11(3), 247–261. Busa, F. and M. Johnston (1996). Cross-linguistic semantics for co</context>
</contexts>
<marker>Nakov, Hearst, 2008</marker>
<rawString>Nakov, P. and M. A. Hearst (2008). Solving relational similarity problems using the web as a corpus. In Proceedings of ACL-08: HLT, Columbus, Ohio, pp. 452–460. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Nastase</author>
<author>S Szpakowicz</author>
</authors>
<title>Exploring noun-modifier semantic relations.</title>
<date>2003</date>
<booktitle>In Proceedings of IWCS-5,</booktitle>
<pages>285--301</pages>
<contexts>
<context position="4873" citStr="Nastase and Szpakowicz (2003)" startWordPosition="758" endWordPosition="761">earst, 2001; Lapata, 2002; Girju, 2007, among others). Current interest in NLP is also shown in the organisation of a SemEval task especially dedicated to noun-noun compound interpretation (Task 4, (Girju et al., 2007)). Indeed, NLP systems which aim at full text understanding for higher NLP tasks, such as question answering, recognising textual entailment and machine translation, need to grasp the semantic relation which noun compounds mostly leave underspecified. One main issue in noun-noun compound interpretation is the lack of general agreement on a well-defined set of semantic relations. Nastase and Szpakowicz (2003), for instance, propose a two-level taxonomy, in which fifteen fine-grained relations are subsumed into five general classes (causal, participant, spatial, temporal, quality). An example of a causal relation (with subtype “purpose”) is “concert hall”, and an example of a participant relation (with subtype “beneficiary”) is “student discount”. Girju et al. (2007) propose the smaller set reported in Table 1, which was tested on English N+N complex nominals within the SemEval 2007 task. They specifically spell out semantic relations as two-poles relationships: for example an effect is an effect a</context>
</contexts>
<marker>Nastase, Szpakowicz, 2003</marker>
<rawString>Nastase, V. and S. Szpakowicz (2003). Exploring noun-modifier semantic relations. In Proceedings of IWCS-5, pp. 285–301.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>M Pennacchiotti</author>
</authors>
<title>Espresso: Leveraging generic patterns for automatically harvesting semantic relations.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL’06,</booktitle>
<pages>113--120</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="7437" citStr="Pantel and Pennacchiotti, 2006" startWordPosition="1130" endWordPosition="1133">ould directly use the information we obtain from them, thus bypassing the qualia structure representation. We plan to include features based on such kinds of patterns in future development of this work (see also (Nakov and Hearst, 2008)). More purely computational approaches include both supervised (Lauer, 1995) as well as unsupervised models, such as (Lapata and Keller, 2005), who use frequencies obtained over the Web. Some researchers also suggest solutions to the data sparsness problem, which affects our approach as well, by using lexical similarity (Turney, 2006) or clustering techniques (Pantel and Pennacchiotti, 2006). Finally, there exists specific work on compound nouns whose head is derived from a verb (Lapata, 2002), and information about verbs deverbal nouns are linked to has proved a useful feature in previous approaches (Girju, 2007). Whereas we have exploited this information in the annotation phase, we have not included corresponding features yet in the statistical model we use, but we plan to do so in future extensions. 3 Annotation Framework and Data For developing an annotation framework, we built on Italian grammars, existing classifications (see Section 2), and a preliminary study of corpus d</context>
</contexts>
<marker>Pantel, Pennacchiotti, 2006</marker>
<rawString>Pantel, P. and M. Pennacchiotti (2006). Espresso: Leveraging generic patterns for automatically harvesting semantic relations. In Proceedings of ACL’06, Sydney, Australia, pp. 113–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Pianta</author>
<author>L Bentivogli</author>
<author>C Girardi</author>
</authors>
<title>MultiWordNet: developing an aligned multilingual database.</title>
<date>2002</date>
<booktitle>In Proceedings of the First International Conference on Global WordNet,</booktitle>
<pages>293--302</pages>
<contexts>
<context position="20894" citStr="Pianta et al., 2002" startWordPosition="3352" endWordPosition="3355">eviates such limitations by tagging words in context, thus tackling the ambiguity issue, and by using a combination of features rather than just the lexical entry, thereby being able to classify also words that are not included in the dictionary. Picca et al. (2008) have developed such a tagger for Italian, building on an existing version for English (Ciaramita and Altun, 2006), retrained on MultiSemCor (Bentivogli and Pianta, 2005), a word-aligned English-Italian corpus which contains the translation of the English texts in SemCor. The set of 26 noun supersense labels come from MultiWordNet (Pianta et al., 2002), a multilingual lexical database in which the Italian WordNet is strictly aligned with Princeton WordNet 1.6, and which is linked to MultiSemCor. The average reported performance of the tagger is about 60% (Picca et al., 2008). This relatively low accuracy introduces a large portion of errors in the classification, thus reducing the advantage of dealing with supersenses rather than words in the identification of semantic relations in CNs. Errors can be of three types: (i) the assignment of a wrong noun class, (ii) the assignment of a class of the wrong part-of-speech type (any non-noun tag), </context>
</contexts>
<marker>Pianta, Bentivogli, Girardi, 2002</marker>
<rawString>Pianta, E., L. Bentivogli, and C. Girardi (2002). MultiWordNet: developing an aligned multilingual database. In Proceedings of the First International Conference on Global WordNet, pp. 293–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Picca</author>
<author>A M Gliozzo</author>
<author>M Ciaramita</author>
</authors>
<title>Supersense Tagger for Italian.</title>
<date>2008</date>
<booktitle>In Proceedings of LREC</booktitle>
<contexts>
<context position="20540" citStr="Picca et al. (2008)" startWordPosition="3299" endWordPosition="3302">g plain assignment of hypernyms. 4.2 Hypernym selection via sense tagging Two major problems related to finding a hypernym for a word are sense ambiguity (one term can easily have more than one hypernym if it has more than one sense) and coverage (even large ontologies/databases might not include some of the encountered terms). A supersense tagger alleviates such limitations by tagging words in context, thus tackling the ambiguity issue, and by using a combination of features rather than just the lexical entry, thereby being able to classify also words that are not included in the dictionary. Picca et al. (2008) have developed such a tagger for Italian, building on an existing version for English (Ciaramita and Altun, 2006), retrained on MultiSemCor (Bentivogli and Pianta, 2005), a word-aligned English-Italian corpus which contains the translation of the English texts in SemCor. The set of 26 noun supersense labels come from MultiWordNet (Pianta et al., 2002), a multilingual lexical database in which the Italian WordNet is strictly aligned with Princeton WordNet 1.6, and which is linked to MultiSemCor. The average reported performance of the tagger is about 60% (Picca et al., 2008). This relatively l</context>
<context position="28098" citStr="Picca et al., 2008" startWordPosition="4558" endWordPosition="4561"> incurring in the noise introduced by wrong hypernym assignments or the backoff strategy, we manually corrected the tagger’s output in 60% of the data. This allowed us to evaluate the tagger’s performance on supersense assignment for this 60% portion as well as to compare on this subset, contaning 1024 CNs, an algorithm using ”gold” supersenses with that built on the tagger’s output (using the backoff strategy, and including string features, see Section 4.2). We found that supersenses were assigned by the tagger with an accuracy of 63.9%, a result in line with previously reported performance (Picca et al., 2008). We also observed that using the manually assigned hypernyms yielded an average improvement of about seven percentage points over using the tagger’s senses, although for some prepositions, instances in this smaller dataset were just too few to draw any solid conclusion. Although more accurate, the gold tags do not boost the performance as much as one might expect. On the one hand, this might suggest that hypernyms can contribute only to a certain extent to this task, and other more expressive features must be found. On the other, it is also possible that the chosen set of 26 supersenses is to</context>
</contexts>
<marker>Picca, Gliozzo, Ciaramita, 2008</marker>
<rawString>Picca, D., A. M. Gliozzo, and M. Ciaramita (2008). Supersense Tagger for Italian. In Proceedings of LREC 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
</authors>
<title>The Generative Lexicon.</title>
<date>1995</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="6133" citStr="Pustejovsky, 1995" startWordPosition="935" endWordPosition="936">e set of 7 semantic relations from Girju et al. (2007) Semantic relation Examples Cause-Effect laugh (cause) wrinkles (effect) Instrument-Agency laser (instrument) printer (agency) Product-Producer honey (product) bee (producer) Origin-Entity message (entity) from outer-space (origin) Theme-Tool news (theme) conference(tool) Part-Whole the door (part) of the car (whole) Content-Container apples (content) in the basket (container) As far as relation detection is concerned, Johnston and Busa (1996), working specifically on Italian, have suggested using information included in qualia structures (Pustejovsky, 1995) for deriving the compound’s interpretation. The use of qualia structures for this task is appropriate and semantically sound but absolutely not straightforward to implement, since there does not exist an electronic repository of qualias, so that the structures 47 would need to be constructed by hand, thereby involving a large amount of manual work. Recent work has shown that the automatic acquisition of qualias can be performed with reasonable success exploiting information obtained using lexico-syntactic patterns over the Web (Cimiano and Wenderoth, 2005). For our purposes, though, if lexico</context>
</contexts>
<marker>Pustejovsky, 1995</marker>
<rawString>Pustejovsky, J. (1995). The Generative Lexicon. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Rosario</author>
<author>M Hearst</author>
</authors>
<title>Classifying the semantic relations in noun compounds via a domain-specific lexical hierarchy. In</title>
<date>2001</date>
<booktitle>Proceedings of EMNLP</booktitle>
<pages>82--90</pages>
<contexts>
<context position="4255" citStr="Rosario and Hearst, 2001" startWordPosition="666" endWordPosition="669">lian CNs and the details of the annotation framework, and discuss the corpus distribution. In Section 4 we describe the experiments for the automatic identification of semantic relations, and discuss the results. We conclude with ideas for future work in Section 5. &apos;In this work we will only consider N+N CNs, thereby excluding A+N CNs. 46 2 Previous work Given their underspecified nature, CNs, especially in English, have received a large amount of attention in the linguistic and computational linguistic literature (Downing, 1977; Levi, 1978; Warren, 1978; Lauer, 1995; Johnston and Busa, 1996; Rosario and Hearst, 2001; Lapata, 2002; Girju, 2007, among others). Current interest in NLP is also shown in the organisation of a SemEval task especially dedicated to noun-noun compound interpretation (Task 4, (Girju et al., 2007)). Indeed, NLP systems which aim at full text understanding for higher NLP tasks, such as question answering, recognising textual entailment and machine translation, need to grasp the semantic relation which noun compounds mostly leave underspecified. One main issue in noun-noun compound interpretation is the lack of general agreement on a well-defined set of semantic relations. Nastase and</context>
</contexts>
<marker>Rosario, Hearst, 2001</marker>
<rawString>Rosario, B. and M. Hearst (2001). Classifying the semantic relations in noun compounds via a domain-specific lexical hierarchy. In L. Lee and D. Harman (Eds.), Proceedings of EMNLP 2001, pp. 82–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rossini Favretti</author>
<author>R</author>
</authors>
<title>Progettazione e costruzione di un corpus di italiano scritto: CORIS/CODIS.</title>
<date>2000</date>
<booktitle>In R. Rossini Favretti (Ed.), Linguistica e informatica. Multimedialit`a, corpora e percorsi di apprendimento. Bulzoni.</booktitle>
<marker>Favretti, R, 2000</marker>
<rawString>Rossini Favretti, R. (2000). Progettazione e costruzione di un corpus di italiano scritto: CORIS/CODIS. In R. Rossini Favretti (Ed.), Linguistica e informatica. Multimedialit`a, corpora e percorsi di apprendimento. Bulzoni.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proc. of the Conference on New Methods in Language Processing,</booktitle>
<pages>44--49</pages>
<contexts>
<context position="12473" citStr="Schmid, 1994" startWordPosition="1922" endWordPosition="1923"> di massima” (en: generally agreed with) 3.2 Data Corpus Selection We used CORISsmall, a reduced version of CORIS, a 100M-word, balanced corpus of written Italian (Rossini Favretti, 2000). CORISsmall was sampled by randomly extracting sentences with a length between 2 and 40 words. We discarded a few domain-specific subcorpora which were likely to contain prepositions used in ways different from common usage, as the legal subcorpus. The resulting corpus, henceforth CORISnominals, contains 75,000 words. The corpus was then automatically tagged with part-of-speech information, using TreeTagger (Schmid, 1994). CN detection We chose to annotate monosyllabic prepositions only, namely a (to), con (with), di (of), da (from), in (in),per (for), su (on) and tra (within), because they are more frequent in CNs, more polysemous and not occurring as any other grammatical category, differently from bisyllabic prepositions which can be used adverbially. In any case, bisyllabic prepositions occurr in less than 2% of all the extracted CNs (42 out of 2298). Exploiting part-of-speech information, we extracted all the N+P+N combinations with a context window of 10 words left and right. The frequency of the CNs fou</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Schmid, H. (1994). Probabilistic part-of-speech tagging using decision trees. In Proc. of the Conference on New Methods in Language Processing, 44-49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
</authors>
<title>Expressing implicit semantic relations without supervision.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL’06,</booktitle>
<pages>313--320</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="7379" citStr="Turney, 2006" startWordPosition="1125" endWordPosition="1126">uccessfully to induce qualia roles, we could directly use the information we obtain from them, thus bypassing the qualia structure representation. We plan to include features based on such kinds of patterns in future development of this work (see also (Nakov and Hearst, 2008)). More purely computational approaches include both supervised (Lauer, 1995) as well as unsupervised models, such as (Lapata and Keller, 2005), who use frequencies obtained over the Web. Some researchers also suggest solutions to the data sparsness problem, which affects our approach as well, by using lexical similarity (Turney, 2006) or clustering techniques (Pantel and Pennacchiotti, 2006). Finally, there exists specific work on compound nouns whose head is derived from a verb (Lapata, 2002), and information about verbs deverbal nouns are linked to has proved a useful feature in previous approaches (Girju, 2007). Whereas we have exploited this information in the annotation phase, we have not included corresponding features yet in the statistical model we use, but we plan to do so in future extensions. 3 Annotation Framework and Data For developing an annotation framework, we built on Italian grammars, existing classifica</context>
</contexts>
<marker>Turney, 2006</marker>
<rawString>Turney, P. D. (2006). Expressing implicit semantic relations without supervision. In Proceedings of ACL’06, Sydney, Australia, pp. 313–320.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Warren</author>
</authors>
<title>Semantic patterns of noun-noun compounds.</title>
<date>1978</date>
<journal>Gothenburg Studies in English</journal>
<volume>41</volume>
<contexts>
<context position="4191" citStr="Warren, 1978" startWordPosition="657" endWordPosition="658">of five semantic relations for the annotation of Italian CNs and the details of the annotation framework, and discuss the corpus distribution. In Section 4 we describe the experiments for the automatic identification of semantic relations, and discuss the results. We conclude with ideas for future work in Section 5. &apos;In this work we will only consider N+N CNs, thereby excluding A+N CNs. 46 2 Previous work Given their underspecified nature, CNs, especially in English, have received a large amount of attention in the linguistic and computational linguistic literature (Downing, 1977; Levi, 1978; Warren, 1978; Lauer, 1995; Johnston and Busa, 1996; Rosario and Hearst, 2001; Lapata, 2002; Girju, 2007, among others). Current interest in NLP is also shown in the organisation of a SemEval task especially dedicated to noun-noun compound interpretation (Task 4, (Girju et al., 2007)). Indeed, NLP systems which aim at full text understanding for higher NLP tasks, such as question answering, recognising textual entailment and machine translation, need to grasp the semantic relation which noun compounds mostly leave underspecified. One main issue in noun-noun compound interpretation is the lack of general ag</context>
</contexts>
<marker>Warren, 1978</marker>
<rawString>Warren, B. (1978). Semantic patterns of noun-noun compounds. Gothenburg Studies in English 41.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>