<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000780">
<title confidence="0.990004">
Translation Assistance by Translation of L1 Fragments in an L2 Context
</title>
<author confidence="0.927583">
Maarten van Gompel &amp; Antal van den Bosch
</author>
<affiliation confidence="0.972007">
Centre for Language Studies
Radboud University Nijmegen
</affiliation>
<email confidence="0.98306">
proycon@anaproy.nl
</email>
<sectionHeader confidence="0.993614" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99951288">
In this paper we present new research in
translation assistance. We describe a sys-
tem capable of translating native language
(L1) fragments to foreign language (L2)
fragments in an L2 context. Practical ap-
plications of this research can be framed in
the context of second language learning.
The type of translation assistance system
under investigation here encourages lan-
guage learners to write in their target lan-
guage while allowing them to fall back to
their native language in case the correct
word or expression is not known. These
code switches are subsequently translated
to L2 given the L2 context. We study
the feasibility of exploiting cross-lingual
context to obtain high-quality translation
suggestions that improve over statistical
language modelling and word-sense dis-
ambiguation baselines. A classification-
based approach is presented that is in-
deed found to improve significantly over
these baselines by making use of a contex-
tual window spanning a small number of
neighbouring words.
</bodyText>
<sectionHeader confidence="0.999131" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999791538461538">
Whereas machine translation generally concerns
the translation of whole sentences or texts from
one language to the other, this study focusses on
the translation of native language (henceforth L1)
words and phrases, i.e. smaller fragments, in a
foreign language (L2) context. Despite the ma-
jor efforts and improvements, automatic transla-
tion does not yet rival human-level quality. Vex-
ing issues are morphology, word-order change and
long-distance dependencies. Although there is a
morpho-syntactic component in this research, our
scope is more constrained; its focus is on the faith-
ful preservation of meaning from L1 to L2, akin to
the role of the translation model in Statistical Ma-
chine Translation (SMT).
The cross-lingual context in our research ques-
tion may at first seem artificial, but its design ex-
plicitly aims at applications related to computer-
aided language learning (Laghos and Panayiotis,
2005; Levy, 1997) and computer-aided transla-
tion (Barrachina et al., 2009). Currently, lan-
guage learners need to refer to a bilingual dictio-
nary when in doubt about a translation of a word or
phrase. Yet, this problem arises in a context, not
in isolation; the learner may have already trans-
lated successfully a part of the text into L2 leading
up to the problematic word or phrase. Dictionar-
ies are not the best source to look up context; they
may contain example usages, but remain biased to-
wards single words or short expressions.
The proposed application allows code switch-
ing and produces context-sensitive suggestions as
writing progresses. In this research we test the
feasibility of the foundation of this idea.The fol-
lowing examples serve to illustrate the idea and
demonstrate what output the proposed translation
assistance system would ideally produce. The
parts in bold correspond to respectively the in-
serted fragment and the system translation.
</bodyText>
<listItem confidence="0.986385538461538">
• Input (L1=English,L2=Spanish): “Hoy va-
mos a the swimming pool.”
Desired output: “Hoy vamos a la piscina.”
• Input (L1-English, L2=German): “Das wet-
ter ist wirklich abominable.”
Desired output: “Das wetter ist wirklich
ekelhaft.”
• Input (L1=French,L2=English): “I rentre a`
la maison because I am tired.”
Desired output: “I return home because I am
tired.”
• Input (L1=Dutch, L2=English): “Workers
are facing a massive aanval op their employ-
</listItem>
<page confidence="0.975535">
871
</page>
<note confidence="0.8433385">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 871–880,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.928893727272727">
ment and social rights.”
Desired output: “Workers are facing a mas-
sive attack on their employment and social
rights.”
The main research question in this research is
how to disambiguate an L1 word or phrase to
its L2 translation based on an L2 context, and
whether such cross-lingual contextual approaches
provide added value compared to baseline models
that are not context informed or compared to stan-
dard language models.
</bodyText>
<sectionHeader confidence="0.967116" genericHeader="method">
2 Data preparation
</sectionHeader>
<bodyText confidence="0.999760289473684">
Preparing the data to build training and test data
for our intended translation assistance system is
not trivial, as the type of interactive translation as-
sistant we aim to develop does not exist yet. We
need to generate training and test data that real-
istically emulates the task. We start with a par-
allel corpus that is tokenised for both L1 and L2.
No further linguistic processing such as part-of-
speech tagging or lemmatisation takes place in our
experiments; adding this remains open for future
research.
The parallel corpus is randomly sampled into
two large and equally-sized parts. One is the basis
for the training set, and the other is the basis for
the test set. The reason for such a large test split
shall become apparent soon.
From each of the splits (5), a phrase-translation
table is constructed automatically in an unsuper-
vised fashion. This is done using the scripts
provided by the Statistical Machine Translation
system Moses (Koehn et al., 2007). It invokes
GIZA++ (Och and Ney, 2000) to establish sta-
tistical word alignments based on the IBM Mod-
els and subsequently extracts phrases using the
grow-diag-final algorithm (Och and Ney,
2003). The result, independent for each set, will
be a phrase-translation table (T) that maps phrases
in L1 to L2. For each phrase-pair (fs, ft) this
phrase-translation table holds the computed trans-
lation probabilities P(fs|ft) and P (ft|fs).
Given these phrase-translation tables, we can
now extract both training data and test data using
the algorithm in Figure 1. In our discourse, the
source language (s) corresponds to L1, the fall-
back language used for by the end-user for insert-
ing fragments, whilst the target language (t) is L2.
Step 4 is effectively a filter: two thresholds
can be configured to discard weak alignments,
</bodyText>
<listItem confidence="0.999076">
1. using phrase-translation table T and par-
allel corpus split 5
2. for each aligned sentence pair
(sentences ∈ 5s, sentencet ∈ 5t)
in the parallel corpus split (5s,5t):
3. for each fragment (fs ∈
sentences, ft ∈ sentencet) where
(fs, ft) ∈ T:
4. if P(fs|ft) · P(ft|fs) ≥ A1
and P(fs|ft) · P (ft|fs) ≥ A2 ·
P(fs|fstrongest t) · P(fstrongest t|fs):
5. Output a pair
</listItem>
<bodyText confidence="0.9497218125">
(sentencet, sentencet) where
sentencet is a copy of t but with
fragment ft substituted by fs, i.e. the
introduction of an L1 word or phrase in
an L2 sentence.
Figure 1: Algorithm for extracting training and
test data on the basis of a phrase-translation ta-
ble (T) and subset/split from a parallel corpus (5).
The indentation indicates the nesting.
i.e. those with low probabilities, from the phrase-
translation table so that only strong couplings
make it into the generated set. The parameter
A1 adds a constraint based on the product of the
two conditional probabilities (P (ft|fs)·P(fs|ft)),
and sets a threshold that has to be surpassed.
A second parameter A2 further limits the con-
sidered phrase pairs (fs, ft) to have the prod-
uct of their conditional probabilities not not devi-
ate more than a fraction A2 from the joint prob-
ability for the strongest possible pairing for fs,
the source fragment. fstrongest t in Figure 1
corresponds to the best scoring translation for a
given source fragment fs. This metric thus effec-
tively prunes weaker alternative translations in the
phrase-translation table from being considered if
there is a much stronger candidate. Nevertheless,
it has to be noted that even with A1 and A2, the test
set will include a certain amount of errors. This is
due to the nature of the unsupervised method with
which the phrase-translation table is constructed.
For our purposes however, the test set suffices to
test our hypothesis.
</bodyText>
<page confidence="0.99604">
872
</page>
<bodyText confidence="0.999946611111111">
In our experiments, we choose fixed values for
these parameters, by manual inspection and judge-
ment of the output. The A1 parameter was set to
0.01 and A2 to 0.8. Whilst other thresholds may
possibly produce cleaner sets, this is hard to eval-
uate as finding optimal values causes a prohibitive
increase in complexity of the search space, and
again this is not necessary to test our hypothesis.
The output of the algorithm in Fig-
ure 1 is a modified set of sentence pairs
(sentence&apos;, sentencet), in which the same
sentence pair may be used multiple times with
different L1 substitutions for different fragments.
The final test set is created by randomly sampling
the desired number of test instances.
Note that the training set and test set are con-
structed on their own respective and indepen-
dently generated phrase-translation tables. This
ensures complete independence of training and
test data. Generating test data using the same
phrase-translation table as the training data would
introduce a bias. The fact that a phrase-translation
table needs to be constructed for the test data is
also the reason that the parallel corpus split from
which the test data is derived has to be large
enough, ensuring better quality.
We concede that our current way of testing is
a mere approximation of the real-world scenario.
An ideal test corpus would consist of L2 sentences
with L1 fallback as crafted by L2 language learn-
ers with an L1 background. However, such cor-
pora do not exist as yet. Nevertheless, we hope to
show that our automated way of test set genera-
tion is sufficient to test the feasibility of our core
hypothesis that L1 fragments can be translated to
L2 using L2 context information.
</bodyText>
<sectionHeader confidence="0.971132" genericHeader="method">
3 System
</sectionHeader>
<bodyText confidence="0.999972666666667">
We develop a classifier-based system composed of
so-called “classifier experts”. Numerous classi-
fiers are trained and each is an expert in translating
a single word or phrase. In other words, for each
word type or phrase type that occurs as a fragment
in the training set, and which does not map to just a
single translation, a classifier is trained. The clas-
sifier maps the L1 word or phrase in its L2 context
to its L2 translation. Words or phrases that always
map to a single translation are stored in a sim-
ple mapping table, as a classifier would have no
added value in such cases. The classifiers use the
IB1 algorithm (Aha et al., 1991) as implemented
in TiMBL (Daelemans et al., 2009).1 IB1 im-
plements k-nearest neighbour classification. The
choice for this algorithm is motivated by the fact
that it handles multiple classes with ease, but first
and foremost because it has been successfully em-
ployed for word sense disambiguation in other
studies (Hoste et al., 2002; Decadt et al., 2004),
in particular in cross-lingual word sense disam-
biguation, a task closely resembling our current
task (van Gompel and van den Bosch, 2013). It
has also been used in machine translation stud-
ies in which local source context is used to clas-
sify source phrases into target phrases, rather than
looking them up in a phrase table (Stroppa et al.,
2007; Haque et al., 2011). The idea of local phrase
selection with a discriminative machine learning
classifier using additional local (source-language)
context was introduced in parallel to Stroppa et al.
(2007) by Carpuat and Wu (2007) and Gim´enez
and M´arquez (2007); cf. Haque et al. (2011) for
an overview of more recent methods.
The feature vector for the classifiers represents
a local context of neighbouring words, and op-
tionally also global context keywords in a binary-
valued bag-of-words configuration. The local con-
text consists of an X number of L2 words to the
left of the L1 fragment, and Y words to the right.
When presented with test data, in which the
L1 fragment is explicitly marked, we first check
whether there is ambiguity for this L1 fragment
and if a direct translation is available in our sim-
ple mapping table. If so, we are done quickly and
need not rely on context information. If not, we
check for the presence of a classifier expert for the
offered L1 fragment; only then we can proceed by
extracting the desired number of L2 local context
words to the immediate left and right of this frag-
ment and adding those to the feature vector. The
classifier will return a probability distribution of
the most likely translations given the context and
we can replace the L1 fragment with the highest
scoring L2 translation and present it back to the
user.
In addition to local context features, we also ex-
perimented with global context features. These
are a set of L2 contextual keywords for each L1
word/phrase and its L2 translation occurring in the
same sentence, not necessarily in the immediate
neighbourhood of the L1 word/phrase. The key-
words are selected to be indicative for a specific
</bodyText>
<footnote confidence="0.981229">
1http://ilk.uvt.nl/timbl
</footnote>
<page confidence="0.998417">
873
</page>
<bodyText confidence="0.9999195">
translation. We used the method of extraction by
Ng and Lee (1996) and encoded all keywords in
a binary bag of words model. The experiments
however showed that inclusion of such keywords
did not make any noticeable impact on any of the
results, so we restrict ourselves to mentioning this
negative result.
Our full system, including the scripts for
data preparation, training, and evaluation, is
implemented in Python and freely available
as open-source from http://github.com/
proycon/colibrita/ . Version tag v0.2.1
is representative for the version used in this re-
search.
</bodyText>
<subsectionHeader confidence="0.983038">
3.1 Language Model
</subsectionHeader>
<bodyText confidence="0.999985619047619">
We also implement a statistical language model as
an optional component of our classifier-based sys-
tem and also as a baseline to compare our system
to. The language model is a trigram-based back-
off language model with Kneser-Ney smooth-
ing, computed using SRILM (Stolcke, 2002) and
trained on the same training data as the translation
model. No additional external data was brought
in, to keep the comparison fair.
For any given hypothesis H, results from the L1
to L2 classifier are combined with results from the
L2 language model. We do so by normalising the
class probability from the classifier (scoreT(H)),
which is our translation model, and the language
model (scorelm(H)), in such a way that the high-
est classifier score for the alternatives under con-
sideration is always 1.0, and the highest language
model score of the sentence is always 1.0. Take
scoreT(H) and scorelm(H) to be log probabili-
ties, the search for the best (most probable) trans-
lation hypothesis Hˆ can then be expressed as:
</bodyText>
<equation confidence="0.99859">
Hˆ = arg max (scoreT(H) + scorelm(H)) (1)
H
</equation>
<bodyText confidence="0.999982714285714">
If desired, the search can be parametrised with
variables A3 and A4, representing the weights we
want to attach to the classifier-based translation
model and the language model, respectively. In
the current study we simply left both weights set to
one, thereby assigning equal importance to trans-
lation model and language model.
</bodyText>
<sectionHeader confidence="0.998413" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999379638888889">
Several automated metrics exist for the evaluation
of L2 system output against the L2 reference out-
put in the test set. We first measure absolute accu-
racy by simply counting all output fragments that
exactly match the reference fragments, as a frac-
tion of the total amount of fragments. This mea-
sure may be too strict, so we add a more flexible
word accuracy measure which takes into account
partial matches at the word level. If output o is
a subset of reference r then a score of �o�
�r� is as-
signed for that sentence pair. If instead, r is a sub-
set of o, then a score of �r�
�o� will be assigned. A
perfect match will result in a score of 1 whereas
a complete lack of overlap will be scored 0. The
word accuracy for the entire set is then computed
by taking the sum of the word accuracies per sen-
tence pair, divided by the total number of sentence
pairs.
We also compute a recall metric that measures
the number of fragments that the system provided
a translation for as a fraction of the total number
of fragments in the input, regardless of whether
the fragment is translated correctly or not. The
system may skip fragments for which it can find
no solution at all.
In addition to these, the system’s output can be
compared against the L2 reference translation(s)
using established Machine Translation evaluation
metrics. We report on BLEU, NIST, METEOR,
and word error rate metrics WER and PER. These
scores should generally be much better than the
typical MT system performances as only local
changes are made to otherwise “perfect” L2 sen-
tences.
</bodyText>
<sectionHeader confidence="0.994544" genericHeader="method">
5 Baselines
</sectionHeader>
<bodyText confidence="0.999986705882353">
A context-insensitive yet informed baseline was
constructed to assess the impact of L2 context in-
formation in translating L1 fragments. The base-
line selects the most probable L1 fragment per L2
fragment according to the phrase-translation ta-
ble. This baseline, henceforth referred to as the
’most likely fragment’ baseline (MLF) is analo-
gous to the ’most frequent sense’-baseline com-
mon in evaluating WSD systems.
A second baseline was constructed by weigh-
ing the probabilities from the translation table di-
rectly with the L2 language model described ear-
lier. It adds a LM component to the MLF base-
line. This LM baseline allows the comparison of
classification through L1 fragments in an L2 con-
text, with a more traditional L2 context modelling
(i.e. target language modelling) which is also cus-
</bodyText>
<page confidence="0.992453">
874
</page>
<bodyText confidence="0.999428833333333">
tomary in MT decoders. Computing this base-
line is done in the same fashion as previously il-
lustrated in Equation 1, where scoreT then repre-
sents the normalised p(tIs) score from the phrase-
translation table rather than the class probability
from the classifier.
</bodyText>
<sectionHeader confidence="0.997413" genericHeader="evaluation">
6 Experiments &amp; Results
</sectionHeader>
<bodyText confidence="0.99988975">
The data for our experiments were drawn from
the Europarl parallel corpus (Koehn, 2005) from
which we extracted two sets of 200, 000 sentence
pairs each for several language pairs. These were
used to form the training and test sets. The final
test sets are a randomly sampled 5, 000 sentence
pairs from the 200, 000-sentence test split for each
language pair.
All input data for the experiments in this section
are publicly available2.
Let us first zoom in to convey a sense of scale
on a specific language pair. The actual Europarl
training set we generate for English (L1) to Span-
ish (L2), i.e. English fallback in a Spanish con-
text, consists of 5,608,015 sentence pairs. This
number is much larger than the 200, 000 we men-
tioned before because single sentence pairs may be
reused multiple times with different marked frag-
ments. From this training set of sentence pairs
over 100, 000 classifier experts are derived. The
eleven largest classifiers are shown in Table 1,
along with the number of training instances per
classifier. The full table would reveal a Zipfian
distribution.
</bodyText>
<table confidence="0.9993835">
Fragment Training instances
the 256,772
of 139,273
and 128,074
to 66,565
a 54,306
is 40,511
for 34,054
this 29,691
European 26,543
on 23,147
of the 22,361
</table>
<tableCaption confidence="0.99891">
Table 1: The top eleven classifier experts for En-
</tableCaption>
<bodyText confidence="0.96257175">
glish to Spanish. The eleventh entry is included as
an example of a common phrasal fragment
Among the classifier experts are only words and
phrases that are ambiguous and may thus map to
</bodyText>
<footnote confidence="0.994827">
2Download and unpack http://lst.science.ru.
nl/˜proycon/colibrita-acl2014-data.zip
</footnote>
<figureCaption confidence="0.9560485">
Figure 2: Accuracy for different local context
sizes, Europarl English to Spanish
</figureCaption>
<bodyText confidence="0.836587304347826">
multiple translations. This implies that such words
and phrases must have occurred at least twice in
the corpus, though this threshold is made config-
urable and could have been set higher to limit the
number of classifiers. The remaining 246, 380 un-
ambiguous mappings are stored in a separate map-
ping table.
For the classifier-based system, we tested var-
ious different feature vector configurations. The
first experiment, of which the results are shown in
Figure 2, sets a fixed and symmetric local context
size across all classifiers, and tests three context
widths. Here we observe that a context width of
one yields the best results. The BLEU scores, not
included in the figure but shown in Table 2, show
a similar trend. This trend holds for all the MT
metrics.
Table 2 shows the results for English to Span-
ish in more detail and adds a comparison with the
two baseline systems. The various lXrY config-
urations use the same feature vector setup for all
classifier experts. Here X indicates the left context
size and Y the right context size. The auto con-
figuration does not uniformly apply the same fea-
ture vector setup to all classifier experts but instead
seeks to find the optimal setup per classifier expert.
This shall be further discussed in Section 6.1.
As expected, the LM baseline substantially out-
performs the context-insensitive MLF baseline.
Second, our classifier approach attains a sub-
stantially higher accuracy than the LM baseline.
Third, we observe that adding the language model
to our classifier leads to another significant gain
Translations
la, el, los, las
de, del
y, de, e
a, para, que, de
un, una
es, est´a, se
para, de, por
este, esta, esto
Europea, Europeo
Europeas, Europeos
sobre, en
de la, de los
</bodyText>
<page confidence="0.722583">
875
</page>
<table confidence="0.9999365">
Configuration Accuracy Word Accuracy BLEU METEOR NIST WER PER
MLF baseline 0.6164 0.6662 0.972 0.9705 17.0784 1.4465 1.4209
LM baseline 0.7158 0.7434 0.9785 0.9739 17.1573 1.1735 1.1574
l1r1 0.7588 0.7824 0.9801 0.9747 17.1550 1.1625 1.1444
l2r2 0.7574 0.7801 0.9800 0.9746 17.1550 1.1750 1.1569
l3r3 0.7514 0.7742 0.9796 0.9744 17.1445 1.1946 1.1780
l1r1+LM 0.7810 0.7973 0.9816 0.9754 17.1685 1.0946 1.077
auto 0.7626 0.7850 0.9803 0.9748 17.1544 1.1594 1.1424
auto+LM 0.7796 0.7966 0.9815 0.9754 17.1664 1.1021 1.0845
l1r0 0.6924 0.7223 0.9757 0.9723 17.1087 1.3415 1.3249
l2r0 0.6960 0.7245 0.9759 0.9724 17.1091 1.3364 1.3193
l2r1 0.7624 0.7849 0.9803 0.9748 17.1558 1.1554 1.1378
</table>
<tableCaption confidence="0.9033405">
Table 2: Europarl results for English to Spanish (i.e English fallback in Spanish context). Recall =
0.9422
</tableCaption>
<bodyText confidence="0.99941814893617">
(configuration l1r1+LM in the results in Ta-
ble 2). It appears that the classifier approach and
the L2 language model are able to complement
each other.
Statistical significance on the BLEU scores was
tested using pairwise bootstrap sampling (Koehn,
2004). All significance tests were performed
with 5, 000 iterations. We compared the out-
comes of several key configurations. We first
tested l1r1 against both baselines; both differ-
ences are significant at p &lt; 0.01 for both. The
same significance level was found when compar-
ing l1r1+LM against l1r1, auto+LM against
auto, as well as the LM baseline against the MLF
baseline. Automatic feature selection auto was
found to perform statistically better than l1r1,
but only at p &lt; 0.05. Conclusions with regard to
context width may have to be tempered somewhat,
as the performance of the l1r1 configuration was
found to not be significantly better than that of the
l2r2 configuration. However, l1r1 performs
significantly better than l3r3 at p &lt; 0.01, and
l2r2 performs significantly better than l3r3 at
p &lt; 0.01.
In Table 3 we present some illustrative exam-
ples from the English→Spanish Europarl data.
We show the difference between the most-likely-
fragment baseline and our system.
Likewise, Table 4 exemplifies small fragments
from the l1r1 configuration compared to the
same configuration enriched with a language
model. We observe in this data that the language
model often has the added power to choose a cor-
rect translation that is not the first prediction of
the classifier, but one of the weaker alternatives
that nevertheless fits better. Though the classifier
generally works best in the l1r1 configuration,
i.e. with context size one, the trigram-based lan-
guage model allows further left-context informa-
tion to be incorporated that influences the weights
of the classifier output, successfully forcing the
system to select alternatives. This combination
of a classifier with context size one and trigram-
based language model proves to be most effective
and reaches the best results so far. We have not
conducted experiments with language models of
other orders.
</bodyText>
<subsectionHeader confidence="0.998761">
6.1 Context optimisation
</subsectionHeader>
<bodyText confidence="0.999948">
It has been argued that classifier experts in a word
sense disambiguation ensemble should be individ-
ually optimised (Decadt et al., 2004; van Gompel
and van den Bosch, 2013). The latter study on
cross-lingual WSD finds a positive impact when
conducting feature selection per classifier. This in-
tuitively makes sense; a context of one may seem
to be better than any other when uniformly applied
to all classifier experts, but it may well be that cer-
tain classifiers benefit from different feature selec-
tions. We therefore proceed with this line of inves-
tigation as well.
Automatic configuration selection was done by
performing leave-one-out testing (for small num-
ber of instances) or 10-fold-cross validation (for
larger number of instances, n ≥ 20) on the train-
ing data per classifier expert. Various configura-
tions were tested. Per classifier expert, the best
scoring configuration was selected, referred to as
the auto configuration in Table 2. The auto
configuration improves results over the uniformly
</bodyText>
<page confidence="0.996738">
876
</page>
<bodyText confidence="0.756715222222222">
Input: Mientras no haya prueba en contrario , la financiaci´on de partidos politicos European s´olo se justifica , incluso
despu´es del tratado de Niza , desde el momento en que concurra a la expresi´on del sufragio universal , que es la ´unica
definici´on aceptable de un partido pol´ıtico .
MLF baseline: Mientras no haya prueba en contrario , la financiaci´on de partidos pol´ıticos Europea s´olo se justifica ,
incluso despu´es del tratado de Niza , desde el momento en que concurra a la expresi´on del sufragio universal , que es la
´unica definici´on aceptable de un partido politico .
l1r1: Mientras no haya prueba en contrario , la financiaci´on de partidos pol´ıticos europeos s´olo se justifica , incluso
despu´es del tratado de Niza , desde el momento en que concurra a la expresi´on del sufragio universal , que es la ´unica
definici´on aceptable de un partido pol´ıtico .
</bodyText>
<table confidence="0.772910111111111">
Input: Esta Directiva es nuestra oportunidad to marcar una verdadera diferencia , reduciendo la tr´agica p´erdida de vidas
en nuestras carreteras .
MLF baseline: Esta Directiva es nuestra oportunidad a marcar una verdadera diferencia , reduciendo la tr´agica p´erdida
de vidas en nuestras carreteras .
l1r1: Esta Directiva es nuestra oportunidad para marcar una verdadera diferencia , reduciendo la tr´agica p´erdida de vidas
en nuestras carreteras .
Input: Es la last vez que me dirijo a esta C´amara .
MLF baseline: Es la pasado vez que me dirijo a esta C´amara .
l1r1: Es la ´ultima vez que me dirijo a esta C´amara .
</table>
<tableCaption confidence="0.9850474">
Input: Pero el enfoque actual de la Comisi´on no puede conducir a una buena pol´ıtica ya que es tributario del fun-
cionamiento del mercado y de las normas establecidas por la OMC , el FMI y el Banco Mundial , normas que siguen
siendo desfavorables para los developing countries.
MLF baseline: Pero el enfoque actual de la Comisi´on no puede conducir a una buena politica ya que es tributario del
funcionamiento del mercado y de las normas establecidas por la OMC , el FMI y el Banco Mundial , normas que siguen
siendo desfavorables para los los paises en desarrollo .
l1r1: Pero el enfoque actual de la Comisi´on no puede conducir a una buena pol´ıtica ya que es tributario del funcionamiento
del mercado y de las normas establecidas por la OMC , el FMI y el Banco Mundial , normas que siguen siendo desfavor-
ables para los paises en desarrollo .
Table 3: Some illustrative examples of MLF-baseline output versus system output, in which system
</tableCaption>
<bodyText confidence="0.987779375">
output matches the correct human reference output. The actual fragments concerned are highlighted in
bold. The first example shows our system correcting for number agreement, the second a correction
in selecting the right preposition, and the third shows that the English word last can be translated in
different ways, only one of which is correct in this context. The last example shows a phrasal translation,
in which the determiner was duplicated in the baseline
applied feature selection. However, if we enable
the language model as we do in the auto+LM
configuration we do not notice an improvement
over l1r1+LM, surprisingly. We suspect the lack
of impact here can be explained by the trigram-
based Language Model having less added value
when the (left) context size of the classifier is two
or three; they are now less complementary.
Table 5 lists what context sizes have been cho-
sen in the automatic feature selection. A context
size of one prevails in the vast majority of cases,
which is not surprising considering the good re-
sults we have already seen with this configuration.
In this study we did not yet conduct optimisa-
tion of the classifier parameters. We used the IB1
algorithm with k = 1 and the default values of
the TiMBL implementation. In earlier work van
Gompel and van den Bosch (2013), we reported
a decrease in performance due to overfitting when
</bodyText>
<footnote confidence="0.7445678">
66.5% l1r1
19.9% l2r2
7.7% l3r3
3.5% l4r4
2.4% l5r5
</footnote>
<tableCaption confidence="0.9124175">
Table 5: Frequency of automatically selected con-
figurations on English to Spanish Europarl dataset
</tableCaption>
<bodyText confidence="0.9976517">
this is done, so we do not expect it to make a pos-
itive impact. The second reason for omitting this
is more practical in nature; to do this in combina-
tion with feature selection would add substantial
search complexity, making experiments far more
time consuming, even prohibitively so.
The bottom lines in Table 2 represent results
when all right-context is omitted, emulating a real-
time prediction when no right context is available
yet. This has a substantial negative impact on re-
</bodyText>
<page confidence="0.99471">
877
</page>
<table confidence="0.950036583333333">
Input: Sin ese tipo de protecci´on la gente no aprovechar´a la oportunidad to vivir , viajar y trabajar donde les parezca en
la Uni´on Europea .
l1r1: Sin ese tipo de protecci´on la gente no aprovechar´a la oportunidad para vivir , viajar y trabajar donde les parezca en
la Uni´on Europea .
l1r1+LM: Sin ese tipo de protecci´on la gente no aprovechar´a la oportunidad de vivir , viajar y trabajar donde les parezca
en la Uni´on Europea .
Input: La Comisi´on tambi´en est´a acometiendo medidas en el ´ambito social y educational con vistas a mejorar la
situaci´on de los ni˜nos .
l1r1: La Comisi´on tambi´en est´a acometiendo medidas en el ´ambito social y educativas con vistas a mejorar la situaci´on
de los ni˜nos .
l1r1+LM: La Comisi´on tambi´en est´a acometiendo medidas en el ´ambito social y educativo con vistas a mejorar la
situaci´on de los ni˜nos .
</table>
<tableCaption confidence="0.999752">
Table 4: Some examples of l1r1 versus the same configuration enriched with a language model.
</tableCaption>
<bodyText confidence="0.999964636363636">
sults. We experimented with several asymmetric
configurations and found that taking two words to
the left and one to the right yields even better re-
sults than symmetric configurations for this data
set. This result is in line with the positive effect of
adding the LM to the l1r1.
In order to draw accurate conclusions, experi-
ments on a single data set and language pair are not
sufficient. We therefore conducted a number of ex-
periments with other language pairs, and present
the abridged results in Table 6.
There are some noticeable discrepancies for
some experiments in Table 6 when compared to
our earlier results in Table 2. We see that the lan-
guage model baseline for English—*French shows
the same substantial improvement over the base-
line as our English—*Spanish results. The same
holds for the Chinese—*English experiment. How-
ever, for English—*Dutch and English—*Chinese
we find that the LM baseline actually performs
slightly worse than baseline. Nevertheless, in all
these cases, the positive effect of including a Lan-
guage Model to our classifier-based system again
shows. Also, we note that in all cases our system
performs better than the two baselines.
Another discrepancy is found in the BLEU
scores of the English—*Chinese experiments,
where we measure an unexpected drop in BLEU
score under baseline. However, all other scores do
show the expected improvement. The error rate
metrics show improvement as well. We therefore
attach low importance to this deviation in BLEU
here.
In all of the aforementioned experiments, the
system produced a single solution for each of the
fragments, the one it deemed best, or no solution
at all if it could not find any. Alternative evaluation
metrics could allow the system to output multiple
alternatives. Omission of a solution by definition
causes a decrease in recall. In all of our experi-
ments recall is high (well above 90%), mostly be-
cause train and test data lie in the same domain and
have been generated in the same fashion, lower re-
call is expected with more real-world data.
</bodyText>
<sectionHeader confidence="0.987397" genericHeader="conclusions">
7 Discussion and conclusion
</sectionHeader>
<bodyText confidence="0.999982">
In this study we have shown the feasibility of
a classifier-based translation assistance system in
which L1 fragments are translated in an L2 con-
text, in which the classifier experts are built indi-
vidually per word or phrase. We have shown that
such a translation assistance system scores both
above a context-insensitive baseline, as well as an
L2 language model baseline.
Furthermore, we found that combining this
cross-language context-sensitive technique with
an L2 language model boosts results further.
The presence of a one-word right-hand side
context proves crucial for good results, which has
implications for practical translation assistance ap-
plication that translate as soon as the user finishes
an L1 fragment. Revisiting the translation when
right context becomes available would be advis-
able.
We tested various configurations and conclude
that small context sizes work better than larger
ones. Automated configuration selection had pos-
itive results, yet the system with context size one
and an L2 language model component often pro-
duces the best results. In static configurations, the
failure of a wider context window to be more suc-
</bodyText>
<page confidence="0.993374">
878
</page>
<table confidence="0.9996628">
Dataset L1 L2 Configuration Accuracy Word Accuracy BLEU
europarl200k en nl baseline 0.7026 0.7283 0.9771
europarl200k en nl LM baseline 0.6958 0.7195 0.9773
europarl200k en nl l1r1 0.7790 0.7941 0.9814
europarl200k en nl l1r1+LM 0.7838 0.7973 0.9818
europarl200k en nl auto 0.7796 0.7947 0.9815
europarl200k en nl auto+LM 0.7812 0.7954 0.9816
europarl200k en fr baseline 0.5874 0.6403 0.9709
europarl200k en fr LM baseline 0.7054 0.7319 0.9787
europarl200k en fr l1r1 0.7416 0.7698 0.9797
europarl200k en fr l1r1+LM 0.7680 0.7885 0.9815
europarl200k en fr auto 0.7484 0.7737 0.9801
europarl200k en fr auto+LM 0.7654 0.7860 0.9813
iwslt12ted en zh baseline 0.6622 0.7122 0.6421
iwslt12ted en zh LM baseline 0.6550 0.6982 0.6416
iwslt12ted en zh l1r1 0.7150 0.7531 0.5736
iwslt12ted en zh l1r1+LM 0.7296 0.7619 0.5826
iwslt12ted en zh auto 0.7150 0.7519 0.5746
iwslt12ted en zh auto+LM 0.7280 0.7605 0.5833
iwslt12ted zh en baseline 0.5784 0.6167 0.9634
iwslt12ted zh en LM baseline 0.6148 0.6463 0.9656
iwslt12ted zh en l1r1 0.7104 0.7338 0.9709
iwslt12ted zh en l1r1+LM 0.7270 0.7460 0.9721
iwslt12ted zh en auto 0.7078 0.7319 0.9709
iwslt12ted zh en auto+LM 0.7230 0.7428 0.9719
</table>
<tableCaption confidence="0.8681215">
Table 6: Results on different datasets and language pairs. The iwslt12ted set is the dataset used in the
IWSLT 2012 Evaluation Campaign (Federico et al., 2012), and is formed by a collection of transcriptions
</tableCaption>
<bodyText confidence="0.997930333333333">
of TED talks. Here we used of just over 70, 000 sentences for training. Recall for each of the four datasets
is 0.9498 (en-nl), 0.9494 (en-fr), 0.9386 (en-zh), and 0.9366 (zh-en)
cesful may be attributed to the increased sparsity
that comes from such an expansion.
The idea of a comprehensive translation assis-
tance system may extend beyond the translation of
L1 fragments in an L2 context. There are more
NLP components that might play a role if such a
system were to find practical application. Word
completion or predictive editing (in combination
with error correction) would for instance seem an
indispensable part of such a system, and can be
implemented alongside the technique proposed in
this study. A point of more practically-oriented
future research is to see how feasible such combi-
nations are and what techniques can be used.
An application of our idea outside the area of
translation assistance is post-correction of the out-
put of some MT systems that, as a last-resort
heuristic, copy source words or phrases into their
output, producing precisely the kind of input our
system is trained on. Our classification-based ap-
proach may be able to resolve some of these cases
operating as an add-on to a regular MT system –
or as a independent post-correction system.
Our system allows L1 fragments to be of arbi-
trary length. If a fragment was not seen during
training stage, and is therefore not covered by a
classifier expert, then the system will be unable
to translate it. Nevertheless, if a longer L1 frag-
ment can be decomposed into subfragments that
are known, then some recombination of the trans-
lations of said sub-fragments may be a good trans-
lation for the whole. We are currently exploring
this line of investigation, in which the gap with
MT narrows further.
Finally, an important line of future research
is the creation of a more representative test set.
Lacking an interactive system that actually does
what we emulate, we hypothesise that good ap-
proximations would be to use gap exercises, or
cloze tests, that test specific aspects difficulties
in language learning. Similarly, we may use
L2 learner corpora with annotations of code-
switching points or errors. Here we then assume
that places where L2 errors occur may be indica-
tive of places where L2 learners are in some trou-
ble, and might want to fall back to generating L1.
By then manually translating gaps or such prob-
lematic fragments into L1 we hope to establish a
more realistic test set.
</bodyText>
<sectionHeader confidence="0.999092" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9623395">
D. W. Aha, D. Kibler, and M. K. Albert. 1991.
Instance-based learning algorithms. Machine
</reference>
<page confidence="0.988821">
879
</page>
<reference confidence="0.99931115625">
Learning, 06(1):37–66, January.
S. Barrachina, O. Bender, F. Casacuberta, J. Civera,
E. Cubel, S. Khadivi, A. L. Lagarda, H. Ney,
J. Tom´as, E. Vidal, and J.M. Vilar. 2009. Statistical
approaches to computer-assisted translation. Com-
putational Linguistics, 35(1):3–28.
M. Carpuat and D. Wu. 2007. Improving statis-
tical machine translation using word sense disam-
biguation. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 61–72.
W. Daelemans, J. Zavrel, K. van der Sloot, and A. van
den Bosch. 2009. TiMBL: Tilburg memory based
learner, version 6.2, reference guide. Technical Re-
port ILK 09-01, ILK Research Group, Tilburg Uni-
versity.
B. Decadt, V. Hoste, W. Daelemans, and A. van den
Bosch. 2004. GAMBL, genetic algorithm optimiza-
tion of memory-based WSD. In R. Mihalcea and
P. Edmonds, editors, Proceedings of the Third In-
ternational Workshop on the Evaluation of Systems
for the Semantic Analysis of Text (Senseval-3), pages
108–112, New Brunswick, NJ. ACL.
M. Federico, M. Cettolo, L. Bentivogli, M. Paul, and
S. St¨uker. 2012. Overview of the IWSLT 2012 eval-
uation campaign. In Proceedings of the seventh In-
ternational Workshop on Spoken Language Transla-
tion (IWSLT), pages 12–33.
J. Gim´enez and L. M`arquez. 2007. Context-aware dis-
criminative phrase selection for statistical machine
translation. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 159–166,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
R. Haque, S. Kumar Naskar, A. van den Bosch, and
A. Way. 2011. Integrating source-language con-
text into phrase-based statistical machine transla-
tion. Machine Translation, 25(3):239–285, Septem-
ber.
V. Hoste, I. Hendrickx, W. Daelemans, and A. van
den Bosch. 2002. Parameter optimization for ma-
chine learning of word sense disambiguation. Natu-
ral Language Engineering, 8(4):311–325.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings of
the 45th Annual Meeting of the Association for Com-
putational Linguistics Companion Volume Proceed-
ings of the Demo and Poster Sessions, pages 177–
180, Prague, Czech Republic, June. Association for
Computational Linguistics.
P. Koehn. 2004. Statistical significance tests for ma-
chine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388–395, Barcelona, Spain, July. Association
for Computational Linguistics.
P. Koehn. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In In Proceedings of the
Machine Translation Summit X ([MT]’05)., pages
79–86.
A. Laghos and Z. Panayiotis. 2005. Computer assist-
ed/aided language learning. pages 331–336.
M. Levy. 1997. Computer-assisted language learning:
Context and conceptualization. Oxford: Clarendon
Press.
H. Tou Ng and H. Beng Lee. 1996. Integrating multi-
ple knowledge sources to disambiguate word sense:
An exemplar-based approach. In ACL, pages 40–47.
F.J. Och and H. Ney. 2000. Giza++: Training of sta-
tistical translation models. Technical report, RWTH
Aachen, University of Technology.
F. J. Och and H. Ney. 2003. A systematic compari-
son of various statistical alignment models. Comput.
Linguist., 29(1):19–51, March.
A. Stolcke. 2002. Srilm - an extensible language mod-
eling toolkit. In John H. L. Hansen and Bryan L.
Pellom, editors, 7th International Conference on
Spoken Language Processing, ICSLP2002 - INTER-
SPEECH 2002, Denver, Colorado, USA, September
16-20, 2002. ISCA.
N. Stroppa, A. van den Bosch, and A. Way. 2007.
Exploiting source similarity for SMT using context-
informed features. In A. Way and B. Gawronska,
editors, Proceedings of the 11th International Con-
ference on Theoretical Issues in Machine Transla-
tion (TMI 2007), pages 231–240, Sk¨ovde, Sweden.
M. van Gompel and A. van den Bosch. 2013. WSD2:
Parameter optimisation for memory-based cross-
lingual word-sense disambiguation. In Proceedings
of the 7th International Workshop on Semantic Eval-
uation (SemEval 2013), in conjunction with the Sec-
ond Joint Conference on Lexical and Computational
Semantics.
</reference>
<page confidence="0.997779">
880
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.946248">
<title confidence="0.994649">Translation Assistance by Translation of L1 Fragments in an L2 Context</title>
<author confidence="0.999323">Maarten van_Gompel</author>
<author confidence="0.999323">Antal van_den</author>
<affiliation confidence="0.998362">Centre for Language Radboud University</affiliation>
<email confidence="0.968506">proycon@anaproy.nl</email>
<abstract confidence="0.999391153846154">In this paper we present new research in translation assistance. We describe a system capable of translating native language (L1) fragments to foreign language (L2) fragments in an L2 context. Practical applications of this research can be framed in the context of second language learning. The type of translation assistance system under investigation here encourages language learners to write in their target language while allowing them to fall back to their native language in case the correct word or expression is not known. These code switches are subsequently translated to L2 given the L2 context. We study the feasibility of exploiting cross-lingual context to obtain high-quality translation suggestions that improve over statistical language modelling and word-sense disambiguation baselines. A classificationbased approach is presented that is indeed found to improve significantly over these baselines by making use of a contextual window spanning a small number of neighbouring words.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D W Aha</author>
<author>D Kibler</author>
<author>M K Albert</author>
</authors>
<title>Instance-based learning algorithms.</title>
<date>1991</date>
<booktitle>Machine Learning,</booktitle>
<volume>06</volume>
<issue>1</issue>
<contexts>
<context position="10118" citStr="Aha et al., 1991" startWordPosition="1654" endWordPosition="1657">ased system composed of so-called “classifier experts”. Numerous classifiers are trained and each is an expert in translating a single word or phrase. In other words, for each word type or phrase type that occurs as a fragment in the training set, and which does not map to just a single translation, a classifier is trained. The classifier maps the L1 word or phrase in its L2 context to its L2 translation. Words or phrases that always map to a single translation are stored in a simple mapping table, as a classifier would have no added value in such cases. The classifiers use the IB1 algorithm (Aha et al., 1991) as implemented in TiMBL (Daelemans et al., 2009).1 IB1 implements k-nearest neighbour classification. The choice for this algorithm is motivated by the fact that it handles multiple classes with ease, but first and foremost because it has been successfully employed for word sense disambiguation in other studies (Hoste et al., 2002; Decadt et al., 2004), in particular in cross-lingual word sense disambiguation, a task closely resembling our current task (van Gompel and van den Bosch, 2013). It has also been used in machine translation studies in which local source context is used to classify s</context>
</contexts>
<marker>Aha, Kibler, Albert, 1991</marker>
<rawString>D. W. Aha, D. Kibler, and M. K. Albert. 1991. Instance-based learning algorithms. Machine Learning, 06(1):37–66, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Barrachina</author>
<author>O Bender</author>
<author>F Casacuberta</author>
<author>J Civera</author>
<author>E Cubel</author>
<author>S Khadivi</author>
<author>A L Lagarda</author>
<author>H Ney</author>
<author>J Tom´as</author>
<author>E Vidal</author>
<author>J M Vilar</author>
</authors>
<title>Statistical approaches to computer-assisted translation.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>1</issue>
<marker>Barrachina, Bender, Casacuberta, Civera, Cubel, Khadivi, Lagarda, Ney, Tom´as, Vidal, Vilar, 2009</marker>
<rawString>S. Barrachina, O. Bender, F. Casacuberta, J. Civera, E. Cubel, S. Khadivi, A. L. Lagarda, H. Ney, J. Tom´as, E. Vidal, and J.M. Vilar. 2009. Statistical approaches to computer-assisted translation. Computational Linguistics, 35(1):3–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Carpuat</author>
<author>D Wu</author>
</authors>
<title>Improving statistical machine translation using word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>61--72</pages>
<contexts>
<context position="11053" citStr="Carpuat and Wu (2007)" startWordPosition="1806" endWordPosition="1809">(Hoste et al., 2002; Decadt et al., 2004), in particular in cross-lingual word sense disambiguation, a task closely resembling our current task (van Gompel and van den Bosch, 2013). It has also been used in machine translation studies in which local source context is used to classify source phrases into target phrases, rather than looking them up in a phrase table (Stroppa et al., 2007; Haque et al., 2011). The idea of local phrase selection with a discriminative machine learning classifier using additional local (source-language) context was introduced in parallel to Stroppa et al. (2007) by Carpuat and Wu (2007) and Gim´enez and M´arquez (2007); cf. Haque et al. (2011) for an overview of more recent methods. The feature vector for the classifiers represents a local context of neighbouring words, and optionally also global context keywords in a binaryvalued bag-of-words configuration. The local context consists of an X number of L2 words to the left of the L1 fragment, and Y words to the right. When presented with test data, in which the L1 fragment is explicitly marked, we first check whether there is ambiguity for this L1 fragment and if a direct translation is available in our simple mapping table.</context>
</contexts>
<marker>Carpuat, Wu, 2007</marker>
<rawString>M. Carpuat and D. Wu. 2007. Improving statistical machine translation using word sense disambiguation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 61–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>J Zavrel</author>
<author>K van der Sloot</author>
<author>A van den Bosch</author>
</authors>
<title>TiMBL: Tilburg memory based learner, version 6.2, reference guide.</title>
<date>2009</date>
<tech>Technical Report ILK 09-01,</tech>
<institution>ILK Research Group, Tilburg University.</institution>
<marker>Daelemans, Zavrel, van der Sloot, van den Bosch, 2009</marker>
<rawString>W. Daelemans, J. Zavrel, K. van der Sloot, and A. van den Bosch. 2009. TiMBL: Tilburg memory based learner, version 6.2, reference guide. Technical Report ILK 09-01, ILK Research Group, Tilburg University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Decadt</author>
<author>V Hoste</author>
<author>W Daelemans</author>
<author>A van den Bosch</author>
</authors>
<title>GAMBL, genetic algorithm optimization of memory-based WSD.</title>
<date>2004</date>
<booktitle>Proceedings of the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text (Senseval-3),</booktitle>
<pages>108--112</pages>
<editor>In R. Mihalcea and P. Edmonds, editors,</editor>
<publisher>ACL.</publisher>
<location>New Brunswick, NJ.</location>
<marker>Decadt, Hoste, Daelemans, van den Bosch, 2004</marker>
<rawString>B. Decadt, V. Hoste, W. Daelemans, and A. van den Bosch. 2004. GAMBL, genetic algorithm optimization of memory-based WSD. In R. Mihalcea and P. Edmonds, editors, Proceedings of the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text (Senseval-3), pages 108–112, New Brunswick, NJ. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Federico</author>
<author>M Cettolo</author>
<author>L Bentivogli</author>
<author>M Paul</author>
<author>S St¨uker</author>
</authors>
<title>Overview of the IWSLT 2012 evaluation campaign.</title>
<date>2012</date>
<booktitle>In Proceedings of the seventh International Workshop on Spoken Language Translation (IWSLT),</booktitle>
<pages>12--33</pages>
<marker>Federico, Cettolo, Bentivogli, Paul, St¨uker, 2012</marker>
<rawString>M. Federico, M. Cettolo, L. Bentivogli, M. Paul, and S. St¨uker. 2012. Overview of the IWSLT 2012 evaluation campaign. In Proceedings of the seventh International Workshop on Spoken Language Translation (IWSLT), pages 12–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gim´enez</author>
<author>L M`arquez</author>
</authors>
<title>Context-aware discriminative phrase selection for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>159--166</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<marker>Gim´enez, M`arquez, 2007</marker>
<rawString>J. Gim´enez and L. M`arquez. 2007. Context-aware discriminative phrase selection for statistical machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 159–166, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Haque</author>
<author>S Kumar Naskar</author>
<author>A van den Bosch</author>
<author>A Way</author>
</authors>
<title>Integrating source-language context into phrase-based statistical machine translation.</title>
<date>2011</date>
<journal>Machine Translation,</journal>
<volume>25</volume>
<issue>3</issue>
<marker>Haque, Naskar, van den Bosch, Way, 2011</marker>
<rawString>R. Haque, S. Kumar Naskar, A. van den Bosch, and A. Way. 2011. Integrating source-language context into phrase-based statistical machine translation. Machine Translation, 25(3):239–285, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Hoste</author>
<author>I Hendrickx</author>
<author>W Daelemans</author>
<author>A van den Bosch</author>
</authors>
<title>Parameter optimization for machine learning of word sense disambiguation.</title>
<date>2002</date>
<journal>Natural Language Engineering,</journal>
<volume>8</volume>
<issue>4</issue>
<marker>Hoste, Hendrickx, Daelemans, van den Bosch, 2002</marker>
<rawString>V. Hoste, I. Hendrickx, W. Daelemans, and A. van den Bosch. 2002. Parameter optimization for machine learning of word sense disambiguation. Natural Language Engineering, 8(4):311–325.</rawString>
</citation>
<citation valid="false">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="5129" citStr="Koehn et al., 2007" startWordPosition="804" endWordPosition="807"> L2. No further linguistic processing such as part-ofspeech tagging or lemmatisation takes place in our experiments; adding this remains open for future research. The parallel corpus is randomly sampled into two large and equally-sized parts. One is the basis for the training set, and the other is the basis for the test set. The reason for such a large test split shall become apparent soon. From each of the splits (5), a phrase-translation table is constructed automatically in an unsupervised fashion. This is done using the scripts provided by the Statistical Machine Translation system Moses (Koehn et al., 2007). It invokes GIZA++ (Och and Ney, 2000) to establish statistical word alignments based on the IBM Models and subsequently extracts phrases using the grow-diag-final algorithm (Och and Ney, 2003). The result, independent for each set, will be a phrase-translation table (T) that maps phrases in L1 to L2. For each phrase-pair (fs, ft) this phrase-translation table holds the computed translation probabilities P(fs|ft) and P (ft|fs). Given these phrase-translation tables, we can now extract both training data and test data using the algorithm in Figure 1. In our discourse, the source language (s) c</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177– 180, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004,</booktitle>
<pages>388--395</pages>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="21612" citStr="Koehn, 2004" startWordPosition="3569" endWordPosition="3570">9748 17.1544 1.1594 1.1424 auto+LM 0.7796 0.7966 0.9815 0.9754 17.1664 1.1021 1.0845 l1r0 0.6924 0.7223 0.9757 0.9723 17.1087 1.3415 1.3249 l2r0 0.6960 0.7245 0.9759 0.9724 17.1091 1.3364 1.3193 l2r1 0.7624 0.7849 0.9803 0.9748 17.1558 1.1554 1.1378 Table 2: Europarl results for English to Spanish (i.e English fallback in Spanish context). Recall = 0.9422 (configuration l1r1+LM in the results in Table 2). It appears that the classifier approach and the L2 language model are able to complement each other. Statistical significance on the BLEU scores was tested using pairwise bootstrap sampling (Koehn, 2004). All significance tests were performed with 5, 000 iterations. We compared the outcomes of several key configurations. We first tested l1r1 against both baselines; both differences are significant at p &lt; 0.01 for both. The same significance level was found when comparing l1r1+LM against l1r1, auto+LM against auto, as well as the LM baseline against the MLF baseline. Automatic feature selection auto was found to perform statistically better than l1r1, but only at p &lt; 0.05. Conclusions with regard to context width may have to be tempered somewhat, as the performance of the l1r1 configuration wa</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>P. Koehn. 2004. Statistical significance tests for machine translation evaluation. In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 388–395, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation. In</title>
<date>2005</date>
<booktitle>In Proceedings of the Machine Translation Summit X ([MT]’05).,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="17279" citStr="Koehn, 2005" startWordPosition="2863" endWordPosition="2864"> It adds a LM component to the MLF baseline. This LM baseline allows the comparison of classification through L1 fragments in an L2 context, with a more traditional L2 context modelling (i.e. target language modelling) which is also cus874 tomary in MT decoders. Computing this baseline is done in the same fashion as previously illustrated in Equation 1, where scoreT then represents the normalised p(tIs) score from the phrasetranslation table rather than the class probability from the classifier. 6 Experiments &amp; Results The data for our experiments were drawn from the Europarl parallel corpus (Koehn, 2005) from which we extracted two sets of 200, 000 sentence pairs each for several language pairs. These were used to form the training and test sets. The final test sets are a randomly sampled 5, 000 sentence pairs from the 200, 000-sentence test split for each language pair. All input data for the experiments in this section are publicly available2. Let us first zoom in to convey a sense of scale on a specific language pair. The actual Europarl training set we generate for English (L1) to Spanish (L2), i.e. English fallback in a Spanish context, consists of 5,608,015 sentence pairs. This number i</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>P. Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In In Proceedings of the Machine Translation Summit X ([MT]’05)., pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Laghos</author>
<author>Z Panayiotis</author>
</authors>
<title>Computer assisted/aided language learning.</title>
<date>2005</date>
<pages>331--336</pages>
<contexts>
<context position="2121" citStr="Laghos and Panayiotis, 2005" startWordPosition="321" endWordPosition="324">e the major efforts and improvements, automatic translation does not yet rival human-level quality. Vexing issues are morphology, word-order change and long-distance dependencies. Although there is a morpho-syntactic component in this research, our scope is more constrained; its focus is on the faithful preservation of meaning from L1 to L2, akin to the role of the translation model in Statistical Machine Translation (SMT). The cross-lingual context in our research question may at first seem artificial, but its design explicitly aims at applications related to computeraided language learning (Laghos and Panayiotis, 2005; Levy, 1997) and computer-aided translation (Barrachina et al., 2009). Currently, language learners need to refer to a bilingual dictionary when in doubt about a translation of a word or phrase. Yet, this problem arises in a context, not in isolation; the learner may have already translated successfully a part of the text into L2 leading up to the problematic word or phrase. Dictionaries are not the best source to look up context; they may contain example usages, but remain biased towards single words or short expressions. The proposed application allows code switching and produces context-se</context>
</contexts>
<marker>Laghos, Panayiotis, 2005</marker>
<rawString>A. Laghos and Z. Panayiotis. 2005. Computer assisted/aided language learning. pages 331–336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Levy</author>
</authors>
<title>Computer-assisted language learning: Context and conceptualization.</title>
<date>1997</date>
<publisher>Press.</publisher>
<location>Oxford: Clarendon</location>
<contexts>
<context position="2134" citStr="Levy, 1997" startWordPosition="325" endWordPosition="326">vements, automatic translation does not yet rival human-level quality. Vexing issues are morphology, word-order change and long-distance dependencies. Although there is a morpho-syntactic component in this research, our scope is more constrained; its focus is on the faithful preservation of meaning from L1 to L2, akin to the role of the translation model in Statistical Machine Translation (SMT). The cross-lingual context in our research question may at first seem artificial, but its design explicitly aims at applications related to computeraided language learning (Laghos and Panayiotis, 2005; Levy, 1997) and computer-aided translation (Barrachina et al., 2009). Currently, language learners need to refer to a bilingual dictionary when in doubt about a translation of a word or phrase. Yet, this problem arises in a context, not in isolation; the learner may have already translated successfully a part of the text into L2 leading up to the problematic word or phrase. Dictionaries are not the best source to look up context; they may contain example usages, but remain biased towards single words or short expressions. The proposed application allows code switching and produces context-sensitive sugge</context>
</contexts>
<marker>Levy, 1997</marker>
<rawString>M. Levy. 1997. Computer-assisted language learning: Context and conceptualization. Oxford: Clarendon Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Tou Ng</author>
<author>H Beng Lee</author>
</authors>
<title>Integrating multiple knowledge sources to disambiguate word sense: An exemplar-based approach.</title>
<date>1996</date>
<booktitle>In ACL,</booktitle>
<pages>40--47</pages>
<contexts>
<context position="12628" citStr="Ng and Lee (1996)" startWordPosition="2077" endWordPosition="2080">probability distribution of the most likely translations given the context and we can replace the L1 fragment with the highest scoring L2 translation and present it back to the user. In addition to local context features, we also experimented with global context features. These are a set of L2 contextual keywords for each L1 word/phrase and its L2 translation occurring in the same sentence, not necessarily in the immediate neighbourhood of the L1 word/phrase. The keywords are selected to be indicative for a specific 1http://ilk.uvt.nl/timbl 873 translation. We used the method of extraction by Ng and Lee (1996) and encoded all keywords in a binary bag of words model. The experiments however showed that inclusion of such keywords did not make any noticeable impact on any of the results, so we restrict ourselves to mentioning this negative result. Our full system, including the scripts for data preparation, training, and evaluation, is implemented in Python and freely available as open-source from http://github.com/ proycon/colibrita/ . Version tag v0.2.1 is representative for the version used in this research. 3.1 Language Model We also implement a statistical language model as an optional component </context>
</contexts>
<marker>Ng, Lee, 1996</marker>
<rawString>H. Tou Ng and H. Beng Lee. 1996. Integrating multiple knowledge sources to disambiguate word sense: An exemplar-based approach. In ACL, pages 40–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Giza++: Training of statistical translation models.</title>
<date>2000</date>
<tech>Technical report,</tech>
<institution>RWTH Aachen, University of Technology.</institution>
<contexts>
<context position="5168" citStr="Och and Ney, 2000" startWordPosition="811" endWordPosition="814">ch as part-ofspeech tagging or lemmatisation takes place in our experiments; adding this remains open for future research. The parallel corpus is randomly sampled into two large and equally-sized parts. One is the basis for the training set, and the other is the basis for the test set. The reason for such a large test split shall become apparent soon. From each of the splits (5), a phrase-translation table is constructed automatically in an unsupervised fashion. This is done using the scripts provided by the Statistical Machine Translation system Moses (Koehn et al., 2007). It invokes GIZA++ (Och and Ney, 2000) to establish statistical word alignments based on the IBM Models and subsequently extracts phrases using the grow-diag-final algorithm (Och and Ney, 2003). The result, independent for each set, will be a phrase-translation table (T) that maps phrases in L1 to L2. For each phrase-pair (fs, ft) this phrase-translation table holds the computed translation probabilities P(fs|ft) and P (ft|fs). Given these phrase-translation tables, we can now extract both training data and test data using the algorithm in Figure 1. In our discourse, the source language (s) corresponds to L1, the fallback language</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>F.J. Och and H. Ney. 2000. Giza++: Training of statistical translation models. Technical report, RWTH Aachen, University of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Comput. Linguist.,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="5323" citStr="Och and Ney, 2003" startWordPosition="835" endWordPosition="838">sampled into two large and equally-sized parts. One is the basis for the training set, and the other is the basis for the test set. The reason for such a large test split shall become apparent soon. From each of the splits (5), a phrase-translation table is constructed automatically in an unsupervised fashion. This is done using the scripts provided by the Statistical Machine Translation system Moses (Koehn et al., 2007). It invokes GIZA++ (Och and Ney, 2000) to establish statistical word alignments based on the IBM Models and subsequently extracts phrases using the grow-diag-final algorithm (Och and Ney, 2003). The result, independent for each set, will be a phrase-translation table (T) that maps phrases in L1 to L2. For each phrase-pair (fs, ft) this phrase-translation table holds the computed translation probabilities P(fs|ft) and P (ft|fs). Given these phrase-translation tables, we can now extract both training data and test data using the algorithm in Figure 1. In our discourse, the source language (s) corresponds to L1, the fallback language used for by the end-user for inserting fragments, whilst the target language (t) is L2. Step 4 is effectively a filter: two thresholds can be configured t</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Comput. Linguist., 29(1):19–51, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>Srilm - an extensible language modeling toolkit. In</title>
<date>2002</date>
<booktitle>7th International Conference on Spoken Language Processing, ICSLP2002 - INTERSPEECH 2002,</booktitle>
<editor>John H. L. Hansen and Bryan L. Pellom, editors,</editor>
<publisher>ISCA.</publisher>
<location>Denver, Colorado, USA,</location>
<contexts>
<context position="13432" citStr="Stolcke, 2002" startWordPosition="2205" endWordPosition="2206">ict ourselves to mentioning this negative result. Our full system, including the scripts for data preparation, training, and evaluation, is implemented in Python and freely available as open-source from http://github.com/ proycon/colibrita/ . Version tag v0.2.1 is representative for the version used in this research. 3.1 Language Model We also implement a statistical language model as an optional component of our classifier-based system and also as a baseline to compare our system to. The language model is a trigram-based backoff language model with Kneser-Ney smoothing, computed using SRILM (Stolcke, 2002) and trained on the same training data as the translation model. No additional external data was brought in, to keep the comparison fair. For any given hypothesis H, results from the L1 to L2 classifier are combined with results from the L2 language model. We do so by normalising the class probability from the classifier (scoreT(H)), which is our translation model, and the language model (scorelm(H)), in such a way that the highest classifier score for the alternatives under consideration is always 1.0, and the highest language model score of the sentence is always 1.0. Take scoreT(H) and scor</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. Srilm - an extensible language modeling toolkit. In John H. L. Hansen and Bryan L. Pellom, editors, 7th International Conference on Spoken Language Processing, ICSLP2002 - INTERSPEECH 2002, Denver, Colorado, USA, September 16-20, 2002. ISCA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Stroppa</author>
<author>A van den Bosch</author>
<author>A Way</author>
</authors>
<title>Exploiting source similarity for SMT using contextinformed features.</title>
<date>2007</date>
<booktitle>Proceedings of the 11th International Conference on Theoretical Issues in Machine Translation (TMI 2007),</booktitle>
<pages>231--240</pages>
<editor>In A. Way and B. Gawronska, editors,</editor>
<location>Sk¨ovde, Sweden.</location>
<marker>Stroppa, van den Bosch, Way, 2007</marker>
<rawString>N. Stroppa, A. van den Bosch, and A. Way. 2007. Exploiting source similarity for SMT using contextinformed features. In A. Way and B. Gawronska, editors, Proceedings of the 11th International Conference on Theoretical Issues in Machine Translation (TMI 2007), pages 231–240, Sk¨ovde, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M van Gompel</author>
<author>A van den Bosch</author>
</authors>
<title>WSD2: Parameter optimisation for memory-based crosslingual word-sense disambiguation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013), in conjunction with the Second Joint Conference on Lexical and Computational Semantics.</booktitle>
<marker>van Gompel, van den Bosch, 2013</marker>
<rawString>M. van Gompel and A. van den Bosch. 2013. WSD2: Parameter optimisation for memory-based crosslingual word-sense disambiguation. In Proceedings of the 7th International Workshop on Semantic Evaluation (SemEval 2013), in conjunction with the Second Joint Conference on Lexical and Computational Semantics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>