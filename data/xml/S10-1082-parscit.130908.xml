<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.019655">
<title confidence="0.623199">
KSU KDD: Word Sense Induction by Clustering in Topic Space
</title>
<author confidence="0.97051">
Wesam Elshamy, Doina Caragea, William H. Hsu
</author>
<affiliation confidence="0.969492">
Kansas State University
</affiliation>
<email confidence="0.968036">
{welshamy, dcaragea, bhsu}@ksu.edu
</email>
<sectionHeader confidence="0.979348" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999957411764706">
We describe our language-independent un-
supervised word sense induction system.
This system only uses topic features to
cluster different word senses in their global
context topic space. Using unlabeled data,
this system trains a latent Dirichlet allo-
cation (LDA) topic model then uses it to
infer the topics distribution of the test in-
stances. By clustering these topics dis-
tributions in their topic space we cluster
them into different senses. Our hypothesis
is that closeness in topic space reflects sim-
ilarity between different word senses. This
system participated in SemEval-2 word
sense induction and disambiguation task
and achieved the second highest V-measure
score among all other systems.
</bodyText>
<sectionHeader confidence="0.996287" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99991612">
Ambiguity of meaning is inherent in natural lan-
guage because the deliverer of words tries to mini-
mize the size of the vocabulary set he uses. There-
fore, a sizable portion of this vocabulary is polyse-
mous and the intended meaning of such words can
be encoded in their context.
Due to the knowledge acquisition bottleneck
problem and scarcity in training data (Cai et
al., 2007), unsupervised corpus based approaches
could be favored over supervised ones in word sense
disambiguation (WSD) tasks.
Similar efforts in this area include work by Cai
et al. (Cai et al., 2007) in which they use latent
Dirichlet allocation (LDA) topic models to extract
the global context topic and use it as a feature
along other baseline features. Another technique
uses clustering based approach with WordNet as an
external resource for disambiguation without rely-
ing on training data (Anaya-S´anchez et al., 2007).
To disambiguate a polysemous word in a text
document, we use the document topic distribution
to represent its context. A document topic distri-
bution is the probabilistic distribution of a docu-
ment over a set of topics. The assumption is that:
given two word senses and the topic distribution
</bodyText>
<figureCaption confidence="0.998235">
Figure 1: A graphical model for LDA
</figureCaption>
<bodyText confidence="0.999440923076923">
of their context, the closeness between these two
topic distributions in their topic space is an indi-
cation of the similarity between those two senses.
Our motivation behind building this system was
the observation that the context of a polysemous
word helps determining its sense to some degree.
In our word sense induction (WSI) system, we use
LDA to create a topic model for the given corpus
and use it to infer the topic distribution of the
documents containing the ambiguous words.
This paper describes our WSI system which par-
ticipated in SemEval-2 word sense induction and
disambiguation task (Manandhar et al., 2010).
</bodyText>
<sectionHeader confidence="0.883506" genericHeader="method">
2 Latent Dirichlet allocation
</sectionHeader>
<bodyText confidence="0.9996864375">
LDA is a probabilistic model for a collection of dis-
crete data (Blei et al., 2003). It can be graphically
represented as shown in Figure 1 as a three level
hierarchical Bayesian model. In this model, the
corpus consists of M documents, each is a multino-
mial distribution over K topics, which are in turn
multinomial distributions over words.
To generate a document d using this probabilis-
tic model, a distribution over topics θd is generated
using a Dirichlet prior with parameter α. Then,
for each of the Nd words wdn in the document,
a topic zdn is drawn from a multinomial distribu-
tion with the parameter θd. Then, a word wdn is
drawn from that topic’s distribution over words,
given βzj = p(w = ilz = j). Where βzj is the proba-
bility of choosing word i given topic j.
</bodyText>
<sectionHeader confidence="0.92469" genericHeader="method">
3 System description
</sectionHeader>
<bodyText confidence="0.9997485">
We wanted to examine the trade-off between sim-
plicity, cost and performance by building a simple
</bodyText>
<equation confidence="0.687662">
α
θ
z w
Q
N
M
</equation>
<page confidence="0.946453">
367
</page>
<bodyText confidence="0.986682243243243">
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 367–370,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
language-independent, totally unsupervised, com-
putationally cheap system and compare its perfor-
mance to other WSI systems participating in the
SemEval-2 WSI task (Manandhar et al., 2010). We
expect a degradation in precision of our simple ap-
proach as the granularity of senses becomes finer;
This is due to the degrading sensitivity in mapping
between the topics space and the senses space. We
note that our simple approach will fail if multiple
senses of the same word appear in the same docu-
ment; Since these senses will be represented by the
same topic distribution of the document, they will
be clustered in the same cluster.
Our system is a language-independent system.
The used LDA topic model has no knowledge of
the training or testing corpus language. Unlike
most other WSI and WSD systems, it doesn’t make
use of part of speech (POS) features which are lan-
guage dependent and require POS annotated train-
ing data. The only features used are the topics dis-
tribution of bag-of-words containing the ambigu-
ous word.
First, for each target polysemous word wp (noun
or verb), we train a MALLET1 parallel topic model
implementation of LDA on all the training in-
stances of that word. Then we use the trained topic
model to infer the topics distribution θl for each of
the test instances of that word. For a K-topics
topic model, each topics distribution can be repre-
sented as a point in a K-dimensional topic space.
These points can be clustered into C different clus-
ters, each representing a word sense. We used
MALLET’s K-means clustering algorithm with co-
sine similarity to measure the distance between dif-
ferent topic distributions in the topic space.
</bodyText>
<sectionHeader confidence="0.984131" genericHeader="method">
4 Evaluation measures
</sectionHeader>
<bodyText confidence="0.9999751">
We use the same unsupervised evaluation mea-
sures used in SemEval-2 (Manandhar and Kla-
paftis, 2009). These measures do not require de-
scriptive
The V-measure is used for unsupervised evalu-
ation. It is the harmonic mean of the homogene-
ity and completeness. Homogeneity is a measure
of the degree that each formed cluster consists of
data points that belong to a single gold standard
(GS) class as defined below.
</bodyText>
<equation confidence="0.9613558">
homogeneity = 1 − H(GSIC)H(GS) (1)
∑SCS ∑SCS
j=1 aij j=1 aij
N log (2)
N
aij
log (3)
SGSS
∑k=1 akj
lhttp://mallet.cs.umass.edu
</equation>
<tableCaption confidence="0.9531195">
Table 1: Effect of varying the number of topics K
on performance
</tableCaption>
<table confidence="0.997969">
K 10 50 200 400 500
V-measure 5.1 5.8 7.2 8.4 8.1
F-score 8.6 32.0 53.9 63.9 64.2
</table>
<bodyText confidence="0.997436857142857">
Where H() is an entropy function, SCS and IGS1
refer to cluster and class sizes, respectively. N is
the number of data points, aij are data points of
class GSi that belong to cluster Cj.
On the other hand, completeness measures the
degree that each class consists of data points that
belong to a single cluster. It is defined as follows.
</bodyText>
<equation confidence="0.9342862">
aij
aij
N log (6)
∑SCS
k=1 aik
</equation>
<bodyText confidence="0.9998435">
Homogeneity and completeness can be seen as
entropy based measures of precision and recall, re-
spectively. The V-measure has a range of 0 (worst
performance) to 1, inclusive.
The other evaluation measure is the F-score,
which is the harmonic mean of precision and re-
call. It has a range of 0 to 1 (best performance),
inclusive.
</bodyText>
<sectionHeader confidence="0.934689" genericHeader="evaluation">
5 Experiments and results
</sectionHeader>
<bodyText confidence="0.999976086956522">
The WSI system described earlier was tested on
SemEval-1 WSI task (task 2) data (65 verbs,
35 nouns), and participated in the same task in
SemEval-2 (task 14) (50 verbs, 50 nouns). The
sense induction process was the same in both cases.
Before running our main experiments, we
wanted to see how the number of topics K used in
the topic model could affect the performance of our
system. We tested our WSI system on SemEval-1
data using different K values as shown in Table 1.
We found that the V-measure and F-score values
increase with increasing K, as more dimensions are
added to the topic space, the different senses in this
K-dimensional space unfold. This trend stops at a
value of K = 400 in a sign to the limited vocabu-
lary of the training data. This K value is used in
all other experiments.
Next, we evaluated the performance of our sys-
tem on SemEval-1 WSI task data. Since no train-
ing data was provided for this task, we used an un-
annotated version of the test instances to create the
LDA topic model. For each target word (verb or
noun), we trained the topic model on its given test
</bodyText>
<equation confidence="0.99513471875">
SGSS
H(GS) = − Q
i=1
aij
H(GSIC) = −
N
SCS
Q
j=1
SGSS
Q
i=1
SCS
Q
j=1
SGSS
∑i=1 aij
N
H(C) = −
log ∑i=1SSaij (5)
N
H(CIGS)
completeness = 1 −
(4)
H(C)
SCS
Q
j=1
H(CIGS) = −
SGSS
Q
i=1
</equation>
<page confidence="0.99796">
368
</page>
<tableCaption confidence="0.724273">
Table 2: V-measure and F-score on SemEval-1
</tableCaption>
<table confidence="0.989688888888889">
All Verbs Nouns
V-measure 8.4 8.0 8.7
F-score 63.9 56.8 69.0
Table 3: V-measure and F-score on SemEval-2
All Verbs Nouns
V-measure 15.7 12.4 18.0
F-score 36.9 54.7 24.6
Class Cluster
1 2 3 4
</table>
<subsectionHeader confidence="0.474968">
Job Offer Encourage Press
</subsectionHeader>
<bodyText confidence="0.999814255813954">
instances. Then we used the generated model’s in-
ferencer to find the topics distribution of each one
of them. These distributions are then clustered in
the topic space using the K-means algorithm and
the cosine similarity measure was used to evalu-
ate the distances between these distributions. The
results of this experiment are shown in Table 2.
Our WSI system took part in the main SemEval-
2 WSI task (task 14). In the unsupervised evalua-
tion, our system had the second highest V-measure
value of 15.7 for all words2. A break down of the
obtained V-measure and F-scores is shown in Table
3.
To analyze the performance of the system, we
examined the clustering of the target noun word
“promotion” to different senses by our system. We
compared it to the GS classes of this word in the
answer key provided by the task organizers. For
a more objective comparison, we ran the K-means
clustering algorithm with K equal to the number
of GS classes. Even though the number of formed
clusters affects the performance of the system, we
assume that the number of senses is known in this
analysis. We focus on the ability of the algorithm
to cluster similar senses together. A graphical com-
parison is given in Figure 2.
The target noun word “promotion” has 27 in-
stances and four senses. The lower four rectangles
in Figure 2 represent the four different GS classes,
and the upper four rectangles represent the four
clusters created by our system. Three of the four
instances representing a job “promotion” (Q) were
clustered together, but the fourth one was clus-
tered in a different class due to terms like “driv-
ing,” “troops,” and “hostile” in its context. The
offer sense of “promotion” (0) was mainly split
between two clusters, cluster 2 which most of its
instances has mentions of numbers and monetary
units, and cluster 4 which describes business and
labor from an employee’s eye.
The 13 instances of the third class which carry
the sense encourage of the word promotion (❑) are
distributed among the four different clusters de-
</bodyText>
<footnote confidence="0.959007666666667">
2A complete evaluation of all partic-
ipating systems is available online at:
http://www.cs.york.ac.uk/semeval2010 WSI/task 14 ranking.html
</footnote>
<figureCaption confidence="0.999792">
Figure 2: Analysis of sense clustering
</figureCaption>
<bodyText confidence="0.999969">
pending on other topic words that classified them
as either belonging to cluster 4 (encouragement in
business), cluster 3 (encouragement in conflict or
war context), cluster 2 (numbers and money con-
text), or cluster 1 (otherwise). We can see that the
topic model is unable to detect and extract topic
words for the “encourage” sense of the word. Fi-
nally, due to the lack of enough training instances
of the sense of a promotional issue of a newspaper
(*), the topic model inferencer clustered it in the
numbers and monetary cluster because it was rich
in numbers.
</bodyText>
<sectionHeader confidence="0.999098" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999959473684211">
Clustering the topics distributions of the global
context of polysemous words in the topic space to
induce their sense is cheap as it does not require
any annotated data and is language-independent.
Even though the clustering produced by our sys-
tem did not fully conform with the set of senses
given by the GS classes, it can be seen from the
analyzed example given earlier that our cluster-
ing carried some different senses. In one case, a
GS sense was not captured by the topic model,
and instead, other cues from its instances context
were used to cluster them accordingly. The in-
duced clustering had some noise though.
This simple WSI approach can be used for cheap
sense induction or for languages for which no
POS tagger has been created yet. This system
which had the second highest V-measure score in
SemEval-2 WSI task achieves a good trade-off be-
tween performance and cost.
</bodyText>
<sectionHeader confidence="0.999483" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9859972">
Henry Anaya-S´anchez, Aurora Pons-Porrata, and
Rafael Berlanga-Llavori. 2007. Tkb-uo: Us-
ing sense clustering for wsd. In Proceedings of
the Fourth International Workshop on Seman-
tic Evaluations (SemEval-2007), pages 322–325,
</reference>
<page confidence="0.988682">
369
</page>
<reference confidence="0.999936846153846">
Prague, Czech Republic, June. Association for
Computational Linguistics.
David M. Blei, Andrew Y. Ng, and Michael I. Jor-
dan. 2003. Latent dirichlet allocation. J. Mach.
Learn. Res., 3:993–1022.
Junfu Cai, Wee Sun Lee, and Yee Whye Teh.
2007. Improving word sense disambiguation us-
ing topic features. In Proceedings of the 2007
Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL),
pages 1015–1023, Prague, Czech Republic, June.
Association for Computational Linguistics.
Suresh Manandhar and Ioannis P. Klapaftis. 2009.
Semeval-2010 task 14: evaluation setting for
word sense induction &amp; disambiguation sys-
tems. In DEW ’09: Proceedings of the Workshop
on Semantic Evaluations: Recent Achievements
and Future Directions, pages 117–122, Morris-
town, NJ, USA. Association for Computational
Linguistics.
Suresh Manandhar, Ioannis P. Klapaftis, Dmitriy
Dligach, and Sameer S. Pradhan. 2010.
Semeval-2010 task 14: Word sense induction &amp;
disambiguation. In Proceedings of SemEval-2,
Uppsala, Sweden. ACL.
</reference>
<page confidence="0.998269">
370
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.924477">
<title confidence="0.987574">KSU KDD: Word Sense Induction by Clustering in Topic Space</title>
<author confidence="0.996024">Wesam Elshamy</author>
<author confidence="0.996024">Doina Caragea</author>
<author confidence="0.996024">William H Hsu</author>
<affiliation confidence="0.998887">Kansas State University</affiliation>
<email confidence="0.946201">dcaragea,</email>
<abstract confidence="0.999611611111111">We describe our language-independent unsupervised word sense induction system. This system only uses topic features to cluster different word senses in their global context topic space. Using unlabeled data, this system trains a latent Dirichlet allocation (LDA) topic model then uses it to infer the topics distribution of the test instances. By clustering these topics distributions in their topic space we cluster them into different senses. Our hypothesis is that closeness in topic space reflects similarity between different word senses. This system participated in SemEval-2 word sense induction and disambiguation task and achieved the second highest V-measure score among all other systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Henry Anaya-S´anchez</author>
<author>Aurora Pons-Porrata</author>
<author>Rafael Berlanga-Llavori</author>
</authors>
<title>Tkb-uo: Using sense clustering for wsd.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>322--325</pages>
<marker>Anaya-S´anchez, Pons-Porrata, Berlanga-Llavori, 2007</marker>
<rawString>Henry Anaya-S´anchez, Aurora Pons-Porrata, and Rafael Berlanga-Llavori. 2007. Tkb-uo: Using sense clustering for wsd. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 322–325,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Czech Republic Prague</author>
</authors>
<date></date>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Prague, </marker>
<rawString>Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>3--993</pages>
<contexts>
<context position="2848" citStr="Blei et al., 2003" startWordPosition="455" endWordPosition="458">arity between those two senses. Our motivation behind building this system was the observation that the context of a polysemous word helps determining its sense to some degree. In our word sense induction (WSI) system, we use LDA to create a topic model for the given corpus and use it to infer the topic distribution of the documents containing the ambiguous words. This paper describes our WSI system which participated in SemEval-2 word sense induction and disambiguation task (Manandhar et al., 2010). 2 Latent Dirichlet allocation LDA is a probabilistic model for a collection of discrete data (Blei et al., 2003). It can be graphically represented as shown in Figure 1 as a three level hierarchical Bayesian model. In this model, the corpus consists of M documents, each is a multinomial distribution over K topics, which are in turn multinomial distributions over words. To generate a document d using this probabilistic model, a distribution over topics θd is generated using a Dirichlet prior with parameter α. Then, for each of the Nd words wdn in the document, a topic zdn is drawn from a multinomial distribution with the parameter θd. Then, a word wdn is drawn from that topic’s distribution over words, g</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. J. Mach. Learn. Res., 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junfu Cai</author>
<author>Wee Sun Lee</author>
<author>Yee Whye Teh</author>
</authors>
<title>Improving word sense disambiguation using topic features.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>1015--1023</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1263" citStr="Cai et al., 2007" startWordPosition="195" endWordPosition="198">ess in topic space reflects similarity between different word senses. This system participated in SemEval-2 word sense induction and disambiguation task and achieved the second highest V-measure score among all other systems. 1 Introduction Ambiguity of meaning is inherent in natural language because the deliverer of words tries to minimize the size of the vocabulary set he uses. Therefore, a sizable portion of this vocabulary is polysemous and the intended meaning of such words can be encoded in their context. Due to the knowledge acquisition bottleneck problem and scarcity in training data (Cai et al., 2007), unsupervised corpus based approaches could be favored over supervised ones in word sense disambiguation (WSD) tasks. Similar efforts in this area include work by Cai et al. (Cai et al., 2007) in which they use latent Dirichlet allocation (LDA) topic models to extract the global context topic and use it as a feature along other baseline features. Another technique uses clustering based approach with WordNet as an external resource for disambiguation without relying on training data (Anaya-S´anchez et al., 2007). To disambiguate a polysemous word in a text document, we use the document topic d</context>
</contexts>
<marker>Cai, Lee, Teh, 2007</marker>
<rawString>Junfu Cai, Wee Sun Lee, and Yee Whye Teh. 2007. Improving word sense disambiguation using topic features. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 1015–1023, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suresh Manandhar</author>
<author>Ioannis P Klapaftis</author>
</authors>
<title>Semeval-2010 task 14: evaluation setting for word sense induction &amp; disambiguation systems.</title>
<date>2009</date>
<booktitle>In DEW ’09: Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions,</booktitle>
<pages>117--122</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="5629" citStr="Manandhar and Klapaftis, 2009" startWordPosition="927" endWordPosition="931"> the training instances of that word. Then we use the trained topic model to infer the topics distribution θl for each of the test instances of that word. For a K-topics topic model, each topics distribution can be represented as a point in a K-dimensional topic space. These points can be clustered into C different clusters, each representing a word sense. We used MALLET’s K-means clustering algorithm with cosine similarity to measure the distance between different topic distributions in the topic space. 4 Evaluation measures We use the same unsupervised evaluation measures used in SemEval-2 (Manandhar and Klapaftis, 2009). These measures do not require descriptive The V-measure is used for unsupervised evaluation. It is the harmonic mean of the homogeneity and completeness. Homogeneity is a measure of the degree that each formed cluster consists of data points that belong to a single gold standard (GS) class as defined below. homogeneity = 1 − H(GSIC)H(GS) (1) ∑SCS ∑SCS j=1 aij j=1 aij N log (2) N aij log (3) SGSS ∑k=1 akj lhttp://mallet.cs.umass.edu Table 1: Effect of varying the number of topics K on performance K 10 50 200 400 500 V-measure 5.1 5.8 7.2 8.4 8.1 F-score 8.6 32.0 53.9 63.9 64.2 Where H() is an</context>
</contexts>
<marker>Manandhar, Klapaftis, 2009</marker>
<rawString>Suresh Manandhar and Ioannis P. Klapaftis. 2009. Semeval-2010 task 14: evaluation setting for word sense induction &amp; disambiguation systems. In DEW ’09: Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 117–122, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suresh Manandhar</author>
<author>Ioannis P Klapaftis</author>
<author>Dmitriy Dligach</author>
<author>Sameer S Pradhan</author>
</authors>
<date>2010</date>
<booktitle>Semeval-2010 task 14: Word sense induction &amp; disambiguation. In Proceedings of SemEval-2,</booktitle>
<publisher>ACL.</publisher>
<location>Uppsala,</location>
<contexts>
<context position="2734" citStr="Manandhar et al., 2010" startWordPosition="435" endWordPosition="438">f their context, the closeness between these two topic distributions in their topic space is an indication of the similarity between those two senses. Our motivation behind building this system was the observation that the context of a polysemous word helps determining its sense to some degree. In our word sense induction (WSI) system, we use LDA to create a topic model for the given corpus and use it to infer the topic distribution of the documents containing the ambiguous words. This paper describes our WSI system which participated in SemEval-2 word sense induction and disambiguation task (Manandhar et al., 2010). 2 Latent Dirichlet allocation LDA is a probabilistic model for a collection of discrete data (Blei et al., 2003). It can be graphically represented as shown in Figure 1 as a three level hierarchical Bayesian model. In this model, the corpus consists of M documents, each is a multinomial distribution over K topics, which are in turn multinomial distributions over words. To generate a document d using this probabilistic model, a distribution over topics θd is generated using a Dirichlet prior with parameter α. Then, for each of the Nd words wdn in the document, a topic zdn is drawn from a mult</context>
<context position="4038" citStr="Manandhar et al., 2010" startWordPosition="656" endWordPosition="659">opic’s distribution over words, given βzj = p(w = ilz = j). Where βzj is the probability of choosing word i given topic j. 3 System description We wanted to examine the trade-off between simplicity, cost and performance by building a simple α θ z w Q N M 367 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 367–370, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics language-independent, totally unsupervised, computationally cheap system and compare its performance to other WSI systems participating in the SemEval-2 WSI task (Manandhar et al., 2010). We expect a degradation in precision of our simple approach as the granularity of senses becomes finer; This is due to the degrading sensitivity in mapping between the topics space and the senses space. We note that our simple approach will fail if multiple senses of the same word appear in the same document; Since these senses will be represented by the same topic distribution of the document, they will be clustered in the same cluster. Our system is a language-independent system. The used LDA topic model has no knowledge of the training or testing corpus language. Unlike most other WSI and</context>
</contexts>
<marker>Manandhar, Klapaftis, Dligach, Pradhan, 2010</marker>
<rawString>Suresh Manandhar, Ioannis P. Klapaftis, Dmitriy Dligach, and Sameer S. Pradhan. 2010. Semeval-2010 task 14: Word sense induction &amp; disambiguation. In Proceedings of SemEval-2, Uppsala, Sweden. ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>