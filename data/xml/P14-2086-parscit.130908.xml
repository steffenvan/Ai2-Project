<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.021747">
<title confidence="0.9987415">
Combining Word Patterns and Discourse Markers
for Paradigmatic Relation Classification
</title>
<author confidence="0.996167">
Michael Roth Sabine Schulte im Walde
</author>
<affiliation confidence="0.996534">
ILCC, School of Informatics Institut f¨ur Maschinelle Sprachverarbeitung
University of Edinburgh Universit¨at Stuttgart
</affiliation>
<email confidence="0.9966">
mroth@inf.ed.ac.uk schulte@ims.uni-stuttgart.de
</email>
<sectionHeader confidence="0.993839" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999923294117647">
Distinguishing between paradigmatic rela-
tions such as synonymy, antonymy and hy-
pernymy is an important prerequisite in a
range of NLP applications. In this paper,
we explore discourse relations as an alter-
native set of features to lexico-syntactic
patterns. We demonstrate that statistics
over discourse relations, collected via ex-
plicit discourse markers as proxies, can be
utilized as salient indicators for paradig-
matic relations in multiple languages, out-
performing patterns in terms of recall and
Fl-score. In addition, we observe that
markers and patterns provide complemen-
tary information, leading to significant
classification improvements when applied
in combination.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999966075471698">
Paradigmatic relations (such as synonymy,
antonymy and hypernymy; cf. Murphy, 2003) are
notoriously difficult to distinguish automatically,
as first-order co-occurrences of the related words
tend to be very similar across the relations. For
example, in The boy/girl/person loves/hates the
cat, the nominal co-hyponyms boy, girl and their
hypernym person as well as the verbal antonyms
love and hate occur in identical contexts, respec-
tively. Vector space models, which represent
words by frequencies of co-occurring words to
enable comparisons in terms of distributional
similarity (Sch¨utze, 1992; Turney and Pantel,
2010), hence perform below their potential when
inferring the type of relation that holds between
two words. This distinction is crucial, however,
in a range of tasks: in sentiment analysis, for
example, words of the same and opposing polarity
need to be distinguished; in textual entailment,
systems further need to identify hypernymy
because of directional inference requirements.
Accordingly, while there is a rich tradition on
identifying word pairs of a single paradigmatic re-
lation, there is little work that has addressed the
distinction between two or more paradigmatic re-
lations (cf. Section 2 for details). In more gen-
eral terms, previous approaches to distinguish-
ing between several semantic relations have pre-
dominantly relied on manually created knowledge
sources, or lexico-syntactic patterns that can be
automatically extracted from text. Each option
comes with its own shortcomings: knowledge
bases, on the one hand, are typically developed for
a single language or domain, meaning that they
might not generalize well; word patterns, on the
other hand, are noisy and can be sparse for infre-
quent word pairs.
In this paper, we propose to strike a balance
between availability and restrictedness by mak-
ing use of discourse markers. This approach has
several advantages: markers are frequently found
across genres (Webber, 2009), they exist in many
languages (Jucker and Yiv, 1998), and capture
various semantic properties (Hutchinson, 2004).
We implement discourse markers within a vector
space model that aims to distinguish between the
three paradigmatic relations synonymy, antonymy
and hypernymy in German and in English, across
the three word classes of nouns, verbs, adjectives.
We examine the performance of discourse markers
as vector space dimensions in isolation and also
explore their contribution in combination with lex-
ical patterns.
</bodyText>
<sectionHeader confidence="0.999853" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999906857142857">
As mentioned above, there is a rich tradition of
research on identifying a single paradigmatic rela-
tions. Work on synonyms includes Edmonds and
Hirst (2002), who employed a co-occurrence net-
work and second-order co-occurrence, and Cur-
ran (2003), who explored word-based and syntax-
based co-occurrence for thesaurus construction.
</bodyText>
<page confidence="0.970987">
524
</page>
<bodyText confidence="0.994865671428572">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 524–530,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
Van der Plas and Tiedemann (2006) compared
a standard distributional approach against cross-
lingual alignment; Erk and Pad´o (2008) defined
a vector space model to identify synonyms and
the substitutability of verbs. Most computational
work on hypernyms was performed for nouns, cf.
the lexico-syntactic patterns by Hearst (1992) and
an extension of the patterns by dependency paths
(Snow et al., 2004). Weeds et al. (2004), Lenci
and Benotto (2012) and Santus et al. (2014) identi-
fied hypernyms in distributional spaces. Computa-
tional work on antonyms includes approaches that
tested the co-occurrence hypothesis (Charles and
Miller, 1989; Fellbaum, 1995), and approaches
driven by text understanding efforts and contradic-
tion frameworks (Harabagiu et al., 2006; Moham-
mad et al., 2008; de Marneffe et al., 2008).
Among the few approaches that distinguished
between paradigmatic semantic relations, Lin et al.
(2003) used patterns and bilingual dictionaries to
retrieve distributionally similar words, and relied
on clear antonym patterns such as ‘either X or Y’
in a post-processing step to distinguish synonyms
from antonyms. The study by Mohammad et al.
(2013) on the identification and ranking of oppo-
sites also included synonym/antonym distinction.
Yih et al. (2012) developed an LSA approach in-
corporating a thesaurus, to distinguish the same
two relations. Chang et al. (2013) extended this
approach to induce vector representations that can
capture multiple relations. Whereas the above
mentioned approaches rely on additional knowl-
edge sources, Turney (2006) developed a corpus-
based approach to model relational similarity, ad-
dressing (among other tasks) the distinction be-
tween synonyms and antonyms. More recently,
Schulte im Walde and K¨oper (2013) proposed to
distinguish between the three relations antonymy,
synonymy and hyponymy based on automatically
acquired word patterns.
Regarding pattern-based approaches to iden-
tify and distinguish lexical semantic relations in
more general terms, Hearst (1992) was the first
to propose lexico-syntactic patterns as empirical
pointers towards relation instances, focusing on
hyponymy. Girju et al. (2003) applied a sin-
gle pattern to distinguish pairs of nouns that are
in a causal relationship from those that are not,
and Girju et al. (2006) extended the work to-
wards part–whole relations, applying a super-
vised, knowledge-intensive approach. Chklovski
and Pantel (2004) were the first to apply pattern-
based relation extraction to verbs, distinguish-
ing five non-disjoint relations (similarity, strength,
antonymy, enablement, happens-before). Pantel
and Pennacchiotti (2006) developed Espresso, a
weakly-supervised system that exploits patterns in
large-scale web data to distinguish between five
noun-noun relations (hypernymy, meronymy, suc-
cession, reaction, production). Similarly to Girju
et al. (2006), they used generic patterns, but relied
on a bootstrapping cycle combined with reliability
measures, rather than manual resources. Whereas
each of the aforementioned approaches considers
only one word class and clearly disjoint categories,
we distinguish between paradigmatic relations that
can be distributionally very similar and propose a
unified framework for nouns, verbs and adjectives.
</bodyText>
<sectionHeader confidence="0.994236" genericHeader="method">
3 Baseline Model and Data Set
</sectionHeader>
<bodyText confidence="0.999933090909091">
The task addressed in this work is to distin-
guish between synonymy, antonymy and hyper-
nymy. As a starting point, we build on the ap-
proach and data set used by Schulte im Walde
and K¨oper (2013, henceforth just S&amp;K). In their
work, frequency statistics over automatically ac-
quired co-occurrence patterns were found to be
good indicators for the paradigmatic relation that
holds between two given words of the same word
class. They further experimented with refinements
of the vector space model, for example, by only
considering patterns of a specific length, weight-
ing by pointwise mutual information and applying
thresholds based on frequency and reliability.
Baseline Model. We re-implemented the best
model from S&amp;K with the same setup: word pairs
are represented by vectors, with each entry corre-
sponding to one out of almost 100,000 patterns of
lemmatized word forms (e.g., X affect how
you Y ). Each value is calculated as the log fre-
quency of the corresponding pattern occurring be-
tween the word pairs in a corpus, based on exact
match. For English, we use the ukWaC corpus
(Baroni et al., 2009); for German, we rely on the
COW corpus instead of deWaC, as it is larger and
better balanced (Sch¨afer and Bildhauer, 2012).
Data Set. The evaluation data set by S&amp;K is a
collection of target and response words in Ger-
man that has been collected via Amazon Mechan-
ical Turk. The data contains a balanced amount
of instances across word categories and relations,
also taking into account corpus frequency, degree
of ambiguity and semantic classes. In total, the
</bodyText>
<page confidence="0.994847">
525
</page>
<table confidence="0.999918636363636">
P S&amp;K F1 Reimplemented
R P R F1
Nouns
SYN–ANT 77.4 65.0 70.7 76.7 62.2 68.7
SYN–HYP 75.0 57.0 64.8 73.3 59.5 65.7
Verbs
SYN–ANT 70.6 40.0 51.1 84.6 36.7 51.2
SYN–HYP 42.0 26.7 32.6 52.6 33.3 40.8
Adjectives
SYN–ANT 88.9 66.7 76.2 94.1 66.7 78.0
SYN–HYP 68.4 54.2 60.5 65.0 54.2 59.1
</table>
<tableCaption confidence="0.998512">
Table 1: 2-way classification results by Schulte
</tableCaption>
<bodyText confidence="0.993968">
im Walde and K¨oper (2013) and our re-
implementation. All numbers in percent.
data set consists of 692 pairs of instances, dis-
tributed over three word classes (nouns, verbs,
adjectives) and three paradigmatic relations (syn-
onymy, antonymy, hypernymy).
Intermediate Evaluation. We compare our re-
implementation to the model by S&amp;K using their
80% training and 20% test split, focusing on 2-
way classifications involving synonymy. The re-
sults, summarized in Table 1, confirm that our re-
implementation achieves similar results. Observed
differences are probably an effect of the distinct
corpora applied to induce patterns and counts.
We notice that the performance of both models
strongly depends on the affected pair of relations
and word category. For example, precision varies
in the 2-way classification between synonymy and
antonymy from 70.6% to 94.1%. Given the small
amount of test data, some of the 80/20 splits might
be better suited for the model than others. To avoid
resulting bias effects, we perform our final evalua-
tion using 5-fold cross-validation on a merged set
of all training and test instances. To illustrate the
performance of models in multiple languages, we
further conduct experiments on a data set for En-
glish relation pairs that has been collected by Giu-
lia Benotto and Alessandro Lenci, following the
same methodology as the German collection. The
English data set consists of 648 pairs of instances,
also distributed over nouns, verbs, adjectives, and
covering synonymy, antonymy, hypernymy.
</bodyText>
<sectionHeader confidence="0.998687" genericHeader="method">
4 Markers for Relation Classification
</sectionHeader>
<bodyText confidence="0.999267">
The aim of this work is to establish corpus statis-
tics over discourse relations as a salient source of
</bodyText>
<construct confidence="0.515073333333333">
CONTRAST but, altough, rather ...
RESTATEMENT indeed, specifically,...
INSTANTIATION (for) example, instance,...
</construct>
<tableCaption confidence="0.949982">
Table 2: Examples of discourse relations/markers.
</tableCaption>
<bodyText confidence="0.999831041666667">
information to distinguish between paradigmatic
relations. Our approach is motivated by linguis-
tic studies that indicated a connection between dis-
course relations and lexical relations of words oc-
curring in the respective discourse segments: Mur-
phy et al. (2009) have shown, for example, that
antonyms frequently serve as indicators for con-
trast relations in English and Swedish. More gen-
erally, pairs of word tokens have been identified as
strong features for classifying discourse relations
when no explicit discourse markers are available
(Pitler et al., 2009; Biran and McKeown, 2013).
Whereas word pairs have frequently been used
as features for disambiguating discourse relations,
to the best of our knowledge, our approach is novel
in that we are the first to apply discourse relations
as features for classifying lexical relations. One
reason for this might be that discourse relations in
general are only available in manually annotated
corpora. Previous work has shown, however, that
such relations can be classified reliably given the
presence of explicit discourse markers.1 We hence
rely on such markers as proxies for discourse rela-
tions (for examples, cf. Table 2).
</bodyText>
<subsectionHeader confidence="0.995473">
4.1 Model and Hypothesis
</subsectionHeader>
<bodyText confidence="0.999508">
We propose a vector space model that represents
pairs of words using as features the discourse
markers that occur between them. The under-
lying hypothesis of this model is as follows: if
two phrases frequently co-occur with a specific
discourse marker, then the discourse relation ex-
pressed by the corresponding marker should also
indicate the relation between the words in the af-
fected phrases. Following this hypothesis, contrast
relations might indicate antonymy, whereas elab-
orations may indicate synonymy or hyponymy.
Although such relations will not hold between
every pair of words in two connected discourse
segments, we hypothesize that correct instances
(of all considered word classes) can be identified
based on high relative frequency.
In our model, frequency statistics are com-
puted over sentence-internal co-occurrences of
</bodyText>
<footnote confidence="0.887857">
1Pitler et al. (2008) report an accuracy of up to 93%.
</footnote>
<page confidence="0.99565">
526
</page>
<bodyText confidence="0.99996888">
word pairs and discourse markers. Since discourse
relations are typically directed, we take into con-
sideration whether a word occurs to the left or
to the right of the respective marker. Accord-
ingly, the features of our model are special cases of
single-word patterns with an arbitrary number of
wild card tokens (e.g., the marker feature ‘though’
corresponds to the pattern “X * though * Y ”).
Yet, our specific choice of features has several ad-
vantages: Whereas strict and potentially long pat-
terns can be rare in text, discourse markers such as
“however”, “for example” and “additionally” are
frequently found across genres (Webber, 2009).
Although combinations of tokens could also be re-
placed by wild cards in any automatically acquired
pattern, this would generally lead to an exponen-
tially growing feature space. In contrast, the set
of discourse markers in our work is fixed: for En-
glish, we use 61 markers annotated in the Penn
Discourse TreeBank 2.0 (Prasad et al., 2008); for
German, we use 155 one-word translations of the
English markers, as obtained from an online dic-
tionary.23 Taking directionality into account, our
vector space model consists of 2x61 and 2x155
features, respectively.
</bodyText>
<subsectionHeader confidence="0.999052">
4.2 Development Set and Hyperparameters
</subsectionHeader>
<bodyText confidence="0.999994">
We select the hyperparameters of our model using
an independent development set, which we extract
from the lexical resource GermaNet (Hamp and
Feldweg, 1997). For each considered word cate-
gory, we extract instances of synonymy, antonymy
and hypernymy. In total, 1502 instances are iden-
tified, with 64 of them overlapping with the evalu-
ation data set described in Section 3. Note though
that the development set is not used for evaluation
but only to select the following hyperparameters.
We experimented with different vector values
(absolute frequency, log frequency, pointwise mu-
tual information (PMI)), distance measures (co-
sine, euclidean) and normalization schemes. In
contrast to S&amp;K, who did not observe any im-
provements using PMI, we found it to perform
best, combined with euclidean distance and no
additional normalization. This finding might be
an immediate effect of discourse markers being
</bodyText>
<footnote confidence="0.610403">
2http://dict.leo.org
3We also experimented with larger sets of markers, in-
cluding conjunctions and adverbials in sentence-initial posi-
tions, but did not notice any considerable effect. Future work
could use manual sets of markers, e.g. those by Pasch et al.
(2003), though such sets are only available in few languages.
</footnote>
<bodyText confidence="0.9851085">
generally more frequent than strict word patterns,
which also leads to more reliable PMI values.
</bodyText>
<sectionHeader confidence="0.995564" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.984522395833334">
In our evaluation, we assess the performance of the
marker-based model and demonstrate the benefits
of incorporating discourse markers into a pattern-
based model, which we apply as a baseline. We
evaluate on several data sets: the collection of
target-response pairs in German from previous
work, and a similar data set that was collected for
English target words (cf. Section 3); for compari-
son reasons, we also apply our models to the bal-
anced data set of related and unrelated noun pairs
by Yap and Baldwin (2009).4 We perform 3-way
and 2-way relation classification experiments, us-
ing 5-fold cross-validation and a nearest centroid
classifier (as applied by S&amp;K).
Results. The 3-way classification results of the
baseline and our marker-based model are summa-
rized in Table 3, with best results for each set-
ting marked in bold. On the German data set,
our model always outperforms a random baseline
(33% F1-score). The results on the English data
set are overall a bit lower, possibly due to corpus
size. In almost all classification tasks, our marker-
based model achieves a higher recall and F1-score
than the pattern-based approach. The precision
results of the marker-based model are overall be-
low the pattern-based model. This drop in perfor-
mance does not come as a surprise though, con-
sidering that the model only makes use of 122 and
310 features, in comparison to tens of thousands
of features in the pattern approach.
A randomized significance test over classified
instances (cf. Yeh, 2000) revealed that only two
differences in results are significant. We hypoth-
esize that one reason for this outcome might be
that both models cover complementary sets of in-
stances. To verify this hypothesis, we apply a
combined model, which is based on a weighted
linear combination of distances computed by the
two individual models.5 As displayed in Table 3,
this combined model yields further improvements
4Note that we could, in principle, also apply our models to
unbalanced data. Our main focus lies however on examining
the direct impact of different feature sets. We hence decided
to keep the evaluation setup simple and used a classifier that
does not take into account class frequency.
5We determined the best weights on the development set
and found these to be 0.9 and 0.1 for the output of the pattern-
based and marker-based model, respectively.
</bodyText>
<page confidence="0.992824">
527
</page>
<table confidence="0.9988893">
Nouns Verbs Adjectives
P R F1 P R F1 P R F1
Patterns 55.6 40.8 47.0 55.6 35.6 43.4 53.5 41.1 46.5
Markers 42.6 38.7 40.5 48.4 46.2** 47.3 51.1 48.6 49.9
Combined 50.4 45.7* 48.0 52.6 50.2** 51.4** 53.4 50.8** 52.1
Patterns 46.4 28.0 34.9 44.7 28.5 34.8 56.6 32.1 41.0
Markers 39.0 34.3 36.5 38.3 36.3 37.2 50.0 41.2** 45.2
Combined 43.0 37.8** 40.3* 41.8 39.6** 40.7* 53.5 44.4** 48.5**
English
German
</table>
<tableCaption confidence="0.975981">
Table 3: 3-way classification results using 5-fold cross-validation. All numbers in percent. Asterisks
indicate significant differences to the pattern-based baseline model (* p&lt;0.10, ** p&lt;0.05).
</tableCaption>
<table confidence="0.999815266666667">
Combined German English
model
P R F1 P R F1
Nouns
SYN–ANT 61.7 55.7 58.5 52.9 44.2 48.2
SYN–HYP 66.5 60.4 63.3 62.2 58.6 60.4
ANT–HYP 70.9 64.6 67.6 59.1 50.6 54.5
Verbs
SYN–ANT 58.9 55.0 56.8 49.6 45.8 47.6
SYN–HYP 67.6 64.0 65.8 66.4 63.0 64.7
ANT–HYP 67.3 66.4 66.9 62.9 60.7 61.8
Adjectives
SYN–ANT 74.8 69.4 72.0 67.0 56.6 61.3
SYN–HYP 58.0 56.1 57.0 56.4 46.0 50.7
ANT–HYP 73.7 70.7 72.2 69.8 57.8 63.2
</table>
<tableCaption confidence="0.972627">
Table 4: 2-way results of the combined model.
</tableCaption>
<bodyText confidence="0.949996764705882">
Bold numbers indicate improvements over both
individual models. All numbers in percent.
in recall and F1-score, leading to the best 3-way
classification results. All gains in recall are sig-
nificant, confirming that the single models in-
deed contribute complementary information. For
example, only the pattern-based model classifies
“intentional”–“accidental” as antonyms, and only
the marker-based model predicts the correct rela-
tion for “double”–“multiple” (hypernymy). The
combined model classifies both pairs correctly.
Table 4 further assesses the strength of the com-
bined model on the 2-way classifications. The
table highlights results indicating improvements
over both individual models. We observe that the
combined model achieves the best recall and F1-
score in 15 out of 18 cases.
</bodyText>
<table confidence="0.9996765">
Relation SYN ANT HYP
Patterns 0.97 0.97 0.94
Markers 0.77* 0.82* 0.91*
Combined 0.93* 0.98 0.96*
</table>
<tableCaption confidence="0.894625">
Table 5: Results in F1-score on the balanced data
set by Yap and Baldwin (* p&lt;0.05).
</tableCaption>
<bodyText confidence="0.999871">
A final experiment is performed on the data set
by Yap and Baldwin (2009) to see whether our
models can also distinguish word pairs of individ-
ual relations from unrelated pairs of words. The
results, listed in Table 5, show that the marker-
based model cannot perform this task as well as
the pattern-based model. The combined model,
however, outperforms both individual models in 2
out of 3 cases. Despite their simplicity, our models
achieve results close to the F1-scores reported by
Yap and Baldwin (0.98–0.99), who employed syn-
tactic pre-processing and an SVM-based classifier,
and experimented with different corpora.
</bodyText>
<sectionHeader confidence="0.99647" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9999276">
In this paper, we proposed to use discourse mark-
ers as indicators for paradigmatic relations be-
tween words and demonstrated that a small set
of such markers can achieve higher recall than a
pattern-based model with tens of thousands of fea-
tures. Combining patterns and markers can further
improve results, leading to significant gains in re-
call and F1. As our new model only relies on a raw
corpus and a fixed list of discourse markers, it can
easily be extended to other languages.
</bodyText>
<sectionHeader confidence="0.989758" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.972989666666667">
The research presented in this paper was funded
by the DFG grant SCHU-2580/2-1 and the DFG
Heiselberg Fellowship SCHU-2580/1-1.
</bodyText>
<page confidence="0.996719">
528
</page>
<sectionHeader confidence="0.91212" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.979555990566038">
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The wacky wide
web: a collection of very large linguistically pro-
cessed web-crawled corpora. Language resources
and evaluation, 43(3):209–226.
Or Biran and Kathleen McKeown. 2013. Aggre-
gated word pair features for implicit discourse rela-
tion disambiguation. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers), pages 69–73,
Sofia, Bulgaria, August.
Kai-Wei Chang, Wen-tau Yih, and Christopher Meek.
2013. Multi-relational latent semantic analysis. In
Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing, pages
1602–1612, Seattle, Washington, USA, October.
Walter G. Charles and George A. Miller. 1989. Con-
texts of antonymous adjectives. Applied Psycholin-
guistics, 10(3):357–375.
Tim Chklovski and Patrick Pantel. 2004. VerbOcean:
Mining the Web for fine-grained semantic verb re-
lations. In Proceedings of the 2004 Conference on
Empirical Methods in Natural Language Process-
ing, Barcelona, Spain, 25–26 July 2004, pages 33–
40.
James Curran. 2003. From Distributional to Semantic
Similarity. Ph.D. thesis, Institute for Communica-
tion and Collaborative Systems, School of Informat-
ics, University of Edinburgh.
Marie-Catherine de Marneffe, Anna N. Rafferty, and
Christopher D. Manning. 2008. Finding contra-
dictions in text. In Proceedings of the 46th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
1039–1047, Columbus, Ohio, USA.
Philip Edmonds and Graeme Hirst. 2002. Near-
synonymy and lexical choice. Computational Lin-
guistics, 28(2):105–144.
Katrin Erk and Sebastian Pad´o. 2008. A structured
vector space model for word meaning in context. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, Waikiki,
Honolulu, Hawaii, 25-27 October 2008.
Christiane Fellbaum. 1995. Co-occurrence and
antonymy. International Journal of Lexicography,
8(4):281–303.
Roxana Girju, Adriana Badulescu, and Dan Moldovan.
2003. Learning semantic constraints for the auto-
matic discovery of part-whole relations. In Proceed-
ings of the Human Language Technology Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics, Edmonton, Al-
berta, Canada, 27 May –1 June 2003, pages 80–87.
Roxana Girju, Adriana Badulescu, and Dan Moldovan.
2006. Automatic discovery of part-whole relations.
Computational Linguistics, 32(1):83–135.
Birgit Hamp and Helmut Feldweg. 1997. GermaNet -
a lexical-semantic net for German. In Proceedings
of the Workshop on Automatic Information Extrac-
tion and Building of Lexical Semantic Resources for
NLP Applications at ACL/EACL-97, Madrid, Spain,
12 July 1997, pages 9–15.
Sanda Harabagiu, Andrew Hickl, and Finley Lacatusu.
2006. Negation, contrast and contradiction in text
processing. In In Proceedings of the 21st National
Conference on Artificial Intelligence, pages 755–
762.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings
of the 15th International Conference on Computa-
tional Linguistics, Nantes, France, 23-28 August
1992, pages 539–545.
Ben Hutchinson. 2004. Acquiring the meaning of dis-
course markers. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Lin-
guistics, Barcelona, Spain, 21–26 July 2004, pages
685–692.
Andreas H. Jucker and Zael Yiv, editors. 1998. Dis-
course Markers: Descriptions and Theory, vol-
ume 57 of Discourse &amp; Beyond New Series. John
Benjamin Publishing Company.
Alessandro Lenci and Giulia Benotto. 2012. Identify-
ing hypernyms in distributional semantic spaces. In
Proceedings of the First Joint Conference on Lexical
and Computational Semantic, pages 75–79.
Dekang Lin, Shaojun Zhao, Lijuan Qin, and Ming
Zhou. 2003. Identifying synonyms among distri-
butionally similar words. In Proceedings of the 18th
International Joint Conference on Artificial Intelli-
gence, pages 1492–1493. Morgan Kaufmann Pub-
lishers Inc.
Saif M. Mohammad, Bonnie Dorr, and Graeme Hirst.
2008. Computing word-pair antonymy. In Proceed-
ings of the 2008 Conference on Empirical Methods
in Natural Language Processing, pages 982–991,
Honolulu, Hawaii, USA.
Saif M. Mohammad, Bonnie J. Dorr, Graeme Hirst, and
Peter D. Turney. 2013. Computing lexical contrast.
Computational Linguistics, 39(3):555–590.
M. Lynne Murphy, Carita Paradis, Caroline Will-
ners, and Steven Jones. 2009. Discourse func-
tions of antonymy: A cross-linguistic investigation
of Swedish and English. Journal of Pragmatics,
41(11):2159–2184.
M. Lynne Murphy. 2003. Semantic relations and the
lexicon. Cambridge University Press.
</reference>
<page confidence="0.993972">
529
</page>
<reference confidence="0.997282556818182">
Patrick Pantel and Marco Pennacchiotti. 2006.
Espresso: Leveraging generic patterns for automati-
cally harvesting semantic relations. In Proceedings
of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, Sydney,
Australia, 17–21 July 2006, pages 113–120.
Renate Pasch, Ursula Brausse, Eva Breindl, and Ulrich
Wassner. 2003. Handbuch der deutschen Konnek-
toren. Walter de Gruyter, Berlin.
Emily Pitler and Ani Nenkova. 2008. Revisiting
readability: A unified framework for predicting text
quality. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 186–195, Honolulu, Hawaii, October.
Emily Pitler, Annie Louis, and Ani Nenkova. 2009.
Automatic sense prediction for implicit discourse re-
lations in text. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 683–691,
Suntec, Singapore, August.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind K Joshi, and Bon-
nie L Webber. 2008. The Penn Discourse Tree-
Bank 2.0. In Proceedings of the Sixth International
Conference on Language Resources and Evaluation
(LREC-2008), Marrakesh, Marocco, May.
Enrico Santus, Alessandro Lenci, Qin Lu, and Sabine
Schulte im Walde. 2014. Chasing hypernyms in
vector spaces with entropy. In Proceedings of the
14th Conference of the European Chapter of the As-
sociation for Computational Linguistics, volume 2:
Short Papers, pages 38–42, Gothenburg, Sweden.
Roland Sch¨afer and Felix Bildhauer. 2012. Build-
ing large corpora from the web using a new effi-
cient tool chain. In Proceedings of the Eighth In-
ternational Conference on Language Resources and
Evaluation (LREC-2012), pages 486–493, Istanbul,
Turkey, May.
Sabine Schulte im Walde and Maximilian K¨oper. 2013.
Pattern-based distinction of paradigmatic relations
for German nouns, verbs, adjectives. In Language
Processing and Knowledge in the Web, pages 184–
198. Springer.
Hinrich Sch¨utze. 1992. Dimensions of meaning. In In
Proceedings of Supercomputing, pages 787–796.
Rion Snow, Daniel Jurafsky, and Andrew Y Ng. 2004.
Learning syntactic patterns for automatic hypernym
discovery. In Advances in Neural Information Pro-
cessing Systems, volume 17, pages 1297–1304.
Peter D. Turney and Patrick Pantel. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37(1):141–188.
Peter D. Turney. 2006. Similarity of semantic rela-
tions. Computational Linguistics, 32(3):379–416.
Lonneke Van der Plas and J¨org Tiedemann. 2006.
Finding synonyms using automatic word alignment
and measures of distributional similarity. In Pro-
ceedings of the COLING/ACL on Main conference
poster sessions, pages 866–873.
Bonnie Webber. 2009. Genre distinctions for dis-
course in the Penn TreeBank. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
674–682, Suntec, Singapore, August.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional
similarity. In Proceedings of the 20th International
Conference on Computational Linguistics, pages
1015–1021.
Willy Yap and Timothy Baldwin. 2009. Experiments
on pattern-based relation learning. In Proceedings
of the 18th ACM Conference on Information and
Knowledge Management, pages 1657–1660.
Alexander Yeh. 2000. More accurate tests for
the statistical significance of result differences.
In Proceedings of the 18th International Confer-
ence on Computational Linguistics, pages 947–953,
Saarbr¨ucken, Germany, August.
Wen-tau Yih, Geoffrey Zweig, and John Platt. 2012.
Polarity inducing latent semantic analysis. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1212–1222, Jeju Island, Korea, July.
</reference>
<page confidence="0.997129">
530
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.485225">
<title confidence="0.9998355">Combining Word Patterns and Discourse Markers for Paradigmatic Relation Classification</title>
<author confidence="0.999824">Michael Roth Sabine Schulte im Walde</author>
<affiliation confidence="0.9760985">ILCC, School of Informatics Institut f¨ur Maschinelle Sprachverarbeitung University of Edinburgh Universit¨at Stuttgart</affiliation>
<email confidence="0.539651">mroth@inf.ed.ac.ukschulte@ims.uni-stuttgart.de</email>
<abstract confidence="0.996911111111111">Distinguishing between paradigmatic relations such as synonymy, antonymy and hypernymy is an important prerequisite in a range of NLP applications. In this paper, we explore discourse relations as an alternative set of features to lexico-syntactic patterns. We demonstrate that statistics over discourse relations, collected via explicit discourse markers as proxies, can be utilized as salient indicators for paradigmatic relations in multiple languages, outperforming patterns in terms of recall and In addition, we observe that markers and patterns provide complementary information, leading to significant classification improvements when applied in combination.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
</authors>
<title>The wacky wide web: a collection of very large linguistically processed web-crawled corpora. Language resources and evaluation,</title>
<date>2009</date>
<pages>43--3</pages>
<contexts>
<context position="8419" citStr="Baroni et al., 2009" startWordPosition="1237" endWordPosition="1240">space model, for example, by only considering patterns of a specific length, weighting by pointwise mutual information and applying thresholds based on frequency and reliability. Baseline Model. We re-implemented the best model from S&amp;K with the same setup: word pairs are represented by vectors, with each entry corresponding to one out of almost 100,000 patterns of lemmatized word forms (e.g., X affect how you Y ). Each value is calculated as the log frequency of the corresponding pattern occurring between the word pairs in a corpus, based on exact match. For English, we use the ukWaC corpus (Baroni et al., 2009); for German, we rely on the COW corpus instead of deWaC, as it is larger and better balanced (Sch¨afer and Bildhauer, 2012). Data Set. The evaluation data set by S&amp;K is a collection of target and response words in German that has been collected via Amazon Mechanical Turk. The data contains a balanced amount of instances across word categories and relations, also taking into account corpus frequency, degree of ambiguity and semantic classes. In total, the 525 P S&amp;K F1 Reimplemented R P R F1 Nouns SYN–ANT 77.4 65.0 70.7 76.7 62.2 68.7 SYN–HYP 75.0 57.0 64.8 73.3 59.5 65.7 Verbs SYN–ANT 70.6 40.</context>
</contexts>
<marker>Baroni, Bernardini, Ferraresi, Zanchetta, 2009</marker>
<rawString>Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The wacky wide web: a collection of very large linguistically processed web-crawled corpora. Language resources and evaluation, 43(3):209–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Or Biran</author>
<author>Kathleen McKeown</author>
</authors>
<title>Aggregated word pair features for implicit discourse relation disambiguation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>69--73</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="11627" citStr="Biran and McKeown, 2013" startWordPosition="1745" endWordPosition="1748">les of discourse relations/markers. information to distinguish between paradigmatic relations. Our approach is motivated by linguistic studies that indicated a connection between discourse relations and lexical relations of words occurring in the respective discourse segments: Murphy et al. (2009) have shown, for example, that antonyms frequently serve as indicators for contrast relations in English and Swedish. More generally, pairs of word tokens have been identified as strong features for classifying discourse relations when no explicit discourse markers are available (Pitler et al., 2009; Biran and McKeown, 2013). Whereas word pairs have frequently been used as features for disambiguating discourse relations, to the best of our knowledge, our approach is novel in that we are the first to apply discourse relations as features for classifying lexical relations. One reason for this might be that discourse relations in general are only available in manually annotated corpora. Previous work has shown, however, that such relations can be classified reliably given the presence of explicit discourse markers.1 We hence rely on such markers as proxies for discourse relations (for examples, cf. Table 2). 4.1 Mod</context>
</contexts>
<marker>Biran, McKeown, 2013</marker>
<rawString>Or Biran and Kathleen McKeown. 2013. Aggregated word pair features for implicit discourse relation disambiguation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 69–73, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai-Wei Chang</author>
<author>Wen-tau Yih</author>
<author>Christopher Meek</author>
</authors>
<title>Multi-relational latent semantic analysis.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1602--1612</pages>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="5412" citStr="Chang et al. (2013)" startWordPosition="786" endWordPosition="789">Mohammad et al., 2008; de Marneffe et al., 2008). Among the few approaches that distinguished between paradigmatic semantic relations, Lin et al. (2003) used patterns and bilingual dictionaries to retrieve distributionally similar words, and relied on clear antonym patterns such as ‘either X or Y’ in a post-processing step to distinguish synonyms from antonyms. The study by Mohammad et al. (2013) on the identification and ranking of opposites also included synonym/antonym distinction. Yih et al. (2012) developed an LSA approach incorporating a thesaurus, to distinguish the same two relations. Chang et al. (2013) extended this approach to induce vector representations that can capture multiple relations. Whereas the above mentioned approaches rely on additional knowledge sources, Turney (2006) developed a corpusbased approach to model relational similarity, addressing (among other tasks) the distinction between synonyms and antonyms. More recently, Schulte im Walde and K¨oper (2013) proposed to distinguish between the three relations antonymy, synonymy and hyponymy based on automatically acquired word patterns. Regarding pattern-based approaches to identify and distinguish lexical semantic relations i</context>
</contexts>
<marker>Chang, Yih, Meek, 2013</marker>
<rawString>Kai-Wei Chang, Wen-tau Yih, and Christopher Meek. 2013. Multi-relational latent semantic analysis. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1602–1612, Seattle, Washington, USA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter G Charles</author>
<author>George A Miller</author>
</authors>
<title>Contexts of antonymous adjectives.</title>
<date>1989</date>
<journal>Applied Psycholinguistics,</journal>
<volume>10</volume>
<issue>3</issue>
<contexts>
<context position="4668" citStr="Charles and Miller, 1989" startWordPosition="673" endWordPosition="676">nd Tiedemann (2006) compared a standard distributional approach against crosslingual alignment; Erk and Pad´o (2008) defined a vector space model to identify synonyms and the substitutability of verbs. Most computational work on hypernyms was performed for nouns, cf. the lexico-syntactic patterns by Hearst (1992) and an extension of the patterns by dependency paths (Snow et al., 2004). Weeds et al. (2004), Lenci and Benotto (2012) and Santus et al. (2014) identified hypernyms in distributional spaces. Computational work on antonyms includes approaches that tested the co-occurrence hypothesis (Charles and Miller, 1989; Fellbaum, 1995), and approaches driven by text understanding efforts and contradiction frameworks (Harabagiu et al., 2006; Mohammad et al., 2008; de Marneffe et al., 2008). Among the few approaches that distinguished between paradigmatic semantic relations, Lin et al. (2003) used patterns and bilingual dictionaries to retrieve distributionally similar words, and relied on clear antonym patterns such as ‘either X or Y’ in a post-processing step to distinguish synonyms from antonyms. The study by Mohammad et al. (2013) on the identification and ranking of opposites also included synonym/antony</context>
</contexts>
<marker>Charles, Miller, 1989</marker>
<rawString>Walter G. Charles and George A. Miller. 1989. Contexts of antonymous adjectives. Applied Psycholinguistics, 10(3):357–375.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Chklovski</author>
<author>Patrick Pantel</author>
</authors>
<title>VerbOcean: Mining the Web for fine-grained semantic verb relations.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>33--40</pages>
<location>Barcelona,</location>
<contexts>
<context position="6457" citStr="Chklovski and Pantel (2004)" startWordPosition="937" endWordPosition="940">e relations antonymy, synonymy and hyponymy based on automatically acquired word patterns. Regarding pattern-based approaches to identify and distinguish lexical semantic relations in more general terms, Hearst (1992) was the first to propose lexico-syntactic patterns as empirical pointers towards relation instances, focusing on hyponymy. Girju et al. (2003) applied a single pattern to distinguish pairs of nouns that are in a causal relationship from those that are not, and Girju et al. (2006) extended the work towards part–whole relations, applying a supervised, knowledge-intensive approach. Chklovski and Pantel (2004) were the first to apply patternbased relation extraction to verbs, distinguishing five non-disjoint relations (similarity, strength, antonymy, enablement, happens-before). Pantel and Pennacchiotti (2006) developed Espresso, a weakly-supervised system that exploits patterns in large-scale web data to distinguish between five noun-noun relations (hypernymy, meronymy, succession, reaction, production). Similarly to Girju et al. (2006), they used generic patterns, but relied on a bootstrapping cycle combined with reliability measures, rather than manual resources. Whereas each of the aforemention</context>
</contexts>
<marker>Chklovski, Pantel, 2004</marker>
<rawString>Tim Chklovski and Patrick Pantel. 2004. VerbOcean: Mining the Web for fine-grained semantic verb relations. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, Barcelona, Spain, 25–26 July 2004, pages 33– 40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Curran</author>
</authors>
<title>From Distributional to Semantic Similarity.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>Institute for Communication and Collaborative Systems, School of Informatics, University of Edinburgh.</institution>
<contexts>
<context position="3731" citStr="Curran (2003)" startWordPosition="540" endWordPosition="542">e model that aims to distinguish between the three paradigmatic relations synonymy, antonymy and hypernymy in German and in English, across the three word classes of nouns, verbs, adjectives. We examine the performance of discourse markers as vector space dimensions in isolation and also explore their contribution in combination with lexical patterns. 2 Related Work As mentioned above, there is a rich tradition of research on identifying a single paradigmatic relations. Work on synonyms includes Edmonds and Hirst (2002), who employed a co-occurrence network and second-order co-occurrence, and Curran (2003), who explored word-based and syntaxbased co-occurrence for thesaurus construction. 524 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 524–530, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Van der Plas and Tiedemann (2006) compared a standard distributional approach against crosslingual alignment; Erk and Pad´o (2008) defined a vector space model to identify synonyms and the substitutability of verbs. Most computational work on hypernyms was performed for nouns, cf. the lexico-syntacti</context>
</contexts>
<marker>Curran, 2003</marker>
<rawString>James Curran. 2003. From Distributional to Semantic Similarity. Ph.D. thesis, Institute for Communication and Collaborative Systems, School of Informatics, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Anna N Rafferty</author>
<author>Christopher D Manning</author>
</authors>
<title>Finding contradictions in text.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1039--1047</pages>
<location>Columbus, Ohio, USA.</location>
<marker>de Marneffe, Rafferty, Manning, 2008</marker>
<rawString>Marie-Catherine de Marneffe, Anna N. Rafferty, and Christopher D. Manning. 2008. Finding contradictions in text. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1039–1047, Columbus, Ohio, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Edmonds</author>
<author>Graeme Hirst</author>
</authors>
<title>Nearsynonymy and lexical choice.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>2</issue>
<contexts>
<context position="3643" citStr="Edmonds and Hirst (2002)" startWordPosition="526" endWordPosition="529">various semantic properties (Hutchinson, 2004). We implement discourse markers within a vector space model that aims to distinguish between the three paradigmatic relations synonymy, antonymy and hypernymy in German and in English, across the three word classes of nouns, verbs, adjectives. We examine the performance of discourse markers as vector space dimensions in isolation and also explore their contribution in combination with lexical patterns. 2 Related Work As mentioned above, there is a rich tradition of research on identifying a single paradigmatic relations. Work on synonyms includes Edmonds and Hirst (2002), who employed a co-occurrence network and second-order co-occurrence, and Curran (2003), who explored word-based and syntaxbased co-occurrence for thesaurus construction. 524 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 524–530, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Van der Plas and Tiedemann (2006) compared a standard distributional approach against crosslingual alignment; Erk and Pad´o (2008) defined a vector space model to identify synonyms and the substitutability of verb</context>
</contexts>
<marker>Edmonds, Hirst, 2002</marker>
<rawString>Philip Edmonds and Graeme Hirst. 2002. Nearsynonymy and lexical choice. Computational Linguistics, 28(2):105–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pad´o</author>
</authors>
<title>A structured vector space model for word meaning in context.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Waikiki, Honolulu, Hawaii,</location>
<marker>Erk, Pad´o, 2008</marker>
<rawString>Katrin Erk and Sebastian Pad´o. 2008. A structured vector space model for word meaning in context. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, Waikiki, Honolulu, Hawaii, 25-27 October 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>Co-occurrence and antonymy.</title>
<date>1995</date>
<journal>International Journal of Lexicography,</journal>
<volume>8</volume>
<issue>4</issue>
<contexts>
<context position="4685" citStr="Fellbaum, 1995" startWordPosition="677" endWordPosition="678">ed a standard distributional approach against crosslingual alignment; Erk and Pad´o (2008) defined a vector space model to identify synonyms and the substitutability of verbs. Most computational work on hypernyms was performed for nouns, cf. the lexico-syntactic patterns by Hearst (1992) and an extension of the patterns by dependency paths (Snow et al., 2004). Weeds et al. (2004), Lenci and Benotto (2012) and Santus et al. (2014) identified hypernyms in distributional spaces. Computational work on antonyms includes approaches that tested the co-occurrence hypothesis (Charles and Miller, 1989; Fellbaum, 1995), and approaches driven by text understanding efforts and contradiction frameworks (Harabagiu et al., 2006; Mohammad et al., 2008; de Marneffe et al., 2008). Among the few approaches that distinguished between paradigmatic semantic relations, Lin et al. (2003) used patterns and bilingual dictionaries to retrieve distributionally similar words, and relied on clear antonym patterns such as ‘either X or Y’ in a post-processing step to distinguish synonyms from antonyms. The study by Mohammad et al. (2013) on the identification and ranking of opposites also included synonym/antonym distinction. Yi</context>
</contexts>
<marker>Fellbaum, 1995</marker>
<rawString>Christiane Fellbaum. 1995. Co-occurrence and antonymy. International Journal of Lexicography, 8(4):281–303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
<author>Adriana Badulescu</author>
<author>Dan Moldovan</author>
</authors>
<title>Learning semantic constraints for the automatic discovery of part-whole relations.</title>
<date>2003</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<volume>1</volume>
<pages>80--87</pages>
<location>Edmonton, Alberta,</location>
<contexts>
<context position="6190" citStr="Girju et al. (2003)" startWordPosition="894" endWordPosition="897">ge sources, Turney (2006) developed a corpusbased approach to model relational similarity, addressing (among other tasks) the distinction between synonyms and antonyms. More recently, Schulte im Walde and K¨oper (2013) proposed to distinguish between the three relations antonymy, synonymy and hyponymy based on automatically acquired word patterns. Regarding pattern-based approaches to identify and distinguish lexical semantic relations in more general terms, Hearst (1992) was the first to propose lexico-syntactic patterns as empirical pointers towards relation instances, focusing on hyponymy. Girju et al. (2003) applied a single pattern to distinguish pairs of nouns that are in a causal relationship from those that are not, and Girju et al. (2006) extended the work towards part–whole relations, applying a supervised, knowledge-intensive approach. Chklovski and Pantel (2004) were the first to apply patternbased relation extraction to verbs, distinguishing five non-disjoint relations (similarity, strength, antonymy, enablement, happens-before). Pantel and Pennacchiotti (2006) developed Espresso, a weakly-supervised system that exploits patterns in large-scale web data to distinguish between five noun-n</context>
</contexts>
<marker>Girju, Badulescu, Moldovan, 2003</marker>
<rawString>Roxana Girju, Adriana Badulescu, and Dan Moldovan. 2003. Learning semantic constraints for the automatic discovery of part-whole relations. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, Edmonton, Alberta, Canada, 27 May –1 June 2003, pages 80–87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
<author>Adriana Badulescu</author>
<author>Dan Moldovan</author>
</authors>
<title>Automatic discovery of part-whole relations.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="6328" citStr="Girju et al. (2006)" startWordPosition="920" endWordPosition="923">between synonyms and antonyms. More recently, Schulte im Walde and K¨oper (2013) proposed to distinguish between the three relations antonymy, synonymy and hyponymy based on automatically acquired word patterns. Regarding pattern-based approaches to identify and distinguish lexical semantic relations in more general terms, Hearst (1992) was the first to propose lexico-syntactic patterns as empirical pointers towards relation instances, focusing on hyponymy. Girju et al. (2003) applied a single pattern to distinguish pairs of nouns that are in a causal relationship from those that are not, and Girju et al. (2006) extended the work towards part–whole relations, applying a supervised, knowledge-intensive approach. Chklovski and Pantel (2004) were the first to apply patternbased relation extraction to verbs, distinguishing five non-disjoint relations (similarity, strength, antonymy, enablement, happens-before). Pantel and Pennacchiotti (2006) developed Espresso, a weakly-supervised system that exploits patterns in large-scale web data to distinguish between five noun-noun relations (hypernymy, meronymy, succession, reaction, production). Similarly to Girju et al. (2006), they used generic patterns, but r</context>
</contexts>
<marker>Girju, Badulescu, Moldovan, 2006</marker>
<rawString>Roxana Girju, Adriana Badulescu, and Dan Moldovan. 2006. Automatic discovery of part-whole relations. Computational Linguistics, 32(1):83–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Birgit Hamp</author>
<author>Helmut Feldweg</author>
</authors>
<title>GermaNet -a lexical-semantic net for German.</title>
<date>1997</date>
<booktitle>In Proceedings of the Workshop on Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications at ACL/EACL-97,</booktitle>
<pages>9--15</pages>
<location>Madrid,</location>
<contexts>
<context position="14541" citStr="Hamp and Feldweg, 1997" startWordPosition="2205" endWordPosition="2208">to an exponentially growing feature space. In contrast, the set of discourse markers in our work is fixed: for English, we use 61 markers annotated in the Penn Discourse TreeBank 2.0 (Prasad et al., 2008); for German, we use 155 one-word translations of the English markers, as obtained from an online dictionary.23 Taking directionality into account, our vector space model consists of 2x61 and 2x155 features, respectively. 4.2 Development Set and Hyperparameters We select the hyperparameters of our model using an independent development set, which we extract from the lexical resource GermaNet (Hamp and Feldweg, 1997). For each considered word category, we extract instances of synonymy, antonymy and hypernymy. In total, 1502 instances are identified, with 64 of them overlapping with the evaluation data set described in Section 3. Note though that the development set is not used for evaluation but only to select the following hyperparameters. We experimented with different vector values (absolute frequency, log frequency, pointwise mutual information (PMI)), distance measures (cosine, euclidean) and normalization schemes. In contrast to S&amp;K, who did not observe any improvements using PMI, we found it to per</context>
</contexts>
<marker>Hamp, Feldweg, 1997</marker>
<rawString>Birgit Hamp and Helmut Feldweg. 1997. GermaNet -a lexical-semantic net for German. In Proceedings of the Workshop on Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications at ACL/EACL-97, Madrid, Spain, 12 July 1997, pages 9–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
<author>Andrew Hickl</author>
<author>Finley Lacatusu</author>
</authors>
<title>Negation, contrast and contradiction in text processing.</title>
<date>2006</date>
<booktitle>In In Proceedings of the 21st National Conference on Artificial Intelligence,</booktitle>
<pages>755--762</pages>
<contexts>
<context position="4791" citStr="Harabagiu et al., 2006" startWordPosition="690" endWordPosition="693">a vector space model to identify synonyms and the substitutability of verbs. Most computational work on hypernyms was performed for nouns, cf. the lexico-syntactic patterns by Hearst (1992) and an extension of the patterns by dependency paths (Snow et al., 2004). Weeds et al. (2004), Lenci and Benotto (2012) and Santus et al. (2014) identified hypernyms in distributional spaces. Computational work on antonyms includes approaches that tested the co-occurrence hypothesis (Charles and Miller, 1989; Fellbaum, 1995), and approaches driven by text understanding efforts and contradiction frameworks (Harabagiu et al., 2006; Mohammad et al., 2008; de Marneffe et al., 2008). Among the few approaches that distinguished between paradigmatic semantic relations, Lin et al. (2003) used patterns and bilingual dictionaries to retrieve distributionally similar words, and relied on clear antonym patterns such as ‘either X or Y’ in a post-processing step to distinguish synonyms from antonyms. The study by Mohammad et al. (2013) on the identification and ranking of opposites also included synonym/antonym distinction. Yih et al. (2012) developed an LSA approach incorporating a thesaurus, to distinguish the same two relations</context>
</contexts>
<marker>Harabagiu, Hickl, Lacatusu, 2006</marker>
<rawString>Sanda Harabagiu, Andrew Hickl, and Finley Lacatusu. 2006. Negation, contrast and contradiction in text processing. In In Proceedings of the 21st National Conference on Artificial Intelligence, pages 755– 762.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics,</booktitle>
<pages>539--545</pages>
<location>Nantes,</location>
<contexts>
<context position="4358" citStr="Hearst (1992)" startWordPosition="627" endWordPosition="628"> word-based and syntaxbased co-occurrence for thesaurus construction. 524 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 524–530, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Van der Plas and Tiedemann (2006) compared a standard distributional approach against crosslingual alignment; Erk and Pad´o (2008) defined a vector space model to identify synonyms and the substitutability of verbs. Most computational work on hypernyms was performed for nouns, cf. the lexico-syntactic patterns by Hearst (1992) and an extension of the patterns by dependency paths (Snow et al., 2004). Weeds et al. (2004), Lenci and Benotto (2012) and Santus et al. (2014) identified hypernyms in distributional spaces. Computational work on antonyms includes approaches that tested the co-occurrence hypothesis (Charles and Miller, 1989; Fellbaum, 1995), and approaches driven by text understanding efforts and contradiction frameworks (Harabagiu et al., 2006; Mohammad et al., 2008; de Marneffe et al., 2008). Among the few approaches that distinguished between paradigmatic semantic relations, Lin et al. (2003) used pattern</context>
<context position="6047" citStr="Hearst (1992)" startWordPosition="876" endWordPosition="877">h to induce vector representations that can capture multiple relations. Whereas the above mentioned approaches rely on additional knowledge sources, Turney (2006) developed a corpusbased approach to model relational similarity, addressing (among other tasks) the distinction between synonyms and antonyms. More recently, Schulte im Walde and K¨oper (2013) proposed to distinguish between the three relations antonymy, synonymy and hyponymy based on automatically acquired word patterns. Regarding pattern-based approaches to identify and distinguish lexical semantic relations in more general terms, Hearst (1992) was the first to propose lexico-syntactic patterns as empirical pointers towards relation instances, focusing on hyponymy. Girju et al. (2003) applied a single pattern to distinguish pairs of nouns that are in a causal relationship from those that are not, and Girju et al. (2006) extended the work towards part–whole relations, applying a supervised, knowledge-intensive approach. Chklovski and Pantel (2004) were the first to apply patternbased relation extraction to verbs, distinguishing five non-disjoint relations (similarity, strength, antonymy, enablement, happens-before). Pantel and Pennac</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the 15th International Conference on Computational Linguistics, Nantes, France, 23-28 August 1992, pages 539–545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Hutchinson</author>
</authors>
<title>Acquiring the meaning of discourse markers.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>685--692</pages>
<location>Barcelona,</location>
<contexts>
<context position="3065" citStr="Hutchinson, 2004" startWordPosition="440" endWordPosition="441">acted from text. Each option comes with its own shortcomings: knowledge bases, on the one hand, are typically developed for a single language or domain, meaning that they might not generalize well; word patterns, on the other hand, are noisy and can be sparse for infrequent word pairs. In this paper, we propose to strike a balance between availability and restrictedness by making use of discourse markers. This approach has several advantages: markers are frequently found across genres (Webber, 2009), they exist in many languages (Jucker and Yiv, 1998), and capture various semantic properties (Hutchinson, 2004). We implement discourse markers within a vector space model that aims to distinguish between the three paradigmatic relations synonymy, antonymy and hypernymy in German and in English, across the three word classes of nouns, verbs, adjectives. We examine the performance of discourse markers as vector space dimensions in isolation and also explore their contribution in combination with lexical patterns. 2 Related Work As mentioned above, there is a rich tradition of research on identifying a single paradigmatic relations. Work on synonyms includes Edmonds and Hirst (2002), who employed a co-oc</context>
</contexts>
<marker>Hutchinson, 2004</marker>
<rawString>Ben Hutchinson. 2004. Acquiring the meaning of discourse markers. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, Barcelona, Spain, 21–26 July 2004, pages 685–692.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Andreas</author>
</authors>
<title>Jucker and Zael Yiv, editors.</title>
<date>1998</date>
<booktitle>Discourse Markers: Descriptions and Theory,</booktitle>
<volume>57</volume>
<publisher>John Benjamin Publishing Company.</publisher>
<marker>Andreas, 1998</marker>
<rawString>Andreas H. Jucker and Zael Yiv, editors. 1998. Discourse Markers: Descriptions and Theory, volume 57 of Discourse &amp; Beyond New Series. John Benjamin Publishing Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Lenci</author>
<author>Giulia Benotto</author>
</authors>
<title>Identifying hypernyms in distributional semantic spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantic,</booktitle>
<pages>75--79</pages>
<contexts>
<context position="4478" citStr="Lenci and Benotto (2012)" startWordPosition="646" endWordPosition="649">ing of the Association for Computational Linguistics (Short Papers), pages 524–530, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Van der Plas and Tiedemann (2006) compared a standard distributional approach against crosslingual alignment; Erk and Pad´o (2008) defined a vector space model to identify synonyms and the substitutability of verbs. Most computational work on hypernyms was performed for nouns, cf. the lexico-syntactic patterns by Hearst (1992) and an extension of the patterns by dependency paths (Snow et al., 2004). Weeds et al. (2004), Lenci and Benotto (2012) and Santus et al. (2014) identified hypernyms in distributional spaces. Computational work on antonyms includes approaches that tested the co-occurrence hypothesis (Charles and Miller, 1989; Fellbaum, 1995), and approaches driven by text understanding efforts and contradiction frameworks (Harabagiu et al., 2006; Mohammad et al., 2008; de Marneffe et al., 2008). Among the few approaches that distinguished between paradigmatic semantic relations, Lin et al. (2003) used patterns and bilingual dictionaries to retrieve distributionally similar words, and relied on clear antonym patterns such as ‘e</context>
</contexts>
<marker>Lenci, Benotto, 2012</marker>
<rawString>Alessandro Lenci and Giulia Benotto. 2012. Identifying hypernyms in distributional semantic spaces. In Proceedings of the First Joint Conference on Lexical and Computational Semantic, pages 75–79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Shaojun Zhao</author>
<author>Lijuan Qin</author>
<author>Ming Zhou</author>
</authors>
<title>Identifying synonyms among distributionally similar words.</title>
<date>2003</date>
<booktitle>In Proceedings of the 18th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1492--1493</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<contexts>
<context position="4945" citStr="Lin et al. (2003)" startWordPosition="714" endWordPosition="717">ctic patterns by Hearst (1992) and an extension of the patterns by dependency paths (Snow et al., 2004). Weeds et al. (2004), Lenci and Benotto (2012) and Santus et al. (2014) identified hypernyms in distributional spaces. Computational work on antonyms includes approaches that tested the co-occurrence hypothesis (Charles and Miller, 1989; Fellbaum, 1995), and approaches driven by text understanding efforts and contradiction frameworks (Harabagiu et al., 2006; Mohammad et al., 2008; de Marneffe et al., 2008). Among the few approaches that distinguished between paradigmatic semantic relations, Lin et al. (2003) used patterns and bilingual dictionaries to retrieve distributionally similar words, and relied on clear antonym patterns such as ‘either X or Y’ in a post-processing step to distinguish synonyms from antonyms. The study by Mohammad et al. (2013) on the identification and ranking of opposites also included synonym/antonym distinction. Yih et al. (2012) developed an LSA approach incorporating a thesaurus, to distinguish the same two relations. Chang et al. (2013) extended this approach to induce vector representations that can capture multiple relations. Whereas the above mentioned approaches </context>
</contexts>
<marker>Lin, Zhao, Qin, Zhou, 2003</marker>
<rawString>Dekang Lin, Shaojun Zhao, Lijuan Qin, and Ming Zhou. 2003. Identifying synonyms among distributionally similar words. In Proceedings of the 18th International Joint Conference on Artificial Intelligence, pages 1492–1493. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif M Mohammad</author>
<author>Bonnie Dorr</author>
<author>Graeme Hirst</author>
</authors>
<title>Computing word-pair antonymy.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>982--991</pages>
<location>Honolulu, Hawaii, USA.</location>
<contexts>
<context position="4814" citStr="Mohammad et al., 2008" startWordPosition="694" endWordPosition="698">identify synonyms and the substitutability of verbs. Most computational work on hypernyms was performed for nouns, cf. the lexico-syntactic patterns by Hearst (1992) and an extension of the patterns by dependency paths (Snow et al., 2004). Weeds et al. (2004), Lenci and Benotto (2012) and Santus et al. (2014) identified hypernyms in distributional spaces. Computational work on antonyms includes approaches that tested the co-occurrence hypothesis (Charles and Miller, 1989; Fellbaum, 1995), and approaches driven by text understanding efforts and contradiction frameworks (Harabagiu et al., 2006; Mohammad et al., 2008; de Marneffe et al., 2008). Among the few approaches that distinguished between paradigmatic semantic relations, Lin et al. (2003) used patterns and bilingual dictionaries to retrieve distributionally similar words, and relied on clear antonym patterns such as ‘either X or Y’ in a post-processing step to distinguish synonyms from antonyms. The study by Mohammad et al. (2013) on the identification and ranking of opposites also included synonym/antonym distinction. Yih et al. (2012) developed an LSA approach incorporating a thesaurus, to distinguish the same two relations. Chang et al. (2013) e</context>
</contexts>
<marker>Mohammad, Dorr, Hirst, 2008</marker>
<rawString>Saif M. Mohammad, Bonnie Dorr, and Graeme Hirst. 2008. Computing word-pair antonymy. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 982–991, Honolulu, Hawaii, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif M Mohammad</author>
<author>Bonnie J Dorr</author>
<author>Graeme Hirst</author>
<author>Peter D Turney</author>
</authors>
<title>Computing lexical contrast.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>3</issue>
<contexts>
<context position="5192" citStr="Mohammad et al. (2013)" startWordPosition="752" endWordPosition="755">on antonyms includes approaches that tested the co-occurrence hypothesis (Charles and Miller, 1989; Fellbaum, 1995), and approaches driven by text understanding efforts and contradiction frameworks (Harabagiu et al., 2006; Mohammad et al., 2008; de Marneffe et al., 2008). Among the few approaches that distinguished between paradigmatic semantic relations, Lin et al. (2003) used patterns and bilingual dictionaries to retrieve distributionally similar words, and relied on clear antonym patterns such as ‘either X or Y’ in a post-processing step to distinguish synonyms from antonyms. The study by Mohammad et al. (2013) on the identification and ranking of opposites also included synonym/antonym distinction. Yih et al. (2012) developed an LSA approach incorporating a thesaurus, to distinguish the same two relations. Chang et al. (2013) extended this approach to induce vector representations that can capture multiple relations. Whereas the above mentioned approaches rely on additional knowledge sources, Turney (2006) developed a corpusbased approach to model relational similarity, addressing (among other tasks) the distinction between synonyms and antonyms. More recently, Schulte im Walde and K¨oper (2013) pr</context>
</contexts>
<marker>Mohammad, Dorr, Hirst, Turney, 2013</marker>
<rawString>Saif M. Mohammad, Bonnie J. Dorr, Graeme Hirst, and Peter D. Turney. 2013. Computing lexical contrast. Computational Linguistics, 39(3):555–590.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lynne Murphy</author>
<author>Carita Paradis</author>
<author>Caroline Willners</author>
<author>Steven Jones</author>
</authors>
<title>Discourse functions of antonymy: A cross-linguistic investigation of Swedish and English.</title>
<date>2009</date>
<journal>Journal of Pragmatics,</journal>
<volume>41</volume>
<issue>11</issue>
<contexts>
<context position="11301" citStr="Murphy et al. (2009)" startWordPosition="1694" endWordPosition="1698">jectives, and covering synonymy, antonymy, hypernymy. 4 Markers for Relation Classification The aim of this work is to establish corpus statistics over discourse relations as a salient source of CONTRAST but, altough, rather ... RESTATEMENT indeed, specifically,... INSTANTIATION (for) example, instance,... Table 2: Examples of discourse relations/markers. information to distinguish between paradigmatic relations. Our approach is motivated by linguistic studies that indicated a connection between discourse relations and lexical relations of words occurring in the respective discourse segments: Murphy et al. (2009) have shown, for example, that antonyms frequently serve as indicators for contrast relations in English and Swedish. More generally, pairs of word tokens have been identified as strong features for classifying discourse relations when no explicit discourse markers are available (Pitler et al., 2009; Biran and McKeown, 2013). Whereas word pairs have frequently been used as features for disambiguating discourse relations, to the best of our knowledge, our approach is novel in that we are the first to apply discourse relations as features for classifying lexical relations. One reason for this mi</context>
</contexts>
<marker>Murphy, Paradis, Willners, Jones, 2009</marker>
<rawString>M. Lynne Murphy, Carita Paradis, Caroline Willners, and Steven Jones. 2009. Discourse functions of antonymy: A cross-linguistic investigation of Swedish and English. Journal of Pragmatics, 41(11):2159–2184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lynne Murphy</author>
</authors>
<title>Semantic relations and the lexicon.</title>
<date>2003</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="1076" citStr="Murphy, 2003" startWordPosition="140" endWordPosition="141"> we explore discourse relations as an alternative set of features to lexico-syntactic patterns. We demonstrate that statistics over discourse relations, collected via explicit discourse markers as proxies, can be utilized as salient indicators for paradigmatic relations in multiple languages, outperforming patterns in terms of recall and Fl-score. In addition, we observe that markers and patterns provide complementary information, leading to significant classification improvements when applied in combination. 1 Introduction Paradigmatic relations (such as synonymy, antonymy and hypernymy; cf. Murphy, 2003) are notoriously difficult to distinguish automatically, as first-order co-occurrences of the related words tend to be very similar across the relations. For example, in The boy/girl/person loves/hates the cat, the nominal co-hyponyms boy, girl and their hypernym person as well as the verbal antonyms love and hate occur in identical contexts, respectively. Vector space models, which represent words by frequencies of co-occurring words to enable comparisons in terms of distributional similarity (Sch¨utze, 1992; Turney and Pantel, 2010), hence perform below their potential when inferring the typ</context>
</contexts>
<marker>Murphy, 2003</marker>
<rawString>M. Lynne Murphy. 2003. Semantic relations and the lexicon. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Marco Pennacchiotti</author>
</authors>
<title>Espresso: Leveraging generic patterns for automatically harvesting semantic relations.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>113--120</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="6661" citStr="Pantel and Pennacchiotti (2006)" startWordPosition="962" endWordPosition="965">ms, Hearst (1992) was the first to propose lexico-syntactic patterns as empirical pointers towards relation instances, focusing on hyponymy. Girju et al. (2003) applied a single pattern to distinguish pairs of nouns that are in a causal relationship from those that are not, and Girju et al. (2006) extended the work towards part–whole relations, applying a supervised, knowledge-intensive approach. Chklovski and Pantel (2004) were the first to apply patternbased relation extraction to verbs, distinguishing five non-disjoint relations (similarity, strength, antonymy, enablement, happens-before). Pantel and Pennacchiotti (2006) developed Espresso, a weakly-supervised system that exploits patterns in large-scale web data to distinguish between five noun-noun relations (hypernymy, meronymy, succession, reaction, production). Similarly to Girju et al. (2006), they used generic patterns, but relied on a bootstrapping cycle combined with reliability measures, rather than manual resources. Whereas each of the aforementioned approaches considers only one word class and clearly disjoint categories, we distinguish between paradigmatic relations that can be distributionally very similar and propose a unified framework for nou</context>
</contexts>
<marker>Pantel, Pennacchiotti, 2006</marker>
<rawString>Patrick Pantel and Marco Pennacchiotti. 2006. Espresso: Leveraging generic patterns for automatically harvesting semantic relations. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, Sydney, Australia, 17–21 July 2006, pages 113–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Renate Pasch</author>
<author>Ursula Brausse</author>
<author>Eva Breindl</author>
<author>Ulrich Wassner</author>
</authors>
<date>2003</date>
<booktitle>Handbuch der deutschen Konnektoren. Walter de Gruyter,</booktitle>
<location>Berlin.</location>
<contexts>
<context position="15551" citStr="Pasch et al. (2003)" startWordPosition="2360" endWordPosition="2363">ency, log frequency, pointwise mutual information (PMI)), distance measures (cosine, euclidean) and normalization schemes. In contrast to S&amp;K, who did not observe any improvements using PMI, we found it to perform best, combined with euclidean distance and no additional normalization. This finding might be an immediate effect of discourse markers being 2http://dict.leo.org 3We also experimented with larger sets of markers, including conjunctions and adverbials in sentence-initial positions, but did not notice any considerable effect. Future work could use manual sets of markers, e.g. those by Pasch et al. (2003), though such sets are only available in few languages. generally more frequent than strict word patterns, which also leads to more reliable PMI values. 5 Evaluation In our evaluation, we assess the performance of the marker-based model and demonstrate the benefits of incorporating discourse markers into a patternbased model, which we apply as a baseline. We evaluate on several data sets: the collection of target-response pairs in German from previous work, and a similar data set that was collected for English target words (cf. Section 3); for comparison reasons, we also apply our models to th</context>
</contexts>
<marker>Pasch, Brausse, Breindl, Wassner, 2003</marker>
<rawString>Renate Pasch, Ursula Brausse, Eva Breindl, and Ulrich Wassner. 2003. Handbuch der deutschen Konnektoren. Walter de Gruyter, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Ani Nenkova</author>
</authors>
<title>Revisiting readability: A unified framework for predicting text quality.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>186--195</pages>
<location>Honolulu, Hawaii,</location>
<marker>Pitler, Nenkova, 2008</marker>
<rawString>Emily Pitler and Ani Nenkova. 2008. Revisiting readability: A unified framework for predicting text quality. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 186–195, Honolulu, Hawaii, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Annie Louis</author>
<author>Ani Nenkova</author>
</authors>
<title>Automatic sense prediction for implicit discourse relations in text.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>683--691</pages>
<location>Suntec, Singapore,</location>
<contexts>
<context position="11601" citStr="Pitler et al., 2009" startWordPosition="1741" endWordPosition="1744">ce,... Table 2: Examples of discourse relations/markers. information to distinguish between paradigmatic relations. Our approach is motivated by linguistic studies that indicated a connection between discourse relations and lexical relations of words occurring in the respective discourse segments: Murphy et al. (2009) have shown, for example, that antonyms frequently serve as indicators for contrast relations in English and Swedish. More generally, pairs of word tokens have been identified as strong features for classifying discourse relations when no explicit discourse markers are available (Pitler et al., 2009; Biran and McKeown, 2013). Whereas word pairs have frequently been used as features for disambiguating discourse relations, to the best of our knowledge, our approach is novel in that we are the first to apply discourse relations as features for classifying lexical relations. One reason for this might be that discourse relations in general are only available in manually annotated corpora. Previous work has shown, however, that such relations can be classified reliably given the presence of explicit discourse markers.1 We hence rely on such markers as proxies for discourse relations (for examp</context>
</contexts>
<marker>Pitler, Louis, Nenkova, 2009</marker>
<rawString>Emily Pitler, Annie Louis, and Ani Nenkova. 2009. Automatic sense prediction for implicit discourse relations in text. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 683–691, Suntec, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Nikhil Dinesh</author>
<author>Alan Lee</author>
<author>Eleni Miltsakaki</author>
<author>Livio Robaldo</author>
<author>Aravind K Joshi</author>
<author>Bonnie L Webber</author>
</authors>
<title>The Penn Discourse TreeBank 2.0.</title>
<date>2008</date>
<booktitle>In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC-2008),</booktitle>
<location>Marrakesh, Marocco,</location>
<contexts>
<context position="14122" citStr="Prasad et al., 2008" startWordPosition="2143" endWordPosition="2146">o the pattern “X * though * Y ”). Yet, our specific choice of features has several advantages: Whereas strict and potentially long patterns can be rare in text, discourse markers such as “however”, “for example” and “additionally” are frequently found across genres (Webber, 2009). Although combinations of tokens could also be replaced by wild cards in any automatically acquired pattern, this would generally lead to an exponentially growing feature space. In contrast, the set of discourse markers in our work is fixed: for English, we use 61 markers annotated in the Penn Discourse TreeBank 2.0 (Prasad et al., 2008); for German, we use 155 one-word translations of the English markers, as obtained from an online dictionary.23 Taking directionality into account, our vector space model consists of 2x61 and 2x155 features, respectively. 4.2 Development Set and Hyperparameters We select the hyperparameters of our model using an independent development set, which we extract from the lexical resource GermaNet (Hamp and Feldweg, 1997). For each considered word category, we extract instances of synonymy, antonymy and hypernymy. In total, 1502 instances are identified, with 64 of them overlapping with the evaluati</context>
</contexts>
<marker>Prasad, Dinesh, Lee, Miltsakaki, Robaldo, Joshi, Webber, 2008</marker>
<rawString>Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind K Joshi, and Bonnie L Webber. 2008. The Penn Discourse TreeBank 2.0. In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC-2008), Marrakesh, Marocco, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Enrico Santus</author>
<author>Alessandro Lenci</author>
<author>Qin Lu</author>
<author>Sabine Schulte im Walde</author>
</authors>
<title>Chasing hypernyms in vector spaces with entropy.</title>
<date>2014</date>
<booktitle>In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<volume>volume</volume>
<pages>38--42</pages>
<location>Gothenburg,</location>
<contexts>
<context position="4503" citStr="Santus et al. (2014)" startWordPosition="651" endWordPosition="654">mputational Linguistics (Short Papers), pages 524–530, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Van der Plas and Tiedemann (2006) compared a standard distributional approach against crosslingual alignment; Erk and Pad´o (2008) defined a vector space model to identify synonyms and the substitutability of verbs. Most computational work on hypernyms was performed for nouns, cf. the lexico-syntactic patterns by Hearst (1992) and an extension of the patterns by dependency paths (Snow et al., 2004). Weeds et al. (2004), Lenci and Benotto (2012) and Santus et al. (2014) identified hypernyms in distributional spaces. Computational work on antonyms includes approaches that tested the co-occurrence hypothesis (Charles and Miller, 1989; Fellbaum, 1995), and approaches driven by text understanding efforts and contradiction frameworks (Harabagiu et al., 2006; Mohammad et al., 2008; de Marneffe et al., 2008). Among the few approaches that distinguished between paradigmatic semantic relations, Lin et al. (2003) used patterns and bilingual dictionaries to retrieve distributionally similar words, and relied on clear antonym patterns such as ‘either X or Y’ in a post-p</context>
</contexts>
<marker>Santus, Lenci, Lu, Walde, 2014</marker>
<rawString>Enrico Santus, Alessandro Lenci, Qin Lu, and Sabine Schulte im Walde. 2014. Chasing hypernyms in vector spaces with entropy. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, volume 2: Short Papers, pages 38–42, Gothenburg, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roland Sch¨afer</author>
<author>Felix Bildhauer</author>
</authors>
<title>Building large corpora from the web using a new efficient tool chain.</title>
<date>2012</date>
<booktitle>In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC-2012),</booktitle>
<pages>486--493</pages>
<location>Istanbul, Turkey,</location>
<marker>Sch¨afer, Bildhauer, 2012</marker>
<rawString>Roland Sch¨afer and Felix Bildhauer. 2012. Building large corpora from the web using a new efficient tool chain. In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC-2012), pages 486–493, Istanbul, Turkey, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Schulte im Walde</author>
<author>Maximilian K¨oper</author>
</authors>
<title>Pattern-based distinction of paradigmatic relations for German nouns, verbs, adjectives.</title>
<date>2013</date>
<booktitle>In Language Processing and Knowledge in the Web,</booktitle>
<pages>184--198</pages>
<publisher>Springer.</publisher>
<marker>Walde, K¨oper, 2013</marker>
<rawString>Sabine Schulte im Walde and Maximilian K¨oper. 2013. Pattern-based distinction of paradigmatic relations for German nouns, verbs, adjectives. In Language Processing and Knowledge in the Web, pages 184– 198. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Dimensions of meaning. In</title>
<date>1992</date>
<booktitle>In Proceedings of Supercomputing,</booktitle>
<pages>787--796</pages>
<marker>Sch¨utze, 1992</marker>
<rawString>Hinrich Sch¨utze. 1992. Dimensions of meaning. In In Proceedings of Supercomputing, pages 787–796.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning syntactic patterns for automatic hypernym discovery.</title>
<date>2004</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<volume>17</volume>
<pages>1297--1304</pages>
<contexts>
<context position="4431" citStr="Snow et al., 2004" startWordPosition="638" endWordPosition="641">. 524 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 524–530, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Van der Plas and Tiedemann (2006) compared a standard distributional approach against crosslingual alignment; Erk and Pad´o (2008) defined a vector space model to identify synonyms and the substitutability of verbs. Most computational work on hypernyms was performed for nouns, cf. the lexico-syntactic patterns by Hearst (1992) and an extension of the patterns by dependency paths (Snow et al., 2004). Weeds et al. (2004), Lenci and Benotto (2012) and Santus et al. (2014) identified hypernyms in distributional spaces. Computational work on antonyms includes approaches that tested the co-occurrence hypothesis (Charles and Miller, 1989; Fellbaum, 1995), and approaches driven by text understanding efforts and contradiction frameworks (Harabagiu et al., 2006; Mohammad et al., 2008; de Marneffe et al., 2008). Among the few approaches that distinguished between paradigmatic semantic relations, Lin et al. (2003) used patterns and bilingual dictionaries to retrieve distributionally similar words, </context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2004</marker>
<rawString>Rion Snow, Daniel Jurafsky, and Andrew Y Ng. 2004. Learning syntactic patterns for automatic hypernym discovery. In Advances in Neural Information Processing Systems, volume 17, pages 1297–1304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="1616" citStr="Turney and Pantel, 2010" startWordPosition="216" endWordPosition="219"> Paradigmatic relations (such as synonymy, antonymy and hypernymy; cf. Murphy, 2003) are notoriously difficult to distinguish automatically, as first-order co-occurrences of the related words tend to be very similar across the relations. For example, in The boy/girl/person loves/hates the cat, the nominal co-hyponyms boy, girl and their hypernym person as well as the verbal antonyms love and hate occur in identical contexts, respectively. Vector space models, which represent words by frequencies of co-occurring words to enable comparisons in terms of distributional similarity (Sch¨utze, 1992; Turney and Pantel, 2010), hence perform below their potential when inferring the type of relation that holds between two words. This distinction is crucial, however, in a range of tasks: in sentiment analysis, for example, words of the same and opposing polarity need to be distinguished; in textual entailment, systems further need to identify hypernymy because of directional inference requirements. Accordingly, while there is a rich tradition on identifying word pairs of a single paradigmatic relation, there is little work that has addressed the distinction between two or more paradigmatic relations (cf. Section 2 fo</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37(1):141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Similarity of semantic relations.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>3</issue>
<contexts>
<context position="5596" citStr="Turney (2006)" startWordPosition="813" endWordPosition="814">ies to retrieve distributionally similar words, and relied on clear antonym patterns such as ‘either X or Y’ in a post-processing step to distinguish synonyms from antonyms. The study by Mohammad et al. (2013) on the identification and ranking of opposites also included synonym/antonym distinction. Yih et al. (2012) developed an LSA approach incorporating a thesaurus, to distinguish the same two relations. Chang et al. (2013) extended this approach to induce vector representations that can capture multiple relations. Whereas the above mentioned approaches rely on additional knowledge sources, Turney (2006) developed a corpusbased approach to model relational similarity, addressing (among other tasks) the distinction between synonyms and antonyms. More recently, Schulte im Walde and K¨oper (2013) proposed to distinguish between the three relations antonymy, synonymy and hyponymy based on automatically acquired word patterns. Regarding pattern-based approaches to identify and distinguish lexical semantic relations in more general terms, Hearst (1992) was the first to propose lexico-syntactic patterns as empirical pointers towards relation instances, focusing on hyponymy. Girju et al. (2003) appli</context>
</contexts>
<marker>Turney, 2006</marker>
<rawString>Peter D. Turney. 2006. Similarity of semantic relations. Computational Linguistics, 32(3):379–416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lonneke Van der Plas</author>
<author>J¨org Tiedemann</author>
</authors>
<title>Finding synonyms using automatic word alignment and measures of distributional similarity.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL on Main conference poster sessions,</booktitle>
<pages>866--873</pages>
<marker>Van der Plas, Tiedemann, 2006</marker>
<rawString>Lonneke Van der Plas and J¨org Tiedemann. 2006. Finding synonyms using automatic word alignment and measures of distributional similarity. In Proceedings of the COLING/ACL on Main conference poster sessions, pages 866–873.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Webber</author>
</authors>
<title>Genre distinctions for discourse in the Penn TreeBank.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>674--682</pages>
<location>Suntec, Singapore,</location>
<contexts>
<context position="2952" citStr="Webber, 2009" startWordPosition="424" endWordPosition="425">tly relied on manually created knowledge sources, or lexico-syntactic patterns that can be automatically extracted from text. Each option comes with its own shortcomings: knowledge bases, on the one hand, are typically developed for a single language or domain, meaning that they might not generalize well; word patterns, on the other hand, are noisy and can be sparse for infrequent word pairs. In this paper, we propose to strike a balance between availability and restrictedness by making use of discourse markers. This approach has several advantages: markers are frequently found across genres (Webber, 2009), they exist in many languages (Jucker and Yiv, 1998), and capture various semantic properties (Hutchinson, 2004). We implement discourse markers within a vector space model that aims to distinguish between the three paradigmatic relations synonymy, antonymy and hypernymy in German and in English, across the three word classes of nouns, verbs, adjectives. We examine the performance of discourse markers as vector space dimensions in isolation and also explore their contribution in combination with lexical patterns. 2 Related Work As mentioned above, there is a rich tradition of research on iden</context>
<context position="13782" citStr="Webber, 2009" startWordPosition="2087" endWordPosition="2088">kers. Since discourse relations are typically directed, we take into consideration whether a word occurs to the left or to the right of the respective marker. Accordingly, the features of our model are special cases of single-word patterns with an arbitrary number of wild card tokens (e.g., the marker feature ‘though’ corresponds to the pattern “X * though * Y ”). Yet, our specific choice of features has several advantages: Whereas strict and potentially long patterns can be rare in text, discourse markers such as “however”, “for example” and “additionally” are frequently found across genres (Webber, 2009). Although combinations of tokens could also be replaced by wild cards in any automatically acquired pattern, this would generally lead to an exponentially growing feature space. In contrast, the set of discourse markers in our work is fixed: for English, we use 61 markers annotated in the Penn Discourse TreeBank 2.0 (Prasad et al., 2008); for German, we use 155 one-word translations of the English markers, as obtained from an online dictionary.23 Taking directionality into account, our vector space model consists of 2x61 and 2x155 features, respectively. 4.2 Development Set and Hyperparameter</context>
</contexts>
<marker>Webber, 2009</marker>
<rawString>Bonnie Webber. 2009. Genre distinctions for discourse in the Penn TreeBank. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 674–682, Suntec, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Weeds</author>
<author>David Weir</author>
<author>Diana McCarthy</author>
</authors>
<title>Characterising measures of lexical distributional similarity.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics,</booktitle>
<pages>1015--1021</pages>
<contexts>
<context position="4452" citStr="Weeds et al. (2004)" startWordPosition="642" endWordPosition="645"> the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 524–530, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Van der Plas and Tiedemann (2006) compared a standard distributional approach against crosslingual alignment; Erk and Pad´o (2008) defined a vector space model to identify synonyms and the substitutability of verbs. Most computational work on hypernyms was performed for nouns, cf. the lexico-syntactic patterns by Hearst (1992) and an extension of the patterns by dependency paths (Snow et al., 2004). Weeds et al. (2004), Lenci and Benotto (2012) and Santus et al. (2014) identified hypernyms in distributional spaces. Computational work on antonyms includes approaches that tested the co-occurrence hypothesis (Charles and Miller, 1989; Fellbaum, 1995), and approaches driven by text understanding efforts and contradiction frameworks (Harabagiu et al., 2006; Mohammad et al., 2008; de Marneffe et al., 2008). Among the few approaches that distinguished between paradigmatic semantic relations, Lin et al. (2003) used patterns and bilingual dictionaries to retrieve distributionally similar words, and relied on clear a</context>
</contexts>
<marker>Weeds, Weir, McCarthy, 2004</marker>
<rawString>Julie Weeds, David Weir, and Diana McCarthy. 2004. Characterising measures of lexical distributional similarity. In Proceedings of the 20th International Conference on Computational Linguistics, pages 1015–1021.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Willy Yap</author>
<author>Timothy Baldwin</author>
</authors>
<title>Experiments on pattern-based relation learning.</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th ACM Conference on Information and Knowledge Management,</booktitle>
<pages>1657--1660</pages>
<contexts>
<context position="16232" citStr="Yap and Baldwin (2009)" startWordPosition="2473" endWordPosition="2476">erally more frequent than strict word patterns, which also leads to more reliable PMI values. 5 Evaluation In our evaluation, we assess the performance of the marker-based model and demonstrate the benefits of incorporating discourse markers into a patternbased model, which we apply as a baseline. We evaluate on several data sets: the collection of target-response pairs in German from previous work, and a similar data set that was collected for English target words (cf. Section 3); for comparison reasons, we also apply our models to the balanced data set of related and unrelated noun pairs by Yap and Baldwin (2009).4 We perform 3-way and 2-way relation classification experiments, using 5-fold cross-validation and a nearest centroid classifier (as applied by S&amp;K). Results. The 3-way classification results of the baseline and our marker-based model are summarized in Table 3, with best results for each setting marked in bold. On the German data set, our model always outperforms a random baseline (33% F1-score). The results on the English data set are overall a bit lower, possibly due to corpus size. In almost all classification tasks, our markerbased model achieves a higher recall and F1-score than the pat</context>
<context position="20172" citStr="Yap and Baldwin (2009)" startWordPosition="3118" endWordPosition="3121">relation for “double”–“multiple” (hypernymy). The combined model classifies both pairs correctly. Table 4 further assesses the strength of the combined model on the 2-way classifications. The table highlights results indicating improvements over both individual models. We observe that the combined model achieves the best recall and F1- score in 15 out of 18 cases. Relation SYN ANT HYP Patterns 0.97 0.97 0.94 Markers 0.77* 0.82* 0.91* Combined 0.93* 0.98 0.96* Table 5: Results in F1-score on the balanced data set by Yap and Baldwin (* p&lt;0.05). A final experiment is performed on the data set by Yap and Baldwin (2009) to see whether our models can also distinguish word pairs of individual relations from unrelated pairs of words. The results, listed in Table 5, show that the markerbased model cannot perform this task as well as the pattern-based model. The combined model, however, outperforms both individual models in 2 out of 3 cases. Despite their simplicity, our models achieve results close to the F1-scores reported by Yap and Baldwin (0.98–0.99), who employed syntactic pre-processing and an SVM-based classifier, and experimented with different corpora. 6 Conclusions In this paper, we proposed to use dis</context>
</contexts>
<marker>Yap, Baldwin, 2009</marker>
<rawString>Willy Yap and Timothy Baldwin. 2009. Experiments on pattern-based relation learning. In Proceedings of the 18th ACM Conference on Information and Knowledge Management, pages 1657–1660.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yeh</author>
</authors>
<title>More accurate tests for the statistical significance of result differences.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics,</booktitle>
<pages>947--953</pages>
<location>Saarbr¨ucken, Germany,</location>
<contexts>
<context position="17216" citStr="Yeh, 2000" startWordPosition="2635" endWordPosition="2636">om baseline (33% F1-score). The results on the English data set are overall a bit lower, possibly due to corpus size. In almost all classification tasks, our markerbased model achieves a higher recall and F1-score than the pattern-based approach. The precision results of the marker-based model are overall below the pattern-based model. This drop in performance does not come as a surprise though, considering that the model only makes use of 122 and 310 features, in comparison to tens of thousands of features in the pattern approach. A randomized significance test over classified instances (cf. Yeh, 2000) revealed that only two differences in results are significant. We hypothesize that one reason for this outcome might be that both models cover complementary sets of instances. To verify this hypothesis, we apply a combined model, which is based on a weighted linear combination of distances computed by the two individual models.5 As displayed in Table 3, this combined model yields further improvements 4Note that we could, in principle, also apply our models to unbalanced data. Our main focus lies however on examining the direct impact of different feature sets. We hence decided to keep the eva</context>
</contexts>
<marker>Yeh, 2000</marker>
<rawString>Alexander Yeh. 2000. More accurate tests for the statistical significance of result differences. In Proceedings of the 18th International Conference on Computational Linguistics, pages 947–953, Saarbr¨ucken, Germany, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
<author>John Platt</author>
</authors>
<title>Polarity inducing latent semantic analysis.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1212--1222</pages>
<location>Jeju Island, Korea,</location>
<contexts>
<context position="5300" citStr="Yih et al. (2012)" startWordPosition="768" endWordPosition="771">5), and approaches driven by text understanding efforts and contradiction frameworks (Harabagiu et al., 2006; Mohammad et al., 2008; de Marneffe et al., 2008). Among the few approaches that distinguished between paradigmatic semantic relations, Lin et al. (2003) used patterns and bilingual dictionaries to retrieve distributionally similar words, and relied on clear antonym patterns such as ‘either X or Y’ in a post-processing step to distinguish synonyms from antonyms. The study by Mohammad et al. (2013) on the identification and ranking of opposites also included synonym/antonym distinction. Yih et al. (2012) developed an LSA approach incorporating a thesaurus, to distinguish the same two relations. Chang et al. (2013) extended this approach to induce vector representations that can capture multiple relations. Whereas the above mentioned approaches rely on additional knowledge sources, Turney (2006) developed a corpusbased approach to model relational similarity, addressing (among other tasks) the distinction between synonyms and antonyms. More recently, Schulte im Walde and K¨oper (2013) proposed to distinguish between the three relations antonymy, synonymy and hyponymy based on automatically acq</context>
</contexts>
<marker>Yih, Zweig, Platt, 2012</marker>
<rawString>Wen-tau Yih, Geoffrey Zweig, and John Platt. 2012. Polarity inducing latent semantic analysis. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1212–1222, Jeju Island, Korea, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>