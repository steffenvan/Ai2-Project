<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002781">
<title confidence="0.927390333333333">
Automation and Evaluation of the Keyword Method
for Second Language Learning
G¨ozde ¨Ozbal Daniele Pighin Carlo Strapparava
</title>
<author confidence="0.855993">
Trento RISE Google FBK-irst
</author>
<affiliation confidence="0.864812">
Trento, Italy Z¨urich, Switzerland Trento, Italy
</affiliation>
<email confidence="0.997185">
gozbalde@gmail.com biondo@google.com strappa@fbk.eu
</email>
<sectionHeader confidence="0.993868" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999878833333333">
In this paper, we combine existing
NLP techniques with minimal supervi-
sion to build memory tips according to
the keyword method, a well established
mnemonic device for second language
learning. We present what we believe to be
the first extrinsic evaluation of a creative
sentence generator on a vocabulary learn-
ing task. The results demonstrate that NLP
techniques can effectively support the de-
velopment of resources for second lan-
guage learning.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99998655">
The keyword method is a mnemonic device (Co-
hen, 1987; Thompson, 1987) that is especially
suitable for vocabulary acquisition in second lan-
guage learning (Mizumoto and Kansai, 2009;
Hummel, 2010; Shen, 2010; Tavakoli and Gerami,
2013). In this method, a target word in a foreign
language L2 can be learned by a native speaker of
another language L1 in two main steps: 1) one or
more L1 words, possibly referring to a concrete
entity, are chosen based on orthographic or pho-
netic similarity with the target word; 2) an L1 sen-
tence is constructed in which an association be-
tween the translation of the target word and the
keyword(s) is established, so that the learner, when
seeing or hearing the word, immediately recalls
the keyword(s). To illustrate, for teaching the Ital-
ian word cuore which means heart in English, the
learner might be asked to imagine “a lonely heart
with a hard core”.
The keyword method has already been proven
to be a valuable teaching device. However, the
preparation of the memorization tips for each new
word is an activity that requires considerable time,
linguistic competence and creativity. To the best
of our knowledge, there is only one study which
attempts to automate the mechanism of the key-
word method. In ( ¨Ozbal and Strapparava, 2011),
we proposed to automate the keyword method by
retrieving sentences from the Web. However, we
did not provide any evaluation to demonstrate the
effectiveness of our approach in a real life sce-
nario. In addition, we observed that retrieval poses
severe limitations in terms of recall and sentence
quality, and it might incur copyright violations.
In this paper, we overcome these limitations by
introducing a semi-automatic system implement-
ing the keyword method that builds upon the key-
word selection mechanism of ¨Ozbal and Strappar-
ava (2011) and combines it with a state-of-the-art
creative sentence generation framework (¨Ozbal et
al., 2013). We set up an experiment to simulate
the situation in which a teacher needs to prepare
material for a vocabulary teaching resource. Ac-
cording to our scenario, the teacher relies on au-
tomatic techniques to generate relatively few, high
quality mnemonics in English to teach Italian vo-
cabulary. She only applies a very light supervi-
sion in the last step of the process, in which the
most suitable among the generated sentences are
selected before being presented to the learners. In
this stage, the teacher may want to consider factors
which are not yet in reach of automatic linguistic
processors, such as the evocativeness or the mem-
orability of a sentence. We show that the automat-
ically generated sentences help learners to estab-
lish memorable connections which augment their
ability to assimilate new vocabulary. To the best of
our knowledge, this work is the first documented
extrinsic evaluation of a creative sentence genera-
tor on a real-world application.
</bodyText>
<sectionHeader confidence="0.999638" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.997583166666667">
The effectiveness of the keyword method (KM)
is a well-established fact (Sarıc¸oban and Bas¸ıbek,
2012). Sommer and Gruneberg (2002) found that
using KM to teach French made learning easier
and faster than conventional methods. Sagarra
and Alba (2006) compared the effectiveness of
</bodyText>
<page confidence="0.97916">
352
</page>
<bodyText confidence="0.989448257142857">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 352–357,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
three learning methods including the semantic
mapping, rote memorization (i.e., memorization
by pure repetition, with no mnemonic aid) and
keyword on beginner learners of a second lan-
guage. Their results show that using KM leads
to better learning of second language vocabulary
for beginners. Similar results have been reported
by Sarıc¸oban and Bas¸ıbek (2012) and Tavakoli
and Gerami (2013). Besides all the experimental
results demonstrating the effectiveness of KM, it
is worthwhile to mention about the computational
efforts to automate the mechanism. In ( ¨Ozbal and
Strapparava, 2011) we proposed an automatic vo-
cabulary teaching system which combines NLP
and IR techniques to automatically generate mem-
ory tips for vocabulary acquisition. The system
exploits orthographic and phonetic similarity met-
rics to find the best L2 keywords for each target L1
word. Sentences containing the keywords and the
translation of the target word are retrieved from
the Web, but we did not carry out an evaluation
of the quality or the coverage of the retrieved sen-
tences. In ¨Ozbal et al. (2013) we proposed an ex-
tensible framework for the generation of creative
sentences in which users are able to force sev-
eral words to appear in the sentences. While we
had discussed the potentiality of creative sentence
generation as a useful teaching device, we had not
validated our claim experimentally yet. As a previ-
ous attempt at using NLP for education, Manurung
et al. (2008) employ a riddle generator to create
a language playground for children with complex
communication needs.
</bodyText>
<sectionHeader confidence="0.956576" genericHeader="method">
3 Memory tip generation
</sectionHeader>
<bodyText confidence="0.9994005">
Preparing memory tips based on KM includes two
main ingredients: one or more keywords which are
orthographically or phonetically similar to the L2
word to be learned; and a sentence in which the
keywords and the translation of the target L2 word
are combined in a meaningful way. In this section,
we detail the process that we employed to generate
such memory tips semi-automatically.
</bodyText>
<subsectionHeader confidence="0.934954">
3.1 Target word selection and keyword
generation
</subsectionHeader>
<bodyText confidence="0.999934785714286">
We started by compiling a collection of Ital-
ian nouns consisting of three syllables from var-
ious resources for vocabulary teaching includ-
ing http://didattica.org/italiano.
htm and http://ielanguages.com, and
produced a list of 185 target L2 words. To gen-
erate the L1 keywords for each target word, we
adopted a similar strategy to ¨Ozbal and Strappa-
rava (2011). For each L2 target word t, the key-
word selection module generates a list of possi-
ble keyword pairs, K. A keyword pair k ∈ K
can either consist of two non-empty strings, i.e.,
k = [wo, wi], or of one non-empty and one empty
string, i.e., wi = c. Each keyword pair has the
property that the concatenation of its elements is
either orthographically or phonetically similar to
the target word t. Orthographic and phonetic sim-
ilarity are evaluated by means of the Levenshtein
distance (Levenshtein, 1966). For orthographic
similarity, the distance is calculated over the char-
acters in the words, while for phonetic similarity
it is calculated over the phonetic representations
of t and wo + wi. We use the CMU pronuncia-
tion dictionary1 to retrieve the phonetic represen-
tation of English words. For Italian words, instead,
their phonetic representation is obtained from an
unpublished phonetic lexicon developed at FBK-
irst.
</bodyText>
<subsectionHeader confidence="0.999915">
3.2 Keyword filtering and ranking
</subsectionHeader>
<bodyText confidence="0.999982192307692">
Unlike in ( ¨Ozbal and Strapparava, 2011), where
we did not enforce any constraints for selecting
the keywords, in this case we applied a more so-
phisticated filtering and ranking strategy. We re-
quire at least one keyword in each pair to be a
content word; then, we require that at least one
keyword has length ≥ 3; finally, we discard pairs
containing at least one proper noun. We allowed
the keyword generation module to consider all the
entries in the CMU dictionary, and rank the key-
word pairs based on the following criteria in de-
creasing order of precedence: 1) Keywords with
a smaller orthographic/phonetic distance are pre-
ferred; 2) Keywords consisting of a single word
are preferred over two words (e.g., for the target
word lavagna, which means blackboard, lasagna
takes precedence over love and onion); 3) Key-
words that do not contain stop words are preferred
(e.g., for the target word pettine, which means
comb, the keyword pair pet and inn is ranked
higher than pet and in, since in is a stop word); 4)
Keyword pairs obtained with orthographic similar-
ity are preferred over those obtained with phonetic
similarity, as learners might be unfamiliar with the
phonetic rules of the target language. For example,
for the target word forbice, which means scissors,
</bodyText>
<footnote confidence="0.995166">
1http://www.speech.cs.cmu.edu/cgi-bin/
cmudict
</footnote>
<page confidence="0.997743">
353
</page>
<figure confidence="0.760600828571429">
Group Target Sentence
A1 campagna a company runs the country
A1 isola an island of remote isolated communities
A1 fabbrica a fabric worker in a factory
A1 bagnino lifeguards carry no bag
A1 inverno the inferno started, winter left
A1 cielo the sky has no ceiling
A1 marrone blood and marrow in a brown water
A1 cuore the lonely heart has hard core
A1 coperta a piece of copper in the corner of a blanket
A1 locanda an inn oak door with lock and key
A2 piazza a square building serves a free pizza
A2 calzino big bloke with sock in the casino
A2 scatola a cardboard box sat in a scuttle of a house
A2 ragazzo boys also have rag dolls
A2 angolo a corner kick came at an angle
A2 cestino a teen movie uses basket to play the chess
A2 carbone the coal is the form of carbon
A2 cassetto a blank cassette tape is in a drawer
A2 farfalla the butterflies are far in the fall
A2 tovaglia a damp cloth towel
B1 duomo the old cathedral has a dome
B1 aceto a vinegar sauce contains the acid
B1 nuvola the sophisticated novel depicts the cloud
B1 chiesa the Catholic church has Swiss cheese
B1 bacino the explosion in the back broke the pelvis
B1 maiale a pork meat comes in the mail
B1 minestra Chinese ministries have soup
B1 estate this estate is for summer
B1 bozzolo a buzz comes wrapped in the cocoon
B1 arnese harness a technology to develop a tool
B2 asino an Asian elephant is riding a donkey
B2 miele do not make honey to walk a mile
B2 polmone crowded pullmans stop the lungs
B2 fagiolo a topical facial bean cream
</figure>
<tableCaption confidence="0.44448425">
B2 fiore afire in a flower market
B2 compressa the clay tablet is in the compressed form
B2 cavallo horse running fast in cavalry
B2 fiume the muddy river has smoke and fumes
B2 pittore a famous painter has precious pictures
B2 manico manic people have broken necks
Table 1: Sentences used in the vocabulary acqui-
sition experiment.
</tableCaption>
<bodyText confidence="0.997380571428572">
the keyword pair for and bid is preferred to for and
beach.
We selected up to three of the highest ranked
keyword pairs for each target word, obtaining 407
keyword combinations for the initial 185 Italian
words, which we used as the input for the sentence
generator.
</bodyText>
<subsectionHeader confidence="0.999015">
3.3 Sentence generation
</subsectionHeader>
<bodyText confidence="0.999973897435898">
In this step, our goal was to generate, for each Ital-
ian word, sentences containing its L1 translation
and the set of orthographically (or phonetically)
similar keywords that we previously selected. For
each keyword combination, starting from the top-
ranked ones, we generated up to 10 sentences by
allowing any known part-of-speech for the key-
words. The sentences were produced by the state
of the art sentence generator of ¨Ozbal et al. (2013).
The system relies on two corpora of automatic
parses as a repository of sentence templates and
lexical statistics. As for the former, we combined
two resources: a corpus of 16,000 proverbs (Mi-
halcea and Strapparava, 2006) and a collection of
5,000 image captions2 collected by Rashtchian et
al. (2010). We chose these two collections since
they offer a combination of catchy or simple sen-
tences that we expect to be especially suitable
for second language learning. As for the sec-
ond corpus, we used LDC’s English GigaWord 5th
Edition3. Of the 12 feature functions described
in ( ¨Ozbal et al., 2013), we only implemented the
following scorers: Variety (to prevent duplicate
words from appearing in the sentences); Seman-
tic Cohesion (to enforce the generation of sentence
as lexically related to the target words as possi-
ble); Alliteration, Rhyme and Plosive (to intro-
duce hooks to echoic memory in the output); De-
pendency Operator and N-gram (to enforce output
grammaticality).
We observed that the sentence generation mod-
ule was not able to generate a sentence for 24%
of the input configurations. For comparison, when
we attempted to retrieve sentences from the Web
as suggested in ¨Ozbal and Strapparava (2011), we
could collect an output for less than 10% of the in-
put configurations. Besides, many of the retrieved
sentences were exceedingly long and complex to
be used in a second language learning experiment.
</bodyText>
<subsectionHeader confidence="0.991305">
3.4 Sentence selection
</subsectionHeader>
<bodyText confidence="0.999928125">
For each L1 keyword pair obtained for each L2
target word, we allowed the system to output up to
10 sentences. We manually assessed the quality of
the generated sentences in terms of meaningful-
ness, evocativeness and grammaticality to select
the most appropriate sentences to be used for the
task. In addition, for keyword pairs not containing
the empty string, we prioritized the sentences in
which the keywords were closer to each other. For
example, let us assume that we have the keywords
call and in for the target word collina. Among
the sentences “The girl received a call in the bath-
room” and “Call the blond girl in case you need”,
the first one is preferred, since the keywords are
closer to each other. Furthermore, we gave pri-
ority to the sentences that included the keywords
</bodyText>
<footnote confidence="0.99546925">
2http://vision.cs.uiuc.edu/
pascal-sentences/
3http://www.ldc.upenn.edu/Catalog/
catalogEntry.jsp?catalogId=LDC2011T07
</footnote>
<page confidence="0.998906">
354
</page>
<bodyText confidence="0.999416882352941">
in the right order. To illustrate, for the same key-
words and the target words, we would prefer the
sentence “I called him in the morning yesterday”
over “You talk a lot in a call”.
Accordingly, for each target word in random or-
der, we sequentially scanned the outputs generated
for each keyword pair. As soon as a sentence of
adequate quality was found, we added it to our
evaluation data and moved on to the next keyword.
We continued this process until we selected a sen-
tence for 40 distinct target words, which we set
as the target size of the experiment. We had to
inspect the outputs generated for 48 target words
before we were able to select 40 good examples,
meaning that for 17% of the target words the sen-
tence generator could not produce a sentence of
acceptable quality.
</bodyText>
<sectionHeader confidence="0.997572" genericHeader="method">
4 Experiment setup
</sectionHeader>
<bodyText confidence="0.999975965517241">
For our experiment, we drew inspiration from
Sagarra and Alba (2006). We compared the re-
tention error rate of learners who tried to memo-
rize new words with or without the aid of the auto-
matically generated sentences. Through academic
channels, we recruited 20 native English speakers
with no prior knowledge of Italian.4
After obtaining the sentences as explained in
Section 3, we shuffled and then divided the whole
set including 40 target words together with their
translation, the generated keywords and sentences
into 2 batches (A, B) and further divided each
batch into 2 groups consisting of 10 elements (A1,
A2, B1 and B2). The set of sentences assigned
to each group is listed in Table 1: Column “Tar-
get” reports the Italian target word being taught;
Column “Sentence” shows the automatically gen-
erated sentence, where the translation of the tar-
get word is shown in bold and the keyword(s) in
italic. For the experiments, we randomly assigned
each subject to one of the batches (A or B). Then,
each subject was asked to memorize all the word
pairs in a batch, but they would see the memory
tips only for one of the two groups, which was
again randomly assigned. This approach resulted
in 4 different memorization exercises, namely 1)
A1 with tips and A2 without, 2) A2 with tips and
A1 without, 3) B1 with tips and B2 without, 4) B2
with tips and B1 without.
</bodyText>
<footnote confidence="0.9422985">
4We preferred to select the experiment subjects in person
as opposed to crowdsourcing the evaluation to be able to ver-
ify the proficiency of the subjects in the two languages and to
ensure the reliability of the outcome of the evaluation.
</footnote>
<table confidence="0.999912">
Group Error rate (%) Reduction
Rote KW Δe %e
A1 4.08 3.39 0.69 16.95
A2 12.07 10.42 1.65 13.69
B1 12.77 10.00 2.77 21.67
B2 22.50 12.50 10.00 44.44
Macro-average 12.85 9.08 3.78 29.39
Micro-average 11.27 8.25 3.02 26.76
</table>
<tableCaption confidence="0.995175">
Table 2: Per-group and overall retention error rate
</tableCaption>
<bodyText confidence="0.967551">
when using rote or keyword-aided (KW) memo-
rization.
When memorizing the translations without the
aid of memory tips, the subjects were instructed
to focus only on the Italian word and its English
translation and to repeat them over and over in
their mind. Conversely, when relying on the au-
tomatic memory tips the subjects were shown the
word, its translation and the generated sentence in-
cluding the keywords. In this case, the subjects
were instructed to read the sentence over and over
trying to visualize it.
After going through each set of slides, we dis-
tracted the subjects with a short video in order to
reset their short term memory. After that, their re-
tention was tested. For each Italian word in the ex-
ercise, they were asked to select the English trans-
lation among 5 alternatives, including the correct
translation and 4 other words randomly selected
from the same group. In this way, the subjects
would always have to choose among the words
that they encountered during the exercise.5 We
also added an extra option “I already knew this
word” that the subjects were instructed to select
in case they already knew the Italian word prior to
taking part in the experiment.
</bodyText>
<sectionHeader confidence="0.969469" genericHeader="evaluation">
5 Experiment results
</sectionHeader>
<bodyText confidence="0.946833076923077">
Table 2 summarizes the outcome of the experi-
ment. The contribution of the automatically gen-
erated sentences to the learning task is assessed
in terms of error rate-reduction, which we mea-
sure both within each group (rows 1-4) and on the
whole evaluation set (rows 5-6). Due to the pres-
ence of the “I already knew this word” option in
the learning-assessment questionnaire, the number
of the actual answers provided by each subject can
be slightly different, hence the difference between
macro- and micro-average.
5Otherwise, they could easily filter out the wrong answers
just because they were not exposed to them recently.
</bodyText>
<page confidence="0.997871">
355
</page>
<bodyText confidence="0.988410462962963">
The error rate for each memorization technique
t (where t = R for “Rote memorization” and
t = K for “keyword-aided memorization”) is cal-
culated as: et = it
ct+it, where ct and it are the
number of correct and incorrect answers provided
by the subjects, respectively. The absolute error
rate reduction De is calculated as the absolute dif-
ference in error rate between rote and keyword-
aided memorization, i.e.: De = eR − eK. Finally,
the relative error rate reduction %e is calculated as
the the ratio between the absolute error rate reduc-
tion(Ae and the error rate of rote memorization eR,
i.e.,: /Oe = Δe= eR−eK
eR eR .
The overall results (rows 5 and 6 in Table 2)
show that vocabulary learning noticeably im-
proves when supported by the generated sen-
tences, with error rates dropping by almost 30%
in terms of macro-average (almost 27% for micro-
average). The breakdown of the error rate across
the 4 groups shows a clear pattern. The results
clearly indicate that one group (A1) by chance
contained easier words to memorize as shown by
the low error rate (between 3% and 4%) obtained
with both methods. Similarly, groups A2 and B1
are of average difficulty, whereas group B2 ap-
pears to be the most difficult, with an error rate
higher than 22% when using only rote memoriza-
tion. Interestingly, there is a strong correlation
(Pearson’s r = 0.85) between the difficulty of
the words in each group (measured as the error
rate on rote memorization) and the positive contri-
bution of the generated sentences to the learning
process. In fact, we can see how the relative er-
ror rate reduction %e increases from ∼17% (group
A1) to almost 45% (group B2). Based on the re-
sults obtained by Sagarra and Alba (2006), who
showed that the keyword method results in bet-
ter long-term word retention than rote memoriza-
tion, we would expect the error rate reduction to be
even higher in a delayed post-test. All in all, these
findings clearly support the claim that a state-of-
the-art sentence generator can be successfully em-
ployed to support keyword-based second language
learning. After completing their exercise, the sub-
jects were asked to provide feedback about their
experience as learners. We set up a 4-items Lik-
ert scale (Likert, 1932) where each item consisted
of a statement and a 5-point scale of values rang-
ing from (1) [I strongly disagree] to (5) [I strongly
agree]. The distribution of the answers to the ques-
tions is shown in Table 3. 60% of the subjects ac-
knowledged that the memory tips helped them in
</bodyText>
<table confidence="0.999099666666667">
Question Rating (%)
1 2 3 4 5
Sentences helped 5 20 15 35 25
Sentences are grammatical - 25 30 35 10
Sentences are catchy - 25 10 50 15
Sentences are witty - 25 25 50 -
</table>
<tableCaption confidence="0.997321">
Table 3: Evaluation of the generated sentences on
a 5-point Likert scale.
</tableCaption>
<bodyText confidence="0.999304076923077">
the memorization process; 45% found that the sen-
tences were overall correct; 65% confirmed that
the sentences were catchy and easy to remember;
and 50% found the sentences to be overall witty
although the sentence generator does not include a
mechanism to generate humor. Finally, it is worth
mentioning that none of the subjects noticed that
the sentences were machine generated, which we
regard as a very positive assessment of the qual-
ity of the sentence generation framework. From
their comments, it emerges that the subjects ac-
tually believed that they were just comparing two
memorization techniques.
</bodyText>
<sectionHeader confidence="0.996168" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999970217391305">
In this paper, we have presented a semi-automatic
system for the automation of the keyword method
and used it to teach 40 Italian words to 20 En-
glish native speakers. We let the system select
appropriate keywords and generate sentences au-
tomatically. For each Italian word, we selected the
most suitable among the 10 highest ranked sug-
gestions and used it for the evaluation. The sig-
nificant reduction in retention error rate (between
17% and 45% on different word groups) for the
words learned with the aid of the automatically
generated sentences shows that they are a viable
low-effort alternative to human-constructed exam-
ples for vocabulary teaching.
As future work, it would be interesting to in-
volve learners in an interactive evaluation to un-
derstand the extent to which learners can bene-
fit from ad-hoc personalization. Furthermore, it
should be possible to use frameworks similar to
the one that we presented to automate other teach-
ing devices based on sentences conforming to spe-
cific requirements (Dehn, 2011), such as verbal
chaining and acrostic.
</bodyText>
<sectionHeader confidence="0.997115" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.902218">
This work was partially supported by the PerTe
project (Trento RISE).
</bodyText>
<page confidence="0.998555">
356
</page>
<sectionHeader confidence="0.995894" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99995820289855">
Andrew D. Cohen. 1987. The use of verbal and
imagery mnemonics in second-language vocabulary
learning. Studies in Second Language Acquisition,
9:43–61, 2.
M.J. Dehn. 2011. Working Memory and Academic
Learning: Assessment and Intervention. Wiley.
K. M. Hummel. 2010. Translation and short-term L2
vocabulary retention: Hindrance or help? Language
Teaching Research, 14(1):61–74.
V. Levenshtein. 1966. Binary codes capable of cor-
recting deletions, insertions, and reversals. Soviet
Physics Doklady, 10:707—710.
R. Likert. 1932. A technique for the measurement of
attitudes. Archives of Psychology, 22(140):1–55.
Ruli Manurung, Graeme Ritchie, Helen Pain, Annalu
Waller, Dave O’Mara, and Rolf Black. 2008. The
Construction of a Pun Generator for Language Skill
Development. Appl. Artif. Intell., 22(9):841–869,
October.
R. Mihalcea and C. Strapparava. 2006. Learning to
laugh (automatically): Computational models for
humor recognition. Journal of Computational In-
telligence, 22(2):126–142, May.
A. Mizumoto and O. T. Kansai. 2009. Examining
the effectiveness of explicit instruction of vocabu-
lary learning strategies with Japanese EFL university
students. Language Teaching Research 13, 4.
G¨ozde ¨Ozbal and Carlo Strapparava. 2011. MEANS:
Moving Effective Assonances for Novice Students.
In Proceedings of the 16th International Confer-
ence on Intelligent User Interfaces (IUI 2011), pages
449–450, New York, NY, USA. ACM.
G¨ozde ¨Ozbal, Daniele Pighin, and Carlo Strapparava.
2013. BRAINSUP: Brainstorming Support for Cre-
ative Sentence Generation. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics (ACL 2013), pages 1446–1455,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Cyrus Rashtchian, Peter Young, Micah Hodosh, and
Julia Hockenmaier. 2010. Collecting image annota-
tions using amazon’s mechanical turk. In Proceed-
ings of the NAACL HLT 2010 Workshop on Creating
Speech and Language Data with Amazon’s Mechan-
ical Turk, CSLDAMT ’10, pages 139–147, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
N. Sagarra and M. Alba. 2006. The key is in the
keyword: L2 vocabulary learning methods with be-
ginning learners of spanish. The Modern Language
Journal, 90(2):228–243.
A. Sarıc¸oban and N. Bas¸ıbek. 2012. Mnemonics tech-
nique versus context method in teaching vocabulary
at upper-intermediate level. Journal of Education
and Science, 37(164):251–266.
Helen H. Shen. 2010. Imagery and verbal coding ap-
proaches in Chinese vocabulary instruction. Lan-
guage Teaching Research, 14(4):485–499.
Steffen Sommer and Michael Gruneberg. 2002. The
use of linkword language computer courses in a
classroom situation: a case study at rugby school.
’Language Learning Journal, 26(1):48–53.
M. Tavakoli and E. Gerami. 2013. The effect of key-
word and pictorial methods on EFL learners’ vocab-
ulary learning and retention. PORTA LINGUARUM,
19:299–316.
G. Thompson. 1987. Using bilingual dictionar-
ies. ELT Journal, 41(4):282–286. cited By (since
1996)6.
</reference>
<page confidence="0.998195">
357
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.775391">
<title confidence="0.9992805">Automation and Evaluation of the Keyword for Second Language Learning</title>
<author confidence="0.910303">Daniele Pighin Carlo Strapparava Trento RISE Google FBK-irst</author>
<affiliation confidence="0.980583">Trento, Italy Z¨urich, Switzerland Trento,</affiliation>
<email confidence="0.993885">gozbalde@gmail.combiondo@google.comstrappa@fbk.eu</email>
<abstract confidence="0.997626230769231">In this paper, we combine existing NLP techniques with minimal supervision to build memory tips according to the keyword method, a well established mnemonic device for second language learning. We present what we believe to be the first extrinsic evaluation of a creative sentence generator on a vocabulary learning task. The results demonstrate that NLP techniques can effectively support the development of resources for second language learning.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Andrew D Cohen</author>
</authors>
<title>The use of verbal and imagery mnemonics in second-language vocabulary learning.</title>
<date>1987</date>
<journal>Studies in Second Language Acquisition,</journal>
<volume>9</volume>
<pages>2</pages>
<contexts>
<context position="778" citStr="Cohen, 1987" startWordPosition="111" endWordPosition="113">¨urich, Switzerland Trento, Italy gozbalde@gmail.com biondo@google.com strappa@fbk.eu Abstract In this paper, we combine existing NLP techniques with minimal supervision to build memory tips according to the keyword method, a well established mnemonic device for second language learning. We present what we believe to be the first extrinsic evaluation of a creative sentence generator on a vocabulary learning task. The results demonstrate that NLP techniques can effectively support the development of resources for second language learning. 1 Introduction The keyword method is a mnemonic device (Cohen, 1987; Thompson, 1987) that is especially suitable for vocabulary acquisition in second language learning (Mizumoto and Kansai, 2009; Hummel, 2010; Shen, 2010; Tavakoli and Gerami, 2013). In this method, a target word in a foreign language L2 can be learned by a native speaker of another language L1 in two main steps: 1) one or more L1 words, possibly referring to a concrete entity, are chosen based on orthographic or phonetic similarity with the target word; 2) an L1 sentence is constructed in which an association between the translation of the target word and the keyword(s) is established, so tha</context>
</contexts>
<marker>Cohen, 1987</marker>
<rawString>Andrew D. Cohen. 1987. The use of verbal and imagery mnemonics in second-language vocabulary learning. Studies in Second Language Acquisition, 9:43–61, 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Dehn</author>
</authors>
<title>Working Memory and Academic Learning: Assessment and Intervention.</title>
<date>2011</date>
<publisher>Wiley.</publisher>
<marker>Dehn, 2011</marker>
<rawString>M.J. Dehn. 2011. Working Memory and Academic Learning: Assessment and Intervention. Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K M Hummel</author>
</authors>
<title>Translation and short-term L2 vocabulary retention: Hindrance or help?</title>
<date>2010</date>
<journal>Language Teaching Research,</journal>
<volume>14</volume>
<issue>1</issue>
<contexts>
<context position="919" citStr="Hummel, 2010" startWordPosition="132" endWordPosition="133">ques with minimal supervision to build memory tips according to the keyword method, a well established mnemonic device for second language learning. We present what we believe to be the first extrinsic evaluation of a creative sentence generator on a vocabulary learning task. The results demonstrate that NLP techniques can effectively support the development of resources for second language learning. 1 Introduction The keyword method is a mnemonic device (Cohen, 1987; Thompson, 1987) that is especially suitable for vocabulary acquisition in second language learning (Mizumoto and Kansai, 2009; Hummel, 2010; Shen, 2010; Tavakoli and Gerami, 2013). In this method, a target word in a foreign language L2 can be learned by a native speaker of another language L1 in two main steps: 1) one or more L1 words, possibly referring to a concrete entity, are chosen based on orthographic or phonetic similarity with the target word; 2) an L1 sentence is constructed in which an association between the translation of the target word and the keyword(s) is established, so that the learner, when seeing or hearing the word, immediately recalls the keyword(s). To illustrate, for teaching the Italian word cuore which </context>
</contexts>
<marker>Hummel, 2010</marker>
<rawString>K. M. Hummel. 2010. Translation and short-term L2 vocabulary retention: Hindrance or help? Language Teaching Research, 14(1):61–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions, and reversals. Soviet Physics Doklady,</title>
<date>1966</date>
<pages>10--707</pages>
<contexts>
<context position="7010" citStr="Levenshtein, 1966" startWordPosition="1119" endWordPosition="1120">rds. To generate the L1 keywords for each target word, we adopted a similar strategy to ¨Ozbal and Strapparava (2011). For each L2 target word t, the keyword selection module generates a list of possible keyword pairs, K. A keyword pair k ∈ K can either consist of two non-empty strings, i.e., k = [wo, wi], or of one non-empty and one empty string, i.e., wi = c. Each keyword pair has the property that the concatenation of its elements is either orthographically or phonetically similar to the target word t. Orthographic and phonetic similarity are evaluated by means of the Levenshtein distance (Levenshtein, 1966). For orthographic similarity, the distance is calculated over the characters in the words, while for phonetic similarity it is calculated over the phonetic representations of t and wo + wi. We use the CMU pronunciation dictionary1 to retrieve the phonetic representation of English words. For Italian words, instead, their phonetic representation is obtained from an unpublished phonetic lexicon developed at FBKirst. 3.2 Keyword filtering and ranking Unlike in ( ¨Ozbal and Strapparava, 2011), where we did not enforce any constraints for selecting the keywords, in this case we applied a more soph</context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>V. Levenshtein. 1966. Binary codes capable of correcting deletions, insertions, and reversals. Soviet Physics Doklady, 10:707—710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Likert</author>
</authors>
<title>A technique for the measurement of attitudes.</title>
<date>1932</date>
<journal>Archives of Psychology,</journal>
<volume>22</volume>
<issue>140</issue>
<contexts>
<context position="20456" citStr="Likert, 1932" startWordPosition="3425" endWordPosition="3426">roup A1) to almost 45% (group B2). Based on the results obtained by Sagarra and Alba (2006), who showed that the keyword method results in better long-term word retention than rote memorization, we would expect the error rate reduction to be even higher in a delayed post-test. All in all, these findings clearly support the claim that a state-ofthe-art sentence generator can be successfully employed to support keyword-based second language learning. After completing their exercise, the subjects were asked to provide feedback about their experience as learners. We set up a 4-items Likert scale (Likert, 1932) where each item consisted of a statement and a 5-point scale of values ranging from (1) [I strongly disagree] to (5) [I strongly agree]. The distribution of the answers to the questions is shown in Table 3. 60% of the subjects acknowledged that the memory tips helped them in Question Rating (%) 1 2 3 4 5 Sentences helped 5 20 15 35 25 Sentences are grammatical - 25 30 35 10 Sentences are catchy - 25 10 50 15 Sentences are witty - 25 25 50 - Table 3: Evaluation of the generated sentences on a 5-point Likert scale. the memorization process; 45% found that the sentences were overall correct; 65%</context>
</contexts>
<marker>Likert, 1932</marker>
<rawString>R. Likert. 1932. A technique for the measurement of attitudes. Archives of Psychology, 22(140):1–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruli Manurung</author>
<author>Graeme Ritchie</author>
<author>Helen Pain</author>
<author>Annalu Waller</author>
<author>Dave O’Mara</author>
<author>Rolf Black</author>
</authors>
<title>The Construction of a Pun Generator for Language Skill Development.</title>
<date>2008</date>
<journal>Appl. Artif. Intell.,</journal>
<volume>22</volume>
<issue>9</issue>
<marker>Manurung, Ritchie, Pain, Waller, O’Mara, Black, 2008</marker>
<rawString>Ruli Manurung, Graeme Ritchie, Helen Pain, Annalu Waller, Dave O’Mara, and Rolf Black. 2008. The Construction of a Pun Generator for Language Skill Development. Appl. Artif. Intell., 22(9):841–869, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>C Strapparava</author>
</authors>
<title>Learning to laugh (automatically): Computational models for humor recognition.</title>
<date>2006</date>
<journal>Journal of Computational Intelligence,</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="11589" citStr="Mihalcea and Strapparava, 2006" startWordPosition="1913" endWordPosition="1917">te, for each Italian word, sentences containing its L1 translation and the set of orthographically (or phonetically) similar keywords that we previously selected. For each keyword combination, starting from the topranked ones, we generated up to 10 sentences by allowing any known part-of-speech for the keywords. The sentences were produced by the state of the art sentence generator of ¨Ozbal et al. (2013). The system relies on two corpora of automatic parses as a repository of sentence templates and lexical statistics. As for the former, we combined two resources: a corpus of 16,000 proverbs (Mihalcea and Strapparava, 2006) and a collection of 5,000 image captions2 collected by Rashtchian et al. (2010). We chose these two collections since they offer a combination of catchy or simple sentences that we expect to be especially suitable for second language learning. As for the second corpus, we used LDC’s English GigaWord 5th Edition3. Of the 12 feature functions described in ( ¨Ozbal et al., 2013), we only implemented the following scorers: Variety (to prevent duplicate words from appearing in the sentences); Semantic Cohesion (to enforce the generation of sentence as lexically related to the target words as possi</context>
</contexts>
<marker>Mihalcea, Strapparava, 2006</marker>
<rawString>R. Mihalcea and C. Strapparava. 2006. Learning to laugh (automatically): Computational models for humor recognition. Journal of Computational Intelligence, 22(2):126–142, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mizumoto</author>
<author>O T Kansai</author>
</authors>
<title>Examining the effectiveness of explicit instruction of vocabulary learning strategies with Japanese EFL university students.</title>
<date>2009</date>
<journal>Language Teaching Research</journal>
<volume>13</volume>
<pages>4</pages>
<contexts>
<context position="905" citStr="Mizumoto and Kansai, 2009" startWordPosition="128" endWordPosition="131">combine existing NLP techniques with minimal supervision to build memory tips according to the keyword method, a well established mnemonic device for second language learning. We present what we believe to be the first extrinsic evaluation of a creative sentence generator on a vocabulary learning task. The results demonstrate that NLP techniques can effectively support the development of resources for second language learning. 1 Introduction The keyword method is a mnemonic device (Cohen, 1987; Thompson, 1987) that is especially suitable for vocabulary acquisition in second language learning (Mizumoto and Kansai, 2009; Hummel, 2010; Shen, 2010; Tavakoli and Gerami, 2013). In this method, a target word in a foreign language L2 can be learned by a native speaker of another language L1 in two main steps: 1) one or more L1 words, possibly referring to a concrete entity, are chosen based on orthographic or phonetic similarity with the target word; 2) an L1 sentence is constructed in which an association between the translation of the target word and the keyword(s) is established, so that the learner, when seeing or hearing the word, immediately recalls the keyword(s). To illustrate, for teaching the Italian wor</context>
</contexts>
<marker>Mizumoto, Kansai, 2009</marker>
<rawString>A. Mizumoto and O. T. Kansai. 2009. Examining the effectiveness of explicit instruction of vocabulary learning strategies with Japanese EFL university students. Language Teaching Research 13, 4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨ozde ¨Ozbal</author>
<author>Carlo Strapparava</author>
</authors>
<title>MEANS: Moving Effective Assonances for Novice Students.</title>
<date>2011</date>
<booktitle>In Proceedings of the 16th International Conference on Intelligent User Interfaces (IUI 2011),</booktitle>
<pages>449--450</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>¨Ozbal, Strapparava, 2011</marker>
<rawString>G¨ozde ¨Ozbal and Carlo Strapparava. 2011. MEANS: Moving Effective Assonances for Novice Students. In Proceedings of the 16th International Conference on Intelligent User Interfaces (IUI 2011), pages 449–450, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨ozde ¨Ozbal</author>
<author>Daniele Pighin</author>
<author>Carlo Strapparava</author>
</authors>
<title>BRAINSUP: Brainstorming Support for Creative Sentence Generation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013),</booktitle>
<pages>1446--1455</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<marker>¨Ozbal, Pighin, Strapparava, 2013</marker>
<rawString>G¨ozde ¨Ozbal, Daniele Pighin, and Carlo Strapparava. 2013. BRAINSUP: Brainstorming Support for Creative Sentence Generation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013), pages 1446–1455, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cyrus Rashtchian</author>
<author>Peter Young</author>
<author>Micah Hodosh</author>
<author>Julia Hockenmaier</author>
</authors>
<title>Collecting image annotations using amazon’s mechanical turk.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, CSLDAMT ’10,</booktitle>
<pages>139--147</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="11669" citStr="Rashtchian et al. (2010)" startWordPosition="1927" endWordPosition="1930">graphically (or phonetically) similar keywords that we previously selected. For each keyword combination, starting from the topranked ones, we generated up to 10 sentences by allowing any known part-of-speech for the keywords. The sentences were produced by the state of the art sentence generator of ¨Ozbal et al. (2013). The system relies on two corpora of automatic parses as a repository of sentence templates and lexical statistics. As for the former, we combined two resources: a corpus of 16,000 proverbs (Mihalcea and Strapparava, 2006) and a collection of 5,000 image captions2 collected by Rashtchian et al. (2010). We chose these two collections since they offer a combination of catchy or simple sentences that we expect to be especially suitable for second language learning. As for the second corpus, we used LDC’s English GigaWord 5th Edition3. Of the 12 feature functions described in ( ¨Ozbal et al., 2013), we only implemented the following scorers: Variety (to prevent duplicate words from appearing in the sentences); Semantic Cohesion (to enforce the generation of sentence as lexically related to the target words as possible); Alliteration, Rhyme and Plosive (to introduce hooks to echoic memory in th</context>
</contexts>
<marker>Rashtchian, Young, Hodosh, Hockenmaier, 2010</marker>
<rawString>Cyrus Rashtchian, Peter Young, Micah Hodosh, and Julia Hockenmaier. 2010. Collecting image annotations using amazon’s mechanical turk. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, CSLDAMT ’10, pages 139–147, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Sagarra</author>
<author>M Alba</author>
</authors>
<title>The key is in the keyword: L2 vocabulary learning methods with beginning learners of spanish.</title>
<date>2006</date>
<journal>The Modern Language Journal,</journal>
<volume>90</volume>
<issue>2</issue>
<contexts>
<context position="3874" citStr="Sagarra and Alba (2006)" startWordPosition="615" endWordPosition="618">he evocativeness or the memorability of a sentence. We show that the automatically generated sentences help learners to establish memorable connections which augment their ability to assimilate new vocabulary. To the best of our knowledge, this work is the first documented extrinsic evaluation of a creative sentence generator on a real-world application. 2 Related work The effectiveness of the keyword method (KM) is a well-established fact (Sarıc¸oban and Bas¸ıbek, 2012). Sommer and Gruneberg (2002) found that using KM to teach French made learning easier and faster than conventional methods. Sagarra and Alba (2006) compared the effectiveness of 352 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 352–357, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics three learning methods including the semantic mapping, rote memorization (i.e., memorization by pure repetition, with no mnemonic aid) and keyword on beginner learners of a second language. Their results show that using KM leads to better learning of second language vocabulary for beginners. Similar results have been reported by Sarıc¸oban and Bas¸ıbek</context>
<context position="14590" citStr="Sagarra and Alba (2006)" startWordPosition="2412" endWordPosition="2415"> scanned the outputs generated for each keyword pair. As soon as a sentence of adequate quality was found, we added it to our evaluation data and moved on to the next keyword. We continued this process until we selected a sentence for 40 distinct target words, which we set as the target size of the experiment. We had to inspect the outputs generated for 48 target words before we were able to select 40 good examples, meaning that for 17% of the target words the sentence generator could not produce a sentence of acceptable quality. 4 Experiment setup For our experiment, we drew inspiration from Sagarra and Alba (2006). We compared the retention error rate of learners who tried to memorize new words with or without the aid of the automatically generated sentences. Through academic channels, we recruited 20 native English speakers with no prior knowledge of Italian.4 After obtaining the sentences as explained in Section 3, we shuffled and then divided the whole set including 40 target words together with their translation, the generated keywords and sentences into 2 batches (A, B) and further divided each batch into 2 groups consisting of 10 elements (A1, A2, B1 and B2). The set of sentences assigned to each</context>
<context position="19934" citStr="Sagarra and Alba (2006)" startWordPosition="3338" endWordPosition="3341">ained with both methods. Similarly, groups A2 and B1 are of average difficulty, whereas group B2 appears to be the most difficult, with an error rate higher than 22% when using only rote memorization. Interestingly, there is a strong correlation (Pearson’s r = 0.85) between the difficulty of the words in each group (measured as the error rate on rote memorization) and the positive contribution of the generated sentences to the learning process. In fact, we can see how the relative error rate reduction %e increases from ∼17% (group A1) to almost 45% (group B2). Based on the results obtained by Sagarra and Alba (2006), who showed that the keyword method results in better long-term word retention than rote memorization, we would expect the error rate reduction to be even higher in a delayed post-test. All in all, these findings clearly support the claim that a state-ofthe-art sentence generator can be successfully employed to support keyword-based second language learning. After completing their exercise, the subjects were asked to provide feedback about their experience as learners. We set up a 4-items Likert scale (Likert, 1932) where each item consisted of a statement and a 5-point scale of values rangin</context>
</contexts>
<marker>Sagarra, Alba, 2006</marker>
<rawString>N. Sagarra and M. Alba. 2006. The key is in the keyword: L2 vocabulary learning methods with beginning learners of spanish. The Modern Language Journal, 90(2):228–243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Sarıc¸oban</author>
<author>N Bas¸ıbek</author>
</authors>
<title>Mnemonics technique versus context method in teaching vocabulary at upper-intermediate level.</title>
<date>2012</date>
<journal>Journal of Education and Science,</journal>
<volume>37</volume>
<issue>164</issue>
<marker>Sarıc¸oban, Bas¸ıbek, 2012</marker>
<rawString>A. Sarıc¸oban and N. Bas¸ıbek. 2012. Mnemonics technique versus context method in teaching vocabulary at upper-intermediate level. Journal of Education and Science, 37(164):251–266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helen H Shen</author>
</authors>
<title>Imagery and verbal coding approaches in Chinese vocabulary instruction.</title>
<date>2010</date>
<journal>Language Teaching Research,</journal>
<volume>14</volume>
<issue>4</issue>
<contexts>
<context position="931" citStr="Shen, 2010" startWordPosition="134" endWordPosition="135">mal supervision to build memory tips according to the keyword method, a well established mnemonic device for second language learning. We present what we believe to be the first extrinsic evaluation of a creative sentence generator on a vocabulary learning task. The results demonstrate that NLP techniques can effectively support the development of resources for second language learning. 1 Introduction The keyword method is a mnemonic device (Cohen, 1987; Thompson, 1987) that is especially suitable for vocabulary acquisition in second language learning (Mizumoto and Kansai, 2009; Hummel, 2010; Shen, 2010; Tavakoli and Gerami, 2013). In this method, a target word in a foreign language L2 can be learned by a native speaker of another language L1 in two main steps: 1) one or more L1 words, possibly referring to a concrete entity, are chosen based on orthographic or phonetic similarity with the target word; 2) an L1 sentence is constructed in which an association between the translation of the target word and the keyword(s) is established, so that the learner, when seeing or hearing the word, immediately recalls the keyword(s). To illustrate, for teaching the Italian word cuore which means heart </context>
</contexts>
<marker>Shen, 2010</marker>
<rawString>Helen H. Shen. 2010. Imagery and verbal coding approaches in Chinese vocabulary instruction. Language Teaching Research, 14(4):485–499.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steffen Sommer</author>
<author>Michael Gruneberg</author>
</authors>
<title>The use of linkword language computer courses in a classroom situation: a case study at rugby school.</title>
<date>2002</date>
<journal>Language Learning Journal,</journal>
<volume>26</volume>
<issue>1</issue>
<contexts>
<context position="3755" citStr="Sommer and Gruneberg (2002)" startWordPosition="596" endWordPosition="599">is stage, the teacher may want to consider factors which are not yet in reach of automatic linguistic processors, such as the evocativeness or the memorability of a sentence. We show that the automatically generated sentences help learners to establish memorable connections which augment their ability to assimilate new vocabulary. To the best of our knowledge, this work is the first documented extrinsic evaluation of a creative sentence generator on a real-world application. 2 Related work The effectiveness of the keyword method (KM) is a well-established fact (Sarıc¸oban and Bas¸ıbek, 2012). Sommer and Gruneberg (2002) found that using KM to teach French made learning easier and faster than conventional methods. Sagarra and Alba (2006) compared the effectiveness of 352 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 352–357, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics three learning methods including the semantic mapping, rote memorization (i.e., memorization by pure repetition, with no mnemonic aid) and keyword on beginner learners of a second language. Their results show that using KM leads to bet</context>
</contexts>
<marker>Sommer, Gruneberg, 2002</marker>
<rawString>Steffen Sommer and Michael Gruneberg. 2002. The use of linkword language computer courses in a classroom situation: a case study at rugby school. ’Language Learning Journal, 26(1):48–53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tavakoli</author>
<author>E Gerami</author>
</authors>
<title>The effect of keyword and pictorial methods on EFL learners’ vocabulary learning and retention.</title>
<date>2013</date>
<journal>PORTA LINGUARUM,</journal>
<pages>19--299</pages>
<contexts>
<context position="959" citStr="Tavakoli and Gerami, 2013" startWordPosition="136" endWordPosition="139">ion to build memory tips according to the keyword method, a well established mnemonic device for second language learning. We present what we believe to be the first extrinsic evaluation of a creative sentence generator on a vocabulary learning task. The results demonstrate that NLP techniques can effectively support the development of resources for second language learning. 1 Introduction The keyword method is a mnemonic device (Cohen, 1987; Thompson, 1987) that is especially suitable for vocabulary acquisition in second language learning (Mizumoto and Kansai, 2009; Hummel, 2010; Shen, 2010; Tavakoli and Gerami, 2013). In this method, a target word in a foreign language L2 can be learned by a native speaker of another language L1 in two main steps: 1) one or more L1 words, possibly referring to a concrete entity, are chosen based on orthographic or phonetic similarity with the target word; 2) an L1 sentence is constructed in which an association between the translation of the target word and the keyword(s) is established, so that the learner, when seeing or hearing the word, immediately recalls the keyword(s). To illustrate, for teaching the Italian word cuore which means heart in English, the learner migh</context>
<context position="4512" citStr="Tavakoli and Gerami (2013)" startWordPosition="706" endWordPosition="709">e effectiveness of 352 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 352–357, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics three learning methods including the semantic mapping, rote memorization (i.e., memorization by pure repetition, with no mnemonic aid) and keyword on beginner learners of a second language. Their results show that using KM leads to better learning of second language vocabulary for beginners. Similar results have been reported by Sarıc¸oban and Bas¸ıbek (2012) and Tavakoli and Gerami (2013). Besides all the experimental results demonstrating the effectiveness of KM, it is worthwhile to mention about the computational efforts to automate the mechanism. In ( ¨Ozbal and Strapparava, 2011) we proposed an automatic vocabulary teaching system which combines NLP and IR techniques to automatically generate memory tips for vocabulary acquisition. The system exploits orthographic and phonetic similarity metrics to find the best L2 keywords for each target L1 word. Sentences containing the keywords and the translation of the target word are retrieved from the Web, but we did not carry out </context>
</contexts>
<marker>Tavakoli, Gerami, 2013</marker>
<rawString>M. Tavakoli and E. Gerami. 2013. The effect of keyword and pictorial methods on EFL learners’ vocabulary learning and retention. PORTA LINGUARUM, 19:299–316.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Thompson</author>
</authors>
<title>Using bilingual dictionaries.</title>
<date>1987</date>
<journal>ELT Journal,</journal>
<volume>41</volume>
<issue>4</issue>
<note>cited By (since</note>
<contexts>
<context position="795" citStr="Thompson, 1987" startWordPosition="114" endWordPosition="115">erland Trento, Italy gozbalde@gmail.com biondo@google.com strappa@fbk.eu Abstract In this paper, we combine existing NLP techniques with minimal supervision to build memory tips according to the keyword method, a well established mnemonic device for second language learning. We present what we believe to be the first extrinsic evaluation of a creative sentence generator on a vocabulary learning task. The results demonstrate that NLP techniques can effectively support the development of resources for second language learning. 1 Introduction The keyword method is a mnemonic device (Cohen, 1987; Thompson, 1987) that is especially suitable for vocabulary acquisition in second language learning (Mizumoto and Kansai, 2009; Hummel, 2010; Shen, 2010; Tavakoli and Gerami, 2013). In this method, a target word in a foreign language L2 can be learned by a native speaker of another language L1 in two main steps: 1) one or more L1 words, possibly referring to a concrete entity, are chosen based on orthographic or phonetic similarity with the target word; 2) an L1 sentence is constructed in which an association between the translation of the target word and the keyword(s) is established, so that the learner, wh</context>
</contexts>
<marker>Thompson, 1987</marker>
<rawString>G. Thompson. 1987. Using bilingual dictionaries. ELT Journal, 41(4):282–286. cited By (since 1996)6.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>