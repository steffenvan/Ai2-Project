<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000011">
<title confidence="0.9973155">
Empirical Study of Unsupervised Chinese Word Segmentation Methods
for SMT on Large-scale Corpora
</title>
<author confidence="0.995847">
Xiaolin Wang Masao Utiyama Andrew Finch Eiichiro Sumita
</author>
<affiliation confidence="0.99501">
National Institute of Information and Communications Technology
</affiliation>
<email confidence="0.991239">
{xiaolin.wang,mutiyama,andrew.finch,eiichiro.sumita}@nict.go.jp
</email>
<sectionHeader confidence="0.994582" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999816272727273">
Unsupervised word segmentation (UWS)
can provide domain-adaptive segmenta-
tion for statistical machine translation
(SMT) without annotated data, and bilin-
gual UWS can even optimize segmenta-
tion for alignment. Monolingual UWS ap-
proaches of explicitly modeling the proba-
bilities of words through Dirichlet process
(DP) models or Pitman-Yor process (PYP)
models have achieved high accuracy, but
their bilingual counterparts have only been
carried out on small corpora such as ba-
sic travel expression corpus (BTEC) due to
the computational complexity. This paper
proposes an efficient unified PYP-based
monolingual and bilingual UWS method.
Experimental results show that the pro-
posed method is comparable to super-
vised segmenters on the in-domain NIST
OpenMT corpus, and yields a 0.96 BLEU
relative increase on NTCIR PatentMT cor-
pus which is out-of-domain.
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999663854545455">
Many languages, especially Asian languages such
as Chinese, Japanese and Myanmar, have no ex-
plicit word boundaries, thus word segmentation
(WS), that is, segmenting the continuous texts of
these languages into isolated words, is a prerequi-
site for many natural language processing applica-
tions including SMT.
Though supervised-learning approaches which
involve training segmenters on manually seg-
mented corpora are widely used (Chang et al.,
2008), yet the criteria for manually annotat-
ing words are arbitrary, and the available anno-
tated corpora are limited in both quantity and
genre variety. For example, in machine transla-
tion, there are various parallel corpora such as
BTEC for tourism-related dialogues (Paul, 2008)
and PatentMT in the patent domain (Goto et
al., 2011)1, but researchers working on Chinese-
related tasks often use the Stanford Chinese seg-
menter (Tseng et al., 2005) which is trained on a
small amount of annotated news text.
In contrast, UWS, spurred by the findings that
infants are able to use statistical cues to determine
word boundaries (Saffran et al., 1996), relies on
statistical criteria instead of manually crafted stan-
dards. UWS learns from unsegmented raw text,
which are available in large quantities, and thus
it has the potential to provide more accurate and
adaptive segmentation than supervised approaches
with less development effort being required.
The approaches of explicitly modeling the
probability of words(Brent, 1999; Venkataraman,
2001; Goldwater et al., 2006; Goldwater et al.,
2009; Mochihashi et al., 2009) significantly out-
performed a heuristic approach (Zhao and Kit,
2008) on the monolingual Chinese SIGHAN-MSR
corpus (Emerson, 2005), which inspired the work
of this paper.
However, bilingual approaches that model word
probabilities suffer from computational complex-
ity. Xu et al. (2008) proposed a bilingual method
by adding alignment into the generative model, but
was only able to test it on small-scale BTEC data.
Nguyen et al. (2010) used the local best alignment
to increase the speed of the Gibbs sampling in
training but the impact on accuracy was not ex-
plored.
This paper is dedicated to bilingual UWS on
large-scale corpora to support SMT. To this end,
we model bilingual UWS under a similar frame-
work with monolingual UWS in order to improve
efficiency, and replace Gibbs sampling with ex-
pectation maximization (EM) in training.
We aware that variational bayes (VB) may be
used for speeding up the training of DP-based
</bodyText>
<footnote confidence="0.983071">
1http://ntcir.nii.ac.jp/PatentMT
</footnote>
<page confidence="0.971496">
752
</page>
<bodyText confidence="0.975189125">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 752–758,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
or PYP-based bilingual UWS. However, VB re-
quires formulating the m expectations of (m −1)-
dimensional marginal distributions, where m is
the number of hidden variables. For UWS, the
hidden variables are indicators that identify sub-
strings of sentences in the corpus as words. These
variables are large in number and it is not clear
how to apply VB to UWS, and as far the authors
aware there is no previous work related to the ap-
plication of VB to monolingual UWS. Therefore,
we have not explored VB methods in this paper,
but we do show that our method is superior to the
existing methods.
The contributions of this paper include,
</bodyText>
<listItem confidence="0.999272714285714">
• state-of-the-art accuracy in monolingual
UWS;
• the first bilingual UWS method practical for
large corpora;
• improvement of BLEU scores compared
to supervised Stanford Chinese word seg-
menter.
</listItem>
<sectionHeader confidence="0.979073" genericHeader="introduction">
2 Methods
</sectionHeader>
<bodyText confidence="0.999882857142857">
This section describes our unified monolingual
and bilingual UWS scheme. Table 1 lists the main
notation. The set F is chosen to represent an un-
segmented foreign language sentence (a sequence
of characters), because an unsegmented sentence
can be seen as the set of all possible segmentations
of the sentence denoted F, i.e. F ∈ F.
</bodyText>
<table confidence="0.961355933333333">
Notation Meaning
F an unsegmented foreign sentence
Fk′ unsegmented substring of the un-
k derlying string of F from k to k′
F a segmented foreign sentence
fj the j-th foreign word
M monolingual segmentation model
PM(x) probability of x being a word ac-
cording to M
E a tokenized English sentence
ez the i-th English word
(F,E) a bilingual sentence pair
B bilingual segmentation model
PB(x|ez) probability of x being a word ac-
cording to B given ez
</table>
<tableCaption confidence="0.999817">
Table 1: Main Notation.
</tableCaption>
<bodyText confidence="0.9900685">
Monolingual and bilingual WS can be formu-
lated as follows, respectively,
</bodyText>
<equation confidence="0.9836785">
Fˆ(F) = argmax P(F|F, M), (1)
F∈F
XFˆ(F, E) = argmax
F∈F a P(F, a|F, E, B), (2)
</equation>
<bodyText confidence="0.999105">
where a is an alignment between F and E. The
English sentence E is used in the generation of a
segmented sentence F.
UWS learns models by maximizing the likeli-
hood of the unsegmented corpus, formulated as,
</bodyText>
<equation confidence="0.997699857142857">
Y 3 X ´
Mˆ = argmax P(F|M) , (3)
M F∈F F∈F
X ´
P (F, a|F, E, B) .
a
(4)
</equation>
<bodyText confidence="0.99940225">
Our method of learning M and B proceeds in a
similar manner to the EM algorithm. The follow-
ing two operations are performed iteratively for
each sentence (pair).
</bodyText>
<listItem confidence="0.8864545">
• Exclude the previous expected counts of the
current sentence (pair) from the model, and
</listItem>
<bodyText confidence="0.607777333333333">
then derive the current sentence in all pos-
sible ways, calculating the new expected
counts for the words (see Section 2.1), that
is, we calculate the expected probabilities of
the Fk′ kbeing words given the data excluding
F, i.e. EF/{F}(P(Fk′
</bodyText>
<equation confidence="0.9812925">
k |F)) = P(Fk′
k |F, M)
</equation>
<bodyText confidence="0.8257012">
in a similar manner to the marginalization in
the Gibbs sampling process which we are re-
placing;
• Update the respective model M or B accord-
ing to these expectations (see Section2.2).
</bodyText>
<subsectionHeader confidence="0.687502">
2.1 Expectation
2.1.1 Monolingual Expectation
</subsectionHeader>
<bodyText confidence="0.995297285714286">
P(Fk′ k|F, M) is the marginal probability of all
the possible F ∈ F that contain Fk′
k as a word,
which can be calculated efficiently through dy-
namic programming (the process is similar to the
foreward-backward algorithm in training a hidden
Markov model (HMM) (Rabiner, 1989)):
</bodyText>
<equation confidence="0.9571885">
Pa(k − u)PM(Fkk−u)
Pb(k′ + u)PM(Fk′+u
k′ )
P(Fk′
k |F,M) = Pa(k)PM(Fk′
k )Pb(k′), (5)
Bˆ = argmax Y 3 X
B (F,E)∈B F∈F
Pa(k) = XU
u=1
Pb(k′) = XU
u=1
</equation>
<page confidence="0.993146">
753
</page>
<bodyText confidence="0.999959833333333">
where U is the predefined maximum length of for-
eign language words, Pa(k) and Pb(k′) are the
forward and backward probabilities, respectively.
This section uses a unigram model for description
convenience, but the method can be extended to
n-gram models.
</bodyText>
<subsubsectionHeader confidence="0.583954">
2.1.2 Bilingual Expectation
</subsubsectionHeader>
<bodyText confidence="0.6597278">
P(Fk′
k |F, E, B) is the marginal probability of all
the possible F ∈ F that contain Fk′
k as a word and
are aligned with E, formulated as:
</bodyText>
<equation confidence="0.946461666666667">
P(Fk′
k |F, E, B) = � � P(F, a|E,B)
a
F∈F
Fk′
k ∈F
P(aj|j, I, J)PB(fj|eaj)
P(aj|j, I, J)PB(fj|eaj),
(6)
</equation>
<bodyText confidence="0.999973555555556">
where J and I are the number of foreign and En-
glish words, respectively, and aj is the position of
the English word that is aligned to fj in the align-
ment a. For the alignment we employ an approx-
imation to IBM model 2 (Brown et al., 1993; Och
and Ney, 2003) described below.
We define the conditional probability of fj
given the corresponding English sentence E and
the model B as:
</bodyText>
<equation confidence="0.9887285">
PB(fj|E) = � P(aj|j,I, J)PB(fj|eaj) (7)
a
</equation>
<bodyText confidence="0.992060666666667">
Then, the previous dynamic programming
method can be extended to the bilingual expecta-
tion
</bodyText>
<equation confidence="0.996017692307692">
Pa(k − u|E)PB(Fkk−u|E)
Pb(k′ + u|E)PB(Fk′+u
k′ |E)
P(Fk′
k |F,E,B) = Pa(k|E)PB(Fk′
k |E)Pb(k′|E).
(8)
Eq. 7 can be rewritten (as in IBM model 2):
I
PB(fj|E) = P∗(i|j, I, J)PB(fj|ei) (9)
i=1
P∗(i|j, I, J) = � P(aj|, j, I, J)
a:aj=i
</equation>
<bodyText confidence="0.999862">
In order to maintain both speed and accuracy, the
following window function is adopted
</bodyText>
<equation confidence="0.9866485">
P∗(i|j, I, J) ≈ P∗(i|k, I, K) =
{ 0 otherwise
e−|i−kI/K|/Q |i − kI/K |S Sb/2
Aφ ei is empty word (10)
</equation>
<bodyText confidence="0.999926166666667">
where K is the number of characters in F, and
the k-th character is the start of the word fj, since
j and J are unknown during the computation of
dynamic programming. Sb is the window size, Aφ
is the prior probability of an empty English word,
and Q ensures all the items sum to 1.
</bodyText>
<subsectionHeader confidence="0.983805">
2.2 Maximization
</subsectionHeader>
<bodyText confidence="0.9733296">
Inspired by (Teh, 2006; Mochihashi et al., 2009;
Neubig et al., 2010; Teh and Jordan, 2010), we
employ a Pitman-Yor process model to build the
segmentation model M or B. The monolingual
model M is
</bodyText>
<equation confidence="0.9970438">
PM(fj) =
max(n(fj) − d, 0) + (0 + d · nM)G0(fj)
Ef′jn(fj) + 0
��,
nM = ��{fj |n(fj) &gt; d} (11)
</equation>
<bodyText confidence="0.9163762">
where fj is a foreign language word, and n(fj) is
the observed counts of fj, 0 is named the strength
parameter, G0(fj) is named the base distribution
of fj, and d is the discount.
The bilingual model is
</bodyText>
<equation confidence="0.9973198">
PB(fj|ei) =
max(n(fj, ei) − d, 0) + (0 + d · nei)G0(fj|ei)
Ef′j n(fj, ei) + 0
��.
nei = ��{x  |n(x, ei) &gt; d} (12)
</equation>
<bodyText confidence="0.882098">
In Eqs. 11 and 12,
</bodyText>
<equation confidence="0.561927142857143">
n(fj) = � P(fj|F,M) (13)
F∈F
n(fj, ei) =
�
(F,E)∈B
�≈
F∈F
Fjk =Fk′
k
�=
F∈F
fjk =Fk′
k
J
�
a j=1
J
�
j=1 a
Pa(k|E) = U
u=1
U
Pb(k′|E) =
u=1
�I .
i′=1 P∗(i′|j, I, J)PB(fj|ei′)
(14)
P(fj|F,E, B) P∗(i|j,I,J)PB(fj|ei)
</equation>
<page confidence="0.997024">
754
</page>
<sectionHeader confidence="0.990552" genericHeader="method">
3 Complexity Analysis
</sectionHeader>
<bodyText confidence="0.994239321428571">
The computational complexity of our method is
linear in the number of iterations, the size of the
corpus, and the complexity of calculating the ex-
pectations on each sentence or sentence pair. In
practical applications, the size of the corpus is
fixed, and we found empirically that the number
of iterations required by the proposed method for
convergence is usually small (less than five itera-
tions). We now look in more detail at the complex-
ity of the expectation calculation in monolingual
and bilingual models.
The monolingual expectation is calculated ac-
cording to Eq. 5; the complexity is linear in the
length of sentences and the square of the prede-
fined maximum length of words. Thus its overall
complexity is
Ounigram
monoling = O(Ni|F|KU2), (15)
where Ni is the number of iterations, K is the av-
erage number of characters per sentence, and U is
the predefined maximum length of words.
For the monolingual bigram model, the number
of states in the HMM is U times more than that
of the monolingual unigram model, as the states at
specific position of F are not only related to the
length of the current word, but also related to the
length of the word before it. Thus its complexity
is U2 times the unigram model’s complexity:
</bodyText>
<equation confidence="0.644397">
Obigram= O(Ni|F|KU4). (16)
monoling
</equation>
<bodyText confidence="0.9996734">
The bilingual expectation is given by Eq. 8,
whose complexity is the same as the monolingual
case. However, the complexity of calculating the
transition probability, in Eqs. 9 and 10, is O(Sb).
Thus its overall complexity is:
</bodyText>
<equation confidence="0.8319545">
Ounigram
biling = O(Ni|F|KU2Sb). (17)
</equation>
<sectionHeader confidence="0.997334" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9998512">
In this section, the proposed method is first val-
idated on monolingual segmentation tasks, and
then evaluated in the context of SMT to study
whether the translation quality, measured by
BLEU, can be improved.
</bodyText>
<subsectionHeader confidence="0.779326">
4.1 Experimental Settings
4.1.1 Experimental Corpora
</subsectionHeader>
<bodyText confidence="0.999011666666667">
Two monolingual corpora and two bilingual cor-
pora are used (Table 2). CHILDES (MacWhin-
ney and Snow, 1985) is the most common test
</bodyText>
<table confidence="0.9987412">
Corpus Type # Sentences # Characters
CHILDES Mono. 9,790 95,809
SIGHAN-MSR Mono. 90,903 4,234,824
OpenMT06 Biling. 437,004 19,692,605
PatentMT9 Biling. 1,004,000 63,130,757
</table>
<tableCaption confidence="0.998596">
Table 2: Experimental Corpora
</tableCaption>
<bodyText confidence="0.998839588235294">
corpus for UWS methods. The SIGHAN-MSR
corpus (Emerson, 2005) consists of manually seg-
mented simplified Chinese news text, released in
the SIGHAN bakeoff 2005 shared tasks.
The first bilingual corpus: OpenMT06 was used
in the NIST open machine translation 2006 Eval-
uation 2. We removed the United Nations cor-
pus and the traditional Chinese data sets from the
constraint training resources. The data sets of
NIST Eval 2002 to 2005 were used as the develop-
ment for MERT tuning (Och, 2003). This data set
mainly consists of news text 3. PatentMT9 is from
the shared task of NTCIR-9 patent machine trans-
lation . The training set consists of 1 million par-
allel sentences extracted from patent documents,
and the development set and test set both consist
of 2000 sentences.
</bodyText>
<subsectionHeader confidence="0.9344925">
4.1.2 Performance Measurement and
Baseline Methods
</subsectionHeader>
<bodyText confidence="0.999974181818182">
For the monolingual tasks, the F1 score against
the gold annotation is adopted to measure the ac-
curacy. The results reported in related papers are
listed for comparison.
For the bilingual tasks, the publicly available
system of Moses (Koehn et al., 2007) with default
settings is employed to perform machine transla-
tion, and BLEU (Papineni et al., 2002) was used
to evaluate the quality. Character-based segmen-
tation, LDC segmenter and Stanford Chinese seg-
menters were used as the baseline methods.
</bodyText>
<subsectionHeader confidence="0.591479">
4.1.3 Parameter settings
</subsectionHeader>
<bodyText confidence="0.999741111111111">
The parameters are tuned on held-out data sets.
The maximum length of foreign language words
is set to 4. For the PYP model, the base distri-
bution adopts the formula in (Chung and Gildea,
2009), and the strength parameter is set to 1.0, and
the discount is set to 1.0 x 10−6.
For bilingual segmentation,the size of the align-
ment window is set to 6; the probability AO of for-
eign language words being generated by an empty
</bodyText>
<footnote confidence="0.998528">
2http://www.itl.nist.gov/iad/mig/
/tests/mt/2006/
3It also contains a small number of web blogs
</footnote>
<page confidence="0.990908">
755
</page>
<table confidence="0.997979375">
Method Accuracy Time
CHILD. MSR CHILD. MSR
NPY(bigram)a 0.750 0.802 17 m –
NPY(trigram)a 0.757 0.807 – –
HDP(bigram)b 0.723 – 10 h –
Fitnessc – 0.667 – –
Prop.(unigram) 0.729 0.804 3 s 50 s
Prop.(bigram) 0.774 0.806 15 s 2530 s
</table>
<tableCaption confidence="0.8259455">
a by (Mochihashi et al.,2009);
b by (Goldwater et al.,2009);
c by (Zhao and Kit, 2008).
Table 3: Results on Monolingual Corpora.
</tableCaption>
<bodyText confidence="0.998926666666667">
English word, was set to 0.3.
The training was started from assuming that
there was no previous segmentations on each sen-
tence (pair), and the number of iterations was
fixed. It was set to 3 for the monolingual unigram
model, and 2 for the bilingual unigram model,
which provided slightly higher BLEU scores on
the development set than the other settings. The
monolingual bigram model, however, was slower
to converge, so we started it from the segmenta-
tions of the unigram model, and using 10 itera-
tions.
</bodyText>
<subsectionHeader confidence="0.996466">
4.2 Monolingual Segmentation Results
</subsectionHeader>
<bodyText confidence="0.999961714285714">
In monolingual segmentation, the proposed meth-
ods with both unigram and bigram models were
tested. Experimental results show that they are
competitive to state-of-the-art baselines in both ac-
curacy and speed (Table 3). Note that the com-
parison of speed is only for reference because the
times are obtained from their respective papers.
</bodyText>
<subsectionHeader confidence="0.999517">
4.3 Bilingual Segmentation Results
</subsectionHeader>
<bodyText confidence="0.998992470588235">
Table 4 presents the BLEU scores for Moses using
different segmentation methods. Each experiment
was performed three times. The proposed method
with monolingual bigram model performed poorly
on the Chinese monolingual segmentation task;
thus, it was not tested. We intended to test (Mochi-
hashi et al., 2009), but found it impracticable on
large-scale corpora.
The experimental results show that the proposed
UWS methods are comparable to the Stanford seg-
menters on the OpenMT06 corpus, while achieves
a 0.96 BLEU increase on the PatentMT9 corpus.
This is because this corpus is out-of-domain for
the supervised segmenters. The CTB and PKU
Stanford segmenter were both trained on anno-
tated news text, which was the major domain of
OpenMT06.
</bodyText>
<table confidence="0.999242">
Method BLEU
OpenMT06 PatentMT9
Character 29.50 f 0.03 28.36 f 0.09
LDC 31.33 f 0.10 30.22 f 0.14
Stanford(CTB) 31.68 f 0.25 30.77 f 0.13
Stanford(PKU) 31.54 f 0.13 30.86 f 0.04
Prop.(mono.) 31.47 f 0.18 31.62 f 0.06
Prop.(biling.) 31.61 f 0.14 31.73 f 0.05
</table>
<tableCaption confidence="0.998672">
Table 4: Results on Bilingual Corpora.
</tableCaption>
<table confidence="0.9996455">
Method Time
OpenMT06 PatentMT9
Prop.(mono.) 28 m 1 h 01 m
Prop.(biling.) 2 h 25 m 5 h 02 m
</table>
<tableCaption confidence="0.999463">
Table 5: Time Costs on Bilingual Corpora.
</tableCaption>
<bodyText confidence="0.9963835">
Table 5 presents the run times of the proposed
methods on the bilingual corpora. The program
is single threaded and implemented in C++. The
time cost of the bilingual models is about 5 times
that of the monolingual model, which is consistent
with the complexity analysis in Section 3.
</bodyText>
<sectionHeader confidence="0.998779" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999880038461539">
This paper is devoted to large-scale Chinese UWS
for SMT. An efficient unified monolingual and
bilingual UWS method is proposed and applied to
large-scale bilingual corpora.
Complexity analysis shows that our method is
capable of scaling to large-scale corpora. This was
verified by experiments on a corpus of 1-million
sentence pairs on which traditional MCMC ap-
proaches would struggle (Xu et al., 2008).
The proposed method does not require any
annotated data, but the SMT system with it
can achieve comparable performance compared
to state-of-the-art supervised word segmenters
trained on precious annotated data. Moreover,
the proposed method yields 0.96 BLEU improve-
ment relative to supervised word segmenters on
an out-of-domain corpus. Thus, we believe that
the proposed method would benefit SMT related to
low-resource languages where annotated data are
scare, and would also find application in domains
that differ too greatly from the domains on which
supervised word segmenters were trained.
In future research, we plan to improve the bilin-
gual UWS through applying VB and integrating
more accurate alignment models such as HMM
models and IBM model 4.
</bodyText>
<page confidence="0.99744">
756
</page>
<sectionHeader confidence="0.983742" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999505218181818">
Michael R Brent. 1999. An efficient, probabilistically
sound algorithm for segmentation and word discov-
ery. Machine Learning, 34(1-3):71–105.
Peter F Brown, Vincent J Della Pietra, Stephen A Della
Pietra, and Robert L Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Computational linguistics, 19(2):263–311.
Pi-Chuan Chang, Michel Galley, and Christopher D
Manning. 2008. Optimizing Chinese word segmen-
tation for machine translation performance. In Pro-
ceedings of the 3rd Workshop on Statistical Machine
Translation, pages 224–232. Association for Com-
putational Linguistics.
Tagyoung Chung and Daniel Gildea. 2009. Unsu-
pervised tokenization for machine translation. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume
2-Volume 2, pages 718–726. Association for Com-
putational Linguistics.
Thomas Emerson. 2005. The second international chi-
nese word segmentation bakeoff. In Proceedings
of the 4th SIGHAN Workshop on Chinese Language
Processing, volume 133.
Sharon Goldwater, Thomas L Griffiths, and Mark John-
son. 2006. Contextual dependencies in unsu-
pervised word segmentation. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th Annual Meeting of the As-
sociation for Computational Linguistics, pages 673–
680. Association for Computational Linguistics.
Sharon Goldwater, Thomas L Griffiths, and Mark John-
son. 2009. A Bayesian framework for word seg-
mentation: exploring the effects of context. Cogni-
tion, 112(1):21–54.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K Tsou. 2011. Overview of the patent
machine translation task at the NTCIR-9 workshop.
In Proceedings ofNTCIR, volume 9, pages 559–578.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177–180. Association for Computational Lin-
guistics.
Brian MacWhinney and Catherine Snow. 1985. The
child language data exchange system. Journal of
child language, 12(2):271–296.
Daichi Mochihashi, Takeshi Yamada, and Naonori
Ueda. 2009. Bayesian unsupervised word segmen-
tation with nested Pitman-Yor language modeling.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP: Volume 1-Volume 1, pages 100–108.
Association for Computational Linguistics.
Graham Neubig, Masato Mimura, Shinsuke Mori, and
Tatsuya Kawahara. 2010. Learning a language
model from continuous speech. In InterSpeech,
pages 1053–1056.
ThuyLinh Nguyen, Stephan Vogel, and Noah A Smith.
2010. Nonparametric word segmentation for ma-
chine translation. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics,
pages 815–823. Association for Computational Lin-
guistics.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19–51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics-Volume 1, pages 160–167. As-
sociation for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, pages 311–318. Association
for Computational Linguistics.
Michael Paul. 2008. Overview of the IWSLT 2008
evaluation campaign. In Proceedings of the Interna-
tional Workshop on Spoken Language Translation,
pages 1–17.
Lawrence R Rabiner. 1989. A tutorial on hidden
Markov models and selected applications in speech
recognition. Proceedings of the IEEE, 77(2):257–
286.
Jenny R Saffran, Richard N Aslin, and Elissa L New-
port. 1996. Statistical learning by 8-month-old in-
fants. Science, 274(5294):1926–1928.
Yee Whye Teh and Michael I Jordan. 2010. Hierar-
chical Bayesian nonparametric models with appli-
cations. Bayesian Nonparametrics: Principles and
Practice, pages 158–207.
Yee Whye Teh. 2006. A hierarchical Bayesian lan-
guage model based on Pitman-Yor processes. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th Annual
Meeting on Association for Computational Linguis-
tics, pages 985–992. Association for Computational
Linguistics.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A con-
ditional random field word segmenter for SIGHAN
Bakeoff 2005. In Proceedings of the 4th SIGHAN
Workshop on Chinese Language Processing, volume
171. Jeju Island, Korea.
</reference>
<page confidence="0.973109">
757
</page>
<reference confidence="0.997691">
Anand Venkataraman. 2001. A statistical model for
word discovery in transcribed speech. Computa-
tional Linguistics, 27(3):351–372.
Jia Xu, Jianfeng Gao, Kristina Toutanova, and Her-
mann Ney. 2008. Bayesian semi-supervised
Chinese word segmentation for statistical machine
translation. In Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics-
Volume 1, pages 1017–1024. Association for Com-
putational Linguistics.
Hai Zhao and Chunyu Kit. 2008. An empirical com-
parison of goodness measures for unsupervised chi-
nese word segmentation with a unified framework.
In Proceedings of the 3rd International Joint Con-
ference on Natural Language Processing, pages 9–
16.
</reference>
<page confidence="0.997071">
758
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.695155">
<title confidence="0.998474">Empirical Study of Unsupervised Chinese Word Segmentation for SMT on Large-scale Corpora</title>
<author confidence="0.809398">Xiaolin Wang Masao Utiyama Andrew Finch Eiichiro Sumita</author>
<affiliation confidence="0.81276">National Institute of Information and Communications</affiliation>
<abstract confidence="0.992757347826087">Unsupervised word segmentation (UWS) can provide domain-adaptive segmentation for statistical machine translation (SMT) without annotated data, and bilingual UWS can even optimize segmentation for alignment. Monolingual UWS approaches of explicitly modeling the probabilities of words through Dirichlet process (DP) models or Pitman-Yor process (PYP) models have achieved high accuracy, but their bilingual counterparts have only been carried out on small corpora such as basic travel expression corpus (BTEC) due to the computational complexity. This paper proposes an efficient unified PYP-based monolingual and bilingual UWS method. Experimental results show that the proposed method is comparable to supervised segmenters on the in-domain NIST OpenMT corpus, and yields a 0.96 BLEU relative increase on NTCIR PatentMT corpus which is out-of-domain.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael R Brent</author>
</authors>
<title>An efficient, probabilistically sound algorithm for segmentation and word discovery.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="2623" citStr="Brent, 1999" startWordPosition="380" endWordPosition="381">ese segmenter (Tseng et al., 2005) which is trained on a small amount of annotated news text. In contrast, UWS, spurred by the findings that infants are able to use statistical cues to determine word boundaries (Saffran et al., 1996), relies on statistical criteria instead of manually crafted standards. UWS learns from unsegmented raw text, which are available in large quantities, and thus it has the potential to provide more accurate and adaptive segmentation than supervised approaches with less development effort being required. The approaches of explicitly modeling the probability of words(Brent, 1999; Venkataraman, 2001; Goldwater et al., 2006; Goldwater et al., 2009; Mochihashi et al., 2009) significantly outperformed a heuristic approach (Zhao and Kit, 2008) on the monolingual Chinese SIGHAN-MSR corpus (Emerson, 2005), which inspired the work of this paper. However, bilingual approaches that model word probabilities suffer from computational complexity. Xu et al. (2008) proposed a bilingual method by adding alignment into the generative model, but was only able to test it on small-scale BTEC data. Nguyen et al. (2010) used the local best alignment to increase the speed of the Gibbs samp</context>
</contexts>
<marker>Brent, 1999</marker>
<rawString>Michael R Brent. 1999. An efficient, probabilistically sound algorithm for segmentation and word discovery. Machine Learning, 34(1-3):71–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<journal>Computational linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="7906" citStr="Brown et al., 1993" startWordPosition="1294" endWordPosition="1297">s section uses a unigram model for description convenience, but the method can be extended to n-gram models. 2.1.2 Bilingual Expectation P(Fk′ k |F, E, B) is the marginal probability of all the possible F ∈ F that contain Fk′ k as a word and are aligned with E, formulated as: P(Fk′ k |F, E, B) = � � P(F, a|E,B) a F∈F Fk′ k ∈F P(aj|j, I, J)PB(fj|eaj) P(aj|j, I, J)PB(fj|eaj), (6) where J and I are the number of foreign and English words, respectively, and aj is the position of the English word that is aligned to fj in the alignment a. For the alignment we employ an approximation to IBM model 2 (Brown et al., 1993; Och and Ney, 2003) described below. We define the conditional probability of fj given the corresponding English sentence E and the model B as: PB(fj|E) = � P(aj|j,I, J)PB(fj|eaj) (7) a Then, the previous dynamic programming method can be extended to the bilingual expectation Pa(k − u|E)PB(Fkk−u|E) Pb(k′ + u|E)PB(Fk′+u k′ |E) P(Fk′ k |F,E,B) = Pa(k|E)PB(Fk′ k |E)Pb(k′|E). (8) Eq. 7 can be rewritten (as in IBM model 2): I PB(fj|E) = P∗(i|j, I, J)PB(fj|ei) (9) i=1 P∗(i|j, I, J) = � P(aj|, j, I, J) a:aj=i In order to maintain both speed and accuracy, the following window function is adopted P∗(i</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F Brown, Vincent J Della Pietra, Stephen A Della Pietra, and Robert L Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pi-Chuan Chang</author>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>Optimizing Chinese word segmentation for machine translation performance.</title>
<date>2008</date>
<booktitle>In Proceedings of the 3rd Workshop on Statistical Machine Translation,</booktitle>
<pages>224--232</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1605" citStr="Chang et al., 2008" startWordPosition="219" endWordPosition="222">able to supervised segmenters on the in-domain NIST OpenMT corpus, and yields a 0.96 BLEU relative increase on NTCIR PatentMT corpus which is out-of-domain. 1 Introduction Many languages, especially Asian languages such as Chinese, Japanese and Myanmar, have no explicit word boundaries, thus word segmentation (WS), that is, segmenting the continuous texts of these languages into isolated words, is a prerequisite for many natural language processing applications including SMT. Though supervised-learning approaches which involve training segmenters on manually segmented corpora are widely used (Chang et al., 2008), yet the criteria for manually annotating words are arbitrary, and the available annotated corpora are limited in both quantity and genre variety. For example, in machine translation, there are various parallel corpora such as BTEC for tourism-related dialogues (Paul, 2008) and PatentMT in the patent domain (Goto et al., 2011)1, but researchers working on Chineserelated tasks often use the Stanford Chinese segmenter (Tseng et al., 2005) which is trained on a small amount of annotated news text. In contrast, UWS, spurred by the findings that infants are able to use statistical cues to determin</context>
</contexts>
<marker>Chang, Galley, Manning, 2008</marker>
<rawString>Pi-Chuan Chang, Michel Galley, and Christopher D Manning. 2008. Optimizing Chinese word segmentation for machine translation performance. In Proceedings of the 3rd Workshop on Statistical Machine Translation, pages 224–232. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tagyoung Chung</author>
<author>Daniel Gildea</author>
</authors>
<title>Unsupervised tokenization for machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>2</volume>
<pages>718--726</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="13448" citStr="Chung and Gildea, 2009" startWordPosition="2261" endWordPosition="2264">accuracy. The results reported in related papers are listed for comparison. For the bilingual tasks, the publicly available system of Moses (Koehn et al., 2007) with default settings is employed to perform machine translation, and BLEU (Papineni et al., 2002) was used to evaluate the quality. Character-based segmentation, LDC segmenter and Stanford Chinese segmenters were used as the baseline methods. 4.1.3 Parameter settings The parameters are tuned on held-out data sets. The maximum length of foreign language words is set to 4. For the PYP model, the base distribution adopts the formula in (Chung and Gildea, 2009), and the strength parameter is set to 1.0, and the discount is set to 1.0 x 10−6. For bilingual segmentation,the size of the alignment window is set to 6; the probability AO of foreign language words being generated by an empty 2http://www.itl.nist.gov/iad/mig/ /tests/mt/2006/ 3It also contains a small number of web blogs 755 Method Accuracy Time CHILD. MSR CHILD. MSR NPY(bigram)a 0.750 0.802 17 m – NPY(trigram)a 0.757 0.807 – – HDP(bigram)b 0.723 – 10 h – Fitnessc – 0.667 – – Prop.(unigram) 0.729 0.804 3 s 50 s Prop.(bigram) 0.774 0.806 15 s 2530 s a by (Mochihashi et al.,2009); b by (Goldwa</context>
</contexts>
<marker>Chung, Gildea, 2009</marker>
<rawString>Tagyoung Chung and Daniel Gildea. 2009. Unsupervised tokenization for machine translation. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2-Volume 2, pages 718–726. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Emerson</author>
</authors>
<title>The second international chinese word segmentation bakeoff.</title>
<date>2005</date>
<booktitle>In Proceedings of the 4th SIGHAN Workshop on Chinese Language Processing,</booktitle>
<volume>133</volume>
<contexts>
<context position="2847" citStr="Emerson, 2005" startWordPosition="412" endWordPosition="413">t al., 1996), relies on statistical criteria instead of manually crafted standards. UWS learns from unsegmented raw text, which are available in large quantities, and thus it has the potential to provide more accurate and adaptive segmentation than supervised approaches with less development effort being required. The approaches of explicitly modeling the probability of words(Brent, 1999; Venkataraman, 2001; Goldwater et al., 2006; Goldwater et al., 2009; Mochihashi et al., 2009) significantly outperformed a heuristic approach (Zhao and Kit, 2008) on the monolingual Chinese SIGHAN-MSR corpus (Emerson, 2005), which inspired the work of this paper. However, bilingual approaches that model word probabilities suffer from computational complexity. Xu et al. (2008) proposed a bilingual method by adding alignment into the generative model, but was only able to test it on small-scale BTEC data. Nguyen et al. (2010) used the local best alignment to increase the speed of the Gibbs sampling in training but the impact on accuracy was not explored. This paper is dedicated to bilingual UWS on large-scale corpora to support SMT. To this end, we model bilingual UWS under a similar framework with monolingual UWS</context>
<context position="11973" citStr="Emerson, 2005" startWordPosition="2019" endWordPosition="2020">rst validated on monolingual segmentation tasks, and then evaluated in the context of SMT to study whether the translation quality, measured by BLEU, can be improved. 4.1 Experimental Settings 4.1.1 Experimental Corpora Two monolingual corpora and two bilingual corpora are used (Table 2). CHILDES (MacWhinney and Snow, 1985) is the most common test Corpus Type # Sentences # Characters CHILDES Mono. 9,790 95,809 SIGHAN-MSR Mono. 90,903 4,234,824 OpenMT06 Biling. 437,004 19,692,605 PatentMT9 Biling. 1,004,000 63,130,757 Table 2: Experimental Corpora corpus for UWS methods. The SIGHAN-MSR corpus (Emerson, 2005) consists of manually segmented simplified Chinese news text, released in the SIGHAN bakeoff 2005 shared tasks. The first bilingual corpus: OpenMT06 was used in the NIST open machine translation 2006 Evaluation 2. We removed the United Nations corpus and the traditional Chinese data sets from the constraint training resources. The data sets of NIST Eval 2002 to 2005 were used as the development for MERT tuning (Och, 2003). This data set mainly consists of news text 3. PatentMT9 is from the shared task of NTCIR-9 patent machine translation . The training set consists of 1 million parallel sente</context>
</contexts>
<marker>Emerson, 2005</marker>
<rawString>Thomas Emerson. 2005. The second international chinese word segmentation bakeoff. In Proceedings of the 4th SIGHAN Workshop on Chinese Language Processing, volume 133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Contextual dependencies in unsupervised word segmentation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>673--680</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2667" citStr="Goldwater et al., 2006" startWordPosition="384" endWordPosition="387">) which is trained on a small amount of annotated news text. In contrast, UWS, spurred by the findings that infants are able to use statistical cues to determine word boundaries (Saffran et al., 1996), relies on statistical criteria instead of manually crafted standards. UWS learns from unsegmented raw text, which are available in large quantities, and thus it has the potential to provide more accurate and adaptive segmentation than supervised approaches with less development effort being required. The approaches of explicitly modeling the probability of words(Brent, 1999; Venkataraman, 2001; Goldwater et al., 2006; Goldwater et al., 2009; Mochihashi et al., 2009) significantly outperformed a heuristic approach (Zhao and Kit, 2008) on the monolingual Chinese SIGHAN-MSR corpus (Emerson, 2005), which inspired the work of this paper. However, bilingual approaches that model word probabilities suffer from computational complexity. Xu et al. (2008) proposed a bilingual method by adding alignment into the generative model, but was only able to test it on small-scale BTEC data. Nguyen et al. (2010) used the local best alignment to increase the speed of the Gibbs sampling in training but the impact on accuracy </context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>Sharon Goldwater, Thomas L Griffiths, and Mark Johnson. 2006. Contextual dependencies in unsupervised word segmentation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics, pages 673– 680. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>A Bayesian framework for word segmentation: exploring the effects of context.</title>
<date>2009</date>
<journal>Cognition,</journal>
<volume>112</volume>
<issue>1</issue>
<contexts>
<context position="2691" citStr="Goldwater et al., 2009" startWordPosition="388" endWordPosition="391">small amount of annotated news text. In contrast, UWS, spurred by the findings that infants are able to use statistical cues to determine word boundaries (Saffran et al., 1996), relies on statistical criteria instead of manually crafted standards. UWS learns from unsegmented raw text, which are available in large quantities, and thus it has the potential to provide more accurate and adaptive segmentation than supervised approaches with less development effort being required. The approaches of explicitly modeling the probability of words(Brent, 1999; Venkataraman, 2001; Goldwater et al., 2006; Goldwater et al., 2009; Mochihashi et al., 2009) significantly outperformed a heuristic approach (Zhao and Kit, 2008) on the monolingual Chinese SIGHAN-MSR corpus (Emerson, 2005), which inspired the work of this paper. However, bilingual approaches that model word probabilities suffer from computational complexity. Xu et al. (2008) proposed a bilingual method by adding alignment into the generative model, but was only able to test it on small-scale BTEC data. Nguyen et al. (2010) used the local best alignment to increase the speed of the Gibbs sampling in training but the impact on accuracy was not explored. This p</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2009</marker>
<rawString>Sharon Goldwater, Thomas L Griffiths, and Mark Johnson. 2009. A Bayesian framework for word segmentation: exploring the effects of context. Cognition, 112(1):21–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isao Goto</author>
<author>Bin Lu</author>
<author>Ka Po Chow</author>
<author>Eiichiro Sumita</author>
<author>Benjamin K Tsou</author>
</authors>
<title>Overview of the patent machine translation task at the NTCIR-9 workshop.</title>
<date>2011</date>
<booktitle>In Proceedings ofNTCIR,</booktitle>
<volume>9</volume>
<pages>559--578</pages>
<contexts>
<context position="1934" citStr="Goto et al., 2011" startWordPosition="272" endWordPosition="275">enting the continuous texts of these languages into isolated words, is a prerequisite for many natural language processing applications including SMT. Though supervised-learning approaches which involve training segmenters on manually segmented corpora are widely used (Chang et al., 2008), yet the criteria for manually annotating words are arbitrary, and the available annotated corpora are limited in both quantity and genre variety. For example, in machine translation, there are various parallel corpora such as BTEC for tourism-related dialogues (Paul, 2008) and PatentMT in the patent domain (Goto et al., 2011)1, but researchers working on Chineserelated tasks often use the Stanford Chinese segmenter (Tseng et al., 2005) which is trained on a small amount of annotated news text. In contrast, UWS, spurred by the findings that infants are able to use statistical cues to determine word boundaries (Saffran et al., 1996), relies on statistical criteria instead of manually crafted standards. UWS learns from unsegmented raw text, which are available in large quantities, and thus it has the potential to provide more accurate and adaptive segmentation than supervised approaches with less development effort b</context>
</contexts>
<marker>Goto, Lu, Chow, Sumita, Tsou, 2011</marker>
<rawString>Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and Benjamin K Tsou. 2011. Overview of the patent machine translation task at the NTCIR-9 workshop. In Proceedings ofNTCIR, volume 9, pages 559–578.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Moses: open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="12985" citStr="Koehn et al., 2007" startWordPosition="2185" endWordPosition="2188">ment for MERT tuning (Och, 2003). This data set mainly consists of news text 3. PatentMT9 is from the shared task of NTCIR-9 patent machine translation . The training set consists of 1 million parallel sentences extracted from patent documents, and the development set and test set both consist of 2000 sentences. 4.1.2 Performance Measurement and Baseline Methods For the monolingual tasks, the F1 score against the gold annotation is adopted to measure the accuracy. The results reported in related papers are listed for comparison. For the bilingual tasks, the publicly available system of Moses (Koehn et al., 2007) with default settings is employed to perform machine translation, and BLEU (Papineni et al., 2002) was used to evaluate the quality. Character-based segmentation, LDC segmenter and Stanford Chinese segmenters were used as the baseline methods. 4.1.3 Parameter settings The parameters are tuned on held-out data sets. The maximum length of foreign language words is set to 4. For the PYP model, the base distribution adopts the formula in (Chung and Gildea, 2009), and the strength parameter is set to 1.0, and the discount is set to 1.0 x 10−6. For bilingual segmentation,the size of the alignment w</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, et al. 2007. Moses: open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, pages 177–180. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian MacWhinney</author>
<author>Catherine Snow</author>
</authors>
<title>The child language data exchange system.</title>
<date>1985</date>
<journal>Journal of child language,</journal>
<volume>12</volume>
<issue>2</issue>
<contexts>
<context position="11684" citStr="MacWhinney and Snow, 1985" startWordPosition="1976" endWordPosition="1980">tation is given by Eq. 8, whose complexity is the same as the monolingual case. However, the complexity of calculating the transition probability, in Eqs. 9 and 10, is O(Sb). Thus its overall complexity is: Ounigram biling = O(Ni|F|KU2Sb). (17) 4 Experiments In this section, the proposed method is first validated on monolingual segmentation tasks, and then evaluated in the context of SMT to study whether the translation quality, measured by BLEU, can be improved. 4.1 Experimental Settings 4.1.1 Experimental Corpora Two monolingual corpora and two bilingual corpora are used (Table 2). CHILDES (MacWhinney and Snow, 1985) is the most common test Corpus Type # Sentences # Characters CHILDES Mono. 9,790 95,809 SIGHAN-MSR Mono. 90,903 4,234,824 OpenMT06 Biling. 437,004 19,692,605 PatentMT9 Biling. 1,004,000 63,130,757 Table 2: Experimental Corpora corpus for UWS methods. The SIGHAN-MSR corpus (Emerson, 2005) consists of manually segmented simplified Chinese news text, released in the SIGHAN bakeoff 2005 shared tasks. The first bilingual corpus: OpenMT06 was used in the NIST open machine translation 2006 Evaluation 2. We removed the United Nations corpus and the traditional Chinese data sets from the constraint tr</context>
</contexts>
<marker>MacWhinney, Snow, 1985</marker>
<rawString>Brian MacWhinney and Catherine Snow. 1985. The child language data exchange system. Journal of child language, 12(2):271–296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daichi Mochihashi</author>
<author>Takeshi Yamada</author>
<author>Naonori Ueda</author>
</authors>
<title>Bayesian unsupervised word segmentation with nested Pitman-Yor language modeling.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>1</volume>
<pages>100--108</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2717" citStr="Mochihashi et al., 2009" startWordPosition="392" endWordPosition="395">d news text. In contrast, UWS, spurred by the findings that infants are able to use statistical cues to determine word boundaries (Saffran et al., 1996), relies on statistical criteria instead of manually crafted standards. UWS learns from unsegmented raw text, which are available in large quantities, and thus it has the potential to provide more accurate and adaptive segmentation than supervised approaches with less development effort being required. The approaches of explicitly modeling the probability of words(Brent, 1999; Venkataraman, 2001; Goldwater et al., 2006; Goldwater et al., 2009; Mochihashi et al., 2009) significantly outperformed a heuristic approach (Zhao and Kit, 2008) on the monolingual Chinese SIGHAN-MSR corpus (Emerson, 2005), which inspired the work of this paper. However, bilingual approaches that model word probabilities suffer from computational complexity. Xu et al. (2008) proposed a bilingual method by adding alignment into the generative model, but was only able to test it on small-scale BTEC data. Nguyen et al. (2010) used the local best alignment to increase the speed of the Gibbs sampling in training but the impact on accuracy was not explored. This paper is dedicated to bilin</context>
<context position="8950" citStr="Mochihashi et al., 2009" startWordPosition="1486" endWordPosition="1489">): I PB(fj|E) = P∗(i|j, I, J)PB(fj|ei) (9) i=1 P∗(i|j, I, J) = � P(aj|, j, I, J) a:aj=i In order to maintain both speed and accuracy, the following window function is adopted P∗(i|j, I, J) ≈ P∗(i|k, I, K) = { 0 otherwise e−|i−kI/K|/Q |i − kI/K |S Sb/2 Aφ ei is empty word (10) where K is the number of characters in F, and the k-th character is the start of the word fj, since j and J are unknown during the computation of dynamic programming. Sb is the window size, Aφ is the prior probability of an empty English word, and Q ensures all the items sum to 1. 2.2 Maximization Inspired by (Teh, 2006; Mochihashi et al., 2009; Neubig et al., 2010; Teh and Jordan, 2010), we employ a Pitman-Yor process model to build the segmentation model M or B. The monolingual model M is PM(fj) = max(n(fj) − d, 0) + (0 + d · nM)G0(fj) Ef′jn(fj) + 0 ��, nM = ��{fj |n(fj) &gt; d} (11) where fj is a foreign language word, and n(fj) is the observed counts of fj, 0 is named the strength parameter, G0(fj) is named the base distribution of fj, and d is the discount. The bilingual model is PB(fj|ei) = max(n(fj, ei) − d, 0) + (0 + d · nei)G0(fj|ei) Ef′j n(fj, ei) + 0 ��. nei = ��{x |n(x, ei) &gt; d} (12) In Eqs. 11 and 12, n(fj) = � P(fj|F,M) (</context>
<context position="15355" citStr="Mochihashi et al., 2009" startWordPosition="2575" endWordPosition="2579"> both unigram and bigram models were tested. Experimental results show that they are competitive to state-of-the-art baselines in both accuracy and speed (Table 3). Note that the comparison of speed is only for reference because the times are obtained from their respective papers. 4.3 Bilingual Segmentation Results Table 4 presents the BLEU scores for Moses using different segmentation methods. Each experiment was performed three times. The proposed method with monolingual bigram model performed poorly on the Chinese monolingual segmentation task; thus, it was not tested. We intended to test (Mochihashi et al., 2009), but found it impracticable on large-scale corpora. The experimental results show that the proposed UWS methods are comparable to the Stanford segmenters on the OpenMT06 corpus, while achieves a 0.96 BLEU increase on the PatentMT9 corpus. This is because this corpus is out-of-domain for the supervised segmenters. The CTB and PKU Stanford segmenter were both trained on annotated news text, which was the major domain of OpenMT06. Method BLEU OpenMT06 PatentMT9 Character 29.50 f 0.03 28.36 f 0.09 LDC 31.33 f 0.10 30.22 f 0.14 Stanford(CTB) 31.68 f 0.25 30.77 f 0.13 Stanford(PKU) 31.54 f 0.13 30.</context>
</contexts>
<marker>Mochihashi, Yamada, Ueda, 2009</marker>
<rawString>Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda. 2009. Bayesian unsupervised word segmentation with nested Pitman-Yor language modeling. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1-Volume 1, pages 100–108. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Neubig</author>
<author>Masato Mimura</author>
<author>Shinsuke Mori</author>
<author>Tatsuya Kawahara</author>
</authors>
<title>Learning a language model from continuous speech.</title>
<date>2010</date>
<booktitle>In InterSpeech,</booktitle>
<pages>1053--1056</pages>
<contexts>
<context position="8971" citStr="Neubig et al., 2010" startWordPosition="1490" endWordPosition="1493">, J)PB(fj|ei) (9) i=1 P∗(i|j, I, J) = � P(aj|, j, I, J) a:aj=i In order to maintain both speed and accuracy, the following window function is adopted P∗(i|j, I, J) ≈ P∗(i|k, I, K) = { 0 otherwise e−|i−kI/K|/Q |i − kI/K |S Sb/2 Aφ ei is empty word (10) where K is the number of characters in F, and the k-th character is the start of the word fj, since j and J are unknown during the computation of dynamic programming. Sb is the window size, Aφ is the prior probability of an empty English word, and Q ensures all the items sum to 1. 2.2 Maximization Inspired by (Teh, 2006; Mochihashi et al., 2009; Neubig et al., 2010; Teh and Jordan, 2010), we employ a Pitman-Yor process model to build the segmentation model M or B. The monolingual model M is PM(fj) = max(n(fj) − d, 0) + (0 + d · nM)G0(fj) Ef′jn(fj) + 0 ��, nM = ��{fj |n(fj) &gt; d} (11) where fj is a foreign language word, and n(fj) is the observed counts of fj, 0 is named the strength parameter, G0(fj) is named the base distribution of fj, and d is the discount. The bilingual model is PB(fj|ei) = max(n(fj, ei) − d, 0) + (0 + d · nei)G0(fj|ei) Ef′j n(fj, ei) + 0 ��. nei = ��{x |n(x, ei) &gt; d} (12) In Eqs. 11 and 12, n(fj) = � P(fj|F,M) (13) F∈F n(fj, ei) = �</context>
</contexts>
<marker>Neubig, Mimura, Mori, Kawahara, 2010</marker>
<rawString>Graham Neubig, Masato Mimura, Shinsuke Mori, and Tatsuya Kawahara. 2010. Learning a language model from continuous speech. In InterSpeech, pages 1053–1056.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ThuyLinh Nguyen</author>
<author>Stephan Vogel</author>
<author>Noah A Smith</author>
</authors>
<title>Nonparametric word segmentation for machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>815--823</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3153" citStr="Nguyen et al. (2010)" startWordPosition="459" endWordPosition="462">being required. The approaches of explicitly modeling the probability of words(Brent, 1999; Venkataraman, 2001; Goldwater et al., 2006; Goldwater et al., 2009; Mochihashi et al., 2009) significantly outperformed a heuristic approach (Zhao and Kit, 2008) on the monolingual Chinese SIGHAN-MSR corpus (Emerson, 2005), which inspired the work of this paper. However, bilingual approaches that model word probabilities suffer from computational complexity. Xu et al. (2008) proposed a bilingual method by adding alignment into the generative model, but was only able to test it on small-scale BTEC data. Nguyen et al. (2010) used the local best alignment to increase the speed of the Gibbs sampling in training but the impact on accuracy was not explored. This paper is dedicated to bilingual UWS on large-scale corpora to support SMT. To this end, we model bilingual UWS under a similar framework with monolingual UWS in order to improve efficiency, and replace Gibbs sampling with expectation maximization (EM) in training. We aware that variational bayes (VB) may be used for speeding up the training of DP-based 1http://ntcir.nii.ac.jp/PatentMT 752 Proceedings of the 52nd Annual Meeting of the Association for Computati</context>
</contexts>
<marker>Nguyen, Vogel, Smith, 2010</marker>
<rawString>ThuyLinh Nguyen, Stephan Vogel, and Noah A Smith. 2010. Nonparametric word segmentation for machine translation. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 815–823. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="7926" citStr="Och and Ney, 2003" startWordPosition="1298" endWordPosition="1301">gram model for description convenience, but the method can be extended to n-gram models. 2.1.2 Bilingual Expectation P(Fk′ k |F, E, B) is the marginal probability of all the possible F ∈ F that contain Fk′ k as a word and are aligned with E, formulated as: P(Fk′ k |F, E, B) = � � P(F, a|E,B) a F∈F Fk′ k ∈F P(aj|j, I, J)PB(fj|eaj) P(aj|j, I, J)PB(fj|eaj), (6) where J and I are the number of foreign and English words, respectively, and aj is the position of the English word that is aligned to fj in the alignment a. For the alignment we employ an approximation to IBM model 2 (Brown et al., 1993; Och and Ney, 2003) described below. We define the conditional probability of fj given the corresponding English sentence E and the model B as: PB(fj|E) = � P(aj|j,I, J)PB(fj|eaj) (7) a Then, the previous dynamic programming method can be extended to the bilingual expectation Pa(k − u|E)PB(Fkk−u|E) Pb(k′ + u|E)PB(Fk′+u k′ |E) P(Fk′ k |F,E,B) = Pa(k|E)PB(Fk′ k |E)Pb(k′|E). (8) Eq. 7 can be rewritten (as in IBM model 2): I PB(fj|E) = P∗(i|j, I, J)PB(fj|ei) (9) i=1 P∗(i|j, I, J) = � P(aj|, j, I, J) a:aj=i In order to maintain both speed and accuracy, the following window function is adopted P∗(i|j, I, J) ≈ P∗(i|k, </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1,</booktitle>
<pages>160--167</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="12398" citStr="Och, 2003" startWordPosition="2091" endWordPosition="2092">. 90,903 4,234,824 OpenMT06 Biling. 437,004 19,692,605 PatentMT9 Biling. 1,004,000 63,130,757 Table 2: Experimental Corpora corpus for UWS methods. The SIGHAN-MSR corpus (Emerson, 2005) consists of manually segmented simplified Chinese news text, released in the SIGHAN bakeoff 2005 shared tasks. The first bilingual corpus: OpenMT06 was used in the NIST open machine translation 2006 Evaluation 2. We removed the United Nations corpus and the traditional Chinese data sets from the constraint training resources. The data sets of NIST Eval 2002 to 2005 were used as the development for MERT tuning (Och, 2003). This data set mainly consists of news text 3. PatentMT9 is from the shared task of NTCIR-9 patent machine translation . The training set consists of 1 million parallel sentences extracted from patent documents, and the development set and test set both consist of 2000 sentences. 4.1.2 Performance Measurement and Baseline Methods For the monolingual tasks, the F1 score against the gold annotation is adopted to measure the accuracy. The results reported in related papers are listed for comparison. For the bilingual tasks, the publicly available system of Moses (Koehn et al., 2007) with default</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 160–167. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="13084" citStr="Papineni et al., 2002" startWordPosition="2201" endWordPosition="2204">m the shared task of NTCIR-9 patent machine translation . The training set consists of 1 million parallel sentences extracted from patent documents, and the development set and test set both consist of 2000 sentences. 4.1.2 Performance Measurement and Baseline Methods For the monolingual tasks, the F1 score against the gold annotation is adopted to measure the accuracy. The results reported in related papers are listed for comparison. For the bilingual tasks, the publicly available system of Moses (Koehn et al., 2007) with default settings is employed to perform machine translation, and BLEU (Papineni et al., 2002) was used to evaluate the quality. Character-based segmentation, LDC segmenter and Stanford Chinese segmenters were used as the baseline methods. 4.1.3 Parameter settings The parameters are tuned on held-out data sets. The maximum length of foreign language words is set to 4. For the PYP model, the base distribution adopts the formula in (Chung and Gildea, 2009), and the strength parameter is set to 1.0, and the discount is set to 1.0 x 10−6. For bilingual segmentation,the size of the alignment window is set to 6; the probability AO of foreign language words being generated by an empty 2http:/</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 311–318. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Paul</author>
</authors>
<title>Overview of the IWSLT</title>
<date>2008</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation,</booktitle>
<pages>1--17</pages>
<contexts>
<context position="1880" citStr="Paul, 2008" startWordPosition="264" endWordPosition="265">ies, thus word segmentation (WS), that is, segmenting the continuous texts of these languages into isolated words, is a prerequisite for many natural language processing applications including SMT. Though supervised-learning approaches which involve training segmenters on manually segmented corpora are widely used (Chang et al., 2008), yet the criteria for manually annotating words are arbitrary, and the available annotated corpora are limited in both quantity and genre variety. For example, in machine translation, there are various parallel corpora such as BTEC for tourism-related dialogues (Paul, 2008) and PatentMT in the patent domain (Goto et al., 2011)1, but researchers working on Chineserelated tasks often use the Stanford Chinese segmenter (Tseng et al., 2005) which is trained on a small amount of annotated news text. In contrast, UWS, spurred by the findings that infants are able to use statistical cues to determine word boundaries (Saffran et al., 1996), relies on statistical criteria instead of manually crafted standards. UWS learns from unsegmented raw text, which are available in large quantities, and thus it has the potential to provide more accurate and adaptive segmentation tha</context>
</contexts>
<marker>Paul, 2008</marker>
<rawString>Michael Paul. 2008. Overview of the IWSLT 2008 evaluation campaign. In Proceedings of the International Workshop on Spoken Language Translation, pages 1–17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence R Rabiner</author>
</authors>
<title>A tutorial on hidden Markov models and selected applications in speech recognition.</title>
<date>1989</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<volume>77</volume>
<issue>2</issue>
<pages>286</pages>
<contexts>
<context position="6986" citStr="Rabiner, 1989" startWordPosition="1118" endWordPosition="1119">probabilities of the Fk′ kbeing words given the data excluding F, i.e. EF/{F}(P(Fk′ k |F)) = P(Fk′ k |F, M) in a similar manner to the marginalization in the Gibbs sampling process which we are replacing; • Update the respective model M or B according to these expectations (see Section2.2). 2.1 Expectation 2.1.1 Monolingual Expectation P(Fk′ k|F, M) is the marginal probability of all the possible F ∈ F that contain Fk′ k as a word, which can be calculated efficiently through dynamic programming (the process is similar to the foreward-backward algorithm in training a hidden Markov model (HMM) (Rabiner, 1989)): Pa(k − u)PM(Fkk−u) Pb(k′ + u)PM(Fk′+u k′ ) P(Fk′ k |F,M) = Pa(k)PM(Fk′ k )Pb(k′), (5) Bˆ = argmax Y 3 X B (F,E)∈B F∈F Pa(k) = XU u=1 Pb(k′) = XU u=1 753 where U is the predefined maximum length of foreign language words, Pa(k) and Pb(k′) are the forward and backward probabilities, respectively. This section uses a unigram model for description convenience, but the method can be extended to n-gram models. 2.1.2 Bilingual Expectation P(Fk′ k |F, E, B) is the marginal probability of all the possible F ∈ F that contain Fk′ k as a word and are aligned with E, formulated as: P(Fk′ k |F, E, B) = �</context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>Lawrence R Rabiner. 1989. A tutorial on hidden Markov models and selected applications in speech recognition. Proceedings of the IEEE, 77(2):257– 286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny R Saffran</author>
<author>Richard N Aslin</author>
<author>Elissa L Newport</author>
</authors>
<title>Statistical learning by 8-month-old infants.</title>
<date>1996</date>
<journal>Science,</journal>
<volume>274</volume>
<issue>5294</issue>
<contexts>
<context position="2245" citStr="Saffran et al., 1996" startWordPosition="324" endWordPosition="327">for manually annotating words are arbitrary, and the available annotated corpora are limited in both quantity and genre variety. For example, in machine translation, there are various parallel corpora such as BTEC for tourism-related dialogues (Paul, 2008) and PatentMT in the patent domain (Goto et al., 2011)1, but researchers working on Chineserelated tasks often use the Stanford Chinese segmenter (Tseng et al., 2005) which is trained on a small amount of annotated news text. In contrast, UWS, spurred by the findings that infants are able to use statistical cues to determine word boundaries (Saffran et al., 1996), relies on statistical criteria instead of manually crafted standards. UWS learns from unsegmented raw text, which are available in large quantities, and thus it has the potential to provide more accurate and adaptive segmentation than supervised approaches with less development effort being required. The approaches of explicitly modeling the probability of words(Brent, 1999; Venkataraman, 2001; Goldwater et al., 2006; Goldwater et al., 2009; Mochihashi et al., 2009) significantly outperformed a heuristic approach (Zhao and Kit, 2008) on the monolingual Chinese SIGHAN-MSR corpus (Emerson, 200</context>
</contexts>
<marker>Saffran, Aslin, Newport, 1996</marker>
<rawString>Jenny R Saffran, Richard N Aslin, and Elissa L Newport. 1996. Statistical learning by 8-month-old infants. Science, 274(5294):1926–1928.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
<author>Michael I Jordan</author>
</authors>
<title>Hierarchical Bayesian nonparametric models with applications. Bayesian Nonparametrics: Principles and Practice,</title>
<date>2010</date>
<pages>158--207</pages>
<contexts>
<context position="8994" citStr="Teh and Jordan, 2010" startWordPosition="1494" endWordPosition="1497"> P∗(i|j, I, J) = � P(aj|, j, I, J) a:aj=i In order to maintain both speed and accuracy, the following window function is adopted P∗(i|j, I, J) ≈ P∗(i|k, I, K) = { 0 otherwise e−|i−kI/K|/Q |i − kI/K |S Sb/2 Aφ ei is empty word (10) where K is the number of characters in F, and the k-th character is the start of the word fj, since j and J are unknown during the computation of dynamic programming. Sb is the window size, Aφ is the prior probability of an empty English word, and Q ensures all the items sum to 1. 2.2 Maximization Inspired by (Teh, 2006; Mochihashi et al., 2009; Neubig et al., 2010; Teh and Jordan, 2010), we employ a Pitman-Yor process model to build the segmentation model M or B. The monolingual model M is PM(fj) = max(n(fj) − d, 0) + (0 + d · nM)G0(fj) Ef′jn(fj) + 0 ��, nM = ��{fj |n(fj) &gt; d} (11) where fj is a foreign language word, and n(fj) is the observed counts of fj, 0 is named the strength parameter, G0(fj) is named the base distribution of fj, and d is the discount. The bilingual model is PB(fj|ei) = max(n(fj, ei) − d, 0) + (0 + d · nei)G0(fj|ei) Ef′j n(fj, ei) + 0 ��. nei = ��{x |n(x, ei) &gt; d} (12) In Eqs. 11 and 12, n(fj) = � P(fj|F,M) (13) F∈F n(fj, ei) = � (F,E)∈B �≈ F∈F Fjk =Fk</context>
</contexts>
<marker>Teh, Jordan, 2010</marker>
<rawString>Yee Whye Teh and Michael I Jordan. 2010. Hierarchical Bayesian nonparametric models with applications. Bayesian Nonparametrics: Principles and Practice, pages 158–207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
</authors>
<title>A hierarchical Bayesian language model based on Pitman-Yor processes.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>985--992</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8925" citStr="Teh, 2006" startWordPosition="1484" endWordPosition="1485">IBM model 2): I PB(fj|E) = P∗(i|j, I, J)PB(fj|ei) (9) i=1 P∗(i|j, I, J) = � P(aj|, j, I, J) a:aj=i In order to maintain both speed and accuracy, the following window function is adopted P∗(i|j, I, J) ≈ P∗(i|k, I, K) = { 0 otherwise e−|i−kI/K|/Q |i − kI/K |S Sb/2 Aφ ei is empty word (10) where K is the number of characters in F, and the k-th character is the start of the word fj, since j and J are unknown during the computation of dynamic programming. Sb is the window size, Aφ is the prior probability of an empty English word, and Q ensures all the items sum to 1. 2.2 Maximization Inspired by (Teh, 2006; Mochihashi et al., 2009; Neubig et al., 2010; Teh and Jordan, 2010), we employ a Pitman-Yor process model to build the segmentation model M or B. The monolingual model M is PM(fj) = max(n(fj) − d, 0) + (0 + d · nM)G0(fj) Ef′jn(fj) + 0 ��, nM = ��{fj |n(fj) &gt; d} (11) where fj is a foreign language word, and n(fj) is the observed counts of fj, 0 is named the strength parameter, G0(fj) is named the base distribution of fj, and d is the discount. The bilingual model is PB(fj|ei) = max(n(fj, ei) − d, 0) + (0 + d · nei)G0(fj|ei) Ef′j n(fj, ei) + 0 ��. nei = ��{x |n(x, ei) &gt; d} (12) In Eqs. 11 and </context>
</contexts>
<marker>Teh, 2006</marker>
<rawString>Yee Whye Teh. 2006. A hierarchical Bayesian language model based on Pitman-Yor processes. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting on Association for Computational Linguistics, pages 985–992. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huihsin Tseng</author>
<author>Pichuan Chang</author>
<author>Galen Andrew</author>
<author>Daniel Jurafsky</author>
<author>Christopher Manning</author>
</authors>
<title>A conditional random field word segmenter for SIGHAN Bakeoff</title>
<date>2005</date>
<booktitle>In Proceedings of the 4th SIGHAN Workshop on Chinese Language Processing,</booktitle>
<volume>171</volume>
<location>Island,</location>
<contexts>
<context position="2046" citStr="Tseng et al., 2005" startWordPosition="290" endWordPosition="293"> processing applications including SMT. Though supervised-learning approaches which involve training segmenters on manually segmented corpora are widely used (Chang et al., 2008), yet the criteria for manually annotating words are arbitrary, and the available annotated corpora are limited in both quantity and genre variety. For example, in machine translation, there are various parallel corpora such as BTEC for tourism-related dialogues (Paul, 2008) and PatentMT in the patent domain (Goto et al., 2011)1, but researchers working on Chineserelated tasks often use the Stanford Chinese segmenter (Tseng et al., 2005) which is trained on a small amount of annotated news text. In contrast, UWS, spurred by the findings that infants are able to use statistical cues to determine word boundaries (Saffran et al., 1996), relies on statistical criteria instead of manually crafted standards. UWS learns from unsegmented raw text, which are available in large quantities, and thus it has the potential to provide more accurate and adaptive segmentation than supervised approaches with less development effort being required. The approaches of explicitly modeling the probability of words(Brent, 1999; Venkataraman, 2001; G</context>
</contexts>
<marker>Tseng, Chang, Andrew, Jurafsky, Manning, 2005</marker>
<rawString>Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel Jurafsky, and Christopher Manning. 2005. A conditional random field word segmenter for SIGHAN Bakeoff 2005. In Proceedings of the 4th SIGHAN Workshop on Chinese Language Processing, volume 171. Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anand Venkataraman</author>
</authors>
<title>A statistical model for word discovery in transcribed speech.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>3</issue>
<contexts>
<context position="2643" citStr="Venkataraman, 2001" startWordPosition="382" endWordPosition="383"> (Tseng et al., 2005) which is trained on a small amount of annotated news text. In contrast, UWS, spurred by the findings that infants are able to use statistical cues to determine word boundaries (Saffran et al., 1996), relies on statistical criteria instead of manually crafted standards. UWS learns from unsegmented raw text, which are available in large quantities, and thus it has the potential to provide more accurate and adaptive segmentation than supervised approaches with less development effort being required. The approaches of explicitly modeling the probability of words(Brent, 1999; Venkataraman, 2001; Goldwater et al., 2006; Goldwater et al., 2009; Mochihashi et al., 2009) significantly outperformed a heuristic approach (Zhao and Kit, 2008) on the monolingual Chinese SIGHAN-MSR corpus (Emerson, 2005), which inspired the work of this paper. However, bilingual approaches that model word probabilities suffer from computational complexity. Xu et al. (2008) proposed a bilingual method by adding alignment into the generative model, but was only able to test it on small-scale BTEC data. Nguyen et al. (2010) used the local best alignment to increase the speed of the Gibbs sampling in training but</context>
</contexts>
<marker>Venkataraman, 2001</marker>
<rawString>Anand Venkataraman. 2001. A statistical model for word discovery in transcribed speech. Computational Linguistics, 27(3):351–372.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia Xu</author>
<author>Jianfeng Gao</author>
<author>Kristina Toutanova</author>
<author>Hermann Ney</author>
</authors>
<title>Bayesian semi-supervised Chinese word segmentation for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational LinguisticsVolume 1,</booktitle>
<pages>1017--1024</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3002" citStr="Xu et al. (2008)" startWordPosition="433" endWordPosition="436">antities, and thus it has the potential to provide more accurate and adaptive segmentation than supervised approaches with less development effort being required. The approaches of explicitly modeling the probability of words(Brent, 1999; Venkataraman, 2001; Goldwater et al., 2006; Goldwater et al., 2009; Mochihashi et al., 2009) significantly outperformed a heuristic approach (Zhao and Kit, 2008) on the monolingual Chinese SIGHAN-MSR corpus (Emerson, 2005), which inspired the work of this paper. However, bilingual approaches that model word probabilities suffer from computational complexity. Xu et al. (2008) proposed a bilingual method by adding alignment into the generative model, but was only able to test it on small-scale BTEC data. Nguyen et al. (2010) used the local best alignment to increase the speed of the Gibbs sampling in training but the impact on accuracy was not explored. This paper is dedicated to bilingual UWS on large-scale corpora to support SMT. To this end, we model bilingual UWS under a similar framework with monolingual UWS in order to improve efficiency, and replace Gibbs sampling with expectation maximization (EM) in training. We aware that variational bayes (VB) may be use</context>
<context position="16919" citStr="Xu et al., 2008" startWordPosition="2838" endWordPosition="2841">rogram is single threaded and implemented in C++. The time cost of the bilingual models is about 5 times that of the monolingual model, which is consistent with the complexity analysis in Section 3. 5 Conclusion This paper is devoted to large-scale Chinese UWS for SMT. An efficient unified monolingual and bilingual UWS method is proposed and applied to large-scale bilingual corpora. Complexity analysis shows that our method is capable of scaling to large-scale corpora. This was verified by experiments on a corpus of 1-million sentence pairs on which traditional MCMC approaches would struggle (Xu et al., 2008). The proposed method does not require any annotated data, but the SMT system with it can achieve comparable performance compared to state-of-the-art supervised word segmenters trained on precious annotated data. Moreover, the proposed method yields 0.96 BLEU improvement relative to supervised word segmenters on an out-of-domain corpus. Thus, we believe that the proposed method would benefit SMT related to low-resource languages where annotated data are scare, and would also find application in domains that differ too greatly from the domains on which supervised word segmenters were trained. I</context>
</contexts>
<marker>Xu, Gao, Toutanova, Ney, 2008</marker>
<rawString>Jia Xu, Jianfeng Gao, Kristina Toutanova, and Hermann Ney. 2008. Bayesian semi-supervised Chinese word segmentation for statistical machine translation. In Proceedings of the 22nd International Conference on Computational LinguisticsVolume 1, pages 1017–1024. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chunyu Kit</author>
</authors>
<title>An empirical comparison of goodness measures for unsupervised chinese word segmentation with a unified framework.</title>
<date>2008</date>
<booktitle>In Proceedings of the 3rd International Joint Conference on Natural Language Processing,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="2786" citStr="Zhao and Kit, 2008" startWordPosition="402" endWordPosition="405">le to use statistical cues to determine word boundaries (Saffran et al., 1996), relies on statistical criteria instead of manually crafted standards. UWS learns from unsegmented raw text, which are available in large quantities, and thus it has the potential to provide more accurate and adaptive segmentation than supervised approaches with less development effort being required. The approaches of explicitly modeling the probability of words(Brent, 1999; Venkataraman, 2001; Goldwater et al., 2006; Goldwater et al., 2009; Mochihashi et al., 2009) significantly outperformed a heuristic approach (Zhao and Kit, 2008) on the monolingual Chinese SIGHAN-MSR corpus (Emerson, 2005), which inspired the work of this paper. However, bilingual approaches that model word probabilities suffer from computational complexity. Xu et al. (2008) proposed a bilingual method by adding alignment into the generative model, but was only able to test it on small-scale BTEC data. Nguyen et al. (2010) used the local best alignment to increase the speed of the Gibbs sampling in training but the impact on accuracy was not explored. This paper is dedicated to bilingual UWS on large-scale corpora to support SMT. To this end, we model</context>
<context position="14091" citStr="Zhao and Kit, 2008" startWordPosition="2375" endWordPosition="2378">eter is set to 1.0, and the discount is set to 1.0 x 10−6. For bilingual segmentation,the size of the alignment window is set to 6; the probability AO of foreign language words being generated by an empty 2http://www.itl.nist.gov/iad/mig/ /tests/mt/2006/ 3It also contains a small number of web blogs 755 Method Accuracy Time CHILD. MSR CHILD. MSR NPY(bigram)a 0.750 0.802 17 m – NPY(trigram)a 0.757 0.807 – – HDP(bigram)b 0.723 – 10 h – Fitnessc – 0.667 – – Prop.(unigram) 0.729 0.804 3 s 50 s Prop.(bigram) 0.774 0.806 15 s 2530 s a by (Mochihashi et al.,2009); b by (Goldwater et al.,2009); c by (Zhao and Kit, 2008). Table 3: Results on Monolingual Corpora. English word, was set to 0.3. The training was started from assuming that there was no previous segmentations on each sentence (pair), and the number of iterations was fixed. It was set to 3 for the monolingual unigram model, and 2 for the bilingual unigram model, which provided slightly higher BLEU scores on the development set than the other settings. The monolingual bigram model, however, was slower to converge, so we started it from the segmentations of the unigram model, and using 10 iterations. 4.2 Monolingual Segmentation Results In monolingual</context>
</contexts>
<marker>Zhao, Kit, 2008</marker>
<rawString>Hai Zhao and Chunyu Kit. 2008. An empirical comparison of goodness measures for unsupervised chinese word segmentation with a unified framework. In Proceedings of the 3rd International Joint Conference on Natural Language Processing, pages 9– 16.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>