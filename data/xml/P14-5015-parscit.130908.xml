<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000006">
<title confidence="0.983203">
kLogNLP: Graph Kernel–based Relational Learning of Natural Language
</title>
<author confidence="0.994363">
Mathias Verbeke♦ Paolo Frasconi♠ Kurt De Grave♦ Fabrizio Costa♣ Luc De Raedt♦
</author>
<affiliation confidence="0.988599">
♦ Department of Computer Science, KU Leuven, Belgium
</affiliation>
<email confidence="0.976492">
{mathias.verbeke, kurt.degrave, luc.deraedt}@cs.kuleuven.be
</email>
<note confidence="0.446543">
♠ Dipartimento di Sistemi e Informatica, Universit`a degli Studi di Firenze, Italy,
</note>
<email confidence="0.674874">
p-f@dsi.unifi.it
</email>
<note confidence="0.283648">
♣ Institut f¨ur Informatik, Albert-Ludwigs-Universit¨at, Germany,
</note>
<email confidence="0.961564">
costa@informatik.uni-freiburg.de
</email>
<sectionHeader confidence="0.993166" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999986294117647">
kLog is a framework for kernel-based
learning that has already proven success-
ful in solving a number of relational tasks
in natural language processing. In this pa-
per, we present kLogNLP, a natural lan-
guage processing module for kLog. This
module enriches kLog with NLP-specific
preprocessors, enabling the use of exist-
ing libraries and toolkits within an elegant
and powerful declarative machine learn-
ing framework. The resulting relational
model of the domain can be extended by
specifying additional relational features in
a declarative way using a logic program-
ming language. This declarative approach
offers a flexible way of experimentation
and a way to insert domain knowledge.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999947555555556">
kLog (Frasconi et al., 2012) is a logical and re-
lational language for kernel-based learning. It has
already proven successful for several tasks in com-
puter vision (Antanas et al., 2012; Antanas et al.,
2013) and natural language processing. For ex-
ample, in the case of binary sentence classifica-
tion, we have shown an increase of 1.2 percent
in F1-score on the best performing system in the
CoNLL 2010 Shared Task on hedge cue detec-
tion (Wikipedia dataset) (Verbeke et al., 2012a).
On a sentence labeling task for evidence-based
medicine, a multi-class multi-label classification
problem, kLog showed improved results over both
the state-of-the-art CRF-based system of Kim et
al. (2011) and a memory-based benchmark (Ver-
beke et al., 2012b). Also for spatial relation ex-
traction from natural language, kLog has shown
to provide a flexible relational representation to
model the task domain (Kordjamshidi et al., 2012).
kLog has two distinguishing features. First, it is
able to transform relational into graph-based rep-
resentations, which allows to incorporate struc-
tural features into the learning process. Subse-
quently, kernel methods are used to work in an ex-
tended high-dimensional feature space, which is
much richer than most of the direct proposition-
alisation approaches. Second, it uses the logic
programming language Prolog for defining and
using (additional) background knowledge, which
renders the model very interpretable and provides
more insights into the importance of individual
(structural) features.
These properties prove especially advantageous
in the case of NLP. The graphical approach of
kLog is able to exploit the full relational represen-
tation that is often a natural way to express lan-
guage structures, and in this way allows to fully
exploit contextual features. On top of this rela-
tional learning approach, the declarative feature
specification allows to include additional back-
ground knowledge, which is often essential for
solving NLP problems.
In this paper, we present kLogNLP1, an NLP
module for kLog. Starting from a dataset and a
declaratively specified model of the domain (based
on entity-relationship modeling from database the-
ory), it transforms the dataset into a graph-based
relational format. We propose a general model
that fits most tasks in NLP, which can be extended
by specifying additional relational features in a
declarative way. The resulting relational represen-
tation then serves as input for kLog, and thus re-
sults in a full relational learning pipeline for NLP.
kLogNLP is most related to Learning-Based
Java (LBJ) (Rizzolo and Roth, 2010) in that it of-
fers a declarative pipeline for modeling and learn-
ing tasks in NLP. The aims are similar, namely ab-
stracting away the technical details from the pro-
grammer, and leaving him to reason about the
modeling. However, whereas LBJ focuses more
on the learning side (by the specification of con-
straints on features which are reconciled at in-
ference time, using the constrained conditional
</bodyText>
<footnote confidence="0.98684">
1Software available at http://dtai.cs.
kuleuven.be/klognlp
</footnote>
<page confidence="0.997035">
85
</page>
<affiliation confidence="0.4143005">
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 85–90,
Baltimore, Maryland USA, June 23-24, 2014. c�2014 Association for Computational Linguistics
</affiliation>
<figure confidence="0.999050380952381">
Raw dataset
(sentence)
Interpretations
(small relational DBs)
Extensionalized
database
Graph
Kernel matrix/
feature vectors
kLogNLP
(E/R-)model
Feature extraction
based on model
Declarative feature Feature
Graphicalization
construction generation
Graph kernel
(NSPDK) kLog
kLogNLP
Statistical
learner
</figure>
<figureCaption confidence="0.99998">
Figure 1: General kLog workflow extended with the kLogNLP module
</figureCaption>
<bodyText confidence="0.9997695">
model framework), due to its embedding in kLog,
kLogNLP focuses on the relational modeling, in
addition to declarative feature construction and
feature generation using graph kernels. kLog in it-
self is related to several frameworks for relational
learning, for which we refer the reader to (Fras-
coni et al., 2012).
The remainder of this paper is organized ac-
cording to the general kLog workflow, preceded
with the kLogNLP module, as outlined in Fig-
ure 1. In Section 2, we discuss the modeling of the
data, and present a general relational data model
for NLP tasks. Also the option to declaratively
construct new features using logic programming is
outlined. In the subsequent parts, we will illustrate
the remaining steps in the kLog pipeline, namely
graphicalization and feature generation (Section
3), and learning (Section 4) in an NLP setting. The
last section draws conclusions and presents ideas
for future work.
</bodyText>
<sectionHeader confidence="0.97509" genericHeader="method">
2 Data Modeling
</sectionHeader>
<bodyText confidence="0.999663954545455">
kLog employs a learning from interpretations set-
ting (De Raedt et al., 2008). In learning from
interpretations, each interpretation is a set of tu-
ples that are true in the example, and can be
seen as a small relational database. Listing 3, to
be discussed later, shows a concise example. In
the NLP setting, an interpretation most commonly
corresponds to a document or a sentence. The
scope of an interpretation is either determined by
the task (e.g., for document classification, the in-
terpretations will at least need to comprise a sin-
gle document), or by the amount of context that
is taken into account (e.g., in case the task is sen-
tence classification, the interpretation can either be
a single sentence, or a full document, depending
on the scope of the context that you want to take
into account).
Since kLog is rooted in database theory, the
modeling of the problem domain is done using an
entity-relationship (E/R) model (Chen, 1976). It
gives an abstract representation of the interpreta-
tions. E/R models can be seen as a tool that is tai-
</bodyText>
<figureCaption confidence="0.9834325">
Figure 2: Entity-relationship diagram of the
kLogNLP model
</figureCaption>
<bodyText confidence="0.997055444444444">
lored to model the domain at hand. As the name
indicates, E/R models consist of entities, which we
will represent as purple rectangles, and relations,
represented as orange diamonds. Both entities and
relations can have several attributes (yellow ovals).
Key attributes (green ovals) uniquely identify an
instance of an entity. We will now discuss the
E/R model we propose as a starting point in the
kLogNLP pipeline.
</bodyText>
<subsectionHeader confidence="0.902593">
2.1 kLogNLP model
</subsectionHeader>
<bodyText confidence="0.9999234">
Since in NLP, most tasks are situated at either
the document, sentence, or token level, we pro-
pose the E/R model in Figure 2 as a general do-
main model suitable for most settings. It is able
to represent interpretations of documents as a se-
quence (nextS) of sentence entities, which
are composed of a sequence (nextW) of word
entities. Next to the sequence relations, also the
dependency relations between words (depRel)
are taken into account, where each relation has
its type (depType) as a property. Furthermore,
also the coreference relationship between words
or phrases (coref) and possibly synonymy re-
lations (synonymous) are taken into account.
The entities in our model also have a primary key,
namely wordID and sentID for words and sen-
tences respectively. Additional properties can be
attached to words such as the wordString it-
self, its lemma and POS-tag, and an indication
whether the word is a namedEntity.
This E/R model of Figure 2 is coded declara-
tively in kLog as shown in Listing 1. The kLog
syntax is an extension of the logical programming
language Prolog. In the next step this script will
be used for feature extraction and generation. Ev-
</bodyText>
<figure confidence="0.999014333333333">
sentence
sentID
nextS
hasWord
wordID
nextW
depType
depRel
word
coref
wordString
lemma
synonymous
POS-tag
namedEntity
</figure>
<page confidence="0.977428">
86
</page>
<bodyText confidence="0.979603745098039">
ery entity or relationship is declared with the key-
word signature. Each signature is of a certain
type; either extensional or intensional.
kLogNLP only acts at the extensional level. Each
signature is characterized by a name and a list
of typed arguments. There are three possible ar-
gument types. First of all, the type can be the
name of an entity set which has been declared
in another signature (e.g., line 4 in Listing 1; the
nextS signature represents the sequence relation
between two entities of type sentence, namely
sent 1 and sent 2). The type self is used to
denote the primary key of an entity. An example is
word id (line 6), which denotes the unique iden-
tifier of a certain word in the interpretation. The
last possible type is property, in case the argu-
ment is neither a reference to another entity nor a
primary key (e.g., postag, line 9).
We will first discuss extensional signatures, and
the automated extensional feature extraction pro-
vided by kLogNLP, before illustrating how the
user can further enrich the model with intensional
predicates.
1 begin_domain.
signature sentence(sent_id::self)::
extensional.
signature nextS(sent_1::sentence, sent_2
::sentence)::extensional.
signature word(word_id::self,
word_string::property,
lemma::property,
postag::property,
namedentity::property
)::extensional.
signature nextW(word_1::word, word_2::
word)::extensional.
signature corefPhrase(coref_id::self)::
extensional.
signature isPartOfCorefPhrase(
coref_phrase::corefPhrase, word::
word)::extensional.
signature coref(coref_phrase_1::
corefPhrase, coref_phrase_2::
corefPhrase)::extensional.
signature synonymous(word_1::word,
word_2::word)::extensional.
signature dependency(word_1::word,
word_2::word,
dep_rel::property
)::extensional.
kernel_points([word]).
</bodyText>
<page confidence="0.56521">
27 end_domain.
</page>
<tableCaption confidence="0.276192">
Listing 1: Declarative representation of the
kLogNLP model
</tableCaption>
<subsectionHeader confidence="0.993085">
2.2 Extensional Feature Extraction
</subsectionHeader>
<bodyText confidence="0.999838">
kLog assumes a closed-world, which means that
atoms that are not known to be true, are assumed
to be false. For extensional signatures, this en-
tails that all ground atoms need to be listed ex-
plicitly in the relational database of interpreta-
tions. These atoms are generated automatically
by the kLogNLP module based on the kLog script
and the input dataset. Considering the defined at-
tributes and relations in the model presented in
Listing 1, the module interfaces with NLP toolk-
its to preprocess the data to the relational format.
The user can remove unnecessary extensional sig-
natures or modify the number of attributes given in
the standard kLogNLP script as given in Listing 1
according to the needs of the task under consider-
ation.
An important choice is the inclusion of the
sentence signature. By inclusion, the gran-
ularity of the interpretation is set to the docu-
ment level, which implies that more context can
be taken into account. By excluding this signa-
ture, the granularity of the interpretation is set to
the sentence level.
Currently, kLogNLP interfaces with the follow-
ing NLP toolkits:
NLTK The Python Natural Language Toolkit
(NLTK) (Bird et al., 2009) offers a suite
of text processing libraries for tokenization,
stemming, tagging and parsing, and offers an
interface to WordNet.
Stanford CoreNLP Stanford CoreNLP2 pro-
vides POS tagging, NER, parsing and
coreference resolution functionality.
The preprocessing toolkit to be used can be
set using the kLogNLP flags mechanism, as il-
lustrated by line 3 of Listing 2. Subsequently,
the dataset predicate (illustrated in line 4 of
Listing 2) calls kLogNLP to preprocess a given
dataset3. This is done according to the speci-
fied kLogNLP model, i.e., the necessary prepro-
cessing modules to be called in the preprocess-
ing toolkit are determined based on the presence
of the entities, relationships, and their attributes in
the kLogNLP script. For example, the presence
</bodyText>
<footnote confidence="0.9097152">
2http://nlp.stanford.edu/software/
corenlp.shtml
3Currently supported dataset formats are directories con-
sisting of (one or more) plain text files or XML files consist-
ing of sentence and/or document elements.
</footnote>
<page confidence="0.561842">
2
</page>
<figure confidence="0.945042833333333">
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
</figure>
<page confidence="0.998111">
87
</page>
<bodyText confidence="0.999951923076923">
of namedentity as a property of word results
in the addition of a named entity recognizer in the
preprocessing toolkit. The resulting set of inter-
pretations is output to a given file. In case sev-
eral instantiations of a preprocessing module are
available in the toolkit, the preferred one can be
chosen by setting the name of the property accord-
ingly. The names as given in Listing 1 outline the
standard settings for each module. For instance, in
case the Snowball stemmer is preferred above the
standard (Wordnet) lemmatizer in NLTK, it can be
selected by changing lemma into snowball as
name for the word lemma property (line 8).
</bodyText>
<equation confidence="0.9169396">
1 experiment :-
% kLogNLP
klognlp_flag(preprocessor,
stanfordnlp),
dataset(’/home/hedgecuedetection/
</equation>
<bodyText confidence="0.790366217391304">
train/’,’trainingset.pl’),
attach(’trainingset.pl’),
% Kernel parametrization
new_feature_generator(my_fg,nspdk),
klog_flag(my_fg,radius,1),
klog_flag(my_fg,distance,1),
klog_flag(my_fg,match_type, hard),
% Learner parametrization
new_model(my_model,libsvm_c_svc),
klog_flag(my_model,c,0.1),
kfold(target, 10, my_model, my_fg).
Listing 2: Full predicate for 10-fold classification
experiment
Each interpretation can be regarded as a small
relational database. We will illustrate the exten-
sional feature extraction step on the CoNLL-2010
dataset on hedge cue detection, a binary classifi-
cation task where the goal is to detect uncertainty
in sentences. This task is situated at the sentence
level, so we left out the sentence and nextS
signatures, as no context from other sentences was
taken into account. A part of a resulting interpre-
tation is shown in Listing 3.
</bodyText>
<equation confidence="0.724264416666667">
1 word(w1,often,often,rb,0,1).
2 depRel(w1,w5,adv).
3 nextW(w1,w2).
4 word(w2,the,the,dt,0,2).
5 depRel(w2,w4,nmod).
6 nextW(w2,w3).
7 word(w3,response,response,nn,0,3).
8 nextW(w3,w4).
9 depRel(w3,w4,nmod).
10 word(w4,may,may,md,0,5).
11 nextW(w4,w5).
Listing 3: Part of an interpretation
</equation>
<bodyText confidence="0.999952666666667">
Optionally, additional extensional signatures
can easily be added to the knowledge base by the
user, as deemed suitable for the task under consid-
eration. At each level of granularity (document,
sentence, or word level), the user is given the
corresponding interpretation and entity IDs, with
which additional extensional facts can be added
using the dedicated Python classes. We will now
turn to declarative feature construction. The fol-
lowing steps are inherently part of the kLog frame-
work. We will briefly illustrate their use in the
context of NLP.
</bodyText>
<subsectionHeader confidence="0.998398">
2.3 Declarative Feature Construction
</subsectionHeader>
<bodyText confidence="0.999057647058824">
The kLog script presented in Listing 1 can now
be extended using declarative feature construction
with intensional signatures. In contrast to ex-
tensional signatures, intensional signatures intro-
duce novel relations using a mechanism resem-
bling deductive databases. This type of signatures
is mostly used to add domain knowledge about the
task at hand. The ground atoms are defined implic-
itly using Prolog definite clauses.
For example, in case of sentence labeling for
evidence-based medicine, the lemma of the root
word proved to be a distinguishing feature (Ver-
beke et al., 2012b), which can be expressed as
Also more complex features can be constructed.
For example, section headers in documents (again
in the case of sentence labeling using document
context) can be identified as follows:
</bodyText>
<equation confidence="0.803906434782609">
1 hasHeaderWord(S,X) :-
word(W,X,_,_,_,_),
hasWord(S,W),
(atom(X) -&gt; name(X,C) ; C = X),
length(C,Len),
Len &gt; 4,
all_upper(C).
8
9 signature isHeaderSentence(sent_id::
sentence)::intensional.
10 isHeaderSentence(S) :-
11 hasHeaderWord(S,_).
12
13 signature hasSectionHeader(sent_id::
sentence, header::property)::
intensional.
14 hasSectionHeader(S,X) :-
15 nextS(S1,S),
16 hasHeaderWord(S1,X).
17 hasSectionHeader(S,X) :-
nextS(S1,S),
not isHeaderSentence(S),
once(hasSectionHeader(S1,X)).
</equation>
<bodyText confidence="0.975664">
In this case, first the sentences that contain a
header word are identified using the helper pred-
</bodyText>
<figure confidence="0.98505224">
1 signature lemmaRoot(sent_id::sentence,
lemmaOfRoot::property)::intensional.
2 lemmaRoot(S,L) :-
3 hasWord(S, I),
4 word(I,_,L,_,_,_),
5 depRel(I,_,root).
2
3
4
5
6
7
8
9
10
11
12
13
14
2
3
4
5
6
7
</figure>
<page confidence="0.723594">
18
19
20
88
</page>
<figure confidence="0.986628307692308">
word(often,often,rb,0,1)
nextW
word(the,the,dt,0,2)
depRel(nmod)
nextW
word(response,response,nn,0,3)
depRel(adv)
depRel(nmod)
nextW
word(variable,variable,nn,0,4)
depRel(sbj)
nextW
word(may,may,md,0,5)
</figure>
<figureCaption confidence="0.996619833333333">
Figure 3: Graphicalization of the (partial) interpretation in Listing 3. For the sake of clarity, attributes of
entities and relationships are depicted inside the respective entity or relationship.
Figure 4: Illustration of the NSPDK feature concept. Left: instance G with 2 vertices v, u as roots for
neighborhood subgraphs (A, B) at distance 2. Right: some of the neighborhood pairs, which form the
NSPDK features, at distance d = 2 and radius r = 0 and 1 respectively. Note that neighborhood subgraphs
can overlap.
</figureCaption>
<figure confidence="0.914576833333333">
INSTANCE G
v u
FEATURES
A
r=0 d=2
r=1 d=2
</figure>
<bodyText confidence="0.999887571428571">
icate hasHeaderWord, where a header word is
defined as an upper case string that has more than
four letters (lines 1-7). Next, all sentences that rep-
resent a section header are identified using the in-
tensional signature isHeaderSentence (lines
9-11), and each sentence in the paragraphs follow-
ing a particular section header is labeled with this
header, using the hasSectionHeader predi-
cate (lines 13-20).
Due to the relational approach, the span can be
very large. Furthermore, since these features are
defined declaratively, there is no need to reprocess
the dataset each time a new feature is introduced,
which renders experimentation very flexible4.
</bodyText>
<sectionHeader confidence="0.9963525" genericHeader="method">
3 Graphicalization and Feature
Generation
</sectionHeader>
<bodyText confidence="0.999316">
In this step, a technique called graphicalization
transforms the relational representations from the
previous step into graph-based ones and derives
features from a grounded entity/relationship dia-
gram using graph kernels. This can be interpreted
as unfolding the E/R diagram over the data. An ex-
ample of the graphicalization of the interpretation
part in Listing 3 can be found in Figure 3.
From the resulting graphs, features can be ex-
tracted using a feature generation technique that is
based on Neighborhood Subgraph Pairwise Dis-
4Note that changes in the extensional signatures do re-
quire reprocessing the dataset. However, for different runs of
an experiment with varying parameters for the feature gener-
ator or the learner, kLogNLP uses a caching mechanism to
check if the extensional signatures have changed, when call-
ing the dataset predicate.
tance Kernel (NSPDK) (Costa and De Grave,
2010), a particular type of graph kernel. Infor-
mally the idea of this kernel is to decompose a
graph into small neighborhood subgraphs of in-
creasing radii r &lt; rmax. Then, all pairs of such
subgraphs whose roots are at a distance not greater
than d &lt; dmax are considered as individual fea-
tures. The kernel notion is finally given as the frac-
tion of features in common between two graphs.
Formally, the kernel is defined as:
</bodyText>
<equation confidence="0.952771">
nr,d(G, G�) = � 1A∼=A0 · 1B∼=B0 (1)
</equation>
<bodyText confidence="0.997452809523809">
where R−1
r,d(G) indicates the multiset of all pairs
of neighborhoods of radius r with roots at distance
d that exist in G, and where 1 denotes the indicator
function and ∼= the isomorphism between graphs.
For the full details, we refer the reader to (Costa
and De Grave, 2010). The neighborhood pairs are
illustrated in Figure 4 for a distance of 2 between
two arbitrary roots (v and u).
In kLog, the feature set is generated in a combi-
natorial fashion by explicitly enumerating all pairs
of neighborhood subgraphs; this yields a high-
dimensional feature space that is much richer than
most of the direct propositionalization approaches.
The result is an extended high-dimensional fea-
ture space on which a statistical learning algorithm
can be applied. The feature generator is initialized
using the new feature generator predicate
and hyperparameters (e.g., maximum distance and
radius, and match type) can be set using the kLog
flags mechanism (Listing 2, lines 6-10).
</bodyText>
<figure confidence="0.93526125">
A,B∈R−1
r,d(G)
A0,B0∈R−1
r,d(G0)
</figure>
<page confidence="0.997179">
89
</page>
<sectionHeader confidence="0.997956" genericHeader="evaluation">
4 Learning
</sectionHeader>
<bodyText confidence="0.999825571428571">
In the last step, different learning tasks can be per-
formed on the resulting extended feature space. To
this end, kLog interfaces with several solvers, in-
cluding LibSVM (Chang and Lin, 2011) and SVM
SGD (Bottou, 2010). Lines 11-15 (Listing 2) illus-
trate the initialization of LibSVM and its use for
10-fold cross-validation.
</bodyText>
<sectionHeader confidence="0.998103" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.9999775">
In this paper, we presented kLogNLP, a natu-
ral language processing module for kLog. Based
on an entity-relationship representation of the do-
main, it transforms a dataset into the graph-based
relational format of kLog. The basic kLogNLP
model can be easily extended with additional
background knowledge by adding relations us-
ing the declarative programming language Prolog.
This offers a more flexible way of experimenta-
tion, as new features can be constructed on top
of existing ones without the need to reprocess the
dataset. In future work, interfaces will be added
to other (domain-specific) NLP frameworks (e.g.,
the BLLIP parser with the self-trained biomedical
parsing model (McClosky, 2010)) and additional
dataset formats will be supported.
</bodyText>
<sectionHeader confidence="0.998226" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999751">
This research is funded by the Research Founda-
tion Flanders (FWO project G.0478.10 - Statistical
Relational Learning of Natural Language). KDG
was supported by ERC StG 240186 “MiGraNT”.
</bodyText>
<sectionHeader confidence="0.999245" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99992884057971">
Laura Antanas, Paolo Frasconi, Fabrizio Costa, Tinne
Tuytelaars, and Luc De Raedt. 2012. A relational
kernel-based framework for hierarchical image un-
derstanding. In Lecture Notes in Computer Science,,
pages 171–180. Springer, November.
Laura Antanas, McElory Hoffmann, Paolo Frasconi,
Tinne Tuytelaars, and Luc De Raedt. 2013. A re-
lational kernel-based approach to scene classifica-
tion. IEEE Workshop on Applications of Computer
Vision, 0:133–139.
Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O’Reilly Media, Inc., 1st edition.
L´eon Bottou. 2010. Large-scale machine learning with
stochastic gradient descent. In Proc. of the 19th In-
ternational Conference on Computational Statistics
(COMPSTAT’2010), pages 177–187. Springer.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technol-
ogy, 2:27:1–27:27. Software available at http://
www.csie.ntu.edu.tw/˜cjlin/libsvm.
Peter Pin-Shan Chen. 1976. The entity-relationship
model - toward a unified view of data. ACM Trans.
Database Syst., 1(1):9–36, March.
Fabrizio Costa and Kurt De Grave. 2010. Fast neigh-
borhood subgraph pairwise distance kernel. In Proc.
of the 26th International Conference on Machine
Learning,, pages 255–262. Omnipress.
Luc De Raedt, Paolo Frasconi, Kristian Kersting, and
Stephen Muggleton, editors. 2008. Probabilistic
Inductive Logic Programming - Theory and Appli-
cations, volume 4911 of Lecture Notes in Computer
Science. Springer.
Paolo Frasconi, Fabrizio Costa, Luc De Raedt, and
Kurt De Grave. 2012. klog: A language for log-
ical and relational learning with kernels. CoRR,
abs/1205.3981.
Su Kim, David Martinez, Lawrence Cavedon, and Lars
Yencken. 2011. Automatic classification of sen-
tences to support evidence based medicine. BMC
Bioinformatics, 12(Suppl 2):S5.
Parisa Kordjamshidi, Paolo Frasconi, Martijn van Ot-
terlo, Marie-Francine Moens, and Luc De Raedt.
2012. Relational learning for spatial relation extrac-
tion from natural language. In Inductive Logic Pro-
gramming, pages 204–220. Springer.
David McClosky. 2010. Any Domain Parsing: Au-
tomatic Domain Adaptation for Natural Language
Parsing. Ph.D. thesis, Brown University, Provi-
dence, RI, USA. AAI3430199.
N. Rizzolo and D. Roth. 2010. Learning based java for
rapid development of nlp systems. In LREC, Val-
letta, Malta, 5.
Mathias Verbeke, Paolo Frasconi, Vincent Van Asch,
Roser Morante, Walter Daelemans, and Luc
De Raedt. 2012a. Kernel-based logical and re-
lational learning with kLog for hedge cue detec-
tion. In Proc. of the 21st International Conference
on Inductive Logic Programming, pages 347–357.
Springer, March.
Mathias Verbeke, Vincent Van Asch, Roser Morante,
Paolo Frasconi, Walter Daelemans, and Luc
De Raedt. 2012b. A statistical relational learn-
ing approach to identifying evidence based medicine
categories. In Proc. of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 579–589. ACL.
</reference>
<page confidence="0.998639">
90
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.261150">
<title confidence="0.999664">kLogNLP: Graph Kernel–based Relational Learning of Natural Language</title>
<author confidence="0.99742">Paolo Kurt De_Fabrizio Luc De</author>
<affiliation confidence="0.868261">Department of Computer Science, KU Leuven, Belgium</affiliation>
<email confidence="0.76174">kurt.degrave,</email>
<author confidence="0.594891">di_Sistemi e Informatica</author>
<author confidence="0.594891">Universit`a degli Studi di_Firenze</author>
<author confidence="0.594891">Italy</author>
<email confidence="0.992875">p-f@dsi.unifi.it</email>
<author confidence="0.62576">f¨ur Informatik</author>
<author confidence="0.62576">Germany Albert-Ludwigs-Universit¨at</author>
<email confidence="0.998315">costa@informatik.uni-freiburg.de</email>
<abstract confidence="0.9996625">kLog is a framework for kernel-based learning that has already proven successful in solving a number of relational tasks in natural language processing. In this pawe present a natural language processing module for kLog. This module enriches kLog with NLP-specific preprocessors, enabling the use of existing libraries and toolkits within an elegant and powerful declarative machine learning framework. The resulting relational model of the domain can be extended by specifying additional relational features in a declarative way using a logic programming language. This declarative approach offers a flexible way of experimentation and a way to insert domain knowledge.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Laura Antanas</author>
<author>Paolo Frasconi</author>
<author>Fabrizio Costa</author>
<author>Tinne Tuytelaars</author>
<author>Luc De Raedt</author>
</authors>
<title>A relational kernel-based framework for hierarchical image understanding.</title>
<date>2012</date>
<booktitle>In Lecture Notes in Computer Science,,</booktitle>
<pages>171--180</pages>
<publisher>Springer,</publisher>
<marker>Antanas, Frasconi, Costa, Tuytelaars, De Raedt, 2012</marker>
<rawString>Laura Antanas, Paolo Frasconi, Fabrizio Costa, Tinne Tuytelaars, and Luc De Raedt. 2012. A relational kernel-based framework for hierarchical image understanding. In Lecture Notes in Computer Science,, pages 171–180. Springer, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Antanas</author>
<author>McElory Hoffmann</author>
<author>Paolo Frasconi</author>
</authors>
<title>Tinne Tuytelaars, and Luc De Raedt.</title>
<date>2013</date>
<booktitle>IEEE Workshop on Applications of Computer Vision,</booktitle>
<pages>0--133</pages>
<contexts>
<context position="1375" citStr="Antanas et al., 2013" startWordPosition="191" endWordPosition="194">essors, enabling the use of existing libraries and toolkits within an elegant and powerful declarative machine learning framework. The resulting relational model of the domain can be extended by specifying additional relational features in a declarative way using a logic programming language. This declarative approach offers a flexible way of experimentation and a way to insert domain knowledge. 1 Introduction kLog (Frasconi et al., 2012) is a logical and relational language for kernel-based learning. It has already proven successful for several tasks in computer vision (Antanas et al., 2012; Antanas et al., 2013) and natural language processing. For example, in the case of binary sentence classification, we have shown an increase of 1.2 percent in F1-score on the best performing system in the CoNLL 2010 Shared Task on hedge cue detection (Wikipedia dataset) (Verbeke et al., 2012a). On a sentence labeling task for evidence-based medicine, a multi-class multi-label classification problem, kLog showed improved results over both the state-of-the-art CRF-based system of Kim et al. (2011) and a memory-based benchmark (Verbeke et al., 2012b). Also for spatial relation extraction from natural language, kLog h</context>
</contexts>
<marker>Antanas, Hoffmann, Frasconi, 2013</marker>
<rawString>Laura Antanas, McElory Hoffmann, Paolo Frasconi, Tinne Tuytelaars, and Luc De Raedt. 2013. A relational kernel-based approach to scene classification. IEEE Workshop on Applications of Computer Vision, 0:133–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
<author>Ewan Klein</author>
<author>Edward Loper</author>
</authors>
<date>2009</date>
<booktitle>Natural Language Processing with Python. O’Reilly Media, Inc., 1st edition.</booktitle>
<contexts>
<context position="11610" citStr="Bird et al., 2009" startWordPosition="1758" endWordPosition="1761"> unnecessary extensional signatures or modify the number of attributes given in the standard kLogNLP script as given in Listing 1 according to the needs of the task under consideration. An important choice is the inclusion of the sentence signature. By inclusion, the granularity of the interpretation is set to the document level, which implies that more context can be taken into account. By excluding this signature, the granularity of the interpretation is set to the sentence level. Currently, kLogNLP interfaces with the following NLP toolkits: NLTK The Python Natural Language Toolkit (NLTK) (Bird et al., 2009) offers a suite of text processing libraries for tokenization, stemming, tagging and parsing, and offers an interface to WordNet. Stanford CoreNLP Stanford CoreNLP2 provides POS tagging, NER, parsing and coreference resolution functionality. The preprocessing toolkit to be used can be set using the kLogNLP flags mechanism, as illustrated by line 3 of Listing 2. Subsequently, the dataset predicate (illustrated in line 4 of Listing 2) calls kLogNLP to preprocess a given dataset3. This is done according to the specified kLogNLP model, i.e., the necessary preprocessing modules to be called in the </context>
</contexts>
<marker>Bird, Klein, Loper, 2009</marker>
<rawString>Steven Bird, Ewan Klein, and Edward Loper. 2009. Natural Language Processing with Python. O’Reilly Media, Inc., 1st edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L´eon Bottou</author>
</authors>
<title>Large-scale machine learning with stochastic gradient descent.</title>
<date>2010</date>
<booktitle>In Proc. of the 19th International Conference on Computational Statistics (COMPSTAT’2010),</booktitle>
<pages>177--187</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="20769" citStr="Bottou, 2010" startWordPosition="3144" endWordPosition="3145">ion approaches. The result is an extended high-dimensional feature space on which a statistical learning algorithm can be applied. The feature generator is initialized using the new feature generator predicate and hyperparameters (e.g., maximum distance and radius, and match type) can be set using the kLog flags mechanism (Listing 2, lines 6-10). A,B∈R−1 r,d(G) A0,B0∈R−1 r,d(G0) 89 4 Learning In the last step, different learning tasks can be performed on the resulting extended feature space. To this end, kLog interfaces with several solvers, including LibSVM (Chang and Lin, 2011) and SVM SGD (Bottou, 2010). Lines 11-15 (Listing 2) illustrate the initialization of LibSVM and its use for 10-fold cross-validation. 5 Conclusions and Future Work In this paper, we presented kLogNLP, a natural language processing module for kLog. Based on an entity-relationship representation of the domain, it transforms a dataset into the graph-based relational format of kLog. The basic kLogNLP model can be easily extended with additional background knowledge by adding relations using the declarative programming language Prolog. This offers a more flexible way of experimentation, as new features can be constructed on</context>
</contexts>
<marker>Bottou, 2010</marker>
<rawString>L´eon Bottou. 2010. Large-scale machine learning with stochastic gradient descent. In Proc. of the 19th International Conference on Computational Statistics (COMPSTAT’2010), pages 177–187. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<pages>2--27</pages>
<note>Software available at http:// www.csie.ntu.edu.tw/˜cjlin/libsvm.</note>
<contexts>
<context position="20742" citStr="Chang and Lin, 2011" startWordPosition="3137" endWordPosition="3140">st of the direct propositionalization approaches. The result is an extended high-dimensional feature space on which a statistical learning algorithm can be applied. The feature generator is initialized using the new feature generator predicate and hyperparameters (e.g., maximum distance and radius, and match type) can be set using the kLog flags mechanism (Listing 2, lines 6-10). A,B∈R−1 r,d(G) A0,B0∈R−1 r,d(G0) 89 4 Learning In the last step, different learning tasks can be performed on the resulting extended feature space. To this end, kLog interfaces with several solvers, including LibSVM (Chang and Lin, 2011) and SVM SGD (Bottou, 2010). Lines 11-15 (Listing 2) illustrate the initialization of LibSVM and its use for 10-fold cross-validation. 5 Conclusions and Future Work In this paper, we presented kLogNLP, a natural language processing module for kLog. Based on an entity-relationship representation of the domain, it transforms a dataset into the graph-based relational format of kLog. The basic kLogNLP model can be easily extended with additional background knowledge by adding relations using the declarative programming language Prolog. This offers a more flexible way of experimentation, as new fea</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1–27:27. Software available at http:// www.csie.ntu.edu.tw/˜cjlin/libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Pin-Shan Chen</author>
</authors>
<title>The entity-relationship model - toward a unified view of data.</title>
<date>1976</date>
<journal>ACM Trans. Database Syst.,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="6675" citStr="Chen, 1976" startWordPosition="1016" endWordPosition="1017">ommonly corresponds to a document or a sentence. The scope of an interpretation is either determined by the task (e.g., for document classification, the interpretations will at least need to comprise a single document), or by the amount of context that is taken into account (e.g., in case the task is sentence classification, the interpretation can either be a single sentence, or a full document, depending on the scope of the context that you want to take into account). Since kLog is rooted in database theory, the modeling of the problem domain is done using an entity-relationship (E/R) model (Chen, 1976). It gives an abstract representation of the interpretations. E/R models can be seen as a tool that is taiFigure 2: Entity-relationship diagram of the kLogNLP model lored to model the domain at hand. As the name indicates, E/R models consist of entities, which we will represent as purple rectangles, and relations, represented as orange diamonds. Both entities and relations can have several attributes (yellow ovals). Key attributes (green ovals) uniquely identify an instance of an entity. We will now discuss the E/R model we propose as a starting point in the kLogNLP pipeline. 2.1 kLogNLP model</context>
</contexts>
<marker>Chen, 1976</marker>
<rawString>Peter Pin-Shan Chen. 1976. The entity-relationship model - toward a unified view of data. ACM Trans. Database Syst., 1(1):9–36, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabrizio Costa</author>
<author>Kurt De Grave</author>
</authors>
<title>Fast neighborhood subgraph pairwise distance kernel.</title>
<date>2010</date>
<booktitle>In Proc. of the 26th International Conference on Machine Learning,,</booktitle>
<pages>255--262</pages>
<publisher>Omnipress.</publisher>
<marker>Costa, De Grave, 2010</marker>
<rawString>Fabrizio Costa and Kurt De Grave. 2010. Fast neighborhood subgraph pairwise distance kernel. In Proc. of the 26th International Conference on Machine Learning,, pages 255–262. Omnipress.</rawString>
</citation>
<citation valid="true">
<date>2008</date>
<booktitle>Probabilistic Inductive Logic Programming - Theory and Applications,</booktitle>
<volume>4911</volume>
<editor>Luc De Raedt, Paolo Frasconi, Kristian Kersting, and Stephen Muggleton, editors.</editor>
<publisher>Springer.</publisher>
<marker>2008</marker>
<rawString>Luc De Raedt, Paolo Frasconi, Kristian Kersting, and Stephen Muggleton, editors. 2008. Probabilistic Inductive Logic Programming - Theory and Applications, volume 4911 of Lecture Notes in Computer Science. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paolo Frasconi</author>
<author>Fabrizio Costa</author>
<author>Luc De Raedt</author>
<author>Kurt De Grave</author>
</authors>
<title>klog: A language for logical and relational learning with kernels.</title>
<date>2012</date>
<location>CoRR, abs/1205.3981.</location>
<marker>Frasconi, Costa, De Raedt, De Grave, 2012</marker>
<rawString>Paolo Frasconi, Fabrizio Costa, Luc De Raedt, and Kurt De Grave. 2012. klog: A language for logical and relational learning with kernels. CoRR, abs/1205.3981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Su Kim</author>
<author>David Martinez</author>
<author>Lawrence Cavedon</author>
<author>Lars Yencken</author>
</authors>
<title>Automatic classification of sentences to support evidence based medicine.</title>
<date>2011</date>
<journal>BMC Bioinformatics,</journal>
<volume>12</volume>
<pages>2--5</pages>
<contexts>
<context position="1854" citStr="Kim et al. (2011)" startWordPosition="266" endWordPosition="269"> kernel-based learning. It has already proven successful for several tasks in computer vision (Antanas et al., 2012; Antanas et al., 2013) and natural language processing. For example, in the case of binary sentence classification, we have shown an increase of 1.2 percent in F1-score on the best performing system in the CoNLL 2010 Shared Task on hedge cue detection (Wikipedia dataset) (Verbeke et al., 2012a). On a sentence labeling task for evidence-based medicine, a multi-class multi-label classification problem, kLog showed improved results over both the state-of-the-art CRF-based system of Kim et al. (2011) and a memory-based benchmark (Verbeke et al., 2012b). Also for spatial relation extraction from natural language, kLog has shown to provide a flexible relational representation to model the task domain (Kordjamshidi et al., 2012). kLog has two distinguishing features. First, it is able to transform relational into graph-based representations, which allows to incorporate structural features into the learning process. Subsequently, kernel methods are used to work in an extended high-dimensional feature space, which is much richer than most of the direct propositionalisation approaches. Second, </context>
</contexts>
<marker>Kim, Martinez, Cavedon, Yencken, 2011</marker>
<rawString>Su Kim, David Martinez, Lawrence Cavedon, and Lars Yencken. 2011. Automatic classification of sentences to support evidence based medicine. BMC Bioinformatics, 12(Suppl 2):S5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Parisa Kordjamshidi</author>
<author>Paolo Frasconi</author>
<author>Martijn van Otterlo</author>
<author>Marie-Francine Moens</author>
<author>Luc De Raedt</author>
</authors>
<title>Relational learning for spatial relation extraction from natural language.</title>
<date>2012</date>
<booktitle>In Inductive Logic Programming,</booktitle>
<pages>204--220</pages>
<publisher>Springer.</publisher>
<marker>Kordjamshidi, Frasconi, van Otterlo, Moens, De Raedt, 2012</marker>
<rawString>Parisa Kordjamshidi, Paolo Frasconi, Martijn van Otterlo, Marie-Francine Moens, and Luc De Raedt. 2012. Relational learning for spatial relation extraction from natural language. In Inductive Logic Programming, pages 204–220. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
</authors>
<title>Any Domain Parsing: Automatic Domain Adaptation for Natural Language Parsing.</title>
<date>2010</date>
<tech>Ph.D. thesis,</tech>
<institution>Brown University,</institution>
<location>Providence, RI, USA. AAI3430199.</location>
<marker>McClosky, 2010</marker>
<rawString>David McClosky. 2010. Any Domain Parsing: Automatic Domain Adaptation for Natural Language Parsing. Ph.D. thesis, Brown University, Providence, RI, USA. AAI3430199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Rizzolo</author>
<author>D Roth</author>
</authors>
<title>Learning based java for rapid development of nlp systems.</title>
<date>2010</date>
<booktitle>In LREC,</booktitle>
<location>Valletta,</location>
<contexts>
<context position="3761" citStr="Rizzolo and Roth, 2010" startWordPosition="559" endWordPosition="562">ems. In this paper, we present kLogNLP1, an NLP module for kLog. Starting from a dataset and a declaratively specified model of the domain (based on entity-relationship modeling from database theory), it transforms the dataset into a graph-based relational format. We propose a general model that fits most tasks in NLP, which can be extended by specifying additional relational features in a declarative way. The resulting relational representation then serves as input for kLog, and thus results in a full relational learning pipeline for NLP. kLogNLP is most related to Learning-Based Java (LBJ) (Rizzolo and Roth, 2010) in that it offers a declarative pipeline for modeling and learning tasks in NLP. The aims are similar, namely abstracting away the technical details from the programmer, and leaving him to reason about the modeling. However, whereas LBJ focuses more on the learning side (by the specification of constraints on features which are reconciled at inference time, using the constrained conditional 1Software available at http://dtai.cs. kuleuven.be/klognlp 85 Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 85–90, Baltimore, Maryland US</context>
</contexts>
<marker>Rizzolo, Roth, 2010</marker>
<rawString>N. Rizzolo and D. Roth. 2010. Learning based java for rapid development of nlp systems. In LREC, Valletta, Malta, 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Verbeke</author>
<author>Paolo Frasconi</author>
<author>Vincent Van Asch</author>
<author>Roser Morante</author>
<author>Walter Daelemans</author>
<author>Luc De Raedt</author>
</authors>
<title>Kernel-based logical and relational learning with kLog for hedge cue detection.</title>
<date>2012</date>
<booktitle>In Proc. of the 21st International Conference on Inductive Logic Programming,</booktitle>
<pages>347--357</pages>
<publisher>Springer,</publisher>
<marker>Verbeke, Frasconi, Van Asch, Morante, Daelemans, De Raedt, 2012</marker>
<rawString>Mathias Verbeke, Paolo Frasconi, Vincent Van Asch, Roser Morante, Walter Daelemans, and Luc De Raedt. 2012a. Kernel-based logical and relational learning with kLog for hedge cue detection. In Proc. of the 21st International Conference on Inductive Logic Programming, pages 347–357. Springer, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathias Verbeke</author>
<author>Vincent Van Asch</author>
<author>Roser Morante</author>
<author>Paolo Frasconi</author>
<author>Walter Daelemans</author>
<author>Luc De Raedt</author>
</authors>
<title>A statistical relational learning approach to identifying evidence based medicine categories.</title>
<date>2012</date>
<booktitle>In Proc. of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>579--589</pages>
<publisher>ACL.</publisher>
<marker>Verbeke, Van Asch, Morante, Frasconi, Daelemans, De Raedt, 2012</marker>
<rawString>Mathias Verbeke, Vincent Van Asch, Roser Morante, Paolo Frasconi, Walter Daelemans, and Luc De Raedt. 2012b. A statistical relational learning approach to identifying evidence based medicine categories. In Proc. of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 579–589. ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>