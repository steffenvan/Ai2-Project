<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002149">
<title confidence="0.94347">
SemEval-2010 Task 1: Coreference Resolution in Multiple Languages
</title>
<author confidence="0.987784">
Marta Recasens, Toni Marti, Mariona Taule Lluis Marquez, Emili Sapena
</author>
<affiliation confidence="0.96143">
Centre de Llenguatge i Computaci6 (CLiC) TALP Research Center,
University of Barcelona Technical University of Catalonia
</affiliation>
<address confidence="0.696237">
Gran Via de les Corts Catalanes 585 Jordi Girona Salgado 1-3
08007 Barcelona 08034 Barcelona
{mrecasens, amarti, mtaule} {lluism, esapena}
</address>
<email confidence="0.99552">
@ub.edu @lsi.upc.edu
</email>
<sectionHeader confidence="0.995579" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999757923076923">
This paper presents the task `Coreference
Resolution in Multiple Languages&apos; to be run
in SemEval-2010 (5th International Workshop
on Semantic Evaluations). This task aims to
evaluate and compare automatic coreference
resolution systems for three different lan-
guages (Catalan, English, and Spanish) by
means of two alternative evaluation metrics,
thus providing an insight into (i) the portabil-
ity of coreference resolution systems across
languages, and (ii) the effect of different scor-
ing metrics on ranking the output of the par-
ticipant systems.
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999855133333334">
Coreference information has been shown to be
beneficial in many NLP applications such as In-
formation Extraction (McCarthy and Lehnert,
1995), Text Summarization (Steinberger et al.,
2007), Question Answering (Morton, 2000), and
Machine Translation. In these systems, there is a
need to identify the different pieces of information
that refer to the same discourse entity in order to
produce coherent and fluent summaries, disam-
biguate the references to an entity, and solve ana-
phoric pronouns.
Coreference is an inherently complex phenome-
non. Some of the limitations of the traditional rule-
based approaches (Mitkov, 1998) could be over-
come by machine learning techniques, which allow
automating the acquisition of knowledge from an-
notated corpora.
This task will promote the development of lin-
guistic resources —annotated corpora1— and ma-
chine-learning techniques oriented to coreference
resolution. In particular, we aim to evaluate and
compare coreference resolution systems in a multi-
lingual context, including Catalan, English, and
Spanish languages, and by means of two different
evaluation metrics.
By setting up a multilingual scenario, we can
explore to what extent it is possible to implement a
general system that is portable to the three lan-
guages, how much language-specific tuning is nec-
essary, and the significant differences between
Romance languages and English, as well as those
between two closely related languages such as
Spanish and Catalan. Besides, we expect to gain
some useful insight into the development of multi-
lingual NLP applications.
As far as the evaluation is concerned, by em-
ploying B-cubed (Bagga and Baldwin, 1998) and
CEAF (Luo, 2005) algorithms we can consider
both the advantages and drawbacks of using one or
the other scoring metric. For comparison purposes,
the MUC score will also be reported. Among oth-
ers, we are interested in the following questions:
Which evaluation metric provides a more accurate
picture of the accuracy of the system performance?
Is there a strong correlation between them? Can
</bodyText>
<footnote confidence="0.38243">
1 Corpora annotated with coreference are scarce, especially for
languages other than English.
</footnote>
<page confidence="0.978269">
70
</page>
<note confidence="0.817531">
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 70–75,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.986715428571428">
statistical systems be optimized under both metrics
at the same time?
The rest of the paper is organized as follows.
Section 2 describes the overall task. The corpora
and the annotation scheme are presented in Section
3. Conclusions and final remarks are given in Sec-
tion 4.
</bodyText>
<sectionHeader confidence="0.888782" genericHeader="method">
2 Task description
</sectionHeader>
<bodyText confidence="0.99295325">
The SemEval-2010 task `Coreference Resolution
in Multiple Languages&apos; is concerned with auto-
matic coreference resolution for three different
languages: Catalan, English, and Spanish.
</bodyText>
<subsectionHeader confidence="0.998933">
2.1 Specific tasks
</subsectionHeader>
<bodyText confidence="0.985396246376812">
Given the complexity of the coreference phenom-
ena, we will concentrate only in two tractable as-
pects, which lead to the two following subtasks for
each of the languages:
i) Detection of full coreference chains, com-
posed by named entities, pronouns, and full
noun phrases (NPs).
ii) Pronominal resolution, i.e. finding the antece-
dents of the pronouns in the text.
[Els beneficiaris de [pensions de [viudetat131211 po-
dran conservar [la paga14 encara_que [015 es tornin
a casar si [016 compleixen [una s6rie de [condi-
cions1817 , segons [el reial decret aprovat ahir pel
[Consell_de_Ministres11019 .
[La nova norma111 afecta [els perceptors d&apos; [una
pensi6 de [viudetat113112 [que114 contreguin [matri-
moni115 a_partir_de [l&apos; 1_de_gener_del_2002116117 .
[La primera de [les condicions118119 6s tenir [m6s
de 61 anys120 o tenir reconeguda [una incapacitat
permanent [que122 inhabiliti per a [tota [professi6124
o [ofici125123121.
[La segona126 6s que [la pensi6127 sigui [la principal
o (mica font d&apos; [ingressos del [pensionista130129128 , i
sempre_que [l&apos; import anual de [la mateixa pen-
si6132131 representi , com_a_minim , [el 75% del
[total dels [ingressos anuals del [pensionis-
ta136135134133.
The example in Figure 1 illustrates the two sub-
tasks.2 Given a text in which NPs are identified and
indexed (including elliptical subjects, represented
as 0), the goal of (i) is to extract all coreference
chains: 1-5-6-30-36, 9-11, and 7-18; while the
goal of (ii) is to identify the antecedents of pro-
nouns 5 and 6, which are 1 and 5 (or 1), respec-
tively. Note that (b) is a simpler subtask of (a) and
that for a given pronoun there can be multiple an-
tecedents (e.g. both 1 and 5 are correct antecedents
for 6).
We restrict the task to solving `identity&apos; rela-
tions between NPs (coreference chains), and be-
tween pronouns and antecedents. Nominal
predicates and appositions as well as NPs with a
non-nominal antecedent (discourse deixis) will not
been taken into consideration in the recognition of
coreference chains (see Section 3.1 for more in-
formation about decisions concerning the annota-
tion scheme).
Although we target at general systems address-
ing the full multilingual task, we will allow taking
part on any subtask of any language in order to
promote participation.
[The beneficiaries of [[spouse&apos;s13 pensions1211 will
be able to keep [the payment14 even if [they15 re-
marry provided that [they16 fulfill [a series of [con-
ditions1817, according to [the royal decree approved
yesterday by [the Council of Ministers11019.
[The new rule111 affects [the recipients of [a
[spouse&apos;s113 pension112 [that114 get married after
[January_1_,_2002116117.
[The first of [the conditions118119 is being older
[than 61 years old120 or having [an officially rec-
ognized permanent disability [that122 makes one
disabled for [any [profession124 or [job125123121.
[The second one126 requires that [the pension127 be
[the main or only source of [the [pensioner&apos;s130 in-
come129128, and provided that [the annual amount
of [the pension132131 represents, at least, [75% of
[the total [yearly income of [the pen-
sioner136135134133.
</bodyText>
<figureCaption confidence="0.97932">
Figure 1. NPs in a sample from the Catalan training
data (left) and the English translation (right).
</figureCaption>
<bodyText confidence="0.9144525">
2 The example in Figure 1 is a simplified version of the anno-
tated format. See Section 2.2 for more details.
</bodyText>
<page confidence="0.991892">
71
</page>
<subsectionHeader confidence="0.981168">
2.2 Evaluation
</subsectionHeader>
<subsubsectionHeader confidence="0.996709">
2.1.1 Input information
</subsubsectionHeader>
<bodyText confidence="0.999913">
The input information for the task will consist of:
word forms, lemmas, POS, full syntax, and seman-
tic role labeling. Two different scenarios will be
considered regarding the source of the input infor-
mation:
i) In the first one, gold standard annotation will
be provided to participants. This input annota-
tion will correctly identify all NPs that are part
of coreference chains. This scenario will be
only available for Catalan and Spanish.
ii) In the second, state-of-the-art automatic lin-
guistic analyzers for the three languages will
be used to generate the input annotation of the
data. The matching between the automatically
generated structure and the real NPs interven-
ing in the chains does not need to be perfect in
this setting.
By defining these two experimental settings, we
will be able to check the performance of corefer-
ence systems when working with perfect linguistic
(syntactic/semantic) information, and the degrada-
tion in performance when moving to a more realis-
tic scenario with noisy input annotation.
</bodyText>
<subsubsectionHeader confidence="0.98604">
2.1.2 Closed/open challenges
</subsubsectionHeader>
<bodyText confidence="0.999777">
In parallel, we will also consider the possibility of
differentiating between closed and open chal-
lenges, that is, when participants are allowed to use
strictly the information contained in the training
data (closed) and when they make use of some ex-
ternal resources/tools (open).
</bodyText>
<subsubsectionHeader confidence="0.802154">
2.1.3 Scoring measures
</subsubsectionHeader>
<bodyText confidence="0.999954933333334">
Regarding evaluation measures, we will have spe-
cific metrics for each of the subtasks, which will be
computed by language and overall.
Several metrics have been proposed for the task
of coreference resolution, and each of them pre-
sents advantages and drawbacks. For the purpose
of the current task, we have selected two of them —
B-cubed and CEAF — as the most appropriate ones.
In what follows we justify our choice.
The MUC scoring algorithm (Vilain et al., 1995)
has been the most widely used for at least two rea-
sons. Firstly, the MUC corpora and the MUC
scorer were the first available systems. Secondly,
the MUC scorer is easy to understand and imple-
ment. However, this metric has two major weak-
nesses: (i) it does not give any credit to the correct
identification of singleton entities (chains consist-
ing of one single mention), and (ii) it intrinsically
favors systems that produce fewer coreference
chains, which may result in higher F-measures for
worse systems.
A second well-known scoring algorithm, the
ACE value (NIST, 2003), owes its popularity to
the ACE evaluation campaign. Each error (a miss-
ing element, a misclassification of a coreference
chain, a mention in the response not included in the
key) made by the response has an associated cost,
which depends on the type of entity (e.g. person,
location, organization) and on the kind of mention
(e.g. name, nominal, pronoun). The fact that this
metric is entity-type and mention-type dependent,
and that it relies on ACE-type entities makes this
measure inappropriate for the current task.
The two measures that we are interested in com-
paring are B-cubed (Bagga and Baldwin, 1998)
and CEAF (Luo, 2005). The former does not look
at the links produced by a system as the MUC al-
gorithm does, but looks at the presence/absence of
mentions for each entity in the system output. Pre-
cision and recall numbers are computed for each
mention, and the average gives the final precision
and recall numbers.
CEAF (Luo, 2005) is a novel metric for evaluat-
ing coreference resolution that has already been
used in some published papers (Ng, 2008; Denis
and Baldridge, 2008). It mainly differs from B-
cubed in that it finds the best one-to-one entity
alignment between the gold and system responses
before computing precision and recall. The best
mapping is that which maximizes the similarity
over pairs of chains. The CEAF measure has two
variants: a mention-based, and an entity-based one.
While the former scores the similarity of two
chains as the absolute number of common men-
tions between them, the latter scores the relative
number of common mentions.
Luo (2005) criticizes the fact that a response
with all mentions in the same chain obtains 100%
B-cubed recall, whereas a response with each men-
tion in a different chain obtains 100% B-cubed
</bodyText>
<page confidence="0.991811">
72
</page>
<bodyText confidence="0.999953615384616">
precision. However, precision will be penalized in
the first case, and recall in the second case, each
captured by the corresponding F-measure. Luo&apos;s
entity alignment might cause that a correctly iden-
tified link between two mentions is ignored by the
scoring metric if that entity is not aligned. Finally,
as far as the two CEAF metrics are concerned, the
entity-based measure rewards alike a correctly
identified one-mention entity and a correctly iden-
tified five-mention entity, while the mention-based
measure takes into account the size of the entity.
Given this series of advantages and drawbacks,
we opted for including both B-cubed and CEAF
measures in the final evaluation of the systems. In
this way we will be able to perform a meta-
evaluation study, i.e. to evaluate and compare the
performance of metrics with respect to the task
objectives and system rankings. It might be inter-
esting to break B-cubed and CEAF into partial re-
sults across different kinds of mentions in order to
get a better understanding of the sources of errors
made by each system. Additionally, the MUC met-
ric will also be included for comparison purposes
with previous results.
Finally, for the setting with automatically gener-
ated input information (second scenario in Section
2.1.1), it might be desirable to devise metric vari-
ants accounting for partial matches of NPs. In this
case, capturing the correct NP head would give
most of the credit. We plan to work in this research
line in the near future.
Official scorers will be developed in advance
and made available to participants when posting
the trial datasets. The period in between the release
of trial datasets and the start of the full evaluation
will serve as a test for the evaluation metrics. De-
pending on the feedback obtained from the partici-
pants we might consider introducing some
improvements in the evaluation setting.
</bodyText>
<sectionHeader confidence="0.997974" genericHeader="method">
3 AnCora-CO corpora
</sectionHeader>
<bodyText confidence="0.998405666666667">
The corpora used in the task are AnCora-CO,
which are the result of enriching the AnCora cor-
pora (Taule et al., 2008) with coreference informa-
tion. AnCora-CO is a multilingual corpus
annotated at different linguistic levels consisting of
400K words in Catalan3, 400K words in Spanish2,
</bodyText>
<footnote confidence="0.6914185">
3 Freely available for research purposes from the following
URL: http://clic.ub.edu/ancora
</footnote>
<bodyText confidence="0.999931448275862">
and 120K words in English. For the purpose of the
task, the corpora are split into a training (85%) and
test (15%) set. Each file corresponds to one news-
paper text.
AnCora-CO consists mainly of newspaper and
newswire articles: 200K words from the Spanish
and Catalan versions of El Periodico newspaper,
and 200K words from the EFE newswire agency in
the Spanish corpus, and from the ACN newswire
agency in the Catalan corpus. The source corpora
for Spanish and Catalan are the AnCora corpora,
which were annotated by hand with full syntax
(constituents and functions) as well as with seman-
tic information (argument structure with thematic
roles, semantic verb classes, named entities, and
WordNet nominal senses). The annotation of
coreference constitutes an additional layer on top
of the previous syntactic-semantic information.
The English part of AnCora-CO consists of a se-
ries of documents of the Reuters newswire corpus
(RCV1 version).4 The RCV1 corpus does not come
with any syntactic nor semantic annotation. This is
why we only count with automatic linguistic anno-
tation produced by statistical taggers and parsers
on this corpus.
Although the Catalan, English, and Spanish cor-
pora used in the task all belong to the domain of
newspaper texts, they do not form a three-way par-
allel corpus.
</bodyText>
<subsectionHeader confidence="0.999212">
3.1 Coreference annotation
</subsectionHeader>
<bodyText confidence="0.999951058823529">
The annotation of a corpus with coreference in-
formation is highly complex due to (i) the lack of
information in descriptive grammars about this
topic, and (ii) the difficulty in generalizing the in-
sights from one language to another. Regarding (i),
a wide range of units and relations occur for which
it is not straightforward to determine whether they
are or not coreferent. Although there are theoretical
studies for English, they cannot always be ex-
tended to Spanish or Catalan since coreference is a
very language-specific phenomenon, which ac-
counts for (ii).
In the following we present some of the linguis-
tic issues more problematic in relation to corefer-
ence annotation, and how we decided to deal with
them in AnCora-CO (Recasens, 2008). Some of
them are language dependent (1); others concern
</bodyText>
<footnote confidence="0.93347">
4 Reuters Corpus RCV1 is distributed by NIST at the follow-
ing URL: http://trec.nist.gov/data/reuters/reuters.html
</footnote>
<page confidence="0.999021">
73
</page>
<bodyText confidence="0.9993265">
the internal structure of the mentions (2), or the
type of coreference link (3). Finally, we present
those NPs that were left out from the annotation
for not being referential (4).
</bodyText>
<sectionHeader confidence="0.703305" genericHeader="method">
1. Language-specific issues
</sectionHeader>
<bodyText confidence="0.99680925">
- Since Spanish and Catalan are pro-drop
languages, elliptical subjects were intro-
duced in the syntactic annotation, and they
are also annotated with coreference.
- Expletive it pronouns, which are frequent
in English and to a lesser extent in Spanish
and Catalan are not referential, and so they
do not participate in coreference links.
- In Spanish, clitic forms for pronouns can
merge into a single word with the verb; in
these cases the whole verbal node is anno-
tated for coreference.
</bodyText>
<listItem confidence="0.392072">
2. Issues concerning the mention structure
</listItem>
<bodyText confidence="0.999714833333333">
- In possessive NPs, only the reference of
the thing possessed (not the possessor) is
taken into account. For instance, su libro
`his book&apos; is linked with a previous refer-
ence of the same book; the possessive de-
terminer su `his&apos; does not constitute an NP
on its own.
- In the case of conjoined NPs, three (or
more) links can be encoded: one between
the entire NPs, and additional ones for
each of the constituent NPs. AnCora-CO
captures links at these different levels.
</bodyText>
<sectionHeader confidence="0.537324" genericHeader="method">
3. Issues concerning types of coreference links
</sectionHeader>
<bodyText confidence="0.9987481">
- Plural NPs can refer to two or more ante-
cedents that appear separately in the text.
In these cases an entity resulting from the
addition of two or more entities is created.
- Discourse deixis is kept under a specific
link tag because not all coreference resolu-
tion systems can handle such relations.
- Metonymy is annotated as a case of iden-
tity because both mentions pragmatically
corefer.
</bodyText>
<sectionHeader confidence="0.980551" genericHeader="method">
4. Non-referential NPs
</sectionHeader>
<bodyText confidence="0.999904285714286">
- In order to be linguistically accurate (van
Deemter and Kibble, 2000), we distinguish
between referring and attributive NPs:
while the first point to an entity, the latter
express some of its properties. Thus, at-
tributive NPs like apposition and predica-
tive phrases are not treated as identity
coreference in AnCora-CO (they are kept
distinct under the `predicative link&apos; tag).
- Bound anaphora and bridging reference go
beyond coreference and so are left out
from consideration.
The annotation process of the corpora is outlined in
the next section.
</bodyText>
<subsectionHeader confidence="0.99876">
3.2 Annotation process
</subsectionHeader>
<bodyText confidence="0.9908806">
The Ancora coreference annotation process in-
volves: (a) marking of mentions, and (b) marking
of coreference chains (entities).
(a) Referential full NPs (including proper nouns)
and pronouns (including elliptical and clitic pro-
nouns) are the potential mentions of a coreference
chain.
(b) In the current task only identity relations
(coreftype=&amp;quot;ident&amp;quot;) will be considered, which link
referential NPs that point to the same discourse
entity. Coreferent mentions are annotated with the
attribute entity. Mentions that point to the same
entity share the same entity number. In Figure 1,
for instance, el reial decret aprovat ahir pel Con-
sell_de_Ministres `the royal decree approved yes-
terday by the Council of Ministers&apos; is
entity=&amp;quot;entity9&amp;quot; and la nova norma `the new rule&apos;
is also entity=&amp;quot;entity9&amp;quot; because they corefer.
Hence, mentions referring to the same discourse
entity all share the same entity number.
The corpora were annotated by a total of seven
annotators (qualified linguists) using the An-
CoraPipe annotation tool (Bertran et al., 2008),
which allows different linguistic levels to be anno-
tated simultaneously and efficiently. AnCoraPipe
supports XML in-line annotations.
An initial reliability study was performed on a
small portion of the Spanish AnCora-CO corpus.
In that study, eight linguists annotated the corpus
material in parallel. Inter-annotator agreement was
computed with Krippendorff&apos;s alpha, achieving a
result above 0.8. Most of the problems detected
were attributed either to a lack of training of the
coders or to ambiguities that are left unresolved in
the discourse itself. After carrying out this reliabil-
ity study, we opted for annotating the corpora in a
two-stage process: a first pass in which all mention
attributes and coreference links were coded, and a
second pass in which the already annotated files
were revised.
</bodyText>
<page confidence="0.998097">
74
</page>
<sectionHeader confidence="0.999549" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999785535714286">
The SemEval-2010 multilingual coreference reso-
lution task has been presented for discussion.
Firstly, we aim to promote research on coreference
resolution from a learning-based perspective in a
multilingual scenario in order to: (a) explore port-
ability issues; (b) analyze language-specific tuning
requirements; (c) facilitate cross-linguistic com-
parisons between two Romance languages and be-
tween Romance languages and English; and (d)
encourage researchers to develop linguistic re-
sources — annotated corpora — oriented to corefer-
ence resolution for other languages.
Secondly, given the complexity of the corefer-
ence phenomena we split the coreference resolu-
tion task into two (full coreference chains and
pronominal resolution), and we propose two dif-
ferent scenarios (gold standard vs. automatically
generated input information) in order to evaluate to
what extent the performance of a coreference reso-
lution system varies depending on the quality of
the other levels of information.
Finally, given that the evaluation of coreference
resolution systems is still an open issue, we are
interested in comparing different coreference reso-
lution metrics: B-cubed and CEAF measures. In
this way we will be able to evaluate and compare
the performance of these metrics with respect to
the task objectives and system rankings.
</bodyText>
<sectionHeader confidence="0.99825" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.991568">
This research has been supported by the projects
Lang2World (TIN2006-15265-C06), TEXT-MESS
(TIN2006-15265-C04), OpenMT (TIN2006-
15307-C03-02), AnCora-Nom (FFI2008-02691-E),
and the FPU grant (AP2006-00994) from the Span-
ish Ministry of Education and Science, and the
funding given by the government of the Generalitat
de Catalunya.
</bodyText>
<sectionHeader confidence="0.9994" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99979784">
Bagga, Amit and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In Proceedings of Lan-
guage Resources and Evaluation Conference.
Bertran, Manuel, Oriol Borrega, Marta Recasens, and
Barbara Soriano. 2008. AnCoraPipe: A tool for mul-
tilevel annotation, Procesamiento del Lenguaje Natu-
ral, n. 41: 291-292, SEPLN.
Denis, Pascal and Jason Baldridge. 2008. Specialized
models and ranking for coreference resolution. Pro-
ceedings of the Empirical Methods in Natural Lan-
guage Processing (EMNLP 2008).
Luo, Xiaoqiang. 2005. On coreference resolution per-
formance metrics. Proceedings ofHLT/NAACL 2005.
McCarthy Joseph and Wendy Lehnert. 1995. Using
decision trees for coreference resolution. Proceed-
ings of the Fourteenth International Joint Conference
on Artificial Intelligence.
Mitkov, Ruslan. 1998. Robust pronoun resolution with
limited knowledge. Proceedings of the 36th Annual
Meeting of the Association for Computational Lin-
guistics, and 17th International Conference on Com-
putational Linguistics (COLING-ACL98).
Morton, Thomas. 2000. Using coreference for question
answering. Proceedings of the 8th Text REtrieval
Conference (TREC-8).
Ng, Vincent. 2008. Unsupervised models for corefer-
ence resolution. Proceedings of the Empirical Meth-
ods in Natural Language Processing (EMNLP 2008).
NIST. 2003. In Proceedings of ACE 2003 workshop.
Booklet, Alexandria, VA.
Recasens, Marta. 2008. Towards Coreference Resolu-
tion for Catalan and Spanish. Master Thesis. Univer-
sity of Barcelona.
Steinberger, Josef, Massimo Poesio, Mijail Kabadjov,
and Karel Jezek. 2007. Two uses of anaphora resolu-
tion in summarization. Information Processing and
Management, 43:1663—1680.
Taul6, Mariona, Ant6nia Marti, and Marta Recasens.
2008. AnCora: Multilevel corpora with coreference
information for Spanish and Catalan. In Proceedings
of the Language Resources and Evaluation Confer-
ence (LREC 2008).
van Deemter, Kees and Rodger Kibble. 2000. Squibs
and Discussions: On coreferring: coreference in
MUC and related annotation schemes. Computa-
tional Linguistics, 26(4):629-637.
Vilain, Marc, John Burger, John Aberdeen, Dennis
Connolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceedings
ofMUC-6.
</reference>
<page confidence="0.999132">
75
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.894965">
<title confidence="0.997421">SemEval-2010 Task 1: Coreference Resolution in Multiple Languages</title>
<author confidence="0.999677">Marta Recasens</author>
<author confidence="0.999677">Toni Marti</author>
<author confidence="0.999677">Mariona Taule Lluis Marquez</author>
<author confidence="0.999677">Emili Sapena</author>
<affiliation confidence="0.999761">Centre de Llenguatge i Computaci6 (CLiC) TALP Research Center, University of Barcelona Technical University of Catalonia</affiliation>
<address confidence="0.9707335">Gran Via de les Corts Catalanes 585 Jordi Girona Salgado 1-3 08007 Barcelona 08034 Barcelona</address>
<email confidence="0.9861095">mrecasens@ub.edu@lsi.upc.edu</email>
<email confidence="0.9861095">amarti@ub.edu@lsi.upc.edu</email>
<email confidence="0.9861095">mtaule}{lluism@ub.edu@lsi.upc.edu</email>
<email confidence="0.9861095">esapena@ub.edu@lsi.upc.edu</email>
<abstract confidence="0.998342642857143">This paper presents the task `Coreference Resolution in Multiple Languages&apos; to be run in SemEval-2010 (5th International Workshop on Semantic Evaluations). This task aims to evaluate and compare automatic coreference resolution systems for three different languages (Catalan, English, and Spanish) by means of two alternative evaluation metrics, thus providing an insight into (i) the portability of coreference resolution systems across languages, and (ii) the effect of different scoring metrics on ranking the output of the participant systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amit Bagga</author>
<author>Breck Baldwin</author>
</authors>
<title>Algorithms for scoring coreference chains.</title>
<date>1998</date>
<booktitle>In Proceedings of Language Resources and Evaluation Conference.</booktitle>
<contexts>
<context position="2639" citStr="Bagga and Baldwin, 1998" startWordPosition="388" endWordPosition="391">lish, and Spanish languages, and by means of two different evaluation metrics. By setting up a multilingual scenario, we can explore to what extent it is possible to implement a general system that is portable to the three languages, how much language-specific tuning is necessary, and the significant differences between Romance languages and English, as well as those between two closely related languages such as Spanish and Catalan. Besides, we expect to gain some useful insight into the development of multilingual NLP applications. As far as the evaluation is concerned, by employing B-cubed (Bagga and Baldwin, 1998) and CEAF (Luo, 2005) algorithms we can consider both the advantages and drawbacks of using one or the other scoring metric. For comparison purposes, the MUC score will also be reported. Among others, we are interested in the following questions: Which evaluation metric provides a more accurate picture of the accuracy of the system performance? Is there a strong correlation between them? Can 1 Corpora annotated with coreference are scarce, especially for languages other than English. 70 Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pa</context>
<context position="10183" citStr="Bagga and Baldwin, 1998" startWordPosition="1585" endWordPosition="1588"> (NIST, 2003), owes its popularity to the ACE evaluation campaign. Each error (a missing element, a misclassification of a coreference chain, a mention in the response not included in the key) made by the response has an associated cost, which depends on the type of entity (e.g. person, location, organization) and on the kind of mention (e.g. name, nominal, pronoun). The fact that this metric is entity-type and mention-type dependent, and that it relies on ACE-type entities makes this measure inappropriate for the current task. The two measures that we are interested in comparing are B-cubed (Bagga and Baldwin, 1998) and CEAF (Luo, 2005). The former does not look at the links produced by a system as the MUC algorithm does, but looks at the presence/absence of mentions for each entity in the system output. Precision and recall numbers are computed for each mention, and the average gives the final precision and recall numbers. CEAF (Luo, 2005) is a novel metric for evaluating coreference resolution that has already been used in some published papers (Ng, 2008; Denis and Baldridge, 2008). It mainly differs from Bcubed in that it finds the best one-to-one entity alignment between the gold and system responses</context>
</contexts>
<marker>Bagga, Baldwin, 1998</marker>
<rawString>Bagga, Amit and Breck Baldwin. 1998. Algorithms for scoring coreference chains. In Proceedings of Language Resources and Evaluation Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manuel Bertran</author>
<author>Oriol Borrega</author>
<author>Marta Recasens</author>
<author>Barbara Soriano</author>
</authors>
<title>AnCoraPipe: A tool for multilevel annotation,</title>
<date>2008</date>
<booktitle>Procesamiento del Lenguaje Natural, n.</booktitle>
<volume>41</volume>
<pages>291--292</pages>
<contexts>
<context position="19151" citStr="Bertran et al., 2008" startWordPosition="3046" endWordPosition="3049">scourse entity. Coreferent mentions are annotated with the attribute entity. Mentions that point to the same entity share the same entity number. In Figure 1, for instance, el reial decret aprovat ahir pel Consell_de_Ministres `the royal decree approved yesterday by the Council of Ministers&apos; is entity=&amp;quot;entity9&amp;quot; and la nova norma `the new rule&apos; is also entity=&amp;quot;entity9&amp;quot; because they corefer. Hence, mentions referring to the same discourse entity all share the same entity number. The corpora were annotated by a total of seven annotators (qualified linguists) using the AnCoraPipe annotation tool (Bertran et al., 2008), which allows different linguistic levels to be annotated simultaneously and efficiently. AnCoraPipe supports XML in-line annotations. An initial reliability study was performed on a small portion of the Spanish AnCora-CO corpus. In that study, eight linguists annotated the corpus material in parallel. Inter-annotator agreement was computed with Krippendorff&apos;s alpha, achieving a result above 0.8. Most of the problems detected were attributed either to a lack of training of the coders or to ambiguities that are left unresolved in the discourse itself. After carrying out this reliability study,</context>
</contexts>
<marker>Bertran, Borrega, Recasens, Soriano, 2008</marker>
<rawString>Bertran, Manuel, Oriol Borrega, Marta Recasens, and Barbara Soriano. 2008. AnCoraPipe: A tool for multilevel annotation, Procesamiento del Lenguaje Natural, n. 41: 291-292, SEPLN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Denis</author>
<author>Jason Baldridge</author>
</authors>
<title>Specialized models and ranking for coreference resolution.</title>
<date>2008</date>
<booktitle>Proceedings of the Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<contexts>
<context position="10660" citStr="Denis and Baldridge, 2008" startWordPosition="1668" endWordPosition="1671">ies makes this measure inappropriate for the current task. The two measures that we are interested in comparing are B-cubed (Bagga and Baldwin, 1998) and CEAF (Luo, 2005). The former does not look at the links produced by a system as the MUC algorithm does, but looks at the presence/absence of mentions for each entity in the system output. Precision and recall numbers are computed for each mention, and the average gives the final precision and recall numbers. CEAF (Luo, 2005) is a novel metric for evaluating coreference resolution that has already been used in some published papers (Ng, 2008; Denis and Baldridge, 2008). It mainly differs from Bcubed in that it finds the best one-to-one entity alignment between the gold and system responses before computing precision and recall. The best mapping is that which maximizes the similarity over pairs of chains. The CEAF measure has two variants: a mention-based, and an entity-based one. While the former scores the similarity of two chains as the absolute number of common mentions between them, the latter scores the relative number of common mentions. Luo (2005) criticizes the fact that a response with all mentions in the same chain obtains 100% B-cubed recall, whe</context>
</contexts>
<marker>Denis, Baldridge, 2008</marker>
<rawString>Denis, Pascal and Jason Baldridge. 2008. Specialized models and ranking for coreference resolution. Proceedings of the Empirical Methods in Natural Language Processing (EMNLP 2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
</authors>
<title>On coreference resolution performance metrics.</title>
<date>2005</date>
<booktitle>Proceedings ofHLT/NAACL</booktitle>
<contexts>
<context position="2660" citStr="Luo, 2005" startWordPosition="394" endWordPosition="395"> means of two different evaluation metrics. By setting up a multilingual scenario, we can explore to what extent it is possible to implement a general system that is portable to the three languages, how much language-specific tuning is necessary, and the significant differences between Romance languages and English, as well as those between two closely related languages such as Spanish and Catalan. Besides, we expect to gain some useful insight into the development of multilingual NLP applications. As far as the evaluation is concerned, by employing B-cubed (Bagga and Baldwin, 1998) and CEAF (Luo, 2005) algorithms we can consider both the advantages and drawbacks of using one or the other scoring metric. For comparison purposes, the MUC score will also be reported. Among others, we are interested in the following questions: Which evaluation metric provides a more accurate picture of the accuracy of the system performance? Is there a strong correlation between them? Can 1 Corpora annotated with coreference are scarce, especially for languages other than English. 70 Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 70–75, Boulder, C</context>
<context position="10204" citStr="Luo, 2005" startWordPosition="1591" endWordPosition="1592">to the ACE evaluation campaign. Each error (a missing element, a misclassification of a coreference chain, a mention in the response not included in the key) made by the response has an associated cost, which depends on the type of entity (e.g. person, location, organization) and on the kind of mention (e.g. name, nominal, pronoun). The fact that this metric is entity-type and mention-type dependent, and that it relies on ACE-type entities makes this measure inappropriate for the current task. The two measures that we are interested in comparing are B-cubed (Bagga and Baldwin, 1998) and CEAF (Luo, 2005). The former does not look at the links produced by a system as the MUC algorithm does, but looks at the presence/absence of mentions for each entity in the system output. Precision and recall numbers are computed for each mention, and the average gives the final precision and recall numbers. CEAF (Luo, 2005) is a novel metric for evaluating coreference resolution that has already been used in some published papers (Ng, 2008; Denis and Baldridge, 2008). It mainly differs from Bcubed in that it finds the best one-to-one entity alignment between the gold and system responses before computing pre</context>
</contexts>
<marker>Luo, 2005</marker>
<rawString>Luo, Xiaoqiang. 2005. On coreference resolution performance metrics. Proceedings ofHLT/NAACL 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>McCarthy Joseph</author>
<author>Wendy Lehnert</author>
</authors>
<title>Using decision trees for coreference resolution.</title>
<date>1995</date>
<booktitle>Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence.</booktitle>
<marker>Joseph, Lehnert, 1995</marker>
<rawString>McCarthy Joseph and Wendy Lehnert. 1995. Using decision trees for coreference resolution. Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruslan Mitkov</author>
</authors>
<title>Robust pronoun resolution with limited knowledge.</title>
<date>1998</date>
<booktitle>Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics, and 17th International Conference on Computational Linguistics (COLING-ACL98).</booktitle>
<contexts>
<context position="1610" citStr="Mitkov, 1998" startWordPosition="232" endWordPosition="233">nce information has been shown to be beneficial in many NLP applications such as Information Extraction (McCarthy and Lehnert, 1995), Text Summarization (Steinberger et al., 2007), Question Answering (Morton, 2000), and Machine Translation. In these systems, there is a need to identify the different pieces of information that refer to the same discourse entity in order to produce coherent and fluent summaries, disambiguate the references to an entity, and solve anaphoric pronouns. Coreference is an inherently complex phenomenon. Some of the limitations of the traditional rulebased approaches (Mitkov, 1998) could be overcome by machine learning techniques, which allow automating the acquisition of knowledge from annotated corpora. This task will promote the development of linguistic resources —annotated corpora1— and machine-learning techniques oriented to coreference resolution. In particular, we aim to evaluate and compare coreference resolution systems in a multilingual context, including Catalan, English, and Spanish languages, and by means of two different evaluation metrics. By setting up a multilingual scenario, we can explore to what extent it is possible to implement a general system th</context>
</contexts>
<marker>Mitkov, 1998</marker>
<rawString>Mitkov, Ruslan. 1998. Robust pronoun resolution with limited knowledge. Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics, and 17th International Conference on Computational Linguistics (COLING-ACL98).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Morton</author>
</authors>
<title>Using coreference for question answering.</title>
<date>2000</date>
<booktitle>Proceedings of the 8th Text REtrieval Conference (TREC-8).</booktitle>
<contexts>
<context position="1211" citStr="Morton, 2000" startWordPosition="169" endWordPosition="170">o evaluate and compare automatic coreference resolution systems for three different languages (Catalan, English, and Spanish) by means of two alternative evaluation metrics, thus providing an insight into (i) the portability of coreference resolution systems across languages, and (ii) the effect of different scoring metrics on ranking the output of the participant systems. 1 Introduction Coreference information has been shown to be beneficial in many NLP applications such as Information Extraction (McCarthy and Lehnert, 1995), Text Summarization (Steinberger et al., 2007), Question Answering (Morton, 2000), and Machine Translation. In these systems, there is a need to identify the different pieces of information that refer to the same discourse entity in order to produce coherent and fluent summaries, disambiguate the references to an entity, and solve anaphoric pronouns. Coreference is an inherently complex phenomenon. Some of the limitations of the traditional rulebased approaches (Mitkov, 1998) could be overcome by machine learning techniques, which allow automating the acquisition of knowledge from annotated corpora. This task will promote the development of linguistic resources —annotated </context>
</contexts>
<marker>Morton, 2000</marker>
<rawString>Morton, Thomas. 2000. Using coreference for question answering. Proceedings of the 8th Text REtrieval Conference (TREC-8).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
</authors>
<title>Unsupervised models for coreference resolution.</title>
<date>2008</date>
<booktitle>Proceedings of the Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<contexts>
<context position="10632" citStr="Ng, 2008" startWordPosition="1666" endWordPosition="1667">type entities makes this measure inappropriate for the current task. The two measures that we are interested in comparing are B-cubed (Bagga and Baldwin, 1998) and CEAF (Luo, 2005). The former does not look at the links produced by a system as the MUC algorithm does, but looks at the presence/absence of mentions for each entity in the system output. Precision and recall numbers are computed for each mention, and the average gives the final precision and recall numbers. CEAF (Luo, 2005) is a novel metric for evaluating coreference resolution that has already been used in some published papers (Ng, 2008; Denis and Baldridge, 2008). It mainly differs from Bcubed in that it finds the best one-to-one entity alignment between the gold and system responses before computing precision and recall. The best mapping is that which maximizes the similarity over pairs of chains. The CEAF measure has two variants: a mention-based, and an entity-based one. While the former scores the similarity of two chains as the absolute number of common mentions between them, the latter scores the relative number of common mentions. Luo (2005) criticizes the fact that a response with all mentions in the same chain obta</context>
</contexts>
<marker>Ng, 2008</marker>
<rawString>Ng, Vincent. 2008. Unsupervised models for coreference resolution. Proceedings of the Empirical Methods in Natural Language Processing (EMNLP 2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>NIST</author>
</authors>
<date>2003</date>
<booktitle>In Proceedings of ACE 2003 workshop. Booklet,</booktitle>
<location>Alexandria, VA.</location>
<contexts>
<context position="9572" citStr="NIST, 2003" startWordPosition="1487" endWordPosition="1488">coring algorithm (Vilain et al., 1995) has been the most widely used for at least two reasons. Firstly, the MUC corpora and the MUC scorer were the first available systems. Secondly, the MUC scorer is easy to understand and implement. However, this metric has two major weaknesses: (i) it does not give any credit to the correct identification of singleton entities (chains consisting of one single mention), and (ii) it intrinsically favors systems that produce fewer coreference chains, which may result in higher F-measures for worse systems. A second well-known scoring algorithm, the ACE value (NIST, 2003), owes its popularity to the ACE evaluation campaign. Each error (a missing element, a misclassification of a coreference chain, a mention in the response not included in the key) made by the response has an associated cost, which depends on the type of entity (e.g. person, location, organization) and on the kind of mention (e.g. name, nominal, pronoun). The fact that this metric is entity-type and mention-type dependent, and that it relies on ACE-type entities makes this measure inappropriate for the current task. The two measures that we are interested in comparing are B-cubed (Bagga and Bal</context>
</contexts>
<marker>NIST, 2003</marker>
<rawString>NIST. 2003. In Proceedings of ACE 2003 workshop. Booklet, Alexandria, VA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Recasens</author>
</authors>
<title>Towards Coreference Resolution for Catalan and Spanish. Master Thesis.</title>
<date>2008</date>
<institution>University of Barcelona.</institution>
<contexts>
<context position="15680" citStr="Recasens, 2008" startWordPosition="2489" endWordPosition="2490">ars about this topic, and (ii) the difficulty in generalizing the insights from one language to another. Regarding (i), a wide range of units and relations occur for which it is not straightforward to determine whether they are or not coreferent. Although there are theoretical studies for English, they cannot always be extended to Spanish or Catalan since coreference is a very language-specific phenomenon, which accounts for (ii). In the following we present some of the linguistic issues more problematic in relation to coreference annotation, and how we decided to deal with them in AnCora-CO (Recasens, 2008). Some of them are language dependent (1); others concern 4 Reuters Corpus RCV1 is distributed by NIST at the following URL: http://trec.nist.gov/data/reuters/reuters.html 73 the internal structure of the mentions (2), or the type of coreference link (3). Finally, we present those NPs that were left out from the annotation for not being referential (4). 1. Language-specific issues - Since Spanish and Catalan are pro-drop languages, elliptical subjects were introduced in the syntactic annotation, and they are also annotated with coreference. - Expletive it pronouns, which are frequent in Englis</context>
</contexts>
<marker>Recasens, 2008</marker>
<rawString>Recasens, Marta. 2008. Towards Coreference Resolution for Catalan and Spanish. Master Thesis. University of Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josef Steinberger</author>
<author>Massimo Poesio</author>
<author>Mijail Kabadjov</author>
<author>Karel Jezek</author>
</authors>
<title>Two uses of anaphora resolution in summarization.</title>
<date>2007</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>43--1663</pages>
<contexts>
<context position="1176" citStr="Steinberger et al., 2007" startWordPosition="163" endWordPosition="166">shop on Semantic Evaluations). This task aims to evaluate and compare automatic coreference resolution systems for three different languages (Catalan, English, and Spanish) by means of two alternative evaluation metrics, thus providing an insight into (i) the portability of coreference resolution systems across languages, and (ii) the effect of different scoring metrics on ranking the output of the participant systems. 1 Introduction Coreference information has been shown to be beneficial in many NLP applications such as Information Extraction (McCarthy and Lehnert, 1995), Text Summarization (Steinberger et al., 2007), Question Answering (Morton, 2000), and Machine Translation. In these systems, there is a need to identify the different pieces of information that refer to the same discourse entity in order to produce coherent and fluent summaries, disambiguate the references to an entity, and solve anaphoric pronouns. Coreference is an inherently complex phenomenon. Some of the limitations of the traditional rulebased approaches (Mitkov, 1998) could be overcome by machine learning techniques, which allow automating the acquisition of knowledge from annotated corpora. This task will promote the development </context>
</contexts>
<marker>Steinberger, Poesio, Kabadjov, Jezek, 2007</marker>
<rawString>Steinberger, Josef, Massimo Poesio, Mijail Kabadjov, and Karel Jezek. 2007. Two uses of anaphora resolution in summarization. Information Processing and Management, 43:1663—1680.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mariona Taul6</author>
<author>Ant6nia Marti</author>
<author>Marta Recasens</author>
</authors>
<title>AnCora: Multilevel corpora with coreference information for Spanish and Catalan.</title>
<date>2008</date>
<booktitle>In Proceedings of the Language Resources and Evaluation Conference (LREC</booktitle>
<marker>Taul6, Marti, Recasens, 2008</marker>
<rawString>Taul6, Mariona, Ant6nia Marti, and Marta Recasens. 2008. AnCora: Multilevel corpora with coreference information for Spanish and Catalan. In Proceedings of the Language Resources and Evaluation Conference (LREC 2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kees van Deemter</author>
<author>Rodger Kibble</author>
</authors>
<title>Squibs and Discussions: On coreferring: coreference in MUC and related annotation schemes.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<pages>26--4</pages>
<marker>van Deemter, Kibble, 2000</marker>
<rawString>van Deemter, Kees and Rodger Kibble. 2000. Squibs and Discussions: On coreferring: coreference in MUC and related annotation schemes. Computational Linguistics, 26(4):629-637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Vilain</author>
<author>John Burger</author>
<author>John Aberdeen</author>
<author>Dennis Connolly</author>
<author>Lynette Hirschman</author>
</authors>
<title>A modeltheoretic coreference scoring scheme.</title>
<date>1995</date>
<booktitle>In Proceedings ofMUC-6.</booktitle>
<contexts>
<context position="8999" citStr="Vilain et al., 1995" startWordPosition="1390" endWordPosition="1393">se strictly the information contained in the training data (closed) and when they make use of some external resources/tools (open). 2.1.3 Scoring measures Regarding evaluation measures, we will have specific metrics for each of the subtasks, which will be computed by language and overall. Several metrics have been proposed for the task of coreference resolution, and each of them presents advantages and drawbacks. For the purpose of the current task, we have selected two of them — B-cubed and CEAF — as the most appropriate ones. In what follows we justify our choice. The MUC scoring algorithm (Vilain et al., 1995) has been the most widely used for at least two reasons. Firstly, the MUC corpora and the MUC scorer were the first available systems. Secondly, the MUC scorer is easy to understand and implement. However, this metric has two major weaknesses: (i) it does not give any credit to the correct identification of singleton entities (chains consisting of one single mention), and (ii) it intrinsically favors systems that produce fewer coreference chains, which may result in higher F-measures for worse systems. A second well-known scoring algorithm, the ACE value (NIST, 2003), owes its popularity to th</context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995</marker>
<rawString>Vilain, Marc, John Burger, John Aberdeen, Dennis Connolly, and Lynette Hirschman. 1995. A modeltheoretic coreference scoring scheme. In Proceedings ofMUC-6.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>