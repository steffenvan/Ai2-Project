<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.996111">
The Structure of Shared Forests
in Ambiguous Parsing
</title>
<author confidence="0.969392">
Sylvie Billott* Bernard Lang*
</author>
<sectionHeader confidence="0.708926" genericHeader="abstract">
INRIA
&apos;and Universite d&apos;Orleans
</sectionHeader>
<bodyText confidence="0.456727">
billotOinria.inria.fr langOinria.inria.fr
</bodyText>
<sectionHeader confidence="0.957659" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.93605752631579">
The Context-Free backbone of some natural language ana-
lyzers produces all possible CF parses as some kind of shared
forest, from which a single tree is to be chosen by a disam-
biguation process that may be based on the finer features of
the language. We study the structure of these forests with
respect to optimality of sharing, and in relation with the
parsing schema used to produce them. In addition to a theo-
retical and experimental framework for studying these issues,
the main results presented are:
- sophistication in chart parsing schemata (e.g. use of
look-ahead) may reduce time and space efficiency instead of
improving it,
- there is a shared forest structure with at most cubic size
for any CF grammar,
- when 0(n3) complexity is required, the shape of a shared
forest is dependent on the parsing schema used.
Though analyzed on CF grammars for simplicity, these re-
sults extend to more complex formalisms such as unification
based grammars.
</bodyText>
<keyword confidence="0.801007333333333">
Key words: Context-Free Parsing, Ambiguity, Dynamic
Programming, Earley Parsing, Chart Parsing, Parsing
Strategies, Parsing Schemata, Parse Tree, Parse Forest.
</keyword>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.963916849056604">
Several natural language parser start with a pure Context-
Free (CF) backbone that makes a first sketch of the struc-
ture of the analyzed sentence, before it is handed to a more
elaborate analyzer (possibly a coroutine), that takes into ac-
count the finer grammatical structure to filter out undesir-
able parses (see for example [24,28]). In [28], Shieber sur-
veys existing variants to this approach before giving his own
tunable approach based on restrictions that &amp;quot;split up the
infinite nonterminal domain into a finite set of equivalence
classes that can be used for parsing&amp;quot;. The basic motivation
for this approach is to benefit from the CF parsing technol-
ogy whose development over 30 years has lead to powerful
and efficient parsers [1,7].
A parser that takes into account only an approximation of
the grammatical features will often find ambiguities it can-
not resolve in the analyzed sentences&apos;. A natural solution
&apos;Address: INRIA, B.P. 105, 78153 Le Chesnay, France.
The work reported here was partially supported by the Eureka
Software Factory project.
I Ambiguity may also have a semantical origin.
is then to produce all possible parses, according to the CF
backbone, and then select among them on the basis of the
complete features information. One hitch is that the num-
ber of parses may be exponential in the size of the input
sentence, or even infinite for cyclic grammars or incomplete
sentences [16]. However chart parsing techniques have been
developed that produce an encoding of all possible parses as
a data structure with a size polynomial in the length of the
input sentence. These techniques are all based on a dynamic
programming paradigm.
The kind of structure they produce to represent all parses
of the analyzed sentence is an essential characteristic of these
algorithms. Some of the published algorithms produce only
a chart as described by Kay in [14], which only associates
nonterminal categories to segments of the analyzed sentence
[11,39,13,3,9], and which thus still requires non-trivial pro-
cessing to extract parse-trees [26]. The worst size complex-
ity of such a chart is only a square function of the size of the
input2.
However, practical parsing algorithms will often produce a
more complex structure that explicitly relates the instances
of nonterminals associated with sentence fragments to their
constituents, possibly in several ways in case of ambiguity,
with a sharing of some common subtrees between the distinct
ambiguous parses [7,4,24,31,25]3
One advantage of this structure is that the chart retains
only these constituents that can actually participate in a
parse. Furthermore it makes the extraction of parse-trees
a trivial matter. A drawback is that this structure may be
cubic in the length of the parsed sentence, and more gener-
ally polynomial4 for some proposed algorithms [31]. How-
ever, these algorithms are rather well behaved in practice,
and this complexity is not a problem.
</bodyText>
<subsectionHeader confidence="0.620913">
In this paper we shall call shared forests such data struc-
</subsectionHeader>
<bodyText confidence="0.914588909090909">
2 We do not consider CF recognizers that have asymptotically
the lowest complexity, but are only of theoretical interest here
[35,5].
3 There are several other published implementation of chart
parsers [23,20,33], but they often do not give much detail on the
output of the parsing process, or even side-step the problem al-
together [33]. We do not consider here the well formed stastring
tables of Shell [26] which falls somewhere in between in our classi-
fication. They do not use pointers and parse-trees are only &amp;quot;indi-
rectly&amp;quot; visible, but may be extracted rather simply in linear time.
,The table may contain useless constituents.
</bodyText>
<note confidence="0.710176">
4 Space cubic algorithms often require the language grammar to
be in Cho/flaky Normal Form, and some authors have incorrectly
conjectured that cubic complexity cannot be obtained otherwise.
</note>
<page confidence="0.988798">
1143
</page>
<bodyText confidence="0.9811595">
tures used to represent simultaneously all parse trees for a
given sentence.
Several questions may be asked in relation with shared
forests:
</bodyText>
<listItem confidence="0.994056">
• How to construct them during the parsing process?
• Can the cubic complexity be attained without modify-
ing the grammar (e.g. into Chomsky Normal Form)?
• What is the appropriate data structure to improve
sharing and reduce time and space complexity?
• How good is the sharing of tree fragments between
ambiguous parses, and how can it be improved?
• Is there a relation between the coding of parse-trees in
the shared forest and the parsing schema used?
• How well formalized is their definition and construc-
tion?
</listItem>
<bodyText confidence="0.99996025">
These questions are of importance in practical systems
because the answers impact both the performance and the
implementation techniques. For example good sharing may
allow a better factorization of the computation that filters
parse trees with the secondary features of the language. The
representation needed for good sharing or low space com-
plexity may be incompatible with the needs of other com-
ponents of the system. These components may also make
assumptions about this representation that are incompatible
with some parsing schemata. The issue of formalization is of
course related to the formal tractability of correctness proof
for algorithms using shared forests.
In section 2 we describe a uniform theoretical framework in
which various parsing strategies are expressed and compared
with respect to the above questions. This approach has been
implemented into a system intended for the experimental
study and comparison of parsing strategies. This system is
described in section 3. Section 4 contains a detailed example
produced with our implementation which illustrates both the
working of the system and the underlying theory.
</bodyText>
<sectionHeader confidence="0.971151" genericHeader="method">
2 A Uniform Framework
</sectionHeader>
<bodyText confidence="0.998351869565217">
To discuss the above issues in a uniform way, we need a gen-
eral framework that encompasses all forms of chart parsing
and shared forest building in a unique formalism. We shall
take as a baths a formalism developed by the second author
in previous papers [15,16]. The idea of this approach is to
separate the dynamic programming constructs needed for ef-
ficient chart parsing from the chosen parsing schema. Com-
parison between the classifications of Kay [14] and Griffith dc
Petrick [10] shows that a parsing schema (or parsing strat-
egy) may be expressed in the construction of a Push-Down
Transducer (PDT), a well studied formalization of left-to-
right CF parsers&apos;. These PDTs are usually non-deterministic
and cannot be used as produced for actual parsing. Their
backtrack simulation does not always terminate, and is often
time-exponential when it does, while breadth-first simula-
tion is usually exponential for both time and space. However,
by extending Earley&apos;s dynamic programming construction to
PDTs, Lang provided in(15] a way of simulating all possible
computations of any PDT in cubic time and space complex-
5 Griffith &amp; Petrick actually use Turing machines for pedagog-
ical reasons.
ity. This approach may thus be used as a uniform framework
for comparing chart parsers&apos;.
</bodyText>
<subsectionHeader confidence="0.998182">
2.1 The algorithm
</subsectionHeader>
<bodyText confidence="0.973268888888889">
The following is a formal overview of parsing by dynamic
programming interpretation of PDTs.
Our aim is to parse sentences in the language L(G) gen-
erated by a CF phrase structure grammar G = (V, E, 11,N)
according to its syntax. The notation used is V for the set
of nonterminal, E for the set of terminals, II for the rules, i4
for the initial nonterminal, and e for the empty string.
We assume that, by some appropriate parser construction
technique (e.g. (12,6,1]) we mechanically produce from the
grammar G a parser for the language L(G) in the. form of
a (possibly non-deterministic) push-down transducer (PDT)
TG. The output of each possible computation of the parser
is a sequence of rules in Ir to be used in a left-to-right
reduction of the input sentence (this is obviously equivalent
to producing a parse-tree).
We assume for the PDT TG a very general formal defini-
tion that can fit most usual PDT construction techniques. It
•
is defined as an 8-tuple TG = (Q, E, A, 11, 5, q, $, F) where:
Q is the set of states, M is the set of input word symbols, A
is the set of stack symbols, 11 is the set of output symbols8
•
(i.e. rules of G), q is the initial state, $ is the initial stack
symbol, F is the set of final states, 6 is a finite set of e
tran-
sitionsof the form: (p A a I—. q B u) with
p,ciQ
</bodyText>
<sectionHeader confidence="0.584329" genericHeader="method">
A,BE A U{e}, a E Ell{e}, and u II*.
</sectionHeader>
<bodyText confidence="0.99990335">
Let the PDT be in a configuration p = (p Aa az u) where
p is the current state, Aa is the stack contents with A on
the top, az is the remaining input where the symbol a is the
next to be shifted and z E E*, and u is the already produced
output. The application of a transition r = (p A a q B v)
results in a new configuration p&apos; = (q Ba z uv) where the
terminal symbol a has been scanned (i.e. shifted), A has been
popped and B has been pushed, and v has been concatenated
to the existing output u. If the terminal symbol a is replaced
by e in the transition, no input symbol is scanned. If A (resp.
B) is replaced bye then no stack symbol is popped from (resp.
pushed on) the stack.
Our algorithm consists in an Earley-like&apos; simulation of the
PDT TG. Using the terminology of [1], the algorithm builds
an item set Si successively for each word symbol x, holding
position i in the input sentence z. An item is constituted
of two modes of the form (p A i) where p is a PDT state,
A is a stack symbol, and Os the index of an input symbol.
The item set Si contains items of the form ((p A 1) (q B j)) .
These items are used as nonterminals of an output grammar
</bodyText>
<tableCaption confidence="0.466265153846154">
6 The original intent of (15] was to show how one can generate
efficient general CF chart parsers, by first producing the PDT with
the efficient techniques for deterministic parsing developed for the
compiler technology (6,12,1]. This idea was later successfully used
by Ton:tits [31] who applied it to LR(1) parsers [6,1], and later to
other pushdown based parsers [32].
7 Implementations usually denote these rules by their index in
the set 11.
8 Actual implementations use output symbols from nuE, since
rules alone do not distinguish words in the same lexical category.
9 We assume the reader to be familiar with some variation of
Earley&apos;s algorithm. Earley&apos;s original paper uses the word state
(from dynamic programming terminology) instead of item.
</tableCaption>
<bodyText confidence="0.99860075">
g = (S, fl, P, Ui), where $ is the set of all items (i.e. the
union of Si), and the rules in P are constructed together
with their left-hand-side item by the algorithm. The initial
nonterminal Uf of g derives on the last items produced by a
successful computation.
Appendix A gives the details of the construction of items
and rules in g by interpretation of the transitions of the PDT.
More details may be found in [15,16].
</bodyText>
<subsectionHeader confidence="0.999912">
2.2 The shared forest
</subsectionHeader>
<bodyText confidence="0.99991262962963">
An apparently major difference between the above algorithm
and other parsers is that it represents a parse as the string of
the grammar rules used in a leftmost reduction of the parsed
sentence, rather than as a parse tree (cf. section 4). When
the sentence has several distinct parses, the set of all possi-
ble parse strings is represented in finite shared form by a CF
grammar that generates that possibly infinite set. Other
published algorithms produce instead a graph structure rep-
resenting all parse-trees with sharing of common subparts,
which corresponds well to the intuitive notion of a shared
forest.
This difference is only appearance. We show here in sec-
tion 4 that the CF grammar of all leftmost parses is just a
theoretical formalization of the shared-forest graph. Context-
Free grammars can be represented by AND-OR graphs that
are closely related to the syntax diagrams often used to de-
scribe the syntax of programming languages [37], and to the
transition networks of Woods [22]. In the case of our gram-
mar of leftmost parses, this AND-OR graph (which is acyclic
when there is only finite ambiguity) is precisely the shared-
forest graph. In this graph, AND-nodes correspond to the
usual parse-tree nodes, while OR-nodes correspond to ambi-
guities, i.e. distinct possible subtrees occurring in the same
context. Sharing of subtrees in represented by nodes accessed
by more than one other node.
The grammar viewpoint is the following (d. the example
in section 4). Non-terminal (reap. terminal) symbols corre-
spond to nodes with (resp. without) outgoing arcs. AND-
nodes correspond to right-hand sides of grammar rules, and
OR-nodes (i.e. ambiguities) correspond to non-terminals de-
fined by several rules. Subtree sharing is represented by sev-
eral uses of the same symbol in rule right-hand sides.
To our knowledge, this representation of parse-forests as
grammars is the simplest and most tractable theoretical for-
malization proposed so far, and the parser presented here is
the only one for which the correctness of the output gram-
mar — i.e. of the shared-forest — has ever been proved.
Though in the examples we use graph(ical) representations
for intuitive understanding (grammars are also sometimes
represented as graphs [37]), they are not the proper formal
tool for manipulating shared forests, and developing formal-
ized (proved) algorithms that use them. Graph formalization
is considerably more complex and awkward to manipulate
than the well understood, specialized and few concepts of
CF grammars. Furthermore, unlike graphs, this grammar
formalization of the shared forest may be tractably extended
to other grammatical formalisms (cf. section 5).
More importantly, our work on the parsing of incomplete
sentences [16] has exhibited the fundamental character of
our grammatical view of shared forests: when parsing the
completely unknown sentence, the shared forest obtained is
precisely the complete grammar of the analyzed language.
This also leads to connections with the work on partial eval-
uation [8].
</bodyText>
<subsectionHeader confidence="0.995848">
2.3 The shape of the forest
</subsectionHeader>
<bodyText confidence="0.999913942307692">
For our shared-forest, a cubic space complexity (in the worst
case — space complexity is often linear in practice) is
achieved, without requiring that the language grammar be in
Chomsky Normal Form, by producing a grammar of parses
that has at most two symbols on the right-hand side of its
rules. This amounts to representing the list of sons of a parse
tree node as a Lisp-like list built with binary nodes (see fig-
ures 1 Sz 2), and it allows partial sharing of the sons
The structure of the parse grammar, i.e. the shape of the
parse forest, is tightly related to the parsing schema used,
hence to the structure of the possible computation of the
non-deterministic PDT from which the parser is constructed.
First we need a precise characterization of parsing strategies,
whose distinction is often blurred by superimposed optimiza-
tions. We call bottom-up a strategy in which the PDT
decides on the nature of a constituent (i.e. on the grammar
rule that structures it), after having made this decision first
on its subconstituents. It corresponds to a postfix left-to-
right walk of the parse tree. Top-Down parsing recognizes
a constituent before recognition of its subconstituents, and
corresponds to a prefix walk. Intermediate strategies are also
possible.
The sequence of operations of a bottom-up parser is basi-
cally of the following form (up to possible simplifying op-
timizations): To parse a constituent A, the parser first
parses and pushes on the stack each sub-constituent B,; at
some point, it decides that it has all the constituents of
A on the stack and it pops them all, and then it pushes
A and outputs the (rule number r of the) recognized rule
: A —■ 131 Ba,. Dynamic programming interpretation
of such a sequence results in a shared forest containing parse-
trees with the shape described in figure 1, i.e. where each
node of the forest points to the beginning of the list of its
sons.
A top-down PDT uses a different sequence of operations,
detailed in appendix B, resulting in the shape of figure 2
where a forest node points to the end of the list of sons, which
is itself chained backward. These two figures are only simple
examples. Many variations on the shape of parse trees and
forests may be obtained by changing the parsing schema.
Sharing in the shared forest may correspond to sharing of
a complete subtree, but also to sharing of a tail of a list of
sons: this is what allows the cubic complexity. Thus bottom-
up parsing may share only the rightmost subconstituents of a
constituent, while top-down parsing may share only the left-
most subconstituents. This relation between parsing schema
and shape of the shared forest (and type of sharing) is a con-
sequence of intrinsic properties of chart parsing, and not of
our specific implementation.
It is for example to be expected that the bidirectional na-
ture of island parsing leads to irregular structure in shared
forests, when optimal sharing is sought for.
</bodyText>
<sectionHeader confidence="0.997056" genericHeader="method">
3 Implementation and Experimental
Results
</sectionHeader>
<bodyText confidence="0.997657">
The ideas presented above have been implemented in an ex-
perimental system called Tin (after the woodman of OZ).
This was noted by Shell [26] and is implicit in his use of &amp;quot;2-
form&amp;quot; grammars.
</bodyText>
<page confidence="0.994536">
145
</page>
<bodyText confidence="0.921079989690721">
The intent is to provide a uniform framework for the con-
struction and experimentation of chart parsers, somewhat
as systems like MCHART [29], but with a more systematic
theoretical foundation. The kernel of the system is a virtual
parsing machine with a stack and a set of primitive com-
mands corresponding essentially to the operation of a practi-
cal Push-Down Transducer. These commands include for ex-
ample: push (resp. pop) to push a symbol on the stack (resp.
pop one), chackwindow to compare the look-ahead symbol(s)
to some given symbol, chsckatack to branch depending on
the top of the stack, scan to read an input word, output to
output a rule number (or a terminal symbol), pto for uncon-
ditional jumps, and a few others. However these commands
are never used directly to program parsers. They are used as
machine instructions for compilers that compile grammatical
definitions into Tin code according to some parsing schema.
A characteristic of these commands is that they may all be
marked as non-deterministic. The intuitive interpretation is
that there is a non-deterministic choice between a command
thus marked and another command whose address in the
virtual machine code is then specified. However execution of
the virtual machine code is done by an all-paths interpreter
that follows the dynamic programming strategy described in
section 2.1 and appendix A.
The Tin interpreter is used in two different ways:
1. to study the effectiveness for chart parsing of known
parsing schemata designed for deterministic parsing.
We have only considered formally defined parsing
schemata, corresponding to established PDA construc-
tion techniques that we use to mechanically translate
CF grammars into Tin code. (e.g. LALR(1) and
LALR(2) [6], weak precedence [12], LL(0) top-down
(recursive descent), LR(0), LR(1) [1] ...).
2. to study the computational behavior of the generated
code, and the optimization techniques that could be
used on the lin code — and more generally chart
parser code — with respect to code size, execution
speed and better sharing in the parse forest.
Experimenting with several compilation schemata hal
shown that sophistication may have a negative effect on thi
efficiency of all-path parsingli . Sophisticated PDT construc-
tion techniques tend to multiply the number of special cases,
thereby increasing the code size of the chart parser. Some-
times it also prevents sharing of locally identical subcom-
putations because of differences in context analysis. Thie
in turn may result in lesser sharing in the parse forest and
sometimes longer computation, as in example SBBL in ap-
pendix C, but of course it does not change the set of parse-
trees encoded in the forest12. Experimentally, weak prece-
dence gives slightly better sharing than LALR(1) parsing
The latter is often viewed as more efficient, whereas it only
has a larger deterministic domain.
One essential guideline to achieve better sharing (and often
also reduced computation time) is to try to recognize ever3
grammar rule in only one place of the generated chart parse]
code, even at the cost of increasing non-determinism.
Thus simpler schemata such as precedence, LL(0) (and
probably LR(0)13) produce the best sharing. However, since
they correspond to a smaller deterministic domain within the
CF grammar realm, they may sometimes be computationally
less efficient because they produce a larger number of uselesE
items (i.e. edges) that correspond to dead-end computationai
paths.
Slight sophistication (e.g. LALR(1) used by Tomita iii
[31], or LR(1) ) may slightly improve computational per-
formance by detecting earlier dead-end computations. Tide
may however be at the expense of the forest sharing quality.
More sophistication (say LR(2)) is usually losing on both
accounts as explained earlier. The duplication of computa-
tional paths due to distinct context analysis overweights the
11 We mean here the sophistication of the CF parser construc.
tion technique rather than the sophistication of the language fea.
tures chosen to be used by this parser.
11 This negative behavior of some techniques originally intendec
to preserve determinism had been remarked and analyzed in a
special case by Bouckaert, Pirotte and Snelling [3]. However we
believe their result to be weaker than ours, since it seems to rely
on the fact that they directly interpret granunars rather than firsi
compile them. Hence each interpretive step include in some sense
compilation steps, which are more expensive when look-ahead
increased. Their paper presents several examples that run less ef
ficiently when look-ahead is increased. For all these examples, thil
behavior disappears in our compiled setting. However the gram.
Mar SBBL in appendix C shows a loss of efficiency with increasec
look-ahead that is due exclusively to loss of sharing caused by ir
relevant contextual distinctions. This effect is particularly visible
when parsing incomplete sentences (14
Efficiency loss with increased look-ahead is mainly due to state
splitting [6]. This should favor LALR techniques over LR ones.
13 Our results do not take into account a newly found optimize),
Lion of PDT interpretation that applies to all and only to bottom.
up PDTs. This should make simple bottom-up schemes compet.
itive for sharing quality, and even increase their computationa
efficiency. However it should not change qualitatively the rein
tive performances of bottom-up parsers, and may emphasize eve,
more the phenomenon that reduces efficiency when look-ahead in
creases.
</bodyText>
<figureCaption confidence="0.99998">
Figure 1: Bottom-up parse-tree
Figure 2: Top-down parse-tree
</figureCaption>
<page confidence="0.981239">
1&apos;46
</page>
<bodyText confidence="0.99982432">
benefits of early elimination of dead-end paths. But there
can be no absolute rule: if a grammar is &amp;quot;close&amp;quot; to the LR(2)
domain, an LR(2) schema is likely to give the best result for
most parsed sentences.
Sophisticated schemata correspond also to larger parsers,
which may be critical in some natural language applications
with very large grammars.
The choice of a parsing schema depends in fine on the
grammar used, on the corpus (or kind) of sentences to be an-
alyzed, and on a balance between computational and sharing
efficiency. It is best decided on an experimental basis with
a system such as ours. Furthermore, we do not believe that
any firm conclusion limited to CF grammars would be of
real practical usefulness. The real purpose of the work pre-
sented is to get a qualitative insight in phenomena which
are best exhibited in the simpler framework of CF parsing.
This insight should help us with more complex formalisms
(cf. section 5) for which the phenomena might be less easily
evidenced.
Note that the evidence gained contradicts the common be-
lief that parsing schemata with a large deterministic domain
(see for example the remarks on LR parsing in (31D are more
effective than simpler ones. Most experiments in this area
were based on incomparable implementations, while our uni-
form framework gives us a common theoretical yardstick.
</bodyText>
<equation confidence="0.999882025">
nt0 ::= ntl 0
nt1 ::= nt2 nt3
nt2 ::= $
nt3 ::= nt4 nt37
nt4 ::= nt5 2
nt4 ::= nt29 1
nt5 ::= nt6 nt21
nt6 ::= nt7 1
nt7 ::= nt8 nt11
nt8 ::= nt9 3
nt9 ::= nt10 nil
nt10 ::= n
ntll ::= nt12 nil
nt12 ::= nt13 7
nt13 ::= nt14 nt15
nt14 ::= v
nt15 ::= nt16 nil
nt16 ::= nt17 4
nt17 ::= nt18 nt19
nt18 ::= det
nt19 ::= nt20 nil
nt20 ::= n
nt21 ::= nt22 nil
nt22 ::= nt23 6
nt23 ::= nt24 nt25
nt24 ::= prep
nt25 ::= nt26 nil
nt26 ::= nt27 3
nt27 ::= nt28 nil
nt28 ::= n
nt29 ::= nt8 nt30
nt30 ::= nt31 nil
nt31 ::= nt32 7
nt32 ::= nt14 nt33
nt33 ::= nt34 nil
nt34 ::= nt35 5
nt35 ::= nt16 nt36
nt36 ::= nt22 nil
nt37 ::= nt38 nil
nt38 ::= $
</equation>
<figureCaption confidence="0.997811">
Figure 3: Grammar of parses of the input sentence
</figureCaption>
<sectionHeader confidence="0.992926" genericHeader="method">
4 A Simple Bottom-Up Example
</sectionHeader>
<bodyText confidence="0.999874666666667">
The following is a simple example based on a bottom-up
PDT generated by our LALR(1) compiler from the following
grammar taken from (34
</bodyText>
<listItem confidence="0.9998425">
(0) lax ::= $ (4) &apos;up : det n
(1) &apos;s ::= &apos;up &apos;vp (5) &apos;up : &apos;pp
(2) &apos;s ::= &apos;s &apos;pp (6) &apos;pp prep &apos;up
(3) &apos;up ::= n (7) &apos;vp ::= • &apos;up
</listItem>
<bodyText confidence="0.997412333333333">
Nonterminals are prefixed with a quote symbol The first
rule is used for initialization and handling of the delimiter
symbol $. The $ delimiters are implicit in the actual input
sentence.
The sample input is &amp;quot;(n v det n prep n)&amp;quot;. It figures
(for example) the sentence: &amp;quot;I see a man at hone&amp;quot;.
</bodyText>
<subsectionHeader confidence="0.996902">
4.1 Output grammar produced by the parser
</subsectionHeader>
<bodyText confidence="0.997982227272727">
The grammar of parses of the input sentence is given in fig-
ure 3.
The initial nonterminal is the left-hand side of the first
rule. For readability, the nonterminals have been given com-
puter generated names of the form ntx, where x is an integer.
All other symbols are terminal. Integer terminals correspond
to rule numbers of the input language grammar given above,
and the other terminals are symbols of the parsed language,
except for the special terminal &amp;quot;nil&amp;quot; which indicates the end
of the list of subconstituents of a sentence constituent, and
may also be read as the empty string e. Note the ambiguity
for nonterminal nt4.
It is possible to simplify this grammar to 7 rules without
losing the sharing of common subparses. However it would
no longer exhibit the structure that makes it readable as a
shared-forest (though this structure could be retrieved).
The two parses of the input sentence defined by this gram-
mar are: $ n3 v det n 4 7 1 prep n 3 6 2 $
$ n 3 v det a 4 prep n 3 6 5 7 1$
Here again the two $ symbols must be read as delimiters.
The &amp;quot;nil&amp;quot; symbols, no longer useful, have been omitted in
these two parses.
</bodyText>
<subsectionHeader confidence="0.923784">
4.2 Parse shared-forest constructed from that
grammar
</subsectionHeader>
<bodyText confidence="0.999367208333334">
To explain the structure of the shared forest, we first build a
graph from the grammar, as shown in figure 4. Each node
corresponds to one terminal or nonterminal of the grammar
in figure 3, and is labelled by it. The labels at the right
of small dashes are rule numbers from the parsed language
grammar (see beginning of section 4). The basic structure is
that of figure 1.
From this first graph, we can trivially derive the more tra-
ditional shared forest given in figure 5. Note that this simpli-
fied representation is not always adequate since it does not
allow partial sharing of their sons between two nodes. Each
node includes a label which is a non-terminal of the parsed
language grammar, and for each possible derivation (several
in case of ambiguity) there is the number of the grammar
rule used for that derivation. Though this simplified version
is more readable, the representation of figure 5 is not ade-
quate to represent partial sharing of the subconstituents of
a constituent.
Of course, the &amp;quot;constructions&amp;quot; given in this section are
purely virtual. In an implementation, the data-structure rep-
resenting the grammar of figure 3 may be directly interpreted
and used as a shared-forest.
A similar construction for top-down parsing is sketched in
appendix B.
</bodyText>
<page confidence="0.998523">
147
</page>
<sectionHeader confidence="0.998891" genericHeader="method">
5 Extensions
</sectionHeader>
<bodyText confidence="0.999974">
As indicated earlier, our intent is mostly to understand phe-
nomena that would be harder to evidence in more complex
grammatical formalisms.
This statement implies that our approach can be extended.
This is indeed the case. It is known that many simple parsing
schemata can be expressed with stack based machines [32].
This is certainly the case for all left-to-right CF chart parsing
schemata.
We have formally extended the concept of PDA into that
of Logical PDA which is an operational push-down stack de-
vice for parsing unification based grammars [17,18] or other
non-CF grammars such as Tree Adjoining Grammars [19].
Hence we are reusing and developing our theoretical [18] and
experimental [36] approach in this much more general set-
ting which is more likely to be effectively usable for natural
language parsing.
Furthermore, these extensions can also express, within the
PDA model, non-left-to-right behavior such as is used in is-
land parsing [38] or in Shell&apos;s approach [26]. More generally
they allow the formal analysis of agenda strategies, which
we have not considered here. In these extensions, the coun-
terpart of parse forests are proof forests of definite clause
programs.
</bodyText>
<sectionHeader confidence="0.998316" genericHeader="method">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999261541666666">
Analysis of all-path parsing schemata within a common
framework exhibits in comparable terms the properties of
these schemata, and gives objective criteria for chosing a
given schema when implementing a language analyzer. The
approach taken here supports both theoretical analysis and
actual experimentation, both for the computational behavior
of parsers and for the structure of the resulting shared forest.
Many experiments and extensions still remain to be made:
improved dynamic programming interpretation of bottom-
up parsers, more extensive experimental measurements with
a variety of languages and parsing schemata, or generaliza-
tion of this approach to more complex situations, such as
word lattice parsing [21,30], or even handling of &amp;quot;secondary&amp;quot;
language features. Early research in that latter direction is
promising: our framework and the corresponding paradigm
for parser construction have been extended to full first-order
Horn clauses [17,18], and are hence applicable to unification
based grammatical formalisms [27]. Shared forest construc-
tion and analysis can be generalized in the same way to these
more advanced formalisms.
Acknowledgements: We are grateful to Véronique
Donzeau-Gouge for many fruitful discussions.
This work has been partially supported by the Eureka
Software Factory (ESF) project.
</bodyText>
<sectionHeader confidence="0.994355" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.997893833333333">
[1] Aho, AN.; and Ullman, J.D. 1972 The Theory
of Parsing, Translation and Compiling. Prentice-
Hall, Englewood Cliffs, New Jersey.
[2] Billot, S. 1988 Analyseurs Syntazigues et Non-
Diterrninisme. These de Doctorat, Universite
d&apos;Orleans la Source, Orleans (France).
</reference>
<figure confidence="0.943750444444445">
41111111110
1PP&amp;quot;
NP VP NP
3 7 5
PP
6
n v
VP
7
</figure>
<figureCaption confidence="0.999614">
Figure 5: The shared forest
</figureCaption>
<page confidence="0.995522">
148
</page>
<reference confidence="0.999902471544715">
[3] Bouckaert, M.; Pirotte, A.; and Snelling, M. 1975
Efficient Parsing Algorithms for General Context-
Free Grammars. Information Sciences 8(1): 1-26
[4] Cocke, J.; and Schwartz, J.T. 1970 Programming
Languages and Their Compilers. Courant Insti-
tute of Mathematical Sciences, New York Univer-
sity, New York.
Coppersmith, D.; and Winograd, S. 1982 On the
Asymptotic Complexity of Matrix Multiplication.
SIAM Journal on Computing, 11(3): 472-492.
[6] DeRemer, F.L. 1971 Simple LR(k) Grammars.
Communications ACM 14(7): 453-460.
[7] Earley, J. 1970 An Efficient Context-Free Parsing
Algorithm. Communications ACM 13(2): 94-102.
[8] Futamura, Y. (ed.) 1988 Proceedings of the Work-
shop on Partial Evaluation and Mixed Computa-
tion. New Generation Computing 6(2,3).
Graham, S.L.; Harrison, M.A.; and Ruzzo W.L.
1980 An Improved Context-Free Recognizer. ACM
Transactions on Programming Languages and Sys-
tems 2(3): 415-462.
[10] Griffiths, I.; and Petrick, S. 1965 On the Relative
Efficiencies of Context-Free Grammar Recogniz-
ers. Communications ACM 8(5): 289-300.
[11] Hays, D.G. 1962 Automatic Language-Data Pro-
cessing. In Computer Applications in the Behav-
ioral Sciences, (H. Borko ed.), Prentice-Hall, pp.
394-423.
[12] Ichbiah, J.D.; and Morse, S.P. 1970 A Technique
for Generating Almost Optimal Floyd-Evans Pro-
ductions for Precedence Grammars. Communica-
tions ACM 13(8): 501-508.
[13] Kasami, J. 1965 An Efficient Recognition and
Syntax Analysis Algorithm for Context-Free Lan-
guages. Report of Univ. of Hawaii, also AFCRL-
65-758, Air Force Cambridge Research Labora-
tory, Bedford (Massachusetts), also 1966, Univer-
sity of Minois Coordinated Science Lab. Report,
No. R-257.
[14] Kay, M. 1980 Algorithm Schemata and Data
Structures in Syntactic Processing. Proceedings of
the Nobel Symposium on Text Processing, Gothen-
burg.
[15] Lang, B. 1974 Deterministic Techniques for Effi-
cient Non-deterministic Parsers. Proc. of the 2nd
Colloquium on Automata, Languages and Pro-
gramming, J. Loeckx (ed.), Saarbriicken, Springer
Lecture Notes in Computer Science 14: 255-269.
Also: Rapport de Recherche 72, IRIA-Laboria,
Rocquencourt (France).
[16] Lang, B. 1988 Parsing Incomplete Sentences. Proc.
of the 12th Internat. Conf. on Computational Lin-
guistics (COLING&apos;88) VoL 1 :365-371, D. Vargha
(ed.), Budapest (Hungary).
[17] Lang, B. 1988 Datalog Automata. Proc. of the
3rd Internat. Conf. on Data and Knowledge Bases,
C. Been, J.W. Schmidt, U. Dayal (eds.), Morgan
Kaufmann Pub., pp. 389-404, Jerusalem (Israel).
[18] Lang, B. 1988 Complete Evaluation of Horn
Clauses, an Automata Theoretic Approach. INRIA
Research Report 913.
[19] Lang, B. 1988 The Systematic Construction of
Earley Parsers: Application to the Production of
0(0) Earley Parsers for Tree Adjoining Gram-
mars. In preparation.
[20] Li, T.; and Chun, H.W. 1987 A Massively Paral-
lel Network-Based Natural Language Parsing Sys-
tem. Proc. of 2nd Int. Conf. on Computers and
Applications Beijing (Peking),: 401-408.
[21] Nakagawa, S. 1987 Spoken Sentence Recogni-
tion by Time-Synchronous Parsing Algorithm of
Context-Free Grammar. Proc. ICASSP 87, Dallas
(Texas), Vol. 2 : 829-832.
[22] Pereira, F.C.N.; and Warren, D.H.D. 1980 Defi-
nite Clause Grammars for Language Analysis —
Asurvey of the Formalism and a Comparison with
Augmented Transition Networks. Artificial Intel-
ligence 13: 231-278.
[23] Phillips, J.D. 1986 A Simple Efficient Parser for
Phrase-Structure Grammars. Quarterly Newslet-
ter of the Soc. for the Study of Artificial Intelli-
gence (AISBQ) 59: 14-19.
[24] Pratt, V.R. 1975 LINGOL — A Progress Report.
In Proceedings of the 4th IJCAL 422-428.
[25] Rekers, J. 1987 A Parser Generator for Finitely
Ambiguous Context-Free Grammars. Report CS-
R8712, Computer Sdence/Dpt. of Software Tech-
nology, Centrum voor Wiskunde en Informatica,
Amsterdam (The Netherlands).
[26] Sheil, B.A. 1976 Observations on Context Free
Parsing. in Statistical Methods in Linguistics: 71-
109, Stockholm (Sweden), Proc. of Internat. Conf.
on Computational Linguistics (COLING-76), Ot-
tawa (Canada).
Also: Technical Report TR. 12-76, Center for Re-
search in Computing Technology, Aiken Computa-
tion Laboratory, Harvard Univ., Cambridge (Mas-
sachusetts).
[27] Shieber, S.M. 1984 The Design of a Computer
Language for Linguistic Information. Proc. of the
10th Internat. Conf. on Computational Linguistics
— COLING&apos;84: 362-366, Stanford (California).
[28] Shieber, S.M. 1985 Using Restriction to Extend
Parsing Algorithms for Complex-Feature-Based
Formalisms. Proceedings of the 23rd Annual Meet-
ing of the Association for Computational Linguis-
tics: 145-152.
[29] Thompson, H. 1983 MCHART: A Flexible, Mod-
ular Chart Parsing System. Proc. of the National
ConL on Artificial Intelligence (AAAI-83), Wash-
ington (D.C.), pp. 408-410.
[30] Tomita, M. 1986 An Efficient Word Lattice Pars-
ing Algorithm for Continuous Speech Recognition.
In Proceedings of IEEE-IECE-ASJ International
Conference on Acoustics, Speech, and Signal Pro-
cessing (ICASSP 86), Vol. 3: 1569-1572.
[31] Tomita, M. 1987 An Efficient Augmented-
Context-Free Parsing Algorithm. Computational
Linguistics 13(1-2): 31-46.
[32] Tomita, M. 1988 Graph-structured Stack and Nat-
ural Language Parsing. Proceedings of the 26&amp;quot;
Annual Meeting of the Association for Computa-
tional Linguistics: 249-257.
</reference>
<figure confidence="0.985724">
(5]
[9]
</figure>
<page confidence="0.419299">
114 9
</page>
<reference confidence="0.999001615384615">
[33] Uehara, K.; Ochitani, R.; Kakusho, O.; Toyoda,
J. 1984 A Bottom-Up Parser based on Predicate
Logic: A Survey of the Formalism and its Im-
plementation Technique. 1984 Internat. Symp. on
Logic Programming, Atlantic City (New Jersey),:
220-227.
(34] U.S. Department of Defense 1983 Reference
Manual for the Ada Programming Language.
ANSI/MIL-STD-1815 A.
(35] Valiant, L.G. 1975 General Context-Free Recog-
nition in Less than Cubic Time. Journal of Com-
puter and System Sciences, 10: 308-315.
[36] Villemonte de in Clergerie, E.; and Zanchetta, A.
1988 Evaluateur de Clauses de Horn. Rapport de
Stage d&apos;Option, Ecole Polytechniqne, Palaiseau
(France).
[37] Wirth, N. 1971 The Programming Language Pas-
cal. Acta Inforrnatica, 1(1).
(38] Ward, W.H.; Hauptmann, A.G.; Stern, R.M.; and
Chanak, T. 1988 Parsing Spoken Phrases Despite
Missing Words. In Proceedings of the 1988 In-
ternational Conference on Acoustics, Speech, and
Signal Processing (ICASSP 88), Vol. 1: 275-278.
(39] Younger, D.H. 1967 Recognition and Parsing of
Context-Free Languages in Time n3. information
and Control, 10(2): 189-208
</reference>
<sectionHeader confidence="0.744146" genericHeader="method">
A The algorithm
</sectionHeader>
<bodyText confidence="0.95272">
This is the formal description of a minimal dynamic pro-
gramming PDT interpreter. The actual Tin interpreter has
a larger instruction set. Comments are prefixed with &amp;quot;—&amp;quot;.
— Begin parse with input sentence x of length n
</bodyText>
<equation confidence="0.403080714285714">
step-A: — Initialization — initial item
o ((4; 0) (4; o)); — first rule of output grammar
;e); — initialize item-set So
So := fih; — rules of output grammar
P := — input-scanner index is set
i := 0; — before the first input symbol
step-B: — Iteration
</equation>
<bodyText confidence="0.828828714285714">
while i &lt; n loop
for every item U = ((p A 0 (q B j)) in Si do
for every transition r in 8 do
— we consider four kinds of transitions, corresponding
— to the instructions of a minimal PDT interpreter.
if r = (pe e re z) then — OUTPUT
V := ((r A i) (4:11 j));
</bodyText>
<figure confidence="0.475770166666667">
• := U {V} ;
P := U {(V U z)} ;
if r = (pe e ■-• r e) then — PUSH C
✓ := ((r C i) (p A 0);
• := U {V);
P :=
</figure>
<bodyText confidence="0.824846">
if r = (p A c 1-+ r c c) then — POP A
for every item Y = ((q B j) (s D k)) in Si do
✓ := ((r B (s D k));
</bodyText>
<listItem confidence="0.528468">
• := u {V} ;
</listItem>
<equation confidence="0.882667375">
P := P u {(V &apos;YU)} ;
if r = (p e a t-s re e) then
✓ := ((r A i-1-1)(qBj));
• := Si4.1 u {V} ;
P :=
i := i+1;
end loop;
step-C: — Termination
</equation>
<bodyText confidence="0.9877775">
for every item U = n)(q$ 0)) in 8n
such that f E F do
</bodyText>
<equation confidence="0.36235">
P :=
</equation>
<bodyText confidence="0.9782955">
Uf is the initial nonterminal of g.
— End of parse
</bodyText>
<subsectionHeader confidence="0.963013">
B Interpretation of a top-down PDT
</subsectionHeader>
<bodyText confidence="0.9999755">
To illustrate the creation of the shared forest, we present
here informally a simplified sequence of transitions in their
order of execution by a top-down parser. We indicate the
transitions as Tin instructions on the left, as defined in ap-
pendix A. On the right we indicate the item and the rule
produced by execution of each instruction: the item is the
left-hand-side of the rule.
The pseudo-instruction scan is given in italics because
it does not exist, and stands for the parsing of a sub-
constituent: either several transitions for a complex con-
stituent or a single shift instruction for a lexical constituent.
The global behavior of scan is the same as that of shift, and
it may be understood as a shift on the whole sub-constituent.
Items are represented by a pair of integer. Hence we give
no details about states or input, but keep just enough infor-
mation to see how items are inter-related when applying a
pop transition: it must use two items of the form (a ,b) and
(b,c) as indicated by the algorithm.
The symbol r stands for the rule used to recognize a con-
stituent s, and ri stands for the rule used to recognize its th
sub-constituent S. The whole sequence, minus the first and
the last two instructions, would be equivalent to &amp;quot;scan s&amp;quot;.
</bodyText>
<table confidence="0.98866875">
• • • (6,5)
push r (7,6) -&gt;
push ri (8,7) -&gt;
scan si (9,7) -&gt; (8,7) Si
out 2.1 (10,7) -&gt; (9,7) ri
pop (11.6) -&gt; (7.6) (10,7)
push r2 (12,11) -&gt;
scan sz (13,11) -&gt; (12,11) s2
out T2 (14,11) -&gt; (13,11) T2
pop (15,6) -&gt; (11.6) (14,11)
push T3 (16,15) -&gt;
scan 83 (17.15) -&gt; (16,15) 33
out r3 (18,15) -&gt; (17,15) 1.3
pop (19,6) -&gt; (15,6) (18.15)
out r (20,6) -&gt; (19,6) r
pop (21,5) -&gt; (6,5)(20,6)
</table>
<bodyText confidence="0.946987">
This grammar may be simplified by eliminating useless
non-terminals, deriving on the empty string c or on a single
other non-terminal. As in section 4, the simplified grammar
may then be represented as a graph which is similar, with
more details (the rules used for the subconstituents), to the
graph given in figure 2.
— SHIFT a
</bodyText>
<page confidence="0.996933">
150
</page>
<sectionHeader confidence="0.993997" genericHeader="method">
C Experimental Comparisons
</sectionHeader>
<bodyText confidence="0.999571352941176">
This appendix gives some of the experimental data gathered to
compare compilation schemata.
For each grammar, the first table gives the size of the PDTs ob-
tained by compiling it according to several compilation schemata.
This size corresponds to the number of instructions generated for
the PDT, which is roughly the number of possible PDT states.
The second table gives two figures for each schema and for
several input sentences. The first figure is the number of items
computed to parse that sentence with the given schema: it may
be read as the number of computation steps and is thus a measure
of computational efficiency. The second figure is the number of
items remaining after simplification of the output grammar: it is
thus an indicator of sharing quality. Sharing is better when this
second figure is low.
In these tables, columns headed with LR/LALR stands for the
LR(0), .LR(1), LALR(1) and LALR(2) cases (which often give the
same results), unless one of these cases has its own explicit column.
Tests were run on the GRE, NSE, UBDA and RR grammars
of [3]: they did not exhibit the loss of efficiency with increased
look-ahead that was reported for the bottom-up look-ahead of [3].
We believe the results presented here are consistent and give
an accurate comparison of performances of the parsers considered,
despite some implementation departure from the strict theoretical
model required by performance considerations. A first version of
our LL(0) compiler gave results that were inconsistent with the
results of the bottom-up parsers. This was a clue to a weakness in
that LL(0) compiler which was then corrected. We consider this
experience to be a confirmation of the usefulness of our uniform
framework.
It must be stressed that these are preliminary experiments. On
the basis of their. analysis, we intend a new set of experiments
that will better exhibit the phenomena discussed in the paper. In
particular we wish to study variants of the schemata and dynamic
programming interpretation that give the best possible sharing.
</bodyText>
<sectionHeader confidence="0.9458695" genericHeader="method">
C.1 Grammar UBDA
C.2 Grammar RR
</sectionHeader>
<subsectionHeader confidence="0.642129">
1::•SAIx
</subsectionHeader>
<bodyText confidence="0.659532">
This grammar is LALR(1) but not LR(0), which explains the
lower performance of the LR(0 parser.
LR(0) LR(1) LALR(1) LALR(2) preced. -
LL(0)
34 37 37 37 48 46
input string LR(0) LR/LALR pieced. LL(0)
x 14-9 14-9 15-9 28-9
xx 23 - 13 20 - 13 25 - 13 43 - 13
LEEK= 99 - 29 44 - 29 56 - 29 123 - 29
</bodyText>
<note confidence="0.4524612">
C.3 Picogrammar of English
S NP VP I S PP
IP ::• n I det a I NP PP
VP ::= v IP
PP ::s prep IP
</note>
<subsectionHeader confidence="0.762486">
C.4 Grammar of Ada expressions
</subsectionHeader>
<bodyText confidence="0.9985804">
This grammar, too long for inclusion here, is the grammar of ex-
pressions of the language Ada, as given in the reference man-
ual [34]. This grammar is ambiguous.
In these examples, the use of look-ahead give approximately a
25% gain in speed efficiency over LR(0) parsing, with the same
forest sharing.
However the use of look-ahead may increase the LR(1) parser
size quadratically with the grammar size. Still, a better engineered
LR(1) construction should not usually increase that size as dra-
matically as indicated by our experimental figure.
</bodyText>
<figure confidence="0.977826615384615">
LR(0) LR(1) LALR(1) preced.
587 32210 534 323
input string LR(0) LR(1) LALR(1) preced.
03 74 - 39 59- 39 59 - 39 80 - 39
(a*3)+b 137 - 75 113 - 75 113 - 75 293 - 75
a*3-1-b**4 169 - 81 122 - 81 122 - 81 227 - 81
1
C.5 Grammar PB
::naldlaBc I bAc IbBd
A : :is •
B : :SI •
LR(0) LR(1) LALR(1) &amp; (2) preced. LL(0)
76 100 80 84 122
</figure>
<bodyText confidence="0.773596666666667">
This grammar is LR(1) but is not LALR. For each compilation
schema, it gives the same result on all possible inputs: eked, aec,
bet and bed.
</bodyText>
<figure confidence="0.831678153846154">
LR(0) LR(1) LALR(1) &amp; (2) preced. LL(0)
26- 15 23- 15 26- 15 29- 15 47 - 15
C.6 Grammar SBBL
E::1•XldIXBcIYAcIYBd
X :vs
Y
A::■••AIg
B::••AIg
LR(0) LR(1) LALR( 1 ) LA LR(2 ) preced.
159 294 158 158 104
input string , LR(0) LR 1) LALR(1) &amp; (2) preced.
f•gd 50-21 57 - 37 50 - 21 84 - 36
tesegd 62 - 29 75 - 49 62. 29 110 - 44
</figure>
<bodyText confidence="0.8565355">
The terminal f may be ambiguously parsed as X or as Y. This
ambiguous left context increases uselessly the complexity of the
LR(1) parses during recognition of the A and B constituents. Hence
LR(0) performs better in this case since it ignores the context.
</bodyText>
<figure confidence="0.982554785714285">
LR(0) LR(1) LALR(1) LALR(2) preced. LL(0)
38 60 41 41 38 46
input string LR/LALR preced. LL(0)
a 14-9 15 - 9 41 - 9
as 23-15 29- 15 75 - 15
aaaaaa 249-156 226 - 124 391 - 112
LR(0) LR(1) LALR(1) LALR(2) preced. LL(0)
110 341 104 104 90 116
input string LR/LALR preced. LL(0)
• a prep n 71 -47 72 - 47 169 - 43
• a (prep n)2 146 - 97 141 - 93 260 - 77
n • a (prep a)3 260 - 172 245 - 161 371 - 122
a • a (prep a)6 854 - 541 775. 491 844 - 317
1 5 1
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.173489">
<title confidence="0.997147">The Structure of Shared Forests in Ambiguous Parsing</title>
<author confidence="0.828902">Sylvie Billott Bernard Lang</author>
<affiliation confidence="0.65218">INRIA &apos;and Universite d&apos;Orleans</affiliation>
<email confidence="0.324539">billotOinria.inria.frlangOinria.inria.fr</email>
<abstract confidence="0.99976775">The Context-Free backbone of some natural language analyzers produces all possible CF parses as some kind of shared forest, from which a single tree is to be chosen by a disambiguation process that may be based on the finer features of the language. We study the structure of these forests with respect to optimality of sharing, and in relation with the parsing schema used to produce them. In addition to a theoretical and experimental framework for studying these issues, the main results presented are: sophistication in chart parsing schemata (e.g. use of look-ahead) may reduce time and space efficiency instead of improving it, there is a shared forest structure with at most cubic size for any CF grammar, when complexity is required, the shape of a shared forest is dependent on the parsing schema used. Though analyzed on CF grammars for simplicity, these results extend to more complex formalisms such as unification based grammars.</abstract>
<keyword confidence="0.9260945">words: Parsing, Ambiguity, Dynamic Programming, Earley Parsing, Chart Parsing, Parsing</keyword>
<address confidence="0.760336">Strategies, Parsing Schemata, Parse Tree, Parse Forest.</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>AN Aho</author>
<author>J D Ullman</author>
</authors>
<title>The Theory of Parsing, Translation and Compiling. PrenticeHall, Englewood Cliffs,</title>
<date>1972</date>
<location>New Jersey.</location>
<contexts>
<context position="2034" citStr="[1,7]" startWordPosition="324" endWordPosition="324">d sentence, before it is handed to a more elaborate analyzer (possibly a coroutine), that takes into account the finer grammatical structure to filter out undesirable parses (see for example [24,28]). In [28], Shieber surveys existing variants to this approach before giving his own tunable approach based on restrictions that &amp;quot;split up the infinite nonterminal domain into a finite set of equivalence classes that can be used for parsing&amp;quot;. The basic motivation for this approach is to benefit from the CF parsing technology whose development over 30 years has lead to powerful and efficient parsers [1,7]. A parser that takes into account only an approximation of the grammatical features will often find ambiguities it cannot resolve in the analyzed sentences&apos;. A natural solution &apos;Address: INRIA, B.P. 105, 78153 Le Chesnay, France. The work reported here was partially supported by the Eureka Software Factory project. I Ambiguity may also have a semantical origin. is then to produce all possible parses, according to the CF backbone, and then select among them on the basis of the complete features information. One hitch is that the number of parses may be exponential in the size of the input sent</context>
<context position="10354" citStr="[1]" startWordPosition="1736" endWordPosition="1736">ext to be shifted and z E E*, and u is the already produced output. The application of a transition r = (p A a q B v) results in a new configuration p&apos; = (q Ba z uv) where the terminal symbol a has been scanned (i.e. shifted), A has been popped and B has been pushed, and v has been concatenated to the existing output u. If the terminal symbol a is replaced by e in the transition, no input symbol is scanned. If A (resp. B) is replaced bye then no stack symbol is popped from (resp. pushed on) the stack. Our algorithm consists in an Earley-like&apos; simulation of the PDT TG. Using the terminology of [1], the algorithm builds an item set Si successively for each word symbol x, holding position i in the input sentence z. An item is constituted of two modes of the form (p A i) where p is a PDT state, A is a stack symbol, and Os the index of an input symbol. The item set Si contains items of the form ((p A 1) (q B j)) . These items are used as nonterminals of an output grammar 6 The original intent of (15] was to show how one can generate efficient general CF chart parsers, by first producing the PDT with the efficient techniques for deterministic parsing developed for the compiler technology (6</context>
<context position="19983" citStr="[1]" startWordPosition="3339" endWordPosition="3339">tion of the virtual machine code is done by an all-paths interpreter that follows the dynamic programming strategy described in section 2.1 and appendix A. The Tin interpreter is used in two different ways: 1. to study the effectiveness for chart parsing of known parsing schemata designed for deterministic parsing. We have only considered formally defined parsing schemata, corresponding to established PDA construction techniques that we use to mechanically translate CF grammars into Tin code. (e.g. LALR(1) and LALR(2) [6], weak precedence [12], LL(0) top-down (recursive descent), LR(0), LR(1) [1] ...). 2. to study the computational behavior of the generated code, and the optimization techniques that could be used on the lin code — and more generally chart parser code — with respect to code size, execution speed and better sharing in the parse forest. Experimenting with several compilation schemata hal shown that sophistication may have a negative effect on thi efficiency of all-path parsingli . Sophisticated PDT construction techniques tend to multiply the number of special cases, thereby increasing the code size of the chart parser. Sometimes it also prevents sharing of locally ident</context>
</contexts>
<marker>[1]</marker>
<rawString>Aho, AN.; and Ullman, J.D. 1972 The Theory of Parsing, Translation and Compiling. PrenticeHall, Englewood Cliffs, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Billot</author>
</authors>
<title>Analyseurs Syntazigues et NonDiterrninisme. These de Doctorat, Universite d&apos;Orleans la Source,</title>
<date>1988</date>
<location>Orleans</location>
<marker>[2]</marker>
<rawString>Billot, S. 1988 Analyseurs Syntazigues et NonDiterrninisme. These de Doctorat, Universite d&apos;Orleans la Source, Orleans (France).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Bouckaert</author>
<author>A Pirotte</author>
<author>M Snelling</author>
</authors>
<title>Efficient Parsing Algorithms for General ContextFree Grammars.</title>
<date>1975</date>
<journal>Information Sciences</journal>
<volume>8</volume>
<issue>1</issue>
<pages>1--26</pages>
<contexts>
<context position="3269" citStr="[11,39,13,3,9]" startWordPosition="523" endWordPosition="523">e for cyclic grammars or incomplete sentences [16]. However chart parsing techniques have been developed that produce an encoding of all possible parses as a data structure with a size polynomial in the length of the input sentence. These techniques are all based on a dynamic programming paradigm. The kind of structure they produce to represent all parses of the analyzed sentence is an essential characteristic of these algorithms. Some of the published algorithms produce only a chart as described by Kay in [14], which only associates nonterminal categories to segments of the analyzed sentence [11,39,13,3,9], and which thus still requires non-trivial processing to extract parse-trees [26]. The worst size complexity of such a chart is only a square function of the size of the input2. However, practical parsing algorithms will often produce a more complex structure that explicitly relates the instances of nonterminals associated with sentence fragments to their constituents, possibly in several ways in case of ambiguity, with a sharing of some common subtrees between the distinct ambiguous parses [7,4,24,31,25]3 One advantage of this structure is that the chart retains only these constituents that </context>
<context position="22385" citStr="[3]" startWordPosition="3718" endWordPosition="3718"> computations. Tide may however be at the expense of the forest sharing quality. More sophistication (say LR(2)) is usually losing on both accounts as explained earlier. The duplication of computational paths due to distinct context analysis overweights the 11 We mean here the sophistication of the CF parser construc. tion technique rather than the sophistication of the language fea. tures chosen to be used by this parser. 11 This negative behavior of some techniques originally intendec to preserve determinism had been remarked and analyzed in a special case by Bouckaert, Pirotte and Snelling [3]. However we believe their result to be weaker than ours, since it seems to rely on the fact that they directly interpret granunars rather than firsi compile them. Hence each interpretive step include in some sense compilation steps, which are more expensive when look-ahead increased. Their paper presents several examples that run less ef ficiently when look-ahead is increased. For all these examples, thil behavior disappears in our compiled setting. However the gram. Mar SBBL in appendix C shows a loss of efficiency with increasec look-ahead that is due exclusively to loss of sharing caused b</context>
</contexts>
<marker>[3]</marker>
<rawString>Bouckaert, M.; Pirotte, A.; and Snelling, M. 1975 Efficient Parsing Algorithms for General ContextFree Grammars. Information Sciences 8(1): 1-26</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cocke</author>
<author>J T Schwartz</author>
</authors>
<title>Programming Languages and Their Compilers. Courant Institute of Mathematical Sciences,</title>
<date>1970</date>
<journal>SIAM Journal on Computing,</journal>
<volume>11</volume>
<issue>3</issue>
<pages>472--492</pages>
<location>New York University, New</location>
<contexts>
<context position="3780" citStr="[7,4,24,31,25]" startWordPosition="601" endWordPosition="601"> [14], which only associates nonterminal categories to segments of the analyzed sentence [11,39,13,3,9], and which thus still requires non-trivial processing to extract parse-trees [26]. The worst size complexity of such a chart is only a square function of the size of the input2. However, practical parsing algorithms will often produce a more complex structure that explicitly relates the instances of nonterminals associated with sentence fragments to their constituents, possibly in several ways in case of ambiguity, with a sharing of some common subtrees between the distinct ambiguous parses [7,4,24,31,25]3 One advantage of this structure is that the chart retains only these constituents that can actually participate in a parse. Furthermore it makes the extraction of parse-trees a trivial matter. A drawback is that this structure may be cubic in the length of the parsed sentence, and more generally polynomial4 for some proposed algorithms [31]. However, these algorithms are rather well behaved in practice, and this complexity is not a problem. In this paper we shall call shared forests such data struc2 We do not consider CF recognizers that have asymptotically the lowest complexity, but are onl</context>
</contexts>
<marker>[4]</marker>
<rawString>Cocke, J.; and Schwartz, J.T. 1970 Programming Languages and Their Compilers. Courant Institute of Mathematical Sciences, New York University, New York. Coppersmith, D.; and Winograd, S. 1982 On the Asymptotic Complexity of Matrix Multiplication. SIAM Journal on Computing, 11(3): 472-492.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F L DeRemer</author>
</authors>
<title>Simple LR(k) Grammars.</title>
<date>1971</date>
<journal>Communications ACM</journal>
<volume>14</volume>
<issue>7</issue>
<pages>453--460</pages>
<contexts>
<context position="11054" citStr="[6,1]" startWordPosition="1867" endWordPosition="1867">n the input sentence z. An item is constituted of two modes of the form (p A i) where p is a PDT state, A is a stack symbol, and Os the index of an input symbol. The item set Si contains items of the form ((p A 1) (q B j)) . These items are used as nonterminals of an output grammar 6 The original intent of (15] was to show how one can generate efficient general CF chart parsers, by first producing the PDT with the efficient techniques for deterministic parsing developed for the compiler technology (6,12,1]. This idea was later successfully used by Ton:tits [31] who applied it to LR(1) parsers [6,1], and later to other pushdown based parsers [32]. 7 Implementations usually denote these rules by their index in the set 11. 8 Actual implementations use output symbols from nuE, since rules alone do not distinguish words in the same lexical category. 9 We assume the reader to be familiar with some variation of Earley&apos;s algorithm. Earley&apos;s original paper uses the word state (from dynamic programming terminology) instead of item. g = (S, fl, P, Ui), where $ is the set of all items (i.e. the union of Si), and the rules in P are constructed together with their left-hand-side item by the algorithm</context>
<context position="19907" citStr="[6]" startWordPosition="3329" endWordPosition="3329">d whose address in the virtual machine code is then specified. However execution of the virtual machine code is done by an all-paths interpreter that follows the dynamic programming strategy described in section 2.1 and appendix A. The Tin interpreter is used in two different ways: 1. to study the effectiveness for chart parsing of known parsing schemata designed for deterministic parsing. We have only considered formally defined parsing schemata, corresponding to established PDA construction techniques that we use to mechanically translate CF grammars into Tin code. (e.g. LALR(1) and LALR(2) [6], weak precedence [12], LL(0) top-down (recursive descent), LR(0), LR(1) [1] ...). 2. to study the computational behavior of the generated code, and the optimization techniques that could be used on the lin code — and more generally chart parser code — with respect to code size, execution speed and better sharing in the parse forest. Experimenting with several compilation schemata hal shown that sophistication may have a negative effect on thi efficiency of all-path parsingli . Sophisticated PDT construction techniques tend to multiply the number of special cases, thereby increasing the code s</context>
<context position="23176" citStr="[6]" startWordPosition="3840" endWordPosition="3840">step include in some sense compilation steps, which are more expensive when look-ahead increased. Their paper presents several examples that run less ef ficiently when look-ahead is increased. For all these examples, thil behavior disappears in our compiled setting. However the gram. Mar SBBL in appendix C shows a loss of efficiency with increasec look-ahead that is due exclusively to loss of sharing caused by ir relevant contextual distinctions. This effect is particularly visible when parsing incomplete sentences (14 Efficiency loss with increased look-ahead is mainly due to state splitting [6]. This should favor LALR techniques over LR ones. 13 Our results do not take into account a newly found optimize), Lion of PDT interpretation that applies to all and only to bottom. up PDTs. This should make simple bottom-up schemes compet. itive for sharing quality, and even increase their computationa efficiency. However it should not change qualitatively the rein tive performances of bottom-up parsers, and may emphasize eve, more the phenomenon that reduces efficiency when look-ahead in creases. Figure 1: Bottom-up parse-tree Figure 2: Top-down parse-tree 1&apos;46 benefits of early elimination </context>
</contexts>
<marker>[6]</marker>
<rawString>DeRemer, F.L. 1971 Simple LR(k) Grammars. Communications ACM 14(7): 453-460.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Earley</author>
</authors>
<title>An Efficient Context-Free Parsing Algorithm.</title>
<date>1970</date>
<journal>Communications ACM</journal>
<volume>13</volume>
<issue>2</issue>
<pages>94--102</pages>
<contexts>
<context position="2034" citStr="[1,7]" startWordPosition="324" endWordPosition="324">d sentence, before it is handed to a more elaborate analyzer (possibly a coroutine), that takes into account the finer grammatical structure to filter out undesirable parses (see for example [24,28]). In [28], Shieber surveys existing variants to this approach before giving his own tunable approach based on restrictions that &amp;quot;split up the infinite nonterminal domain into a finite set of equivalence classes that can be used for parsing&amp;quot;. The basic motivation for this approach is to benefit from the CF parsing technology whose development over 30 years has lead to powerful and efficient parsers [1,7]. A parser that takes into account only an approximation of the grammatical features will often find ambiguities it cannot resolve in the analyzed sentences&apos;. A natural solution &apos;Address: INRIA, B.P. 105, 78153 Le Chesnay, France. The work reported here was partially supported by the Eureka Software Factory project. I Ambiguity may also have a semantical origin. is then to produce all possible parses, according to the CF backbone, and then select among them on the basis of the complete features information. One hitch is that the number of parses may be exponential in the size of the input sent</context>
<context position="3780" citStr="[7,4,24,31,25]" startWordPosition="601" endWordPosition="601"> [14], which only associates nonterminal categories to segments of the analyzed sentence [11,39,13,3,9], and which thus still requires non-trivial processing to extract parse-trees [26]. The worst size complexity of such a chart is only a square function of the size of the input2. However, practical parsing algorithms will often produce a more complex structure that explicitly relates the instances of nonterminals associated with sentence fragments to their constituents, possibly in several ways in case of ambiguity, with a sharing of some common subtrees between the distinct ambiguous parses [7,4,24,31,25]3 One advantage of this structure is that the chart retains only these constituents that can actually participate in a parse. Furthermore it makes the extraction of parse-trees a trivial matter. A drawback is that this structure may be cubic in the length of the parsed sentence, and more generally polynomial4 for some proposed algorithms [31]. However, these algorithms are rather well behaved in practice, and this complexity is not a problem. In this paper we shall call shared forests such data struc2 We do not consider CF recognizers that have asymptotically the lowest complexity, but are onl</context>
</contexts>
<marker>[7]</marker>
<rawString>Earley, J. 1970 An Efficient Context-Free Parsing Algorithm. Communications ACM 13(2): 94-102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Futamura</author>
</authors>
<title>An Improved Context-Free Recognizer.</title>
<date>1988</date>
<journal>ACM Transactions on Programming Languages and Systems</journal>
<booktitle>Proceedings of the Workshop on Partial Evaluation and Mixed Computation. New Generation Computing 6(2,3).</booktitle>
<volume>2</volume>
<issue>3</issue>
<pages>415--462</pages>
<editor>Graham, S.L.; Harrison, M.A.; and Ruzzo</editor>
<contexts>
<context position="14960" citStr="[8]" startWordPosition="2499" endWordPosition="2499">d awkward to manipulate than the well understood, specialized and few concepts of CF grammars. Furthermore, unlike graphs, this grammar formalization of the shared forest may be tractably extended to other grammatical formalisms (cf. section 5). More importantly, our work on the parsing of incomplete sentences [16] has exhibited the fundamental character of our grammatical view of shared forests: when parsing the completely unknown sentence, the shared forest obtained is precisely the complete grammar of the analyzed language. This also leads to connections with the work on partial evaluation [8]. 2.3 The shape of the forest For our shared-forest, a cubic space complexity (in the worst case — space complexity is often linear in practice) is achieved, without requiring that the language grammar be in Chomsky Normal Form, by producing a grammar of parses that has at most two symbols on the right-hand side of its rules. This amounts to representing the list of sons of a parse tree node as a Lisp-like list built with binary nodes (see figures 1 Sz 2), and it allows partial sharing of the sons The structure of the parse grammar, i.e. the shape of the parse forest, is tightly related to the</context>
</contexts>
<marker>[8]</marker>
<rawString>Futamura, Y. (ed.) 1988 Proceedings of the Workshop on Partial Evaluation and Mixed Computation. New Generation Computing 6(2,3). Graham, S.L.; Harrison, M.A.; and Ruzzo W.L. 1980 An Improved Context-Free Recognizer. ACM Transactions on Programming Languages and Systems 2(3): 415-462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Griffiths</author>
<author>S Petrick</author>
</authors>
<title>On the Relative Efficiencies of Context-Free Grammar Recognizers.</title>
<date>1965</date>
<journal>Communications ACM</journal>
<volume>8</volume>
<issue>5</issue>
<pages>289--300</pages>
<contexts>
<context position="7418" citStr="[10]" startWordPosition="1189" endWordPosition="1189"> our implementation which illustrates both the working of the system and the underlying theory. 2 A Uniform Framework To discuss the above issues in a uniform way, we need a general framework that encompasses all forms of chart parsing and shared forest building in a unique formalism. We shall take as a baths a formalism developed by the second author in previous papers [15,16]. The idea of this approach is to separate the dynamic programming constructs needed for efficient chart parsing from the chosen parsing schema. Comparison between the classifications of Kay [14] and Griffith dc Petrick [10] shows that a parsing schema (or parsing strategy) may be expressed in the construction of a Push-Down Transducer (PDT), a well studied formalization of left-toright CF parsers&apos;. These PDTs are usually non-deterministic and cannot be used as produced for actual parsing. Their backtrack simulation does not always terminate, and is often time-exponential when it does, while breadth-first simulation is usually exponential for both time and space. However, by extending Earley&apos;s dynamic programming construction to PDTs, Lang provided in(15] a way of simulating all possible computations of any PDT i</context>
</contexts>
<marker>[10]</marker>
<rawString>Griffiths, I.; and Petrick, S. 1965 On the Relative Efficiencies of Context-Free Grammar Recognizers. Communications ACM 8(5): 289-300.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D G Hays</author>
</authors>
<title>Automatic Language-Data Processing.</title>
<date>1962</date>
<booktitle>In Computer Applications in the Behavioral Sciences,</booktitle>
<pages>394--423</pages>
<editor>(H. Borko ed.), Prentice-Hall,</editor>
<contexts>
<context position="3269" citStr="[11,39,13,3,9]" startWordPosition="523" endWordPosition="523">e for cyclic grammars or incomplete sentences [16]. However chart parsing techniques have been developed that produce an encoding of all possible parses as a data structure with a size polynomial in the length of the input sentence. These techniques are all based on a dynamic programming paradigm. The kind of structure they produce to represent all parses of the analyzed sentence is an essential characteristic of these algorithms. Some of the published algorithms produce only a chart as described by Kay in [14], which only associates nonterminal categories to segments of the analyzed sentence [11,39,13,3,9], and which thus still requires non-trivial processing to extract parse-trees [26]. The worst size complexity of such a chart is only a square function of the size of the input2. However, practical parsing algorithms will often produce a more complex structure that explicitly relates the instances of nonterminals associated with sentence fragments to their constituents, possibly in several ways in case of ambiguity, with a sharing of some common subtrees between the distinct ambiguous parses [7,4,24,31,25]3 One advantage of this structure is that the chart retains only these constituents that </context>
</contexts>
<marker>[11]</marker>
<rawString>Hays, D.G. 1962 Automatic Language-Data Processing. In Computer Applications in the Behavioral Sciences, (H. Borko ed.), Prentice-Hall, pp. 394-423.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D Ichbiah</author>
<author>S P Morse</author>
</authors>
<title>A Technique for Generating Almost Optimal Floyd-Evans Productions for Precedence Grammars.</title>
<date>1970</date>
<journal>Communications ACM</journal>
<volume>13</volume>
<issue>8</issue>
<pages>501--508</pages>
<contexts>
<context position="19929" citStr="[12]" startWordPosition="3332" endWordPosition="3332">e virtual machine code is then specified. However execution of the virtual machine code is done by an all-paths interpreter that follows the dynamic programming strategy described in section 2.1 and appendix A. The Tin interpreter is used in two different ways: 1. to study the effectiveness for chart parsing of known parsing schemata designed for deterministic parsing. We have only considered formally defined parsing schemata, corresponding to established PDA construction techniques that we use to mechanically translate CF grammars into Tin code. (e.g. LALR(1) and LALR(2) [6], weak precedence [12], LL(0) top-down (recursive descent), LR(0), LR(1) [1] ...). 2. to study the computational behavior of the generated code, and the optimization techniques that could be used on the lin code — and more generally chart parser code — with respect to code size, execution speed and better sharing in the parse forest. Experimenting with several compilation schemata hal shown that sophistication may have a negative effect on thi efficiency of all-path parsingli . Sophisticated PDT construction techniques tend to multiply the number of special cases, thereby increasing the code size of the chart parse</context>
</contexts>
<marker>[12]</marker>
<rawString>Ichbiah, J.D.; and Morse, S.P. 1970 A Technique for Generating Almost Optimal Floyd-Evans Productions for Precedence Grammars. Communications ACM 13(8): 501-508.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kasami</author>
</authors>
<title>An Efficient Recognition and Syntax Analysis Algorithm for Context-Free Languages. Report of Univ. of Hawaii, also AFCRL65-758, Air Force Cambridge Research Laboratory,</title>
<date>1965</date>
<tech>Report, No. R-257.</tech>
<institution>University of Minois Coordinated Science Lab.</institution>
<location>Bedford (Massachusetts), also</location>
<contexts>
<context position="3269" citStr="[11,39,13,3,9]" startWordPosition="523" endWordPosition="523">e for cyclic grammars or incomplete sentences [16]. However chart parsing techniques have been developed that produce an encoding of all possible parses as a data structure with a size polynomial in the length of the input sentence. These techniques are all based on a dynamic programming paradigm. The kind of structure they produce to represent all parses of the analyzed sentence is an essential characteristic of these algorithms. Some of the published algorithms produce only a chart as described by Kay in [14], which only associates nonterminal categories to segments of the analyzed sentence [11,39,13,3,9], and which thus still requires non-trivial processing to extract parse-trees [26]. The worst size complexity of such a chart is only a square function of the size of the input2. However, practical parsing algorithms will often produce a more complex structure that explicitly relates the instances of nonterminals associated with sentence fragments to their constituents, possibly in several ways in case of ambiguity, with a sharing of some common subtrees between the distinct ambiguous parses [7,4,24,31,25]3 One advantage of this structure is that the chart retains only these constituents that </context>
</contexts>
<marker>[13]</marker>
<rawString>Kasami, J. 1965 An Efficient Recognition and Syntax Analysis Algorithm for Context-Free Languages. Report of Univ. of Hawaii, also AFCRL65-758, Air Force Cambridge Research Laboratory, Bedford (Massachusetts), also 1966, University of Minois Coordinated Science Lab. Report, No. R-257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kay</author>
</authors>
<title>Algorithm Schemata and Data Structures in Syntactic Processing.</title>
<date>1980</date>
<booktitle>Proceedings of the Nobel Symposium on Text Processing, Gothenburg.</booktitle>
<contexts>
<context position="3171" citStr="[14]" startWordPosition="511" endWordPosition="511">e number of parses may be exponential in the size of the input sentence, or even infinite for cyclic grammars or incomplete sentences [16]. However chart parsing techniques have been developed that produce an encoding of all possible parses as a data structure with a size polynomial in the length of the input sentence. These techniques are all based on a dynamic programming paradigm. The kind of structure they produce to represent all parses of the analyzed sentence is an essential characteristic of these algorithms. Some of the published algorithms produce only a chart as described by Kay in [14], which only associates nonterminal categories to segments of the analyzed sentence [11,39,13,3,9], and which thus still requires non-trivial processing to extract parse-trees [26]. The worst size complexity of such a chart is only a square function of the size of the input2. However, practical parsing algorithms will often produce a more complex structure that explicitly relates the instances of nonterminals associated with sentence fragments to their constituents, possibly in several ways in case of ambiguity, with a sharing of some common subtrees between the distinct ambiguous parses [7,4,</context>
<context position="7389" citStr="[14]" startWordPosition="1184" endWordPosition="1184">etailed example produced with our implementation which illustrates both the working of the system and the underlying theory. 2 A Uniform Framework To discuss the above issues in a uniform way, we need a general framework that encompasses all forms of chart parsing and shared forest building in a unique formalism. We shall take as a baths a formalism developed by the second author in previous papers [15,16]. The idea of this approach is to separate the dynamic programming constructs needed for efficient chart parsing from the chosen parsing schema. Comparison between the classifications of Kay [14] and Griffith dc Petrick [10] shows that a parsing schema (or parsing strategy) may be expressed in the construction of a Push-Down Transducer (PDT), a well studied formalization of left-toright CF parsers&apos;. These PDTs are usually non-deterministic and cannot be used as produced for actual parsing. Their backtrack simulation does not always terminate, and is often time-exponential when it does, while breadth-first simulation is usually exponential for both time and space. However, by extending Earley&apos;s dynamic programming construction to PDTs, Lang provided in(15] a way of simulating all possi</context>
</contexts>
<marker>[14]</marker>
<rawString>Kay, M. 1980 Algorithm Schemata and Data Structures in Syntactic Processing. Proceedings of the Nobel Symposium on Text Processing, Gothenburg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Lang</author>
</authors>
<title>Deterministic Techniques for Efficient Non-deterministic Parsers.</title>
<date>1974</date>
<journal>Lecture Notes in Computer Science</journal>
<booktitle>Proc. of the 2nd Colloquium on Automata, Languages and Programming,</booktitle>
<volume>14</volume>
<pages>255--269</pages>
<editor>J. Loeckx (ed.), Saarbriicken,</editor>
<publisher>Springer</publisher>
<location>Rocquencourt</location>
<contexts>
<context position="7194" citStr="[15,16]" startWordPosition="1153" endWordPosition="1153">ve questions. This approach has been implemented into a system intended for the experimental study and comparison of parsing strategies. This system is described in section 3. Section 4 contains a detailed example produced with our implementation which illustrates both the working of the system and the underlying theory. 2 A Uniform Framework To discuss the above issues in a uniform way, we need a general framework that encompasses all forms of chart parsing and shared forest building in a unique formalism. We shall take as a baths a formalism developed by the second author in previous papers [15,16]. The idea of this approach is to separate the dynamic programming constructs needed for efficient chart parsing from the chosen parsing schema. Comparison between the classifications of Kay [14] and Griffith dc Petrick [10] shows that a parsing schema (or parsing strategy) may be expressed in the construction of a Push-Down Transducer (PDT), a well studied formalization of left-toright CF parsers&apos;. These PDTs are usually non-deterministic and cannot be used as produced for actual parsing. Their backtrack simulation does not always terminate, and is often time-exponential when it does, while b</context>
<context position="11910" citStr="[15,16]" startWordPosition="2015" endWordPosition="2015">y. 9 We assume the reader to be familiar with some variation of Earley&apos;s algorithm. Earley&apos;s original paper uses the word state (from dynamic programming terminology) instead of item. g = (S, fl, P, Ui), where $ is the set of all items (i.e. the union of Si), and the rules in P are constructed together with their left-hand-side item by the algorithm. The initial nonterminal Uf of g derives on the last items produced by a successful computation. Appendix A gives the details of the construction of items and rules in g by interpretation of the transitions of the PDT. More details may be found in [15,16]. 2.2 The shared forest An apparently major difference between the above algorithm and other parsers is that it represents a parse as the string of the grammar rules used in a leftmost reduction of the parsed sentence, rather than as a parse tree (cf. section 4). When the sentence has several distinct parses, the set of all possible parse strings is represented in finite shared form by a CF grammar that generates that possibly infinite set. Other published algorithms produce instead a graph structure representing all parse-trees with sharing of common subparts, which corresponds well to the in</context>
</contexts>
<marker>[15]</marker>
<rawString>Lang, B. 1974 Deterministic Techniques for Efficient Non-deterministic Parsers. Proc. of the 2nd Colloquium on Automata, Languages and Programming, J. Loeckx (ed.), Saarbriicken, Springer Lecture Notes in Computer Science 14: 255-269. Also: Rapport de Recherche 72, IRIA-Laboria, Rocquencourt (France).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Lang</author>
</authors>
<title>Parsing Incomplete Sentences.</title>
<date>1988</date>
<booktitle>Proc. of the 12th Internat. Conf. on Computational Linguistics (COLING&apos;88) VoL</booktitle>
<volume>1</volume>
<editor>365-371, D. Vargha (ed.),</editor>
<location>Budapest</location>
<contexts>
<context position="2705" citStr="[16]" startWordPosition="435" endWordPosition="435">mmatical features will often find ambiguities it cannot resolve in the analyzed sentences&apos;. A natural solution &apos;Address: INRIA, B.P. 105, 78153 Le Chesnay, France. The work reported here was partially supported by the Eureka Software Factory project. I Ambiguity may also have a semantical origin. is then to produce all possible parses, according to the CF backbone, and then select among them on the basis of the complete features information. One hitch is that the number of parses may be exponential in the size of the input sentence, or even infinite for cyclic grammars or incomplete sentences [16]. However chart parsing techniques have been developed that produce an encoding of all possible parses as a data structure with a size polynomial in the length of the input sentence. These techniques are all based on a dynamic programming paradigm. The kind of structure they produce to represent all parses of the analyzed sentence is an essential characteristic of these algorithms. Some of the published algorithms produce only a chart as described by Kay in [14], which only associates nonterminal categories to segments of the analyzed sentence [11,39,13,3,9], and which thus still requires non-</context>
<context position="7194" citStr="[15,16]" startWordPosition="1153" endWordPosition="1153">ve questions. This approach has been implemented into a system intended for the experimental study and comparison of parsing strategies. This system is described in section 3. Section 4 contains a detailed example produced with our implementation which illustrates both the working of the system and the underlying theory. 2 A Uniform Framework To discuss the above issues in a uniform way, we need a general framework that encompasses all forms of chart parsing and shared forest building in a unique formalism. We shall take as a baths a formalism developed by the second author in previous papers [15,16]. The idea of this approach is to separate the dynamic programming constructs needed for efficient chart parsing from the chosen parsing schema. Comparison between the classifications of Kay [14] and Griffith dc Petrick [10] shows that a parsing schema (or parsing strategy) may be expressed in the construction of a Push-Down Transducer (PDT), a well studied formalization of left-toright CF parsers&apos;. These PDTs are usually non-deterministic and cannot be used as produced for actual parsing. Their backtrack simulation does not always terminate, and is often time-exponential when it does, while b</context>
<context position="11910" citStr="[15,16]" startWordPosition="2015" endWordPosition="2015">y. 9 We assume the reader to be familiar with some variation of Earley&apos;s algorithm. Earley&apos;s original paper uses the word state (from dynamic programming terminology) instead of item. g = (S, fl, P, Ui), where $ is the set of all items (i.e. the union of Si), and the rules in P are constructed together with their left-hand-side item by the algorithm. The initial nonterminal Uf of g derives on the last items produced by a successful computation. Appendix A gives the details of the construction of items and rules in g by interpretation of the transitions of the PDT. More details may be found in [15,16]. 2.2 The shared forest An apparently major difference between the above algorithm and other parsers is that it represents a parse as the string of the grammar rules used in a leftmost reduction of the parsed sentence, rather than as a parse tree (cf. section 4). When the sentence has several distinct parses, the set of all possible parse strings is represented in finite shared form by a CF grammar that generates that possibly infinite set. Other published algorithms produce instead a graph structure representing all parse-trees with sharing of common subparts, which corresponds well to the in</context>
<context position="14673" citStr="[16]" startWordPosition="2455" endWordPosition="2455">cal) representations for intuitive understanding (grammars are also sometimes represented as graphs [37]), they are not the proper formal tool for manipulating shared forests, and developing formalized (proved) algorithms that use them. Graph formalization is considerably more complex and awkward to manipulate than the well understood, specialized and few concepts of CF grammars. Furthermore, unlike graphs, this grammar formalization of the shared forest may be tractably extended to other grammatical formalisms (cf. section 5). More importantly, our work on the parsing of incomplete sentences [16] has exhibited the fundamental character of our grammatical view of shared forests: when parsing the completely unknown sentence, the shared forest obtained is precisely the complete grammar of the analyzed language. This also leads to connections with the work on partial evaluation [8]. 2.3 The shape of the forest For our shared-forest, a cubic space complexity (in the worst case — space complexity is often linear in practice) is achieved, without requiring that the language grammar be in Chomsky Normal Form, by producing a grammar of parses that has at most two symbols on the right-hand side</context>
</contexts>
<marker>[16]</marker>
<rawString>Lang, B. 1988 Parsing Incomplete Sentences. Proc. of the 12th Internat. Conf. on Computational Linguistics (COLING&apos;88) VoL 1 :365-371, D. Vargha (ed.), Budapest (Hungary).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Lang</author>
</authors>
<title>Datalog Automata.</title>
<date>1988</date>
<booktitle>Proc. of the 3rd Internat. Conf. on Data and Knowledge Bases,</booktitle>
<pages>389--404</pages>
<editor>C. Been, J.W. Schmidt, U. Dayal (eds.),</editor>
<publisher>Morgan Kaufmann Pub.,</publisher>
<location>Jerusalem</location>
<contexts>
<context position="29440" citStr="[17,18]" startWordPosition="4965" endWordPosition="4965">rsing is sketched in appendix B. 147 5 Extensions As indicated earlier, our intent is mostly to understand phenomena that would be harder to evidence in more complex grammatical formalisms. This statement implies that our approach can be extended. This is indeed the case. It is known that many simple parsing schemata can be expressed with stack based machines [32]. This is certainly the case for all left-to-right CF chart parsing schemata. We have formally extended the concept of PDA into that of Logical PDA which is an operational push-down stack device for parsing unification based grammars [17,18] or other non-CF grammars such as Tree Adjoining Grammars [19]. Hence we are reusing and developing our theoretical [18] and experimental [36] approach in this much more general setting which is more likely to be effectively usable for natural language parsing. Furthermore, these extensions can also express, within the PDA model, non-left-to-right behavior such as is used in island parsing [38] or in Shell&apos;s approach [26]. More generally they allow the formal analysis of agenda strategies, which we have not considered here. In these extensions, the counterpart of parse forests are proof forest</context>
</contexts>
<marker>[17]</marker>
<rawString>Lang, B. 1988 Datalog Automata. Proc. of the 3rd Internat. Conf. on Data and Knowledge Bases, C. Been, J.W. Schmidt, U. Dayal (eds.), Morgan Kaufmann Pub., pp. 389-404, Jerusalem (Israel).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Lang</author>
</authors>
<title>Complete Evaluation of Horn Clauses, an Automata Theoretic Approach.</title>
<date>1988</date>
<journal>INRIA Research Report</journal>
<volume>913</volume>
<contexts>
<context position="29440" citStr="[17,18]" startWordPosition="4965" endWordPosition="4965">rsing is sketched in appendix B. 147 5 Extensions As indicated earlier, our intent is mostly to understand phenomena that would be harder to evidence in more complex grammatical formalisms. This statement implies that our approach can be extended. This is indeed the case. It is known that many simple parsing schemata can be expressed with stack based machines [32]. This is certainly the case for all left-to-right CF chart parsing schemata. We have formally extended the concept of PDA into that of Logical PDA which is an operational push-down stack device for parsing unification based grammars [17,18] or other non-CF grammars such as Tree Adjoining Grammars [19]. Hence we are reusing and developing our theoretical [18] and experimental [36] approach in this much more general setting which is more likely to be effectively usable for natural language parsing. Furthermore, these extensions can also express, within the PDA model, non-left-to-right behavior such as is used in island parsing [38] or in Shell&apos;s approach [26]. More generally they allow the formal analysis of agenda strategies, which we have not considered here. In these extensions, the counterpart of parse forests are proof forest</context>
</contexts>
<marker>[18]</marker>
<rawString>Lang, B. 1988 Complete Evaluation of Horn Clauses, an Automata Theoretic Approach. INRIA Research Report 913.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Lang</author>
</authors>
<title>The Systematic Construction of Earley Parsers: Application to the Production of 0(0) Earley Parsers for Tree Adjoining Grammars. In preparation.</title>
<date>1988</date>
<contexts>
<context position="29502" citStr="[19]" startWordPosition="4975" endWordPosition="4975">rlier, our intent is mostly to understand phenomena that would be harder to evidence in more complex grammatical formalisms. This statement implies that our approach can be extended. This is indeed the case. It is known that many simple parsing schemata can be expressed with stack based machines [32]. This is certainly the case for all left-to-right CF chart parsing schemata. We have formally extended the concept of PDA into that of Logical PDA which is an operational push-down stack device for parsing unification based grammars [17,18] or other non-CF grammars such as Tree Adjoining Grammars [19]. Hence we are reusing and developing our theoretical [18] and experimental [36] approach in this much more general setting which is more likely to be effectively usable for natural language parsing. Furthermore, these extensions can also express, within the PDA model, non-left-to-right behavior such as is used in island parsing [38] or in Shell&apos;s approach [26]. More generally they allow the formal analysis of agenda strategies, which we have not considered here. In these extensions, the counterpart of parse forests are proof forests of definite clause programs. 6 Conclusion Analysis of all-pa</context>
</contexts>
<marker>[19]</marker>
<rawString>Lang, B. 1988 The Systematic Construction of Earley Parsers: Application to the Production of 0(0) Earley Parsers for Tree Adjoining Grammars. In preparation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Li</author>
<author>H W Chun</author>
</authors>
<title>A Massively Parallel Network-Based Natural Language Parsing System.</title>
<date>1987</date>
<booktitle>Proc. of 2nd Int. Conf. on Computers and Applications</booktitle>
<pages>401--408</pages>
<location>Beijing (Peking),:</location>
<contexts>
<context position="4497" citStr="[23,20,33]" startWordPosition="717" endWordPosition="717">cipate in a parse. Furthermore it makes the extraction of parse-trees a trivial matter. A drawback is that this structure may be cubic in the length of the parsed sentence, and more generally polynomial4 for some proposed algorithms [31]. However, these algorithms are rather well behaved in practice, and this complexity is not a problem. In this paper we shall call shared forests such data struc2 We do not consider CF recognizers that have asymptotically the lowest complexity, but are only of theoretical interest here [35,5]. 3 There are several other published implementation of chart parsers [23,20,33], but they often do not give much detail on the output of the parsing process, or even side-step the problem altogether [33]. We do not consider here the well formed stastring tables of Shell [26] which falls somewhere in between in our classification. They do not use pointers and parse-trees are only &amp;quot;indirectly&amp;quot; visible, but may be extracted rather simply in linear time. ,The table may contain useless constituents. 4 Space cubic algorithms often require the language grammar to be in Cho/flaky Normal Form, and some authors have incorrectly conjectured that cubic complexity cannot be obtained </context>
</contexts>
<marker>[20]</marker>
<rawString>Li, T.; and Chun, H.W. 1987 A Massively Parallel Network-Based Natural Language Parsing System. Proc. of 2nd Int. Conf. on Computers and Applications Beijing (Peking),: 401-408.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Nakagawa</author>
</authors>
<title>Spoken Sentence Recognition by Time-Synchronous Parsing Algorithm of Context-Free Grammar.</title>
<date>1987</date>
<booktitle>Proc. ICASSP 87,</booktitle>
<volume>2</volume>
<pages>829--832</pages>
<location>Dallas (Texas),</location>
<contexts>
<context position="30806" citStr="[21,30]" startWordPosition="5171" endWordPosition="5171">e schemata, and gives objective criteria for chosing a given schema when implementing a language analyzer. The approach taken here supports both theoretical analysis and actual experimentation, both for the computational behavior of parsers and for the structure of the resulting shared forest. Many experiments and extensions still remain to be made: improved dynamic programming interpretation of bottomup parsers, more extensive experimental measurements with a variety of languages and parsing schemata, or generalization of this approach to more complex situations, such as word lattice parsing [21,30], or even handling of &amp;quot;secondary&amp;quot; language features. Early research in that latter direction is promising: our framework and the corresponding paradigm for parser construction have been extended to full first-order Horn clauses [17,18], and are hence applicable to unification based grammatical formalisms [27]. Shared forest construction and analysis can be generalized in the same way to these more advanced formalisms. Acknowledgements: We are grateful to Véronique Donzeau-Gouge for many fruitful discussions. This work has been partially supported by the Eureka Software Factory (ESF) project. R</context>
</contexts>
<marker>[21]</marker>
<rawString>Nakagawa, S. 1987 Spoken Sentence Recognition by Time-Synchronous Parsing Algorithm of Context-Free Grammar. Proc. ICASSP 87, Dallas (Texas), Vol. 2 : 829-832.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F C N Pereira</author>
<author>D H D Warren</author>
</authors>
<title>Definite Clause Grammars for Language Analysis — Asurvey of the Formalism and a Comparison with Augmented Transition Networks.</title>
<date>1980</date>
<journal>Artificial Intelligence</journal>
<volume>13</volume>
<pages>231--278</pages>
<contexts>
<context position="12928" citStr="[22]" startWordPosition="2185" endWordPosition="2185">es that possibly infinite set. Other published algorithms produce instead a graph structure representing all parse-trees with sharing of common subparts, which corresponds well to the intuitive notion of a shared forest. This difference is only appearance. We show here in section 4 that the CF grammar of all leftmost parses is just a theoretical formalization of the shared-forest graph. ContextFree grammars can be represented by AND-OR graphs that are closely related to the syntax diagrams often used to describe the syntax of programming languages [37], and to the transition networks of Woods [22]. In the case of our grammar of leftmost parses, this AND-OR graph (which is acyclic when there is only finite ambiguity) is precisely the sharedforest graph. In this graph, AND-nodes correspond to the usual parse-tree nodes, while OR-nodes correspond to ambiguities, i.e. distinct possible subtrees occurring in the same context. Sharing of subtrees in represented by nodes accessed by more than one other node. The grammar viewpoint is the following (d. the example in section 4). Non-terminal (reap. terminal) symbols correspond to nodes with (resp. without) outgoing arcs. ANDnodes correspond to </context>
</contexts>
<marker>[22]</marker>
<rawString>Pereira, F.C.N.; and Warren, D.H.D. 1980 Definite Clause Grammars for Language Analysis — Asurvey of the Formalism and a Comparison with Augmented Transition Networks. Artificial Intelligence 13: 231-278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D Phillips</author>
</authors>
<title>A Simple Efficient Parser for Phrase-Structure Grammars.</title>
<date>1986</date>
<journal>Quarterly Newsletter of the Soc. for the Study of Artificial Intelligence (AISBQ)</journal>
<volume>59</volume>
<pages>14--19</pages>
<contexts>
<context position="4497" citStr="[23,20,33]" startWordPosition="717" endWordPosition="717">cipate in a parse. Furthermore it makes the extraction of parse-trees a trivial matter. A drawback is that this structure may be cubic in the length of the parsed sentence, and more generally polynomial4 for some proposed algorithms [31]. However, these algorithms are rather well behaved in practice, and this complexity is not a problem. In this paper we shall call shared forests such data struc2 We do not consider CF recognizers that have asymptotically the lowest complexity, but are only of theoretical interest here [35,5]. 3 There are several other published implementation of chart parsers [23,20,33], but they often do not give much detail on the output of the parsing process, or even side-step the problem altogether [33]. We do not consider here the well formed stastring tables of Shell [26] which falls somewhere in between in our classification. They do not use pointers and parse-trees are only &amp;quot;indirectly&amp;quot; visible, but may be extracted rather simply in linear time. ,The table may contain useless constituents. 4 Space cubic algorithms often require the language grammar to be in Cho/flaky Normal Form, and some authors have incorrectly conjectured that cubic complexity cannot be obtained </context>
</contexts>
<marker>[23]</marker>
<rawString>Phillips, J.D. 1986 A Simple Efficient Parser for Phrase-Structure Grammars. Quarterly Newsletter of the Soc. for the Study of Artificial Intelligence (AISBQ) 59: 14-19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V R Pratt</author>
</authors>
<title>LINGOL — A Progress Report.</title>
<date>1975</date>
<booktitle>In Proceedings of the 4th IJCAL</booktitle>
<pages>422--428</pages>
<contexts>
<context position="1627" citStr="[24,28]" startWordPosition="257" endWordPosition="257"> for simplicity, these results extend to more complex formalisms such as unification based grammars. Key words: Context-Free Parsing, Ambiguity, Dynamic Programming, Earley Parsing, Chart Parsing, Parsing Strategies, Parsing Schemata, Parse Tree, Parse Forest. 1 Introduction Several natural language parser start with a pure ContextFree (CF) backbone that makes a first sketch of the structure of the analyzed sentence, before it is handed to a more elaborate analyzer (possibly a coroutine), that takes into account the finer grammatical structure to filter out undesirable parses (see for example [24,28]). In [28], Shieber surveys existing variants to this approach before giving his own tunable approach based on restrictions that &amp;quot;split up the infinite nonterminal domain into a finite set of equivalence classes that can be used for parsing&amp;quot;. The basic motivation for this approach is to benefit from the CF parsing technology whose development over 30 years has lead to powerful and efficient parsers [1,7]. A parser that takes into account only an approximation of the grammatical features will often find ambiguities it cannot resolve in the analyzed sentences&apos;. A natural solution &apos;Address: INRIA</context>
<context position="3780" citStr="[7,4,24,31,25]" startWordPosition="601" endWordPosition="601"> [14], which only associates nonterminal categories to segments of the analyzed sentence [11,39,13,3,9], and which thus still requires non-trivial processing to extract parse-trees [26]. The worst size complexity of such a chart is only a square function of the size of the input2. However, practical parsing algorithms will often produce a more complex structure that explicitly relates the instances of nonterminals associated with sentence fragments to their constituents, possibly in several ways in case of ambiguity, with a sharing of some common subtrees between the distinct ambiguous parses [7,4,24,31,25]3 One advantage of this structure is that the chart retains only these constituents that can actually participate in a parse. Furthermore it makes the extraction of parse-trees a trivial matter. A drawback is that this structure may be cubic in the length of the parsed sentence, and more generally polynomial4 for some proposed algorithms [31]. However, these algorithms are rather well behaved in practice, and this complexity is not a problem. In this paper we shall call shared forests such data struc2 We do not consider CF recognizers that have asymptotically the lowest complexity, but are onl</context>
</contexts>
<marker>[24]</marker>
<rawString>Pratt, V.R. 1975 LINGOL — A Progress Report. In Proceedings of the 4th IJCAL 422-428.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Rekers</author>
</authors>
<title>A Parser Generator for Finitely Ambiguous Context-Free Grammars. Report</title>
<date>1987</date>
<booktitle>CSR8712, Computer Sdence/Dpt. of Software Technology, Centrum voor Wiskunde en Informatica,</booktitle>
<location>Amsterdam (The Netherlands).</location>
<contexts>
<context position="3780" citStr="[7,4,24,31,25]" startWordPosition="601" endWordPosition="601"> [14], which only associates nonterminal categories to segments of the analyzed sentence [11,39,13,3,9], and which thus still requires non-trivial processing to extract parse-trees [26]. The worst size complexity of such a chart is only a square function of the size of the input2. However, practical parsing algorithms will often produce a more complex structure that explicitly relates the instances of nonterminals associated with sentence fragments to their constituents, possibly in several ways in case of ambiguity, with a sharing of some common subtrees between the distinct ambiguous parses [7,4,24,31,25]3 One advantage of this structure is that the chart retains only these constituents that can actually participate in a parse. Furthermore it makes the extraction of parse-trees a trivial matter. A drawback is that this structure may be cubic in the length of the parsed sentence, and more generally polynomial4 for some proposed algorithms [31]. However, these algorithms are rather well behaved in practice, and this complexity is not a problem. In this paper we shall call shared forests such data struc2 We do not consider CF recognizers that have asymptotically the lowest complexity, but are onl</context>
</contexts>
<marker>[25]</marker>
<rawString>Rekers, J. 1987 A Parser Generator for Finitely Ambiguous Context-Free Grammars. Report CSR8712, Computer Sdence/Dpt. of Software Technology, Centrum voor Wiskunde en Informatica, Amsterdam (The Netherlands).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B A Sheil</author>
</authors>
<title>Observations on Context Free Parsing. in Statistical Methods in Linguistics: 71-109,</title>
<date>1976</date>
<booktitle>Proc. of Internat. Conf. on Computational Linguistics (COLING-76),</booktitle>
<tech>Also: Technical Report TR. 12-76,</tech>
<institution>Center for Research in Computing Technology, Aiken Computation Laboratory, Harvard Univ.,</institution>
<location>Stockholm</location>
<contexts>
<context position="3351" citStr="[26]" startWordPosition="535" endWordPosition="535">en developed that produce an encoding of all possible parses as a data structure with a size polynomial in the length of the input sentence. These techniques are all based on a dynamic programming paradigm. The kind of structure they produce to represent all parses of the analyzed sentence is an essential characteristic of these algorithms. Some of the published algorithms produce only a chart as described by Kay in [14], which only associates nonterminal categories to segments of the analyzed sentence [11,39,13,3,9], and which thus still requires non-trivial processing to extract parse-trees [26]. The worst size complexity of such a chart is only a square function of the size of the input2. However, practical parsing algorithms will often produce a more complex structure that explicitly relates the instances of nonterminals associated with sentence fragments to their constituents, possibly in several ways in case of ambiguity, with a sharing of some common subtrees between the distinct ambiguous parses [7,4,24,31,25]3 One advantage of this structure is that the chart retains only these constituents that can actually participate in a parse. Furthermore it makes the extraction of parse-</context>
<context position="4693" citStr="[26]" startWordPosition="753" endWordPosition="753">4 for some proposed algorithms [31]. However, these algorithms are rather well behaved in practice, and this complexity is not a problem. In this paper we shall call shared forests such data struc2 We do not consider CF recognizers that have asymptotically the lowest complexity, but are only of theoretical interest here [35,5]. 3 There are several other published implementation of chart parsers [23,20,33], but they often do not give much detail on the output of the parsing process, or even side-step the problem altogether [33]. We do not consider here the well formed stastring tables of Shell [26] which falls somewhere in between in our classification. They do not use pointers and parse-trees are only &amp;quot;indirectly&amp;quot; visible, but may be extracted rather simply in linear time. ,The table may contain useless constituents. 4 Space cubic algorithms often require the language grammar to be in Cho/flaky Normal Form, and some authors have incorrectly conjectured that cubic complexity cannot be obtained otherwise. 1143 tures used to represent simultaneously all parse trees for a given sentence. Several questions may be asked in relation with shared forests: • How to construct them during the pars</context>
<context position="18108" citStr="[26]" startWordPosition="3039" endWordPosition="3039">while top-down parsing may share only the leftmost subconstituents. This relation between parsing schema and shape of the shared forest (and type of sharing) is a consequence of intrinsic properties of chart parsing, and not of our specific implementation. It is for example to be expected that the bidirectional nature of island parsing leads to irregular structure in shared forests, when optimal sharing is sought for. 3 Implementation and Experimental Results The ideas presented above have been implemented in an experimental system called Tin (after the woodman of OZ). This was noted by Shell [26] and is implicit in his use of &amp;quot;2- form&amp;quot; grammars. 145 The intent is to provide a uniform framework for the construction and experimentation of chart parsers, somewhat as systems like MCHART [29], but with a more systematic theoretical foundation. The kernel of the system is a virtual parsing machine with a stack and a set of primitive commands corresponding essentially to the operation of a practical Push-Down Transducer. These commands include for example: push (resp. pop) to push a symbol on the stack (resp. pop one), chackwindow to compare the look-ahead symbol(s) to some given symbol, chs</context>
<context position="29865" citStr="[26]" startWordPosition="5033" endWordPosition="5033">rsing schemata. We have formally extended the concept of PDA into that of Logical PDA which is an operational push-down stack device for parsing unification based grammars [17,18] or other non-CF grammars such as Tree Adjoining Grammars [19]. Hence we are reusing and developing our theoretical [18] and experimental [36] approach in this much more general setting which is more likely to be effectively usable for natural language parsing. Furthermore, these extensions can also express, within the PDA model, non-left-to-right behavior such as is used in island parsing [38] or in Shell&apos;s approach [26]. More generally they allow the formal analysis of agenda strategies, which we have not considered here. In these extensions, the counterpart of parse forests are proof forests of definite clause programs. 6 Conclusion Analysis of all-path parsing schemata within a common framework exhibits in comparable terms the properties of these schemata, and gives objective criteria for chosing a given schema when implementing a language analyzer. The approach taken here supports both theoretical analysis and actual experimentation, both for the computational behavior of parsers and for the structure of </context>
</contexts>
<marker>[26]</marker>
<rawString>Sheil, B.A. 1976 Observations on Context Free Parsing. in Statistical Methods in Linguistics: 71-109, Stockholm (Sweden), Proc. of Internat. Conf. on Computational Linguistics (COLING-76), Ottawa (Canada). Also: Technical Report TR. 12-76, Center for Research in Computing Technology, Aiken Computation Laboratory, Harvard Univ., Cambridge (Massachusetts).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Shieber</author>
</authors>
<title>The Design of a Computer Language for Linguistic Information.</title>
<date>1984</date>
<booktitle>Proc. of the 10th Internat. Conf. on Computational Linguistics — COLING&apos;84:</booktitle>
<pages>362--366</pages>
<location>Stanford (California).</location>
<marker>[27]</marker>
<rawString>Shieber, S.M. 1984 The Design of a Computer Language for Linguistic Information. Proc. of the 10th Internat. Conf. on Computational Linguistics — COLING&apos;84: 362-366, Stanford (California).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Shieber</author>
</authors>
<title>Using Restriction to Extend Parsing Algorithms for Complex-Feature-Based Formalisms.</title>
<date>1985</date>
<booktitle>Proceedings of the 23rd Annual Meeting of the Association for Computational Linguistics:</booktitle>
<pages>145--152</pages>
<contexts>
<context position="1627" citStr="[24,28]" startWordPosition="257" endWordPosition="257"> for simplicity, these results extend to more complex formalisms such as unification based grammars. Key words: Context-Free Parsing, Ambiguity, Dynamic Programming, Earley Parsing, Chart Parsing, Parsing Strategies, Parsing Schemata, Parse Tree, Parse Forest. 1 Introduction Several natural language parser start with a pure ContextFree (CF) backbone that makes a first sketch of the structure of the analyzed sentence, before it is handed to a more elaborate analyzer (possibly a coroutine), that takes into account the finer grammatical structure to filter out undesirable parses (see for example [24,28]). In [28], Shieber surveys existing variants to this approach before giving his own tunable approach based on restrictions that &amp;quot;split up the infinite nonterminal domain into a finite set of equivalence classes that can be used for parsing&amp;quot;. The basic motivation for this approach is to benefit from the CF parsing technology whose development over 30 years has lead to powerful and efficient parsers [1,7]. A parser that takes into account only an approximation of the grammatical features will often find ambiguities it cannot resolve in the analyzed sentences&apos;. A natural solution &apos;Address: INRIA</context>
</contexts>
<marker>[28]</marker>
<rawString>Shieber, S.M. 1985 Using Restriction to Extend Parsing Algorithms for Complex-Feature-Based Formalisms. Proceedings of the 23rd Annual Meeting of the Association for Computational Linguistics: 145-152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Thompson</author>
</authors>
<title>MCHART: A Flexible, Modular Chart Parsing System.</title>
<date>1983</date>
<booktitle>Proc. of the National ConL on Artificial Intelligence (AAAI-83),</booktitle>
<pages>408--410</pages>
<location>Washington (D.C.),</location>
<contexts>
<context position="18303" citStr="[29]" startWordPosition="3073" endWordPosition="3073">ies of chart parsing, and not of our specific implementation. It is for example to be expected that the bidirectional nature of island parsing leads to irregular structure in shared forests, when optimal sharing is sought for. 3 Implementation and Experimental Results The ideas presented above have been implemented in an experimental system called Tin (after the woodman of OZ). This was noted by Shell [26] and is implicit in his use of &amp;quot;2- form&amp;quot; grammars. 145 The intent is to provide a uniform framework for the construction and experimentation of chart parsers, somewhat as systems like MCHART [29], but with a more systematic theoretical foundation. The kernel of the system is a virtual parsing machine with a stack and a set of primitive commands corresponding essentially to the operation of a practical Push-Down Transducer. These commands include for example: push (resp. pop) to push a symbol on the stack (resp. pop one), chackwindow to compare the look-ahead symbol(s) to some given symbol, chsckatack to branch depending on the top of the stack, scan to read an input word, output to output a rule number (or a terminal symbol), pto for unconditional jumps, and a few others. However thes</context>
</contexts>
<marker>[29]</marker>
<rawString>Thompson, H. 1983 MCHART: A Flexible, Modular Chart Parsing System. Proc. of the National ConL on Artificial Intelligence (AAAI-83), Washington (D.C.), pp. 408-410.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tomita</author>
</authors>
<title>An Efficient Word Lattice Parsing Algorithm for Continuous Speech Recognition.</title>
<date>1986</date>
<booktitle>In Proceedings of IEEE-IECE-ASJ International Conference on Acoustics, Speech, and Signal Processing (ICASSP 86),</booktitle>
<volume>3</volume>
<pages>1569--1572</pages>
<contexts>
<context position="30806" citStr="[21,30]" startWordPosition="5171" endWordPosition="5171">e schemata, and gives objective criteria for chosing a given schema when implementing a language analyzer. The approach taken here supports both theoretical analysis and actual experimentation, both for the computational behavior of parsers and for the structure of the resulting shared forest. Many experiments and extensions still remain to be made: improved dynamic programming interpretation of bottomup parsers, more extensive experimental measurements with a variety of languages and parsing schemata, or generalization of this approach to more complex situations, such as word lattice parsing [21,30], or even handling of &amp;quot;secondary&amp;quot; language features. Early research in that latter direction is promising: our framework and the corresponding paradigm for parser construction have been extended to full first-order Horn clauses [17,18], and are hence applicable to unification based grammatical formalisms [27]. Shared forest construction and analysis can be generalized in the same way to these more advanced formalisms. Acknowledgements: We are grateful to Véronique Donzeau-Gouge for many fruitful discussions. This work has been partially supported by the Eureka Software Factory (ESF) project. R</context>
</contexts>
<marker>[30]</marker>
<rawString>Tomita, M. 1986 An Efficient Word Lattice Parsing Algorithm for Continuous Speech Recognition. In Proceedings of IEEE-IECE-ASJ International Conference on Acoustics, Speech, and Signal Processing (ICASSP 86), Vol. 3: 1569-1572.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tomita</author>
</authors>
<title>An Efficient AugmentedContext-Free Parsing Algorithm.</title>
<date>1987</date>
<journal>Computational Linguistics</journal>
<volume>13</volume>
<issue>1</issue>
<pages>31--46</pages>
<contexts>
<context position="3780" citStr="[7,4,24,31,25]" startWordPosition="601" endWordPosition="601"> [14], which only associates nonterminal categories to segments of the analyzed sentence [11,39,13,3,9], and which thus still requires non-trivial processing to extract parse-trees [26]. The worst size complexity of such a chart is only a square function of the size of the input2. However, practical parsing algorithms will often produce a more complex structure that explicitly relates the instances of nonterminals associated with sentence fragments to their constituents, possibly in several ways in case of ambiguity, with a sharing of some common subtrees between the distinct ambiguous parses [7,4,24,31,25]3 One advantage of this structure is that the chart retains only these constituents that can actually participate in a parse. Furthermore it makes the extraction of parse-trees a trivial matter. A drawback is that this structure may be cubic in the length of the parsed sentence, and more generally polynomial4 for some proposed algorithms [31]. However, these algorithms are rather well behaved in practice, and this complexity is not a problem. In this paper we shall call shared forests such data struc2 We do not consider CF recognizers that have asymptotically the lowest complexity, but are onl</context>
<context position="11016" citStr="[31]" startWordPosition="1860" endWordPosition="1860">h word symbol x, holding position i in the input sentence z. An item is constituted of two modes of the form (p A i) where p is a PDT state, A is a stack symbol, and Os the index of an input symbol. The item set Si contains items of the form ((p A 1) (q B j)) . These items are used as nonterminals of an output grammar 6 The original intent of (15] was to show how one can generate efficient general CF chart parsers, by first producing the PDT with the efficient techniques for deterministic parsing developed for the compiler technology (6,12,1]. This idea was later successfully used by Ton:tits [31] who applied it to LR(1) parsers [6,1], and later to other pushdown based parsers [32]. 7 Implementations usually denote these rules by their index in the set 11. 8 Actual implementations use output symbols from nuE, since rules alone do not distinguish words in the same lexical category. 9 We assume the reader to be familiar with some variation of Earley&apos;s algorithm. Earley&apos;s original paper uses the word state (from dynamic programming terminology) instead of item. g = (S, fl, P, Ui), where $ is the set of all items (i.e. the union of Si), and the rules in P are constructed together with thei</context>
<context position="21693" citStr="[31]" startWordPosition="3610" endWordPosition="3610">and often also reduced computation time) is to try to recognize ever3 grammar rule in only one place of the generated chart parse] code, even at the cost of increasing non-determinism. Thus simpler schemata such as precedence, LL(0) (and probably LR(0)13) produce the best sharing. However, since they correspond to a smaller deterministic domain within the CF grammar realm, they may sometimes be computationally less efficient because they produce a larger number of uselesE items (i.e. edges) that correspond to dead-end computationai paths. Slight sophistication (e.g. LALR(1) used by Tomita iii [31], or LR(1) ) may slightly improve computational performance by detecting earlier dead-end computations. Tide may however be at the expense of the forest sharing quality. More sophistication (say LR(2)) is usually losing on both accounts as explained earlier. The duplication of computational paths due to distinct context analysis overweights the 11 We mean here the sophistication of the CF parser construc. tion technique rather than the sophistication of the language fea. tures chosen to be used by this parser. 11 This negative behavior of some techniques originally intendec to preserve determi</context>
</contexts>
<marker>[31]</marker>
<rawString>Tomita, M. 1987 An Efficient AugmentedContext-Free Parsing Algorithm. Computational Linguistics 13(1-2): 31-46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tomita</author>
</authors>
<title>Graph-structured Stack and Natural Language Parsing.</title>
<date>1988</date>
<booktitle>Proceedings of the 26&amp;quot; Annual Meeting of the Association for Computational Linguistics:</booktitle>
<pages>249--257</pages>
<contexts>
<context position="11102" citStr="[32]" startWordPosition="1875" endWordPosition="1875"> two modes of the form (p A i) where p is a PDT state, A is a stack symbol, and Os the index of an input symbol. The item set Si contains items of the form ((p A 1) (q B j)) . These items are used as nonterminals of an output grammar 6 The original intent of (15] was to show how one can generate efficient general CF chart parsers, by first producing the PDT with the efficient techniques for deterministic parsing developed for the compiler technology (6,12,1]. This idea was later successfully used by Ton:tits [31] who applied it to LR(1) parsers [6,1], and later to other pushdown based parsers [32]. 7 Implementations usually denote these rules by their index in the set 11. 8 Actual implementations use output symbols from nuE, since rules alone do not distinguish words in the same lexical category. 9 We assume the reader to be familiar with some variation of Earley&apos;s algorithm. Earley&apos;s original paper uses the word state (from dynamic programming terminology) instead of item. g = (S, fl, P, Ui), where $ is the set of all items (i.e. the union of Si), and the rules in P are constructed together with their left-hand-side item by the algorithm. The initial nonterminal Uf of g derives on the</context>
<context position="29199" citStr="[32]" startWordPosition="4926" endWordPosition="4926">e, the &amp;quot;constructions&amp;quot; given in this section are purely virtual. In an implementation, the data-structure representing the grammar of figure 3 may be directly interpreted and used as a shared-forest. A similar construction for top-down parsing is sketched in appendix B. 147 5 Extensions As indicated earlier, our intent is mostly to understand phenomena that would be harder to evidence in more complex grammatical formalisms. This statement implies that our approach can be extended. This is indeed the case. It is known that many simple parsing schemata can be expressed with stack based machines [32]. This is certainly the case for all left-to-right CF chart parsing schemata. We have formally extended the concept of PDA into that of Logical PDA which is an operational push-down stack device for parsing unification based grammars [17,18] or other non-CF grammars such as Tree Adjoining Grammars [19]. Hence we are reusing and developing our theoretical [18] and experimental [36] approach in this much more general setting which is more likely to be effectively usable for natural language parsing. Furthermore, these extensions can also express, within the PDA model, non-left-to-right behavior </context>
</contexts>
<marker>[32]</marker>
<rawString>Tomita, M. 1988 Graph-structured Stack and Natural Language Parsing. Proceedings of the 26&amp;quot; Annual Meeting of the Association for Computational Linguistics: 249-257.</rawString>
</citation>
<citation valid="false">
<authors>
<author>K Uehara</author>
<author>R Ochitani</author>
<author>O Kakusho</author>
<author>J Toyoda</author>
</authors>
<title>A Bottom-Up Parser based on Predicate Logic: A Survey of the Formalism and its Implementation Technique.</title>
<date>1984</date>
<journal>Journal of Computer and System Sciences,</journal>
<booktitle>Internat. Symp. on Logic Programming,</booktitle>
<volume>10</volume>
<pages>308--315</pages>
<institution>U.S. Department of Defense</institution>
<location>Atlantic City (New Jersey),:</location>
<contexts>
<context position="4497" citStr="[23,20,33]" startWordPosition="717" endWordPosition="717">cipate in a parse. Furthermore it makes the extraction of parse-trees a trivial matter. A drawback is that this structure may be cubic in the length of the parsed sentence, and more generally polynomial4 for some proposed algorithms [31]. However, these algorithms are rather well behaved in practice, and this complexity is not a problem. In this paper we shall call shared forests such data struc2 We do not consider CF recognizers that have asymptotically the lowest complexity, but are only of theoretical interest here [35,5]. 3 There are several other published implementation of chart parsers [23,20,33], but they often do not give much detail on the output of the parsing process, or even side-step the problem altogether [33]. We do not consider here the well formed stastring tables of Shell [26] which falls somewhere in between in our classification. They do not use pointers and parse-trees are only &amp;quot;indirectly&amp;quot; visible, but may be extracted rather simply in linear time. ,The table may contain useless constituents. 4 Space cubic algorithms often require the language grammar to be in Cho/flaky Normal Form, and some authors have incorrectly conjectured that cubic complexity cannot be obtained </context>
</contexts>
<marker>[33]</marker>
<rawString>Uehara, K.; Ochitani, R.; Kakusho, O.; Toyoda, J. 1984 A Bottom-Up Parser based on Predicate Logic: A Survey of the Formalism and its Implementation Technique. 1984 Internat. Symp. on Logic Programming, Atlantic City (New Jersey),: 220-227. (34] U.S. Department of Defense 1983 Reference Manual for the Ada Programming Language. ANSI/MIL-STD-1815 A. (35] Valiant, L.G. 1975 General Context-Free Recognition in Less than Cubic Time. Journal of Computer and System Sciences, 10: 308-315.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Villemonte de in Clergerie</author>
<author>E</author>
<author>A Zanchetta</author>
</authors>
<title>Evaluateur de Clauses de Horn. Rapport de Stage d&apos;Option, Ecole Polytechniqne,</title>
<date>1988</date>
<location>Palaiseau</location>
<contexts>
<context position="29582" citStr="[36]" startWordPosition="4987" endWordPosition="4987">ence in more complex grammatical formalisms. This statement implies that our approach can be extended. This is indeed the case. It is known that many simple parsing schemata can be expressed with stack based machines [32]. This is certainly the case for all left-to-right CF chart parsing schemata. We have formally extended the concept of PDA into that of Logical PDA which is an operational push-down stack device for parsing unification based grammars [17,18] or other non-CF grammars such as Tree Adjoining Grammars [19]. Hence we are reusing and developing our theoretical [18] and experimental [36] approach in this much more general setting which is more likely to be effectively usable for natural language parsing. Furthermore, these extensions can also express, within the PDA model, non-left-to-right behavior such as is used in island parsing [38] or in Shell&apos;s approach [26]. More generally they allow the formal analysis of agenda strategies, which we have not considered here. In these extensions, the counterpart of parse forests are proof forests of definite clause programs. 6 Conclusion Analysis of all-path parsing schemata within a common framework exhibits in comparable terms the p</context>
</contexts>
<marker>[36]</marker>
<rawString>Villemonte de in Clergerie, E.; and Zanchetta, A. 1988 Evaluateur de Clauses de Horn. Rapport de Stage d&apos;Option, Ecole Polytechniqne, Palaiseau (France).</rawString>
</citation>
<citation valid="false">
<authors>
<author>N Wirth</author>
</authors>
<title>The Programming Language Pascal. Acta Inforrnatica,</title>
<date>1971</date>
<booktitle>In Proceedings of the 1988 International Conference on Acoustics, Speech, and Signal Processing (ICASSP 88),</booktitle>
<volume>1</volume>
<issue>1</issue>
<pages>275--278</pages>
<location>Younger, D.H.</location>
<contexts>
<context position="12882" citStr="[37]" startWordPosition="2177" endWordPosition="2177">inite shared form by a CF grammar that generates that possibly infinite set. Other published algorithms produce instead a graph structure representing all parse-trees with sharing of common subparts, which corresponds well to the intuitive notion of a shared forest. This difference is only appearance. We show here in section 4 that the CF grammar of all leftmost parses is just a theoretical formalization of the shared-forest graph. ContextFree grammars can be represented by AND-OR graphs that are closely related to the syntax diagrams often used to describe the syntax of programming languages [37], and to the transition networks of Woods [22]. In the case of our grammar of leftmost parses, this AND-OR graph (which is acyclic when there is only finite ambiguity) is precisely the sharedforest graph. In this graph, AND-nodes correspond to the usual parse-tree nodes, while OR-nodes correspond to ambiguities, i.e. distinct possible subtrees occurring in the same context. Sharing of subtrees in represented by nodes accessed by more than one other node. The grammar viewpoint is the following (d. the example in section 4). Non-terminal (reap. terminal) symbols correspond to nodes with (resp. w</context>
<context position="14173" citStr="[37]" startWordPosition="2382" endWordPosition="2382">-nodes (i.e. ambiguities) correspond to non-terminals defined by several rules. Subtree sharing is represented by several uses of the same symbol in rule right-hand sides. To our knowledge, this representation of parse-forests as grammars is the simplest and most tractable theoretical formalization proposed so far, and the parser presented here is the only one for which the correctness of the output grammar — i.e. of the shared-forest — has ever been proved. Though in the examples we use graph(ical) representations for intuitive understanding (grammars are also sometimes represented as graphs [37]), they are not the proper formal tool for manipulating shared forests, and developing formalized (proved) algorithms that use them. Graph formalization is considerably more complex and awkward to manipulate than the well understood, specialized and few concepts of CF grammars. Furthermore, unlike graphs, this grammar formalization of the shared forest may be tractably extended to other grammatical formalisms (cf. section 5). More importantly, our work on the parsing of incomplete sentences [16] has exhibited the fundamental character of our grammatical view of shared forests: when parsing the</context>
</contexts>
<marker>[37]</marker>
<rawString>Wirth, N. 1971 The Programming Language Pascal. Acta Inforrnatica, 1(1). (38] Ward, W.H.; Hauptmann, A.G.; Stern, R.M.; and Chanak, T. 1988 Parsing Spoken Phrases Despite Missing Words. In Proceedings of the 1988 International Conference on Acoustics, Speech, and Signal Processing (ICASSP 88), Vol. 1: 275-278. (39] Younger, D.H. 1967 Recognition and Parsing of Context-Free Languages in Time n3. information and Control, 10(2): 189-208</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>