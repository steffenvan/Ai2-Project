<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000859">
<title confidence="0.987991">
In-House: An Ensemble of Pre-Existing Off-the-Shelf Parsers
</title>
<author confidence="0.999373">
Yusuke Miyao♣, Stephan Oepen♠V, and Daniel Zeman♦
</author>
<affiliation confidence="0.8746775">
♣ National Institute of Informatics, Tokyo
♠ University of Oslo, Department of Informatics
♥ Potsdam University, Department of Linguistics
♦ Charles University in Prague, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics
</affiliation>
<email confidence="0.98484">
yusuke@nii.ac.jp,oe@ifi.uio.no,zeman@ufal.mff.cuni.cz
</email>
<sectionHeader confidence="0.993677" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9994564">
This submission to the open track of
Task 8 at SemEval 2014 seeks to connect
the Task to pre-existing, ‘in-house’ pars-
ing systems for the same types of target
semantic dependency graphs.
</bodyText>
<sectionHeader confidence="0.924324" genericHeader="categories and subject descriptors">
1 Background and Motivation
</sectionHeader>
<bodyText confidence="0.999958666666667">
The three target representations for Task 8 at
SemEval 2014, Broad-Coverage Semantic Depen-
dency Parsing (SDP; Oepen et al., 2014), are
rooted in language engineering efforts that have
been under continuous development for at least
the past decade. The gold-standard semantic de-
pendency graphs used for training and testing in
the Task result from largely manual annotation, in
part re-purposing and adapting resources like the
Penn Treebank (PTB; Marcus et al., 1993), Prop-
Bank (Palmer et al., 2005), and others. But the
groups who prepared the SDP target data have also
worked in parallel on automated parsing systems
for these representations.
Thus, for each of the target representations,
there is a pre-existing parser, often developed in
parallel to the creation of the target dependency
graphs, viz. (a) for the DM representation, the
parser of the hand-engineered LinGO English Re-
source Grammar (ERG; Flickinger, 2000); (b) for
PAS, the Enju parsing system (Miyao, 2006), with
its probabilistic HPSG acquired through linguis-
tic projection of the PTB; and (c) for PCEDT,
the scenario for English analysis within the Treex
framework (Popel and Žabokrtský, 2010), com-
bining data-driven dependency parsing with hand-
engineered tectogrammatical conversion. At least
</bodyText>
<footnote confidence="0.5484005">
This work is licenced under a Creative Commons At-
tribution 4.0 International License; page numbers and the
proceedings footer are added by the organizers. http://
creativecommons.org/licenses/by/4.0/
</footnote>
<bodyText confidence="0.9996965">
for DM and PAS, these parsers have been exten-
sively engineered and applied successfully in a
variety of applications, hence represent relevant
points of comparison. Through this ‘in-house’
submission (of our ‘own’ parsers to our ‘own’
task), we hope to facilitate the comparison of dif-
ferent approaches submitted to the Task with this
pre-existing line of parser engineering.
</bodyText>
<sectionHeader confidence="0.957761" genericHeader="method">
2 DM: The English Resource Grammar
</sectionHeader>
<bodyText confidence="0.997750071428571">
Semantic dependency graphs in the DM target rep-
resentation, DELPH-IN MRS-Derived Bi-Lexical
Dependencies, stem from a two-step ‘reduc-
tion’ (simplification) of the underspecified logical-
form meaning representations output natively by
the ERG parser, which implements the linguis-
tic framework of Head-Driven Phrase Structure
Grammar (HPSG; Pollard and Sag, 1994). Gold-
standard DM training and test data for the Task
were derived from the manually annotated Deep-
Bank Treebank (Flickinger et al., 2012), which
pairs Sections 00–21 of the venerable PTB Wall
Street Journal (WSJ) Corpus with complete ERG-
compatible HPSG syntactico-semantic analyses.
DeepBank as well as the ERG rely on Minimal Re-
cursion Semantics (MRS; Copestake et al., 2005)
for meaning representation, such that the exact
same post-processing steps could be applied to the
parser outputs as were used in originally reducing
the gold-standard MRSs from DeepBank into the
SDP bi-lexical semantic dependency graphs.
Parsing Setup The ERG parsing system is a hy-
brid, combining (a) the hand-built, broad-coverage
ERG with (b) an efficient chart parser for uni-
fication grammars and (c) a conditional proba-
bility distribution over candidate analyses. The
parser most commonly used with the ERG, called
PET (Callmeier, 2002),1 constructs a complete,
</bodyText>
<footnote confidence="0.9971705">
1The SDP test data was parsed using the 1212 release
of the ERG, using PET and converter versions from what
</footnote>
<page confidence="0.962937">
335
</page>
<note confidence="0.7334275">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 335–340,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.996974982142857">
subsumption-based parse forest of partial HPSG
derivations (Oepen and Carroll, 2000), and then
extracts from the forest n-best lists (in globally
correct rank order) of complete analyses according
to a discriminative parse ranking model (Zhang et
al., 2007). For our experiments, we trained the
parse ranker on Sections 00–20 of DeepBank and
otherwise used the default, non-pruning develop-
ment configuration, which is optimized for accu-
racy. In this setup, ERG parsing on average takes
close to ten seconds per sentence.
Post-Parsing Conversion After parsing, MRSs
are reduced to DM bi-lexical semantic dependen-
cies in two steps. First, Oepen and Lønning
(2006) define a conversion to variable-free Ele-
mentary Dependency Structures (EDS), which (a)
maps each predication in the MRS logical-form
meaning representation to a node in a dependency
graph and (b) transforms argument relations rep-
resented by shared logical variables into directed
dependency links between graph nodes. This first
step of the conversion is ‘mildly’ lossy, in that
some scope-related information is discarded; the
EDS graph, however, will contain the same num-
ber of nodes and the same set of argument de-
pendencies as there are predications and semantic
role assignments in the original MRS. In particu-
lar, the EDS may still reflect non-lexical semantic
predications introduced by grammatical construc-
tions like covert quantifiers, nominalization, com-
pounding, or implicit conjunction.2
Second, in another conversion step that is not
information-preserving, the EDS graphs are fur-
ther reduced into strictly bi-lexical form, i.e. a set
of directed, binary dependency relations holding
exclusively between lexical units. This conversion
is defined by Ivanova et al. (2012) and seeks to
(a) project some aspects of construction seman-
tics onto word-to-word dependencies (for example
introducing specific dependency types for com-
pounding or implicit conjunction) and (b) relate
the linguistically informed ERG-internal tokeniza-
tion to the conventions of the PTB.3 Seeing as both
is called the LOGON SVN trunk as of January 2014; see
http://moin.delph-in.net/LogonTop for detail.
2Conversely, semantically vacuous parts of the original
input (e.g. infinitival particles, complementizers, relative pro-
nouns, argument-marking prepositions, auxiliaries, and most
punctuation marks) were not represented in the MRS in the
first place, hence have no bearing on the conversion.
3Adaptations of tokenization encompass splitting ‘multi-
word’ ERG tokens (like such as or ad hoc), as well as ‘hiding’
ERG token boundaries at hyphens or slashes (e.g. 77-year-
conversion steps are by design lossy, DM seman-
tic dependency graphs present a true subset of the
information encoded in the full, original MRS.
</bodyText>
<sectionHeader confidence="0.967545" genericHeader="method">
3 PAS: The Enju Parsing System
</sectionHeader>
<bodyText confidence="0.99884575">
Enju Predicate–Argument Structures (PAS) are
derived from the automatic HPSG-style annota-
tion of the PTB, which was primarily used for the
development of the Enju parsing system4 (Miyao,
2006). A notable feature of this parser is that the
grammar is not developed by hand; instead, the
Enju HPSG-style treebank is first developed, and
the grammar (or, more precisely, the vast major-
ity of lexical entries) is automatically extracted
from the treebank (Miyao et al., 2004). In this
‘projection’ step, PTB annotations such as empty
categories and coindexation are used for deriv-
ing the semantic representations that correspond
to HPSG derivations. Its probabilistic model for
disambiguation is also trained using this treebank
(Miyao and Tsujii, 2008).5
The PAS data set is an extraction of predicate–
argument structures from the Enju HPSG tree-
bank. The Enju parser outputs results in ‘ready-
to-use’ formats like phrase structure trees and
predicate–argument structures, as full HPSG anal-
yses are not friendly to users who are not famil-
iar with the HPSG theory. The gold-standard PAS
target data in the Task was developed using this
function; the conversion program from full HPSG
analyses to predicate–argument structures was ap-
plied to the Enju Treebank.
Predicate–argument structures (PAS) represent
word-to-word semantic dependencies, such as se-
mantic subject and object. Each dependency type
is represented with two elements: the type of the
predicate, such as verb and adjective, and the ar-
gument label, such as ARG1 and ARG2.6
old), which the PTB does not split.
4Seehttp://kmcs.nii.ac.jp/enju/.
5Abstractly similar to the ERG, the annotations of the
Enju treebank instantiate the linguistic theory of HPSG.
However, the two resources have been developed indepen-
dently and implementation details are quite different. The
most significant difference is that the Enju HPSG treebank is
developed by linguistic projection of PTB annotations, and
the Enju parser derived from the treebank; conversely, the
ERG was predominantly manually crafted, and it was later
applied in the DeepBank re-annotation of the WSJ Corpus.
</bodyText>
<footnote confidence="0.98191925">
6Full details of the predicate–argument structures in the
Enju HPSG Treebank, are available in two documents linked
from the Enju web site (see above), viz. the Enju Output
Specification Manual and the XML Format Documentation.
</footnote>
<page confidence="0.998346">
336
</page>
<bodyText confidence="0.99805425">
Parsing Setup Basically we used the publicly
available package of the Enju parser ‘as is’ (see the
above web site). We did not change default pars-
ing parameters (beam width, etc.) and features.
However, the release version of the Enju parser is
trained with the HPSG treebank corresponding to
the Penn Treebank WSJ Sections 2–21, which in-
cludes the test set of the Task (Section 21). There-
fore, we re-trained the Enju parser using Sections
0–20, and used this re-trained parser in preparing
the PAS semantic dependency graphs in this en-
semble submission.
Post-Parsing Conversion The dependency for-
mat of the Enju parser is almost equivalent to what
is provided as the PAS data set in this shared task.
Therefore, the post-parsing conversion for the PAS
data involves only formatting, viz. (a) format con-
version into the tabular file format of the Task; and
(b) insertion of dummy relations for punctuation
tokens ignored in the output of Enju.7
</bodyText>
<sectionHeader confidence="0.969714" genericHeader="method">
4 PCEDT: The Treex Parsing Scenario
</sectionHeader>
<bodyText confidence="0.998944695652174">
The Prague Czech-English Dependency Treebank
(PCEDT; Hajiˇc et al., 2012)8 is a set of parallel de-
pendency trees over the same WSJ texts from the
Penn Treebank, and their Czech translations. Sim-
ilarly to other treebanks in the Prague family, there
are two layers of syntactic annotation: analytical
(a-trees) and tectogrammatical (t-trees). Unlike
for the other two representations used in the Task,
for PCEDT there is no pre-existing parsing system
designed to deliver the full scale of annotations
of the SDP gold-standard data. The closest avail-
able match is a parsing scenario implemented in
the Treex natural language processing framework.
Parsing Setup Treex9 (Popel and Žabokrtský,
2010) is a modular, open-source framework origi-
nally developed for transfer-based machine trans-
lation. It can accomplish any NLP-related task
by sequentially applying to the same piece of data
various blocks of code. Blocks operate on a com-
mon data structure and are chained in scenarios.
Some early experiments with scenarios for tec-
togrammatical analysis of English were described
by Klimeš (2007). It is of interest that they report
</bodyText>
<footnote confidence="0.9930155">
7The Enju parser ignores tokens tagged as ‘.’, while
the PAS representation includes them with dummy relations;
thus, missing periods are inserted in post-processing by com-
parison to the original PTB token sequence.
8See http://ufal.mff.cuni.cz/pcedt2.0/.
9See http://ufal.mff.cuni.cz/treex/.
</footnote>
<figureCaption confidence="0.82129675">
Figure 1: PCEDT asserts two copies of the token
regulate (shown here as ‘regulate’ and ‘E’, under-
lined). Projecting t-nodes onto the original tokens,
required by the SDP data format, means that the
</figureCaption>
<bodyText confidence="0.992845236842106">
E node will be merged with regulate. The edges
going to and from E will now lead to and from reg-
ulate (see the dotted arcs), which results in a cycle.
To get rid of the cycle, we skip E and connect di-
rectly its children, as shown in the final SDP graph
below the sentence.
an F1 score of assigning functors (dependency la-
bels in PCEDT terminology) of 70.3%; however,
their results are not directly comparable to ours.
Due to the modular nature of Treex, there are
various conceivable scenarios to get the t-tree of
a sentence. We use the default scenario that con-
sists of 48 blocks: two initial blocks (reading the
input), one final block (writing the output), two
A2N blocks (named entity recognition), twelve
W2A blocks (dependency parsing at the analytical
layer) and 31 A2T and T2T blocks (creating the
t-tree based on the a-tree).
Most blocks are highly specialized in one par-
ticular subtask (e.g. there is a block just to make
sure that quotation marks are attached to the root
of the quoted subtree). A few blocks are respon-
sible for the bulk of the work. The a-tree is con-
structed by a block that contains the MST Parser
(McDonald et al., 2005), trained on the CoNLL
2007 English data (Nivre et al., 2007), i.e. Sec-
tions 2–11 of the PTB, converted to dependencies.
The annotation style of CoNLL 2007 differs from
PCEDT 2.0, and thus the unlabeled attachment
score of the analytical parser is only 66%.
Obviously one could expect better results if we
retrained the MST Parser directly on the PCEDT
a-trees, and on the whole training data. The only
reason why we did not do so was lack of time.
Our results thus really demonstrate what is avail-
able ‘off-the-shelf’; on the other hand, the PCEDT
component of our ensemble fails to set any ‘upper
bound’ of output quality, as it definitely is not bet-
</bodyText>
<figure confidence="0.9185518">
U.S. should regulate X more stringently than e Y
PAT
MANN CPR
PRED
ACT
ACT
PRED
PAT
CPR
MANN
PAT
CPR
PAT
337
John brought and ate ripe apples and pears
</figure>
<figureCaption confidence="0.976379">
Figure 2: Coordination in PCEDT t-tree (above)
and in the corresponding SDP graph (below).
</figureCaption>
<bodyText confidence="0.971375846153846">
ter informed than the other systems participating
in the Task.
Functor assignment is done heuristically, based
on POS tags and function words. The primary
focus of the scenario was on functors that could
help machine translation, thus it only generated
25 different labels (of the total set of 65 labels in
the SDP gold-standard data)10 and left about 12%
of all nodes without functors. Precision peaks at
78% for ACT(or) relations, while the most fre-
quent error type (besides labelless dependencies)
is a falsely proposed RSTR(iction) relation. Both
ACT and RSTR are among the most frequent de-
pendency types in PCEDT.
Post-Parsing Conversion Once the t-tree has
been constructed, it is converted to the PCEDT
target representation of the Task, using the same
conversion code that was used to prepare the gold-
standard SDP data.11
SDP graphs are defined over surface tokens but
the set of nodes of a t-tree need not correspond
one-to-one to the set of tokens. For example, there
are no t-nodes for punctuation and function words
(except in coordination); these tokens are rendered
as semantically vacuous in SDP, i.e. they do not
participate in edges. On the other hand, t-trees can
contain generated nodes, which represent elided
words and do not correspond to any surface to-
10The system was able to output the following functors (or-
dered in the descending order of their frequency in the sys-
tem output): RSTR, PAT, ACT, CONJ.member, APP, MANN,
LOC, TWHEN, DISJ.member, BEN, RHEM, PREC, ACMP,
MEANS, ADVS.member, CPR, EXT, DIR3, CAUS, COND,
TSIN, REG, DIR2, CNCS, and TTILL.
11In the SDP context, the target representation derived
from the PCEDT is called by the same name as the origi-
nal treebank; but note that the PCEDT semantic dependency
graphs only encode a subset of the information annotated at
the tectogrammatical layer of the full treebank.
</bodyText>
<table confidence="0.998901714285714">
DM PAS PCEDT
LF LM LF LM LF LM
Priberam .8916 .2685 .9176 .3783 .7790 .1068
In-House .9246 .4807 .9206 .4384 .4315 .0030
UF UM UF UM UF UM
Priberam .9032 .2990 .9281 .3924 .8903 .3071
In-House .9349 .5230 .9317 .4429 .6919 .0148
</table>
<tableCaption confidence="0.999938">
Table 1: End-to-end ‘in-house’ parsing results.
</tableCaption>
<bodyText confidence="0.999864458333333">
ken. Most generated nodes are leaves and, thus,
can simply be omitted from the SDP graphs. Other
generated nodes are copies of normal nodes and
they are linked to the same token to which the
source node is mapped. As a result, one token can
appear at several different positions in the tree; if
we project these occurrences into one node, the
graph will contain cycles. We decided to remove
all generated nodes causing cycles. Their chil-
dren are attached to their parents and inherit the
functor of the generated node (Figure 1). The con-
version procedure also removes cycles caused by
more fine-grained tokenization of the t-layer.
Furthermore, t-trees use technical edges to cap-
ture paratactic constructions where the relations
are not ‘true’ dependencies. The conversion pro-
cedure extracts true dependency relations: Each
conjunct is linked to the parent or to a shared child
of the coordination. In addition, there are also
links from the conjunction to the conjuncts and
they are labeled CONJ.m(ember). These links pre-
serve the paratactic structure (which can even be
nested) and the type of coordination. See Figure 2
for an example.
</bodyText>
<sectionHeader confidence="0.998498" genericHeader="evaluation">
5 Results and Reflections
</sectionHeader>
<bodyText confidence="0.953151">
Seeing as our ‘in-house’ parsers are not directly
trained on the semantic dependency graphs pro-
vided for the Task, but rather are built from ad-
ditional linguistic resources, we submitted results
from the parsing pipelines sketched in Sections 2
to 4 above to the open SDP track. Table 1
summarizes parser performance in terms of la-
beled and unlabeled F1 (LF and UF)12 and full-
sentence exact match (LM and UM), comparing
to the best-performing submission (dubbed Prib-
eram; Martins and Almeida, 2014) to this track.
Judging by the official SDP evaluation metric, av-
erage labeled F1 over the three representations,
our ensemble ranked last among six participating
12Our ensemble members exhibit comparatively small dif-
ferences in recall vs. precision.
</bodyText>
<figure confidence="0.9969626">
ACT
PRED.m
CONJ
PRED.m
CONJ
RSTR
PAT.m PAT.m
ACT
TOP TOP
CONJ.m
ACT
CONJ.m
PAT
PAT
PAT
PAT
RSTR
CONJ.m
RSTR
CONJ.m
</figure>
<page confidence="0.994834">
338
</page>
<bodyText confidence="0.9997938">
teams; in terms of unlabeled average F1, the ‘in-
house’ submission achieved the fourth rank.
As explained in the task description (Oepen et
al., 2014), parts of the WSJ Corpus were excluded
from the SDP training and testing data because
of gaps in the DeepBank and Enju treebanks, and
to exclude cyclic dependency graphs, which can
sometimes arise in the DM and PCEDT conver-
sions. For these reasons, one has to allow for the
possibility that the testing data is positively bi-
ased towards our ensemble members.13 But even
with this caveat, it seems fair to observe that the
ERG and Enju parsers both are very competitive
for the DM and PAS target representations, respec-
tively, specifically so when judged in exact match
scores. A possible explanation for these results
lies in the depth of grammatical information avail-
able to these parsers, where DM or PAS seman-
tic dependency graphs are merely a simpliefied
view on the complete underlying HPSG analyses.
These parsers have performed well in earlier con-
trastive evaluation too (Miyao et al., 2007; Bender
et al., 2011; Ivanova et al., 2013; inter alios).
Results for the Treex English parsing scenario,
on the other hand, show that this ensemble mem-
ber is not fine-tuned for the PCEDT target rep-
resentation; due to the reasons mentioned above,
its performance even falls behind the shared task
baseline. As is evident from the comparison of
labeled vs. unlabeled F1 scores, (a) the PCEDT
parser is comparatively stronger at recovering se-
mantic dependency structure than at assigning la-
bels, and (b) about the same appears to be the case
for the best-performing Priberam system (on this
target representation).
</bodyText>
<sectionHeader confidence="0.994954" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.907555117647059">
Data preparation and large-scale parsing in the
DM target representation was supported through
access to the ABEL high-performance computing
facilities at the University of Oslo, and we ac-
knowledge the Scientific Computing staff at UiO,
the Norwegian Metacenter for Computational Sci-
ence, and the Norwegian tax payers. This project
has been supported by the infrastructural funding
13There is no specific evidence that the WSJ sentences ex-
cluded in the Task for technical issues in either of the under-
lying treebanks or conversion procedures would be compara-
tively much easier to parse for other submissions than for the
members of our ‘in-house’ ensemble, but unlike other sys-
tems these parsers ‘had a vote’ in the selection of the data,
particularly so for the DM and PAS target representations.
by the Ministry of Education, Youth and Sports of
the Czech Republic (CEP ID LM2010013).
</bodyText>
<sectionHeader confidence="0.97518" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.698213857142857">
Bender, E. M., Flickinger, D., Oepen, S., and
Zhang, Y. (2011). Parser evaluation over local
and non-local deep dependencies in a large cor-
pus. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Pro-
cessing (p. 397–408). Edinburgh, Scotland,
UK.
Callmeier, U. (2002). Preprocessing and encoding
techniques in PET. In S. Oepen, D. Flickinger,
J. Tsujii, and H. Uszkoreit (Eds.), Collabora-
tive language engineering. A case study in effi-
cient grammar-based processing (p. 127 –140).
Stanford, CA: CSLI Publications.
Copestake, A., Flickinger, D., Pollard, C., and
Sag, I. A. (2005). Minimal Recursion Seman-
tics. An introduction. Research on Language
and Computation, 3(4), 281– 332.
Flickinger, D. (2000). On building a more ef-
ficient grammar by exploiting types. Natural
Language Engineering, 6 (1), 15 – 28.
Flickinger, D., Zhang, Y., and Kordoni, V. (2012).
DeepBank. A dynamically annotated treebank
of the Wall Street Journal. In Proceedings of the
11th International Workshop on Treebanks and
Linguistic Theories (p. 85 – 96). Lisbon, Portu-
gal: Edições Colibri.
Hajiˇc, J., Hajiˇcová, E., Panevová, J., Sgall, P.,
Bojar, O., Cinková, S., ... Žabokrtský, Z.
(2012). Announcing Prague Czech-English De-
pendency Treebank 2.0. In Proceedings of the
8th International Conference on Language Re-
sources and Evaluation (p. 3153 – 3160). Istan-
bul, Turkey.
Ivanova, A., Oepen, S., Dridan, R., Flickinger, D.,
and Øvrelid, L. (2013). On different approaches
to syntactic analysis into bi-lexical dependen-
cies. An empirical comparison of direct, PCFG-
based, and HPSG-based parsers. In Proceedings
of the 13th International Conference on Parsing
Technologies (p. 63–72). Nara, Japan.
Ivanova, A., Oepen, S., Øvrelid, L., and
Flickinger, D. (2012). Who did what to whom?
</reference>
<page confidence="0.998154">
339
</page>
<bodyText confidence="0.930794666666667">
A contrastive study of syntacto-semantic depen-
dencies. In Proceedings of the Sixth Linguistic
Annotation Workshop (p. 2–11). Jeju, Republic
of Korea.
Klimeš, V. (2007). Transformation-based tec-
togrammatical dependency analysis of English.
In V. Matoušek and P. Mautner (Eds.), Text,
speech and dialogue 2007, LNAI 4629 (p. 15 –
22). Berlin / Heidelberg, Germany: Springer.
</bodyText>
<reference confidence="0.999625342105263">
Marcus, M., Santorini, B., and Marcinkiewicz,
M. A. (1993). Building a large annotated cor-
pora of English: The Penn Treebank. Computa-
tional Linguistics, 19, 313 – 330.
Martins, A. F. T., and Almeida, M. S. C. (2014).
Priberam. A turbo semantic parser with second
order features. In Proceedings of the 8th In-
ternational Workshop on Semantic Evaluation.
Dublin, Ireland.
McDonald, R., Pereira, F., Ribarov, K., and Hajiˇc,
J. (2005). Non-projective dependency parsing
using spanning tree algorithms. In Proceedings
of the Human Language Technology Conference
and Conference on Empirical Methods in Nat-
ural Language Processing (p. 523 – 530). Van-
couver, British Columbia, Canada.
Miyao, Y. (2006). From linguistic theory to
syntactic analysis. Corpus-oriented grammar
development and feature forest model. Doc-
toral Dissertation, University of Tokyo, Tokyo,
Japan.
Miyao, Y., Ninomiya, T., and Tsujii, J. (2004).
Corpus-oriented grammar development for ac-
quiring a Head-Driven Phrase Structure Gram-
mar from the Penn Treebank. In Proceedings of
the 1st International Joint Conference on Natu-
ral Language Processing (p. 684 – 693).
Miyao, Y., Sagae, K., and Tsujii, J. (2007).
Towards framework-independent evaluation of
deep linguistic parsers. In Proceedings of
the 2007 Workshop on Grammar Engineering
across Frameworks (p. 238 – 258). Palo Alto,
California.
Miyao, Y., and Tsujii, J. (2008). Feature forest
models for probabilistic HPSG parsing. Com-
putational Linguistics, 34(1), 35 – 80.
Nivre, J., Hall, J., Kübler, S., McDonald, R., Nils-
son, J., Riedel, S., and Yuret, D. (2007). The
CoNLL 2007 shared task on dependency pars-
ing. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Lan-
guage Processing and Conference on Natural
Language Learning (p. 915 – 932). Prague,
Czech Republic.
Oepen, S., and Carroll, J. (2000). Ambiguity
packing in constraint-based parsing. Practical
results. In Proceedings of the 1st Meeting of the
North American Chapter of the Association for
Computational Linguistics (p. 162–169). Seat-
tle, WA, USA.
Oepen, S., Kuhlmann, M., Miyao, Y., Zeman, D.,
Flickinger, D., Hajiˇc, J., ... Zhang, Y. (2014).
SemEval 2014 Task 8. Broad-coverage seman-
tic dependency parsing. In Proceedings of the
8th International Workshop on Semantic Evalu-
ation. Dublin, Ireland.
Oepen, S., and Lønning, J. T. (2006).
Discriminant-based MRS banking. In Proceed-
ings of the 5th International Conference on
Language Resources and Evaluation (p. 1250 –
1255). Genoa, Italy.
Palmer, M., Gildea, D., and Kingsbury, P. (2005).
The Proposition Bank. A corpus annotated with
semantic roles. Computational Linguistics,
31(1), 71–106.
Pollard, C., and Sag, I. A. (1994). Head-Driven
Phrase Structure Grammar. Chicago, USA:
The University of Chicago Press.
Popel, M., and Žabokrtský, Z. (2010). TectoMT.
Modular NLP framework. Advances in Natural
Language Processing, 293 – 304.
Zhang, Y., Oepen, S., and Carroll, J. (2007).
Efficiency in unification-based n-best parsing.
In Proceedings of the 10th International Con-
ference on Parsing Technologies (p. 48–59).
Prague, Czech Republic.
</reference>
<page confidence="0.998293">
340
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.9999">In-House: An Ensemble of Pre-Existing Off-the-Shelf Parsers</title>
<author confidence="0.955093">Stephan</author>
<author confidence="0.955093">Daniel</author>
<affiliation confidence="0.997768">Institute of Informatics, Tokyo</affiliation>
<address confidence="0.791988">of Oslo, Department of</address>
<affiliation confidence="0.773199">University, Department of Linguistics</affiliation>
<address confidence="0.290949">Charles University in Prague, Faculty of Mathematics and Physics, Institute of Formal and Applied</address>
<abstract confidence="0.985919180505416">submission to the of Task 8 at SemEval 2014 seeks to connect the Task to pre-existing, ‘in-house’ parsing systems for the same types of target semantic dependency graphs. 1 Background and Motivation The three target representations for Task 8 at 2014, Semantic Depen- Parsing Oepen et al., 2014), are rooted in language engineering efforts that have been under continuous development for at least the past decade. The gold-standard semantic dependency graphs used for training and testing in the Task result from largely manual annotation, in part re-purposing and adapting resources like the Penn Treebank (PTB; Marcus et al., 1993), Prop- Bank (Palmer et al., 2005), and others. But the groups who prepared the SDP target data have also worked in parallel on automated parsing systems for these representations. Thus, for each of the target representations, there is a pre-existing parser, often developed in parallel to the creation of the target dependency graphs, viz. (a) for the DM representation, the parser of the hand-engineered LinGO English Resource Grammar (ERG; Flickinger, 2000); (b) for PAS, the Enju parsing system (Miyao, 2006), with its probabilistic HPSG acquired through linguistic projection of the PTB; and (c) for PCEDT, the scenario for English analysis within the Treex framework (Popel and Žabokrtský, 2010), combining data-driven dependency parsing with handengineered tectogrammatical conversion. At least This work is licenced under a Creative Commons Attribution 4.0 International License; page numbers and the footer are added by the organizers. creativecommons.org/licenses/by/4.0/ for DM and PAS, these parsers have been extensively engineered and applied successfully in a variety of applications, hence represent relevant points of comparison. Through this ‘in-house’ submission (of our ‘own’ parsers to our ‘own’ task), we hope to facilitate the comparison of different approaches submitted to the Task with this pre-existing line of parser engineering. 2 DM: The English Resource Grammar Semantic dependency graphs in the DM target rep- MRS-Derived Bi-Lexical stem from a two-step ‘reduction’ (simplification) of the underspecified logicalform meaning representations output natively by the ERG parser, which implements the linguistic framework of Head-Driven Phrase Structure Grammar (HPSG; Pollard and Sag, 1994). Goldstandard DM training and test data for the Task were derived from the manually annotated Deep- Bank Treebank (Flickinger et al., 2012), which pairs Sections 00–21 of the venerable PTB Wall Street Journal (WSJ) Corpus with complete ERGcompatible HPSG syntactico-semantic analyses. DeepBank as well as the ERG rely on Minimal Recursion Semantics (MRS; Copestake et al., 2005) for meaning representation, such that the exact same post-processing steps could be applied to the parser outputs as were used in originally reducing the gold-standard MRSs from DeepBank into the SDP bi-lexical semantic dependency graphs. Setup ERG parsing system is a hybrid, combining (a) the hand-built, broad-coverage ERG with (b) an efficient chart parser for unification grammars and (c) a conditional probability distribution over candidate analyses. The parser most commonly used with the ERG, called (Callmeier, constructs a complete, SDP test data was parsed using the 1212 release of the ERG, using PET and converter versions from what 335 of the 8th International Workshop on Semantic Evaluation (SemEval pages Dublin, Ireland, August 23-24, 2014. subsumption-based parse forest of partial HPSG derivations (Oepen and Carroll, 2000), and then extracts from the forest n-best lists (in globally correct rank order) of complete analyses according to a discriminative parse ranking model (Zhang et al., 2007). For our experiments, we trained the parse ranker on Sections 00–20 of DeepBank and otherwise used the default, non-pruning development configuration, which is optimized for accuracy. In this setup, ERG parsing on average takes close to ten seconds per sentence. Conversion parsing, MRSs are reduced to DM bi-lexical semantic dependencies in two steps. First, Oepen and Lønning define a conversion to variable-free Ele- Dependency Structures which (a) maps each predication in the MRS logical-form meaning representation to a node in a dependency graph and (b) transforms argument relations represented by shared logical variables into directed dependency links between graph nodes. This first step of the conversion is ‘mildly’ lossy, in that some scope-related information is discarded; the EDS graph, however, will contain the same number of nodes and the same set of argument dependencies as there are predications and semantic role assignments in the original MRS. In particular, the EDS may still reflect non-lexical semantic predications introduced by grammatical constructions like covert quantifiers, nominalization, comor implicit Second, in another conversion step that is not information-preserving, the EDS graphs are further reduced into strictly bi-lexical form, i.e. a set of directed, binary dependency relations holding exclusively between lexical units. This conversion is defined by Ivanova et al. (2012) and seeks to (a) project some aspects of construction semantics onto word-to-word dependencies (for example introducing specific dependency types for compounding or implicit conjunction) and (b) relate the linguistically informed ERG-internal tokenizato the conventions of the Seeing as both is called the LOGON SVN trunk as of January 2014; see detail. semantically vacuous parts of the original input (e.g. infinitival particles, complementizers, relative pronouns, argument-marking prepositions, auxiliaries, and most punctuation marks) were not represented in the MRS in the first place, hence have no bearing on the conversion. of tokenization encompass splitting ‘multi- ERG tokens (like as as well as ‘hiding’ token boundaries at hyphens or slashes (e.g. 77-yearconversion steps are by design lossy, DM semantic dependency graphs present a true subset of the information encoded in the full, original MRS. 3 PAS: The Enju Parsing System Structures are derived from the automatic HPSG-style annotation of the PTB, which was primarily used for the of the Enju parsing (Miyao, 2006). A notable feature of this parser is that the grammar is not developed by hand; instead, the Enju HPSG-style treebank is first developed, and the grammar (or, more precisely, the vast majority of lexical entries) is automatically extracted from the treebank (Miyao et al., 2004). In this ‘projection’ step, PTB annotations such as empty categories and coindexation are used for deriving the semantic representations that correspond to HPSG derivations. Its probabilistic model for disambiguation is also trained using this treebank and Tsujii, The PAS data set is an extraction of predicate– argument structures from the Enju HPSG treebank. The Enju parser outputs results in ‘readyto-use’ formats like phrase structure trees and predicate–argument structures, as full HPSG analyses are not friendly to users who are not familiar with the HPSG theory. The gold-standard PAS target data in the Task was developed using this function; the conversion program from full HPSG analyses to predicate–argument structures was applied to the Enju Treebank. Predicate–argument structures (PAS) represent word-to-word semantic dependencies, such as semantic subject and object. Each dependency type is represented with two elements: the type of the predicate, such as verb and adjective, and the arlabel, such as which the PTB does not split. similar to the ERG, the annotations of the Enju treebank instantiate the linguistic theory of HPSG. However, the two resources have been developed independently and implementation details are quite different. The most significant difference is that the Enju HPSG treebank is developed by linguistic projection of PTB annotations, and the Enju parser derived from the treebank; conversely, the ERG was predominantly manually crafted, and it was later applied in the DeepBank re-annotation of the WSJ Corpus. details of the predicate–argument structures in the Enju HPSG Treebank, are available in two documents linked the Enju web site (see above), viz. the Enju Manual the Format 336 Setup we used the publicly available package of the Enju parser ‘as is’ (see the above web site). We did not change default parsing parameters (beam width, etc.) and features. However, the release version of the Enju parser is trained with the HPSG treebank corresponding to the Penn Treebank WSJ Sections 2–21, which includes the test set of the Task (Section 21). Therefore, we re-trained the Enju parser using Sections 0–20, and used this re-trained parser in preparing the PAS semantic dependency graphs in this ensemble submission. Conversion dependency format of the Enju parser is almost equivalent to what is provided as the PAS data set in this shared task. Therefore, the post-parsing conversion for the PAS data involves only formatting, viz. (a) format conversion into the tabular file format of the Task; and (b) insertion of dummy relations for punctuation ignored in the output of 4 PCEDT: The Treex Parsing Scenario Czech-English Dependency Treebank Hajiˇc et al., a set of parallel dependency trees over the same WSJ texts from the Penn Treebank, and their Czech translations. Similarly to other treebanks in the Prague family, there two layers of syntactic annotation: and Unlike for the other two representations used in the Task, for PCEDT there is no pre-existing parsing system designed to deliver the full scale of annotations of the SDP gold-standard data. The closest available match is a parsing scenario implemented in the Treex natural language processing framework. Setup and Žabokrtský, 2010) is a modular, open-source framework originally developed for transfer-based machine translation. It can accomplish any NLP-related task by sequentially applying to the same piece of data code. Blocks operate on a comdata structure and are chained in Some early experiments with scenarios for tectogrammatical analysis of English were described by Klimeš (2007). It is of interest that they report Enju parser ignores tokens tagged as while the PAS representation includes them with dummy relations; thus, missing periods are inserted in post-processing by comparison to the original PTB token sequence. Figure 1: PCEDT asserts two copies of the token here as ‘regulate’ and underlined). Projecting t-nodes onto the original tokens, required by the SDP data format, means that the node will be merged with The edges to and from E will now lead to and from regthe dotted arcs), which results in a cycle. To get rid of the cycle, we skip E and connect directly its children, as shown in the final SDP graph below the sentence. of assigning labels in PCEDT terminology) of 70.3%; however, their results are not directly comparable to ours. Due to the modular nature of Treex, there are various conceivable scenarios to get the t-tree of a sentence. We use the default scenario that consists of 48 blocks: two initial blocks (reading the input), one final block (writing the output), two (named entity recognition), twelve (dependency parsing at the analytical and 31 (creating the t-tree based on the a-tree). Most blocks are highly specialized in one particular subtask (e.g. there is a block just to make sure that quotation marks are attached to the root of the quoted subtree). A few blocks are responsible for the bulk of the work. The a-tree is constructed by a block that contains the MST Parser (McDonald et al., 2005), trained on the CoNLL 2007 English data (Nivre et al., 2007), i.e. Sections 2–11 of the PTB, converted to dependencies. The annotation style of CoNLL 2007 differs from PCEDT 2.0, and thus the unlabeled attachment score of the analytical parser is only 66%. Obviously one could expect better results if we retrained the MST Parser directly on the PCEDT a-trees, and on the whole training data. The only reason why we did not do so was lack of time. Our results thus really demonstrate what is available ‘off-the-shelf’; on the other hand, the PCEDT component of our ensemble fails to set any ‘upper of output quality, as it definitely is not betshould regulate stringently than PAT</abstract>
<title confidence="0.7801783">MANN CPR PRED ACT ACT PRED PAT CPR MANN PAT CPR</title>
<author confidence="0.420959">PAT</author>
<pubnum confidence="0.181981">337</pubnum>
<abstract confidence="0.93843551552795">John brought and ate ripe apples and pears Figure 2: Coordination in PCEDT t-tree (above) and in the corresponding SDP graph (below). ter informed than the other systems participating in the Task. Functor assignment is done heuristically, based on POS tags and function words. The primary focus of the scenario was on functors that could help machine translation, thus it only generated 25 different labels (of the total set of 65 labels in SDP gold-standard and left about 12% of all nodes without functors. Precision peaks at for relations, while the most frequent error type (besides labelless dependencies) a falsely proposed relation. Both among the most frequent dependency types in PCEDT. Conversion the t-tree has been constructed, it is converted to the PCEDT target representation of the Task, using the same conversion code that was used to prepare the gold- SDP SDP graphs are defined over surface tokens but the set of nodes of a t-tree need not correspond one-to-one to the set of tokens. For example, there are no t-nodes for punctuation and function words (except in coordination); these tokens are rendered as semantically vacuous in SDP, i.e. they do not participate in edges. On the other hand, t-trees can which represent elided and do not correspond to any surface tosystem was able to output the following functors (ordered in the descending order of their frequency in the sysoutput): and the SDP context, the target representation derived from the PCEDT is called by the same name as the original treebank; but note that the PCEDT semantic dependency graphs only encode a subset of the information annotated at the tectogrammatical layer of the full treebank. DM PAS PCEDT LF LM LF LM LF LM Priberam .8916 .2685 .9176 .3783 .7790 .1068 In-House .9246 .4807 .9206 .4384 .4315 .0030 UF UM UF UM UF UM Priberam .9032 .2990 .9281 .3924 .8903 .3071 In-House .9349 .5230 .9317 .4429 .6919 .0148 Table 1: End-to-end ‘in-house’ parsing results. ken. Most generated nodes are leaves and, thus, can simply be omitted from the SDP graphs. Other nodes are normal nodes and they are linked to the same token to which the source node is mapped. As a result, one token can appear at several different positions in the tree; if we project these occurrences into one node, the graph will contain cycles. We decided to remove all generated nodes causing cycles. Their children are attached to their parents and inherit the functor of the generated node (Figure 1). The conversion procedure also removes cycles caused by more fine-grained tokenization of the t-layer. Furthermore, t-trees use technical edges to capture paratactic constructions where the relations are not ‘true’ dependencies. The conversion procedure extracts true dependency relations: Each conjunct is linked to the parent or to a shared child of the coordination. In addition, there are also links from the conjunction to the conjuncts and are labeled These links preserve the paratactic structure (which can even be nested) and the type of coordination. See Figure 2 for an example. 5 Results and Reflections Seeing as our ‘in-house’ parsers are not directly trained on the semantic dependency graphs provided for the Task, but rather are built from additional linguistic resources, we submitted results from the parsing pipelines sketched in Sections 2 4 above to the track. Table 1 summarizes parser performance in terms of laand unlabeled and fullexact match comparing to the best-performing submission (dubbed Priberam; Martins and Almeida, 2014) to this track. Judging by the official SDP evaluation metric, avlabeled the three representations, our ensemble ranked last among six participating ensemble members exhibit comparatively small differences in recall vs. precision. ACT PRED.m CONJ PRED.m CONJ RSTR PAT.m PAT.m ACT TOP TOP CONJ.m ACT CONJ.m PAT PAT PAT PAT RSTR CONJ.m RSTR CONJ.m 338 in terms of unlabeled average the ‘inhouse’ submission achieved the fourth rank. As explained in the task description (Oepen et al., 2014), parts of the WSJ Corpus were excluded from the SDP training and testing data because of gaps in the DeepBank and Enju treebanks, and to exclude cyclic dependency graphs, which can sometimes arise in the DM and PCEDT conversions. For these reasons, one has to allow for the possibility that the testing data is positively bitowards our ensemble But even with this caveat, it seems fair to observe that the ERG and Enju parsers both are very competitive for the DM and PAS target representations, respectively, specifically so when judged in exact match scores. A possible explanation for these results lies in the depth of grammatical information available to these parsers, where DM or PAS semantic dependency graphs are merely a simpliefied view on the complete underlying HPSG analyses. These parsers have performed well in earlier contrastive evaluation too (Miyao et al., 2007; Bender et al., 2011; Ivanova et al., 2013; inter alios). Results for the Treex English parsing scenario, on the other hand, show that this ensemble member is not fine-tuned for the PCEDT target representation; due to the reasons mentioned above, its performance even falls behind the shared task baseline. As is evident from the comparison of vs. unlabeled (a) the PCEDT parser is comparatively stronger at recovering sedependency at assigning laand (b) about the same appears to be the case for the best-performing Priberam system (on this target representation). Acknowledgements Data preparation and large-scale parsing in the DM target representation was supported through to the computing facilities at the University of Oslo, and we acknowledge the Scientific Computing staff at UiO, the Norwegian Metacenter for Computational Science, and the Norwegian tax payers. This project has been supported by the infrastructural funding is no specific evidence that the WSJ sentences excluded in the Task for technical issues in either of the underlying treebanks or conversion procedures would be comparatively much easier to parse for other submissions than for the members of our ‘in-house’ ensemble, but unlike other systems these parsers ‘had a vote’ in the selection of the data, particularly so for the DM and PAS target representations.</abstract>
<note confidence="0.496932111111111">by the Ministry of Education, Youth and Sports of the Czech Republic (CEP ID LM2010013). References Bender, E. M., Flickinger, D., Oepen, S., and Zhang, Y. (2011). Parser evaluation over local and non-local deep dependencies in a large cor- In of the 2011 Conference on Empirical Methods in Natural Language Pro- 397–408). Edinburgh, Scotland, UK. Callmeier, U. (2002). Preprocessing and encoding techniques in PET. In S. Oepen, D. Flickinger, Tsujii, and H. Uszkoreit (Eds.), Collaborative language engineering. A case study in effigrammar-based processing 127 –140). Stanford, CA: CSLI Publications. Copestake, A., Flickinger, D., Pollard, C., and Sag, I. A. (2005). Minimal Recursion Seman-</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E M Bender</author>
<author>D Flickinger</author>
<author>S Oepen</author>
<author>Y Zhang</author>
</authors>
<title>Parser evaluation over local and non-local deep dependencies in a large corpus.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (p. 397–408).</booktitle>
<location>Edinburgh, Scotland, UK.</location>
<contexts>
<context position="19088" citStr="Bender et al., 2011" startWordPosition="3026" endWordPosition="3029">ing data is positively biased towards our ensemble members.13 But even with this caveat, it seems fair to observe that the ERG and Enju parsers both are very competitive for the DM and PAS target representations, respectively, specifically so when judged in exact match scores. A possible explanation for these results lies in the depth of grammatical information available to these parsers, where DM or PAS semantic dependency graphs are merely a simpliefied view on the complete underlying HPSG analyses. These parsers have performed well in earlier contrastive evaluation too (Miyao et al., 2007; Bender et al., 2011; Ivanova et al., 2013; inter alios). Results for the Treex English parsing scenario, on the other hand, show that this ensemble member is not fine-tuned for the PCEDT target representation; due to the reasons mentioned above, its performance even falls behind the shared task baseline. As is evident from the comparison of labeled vs. unlabeled F1 scores, (a) the PCEDT parser is comparatively stronger at recovering semantic dependency structure than at assigning labels, and (b) about the same appears to be the case for the best-performing Priberam system (on this target representation). Acknowl</context>
</contexts>
<marker>Bender, Flickinger, Oepen, Zhang, 2011</marker>
<rawString>Bender, E. M., Flickinger, D., Oepen, S., and Zhang, Y. (2011). Parser evaluation over local and non-local deep dependencies in a large corpus. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (p. 397–408). Edinburgh, Scotland, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Callmeier</author>
</authors>
<title>Preprocessing and encoding techniques in</title>
<date>2002</date>
<publisher>CSLI Publications.</publisher>
<location>Stanford, CA:</location>
<contexts>
<context position="3797" citStr="Callmeier, 2002" startWordPosition="561" endWordPosition="562">the ERG rely on Minimal Recursion Semantics (MRS; Copestake et al., 2005) for meaning representation, such that the exact same post-processing steps could be applied to the parser outputs as were used in originally reducing the gold-standard MRSs from DeepBank into the SDP bi-lexical semantic dependency graphs. Parsing Setup The ERG parsing system is a hybrid, combining (a) the hand-built, broad-coverage ERG with (b) an efficient chart parser for unification grammars and (c) a conditional probability distribution over candidate analyses. The parser most commonly used with the ERG, called PET (Callmeier, 2002),1 constructs a complete, 1The SDP test data was parsed using the 1212 release of the ERG, using PET and converter versions from what 335 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 335–340, Dublin, Ireland, August 23-24, 2014. subsumption-based parse forest of partial HPSG derivations (Oepen and Carroll, 2000), and then extracts from the forest n-best lists (in globally correct rank order) of complete analyses according to a discriminative parse ranking model (Zhang et al., 2007). For our experiments, we trained the parse ranker on Sections 00–20</context>
</contexts>
<marker>Callmeier, 2002</marker>
<rawString>Callmeier, U. (2002). Preprocessing and encoding techniques in PET. In S. Oepen, D. Flickinger, J. Tsujii, and H. Uszkoreit (Eds.), Collaborative language engineering. A case study in efficient grammar-based processing (p. 127 –140). Stanford, CA: CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Copestake</author>
<author>D Flickinger</author>
<author>C Pollard</author>
<author>I A Sag</author>
</authors>
<title>Minimal Recursion Semantics. An introduction.</title>
<date>2005</date>
<journal>Research on Language and Computation,</journal>
<volume>3</volume>
<issue>4</issue>
<pages>281--332</pages>
<contexts>
<context position="3254" citStr="Copestake et al., 2005" startWordPosition="476" endWordPosition="479">m a two-step ‘reduction’ (simplification) of the underspecified logicalform meaning representations output natively by the ERG parser, which implements the linguistic framework of Head-Driven Phrase Structure Grammar (HPSG; Pollard and Sag, 1994). Goldstandard DM training and test data for the Task were derived from the manually annotated DeepBank Treebank (Flickinger et al., 2012), which pairs Sections 00–21 of the venerable PTB Wall Street Journal (WSJ) Corpus with complete ERGcompatible HPSG syntactico-semantic analyses. DeepBank as well as the ERG rely on Minimal Recursion Semantics (MRS; Copestake et al., 2005) for meaning representation, such that the exact same post-processing steps could be applied to the parser outputs as were used in originally reducing the gold-standard MRSs from DeepBank into the SDP bi-lexical semantic dependency graphs. Parsing Setup The ERG parsing system is a hybrid, combining (a) the hand-built, broad-coverage ERG with (b) an efficient chart parser for unification grammars and (c) a conditional probability distribution over candidate analyses. The parser most commonly used with the ERG, called PET (Callmeier, 2002),1 constructs a complete, 1The SDP test data was parsed u</context>
</contexts>
<marker>Copestake, Flickinger, Pollard, Sag, 2005</marker>
<rawString>Copestake, A., Flickinger, D., Pollard, C., and Sag, I. A. (2005). Minimal Recursion Semantics. An introduction. Research on Language and Computation, 3(4), 281– 332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Flickinger</author>
</authors>
<title>On building a more efficient grammar by exploiting types.</title>
<date>2000</date>
<journal>Natural Language Engineering,</journal>
<volume>6</volume>
<issue>1</issue>
<pages>15--28</pages>
<contexts>
<context position="1564" citStr="Flickinger, 2000" startWordPosition="229" endWordPosition="230">ng and testing in the Task result from largely manual annotation, in part re-purposing and adapting resources like the Penn Treebank (PTB; Marcus et al., 1993), PropBank (Palmer et al., 2005), and others. But the groups who prepared the SDP target data have also worked in parallel on automated parsing systems for these representations. Thus, for each of the target representations, there is a pre-existing parser, often developed in parallel to the creation of the target dependency graphs, viz. (a) for the DM representation, the parser of the hand-engineered LinGO English Resource Grammar (ERG; Flickinger, 2000); (b) for PAS, the Enju parsing system (Miyao, 2006), with its probabilistic HPSG acquired through linguistic projection of the PTB; and (c) for PCEDT, the scenario for English analysis within the Treex framework (Popel and Žabokrtský, 2010), combining data-driven dependency parsing with handengineered tectogrammatical conversion. At least This work is licenced under a Creative Commons Attribution 4.0 International License; page numbers and the proceedings footer are added by the organizers. http:// creativecommons.org/licenses/by/4.0/ for DM and PAS, these parsers have been extensively engine</context>
</contexts>
<marker>Flickinger, 2000</marker>
<rawString>Flickinger, D. (2000). On building a more efficient grammar by exploiting types. Natural Language Engineering, 6 (1), 15 – 28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Flickinger</author>
<author>Y Zhang</author>
<author>V Kordoni</author>
</authors>
<title>DeepBank. A dynamically annotated treebank of the Wall Street Journal.</title>
<date>2012</date>
<booktitle>In Proceedings of the 11th International Workshop on Treebanks and Linguistic Theories (p. 85 – 96).</booktitle>
<location>Lisbon, Portugal: Edições Colibri.</location>
<contexts>
<context position="3015" citStr="Flickinger et al., 2012" startWordPosition="439" endWordPosition="442">ferent approaches submitted to the Task with this pre-existing line of parser engineering. 2 DM: The English Resource Grammar Semantic dependency graphs in the DM target representation, DELPH-IN MRS-Derived Bi-Lexical Dependencies, stem from a two-step ‘reduction’ (simplification) of the underspecified logicalform meaning representations output natively by the ERG parser, which implements the linguistic framework of Head-Driven Phrase Structure Grammar (HPSG; Pollard and Sag, 1994). Goldstandard DM training and test data for the Task were derived from the manually annotated DeepBank Treebank (Flickinger et al., 2012), which pairs Sections 00–21 of the venerable PTB Wall Street Journal (WSJ) Corpus with complete ERGcompatible HPSG syntactico-semantic analyses. DeepBank as well as the ERG rely on Minimal Recursion Semantics (MRS; Copestake et al., 2005) for meaning representation, such that the exact same post-processing steps could be applied to the parser outputs as were used in originally reducing the gold-standard MRSs from DeepBank into the SDP bi-lexical semantic dependency graphs. Parsing Setup The ERG parsing system is a hybrid, combining (a) the hand-built, broad-coverage ERG with (b) an efficient </context>
</contexts>
<marker>Flickinger, Zhang, Kordoni, 2012</marker>
<rawString>Flickinger, D., Zhang, Y., and Kordoni, V. (2012). DeepBank. A dynamically annotated treebank of the Wall Street Journal. In Proceedings of the 11th International Workshop on Treebanks and Linguistic Theories (p. 85 – 96). Lisbon, Portugal: Edições Colibri.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hajiˇc</author>
<author>E Hajiˇcová</author>
<author>J Panevová</author>
<author>P Sgall</author>
<author>O Bojar</author>
<author>S Cinková</author>
</authors>
<title>Announcing Prague Czech-English Dependency Treebank 2.0.</title>
<date>2012</date>
<booktitle>In Proceedings of the 8th International Conference on Language Resources and Evaluation (p. 3153 – 3160).</booktitle>
<location>Istanbul, Turkey.</location>
<marker>Hajiˇc, Hajiˇcová, Panevová, Sgall, Bojar, Cinková, 2012</marker>
<rawString>Hajiˇc, J., Hajiˇcová, E., Panevová, J., Sgall, P., Bojar, O., Cinková, S., ... Žabokrtský, Z. (2012). Announcing Prague Czech-English Dependency Treebank 2.0. In Proceedings of the 8th International Conference on Language Resources and Evaluation (p. 3153 – 3160). Istanbul, Turkey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ivanova</author>
<author>S Oepen</author>
<author>R Dridan</author>
<author>D Flickinger</author>
<author>L Øvrelid</author>
</authors>
<title>On different approaches to syntactic analysis into bi-lexical dependencies. An empirical comparison of direct, PCFGbased, and HPSG-based parsers.</title>
<date>2013</date>
<booktitle>In Proceedings of the 13th International Conference on Parsing Technologies (p.</booktitle>
<pages>63--72</pages>
<location>Nara, Japan.</location>
<contexts>
<context position="19110" citStr="Ivanova et al., 2013" startWordPosition="3030" endWordPosition="3033">y biased towards our ensemble members.13 But even with this caveat, it seems fair to observe that the ERG and Enju parsers both are very competitive for the DM and PAS target representations, respectively, specifically so when judged in exact match scores. A possible explanation for these results lies in the depth of grammatical information available to these parsers, where DM or PAS semantic dependency graphs are merely a simpliefied view on the complete underlying HPSG analyses. These parsers have performed well in earlier contrastive evaluation too (Miyao et al., 2007; Bender et al., 2011; Ivanova et al., 2013; inter alios). Results for the Treex English parsing scenario, on the other hand, show that this ensemble member is not fine-tuned for the PCEDT target representation; due to the reasons mentioned above, its performance even falls behind the shared task baseline. As is evident from the comparison of labeled vs. unlabeled F1 scores, (a) the PCEDT parser is comparatively stronger at recovering semantic dependency structure than at assigning labels, and (b) about the same appears to be the case for the best-performing Priberam system (on this target representation). Acknowledgements Data prepara</context>
</contexts>
<marker>Ivanova, Oepen, Dridan, Flickinger, Øvrelid, 2013</marker>
<rawString>Ivanova, A., Oepen, S., Dridan, R., Flickinger, D., and Øvrelid, L. (2013). On different approaches to syntactic analysis into bi-lexical dependencies. An empirical comparison of direct, PCFGbased, and HPSG-based parsers. In Proceedings of the 13th International Conference on Parsing Technologies (p. 63–72). Nara, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ivanova</author>
<author>S Oepen</author>
<author>L Øvrelid</author>
<author>D Flickinger</author>
</authors>
<date>2012</date>
<note>Who did what to whom?</note>
<contexts>
<context position="5817" citStr="Ivanova et al. (2012)" startWordPosition="863" endWordPosition="866">he same number of nodes and the same set of argument dependencies as there are predications and semantic role assignments in the original MRS. In particular, the EDS may still reflect non-lexical semantic predications introduced by grammatical constructions like covert quantifiers, nominalization, compounding, or implicit conjunction.2 Second, in another conversion step that is not information-preserving, the EDS graphs are further reduced into strictly bi-lexical form, i.e. a set of directed, binary dependency relations holding exclusively between lexical units. This conversion is defined by Ivanova et al. (2012) and seeks to (a) project some aspects of construction semantics onto word-to-word dependencies (for example introducing specific dependency types for compounding or implicit conjunction) and (b) relate the linguistically informed ERG-internal tokenization to the conventions of the PTB.3 Seeing as both is called the LOGON SVN trunk as of January 2014; see http://moin.delph-in.net/LogonTop for detail. 2Conversely, semantically vacuous parts of the original input (e.g. infinitival particles, complementizers, relative pronouns, argument-marking prepositions, auxiliaries, and most punctuation mark</context>
</contexts>
<marker>Ivanova, Oepen, Øvrelid, Flickinger, 2012</marker>
<rawString>Ivanova, A., Oepen, S., Øvrelid, L., and Flickinger, D. (2012). Who did what to whom?</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpora of English: The Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<pages>313--330</pages>
<contexts>
<context position="1106" citStr="Marcus et al., 1993" startWordPosition="155" endWordPosition="158">eks to connect the Task to pre-existing, ‘in-house’ parsing systems for the same types of target semantic dependency graphs. 1 Background and Motivation The three target representations for Task 8 at SemEval 2014, Broad-Coverage Semantic Dependency Parsing (SDP; Oepen et al., 2014), are rooted in language engineering efforts that have been under continuous development for at least the past decade. The gold-standard semantic dependency graphs used for training and testing in the Task result from largely manual annotation, in part re-purposing and adapting resources like the Penn Treebank (PTB; Marcus et al., 1993), PropBank (Palmer et al., 2005), and others. But the groups who prepared the SDP target data have also worked in parallel on automated parsing systems for these representations. Thus, for each of the target representations, there is a pre-existing parser, often developed in parallel to the creation of the target dependency graphs, viz. (a) for the DM representation, the parser of the hand-engineered LinGO English Resource Grammar (ERG; Flickinger, 2000); (b) for PAS, the Enju parsing system (Miyao, 2006), with its probabilistic HPSG acquired through linguistic projection of the PTB; and (c) f</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Marcus, M., Santorini, B., and Marcinkiewicz, M. A. (1993). Building a large annotated corpora of English: The Penn Treebank. Computational Linguistics, 19, 313 – 330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A F T Martins</author>
<author>M S C Almeida</author>
</authors>
<title>Priberam. A turbo semantic parser with second order features.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation.</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="17650" citStr="Martins and Almeida, 2014" startWordPosition="2786" endWordPosition="2789">actic structure (which can even be nested) and the type of coordination. See Figure 2 for an example. 5 Results and Reflections Seeing as our ‘in-house’ parsers are not directly trained on the semantic dependency graphs provided for the Task, but rather are built from additional linguistic resources, we submitted results from the parsing pipelines sketched in Sections 2 to 4 above to the open SDP track. Table 1 summarizes parser performance in terms of labeled and unlabeled F1 (LF and UF)12 and fullsentence exact match (LM and UM), comparing to the best-performing submission (dubbed Priberam; Martins and Almeida, 2014) to this track. Judging by the official SDP evaluation metric, average labeled F1 over the three representations, our ensemble ranked last among six participating 12Our ensemble members exhibit comparatively small differences in recall vs. precision. ACT PRED.m CONJ PRED.m CONJ RSTR PAT.m PAT.m ACT TOP TOP CONJ.m ACT CONJ.m PAT PAT PAT PAT RSTR CONJ.m RSTR CONJ.m 338 teams; in terms of unlabeled average F1, the ‘inhouse’ submission achieved the fourth rank. As explained in the task description (Oepen et al., 2014), parts of the WSJ Corpus were excluded from the SDP training and testing data be</context>
</contexts>
<marker>Martins, Almeida, 2014</marker>
<rawString>Martins, A. F. T., and Almeida, M. S. C. (2014). Priberam. A turbo semantic parser with second order features. In Proceedings of the 8th International Workshop on Semantic Evaluation. Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
<author>K Ribarov</author>
<author>J Hajiˇc</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (p. 523 – 530).</booktitle>
<location>Vancouver, British Columbia, Canada.</location>
<marker>McDonald, Pereira, Ribarov, Hajiˇc, 2005</marker>
<rawString>McDonald, R., Pereira, F., Ribarov, K., and Hajiˇc, J. (2005). Non-projective dependency parsing using spanning tree algorithms. In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing (p. 523 – 530). Vancouver, British Columbia, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Miyao</author>
</authors>
<title>From linguistic theory to syntactic analysis. Corpus-oriented grammar development and feature forest model. Doctoral Dissertation,</title>
<date>2006</date>
<institution>University of Tokyo,</institution>
<location>Tokyo, Japan.</location>
<contexts>
<context position="1616" citStr="Miyao, 2006" startWordPosition="238" endWordPosition="239">otation, in part re-purposing and adapting resources like the Penn Treebank (PTB; Marcus et al., 1993), PropBank (Palmer et al., 2005), and others. But the groups who prepared the SDP target data have also worked in parallel on automated parsing systems for these representations. Thus, for each of the target representations, there is a pre-existing parser, often developed in parallel to the creation of the target dependency graphs, viz. (a) for the DM representation, the parser of the hand-engineered LinGO English Resource Grammar (ERG; Flickinger, 2000); (b) for PAS, the Enju parsing system (Miyao, 2006), with its probabilistic HPSG acquired through linguistic projection of the PTB; and (c) for PCEDT, the scenario for English analysis within the Treex framework (Popel and Žabokrtský, 2010), combining data-driven dependency parsing with handengineered tectogrammatical conversion. At least This work is licenced under a Creative Commons Attribution 4.0 International License; page numbers and the proceedings footer are added by the organizers. http:// creativecommons.org/licenses/by/4.0/ for DM and PAS, these parsers have been extensively engineered and applied successfully in a variety of applic</context>
<context position="7054" citStr="Miyao, 2006" startWordPosition="1049" endWordPosition="1050">n the MRS in the first place, hence have no bearing on the conversion. 3Adaptations of tokenization encompass splitting ‘multiword’ ERG tokens (like such as or ad hoc), as well as ‘hiding’ ERG token boundaries at hyphens or slashes (e.g. 77-yearconversion steps are by design lossy, DM semantic dependency graphs present a true subset of the information encoded in the full, original MRS. 3 PAS: The Enju Parsing System Enju Predicate–Argument Structures (PAS) are derived from the automatic HPSG-style annotation of the PTB, which was primarily used for the development of the Enju parsing system4 (Miyao, 2006). A notable feature of this parser is that the grammar is not developed by hand; instead, the Enju HPSG-style treebank is first developed, and the grammar (or, more precisely, the vast majority of lexical entries) is automatically extracted from the treebank (Miyao et al., 2004). In this ‘projection’ step, PTB annotations such as empty categories and coindexation are used for deriving the semantic representations that correspond to HPSG derivations. Its probabilistic model for disambiguation is also trained using this treebank (Miyao and Tsujii, 2008).5 The PAS data set is an extraction of pre</context>
</contexts>
<marker>Miyao, 2006</marker>
<rawString>Miyao, Y. (2006). From linguistic theory to syntactic analysis. Corpus-oriented grammar development and feature forest model. Doctoral Dissertation, University of Tokyo, Tokyo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Miyao</author>
<author>T Ninomiya</author>
<author>J Tsujii</author>
</authors>
<title>Corpus-oriented grammar development for acquiring a Head-Driven Phrase Structure Grammar from the Penn Treebank.</title>
<date>2004</date>
<booktitle>In Proceedings of the 1st International Joint Conference on Natural Language Processing (p. 684 – 693).</booktitle>
<contexts>
<context position="7333" citStr="Miyao et al., 2004" startWordPosition="1093" endWordPosition="1096">ign lossy, DM semantic dependency graphs present a true subset of the information encoded in the full, original MRS. 3 PAS: The Enju Parsing System Enju Predicate–Argument Structures (PAS) are derived from the automatic HPSG-style annotation of the PTB, which was primarily used for the development of the Enju parsing system4 (Miyao, 2006). A notable feature of this parser is that the grammar is not developed by hand; instead, the Enju HPSG-style treebank is first developed, and the grammar (or, more precisely, the vast majority of lexical entries) is automatically extracted from the treebank (Miyao et al., 2004). In this ‘projection’ step, PTB annotations such as empty categories and coindexation are used for deriving the semantic representations that correspond to HPSG derivations. Its probabilistic model for disambiguation is also trained using this treebank (Miyao and Tsujii, 2008).5 The PAS data set is an extraction of predicate– argument structures from the Enju HPSG treebank. The Enju parser outputs results in ‘readyto-use’ formats like phrase structure trees and predicate–argument structures, as full HPSG analyses are not friendly to users who are not familiar with the HPSG theory. The gold-st</context>
</contexts>
<marker>Miyao, Ninomiya, Tsujii, 2004</marker>
<rawString>Miyao, Y., Ninomiya, T., and Tsujii, J. (2004). Corpus-oriented grammar development for acquiring a Head-Driven Phrase Structure Grammar from the Penn Treebank. In Proceedings of the 1st International Joint Conference on Natural Language Processing (p. 684 – 693).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Miyao</author>
<author>K Sagae</author>
<author>J Tsujii</author>
</authors>
<title>Towards framework-independent evaluation of deep linguistic parsers.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Workshop on Grammar Engineering across Frameworks (p. 238 – 258).</booktitle>
<location>Palo Alto, California.</location>
<contexts>
<context position="19067" citStr="Miyao et al., 2007" startWordPosition="3022" endWordPosition="3025">bility that the testing data is positively biased towards our ensemble members.13 But even with this caveat, it seems fair to observe that the ERG and Enju parsers both are very competitive for the DM and PAS target representations, respectively, specifically so when judged in exact match scores. A possible explanation for these results lies in the depth of grammatical information available to these parsers, where DM or PAS semantic dependency graphs are merely a simpliefied view on the complete underlying HPSG analyses. These parsers have performed well in earlier contrastive evaluation too (Miyao et al., 2007; Bender et al., 2011; Ivanova et al., 2013; inter alios). Results for the Treex English parsing scenario, on the other hand, show that this ensemble member is not fine-tuned for the PCEDT target representation; due to the reasons mentioned above, its performance even falls behind the shared task baseline. As is evident from the comparison of labeled vs. unlabeled F1 scores, (a) the PCEDT parser is comparatively stronger at recovering semantic dependency structure than at assigning labels, and (b) about the same appears to be the case for the best-performing Priberam system (on this target rep</context>
</contexts>
<marker>Miyao, Sagae, Tsujii, 2007</marker>
<rawString>Miyao, Y., Sagae, K., and Tsujii, J. (2007). Towards framework-independent evaluation of deep linguistic parsers. In Proceedings of the 2007 Workshop on Grammar Engineering across Frameworks (p. 238 – 258). Palo Alto, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Miyao</author>
<author>J Tsujii</author>
</authors>
<title>Feature forest models for probabilistic HPSG parsing.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>1</issue>
<pages>35--80</pages>
<contexts>
<context position="7611" citStr="Miyao and Tsujii, 2008" startWordPosition="1133" endWordPosition="1136">used for the development of the Enju parsing system4 (Miyao, 2006). A notable feature of this parser is that the grammar is not developed by hand; instead, the Enju HPSG-style treebank is first developed, and the grammar (or, more precisely, the vast majority of lexical entries) is automatically extracted from the treebank (Miyao et al., 2004). In this ‘projection’ step, PTB annotations such as empty categories and coindexation are used for deriving the semantic representations that correspond to HPSG derivations. Its probabilistic model for disambiguation is also trained using this treebank (Miyao and Tsujii, 2008).5 The PAS data set is an extraction of predicate– argument structures from the Enju HPSG treebank. The Enju parser outputs results in ‘readyto-use’ formats like phrase structure trees and predicate–argument structures, as full HPSG analyses are not friendly to users who are not familiar with the HPSG theory. The gold-standard PAS target data in the Task was developed using this function; the conversion program from full HPSG analyses to predicate–argument structures was applied to the Enju Treebank. Predicate–argument structures (PAS) represent word-to-word semantic dependencies, such as sema</context>
</contexts>
<marker>Miyao, Tsujii, 2008</marker>
<rawString>Miyao, Y., and Tsujii, J. (2008). Feature forest models for probabilistic HPSG parsing. Computational Linguistics, 34(1), 35 – 80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>S Kübler</author>
<author>R McDonald</author>
<author>J Nilsson</author>
<author>S Riedel</author>
<author>D Yuret</author>
</authors>
<title>The CoNLL</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Conference on Natural Language Learning (p. 915 – 932).</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="13024" citStr="Nivre et al., 2007" startWordPosition="2008" endWordPosition="2011"> blocks (reading the input), one final block (writing the output), two A2N blocks (named entity recognition), twelve W2A blocks (dependency parsing at the analytical layer) and 31 A2T and T2T blocks (creating the t-tree based on the a-tree). Most blocks are highly specialized in one particular subtask (e.g. there is a block just to make sure that quotation marks are attached to the root of the quoted subtree). A few blocks are responsible for the bulk of the work. The a-tree is constructed by a block that contains the MST Parser (McDonald et al., 2005), trained on the CoNLL 2007 English data (Nivre et al., 2007), i.e. Sections 2–11 of the PTB, converted to dependencies. The annotation style of CoNLL 2007 differs from PCEDT 2.0, and thus the unlabeled attachment score of the analytical parser is only 66%. Obviously one could expect better results if we retrained the MST Parser directly on the PCEDT a-trees, and on the whole training data. The only reason why we did not do so was lack of time. Our results thus really demonstrate what is available ‘off-the-shelf’; on the other hand, the PCEDT component of our ensemble fails to set any ‘upper bound’ of output quality, as it definitely is not betU.S. shou</context>
</contexts>
<marker>Nivre, Hall, Kübler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>Nivre, J., Hall, J., Kübler, S., McDonald, R., Nilsson, J., Riedel, S., and Yuret, D. (2007). The CoNLL 2007 shared task on dependency parsing. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Conference on Natural Language Learning (p. 915 – 932). Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Oepen</author>
<author>J Carroll</author>
</authors>
<title>Ambiguity packing in constraint-based parsing. Practical results.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st Meeting of the North American Chapter of the Association for Computational Linguistics (p.</booktitle>
<pages>162--169</pages>
<location>Seattle, WA, USA.</location>
<contexts>
<context position="4156" citStr="Oepen and Carroll, 2000" startWordPosition="612" endWordPosition="615">s a hybrid, combining (a) the hand-built, broad-coverage ERG with (b) an efficient chart parser for unification grammars and (c) a conditional probability distribution over candidate analyses. The parser most commonly used with the ERG, called PET (Callmeier, 2002),1 constructs a complete, 1The SDP test data was parsed using the 1212 release of the ERG, using PET and converter versions from what 335 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 335–340, Dublin, Ireland, August 23-24, 2014. subsumption-based parse forest of partial HPSG derivations (Oepen and Carroll, 2000), and then extracts from the forest n-best lists (in globally correct rank order) of complete analyses according to a discriminative parse ranking model (Zhang et al., 2007). For our experiments, we trained the parse ranker on Sections 00–20 of DeepBank and otherwise used the default, non-pruning development configuration, which is optimized for accuracy. In this setup, ERG parsing on average takes close to ten seconds per sentence. Post-Parsing Conversion After parsing, MRSs are reduced to DM bi-lexical semantic dependencies in two steps. First, Oepen and Lønning (2006) define a conversion to</context>
</contexts>
<marker>Oepen, Carroll, 2000</marker>
<rawString>Oepen, S., and Carroll, J. (2000). Ambiguity packing in constraint-based parsing. Practical results. In Proceedings of the 1st Meeting of the North American Chapter of the Association for Computational Linguistics (p. 162–169). Seattle, WA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Oepen</author>
<author>M Kuhlmann</author>
<author>Y Miyao</author>
<author>D Zeman</author>
<author>D Flickinger</author>
<author>J Hajiˇc</author>
</authors>
<title>Task 8. Broad-coverage semantic dependency parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation.</booktitle>
<location>Dublin, Ireland.</location>
<note>SemEval</note>
<marker>Oepen, Kuhlmann, Miyao, Zeman, Flickinger, Hajiˇc, 2014</marker>
<rawString>Oepen, S., Kuhlmann, M., Miyao, Y., Zeman, D., Flickinger, D., Hajiˇc, J., ... Zhang, Y. (2014). SemEval 2014 Task 8. Broad-coverage semantic dependency parsing. In Proceedings of the 8th International Workshop on Semantic Evaluation. Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Oepen</author>
<author>J T Lønning</author>
</authors>
<title>Discriminant-based MRS banking.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation (p. 1250 –</booktitle>
<pages>1255</pages>
<location>Genoa, Italy.</location>
<contexts>
<context position="4733" citStr="Oepen and Lønning (2006)" startWordPosition="702" endWordPosition="705">tial HPSG derivations (Oepen and Carroll, 2000), and then extracts from the forest n-best lists (in globally correct rank order) of complete analyses according to a discriminative parse ranking model (Zhang et al., 2007). For our experiments, we trained the parse ranker on Sections 00–20 of DeepBank and otherwise used the default, non-pruning development configuration, which is optimized for accuracy. In this setup, ERG parsing on average takes close to ten seconds per sentence. Post-Parsing Conversion After parsing, MRSs are reduced to DM bi-lexical semantic dependencies in two steps. First, Oepen and Lønning (2006) define a conversion to variable-free Elementary Dependency Structures (EDS), which (a) maps each predication in the MRS logical-form meaning representation to a node in a dependency graph and (b) transforms argument relations represented by shared logical variables into directed dependency links between graph nodes. This first step of the conversion is ‘mildly’ lossy, in that some scope-related information is discarded; the EDS graph, however, will contain the same number of nodes and the same set of argument dependencies as there are predications and semantic role assignments in the original</context>
</contexts>
<marker>Oepen, Lønning, 2006</marker>
<rawString>Oepen, S., and Lønning, J. T. (2006). Discriminant-based MRS banking. In Proceedings of the 5th International Conference on Language Resources and Evaluation (p. 1250 – 1255). Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>D Gildea</author>
<author>P Kingsbury</author>
</authors>
<title>The Proposition Bank. A corpus annotated with semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<pages>71--106</pages>
<contexts>
<context position="1138" citStr="Palmer et al., 2005" startWordPosition="161" endWordPosition="164">xisting, ‘in-house’ parsing systems for the same types of target semantic dependency graphs. 1 Background and Motivation The three target representations for Task 8 at SemEval 2014, Broad-Coverage Semantic Dependency Parsing (SDP; Oepen et al., 2014), are rooted in language engineering efforts that have been under continuous development for at least the past decade. The gold-standard semantic dependency graphs used for training and testing in the Task result from largely manual annotation, in part re-purposing and adapting resources like the Penn Treebank (PTB; Marcus et al., 1993), PropBank (Palmer et al., 2005), and others. But the groups who prepared the SDP target data have also worked in parallel on automated parsing systems for these representations. Thus, for each of the target representations, there is a pre-existing parser, often developed in parallel to the creation of the target dependency graphs, viz. (a) for the DM representation, the parser of the hand-engineered LinGO English Resource Grammar (ERG; Flickinger, 2000); (b) for PAS, the Enju parsing system (Miyao, 2006), with its probabilistic HPSG acquired through linguistic projection of the PTB; and (c) for PCEDT, the scenario for Engli</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Palmer, M., Gildea, D., and Kingsbury, P. (2005). The Proposition Bank. A corpus annotated with semantic roles. Computational Linguistics, 31(1), 71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Pollard</author>
<author>I A Sag</author>
</authors>
<title>Head-Driven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>The University of Chicago Press.</publisher>
<location>Chicago, USA:</location>
<contexts>
<context position="2877" citStr="Pollard and Sag, 1994" startWordPosition="416" endWordPosition="419"> of comparison. Through this ‘in-house’ submission (of our ‘own’ parsers to our ‘own’ task), we hope to facilitate the comparison of different approaches submitted to the Task with this pre-existing line of parser engineering. 2 DM: The English Resource Grammar Semantic dependency graphs in the DM target representation, DELPH-IN MRS-Derived Bi-Lexical Dependencies, stem from a two-step ‘reduction’ (simplification) of the underspecified logicalform meaning representations output natively by the ERG parser, which implements the linguistic framework of Head-Driven Phrase Structure Grammar (HPSG; Pollard and Sag, 1994). Goldstandard DM training and test data for the Task were derived from the manually annotated DeepBank Treebank (Flickinger et al., 2012), which pairs Sections 00–21 of the venerable PTB Wall Street Journal (WSJ) Corpus with complete ERGcompatible HPSG syntactico-semantic analyses. DeepBank as well as the ERG rely on Minimal Recursion Semantics (MRS; Copestake et al., 2005) for meaning representation, such that the exact same post-processing steps could be applied to the parser outputs as were used in originally reducing the gold-standard MRSs from DeepBank into the SDP bi-lexical semantic de</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Pollard, C., and Sag, I. A. (1994). Head-Driven Phrase Structure Grammar. Chicago, USA: The University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Popel</author>
<author>Z Žabokrtský</author>
</authors>
<date>2010</date>
<booktitle>TectoMT. Modular NLP framework. Advances in Natural Language Processing, 293 –</booktitle>
<pages>304</pages>
<contexts>
<context position="1805" citStr="Popel and Žabokrtský, 2010" startWordPosition="265" endWordPosition="268">d the SDP target data have also worked in parallel on automated parsing systems for these representations. Thus, for each of the target representations, there is a pre-existing parser, often developed in parallel to the creation of the target dependency graphs, viz. (a) for the DM representation, the parser of the hand-engineered LinGO English Resource Grammar (ERG; Flickinger, 2000); (b) for PAS, the Enju parsing system (Miyao, 2006), with its probabilistic HPSG acquired through linguistic projection of the PTB; and (c) for PCEDT, the scenario for English analysis within the Treex framework (Popel and Žabokrtský, 2010), combining data-driven dependency parsing with handengineered tectogrammatical conversion. At least This work is licenced under a Creative Commons Attribution 4.0 International License; page numbers and the proceedings footer are added by the organizers. http:// creativecommons.org/licenses/by/4.0/ for DM and PAS, these parsers have been extensively engineered and applied successfully in a variety of applications, hence represent relevant points of comparison. Through this ‘in-house’ submission (of our ‘own’ parsers to our ‘own’ task), we hope to facilitate the comparison of different approac</context>
<context position="10889" citStr="Popel and Žabokrtský, 2010" startWordPosition="1650" endWordPosition="1653"> 2012)8 is a set of parallel dependency trees over the same WSJ texts from the Penn Treebank, and their Czech translations. Similarly to other treebanks in the Prague family, there are two layers of syntactic annotation: analytical (a-trees) and tectogrammatical (t-trees). Unlike for the other two representations used in the Task, for PCEDT there is no pre-existing parsing system designed to deliver the full scale of annotations of the SDP gold-standard data. The closest available match is a parsing scenario implemented in the Treex natural language processing framework. Parsing Setup Treex9 (Popel and Žabokrtský, 2010) is a modular, open-source framework originally developed for transfer-based machine translation. It can accomplish any NLP-related task by sequentially applying to the same piece of data various blocks of code. Blocks operate on a common data structure and are chained in scenarios. Some early experiments with scenarios for tectogrammatical analysis of English were described by Klimeš (2007). It is of interest that they report 7The Enju parser ignores tokens tagged as ‘.’, while the PAS representation includes them with dummy relations; thus, missing periods are inserted in post-processing by </context>
</contexts>
<marker>Popel, Žabokrtský, 2010</marker>
<rawString>Popel, M., and Žabokrtský, Z. (2010). TectoMT. Modular NLP framework. Advances in Natural Language Processing, 293 – 304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>S Oepen</author>
<author>J Carroll</author>
</authors>
<title>Efficiency in unification-based n-best parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of the 10th International Conference on Parsing Technologies (p. 48–59).</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="4329" citStr="Zhang et al., 2007" startWordPosition="639" endWordPosition="642">date analyses. The parser most commonly used with the ERG, called PET (Callmeier, 2002),1 constructs a complete, 1The SDP test data was parsed using the 1212 release of the ERG, using PET and converter versions from what 335 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 335–340, Dublin, Ireland, August 23-24, 2014. subsumption-based parse forest of partial HPSG derivations (Oepen and Carroll, 2000), and then extracts from the forest n-best lists (in globally correct rank order) of complete analyses according to a discriminative parse ranking model (Zhang et al., 2007). For our experiments, we trained the parse ranker on Sections 00–20 of DeepBank and otherwise used the default, non-pruning development configuration, which is optimized for accuracy. In this setup, ERG parsing on average takes close to ten seconds per sentence. Post-Parsing Conversion After parsing, MRSs are reduced to DM bi-lexical semantic dependencies in two steps. First, Oepen and Lønning (2006) define a conversion to variable-free Elementary Dependency Structures (EDS), which (a) maps each predication in the MRS logical-form meaning representation to a node in a dependency graph and (b)</context>
</contexts>
<marker>Zhang, Oepen, Carroll, 2007</marker>
<rawString>Zhang, Y., Oepen, S., and Carroll, J. (2007). Efficiency in unification-based n-best parsing. In Proceedings of the 10th International Conference on Parsing Technologies (p. 48–59). Prague, Czech Republic.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>