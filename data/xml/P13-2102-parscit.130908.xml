<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.016788">
<title confidence="0.999103">
A System for Summarizing Scientific Topics Starting from Keywords
</title>
<author confidence="0.994702">
Rahul Jha
</author>
<affiliation confidence="0.9973695">
Department of EECS
University of Michigan
</affiliation>
<address confidence="0.841111">
Ann Arbor, MI, USA
</address>
<email confidence="0.999164">
rahuljha@umich.edu
</email>
<author confidence="0.972976">
Amjad Abu-Jbara
</author>
<affiliation confidence="0.9962295">
Department of EECS
University of Michigan
</affiliation>
<address confidence="0.840983">
Ann Arbor, MI, USA
</address>
<email confidence="0.999092">
amjbara@umich.edu
</email>
<author confidence="0.98636">
Dragomir Radev
</author>
<affiliation confidence="0.998186333333333">
Department of EECS and
School of Information
University of Michigan
</affiliation>
<address confidence="0.855806">
Ann Arbor, MI, USA
</address>
<email confidence="0.999414">
radev@umich.edu
</email>
<sectionHeader confidence="0.993914" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999922904761905">
In this paper, we investigate the problem
of automatic generation of scientific sur-
veys starting from keywords provided by
a user. We present a system that can take
a topic query as input and generate a sur-
vey of the topic by first selecting a set
of relevant documents, and then selecting
relevant sentences from those documents.
We discuss the issues of robust evalua-
tion of such systems and describe an eval-
uation corpus we generated by manually
extracting factoids, or information units,
from 47 gold standard documents (surveys
and tutorials) on seven topics in Natural
Language Processing. We have manually
annotated 2,625 sentences with these fac-
toids (around 375 sentences per topic) to
build an evaluation corpus for this task.
We present evaluation results for the per-
formance of our system using this anno-
tated data.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999965230769231">
The rise of the number of publications in all sci-
entific fields is making it more and more difficult
to get quickly acquainted with the new develop-
ments in a new area. One way to wade through this
huge amount of scholarly information is to consult
topical surveys written by experts in an area. For
example, for machine translation, one might read
(Lopez, 2008)1. Such surveys can be very help-
ful when available, but unfortunately, may not be
available for all areas. Additionally, the manual
surveys quickly go out of date within a few years
of publication as additional papers are published
in the field.
</bodyText>
<note confidence="0.837636">
1Adam Lopez. 2008. Statistical machine translation.
ACM Comput. Surv. 40, 3, Article 8
</note>
<bodyText confidence="0.999959533333333">
Thus, a system that can generate such surveys
automatically would be a useful tool. Short sum-
maries in the form of abstracts are available for
individual papers, but no such information is avail-
able for scientific topics. In this paper, we ex-
plore strategies for generating and evaluating such
surveys of scientific topics automatically starting
from a phrase representing a topic area. We evalu-
ate our system on a set of topics in the field of Nat-
ural Language Processing. In earlier work, (Teufel
and Moens, 2002) have examined the problem
of summarizing scientific articles using rhetorical
analysis of sentences. Nanba and Okumura (1999)
have also discussed the problem of generating sur-
veys of multiple papers. Mohammad et al. (2009)
presented experiments on generating surveys of
scientific topics starting from papers to be summa-
rized. More recently, Hoang and Kan (2010) have
presented initial results on automatically generat-
ing related work section for a target paper by tak-
ing a hierarchical topic tree as an input.
In this paper, we tackle the more challenging
problem of summarizing a topic starting from a
topic query. Our system takes as an input a string
describing the topic area, selects the relevant pa-
pers from a corpus of papers, and then selects sen-
tences from the citing sentences to these papers to
generate a survey of the topic. A sample output of
our system for the topic of “Word Sense Disam-
biguation” is shown in Figure 1.
</bodyText>
<sectionHeader confidence="0.964467" genericHeader="method">
2 Candidate Document Selection
</sectionHeader>
<bodyText confidence="0.999811333333333">
Given a query representing the topic to be sum-
marized, our first task is to find the set of rele-
vant documents from the corpus. The simplest
way to do this for a corpus of scientific publica-
tions is to do a query search using exact match or
a standard TF*IDF system such Lucene, rank the
documents using either citation counts or pager-
ank in the bibliometric citation network, and se-
lect the top n documents. However, comparing
</bodyText>
<page confidence="0.962717">
572
</page>
<note confidence="0.4465775">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 572–577,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<construct confidence="0.964695">
Many corpus based methods have been proposed to deal with the sense disambiguation problem when given de nition for each possible sense of a target word or a
tagged corpus with the instances of each possible sense, e.g., supervised sense disambiguation (Leacock et al. , 1998), and semi-supervised sense disambiguation
(Yarowsky, 1995).
Most researchers working on word sense disambiguation (WSD) use manually sense tagged data such as SemCor (Miller et al. , 1993) to train statistical
classifiers, but also use the information in SemCor on the overall sense distribution for each word as a backoff model.
Yarowsky (1995) has proposed a bootstrapping method for word sense disambiguation.
Training of WSD Classifier Much research has been done on the best supervised learning approach for WSD (Florian and Yarowsky, 2002; Lee and Ng, 2002;
Mihalcea and Moldovan, 2001; Yarowsky et al. , 2001).
For example, the use ofparallel corpora for sense tagging can help with word sense disambiguation (Brown et al. , 1991; Dagan, 1991; Dagan and Itai, 1994;
Ide, 2000; Resnik and Yarowsky, 1999).
</construct>
<figureCaption confidence="0.930504">
Figure 1: A sample output survey of our system on the topic of “Word Sense Disambiguation” produced
by paper selection using Restricted Expansion and sentence selection using Lexrank. In our evaluations,
this survey achieved a pyramid score of 0.82 and Unnormalized RU score of 0.31.
</figureCaption>
<table confidence="0.9980775">
Document selection algorithm CG5 CGr0 CG20
Title match sorted with citation count 1.82 2.75 3.29
Title match sorted with pagerank 1.77 2.55 3.34
Citation expansion sorted with citation 0.53 1.20 2.29
count
Citation expansion sorted with pagerank 0.20 0.78 1.99
TF*IDF ranked 0.14 0.14 0.56
TF*IDF sorted with citation count 0.44 2.25 3.18
TF*IDF sorted with pagerank 1.54 2.22 2.85
Restricted Expansion 2.52 3.91 6.01
</table>
<tableCaption confidence="0.999822">
Table 1: Comparison of different methods for
</tableCaption>
<bodyText confidence="0.9763295">
document selection by measuring the Cumulative
Gain (CG) of top 5, 10 and 20 results.
the results of these techniques with the papers cov-
ered by gold standard surveys on a few topics, we
found that some important papers are missed by
these simple approaches. One reason for this is
that early papers in a field might use non-standard
terms in the absence of a stable, accepted termi-
nology. Some early Word Sense Disambiguation
papers, for example, refer to the problem as Lex-
ical Ambiguity Resolution. Additionally, papers
might use alternative forms or abbreviations of
topics in their titles and abstracts, e.g. for input
query “Semantic Role Labelling”, papers such as
(Dahlmeier et al., 2009) titled “Joint Learning of
Preposition Senses and Semantic Roles of Prepo-
sitional Phrases” and (Che and Liu, 2010) titled
“Jointly Modeling WSD and SRL with Markov
Logic” might be missed.
To find these papers, we add a simple heuristic
called Restricted Expansion. In this method, we
first create a base set B, by finding papers with an
exact match to the query. This is a high precision
set since a paper with a title that contains the ex-
act query phrase is very likely to be relevant to the
topic. We then find additional papers by expand-
ing in the citation network around B, that is, by
finding all the papers that are cited by or cite the
papers in B, to create an extended set E. From
this combined set (B U E), we create a new set F
by filtering out the set of papers that are not cited
by or cite a minimum threshold tinit of papers in
B. If the total number of papers is lower than fmin
or higher than fmax, we iteratively increase or de-
crease t till fmin &lt; |F |&lt; fmax. This method
allows us to increase our recall without losing pre-
cision. The values for our current experiments are:
tinit = 5, fmin = 150, fmax = 250.
</bodyText>
<table confidence="0.995961">
Authors Year Size
Surveys
ACL Wiki 2012 4
Roberto Navigli 2009 68
Eneko Agirre; Philip Edmonds 2006 28
Xiaohua Zhou; Hyoil Han 2005 6
Nancy Ide; Jean Vronis 1998 41
Tutorials
Sanda Harabagiu 2011 45
Diana McCarthy 2011 120
Philipp Koehn 2008 17
Rada Mihalcea 2005 186
</table>
<tableCaption confidence="0.993118">
Table 2: The set of surveys and tutorials col-
</tableCaption>
<bodyText confidence="0.994732166666667">
lected for the topic of “Word Sense Disambigua-
tion”. Sizes for surveys are expressed in number
of pages, sizes for tutorials are expressed in num-
ber of slides.
To evaluate different methods of candidate doc-
ument selection, we use Cumulative Gain (CG),
where the weight for each paper is estimated by
the fraction of surveys it appears in. Table 1
shows the average Cumulative Gain of top 5, 10
and 20 documents for each of eight methods we
tried. Restricted Expansion outperformed every
other method. Once we obtain a set of papers to
be summarized, we select the top n most cited pa-
pers in the document set as the papers to be sum-
marized, and extract the set of citing sentences 5
from all the papers in the document set to these n
papers. 5 is the input for our sentence selection
algorithms, described in Section 4.
</bodyText>
<page confidence="0.989523">
573
</page>
<table confidence="0.999746636363636">
Factoid S1 S2 S3 S4 S5 T1 T2 T3 T4 Factoid Weight
definition of wsd X X X X X X X X X 9
wordnet X X X X X X X X 8
knowledge based wsd X X X X X X X 7
supervised wsd X X X X X X X 7
senseval X X X X X X X 7
definition of word senses X X X X X X 7
knowledge based wsd using machine readable dictionaries X X X X X X 6
unsupervised wsd X X X X X X 6
bootstrapping algorithms X X X X X X 6
supervised wsd using decision lists X X X X X X 6
</table>
<tableCaption confidence="0.9951355">
Table 3: Top 10 factoids for the topic of “Word Sense Disambiguation” and their distribution across
various data sources.
</tableCaption>
<sectionHeader confidence="0.953109" genericHeader="method">
3 Evaluation Data for Survey Generation
</sectionHeader>
<bodyText confidence="0.999997235294118">
We use the ACL Anthology Network (AAN) as the
corpus for our experiments (Radev et al., 2013).
We built a factoid inventory for seven topics in
NLP based on manual written surveys in the fol-
lowing way. For each topic, we found at least 3
recent tutorials and 3 recent surveys on the topic
and extracted the factoids that are covered in each
of them. Table 2 shows the complete list of ma-
terial collected for the topic of “Word Sense Dis-
ambiguation”. We found around 80 factoids per
topic on an average. Once the factoids were ex-
tracted, each factoid was assigned a weight based
on the number of documents it appears in, and any
factoids with weight one were removed. Table 3
shows the top ten factoids in the topic of Word
Sense Disambiguation along with their distribu-
tion across the different surveys and tutorials and
final weight.
For each of the topics, we used the method de-
scribed in Section 2 to create a candidate docu-
ment set and extracted the candidate citing sen-
tences to be used as the input for the content se-
lection component. Each sentence in each topic
was then annotated by a human judge against the
factoid list for that topic. A sentence is allowed
to have zero or more than one factoid. The human
assessors were graduate students in Computer Sci-
ence who have taken a basic “Natural Language
Processing” course or an equivalent course. On an
average, 375 citing sentences were annotated for
each topic, with 2,625 sentences being annotated
in total. We present all our experimental results on
this large annotated corpora which is also available
for download 2.
</bodyText>
<sectionHeader confidence="0.997155" genericHeader="method">
4 Content Models
</sectionHeader>
<bodyText confidence="0.9016425">
Once we have the set of input sentences, our sys-
tem must select the sentences that should be part
</bodyText>
<footnote confidence="0.961143">
2http://clair.si.umich.edu/corpora/survey data/
</footnote>
<bodyText confidence="0.9676595">
of the survey. For this task, we experimented with
three content models, described below.
</bodyText>
<subsectionHeader confidence="0.97776">
4.1 Centroid
</subsectionHeader>
<bodyText confidence="0.999944909090909">
The centroid of a set of documents is a set of words
that are statistically important to the cluster of doc-
uments. Centroid based summarization of a docu-
ment set involves first creating the centroid of the
documents, and then judging the salience of each
document based on its similarity to the centroid
of the document set. In our case, the input citing
sentences represent the documents from which we
extract the centroid. We use the centroid imple-
mentation from the publicly available summariza-
tion toolkit, MEAD (Radev et al., 2004).
</bodyText>
<subsectionHeader confidence="0.976538">
4.2 Lexrank
</subsectionHeader>
<bodyText confidence="0.999964375">
LexRank (Erkan and Radev, 2004) is a network
based content selection algorithm that works by
first building a graph of all the documents in a
cluster. The edges between corresponding nodes
represent the cosine similarity between them.
Once the network is built, the algorithm computes
the salience of sentences in this graph based on
their eigenvector centrality in the network.
</bodyText>
<subsectionHeader confidence="0.999518">
4.3 C-Lexrank
</subsectionHeader>
<bodyText confidence="0.999922">
C-Lexrank is another network based content selec-
tion algorithm that focuses on diversity (Qazvinian
and Radev, 2008). Given a set of sentences, it first
creates a network using these sentences and then
runs a clustering algorithm to partition the net-
work into smaller clusters that represent different
aspects of the paper. The motivation behind the
clustering is to include more diverse facts in the
summary.
</bodyText>
<sectionHeader confidence="0.997436" genericHeader="evaluation">
5 Experiments and Results
</sectionHeader>
<bodyText confidence="0.996646666666667">
To do an evaluation of our different content selec-
tion methods, we first select the documents using
our Restricted Expansion method, and then pick
</bodyText>
<page confidence="0.996165">
574
</page>
<table confidence="0.999767">
Topic Rand Cent LR C-LR
Summarization 0.68 0.61 0.91 0.82
Question Answering 0.52 0.50 0.65 0.56
Word Sense Disambiguation 0.78 0.73 0.82 0.76
Named Entity Recognition 0.90 0.90 0.94 0.94
Sentiment Analysis 0.75 0.78 0.77 0.78
Semantic Role Labeling 0.78 0.79 0.88 0.94
Dependency Parsing 0.67 0.38 0.71 0.53
Average 0.72 0.68 0.81* 0.76
</table>
<tableCaption confidence="0.990691">
Table 4: Results of pyramid evaluation for each
</tableCaption>
<bodyText confidence="0.990405034482759">
of the three methods and the random baseline on
each topic.
the citing sentences to be used as the input to the
summarization module as described in Section 2.
Given this input, we generate 500 word summaries
for each of the seven topics using the four meth-
ods: Centroid, Lexrank, C-Lexrank and a random
baseline.
For each summary, we compute two evaluation
metrics. The first is the Pyramid score (Nenkova
and Passonneau, 2004) computed by treating the
factoids as Summary Content Units (SCU’s). The
Pyramid scores for each summary is shown in Ta-
ble 4. The second metric is an Unnormalized Rel-
ative Utility score (Radev and Tam, 2003), com-
puted using the factoid scores of sentences based
on the method presented in (Qazvinian, 2012). We
call this Unnormalized RU since we are not able to
normalize the scores with human generated gold
summaries. The results for Unnormalized RU are
shown in Table 5. The parameter α is the RU
penalty for including a redundant sentence sub-
sumed by an earlier sentence. If the summary
chooses a sentence sZ with score woTZg that is sub-
sumed by an earlier summary sentence, the score
is reduced as wsubsumed = (α ∗ woTZg). We ap-
proximate subsumption by marking a sentence sj
as being subsumed by sZ if Fj ⊂ FZ, where FZ and
Fj are sets of factoids covered in each sentence.
</bodyText>
<table confidence="0.999634888888889">
Topic Rand Cent LR C-LR
Summarization 0.16 0.57 0.29 0.17
Question Answering 0.32 0.39 0.48 0.30
Word Sense Disambiguation 0.28 0.33 0.31 0.30
Named Entity Recognition 0.36 0.38 0.34 0.31
Sentiment Analysis 0.23 0.34 0.48 0.33
Semantic Role Labeling 0.11 0.17 0.16 0.21
Dependency Parsing 0.16 0.05 0.30 0.15
Average 0.23 0.32 0.34* 0.25
</table>
<tableCaption confidence="0.990765">
Table 5: Results of Unnormalized Relative Utility
</tableCaption>
<bodyText confidence="0.990352717948718">
evaluation for the three methods and random base-
line using α = 0.5.
The reason for the relatively high scores for the
random baseline is that our process to select the
initial set of sentences eliminates many bad sen-
tences. For example, for a subset of 5 topics,
the total input set contains 1508 sentences, out of
which 922 of the sentences (60%) have at least one
factoid. This makes it highly likely to pick good
content sentences even when we are picking sen-
tences at random.
We find that the Lexrank method outperforms
other sentence selection methods on both evalua-
tion metrics. The higher performance of Lexrank
compared to Centroid is consistent with earlier
published results (Erkan and Radev, 2004). The
reason for the low performance of C-Lexrank as
compared to Lexrank on this data set can be at-
tributed to the fact that the input sentence set is
derived from a much more diverse set of papers
which can have a high diversity in lexical choice
when describing the same factoid. Thus simple
lexical similarity is not enough to find good clus-
ters in this sentence set.
The lower Unnormalized RU scores compared
to Pyramid scores indicate that we are selecting
sentences containing highly weighted factoids, but
we do not select the most informative sentences
that contain a large number of factoids. This
also shows that we select some redundant factoids,
since Unnormalized RU contains a penalty for re-
dundancy. This is again, explained by the fact
that the simple lexical diversity based model in C-
Lexrank is not able to detect the same factoids be-
ing present in two sentences. Despite these short-
comings, our system works quite well in terms
of content selection for unseen topics, Figure 2
shows the top 5 sentences for the query “Condi-
tional Random Fields”.
</bodyText>
<sectionHeader confidence="0.984283" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999941733333333">
In this paper, we described a pipeline for the gen-
eration of scientific surveys starting from a topic
query. Our system is divided into two components.
The first component finds the set of papers from
the corpus relevant to the query using a simple
heuristic called Restricted Expansion. The second
component selects sentences from these papers to
generate a survey of the topic. One of the main
contributions of this work is a manually annotated
data set for evaluating both the tasks. We collected
47 gold standard documents (surveys and tutori-
als) on seven topics in Natural Language Process-
ing and extracted factoids for each topic. Each
factoid is given an importance score based on the
number of gold standard documents it appears in.
</bodyText>
<page confidence="0.996045">
575
</page>
<construct confidence="0.941851045454546">
In recent years, conditional random fields (CRFs) (Lafferty et al. , 2001)
have shown success on a number of natural language processing (NLP)
tasks, including shallow parsing (Sha and Pereira, 2003), named entity
recognition (McCallum and Li, 2003) and information extraction from
research papers (Peng and McCallum, 2004).
In natural language processing, two aspects of CRFs have been
investigated sufficiently: one is to apply it to new tasks, such as named
entity recognition (McCallum and Li, 2003; Li and McCallum, 2003;
Settles, 2004), part-of-speech tagging (Lafferty et al., 2001), shallow
parsing (Sha and Pereira, 2003), and language modeling (Roark et al.,
2004); the other is to exploit new training methods for CRFs, such as
improved iterative scaling (Lafferty et al., 2001), L-BFGS (McCallum,
2003) and gradient tree boosting (Dietterich et al., 2004)
NP chunks are very similar to the ones of Ramshaw and Marcus (1995).
CRFs have shown empirical successes recently in POS tagging (Lafferty
et al. , 2001), noun phrase segmentation (Sha and Pereira, 2003) and
Chinese word segmentation (McCallum and Feng, 2003)
CRFs have been successfully applied to a number of real-world tasks,
including NP chunking (Sha and Pereira, 2003), Chinese word
segmentation (Peng et al., 2004), information extraction (Pinto et al.,
2003; Peng and McCallum, 2004), named entity identification (McCallum
and Li, 2003; Settles, 2004), and many others.
</construct>
<figureCaption confidence="0.997721333333333">
Figure 2: A sample output survey produced by
our system on the topic of “Conditional Random
Fields” using Restricted Expansion and Lexrank.
</figureCaption>
<bodyText confidence="0.999971666666667">
Additionally, we manually annotated 2,625 input
sentences, about 375 sentences per topic, with the
factoids extracted from the gold standard docu-
ments for each topic. Using this corpus, we pre-
sented experimental results for the performance of
our document selection component and three sen-
tence selection strategies.
Our results indicate three main directions for
future work. We plan to look at better models
of diversity in sentence selection, since methods
based on simple lexical similarity do not seem to
work well. The low factoid recall shown by low
unnormalized RU scores suggests integrating the
full text of papers with citation based summaries
which might help us find factoids such as topic
definitions that are unlikely to be present in citing
sentences. A final goal would be to improve the
readability and coherence of our system output.
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999794">
We thank Vahed Qazvinian, Wanchen Lu, Ben
King, and Shiwali Mohan for extremely useful
discussions and help with the data annotation.
This research is supported by the Intelligence
Advanced Research Projects Activity (IARPA) via
Department of Interior National Business Cen-
ter (DoI/NBC) contract number D11PC20153.
The U.S. Government is authorized to reproduce
and distribute reprints for Governmental purposes
notwithstanding any copyright annotation thereon.
Disclaimer: The views and conclusions contained
herein are those of the authors and should not be
interpreted as necessarily representing the official
policies or endorsements, either expressed or im-
plied, of IARPA, DoI/NBC, or the U.S. Govern-
ment.
</bodyText>
<sectionHeader confidence="0.996293" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999725212765958">
G¨unes¸ Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based centrality as salience in text summa-
rization. Journal of Artificial Intelligence Research
(JAIR).
Cong Duy Vu Hoang and Min-Yen Kan. 2010. To-
wards automated related work summarization. In
Proceedings of the 23rd International Conference on
Computational Linguistics: Posters, COLING ’10,
pages 427–435, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Saif Mohammad, Bonnie Dorr, Melissa Egan, Ahmed
Hassan, Pradeep Muthukrishan, Vahed Qazvinian,
Dragomir Radev, and David Zajic. 2009. Using ci-
tations to generate surveys of scientific paradigms.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, NAACL ’09, pages 584–592, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Hidetsugu Nanba and Manabu Okumura. 1999. To-
wards multi-paper summarization using reference
information. In Proceedings of the 16th Interna-
tional Joint Conference on Artificial Intelligence
(IJCAI-99), pages 926–931.
Ani Nenkova and Rebecca Passonneau. 2004. Evalu-
ating content selection in summarization: The pyra-
mid method. In Proceedings of the North Ameri-
can Chapter of the Association for Computational
Linguistics - Human Language Technologies (HLT-
NAACL ’04).
Vahed Qazvinian and Dragomir R. Radev. 2008.
Scientific paper summarization using citation sum-
mary networks. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics
(COLING-08), Manchester, UK.
Vahed Qazvinian. 2012. Using Collective Discourse
to Generate Surveys of Scientific Paradigms. Ph.D.
thesis.
Dragomir R. Radev and Daniel Tam. 2003. Sum-
marization evaluation using relative utility. In
CIKM2003, pages 508–511.
Dragomir Radev, Timothy Allison, Sasha Blair-
Goldensohn, John Blitzer, Arda C¸elebi, Stanko
Dimitrov, Elliott Drabek, Ali Hakim, Wai Lam,
Danyu Liu, Jahna Otterbacher, Hong Qi, Horacio
Saggion, Simone Teufel, Michael Topper, Adam
</reference>
<page confidence="0.983552">
576
</page>
<reference confidence="0.988713090909091">
Winkel, and Zhu Zhang. 2004. MEAD - a platform
for multidocument multilingual text summarization.
In LREC 2004, Lisbon, Portugal, May.
Dragomir R. Radev, Pradeep Muthukrishnan, Vahed
Qazvinian, and Amjad Abu-Jbara. 2013. The acl
anthology network corpus. Language Resources
and Evaluation, pages 1–26.
Simone Teufel and Marc Moens. 2002. Summariz-
ing scientific articles: experiments with relevance
and rhetorical status. Computational Linguistics,
28(4):409–445.
</reference>
<page confidence="0.997292">
577
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.031464">
<title confidence="0.999495">A System for Summarizing Scientific Topics Starting from Keywords</title>
<author confidence="0.872813">Rahul</author>
<affiliation confidence="0.995134">Department of University of</affiliation>
<author confidence="0.44597">Ann Arbor</author>
<author confidence="0.44597">MI</author>
<email confidence="0.998965">rahuljha@umich.edu</email>
<author confidence="0.663484">Amjad</author>
<affiliation confidence="0.982522">Department of University of</affiliation>
<author confidence="0.538956">Ann Arbor</author>
<author confidence="0.538956">MI</author>
<email confidence="0.998947">amjbara@umich.edu</email>
<author confidence="0.567075">Dragomir</author>
<affiliation confidence="0.996808">Department of EECS School of University of</affiliation>
<author confidence="0.445802">Ann Arbor</author>
<author confidence="0.445802">MI</author>
<email confidence="0.999638">radev@umich.edu</email>
<abstract confidence="0.997350227272727">In this paper, we investigate the problem of automatic generation of scientific surveys starting from keywords provided by a user. We present a system that can take a topic query as input and generate a survey of the topic by first selecting a set of relevant documents, and then selecting relevant sentences from those documents. We discuss the issues of robust evaluation of such systems and describe an evaluation corpus we generated by manually extracting factoids, or information units, from 47 gold standard documents (surveys and tutorials) on seven topics in Natural Language Processing. We have manually annotated 2,625 sentences with these factoids (around 375 sentences per topic) to build an evaluation corpus for this task. We present evaluation results for the performance of our system using this annotated data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G¨unes¸ Erkan</author>
<author>Dragomir R Radev</author>
</authors>
<title>Lexrank: Graph-based centrality as salience in text summarization.</title>
<date>2004</date>
<journal>Journal of Artificial Intelligence Research (JAIR).</journal>
<contexts>
<context position="11840" citStr="Erkan and Radev, 2004" startWordPosition="2047" endWordPosition="2050">nt models, described below. 4.1 Centroid The centroid of a set of documents is a set of words that are statistically important to the cluster of documents. Centroid based summarization of a document set involves first creating the centroid of the documents, and then judging the salience of each document based on its similarity to the centroid of the document set. In our case, the input citing sentences represent the documents from which we extract the centroid. We use the centroid implementation from the publicly available summarization toolkit, MEAD (Radev et al., 2004). 4.2 Lexrank LexRank (Erkan and Radev, 2004) is a network based content selection algorithm that works by first building a graph of all the documents in a cluster. The edges between corresponding nodes represent the cosine similarity between them. Once the network is built, the algorithm computes the salience of sentences in this graph based on their eigenvector centrality in the network. 4.3 C-Lexrank C-Lexrank is another network based content selection algorithm that focuses on diversity (Qazvinian and Radev, 2008). Given a set of sentences, it first creates a network using these sentences and then runs a clustering algorithm to parti</context>
<context position="15578" citStr="Erkan and Radev, 2004" startWordPosition="2673" endWordPosition="2676"> relatively high scores for the random baseline is that our process to select the initial set of sentences eliminates many bad sentences. For example, for a subset of 5 topics, the total input set contains 1508 sentences, out of which 922 of the sentences (60%) have at least one factoid. This makes it highly likely to pick good content sentences even when we are picking sentences at random. We find that the Lexrank method outperforms other sentence selection methods on both evaluation metrics. The higher performance of Lexrank compared to Centroid is consistent with earlier published results (Erkan and Radev, 2004). The reason for the low performance of C-Lexrank as compared to Lexrank on this data set can be attributed to the fact that the input sentence set is derived from a much more diverse set of papers which can have a high diversity in lexical choice when describing the same factoid. Thus simple lexical similarity is not enough to find good clusters in this sentence set. The lower Unnormalized RU scores compared to Pyramid scores indicate that we are selecting sentences containing highly weighted factoids, but we do not select the most informative sentences that contain a large number of factoids</context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>G¨unes¸ Erkan and Dragomir R. Radev. 2004. Lexrank: Graph-based centrality as salience in text summarization. Journal of Artificial Intelligence Research (JAIR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cong Duy Vu Hoang</author>
<author>Min-Yen Kan</author>
</authors>
<title>Towards automated related work summarization.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10,</booktitle>
<pages>427--435</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2793" citStr="Hoang and Kan (2010)" startWordPosition="447" endWordPosition="450"> for generating and evaluating such surveys of scientific topics automatically starting from a phrase representing a topic area. We evaluate our system on a set of topics in the field of Natural Language Processing. In earlier work, (Teufel and Moens, 2002) have examined the problem of summarizing scientific articles using rhetorical analysis of sentences. Nanba and Okumura (1999) have also discussed the problem of generating surveys of multiple papers. Mohammad et al. (2009) presented experiments on generating surveys of scientific topics starting from papers to be summarized. More recently, Hoang and Kan (2010) have presented initial results on automatically generating related work section for a target paper by taking a hierarchical topic tree as an input. In this paper, we tackle the more challenging problem of summarizing a topic starting from a topic query. Our system takes as an input a string describing the topic area, selects the relevant papers from a corpus of papers, and then selects sentences from the citing sentences to these papers to generate a survey of the topic. A sample output of our system for the topic of “Word Sense Disambiguation” is shown in Figure 1. 2 Candidate Document Selec</context>
</contexts>
<marker>Hoang, Kan, 2010</marker>
<rawString>Cong Duy Vu Hoang and Min-Yen Kan. 2010. Towards automated related work summarization. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10, pages 427–435, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Saif Mohammad</author>
<author>Bonnie Dorr</author>
<author>Melissa Egan</author>
<author>Ahmed Hassan</author>
<author>Pradeep Muthukrishan</author>
<author>Vahed Qazvinian</author>
<author>Dragomir Radev</author>
<author>David Zajic</author>
</authors>
<title>Using citations to generate surveys of scientific paradigms.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09,</booktitle>
<pages>584--592</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2653" citStr="Mohammad et al. (2009)" startWordPosition="426" endWordPosition="429">bstracts are available for individual papers, but no such information is available for scientific topics. In this paper, we explore strategies for generating and evaluating such surveys of scientific topics automatically starting from a phrase representing a topic area. We evaluate our system on a set of topics in the field of Natural Language Processing. In earlier work, (Teufel and Moens, 2002) have examined the problem of summarizing scientific articles using rhetorical analysis of sentences. Nanba and Okumura (1999) have also discussed the problem of generating surveys of multiple papers. Mohammad et al. (2009) presented experiments on generating surveys of scientific topics starting from papers to be summarized. More recently, Hoang and Kan (2010) have presented initial results on automatically generating related work section for a target paper by taking a hierarchical topic tree as an input. In this paper, we tackle the more challenging problem of summarizing a topic starting from a topic query. Our system takes as an input a string describing the topic area, selects the relevant papers from a corpus of papers, and then selects sentences from the citing sentences to these papers to generate a surv</context>
</contexts>
<marker>Mohammad, Dorr, Egan, Hassan, Muthukrishan, Qazvinian, Radev, Zajic, 2009</marker>
<rawString>Saif Mohammad, Bonnie Dorr, Melissa Egan, Ahmed Hassan, Pradeep Muthukrishan, Vahed Qazvinian, Dragomir Radev, and David Zajic. 2009. Using citations to generate surveys of scientific paradigms. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL ’09, pages 584–592, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hidetsugu Nanba</author>
<author>Manabu Okumura</author>
</authors>
<title>Towards multi-paper summarization using reference information.</title>
<date>1999</date>
<booktitle>In Proceedings of the 16th International Joint Conference on Artificial Intelligence (IJCAI-99),</booktitle>
<pages>926--931</pages>
<contexts>
<context position="2556" citStr="Nanba and Okumura (1999)" startWordPosition="410" endWordPosition="413">at can generate such surveys automatically would be a useful tool. Short summaries in the form of abstracts are available for individual papers, but no such information is available for scientific topics. In this paper, we explore strategies for generating and evaluating such surveys of scientific topics automatically starting from a phrase representing a topic area. We evaluate our system on a set of topics in the field of Natural Language Processing. In earlier work, (Teufel and Moens, 2002) have examined the problem of summarizing scientific articles using rhetorical analysis of sentences. Nanba and Okumura (1999) have also discussed the problem of generating surveys of multiple papers. Mohammad et al. (2009) presented experiments on generating surveys of scientific topics starting from papers to be summarized. More recently, Hoang and Kan (2010) have presented initial results on automatically generating related work section for a target paper by taking a hierarchical topic tree as an input. In this paper, we tackle the more challenging problem of summarizing a topic starting from a topic query. Our system takes as an input a string describing the topic area, selects the relevant papers from a corpus o</context>
</contexts>
<marker>Nanba, Okumura, 1999</marker>
<rawString>Hidetsugu Nanba and Manabu Okumura. 1999. Towards multi-paper summarization using reference information. In Proceedings of the 16th International Joint Conference on Artificial Intelligence (IJCAI-99), pages 926–931.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Rebecca Passonneau</author>
</authors>
<title>Evaluating content selection in summarization: The pyramid method.</title>
<date>2004</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies (HLTNAACL ’04).</booktitle>
<contexts>
<context position="13603" citStr="Nenkova and Passonneau, 2004" startWordPosition="2333" endWordPosition="2336">94 Sentiment Analysis 0.75 0.78 0.77 0.78 Semantic Role Labeling 0.78 0.79 0.88 0.94 Dependency Parsing 0.67 0.38 0.71 0.53 Average 0.72 0.68 0.81* 0.76 Table 4: Results of pyramid evaluation for each of the three methods and the random baseline on each topic. the citing sentences to be used as the input to the summarization module as described in Section 2. Given this input, we generate 500 word summaries for each of the seven topics using the four methods: Centroid, Lexrank, C-Lexrank and a random baseline. For each summary, we compute two evaluation metrics. The first is the Pyramid score (Nenkova and Passonneau, 2004) computed by treating the factoids as Summary Content Units (SCU’s). The Pyramid scores for each summary is shown in Table 4. The second metric is an Unnormalized Relative Utility score (Radev and Tam, 2003), computed using the factoid scores of sentences based on the method presented in (Qazvinian, 2012). We call this Unnormalized RU since we are not able to normalize the scores with human generated gold summaries. The results for Unnormalized RU are shown in Table 5. The parameter α is the RU penalty for including a redundant sentence subsumed by an earlier sentence. If the summary chooses a</context>
</contexts>
<marker>Nenkova, Passonneau, 2004</marker>
<rawString>Ani Nenkova and Rebecca Passonneau. 2004. Evaluating content selection in summarization: The pyramid method. In Proceedings of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies (HLTNAACL ’04).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vahed Qazvinian</author>
<author>Dragomir R Radev</author>
</authors>
<title>Scientific paper summarization using citation summary networks.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (COLING-08),</booktitle>
<location>Manchester, UK.</location>
<contexts>
<context position="12318" citStr="Qazvinian and Radev, 2008" startWordPosition="2121" endWordPosition="2124">he centroid implementation from the publicly available summarization toolkit, MEAD (Radev et al., 2004). 4.2 Lexrank LexRank (Erkan and Radev, 2004) is a network based content selection algorithm that works by first building a graph of all the documents in a cluster. The edges between corresponding nodes represent the cosine similarity between them. Once the network is built, the algorithm computes the salience of sentences in this graph based on their eigenvector centrality in the network. 4.3 C-Lexrank C-Lexrank is another network based content selection algorithm that focuses on diversity (Qazvinian and Radev, 2008). Given a set of sentences, it first creates a network using these sentences and then runs a clustering algorithm to partition the network into smaller clusters that represent different aspects of the paper. The motivation behind the clustering is to include more diverse facts in the summary. 5 Experiments and Results To do an evaluation of our different content selection methods, we first select the documents using our Restricted Expansion method, and then pick 574 Topic Rand Cent LR C-LR Summarization 0.68 0.61 0.91 0.82 Question Answering 0.52 0.50 0.65 0.56 Word Sense Disambiguation 0.78 0</context>
</contexts>
<marker>Qazvinian, Radev, 2008</marker>
<rawString>Vahed Qazvinian and Dragomir R. Radev. 2008. Scientific paper summarization using citation summary networks. In Proceedings of the 22nd International Conference on Computational Linguistics (COLING-08), Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vahed Qazvinian</author>
</authors>
<title>Using Collective Discourse to Generate Surveys of Scientific Paradigms.</title>
<date>2012</date>
<tech>Ph.D. thesis.</tech>
<contexts>
<context position="13909" citStr="Qazvinian, 2012" startWordPosition="2387" endWordPosition="2388">ization module as described in Section 2. Given this input, we generate 500 word summaries for each of the seven topics using the four methods: Centroid, Lexrank, C-Lexrank and a random baseline. For each summary, we compute two evaluation metrics. The first is the Pyramid score (Nenkova and Passonneau, 2004) computed by treating the factoids as Summary Content Units (SCU’s). The Pyramid scores for each summary is shown in Table 4. The second metric is an Unnormalized Relative Utility score (Radev and Tam, 2003), computed using the factoid scores of sentences based on the method presented in (Qazvinian, 2012). We call this Unnormalized RU since we are not able to normalize the scores with human generated gold summaries. The results for Unnormalized RU are shown in Table 5. The parameter α is the RU penalty for including a redundant sentence subsumed by an earlier sentence. If the summary chooses a sentence sZ with score woTZg that is subsumed by an earlier summary sentence, the score is reduced as wsubsumed = (α ∗ woTZg). We approximate subsumption by marking a sentence sj as being subsumed by sZ if Fj ⊂ FZ, where FZ and Fj are sets of factoids covered in each sentence. Topic Rand Cent LR C-LR Sum</context>
</contexts>
<marker>Qazvinian, 2012</marker>
<rawString>Vahed Qazvinian. 2012. Using Collective Discourse to Generate Surveys of Scientific Paradigms. Ph.D. thesis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Daniel Tam</author>
</authors>
<title>Summarization evaluation using relative utility.</title>
<date>2003</date>
<booktitle>In CIKM2003,</booktitle>
<pages>508--511</pages>
<contexts>
<context position="13810" citStr="Radev and Tam, 2003" startWordPosition="2369" endWordPosition="2372">thods and the random baseline on each topic. the citing sentences to be used as the input to the summarization module as described in Section 2. Given this input, we generate 500 word summaries for each of the seven topics using the four methods: Centroid, Lexrank, C-Lexrank and a random baseline. For each summary, we compute two evaluation metrics. The first is the Pyramid score (Nenkova and Passonneau, 2004) computed by treating the factoids as Summary Content Units (SCU’s). The Pyramid scores for each summary is shown in Table 4. The second metric is an Unnormalized Relative Utility score (Radev and Tam, 2003), computed using the factoid scores of sentences based on the method presented in (Qazvinian, 2012). We call this Unnormalized RU since we are not able to normalize the scores with human generated gold summaries. The results for Unnormalized RU are shown in Table 5. The parameter α is the RU penalty for including a redundant sentence subsumed by an earlier sentence. If the summary chooses a sentence sZ with score woTZg that is subsumed by an earlier summary sentence, the score is reduced as wsubsumed = (α ∗ woTZg). We approximate subsumption by marking a sentence sj as being subsumed by sZ if </context>
</contexts>
<marker>Radev, Tam, 2003</marker>
<rawString>Dragomir R. Radev and Daniel Tam. 2003. Summarization evaluation using relative utility. In CIKM2003, pages 508–511.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Dragomir Radev</author>
<author>Timothy Allison</author>
<author>Sasha BlairGoldensohn</author>
<author>John Blitzer</author>
<author>Arda C¸elebi</author>
<author>Stanko Dimitrov</author>
<author>Elliott Drabek</author>
<author>Ali Hakim</author>
</authors>
<location>Wai Lam, Danyu Liu, Jahna Otterbacher, Hong Qi, Horacio Saggion, Simone Teufel, Michael Topper, Adam</location>
<marker>Radev, Allison, BlairGoldensohn, Blitzer, C¸elebi, Dimitrov, Drabek, Hakim, </marker>
<rawString>Dragomir Radev, Timothy Allison, Sasha BlairGoldensohn, John Blitzer, Arda C¸elebi, Stanko Dimitrov, Elliott Drabek, Ali Hakim, Wai Lam, Danyu Liu, Jahna Otterbacher, Hong Qi, Horacio Saggion, Simone Teufel, Michael Topper, Adam</rawString>
</citation>
<citation valid="true">
<authors>
<author>Winkel</author>
<author>Zhu Zhang</author>
</authors>
<title>MEAD - a platform for multidocument multilingual text summarization.</title>
<date>2004</date>
<booktitle>In LREC 2004,</booktitle>
<location>Lisbon, Portugal,</location>
<marker>Winkel, Zhang, 2004</marker>
<rawString>Winkel, and Zhu Zhang. 2004. MEAD - a platform for multidocument multilingual text summarization. In LREC 2004, Lisbon, Portugal, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Pradeep Muthukrishnan</author>
<author>Vahed Qazvinian</author>
<author>Amjad Abu-Jbara</author>
</authors>
<title>The acl anthology network corpus. Language Resources and Evaluation,</title>
<date>2013</date>
<pages>1--26</pages>
<contexts>
<context position="9504" citStr="Radev et al., 2013" startWordPosition="1646" endWordPosition="1649">X X X X X X X 9 wordnet X X X X X X X X 8 knowledge based wsd X X X X X X X 7 supervised wsd X X X X X X X 7 senseval X X X X X X X 7 definition of word senses X X X X X X 7 knowledge based wsd using machine readable dictionaries X X X X X X 6 unsupervised wsd X X X X X X 6 bootstrapping algorithms X X X X X X 6 supervised wsd using decision lists X X X X X X 6 Table 3: Top 10 factoids for the topic of “Word Sense Disambiguation” and their distribution across various data sources. 3 Evaluation Data for Survey Generation We use the ACL Anthology Network (AAN) as the corpus for our experiments (Radev et al., 2013). We built a factoid inventory for seven topics in NLP based on manual written surveys in the following way. For each topic, we found at least 3 recent tutorials and 3 recent surveys on the topic and extracted the factoids that are covered in each of them. Table 2 shows the complete list of material collected for the topic of “Word Sense Disambiguation”. We found around 80 factoids per topic on an average. Once the factoids were extracted, each factoid was assigned a weight based on the number of documents it appears in, and any factoids with weight one were removed. Table 3 shows the top ten </context>
</contexts>
<marker>Radev, Muthukrishnan, Qazvinian, Abu-Jbara, 2013</marker>
<rawString>Dragomir R. Radev, Pradeep Muthukrishnan, Vahed Qazvinian, and Amjad Abu-Jbara. 2013. The acl anthology network corpus. Language Resources and Evaluation, pages 1–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Teufel</author>
<author>Marc Moens</author>
</authors>
<title>Summarizing scientific articles: experiments with relevance and rhetorical status.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>4</issue>
<contexts>
<context position="2430" citStr="Teufel and Moens, 2002" startWordPosition="393" endWordPosition="396">lished in the field. 1Adam Lopez. 2008. Statistical machine translation. ACM Comput. Surv. 40, 3, Article 8 Thus, a system that can generate such surveys automatically would be a useful tool. Short summaries in the form of abstracts are available for individual papers, but no such information is available for scientific topics. In this paper, we explore strategies for generating and evaluating such surveys of scientific topics automatically starting from a phrase representing a topic area. We evaluate our system on a set of topics in the field of Natural Language Processing. In earlier work, (Teufel and Moens, 2002) have examined the problem of summarizing scientific articles using rhetorical analysis of sentences. Nanba and Okumura (1999) have also discussed the problem of generating surveys of multiple papers. Mohammad et al. (2009) presented experiments on generating surveys of scientific topics starting from papers to be summarized. More recently, Hoang and Kan (2010) have presented initial results on automatically generating related work section for a target paper by taking a hierarchical topic tree as an input. In this paper, we tackle the more challenging problem of summarizing a topic starting fr</context>
</contexts>
<marker>Teufel, Moens, 2002</marker>
<rawString>Simone Teufel and Marc Moens. 2002. Summarizing scientific articles: experiments with relevance and rhetorical status. Computational Linguistics, 28(4):409–445.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>