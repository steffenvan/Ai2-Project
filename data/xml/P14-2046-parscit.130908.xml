<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000088">
<title confidence="0.883408">
How to Speak a Language without Knowing It
</title>
<author confidence="0.99045">
Xing Shi and Kevin Knight
</author>
<affiliation confidence="0.997508">
Information Sciences Institute
Computer Science Department
University of Southern California
</affiliation>
<email confidence="0.99307">
{xingshi, knight}@isi.edu
</email>
<sectionHeader confidence="0.997291" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999880444444444">
We develop a system that lets people over-
come language barriers by letting them
speak a language they do not know. Our
system accepts text entered by a user,
translates the text, then converts the trans-
lation into a phonetic spelling in the user’s
own orthography. We trained the sys-
tem on phonetic spellings in travel phrase-
books.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.960359857142857">
Can people speak a language they don’t know?
Actually, it happens frequently. Travel phrase-
books contain phrases in the speaker’s language
(e.g., “thank you”) paired with foreign-language
translations (e.g., “ ciiacH6o”). Since the speaker
may not be able to pronounce the foreign-language
orthography, phrasebooks additionally provide
phonetic spellings that approximate the sounds of
the foreign phrase. These spellings employ the fa-
miliar writing system and sounds of the speaker’s
language. Here is a sample entry from a French
phrasebook for English speakers:
English: Leave me alone.
French: Laissez-moi tranquille.
Franglish: Less-ay mwah trahn-KEEL.
The user ignores the French and goes straight
to the Franglish. If the Franglish is well designed,
an English speaker can pronounce it and be under-
stood by a French listener.
Figure 1 shows a sample entry from another
book—an English phrasebook for Chinese speak-
ers. If a Chinese speaker wants to say “非 常
感谢你这顿美餐”, she need only read off the
Chinglish “三可 油 否 热斯 弯德否 米欧”, which
approximates the sounds of “Thank you for this
wonderful meal” using Chinese characters.
Phrasebooks permit a form of accurate, per-
sonal, oral communication that speech-to-speech
</bodyText>
<author confidence="0.752507">
Heng Ji
</author>
<affiliation confidence="0.799667333333333">
Computer Science Department
Rensselaer Polytechnic Institute
Troy, NY 12180, USA
</affiliation>
<email confidence="0.861091">
jih@rpi.edu
</email>
<figureCaption confidence="0.997977">
Figure 1: Snippet from phrasebook
</figureCaption>
<bodyText confidence="0.87706725">
translation devices lack. However, the user is lim-
ited to a small set of fixed phrases. In this paper,
we lift this restriction by designing and evaluating
a software program with the following:
</bodyText>
<listItem confidence="0.990921833333333">
• Input: Text entered by the speaker, in her own
language.
• Output: Phonetic rendering of a foreign-
language translation of that text, which, when
pronounced by the speaker, can be under-
stood by the listener.
</listItem>
<bodyText confidence="0.999910142857143">
The main challenge is that different languages
have different orthographies, different phoneme
inventories, and different phonotactic constraints,
so mismatches are inevitable. Despite this, the
system’s output should be both unambiguously
pronounceable by the speaker and readily under-
stood by the listener.
Our goal is to build an application that covers
many language pairs and directions. The current
paper describes a single system that lets a Chinese
person speak English.
We take a statistical modeling approach to this
problem, as is done in two lines of research that are
most related. The first is machine transliteration
(Knight and Graehl, 1998), in which names and
technical terms are translated across languages
with different sound systems. The other is re-
spelling generation (Hauer and Kondrak, 2013),
where an English speaker is given a phonetic hint
about how to pronounce a rare or foreign word
to another English speaker. By contrast, we aim
</bodyText>
<page confidence="0.954133">
278
</page>
<note confidence="0.4502965">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 278–282,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<table confidence="0.994704666666667">
Chinese 已经k点T
English It’s eight o’clock now
Chinglish ;C思埃4+9t劳tN (yi si ai te e ke lao ke nao)
Chinese 这#衬衫5L时髦5L便宜
English this shirt is very stylish and not very expensive
Chinglish 迪思舍4+;C思A锐思掉ill失安的N4+A锐伊t思班西Ji-
Chinese 我Ill外送的最低金9是15美金
English our minimum charge for delivery is fifteen dollars
Chinglish 奥?米尼Ill差2佛低ill沃锐;C思&amp;Ji-听到Jq�,思
</table>
<tableCaption confidence="0.999884">
Table 1: Examples of &lt;Chinese, English, Chinglish&gt; tuples from a phrasebook.
</tableCaption>
<bodyText confidence="0.9772515">
to help people issue full utterances that cross lan-
guage barriers.
</bodyText>
<sectionHeader confidence="0.997323" genericHeader="introduction">
2 Evaluation
</sectionHeader>
<bodyText confidence="0.9991524">
Our system’s input is Chinese. The output is
a string of Chinese characters that approximate
English sounds, which we call Chinglish. We
build several candidate Chinese-to-Chinglish sys-
tems and evaluate them as follows:
</bodyText>
<listItem confidence="0.987535272727273">
• We compute the normalized edit distance
between the system’s output and a human-
generated Chinglish reference.
• A Chinese speaker pronounces the system’s
output out loud, and an English listener takes
dictation. We measure the normalized edit
distance against an English reference.
• We automate the previous evaluation by re-
place the two humans with: (1) a Chinese
speech synthesizer, and (2) a English speech
recognizer.
</listItem>
<sectionHeader confidence="0.987895" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.999989769230769">
We seek to imitate phonetic transformations found
in phrasebooks, so phrasebooks themselves are a
good source of training data. We obtained a col-
lection of 1312 &lt;Chinese, English, Chinglish&gt;
phrasebook tuples 1 (see Table 1).
We use 1182 utterances for training, 65 for de-
velopment, and 65 for test. We know of no other
computational work on this type of corpus.
Our Chinglish has interesting gross empirical
properties. First, because Chinglish and Chinese
are written with the same characters, they render
the same inventory of 416 distinct syllables. How-
ever, the distribution of Chinglish syllables differs
</bodyText>
<footnote confidence="0.828712">
1Dataset can be found at http://www.isi.edu/
natural-language/mt/chinglish-data.txt
</footnote>
<bodyText confidence="0.9996802">
a great deal from Chinese (Table 2). Syllables “si”
and “te” are very popular, because while conso-
nant clusters like English “st” are impossible to re-
produce exactly, the particular vowels in “si” and
“te” are fortunately very weak.
</bodyText>
<figure confidence="0.662950666666667">
Frequency Rank Chinese Chinglish
1 de si
2 shi te
3 yi de
4 ji yi
5 zhi fu
</figure>
<tableCaption confidence="0.62606">
Table 2: Top 5 frequent syllables in Chinese
(McEnery and Xiao, 2004) and Chinglish
</tableCaption>
<bodyText confidence="0.999902571428571">
We find that multiple occurrences of an English
word type are generally associated with the same
Chinglish sequence. Also, Chinglish characters do
not generally span multiple English words. It is
reasonable for “can I” to be rendered as “kan nai”,
with “nai” spanning both English words, but this
is rare.
</bodyText>
<sectionHeader confidence="0.993404" genericHeader="method">
4 Model
</sectionHeader>
<bodyText confidence="0.988494928571429">
We model Chinese-to-Chinglish translation with
a cascade of weighted finite-state transducers
(wFST), shown in Figure 2. We use an online
MT system to convert Chinese to an English word
sequence (Eword), which is then passed through
FST A to generate an English sound sequence
(Epron). FST A is constructed from the CMU Pro-
nouncing Dictionary (Weide, 2007).
Next, wFST B translates English sounds into
Chinese sounds (Pinyin-split). Pinyin is an official
syllable-based romanization of Mandarin Chinese
characters, and Pinyin-split is a standard separa-
tion of Pinyin syllables into initial and final parts.
Our wFST allows one English sound token to map
</bodyText>
<page confidence="0.99623">
279
</page>
<figureCaption confidence="0.993008">
Figure 2: Finite-state cascade for modeling the re-
lation between Chinese and Chinglish.
</figureCaption>
<bodyText confidence="0.994240714285714">
to one or two Pinyin-split tokens, and it also allows
two English sounds to map to one Pinyin-split to-
ken.
Finally, FST C converts Pinyin-split into Pinyin,
and FST D chooses Chinglish characters. We also
experiment with an additional wFST E that trans-
lates English words directly into Chinglish.
</bodyText>
<sectionHeader confidence="0.995432" genericHeader="method">
5 Training
</sectionHeader>
<bodyText confidence="0.96858">
FSTs A, C, and D are unweighted, and remain so
throughout this paper.
</bodyText>
<subsectionHeader confidence="0.775086">
5.1 Phoneme-based model
</subsectionHeader>
<bodyText confidence="0.926264636363636">
We must now estimate the values of FST B pa-
rameters, such as P(si|S). To do this, we first
take our phrasebook triples and construct sample
string pairs &lt;Epron, Pinyin-split&gt; by pronounc-
ing the phrasebook English with FST A, and by
pronouncing the phrasebook Chinglish with FSTs
D and C. Then we run the EM algorithm to learn
FST B parameters (Table 3) and Viterbi align-
ments, such as:
g r ae n d
g e r uan d e
</bodyText>
<subsectionHeader confidence="0.87938">
5.2 Phoneme-phrase-based model
</subsectionHeader>
<bodyText confidence="0.896478333333333">
Mappings between phonemes are context-
sensitive. For example, when we decode English
“grandmother”, we get:
</bodyText>
<table confidence="0.999727">
labeled Epron Pinyin-split P(p|e)
d d 0.46
d e 0.40
d i 0.06
s 0.01
ao r u 0.26
o 0.13
ao 0.06
ou 0.01
</table>
<tableCaption confidence="0.9508245">
Table 3: Learned translation tables for the
phoneme based model
</tableCaption>
<bodyText confidence="0.876995636363636">
g r ae n d m ah dh er
g e r an d e m u e d e
where as the reference Pinyin-split sequence is:
g e r uan d e m a d e
Here, “ae n” should be decoded as “uan” when
preceded by “r”. Following phrase-based meth-
ods in statistical machine translation (Koehn et
al., 2003) and machine transliteration (Finch and
Sumita, 2008), we model substitution of longer se-
quences. First, we obtain Viterbi alignments using
the phoneme-based model, e.g.:
</bodyText>
<equation confidence="0.8618945">
g r ae n d m ah dh er
g e r uan d e m a d e
</equation>
<bodyText confidence="0.9966748">
Second, we extract phoneme phrase pairs con-
sistent with these alignments. We use no phrase-
size limit, but we do not cross word boundaries.
From the example above, we pull out phrase pairs
like:
</bodyText>
<equation confidence="0.993786">
g → g e
g r → g e r
...
r → r
r ae n → r uan
</equation>
<bodyText confidence="0.874953333333333">
...
We add these phrase pairs to FST B, and call
this the phoneme-phrase-based model.
</bodyText>
<subsectionHeader confidence="0.960797">
5.3 Word-based model
</subsectionHeader>
<bodyText confidence="0.999918111111111">
We now turn to WFST E, which short-cuts di-
rectly from English words to Pinyin. We create
&lt;English, Pinyin&gt; training pairs from our phrase-
book simply by pronouncing the Chinglish with
FST D. We initially allow each English word type
to map to any sequence of Pinyin, up to length 7,
with uniform probability. EM learns values for pa-
rameters like P(nai te|night), plus Viterbi align-
ments such as:
</bodyText>
<page confidence="0.99154">
280
</page>
<table confidence="0.999590875">
Model Top-1 Valid Coverage
Top-1 Overall Average Edit Distance
Average Edit Distance
Word based 0.664 0.042 29/65
Word-based hybrid training 0.659 0.029 29/65
Phoneme based 0.611 0.583 63/65
Phoneme-phrase based 0.194 0.136 63/65
Hybrid training and decoding 0.175 0.115 63/65
</table>
<tableCaption confidence="0.994782333333333">
Table 4: English-to-Pinyin decoding accuracy on a test set of 65 utterances. Numbers are average edit
distances between system output and Pinyin references. Valid average edit distance is calculated based
only on valid outputs (e.g. 29 outputs for word based model).
</tableCaption>
<bodyText confidence="0.904192">
accept
a ke sha pu
Notice that this model makes alignment errors
due to sparser data (e.g., the word “tips” and “ti pu
si” only appear once each in the training data).
</bodyText>
<subsectionHeader confidence="0.996249">
5.4 Hybrid training
</subsectionHeader>
<bodyText confidence="0.999744384615385">
To improve the accuracy of word-based EM align-
ment, we use the phoneme based model to de-
code each English word in the training data to
Pinyin. From the 100-best list of decodings, we
collect combinations of start/end Pinyin syllables
for the word. We then modify the initial, uniform
English-to-Pinyin mapping probabilities by giving
higher initial weight to mappings that respect ob-
served start/end pairs. When we run EM, we find
that alignment errors for “tips” in section 5.3 are
fixed:
the test portion of our phrasebook, using edit dis-
tance. Here, we start with reference English and
measure the accuracy of Pinyin syllable produc-
tion, since the choice of Chinglish character does
not affect the Chinglish pronunciation. We see that
the Word-based method has very high accuracy,
but low coverage. Our best system uses the Hy-
brid training/decoding method. As Table 6 shows,
the ratio of unseen English word tokens is small,
thus large portion of tokens are transformed us-
ing word-based method. The average edit dis-
tance of phoneme-phrase model and that of hy-
brid training/decoding model are close, indicating
that long phoneme-phrase pairs can emulate word-
pinyin mappings.
</bodyText>
<table confidence="0.999357">
Unseen Total Ratio
Word Type 62 249 0.249
Token 62 436 0.142
</table>
<tableCaption confidence="0.9860675">
Table 6: Unseen English word type and tokens in
test data.
</tableCaption>
<bodyText confidence="0.62176025">
tips
te ti pu si
accept tips
a ke sha pu te ti pu si
</bodyText>
<subsectionHeader confidence="0.986172">
5.5 Hybrid decoding
</subsectionHeader>
<bodyText confidence="0.99990775">
The word-based model can only decode 29 of the
65 test utterances, because wFST E fails if an ut-
terance contains a new English word type, pre-
viously unseen in training. The phoneme-based
models are more robust, able to decode 63 of the
65 utterances, failing only when some English
word type falls outside the CMU pronouncing dic-
tionary (FST A).
Our final model combines these two, using the
word-based model for known English words, and
the phoneme-based models for unknown English
words.
</bodyText>
<sectionHeader confidence="0.999849" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.935668916666667">
Our first evaluation (Table 4) is intrinsic, measur-
ing our Chinglish output against references from
Table 7: Chinglish-to-English accuracy in dicta-
tion task.
Our second evaluation is a dictation task. We
speak our Chinglish character sequence output
aloud and ask an English monolingual person to
transcribe it. (Actually, we use a Chinese synthe-
sizer to remove bias.) Then we measure edit dis-
tance between the human transcription and the ref-
erence English from our phrasebook. Results are
shown in Table 7.
</bodyText>
<figure confidence="0.917556571428571">
Valid Average
Edit Distance
Model
Phoneme based
0.696
Reference English 0.477
Hybrid training and decoding 0.496
</figure>
<page confidence="0.900108">
281
</page>
<table confidence="0.997166666666667">
Chinese 年夜饭都要吃些什么
Reference English what do you have for the Reunion dinner
Reference Chinglish 沃特 杜 5L 海夫 佛 则 锐5L尼恩 低呢
Hybrid training/decoding Chinglish 我忒 度 优 嗨佛 佛 得 瑞优你恩 低呢
Dictation English what do you have for the reunion dinner
ASR English what do you high for 43 Union Cena
Chinese 等等我
Reference English wait for me
Reference Chinglish 唯特 佛 密 (wei te fo mi)
Hybrid training/decoding Chinglish 位忒 佛 密 (wei te fo mi)
Dictation English wait for me
ASR English wait for me
</table>
<tableCaption confidence="0.986538">
Table 5: Chinglish generated by hybrid training and decoding method and corresponding recognized
English by dictation and automatic synthesis-recognition method.
</tableCaption>
<table confidence="0.999555714285714">
Model Valid Average
Edit Distance
Word based 0.925
Word-based hybrid training 0.925
Phoneme based 0.937
Phoneme-phrase based 0.896
Hybrid training and decoding 0.898
</table>
<tableCaption confidence="0.8440465">
Table 8: Chinglish-to-English accuracy in auto-
matic synthesis-recognition (ASR) task. Numbers
</tableCaption>
<bodyText confidence="0.950645">
are average edit distance between recognized En-
glish and reference English.
Finally, we repeat the last experiment, but re-
moving the human from the loop, using both
automatic Chinese speech synthesis and English
speech recognition. Results are shown in Table 8.
Speech recognition is more fragile than human
transcription, so edit distances are greater. Table 5
shows a few examples of the Chinglish generated
by the hybrid training and decoding method, as
well as the recognized English from the dictation
and ASR tasks.
</bodyText>
<sectionHeader confidence="0.999584" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.99996925">
Our work aims to help people speak foreign lan-
guages they don’t know, by providing native pho-
netic spellings that approximate the sounds of for-
eign phrases. We use a cascade of finite-state
transducers to accomplish the task. We improve
the model by adding phrases, word boundary con-
straints, and improved alignment.
In the future, we plan to cover more language
pairs and directions. Each target language raises
interesting new challenges that come from its nat-
ural constraints on allowed phonemes, syllables,
words, and orthography.
</bodyText>
<sectionHeader confidence="0.999446" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999251217391304">
Andrew Finch and Eiichiro Sumita. 2008. Phrase-
based machine transliteration. In Proceedings of the
Workshop on Technologies and Corpora for Asia-
Pacific Speech Translation (TCAST), pages 13–18.
Bradley Hauer and Grzegorz Kondrak. 2013. Auto-
matic generation of English respellings. In Proceed-
ings of NAACL-HLT, pages 634–643.
Kevin Knight and Jonathan Graehl. 1998. Ma-
chine transliteration. Computational Linguistics,
24(4):599–612.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 48–54. Association for Computa-
tional Linguistics.
Anthony McEnery and Zhonghua Xiao. 2004. The
lancaster corpus of Mandarin Chinese: A corpus for
monolingual and contrastive language study. Reli-
gion, 17:3–4.
R Weide. 2007. The CMU pronunciation dictionary,
release 0.7a.
</reference>
<page confidence="0.997422">
282
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.884585">
<title confidence="0.998785">How to Speak a Language without Knowing It</title>
<author confidence="0.984259">Xing Shi</author>
<author confidence="0.984259">Kevin</author>
<affiliation confidence="0.998696333333333">Information Sciences Computer Science University of Southern</affiliation>
<email confidence="0.999777">xingshi@isi.edu</email>
<email confidence="0.999777">knight@isi.edu</email>
<abstract confidence="0.9899135">We develop a system that lets people overcome language barriers by letting them speak a language they do not know. Our system accepts text entered by a user, translates the text, then converts the translation into a phonetic spelling in the user’s own orthography. We trained the system on phonetic spellings in travel phrasebooks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Andrew Finch</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Phrasebased machine transliteration.</title>
<date>2008</date>
<booktitle>In Proceedings of the Workshop on Technologies and Corpora for AsiaPacific Speech Translation (TCAST),</booktitle>
<pages>13--18</pages>
<contexts>
<context position="8208" citStr="Finch and Sumita, 2008" startWordPosition="1315" endWordPosition="1318"> 5.2 Phoneme-phrase-based model Mappings between phonemes are contextsensitive. For example, when we decode English “grandmother”, we get: labeled Epron Pinyin-split P(p|e) d d 0.46 d e 0.40 d i 0.06 s 0.01 ao r u 0.26 o 0.13 ao 0.06 ou 0.01 Table 3: Learned translation tables for the phoneme based model g r ae n d m ah dh er g e r an d e m u e d e where as the reference Pinyin-split sequence is: g e r uan d e m a d e Here, “ae n” should be decoded as “uan” when preceded by “r”. Following phrase-based methods in statistical machine translation (Koehn et al., 2003) and machine transliteration (Finch and Sumita, 2008), we model substitution of longer sequences. First, we obtain Viterbi alignments using the phoneme-based model, e.g.: g r ae n d m ah dh er g e r uan d e m a d e Second, we extract phoneme phrase pairs consistent with these alignments. We use no phrasesize limit, but we do not cross word boundaries. From the example above, we pull out phrase pairs like: g → g e g r → g e r ... r → r r ae n → r uan ... We add these phrase pairs to FST B, and call this the phoneme-phrase-based model. 5.3 Word-based model We now turn to WFST E, which short-cuts directly from English words to Pinyin. We create &lt;En</context>
</contexts>
<marker>Finch, Sumita, 2008</marker>
<rawString>Andrew Finch and Eiichiro Sumita. 2008. Phrasebased machine transliteration. In Proceedings of the Workshop on Technologies and Corpora for AsiaPacific Speech Translation (TCAST), pages 13–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bradley Hauer</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>Automatic generation of English respellings.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>634--643</pages>
<contexts>
<context position="3115" citStr="Hauer and Kondrak, 2013" startWordPosition="480" endWordPosition="483">he system’s output should be both unambiguously pronounceable by the speaker and readily understood by the listener. Our goal is to build an application that covers many language pairs and directions. The current paper describes a single system that lets a Chinese person speak English. We take a statistical modeling approach to this problem, as is done in two lines of research that are most related. The first is machine transliteration (Knight and Graehl, 1998), in which names and technical terms are translated across languages with different sound systems. The other is respelling generation (Hauer and Kondrak, 2013), where an English speaker is given a phonetic hint about how to pronounce a rare or foreign word to another English speaker. By contrast, we aim 278 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 278–282, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Chinese 已经k点T English It’s eight o’clock now Chinglish ;C思埃4+9t劳tN (yi si ai te e ke lao ke nao) Chinese 这#衬衫5L时髦5L便宜 English this shirt is very stylish and not very expensive Chinglish 迪思舍4+;C思A锐思掉ill失安的N4+A锐伊t思班西JiChinese 我Ill外送的最低金9是15美</context>
</contexts>
<marker>Hauer, Kondrak, 2013</marker>
<rawString>Bradley Hauer and Grzegorz Kondrak. 2013. Automatic generation of English respellings. In Proceedings of NAACL-HLT, pages 634–643.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Jonathan Graehl</author>
</authors>
<date>1998</date>
<booktitle>Machine transliteration. Computational Linguistics,</booktitle>
<pages>24--4</pages>
<contexts>
<context position="2956" citStr="Knight and Graehl, 1998" startWordPosition="456" endWordPosition="459">ent languages have different orthographies, different phoneme inventories, and different phonotactic constraints, so mismatches are inevitable. Despite this, the system’s output should be both unambiguously pronounceable by the speaker and readily understood by the listener. Our goal is to build an application that covers many language pairs and directions. The current paper describes a single system that lets a Chinese person speak English. We take a statistical modeling approach to this problem, as is done in two lines of research that are most related. The first is machine transliteration (Knight and Graehl, 1998), in which names and technical terms are translated across languages with different sound systems. The other is respelling generation (Hauer and Kondrak, 2013), where an English speaker is given a phonetic hint about how to pronounce a rare or foreign word to another English speaker. By contrast, we aim 278 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 278–282, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Chinese 已经k点T English It’s eight o’clock now Chinglish ;C思埃4+9t劳tN (yi si ai te </context>
</contexts>
<marker>Knight, Graehl, 1998</marker>
<rawString>Kevin Knight and Jonathan Graehl. 1998. Machine transliteration. Computational Linguistics, 24(4):599–612.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume 1,</booktitle>
<pages>48--54</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8155" citStr="Koehn et al., 2003" startWordPosition="1308" endWordPosition="1311">rbi alignments, such as: g r ae n d g e r uan d e 5.2 Phoneme-phrase-based model Mappings between phonemes are contextsensitive. For example, when we decode English “grandmother”, we get: labeled Epron Pinyin-split P(p|e) d d 0.46 d e 0.40 d i 0.06 s 0.01 ao r u 0.26 o 0.13 ao 0.06 ou 0.01 Table 3: Learned translation tables for the phoneme based model g r ae n d m ah dh er g e r an d e m u e d e where as the reference Pinyin-split sequence is: g e r uan d e m a d e Here, “ae n” should be decoded as “uan” when preceded by “r”. Following phrase-based methods in statistical machine translation (Koehn et al., 2003) and machine transliteration (Finch and Sumita, 2008), we model substitution of longer sequences. First, we obtain Viterbi alignments using the phoneme-based model, e.g.: g r ae n d m ah dh er g e r uan d e m a d e Second, we extract phoneme phrase pairs consistent with these alignments. We use no phrasesize limit, but we do not cross word boundaries. From the example above, we pull out phrase pairs like: g → g e g r → g e r ... r → r r ae n → r uan ... We add these phrase pairs to FST B, and call this the phoneme-phrase-based model. 5.3 Word-based model We now turn to WFST E, which short-cuts</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume 1, pages 48–54. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony McEnery</author>
<author>Zhonghua Xiao</author>
</authors>
<title>The lancaster corpus of Mandarin Chinese: A corpus for monolingual and contrastive language study.</title>
<date>2004</date>
<journal>Religion,</journal>
<pages>17--3</pages>
<contexts>
<context position="5698" citStr="McEnery and Xiao, 2004" startWordPosition="878" endWordPosition="881"> Chinese are written with the same characters, they render the same inventory of 416 distinct syllables. However, the distribution of Chinglish syllables differs 1Dataset can be found at http://www.isi.edu/ natural-language/mt/chinglish-data.txt a great deal from Chinese (Table 2). Syllables “si” and “te” are very popular, because while consonant clusters like English “st” are impossible to reproduce exactly, the particular vowels in “si” and “te” are fortunately very weak. Frequency Rank Chinese Chinglish 1 de si 2 shi te 3 yi de 4 ji yi 5 zhi fu Table 2: Top 5 frequent syllables in Chinese (McEnery and Xiao, 2004) and Chinglish We find that multiple occurrences of an English word type are generally associated with the same Chinglish sequence. Also, Chinglish characters do not generally span multiple English words. It is reasonable for “can I” to be rendered as “kan nai”, with “nai” spanning both English words, but this is rare. 4 Model We model Chinese-to-Chinglish translation with a cascade of weighted finite-state transducers (wFST), shown in Figure 2. We use an online MT system to convert Chinese to an English word sequence (Eword), which is then passed through FST A to generate an English sound seq</context>
</contexts>
<marker>McEnery, Xiao, 2004</marker>
<rawString>Anthony McEnery and Zhonghua Xiao. 2004. The lancaster corpus of Mandarin Chinese: A corpus for monolingual and contrastive language study. Religion, 17:3–4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Weide</author>
</authors>
<title>The CMU pronunciation dictionary, release 0.7a.</title>
<date>2007</date>
<contexts>
<context position="6383" citStr="Weide, 2007" startWordPosition="991" endWordPosition="992">are generally associated with the same Chinglish sequence. Also, Chinglish characters do not generally span multiple English words. It is reasonable for “can I” to be rendered as “kan nai”, with “nai” spanning both English words, but this is rare. 4 Model We model Chinese-to-Chinglish translation with a cascade of weighted finite-state transducers (wFST), shown in Figure 2. We use an online MT system to convert Chinese to an English word sequence (Eword), which is then passed through FST A to generate an English sound sequence (Epron). FST A is constructed from the CMU Pronouncing Dictionary (Weide, 2007). Next, wFST B translates English sounds into Chinese sounds (Pinyin-split). Pinyin is an official syllable-based romanization of Mandarin Chinese characters, and Pinyin-split is a standard separation of Pinyin syllables into initial and final parts. Our wFST allows one English sound token to map 279 Figure 2: Finite-state cascade for modeling the relation between Chinese and Chinglish. to one or two Pinyin-split tokens, and it also allows two English sounds to map to one Pinyin-split token. Finally, FST C converts Pinyin-split into Pinyin, and FST D chooses Chinglish characters. We also exper</context>
</contexts>
<marker>Weide, 2007</marker>
<rawString>R Weide. 2007. The CMU pronunciation dictionary, release 0.7a.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>