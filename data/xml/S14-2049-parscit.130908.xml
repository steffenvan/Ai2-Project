<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000310">
<title confidence="0.9943965">
haLF: Comparing a Pure CDSM Approach with a Standard
Machine Learning System for RTE
</title>
<author confidence="0.996837">
Lorenzo Ferrone
</author>
<affiliation confidence="0.990885">
University of Rome “Tor Vergata”
</affiliation>
<address confidence="0.484965">
Via del Politecnico 1
00133 Roma, Italy
</address>
<email confidence="0.990362">
lorenzo.ferrone@gmail.com
</email>
<author confidence="0.964189">
Fabio Massimo Zanzotto
</author>
<affiliation confidence="0.961848">
University of Rome “Tor Vergata”
</affiliation>
<address confidence="0.471425">
Via del Politecnico 1
00133 Roma, Italy
</address>
<email confidence="0.987065">
fabio.massimo.zanzotto@uniroma2.it
</email>
<sectionHeader confidence="0.979292" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999969473684211">
In this paper, we describe our sub-
mission to the Shared Task #1. We
tried to follow the underlying idea of
the task, that is, evaluating the gap
of full-fledged recognizing textual en-
tailment systems with respect to com-
positional distributional semantic mod-
els (CDSMs) applied to this task. We
thus submitted two runs: 1) a sys-
tem obtained with a machine learning
approach based on the feature spaces
of rules with variables and 2) a sys-
tem completely based on a CDSM that
mixes structural and syntactic infor-
mation by using distributed tree ker-
nels. Our analysis shows that, under
the same conditions, the fully CDSM
system is still far from being competi-
tive with more complex methods.
</bodyText>
<sectionHeader confidence="0.996244" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999540230769231">
Recognizing Textual Entailment is a largely
explored problem (Dagan et al., 2013). Past
challenges (Dagan et al., 2006; Bar-Haim et
al., 2006; Giampiccolo et al., 2007) explored
methods and models applied in complex and
natural texts. In this context, machine learn-
ing solutions show interesting results. The
Shared Task #1 of SemEval instead wants to
explore systems in a more controlled textual
environment where the phenomena to model
are clearer. The aim of the Shared Task is to
study how RTE systems built upon composi-
tional distributional semantic models behave
</bodyText>
<footnote confidence="0.81301">
This work is licenced under a Creative Commons At-
tribution 4.0 International License. Page numbers and
proceedings footer are added by the organizers. License
details: http://creativecommons.org/licenses/by/
4.0/
</footnote>
<bodyText confidence="0.999741862068966">
with respect to the above tradition. We tried
to capture this underlying idea of the task.
In this paper, we describe our submission
to the Shared Task #1. We tried to fol-
low the underlying idea of the task, that is,
evaluating the gap of full-fledged recognizing
textual entailment systems with respect to
compositional distributional semantic models
(CDSMs) applied to this task. We thus sub-
mitted two runs: 1) a system obtained with a
machine learning approach based on the fea-
ture spaces of rules with variables (Zanzotto
et al., 2009) and 2) a system completely based
on a CDSM that mixes structural and syntac-
tic information by using distributed tree ker-
nels (Zanzotto and Dell’Arciprete, 2012). Our
analysis shows that, under the same condi-
tions, the fully CDSM system is still far from
being competitive with more complete meth-
ods.
The rest of the paper is organized as follows.
Section 2 describes the full-fledged recognizing
textual entailment system that is used for com-
parison. Section 3 introduces a novel composi-
tional distributional semantic model, namely,
the distributed smoothed tree kernels, and the
way this model is applied to the task of RTE.
Section 4 describes the results in the challenge
and it draws some preliminary conclusions.
</bodyText>
<sectionHeader confidence="0.7818185" genericHeader="method">
2 A Standard full-fledged Machine
Learning Approach for RTE
</sectionHeader>
<bodyText confidence="0.988714625">
For now on, the task of recognizing textual en-
tailment (RTE) is defined as the task to decide
if a pair p = (a, b) like:
(“Two children are lying in the snow and are
making snow angels”, “Two angels are
making snow on the lying children”)
is in entailment, in contradiction, or neutral.
As in the tradition of applied machine learn-
</bodyText>
<page confidence="0.979079">
300
</page>
<note confidence="0.730098">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 300–304,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.999001571428571">
ing models, the task is framed as a multi-
classification problem. The difficulty is to de-
termine the best feature space on which to
train the classifier.
A full-fledged RTE systems based on ma-
chine learning that has to deal with natural
occurring text is generally based on:
</bodyText>
<listItem confidence="0.860382">
• some within-pair features that model the
similarity between the sentence a and the
sentence b
• some features representing more complex
information of the pair (a, b) such as rules
with variables that fire (Zanzotto and
Moschitti, 2006)
</listItem>
<bodyText confidence="0.996975428571429">
In the following, we describe the within-pair
feature and the syntactic rules with variable
features used in the full-fledged RTE system.
As the second space of features is generally
huge, the full feature space is generally used in
kernel machines where the final kernel between
two instances p1 = (a1, b1) and p2 = (a2, b2) is:
</bodyText>
<equation confidence="0.9997565">
K(p1,p2) = FR(p1,p2) +
+ (WT5(a1, b1) · WT5(a2, b2) + 1)2
</equation>
<bodyText confidence="0.99994">
where FR counts how many rules are in com-
mon between p1 and p2 and WT5 computes a
lexical similarity between a and b. In the fol-
lowing sections we describe the nature of WT5
and of FR
</bodyText>
<subsectionHeader confidence="0.964424">
2.1 Weighted Token Similarity (WTS)
</subsectionHeader>
<bodyText confidence="0.999975458333333">
This similarity model was first defined bt Cor-
ley and Mihalcea (2005) and since then has
been used by many RTE systems. The model
extends a classical bag-of-word model to a
Weighted-Bag-of-Word (wBow) by measuring
similarity between the two sentences of the
pair at the semantic level, instead of the lexical
level.
For example, consider the pair: “Os-
cars forgot Farrah Fawcett”, “Farrah Fawcett
snubbed at Academy Awards”. This pair is
redundant, and, hence, should be assigned
a very high similarity. Yet, a bag-of-word
model would assign a low score, since many
words are not shared across the two sen-
tences. wBow fixes this problem by match-
ing ‘Oscar’-‘Academy Awards’ and ‘forgot’-
‘snubbed’ at the semantic level. To provide
these matches, wBow relies on specific word
similarity measures over WordNet (Miller,
1995), that allow synonymy and hyperonymy
matches: in our experiments we specifically
use Jiang&amp;Conrath similarity (Jiang and Con-
rath, 1997).
</bodyText>
<subsectionHeader confidence="0.999705">
2.2 Rules with Variables as Features
</subsectionHeader>
<bodyText confidence="0.999988235294118">
The above model alone is not sufficient to
capture all interesting entailment features as
the relation of entailment is not only related
to the notion of similarity between a and b.
In the tradition of RTE, an interesting feature
space is the one where each feature represents
a rule with variables, i.e. a first order rule
that is activated by the pairs if the variables
are unified. This feature space has been
introduced in (Zanzotto and Moschitti, 2006)
and shown to improve over the one above.
Each feature (fr1, fr2) is a pair of syntactic
tree fragments augmented with variables.
The feature is active for a pair (t1, t2) if the
syntactic interpretations of t1 and t2 can
be unified with &lt; fr1, fr2 &gt;. For example,
consider the following feature:
</bodyText>
<equation confidence="0.9863312">
NP X
✏✏ ✏ PPP
S ✏ ✏ ��
S
(
</equation>
<bodyText confidence="0.989836928571428">
This feature is active for the pair (“GM bought
Opel”,“GM owns Opel”), with the variable
unification X = “GM” and Y= “Opel”. On
the contrary, this feature is not active for the
pair (“GM bought Opel”,“Opel owns GM”) as
there is no possibility of unifying the two vari-
ables.
FR(p1, p2) is a kernel function that counts
the number of common rules with variables
between p1 and p2. Efficient algorithms for
the computation of the related kernel func-
tions can be found in (Moschitti and Zanzotto,
2007; Zanzotto and Dell’Arciprete, 2009; Zan-
zotto et al., 2011).
</bodyText>
<table confidence="0.573578111111111">
VP , NP X VP )
✟✟❍❍ ✟✟ ❍❍
VBP NP Y VBP NP Y
bought owns
301
S(t) = { S:booked::v , VP:booked::v , NP:we::p , S:booked::v VP:booked::v , ... }
✑ ◗ ✚ ❩ PRP ✚ ❩ ✟ ✟ ❍❍
NP VP V NP NP VP , . . . , V NP
PRP booked DT NN
</table>
<figureCaption confidence="0.988721">
Figure 1: Subtrees of the tree t in Figure 2 (a non-exhaustive list.)
</figureCaption>
<sectionHeader confidence="0.8678835" genericHeader="method">
3 Distributed Smoothed Tree
Kernel: a Compositional
Distributional Semantic Model
for RTE
</sectionHeader>
<bodyText confidence="0.999895055555556">
The above full-fledged RTE system, although
it may use distributional semantics, is not a
model that applies a compositional distribu-
tional semantic model as it does not explic-
itly transform sentences in vectors, matrices,
or tensors that represent their meaning.
We here propose a model that can be con-
sidered a compositional distributional seman-
tic model as it transforms sentences into ma-
trices that are then used by the learner as fea-
ture vectors. Our model is called Distributed
Smoothed Tree Kernel (Ferrone and Zanzotto,
2014) as it mixes the distributed trees (Zan-
zotto and Dell’Arciprete, 2012) representing
syntactic information with distributional se-
mantic vectors representing semantic informa-
tion. The computation of the final matrix for
each sentence is done compositionally.
</bodyText>
<subsectionHeader confidence="0.96252">
3.1 Notation
</subsectionHeader>
<bodyText confidence="0.999950592592592">
Before describing the distributed smoothed
trees (DST) we introduce a formal way to de-
note constituency-based lexicalized parse trees,
as DSTs exploit this kind of data structures.
Lexicalized trees are denoted with the letter t
and N(t) denotes the set of non terminal nodes
of tree t. Each non-terminal node n E N(t)
has a label ln composed of two parts ln =
(sn, wn): sn is the syntactic label, while wn is
the semantic headword of the tree headed by
n, along with its part-of-speech tag. Termi-
nal nodes of trees are treated differently, these
nodes represent only words wn without any
additional information, and their labels thus
only consist of the word itself (see Fig. 2).
The structure of a DST is represented as fol-
lows: Given a tree t, h(t) is its root node and
s(t) is the tree formed from t but considering
only the syntactic structure (that is, only the
sn part of the labels), ci(n) denotes i-th child
of a node n. As usual for constituency-based
parse trees, pre-terminal nodes are nodes that
have a single terminal node as child.
Finally, we use wn E Rk to denote the distri-
butional vector for word wn, whereas T repre-
sents the matrix of a tree t encoding structure
and distributional meaning.
</bodyText>
<subsectionHeader confidence="0.999255">
3.2 The Method in a Glance
</subsectionHeader>
<bodyText confidence="0.999917416666667">
We describe here the approach in a few sen-
tences. In line with tree kernels over struc-
tures (Collins and Duffy, 2002), we introduce
the set S(t) of the subtrees ti of a given lexi-
calized tree t. A subtree ti is in the set S(t) if
s(ti) is a subtree of s(t) and, if n is a node in
ti, all the siblings of n in t are in ti. For each
node of ti we only consider its syntactic label
sn, except for the head h(ti) for which we also
consider its semantic component wn (see Fig.
1). The functions DSTs we define compute the
following:
</bodyText>
<equation confidence="0.997787">
DST (t) = T = � Ti
tiES(t)
</equation>
<bodyText confidence="0.99726">
where Ti is the matrix associated to each sub-
tree ti. The similarity between two text frag-
ments a and b represented as lexicalized trees
ta and tb can be computed using the Frobenius
product between the two matrices Ta and Tb,
that is:
</bodyText>
<equation confidence="0.930875333333333">
(Ta,Tb)F = � (Tai ,Tbj)F (1)
tai ES(ta)
tbjES(tb)
</equation>
<figure confidence="0.758477375">
S:booked::v
✥✥ ✥ ✥ ❵❵❵❵
VP:booked::v
✘✘ ✘ ✘ ❳❳❳❳
V:booked::v NP:flight::n
✏✏ ✏ PPP
booked DT:the::d NN:flight::n
the flight
</figure>
<figureCaption confidence="0.981011">
Figure 2: A lexicalized tree.
</figureCaption>
<bodyText confidence="0.961249">
NP:we::p
PRP:we::p
We
</bodyText>
<page confidence="0.997316">
302
</page>
<bodyText confidence="0.99282875">
We want to obtain that the product hTai ,TbjiF
approximates the dot product between the
distributional vectors of the head words
→ →
(hTai , TbjiF ≈ h h(tai ), h(tbj)i) whenever the syn-
tactic structure of the subtrees is the same
(that is s(tai ) = s(tbj)), and hTai ,TbjiF ≈ 0 oth-
erwise. This property is expressed as:
</bodyText>
<equation confidence="0.925339333333333">
→ →
hTai , TbjiF ≈ δ(s(tai ), s(tbj)) · h h(ta i ), h(tb j)i (2)
To obtain the above property, we define
→ → &gt;
Ti = s(ti)wh(ti)
→
</equation>
<bodyText confidence="0.978563909090909">
where s(ti) are distributed tree fragment
(Zanzotto and Dell’Arciprete, 2012) for the
subtree t and →
wh(ti) is the distributional
vector of the head of the subtree t. Dis-
tributed tree fragments have the property
→ →
that s(ti) s(tj) ≈ δ(ti,tj). Thus, given the
important property of the outer product
that applies in the Frobenius product:
→
</bodyText>
<equation confidence="0.9879391">
h→a→w&gt;,→b →v &gt;iF = h→ w, →
a,b i·h→ v i. we have that
Equation 2 is satisfied as:
hTi, TjiF = hs(ti),
→ s(tj)i · h→
→ wh(ti),→
wh(tj)i
≈ δ(s(ti), s(tj)) · h→
wh(ti), →
wh(tj)i
</equation>
<bodyText confidence="0.998440666666667">
It is possible to show that the overall com-
positional distributional model DST (t) can be
obtained with a recursive algorithm that ex-
ploit vectors of the nodes of the tree.
The compositional distributional model is
then used in the same learning machine used
for the traditional RTE system with the fol-
lowing kernel function:
For the submission we used the java ver-
sion of LIBSVM (Chang and Lin, 2011).
Distributional vectors are derived with
DISSECT (Dinu et al., 2013) from a
corpus obtained by the concatenation of
ukWaC (wacky.sslmit.unibo.it), a mid-
2009 dump of the English Wikipedia
</bodyText>
<table confidence="0.9553405">
Model Accuracy (3-ways)
DST 69.42
full-fledged RTE System 75.66
84.57
48.73
Average 75.35
</table>
<tableCaption confidence="0.901407">
Table 1: Accuracies of the two systems on the
test set, together with the maximum, mini-
mum and average score for the challenge.
</tableCaption>
<bodyText confidence="0.999706484848485">
(en.wikipedia.org) and the British Na-
tional Corpus (www.natcorp.ox.ac.uk), for a
total of about 2.8 billion words. The raw co-
occurrences count vectors were transformed
into positive Pointwise Mutual Information
scores and reduced to 300 dimensions by
Singular Value Decomposition. This setup
was picked without tuning, as we found it
effective in previous, unrelated experiments.
We parsed the sentence with the Stanford
Parser (Klein and Manning, 2003) and ex-
tracted the heads for use in the lexicalized
trees with Collins’ rules (Collins, 2003).
Table 1 reports our results on the textual en-
tailment classification task, together with the
maximum, minimum and average score for the
challenge. The first observation is that the
full-fledged RTE system is still definitely bet-
ter than our CDSM system. We believe that
the main reason is that the DST cannot en-
code variables which is an important aspect
to capture when dealing with textual entail-
ment recognition. This is particularly true
for this dataset as it focuses on word order-
ing and on specific and recurrent entailment
rules. Our full-fledged system scored among
the first 10 systems, slightly above the over-
all average score, but our pure CDSM system
is instead ranked within the last 3. We think
that a more in-depth comparison with other
fully CDSM systems will give us a better in-
sight on our model and will also assess more
realistically the quality of our system.
</bodyText>
<sectionHeader confidence="0.997454" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9059376">
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,
Danilo Giampiccolo, Bernardo Magnini, and
Idan Szpektor. 2006. The second pascal recog-
nising textual entailment challenge. In Proceed-
ings of the Second PASCAL Challenges Work-
</reference>
<figure confidence="0.695063166666667">
K(p1, p2) =
hDST(a1),DST(a2)i + hDST(b1),DST(b2)i +
+(WTS(a1, b1) · WTS(a2, b2) + 1)2
4 Results and Conclusions
Max
Min
</figure>
<page confidence="0.975287">
303
</page>
<reference confidence="0.997236134831461">
shop on Recognising Textual Entailment. Venice,
Italy.
Chih-Chung Chang and Chih-Jen Lin. 2011.
LIBSVM: A library for support vector ma-
chines. ACM Transactions on Intelligent Sys-
tems and Technology, 2:27:1–27:27. Soft-
ware available at http://www.csie.ntu.edu.
tw/~cjlin/libsvm.
Michael Collins and Nigel Duffy. 2002. New rank-
ing algorithms for parsing and tagging: Kernels
over discrete structures, and the voted percep-
tron. In Proceedings of ACL02.
Michael Collins. 2003. Head-driven statistical
models for natural language parsing. Comput.
Linguist., 29(4):589–637.
Courtney Corley and Rada Mihalcea. 2005. Mea-
suring the semantic similarity of texts. In Proc.
of the ACL Workshop on Empirical Modeling
of Semantic Equivalence and Entailment, pages
13–18. Association for Computational Linguis-
tics, Ann Arbor, Michigan, June.
Ido Dagan, Oren Glickman, and Bernardo
Magnini. 2006. The pascal recognising tex-
tual entailment challenge. In Quionero-Candela
et al., editor, LNAI 3944: MLCW 2005, pages
177–190. Springer-Verlag, Milan, Italy.
Ido Dagan, Dan Roth, Mark Sammons, and
Fabio Massimo Zanzotto. 2013. Recognizing
Textual Entailment: Models and Applications.
Synthesis Lectures on Human Language Tech-
nologies. Morgan &amp; Claypool Publishers.
Georgiana Dinu, Nghia The Pham, and Marco
Baroni. 2013. DISSECT: DIStributional SE-
mantics Composition Toolkit. In Proceedings
of ACL (System Demonstrations), pages 31–36,
Sofia, Bulgaria.
Lorenzo Ferrone and Fabio Massimo Zanzotto.
2014. Towards syntax-aware compositional dis-
tributional semantic models. In Proceedings of
Coling 2014. COLING, Dublin, Ireland, Aug 23–
Aug 29.
Danilo Giampiccolo, Bernardo Magnini, Ido Da-
gan, and Bill Dolan. 2007. The third pas-
cal recognizing textual entailment challenge. In
Proceedings of the ACL-PASCAL Workshop on
Textual Entailment and Paraphrasing, pages 1–
9. Association for Computational Linguistics,
Prague, June.
Jay J. Jiang and David W. Conrath. 1997. Seman-
tic similarity based on corpus statistics and lex-
ical taxonomy. In Proc. of the 10th ROCLING,
pages 132–139. Tapei, Taiwan.
Dan Klein and Christopher D. Manning. 2003.
Accurate unlexicalized parsing. In Proceedings
of the 41st Annual Meeting on Association for
Computational Linguistics - Volume 1, ACL ’03,
pages 423–430, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
George A. Miller. 1995. WordNet: A lexical
database for English. Communications of the
ACM, 38(11):39–41, November.
Alessandro Moschitti and Fabio Massimo Zan-
zotto. 2007. Fast and effective kernels for re-
lational learning from texts. In Proceedings of
the International Conference of Machine Learn-
ing (ICML). Corvallis, Oregon.
Fabio Massimo Zanzotto and Lorenzo
Dell’Arciprete. 2009. Efficient kernels for
sentence pair classification. In Conference
on Empirical Methods on Natural Language
Processing, pages 91–100, 6-7 August.
F.M. Zanzotto and L. Dell’Arciprete. 2012. Dis-
tributed tree kernels. In Proceedings of Interna-
tional Conference on Machine Learning, pages
193–200.
Fabio Massimo Zanzotto and Alessandro Mos-
chitti. 2006. Automatic learning of textual en-
tailments with cross-pair similarities. In Pro-
ceedings of the 21st Coling and 44th ACL, pages
401–408. Sydney, Australia, July.
Fabio Massimo Zanzotto, Marco Pennacchiotti,
and Alessandro Moschitti. 2009. A machine
learning approach to textual entailment recog-
nition. NATURAL LANGUAGE ENGINEER-
ING, 15-04:551–582.
Fabio Massimo Zanzotto, Lorenzo Dell’Arciprete,
and Alessandro Moschitti. 2011. Efficient graph
kernels for textual entailment recognition. Fun-
damenta Informaticae, 107(2-3):199 – 222.
</reference>
<page confidence="0.999329">
304
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.225411">
<title confidence="0.996171">haLF: Comparing a Pure CDSM Approach with a Machine Learning System for RTE</title>
<author confidence="0.995803">Lorenzo</author>
<affiliation confidence="0.998328">University of Rome “Tor</affiliation>
<address confidence="0.740197">Via del Politecnico 00133 Roma,</address>
<email confidence="0.999946">lorenzo.ferrone@gmail.com</email>
<author confidence="0.999918">Fabio Massimo</author>
<affiliation confidence="0.997554">University of Rome “Tor</affiliation>
<address confidence="0.7327215">Via del Politecnico 00133 Roma,</address>
<email confidence="0.998784">fabio.massimo.zanzotto@uniroma2.it</email>
<abstract confidence="0.9985235">In this paper, we describe our submission to the Shared Task #1. We tried to follow the underlying idea of the task, that is, evaluating the gap of full-fledged recognizing textual entailment systems with respect to compositional distributional semantic models (CDSMs) applied to this task. We thus submitted two runs: 1) a system obtained with a machine learning approach based on the feature spaces of rules with variables and 2) a system completely based on a CDSM that mixes structural and syntactic information by using distributed tree kernels. Our analysis shows that, under the same conditions, the fully CDSM system is still far from being competitive with more complex methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Roy Bar-Haim</author>
<author>Ido Dagan</author>
<author>Bill Dolan</author>
<author>Lisa Ferro</author>
<author>Danilo Giampiccolo</author>
<author>Bernardo Magnini</author>
<author>Idan Szpektor</author>
</authors>
<title>The second pascal recognising textual entailment challenge.</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment.</booktitle>
<location>Venice, Italy.</location>
<contexts>
<context position="1184" citStr="Bar-Haim et al., 2006" startWordPosition="185" endWordPosition="188">tional distributional semantic models (CDSMs) applied to this task. We thus submitted two runs: 1) a system obtained with a machine learning approach based on the feature spaces of rules with variables and 2) a system completely based on a CDSM that mixes structural and syntactic information by using distributed tree kernels. Our analysis shows that, under the same conditions, the fully CDSM system is still far from being competitive with more complex methods. 1 Introduction Recognizing Textual Entailment is a largely explored problem (Dagan et al., 2013). Past challenges (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007) explored methods and models applied in complex and natural texts. In this context, machine learning solutions show interesting results. The Shared Task #1 of SemEval instead wants to explore systems in a more controlled textual environment where the phenomena to model are clearer. The aim of the Shared Task is to study how RTE systems built upon compositional distributional semantic models behave This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http</context>
</contexts>
<marker>Bar-Haim, Dagan, Dolan, Ferro, Giampiccolo, Magnini, Szpektor, 2006</marker>
<rawString>Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor. 2006. The second pascal recognising textual entailment challenge. In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment. Venice, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<pages>2--27</pages>
<note>Software available at http://www.csie.ntu.edu. tw/~cjlin/libsvm.</note>
<contexts>
<context position="11877" citStr="Chang and Lin, 2011" startWordPosition="2055" endWordPosition="2058">t that applies in the Frobenius product: → h→a→w&gt;,→b →v &gt;iF = h→ w, → a,b i·h→ v i. we have that Equation 2 is satisfied as: hTi, TjiF = hs(ti), → s(tj)i · h→ → wh(ti),→ wh(tj)i ≈ δ(s(ti), s(tj)) · h→ wh(ti), → wh(tj)i It is possible to show that the overall compositional distributional model DST (t) can be obtained with a recursive algorithm that exploit vectors of the nodes of the tree. The compositional distributional model is then used in the same learning machine used for the traditional RTE system with the following kernel function: For the submission we used the java version of LIBSVM (Chang and Lin, 2011). Distributional vectors are derived with DISSECT (Dinu et al., 2013) from a corpus obtained by the concatenation of ukWaC (wacky.sslmit.unibo.it), a mid2009 dump of the English Wikipedia Model Accuracy (3-ways) DST 69.42 full-fledged RTE System 75.66 84.57 48.73 Average 75.35 Table 1: Accuracies of the two systems on the test set, together with the maximum, minimum and average score for the challenge. (en.wikipedia.org) and the British National Corpus (www.natcorp.ox.ac.uk), for a total of about 2.8 billion words. The raw cooccurrences count vectors were transformed into positive Pointwise Mu</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1–27:27. Software available at http://www.csie.ntu.edu. tw/~cjlin/libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL02.</booktitle>
<contexts>
<context position="9603" citStr="Collins and Duffy, 2002" startWordPosition="1622" endWordPosition="1625">Given a tree t, h(t) is its root node and s(t) is the tree formed from t but considering only the syntactic structure (that is, only the sn part of the labels), ci(n) denotes i-th child of a node n. As usual for constituency-based parse trees, pre-terminal nodes are nodes that have a single terminal node as child. Finally, we use wn E Rk to denote the distributional vector for word wn, whereas T represents the matrix of a tree t encoding structure and distributional meaning. 3.2 The Method in a Glance We describe here the approach in a few sentences. In line with tree kernels over structures (Collins and Duffy, 2002), we introduce the set S(t) of the subtrees ti of a given lexicalized tree t. A subtree ti is in the set S(t) if s(ti) is a subtree of s(t) and, if n is a node in ti, all the siblings of n in t are in ti. For each node of ti we only consider its syntactic label sn, except for the head h(ti) for which we also consider its semantic component wn (see Fig. 1). The functions DSTs we define compute the following: DST (t) = T = � Ti tiES(t) where Ti is the matrix associated to each subtree ti. The similarity between two text fragments a and b represented as lexicalized trees ta and tb can be computed</context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>Michael Collins and Nigel Duffy. 2002. New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron. In Proceedings of ACL02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>2003</date>
<journal>Comput. Linguist.,</journal>
<volume>29</volume>
<issue>4</issue>
<contexts>
<context position="12829" citStr="Collins, 2003" startWordPosition="2203" endWordPosition="2204">ther with the maximum, minimum and average score for the challenge. (en.wikipedia.org) and the British National Corpus (www.natcorp.ox.ac.uk), for a total of about 2.8 billion words. The raw cooccurrences count vectors were transformed into positive Pointwise Mutual Information scores and reduced to 300 dimensions by Singular Value Decomposition. This setup was picked without tuning, as we found it effective in previous, unrelated experiments. We parsed the sentence with the Stanford Parser (Klein and Manning, 2003) and extracted the heads for use in the lexicalized trees with Collins’ rules (Collins, 2003). Table 1 reports our results on the textual entailment classification task, together with the maximum, minimum and average score for the challenge. The first observation is that the full-fledged RTE system is still definitely better than our CDSM system. We believe that the main reason is that the DST cannot encode variables which is an important aspect to capture when dealing with textual entailment recognition. This is particularly true for this dataset as it focuses on word ordering and on specific and recurrent entailment rules. Our full-fledged system scored among the first 10 systems, s</context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>Michael Collins. 2003. Head-driven statistical models for natural language parsing. Comput. Linguist., 29(4):589–637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Courtney Corley</author>
<author>Rada Mihalcea</author>
</authors>
<title>Measuring the semantic similarity of texts.</title>
<date>2005</date>
<booktitle>In Proc. of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment,</booktitle>
<pages>13--18</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="4805" citStr="Corley and Mihalcea (2005)" startWordPosition="787" endWordPosition="791">re and the syntactic rules with variable features used in the full-fledged RTE system. As the second space of features is generally huge, the full feature space is generally used in kernel machines where the final kernel between two instances p1 = (a1, b1) and p2 = (a2, b2) is: K(p1,p2) = FR(p1,p2) + + (WT5(a1, b1) · WT5(a2, b2) + 1)2 where FR counts how many rules are in common between p1 and p2 and WT5 computes a lexical similarity between a and b. In the following sections we describe the nature of WT5 and of FR 2.1 Weighted Token Similarity (WTS) This similarity model was first defined bt Corley and Mihalcea (2005) and since then has been used by many RTE systems. The model extends a classical bag-of-word model to a Weighted-Bag-of-Word (wBow) by measuring similarity between the two sentences of the pair at the semantic level, instead of the lexical level. For example, consider the pair: “Oscars forgot Farrah Fawcett”, “Farrah Fawcett snubbed at Academy Awards”. This pair is redundant, and, hence, should be assigned a very high similarity. Yet, a bag-of-word model would assign a low score, since many words are not shared across the two sentences. wBow fixes this problem by matching ‘Oscar’-‘Academy Awar</context>
</contexts>
<marker>Corley, Mihalcea, 2005</marker>
<rawString>Courtney Corley and Rada Mihalcea. 2005. Measuring the semantic similarity of texts. In Proc. of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 13–18. Association for Computational Linguistics, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The pascal recognising textual entailment challenge.</title>
<date>2006</date>
<booktitle>LNAI 3944: MLCW 2005,</booktitle>
<pages>177--190</pages>
<editor>In Quionero-Candela et al., editor,</editor>
<publisher>Springer-Verlag,</publisher>
<location>Milan, Italy.</location>
<contexts>
<context position="1161" citStr="Dagan et al., 2006" startWordPosition="181" endWordPosition="184">h respect to compositional distributional semantic models (CDSMs) applied to this task. We thus submitted two runs: 1) a system obtained with a machine learning approach based on the feature spaces of rules with variables and 2) a system completely based on a CDSM that mixes structural and syntactic information by using distributed tree kernels. Our analysis shows that, under the same conditions, the fully CDSM system is still far from being competitive with more complex methods. 1 Introduction Recognizing Textual Entailment is a largely explored problem (Dagan et al., 2013). Past challenges (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007) explored methods and models applied in complex and natural texts. In this context, machine learning solutions show interesting results. The Shared Task #1 of SemEval instead wants to explore systems in a more controlled textual environment where the phenomena to model are clearer. The aim of the Shared Task is to study how RTE systems built upon compositional distributional semantic models behave This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2006</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The pascal recognising textual entailment challenge. In Quionero-Candela et al., editor, LNAI 3944: MLCW 2005, pages 177–190. Springer-Verlag, Milan, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Dan Roth</author>
<author>Mark Sammons</author>
<author>Fabio Massimo Zanzotto</author>
</authors>
<title>Recognizing Textual Entailment: Models and Applications. Synthesis Lectures on Human Language Technologies.</title>
<date>2013</date>
<publisher>Morgan &amp; Claypool Publishers.</publisher>
<contexts>
<context position="1124" citStr="Dagan et al., 2013" startWordPosition="175" endWordPosition="178">gnizing textual entailment systems with respect to compositional distributional semantic models (CDSMs) applied to this task. We thus submitted two runs: 1) a system obtained with a machine learning approach based on the feature spaces of rules with variables and 2) a system completely based on a CDSM that mixes structural and syntactic information by using distributed tree kernels. Our analysis shows that, under the same conditions, the fully CDSM system is still far from being competitive with more complex methods. 1 Introduction Recognizing Textual Entailment is a largely explored problem (Dagan et al., 2013). Past challenges (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007) explored methods and models applied in complex and natural texts. In this context, machine learning solutions show interesting results. The Shared Task #1 of SemEval instead wants to explore systems in a more controlled textual environment where the phenomena to model are clearer. The aim of the Shared Task is to study how RTE systems built upon compositional distributional semantic models behave This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedin</context>
</contexts>
<marker>Dagan, Roth, Sammons, Zanzotto, 2013</marker>
<rawString>Ido Dagan, Dan Roth, Mark Sammons, and Fabio Massimo Zanzotto. 2013. Recognizing Textual Entailment: Models and Applications. Synthesis Lectures on Human Language Technologies. Morgan &amp; Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgiana Dinu</author>
<author>Nghia The Pham</author>
<author>Marco Baroni</author>
</authors>
<title>DISSECT: DIStributional SEmantics Composition Toolkit.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL (System Demonstrations),</booktitle>
<pages>31--36</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="11946" citStr="Dinu et al., 2013" startWordPosition="2065" endWordPosition="2068">,b i·h→ v i. we have that Equation 2 is satisfied as: hTi, TjiF = hs(ti), → s(tj)i · h→ → wh(ti),→ wh(tj)i ≈ δ(s(ti), s(tj)) · h→ wh(ti), → wh(tj)i It is possible to show that the overall compositional distributional model DST (t) can be obtained with a recursive algorithm that exploit vectors of the nodes of the tree. The compositional distributional model is then used in the same learning machine used for the traditional RTE system with the following kernel function: For the submission we used the java version of LIBSVM (Chang and Lin, 2011). Distributional vectors are derived with DISSECT (Dinu et al., 2013) from a corpus obtained by the concatenation of ukWaC (wacky.sslmit.unibo.it), a mid2009 dump of the English Wikipedia Model Accuracy (3-ways) DST 69.42 full-fledged RTE System 75.66 84.57 48.73 Average 75.35 Table 1: Accuracies of the two systems on the test set, together with the maximum, minimum and average score for the challenge. (en.wikipedia.org) and the British National Corpus (www.natcorp.ox.ac.uk), for a total of about 2.8 billion words. The raw cooccurrences count vectors were transformed into positive Pointwise Mutual Information scores and reduced to 300 dimensions by Singular Val</context>
</contexts>
<marker>Dinu, Pham, Baroni, 2013</marker>
<rawString>Georgiana Dinu, Nghia The Pham, and Marco Baroni. 2013. DISSECT: DIStributional SEmantics Composition Toolkit. In Proceedings of ACL (System Demonstrations), pages 31–36, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lorenzo Ferrone</author>
<author>Fabio Massimo Zanzotto</author>
</authors>
<title>Towards syntax-aware compositional distributional semantic models.</title>
<date>2014</date>
<booktitle>In Proceedings of Coling 2014. COLING,</booktitle>
<location>Dublin, Ireland,</location>
<contexts>
<context position="7978" citStr="Ferrone and Zanzotto, 2014" startWordPosition="1341" endWordPosition="1344">t.) 3 Distributed Smoothed Tree Kernel: a Compositional Distributional Semantic Model for RTE The above full-fledged RTE system, although it may use distributional semantics, is not a model that applies a compositional distributional semantic model as it does not explicitly transform sentences in vectors, matrices, or tensors that represent their meaning. We here propose a model that can be considered a compositional distributional semantic model as it transforms sentences into matrices that are then used by the learner as feature vectors. Our model is called Distributed Smoothed Tree Kernel (Ferrone and Zanzotto, 2014) as it mixes the distributed trees (Zanzotto and Dell’Arciprete, 2012) representing syntactic information with distributional semantic vectors representing semantic information. The computation of the final matrix for each sentence is done compositionally. 3.1 Notation Before describing the distributed smoothed trees (DST) we introduce a formal way to denote constituency-based lexicalized parse trees, as DSTs exploit this kind of data structures. Lexicalized trees are denoted with the letter t and N(t) denotes the set of non terminal nodes of tree t. Each non-terminal node n E N(t) has a label</context>
</contexts>
<marker>Ferrone, Zanzotto, 2014</marker>
<rawString>Lorenzo Ferrone and Fabio Massimo Zanzotto. 2014. Towards syntax-aware compositional distributional semantic models. In Proceedings of Coling 2014. COLING, Dublin, Ireland, Aug 23– Aug 29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Giampiccolo</author>
<author>Bernardo Magnini</author>
<author>Ido Dagan</author>
<author>Bill Dolan</author>
</authors>
<title>The third pascal recognizing textual entailment challenge.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing,</booktitle>
<pages>pages</pages>
<location>Prague,</location>
<contexts>
<context position="1211" citStr="Giampiccolo et al., 2007" startWordPosition="189" endWordPosition="192">emantic models (CDSMs) applied to this task. We thus submitted two runs: 1) a system obtained with a machine learning approach based on the feature spaces of rules with variables and 2) a system completely based on a CDSM that mixes structural and syntactic information by using distributed tree kernels. Our analysis shows that, under the same conditions, the fully CDSM system is still far from being competitive with more complex methods. 1 Introduction Recognizing Textual Entailment is a largely explored problem (Dagan et al., 2013). Past challenges (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007) explored methods and models applied in complex and natural texts. In this context, machine learning solutions show interesting results. The Shared Task #1 of SemEval instead wants to explore systems in a more controlled textual environment where the phenomena to model are clearer. The aim of the Shared Task is to study how RTE systems built upon compositional distributional semantic models behave This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/lice</context>
</contexts>
<marker>Giampiccolo, Magnini, Dagan, Dolan, 2007</marker>
<rawString>Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. 2007. The third pascal recognizing textual entailment challenge. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pages 1– 9. Association for Computational Linguistics, Prague, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay J Jiang</author>
<author>David W Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In Proc. of the 10th ROCLING,</booktitle>
<pages>132--139</pages>
<location>Tapei, Taiwan.</location>
<contexts>
<context position="5693" citStr="Jiang and Conrath, 1997" startWordPosition="926" endWordPosition="930">ider the pair: “Oscars forgot Farrah Fawcett”, “Farrah Fawcett snubbed at Academy Awards”. This pair is redundant, and, hence, should be assigned a very high similarity. Yet, a bag-of-word model would assign a low score, since many words are not shared across the two sentences. wBow fixes this problem by matching ‘Oscar’-‘Academy Awards’ and ‘forgot’- ‘snubbed’ at the semantic level. To provide these matches, wBow relies on specific word similarity measures over WordNet (Miller, 1995), that allow synonymy and hyperonymy matches: in our experiments we specifically use Jiang&amp;Conrath similarity (Jiang and Conrath, 1997). 2.2 Rules with Variables as Features The above model alone is not sufficient to capture all interesting entailment features as the relation of entailment is not only related to the notion of similarity between a and b. In the tradition of RTE, an interesting feature space is the one where each feature represents a rule with variables, i.e. a first order rule that is activated by the pairs if the variables are unified. This feature space has been introduced in (Zanzotto and Moschitti, 2006) and shown to improve over the one above. Each feature (fr1, fr2) is a pair of syntactic tree fragments </context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>Jay J. Jiang and David W. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proc. of the 10th ROCLING, pages 132–139. Tapei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL ’03,</booktitle>
<pages>423--430</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="12736" citStr="Klein and Manning, 2003" startWordPosition="2185" endWordPosition="2188">RTE System 75.66 84.57 48.73 Average 75.35 Table 1: Accuracies of the two systems on the test set, together with the maximum, minimum and average score for the challenge. (en.wikipedia.org) and the British National Corpus (www.natcorp.ox.ac.uk), for a total of about 2.8 billion words. The raw cooccurrences count vectors were transformed into positive Pointwise Mutual Information scores and reduced to 300 dimensions by Singular Value Decomposition. This setup was picked without tuning, as we found it effective in previous, unrelated experiments. We parsed the sentence with the Stanford Parser (Klein and Manning, 2003) and extracted the heads for use in the lexicalized trees with Collins’ rules (Collins, 2003). Table 1 reports our results on the textual entailment classification task, together with the maximum, minimum and average score for the challenge. The first observation is that the full-fledged RTE system is still definitely better than our CDSM system. We believe that the main reason is that the DST cannot encode variables which is an important aspect to capture when dealing with textual entailment recognition. This is particularly true for this dataset as it focuses on word ordering and on specific</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL ’03, pages 423–430, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: A lexical database for English.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="5558" citStr="Miller, 1995" startWordPosition="910" endWordPosition="911">ring similarity between the two sentences of the pair at the semantic level, instead of the lexical level. For example, consider the pair: “Oscars forgot Farrah Fawcett”, “Farrah Fawcett snubbed at Academy Awards”. This pair is redundant, and, hence, should be assigned a very high similarity. Yet, a bag-of-word model would assign a low score, since many words are not shared across the two sentences. wBow fixes this problem by matching ‘Oscar’-‘Academy Awards’ and ‘forgot’- ‘snubbed’ at the semantic level. To provide these matches, wBow relies on specific word similarity measures over WordNet (Miller, 1995), that allow synonymy and hyperonymy matches: in our experiments we specifically use Jiang&amp;Conrath similarity (Jiang and Conrath, 1997). 2.2 Rules with Variables as Features The above model alone is not sufficient to capture all interesting entailment features as the relation of entailment is not only related to the notion of similarity between a and b. In the tradition of RTE, an interesting feature space is the one where each feature represents a rule with variables, i.e. a first order rule that is activated by the pairs if the variables are unified. This feature space has been introduced in</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. WordNet: A lexical database for English. Communications of the ACM, 38(11):39–41, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Fabio Massimo Zanzotto</author>
</authors>
<title>Fast and effective kernels for relational learning from texts.</title>
<date>2007</date>
<booktitle>In Proceedings of the International Conference of Machine Learning (ICML).</booktitle>
<location>Corvallis, Oregon.</location>
<contexts>
<context position="7010" citStr="Moschitti and Zanzotto, 2007" startWordPosition="1162" endWordPosition="1165">rpretations of t1 and t2 can be unified with &lt; fr1, fr2 &gt;. For example, consider the following feature: NP X ✏✏ ✏ PPP S ✏ ✏ �� S ( This feature is active for the pair (“GM bought Opel”,“GM owns Opel”), with the variable unification X = “GM” and Y= “Opel”. On the contrary, this feature is not active for the pair (“GM bought Opel”,“Opel owns GM”) as there is no possibility of unifying the two variables. FR(p1, p2) is a kernel function that counts the number of common rules with variables between p1 and p2. Efficient algorithms for the computation of the related kernel functions can be found in (Moschitti and Zanzotto, 2007; Zanzotto and Dell’Arciprete, 2009; Zanzotto et al., 2011). VP , NP X VP ) ✟✟❍❍ ✟✟ ❍❍ VBP NP Y VBP NP Y bought owns 301 S(t) = { S:booked::v , VP:booked::v , NP:we::p , S:booked::v VP:booked::v , ... } ✑ ◗ ✚ ❩ PRP ✚ ❩ ✟ ✟ ❍❍ NP VP V NP NP VP , . . . , V NP PRP booked DT NN Figure 1: Subtrees of the tree t in Figure 2 (a non-exhaustive list.) 3 Distributed Smoothed Tree Kernel: a Compositional Distributional Semantic Model for RTE The above full-fledged RTE system, although it may use distributional semantics, is not a model that applies a compositional distributional semantic model as it does</context>
</contexts>
<marker>Moschitti, Zanzotto, 2007</marker>
<rawString>Alessandro Moschitti and Fabio Massimo Zanzotto. 2007. Fast and effective kernels for relational learning from texts. In Proceedings of the International Conference of Machine Learning (ICML). Corvallis, Oregon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Massimo Zanzotto</author>
<author>Lorenzo Dell’Arciprete</author>
</authors>
<title>Efficient kernels for sentence pair classification.</title>
<date>2009</date>
<booktitle>In Conference on Empirical Methods on Natural Language Processing,</booktitle>
<pages>91--100</pages>
<marker>Zanzotto, Dell’Arciprete, 2009</marker>
<rawString>Fabio Massimo Zanzotto and Lorenzo Dell’Arciprete. 2009. Efficient kernels for sentence pair classification. In Conference on Empirical Methods on Natural Language Processing, pages 91–100, 6-7 August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F M Zanzotto</author>
<author>L Dell’Arciprete</author>
</authors>
<title>Distributed tree kernels.</title>
<date>2012</date>
<booktitle>In Proceedings of International Conference on Machine Learning,</booktitle>
<pages>193--200</pages>
<marker>Zanzotto, Dell’Arciprete, 2012</marker>
<rawString>F.M. Zanzotto and L. Dell’Arciprete. 2012. Distributed tree kernels. In Proceedings of International Conference on Machine Learning, pages 193–200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Massimo Zanzotto</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Automatic learning of textual entailments with cross-pair similarities.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st Coling and 44th ACL,</booktitle>
<pages>401--408</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="4127" citStr="Zanzotto and Moschitti, 2006" startWordPosition="664" endWordPosition="667">th International Workshop on Semantic Evaluation (SemEval 2014), pages 300–304, Dublin, Ireland, August 23-24, 2014. ing models, the task is framed as a multiclassification problem. The difficulty is to determine the best feature space on which to train the classifier. A full-fledged RTE systems based on machine learning that has to deal with natural occurring text is generally based on: • some within-pair features that model the similarity between the sentence a and the sentence b • some features representing more complex information of the pair (a, b) such as rules with variables that fire (Zanzotto and Moschitti, 2006) In the following, we describe the within-pair feature and the syntactic rules with variable features used in the full-fledged RTE system. As the second space of features is generally huge, the full feature space is generally used in kernel machines where the final kernel between two instances p1 = (a1, b1) and p2 = (a2, b2) is: K(p1,p2) = FR(p1,p2) + + (WT5(a1, b1) · WT5(a2, b2) + 1)2 where FR counts how many rules are in common between p1 and p2 and WT5 computes a lexical similarity between a and b. In the following sections we describe the nature of WT5 and of FR 2.1 Weighted Token Similari</context>
<context position="6189" citStr="Zanzotto and Moschitti, 2006" startWordPosition="1011" endWordPosition="1014">that allow synonymy and hyperonymy matches: in our experiments we specifically use Jiang&amp;Conrath similarity (Jiang and Conrath, 1997). 2.2 Rules with Variables as Features The above model alone is not sufficient to capture all interesting entailment features as the relation of entailment is not only related to the notion of similarity between a and b. In the tradition of RTE, an interesting feature space is the one where each feature represents a rule with variables, i.e. a first order rule that is activated by the pairs if the variables are unified. This feature space has been introduced in (Zanzotto and Moschitti, 2006) and shown to improve over the one above. Each feature (fr1, fr2) is a pair of syntactic tree fragments augmented with variables. The feature is active for a pair (t1, t2) if the syntactic interpretations of t1 and t2 can be unified with &lt; fr1, fr2 &gt;. For example, consider the following feature: NP X ✏✏ ✏ PPP S ✏ ✏ �� S ( This feature is active for the pair (“GM bought Opel”,“GM owns Opel”), with the variable unification X = “GM” and Y= “Opel”. On the contrary, this feature is not active for the pair (“GM bought Opel”,“Opel owns GM”) as there is no possibility of unifying the two variables. FR</context>
</contexts>
<marker>Zanzotto, Moschitti, 2006</marker>
<rawString>Fabio Massimo Zanzotto and Alessandro Moschitti. 2006. Automatic learning of textual entailments with cross-pair similarities. In Proceedings of the 21st Coling and 44th ACL, pages 401–408. Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Massimo Zanzotto</author>
<author>Marco Pennacchiotti</author>
<author>Alessandro Moschitti</author>
</authors>
<title>A machine learning approach to textual entailment recognition.</title>
<date>2009</date>
<journal>NATURAL LANGUAGE ENGINEERING,</journal>
<pages>15--04</pages>
<contexts>
<context position="2364" citStr="Zanzotto et al., 2009" startWordPosition="371" endWordPosition="374">d by the organizers. License details: http://creativecommons.org/licenses/by/ 4.0/ with respect to the above tradition. We tried to capture this underlying idea of the task. In this paper, we describe our submission to the Shared Task #1. We tried to follow the underlying idea of the task, that is, evaluating the gap of full-fledged recognizing textual entailment systems with respect to compositional distributional semantic models (CDSMs) applied to this task. We thus submitted two runs: 1) a system obtained with a machine learning approach based on the feature spaces of rules with variables (Zanzotto et al., 2009) and 2) a system completely based on a CDSM that mixes structural and syntactic information by using distributed tree kernels (Zanzotto and Dell’Arciprete, 2012). Our analysis shows that, under the same conditions, the fully CDSM system is still far from being competitive with more complete methods. The rest of the paper is organized as follows. Section 2 describes the full-fledged recognizing textual entailment system that is used for comparison. Section 3 introduces a novel compositional distributional semantic model, namely, the distributed smoothed tree kernels, and the way this model is a</context>
</contexts>
<marker>Zanzotto, Pennacchiotti, Moschitti, 2009</marker>
<rawString>Fabio Massimo Zanzotto, Marco Pennacchiotti, and Alessandro Moschitti. 2009. A machine learning approach to textual entailment recognition. NATURAL LANGUAGE ENGINEERING, 15-04:551–582.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Massimo Zanzotto</author>
<author>Lorenzo Dell’Arciprete</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Efficient graph kernels for textual entailment recognition. Fundamenta Informaticae,</title>
<date>2011</date>
<pages>107--2</pages>
<marker>Zanzotto, Dell’Arciprete, Moschitti, 2011</marker>
<rawString>Fabio Massimo Zanzotto, Lorenzo Dell’Arciprete, and Alessandro Moschitti. 2011. Efficient graph kernels for textual entailment recognition. Fundamenta Informaticae, 107(2-3):199 – 222.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>