<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.189057">
<title confidence="0.997833">
Twitter in Mass Emergency:
What NLP Techniques Can Contribute
</title>
<author confidence="0.999001">
William J. Corvey1, Sarah Vieweg2, Travis Rood1 &amp; Martha Palmer1
</author>
<affiliation confidence="0.998994">
1Department of Linguistics, 2ATLAS Institute
University of Colorado
</affiliation>
<address confidence="0.782731">
Boulder, CO 80309
</address>
<email confidence="0.972894">
William.Corvey, Sarah.Vieweg, Travis.Rood, Martha.Palmer@colorado.edu
</email>
<sectionHeader confidence="0.992549" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997376">
We detail methods for entity span identifica-
tion and entity class annotation of Twitter
communications that take place during times
of mass emergency. We present our motiva-
tion, method and preliminary results.
</bodyText>
<sectionHeader confidence="0.999266" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.981564807692308">
During times of mass emergency, many turn to
Twitter to gather and disperse relevant, timely in-
formation (Starbird et al. 2010; Vieweg et al.
2010). However, the sheer amount of information
now communicated via Twitter during these time-
and safety-critical situations can make it difficult
for individuals to locate personally meaningful and
actionable information. In this paper, we discuss
natural language processing (NLP) techniques de-
signed for Twitter data that will lead to the location
and extraction of specific information during times
of mass emergency.
2 Twitter Use in Mass Emergency
Twitter communications are comprised of 140-
character messages called “tweets.” During times
of mass emergency, Twitter users send detailed
information that may help those affected to better
make critical decisions.
Our goal is to develop techniques to automatically
identify crucial pieces of information in these
tweets. This process will lead to the automatic ex-
traction of information that helps people under-
stand the situation “on the ground” during mass
emergencies. Relevant information would include
such things as warnings, road closures, and
evacuations among other timely information.
</bodyText>
<sectionHeader confidence="0.984032" genericHeader="method">
3 The Annotation Process
</sectionHeader>
<bodyText confidence="0.999947677419355">
A foundational level of linguistic annotation for
many natural language processing tasks is Named
Entity (or nominal entity) tagging (Bikel 1999).
Typical labeled entities that were included in the
Automatic Content Extraction (ACE) guidelines
(LDC 2004) are: Person, Location, Organization,
and Facility, the four maximal entity classes. Our
preliminary annotation task consists of identifying
the syntactic span and entity class for these four
types of entities in a pilot set of Twitter data (200
tweets from a data set generated during the 2009
Oklahoma grassfires). In future annotation, the
ontology will be expanded to include event and
relation annotations, as well as additional sub-
classes of the entities now examined. Annotations
are done using Knowtator (Ogren 2006), a tool
built within the Protégé framework
(http://protege.stanford.edu/). The ontology devel-
opment is data-driven; as such it is likely that cer-
tain ACE annotations will never emerge and other
annotations (such as disaster-relevant materials)
will be necessary additions.
Three annotators undertook pilot annotation as part
of the construction of preliminary annotation
guidelines; the top pairwise ITA score is reported
below. Twitter data makes reference to numerous
entity spans that are of specific interest to this an-
notation task, such as road intersections and multi-
word named entities. The example below, from the
pilot annotation set, shows a relatively simple span
delineation.
</bodyText>
<listItem confidence="0.652051">
[PERSON Velma area residents]: [PERSON
Officials] say to take [FACILTY Old Hwy
7] to [FACILTY Speedy G] to safely
evacuate. [LOCATION Stephens Co Fair-
grounds] in [LOCATION Duncan] for shel-
ter
</listItem>
<page confidence="0.989987">
23
</page>
<note confidence="0.8782815">
Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics in a World of Social Media, pages 23–24,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999781333333333">
Because of the varying length of entities, annota-
tors cannot be given simple rules for deciding the
spans for annotations. This difficulty is reflected in
markedly lower rates for span identification inter-
annotator agreement (IAA) rates than for simple
class assignment.
</bodyText>
<sectionHeader confidence="0.989444" genericHeader="method">
4 Preliminary Results
</sectionHeader>
<bodyText confidence="0.999900272727273">
IAA calculations were performed using the Know-
tator IAA functionality. When annotations are re-
quired to be both the same span and class, the pilot
annotation yielded an F-score of 56.27 (An addi-
tional 4% have exact span matches but different
classes). However, when annotations are required
to have the same class assignment but only over-
lapping spans, this F-score rises to 72.85. While
Facility and Location are the most commonly con-
fused classes, span-matching remains a difficult
issue for all entity classes.
</bodyText>
<sectionHeader confidence="0.999504" genericHeader="discussions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999928214285714">
While these ITA rates are significantly lower than
published results from previous ACE annotation
efforts (LDC 2004), we believe that the crisis
communications domain, particularly with regard
to Twitter analysis, provides challenges not en-
countered in newswire, broadcast transcripts, or
newspaper data. First, determining the maximal
span of interest for a given class assignment is
non-trivial. The constraint of 140 characters neces-
sarily results in very limited syntactic and semantic
contexts, making spans and entity class assign-
ments much harder to determine.
A large source of disagreement was on the treat-
ment of coordinated or listed noun phrases. In cer-
tain contexts, each entity (cities below) requires its
own span (e.g. “Firestorms in Oklahoma. [Midwest
City], [Lake Draper]. Some houses lost”), whereas
in other contexts we find multiple entities per span
(e.g. “Midwest City to evacuate between SE 15th
and Rena and Anderson and Hiwassee also [Tur-
tlewood, Wingsong, and Oakwood additions]”).
Equally, class assignment cannot be a mechanistic
process or accomplished by reference to lists, as it
is important to distinguish between cases where
terms have been elided due to limited space and
cases where no elision has taken place. For in-
stance, the entity “Attorney General” (as opposed
to “Attorney General’s Office”) might be anno-
tated ‘Person’ or ‘Organization’ depending on con-
text, or simply ambiguous, i.e. lacking sufficient
context. It is primarily these unclear cases of class
assignment that will require careful discussion in
the annotation guidelines and in future mappings to
an ontology.
In summary, this pilot study represents a new ap-
plication of ACE annotation practices to a uniquely
challenging domain. We outline issues that place
special demands on annotators and future direc-
tions for ongoing research. We are confident that
as we refine our guidelines and provide more cues
and examples for the annotators that the determina-
tion of spans and entity classes will improve.
</bodyText>
<sectionHeader confidence="0.99856" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998524">
This work is supported by the US National Science
Foundation IIS-0546315 and IIS-0910586 but does
not represent the views of the NSF. This work was
conducted using the Protégé resource, supported
by grant LM007885 from the US NLM.
</bodyText>
<sectionHeader confidence="0.99806" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999222814814815">
Daniel M. Bikel, Richard Schwartz and Ralph M.
Weischedel. 1999. An Algorithm that Learns What’s
in a Name. In: the Machine Learning Journal Special
Issue on Natural Language Learning.
George Doddington, A. Mitchell, M. Przybocki, L.
Ramshaw, S. Strassel, and R. Weischedel. (2004).
The Automatic Content Extraction (ACE) Program –
Tasks, Data, and Evaluation. In: Proceedings of Con-
ference on Language Resources and Evaluation
(LREC 2004).
Kate Starbird, Leysia Palen, Amanda L. Hughes and
Sarah Vieweg. 2010. Chatter on The Red: What
Hazards Threat Reveals About the Social Life of Mi-
croblogged Information. In: Proc. CSCW 2010.
ACM Press.
LDC, 2004, Automatic Content Extraction
[www.ldc.upenn.edu/Projects/ACE/]
Philip Ogren. 2006. Knowtator: A Protégé plug-in for
annotated corpus construction. In : Proceedings of the
2006 Conference of the North American Chapter of
the Association for Computational Linguistics on
Human Language Technology 2006. ACM Press.
Sarah Vieweg, Amanda L. Hughes, Kate Starbird and
Leysia Palen. 2010. Microblogging During Two
Natural Hazards Events: What Twitter May Contrib-
ute to Situational Awareness. In: Proc. CHI 2010.
ACM Press.
</reference>
<page confidence="0.999176">
24
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.846656">
<title confidence="0.9716085">Twitter in Mass What NLP Techniques Can Contribute</title>
<author confidence="0.997117">J Sarah Travis</author>
<author confidence="0.997117">Martha</author>
<affiliation confidence="0.9979205">of Linguistics, University of</affiliation>
<address confidence="0.992706">Boulder, CO</address>
<email confidence="0.946375">William.Corvey,Sarah.Vieweg,Travis.Rood,Martha.Palmer@colorado.edu</email>
<abstract confidence="0.993188333333333">We detail methods for entity span identification and entity class annotation of Twitter communications that take place during times of mass emergency. We present our motivation, method and preliminary results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
<author>Richard Schwartz</author>
<author>Ralph M Weischedel</author>
</authors>
<title>An Algorithm that Learns What’s in a Name. In:</title>
<date>1999</date>
<journal>the Machine Learning Journal Special Issue on Natural Language Learning.</journal>
<marker>Bikel, Schwartz, Weischedel, 1999</marker>
<rawString>Daniel M. Bikel, Richard Schwartz and Ralph M. Weischedel. 1999. An Algorithm that Learns What’s in a Name. In: the Machine Learning Journal Special Issue on Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
<author>A Mitchell</author>
<author>M Przybocki</author>
<author>L Ramshaw</author>
<author>S Strassel</author>
<author>R Weischedel</author>
</authors>
<title>The Automatic Content Extraction (ACE) Program – Tasks, Data, and Evaluation. In:</title>
<date>2004</date>
<booktitle>Proceedings of Conference on Language Resources and Evaluation (LREC</booktitle>
<marker>Doddington, Mitchell, Przybocki, Ramshaw, Strassel, Weischedel, 2004</marker>
<rawString>George Doddington, A. Mitchell, M. Przybocki, L. Ramshaw, S. Strassel, and R. Weischedel. (2004). The Automatic Content Extraction (ACE) Program – Tasks, Data, and Evaluation. In: Proceedings of Conference on Language Resources and Evaluation (LREC 2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kate Starbird</author>
<author>Leysia Palen</author>
<author>Amanda L Hughes</author>
<author>Sarah Vieweg</author>
</authors>
<title>Chatter on The Red: What Hazards Threat Reveals About the Social Life of Microblogged Information. In:</title>
<date>2010</date>
<booktitle>Proc. CSCW</booktitle>
<publisher>ACM Press.</publisher>
<contexts>
<context position="643" citStr="Starbird et al. 2010" startWordPosition="86" endWordPosition="89">cy: What NLP Techniques Can Contribute William J. Corvey1, Sarah Vieweg2, Travis Rood1 &amp; Martha Palmer1 1Department of Linguistics, 2ATLAS Institute University of Colorado Boulder, CO 80309 William.Corvey, Sarah.Vieweg, Travis.Rood, Martha.Palmer@colorado.edu Abstract We detail methods for entity span identification and entity class annotation of Twitter communications that take place during times of mass emergency. We present our motivation, method and preliminary results. 1 Introduction During times of mass emergency, many turn to Twitter to gather and disperse relevant, timely information (Starbird et al. 2010; Vieweg et al. 2010). However, the sheer amount of information now communicated via Twitter during these timeand safety-critical situations can make it difficult for individuals to locate personally meaningful and actionable information. In this paper, we discuss natural language processing (NLP) techniques designed for Twitter data that will lead to the location and extraction of specific information during times of mass emergency. 2 Twitter Use in Mass Emergency Twitter communications are comprised of 140- character messages called “tweets.” During times of mass emergency, Twitter users sen</context>
</contexts>
<marker>Starbird, Palen, Hughes, Vieweg, 2010</marker>
<rawString>Kate Starbird, Leysia Palen, Amanda L. Hughes and Sarah Vieweg. 2010. Chatter on The Red: What Hazards Threat Reveals About the Social Life of Microblogged Information. In: Proc. CSCW 2010. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>LDC</author>
</authors>
<title>Automatic Content Extraction [www.ldc.upenn.edu/Projects/ACE/]</title>
<date>2004</date>
<contexts>
<context position="1989" citStr="LDC 2004" startWordPosition="284" endWordPosition="285"> identify crucial pieces of information in these tweets. This process will lead to the automatic extraction of information that helps people understand the situation “on the ground” during mass emergencies. Relevant information would include such things as warnings, road closures, and evacuations among other timely information. 3 The Annotation Process A foundational level of linguistic annotation for many natural language processing tasks is Named Entity (or nominal entity) tagging (Bikel 1999). Typical labeled entities that were included in the Automatic Content Extraction (ACE) guidelines (LDC 2004) are: Person, Location, Organization, and Facility, the four maximal entity classes. Our preliminary annotation task consists of identifying the syntactic span and entity class for these four types of entities in a pilot set of Twitter data (200 tweets from a data set generated during the 2009 Oklahoma grassfires). In future annotation, the ontology will be expanded to include event and relation annotations, as well as additional subclasses of the entities now examined. Annotations are done using Knowtator (Ogren 2006), a tool built within the Protégé framework (http://protege.stanford.edu/). </context>
<context position="4532" citStr="LDC 2004" startWordPosition="672" endWordPosition="673">ing the Knowtator IAA functionality. When annotations are required to be both the same span and class, the pilot annotation yielded an F-score of 56.27 (An additional 4% have exact span matches but different classes). However, when annotations are required to have the same class assignment but only overlapping spans, this F-score rises to 72.85. While Facility and Location are the most commonly confused classes, span-matching remains a difficult issue for all entity classes. 5 Discussion While these ITA rates are significantly lower than published results from previous ACE annotation efforts (LDC 2004), we believe that the crisis communications domain, particularly with regard to Twitter analysis, provides challenges not encountered in newswire, broadcast transcripts, or newspaper data. First, determining the maximal span of interest for a given class assignment is non-trivial. The constraint of 140 characters necessarily results in very limited syntactic and semantic contexts, making spans and entity class assignments much harder to determine. A large source of disagreement was on the treatment of coordinated or listed noun phrases. In certain contexts, each entity (cities below) requires </context>
</contexts>
<marker>LDC, 2004</marker>
<rawString>LDC, 2004, Automatic Content Extraction [www.ldc.upenn.edu/Projects/ACE/]</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Ogren</author>
</authors>
<title>Knowtator: A Protégé plug-in for annotated corpus construction.</title>
<date>2006</date>
<booktitle>In : Proceedings of the 2006 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology</booktitle>
<publisher>ACM Press.</publisher>
<contexts>
<context position="2513" citStr="Ogren 2006" startWordPosition="365" endWordPosition="366">ties that were included in the Automatic Content Extraction (ACE) guidelines (LDC 2004) are: Person, Location, Organization, and Facility, the four maximal entity classes. Our preliminary annotation task consists of identifying the syntactic span and entity class for these four types of entities in a pilot set of Twitter data (200 tweets from a data set generated during the 2009 Oklahoma grassfires). In future annotation, the ontology will be expanded to include event and relation annotations, as well as additional subclasses of the entities now examined. Annotations are done using Knowtator (Ogren 2006), a tool built within the Protégé framework (http://protege.stanford.edu/). The ontology development is data-driven; as such it is likely that certain ACE annotations will never emerge and other annotations (such as disaster-relevant materials) will be necessary additions. Three annotators undertook pilot annotation as part of the construction of preliminary annotation guidelines; the top pairwise ITA score is reported below. Twitter data makes reference to numerous entity spans that are of specific interest to this annotation task, such as road intersections and multiword named entities. The </context>
</contexts>
<marker>Ogren, 2006</marker>
<rawString>Philip Ogren. 2006. Knowtator: A Protégé plug-in for annotated corpus construction. In : Proceedings of the 2006 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology 2006. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sarah Vieweg</author>
<author>Amanda L Hughes</author>
<author>Kate Starbird</author>
<author>Leysia Palen</author>
</authors>
<title>Microblogging During Two Natural Hazards Events: What Twitter May Contribute to Situational Awareness. In:</title>
<date>2010</date>
<booktitle>Proc. CHI</booktitle>
<publisher>ACM Press.</publisher>
<contexts>
<context position="664" citStr="Vieweg et al. 2010" startWordPosition="90" endWordPosition="93">s Can Contribute William J. Corvey1, Sarah Vieweg2, Travis Rood1 &amp; Martha Palmer1 1Department of Linguistics, 2ATLAS Institute University of Colorado Boulder, CO 80309 William.Corvey, Sarah.Vieweg, Travis.Rood, Martha.Palmer@colorado.edu Abstract We detail methods for entity span identification and entity class annotation of Twitter communications that take place during times of mass emergency. We present our motivation, method and preliminary results. 1 Introduction During times of mass emergency, many turn to Twitter to gather and disperse relevant, timely information (Starbird et al. 2010; Vieweg et al. 2010). However, the sheer amount of information now communicated via Twitter during these timeand safety-critical situations can make it difficult for individuals to locate personally meaningful and actionable information. In this paper, we discuss natural language processing (NLP) techniques designed for Twitter data that will lead to the location and extraction of specific information during times of mass emergency. 2 Twitter Use in Mass Emergency Twitter communications are comprised of 140- character messages called “tweets.” During times of mass emergency, Twitter users send detailed informatio</context>
</contexts>
<marker>Vieweg, Hughes, Starbird, Palen, 2010</marker>
<rawString>Sarah Vieweg, Amanda L. Hughes, Kate Starbird and Leysia Palen. 2010. Microblogging During Two Natural Hazards Events: What Twitter May Contribute to Situational Awareness. In: Proc. CHI 2010. ACM Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>