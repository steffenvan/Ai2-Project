<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.069207">
<title confidence="0.9960635">
Ebiquity: Paraphrase and Semantic
Similarity in Twitter using Skipgram
</title>
<author confidence="0.997794">
Taneeya Satyapanich, Hang Gao and Tim Finin
</author>
<affiliation confidence="0.999664">
University of Maryland, Baltimore County
</affiliation>
<address confidence="0.786095">
Baltimore, MD, 21250, USA
</address>
<email confidence="0.986895">
taneeya1@umbc.edu, hanggao1@umbc.edu, finin@umbc.edu
</email>
<sectionHeader confidence="0.995435" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99933075">
We describe the system we developed to partic-
ipate in SemEval 2015 Task 1, Paraphrase and
Semantic Similarity in Twitter. We create simi-
larity vectors from two-skip trigrams of prepro-
cessed tweets and measure their semantic simi-
larity using our UMBC-STS system. We sub-
mit two runs. The best result is ranked eleventh
out of eighteen teams with F1 score of 0.599.
</bodyText>
<sectionHeader confidence="0.998212" genericHeader="introduction">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999873631578947">
In this task (Wei, et al., 2015), participants were
given pairs of text sequences from Twitter trends
and produced a binary judgment for each stating
whether or not they are paraphrases (e.g., semanti-
cally the same) and optionally a graded score (0.0
to 1.0) measuring their degree of semantic equiva-
lence. For example, for the trending topic “A Walk
to Remember” (a film released in 2002), the pair A
Walk to Remember is the definition of true love”
and “A Walk to Remember is on and Im in town
and Im upset” might be judged as not paraphrases
with score 0.2 whereas the pair “A Walk to Re-
member is the definition of true love” and “A Walk
to Remember is the cutest thing” could be judged
as paraphrases with a score of 0.6.
Many methods have been proposed to solve the
paraphrase detection problem. Early approaches
were often based on lexical matching techniques,
e.g., word n-gram overlap (Barzilay and Lee,
</bodyText>
<page confidence="0.977322">
51
</page>
<bodyText confidence="0.978269171428571">
2003) or predicate argument tuple matching (Qiu,
et al., 2006). Some other approaches that go be-
yond simple lexical matching have also been de-
veloped. For example, (Mihalcea, et al., 2006) es-
timated semantic similarity of sentence pairs with
word-to-word similarity measures and a word
specificity measure. (Zhang and Patrick, 2005)
uses text canonicalization to transfer texts of simi-
lar meaning into the same surface text with a high-
er probability than those with different meaning.
Many of these approaches adopt distributional
semantic models, but limited to a word level. To
extend distributional semantic models beyond
words, several researchers have learned phrase or
sentence representation by composing the repre-
sentation of individual words (Mitchell and Lapata,
2010; Baroni and Zamparelli, 2010). An alternative
approach by (Socher et al., 2011) represents
phrases and sentences with fixed matrices consist-
ing of pooled word and phrase pairwise similari-
ties. (Le and Mikolov, 2014) learns representation
of sentences directly by predicting context without
composition of words.
In our work, we judge that two sentences are
paraphrases if they have high degree of semantic
similarity. We use the UMBC-Semantic Textual
Similarity system (Lushan Han et al., 2013), which
provides high accurate semantic similarity meas-
urement. The remainder of this paper is organized
as follows. Section 2 describes the task and the
details of our method. Section 3 presents our re-
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 51–55,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
sults and a brief discussion. The last section offers
conclusions.
</bodyText>
<sectionHeader confidence="0.973887" genericHeader="method">
2. Our Method
</sectionHeader>
<bodyText confidence="0.999748333333333">
To decide whether two tweets are paraphrases or
not, we use a measurement based on semantic sim-
ilarity values. If two tweets are semantically simi-
lar, they are judged as paraphrases, otherwise they
are not. We described steps of our method as fol-
lows.
</bodyText>
<subsectionHeader confidence="0.994474">
1.1. Preprocessing
</subsectionHeader>
<bodyText confidence="0.999936419354839">
Generally, tweets are informal text sequences that
include abbreviations, neologisms, emoticons and
slang terms as well genre-specific elements such as
hashtags, URLs and @mentions of other Twitter
accounts. This is due to both the informal nature
of the medium and the requirement to limit content
to at most 140 characters. Thus, before measuring
the semantic similarity, we replace abbreviation
and slang to the readable version. We collected
about 685 popular abbreviations and slang terms
from several Web resources1 and combined these
with the provided twitter normalization lexicon
developed by Han Bo and Timothy Baldwin
(2011).
After replacing abbreviations and slang terms,
we remove all stop words to get our final desired
processed tweets. Then we produce a set of two-
skip trigrams for each tweet and name these sets as
trigram sets. We adapted the skip-gram technique
from (Guthrie, et al., 2006).
Take the tweet “Google Now for iOS simply
beautiful” as an example, after removing stop word
s, we get ‘Google Now iOS simply beautiful’. Then
a two-skip trigram set is produced: {‘Google Now
iOS’, ‘Now iOS simply’, ‘iOS simply beautiful’,
‘Google iOS simply’, ‘Google simply beautiful’,
‘Now simply beautiful’, ‘Google Now beautiful’,
‘Google Now simply’, ‘Now iOS beautiful’}, which
is referred as trigram set. We transform every raw
tweet into its processed version and then corre-
sponding trigram set.
</bodyText>
<footnote confidence="0.971458">
1 These included http://webopedia.com, http://blog.-
mltcreative.com and http://internetslang.com and others.
</footnote>
<subsectionHeader confidence="0.821813">
1.2. LSA Word Similarity Model
</subsectionHeader>
<bodyText confidence="0.999983651162791">
Our LSA word similarity model is a revised ver-
sion of the one we used in the 2013 and 2014
SemEval semantic text similarity tasks (Han, et al.,
2013, Kashyap et al., 2014). LSA relies on the fact
that semantically similar words (e.g., cat and feline
or nurse and doctor) are more likely to occur near
one another in text. Thus evidence for word simi-
larity can be computed from a statistical analysis of
a large text corpus. We extract raw word co-
occurrence statistics from a portion of a 2007 Stan-
ford WebBase dataset (Stanford, 2001).
We performed part of speech tagging and lem-
matization on the corpus using the Stanford POS
tagger (Toutanova et al., 2000). Word/term co-
occurrences were counted with a sliding window
of fixed size over the entire corpus. We generate
two co-occurrence models using window sizes ±1
and ±4. The smaller window provides more precise
context which is better for comparing words of the
same part of speech while the larger one is more
suitable for computing the semantic similarity be-
tween words of different syntactic categories.
Our word co-occurrence models are based on a
predefined vocabulary of 22,000 common English
open-class words and noun phrases, extended with
about 2,000 verb phrases from WordNet. The final
dimensions of our word/phrase co-occurrence ma-
trices are 29,000×29,000 when words/phrases are
POS tagged. We apply singular value decomposi-
tion on the word/phrase co-occurrence matrices
(Burgess 1998) after transforming the raw
word/phrase co-occurrence counts into their log
frequencies, and select the 300 largest singular
values. The LSA similarity between two
words/phrases is then defined as the cosine similar-
ity of their corresponding LSA vectors generated
by the SVD transformation.
To compute the semantic similarity of two text
sequences, we use the simple align-and-penalize
algorithm described in (Han et al., 2013) with a
few improvements. These improvements include
some sets of common disjoint concepts and an en-
hanced stop word list.
</bodyText>
<sectionHeader confidence="0.698985" genericHeader="method">
1.3. Features
</sectionHeader>
<bodyText confidence="0.999778333333333">
For two trigram sets, we compute the semantic
similarity of every possible pair of trigrams in the-
se two sets using the UMBC Semantic Textual
</bodyText>
<page confidence="0.992532">
52
</page>
<bodyText confidence="0.958596">
Similarity system. For each pair of tweet (T1 and
T2), six features are produced as:
</bodyText>
<listItem confidence="0.9946789">
• Feature1 = semantic similarity value between
each pair of tweets (whole sentence with ab-
breviation and slangs replaced, and stop words
removed)
• Feature2 =
• Feature3 =
• Feature4 =
• Feature5 =
• Feature6 = the weighted average on length of
tweets of two averages above.
</listItem>
<subsectionHeader confidence="0.798449">
1.5. Training
</subsectionHeader>
<bodyText confidence="0.974255388888889">
We used the LIBSVM system (Chang and Lin,
2011) for training a logistic regression model and a
support vector regression model. We run a grid
search to find the best parameters for both models.
All training data (13,063 pairs of tweets) were used
to train the models without discarding any debata-
ble data. We tested the contribution for of each of
the features through ablation experiments on the
development data in which each feature was delet-
ed in each experimental run. Table 1 shows the
statistical results for each feature ablation run.
Feature deleted F1 Precision Recall
Feature 1 0.7 0.709 0.728
Feature 2 0.697 0.706 0.726
Feature 3 0.697 0.706 0.726
Feature 4 0.691 0.700 0.722
Feature 5 0.696 0.706 0.726
Feature 6 0.695 0.705 0.725
</bodyText>
<tableCaption confidence="0.9987765">
Table 1. Performance of our system on runs against the
development data in which each feature was removed.
</tableCaption>
<bodyText confidence="0.998680714285714">
From Table 1, we can see that the feature of lowest
performance is Feature 1, the semantic similarity
computed with entire tweets without using the
skip-gram technique. But we still keep Feature 1
since performance of these six features is not sig-
nificantly different. We show the performance of
each model on development data in Table 2.
</bodyText>
<table confidence="0.9987812">
Model F1 Precision Recall
Logistic 0.697 0.706 0.726
Regression
Support Vector 0.691 0.707 0.726
Regression
</table>
<tableCaption confidence="0.99992">
Table 2. Performance of system on development data.
</tableCaption>
<bodyText confidence="0.999127">
Since the performance of both systems is almost
the same, we decide to submit one run of each sys-
tem.
</bodyText>
<sectionHeader confidence="0.998368" genericHeader="evaluation">
3. Results and Discussions
</sectionHeader>
<bodyText confidence="0.999947914285714">
We submit two runs: Run1 (Logistic Regression)
obtained an F1 score of 0.599, precision score of
0.651 and recall score of 0.554, and Run2 (Support
Vector Regression), which received an F1 of
0.590, precision of 0.646, and recall of 0.543.
When ranked, we are in the eighteenth (Run1) and
the nineteenth (Run2) out of the 38 runs. The first
rank has F1 score of 0.674. The full distribution of
F1 score is shown in Figure 1. The relatively low
ranking of our system might be the result of sever-
al factors.
First factor is the prevalence of neologisms,
misspellings, informal slang and abbreviations in
tweets. Better preprocessing to make the tweets
closer to normal text might improve our results.
Another factor is the UMBC STS system. Ex-
amples of input on which UMBC STS system per-
form poorly are shown in Table 3. We can group
these into two sets, each associated with problem
in performing the paraphrase task.
The first problem is that a slang word may have
different meanings when it is used in different gen-
res. As we can see in the first example in Table 3,
‘bombs’ does not mean ‘a container filled with
explosive’ but is a synonym of ‘home runs’ when
mentioned in a sports or baseball context. We can
recognize this meaning by reading sport articles
but it is not included in any dictionaries or
WordNet. Thus our system predicts that the two
tweets, each containing either ‘bombs’ or ‘home
runs’, have low semantic similarity and thus are
not paraphrases.
The second problem involves out-of-vocabulary
words, such as the named entities found in the ex-
amples in Table 3. Tweet 2 of the second example
</bodyText>
<page confidence="0.997244">
53
</page>
<bodyText confidence="0.979540263157895">
‘NOW YOU SEE ME and AFTER EARTH Cant
Outpace FAST FURIOUS 6’ is full of movie
names whose meanings our STS system cannot
recognize. We can solve this problem by adding
name entity recognition to the system. Another
potential solution would be to adopt a simple
string-matching component. With string matching,
we may handle those out-of-vocabulary words sit-
uations similar to the third and fourth example. We
can match ‘orr’ and ‘chara’ between two tweets of
the third example and ‘new ciroc’ in the fourth ex-
ample.
To improve our STS performance, which is
trained on a corpus that mostly consisted of rea-
sonably well-written narrative text, we need to ex-
pand training corpus. Training a LSA model on a
collection of tweets or a mixture of tweets and nar-
rative text, and adding name entity recognition
process may lead to better results.
</bodyText>
<figureCaption confidence="0.997501">
Figure 1. Ranked F1 score of 38 runs
</figureCaption>
<table confidence="0.999604333333333">
# Tweet 1 Tweet 2 System Gold
1 chris davis is 44 with two bombs Chris Davis has 2 home runs tonight False True
2 I wanna see the movie after earth NOW YOU SEE ME and AFTER EARTH True False
Cant Outpace FAST FURIOUS 6
3 Orr with a big hit on Chara I keep waiting for the chara vs orr fight False True
4 New Ciroc Amaretto I NEED THAT Oh shit I gotta try that new ciroc flavor False True
</table>
<tableCaption confidence="0.999524">
Table 3. Examples of input pairs on which our system performed poorly
</tableCaption>
<sectionHeader confidence="0.969863" genericHeader="conclusions">
4. Conclusion
</sectionHeader>
<bodyText confidence="0.999922545454545">
We describe our system submitted in participating
the SemEval 2015 Task 1 Paraphrase and Seman-
tic Similarity in Twitter. We preprocess tweets us-
ing two-skip trigrams to produce sets of possible
trigrams and measure their semantic similarity us-
ing the UMBC-STS system. We computed the sta-
tistical value as maximum and average of each pair
and use two regression models; logistic regression
and support vector regression. Our best performing
run achieved an F1 score of 0.599 and was ranked
eleventh out of eighteen teams.
</bodyText>
<sectionHeader confidence="0.984403" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99902725">
Partial support for this research was provided by
grants from the National Science Foundation
(1228198 and 1250627) and a grant from the Mar-
yland Industrial Partnerships program.
</bodyText>
<page confidence="0.997711">
54
</page>
<sectionHeader confidence="0.988807" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.990354489130435">
Marco Baroni and Roberto Zamparelli. 2010. Nouns are
vectors, adjectives are matrices: Representing adjec-
tive-noun constructions in semantic space. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP 2010).
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multi-
ple-sequence alignment. In Proceedings of Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics – Human Lan-
guage Technologies(HLT-NAACL)
William Blacoe. and Mirella Lapata 2012. A compari-
son of vector-based representations for semantic
composition, Proceedings of EMNLP, Jeju Island,
Korea, pp. 546-556.
Han, Bo, and Timothy Baldwin. 2011. Lexical normali-
sation of short text messages: Makn sens a# twitter.
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1. Association for
Computational Linguistics, 2011.
Curt Burgessa, Kay Livesayb and Kevin Lundb 1998.
Explorations in context space: Words, sentences, dis-
course. Discourse Processes, 25:211–257.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM :
a library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2:27:1-
-27:27, 2011.
David Guthrie, Ben Allison, Wei Liu, Louise Guthrie,
Yorick Wilks. 2006. &amp;quot;A closer look at skip-gram
modelling.&amp;quot; In Proceedings of the 5th international
Conference on Language Resources and Evaluation
(LREC-2006), pp. 1-4. 2006.
Lushan Han, Tim Finin, Paul McNamee, Anupam Joshi
and Yelena Yesha, Improving Word Similarity by
Augmenting PMI with Estimates of Word Polysemy,
IEEE Transactions on Knowledge and Data Engi-
neering, IEEE Computer Society, v25n6, pp. 1307-
1322, 2013.
Lushan Han, Abhay L. Kashyap, Tim Finin, James
Mayfield, and Johnathan Weese. 2013. UMBC
EBIQUITY-CORE: Semantic Textual Similarity
Systems, In Second Joint Conf. on Lexical and Com-
putational Semantics. Association for Computational
Linguistics , June.
Lushan Han, Schema Free Querying of Semantic Data,
Ph.D. Dissertation, University of Maryland, Balti-
more County, August 2014.
Abhay Kashyap, Lushan Han, Roberto Yus, Jennifer
Sleeman, Taneeya Satyapanich, Sunil Gandhi and
Tim Finin. 2014. Meerkat Mafia: Multilingual and
Cross-Level Semantic Textual Similarity Systems,
Int. Workshop on Semantic Evaluation, Association
for Computational Linguistics.
Rada Mihalcea, Courtney Corley and Carlo Strapparava.
2006. Corpus-based and knowledge-based measures
of text semantic similarity, Proceedings of the Na-
tional Conference on Artificial Intelligence (AAAI
2006), Boston, Massachusetts, pp. 775-780
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Sci-
ence, 34(8).
Long Qiu, Min-Yen Kan, and Tat-Seng Chua. 2006.
Paraphrase recognition via dissimilarity significance
classification. In Proceedings of the 2006 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 18–26, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.
Le, Quoc V., and Tomas Mikolov. &amp;quot;Distributed repre-
sentations of sentences and documents.&amp;quot; arXiv pre-
print arXiv:1405.4053 (2014).
Richard Socher, Eric H. Huang, Jeffrey Pennington,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Dynamic pooling and unfolding recursive
autoencoders for paraphrase detection. In Advances
in Neural Information Processing Systems (NIPS
2011).
Stanford. 2001. Stanford WebBase project.
http://bit.ly/WebBase.
Kristina Toutanova, Dan Klein, Christopher Manning,
William Morgan, Anna Rafferty, and Michel Galley.
2000. Stanford log-linear part-of-speech tagger.
http://nlp.stanford.edu/software/tagger.shtml.
Wei Xu, Chris Callison-Burch and William B. Dolan.
2015. SemEval-2015 Task 1: Paraphrase and Seman-
tic Similarity in Twitter ,Proceedings of the 9th In-
ternational Workshop on Semantic Evaluation
(SemEval),2015.
Yitao Zhang and Jon Patrick. 2005. Paraphrase identifi-
cation by text canonicalization. In Proceedings of the
Australasian Language Technology Workshop 2005,
pages 160–166, Sydney, Australia, December.
</reference>
<page confidence="0.999064">
55
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.838373">
<title confidence="0.9786405">Ebiquity: Paraphrase and Similarity in Twitter using Skipgram</title>
<author confidence="0.906501">Gao</author>
<affiliation confidence="0.994009">University of Maryland, Baltimore</affiliation>
<address confidence="0.999408">Baltimore, MD, 21250,</address>
<email confidence="0.998218">taneeya1@umbc.edu,hanggao1@umbc.edu,finin@umbc.edu</email>
<abstract confidence="0.993916666666667">We describe the system we developed to particin 2015 Task 1, Paraphrase and Similarity in We create similarity vectors from two-skip trigrams of preprocessed tweets and measure their semantic similarity using our UMBC-STS system. We submit two runs. The best result is ranked eleventh out of eighteen teams with F1 score of 0.599.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<contexts>
<context position="2345" citStr="Baroni and Zamparelli, 2010" startWordPosition="372" endWordPosition="375">l., 2006) estimated semantic similarity of sentence pairs with word-to-word similarity measures and a word specificity measure. (Zhang and Patrick, 2005) uses text canonicalization to transfer texts of similar meaning into the same surface text with a higher probability than those with different meaning. Many of these approaches adopt distributional semantic models, but limited to a word level. To extend distributional semantic models beyond words, several researchers have learned phrase or sentence representation by composing the representation of individual words (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010). An alternative approach by (Socher et al., 2011) represents phrases and sentences with fixed matrices consisting of pooled word and phrase pairwise similarities. (Le and Mikolov, 2014) learns representation of sentences directly by predicting context without composition of words. In our work, we judge that two sentences are paraphrases if they have high degree of semantic similarity. We use the UMBC-Semantic Textual Similarity system (Lushan Han et al., 2013), which provides high accurate semantic similarity measurement. The remainder of this paper is organized as follows. Section 2 describe</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP 2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Lillian Lee</author>
</authors>
<title>Learning to paraphrase: An unsupervised approach using multiple-sequence alignment.</title>
<date>2003</date>
<booktitle>In Proceedings of Conference of the North American Chapter of the Association for Computational Linguistics – Human Language Technologies(HLT-NAACL)</booktitle>
<marker>Barzilay, Lee, 2003</marker>
<rawString>Regina Barzilay and Lillian Lee. 2003. Learning to paraphrase: An unsupervised approach using multiple-sequence alignment. In Proceedings of Conference of the North American Chapter of the Association for Computational Linguistics – Human Language Technologies(HLT-NAACL)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
</authors>
<title>A comparison of vector-based representations for semantic composition,</title>
<date>2012</date>
<booktitle>Proceedings of EMNLP, Jeju Island, Korea,</booktitle>
<pages>546--556</pages>
<marker>Lapata, 2012</marker>
<rawString>William Blacoe. and Mirella Lapata 2012. A comparison of vector-based representations for semantic composition, Proceedings of EMNLP, Jeju Island, Korea, pp. 546-556.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Han</author>
<author>Timothy Baldwin</author>
</authors>
<title>Lexical normalisation of short text messages: Makn sens a# twitter.</title>
<date>2011</date>
<booktitle>Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1. Association for Computational Linguistics,</booktitle>
<marker>Han, Baldwin, 2011</marker>
<rawString>Han, Bo, and Timothy Baldwin. 2011. Lexical normalisation of short text messages: Makn sens a# twitter. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1. Association for Computational Linguistics, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Curt Burgessa</author>
</authors>
<title>Kay Livesayb and Kevin Lundb</title>
<date>1998</date>
<booktitle>Explorations in context space: Words, sentences, discourse. Discourse Processes,</booktitle>
<pages>25--211</pages>
<marker>Burgessa, 1998</marker>
<rawString>Curt Burgessa, Kay Livesayb and Kevin Lundb 1998. Explorations in context space: Words, sentences, discourse. Discourse Processes, 25:211–257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM : a library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<pages>2--27</pages>
<contexts>
<context position="7683" citStr="Chang and Lin, 2011" startWordPosition="1219" endWordPosition="1222">oncepts and an enhanced stop word list. 1.3. Features For two trigram sets, we compute the semantic similarity of every possible pair of trigrams in these two sets using the UMBC Semantic Textual 52 Similarity system. For each pair of tweet (T1 and T2), six features are produced as: • Feature1 = semantic similarity value between each pair of tweets (whole sentence with abbreviation and slangs replaced, and stop words removed) • Feature2 = • Feature3 = • Feature4 = • Feature5 = • Feature6 = the weighted average on length of tweets of two averages above. 1.5. Training We used the LIBSVM system (Chang and Lin, 2011) for training a logistic regression model and a support vector regression model. We run a grid search to find the best parameters for both models. All training data (13,063 pairs of tweets) were used to train the models without discarding any debatable data. We tested the contribution for of each of the features through ablation experiments on the development data in which each feature was deleted in each experimental run. Table 1 shows the statistical results for each feature ablation run. Feature deleted F1 Precision Recall Feature 1 0.7 0.709 0.728 Feature 2 0.697 0.706 0.726 Feature 3 0.69</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM : a library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1--27:27, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Guthrie</author>
<author>Ben Allison</author>
<author>Wei Liu</author>
<author>Louise Guthrie</author>
<author>Yorick Wilks</author>
</authors>
<title>A closer look at skip-gram modelling.&amp;quot;</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th international Conference on Language Resources and Evaluation (LREC-2006),</booktitle>
<pages>1--4</pages>
<contexts>
<context position="4454" citStr="Guthrie, et al., 2006" startWordPosition="700" endWordPosition="703">mit content to at most 140 characters. Thus, before measuring the semantic similarity, we replace abbreviation and slang to the readable version. We collected about 685 popular abbreviations and slang terms from several Web resources1 and combined these with the provided twitter normalization lexicon developed by Han Bo and Timothy Baldwin (2011). After replacing abbreviations and slang terms, we remove all stop words to get our final desired processed tweets. Then we produce a set of twoskip trigrams for each tweet and name these sets as trigram sets. We adapted the skip-gram technique from (Guthrie, et al., 2006). Take the tweet “Google Now for iOS simply beautiful” as an example, after removing stop word s, we get ‘Google Now iOS simply beautiful’. Then a two-skip trigram set is produced: {‘Google Now iOS’, ‘Now iOS simply’, ‘iOS simply beautiful’, ‘Google iOS simply’, ‘Google simply beautiful’, ‘Now simply beautiful’, ‘Google Now beautiful’, ‘Google Now simply’, ‘Now iOS beautiful’}, which is referred as trigram set. We transform every raw tweet into its processed version and then corresponding trigram set. 1 These included http://webopedia.com, http://blog.- mltcreative.com and http://internetslang</context>
</contexts>
<marker>Guthrie, Allison, Liu, Guthrie, Wilks, 2006</marker>
<rawString>David Guthrie, Ben Allison, Wei Liu, Louise Guthrie, Yorick Wilks. 2006. &amp;quot;A closer look at skip-gram modelling.&amp;quot; In Proceedings of the 5th international Conference on Language Resources and Evaluation (LREC-2006), pp. 1-4. 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lushan Han</author>
<author>Tim Finin</author>
<author>Paul McNamee</author>
<author>Anupam Joshi</author>
<author>Yelena Yesha</author>
</authors>
<title>Improving Word Similarity by Augmenting PMI with Estimates of Word Polysemy,</title>
<date>2013</date>
<journal>IEEE Transactions on Knowledge and Data Engineering, IEEE Computer Society,</journal>
<volume>25</volume>
<pages>1307--1322</pages>
<contexts>
<context position="2810" citStr="Han et al., 2013" startWordPosition="443" endWordPosition="446">e learned phrase or sentence representation by composing the representation of individual words (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010). An alternative approach by (Socher et al., 2011) represents phrases and sentences with fixed matrices consisting of pooled word and phrase pairwise similarities. (Le and Mikolov, 2014) learns representation of sentences directly by predicting context without composition of words. In our work, we judge that two sentences are paraphrases if they have high degree of semantic similarity. We use the UMBC-Semantic Textual Similarity system (Lushan Han et al., 2013), which provides high accurate semantic similarity measurement. The remainder of this paper is organized as follows. Section 2 describes the task and the details of our method. Section 3 presents our reProceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 51–55, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics sults and a brief discussion. The last section offers conclusions. 2. Our Method To decide whether two tweets are paraphrases or not, we use a measurement based on semantic similarity values. If two tweets are semanti</context>
<context position="5250" citStr="Han, et al., 2013" startWordPosition="823" endWordPosition="826">: {‘Google Now iOS’, ‘Now iOS simply’, ‘iOS simply beautiful’, ‘Google iOS simply’, ‘Google simply beautiful’, ‘Now simply beautiful’, ‘Google Now beautiful’, ‘Google Now simply’, ‘Now iOS beautiful’}, which is referred as trigram set. We transform every raw tweet into its processed version and then corresponding trigram set. 1 These included http://webopedia.com, http://blog.- mltcreative.com and http://internetslang.com and others. 1.2. LSA Word Similarity Model Our LSA word similarity model is a revised version of the one we used in the 2013 and 2014 SemEval semantic text similarity tasks (Han, et al., 2013, Kashyap et al., 2014). LSA relies on the fact that semantically similar words (e.g., cat and feline or nurse and doctor) are more likely to occur near one another in text. Thus evidence for word similarity can be computed from a statistical analysis of a large text corpus. We extract raw word cooccurrence statistics from a portion of a 2007 Stanford WebBase dataset (Stanford, 2001). We performed part of speech tagging and lemmatization on the corpus using the Stanford POS tagger (Toutanova et al., 2000). Word/term cooccurrences were counted with a sliding window of fixed size over the entire</context>
<context position="6980" citStr="Han et al., 2013" startWordPosition="1096" endWordPosition="1099"> of our word/phrase co-occurrence matrices are 29,000×29,000 when words/phrases are POS tagged. We apply singular value decomposition on the word/phrase co-occurrence matrices (Burgess 1998) after transforming the raw word/phrase co-occurrence counts into their log frequencies, and select the 300 largest singular values. The LSA similarity between two words/phrases is then defined as the cosine similarity of their corresponding LSA vectors generated by the SVD transformation. To compute the semantic similarity of two text sequences, we use the simple align-and-penalize algorithm described in (Han et al., 2013) with a few improvements. These improvements include some sets of common disjoint concepts and an enhanced stop word list. 1.3. Features For two trigram sets, we compute the semantic similarity of every possible pair of trigrams in these two sets using the UMBC Semantic Textual 52 Similarity system. For each pair of tweet (T1 and T2), six features are produced as: • Feature1 = semantic similarity value between each pair of tweets (whole sentence with abbreviation and slangs replaced, and stop words removed) • Feature2 = • Feature3 = • Feature4 = • Feature5 = • Feature6 = the weighted average o</context>
</contexts>
<marker>Han, Finin, McNamee, Joshi, Yesha, 2013</marker>
<rawString>Lushan Han, Tim Finin, Paul McNamee, Anupam Joshi and Yelena Yesha, Improving Word Similarity by Augmenting PMI with Estimates of Word Polysemy, IEEE Transactions on Knowledge and Data Engineering, IEEE Computer Society, v25n6, pp. 1307-1322, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lushan Han</author>
<author>Abhay L Kashyap</author>
<author>Tim Finin</author>
<author>James Mayfield</author>
<author>Johnathan Weese</author>
</authors>
<title>UMBC EBIQUITY-CORE: Semantic Textual Similarity Systems,</title>
<date>2013</date>
<booktitle>In Second Joint Conf. on Lexical and Computational Semantics. Association for Computational Linguistics ,</booktitle>
<contexts>
<context position="2810" citStr="Han et al., 2013" startWordPosition="443" endWordPosition="446">e learned phrase or sentence representation by composing the representation of individual words (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010). An alternative approach by (Socher et al., 2011) represents phrases and sentences with fixed matrices consisting of pooled word and phrase pairwise similarities. (Le and Mikolov, 2014) learns representation of sentences directly by predicting context without composition of words. In our work, we judge that two sentences are paraphrases if they have high degree of semantic similarity. We use the UMBC-Semantic Textual Similarity system (Lushan Han et al., 2013), which provides high accurate semantic similarity measurement. The remainder of this paper is organized as follows. Section 2 describes the task and the details of our method. Section 3 presents our reProceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 51–55, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics sults and a brief discussion. The last section offers conclusions. 2. Our Method To decide whether two tweets are paraphrases or not, we use a measurement based on semantic similarity values. If two tweets are semanti</context>
<context position="5250" citStr="Han, et al., 2013" startWordPosition="823" endWordPosition="826">: {‘Google Now iOS’, ‘Now iOS simply’, ‘iOS simply beautiful’, ‘Google iOS simply’, ‘Google simply beautiful’, ‘Now simply beautiful’, ‘Google Now beautiful’, ‘Google Now simply’, ‘Now iOS beautiful’}, which is referred as trigram set. We transform every raw tweet into its processed version and then corresponding trigram set. 1 These included http://webopedia.com, http://blog.- mltcreative.com and http://internetslang.com and others. 1.2. LSA Word Similarity Model Our LSA word similarity model is a revised version of the one we used in the 2013 and 2014 SemEval semantic text similarity tasks (Han, et al., 2013, Kashyap et al., 2014). LSA relies on the fact that semantically similar words (e.g., cat and feline or nurse and doctor) are more likely to occur near one another in text. Thus evidence for word similarity can be computed from a statistical analysis of a large text corpus. We extract raw word cooccurrence statistics from a portion of a 2007 Stanford WebBase dataset (Stanford, 2001). We performed part of speech tagging and lemmatization on the corpus using the Stanford POS tagger (Toutanova et al., 2000). Word/term cooccurrences were counted with a sliding window of fixed size over the entire</context>
<context position="6980" citStr="Han et al., 2013" startWordPosition="1096" endWordPosition="1099"> of our word/phrase co-occurrence matrices are 29,000×29,000 when words/phrases are POS tagged. We apply singular value decomposition on the word/phrase co-occurrence matrices (Burgess 1998) after transforming the raw word/phrase co-occurrence counts into their log frequencies, and select the 300 largest singular values. The LSA similarity between two words/phrases is then defined as the cosine similarity of their corresponding LSA vectors generated by the SVD transformation. To compute the semantic similarity of two text sequences, we use the simple align-and-penalize algorithm described in (Han et al., 2013) with a few improvements. These improvements include some sets of common disjoint concepts and an enhanced stop word list. 1.3. Features For two trigram sets, we compute the semantic similarity of every possible pair of trigrams in these two sets using the UMBC Semantic Textual 52 Similarity system. For each pair of tweet (T1 and T2), six features are produced as: • Feature1 = semantic similarity value between each pair of tweets (whole sentence with abbreviation and slangs replaced, and stop words removed) • Feature2 = • Feature3 = • Feature4 = • Feature5 = • Feature6 = the weighted average o</context>
</contexts>
<marker>Han, Kashyap, Finin, Mayfield, Weese, 2013</marker>
<rawString>Lushan Han, Abhay L. Kashyap, Tim Finin, James Mayfield, and Johnathan Weese. 2013. UMBC EBIQUITY-CORE: Semantic Textual Similarity Systems, In Second Joint Conf. on Lexical and Computational Semantics. Association for Computational Linguistics , June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lushan Han</author>
</authors>
<title>Schema Free Querying of Semantic Data,</title>
<date>2014</date>
<institution>Ph.D. Dissertation, University of Maryland, Baltimore County,</institution>
<marker>Han, 2014</marker>
<rawString>Lushan Han, Schema Free Querying of Semantic Data, Ph.D. Dissertation, University of Maryland, Baltimore County, August 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abhay Kashyap</author>
<author>Lushan Han</author>
<author>Roberto Yus</author>
<author>Jennifer Sleeman</author>
</authors>
<title>Taneeya Satyapanich, Sunil Gandhi and Tim Finin.</title>
<date>2014</date>
<booktitle>Meerkat Mafia: Multilingual and Cross-Level Semantic Textual Similarity Systems, Int. Workshop on Semantic Evaluation, Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5273" citStr="Kashyap et al., 2014" startWordPosition="827" endWordPosition="830">, ‘Now iOS simply’, ‘iOS simply beautiful’, ‘Google iOS simply’, ‘Google simply beautiful’, ‘Now simply beautiful’, ‘Google Now beautiful’, ‘Google Now simply’, ‘Now iOS beautiful’}, which is referred as trigram set. We transform every raw tweet into its processed version and then corresponding trigram set. 1 These included http://webopedia.com, http://blog.- mltcreative.com and http://internetslang.com and others. 1.2. LSA Word Similarity Model Our LSA word similarity model is a revised version of the one we used in the 2013 and 2014 SemEval semantic text similarity tasks (Han, et al., 2013, Kashyap et al., 2014). LSA relies on the fact that semantically similar words (e.g., cat and feline or nurse and doctor) are more likely to occur near one another in text. Thus evidence for word similarity can be computed from a statistical analysis of a large text corpus. We extract raw word cooccurrence statistics from a portion of a 2007 Stanford WebBase dataset (Stanford, 2001). We performed part of speech tagging and lemmatization on the corpus using the Stanford POS tagger (Toutanova et al., 2000). Word/term cooccurrences were counted with a sliding window of fixed size over the entire corpus. We generate tw</context>
</contexts>
<marker>Kashyap, Han, Yus, Sleeman, 2014</marker>
<rawString>Abhay Kashyap, Lushan Han, Roberto Yus, Jennifer Sleeman, Taneeya Satyapanich, Sunil Gandhi and Tim Finin. 2014. Meerkat Mafia: Multilingual and Cross-Level Semantic Textual Similarity Systems, Int. Workshop on Semantic Evaluation, Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and knowledge-based measures of text semantic similarity,</title>
<date>2006</date>
<booktitle>Proceedings of the National Conference on Artificial Intelligence (AAAI 2006),</booktitle>
<pages>775--780</pages>
<location>Boston, Massachusetts,</location>
<contexts>
<context position="1726" citStr="Mihalcea, et al., 2006" startWordPosition="282" endWordPosition="285"> and Im in town and Im upset” might be judged as not paraphrases with score 0.2 whereas the pair “A Walk to Remember is the definition of true love” and “A Walk to Remember is the cutest thing” could be judged as paraphrases with a score of 0.6. Many methods have been proposed to solve the paraphrase detection problem. Early approaches were often based on lexical matching techniques, e.g., word n-gram overlap (Barzilay and Lee, 51 2003) or predicate argument tuple matching (Qiu, et al., 2006). Some other approaches that go beyond simple lexical matching have also been developed. For example, (Mihalcea, et al., 2006) estimated semantic similarity of sentence pairs with word-to-word similarity measures and a word specificity measure. (Zhang and Patrick, 2005) uses text canonicalization to transfer texts of similar meaning into the same surface text with a higher probability than those with different meaning. Many of these approaches adopt distributional semantic models, but limited to a word level. To extend distributional semantic models beyond words, several researchers have learned phrase or sentence representation by composing the representation of individual words (Mitchell and Lapata, 2010; Baroni an</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Rada Mihalcea, Courtney Corley and Carlo Strapparava. 2006. Corpus-based and knowledge-based measures of text semantic similarity, Proceedings of the National Conference on Artificial Intelligence (AAAI 2006), Boston, Massachusetts, pp. 775-780</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>8</issue>
<contexts>
<context position="2315" citStr="Mitchell and Lapata, 2010" startWordPosition="368" endWordPosition="371">or example, (Mihalcea, et al., 2006) estimated semantic similarity of sentence pairs with word-to-word similarity measures and a word specificity measure. (Zhang and Patrick, 2005) uses text canonicalization to transfer texts of similar meaning into the same surface text with a higher probability than those with different meaning. Many of these approaches adopt distributional semantic models, but limited to a word level. To extend distributional semantic models beyond words, several researchers have learned phrase or sentence representation by composing the representation of individual words (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010). An alternative approach by (Socher et al., 2011) represents phrases and sentences with fixed matrices consisting of pooled word and phrase pairwise similarities. (Le and Mikolov, 2014) learns representation of sentences directly by predicting context without composition of words. In our work, we judge that two sentences are paraphrases if they have high degree of semantic similarity. We use the UMBC-Semantic Textual Similarity system (Lushan Han et al., 2013), which provides high accurate semantic similarity measurement. The remainder of this paper is organized </context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of semantics. Cognitive Science, 34(8).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Long Qiu</author>
<author>Min-Yen Kan</author>
<author>Tat-Seng Chua</author>
</authors>
<title>Paraphrase recognition via dissimilarity significance classification.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>18--26</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="1600" citStr="Qiu, et al., 2006" startWordPosition="261" endWordPosition="264">ber” (a film released in 2002), the pair A Walk to Remember is the definition of true love” and “A Walk to Remember is on and Im in town and Im upset” might be judged as not paraphrases with score 0.2 whereas the pair “A Walk to Remember is the definition of true love” and “A Walk to Remember is the cutest thing” could be judged as paraphrases with a score of 0.6. Many methods have been proposed to solve the paraphrase detection problem. Early approaches were often based on lexical matching techniques, e.g., word n-gram overlap (Barzilay and Lee, 51 2003) or predicate argument tuple matching (Qiu, et al., 2006). Some other approaches that go beyond simple lexical matching have also been developed. For example, (Mihalcea, et al., 2006) estimated semantic similarity of sentence pairs with word-to-word similarity measures and a word specificity measure. (Zhang and Patrick, 2005) uses text canonicalization to transfer texts of similar meaning into the same surface text with a higher probability than those with different meaning. Many of these approaches adopt distributional semantic models, but limited to a word level. To extend distributional semantic models beyond words, several researchers have learn</context>
</contexts>
<marker>Qiu, Kan, Chua, 2006</marker>
<rawString>Long Qiu, Min-Yen Kan, and Tat-Seng Chua. 2006. Paraphrase recognition via dissimilarity significance classification. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 18–26, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quoc V Le</author>
<author>Tomas Mikolov</author>
</authors>
<title>Distributed representations of sentences and documents.&amp;quot; arXiv preprint arXiv:1405.4053</title>
<date>2014</date>
<contexts>
<context position="2531" citStr="Le and Mikolov, 2014" startWordPosition="401" endWordPosition="404">texts of similar meaning into the same surface text with a higher probability than those with different meaning. Many of these approaches adopt distributional semantic models, but limited to a word level. To extend distributional semantic models beyond words, several researchers have learned phrase or sentence representation by composing the representation of individual words (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010). An alternative approach by (Socher et al., 2011) represents phrases and sentences with fixed matrices consisting of pooled word and phrase pairwise similarities. (Le and Mikolov, 2014) learns representation of sentences directly by predicting context without composition of words. In our work, we judge that two sentences are paraphrases if they have high degree of semantic similarity. We use the UMBC-Semantic Textual Similarity system (Lushan Han et al., 2013), which provides high accurate semantic similarity measurement. The remainder of this paper is organized as follows. Section 2 describes the task and the details of our method. Section 3 presents our reProceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 51–55, Denver, Colorado, Jun</context>
</contexts>
<marker>Le, Mikolov, 2014</marker>
<rawString>Le, Quoc V., and Tomas Mikolov. &amp;quot;Distributed representations of sentences and documents.&amp;quot; arXiv preprint arXiv:1405.4053 (2014).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Eric H Huang</author>
<author>Jeffrey Pennington</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Dynamic pooling and unfolding recursive autoencoders for paraphrase detection.</title>
<date>2011</date>
<booktitle>In Advances in Neural Information Processing Systems (NIPS</booktitle>
<contexts>
<context position="2395" citStr="Socher et al., 2011" startWordPosition="380" endWordPosition="383">with word-to-word similarity measures and a word specificity measure. (Zhang and Patrick, 2005) uses text canonicalization to transfer texts of similar meaning into the same surface text with a higher probability than those with different meaning. Many of these approaches adopt distributional semantic models, but limited to a word level. To extend distributional semantic models beyond words, several researchers have learned phrase or sentence representation by composing the representation of individual words (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010). An alternative approach by (Socher et al., 2011) represents phrases and sentences with fixed matrices consisting of pooled word and phrase pairwise similarities. (Le and Mikolov, 2014) learns representation of sentences directly by predicting context without composition of words. In our work, we judge that two sentences are paraphrases if they have high degree of semantic similarity. We use the UMBC-Semantic Textual Similarity system (Lushan Han et al., 2013), which provides high accurate semantic similarity measurement. The remainder of this paper is organized as follows. Section 2 describes the task and the details of our method. Section </context>
</contexts>
<marker>Socher, Huang, Pennington, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Eric H. Huang, Jeffrey Pennington, Andrew Y. Ng, and Christopher D. Manning. 2011. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In Advances in Neural Information Processing Systems (NIPS 2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanford</author>
</authors>
<date>2001</date>
<note>Stanford WebBase project. http://bit.ly/WebBase.</note>
<contexts>
<context position="5636" citStr="Stanford, 2001" startWordPosition="893" endWordPosition="894">tive.com and http://internetslang.com and others. 1.2. LSA Word Similarity Model Our LSA word similarity model is a revised version of the one we used in the 2013 and 2014 SemEval semantic text similarity tasks (Han, et al., 2013, Kashyap et al., 2014). LSA relies on the fact that semantically similar words (e.g., cat and feline or nurse and doctor) are more likely to occur near one another in text. Thus evidence for word similarity can be computed from a statistical analysis of a large text corpus. We extract raw word cooccurrence statistics from a portion of a 2007 Stanford WebBase dataset (Stanford, 2001). We performed part of speech tagging and lemmatization on the corpus using the Stanford POS tagger (Toutanova et al., 2000). Word/term cooccurrences were counted with a sliding window of fixed size over the entire corpus. We generate two co-occurrence models using window sizes ±1 and ±4. The smaller window provides more precise context which is better for comparing words of the same part of speech while the larger one is more suitable for computing the semantic similarity between words of different syntactic categories. Our word co-occurrence models are based on a predefined vocabulary of 22,</context>
</contexts>
<marker>Stanford, 2001</marker>
<rawString>Stanford. 2001. Stanford WebBase project. http://bit.ly/WebBase.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher Manning</author>
<author>William Morgan</author>
<author>Anna Rafferty</author>
<author>Michel Galley</author>
</authors>
<date>2000</date>
<note>Stanford log-linear part-of-speech tagger. http://nlp.stanford.edu/software/tagger.shtml.</note>
<contexts>
<context position="5760" citStr="Toutanova et al., 2000" startWordPosition="912" endWordPosition="915">revised version of the one we used in the 2013 and 2014 SemEval semantic text similarity tasks (Han, et al., 2013, Kashyap et al., 2014). LSA relies on the fact that semantically similar words (e.g., cat and feline or nurse and doctor) are more likely to occur near one another in text. Thus evidence for word similarity can be computed from a statistical analysis of a large text corpus. We extract raw word cooccurrence statistics from a portion of a 2007 Stanford WebBase dataset (Stanford, 2001). We performed part of speech tagging and lemmatization on the corpus using the Stanford POS tagger (Toutanova et al., 2000). Word/term cooccurrences were counted with a sliding window of fixed size over the entire corpus. We generate two co-occurrence models using window sizes ±1 and ±4. The smaller window provides more precise context which is better for comparing words of the same part of speech while the larger one is more suitable for computing the semantic similarity between words of different syntactic categories. Our word co-occurrence models are based on a predefined vocabulary of 22,000 common English open-class words and noun phrases, extended with about 2,000 verb phrases from WordNet. The final dimensi</context>
</contexts>
<marker>Toutanova, Klein, Manning, Morgan, Rafferty, Galley, 2000</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher Manning, William Morgan, Anna Rafferty, and Michel Galley. 2000. Stanford log-linear part-of-speech tagger. http://nlp.stanford.edu/software/tagger.shtml.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Xu</author>
<author>Chris Callison-Burch</author>
<author>William B Dolan</author>
</authors>
<date>2015</date>
<booktitle>SemEval-2015 Task 1: Paraphrase and Semantic Similarity in Twitter ,Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval),2015.</booktitle>
<marker>Xu, Callison-Burch, Dolan, 2015</marker>
<rawString>Wei Xu, Chris Callison-Burch and William B. Dolan. 2015. SemEval-2015 Task 1: Paraphrase and Semantic Similarity in Twitter ,Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval),2015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yitao Zhang</author>
<author>Jon Patrick</author>
</authors>
<title>Paraphrase identification by text canonicalization.</title>
<date>2005</date>
<booktitle>In Proceedings of the Australasian Language Technology Workshop</booktitle>
<pages>160--166</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="1870" citStr="Zhang and Patrick, 2005" startWordPosition="302" endWordPosition="305">love” and “A Walk to Remember is the cutest thing” could be judged as paraphrases with a score of 0.6. Many methods have been proposed to solve the paraphrase detection problem. Early approaches were often based on lexical matching techniques, e.g., word n-gram overlap (Barzilay and Lee, 51 2003) or predicate argument tuple matching (Qiu, et al., 2006). Some other approaches that go beyond simple lexical matching have also been developed. For example, (Mihalcea, et al., 2006) estimated semantic similarity of sentence pairs with word-to-word similarity measures and a word specificity measure. (Zhang and Patrick, 2005) uses text canonicalization to transfer texts of similar meaning into the same surface text with a higher probability than those with different meaning. Many of these approaches adopt distributional semantic models, but limited to a word level. To extend distributional semantic models beyond words, several researchers have learned phrase or sentence representation by composing the representation of individual words (Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010). An alternative approach by (Socher et al., 2011) represents phrases and sentences with fixed matrices consisting of pooled </context>
</contexts>
<marker>Zhang, Patrick, 2005</marker>
<rawString>Yitao Zhang and Jon Patrick. 2005. Paraphrase identification by text canonicalization. In Proceedings of the Australasian Language Technology Workshop 2005, pages 160–166, Sydney, Australia, December.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>