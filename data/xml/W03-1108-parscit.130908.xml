<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001108">
<title confidence="0.998816333333333">
Learning Bilingual Translations from Comparable Corpora to
Cross-Language Information Retrieval: Hybrid Statistics-based and
Linguistics-based Approach
</title>
<author confidence="0.989292">
Fatiha Sadat Masatoshi Yoshikawa Shunsuke Uemura
</author>
<affiliation confidence="0.991816">
Nagoya University
</affiliation>
<address confidence="0.6539235">
Furo-cho, Chikusa-ku,
Nagoya, 464-8601, Japan
</address>
<affiliation confidence="0.74279375">
Nara Institute of Science and Technology
8916-5 Takayama-cho, Ikoma-shi
Nara, 630-0101, Japan
Nara Institute of Science and Technology
</affiliation>
<address confidence="0.963888">
8916-5 Takayama-cho, Ikoma-shi,
Nara, 630-0101, Japan
</address>
<email confidence="0.999397">
{fatia-s, uemura}@is.aist-nara.ac.jp, yosikawa@itc.nagoya-u.ac.jp
</email>
<sectionHeader confidence="0.943303" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9845068">
Recent years saw an increased interest
in the use and the construction of large
corpora. With this increased interest
and awareness has come an expansion
in the application to knowledge acqui-
sition and bilingual terminology extrac-
tion. The present paper will seek to
present an approach to bilingual lexi-
con extraction from non-aligned compa-
rable corpora, combination to linguistics-
based pruning and evaluations on Cross-
Language Information Retrieval. We pro-
pose and explore a two-stages translation
model for the acquisition of bilingual ter-
minology from comparable corpora, dis-
ambiguation and selection of best transla-
tion alternatives on the basis of their mor-
phological knowledge. Evaluations using
a large-scale test collection on Japanese-
English and different weighting schemes
of SMART retrieval system confirmed the
effectiveness of the proposed combina-
tion of two-stages comparable corpora
and linguistics-based pruning on Cross-
Language Information Retrieval.
</bodyText>
<keyword confidence="0.959720333333333">
Keywords: Cross-Language Information
Retrieval, Comparable corpora, Transla-
tion, Disambiguation, Part-of-Speech.
</keyword>
<sectionHeader confidence="0.999702" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999254105263158">
Researches on corpus-based approaches to machine
translation (MT) have been on the rise, particularly
because of their promise to provide bilingual termi-
nology and enrich lexical resources such as bilingual
dictionaries and thesauri. These approaches gener-
ally rely on large text corpora, which play an impor-
tant role in Natural Language Processing (NLP) and
Information Retrieval (IR). Moreover, non-aligned
comparable corpora have been given a special in-
terest in bilingual terminology acquisition and lex-
ical resources enrichment (Dagan and Itai, 1994;
Dejean et al., 2002; Diab and Finch, 2000; Fung,
2000; Koehn and Knight, 2002; Nakagawa, 2000;
Peters and Picchi, 1995; Rapp, 1999; Shahzad and
al., 1999; Tanaka and Iwasaki, 1996).
Unlike parallel corpora, comparable corpora are
collections of texts from pairs or multiples of lan-
guages, which can be contrasted because of their
common features, in the topic, the domain, the au-
thors or the time period. This property made com-
parable corpora more abundant, less expensive and
more accessible through the World Wide Web.
In the present paper, we are concerned by exploit-
ing scarce resources for bilingual terminology ac-
quisition, then evaluations on Cross-Language In-
formation Retrieval (CLIR). CLIR consists of re-
trieving documents written in one language using
queries written in another language. An application
is conducted on NTCIR, a large-scale data collection
for (Japanese, English) language pair.
The remainder of the present paper is organized
as follows: Section 2 presents the proposed two-
stages approach for bilingual terminology acquisi-
tion from comparable corpora. Section 3 describes
the integration of linguistic knowledge for pruning
the translation candidates. Experiments and evalua-
tions in CLIR are discussed in Sections 4. Section 5
concludes the present paper.
</bodyText>
<sectionHeader confidence="0.9983945" genericHeader="method">
2 Two-stages Comparable Corpora-based
Approach
</sectionHeader>
<bodyText confidence="0.998872153846154">
Our proposed approach to bilingual terminology ac-
quisition from comparable corpora (Sadat et al.,
2003; Sadat et al., 2003) is based on the assump-
tion of similar collocation, i.e., If two words are mu-
tual translations, then their most frequent collocates
are likely to be mutual translations as well. More-
over, we apply this assumption in both directions of
the corpora, i.e., find translations of the source term
in the target language corpus but also translations
of the target terms in the source language corpus.
The proposed two-stages approach for the acquisi-
tion, disambiguation and selection of bilingual ter-
minology is described as follows:
</bodyText>
<listItem confidence="0.971511416666667">
• Bilingual terminology acquisition from source
language to target language to yield a first
translation model, represented by similarity
SIMST.
• Bilingual terminology acquisition from target
language to source language to yield a sec-
ond translation model, represented by similar-
ity SIMTS.
• Merge the first and second models to yield
a two-stages translation model, based on bi-
directional comparable corpora and repre-
sented by similarity SIMST.
</listItem>
<bodyText confidence="0.995835264705882">
We follow strategies of previous researches (De-
jean et al., 2002; Fung, 2000; Rapp, 1999) for
the first and second translation models and propose
a merging strategy for the two-stages translation
model (Sadat et al., 2003).
First, word frequencies, context word frequencies
in surrounding positions (here three-words window)
are computed following a statistics-based metrics,
the log-likelihood ratio (Dunning, 1993). Context
vectors for each source term and each target term
are constructed. Next, context vectors of the tar-
get words are translated using a preliminary bilin-
gual dictionary. We consider all translation candi-
dates, keeping the same context frequency value as
the source term. This step requires a seed lexicon, to
expand using the proposed bootstrapping approach
of this paper. Similarity vectors are constructed for
each pair of source term and target term using the
cosine metric (Salton and McGill, 1983).
Therefore, similarity vectors SIMST and
SIMTS for the first and second models are con-
structed and merged for a bi-directional acquisition
of bilingual terminology from source language to
target language. The merging process will keep
common pairs of source term and target transla-
tion (s,t) which appear in SIMST as pairs of (s,t)
but also in SIMTS as pairs of (t,s), to result in
combined similarity vectors SIMST for each pair
(s,t).The product of similarity values of both simi-
larity vectors SIMST for pairs (s,t) and SIMTS
for pairs (t,s) will result in similarity values in vec-
tors SIMST.
Therefore, similarity vectors of the two-stages
translation model are expressed as follows:
</bodyText>
<equation confidence="0.852037666666667">
SIMST = {(s, t, simST(t|s))  |(s, t, simST(t|s))
E SIMST A (t, s, simTS(s|t)) E SIMTS
A simST(t|s) = simST(t|s) X simTS(s|t)}
</equation>
<sectionHeader confidence="0.994636" genericHeader="method">
3 Linguistics-based Pruning
</sectionHeader>
<bodyText confidence="0.997394315789474">
Combining linguistic and statistical methods is be-
coming increasingly common in computational lin-
guistics, especially as more corpora become avail-
able (Klanvans and Tzoukermann, 1996; Sadat et
al., 2003). We propose to integrate linguistic con-
cepts into the corpora-based translation model. Mor-
phological knowledge such as Part-of-Speech (POS)
tags, context of terms, etc., could be valuable to filter
and prune the extracted translation candidates. The
objective of the linguistics-based pruning technique
is the detection of terms and their translations that
are morphologically close enough, i.e., close or sim-
ilar POS tags. This proposed approach will select a
fixed number of equivalents from the set of extracted
target translation alternatives that match the Part-of-
Speech of the source term.
Therefore, POS tags are assigned to each source
term (Japanese) via morphological analysis. As
well, a target language morphological analysis will
assign POS tags to the translation candidates. We
restricted the pruning technique to nouns, verbs, ad-
jectives and adverbs, although other POS tags could
be treated in a similar way. For Japanese-English&apos;
pair of languages, Japanese nouns ( ) are com-
pared to English nouns (NN) and Japanese verbs (
) to English verbs (VB). Japanese adverbs ( )
are compared to English adverbs (RB) and adjec-
tives (JJ); while, Japanese adjectives ( ) are
compared to English adverbs (RB) and adjectives
(JJ). This is because most adverbs in Japanese are
formed from adjectives. Thus. We select pairs or
source term and target translation (s,t) such as:
(ajia, kyougi, taikai, ajia, saidai, supoutsu, kyougi,
kai). The combined translation model is applied
on each source term of the associated list and
top-ranked word translation alternatives are selected
according to their highest similarities as follows:
(ajia):{(asia, 1.035), (assembly, 0.0611),
(city, 0.0589), (event, 0.0376), etc.}
(kyougi): {(competition, 0.057), (sport,
0.0561), (representative, 0.0337), (international,
0.0331), etc.}
(taikai): {(meeting, 0.176), (tournament,
0.0588), (assembly, 0.0582), (dialogue, 0.0437),
etc.}
’
, , , , , ,
Japanese foreign words (tagged FW) were consid-
ered as loanwords, i.e., technical terms and proper
nouns imported from foreign languages; and there-
fore were not pruned with the proposed linguistics-
based technique but could be treated via translitera-
tion.
The generated translation alternatives are sorted
in decreasing order by similarity values. Rank
counts are assigned in increasing order, starting at
1 for the first sorted list item. A fixed number of
top-ranked translation alternatives are selected and
misleading candidates are discarded.
In order to demonstrate the procedure of our
translation model, we give an example in Japanese
and explain how the English translations are ex-
tracted, disambiguated and selected and how the
phrasal translation is constructed.
Given a simple Japanese query ’
’ (ajia
kyougi taikai wa, ajia saidai no supoutsu kyougikai
de aru).
After segmentation, removing stop words
and keeping only content words (nouns, verbs,
adverbs, adjectives and foreign words), the asso-
ciated list of Japanese terms becomes ,
&apos;English POS tags NN refers to noun, VB to verb, RB to
adverb, JJ to adjective; while Japanese POS tags refers to
a noun, to a verb, to an adverb and to an
adjective, with respect to their extensions.
’ (saidai): {(general, 0.0459), (great, 0.0371),
(famous, 0.0362), (global, 0.0329), (group, 0.032),
(measure, 0.0271), (factor, 0.0268), etc.}
’ (supoutsu): {(sport, 1.098), (union,
0.0399), (day, 0.0392), (international, 0.0375), etc.}
’ (kai): {(taikai, 0.0489), (great, 0.0442), (meet-
ing, 0.0365), (gather, 0.0348), (person, 0.0312),
etc.}
The phrasal translation associated to the Japanese
query is formed by selecting a number of top-ranked
translation alternatives (here set to 3) and illustrated
as follows: ’asia assembly city competition sport
representative meeting tournament assembly gen-
eral great famous sport union day taikai great meet-
ing’.
Linguistics-based pruning was applied on the
Japanese terms and the extracted English translation
alternatives. Chasen morphological analyzer (Mat-
sumoto and al., 1997)for Japanese has associated
</bodyText>
<equation confidence="0.611729">
POS tags as (noun) to all Japanese terms:
(ajia)
</equation>
<bodyText confidence="0.998117857142857">
Therefore, English translation alternatives associ-
ated with POS tags as nouns (NN) via a morpholog-
ical analyzer for English (Sekine, 2001)are selected
and translation candidates having POS tags other
than NN (noun) are discarded. Selected translation
alternatives for the Japanese noun (saidai)
become ’group, measure, factor’. As well, the
</bodyText>
<figure confidence="0.9562975625">
POS(s) = ’JJ’ and [POS(t) = ’
’]
’ or ’
(saidai) -
(supoutsu) -
-
(kai)
(kyougi)
-
(taikai)
-
POS(s) = ’NN’ and POS(t) = ’ ’
POS(s) = ’VB’ and POS(t) = ’ ’
POS(s) = ’RB’ and [POS(t) = ’ ’ or ’
’]
-
</figure>
<bodyText confidence="0.948316454545455">
Japanese term ’ ’ (kai) is associated to the En-
glish translations: ’taikai, meeting, person’.
The phrasal translation associated to the Japanese
query after the linguistics-based pruning is illus-
trated as follows: ’asia assembly city competition
sport representative meeting tournament assembly
group measure factor sport union day taikai meet-
ing person’.
Possible re-scoring techniques could be applied
on phrasal translation in order to select best trans-
lation alternatives among the extracted ones.
</bodyText>
<sectionHeader confidence="0.998533" genericHeader="evaluation">
4 Experiments and Evaluations
</sectionHeader>
<bodyText confidence="0.9998944">
Experiments have been carried out to measure the
improvement of our proposal on bilingual termi-
nology acquisition from comparable corpora on
Japanese-English tasks in CLIR, i.e. Japanese
queries to retrieve English documents.
</bodyText>
<subsectionHeader confidence="0.991401">
4.1 Linguistic Resources
</subsectionHeader>
<bodyText confidence="0.999776272727273">
Collections of news articles from Mainichi Newspa-
pers (1998-1999) for Japanese and Mainichi Daily
News (1998-199) for English were considered as
comparable corpora, because of the common fea-
ture in the time period and the generalized domain.
We have also considered documents ofNTCIR-2 test
collection as comparable corpora in order to cope
with special features of the test collection during
evaluations.
Morphological analyzers, ChaSen version 2.2.9
(Matsumoto and al., 1997) for texts in Japanese and
OAK2 (Sekine, 2001) were used in the linguistic pre-
processing.
EDR bilingual dictionary (EDR, 1996) was used
to translate context vectors of source and target lan-
guages.
NTCIR-2 (Kando, 2001), a large-scale test collec-
tion was used to evaluate the proposed strategies in
CLIR.
SMART information retrieval system (Salton,
1971), which is based on vector space model, was
used to retrieve English documents.
</bodyText>
<subsectionHeader confidence="0.478244">
4.2 Evaluations on the Proposed Translation
Model
</subsectionHeader>
<bodyText confidence="0.997601543478261">
We considered the set of news articles as well as
the abstracts of NTCIR-2 test collection as compa-
rable corpora for Japanese-English language pairs.
The abstracts of NTCIR-2 test collection are par-
tially aligned (more than half are Japanese-English
paired documents) but the alignment was not con-
sidered in the present research to treat the set of
documents as comparable. Content words (nouns,
verbs, adjectives, adverbs) were extracted from En-
glish and Japanese corpora. In addition, foreign
words (mostly represented in katakana) were ex-
tracted from Japanese texts. Thus, context vectors
were constructed for 13,552,481 Japanese terms and
1,517,281 English terms. Similarity vectors were
constructed for 96,895,255 (Japanese, English) pairs
of terms and 92,765,129 (English, Japanese) pairs
of terms. Bi-directional similarity vectors (after
merging and disambiguation) resulted in 58,254,841
(Japanese, English) pairs of terms.
Table 1 illustrates some situations with the ex-
tracted English translation alternatives for Japanese
terms (eiga), using the two-stages compara-
ble corpora approach and combination to linguistics-
based pruning. Using the two-stages comparable
corpora-based approach, correct translations of the
Japanese term (eiga) were ranked in top 3
(movie) and top 5 (film). We notice that top
ranked translations, which are considered as wrong
translations, are related mostly to the context of the
source Japanese term and could help the query ex-
pansion in CLIR. Combined two-stages compara-
ble corpora with the linguistics-based pruning shows
better results with ranks 2 (movie) and 4 (film).
Japanese vocabulary is frequently imported from
other languages, primarily (but not exclusively)
from English. The special phonetic alphabet (here
Japanese katakana) is used to write down for-
eign words and loanwords, example names of per-
sons and others. Katakana terms could be treated
via transliteration or possible romanization, i.e.,
conversion of Japanese katakana to their English
equivalence or the alphabetical description of their
pronunciation. Transliteration is the phonetic or
spelling representation of one language using the
alphabet of another language (Knight and Graehl,
1998).
</bodyText>
<subsectionHeader confidence="0.950284">
4.3 Evaluations on SMART Weighting
Schemes
</subsectionHeader>
<bodyText confidence="0.739497">
Conducted experiments and evaluations were com-
pleted on NTCIR test collection using the monolin-
</bodyText>
<tableCaption confidence="0.998594">
Table 1: An example for the two-stages comparable corpora translation model and linguistics-based pruning
</tableCaption>
<figure confidence="0.980438611111111">
Two-stages Comparable Corpora Linguistics-based Pruning
English Similarity
Translation Value
famous 0.449
picture 0.361
movie 0.2163
oscar 0.1167
film 0.1116
English Similarity
Rank Translation Value Rank
1
2 picture 0.361 1
3 movie 0.2163 2
4 oscar 0.1167 3
5 film 0.1116 4
Japanese
Term
(eiga)
</figure>
<bodyText confidence="0.999513865384615">
gual English runs, i.e., English queries to retrieve
English documents and the bilingual Japanese-
English runs, i.e., Japanese queries to retrieve En-
glish document. Topics 0101 to 0149 were con-
sidered and key terms contained in the fields, title
&lt;TITLE&gt;, description &lt;DESCRIPTION&gt;
and concept &lt;CONCEPT&gt; were used to gener-
ate 49 queries in Japanese and English.
There is a variety of techniques implemented in
SMART to calculate weights for individual terms in
both documents and queries. These weighting tech-
niques are formulated by combining three parame-
ters: Term Frequency component, Inverted Docu-
ment Frequency component and Vector Normaliza-
tion component.
The standard SMART notation to describe the
combined schemes is ”XXX.YYY”. The three char-
acters to the left (XXX) and right (YYY) of the pe-
riod refer to the document and query vector compo-
nents, respectively. For example, ATC.ATN applies
augmented normalized term frequency, tf xidf doc-
ument frequency (term frequency times inverse doc-
ument frequency components) to weigh terms in the
collection of documents. Similarly ATN refers to the
weighting scheme applied to the query.
First experiments were conducted on several com-
binations of weighting parameters and schemes of
SMART retrieval system for documents terms and
query terms, such as ATN, ATC, LTN, LTC, NNN,
NTC, etc. Best performances in terms of aver-
age precision were realized by the following com-
bined weighting schemes: ATN.NTC, LTN.NTC,
LTC.NTC, ATC.NTC and NTC.NTC, respectively.
The best weighting scheme for the monolingual
runs turned out to be the ATN.NTC. This finding
is somewhat different from previous results where
ANN (Fox and Shaw, 1994), LTC (Fuhr and al.,
1994) weighting schemes on query terms, LNC.LTC
(Buckley and al., 1994) and LNC.LTN (Knaus and
Shauble, 1993) combined weighting schemes on
document terms and query terms showed the best re-
sults. On the other hand, our findings were quite
similar to the result presented by Savoy (Savoy,
2003), where the ATN.NTC showed the best per-
formance among the existing weighting schemes in
SMART for English monolingual runs.
Table 2 shows some weighting schemes of
SMART retrieval system, among others. To assign
an indexing weight wij that reflects the importance
of each single-term Tj in a document Di, differ-
ent factors should be considered (Salton and McGill,
1983), as follows:
</bodyText>
<listItem confidence="0.993910777777778">
• within-document term frequency tfij, which
represents the first letter of the SMART label.
• collection-wide term frequency dfj, which rep-
resents the second letter of the SMART label.
In Table 2, idfj = log FjN ; where, N represents
the number of documents and Fj represents the
document frequency of term Tj.
• normalization scheme, which represents the
third letter of the SMART label.
</listItem>
<subsectionHeader confidence="0.998878">
4.4 Evaluations on CLIR
</subsectionHeader>
<bodyText confidence="0.999910909090909">
Bilingual translations were extracted from compara-
ble corpora using the proposed two-stages model. A
fixed number (set to five) of top-ranked translation
alternatives was retained for evaluations in CLIR.
Results and performances on the monolingual and
bilingual runs for the proposed translation models
and the combination to linguistics-based pruning are
described in Table 3. Evaluations were based on
the average precision, differences in term of aver-
age precision of the monolingual counterpart and the
improvement over the monolingual counterpart. As
</bodyText>
<tableCaption confidence="0.981216">
Table 2: Weighting Schemes on SMART Retrieval System
</tableCaption>
<table confidence="0.998359192307692">
SMART Label Weighting Scheme
NNN wij = tfij
ATN tfij
idfj
wij = × [0.5 + 2×max tfi ]
LTN wij = idfj × [ln(tfij) + 1.0]
LTC idfj ×[ln(tfij )+0.1]
=
wij
[idfk×(ln(tfik )+0.1)]2
n
k=1
ATC tfij
×(0.5+ )
idfj
2×max tfi
=
wij
tfik
n
[idfk×(0.5+ )]2
k=1 2×max tfi
NTC idfj ×tfij
wij =
/ n
�V/ �k=1 [idfk ×tfik]2
</table>
<tableCaption confidence="0.9966335">
well, evaluations using R-precision are illustrated in
Table 3.
</tableCaption>
<bodyText confidence="0.932247592592593">
Figure 1 represents the recall/precision curves of
the proposed two-stages comparable corpora-based
translation model and combination to linguistics-
based pruning, in the case of ATN.NTC weighting
scheme.
The proposed two-stages model using compara-
ble corpora ’BCC’ showed a better improvement in
terms of average precision compared to the sim-
ple model ’SCC’ (one stage, i.e., simple compara-
ble corpora-based translation) with +27.1%. Com-
bination to Linguistics-based pruning showed the
best performance in terms of average precision with
+41.7% and +11.5% compared to the simple compa-
rable corpora-based model ’SCC’ and the two-stages
comparable corpora-based model ’BCC’, respec-
tively, in the case of ATN.NTC weighting scheme.
Different weighting schemes of SMART retrieval
system showed an improvement in term of average
precision for the proposed translation models ’BCC’
and ’BCC+Morph’.
The approach based on comparable corpora
largely affected the translation because related
words could be added as translation alternatives or
expansion terms. The acquisition of bilingual ter-
minology from bi-directional comparable corpora
yields a significantly better result than using the sim-
ple model. Moreover, the linguistics-based pruning
</bodyText>
<figureCaption confidence="0.885311666666667">
Figure 1: Recall/Precision curves for the proposed
translation models and combination to linguistics-
based pruning (weighting scheme = ATN.NTC)
</figureCaption>
<bodyText confidence="0.995521222222222">
technique has allowed an improvement in the effec-
tiveness of CLIR.
Finally, statistical t-test (Hull, 1993) was carried
out in order to measure significant differences be-
tween paired retrieval models. The improvement by
using the proposed two-stages comparable corpora-
based method ’BCC’ was statistically significant
(p-value=0.0011). The combined statistics-based
and linguistics-based pruning ’BCC+Morph’ was
</bodyText>
<figure confidence="0.998226625">
0 0.2 0.4 0.6 0.8 1
Recall
Precision
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
ME
SCC
BCC
BCC+Morph
</figure>
<tableCaption confidence="0.970061">
Table 3: Best results on different weighting schemes for the proposed translation models and the linguistics-
based pruning
</tableCaption>
<table confidence="0.99945545">
Weighting Average Precision, % Monolingual, and % Improvement R-Precision, % Monolingual, and % Improvement
Models
ME SCC BCC BCC+Morph ME SCC BCC BCC+Morph
(Monolingual (Simple Comp. (Two-stages Comp. (Linguistics- (Monolingual (Simple Comp. (Two-stages Comp. (Linguistics-
English) Corpora) Corpora) (based pruning) English) Corpora) Corpora) (based pruning)
0.2683 0.1417 0.1801 0.2008 0.2982 0.1652 0.2143 0.2391
ATN.NTC (100%) (52.81%) (67.12%) (74.84%) (100%) (55.34%) (71.86%) (80.18%)
(-47.18%) (-32.87%) (-25.16%) (-44.6%) (-28.13%) (-19.82%)
0.2236 0.091 0.1544 0.1729 0.2508 0.1339 0.1823 0.2066
LTN.NTC (100%) (40.69%) (69.05%) (77.32%) (100%) (53.39%) (72.69%) (82.37%)
(-59.3%) (-30.94%) (-22.67%) (-46.61%) (-27.31%) (-17.62%)
0.1703 0.0787 0.1138 0.1327 0.1943 0.0966 0.1396 0.1663
LTC.NTC (100%) (46.21%) (66.82%) (77.92%) (100%) (49.71%) (71.85%) (85.59%)
(-53.78%) (-33.17%) (-22.08%) (-50.28%) (-28.15%) (-14.41%)
0.1665 0.0707 0.1091 0.1252 0.2004 0.0923 0.1368 0.1481
ATC.NTC (100%) (42.46%) (65.52%) 75.19% (100%) (46.05%) (68.26%) (73.9%)
(-57.53%) (-34.47%) (-24.8%) (-53.94%) (-31.73%) (-26.1%)
0.1254 0.0575 0.073 0.0915 0.154 0.079 0.0989 0.1175
NTC.NTC (100%) (45.85%) (58.21%) (72.96%) (100%) (51.3%) (64.22%) (76.3%)
(-54.15%) (-41.78%) (-27.03%) (-48.7%) (-35.78%) (-23.7%)
</table>
<bodyText confidence="0.978656">
found statistically significant (p-value= 0.05) over
the monolingual retrieval ’ME’.
</bodyText>
<sectionHeader confidence="0.998142" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999966894736842">
Dictionary-based translation has been widely used
in CLIR because of its simplicity and availability.
However, failure to translate words and compounds
as well as limitations of general-purpose dictionaries
especially for specialized vocabulary are among the
reasons of drop in retrieval performance especially
when dealing with CLIR. Enriching bilingual dic-
tionaries and thesauri is possible through bilingual
terminology acquisition from large corpora. Parallel
corpora are costly to acquire and their availability is
extremely limited for any pair of languages or even
not existing for some languages, which are charac-
terized by few amounts of Web pages on the WWW.
In contrast, comparable corpora are more abundant,
more available in different domains, less expensive
and more accessible through the WWW.
In the present paper, we investigated the ap-
proach of extracting bilingual terminology from
comparable corpora in order to enrich existing bilin-
gual lexicons and thus enhance Cross-Language In-
formation Retrieval. We proposed a two-stages
translation model consisting of bi-directional ex-
traction, merging and disambiguation of the ex-
tracted bilingual terminology. A hybrid combination
to linguistics-based pruning showed its efficiency
across Japanese-English pair of languages. Most of
the selected terms could be considered as translation
candidates or expansion terms in CLIR.
Ongoing research is focused on the integration
of transliteration for the special phonetic alphabet.
Techniques on phrasal translation will be investi-
gated in order to select best phrasal translation al-
ternatives in CLIR. Evaluations using other combi-
nations and more efficient weighting schemes that
are not included in SMART retrieval system such
as OKAPI, which showed great success in informa-
tion retrieval, are among the future subjects of our
research on CLIR.
</bodyText>
<sectionHeader confidence="0.996513" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999771">
The present research is supported in part by the
Ministry of Education, Culture, Sports, Science and
Technology of Japan. Our thanks go to all reviewers
for their valuable comments on the earlier version of
this paper.
</bodyText>
<sectionHeader confidence="0.989972" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999808319148936">
C. Buckley, J. Allan and G. Salton. 1994. Automatic
Routing and Ad-hoc Retrieval using Smart. Proc. Sec-
ond Text Retrieval Conference TREC-2, pages 45–56,
I. Dagan and I. Itai. 1994. Word Sense Disambiguation
using a Second Language Monolingual Corpus. Com-
putational Linguistics, 20(4):563–596.
H. Dejean, E. Gaussier and F. Sadat. 2002. An Approach
based on Multilingual Thesauri and Model Combina-
tion for Bilingual Lexicon Extraction. In Proc. COL-
ING 2002.
M. Diab and S. Finch. 2000. A Statistical Word-level
Translation Model for Comparable Corpora. Proc. of
the Conference on Content-based Multimedia Infor-
mation Access RIAO.
T. Dunning. 1993. Accurate Methods for the Statistics
of Surprise and Coincidence. Computational linguis-
tics 19(1).
EDR. 1996. Japan Electronic Dictionary Research Insti-
tute, Ltd. EDR electronic dictionary version 1.5 EDR.
Technical guide. Technical report TR2-007.
A. E. Fox and A. J. Shaw. 1994. Combination of Multi-
ple Searches. Proc. Second Text Retrieval Conference
TREC-2, pages 243–252.
N. Fuhr, U. Pfeifer, C. Bremkamp, M. Pollmann and C.
Buckley. 1994. Probabilistic Learning Approaches
for Indexing and Retrieval with the TREC-2 Collec-
tion. Proc. Second Text Retrieval Conference TREC-2,
pages 67–74.
P. Fung. 2000. A Statistical View of Bilingual Lexi-
con Extraction: From Parallel Corpora to Non-Parallel
Corpora. In Jean Veronis, Ed. Parallel Text Process-
ing.
D. Hull. 1993. Using Statistical Testing in the Evalua-
tion of Retrieval Experiments. Proc. ACM SIGIR’93,
pages 329–338.
N. Kando. 2001. Overview of the Second NTCIR Work-
shop. In Proc. Second NTCIR Workshop on Research
in Chinese and Japanese Text Retrieval and Text Sum-
marization.
J. Klavans and E. Tzoukermann. 1996. Combining Cor-
pus and Machine-Readable Dictionary Data for Build-
ing Bilingual Lexicons. Machine Translation, 10(3-
4):1–34.
D. Knaus and P. Shauble. 1993. Effective and Efficient
Retrieval from Large and Dynamic Document Collec-
tions. Proc. Second Text Retrieval Conference TREC-
3, pages 163–170.
K. Knight and J. Graehl. 1998. Machine Transliteration.
Computational Linguistics, 24(4).
P. Koehn and K. Knight. 2002. Learning a Translation
Lexicon from Monolingual Corpora. In Proc. ACL-02
Workshop on Unsupervised Lexical Acquisition.
Y. Matsumoto, A. Kitauchi, T. Yamashita, O. Imaichi and
T. Imamura. 1997. Japanese Morphological Analysis
System ChaSen Manual. Technical Report NAIST-IS-
TR97007.
H. Nakagawa. 2000. Disambiguation of Lexical Trans-
lations based on Bilingual Comparable Corpora. Proc.
LREC2000, Workshop of Terminology Resources and
Computation WTRC2000, pages 33–38.
C. Peters and E. Picchi. 1995. Capturing the compa-
rable: A System for Querying Comparable Text Cor-
pora. Proc. 3rd International Conference on Statisti-
cal Analysis of Textual Data, pages 255–262.
R. Rapp. 1999. Automatic Identification of Word Trans-
lations from Unrelated English and German Corpora.
In Proc. European Association for Computational Lin-
guistics.
F. Sadat, M. Yoshikawa and S. Uemura. 2003. En-
hancing Cross-language Information Retrieval by an
Automatic Acquisition of Bilingual Terminology from
Comparable Corpora. In Proc. ACM SIGIR 2003,
Toronto, Canada.
F. Sadat, M. Yoshikawa and S. Uemura. 2003. Bilin-
gual Terminology Acquisition from Comparable Cor-
pora and Phrasal Translation to Cross-Language Infor-
mation Retrieval. In Proc. ACL 2003, Sapporo, Japan.
G. Salton. 1971. The SMART Retrieval System, Experi-
ments in Automatic Documents Processing. Prentice-
Hall, Inc., Englewood Cliffs, NJ.
G. Salton and J. McGill. 1983. Introduction to Modern
Information Retrieval. New York, Mc Graw-Hill.
J. Savoy. 2003. Cross-Language Information Retrieval:
Experiments based on CLEF 2000 Corpora. Informa-
tion Processing &amp; Management 39(1):75–115.
S. Sekine. 2001. OAK System-Manual. New York Uni-
versity.
I. Shahzad, K. Ohtake, S. Masuyama and K. Yamamoto.
1999. Identifying Translations of Compound using
Non-aligned Corpora. Proc. Workshop MAL, pages
108–113.
K. Tanaka and H. Iwasaki. 1996 Extraction of Lexical
Translations from Non-aligned Corpora. Proc. COL-
ING 96.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.215755">
<title confidence="0.998725333333333">Learning Bilingual Translations from Comparable Corpora Cross-Language Information Retrieval: Hybrid Statistics-based Linguistics-based Approach</title>
<author confidence="0.990644">Fatiha Sadat Masatoshi Yoshikawa Shunsuke Uemura</author>
<affiliation confidence="0.999997">Nagoya University</affiliation>
<address confidence="0.8694395">Furo-cho, Nagoya, 464-8601, Japan</address>
<affiliation confidence="0.995636">Nara Institute of Science and</affiliation>
<address confidence="0.9310625">8916-5 Takayama-cho, Nara, 630-0101, Japan</address>
<affiliation confidence="0.992584">Nara Institute of Science and</affiliation>
<address confidence="0.946909">8916-5 Takayama-cho, Nara, 630-0101, Japan</address>
<email confidence="0.982681">yosikawa@itc.nagoya-u.ac.jp</email>
<abstract confidence="0.981846461538462">Recent years saw an increased interest in the use and the construction of large corpora. With this increased interest and awareness has come an expansion in the application to knowledge acquisition and bilingual terminology extraction. The present paper will seek to present an approach to bilingual lexicon extraction from non-aligned comparable corpora, combination to linguisticsbased pruning and evaluations on Cross- Language Information Retrieval. We propose and explore a two-stages translation model for the acquisition of bilingual terminology from comparable corpora, disambiguation and selection of best translation alternatives on the basis of their morphological knowledge. Evaluations using a large-scale test collection on Japanese- English and different weighting schemes of SMART retrieval system confirmed the effectiveness of the proposed combination of two-stages comparable corpora and linguistics-based pruning on Cross- Language Information Retrieval.</abstract>
<keyword confidence="0.680148666666667">Information Retrieval, Comparable corpora, Translation, Disambiguation, Part-of-Speech.</keyword>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Buckley</author>
<author>J Allan</author>
<author>G Salton</author>
</authors>
<title>Automatic Routing and Ad-hoc Retrieval using Smart.</title>
<date>1994</date>
<booktitle>Proc. Second Text Retrieval Conference TREC-2,</booktitle>
<pages>45--56</pages>
<marker>Buckley, Allan, Salton, 1994</marker>
<rawString>C. Buckley, J. Allan and G. Salton. 1994. Automatic Routing and Ad-hoc Retrieval using Smart. Proc. Second Text Retrieval Conference TREC-2, pages 45–56,</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>I Itai</author>
</authors>
<title>Word Sense Disambiguation using a Second Language Monolingual Corpus.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>4</issue>
<contexts>
<context position="2180" citStr="Dagan and Itai, 1994" startWordPosition="286" endWordPosition="289">pora, Translation, Disambiguation, Part-of-Speech. 1 Introduction Researches on corpus-based approaches to machine translation (MT) have been on the rise, particularly because of their promise to provide bilingual terminology and enrich lexical resources such as bilingual dictionaries and thesauri. These approaches generally rely on large text corpora, which play an important role in Natural Language Processing (NLP) and Information Retrieval (IR). Moreover, non-aligned comparable corpora have been given a special interest in bilingual terminology acquisition and lexical resources enrichment (Dagan and Itai, 1994; Dejean et al., 2002; Diab and Finch, 2000; Fung, 2000; Koehn and Knight, 2002; Nakagawa, 2000; Peters and Picchi, 1995; Rapp, 1999; Shahzad and al., 1999; Tanaka and Iwasaki, 1996). Unlike parallel corpora, comparable corpora are collections of texts from pairs or multiples of languages, which can be contrasted because of their common features, in the topic, the domain, the authors or the time period. This property made comparable corpora more abundant, less expensive and more accessible through the World Wide Web. In the present paper, we are concerned by exploiting scarce resources for bil</context>
</contexts>
<marker>Dagan, Itai, 1994</marker>
<rawString>I. Dagan and I. Itai. 1994. Word Sense Disambiguation using a Second Language Monolingual Corpus. Computational Linguistics, 20(4):563–596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Dejean</author>
<author>E Gaussier</author>
<author>F Sadat</author>
</authors>
<title>An Approach based on Multilingual Thesauri and Model Combination for Bilingual Lexicon Extraction.</title>
<date>2002</date>
<booktitle>In Proc. COLING</booktitle>
<contexts>
<context position="2201" citStr="Dejean et al., 2002" startWordPosition="290" endWordPosition="293">ambiguation, Part-of-Speech. 1 Introduction Researches on corpus-based approaches to machine translation (MT) have been on the rise, particularly because of their promise to provide bilingual terminology and enrich lexical resources such as bilingual dictionaries and thesauri. These approaches generally rely on large text corpora, which play an important role in Natural Language Processing (NLP) and Information Retrieval (IR). Moreover, non-aligned comparable corpora have been given a special interest in bilingual terminology acquisition and lexical resources enrichment (Dagan and Itai, 1994; Dejean et al., 2002; Diab and Finch, 2000; Fung, 2000; Koehn and Knight, 2002; Nakagawa, 2000; Peters and Picchi, 1995; Rapp, 1999; Shahzad and al., 1999; Tanaka and Iwasaki, 1996). Unlike parallel corpora, comparable corpora are collections of texts from pairs or multiples of languages, which can be contrasted because of their common features, in the topic, the domain, the authors or the time period. This property made comparable corpora more abundant, less expensive and more accessible through the World Wide Web. In the present paper, we are concerned by exploiting scarce resources for bilingual terminology ac</context>
<context position="4678" citStr="Dejean et al., 2002" startWordPosition="669" endWordPosition="673">oach for the acquisition, disambiguation and selection of bilingual terminology is described as follows: • Bilingual terminology acquisition from source language to target language to yield a first translation model, represented by similarity SIMST. • Bilingual terminology acquisition from target language to source language to yield a second translation model, represented by similarity SIMTS. • Merge the first and second models to yield a two-stages translation model, based on bidirectional comparable corpora and represented by similarity SIMST. We follow strategies of previous researches (Dejean et al., 2002; Fung, 2000; Rapp, 1999) for the first and second translation models and propose a merging strategy for the two-stages translation model (Sadat et al., 2003). First, word frequencies, context word frequencies in surrounding positions (here three-words window) are computed following a statistics-based metrics, the log-likelihood ratio (Dunning, 1993). Context vectors for each source term and each target term are constructed. Next, context vectors of the target words are translated using a preliminary bilingual dictionary. We consider all translation candidates, keeping the same context frequen</context>
</contexts>
<marker>Dejean, Gaussier, Sadat, 2002</marker>
<rawString>H. Dejean, E. Gaussier and F. Sadat. 2002. An Approach based on Multilingual Thesauri and Model Combination for Bilingual Lexicon Extraction. In Proc. COLING 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Diab</author>
<author>S Finch</author>
</authors>
<title>A Statistical Word-level Translation Model for Comparable Corpora.</title>
<date>2000</date>
<booktitle>Proc. of the Conference on Content-based Multimedia Information Access RIAO.</booktitle>
<contexts>
<context position="2223" citStr="Diab and Finch, 2000" startWordPosition="294" endWordPosition="297">Speech. 1 Introduction Researches on corpus-based approaches to machine translation (MT) have been on the rise, particularly because of their promise to provide bilingual terminology and enrich lexical resources such as bilingual dictionaries and thesauri. These approaches generally rely on large text corpora, which play an important role in Natural Language Processing (NLP) and Information Retrieval (IR). Moreover, non-aligned comparable corpora have been given a special interest in bilingual terminology acquisition and lexical resources enrichment (Dagan and Itai, 1994; Dejean et al., 2002; Diab and Finch, 2000; Fung, 2000; Koehn and Knight, 2002; Nakagawa, 2000; Peters and Picchi, 1995; Rapp, 1999; Shahzad and al., 1999; Tanaka and Iwasaki, 1996). Unlike parallel corpora, comparable corpora are collections of texts from pairs or multiples of languages, which can be contrasted because of their common features, in the topic, the domain, the authors or the time period. This property made comparable corpora more abundant, less expensive and more accessible through the World Wide Web. In the present paper, we are concerned by exploiting scarce resources for bilingual terminology acquisition, then evalua</context>
</contexts>
<marker>Diab, Finch, 2000</marker>
<rawString>M. Diab and S. Finch. 2000. A Statistical Word-level Translation Model for Comparable Corpora. Proc. of the Conference on Content-based Multimedia Information Access RIAO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Dunning</author>
</authors>
<title>Accurate Methods for the Statistics of Surprise and Coincidence. Computational linguistics 19(1).</title>
<date>1993</date>
<contexts>
<context position="5030" citStr="Dunning, 1993" startWordPosition="720" endWordPosition="721">model, represented by similarity SIMTS. • Merge the first and second models to yield a two-stages translation model, based on bidirectional comparable corpora and represented by similarity SIMST. We follow strategies of previous researches (Dejean et al., 2002; Fung, 2000; Rapp, 1999) for the first and second translation models and propose a merging strategy for the two-stages translation model (Sadat et al., 2003). First, word frequencies, context word frequencies in surrounding positions (here three-words window) are computed following a statistics-based metrics, the log-likelihood ratio (Dunning, 1993). Context vectors for each source term and each target term are constructed. Next, context vectors of the target words are translated using a preliminary bilingual dictionary. We consider all translation candidates, keeping the same context frequency value as the source term. This step requires a seed lexicon, to expand using the proposed bootstrapping approach of this paper. Similarity vectors are constructed for each pair of source term and target term using the cosine metric (Salton and McGill, 1983). Therefore, similarity vectors SIMST and SIMTS for the first and second models are constr</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>T. Dunning. 1993. Accurate Methods for the Statistics of Surprise and Coincidence. Computational linguistics 19(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>EDR</author>
</authors>
<title>Japan Electronic Dictionary Research Institute, Ltd. EDR electronic dictionary version 1.5 EDR. Technical guide.</title>
<date>1996</date>
<tech>Technical report TR2-007.</tech>
<contexts>
<context position="12609" citStr="EDR, 1996" startWordPosition="1875" endWordPosition="1876">ources Collections of news articles from Mainichi Newspapers (1998-1999) for Japanese and Mainichi Daily News (1998-199) for English were considered as comparable corpora, because of the common feature in the time period and the generalized domain. We have also considered documents ofNTCIR-2 test collection as comparable corpora in order to cope with special features of the test collection during evaluations. Morphological analyzers, ChaSen version 2.2.9 (Matsumoto and al., 1997) for texts in Japanese and OAK2 (Sekine, 2001) were used in the linguistic preprocessing. EDR bilingual dictionary (EDR, 1996) was used to translate context vectors of source and target languages. NTCIR-2 (Kando, 2001), a large-scale test collection was used to evaluate the proposed strategies in CLIR. SMART information retrieval system (Salton, 1971), which is based on vector space model, was used to retrieve English documents. 4.2 Evaluations on the Proposed Translation Model We considered the set of news articles as well as the abstracts of NTCIR-2 test collection as comparable corpora for Japanese-English language pairs. The abstracts of NTCIR-2 test collection are partially aligned (more than half are Japanese-E</context>
</contexts>
<marker>EDR, 1996</marker>
<rawString>EDR. 1996. Japan Electronic Dictionary Research Institute, Ltd. EDR electronic dictionary version 1.5 EDR. Technical guide. Technical report TR2-007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A E Fox</author>
<author>A J Shaw</author>
</authors>
<title>Combination of Multiple Searches.</title>
<date>1994</date>
<booktitle>Proc. Second Text Retrieval Conference TREC-2,</booktitle>
<pages>243--252</pages>
<contexts>
<context position="17389" citStr="Fox and Shaw, 1994" startWordPosition="2588" endWordPosition="2591">n of documents. Similarly ATN refers to the weighting scheme applied to the query. First experiments were conducted on several combinations of weighting parameters and schemes of SMART retrieval system for documents terms and query terms, such as ATN, ATC, LTN, LTC, NNN, NTC, etc. Best performances in terms of average precision were realized by the following combined weighting schemes: ATN.NTC, LTN.NTC, LTC.NTC, ATC.NTC and NTC.NTC, respectively. The best weighting scheme for the monolingual runs turned out to be the ATN.NTC. This finding is somewhat different from previous results where ANN (Fox and Shaw, 1994), LTC (Fuhr and al., 1994) weighting schemes on query terms, LNC.LTC (Buckley and al., 1994) and LNC.LTN (Knaus and Shauble, 1993) combined weighting schemes on document terms and query terms showed the best results. On the other hand, our findings were quite similar to the result presented by Savoy (Savoy, 2003), where the ATN.NTC showed the best performance among the existing weighting schemes in SMART for English monolingual runs. Table 2 shows some weighting schemes of SMART retrieval system, among others. To assign an indexing weight wij that reflects the importance of each single-term Tj</context>
</contexts>
<marker>Fox, Shaw, 1994</marker>
<rawString>A. E. Fox and A. J. Shaw. 1994. Combination of Multiple Searches. Proc. Second Text Retrieval Conference TREC-2, pages 243–252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Fuhr</author>
<author>U Pfeifer</author>
<author>C Bremkamp</author>
<author>M Pollmann</author>
<author>C Buckley</author>
</authors>
<date>1994</date>
<booktitle>Probabilistic Learning Approaches for Indexing and Retrieval with the TREC-2 Collection. Proc. Second Text Retrieval Conference TREC-2,</booktitle>
<pages>67--74</pages>
<marker>Fuhr, Pfeifer, Bremkamp, Pollmann, Buckley, 1994</marker>
<rawString>N. Fuhr, U. Pfeifer, C. Bremkamp, M. Pollmann and C. Buckley. 1994. Probabilistic Learning Approaches for Indexing and Retrieval with the TREC-2 Collection. Proc. Second Text Retrieval Conference TREC-2, pages 67–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Fung</author>
</authors>
<title>A Statistical View of Bilingual Lexicon Extraction: From Parallel Corpora to Non-Parallel Corpora.</title>
<date>2000</date>
<booktitle>In Jean Veronis, Ed. Parallel Text Processing.</booktitle>
<contexts>
<context position="2235" citStr="Fung, 2000" startWordPosition="298" endWordPosition="299"> Researches on corpus-based approaches to machine translation (MT) have been on the rise, particularly because of their promise to provide bilingual terminology and enrich lexical resources such as bilingual dictionaries and thesauri. These approaches generally rely on large text corpora, which play an important role in Natural Language Processing (NLP) and Information Retrieval (IR). Moreover, non-aligned comparable corpora have been given a special interest in bilingual terminology acquisition and lexical resources enrichment (Dagan and Itai, 1994; Dejean et al., 2002; Diab and Finch, 2000; Fung, 2000; Koehn and Knight, 2002; Nakagawa, 2000; Peters and Picchi, 1995; Rapp, 1999; Shahzad and al., 1999; Tanaka and Iwasaki, 1996). Unlike parallel corpora, comparable corpora are collections of texts from pairs or multiples of languages, which can be contrasted because of their common features, in the topic, the domain, the authors or the time period. This property made comparable corpora more abundant, less expensive and more accessible through the World Wide Web. In the present paper, we are concerned by exploiting scarce resources for bilingual terminology acquisition, then evaluations on Cro</context>
<context position="4690" citStr="Fung, 2000" startWordPosition="674" endWordPosition="675">ion, disambiguation and selection of bilingual terminology is described as follows: • Bilingual terminology acquisition from source language to target language to yield a first translation model, represented by similarity SIMST. • Bilingual terminology acquisition from target language to source language to yield a second translation model, represented by similarity SIMTS. • Merge the first and second models to yield a two-stages translation model, based on bidirectional comparable corpora and represented by similarity SIMST. We follow strategies of previous researches (Dejean et al., 2002; Fung, 2000; Rapp, 1999) for the first and second translation models and propose a merging strategy for the two-stages translation model (Sadat et al., 2003). First, word frequencies, context word frequencies in surrounding positions (here three-words window) are computed following a statistics-based metrics, the log-likelihood ratio (Dunning, 1993). Context vectors for each source term and each target term are constructed. Next, context vectors of the target words are translated using a preliminary bilingual dictionary. We consider all translation candidates, keeping the same context frequency value as </context>
</contexts>
<marker>Fung, 2000</marker>
<rawString>P. Fung. 2000. A Statistical View of Bilingual Lexicon Extraction: From Parallel Corpora to Non-Parallel Corpora. In Jean Veronis, Ed. Parallel Text Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hull</author>
</authors>
<title>Using Statistical Testing in the Evaluation of Retrieval Experiments.</title>
<date>1993</date>
<booktitle>Proc. ACM SIGIR’93,</booktitle>
<pages>329--338</pages>
<contexts>
<context position="20970" citStr="Hull, 1993" startWordPosition="3143" endWordPosition="3144">nd ’BCC+Morph’. The approach based on comparable corpora largely affected the translation because related words could be added as translation alternatives or expansion terms. The acquisition of bilingual terminology from bi-directional comparable corpora yields a significantly better result than using the simple model. Moreover, the linguistics-based pruning Figure 1: Recall/Precision curves for the proposed translation models and combination to linguisticsbased pruning (weighting scheme = ATN.NTC) technique has allowed an improvement in the effectiveness of CLIR. Finally, statistical t-test (Hull, 1993) was carried out in order to measure significant differences between paired retrieval models. The improvement by using the proposed two-stages comparable corporabased method ’BCC’ was statistically significant (p-value=0.0011). The combined statistics-based and linguistics-based pruning ’BCC+Morph’ was 0 0.2 0.4 0.6 0.8 1 Recall Precision 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 ME SCC BCC BCC+Morph Table 3: Best results on different weighting schemes for the proposed translation models and the linguisticsbased pruning Weighting Average Precision, % Monolingual, and % Improvement R-Precision, % Monol</context>
</contexts>
<marker>Hull, 1993</marker>
<rawString>D. Hull. 1993. Using Statistical Testing in the Evaluation of Retrieval Experiments. Proc. ACM SIGIR’93, pages 329–338.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Kando</author>
</authors>
<title>Overview of the Second NTCIR Workshop.</title>
<date>2001</date>
<booktitle>In Proc. Second NTCIR Workshop on Research in Chinese and Japanese Text Retrieval and Text Summarization.</booktitle>
<contexts>
<context position="12701" citStr="Kando, 2001" startWordPosition="1890" endWordPosition="1891">Mainichi Daily News (1998-199) for English were considered as comparable corpora, because of the common feature in the time period and the generalized domain. We have also considered documents ofNTCIR-2 test collection as comparable corpora in order to cope with special features of the test collection during evaluations. Morphological analyzers, ChaSen version 2.2.9 (Matsumoto and al., 1997) for texts in Japanese and OAK2 (Sekine, 2001) were used in the linguistic preprocessing. EDR bilingual dictionary (EDR, 1996) was used to translate context vectors of source and target languages. NTCIR-2 (Kando, 2001), a large-scale test collection was used to evaluate the proposed strategies in CLIR. SMART information retrieval system (Salton, 1971), which is based on vector space model, was used to retrieve English documents. 4.2 Evaluations on the Proposed Translation Model We considered the set of news articles as well as the abstracts of NTCIR-2 test collection as comparable corpora for Japanese-English language pairs. The abstracts of NTCIR-2 test collection are partially aligned (more than half are Japanese-English paired documents) but the alignment was not considered in the present research to tre</context>
</contexts>
<marker>Kando, 2001</marker>
<rawString>N. Kando. 2001. Overview of the Second NTCIR Workshop. In Proc. Second NTCIR Workshop on Research in Chinese and Japanese Text Retrieval and Text Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Klavans</author>
<author>E Tzoukermann</author>
</authors>
<title>Combining Corpus and Machine-Readable Dictionary Data for Building Bilingual Lexicons.</title>
<date>1996</date>
<journal>Machine Translation,</journal>
<pages>10--3</pages>
<marker>Klavans, Tzoukermann, 1996</marker>
<rawString>J. Klavans and E. Tzoukermann. 1996. Combining Corpus and Machine-Readable Dictionary Data for Building Bilingual Lexicons. Machine Translation, 10(3-4):1–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Knaus</author>
<author>P Shauble</author>
</authors>
<title>Effective and Efficient Retrieval from Large and Dynamic Document Collections.</title>
<date>1993</date>
<booktitle>Proc. Second Text Retrieval Conference TREC3,</booktitle>
<pages>163--170</pages>
<contexts>
<context position="17519" citStr="Knaus and Shauble, 1993" startWordPosition="2609" endWordPosition="2612">l combinations of weighting parameters and schemes of SMART retrieval system for documents terms and query terms, such as ATN, ATC, LTN, LTC, NNN, NTC, etc. Best performances in terms of average precision were realized by the following combined weighting schemes: ATN.NTC, LTN.NTC, LTC.NTC, ATC.NTC and NTC.NTC, respectively. The best weighting scheme for the monolingual runs turned out to be the ATN.NTC. This finding is somewhat different from previous results where ANN (Fox and Shaw, 1994), LTC (Fuhr and al., 1994) weighting schemes on query terms, LNC.LTC (Buckley and al., 1994) and LNC.LTN (Knaus and Shauble, 1993) combined weighting schemes on document terms and query terms showed the best results. On the other hand, our findings were quite similar to the result presented by Savoy (Savoy, 2003), where the ATN.NTC showed the best performance among the existing weighting schemes in SMART for English monolingual runs. Table 2 shows some weighting schemes of SMART retrieval system, among others. To assign an indexing weight wij that reflects the importance of each single-term Tj in a document Di, different factors should be considered (Salton and McGill, 1983), as follows: • within-document term frequency </context>
</contexts>
<marker>Knaus, Shauble, 1993</marker>
<rawString>D. Knaus and P. Shauble. 1993. Effective and Efficient Retrieval from Large and Dynamic Document Collections. Proc. Second Text Retrieval Conference TREC3, pages 163–170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>J Graehl</author>
</authors>
<date>1998</date>
<journal>Machine Transliteration. Computational Linguistics,</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="15174" citStr="Knight and Graehl, 1998" startWordPosition="2249" endWordPosition="2252">ranks 2 (movie) and 4 (film). Japanese vocabulary is frequently imported from other languages, primarily (but not exclusively) from English. The special phonetic alphabet (here Japanese katakana) is used to write down foreign words and loanwords, example names of persons and others. Katakana terms could be treated via transliteration or possible romanization, i.e., conversion of Japanese katakana to their English equivalence or the alphabetical description of their pronunciation. Transliteration is the phonetic or spelling representation of one language using the alphabet of another language (Knight and Graehl, 1998). 4.3 Evaluations on SMART Weighting Schemes Conducted experiments and evaluations were completed on NTCIR test collection using the monolinTable 1: An example for the two-stages comparable corpora translation model and linguistics-based pruning Two-stages Comparable Corpora Linguistics-based Pruning English Similarity Translation Value famous 0.449 picture 0.361 movie 0.2163 oscar 0.1167 film 0.1116 English Similarity Rank Translation Value Rank 1 2 picture 0.361 1 3 movie 0.2163 2 4 oscar 0.1167 3 5 film 0.1116 4 Japanese Term (eiga) gual English runs, i.e., English queries to retrieve Engli</context>
</contexts>
<marker>Knight, Graehl, 1998</marker>
<rawString>K. Knight and J. Graehl. 1998. Machine Transliteration. Computational Linguistics, 24(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>K Knight</author>
</authors>
<title>Learning a Translation Lexicon from Monolingual Corpora.</title>
<date>2002</date>
<booktitle>In Proc. ACL-02 Workshop on Unsupervised Lexical</booktitle>
<tech>Technical Report NAIST-ISTR97007.</tech>
<contexts>
<context position="2259" citStr="Koehn and Knight, 2002" startWordPosition="300" endWordPosition="303">on corpus-based approaches to machine translation (MT) have been on the rise, particularly because of their promise to provide bilingual terminology and enrich lexical resources such as bilingual dictionaries and thesauri. These approaches generally rely on large text corpora, which play an important role in Natural Language Processing (NLP) and Information Retrieval (IR). Moreover, non-aligned comparable corpora have been given a special interest in bilingual terminology acquisition and lexical resources enrichment (Dagan and Itai, 1994; Dejean et al., 2002; Diab and Finch, 2000; Fung, 2000; Koehn and Knight, 2002; Nakagawa, 2000; Peters and Picchi, 1995; Rapp, 1999; Shahzad and al., 1999; Tanaka and Iwasaki, 1996). Unlike parallel corpora, comparable corpora are collections of texts from pairs or multiples of languages, which can be contrasted because of their common features, in the topic, the domain, the authors or the time period. This property made comparable corpora more abundant, less expensive and more accessible through the World Wide Web. In the present paper, we are concerned by exploiting scarce resources for bilingual terminology acquisition, then evaluations on Cross-Language Information </context>
</contexts>
<marker>Koehn, Knight, 2002</marker>
<rawString>P. Koehn and K. Knight. 2002. Learning a Translation Lexicon from Monolingual Corpora. In Proc. ACL-02 Workshop on Unsupervised Lexical Acquisition. Y. Matsumoto, A. Kitauchi, T. Yamashita, O. Imaichi and T. Imamura. 1997. Japanese Morphological Analysis System ChaSen Manual. Technical Report NAIST-ISTR97007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Nakagawa</author>
</authors>
<title>Disambiguation of Lexical Translations based on Bilingual Comparable Corpora.</title>
<date>2000</date>
<booktitle>Proc. LREC2000, Workshop of Terminology Resources and Computation WTRC2000,</booktitle>
<pages>33--38</pages>
<contexts>
<context position="2275" citStr="Nakagawa, 2000" startWordPosition="304" endWordPosition="305">es to machine translation (MT) have been on the rise, particularly because of their promise to provide bilingual terminology and enrich lexical resources such as bilingual dictionaries and thesauri. These approaches generally rely on large text corpora, which play an important role in Natural Language Processing (NLP) and Information Retrieval (IR). Moreover, non-aligned comparable corpora have been given a special interest in bilingual terminology acquisition and lexical resources enrichment (Dagan and Itai, 1994; Dejean et al., 2002; Diab and Finch, 2000; Fung, 2000; Koehn and Knight, 2002; Nakagawa, 2000; Peters and Picchi, 1995; Rapp, 1999; Shahzad and al., 1999; Tanaka and Iwasaki, 1996). Unlike parallel corpora, comparable corpora are collections of texts from pairs or multiples of languages, which can be contrasted because of their common features, in the topic, the domain, the authors or the time period. This property made comparable corpora more abundant, less expensive and more accessible through the World Wide Web. In the present paper, we are concerned by exploiting scarce resources for bilingual terminology acquisition, then evaluations on Cross-Language Information Retrieval (CLIR)</context>
</contexts>
<marker>Nakagawa, 2000</marker>
<rawString>H. Nakagawa. 2000. Disambiguation of Lexical Translations based on Bilingual Comparable Corpora. Proc. LREC2000, Workshop of Terminology Resources and Computation WTRC2000, pages 33–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Peters</author>
<author>E Picchi</author>
</authors>
<title>Capturing the comparable: A System for Querying Comparable Text Corpora.</title>
<date>1995</date>
<booktitle>Proc. 3rd International Conference on Statistical Analysis of Textual Data,</booktitle>
<pages>255--262</pages>
<contexts>
<context position="2300" citStr="Peters and Picchi, 1995" startWordPosition="306" endWordPosition="309">anslation (MT) have been on the rise, particularly because of their promise to provide bilingual terminology and enrich lexical resources such as bilingual dictionaries and thesauri. These approaches generally rely on large text corpora, which play an important role in Natural Language Processing (NLP) and Information Retrieval (IR). Moreover, non-aligned comparable corpora have been given a special interest in bilingual terminology acquisition and lexical resources enrichment (Dagan and Itai, 1994; Dejean et al., 2002; Diab and Finch, 2000; Fung, 2000; Koehn and Knight, 2002; Nakagawa, 2000; Peters and Picchi, 1995; Rapp, 1999; Shahzad and al., 1999; Tanaka and Iwasaki, 1996). Unlike parallel corpora, comparable corpora are collections of texts from pairs or multiples of languages, which can be contrasted because of their common features, in the topic, the domain, the authors or the time period. This property made comparable corpora more abundant, less expensive and more accessible through the World Wide Web. In the present paper, we are concerned by exploiting scarce resources for bilingual terminology acquisition, then evaluations on Cross-Language Information Retrieval (CLIR). CLIR consists of retrie</context>
</contexts>
<marker>Peters, Picchi, 1995</marker>
<rawString>C. Peters and E. Picchi. 1995. Capturing the comparable: A System for Querying Comparable Text Corpora. Proc. 3rd International Conference on Statistical Analysis of Textual Data, pages 255–262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rapp</author>
</authors>
<title>Automatic Identification of Word Translations from Unrelated English and German Corpora. In</title>
<date>1999</date>
<booktitle>Proc. European Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2312" citStr="Rapp, 1999" startWordPosition="310" endWordPosition="311">on the rise, particularly because of their promise to provide bilingual terminology and enrich lexical resources such as bilingual dictionaries and thesauri. These approaches generally rely on large text corpora, which play an important role in Natural Language Processing (NLP) and Information Retrieval (IR). Moreover, non-aligned comparable corpora have been given a special interest in bilingual terminology acquisition and lexical resources enrichment (Dagan and Itai, 1994; Dejean et al., 2002; Diab and Finch, 2000; Fung, 2000; Koehn and Knight, 2002; Nakagawa, 2000; Peters and Picchi, 1995; Rapp, 1999; Shahzad and al., 1999; Tanaka and Iwasaki, 1996). Unlike parallel corpora, comparable corpora are collections of texts from pairs or multiples of languages, which can be contrasted because of their common features, in the topic, the domain, the authors or the time period. This property made comparable corpora more abundant, less expensive and more accessible through the World Wide Web. In the present paper, we are concerned by exploiting scarce resources for bilingual terminology acquisition, then evaluations on Cross-Language Information Retrieval (CLIR). CLIR consists of retrieving documen</context>
<context position="4703" citStr="Rapp, 1999" startWordPosition="676" endWordPosition="677">guation and selection of bilingual terminology is described as follows: • Bilingual terminology acquisition from source language to target language to yield a first translation model, represented by similarity SIMST. • Bilingual terminology acquisition from target language to source language to yield a second translation model, represented by similarity SIMTS. • Merge the first and second models to yield a two-stages translation model, based on bidirectional comparable corpora and represented by similarity SIMST. We follow strategies of previous researches (Dejean et al., 2002; Fung, 2000; Rapp, 1999) for the first and second translation models and propose a merging strategy for the two-stages translation model (Sadat et al., 2003). First, word frequencies, context word frequencies in surrounding positions (here three-words window) are computed following a statistics-based metrics, the log-likelihood ratio (Dunning, 1993). Context vectors for each source term and each target term are constructed. Next, context vectors of the target words are translated using a preliminary bilingual dictionary. We consider all translation candidates, keeping the same context frequency value as the source te</context>
</contexts>
<marker>Rapp, 1999</marker>
<rawString>R. Rapp. 1999. Automatic Identification of Word Translations from Unrelated English and German Corpora. In Proc. European Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sadat</author>
<author>M Yoshikawa</author>
<author>S Uemura</author>
</authors>
<title>Enhancing Cross-language Information Retrieval by an Automatic Acquisition of Bilingual Terminology from Comparable Corpora. In</title>
<date>2003</date>
<booktitle>Proc. ACM SIGIR</booktitle>
<location>Toronto, Canada.</location>
<contexts>
<context position="3616" citStr="Sadat et al., 2003" startWordPosition="503" endWordPosition="506">s conducted on NTCIR, a large-scale data collection for (Japanese, English) language pair. The remainder of the present paper is organized as follows: Section 2 presents the proposed twostages approach for bilingual terminology acquisition from comparable corpora. Section 3 describes the integration of linguistic knowledge for pruning the translation candidates. Experiments and evaluations in CLIR are discussed in Sections 4. Section 5 concludes the present paper. 2 Two-stages Comparable Corpora-based Approach Our proposed approach to bilingual terminology acquisition from comparable corpora (Sadat et al., 2003; Sadat et al., 2003) is based on the assumption of similar collocation, i.e., If two words are mutual translations, then their most frequent collocates are likely to be mutual translations as well. Moreover, we apply this assumption in both directions of the corpora, i.e., find translations of the source term in the target language corpus but also translations of the target terms in the source language corpus. The proposed two-stages approach for the acquisition, disambiguation and selection of bilingual terminology is described as follows: • Bilingual terminology acquisition from source lang</context>
<context position="4836" citStr="Sadat et al., 2003" startWordPosition="695" endWordPosition="698">age to target language to yield a first translation model, represented by similarity SIMST. • Bilingual terminology acquisition from target language to source language to yield a second translation model, represented by similarity SIMTS. • Merge the first and second models to yield a two-stages translation model, based on bidirectional comparable corpora and represented by similarity SIMST. We follow strategies of previous researches (Dejean et al., 2002; Fung, 2000; Rapp, 1999) for the first and second translation models and propose a merging strategy for the two-stages translation model (Sadat et al., 2003). First, word frequencies, context word frequencies in surrounding positions (here three-words window) are computed following a statistics-based metrics, the log-likelihood ratio (Dunning, 1993). Context vectors for each source term and each target term are constructed. Next, context vectors of the target words are translated using a preliminary bilingual dictionary. We consider all translation candidates, keeping the same context frequency value as the source term. This step requires a seed lexicon, to expand using the proposed bootstrapping approach of this paper. Similarity vectors are cons</context>
<context position="6599" citStr="Sadat et al., 2003" startWordPosition="964" endWordPosition="967">The product of similarity values of both similarity vectors SIMST for pairs (s,t) and SIMTS for pairs (t,s) will result in similarity values in vectors SIMST. Therefore, similarity vectors of the two-stages translation model are expressed as follows: SIMST = {(s, t, simST(t|s)) |(s, t, simST(t|s)) E SIMST A (t, s, simTS(s|t)) E SIMTS A simST(t|s) = simST(t|s) X simTS(s|t)} 3 Linguistics-based Pruning Combining linguistic and statistical methods is becoming increasingly common in computational linguistics, especially as more corpora become available (Klanvans and Tzoukermann, 1996; Sadat et al., 2003). We propose to integrate linguistic concepts into the corpora-based translation model. Morphological knowledge such as Part-of-Speech (POS) tags, context of terms, etc., could be valuable to filter and prune the extracted translation candidates. The objective of the linguistics-based pruning technique is the detection of terms and their translations that are morphologically close enough, i.e., close or similar POS tags. This proposed approach will select a fixed number of equivalents from the set of extracted target translation alternatives that match the Part-ofSpeech of the source term. The</context>
</contexts>
<marker>Sadat, Yoshikawa, Uemura, 2003</marker>
<rawString>F. Sadat, M. Yoshikawa and S. Uemura. 2003. Enhancing Cross-language Information Retrieval by an Automatic Acquisition of Bilingual Terminology from Comparable Corpora. In Proc. ACM SIGIR 2003, Toronto, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sadat</author>
<author>M Yoshikawa</author>
<author>S Uemura</author>
</authors>
<title>Bilingual Terminology Acquisition from Comparable Corpora and Phrasal Translation to Cross-Language Information Retrieval.</title>
<date>2003</date>
<booktitle>In Proc. ACL 2003,</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="3616" citStr="Sadat et al., 2003" startWordPosition="503" endWordPosition="506">s conducted on NTCIR, a large-scale data collection for (Japanese, English) language pair. The remainder of the present paper is organized as follows: Section 2 presents the proposed twostages approach for bilingual terminology acquisition from comparable corpora. Section 3 describes the integration of linguistic knowledge for pruning the translation candidates. Experiments and evaluations in CLIR are discussed in Sections 4. Section 5 concludes the present paper. 2 Two-stages Comparable Corpora-based Approach Our proposed approach to bilingual terminology acquisition from comparable corpora (Sadat et al., 2003; Sadat et al., 2003) is based on the assumption of similar collocation, i.e., If two words are mutual translations, then their most frequent collocates are likely to be mutual translations as well. Moreover, we apply this assumption in both directions of the corpora, i.e., find translations of the source term in the target language corpus but also translations of the target terms in the source language corpus. The proposed two-stages approach for the acquisition, disambiguation and selection of bilingual terminology is described as follows: • Bilingual terminology acquisition from source lang</context>
<context position="4836" citStr="Sadat et al., 2003" startWordPosition="695" endWordPosition="698">age to target language to yield a first translation model, represented by similarity SIMST. • Bilingual terminology acquisition from target language to source language to yield a second translation model, represented by similarity SIMTS. • Merge the first and second models to yield a two-stages translation model, based on bidirectional comparable corpora and represented by similarity SIMST. We follow strategies of previous researches (Dejean et al., 2002; Fung, 2000; Rapp, 1999) for the first and second translation models and propose a merging strategy for the two-stages translation model (Sadat et al., 2003). First, word frequencies, context word frequencies in surrounding positions (here three-words window) are computed following a statistics-based metrics, the log-likelihood ratio (Dunning, 1993). Context vectors for each source term and each target term are constructed. Next, context vectors of the target words are translated using a preliminary bilingual dictionary. We consider all translation candidates, keeping the same context frequency value as the source term. This step requires a seed lexicon, to expand using the proposed bootstrapping approach of this paper. Similarity vectors are cons</context>
<context position="6599" citStr="Sadat et al., 2003" startWordPosition="964" endWordPosition="967">The product of similarity values of both similarity vectors SIMST for pairs (s,t) and SIMTS for pairs (t,s) will result in similarity values in vectors SIMST. Therefore, similarity vectors of the two-stages translation model are expressed as follows: SIMST = {(s, t, simST(t|s)) |(s, t, simST(t|s)) E SIMST A (t, s, simTS(s|t)) E SIMTS A simST(t|s) = simST(t|s) X simTS(s|t)} 3 Linguistics-based Pruning Combining linguistic and statistical methods is becoming increasingly common in computational linguistics, especially as more corpora become available (Klanvans and Tzoukermann, 1996; Sadat et al., 2003). We propose to integrate linguistic concepts into the corpora-based translation model. Morphological knowledge such as Part-of-Speech (POS) tags, context of terms, etc., could be valuable to filter and prune the extracted translation candidates. The objective of the linguistics-based pruning technique is the detection of terms and their translations that are morphologically close enough, i.e., close or similar POS tags. This proposed approach will select a fixed number of equivalents from the set of extracted target translation alternatives that match the Part-ofSpeech of the source term. The</context>
</contexts>
<marker>Sadat, Yoshikawa, Uemura, 2003</marker>
<rawString>F. Sadat, M. Yoshikawa and S. Uemura. 2003. Bilingual Terminology Acquisition from Comparable Corpora and Phrasal Translation to Cross-Language Information Retrieval. In Proc. ACL 2003, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
</authors>
<title>The SMART Retrieval System, Experiments</title>
<date>1971</date>
<booktitle>in Automatic Documents Processing.</booktitle>
<publisher>PrenticeHall, Inc.,</publisher>
<location>Englewood Cliffs, NJ.</location>
<contexts>
<context position="12836" citStr="Salton, 1971" startWordPosition="1910" endWordPosition="1911"> the generalized domain. We have also considered documents ofNTCIR-2 test collection as comparable corpora in order to cope with special features of the test collection during evaluations. Morphological analyzers, ChaSen version 2.2.9 (Matsumoto and al., 1997) for texts in Japanese and OAK2 (Sekine, 2001) were used in the linguistic preprocessing. EDR bilingual dictionary (EDR, 1996) was used to translate context vectors of source and target languages. NTCIR-2 (Kando, 2001), a large-scale test collection was used to evaluate the proposed strategies in CLIR. SMART information retrieval system (Salton, 1971), which is based on vector space model, was used to retrieve English documents. 4.2 Evaluations on the Proposed Translation Model We considered the set of news articles as well as the abstracts of NTCIR-2 test collection as comparable corpora for Japanese-English language pairs. The abstracts of NTCIR-2 test collection are partially aligned (more than half are Japanese-English paired documents) but the alignment was not considered in the present research to treat the set of documents as comparable. Content words (nouns, verbs, adjectives, adverbs) were extracted from English and Japanese corpo</context>
</contexts>
<marker>Salton, 1971</marker>
<rawString>G. Salton. 1971. The SMART Retrieval System, Experiments in Automatic Documents Processing. PrenticeHall, Inc., Englewood Cliffs, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>J McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval.</title>
<date>1983</date>
<location>New York, Mc Graw-Hill.</location>
<contexts>
<context position="5538" citStr="Salton and McGill, 1983" startWordPosition="799" endWordPosition="802">here three-words window) are computed following a statistics-based metrics, the log-likelihood ratio (Dunning, 1993). Context vectors for each source term and each target term are constructed. Next, context vectors of the target words are translated using a preliminary bilingual dictionary. We consider all translation candidates, keeping the same context frequency value as the source term. This step requires a seed lexicon, to expand using the proposed bootstrapping approach of this paper. Similarity vectors are constructed for each pair of source term and target term using the cosine metric (Salton and McGill, 1983). Therefore, similarity vectors SIMST and SIMTS for the first and second models are constructed and merged for a bi-directional acquisition of bilingual terminology from source language to target language. The merging process will keep common pairs of source term and target translation (s,t) which appear in SIMST as pairs of (s,t) but also in SIMTS as pairs of (t,s), to result in combined similarity vectors SIMST for each pair (s,t).The product of similarity values of both similarity vectors SIMST for pairs (s,t) and SIMTS for pairs (t,s) will result in similarity values in vectors SIMS</context>
<context position="18072" citStr="Salton and McGill, 1983" startWordPosition="2699" endWordPosition="2702">, LNC.LTC (Buckley and al., 1994) and LNC.LTN (Knaus and Shauble, 1993) combined weighting schemes on document terms and query terms showed the best results. On the other hand, our findings were quite similar to the result presented by Savoy (Savoy, 2003), where the ATN.NTC showed the best performance among the existing weighting schemes in SMART for English monolingual runs. Table 2 shows some weighting schemes of SMART retrieval system, among others. To assign an indexing weight wij that reflects the importance of each single-term Tj in a document Di, different factors should be considered (Salton and McGill, 1983), as follows: • within-document term frequency tfij, which represents the first letter of the SMART label. • collection-wide term frequency dfj, which represents the second letter of the SMART label. In Table 2, idfj = log FjN ; where, N represents the number of documents and Fj represents the document frequency of term Tj. • normalization scheme, which represents the third letter of the SMART label. 4.4 Evaluations on CLIR Bilingual translations were extracted from comparable corpora using the proposed two-stages model. A fixed number (set to five) of top-ranked translation alternatives was r</context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>G. Salton and J. McGill. 1983. Introduction to Modern Information Retrieval. New York, Mc Graw-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Savoy</author>
</authors>
<title>Cross-Language Information Retrieval: Experiments based on CLEF</title>
<date>2003</date>
<journal>Corpora. Information Processing &amp; Management</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="17703" citStr="Savoy, 2003" startWordPosition="2642" endWordPosition="2643">recision were realized by the following combined weighting schemes: ATN.NTC, LTN.NTC, LTC.NTC, ATC.NTC and NTC.NTC, respectively. The best weighting scheme for the monolingual runs turned out to be the ATN.NTC. This finding is somewhat different from previous results where ANN (Fox and Shaw, 1994), LTC (Fuhr and al., 1994) weighting schemes on query terms, LNC.LTC (Buckley and al., 1994) and LNC.LTN (Knaus and Shauble, 1993) combined weighting schemes on document terms and query terms showed the best results. On the other hand, our findings were quite similar to the result presented by Savoy (Savoy, 2003), where the ATN.NTC showed the best performance among the existing weighting schemes in SMART for English monolingual runs. Table 2 shows some weighting schemes of SMART retrieval system, among others. To assign an indexing weight wij that reflects the importance of each single-term Tj in a document Di, different factors should be considered (Salton and McGill, 1983), as follows: • within-document term frequency tfij, which represents the first letter of the SMART label. • collection-wide term frequency dfj, which represents the second letter of the SMART label. In Table 2, idfj = log FjN ; wh</context>
</contexts>
<marker>Savoy, 2003</marker>
<rawString>J. Savoy. 2003. Cross-Language Information Retrieval: Experiments based on CLEF 2000 Corpora. Information Processing &amp; Management 39(1):75–115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sekine</author>
</authors>
<title>OAK System-Manual.</title>
<date>2001</date>
<location>New York University.</location>
<contexts>
<context position="10822" citStr="Sekine, 2001" startWordPosition="1597" endWordPosition="1598">number of top-ranked translation alternatives (here set to 3) and illustrated as follows: ’asia assembly city competition sport representative meeting tournament assembly general great famous sport union day taikai great meeting’. Linguistics-based pruning was applied on the Japanese terms and the extracted English translation alternatives. Chasen morphological analyzer (Matsumoto and al., 1997)for Japanese has associated POS tags as (noun) to all Japanese terms: (ajia) Therefore, English translation alternatives associated with POS tags as nouns (NN) via a morphological analyzer for English (Sekine, 2001)are selected and translation candidates having POS tags other than NN (noun) are discarded. Selected translation alternatives for the Japanese noun (saidai) become ’group, measure, factor’. As well, the POS(s) = ’JJ’ and [POS(t) = ’ ’] ’ or ’ (saidai) - (supoutsu) - - (kai) (kyougi) - (taikai) - POS(s) = ’NN’ and POS(t) = ’ ’ POS(s) = ’VB’ and POS(t) = ’ ’ POS(s) = ’RB’ and [POS(t) = ’ ’ or ’ ’] - Japanese term ’ ’ (kai) is associated to the English translations: ’taikai, meeting, person’. The phrasal translation associated to the Japanese query after the linguistics-based pruning is illustrat</context>
<context position="12529" citStr="Sekine, 2001" startWordPosition="1863" endWordPosition="1864">ks in CLIR, i.e. Japanese queries to retrieve English documents. 4.1 Linguistic Resources Collections of news articles from Mainichi Newspapers (1998-1999) for Japanese and Mainichi Daily News (1998-199) for English were considered as comparable corpora, because of the common feature in the time period and the generalized domain. We have also considered documents ofNTCIR-2 test collection as comparable corpora in order to cope with special features of the test collection during evaluations. Morphological analyzers, ChaSen version 2.2.9 (Matsumoto and al., 1997) for texts in Japanese and OAK2 (Sekine, 2001) were used in the linguistic preprocessing. EDR bilingual dictionary (EDR, 1996) was used to translate context vectors of source and target languages. NTCIR-2 (Kando, 2001), a large-scale test collection was used to evaluate the proposed strategies in CLIR. SMART information retrieval system (Salton, 1971), which is based on vector space model, was used to retrieve English documents. 4.2 Evaluations on the Proposed Translation Model We considered the set of news articles as well as the abstracts of NTCIR-2 test collection as comparable corpora for Japanese-English language pairs. The abstracts</context>
</contexts>
<marker>Sekine, 2001</marker>
<rawString>S. Sekine. 2001. OAK System-Manual. New York University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Shahzad</author>
<author>K Ohtake</author>
<author>S Masuyama</author>
<author>K Yamamoto</author>
</authors>
<title>Identifying Translations of Compound using Non-aligned Corpora.</title>
<date>1999</date>
<booktitle>Proc. Workshop MAL,</booktitle>
<pages>108--113</pages>
<marker>Shahzad, Ohtake, Masuyama, Yamamoto, 1999</marker>
<rawString>I. Shahzad, K. Ohtake, S. Masuyama and K. Yamamoto. 1999. Identifying Translations of Compound using Non-aligned Corpora. Proc. Workshop MAL, pages 108–113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Tanaka</author>
<author>H Iwasaki</author>
</authors>
<date>1996</date>
<booktitle>Extraction of Lexical Translations from Non-aligned Corpora. Proc. COLING 96.</booktitle>
<contexts>
<context position="2362" citStr="Tanaka and Iwasaki, 1996" startWordPosition="316" endWordPosition="319">f their promise to provide bilingual terminology and enrich lexical resources such as bilingual dictionaries and thesauri. These approaches generally rely on large text corpora, which play an important role in Natural Language Processing (NLP) and Information Retrieval (IR). Moreover, non-aligned comparable corpora have been given a special interest in bilingual terminology acquisition and lexical resources enrichment (Dagan and Itai, 1994; Dejean et al., 2002; Diab and Finch, 2000; Fung, 2000; Koehn and Knight, 2002; Nakagawa, 2000; Peters and Picchi, 1995; Rapp, 1999; Shahzad and al., 1999; Tanaka and Iwasaki, 1996). Unlike parallel corpora, comparable corpora are collections of texts from pairs or multiples of languages, which can be contrasted because of their common features, in the topic, the domain, the authors or the time period. This property made comparable corpora more abundant, less expensive and more accessible through the World Wide Web. In the present paper, we are concerned by exploiting scarce resources for bilingual terminology acquisition, then evaluations on Cross-Language Information Retrieval (CLIR). CLIR consists of retrieving documents written in one language using queries written i</context>
</contexts>
<marker>Tanaka, Iwasaki, 1996</marker>
<rawString>K. Tanaka and H. Iwasaki. 1996 Extraction of Lexical Translations from Non-aligned Corpora. Proc. COLING 96.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>