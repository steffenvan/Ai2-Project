<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.013623">
<title confidence="0.996857">
UEdin: Translating L1 Phrases in L2 Context
using Context-Sensitive SMT
</title>
<author confidence="0.997667">
Eva Hasler
</author>
<affiliation confidence="0.99745">
ILCC, School of Informatics
University of Edinburgh
</affiliation>
<email confidence="0.997546">
e.hasler@ed.ac.uk
</email>
<sectionHeader confidence="0.993884" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999927538461539">
We describe our systems for the SemEval
2014 Task 5: L2 writing assistant where a
system has to find appropriate translations
of L1 segments in a given L2 context. We
participated in three out of four possible
language pairs (English-Spanish, French-
English and Dutch-English) and achieved
the best performance for all our submit-
ted systems according to word-based ac-
curacy. Our models are based on phrase-
based machine translation systems and
combine topical context information and
language model scoring.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999902809523809">
In the past years, the fields of statistical machine
translation (SMT) and word sense disambigua-
tion (WSD) have developed largely in parallel,
with each field organising their own shared tasks
aimed at improving translation quality (Bojar et
al., 2013) and predicting word senses, e.g. Agirre
et al. (2010). Because sense disambiguation is
a central problem in machine translation, there
has been work on integrating WSD classifiers into
MT systems (Carpuat and Wu, 2007a; Carpuat
and Wu, 2007b; Chan et al., 2007). However,
one problem with the direct integration of WSD
techniques into MT has been the mismatch be-
tween word predictions of WSD systems and the
phrase segmentations of MT system. This prob-
lem was adressed in Carpuat and Wu (2007b) by
extending word sense disambiguation to phrase
sense disambiguation. The relation between word
sense distinctions and translation has also been
explored in past SemEval tasks on cross-lingual
word sense disambiguation, where senses are not
</bodyText>
<footnote confidence="0.81027425">
This work is licensed under a Creative Commons Attribution
4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http:
//creativecommons.org/licenses/by/4.0/
</footnote>
<bodyText confidence="0.999634653846154">
defined in terms of WordNet senses as in previ-
ous tasks, but instead as translations to another
language (Lefever and Hoste, 2010; Lefever and
Hoste, 2013).
This year’s L2 writing assistant task is simi-
lar to the cross-lingual word sense disambigua-
tion task but differs in the context provided for
disambiguation and the length of the fragments
(source phrases instead of words). While in other
translation and disambiguation tasks the source
language context is given, the L2 writing assis-
tant task assumes a given target language con-
text that constrains the possible translations of L1
fragments. This is interesting from a machine
translation point-of-view because it allows for a
direct comparison with systems that exploit the
target context using a language model. As lan-
guage models have become more and more power-
ful over the years, mostly thanks to increased com-
puting power, new machine translation techniques
are also judged by their ability to improve perfor-
mance over a baseline system with a strong lan-
guage model. Another difference to previous Se-
mEval tasks is the focus on both lexical and gram-
matical forms, while previous tasks have mostly
focused on lexical selection.
</bodyText>
<sectionHeader confidence="0.9895335" genericHeader="method">
2 Translation Model for L1 Fragments in
L2 Context
</sectionHeader>
<bodyText confidence="0.999982090909091">
Our model for translating L1 fragments in L2 con-
text is a phrase-based machine translation system
with an additional context similarity feature. We
aim to resolve lexical ambiguities by taking the
entire topical L2 context of an L1 fragment into
account rather than only relying on the phrasal L1
context. We do not explicitly model the grammat-
icality of target word forms but rather use a stan-
dard 5-gram language model to score target word
sequences. We describe the context similarity fea-
ture in the following section.
</bodyText>
<page confidence="0.963185">
688
</page>
<note confidence="0.7893935">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 688–693,
Dublin, Ireland, August 23-24, 2014.
</note>
<subsectionHeader confidence="0.98903">
2.1 Context Similarity Feature
</subsectionHeader>
<bodyText confidence="0.99805825">
The context similarity feature is derived from the
phrase pair topic model (PPT) described in Hasler
et al. (2014). At training time, this model learns
topic distributions for all phrase pairs in the phrase
table in an unsupervised fashion, using a variant of
Latent Dirichlet Allocation (LDA). The underly-
ing assumption is that all phrase pairs share a set of
global topics of predefined size, thus each phrase
pair is assigned a distribution over the same set of
global topics. This is in contrast to Word Sense In-
duction (WSI) methods which typically learn a set
of topics or senses for each word type, for example
in Lau et al. (2010).
The input to the model are distributional profiles
of words occurring in the context of each phrase
pair, thus, the model learns lower-dimensional
representations of the likely context words of a
phrase pair. While in a normal machine transla-
tion setup the source sentence context is given, it is
straightforward to replace source language words
with target language words as given in the L2 con-
text for each test example. At test time, the topic
model is applied to the given L2 context to infer a
topic distribution of the test context. The topic dis-
tribution of an applicable phrase pair is compared
to the topic distribution of a given test context (de-
fined as all L2 words in the same sentence as the
L1 fragment, excluding stop words) using cosine
similarity.
To adapt the translation system to the context
of each test sentence, the phrase table is filtered
per test sentence and each applicable phrase pair
receives one additional feature that expresses its
topical similarity with the test context. While
the baseline system (the system without similar-
ity feature) translates the entire test set with the
same translation model, the context-sensitive sys-
tem loads an adapted phrase table for each test sen-
tence. While the phrase pair topic model can also
deal with document-level context, here we con-
sider only sentence-level context as no wider con-
text was available. We evaluate three variations of
the context similarity feature on top of a standard
phrase-based MT system:
</bodyText>
<listItem confidence="0.91014825">
• 50-topics The cosine similarity according to
the PPT model trained with 50 topics (sub-
mitted as run1)
• mixture:geoAvg The geometric average of
the cosine similarities according to PPT mod-
els trained with 20, 50 and 100 topics (sub-
mitted as run2)
• mixture:max For each source phrase, the co-
</listItem>
<bodyText confidence="0.976576">
sine similarity according to the PPT model
that yields the lowest entropy (out of the
models with 20, 50 and 100 topics) when
converting the similarities into probabilities
(submitted as run3)
</bodyText>
<subsectionHeader confidence="0.981136">
2.2 Language Model Scoring of L2 Context
</subsectionHeader>
<bodyText confidence="0.999024941176471">
On top of using the words in the L2 context for
computing the similarity feature described above,
we introduce a simple method for using a language
model to score the target sequence that includes
the translated L1 segments and the words to the
left and right of the translated segments. In order
to use the language model scoring implemented in
the Moses decoder, we present the decoder with
an input sentence that contains the L1 fragment as
well as the L2 context with XML markup. While
the L1 fragments are translated without special
treatment, the L2 tokens are passed through un-
translated by specifying the identity translation as
markup. The XML markup also includes reorder-
ing walls to prevent the decoder from reordering
the L2 context. An example input sentence with
markup (French-English) is shown below:
</bodyText>
<figure confidence="0.828199857142857">
&lt;wall/&gt;les manifesteurs&lt;wall/&gt;
&lt;np translation=”want”&gt;want&lt;/np&gt;&lt;wall/&gt;
&lt;np translation=”to”&gt;to&lt;/np&gt;&lt;wall/&gt;
&lt;np translation=”replace”&gt;replace&lt;/np&gt;&lt;wall/&gt;
&lt;np translation=”the”&gt;the&lt;/np&gt;&lt;wall/&gt;
&lt;np translation=”government”&gt;government&lt;/np&gt;&lt;wall/&gt;
&lt;np translation=”.”&gt;.&lt;/np&gt;&lt;wall/&gt;
</figure>
<sectionHeader confidence="0.983189" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999984882352941">
Although the task is defined as building a transla-
tion assistance system rather than a full machine
translation system, we use a standard machine
translation setup to translate L1 phrases. We used
the Moses toolkit (Koehn et al., 2007) to build
phrase-based translation systems for the language
pairs English-Spanish, French-English and Dutch-
English1. For preprocessing, we applied punctua-
tion normalisation, truecasing and tokenisation us-
ing the scripts provided with the Moses toolkit.
The model contains the following standard fea-
tures: direct and inverse phrase translation prob-
abilities, lexical weights, word and phrase penalty,
lexicalised reordering and distortion features and
a 5-gram language model with modified Kneser-
Ney smoothing. In addition, we add the context
similarity feature described in Section 2.1.
</bodyText>
<footnote confidence="0.8973985">
1We left out the English-German language pair for time
reasons.
</footnote>
<page confidence="0.994202">
689
</page>
<table confidence="0.999881833333333">
Training data English-Spanish French-English Dutch-English
Europarl 1.92M 1.96M 1.95M
News Commentary 192K 181K n/a
TED 157K 159K 145K
News 2.1G 2.1G 2.1G
Commoncrawl 50M 82M -
</table>
<tableCaption confidence="0.999843">
Table 1: Overview of parallel and monolingual training data (top/bottom, in number of sentences/words).
</tableCaption>
<subsectionHeader confidence="0.999067">
3.1 Training Data
</subsectionHeader>
<bodyText confidence="0.9999805">
Most of the training data was taken from the
WMT13 shared task (Bojar et al., 2013), ex-
cept where specified otherwise. For the English-
Spanish and French-English systems, we used par-
allel training data from the Europarl and News
Commentary corpora, as well as the TED corpus
(Cettolo et al., 2012). For Dutch-English, we used
parallel data from the Europarl and TED corpus.
The language models were trained on the target
sides of the parallel data and additional news data
from the years 2007-2012. For English-Spanish
and French-English, we used additional language
model data from the Commoncrawl corpus2. Sep-
arate language models were trained for each cor-
pus and interpolated on a development set. An
overview of the training data is shown in Table 1.
</bodyText>
<subsectionHeader confidence="0.996137">
3.2 Tuning Model Parameters
</subsectionHeader>
<bodyText confidence="0.999629176470588">
The parameters of the baseline MT excluding
the similarity feature were tuned with kbest-mira
(Cherry and Foster, 2012) on mixed development
sets containing the trial data (500 sentence pairs
with XML markup) distributed for the task as well
as development data from the news and TED cor-
pora for the English-Spanish and French-English
systems and development data from the TED cor-
pus for the Dutch-English system. Because the do-
main(s) of the test examples was not known be-
forehand, we aimed for learning model weights
that would generalise across domains by using
rather diverse tuning sets. In total, the develop-
ment sets consisted of 3435, 3705 and 3516 sen-
tence pairs, respectively. We did not tune the
weight of the similarity feature automatically, but
set it to an empirically determined value instead.
</bodyText>
<subsectionHeader confidence="0.9761155">
3.3 Simulating Ambiguous Development
Data
</subsectionHeader>
<bodyText confidence="0.9771595">
When developing our systems using the trial data
supplied by the task organisers, we noticed that
</bodyText>
<footnote confidence="0.993984">
2For the Dutch-English system, the Commoncrawl data
did not seem to improve performance.
</footnote>
<table confidence="0.9991095">
Source words Translations
chaine chain, string, channel, station
mati`ere matter, material, subject
flux stream, flow, feed
d´emon demon, daemon, devil
r´egime regime, diet, rule
</table>
<tableCaption confidence="0.978867">
Table 2: Examples of ambiguous source words
and their different translations in the simulated de-
velopment set.
</tableCaption>
<table confidence="0.999912266666667">
System French-English
Baseline 0.314
+ LM context 0.726
20-topics 0.603
+ LM context 0.845
50-topics 0.674
+ LM context 0.886
100-topics 0.628
+ LM context 0.872
mixture:arithmAvg 0.650
+ LM context 0.869
mixture:geoAvg 0.670
+ LM context 0.883
mixture:max 0.690
+ LM context 0.889
</table>
<tableCaption confidence="0.995419">
Table 3: Word accuracy (best) on the simulated de-
</tableCaption>
<bodyText confidence="0.997710769230769">
velopment set for the smaller baseline system and
the systems with added context similarity feature,
with and without language model context.
the context similarity feature did not add much to
the overall performance, which we attributed to
the small number of ambiguous examples in the
trial data. Therefore, we extracted a set of 1076
development instances containing 14 ambiguous
French words and their English translations from
a mixed corpus containing data from the News
Commentary, TED and Commoncrawl corpora as
used in Hasler et al. (2014). Examples of ambigu-
ous source words and their translations in that de-
</bodyText>
<page confidence="0.99466">
690
</page>
<bodyText confidence="0.999908178571428">
velopment set are shown in Table 2.
Translating the L1 fragments in the simulated
development set using a smaller baseline system
trained on this mixed data set yields the results at
the top of Table 3. Note that even though the in-
stances were extracted from the training set, this
does not affect the translation model since the
L1 fragments contain only the ambiguous source
words and no further source context that could be
memorised.
The bottom part of Table 3 shows the perfor-
mance of the three context similarity features de-
scribed in Section 2.1 plus some further variants
(the models with 20 and 100 topics as well as
the arithmetic average of the cosine similarities
of models trained with 20, 50 and 100 topics).
First, we observe that each of the features clearly
outperforms the baseline system without language
model context. Second, each context similarity
feature together with the language model context
still outperforms the Baseline + LM context. Even
though the gain of the context similarity features
is smaller when the target context is scored with
a language model, the topical context still pro-
vides additional information that improves lexical
choice. We trained versions of the three best mod-
els from Table 3 (in bold) for our submissions on
the official test sets.
</bodyText>
<sectionHeader confidence="0.999604" genericHeader="evaluation">
4 Results and Discussion
</sectionHeader>
<bodyText confidence="0.992979681818182">
In this section we report the experimental results
of our systems on the official test sets. The re-
sults without scoring the L2 context with a lan-
guage model are shown in Table 4 and including
language model scoring of L2 context in Table 5.
We limit the reported scores to word accuracy and
do not report recall because our systems produce
output for every L1 phrase.
In Table 4, we compare the performance of the
baseline MT system to systems including one of
three variants of the similarity feature as described
in Section 2.1, according to the 1-best transla-
tion (best) as well as the 5-best translations (out-
of-five) in a distinct n-best list. For five out of
the six tasks, at least one of the systems includ-
ing the similiary feature yields better performance
than the baseline system. Only for French-English
best, the baseline system yields the best word ac-
curacy. Among the three variants, 50-topics and
mixture:geoAvg perform slightly better than mix-
ture:max in most cases.
Table 5 shows the results of our submitted runs
</bodyText>
<tableCaption confidence="0.818293375">
Input: There are many ways of cooking
&lt;f&gt;des œufs&lt;/f&gt; for breakfast.
Reference: There are many ways of cooking
&lt;f&gt;eggs&lt;/f&gt; for breakfast.
Input: I loved animals when I was &lt;f&gt;un
enfant&lt;/f&gt;.
Reference: I loved animals when I was &lt;f&gt;a
kid&lt;alt&gt;a child&lt;/alt&gt;&lt;/f&gt;.
</tableCaption>
<figureCaption confidence="0.999525">
Figure 1: Examples of official test instances.
</figureCaption>
<bodyText confidence="0.999561511627907">
(run1-run3) as well as the baseline system, all
with language model scoring of L2 context via
XML markup. The first thing to note in com-
parison to Table 4 is that providing the L2 con-
text for language model scoring yields quite sub-
stantial improvements (0.165, 0.101 and 0.073, re-
spectively). Again, in five out of six cases at least
one of the systems with context similarity feature
performs better than the baseline system. Only for
Spanish-English best, the baseline system yields
higher word accuracy than the three submitted
runs. As before, 50-topics and mixture:geoAvg
perform slightly better than mixture:max, with a
preference for 50-topics. For comparison, we also
show the word accuracies of the 2nd-ranked sys-
tem for both tasks and each language pair. We
note that the distance to the respective runner-up
system is largest for French-English and on aver-
age larger for the out-of-five task than for the best
task.
As a general observation, we can state that
although the similarity feature improves perfor-
mance in most cases, the improvements are small
compared to the improvements achieved by scor-
ing the L2 language model contexts. We suspect
two reasons for this effect: first, we do not explic-
itly model grammaticality of word forms. There-
fore, our system relies on the language model to
choose the best word form for those test examples
that do not contain any lexical ambiguity. Second,
we have noticed that for some of the test exam-
ples, the correct translations do not depend partic-
ularly on words in the L2 context, as shown in Fig-
ure 1 where the most common translations of the
source phrases without context would match the
reference translations. These are cases where we
do not expect much of an improvement in transla-
tion by taking the L2 context into account.
Since in Section 3.3 we have provided evidence
that topical similarity features can improve lexical
choice over simply using a target language model,
we believe that the lower performance of the sim-
ilarity features on the official test set is caused by
</bodyText>
<page confidence="0.99575">
691
</page>
<table confidence="0.999779166666667">
System English-Spanish French-English Dutch-English
best oof best oof best oof
Baseline 0.674 0.854 0.722 0.884 0.613 0.750
50-topics 0.682 0.860 0.719 0.896 0.616 0.759
mixture:geoAvg 0.677 0.863 0.715 0.896 0.619 0.756
mixture:max 0.679 0.860 0.712 0.887 0.618 0.753
</table>
<tableCaption confidence="0.967583">
Table 4: Word accuracy (best and out-of-five) of the baseline system and the systems with added context
similarity feature. All systems were run without scoring the language model context.
</tableCaption>
<table confidence="0.999821571428571">
System English-Spanish French-English Dutch-English
best oof best oof best oof
Baseline + LM context 0.839 0.944 0.823 0.934 0.686 0.809
50-topics + LM context 0.827 0.946 0.824 0.938 0.692 0.811
mixture:geoAvg + LM context 0.827 0.944 0.821 0.939 0.688 0.808
mixture:max + LM context 0.820 0.949 0.816 0.937 0.688 0.808
2nd-ranked systems 0.8091 0.8872 0.6942 0.8392 0.6793 0.7533
</table>
<tableCaption confidence="0.968855">
Table 5: Word accuracy (best and out-of-five) of all submitted systems (runs 1-3) as well as the baseline
</tableCaption>
<bodyText confidence="0.978236818181818">
system without the context similarity feature. All systems were run with the language model context
provided via XML input. Systems on 2nd rank: 1UNAL-run2, 2CNRC-run1, 3IUCL-run1
different levels of ambiguity in the simulated de-
velopment set and the official test set. For the
simulated development set, we explicitly selected
ambiguous source words in contexts which trig-
ger multiple different translations, while the offi-
cial test set also contains examples where the fo-
cus is on correct verb forms. It further contains ex-
amples where the baseline system without context
information could easily provide the correct trans-
lation, as shown above. Thus, the performance of
our topical context models should ideally be eval-
uated on test sets that contain a sufficient number
of ambiguous source phrases in order to measure
its ability to improve lexical selection.
Finally, in Figure 2 we show some examples
where the 50-topics system (with LM context)
produced semantically better translations than the
baseline system and where words in the L2 con-
text would have helped in promoting them over the
choice of the baseline system.
</bodyText>
<sectionHeader confidence="0.999259" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99986075">
We have described our systems for the SemEval
2014 Task 5: L2 writing assistant which achieved
the best performance for all submitted language
pairs and both the best and out-of-five tasks. All
</bodyText>
<table confidence="0.865266583333333">
Input: Why has Air France authorised &lt;f&gt;les ap-
pareils ´electroniques&lt;/f&gt; at take-off?
Baseline: .. &lt;f&gt;the electronics&lt;/f&gt; ..
50-topics: .. &lt;f&gt;electronic devices&lt;/f&gt; ..
Reference: .. &lt;f&gt;electronic devices&lt;/f&gt; ..
Input: This project represents one of the rare ad-
vances in strenghtening &lt;f&gt;les liens&lt;/f&gt;
between Brazil and the European Union.
Baseline: .. &lt;f&gt;the links&lt;/f&gt; ..
50-topics: .. &lt;f&gt;the ties&lt;/f&gt; ..
Reference: .. &lt;f&gt;the ties&lt;alt&gt;relations&lt;/alt&gt;&lt;alt&gt;
the bonds&lt;/alt&gt;&lt;/f&gt; ..
</table>
<figureCaption confidence="0.9076725">
Figure 2: Examples of improved translation output
with the context similarity feature.
</figureCaption>
<bodyText confidence="0.999281692307692">
systems are based on phrase-based machine trans-
lation systems with an added context similarity
feature derived from a topic model that learns
topic distributions for phrase pairs. We show that
the additional similarity feature improves perfor-
mance over our baseline models and that further
gains can be achieved by passing the L2 context
through the decoder via XML markup, thereby
producing language model scores of the sequences
of L2 context words and translated L1 fragments.
We also provide evidence that the relative perfor-
mance of the context similarity features depends
on the level of ambiguity in the L1 fragments.
</bodyText>
<page confidence="0.997702">
692
</page>
<sectionHeader confidence="0.997373" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999352">
This work was supported by funding from the
Scottish Informatics and Computer Science Al-
liance and funding from the European Union Sev-
enth Framework Programme (FP7/2007-2013) un-
der grant agreement 287658 (EU BRIDGE).
</bodyText>
<sectionHeader confidence="0.999184" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999857327272727">
Eneko Agirre, Oier Lopez De Lacalle, Christiane Fell-
baum, Maurizio Tesconi, Monica Monachini, Piek
Vossen, and Roxanne Segers. 2010. SemEval-2010
Task 17: All-words Word Sense Disambiguation on
a Specific Domain. In Proceedings of the 5th Inter-
national Workshop on Semantic Evaluation.
Ondrej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 workshop
on statistical machine translation. In Proceedings of
WMT 2013.
Marine Carpuat and Dekai Wu. 2007a. How Phrase
Sense Disambiguation outperforms Word Sense Dis-
ambiguation for Statistical Machine Translation.
In International Conference on Theoretical and
Methodological Issues in MT.
Marine Carpuat and Dekai Wu. 2007b. Improving Sta-
tistical Machine Translation using Word Sense Dis-
ambiguation. In Proceedings of EMNLP, pages 61–
72.
Mauro Cettolo, Christian Girardi, and Marcello Fed-
erico. 2012. WIT3: Web Inventory of Transcribed
and Translated Talks. In Proceedings of EAMT.
Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007. Word Sense Disambiguation Improves Statis-
tical Machine Translation. In Proceedings of ACL.
Colin Cherry and George Foster. 2012. Batch Tun-
ing Strategies for Statistical Machine Translation. In
Proceedings of NAACL.
Eva Hasler, Barry Haddow, and Philipp Koehn. 2014.
Dynamic Topic Adaptation for SMT using Distribu-
tional Profiles. In Proceedings of the 9th Workshop
on Statistical Machine Translation.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for SMT. In Proceedings of ACL:
Demo and poster sessions.
Jey Han Lau, Paul Cook, Diana Mccarthy, David New-
man, and Timothy Baldwin. 2010. Word Sense In-
duction for Novel Sense Detection. In Proceedings
of EACL.
Els Lefever and Veronique Hoste. 2010. SemEval-
2010 Task 3: Cross-lingual Word Sense Disam-
biguation. In Proceedings of the 5th International
Workshop on Semantic Evaluation.
Els Lefever and Veronique Hoste. 2013. SemEval-
2013 Task 10: Cross-lingual Word Sense Disam-
biguation. In Proceedings of the 7th International
Workshop on Semantic Evaluation, in Conjunction
with the Second Joint Conference on Lexical and
Computational Semantics.
</reference>
<page confidence="0.999129">
693
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.635095">
<title confidence="0.8363585">UEdin: Translating L1 Phrases in L2 using Context-Sensitive SMT</title>
<author confidence="0.995466">Eva</author>
<affiliation confidence="0.9806495">ILCC, School of University of</affiliation>
<email confidence="0.994155">e.hasler@ed.ac.uk</email>
<abstract confidence="0.999199357142857">We describe our systems for the SemEval Task 5: writing assistant a system has to find appropriate translations of L1 segments in a given L2 context. We participated in three out of four possible language pairs (English-Spanish, French- English and Dutch-English) and achieved the best performance for all our submitted systems according to word-based accuracy. Our models are based on phrasebased machine translation systems and combine topical context information and language model scoring.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Oier Lopez De Lacalle</author>
<author>Christiane Fellbaum</author>
<author>Maurizio Tesconi</author>
<author>Monica Monachini</author>
<author>Piek Vossen</author>
<author>Roxanne Segers</author>
</authors>
<title>SemEval-2010 Task 17: All-words Word Sense Disambiguation on a Specific Domain.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation.</booktitle>
<marker>Agirre, De Lacalle, Fellbaum, Tesconi, Monachini, Vossen, Segers, 2010</marker>
<rawString>Eneko Agirre, Oier Lopez De Lacalle, Christiane Fellbaum, Maurizio Tesconi, Monica Monachini, Piek Vossen, and Roxanne Segers. 2010. SemEval-2010 Task 17: All-words Word Sense Disambiguation on a Specific Domain. In Proceedings of the 5th International Workshop on Semantic Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondrej Bojar</author>
<author>Christian Buck</author>
<author>Chris Callison-Burch</author>
<author>Christian Federmann</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<title>Findings of the 2013 workshop on statistical machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of WMT</booktitle>
<contexts>
<context position="935" citStr="Bojar et al., 2013" startWordPosition="137" endWordPosition="140"> context. We participated in three out of four possible language pairs (English-Spanish, FrenchEnglish and Dutch-English) and achieved the best performance for all our submitted systems according to word-based accuracy. Our models are based on phrasebased machine translation systems and combine topical context information and language model scoring. 1 Introduction In the past years, the fields of statistical machine translation (SMT) and word sense disambiguation (WSD) have developed largely in parallel, with each field organising their own shared tasks aimed at improving translation quality (Bojar et al., 2013) and predicting word senses, e.g. Agirre et al. (2010). Because sense disambiguation is a central problem in machine translation, there has been work on integrating WSD classifiers into MT systems (Carpuat and Wu, 2007a; Carpuat and Wu, 2007b; Chan et al., 2007). However, one problem with the direct integration of WSD techniques into MT has been the mismatch between word predictions of WSD systems and the phrase segmentations of MT system. This problem was adressed in Carpuat and Wu (2007b) by extending word sense disambiguation to phrase sense disambiguation. The relation between word sense d</context>
<context position="8860" citStr="Bojar et al., 2013" startWordPosition="1381" endWordPosition="1384">ring and distortion features and a 5-gram language model with modified KneserNey smoothing. In addition, we add the context similarity feature described in Section 2.1. 1We left out the English-German language pair for time reasons. 689 Training data English-Spanish French-English Dutch-English Europarl 1.92M 1.96M 1.95M News Commentary 192K 181K n/a TED 157K 159K 145K News 2.1G 2.1G 2.1G Commoncrawl 50M 82M - Table 1: Overview of parallel and monolingual training data (top/bottom, in number of sentences/words). 3.1 Training Data Most of the training data was taken from the WMT13 shared task (Bojar et al., 2013), except where specified otherwise. For the EnglishSpanish and French-English systems, we used parallel training data from the Europarl and News Commentary corpora, as well as the TED corpus (Cettolo et al., 2012). For Dutch-English, we used parallel data from the Europarl and TED corpus. The language models were trained on the target sides of the parallel data and additional news data from the years 2007-2012. For English-Spanish and French-English, we used additional language model data from the Commoncrawl corpus2. Separate language models were trained for each corpus and interpolated on a </context>
</contexts>
<marker>Bojar, Buck, Callison-Burch, Federmann, Haddow, Koehn, Monz, Post, Soricut, Specia, 2013</marker>
<rawString>Ondrej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 workshop on statistical machine translation. In Proceedings of WMT 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Dekai Wu</author>
</authors>
<title>How Phrase Sense Disambiguation outperforms Word Sense Disambiguation for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In International Conference on Theoretical and Methodological Issues in MT.</booktitle>
<contexts>
<context position="1153" citStr="Carpuat and Wu, 2007" startWordPosition="171" endWordPosition="174">y. Our models are based on phrasebased machine translation systems and combine topical context information and language model scoring. 1 Introduction In the past years, the fields of statistical machine translation (SMT) and word sense disambiguation (WSD) have developed largely in parallel, with each field organising their own shared tasks aimed at improving translation quality (Bojar et al., 2013) and predicting word senses, e.g. Agirre et al. (2010). Because sense disambiguation is a central problem in machine translation, there has been work on integrating WSD classifiers into MT systems (Carpuat and Wu, 2007a; Carpuat and Wu, 2007b; Chan et al., 2007). However, one problem with the direct integration of WSD techniques into MT has been the mismatch between word predictions of WSD systems and the phrase segmentations of MT system. This problem was adressed in Carpuat and Wu (2007b) by extending word sense disambiguation to phrase sense disambiguation. The relation between word sense distinctions and translation has also been explored in past SemEval tasks on cross-lingual word sense disambiguation, where senses are not This work is licensed under a Creative Commons Attribution 4.0 International Lic</context>
</contexts>
<marker>Carpuat, Wu, 2007</marker>
<rawString>Marine Carpuat and Dekai Wu. 2007a. How Phrase Sense Disambiguation outperforms Word Sense Disambiguation for Statistical Machine Translation. In International Conference on Theoretical and Methodological Issues in MT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Dekai Wu</author>
</authors>
<title>Improving Statistical Machine Translation using Word Sense Disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>61--72</pages>
<contexts>
<context position="1153" citStr="Carpuat and Wu, 2007" startWordPosition="171" endWordPosition="174">y. Our models are based on phrasebased machine translation systems and combine topical context information and language model scoring. 1 Introduction In the past years, the fields of statistical machine translation (SMT) and word sense disambiguation (WSD) have developed largely in parallel, with each field organising their own shared tasks aimed at improving translation quality (Bojar et al., 2013) and predicting word senses, e.g. Agirre et al. (2010). Because sense disambiguation is a central problem in machine translation, there has been work on integrating WSD classifiers into MT systems (Carpuat and Wu, 2007a; Carpuat and Wu, 2007b; Chan et al., 2007). However, one problem with the direct integration of WSD techniques into MT has been the mismatch between word predictions of WSD systems and the phrase segmentations of MT system. This problem was adressed in Carpuat and Wu (2007b) by extending word sense disambiguation to phrase sense disambiguation. The relation between word sense distinctions and translation has also been explored in past SemEval tasks on cross-lingual word sense disambiguation, where senses are not This work is licensed under a Creative Commons Attribution 4.0 International Lic</context>
</contexts>
<marker>Carpuat, Wu, 2007</marker>
<rawString>Marine Carpuat and Dekai Wu. 2007b. Improving Statistical Machine Translation using Word Sense Disambiguation. In Proceedings of EMNLP, pages 61– 72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mauro Cettolo</author>
<author>Christian Girardi</author>
<author>Marcello Federico</author>
</authors>
<title>WIT3: Web Inventory of Transcribed and Translated Talks.</title>
<date>2012</date>
<booktitle>In Proceedings of EAMT.</booktitle>
<contexts>
<context position="9073" citStr="Cettolo et al., 2012" startWordPosition="1416" endWordPosition="1419">air for time reasons. 689 Training data English-Spanish French-English Dutch-English Europarl 1.92M 1.96M 1.95M News Commentary 192K 181K n/a TED 157K 159K 145K News 2.1G 2.1G 2.1G Commoncrawl 50M 82M - Table 1: Overview of parallel and monolingual training data (top/bottom, in number of sentences/words). 3.1 Training Data Most of the training data was taken from the WMT13 shared task (Bojar et al., 2013), except where specified otherwise. For the EnglishSpanish and French-English systems, we used parallel training data from the Europarl and News Commentary corpora, as well as the TED corpus (Cettolo et al., 2012). For Dutch-English, we used parallel data from the Europarl and TED corpus. The language models were trained on the target sides of the parallel data and additional news data from the years 2007-2012. For English-Spanish and French-English, we used additional language model data from the Commoncrawl corpus2. Separate language models were trained for each corpus and interpolated on a development set. An overview of the training data is shown in Table 1. 3.2 Tuning Model Parameters The parameters of the baseline MT excluding the similarity feature were tuned with kbest-mira (Cherry and Foster, </context>
</contexts>
<marker>Cettolo, Girardi, Federico, 2012</marker>
<rawString>Mauro Cettolo, Christian Girardi, and Marcello Federico. 2012. WIT3: Web Inventory of Transcribed and Translated Talks. In Proceedings of EAMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
<author>David Chiang</author>
</authors>
<title>Word Sense Disambiguation Improves Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1197" citStr="Chan et al., 2007" startWordPosition="179" endWordPosition="182"> translation systems and combine topical context information and language model scoring. 1 Introduction In the past years, the fields of statistical machine translation (SMT) and word sense disambiguation (WSD) have developed largely in parallel, with each field organising their own shared tasks aimed at improving translation quality (Bojar et al., 2013) and predicting word senses, e.g. Agirre et al. (2010). Because sense disambiguation is a central problem in machine translation, there has been work on integrating WSD classifiers into MT systems (Carpuat and Wu, 2007a; Carpuat and Wu, 2007b; Chan et al., 2007). However, one problem with the direct integration of WSD techniques into MT has been the mismatch between word predictions of WSD systems and the phrase segmentations of MT system. This problem was adressed in Carpuat and Wu (2007b) by extending word sense disambiguation to phrase sense disambiguation. The relation between word sense distinctions and translation has also been explored in past SemEval tasks on cross-lingual word sense disambiguation, where senses are not This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer ar</context>
</contexts>
<marker>Chan, Ng, Chiang, 2007</marker>
<rawString>Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007. Word Sense Disambiguation Improves Statistical Machine Translation. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>George Foster</author>
</authors>
<title>Batch Tuning Strategies for Statistical Machine Translation.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL.</booktitle>
<contexts>
<context position="9678" citStr="Cherry and Foster, 2012" startWordPosition="1512" endWordPosition="1515">ttolo et al., 2012). For Dutch-English, we used parallel data from the Europarl and TED corpus. The language models were trained on the target sides of the parallel data and additional news data from the years 2007-2012. For English-Spanish and French-English, we used additional language model data from the Commoncrawl corpus2. Separate language models were trained for each corpus and interpolated on a development set. An overview of the training data is shown in Table 1. 3.2 Tuning Model Parameters The parameters of the baseline MT excluding the similarity feature were tuned with kbest-mira (Cherry and Foster, 2012) on mixed development sets containing the trial data (500 sentence pairs with XML markup) distributed for the task as well as development data from the news and TED corpora for the English-Spanish and French-English systems and development data from the TED corpus for the Dutch-English system. Because the domain(s) of the test examples was not known beforehand, we aimed for learning model weights that would generalise across domains by using rather diverse tuning sets. In total, the development sets consisted of 3435, 3705 and 3516 sentence pairs, respectively. We did not tune the weight of th</context>
</contexts>
<marker>Cherry, Foster, 2012</marker>
<rawString>Colin Cherry and George Foster. 2012. Batch Tuning Strategies for Statistical Machine Translation. In Proceedings of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eva Hasler</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
</authors>
<title>Dynamic Topic Adaptation for SMT using Distributional Profiles.</title>
<date>2014</date>
<booktitle>In Proceedings of the 9th Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="3934" citStr="Hasler et al. (2014)" startWordPosition="610" endWordPosition="613">king the entire topical L2 context of an L1 fragment into account rather than only relying on the phrasal L1 context. We do not explicitly model the grammaticality of target word forms but rather use a standard 5-gram language model to score target word sequences. We describe the context similarity feature in the following section. 688 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 688–693, Dublin, Ireland, August 23-24, 2014. 2.1 Context Similarity Feature The context similarity feature is derived from the phrase pair topic model (PPT) described in Hasler et al. (2014). At training time, this model learns topic distributions for all phrase pairs in the phrase table in an unsupervised fashion, using a variant of Latent Dirichlet Allocation (LDA). The underlying assumption is that all phrase pairs share a set of global topics of predefined size, thus each phrase pair is assigned a distribution over the same set of global topics. This is in contrast to Word Sense Induction (WSI) methods which typically learn a set of topics or senses for each word type, for example in Lau et al. (2010). The input to the model are distributional profiles of words occurring in t</context>
<context position="11767" citStr="Hasler et al. (2014)" startWordPosition="1840" endWordPosition="1843">690 + LM context 0.889 Table 3: Word accuracy (best) on the simulated development set for the smaller baseline system and the systems with added context similarity feature, with and without language model context. the context similarity feature did not add much to the overall performance, which we attributed to the small number of ambiguous examples in the trial data. Therefore, we extracted a set of 1076 development instances containing 14 ambiguous French words and their English translations from a mixed corpus containing data from the News Commentary, TED and Commoncrawl corpora as used in Hasler et al. (2014). Examples of ambiguous source words and their translations in that de690 velopment set are shown in Table 2. Translating the L1 fragments in the simulated development set using a smaller baseline system trained on this mixed data set yields the results at the top of Table 3. Note that even though the instances were extracted from the training set, this does not affect the translation model since the L1 fragments contain only the ambiguous source words and no further source context that could be memorised. The bottom part of Table 3 shows the performance of the three context similarity feature</context>
</contexts>
<marker>Hasler, Haddow, Koehn, 2014</marker>
<rawString>Eva Hasler, Barry Haddow, and Philipp Koehn. 2014. Dynamic Topic Adaptation for SMT using Distributional Profiles. In Proceedings of the 9th Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for SMT.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL: Demo</booktitle>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<note>and poster sessions.</note>
<contexts>
<context position="7823" citStr="Koehn et al., 2007" startWordPosition="1230" endWordPosition="1233">he L2 context. An example input sentence with markup (French-English) is shown below: &lt;wall/&gt;les manifesteurs&lt;wall/&gt; &lt;np translation=”want”&gt;want&lt;/np&gt;&lt;wall/&gt; &lt;np translation=”to”&gt;to&lt;/np&gt;&lt;wall/&gt; &lt;np translation=”replace”&gt;replace&lt;/np&gt;&lt;wall/&gt; &lt;np translation=”the”&gt;the&lt;/np&gt;&lt;wall/&gt; &lt;np translation=”government”&gt;government&lt;/np&gt;&lt;wall/&gt; &lt;np translation=”.”&gt;.&lt;/np&gt;&lt;wall/&gt; 3 Experimental Setup Although the task is defined as building a translation assistance system rather than a full machine translation system, we use a standard machine translation setup to translate L1 phrases. We used the Moses toolkit (Koehn et al., 2007) to build phrase-based translation systems for the language pairs English-Spanish, French-English and DutchEnglish1. For preprocessing, we applied punctuation normalisation, truecasing and tokenisation using the scripts provided with the Moses toolkit. The model contains the following standard features: direct and inverse phrase translation probabilities, lexical weights, word and phrase penalty, lexicalised reordering and distortion features and a 5-gram language model with modified KneserNey smoothing. In addition, we add the context similarity feature described in Section 2.1. 1We left out </context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for SMT. In Proceedings of ACL: Demo and poster sessions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jey Han Lau</author>
<author>Paul Cook</author>
<author>Diana Mccarthy</author>
<author>David Newman</author>
<author>Timothy Baldwin</author>
</authors>
<title>Word Sense Induction for Novel Sense Detection.</title>
<date>2010</date>
<booktitle>In Proceedings of EACL.</booktitle>
<contexts>
<context position="4458" citStr="Lau et al. (2010)" startWordPosition="702" endWordPosition="705">y feature is derived from the phrase pair topic model (PPT) described in Hasler et al. (2014). At training time, this model learns topic distributions for all phrase pairs in the phrase table in an unsupervised fashion, using a variant of Latent Dirichlet Allocation (LDA). The underlying assumption is that all phrase pairs share a set of global topics of predefined size, thus each phrase pair is assigned a distribution over the same set of global topics. This is in contrast to Word Sense Induction (WSI) methods which typically learn a set of topics or senses for each word type, for example in Lau et al. (2010). The input to the model are distributional profiles of words occurring in the context of each phrase pair, thus, the model learns lower-dimensional representations of the likely context words of a phrase pair. While in a normal machine translation setup the source sentence context is given, it is straightforward to replace source language words with target language words as given in the L2 context for each test example. At test time, the topic model is applied to the given L2 context to infer a topic distribution of the test context. The topic distribution of an applicable phrase pair is comp</context>
</contexts>
<marker>Lau, Cook, Mccarthy, Newman, Baldwin, 2010</marker>
<rawString>Jey Han Lau, Paul Cook, Diana Mccarthy, David Newman, and Timothy Baldwin. 2010. Word Sense Induction for Novel Sense Detection. In Proceedings of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Els Lefever</author>
<author>Veronique Hoste</author>
</authors>
<title>SemEval2010 Task 3: Cross-lingual Word Sense Disambiguation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation.</booktitle>
<contexts>
<context position="2015" citStr="Lefever and Hoste, 2010" startWordPosition="301" endWordPosition="304">was adressed in Carpuat and Wu (2007b) by extending word sense disambiguation to phrase sense disambiguation. The relation between word sense distinctions and translation has also been explored in past SemEval tasks on cross-lingual word sense disambiguation, where senses are not This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http: //creativecommons.org/licenses/by/4.0/ defined in terms of WordNet senses as in previous tasks, but instead as translations to another language (Lefever and Hoste, 2010; Lefever and Hoste, 2013). This year’s L2 writing assistant task is similar to the cross-lingual word sense disambiguation task but differs in the context provided for disambiguation and the length of the fragments (source phrases instead of words). While in other translation and disambiguation tasks the source language context is given, the L2 writing assistant task assumes a given target language context that constrains the possible translations of L1 fragments. This is interesting from a machine translation point-of-view because it allows for a direct comparison with systems that exploit t</context>
</contexts>
<marker>Lefever, Hoste, 2010</marker>
<rawString>Els Lefever and Veronique Hoste. 2010. SemEval2010 Task 3: Cross-lingual Word Sense Disambiguation. In Proceedings of the 5th International Workshop on Semantic Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Els Lefever</author>
<author>Veronique Hoste</author>
</authors>
<title>SemEval2013 Task 10: Cross-lingual Word Sense Disambiguation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation, in Conjunction with the Second Joint Conference on Lexical and Computational Semantics.</booktitle>
<contexts>
<context position="2041" citStr="Lefever and Hoste, 2013" startWordPosition="305" endWordPosition="308">nd Wu (2007b) by extending word sense disambiguation to phrase sense disambiguation. The relation between word sense distinctions and translation has also been explored in past SemEval tasks on cross-lingual word sense disambiguation, where senses are not This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http: //creativecommons.org/licenses/by/4.0/ defined in terms of WordNet senses as in previous tasks, but instead as translations to another language (Lefever and Hoste, 2010; Lefever and Hoste, 2013). This year’s L2 writing assistant task is similar to the cross-lingual word sense disambiguation task but differs in the context provided for disambiguation and the length of the fragments (source phrases instead of words). While in other translation and disambiguation tasks the source language context is given, the L2 writing assistant task assumes a given target language context that constrains the possible translations of L1 fragments. This is interesting from a machine translation point-of-view because it allows for a direct comparison with systems that exploit the target context using a </context>
</contexts>
<marker>Lefever, Hoste, 2013</marker>
<rawString>Els Lefever and Veronique Hoste. 2013. SemEval2013 Task 10: Cross-lingual Word Sense Disambiguation. In Proceedings of the 7th International Workshop on Semantic Evaluation, in Conjunction with the Second Joint Conference on Lexical and Computational Semantics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>