<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.123981">
<title confidence="0.961737">
Improvements to Monolingual English Word Sense Disambiguation∗
</title>
<author confidence="0.997425">
Weiwei Guo
</author>
<affiliation confidence="0.9957675">
Computer Science Department
Columbia University
</affiliation>
<address confidence="0.978659">
New York, NY, 10115, USA
</address>
<email confidence="0.998679">
wg2162@cs.columbia.edu
</email>
<author confidence="0.983107">
Mona T. Diab
</author>
<affiliation confidence="0.994131">
Center for Computational Learning Systems
Columbia University
</affiliation>
<address confidence="0.976373">
New York, NY 10115, USA
</address>
<email confidence="0.999554">
mdiab@ccls.columbia.edu
</email>
<sectionHeader confidence="0.993839" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.994384222222222">
Word Sense Disambiguation remains one of
the most complex problems facing compu-
tational linguists to date. In this paper we
present modification to the graph based state
of the art algorithm In-Degree. Our modifi-
cations entail augmenting the basic Lesk sim-
ilarity measure with more relations based on
the structure of WordNet, adding SemCor ex-
amples to the basic WordNet lexical resource
and finally instead of using the LCH similarity
measure for computing verb verb similarity in
the In-Degree algorithm, we use JCN. We re-
port results on three standard data sets using
three different versions of WordNet. We re-
port the highest performing monolingual un-
supervised results to date on the Senseval 2 all
words data set. Our system yields a perfor-
mance of 62.7% using WordNet 1.7.1.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999721694444444">
Despite the advances in natural language process-
ing (NLP), Word Sense Disambiguation (WSD) is
still considered one of the most challenging prob-
lems in the field. Ever since the field’s inception,
WSD has been perceived as one of the central prob-
lems in NLP as an enabling technology that could
potentially have far reaching impact on NLP appli-
cations in general. We are starting to see the be-
ginnings of a positive effect of WSD in NLP appli-
cations such as Machine Translation (Carpuat and
Wu, 2007; Chan et al., 2007). Advances in re-
search on WSD in the current millennium can be
attributed to several key factors: the availability of
large scale computational lexical resources such as
∗The second author has been partially funded by DARPA
GALE project. We would also like to thank the useful com-
ments rendered by three anonymous reviewers.
WordNets (Fellbaum, 1998; Miller, 1990), the avail-
ability of large scale corpora, the existence and dis-
semination of standardized data sets over the past 10
years through the different test beds of SENSEVAL
and SEMEVAL competitions,1 devising more robust
computing algorithms to handle large scale data sets,
and simply advancement in hardware machinery.
In this paper, we address the problem of WSD of
all the content words in a sentence. In this frame-
work, the task is to associate all tokens with their
contextually relevant meaning definitions from some
computational lexical resource. We present an en-
hancement on an existing graph based algorithm, In-
Degree, as described in (Sinha and Mihalcea, 2007).
Like the previous work, our algorithm is unsuper-
vised. We show significant improvements over pre-
vious state of the art performance on several exist-
ing data sets, SENSEVAL2, SENSEVAL3 and SE-
MEVAL.
</bodyText>
<sectionHeader confidence="0.9599" genericHeader="method">
2 Word Sense Disambiguation
</sectionHeader>
<bodyText confidence="0.99801275">
The definition of WSD has taken on several different
meanings in recent years. In the latest SEMEVAL
(2007) workshop, there were 18 tasks defined, sev-
eral of which were on different languages, however
we notably recognize the widening of the defini-
tion of the task of WSD. In addition to the tradi-
tional all words and lexical sample tasks, we note
new tasks on word sense discrimination (no sense
inventory is needed, the different senses are merely
distinguished), lexical substitution using synonyms
of words as substitutes, as well as meaning defini-
tions obtained from different languages namely us-
ing words in translation.
Our paper is about the classical all words task of
WSD. In this task, all the content bearing words in a
running text are disambiguated from a static lexical
</bodyText>
<footnote confidence="0.9983">
1http://www.semeval.org
</footnote>
<page confidence="0.985955">
64
</page>
<note confidence="0.8170595">
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 64–69,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.99987025">
resource. For example a sentence such as I walked
by the bank and saw many beautiful plants there.
will have the verbs walked, saw, the nouns bank,
plants, the adjectives many, beautiful, and the ad-
verb there, be disambiguated from a standard lexi-
cal resource. Hence using WordNet,2 walked will be
assigned the meaning to use one’s feet to advance;
advance by steps, saw will be assigned the meaning
to perceive by sight or have the power to perceive
by sight, the noun bank will be assigned the mean-
ing sloping land especially the slope beside a body
of water and so on.
</bodyText>
<sectionHeader confidence="0.99987" genericHeader="method">
3 Related Works
</sectionHeader>
<bodyText confidence="0.99975725">
Many systems over the years have been used for the
task. A thorough review of the current state of the
art is in (Navigli, 2009). Several techniques have
been used to tackle the problem ranging from rule
based/knowledge based approaches to unsupervised
and supervised machine learning approaches. To
date, the best approaches that solve the all words
WSD task are supervised as illustrated in the dif-
ferent SenseEval and SEMEVAL All Words tasks
(M. Palmer and Dang, 2001; Snyder and Palmer,
2004; Pradhan et al., 2007).
In this paper, we present an unsupervised ap-
proach to the all words WSD problem relying on
WordNet similarity measures. We will review only
three of the most relevant related research due to
space limitations. We acknowledge the existence of
many research papers that tackled the problem using
unsupervised approaches.
Firstly, in work by (Pedersen and Patwardhan,
2005), the authors investigate different word simi-
larity measures as a means of disambiguating words
in context. They compare among different similar-
ity measures. They show that using an extension on
the Lesk similarity measure (Lesk, 1986) between
the target words and their contexts and the contexts
of those of the WordNet synset entries (Gloss Over-
lap), outperforms all the other similarity measures.
Their approach is unsupervised. They exploit the
different relations in WordNet. They also go beyond
the single word overlap, they calculate the overlap
in n-grams. They report results on the English Lex-
ical sample task from Senseval 2 which comprised
</bodyText>
<footnote confidence="0.732373">
2http://wordnet.princeton.edu
</footnote>
<bodyText confidence="0.999808229166667">
nouns, verbs and adjectives. The majority of the
words in this set is polysemous. They achieve an
F-measure of 41.2% on nouns, 21.2% on verbs, and
25.1% on adjectives.
The second related work to ours is the work by
(Mihalcea, 2005). Mihalcea (2005) introduced a
graph based unsupervised technique for all word
sense disambiguation. Similar to the previous study,
the author relied on the similarity of the WordNet
entry glosses using the Lesk similarity measure. The
study introduces a graph based sequence model of
the problem. All the open class words in a sentence
are linked via an undirected graph where all the pos-
sible senses are listed. Then dependency links are
drawn between all the sense pairs. Weights on the
arcs are determined based on the semantic similar-
ity using the Lesk measure. The algorithm is basi-
cally to walk the graph and find the links with the
highest possible weights deciding on the appropri-
ate sense for the target words in question. This algo-
rithm yields an overall F-score of 54.2% on the Sen-
seval 2 all words data set and an F-score of 64.2%
on nouns alone.
Finally, the closest study relevant to the current
paper yields state of the art performance is an un-
supervised approach described in (Sinha and Mihal-
cea, 2007). In this work, the authors combine dif-
ferent semantic similarity measures with different
graph based algorithms as an extension to work in
(Mihalcea, 2005). The authors proposed a graph-
based WSD algorithm. Given a sequence of words
W = {w1, w2...wn}, each word wi with several
senses {si1, si2...sim}. A graph G = (V,E) is defined
such that there exists a vertex v for each sense. Two
senses of two different words may be connected by
an edge e, depending on their distance. That two
senses are connected suggests they should have in-
fluence on each other, so normally a maximum al-
lowable distance is set. They explore 4 different
graph based algorithms. The highest yielding algo-
rithm in their work is the In-Degree algorithm
combining different WordNet similarity measures
depending on POS. They used the Jiang and Conrath
(JCN) (Jiang and Conrath., 1997) similarity mea-
sure within nouns, the Leacock &amp; Chodorow (LCH)
(Leacock and Chodorow, 1998) similarity measure
within verbs, and the Lesk (Lesk, 1986) similarity
measure within adjectives and within adverbs and
</bodyText>
<page confidence="0.99789">
65
</page>
<bodyText confidence="0.999951285714286">
across different POS tags. They evaluate their work
against the Senseval 2 all words task. They tune the
parameters of their algorithm – specifically the nor-
malization ratio for some of these measures —based
on the Senseval 3 data set. They report a state of the
art unsupervised system that yields an overall per-
formance of 57.2%.
</bodyText>
<sectionHeader confidence="0.974099" genericHeader="method">
4 Our Approach
</sectionHeader>
<bodyText confidence="0.998860361111111">
In this paper, we extend the (Sinha and Mihalcea,
2007) work (hence forth SM07) in some interesting
ways. We focus on the In-Degree graph based
algorithm as it was the best performer in the SM07
work. The In-Degree algorithm presents the prob-
lem as a weighted graph with senses as nodes and
similarity between senses as weights on edges. The
In-Degree of a vertex refers to the number of edges
incident on that vertex. In the weighted graph, the
In-Degree for each vertex is calculated by summing
the weights on the edges that are incident on it.
After all the In-Degree values for each sense is
computed, the sense with maximum value is cho-
sen as the final sense for that word. SM07 com-
bine different similarity measures. They show that
best combination is JCN for noun pairs and LCH
for verb pairs, and Lesk for within adjectives and
within adverbs and also across different POS, for ex-
ample comparing senses of verbs and nouns. Since
different similarity measures use different similarity
scales, SM07 did not directly use the value returned
from the similarity metrics. Instead, the values were
normalized. Lesk value is observed in a range from
0 to an arbitrary value, so values larger than 240
were set to 1, and the rest is mapped to an interval
[0,1]. Similarily JCN and LCH were normalized to
the interval from [0,1].3
In this paper, we use the basic In-Degree algo-
rithm while applying some modifications to the ba-
sic similarity measures exploited and the WordNet
lexical resource. Similar to the original In-Degree
algorithm, we produce a probabilistic ranked list of
senses. Our modifications are described as follows:
JCN for Verb-Verb Similarity In our implemen-
tation of the In-Degree algorithm, we use the JCN
similarity measure for both Noun-Noun similarity
</bodyText>
<footnote confidence="0.8331315">
3These values were decided on based on calibrations on the
SENSEVAL 3 data set.
</footnote>
<bodyText confidence="0.999923340909091">
calculation similar to SM07. In addition, instead of
using LCH for Verb-Verb similarity, we use JCN for
Verb Verb similarity based on our empirical obser-
vation on SENSEVAL 3 data, JCN yields better per-
formance than when employing LCH among verbs.
Expand Lesk Following the intuition in (Peder-
sen and Patwardhan, 2005) – henceforth (PEA05)
– we expand the basic Lesk similarity measure to
take into account the glosses for all the relations
for the synsets on the contextual words and com-
pare them with the glosses of the target word senses,
hence going beyond the is-a relation. The idea is
based on the observation that WordNet senses are
too fine-grained, therefore the neighbors share a lot
of semantic meanings. To find similar senses, we
use the relations: hypernym, hyponym, similar at-
tributes, similar verb group, pertinym, holonym, and
meronyms.4 The algorithm assumes that the words
in the input are POS tagged. It is worth noting the
differences between our algorithm and the PEA05
algorithm, though we take our cue from it. In
PEA05, the authors retrieve all the relevant neigh-
bors to form a large bag of words for both the target
sense and the surrounding sense and they specifi-
cally focus on the Lesk similarity measure. In our
current work, we employ the neighbors in a dis-
ambiguation strategy using different similarity mea-
sures one pair at a time.
This algorithm takes as input a target sense and a
sense pertaining to a word in the surrounding con-
text, and returns a sense similarity score. It is worth
noting that we do not apply the WN relations ex-
pansion to the target sense. It is only applied to the
contextual word. We experimented with expanding
both the contextual sense and the target sense and
we found that the unreliability of some of the rela-
tions is detrimental to the algorithm’s performance.
Hence we decided empirically to expand only the
contextual word.
We employ the same normalization values used in
SM07 for the different similarity measures. Namely
for the Lesk and Expand-Lesk we use the same cut
off value of 240, accordingly, if the Lesk or Expand-
Lesk similarity value returns 0 &lt;= 240 it is con-
</bodyText>
<footnote confidence="0.959111333333333">
4We have run experiments varying the number of relations to
employ and they all yielded relatively similar results. Hence in
this paper, we report results using all the relations listed above.
</footnote>
<page confidence="0.993748">
66
</page>
<bodyText confidence="0.999970703703703">
verted to a real number in the interval [0,1], any sim-
ilarity over 240 is by default mapped to a 1. For
JCN, similar to SM07, the values are from 0.04 to
0.2, we mapped them to the interval [0,1]. It is worth
noting that we did not run any calibration studies be-
yond the what was reported in SM07.
SemCor Expansion of WordNet A basic part of
our approach relies on using the Lesk algorithm. Ac-
cordingly, the availability of glosses associated with
the WordNet entries is extremely beneficial. There-
fore, we expand the number of glosses available
in WordNet by using the SemCor data set, thereby
adding more examples to compare. The SemCor
corpus is a corpus that is manually sense tagged
(Miller, 1990). In this expansion, depending on the
version of WordNet, we use the sense-index file in
the WordNet Database to convert the SemCor data to
the appropriate version sense annotations. We aug-
ment the sense entries for the different POS Word-
Net databases with example usages from SemCor.
The augmentation is done as a look up table external
to WordNet proper since we did not want to dabble
with the WordNet offsets. We set a cap of 30 addi-
tional examples per synset. Many of the synsets had
no additional examples. A total of 26875 synsets in
WordNet 1.7.1 and a total of 25940 synsets are aug-
mented with SemCor examples.5
</bodyText>
<sectionHeader confidence="0.997061" genericHeader="method">
5 Experiments and Results
</sectionHeader>
<subsectionHeader confidence="0.899217">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.999838357142857">
We experiment with all the standard data sets,
namely, Senseval 2 (SV2) (M. Palmer and Dang,
2001), Senseval 3 (SV3) (Snyder and Palmer, 2004),
and SEMEVAL (SM) (Pradhan et al., 2007) English
All Words data sets. We used the true POS tag sets
in the test data as rendered in the Penn Tree Bank.
We exclude the data points that have a tag of ”U”
in the gold standard since our system does not al-
low for an unknown option (i.e. it has to produce a
sense tag). We present our results on 3 versions of
WordNet (WN), 1.7.1 for ease of comparison with
previous systems, 2.1 for SEMEVAL data, and 3.0
in order to see whether the trends in performance
hold across WN versions.
</bodyText>
<footnote confidence="0.978198333333333">
5It is worth noting that some example sentences are repeated
across different synsets and POS since the SemCor data is an-
notated as an All-Words tagged data set.
</footnote>
<subsectionHeader confidence="0.99033">
5.2 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999862333333333">
We use the scorer2 software to report fine-
grained (P)recision and (R)ecall and (F)-measure on
the different data sets.
</bodyText>
<subsectionHeader confidence="0.994345">
5.3 Baselines
</subsectionHeader>
<bodyText confidence="0.995810363636364">
We consider here the two different baselines. 1. A
random baseline (RAND) is the most appropriate
baseline for an unsupervised approach. We consider
the first sense baseline to be a supervised baseline
since it depends crucially on SemCor in ranking the
senses within WordNet.6 It is worth pointing out that
our algorithm is still an unsupervised algorithm even
though we use SemCor to augment WordNet since
we do not use any annotated data in our algorithm
proper. 2. The SM07 baseline which we consider
our true baseline.
</bodyText>
<subsectionHeader confidence="0.924072">
5.4 Experimental Conditions
</subsectionHeader>
<bodyText confidence="0.999961">
We explore 4 different experimental conditions:
JCN-V which uses JCN instead of LCH for verb-
verb similarity comparison, we consider this our
base condition; +ExpandL is adding the Lesk Ex-
pansion to the base condition; +SemCor adds the
SemCor expansion to the base condition; and finally
+ExpandL SemCor, adds the latter both conditions
simultaneously.
</bodyText>
<sectionHeader confidence="0.538304" genericHeader="evaluation">
5.5 Results
</sectionHeader>
<bodyText confidence="0.998812">
Table 1 illustrates the obtained results on the three
data sets reporting only overall F-measure. The cov-
erage for SV2 is 98.36% losing some of the verb and
adverb target words. The coverage for SV3 is 99.7%
and that of SM is 100%. These results are on the
entire data set as described in Table ??. Moreover,
Table 2 presents the detailed results for the Senseval
2 data set using WN 1.7.1 since it is the most stud-
ied data set and for ease of comparison with previ-
ous studies. We break the results down by POS tag
(N)oun, (V)erb, (A)djective, and Adve(R)b.
</bodyText>
<footnote confidence="0.993788333333333">
6From an application standpoint, we do not find the first
sense baseline to be of interest since it introduces a strong level
of uniformity – removing semantic variability – that is not de-
sirable. Even if the frist sense achieves higher results in these
data sets, it is an artifact of the size of the data and the very
limited number of documents under investigation.
</footnote>
<page confidence="0.996711">
67
</page>
<table confidence="0.999975142857143">
Condition SV2-WN171 SV2-WN30 SV3-WN171 SV3-WN30 SM-WN2.1 SM-WN30
RAND 39.9 41.8 32.9 33.4 25.4
SM07 59.7 59.8 54 53.8 40.4 40.8
JCN-V 60.2 60.2 55.9 55.5 44.1 45.5
+ExpandL 60.9 60.6 55.7 55.5 43.7 45.1
+SemCor 62.04 62.2 59.7 60.3 46.8 46.8
+ExpandL SemCor 62.7 62.9 59.5 59.6 45.9 45.7
</table>
<tableCaption confidence="0.995614">
Table 1: F-measure % for all experimental conditions on all data sets
</tableCaption>
<table confidence="0.999945285714286">
Condition N V A R
RAND 43.7 21 41.2 57.4
SM07 68.7 33.01 65.2 63.1
JCN-V 68.7 35.46 65.2 63.1
+ExpandL 70 35.86 65.6 62.8
+SemCor 68.3 37.86 68.6 68.75
+ExpandL SemCor 69.5 38.66 68.2 69.15
</table>
<tableCaption confidence="0.995331">
Table 2: F-measure results per POS tag per condition for SV2 using WN 1.7.1.
</tableCaption>
<sectionHeader confidence="0.997328" genericHeader="discussions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999969725806452">
Our overall results on all the data sets clearly out-
perform the baseline as well as state of the art per-
formance using an unsupervised system (SM07) in
overall accuracy across all the data sets. Our im-
plementation of SM07 is slightly higher than those
reported in (Sinha and Mihalcea, 2007), 57.12% is
probably due to the fact that we do not consider the
items tagged as ”U” and also we resolve some of
the POS tag mismatches between the gold set and
the test data. We note that for the SV2 data set our
coverage is not 100% due to some POS tag mis-
matches that could not have been resolved automat-
ically. These POS tag problems have to do mainly
with multiword expressions and the like.
In observing the performance of the overall sys-
tem, we note that using JCN for verbs clearly outper-
forms using the LCH similarity measure across the
board on all data sets as illustrated in Table 1. Us-
ing SemCor to augment WordNet examples seems to
have the biggest impact on SV3 and SM compared
to ExpandL. This may be attributed to the fact that
the percentage of polysemous words in the latter two
sets is much higher than it is for SV2. Combining
SemCor with ExpandL yields the best results for the
SV2 data sets. There seems to be no huge notable
difference between the three versions of WN, though
WN3.0 seems to yield slightly higher results maybe
due to higher consistency in the overall structure
when comparing WN1.7.1, WN2.1, and WN3.0. We
do recognize that we can’t directly compare the var-
ious WordNets except to draw conclusions on struc-
tural differences remotely. It is also worth noting
that less words in WN3.0 used SemCor expansions.
Observing the results yielded per POS in Table
2, ExpandL seems to have the biggest impact on
the Nouns only. This is understandable since the
nouns hierachy has the most dense relations and the
most consistent ones. SemCor augmentation of WN
seemed to benefit all POS significantly except for
nouns. In fact the performance on the nouns deteri-
orated from the base condition JCN-V from 68.7 to
68.3%. This maybe due to inconsistencies in the an-
notations of nouns in SemCor or the very fine granu-
larity of the nouns in WN. We know that 72% of the
nouns, 74% of the verbs, 68.9% of the adjectives,
and 81.9% of the adverbs directly exploited the use
of SemCor augmented examples. Combining Sem-
Cor and ExpandL seems to have a positive impact
on the verbs and adverbs, but not on the nouns and
adjectives. These trends are not held consistently
across data sets. For example, we see that SemCor
augmentation helps both all POS tag sets over using
ExpandL alone or when combined with SemCor. In
order to analyze this further, we explore the perfor-
mance on the polysemous POS only in all the data
sets. We note that the same trend persists, SemCor
augmentation has a negative impact on the SV2 data
set in both WN 1.7.1. and WN 3.0. yet it benefits
all POS in the other data sets, namely SV3 WN1.7.1
and SV3 WN3.0, SM WN2.1 and SM WN3.0.
We did some basic data analysis on the items we
are incapable of capturing. Several of them are cases
</bodyText>
<page confidence="0.997926">
68
</page>
<bodyText confidence="0.999918">
of metonymy in examples such as ”the English are
known...”, the sense of English here is clearly in ref-
erence to the people of England, however, our WSD
system preferred the language sense of the word. If
it had access to syntactic/semantic role we would as-
sume it could capture that this sense of the word
entails volition for example. Other types of errors
resulted from the lack of a method to help identify
multiwords.
</bodyText>
<sectionHeader confidence="0.996964" genericHeader="conclusions">
7 Conclusions and Future Directions
</sectionHeader>
<bodyText confidence="0.999959473684211">
In this paper, we presented improvements on state
of the art monolingual all words WSD using a well
established graph based algorithm coupled with en-
hancements on basic similarity measures. We also
explored the impact of augmenting WordNet with
more gloss examples from a hand annotated re-
source as a means of improving WSD performance.
We present the best results to date for an unsuper-
vised approach on standard data sets: Senseval 2
(62.7%) using WN1.7.1, and Senseval 3 (59.7%) us-
ing WN1.7.1. In the future, we would like to explore
the incorporation of multiword chunks, document
level lexical chains, and syntactic features in the
modeling of the Lesk overlap measure. We would
like to further explore why ExpandL conditions did
not yield the expected high performance across the
different POS tags. Moreover, we are still curious
as to why SemCor expansion did not help the nouns
performance in SV2 conditions specifically.
</bodyText>
<sectionHeader confidence="0.99854" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999753117647059">
Marine Carpuat and Dekai Wu. 2007. Improving statisti-
cal machine translation using word sense disambigua-
tion. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 61–72, Prague, Czech Re-
public, June. Association for Computational Linguis-
tics.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguis-
tics, pages 33–40, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Christiane Fellbaum. 1998. ”wordnet: An electronic lex-
ical database”. MIT Press.
J. Jiang and D. Conrath. 1997. Semantic similarity based
on corpus statistics and lexical taxonomy. In Proceed-
ings of the International Conference on Research in
Computational Linguistics, Taiwan.
C. Leacock and M. Chodorow. 1998. Combining lo-
cal context and wordnet sense similarity for word
sense identification. In WordNet, An Electronic Lex-
ical Database. The MIT Press.
M. Lesk. 1986. Automatic sense disambiguation using
machine readable dictionaries: How to tell a pine cone
from an ice cream cone. In In Proceedings of the SIG-
DOC Conference, Toronto, June.
S. Cotton L. Delfs M. Palmer, C. Fellbaum and H. Dang.
2001. English tasks: all-words and verb lexical sam-
ple. In In Proceedings of ACL/SIGLEX Senseval-2,
Toulouse, France, June.
Rada Mihalcea. 2005. Unsupervised large-vocabulary
word sense disambiguation with graph-based algo-
rithms for sequence data labeling. In Proceedings of
Human Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 411–418, Vancouver, British Columbia,
Canada, October. Association for Computational Lin-
guistics.
George A. Miller. 1990. Wordnet: a lexical database for
english. In Communications of the ACM, pages 39–41.
Roberto Navigli. 2009. Word sense disambiguation:
a survey. In ACM Computing Surveys, pages 1–69.
ACM Press.
Banerjee Pedersen and Patwardhan. 2005. Maximiz-
ing semantic relatedness to perform word sense disam-
biguation. In University ofMinnesota Supercomputing
Institute Research Report UMSI 2005/25, Minnesotta,
March.
Sameer Pradhan, Edward Loper, Dmitriy Dligach, and
Martha Palmer. 2007. Semeval-2007 task-17: En-
glish lexical sample, srl and all words. In Proceed-
ings of the Fourth International Workshop on Seman-
tic Evaluations (SemEval-2007), pages 87–92, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Ravi Sinha and Rada Mihalcea. 2007. Unsupervised
graph-based word sense disambiguation using mea-
sures of word semantic similarity. In Proceedings of
the IEEE International Conference on Semantic Com-
puting (ICSC 2007), Irvine, CA.
Benjamin Snyder and Martha Palmer. 2004. The english
all-words task. In Rada Mihalcea and Phil Edmonds,
editors, Senseval-3: Third International Workshop on
the Evaluation of Systems for the Semantic Analysis of
Text, pages 41–43, Barcelona, Spain, July. Association
for Computational Linguistics.
</reference>
<page confidence="0.999317">
69
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.544844">
<title confidence="0.996847">to Monolingual English Word Sense</title>
<author confidence="0.978774">Weiwei</author>
<affiliation confidence="0.999787">Computer Science</affiliation>
<address confidence="0.855141">Columbia New York, NY, 10115,</address>
<email confidence="0.99326">wg2162@cs.columbia.edu</email>
<author confidence="0.996728">T Mona</author>
<affiliation confidence="0.999443">Center for Computational Learning</affiliation>
<address confidence="0.927035">Columbia New York, NY 10115,</address>
<email confidence="0.999678">mdiab@ccls.columbia.edu</email>
<abstract confidence="0.995588684210526">Word Sense Disambiguation remains one of the most complex problems facing computational linguists to date. In this paper we present modification to the graph based state of the art algorithm In-Degree. Our modifications entail augmenting the basic Lesk similarity measure with more relations based on the structure of WordNet, adding SemCor examples to the basic WordNet lexical resource and finally instead of using the LCH similarity measure for computing verb verb similarity in the In-Degree algorithm, we use JCN. We report results on three standard data sets using three different versions of WordNet. We report the highest performing monolingual unsupervised results to date on the Senseval 2 all words data set. Our system yields a performance of 62.7% using WordNet 1.7.1.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Dekai Wu</author>
</authors>
<title>Improving statistical machine translation using word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>61--72</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1597" citStr="Carpuat and Wu, 2007" startWordPosition="251" endWordPosition="254">o date on the Senseval 2 all words data set. Our system yields a performance of 62.7% using WordNet 1.7.1. 1 Introduction Despite the advances in natural language processing (NLP), Word Sense Disambiguation (WSD) is still considered one of the most challenging problems in the field. Ever since the field’s inception, WSD has been perceived as one of the central problems in NLP as an enabling technology that could potentially have far reaching impact on NLP applications in general. We are starting to see the beginnings of a positive effect of WSD in NLP applications such as Machine Translation (Carpuat and Wu, 2007; Chan et al., 2007). Advances in research on WSD in the current millennium can be attributed to several key factors: the availability of large scale computational lexical resources such as ∗The second author has been partially funded by DARPA GALE project. We would also like to thank the useful comments rendered by three anonymous reviewers. WordNets (Fellbaum, 1998; Miller, 1990), the availability of large scale corpora, the existence and dissemination of standardized data sets over the past 10 years through the different test beds of SENSEVAL and SEMEVAL competitions,1 devising more robust </context>
</contexts>
<marker>Carpuat, Wu, 2007</marker>
<rawString>Marine Carpuat and Dekai Wu. 2007. Improving statistical machine translation using word sense disambiguation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 61–72, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
<author>David Chiang</author>
</authors>
<title>Word sense disambiguation improves statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>33--40</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1617" citStr="Chan et al., 2007" startWordPosition="255" endWordPosition="258"> 2 all words data set. Our system yields a performance of 62.7% using WordNet 1.7.1. 1 Introduction Despite the advances in natural language processing (NLP), Word Sense Disambiguation (WSD) is still considered one of the most challenging problems in the field. Ever since the field’s inception, WSD has been perceived as one of the central problems in NLP as an enabling technology that could potentially have far reaching impact on NLP applications in general. We are starting to see the beginnings of a positive effect of WSD in NLP applications such as Machine Translation (Carpuat and Wu, 2007; Chan et al., 2007). Advances in research on WSD in the current millennium can be attributed to several key factors: the availability of large scale computational lexical resources such as ∗The second author has been partially funded by DARPA GALE project. We would also like to thank the useful comments rendered by three anonymous reviewers. WordNets (Fellbaum, 1998; Miller, 1990), the availability of large scale corpora, the existence and dissemination of standardized data sets over the past 10 years through the different test beds of SENSEVAL and SEMEVAL competitions,1 devising more robust computing algorithms</context>
</contexts>
<marker>Chan, Ng, Chiang, 2007</marker>
<rawString>Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007. Word sense disambiguation improves statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 33–40, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>wordnet: An electronic lexical database”.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1966" citStr="Fellbaum, 1998" startWordPosition="313" endWordPosition="314">NLP as an enabling technology that could potentially have far reaching impact on NLP applications in general. We are starting to see the beginnings of a positive effect of WSD in NLP applications such as Machine Translation (Carpuat and Wu, 2007; Chan et al., 2007). Advances in research on WSD in the current millennium can be attributed to several key factors: the availability of large scale computational lexical resources such as ∗The second author has been partially funded by DARPA GALE project. We would also like to thank the useful comments rendered by three anonymous reviewers. WordNets (Fellbaum, 1998; Miller, 1990), the availability of large scale corpora, the existence and dissemination of standardized data sets over the past 10 years through the different test beds of SENSEVAL and SEMEVAL competitions,1 devising more robust computing algorithms to handle large scale data sets, and simply advancement in hardware machinery. In this paper, we address the problem of WSD of all the content words in a sentence. In this framework, the task is to associate all tokens with their contextually relevant meaning definitions from some computational lexical resource. We present an enhancement on an ex</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. ”wordnet: An electronic lexical database”. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Jiang</author>
<author>D Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In Proceedings of the International Conference on Research in Computational Linguistics,</booktitle>
<marker>Jiang, Conrath, 1997</marker>
<rawString>J. Jiang and D. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proceedings of the International Conference on Research in Computational Linguistics, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Leacock</author>
<author>M Chodorow</author>
</authors>
<title>Combining local context and wordnet sense similarity for word sense identification. In WordNet, An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="8243" citStr="Leacock and Chodorow, 1998" startWordPosition="1342" endWordPosition="1345">is defined such that there exists a vertex v for each sense. Two senses of two different words may be connected by an edge e, depending on their distance. That two senses are connected suggests they should have influence on each other, so normally a maximum allowable distance is set. They explore 4 different graph based algorithms. The highest yielding algorithm in their work is the In-Degree algorithm combining different WordNet similarity measures depending on POS. They used the Jiang and Conrath (JCN) (Jiang and Conrath., 1997) similarity measure within nouns, the Leacock &amp; Chodorow (LCH) (Leacock and Chodorow, 1998) similarity measure within verbs, and the Lesk (Lesk, 1986) similarity measure within adjectives and within adverbs and 65 across different POS tags. They evaluate their work against the Senseval 2 all words task. They tune the parameters of their algorithm – specifically the normalization ratio for some of these measures —based on the Senseval 3 data set. They report a state of the art unsupervised system that yields an overall performance of 57.2%. 4 Our Approach In this paper, we extend the (Sinha and Mihalcea, 2007) work (hence forth SM07) in some interesting ways. We focus on the In-Degre</context>
</contexts>
<marker>Leacock, Chodorow, 1998</marker>
<rawString>C. Leacock and M. Chodorow. 1998. Combining local context and wordnet sense similarity for word sense identification. In WordNet, An Electronic Lexical Database. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lesk</author>
</authors>
<title>Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from an ice cream cone. In</title>
<date>1986</date>
<booktitle>In Proceedings of the SIGDOC Conference,</booktitle>
<location>Toronto,</location>
<contexts>
<context position="5597" citStr="Lesk, 1986" startWordPosition="902" endWordPosition="903">007). In this paper, we present an unsupervised approach to the all words WSD problem relying on WordNet similarity measures. We will review only three of the most relevant related research due to space limitations. We acknowledge the existence of many research papers that tackled the problem using unsupervised approaches. Firstly, in work by (Pedersen and Patwardhan, 2005), the authors investigate different word similarity measures as a means of disambiguating words in context. They compare among different similarity measures. They show that using an extension on the Lesk similarity measure (Lesk, 1986) between the target words and their contexts and the contexts of those of the WordNet synset entries (Gloss Overlap), outperforms all the other similarity measures. Their approach is unsupervised. They exploit the different relations in WordNet. They also go beyond the single word overlap, they calculate the overlap in n-grams. They report results on the English Lexical sample task from Senseval 2 which comprised 2http://wordnet.princeton.edu nouns, verbs and adjectives. The majority of the words in this set is polysemous. They achieve an F-measure of 41.2% on nouns, 21.2% on verbs, and 25.1% </context>
<context position="8302" citStr="Lesk, 1986" startWordPosition="1353" endWordPosition="1354">two different words may be connected by an edge e, depending on their distance. That two senses are connected suggests they should have influence on each other, so normally a maximum allowable distance is set. They explore 4 different graph based algorithms. The highest yielding algorithm in their work is the In-Degree algorithm combining different WordNet similarity measures depending on POS. They used the Jiang and Conrath (JCN) (Jiang and Conrath., 1997) similarity measure within nouns, the Leacock &amp; Chodorow (LCH) (Leacock and Chodorow, 1998) similarity measure within verbs, and the Lesk (Lesk, 1986) similarity measure within adjectives and within adverbs and 65 across different POS tags. They evaluate their work against the Senseval 2 all words task. They tune the parameters of their algorithm – specifically the normalization ratio for some of these measures —based on the Senseval 3 data set. They report a state of the art unsupervised system that yields an overall performance of 57.2%. 4 Our Approach In this paper, we extend the (Sinha and Mihalcea, 2007) work (hence forth SM07) in some interesting ways. We focus on the In-Degree graph based algorithm as it was the best performer in the</context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>M. Lesk. 1986. Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from an ice cream cone. In In Proceedings of the SIGDOC Conference, Toronto, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Cotton L Delfs M Palmer</author>
<author>C Fellbaum</author>
<author>H Dang</author>
</authors>
<title>English tasks: all-words and verb lexical sample. In</title>
<date>2001</date>
<booktitle>In Proceedings of ACL/SIGLEX Senseval-2,</booktitle>
<location>Toulouse, France,</location>
<marker>Palmer, Fellbaum, Dang, 2001</marker>
<rawString>S. Cotton L. Delfs M. Palmer, C. Fellbaum and H. Dang. 2001. English tasks: all-words and verb lexical sample. In In Proceedings of ACL/SIGLEX Senseval-2, Toulouse, France, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
</authors>
<title>Unsupervised large-vocabulary word sense disambiguation with graph-based algorithms for sequence data labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>411--418</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="6275" citStr="Mihalcea, 2005" startWordPosition="1010" endWordPosition="1011">f those of the WordNet synset entries (Gloss Overlap), outperforms all the other similarity measures. Their approach is unsupervised. They exploit the different relations in WordNet. They also go beyond the single word overlap, they calculate the overlap in n-grams. They report results on the English Lexical sample task from Senseval 2 which comprised 2http://wordnet.princeton.edu nouns, verbs and adjectives. The majority of the words in this set is polysemous. They achieve an F-measure of 41.2% on nouns, 21.2% on verbs, and 25.1% on adjectives. The second related work to ours is the work by (Mihalcea, 2005). Mihalcea (2005) introduced a graph based unsupervised technique for all word sense disambiguation. Similar to the previous study, the author relied on the similarity of the WordNet entry glosses using the Lesk similarity measure. The study introduces a graph based sequence model of the problem. All the open class words in a sentence are linked via an undirected graph where all the possible senses are listed. Then dependency links are drawn between all the sense pairs. Weights on the arcs are determined based on the semantic similarity using the Lesk measure. The algorithm is basically to wal</context>
</contexts>
<marker>Mihalcea, 2005</marker>
<rawString>Rada Mihalcea. 2005. Unsupervised large-vocabulary word sense disambiguation with graph-based algorithms for sequence data labeling. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 411–418, Vancouver, British Columbia, Canada, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: a lexical database for english.</title>
<date>1990</date>
<booktitle>In Communications of the ACM,</booktitle>
<pages>39--41</pages>
<contexts>
<context position="1981" citStr="Miller, 1990" startWordPosition="315" endWordPosition="316">ng technology that could potentially have far reaching impact on NLP applications in general. We are starting to see the beginnings of a positive effect of WSD in NLP applications such as Machine Translation (Carpuat and Wu, 2007; Chan et al., 2007). Advances in research on WSD in the current millennium can be attributed to several key factors: the availability of large scale computational lexical resources such as ∗The second author has been partially funded by DARPA GALE project. We would also like to thank the useful comments rendered by three anonymous reviewers. WordNets (Fellbaum, 1998; Miller, 1990), the availability of large scale corpora, the existence and dissemination of standardized data sets over the past 10 years through the different test beds of SENSEVAL and SEMEVAL competitions,1 devising more robust computing algorithms to handle large scale data sets, and simply advancement in hardware machinery. In this paper, we address the problem of WSD of all the content words in a sentence. In this framework, the task is to associate all tokens with their contextually relevant meaning definitions from some computational lexical resource. We present an enhancement on an existing graph ba</context>
<context position="13603" citStr="Miller, 1990" startWordPosition="2262" endWordPosition="2263">ed to a 1. For JCN, similar to SM07, the values are from 0.04 to 0.2, we mapped them to the interval [0,1]. It is worth noting that we did not run any calibration studies beyond the what was reported in SM07. SemCor Expansion of WordNet A basic part of our approach relies on using the Lesk algorithm. Accordingly, the availability of glosses associated with the WordNet entries is extremely beneficial. Therefore, we expand the number of glosses available in WordNet by using the SemCor data set, thereby adding more examples to compare. The SemCor corpus is a corpus that is manually sense tagged (Miller, 1990). In this expansion, depending on the version of WordNet, we use the sense-index file in the WordNet Database to convert the SemCor data to the appropriate version sense annotations. We augment the sense entries for the different POS WordNet databases with example usages from SemCor. The augmentation is done as a look up table external to WordNet proper since we did not want to dabble with the WordNet offsets. We set a cap of 30 additional examples per synset. Many of the synsets had no additional examples. A total of 26875 synsets in WordNet 1.7.1 and a total of 25940 synsets are augmented wi</context>
</contexts>
<marker>Miller, 1990</marker>
<rawString>George A. Miller. 1990. Wordnet: a lexical database for english. In Communications of the ACM, pages 39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>Word sense disambiguation: a survey. In</title>
<date>2009</date>
<journal>ACM Computing Surveys,</journal>
<pages>1--69</pages>
<publisher>ACM Press.</publisher>
<contexts>
<context position="4600" citStr="Navigli, 2009" startWordPosition="747" endWordPosition="748">l have the verbs walked, saw, the nouns bank, plants, the adjectives many, beautiful, and the adverb there, be disambiguated from a standard lexical resource. Hence using WordNet,2 walked will be assigned the meaning to use one’s feet to advance; advance by steps, saw will be assigned the meaning to perceive by sight or have the power to perceive by sight, the noun bank will be assigned the meaning sloping land especially the slope beside a body of water and so on. 3 Related Works Many systems over the years have been used for the task. A thorough review of the current state of the art is in (Navigli, 2009). Several techniques have been used to tackle the problem ranging from rule based/knowledge based approaches to unsupervised and supervised machine learning approaches. To date, the best approaches that solve the all words WSD task are supervised as illustrated in the different SenseEval and SEMEVAL All Words tasks (M. Palmer and Dang, 2001; Snyder and Palmer, 2004; Pradhan et al., 2007). In this paper, we present an unsupervised approach to the all words WSD problem relying on WordNet similarity measures. We will review only three of the most relevant related research due to space limitations</context>
</contexts>
<marker>Navigli, 2009</marker>
<rawString>Roberto Navigli. 2009. Word sense disambiguation: a survey. In ACM Computing Surveys, pages 1–69. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Banerjee Pedersen</author>
<author>Patwardhan</author>
</authors>
<title>Maximizing semantic relatedness to perform word sense disambiguation.</title>
<date>2005</date>
<booktitle>In University ofMinnesota Supercomputing Institute Research Report UMSI 2005/25,</booktitle>
<location>Minnesotta,</location>
<contexts>
<context position="5362" citStr="Pedersen and Patwardhan, 2005" startWordPosition="864" endWordPosition="867">ervised machine learning approaches. To date, the best approaches that solve the all words WSD task are supervised as illustrated in the different SenseEval and SEMEVAL All Words tasks (M. Palmer and Dang, 2001; Snyder and Palmer, 2004; Pradhan et al., 2007). In this paper, we present an unsupervised approach to the all words WSD problem relying on WordNet similarity measures. We will review only three of the most relevant related research due to space limitations. We acknowledge the existence of many research papers that tackled the problem using unsupervised approaches. Firstly, in work by (Pedersen and Patwardhan, 2005), the authors investigate different word similarity measures as a means of disambiguating words in context. They compare among different similarity measures. They show that using an extension on the Lesk similarity measure (Lesk, 1986) between the target words and their contexts and the contexts of those of the WordNet synset entries (Gloss Overlap), outperforms all the other similarity measures. Their approach is unsupervised. They exploit the different relations in WordNet. They also go beyond the single word overlap, they calculate the overlap in n-grams. They report results on the English </context>
<context position="10884" citStr="Pedersen and Patwardhan, 2005" startWordPosition="1785" endWordPosition="1789">roduce a probabilistic ranked list of senses. Our modifications are described as follows: JCN for Verb-Verb Similarity In our implementation of the In-Degree algorithm, we use the JCN similarity measure for both Noun-Noun similarity 3These values were decided on based on calibrations on the SENSEVAL 3 data set. calculation similar to SM07. In addition, instead of using LCH for Verb-Verb similarity, we use JCN for Verb Verb similarity based on our empirical observation on SENSEVAL 3 data, JCN yields better performance than when employing LCH among verbs. Expand Lesk Following the intuition in (Pedersen and Patwardhan, 2005) – henceforth (PEA05) – we expand the basic Lesk similarity measure to take into account the glosses for all the relations for the synsets on the contextual words and compare them with the glosses of the target word senses, hence going beyond the is-a relation. The idea is based on the observation that WordNet senses are too fine-grained, therefore the neighbors share a lot of semantic meanings. To find similar senses, we use the relations: hypernym, hyponym, similar attributes, similar verb group, pertinym, holonym, and meronyms.4 The algorithm assumes that the words in the input are POS tagg</context>
</contexts>
<marker>Pedersen, Patwardhan, 2005</marker>
<rawString>Banerjee Pedersen and Patwardhan. 2005. Maximizing semantic relatedness to perform word sense disambiguation. In University ofMinnesota Supercomputing Institute Research Report UMSI 2005/25, Minnesotta, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Edward Loper</author>
<author>Dmitriy Dligach</author>
<author>Martha Palmer</author>
</authors>
<title>Semeval-2007 task-17: English lexical sample, srl and all words.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>87--92</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="4990" citStr="Pradhan et al., 2007" startWordPosition="806" endWordPosition="809">signed the meaning sloping land especially the slope beside a body of water and so on. 3 Related Works Many systems over the years have been used for the task. A thorough review of the current state of the art is in (Navigli, 2009). Several techniques have been used to tackle the problem ranging from rule based/knowledge based approaches to unsupervised and supervised machine learning approaches. To date, the best approaches that solve the all words WSD task are supervised as illustrated in the different SenseEval and SEMEVAL All Words tasks (M. Palmer and Dang, 2001; Snyder and Palmer, 2004; Pradhan et al., 2007). In this paper, we present an unsupervised approach to the all words WSD problem relying on WordNet similarity measures. We will review only three of the most relevant related research due to space limitations. We acknowledge the existence of many research papers that tackled the problem using unsupervised approaches. Firstly, in work by (Pedersen and Patwardhan, 2005), the authors investigate different word similarity measures as a means of disambiguating words in context. They compare among different similarity measures. They show that using an extension on the Lesk similarity measure (Lesk</context>
<context position="14442" citStr="Pradhan et al., 2007" startWordPosition="2407" endWordPosition="2410">e different POS WordNet databases with example usages from SemCor. The augmentation is done as a look up table external to WordNet proper since we did not want to dabble with the WordNet offsets. We set a cap of 30 additional examples per synset. Many of the synsets had no additional examples. A total of 26875 synsets in WordNet 1.7.1 and a total of 25940 synsets are augmented with SemCor examples.5 5 Experiments and Results 5.1 Data We experiment with all the standard data sets, namely, Senseval 2 (SV2) (M. Palmer and Dang, 2001), Senseval 3 (SV3) (Snyder and Palmer, 2004), and SEMEVAL (SM) (Pradhan et al., 2007) English All Words data sets. We used the true POS tag sets in the test data as rendered in the Penn Tree Bank. We exclude the data points that have a tag of ”U” in the gold standard since our system does not allow for an unknown option (i.e. it has to produce a sense tag). We present our results on 3 versions of WordNet (WN), 1.7.1 for ease of comparison with previous systems, 2.1 for SEMEVAL data, and 3.0 in order to see whether the trends in performance hold across WN versions. 5It is worth noting that some example sentences are repeated across different synsets and POS since the SemCor dat</context>
</contexts>
<marker>Pradhan, Loper, Dligach, Palmer, 2007</marker>
<rawString>Sameer Pradhan, Edward Loper, Dmitriy Dligach, and Martha Palmer. 2007. Semeval-2007 task-17: English lexical sample, srl and all words. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 87–92, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ravi Sinha</author>
<author>Rada Mihalcea</author>
</authors>
<title>Unsupervised graph-based word sense disambiguation using measures of word semantic similarity.</title>
<date>2007</date>
<booktitle>In Proceedings of the IEEE International Conference on Semantic Computing (ICSC</booktitle>
<location>Irvine, CA.</location>
<contexts>
<context position="2648" citStr="Sinha and Mihalcea, 2007" startWordPosition="420" endWordPosition="423"> the existence and dissemination of standardized data sets over the past 10 years through the different test beds of SENSEVAL and SEMEVAL competitions,1 devising more robust computing algorithms to handle large scale data sets, and simply advancement in hardware machinery. In this paper, we address the problem of WSD of all the content words in a sentence. In this framework, the task is to associate all tokens with their contextually relevant meaning definitions from some computational lexical resource. We present an enhancement on an existing graph based algorithm, InDegree, as described in (Sinha and Mihalcea, 2007). Like the previous work, our algorithm is unsupervised. We show significant improvements over previous state of the art performance on several existing data sets, SENSEVAL2, SENSEVAL3 and SEMEVAL. 2 Word Sense Disambiguation The definition of WSD has taken on several different meanings in recent years. In the latest SEMEVAL (2007) workshop, there were 18 tasks defined, several of which were on different languages, however we notably recognize the widening of the definition of the task of WSD. In addition to the traditional all words and lexical sample tasks, we note new tasks on word sense di</context>
<context position="7295" citStr="Sinha and Mihalcea, 2007" startWordPosition="1183" endWordPosition="1187">senses are listed. Then dependency links are drawn between all the sense pairs. Weights on the arcs are determined based on the semantic similarity using the Lesk measure. The algorithm is basically to walk the graph and find the links with the highest possible weights deciding on the appropriate sense for the target words in question. This algorithm yields an overall F-score of 54.2% on the Senseval 2 all words data set and an F-score of 64.2% on nouns alone. Finally, the closest study relevant to the current paper yields state of the art performance is an unsupervised approach described in (Sinha and Mihalcea, 2007). In this work, the authors combine different semantic similarity measures with different graph based algorithms as an extension to work in (Mihalcea, 2005). The authors proposed a graphbased WSD algorithm. Given a sequence of words W = {w1, w2...wn}, each word wi with several senses {si1, si2...sim}. A graph G = (V,E) is defined such that there exists a vertex v for each sense. Two senses of two different words may be connected by an edge e, depending on their distance. That two senses are connected suggests they should have influence on each other, so normally a maximum allowable distance is</context>
<context position="8768" citStr="Sinha and Mihalcea, 2007" startWordPosition="1430" endWordPosition="1433">h., 1997) similarity measure within nouns, the Leacock &amp; Chodorow (LCH) (Leacock and Chodorow, 1998) similarity measure within verbs, and the Lesk (Lesk, 1986) similarity measure within adjectives and within adverbs and 65 across different POS tags. They evaluate their work against the Senseval 2 all words task. They tune the parameters of their algorithm – specifically the normalization ratio for some of these measures —based on the Senseval 3 data set. They report a state of the art unsupervised system that yields an overall performance of 57.2%. 4 Our Approach In this paper, we extend the (Sinha and Mihalcea, 2007) work (hence forth SM07) in some interesting ways. We focus on the In-Degree graph based algorithm as it was the best performer in the SM07 work. The In-Degree algorithm presents the problem as a weighted graph with senses as nodes and similarity between senses as weights on edges. The In-Degree of a vertex refers to the number of edges incident on that vertex. In the weighted graph, the In-Degree for each vertex is calculated by summing the weights on the edges that are incident on it. After all the In-Degree values for each sense is computed, the sense with maximum value is chosen as the fin</context>
<context position="18020" citStr="Sinha and Mihalcea, 2007" startWordPosition="3032" endWordPosition="3035">ure % for all experimental conditions on all data sets Condition N V A R RAND 43.7 21 41.2 57.4 SM07 68.7 33.01 65.2 63.1 JCN-V 68.7 35.46 65.2 63.1 +ExpandL 70 35.86 65.6 62.8 +SemCor 68.3 37.86 68.6 68.75 +ExpandL SemCor 69.5 38.66 68.2 69.15 Table 2: F-measure results per POS tag per condition for SV2 using WN 1.7.1. 6 Discussion Our overall results on all the data sets clearly outperform the baseline as well as state of the art performance using an unsupervised system (SM07) in overall accuracy across all the data sets. Our implementation of SM07 is slightly higher than those reported in (Sinha and Mihalcea, 2007), 57.12% is probably due to the fact that we do not consider the items tagged as ”U” and also we resolve some of the POS tag mismatches between the gold set and the test data. We note that for the SV2 data set our coverage is not 100% due to some POS tag mismatches that could not have been resolved automatically. These POS tag problems have to do mainly with multiword expressions and the like. In observing the performance of the overall system, we note that using JCN for verbs clearly outperforms using the LCH similarity measure across the board on all data sets as illustrated in Table 1. Usin</context>
</contexts>
<marker>Sinha, Mihalcea, 2007</marker>
<rawString>Ravi Sinha and Rada Mihalcea. 2007. Unsupervised graph-based word sense disambiguation using measures of word semantic similarity. In Proceedings of the IEEE International Conference on Semantic Computing (ICSC 2007), Irvine, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Martha Palmer</author>
</authors>
<title>The english all-words task.</title>
<date>2004</date>
<booktitle>Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text,</booktitle>
<pages>41--43</pages>
<editor>In Rada Mihalcea and Phil Edmonds, editors,</editor>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="4967" citStr="Snyder and Palmer, 2004" startWordPosition="802" endWordPosition="805"> the noun bank will be assigned the meaning sloping land especially the slope beside a body of water and so on. 3 Related Works Many systems over the years have been used for the task. A thorough review of the current state of the art is in (Navigli, 2009). Several techniques have been used to tackle the problem ranging from rule based/knowledge based approaches to unsupervised and supervised machine learning approaches. To date, the best approaches that solve the all words WSD task are supervised as illustrated in the different SenseEval and SEMEVAL All Words tasks (M. Palmer and Dang, 2001; Snyder and Palmer, 2004; Pradhan et al., 2007). In this paper, we present an unsupervised approach to the all words WSD problem relying on WordNet similarity measures. We will review only three of the most relevant related research due to space limitations. We acknowledge the existence of many research papers that tackled the problem using unsupervised approaches. Firstly, in work by (Pedersen and Patwardhan, 2005), the authors investigate different word similarity measures as a means of disambiguating words in context. They compare among different similarity measures. They show that using an extension on the Lesk s</context>
<context position="14401" citStr="Snyder and Palmer, 2004" startWordPosition="2400" endWordPosition="2403">tations. We augment the sense entries for the different POS WordNet databases with example usages from SemCor. The augmentation is done as a look up table external to WordNet proper since we did not want to dabble with the WordNet offsets. We set a cap of 30 additional examples per synset. Many of the synsets had no additional examples. A total of 26875 synsets in WordNet 1.7.1 and a total of 25940 synsets are augmented with SemCor examples.5 5 Experiments and Results 5.1 Data We experiment with all the standard data sets, namely, Senseval 2 (SV2) (M. Palmer and Dang, 2001), Senseval 3 (SV3) (Snyder and Palmer, 2004), and SEMEVAL (SM) (Pradhan et al., 2007) English All Words data sets. We used the true POS tag sets in the test data as rendered in the Penn Tree Bank. We exclude the data points that have a tag of ”U” in the gold standard since our system does not allow for an unknown option (i.e. it has to produce a sense tag). We present our results on 3 versions of WordNet (WN), 1.7.1 for ease of comparison with previous systems, 2.1 for SEMEVAL data, and 3.0 in order to see whether the trends in performance hold across WN versions. 5It is worth noting that some example sentences are repeated across diffe</context>
</contexts>
<marker>Snyder, Palmer, 2004</marker>
<rawString>Benjamin Snyder and Martha Palmer. 2004. The english all-words task. In Rada Mihalcea and Phil Edmonds, editors, Senseval-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, pages 41–43, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>