<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000015">
<title confidence="0.99159">
Feature Selection in Kernel Space: A Case Study on Dependency Parsing
</title>
<author confidence="0.90024">
Xian Qian and Yang Liu
</author>
<affiliation confidence="0.833128">
The University of Texas at Dallas
</affiliation>
<address confidence="0.600929">
800 W. Campbell Rd., Richardson, TX, USA
</address>
<email confidence="0.994113">
{qx,yangl}@hlt.utdallas.edu
</email>
<sectionHeader confidence="0.994694" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999854571428571">
Given a set of basic binary features, we
propose a new L1 norm SVM based
feature selection method that explicitly
selects the features in their polynomial
or tree kernel spaces. The efficiency
comes from the anti-monotone property
of the subgradients: the subgradient with
respect to a combined feature can be
bounded by the subgradient with respect
to each of its component features, and
a feature can be pruned safely without
further consideration if its corresponding
subgradient is not steep enough. We
conduct experiments on the English
dependency parsing task with a third
order graph-based parser. Benefiting
from the rich features selected in the
tree kernel space, our model achieved the
best reported unlabeled attachment score
of 93.72 without using any additional
resource.
</bodyText>
<sectionHeader confidence="0.998777" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999919592592593">
In Natural Language Processing (NLP) domain,
existing linear models typically adopt exhaustive
search to generate tons of features such that
the important features are included. However,
the brute-force approach will guickly run out
of memory when the feature space is extremely
large. Unlike linear models, kernel methods
provide a powerful and unified framework for
learning a large or even infinite number of features
implicitly using limited memory. However, many
kernel methods scale quadratically in the number
of training samples, and can hardly reap the
benefits of learning a large dataset. For example,
the popular Penn Tree Bank (PTB) corpus for
training an English part of speech (POS) tagger
has approximately 1M words, thus it takes 1M2
time to compute the kernel matrix, which is
unacceptable using current hardwares.
In this paper, we propose a new feature selection
method that can efficiently select representative
features in the kernel space to improve the
quality of linear models. Specifically, given
a limited number of basic features such as
the commonly used unigrams and bigrams, our
method performs feature selection in the space
of their combinations, e.g, the concatenation of
these n-grams. A sparse discriminative model
is produced by training L1 norm SVMs using
subgradient methods. Different from traditional
training procedures, we divide the feature vector
into a number of segments, and sort them in a
coarse-to-fine order: the first segment includes
the basic features, the second segment includes
the combined features composed of two basic
features, and so on. In each iteration, we calculate
the subgradient segment by segment. A combined
feature and all its further combinations in the
following segments can be safely pruned if the
absolute value of its corresponding subgradient is
not sufficiently large. The algorithm stops until
all features are pruned. Besides, two simple yet
effective pruning strategies are proposed to filter
the combinations.
We conduct experiments on English
dependency parsing task. Millions of deep,
high order features derived by concatenating
contextual words, POS tags, directions and
distances of dependencies are selected in the
polynomial kernel and tree kernel spaces. The
result is promising: these features significantly
improved a state-of-the-art third order dependency
parser, yielding the best reported unlabeled
attachment score of 93.72 without using any
additional resource.
</bodyText>
<page confidence="0.931">
1180
</page>
<note confidence="0.982227666666667">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1180–1190,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.994509" genericHeader="introduction">
2 Related Works
</sectionHeader>
<bodyText confidence="0.994289587301588">
There are two solutions for learning in ultra high
dimensional feature space: kernel method and
feature selection.
Fast kernel methods have been intensively
studied in the past few years. Recently,
randomized methods have attracted more attention
due to its theoretical and empirical success, such
as the Nystr¨om method (Williams and Seeger,
2001) and random projection (Lu et al., 2014).
In NLP domain, previous studies mainly focused
on polynomial kernels, such as the splitSVM and
approximate polynomial kernel (Wu et al., 2007).
In feature selection domain, there has been
plenty of work focusing on fast computation,
while feature selection in extremely high
dimensional feature space is relatively less
studied. Zhang et al. (2006) proposed a
progressive feature selection framework that splits
the feature space into tractable disjoint sub-spaces
such that a feature selection algorithm can be
performed on each one of them, and then merges
the selected features from different sub-spaces.
The search space they studied contained more
than 20 million features. Tan et al. (2012)
proposed adaptive feature scaling (AFS) scheme
for ultra-high dimensional feature selection. The
dimensionality of the features in their experiments
is up to 30 millions.
Previous studies on feature selection in kernel
space typically used mining based approaches
to prune feature candidates. The key idea for
efficient pruning is to estimate the upper bound of
statistics of features without explicit calculation.
The simplest example is frequent mining where
for any n-gram feature, its frequency is bounded
by any of its substrings.
Suzuki et al. (Suzuki et al., 2004) proposed to
select features in convolution kernel space based
on their chi-squared values. They derived a
concise form to estimate the upper bound of chi-
square values, and used PrefixScan algorithm to
enumerates all the significant sub-sequences of
features efficiently.
Okanohara and Tsujii (Okanohara and Tsujii,
2009) further combined the pruning technique
with L1 regularization. They showed the
connection between L1 regularization and
frequent mining: the L1 regularizer provides a
minimum support threshold to prune the gradients
of parameters. They selected the combination
features in a coarse-to-fine order, the gradient
value for a combination feature can be bounded
by each of its component feature, hence may be
pruned without explicit calculation. They also
sorted the features to tighten the bound. Our idea
is similar with theirs, the difference is that our
search space is much larger: we did not restrict the
number of component features. We recursively
pruned the feature set and in each recursion we
selected feature in a batch manner. We further
adopted an efficient data structure, spectral bloom
filter, to estimate the gradients for the candidate
features without generating them.
</bodyText>
<sectionHeader confidence="0.990916" genericHeader="method">
3 The Proposed Method
</sectionHeader>
<subsectionHeader confidence="0.99883">
3.1 Basic Idea
</subsectionHeader>
<bodyText confidence="0.993222147058824">
Given n training samples x1 ... xn with labels
y1 ... yn E Y, we extend the kernel over the input
space to the joint input and output space by simply
defining fT (xi, y)f(xi, y′) = K(xi, xj)I(y ==
y′), which is the same as Taskar’s (see (Taskar,
2004), Page 68), where f is the explicit feature
map for the kernel, and I(·,·) is the indicator
function.
Our task is to select a subset of representative
elements in the feature vector f. Unlike previously
studied feature selection problems, the dimension
of f could be extremely high. It is impossible to
store the feature vector in the memory or even on
the disk.
For easy illustration, we describe our method
for the polynomial kernel, and it can be easily
extended to the tree kernel space.
The R degree polynomial kernel space is
established by a set of basic features B = {b0 =
1, b1, ... , b|B|} and their combinations. In other
words, each feature is the product of at most R
basic features fj = bj1 * bj2 * · · · * bjr, r &lt;
R. As we assume that all features are binary
1, fj can be rewritten as the minimum of these
basic features: fj = min{bj1, bj2, ... , bjr}. We
use Bj = {bj1, bj2, ..., bjr} to denote the set of
component basic features for fj. r is called the
order of feature fj. For two features fj, fk, we
say fk is an extension of fj if Bj C Bk.
Take the document classification task as an
example, the basic features could be word n-
grams, and the quadratic kernel (degree=2) space
includes the combinated features composed of two
&apos;Binary features are often used in NLP.
</bodyText>
<page confidence="0.987492">
1181
</page>
<bodyText confidence="0.999469888888889">
n-grams, a second order feature is true if both n-
grams appear in the document, it is an extension of
any of its component n-grams (first order features).
We use L1 norm SVMs for feature selection.
Traditionally, the L1 norm SVMs can be trained
using subgradient descent and generate a sparse
weight vector w for feature f. Due to the high
dimensionality in our case, we divide f into a
number of segments according to the order of
the feature, the k-th segment includes the k-order
features. In each iteration, we update the weights
of features segment by segment. When updating
the weight of feature fj in the k-th segment, we
estimate the subgradients with respective to fj’s
extensions in the rest k + 1, k + 2, ... segments
and keep their weights at zero if the subgradients
are not sufficiently steep. In this way, we could
ignore these features without explicit calculation.
</bodyText>
<subsectionHeader confidence="0.990393">
3.2 L1 Norm SVMs
</subsectionHeader>
<bodyText confidence="0.9912225">
Specifically, the objective function for learning L1
norm SVMs is:
</bodyText>
<equation confidence="0.983019666666667">
min
W O(w) = C∥w∥1 + ∑ loss(i)
i
</equation>
<bodyText confidence="0.540667">
where
</bodyText>
<equation confidence="0.9978185">
loss(i) = max{wT∆f(xi, y) + S(yi, y)}
yEY
</equation>
<bodyText confidence="0.984383083333333">
is the hinge loss function for the i-th sample.
∆f(xi, y) = f(xi, yi) − f(xi, y) is the residual
feature vector, S(a, b) = 0 if a = b, otherwise
S(a, b) = 1. Regularization parameter C controls
the sparsity of w. With higher C, more zero
elements are generated. We call a feature is fired
if its value is 1.
The objective function is a sum of piecewise
linear functions, hence is convex. Subgradient
descent algorithm is one poplar approach for
minimizing non-differentiable convex functions, it
updates w using
</bodyText>
<equation confidence="0.799828">
wnew = w − gαt
</equation>
<bodyText confidence="0.999591">
where g is the subgradient of w, αt is the step
size in the t-th iteration. Subgradient algorithm
converges if the step size sequence is properly
selected (Boyd and Mutapcic, 2006).
We are interested in the non-differentiable point
wj = 0. Let y*i = maxy{wT ∆f(xi, y) +
S(yi, y)}, the prediction of the current model.
According to the definition of subgradient, we
have, for each sample xi, ∆f(xi, y*i ) is a
subgradient of loss(i), thus, ∑i ∆f(xi, y*i ) is a
subgradient of ∑i loss(i).
Adding the penalty term C∥w∥1, we get the
subset of subgradients at wj = 0 for the objective
function
</bodyText>
<equation confidence="0.980541">
∑ ∆fj(xi, y*i ) − C ≤ gj ≤ ∑ ∆fj(xi, y*i ) + C
i i
</equation>
<bodyText confidence="0.9324332">
We can pick any gj to update wj. Remind that
our purpose is to keep the model sparse, and we
would like to pick gj = 0 if possible. That is, we
can keep wj = 0 if  |∑i ∆fj(xi, y*i ) |≤ C.
Obviously, for any j, we have
</bodyText>
<equation confidence="0.948865">
 |∑ ∑
i ∆fj(xi, y* i ) |≤ ∑ y fj(xi, y) = #fj,
i
</equation>
<bodyText confidence="0.9263365">
i.e., the frequency of feature fj. Thus, we have
Proposition 1 Let C be the threshold of the
frequency, the model generated by the subgradient
method is sparser than frequent mining.
</bodyText>
<subsectionHeader confidence="0.997468">
3.3 Feature Selection Using Gradient Mining
</subsectionHeader>
<bodyText confidence="0.999877142857143">
Now the problem is how to estimate
 |∑i ∆fj(xi, y*i ) |without explicit calculation
for each fj.
In the following, we mix the terminology
gradient and subgradient without loss of clarity.
We define the positive gradient and negative
gradient for wj
</bodyText>
<equation confidence="0.987924888888889">
#f+ j = ∑ fj(xi, yi)
i,yi y∗i
#f, = ∑ fj(xi, yi *)
i,yi y∗i
We have ∑ *
∑ ∆fj(xi, y*i ) ∆fj(xi, yi )
i =
i,y∗i 7�yi
= #f+j − #fj
</equation>
<bodyText confidence="0.997132636363636">
The estimation problem turns out to be a counting
problem: we collect all the incorrectly predicted
samples, and count #f+j , the frequency of fj fired
by the gold labels, and #f� the frequency of fj
fired by the predictions.
As mentioned above, each feature in
polynomial kernel space is defined as
fj = min{b ∈ Bj} = min{bjl, ... , bjr}.
Equivalently, we can define fj in a recursive
way, which is more frequently used in
the rest of the paper. That is, fj =
</bodyText>
<page confidence="0.792547">
1182
</page>
<bodyText confidence="0.9572988">
min{min{bj2, ... , bjr}, min{bj1, bj3, ... , bjr}, ... },
which is the mimum of r features of order r − 1.
Formally, denote B−i
j as the subset of Bj by
removing its i-th element, then the r-order
feature, we have fj = min{h1, ... , hr}, where
hk = min{b ∈ B−k
j }, 1 ≤ k ≤ r.
We have the following anti-monotone property,
which is the basis of our method
</bodyText>
<equation confidence="0.9382132">
#f+ j ≤ #h+k ∀k
#f−j ≤ #h−k ∀k
If there exists a k, such that #h+k ≤ C and #h−k ≤
C, we have
�
 |∆fj(xi, y∗i )|
i
= |#f+j − #f−j |
≤ max{#f+j , #f−j }
≤ max{min{#h+ k }, min
k {#h− k }}
k
≤ min{max{#h+k , #h−k }}
k
≤ C
</equation>
<bodyText confidence="0.998420888888889">
The third inequality comes from the well
known min-max inequality: maxi minj{aij} ≤
minj maxi{aij}. Thus, we could prune fj without
calculating its corresponding gradient.
This is a chain rule, which means that any
feature that has fj as its component can also
be pruned safely. To see this, suppose O
min{..., fj, ... } is such a combined feature, we
have
</bodyText>
<equation confidence="0.982583">
|#O+ − #O− |≤ max{#O+, #O−}
≤ max{#f+j , #f−j }
≤ C
</equation>
<bodyText confidence="0.9086925">
Based on this, we present the gradient mining
based feature selection framework in Algorithm 1.
Algorithm 1 Feature Selection Using Gradient
Mining
</bodyText>
<listItem confidence="0.856314571428571">
Require: Samples X = {x1, ... , xn} with labels
{y1, ... , yn}, basic features 13 = {b1, ... , bjgj},
threshold C &gt; 0, max iteration number M, degree of
polynomial kernel R, sequence of learning step {αt}.
Ensure: Set of selected features S = {fj}, where fj =
min{b G 13j}, 13j C 13,|13j |≤ R.
1: Sr = ∅, r = 1, ... ,R {Sr denotes the selected r-order
features}
2: for t = 1 → M do
3: Set S = URr�1 Sr, f = the vector of features in S.
4: Calculate yz = maxy{wTf(xi, y) + S(yi, y)}, di.
5: Initialize candidate set A = 13
6: for r = 1 → R do
7: for all fj G A do
</listItem>
<figure confidence="0.845550235294118">
j = ∑
8: Calculate #f+ i,yi��y� i fj(xi, yi) and
#f−j = ∑i,yioyl fj(xi, yi )
9: if #f+j , #f−j ≤ C and wj = 0 then
10: Remove fj from A
11: else
12: wj = wj +(#f+j −#f−j +Csign(wj))αt
13: end if
14: end for
15: Sr = A
16: if r &lt; R then
17: Generate order-r + 1 candidates: A =
Sr+1 U{h|h = min{f1,... fr G Sr}, order
of h is r + 1}
18: end if
19: end for
20: end for
</figure>
<subsectionHeader confidence="0.994005">
4.1 Pre-Training
</subsectionHeader>
<bodyText confidence="0.999551533333333">
Usually, the weights of features are initialized
with 0 in the training procedure. However,
this will select too many features in the first
iteration, because all samples are mis-classified
in Line 4, the gradients #f+j and #f−j equal
to the frequencies of the features, and many of
them could be larger than C. Luckily, due to
the convexity of piecewise linear function, the
optimality of subgradient method is irrelevant with
the initial point. So we can start with a well trained
model using a small subset of features such as the
set of lower order features so that the prediction
is more accurate and the gradients #f+ and #f−
are much lower.
=
</bodyText>
<subsectionHeader confidence="0.953718">
4.2 Bloom Filter
</subsectionHeader>
<sectionHeader confidence="0.961939" genericHeader="method">
4 Prune the Candidate Set
</sectionHeader>
<bodyText confidence="0.993757428571429">
The second strategy is to use bloom filter to reduce
In practice, Algorithm 1 is far from efficient candidates before putting them into the candidate
because Line 17 may generate large amounts set A.
of candidate features that quickly consume the A bloom filter (Bloom, 1970) is a space efficient
memory. In this section, we introduce two pruning probabilistic data structure designed to rapidly
strategies that could greatly reduce the size of check whether an element is present in a set. In
candidates. this paper, we use one of its extension, spectral
</bodyText>
<page confidence="0.948063">
1183
</page>
<bodyText confidence="0.99901636">
bloom filter (Cohen and Matias, 2003), which
can efficiently calculate the upper bound of the
frequencies of elements.
The base data structure of a spectral bloom
filter is a vector of L counters, where all counters
are initialized with 0. The spectral bloom filter
uses m hash functions, h1, ... , hm, that map the
elements to the range {1,... L}. When adding an
element f to the bloom filter, we hash it using
the m hash functions, and get the hash codes
h1(f), ... , hm(f), then we check the counters at
positions h1(f), ... , hm(f), and get the counts
{c1, ... , cm}. Let c* be the minimal count among
these counts: c* = min{c1, ... , cm}, we increase
only the counters whose counts are c*, while
keeping other counters unchanged.
To check the frequency of an element, we hash
the element and check the counters in the same
way. The minimum count c* provides the upper
bound of the frequency. In other words, when
pruning elements with frequencies no greater than
a predefined threshold 0, we could safely prune the
element if c* ≤ 0.
In our case, we use the spectral bloom filter to
eliminate the low-frequency candidates.
To estimate the gradients of newly generated
r + 1-order candidates, we run Line 17 twice. In
the first round, we estimate the upper bound of
#h+ for each candidate and add the candidate
to A if its upper bound is greater than a
predefined threshold 0. The second round is
similar, we add the candidates using the upper
bound of h−. We did not estimate #h+ and
#h− simultaneously, because this needs two
bloom filters for positive and negative gradients
respectively, which consumes too much memory.
Specifically, in the first round, we initialize
the spectral bloom filter so that all counters are
set to zero. Then for each incorrectly predicted
sample xi, we generate r + 1-order candidates
by combining r-order candidates that are fired by
the gold label i.e., f(xi, yi) = 1. Once a new
candidate is generated, we hash it and check its
corresponding m counters in the spectral bloom
filter. If the minimal count c* = 0, we know
that its positive gradient #f+ may be greater than
0. So we keep all counts unchanged, and add
the candidate to A. Otherwise, we increase the
counts by 1 using the method described above.
The second round is similar.
</bodyText>
<figure confidence="0.841187">
won
the
</figure>
<figureCaption confidence="0.94006475">
Figure 1: A dependency parse tree (top), one of
its feature trees (middle) and some of its subtrees
(bottom). He ← won → today is not a subtree
because He and today are not adjacent siblings.
</figureCaption>
<sectionHeader confidence="0.981464" genericHeader="method">
5 Efficient Candidate Generation
</sectionHeader>
<subsectionHeader confidence="0.946476">
5.1 Polynomial Kernel
</subsectionHeader>
<bodyText confidence="0.999910545454545">
As mentioned above, we generate the r + 1-
order candidates by combining the candidates of
order r. An efficient feature generation algorithm
should be carefully designed to avoid duplicates,
otherwise #f+ and #f− may be over counted.
The candidate generation algorithm is kernel
dependent. For polynomial kernel, we just
combine any two r-order candidates and remove
the combined feature if its order is not r + 1.
This method requires square running time for each
example.
</bodyText>
<subsectionHeader confidence="0.9529385">
5.2 Dependency Tree Kernel
5.2.1 Definition
</subsectionHeader>
<bodyText confidence="0.998874533333333">
Collins and Duffy (2002) proposed tree kernels for
constituent parsing which includes the all-subtree
features. Similarly, we define dependency tree
kernel for dependency parsing. For compatibility
with the previously studied subtree features
for dependency parsing, we propose a new
dependency tree kernel that is different from
Culotta and Sorensen’s (Culotta and Sorensen,
2004). Given a dependency parse tree T
composed of L words, L − 1 arcs, each arc has
several basic features, such as the concatenation
of the head word and the modifier word, the
concatenation of the word left to the head and the
lower case of the word right to the modifier, the
distance of the arc, the direction of the arc, the
</bodyText>
<figure confidence="0.9917922">
He
game
today
the
won
won
game today He
game
today
won
</figure>
<page confidence="0.988909">
1184
</page>
<bodyText confidence="0.999918485714286">
concatenation of the POS tags of the head and the
modifier, etc.
A feature tree of T is a tree that has the same
structure as T, while each arc is replaced by any
of its basic features. For a parse tree that has
L − 1 arcs, and each arc has d basic features, the
number of the feature trees is dL−1. For example,
the dependency parse tree for sentence He won the
game today is shown in Figure 1. Suppose each
arc has two basic features: word pair and POS tag
pair. Then there are 24 feature trees, because each
arc can be replaced by either word pair or POS tag
pair.
A subtree of a tree is a connected fragment in
the tree. In this paper, to reduce computational
cost, we restrict that adjacent siblings in the
subtrees must be adjacent in the original tree. For
example He ← won → game is a subtree, but He
← won → today is not a subtree. The motivation
of the restriction is to reduce the number of
subtrees, for a node having k children, there are
k(k −1)/2 subtrees, but without the restriction the
number of subtrees is exponential: 2k.
A sub feature tree of a dependency tree T is a
feature tree of any of its subtrees. For example,
the dependency tree in Figure 1 has 12 subtrees
including four arcs, four arc pairs, the three arc
triples and the full feature tree, and each subtree
having s arcs has 2s sub feature trees. Thus the
dependency tree has 2∗4+4∗22+3∗23+24 = 64
sub feature trees.
Given two dependency trees T1 and T2, the
dependency tree kernel is defined as the number of
common sub feature trees of T1 and T2. Formally,
the kernel function is defined as
</bodyText>
<equation confidence="0.9899945">
K(T1,T2) = � ∆(n1, n2)
n1ET1,n2ET2
</equation>
<bodyText confidence="0.996388777777778">
where ∆(n1, n2) denotes the number of common
sub feature trees rooted in n1 and n2 nodes.
Like tree kernel, we can calculate ∆(n1, n2)
recursively. Let ci and c′j denote the i-th
child of n1 and j-th child of n2 respectively,
let STp,l(n1) denote the set of the sub feature
trees rooted in node n1 and the children of the
root are cp, cp+1, ... , cp+l−1, we denote STq,l(n2)
similarly. Then we define
</bodyText>
<equation confidence="0.837963">
�∆p,q,l(n1, n2) = |STp,l(n1) STq,l(n2)|
n
p,q
</equation>
<bodyText confidence="0.5149365">
the number of common sub feature trees in
STp,l(n1) and STq,l(n2).
</bodyText>
<figureCaption confidence="0.79375275">
Figure 2: For any subtree rooted in a with the
rightmost leaf b, we could extend the subtree by
any arc below or right to the path from a to b
(shown in black)
</figureCaption>
<bodyText confidence="0.990097285714286">
To calculate ∆p,q,l(n1, n2), we first consider the
sub feature trees with only two levels, i.e., sub
feature trees that are composed of n1, n2 and some
of their children. We initialize ∆p,q,1(n1, n2) with
number of the common features of arcs n1 → cp
and n2 → c′q. Then we calculate ∆p,q,l(n1, n2)
recursively using
</bodyText>
<equation confidence="0.874492857142857">
∆p,q,l(n1, n2)
=∆p,q,l−1(n1, n2) ∗ ∆p+l,q+l,1(n1, n2)
And ∆(n1, n2) = Ep,q,l ∆p,q,l(n1, n2)
Next we consider all the sub feature trees, we
have
∆p,q,l(n1, n2)
=∆p,q,l−1(n1, n2) ∗ (1 + ∆(cp+l−1, c′q+l−1))
</equation>
<bodyText confidence="0.9983419">
Computing the dependency tree kernel for two
parse trees requires |T1|2 ∗ |T2|2 ∗ min{|T1|, |T2|}
running time in the worst case, as we need to
enumerate p, q, l and n1, n2.
One way to incorporate the dependency tree
kernel for parsing is to rerank the K best candidate
parse trees generated by a simple linear model.
Suppose there are n training samples, the size
of the kernel matrix is (K ∗ n)2, which is
unacceptable for large datasets.
</bodyText>
<subsubsectionHeader confidence="0.565454">
5.2.2 Candidate Generation
</subsubsectionHeader>
<bodyText confidence="0.999950625">
For constituent parsing, Kudo et al. showed
such an all-subtrees representation is extremely
redundant and a comparable accuracy can be
achieved using just a small set of subtrees (Kudo
et al., 2005). Suzuki et al. even showed that the
over-fitting problem often arises when convolution
kernels are used in NLP tasks (Suzuki et al., 2004).
Now we attempt to select representative sub
</bodyText>
<figure confidence="0.999141">
b
a
</figure>
<page confidence="0.987388">
1185
</page>
<bodyText confidence="0.999863717391304">
feature trees in the kernel space using Algorithm
1. The r-order features in dependency tree kernel
space are the sub feature trees with r arcs. The
candidate feature generation in Line 17 has two
steps: first we generate the subtrees with r arcs,
then we generate the sub feature trees for each
subtree.
The simplest way for subtree generation is to
enumerate the combinations of r + 2 words in the
sentence, and check if these words form a subtree.
We can speed up the generation by using the
results of the subtrees with r + 1 words (r arcs).
For each subtree 5r with r arcs, we can add an
extra word to 5r and generate 5r+1 if the words
form a subtree.
This method has three issues: first, the time
complexity is exponential in the length of the
sentence, as there are 2L combinations of words,
L is the sentence length; second, it may generate
duplicated subtrees, and over counts the gradients.
For example, there are two ways to generate the
subtree He won the game in Figure 1: we can
either add word He to the subtree won the game,
or add word the to the subtree He won game; third,
checking a fragment requires O(L) time.
These issues can be solved using the well
known rightmost-extension method (Zaki, 2002;
Asai et al., 2002; Kudo et al., 2005) which
enumerates all subtrees from a given tree
efficiently. This method starts with a set of trees
consisting of single nodes, and then expands each
subtree attaching a new node.
Specifically, it first indexes the words in the pre-
order of the parse tree. When generating 5r+1,
only the words whose indices are larger than the
greatest index of the words in 5r are considered.
In this way, each subtree is generated only once.
Thus, we only need to consider two types of
words: (i) the children of the rightmost leaf of 5r,
(ii) the adjacent right sibling of the any node in 5r,
as shown in Figure 2.
The total number of subtrees is no greater than
L3, because the level of a subtree is less than L,
and for the children of each node, there are at most
L2 subsequences of siblings. Therefore the time
complexity for subtree extraction is O(L3).
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="method">
6 Experiments
</sectionHeader>
<subsectionHeader confidence="0.99808">
6.1 Experimental Results on English Dataset
6.1.1 Settings
</subsectionHeader>
<bodyText confidence="0.999614652173913">
First we used the English Penn Tree Bank (PTB)
with standard train/develop/test for evaluation.
Sections 2-21 (around 40K sentences) were used
as training data, section 22 was used as the
development set and section 23 was used as the
final test set.
We extracted dependencies using Joakim
Nivre’s Penn2Malt tool with Yamada and
Matsumoto’s rules (Yamada and Matsumoto,
2003). Unlabeled attachment score (UAS)
ignoring punctuation is used to evaluate parsing
quality.
We apply our technique to rerank the parse trees
generated by a third order parser (Koo and Collins,
2010) trained using 10 best MIRA algorithm
with 10 iterations. We generate the top 10 best
candidate parse trees using 10 fold cross validation
for each sentence in the training data. The gold
parse tree is added if it is not in the candidate
list. Then we learn a reranking model using these
candidate trees. During testing, the score for a
parse tree T is a linear combination of the two
models:
</bodyText>
<equation confidence="0.998381">
score(T) = QscoreO3(T) + scorererank(T)
</equation>
<bodyText confidence="0.994465142857143">
where the meta-parameter Q = 5 is tuned
by grid search using the development dataset.
scoreO3(T) and scorererank(T) are the outputs of
the third order parser and the reranking classifier
respectively.
For comparison, we implement the following
reranking models:
</bodyText>
<listItem confidence="0.997519">
• Perceptron with Polynomial kernels
K(a, b) = (aT b + 1)d, d = 2, 4, 8
• Perceptron with Dependency tree kernel.
• Perceptron with features generated by
templates, including all siblings and fourth
order features.
• Perceptron with the features selected in
polynomial and tree kernel spaces, where
threshold C = 3.
</listItem>
<bodyText confidence="0.932883">
The basic features to establish the kernel spaces
include the combinations of contextual words or
POS tags of head and modifier, the length and
</bodyText>
<page confidence="0.973352">
1186
</page>
<table confidence="0.9991155">
whwm, phpm, whpm, phwm
ph−1pm, ph−1wm, phpm−1, whpm−1
ph+1pm, ph+1wm, phpm+1, whpm+1
ph−1phpm, phph+1pm, phpm−1pm, phpmpm+1
Concatenate features above with length and direction
phpbpm
</table>
<tableCaption confidence="0.979906">
Table 1: Basic features in polynomial and
</tableCaption>
<bodyText confidence="0.990137454545455">
dependency tree kernel spaces, wh: the word of
head node, wm denotes the word of modifier node,
ph: the POS of head node, pm denotes the POS
of modifier node, ph+1: POS to the right of head
node, ph−1: POS to the left of modifier node,
pm+1: POS to the right of head node, pm−1: POS
to the left of modifier node, pb: POS of a word in
between head and modifier nodes.
direction of the arcs, and the POS tags of the words
lying between the head and modifier, as shown in
Table 1. The POS tags are automatically generated
by 10 fold cross validation during training, and
a POS tagger trained using the full training data
during testing which has an accuracy of 96.9% on
the development data and 97.3% on the test data.
As kernel methods are not scalable for large
datasets, we applied the strategy proposed by
Collins and Duffy (2002), to break the training set
into 10 chunks of roughly equal size, and trained
10 separate kernel perceptrons on these data sets.
The outputs from the 10 runs on test examples
were combined through the voting procedure.
For feature selection, we set the maximum
iteration number M = 100. We use the first order
and second order features for pre-training. We
choose the constant step size αt = 1 because we
find this could quickly reduce the prediction error
in very few iterations.
We use the SHA-1 hash function to generate
the hash codes for the spectral bloom filter. The
SHA-1 hash function produces a 160-bit hash code
for each candidate feature. The hash code is then
segmented into 5 segments, in this way we get
five hash codes h1, ... , h5. Each code has 32 bits.
Then we create 232(4G) counters. The threshold
θ is set to 3, thus each counter requires 2 bits to
store the counts. The spectral bloom filter costs
1G memory in total.
Furthermore, to reduce memory cost, we save
the local data structure such as the selected
features in Step 15 of Algorithm 1 whenever
possible, and load them into the memory when
needed.
After feature selection, we did not use the L1
</bodyText>
<table confidence="0.999657869565218">
System UAS Training
Time
Third Order Parser 93.07 20 hrs
Quadratic Kernel(QK) 93.41 6 hrs
Biquadratic Kernel(BK) 93.45 6 hrs
8-th Degree Polynomial Kernel(8K) 93.27 6 hrs
Dependency Tree Kernel (DTK) 93.65 10 days
LM with Template Features 93.39 4 mins
LM with Features in QK 93.39 9 mins
LM with Features in BK 93.44 0.5 hrs
LM with Features in 8K 93.30 6 hrs
LM with Features in DTK 93.72 36 hrs
(Zhang and McDonald, 2014) 93.57 N/A
(Zhang et al., 2013) 93.50 N/A
(Ma and Zhao, 2012) 93.40 N/A
(Bohnet and Kuhn, 2012) 93.39 N/A
(Rush and Petrov, 2012) 93.30 N/A
(Qian and Liu, 2013) 93.17 N/A
(Hayashi et al., 2013) 93.12 1 hr
(Martins et al., 2013) 93.07 N/A
(Zhang and McDonald, 2012) 93.06 N/A
(Koo and Collins, 2010) 93.04 N/A
(Zhang and Nivre, 2011) 92.90 N/A
</table>
<tableCaption confidence="0.999026">
Table 2: Comparison between our system and the
</tableCaption>
<bodyText confidence="0.983268714285714">
state-of-art systems on English dataset. LM is
short for Linear Model, hrs, mins are short for
hours and minutes respectively
SVM for testing, instead, we trained an averaged
perceptron with the selected features. Because
we find that the averaged perceptron significantly
outperforms L1 SVM.
</bodyText>
<sectionHeader confidence="0.900474" genericHeader="evaluation">
6.1.2 Results
</sectionHeader>
<bodyText confidence="0.999743095238095">
Experimental results are listed in Table 2, all
systems run on a 64 bit Fedora operation system
with a single Intel core i7 3.40GHz and 32G
memory. We also include results of representative
state-of-the art systems.
It is clear that the use of kernels or the deep
features in kernel spaces significantly improves
the baseline third order parser and outperforms
the reranking model with shallow, template-
generated features. Besides, our feature selection
outperforms kernel methods in both efficiency and
accuracy.
It is unsurprising that the dependency tree
kernel outperforms polynomial kernels, because
it captures the structured information. For
example, polynomial kernels can not distinguish
the grand-child feature or sibling feature from the
combination of two separated arc features.
When no additional resource is available, our
parser achieved the best reported performance
93.72% UAS on English PTB dataset. It is
</bodyText>
<page confidence="0.977295">
1187
</page>
<table confidence="0.999562333333333">
C #Feat #Template Hours Mem(G) UAS
1 0.34G N/A stalled OOM N/A
2 0.34G N/A stalled OOM N/A
3 33.1M 11.4K 36 4.0 93.72
5 6.32M 2.1K 20 2.2 93.55
10 2.10M 1.6K 5 1.4 93.40
</table>
<tableCaption confidence="0.8175045">
Table 3: Feature selection in dependency kernel
space with different threshold C.
</tableCaption>
<bodyText confidence="0.999830842105263">
worth pointing that our method is orthogonal to
other reported systems that benefit from advanced
inference algorthms, such as cube pruning (Zhang
and McDonald, 2014), AD3 (Martins et al., 2013),
etc. We believe that combining our techniques
with others’ will achieve further improvement.
Reranking the candidate parse trees of 2416
testing sentences takes 67 seconds, about 36
sentences per second.
To further understand the complexity of our
algorithm, we perform feature selection in
dependency tree kernel space with different
thresholds C and record the number of selected
features and feature templates, the speed and
memory cost. Table 3 shows the results. We
can see that our algorithm works efficiently when
C ≥ 3, but for C &lt; 3, the number of selected
features grows drastically, and the program runs
out of memory (OOM).
</bodyText>
<subsectionHeader confidence="0.8988265">
6.2 Experimental Results on CoNLL 2009
Dataset
</subsectionHeader>
<bodyText confidence="0.999964619047619">
Now we looked at the impact of our system on
non-English treebanks. We evaluate our system on
six other languages from the CoNLL 2009 shared-
task. We used the best setting in the previous
experiment: reranking model is trained using the
features selected in the dependency tree kernel
space. For POS tag features we used the predicted
tags.
As the third order parser can not handle
non-projective parse trees, we used the graph
transformation techniques to produce non-
projective structures (Nivre and Nilsson,
2005). First, the training data for the parser
is projectivized by applying a number of lifting
operations (Kahane et al., 1998) and encoding
information about these lifts in arc labels. We
used the path encoding scheme where the label of
each arc is concatenated with two binary tags, one
indicates if the arc is lifted, the other indicates if
the arc is along the lifting path from the syntactic
to the linear head. Then we train a projective
</bodyText>
<table confidence="0.992810142857143">
Language Ours Official Best
Chinese 76.77 79.17
Japanese 92.68 92.57
German 87.40 87.48
Spanish 87.82 87.64
Czech 80.51 80.38
Catalan 86.98 87.86
</table>
<tableCaption confidence="0.9372325">
Table 4: Experimental Results on CoNLL 2009
non-English datasets.
</tableCaption>
<bodyText confidence="0.999668733333334">
parser on the transformed data without arc label
information and a classifier to predict the arc
labels based on the projectivized gold parse tree
structure. During testing, we run the parser and
the classifier in a pipeline to generate a labeled
parse tree. Labeled syntactic accuracy is reported
for comparison.
Comparison results are listed in Table 4.
We achieved the best reported results on three
languages, Japanese, Spanish and Czech. Note
that CoNLL 2009 also provide the semantic
labeling annotation which we did not used in our
system. While some official systems benefit from
jointly learning parsing and semantic role labeling
models.
</bodyText>
<sectionHeader confidence="0.998212" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999456909090909">
In this paper we proposed a new feature selection
algorithm that selects features in kernel spaces
in a coarse to fine order. Like frequent mining,
the efficiency of our approach comes from
the anti-monotone property of the subgradients.
Experimental results on the English dependency
parsing task show that our approach outperforms
standard kernel methods. In the future, we would
like to extend our technique to other real valued
kernels such as the string kernels and tagging
kernels.
</bodyText>
<sectionHeader confidence="0.998026" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999827142857143">
We thank three anonymous reviewers for their
valuable comments. This work is partly supported
by NSF award IIS-0845484 and DARPA under
Contract No. FA8750-13-2-0041. Any opinions
expressed in this material are those of the authors
and do not necessarily reflect the views of the
funding agencies.
</bodyText>
<page confidence="0.992317">
1188
</page>
<sectionHeader confidence="0.982703" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999710339805826">
Tatsuya Asai, Kenji Abe, Shinji Kawasoe, Hiroki
Arimura, Hiroshi Sakamoto, and Setsuo Arikawa.
2002. Efficient substructure discovery from large
semi-structured data. In Proceedings of the Second
SIAM International Conference on Data Mining,
Arlington, VA, USA, April 11-13, 2002, pages 158–
174.
Burton H. Bloom. 1970. Space/time trade-offs in
hash coding with allowable errors. Commun. ACM,
13(7):422–426, July.
Bernd Bohnet and Jonas Kuhn. 2012. The best of
bothworlds – a graph-based completion model for
transition-based parsers. In Proc. of EACL.
S. Boyd and A. Mutapcic. 2006. Subgradient methods.
notes for EE364.
Saar Cohen and Yossi Matias. 2003. Spectral bloom
filters. In Proc. of SIGMOD, SIGMOD ’03.
Michael Collins and Nigel Duffy. 2002. New ranking
algorithms for parsing and tagging: Kernels over
discrete structures, and the voted perceptron. In
Proc. of ACL, ACL ’02.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proc. of ACL,
ACL ’04.
Katsuhiko Hayashi, Shuhei Kondo, and Yuji
Matsumoto. 2013. Efficient stacked dependency
parsing by forest reranking. TACL, 1.
Sylvain Kahane, Alexis Nasr, and Owen Rambow.
1998. Pseudo-projectivity, a polynomially
parsable non-projective dependency grammar.
In Proceedings of the 36th Annual Meeting of the
Association for Computational Linguistics and
17th International Conference on Computational
Linguistics, Volume 1, pages 646–652, Montreal,
Quebec, Canada, August. Association for
Computational Linguistics.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proc. of ACL.
Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005.
Boosting-based parse reranking with subtree
features. In Proceedings of the 43rd Annual
Meeting of the Association for Computational
Linguistics (ACL’05), pages 189–196, Ann Arbor,
Michigan, June. Association for Computational
Linguistics.
Zhiyun Lu, Avner May, Kuan Liu, Alireza Bagheri
Garakani, Dong Guo, Aur´elien Bellet, Linxi Fan,
Michael Collins, Brian Kingsbury, Michael Picheny,
and Fei Sha. 2014. How to scale up kernel
methods to be as good as deep neural nets. CoRR,
abs/1411.4000.
Xuezhe Ma and Hai Zhao. 2012. Fourth-
order dependency parsing. In Proceedings of
COLING 2012: Posters, pages 785–796, Mumbai,
India, December. The COLING 2012 Organizing
Committee.
Andre Martins, Miguel Almeida, and Noah A. Smith.
2013. Turning on the turbo: Fast third-order non-
projective turbo parsers. In Proc. of ACL.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-
projective dependency parsing. In Proceedings
of the 43rd Annual Meeting on Association for
Computational Linguistics, ACL ’05, pages 99–
106, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Daisuke Okanohara and Jun’ichi Tsujii. 2009.
Learning combination features with l1
regularization. In Proceedings of Human Language
Technologies: The 2009 Annual Conference of the
North American Chapter of the Association for
Computational Linguistics, Companion Volume:
Short Papers, pages 97–100, Boulder, Colorado,
June. Association for Computational Linguistics.
Xian Qian and Yang Liu. 2013. Branch and bound
algorithm for dependency parsing with non-local
features. TACL, 1.
Alexander Rush and Slav Petrov. 2012. Vine pruning
for efficient multi-pass dependency parsing. In
Proc. of NAACL. Association for Computational
Linguistics.
Jun Suzuki, Hideki Isozaki, and Eisaku Maeda.
2004. Convolution kernels with feature selection for
natural language processing tasks. In Proceedings
of the 42nd Meeting of the Association for
Computational Linguistics (ACL’04), Main Volume,
pages 119–126, Barcelona, Spain, July.
Mingkui Tan, Ivor W. Tsang, and Li Wang. 2012.
Towards large-scale and ultrahigh dimensional
feature selection via feature generation. CoRR,
abs/1209.5260.
Ben Taskar. 2004. Learning Structured Prediction
Models: A Large Margin Approach. Ph.D. thesis,
Stanford University.
Christopher K. I. Williams and Matthias Seeger. 2001.
Using the nystr¨om method to speed up kernel
machines. In NIPS.
Yu-Chieh Wu, Jie-Chi Yang, and Yue-Shi Lee. 2007.
An approximate approach for training polynomial
kernel svms in linear time. In Proc. of ACL, ACL
’07.
Hiroyasu Yamada and Yuji Matsumoto. 2003.
Statistical dependency analysis with support vector
machines. In Proc. of IWPT.
</reference>
<page confidence="0.913739">
1189
</page>
<reference confidence="0.998207333333333">
Mohammed J. Zaki. 2002. Efficiently mining frequent
trees in a forest. In Proceedings of the Eighth ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining, KDD ’02, pages 71–
80, New York, NY, USA. ACM.
Hao Zhang and Ryan McDonald. 2012. Generalized
higher-order dependency parsing with cube pruning.
In Proc. of EMNLP.
Hao Zhang and Ryan McDonald. 2014. Enforcing
structural diversity in cube-pruned dependency
parsing. In Proc. of ACL.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proc. of ACL-HLT.
Qi Zhang, Fuliang Weng, and Zhe Feng. 2006.
A progressive feature selection algorithm for ultra
large feature spaces. In Proc. of ACL.
Hao Zhang, Liang Huang, Kai Zhao, and Ryan
McDonald. 2013. Online learning for inexact
hypergraph search. In Proc. of EMNLP, pages 908–
913. Association for Computational Linguistics.
</reference>
<page confidence="0.992079">
1190
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.877221">
<title confidence="0.999977">Feature Selection in Kernel Space: A Case Study on Dependency Parsing</title>
<author confidence="0.977881">Qian</author>
<affiliation confidence="0.976491">The University of Texas at</affiliation>
<address confidence="0.961637">800 W. Campbell Rd., Richardson, TX,</address>
<abstract confidence="0.996858318181818">Given a set of basic binary features, we a new SVM based feature selection method that explicitly selects the features in their polynomial or tree kernel spaces. The efficiency comes from the anti-monotone property of the subgradients: the subgradient with respect to a combined feature can be bounded by the subgradient with respect to each of its component features, and a feature can be pruned safely without further consideration if its corresponding subgradient is not steep enough. We conduct experiments on the English dependency parsing task with a third order graph-based parser. from the rich features selected in the tree kernel space, our model achieved the best reported unlabeled attachment score using any additional resource.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Tatsuya Asai</author>
<author>Kenji Abe</author>
<author>Shinji Kawasoe</author>
<author>Hiroki Arimura</author>
<author>Hiroshi Sakamoto</author>
<author>Setsuo Arikawa</author>
</authors>
<title>Efficient substructure discovery from large semi-structured data.</title>
<date>2002</date>
<booktitle>In Proceedings of the Second SIAM International Conference on Data Mining,</booktitle>
<pages>158--174</pages>
<location>Arlington, VA, USA,</location>
<contexts>
<context position="23798" citStr="Asai et al., 2002" startWordPosition="4191" endWordPosition="4194">te 5r+1 if the words form a subtree. This method has three issues: first, the time complexity is exponential in the length of the sentence, as there are 2L combinations of words, L is the sentence length; second, it may generate duplicated subtrees, and over counts the gradients. For example, there are two ways to generate the subtree He won the game in Figure 1: we can either add word He to the subtree won the game, or add word the to the subtree He won game; third, checking a fragment requires O(L) time. These issues can be solved using the well known rightmost-extension method (Zaki, 2002; Asai et al., 2002; Kudo et al., 2005) which enumerates all subtrees from a given tree efficiently. This method starts with a set of trees consisting of single nodes, and then expands each subtree attaching a new node. Specifically, it first indexes the words in the preorder of the parse tree. When generating 5r+1, only the words whose indices are larger than the greatest index of the words in 5r are considered. In this way, each subtree is generated only once. Thus, we only need to consider two types of words: (i) the children of the rightmost leaf of 5r, (ii) the adjacent right sibling of the any node in 5r, </context>
</contexts>
<marker>Asai, Abe, Kawasoe, Arimura, Sakamoto, Arikawa, 2002</marker>
<rawString>Tatsuya Asai, Kenji Abe, Shinji Kawasoe, Hiroki Arimura, Hiroshi Sakamoto, and Setsuo Arikawa. 2002. Efficient substructure discovery from large semi-structured data. In Proceedings of the Second SIAM International Conference on Data Mining, Arlington, VA, USA, April 11-13, 2002, pages 158– 174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Burton H Bloom</author>
</authors>
<title>Space/time trade-offs in hash coding with allowable errors.</title>
<date>1970</date>
<journal>Commun. ACM,</journal>
<volume>13</volume>
<issue>7</issue>
<contexts>
<context position="14790" citStr="Bloom, 1970" startWordPosition="2585" endWordPosition="2586">inear function, the optimality of subgradient method is irrelevant with the initial point. So we can start with a well trained model using a small subset of features such as the set of lower order features so that the prediction is more accurate and the gradients #f+ and #f− are much lower. = 4.2 Bloom Filter 4 Prune the Candidate Set The second strategy is to use bloom filter to reduce In practice, Algorithm 1 is far from efficient candidates before putting them into the candidate because Line 17 may generate large amounts set A. of candidate features that quickly consume the A bloom filter (Bloom, 1970) is a space efficient memory. In this section, we introduce two pruning probabilistic data structure designed to rapidly strategies that could greatly reduce the size of check whether an element is present in a set. In candidates. this paper, we use one of its extension, spectral 1183 bloom filter (Cohen and Matias, 2003), which can efficiently calculate the upper bound of the frequencies of elements. The base data structure of a spectral bloom filter is a vector of L counters, where all counters are initialized with 0. The spectral bloom filter uses m hash functions, h1, ... , hm, that map th</context>
</contexts>
<marker>Bloom, 1970</marker>
<rawString>Burton H. Bloom. 1970. Space/time trade-offs in hash coding with allowable errors. Commun. ACM, 13(7):422–426, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
<author>Jonas Kuhn</author>
</authors>
<title>The best of bothworlds – a graph-based completion model for transition-based parsers.</title>
<date>2012</date>
<booktitle>In Proc. of EACL.</booktitle>
<contexts>
<context position="29222" citStr="Bohnet and Kuhn, 2012" startWordPosition="5133" endWordPosition="5136">er possible, and load them into the memory when needed. After feature selection, we did not use the L1 System UAS Training Time Third Order Parser 93.07 20 hrs Quadratic Kernel(QK) 93.41 6 hrs Biquadratic Kernel(BK) 93.45 6 hrs 8-th Degree Polynomial Kernel(8K) 93.27 6 hrs Dependency Tree Kernel (DTK) 93.65 10 days LM with Template Features 93.39 4 mins LM with Features in QK 93.39 9 mins LM with Features in BK 93.44 0.5 hrs LM with Features in 8K 93.30 6 hrs LM with Features in DTK 93.72 36 hrs (Zhang and McDonald, 2014) 93.57 N/A (Zhang et al., 2013) 93.50 N/A (Ma and Zhao, 2012) 93.40 N/A (Bohnet and Kuhn, 2012) 93.39 N/A (Rush and Petrov, 2012) 93.30 N/A (Qian and Liu, 2013) 93.17 N/A (Hayashi et al., 2013) 93.12 1 hr (Martins et al., 2013) 93.07 N/A (Zhang and McDonald, 2012) 93.06 N/A (Koo and Collins, 2010) 93.04 N/A (Zhang and Nivre, 2011) 92.90 N/A Table 2: Comparison between our system and the state-of-art systems on English dataset. LM is short for Linear Model, hrs, mins are short for hours and minutes respectively SVM for testing, instead, we trained an averaged perceptron with the selected features. Because we find that the averaged perceptron significantly outperforms L1 SVM. 6.1.2 Result</context>
</contexts>
<marker>Bohnet, Kuhn, 2012</marker>
<rawString>Bernd Bohnet and Jonas Kuhn. 2012. The best of bothworlds – a graph-based completion model for transition-based parsers. In Proc. of EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Boyd</author>
<author>A Mutapcic</author>
</authors>
<title>Subgradient methods. notes for EE364.</title>
<date>2006</date>
<contexts>
<context position="9949" citStr="Boyd and Mutapcic, 2006" startWordPosition="1609" endWordPosition="1612"> the residual feature vector, S(a, b) = 0 if a = b, otherwise S(a, b) = 1. Regularization parameter C controls the sparsity of w. With higher C, more zero elements are generated. We call a feature is fired if its value is 1. The objective function is a sum of piecewise linear functions, hence is convex. Subgradient descent algorithm is one poplar approach for minimizing non-differentiable convex functions, it updates w using wnew = w − gαt where g is the subgradient of w, αt is the step size in the t-th iteration. Subgradient algorithm converges if the step size sequence is properly selected (Boyd and Mutapcic, 2006). We are interested in the non-differentiable point wj = 0. Let y*i = maxy{wT ∆f(xi, y) + S(yi, y)}, the prediction of the current model. According to the definition of subgradient, we have, for each sample xi, ∆f(xi, y*i ) is a subgradient of loss(i), thus, ∑i ∆f(xi, y*i ) is a subgradient of ∑i loss(i). Adding the penalty term C∥w∥1, we get the subset of subgradients at wj = 0 for the objective function ∑ ∆fj(xi, y*i ) − C ≤ gj ≤ ∑ ∆fj(xi, y*i ) + C i i We can pick any gj to update wj. Remind that our purpose is to keep the model sparse, and we would like to pick gj = 0 if possible. That is,</context>
</contexts>
<marker>Boyd, Mutapcic, 2006</marker>
<rawString>S. Boyd and A. Mutapcic. 2006. Subgradient methods. notes for EE364.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saar Cohen</author>
<author>Yossi Matias</author>
</authors>
<title>Spectral bloom filters.</title>
<date>2003</date>
<booktitle>In Proc. of SIGMOD, SIGMOD ’03.</booktitle>
<contexts>
<context position="15113" citStr="Cohen and Matias, 2003" startWordPosition="2636" endWordPosition="2639"> Prune the Candidate Set The second strategy is to use bloom filter to reduce In practice, Algorithm 1 is far from efficient candidates before putting them into the candidate because Line 17 may generate large amounts set A. of candidate features that quickly consume the A bloom filter (Bloom, 1970) is a space efficient memory. In this section, we introduce two pruning probabilistic data structure designed to rapidly strategies that could greatly reduce the size of check whether an element is present in a set. In candidates. this paper, we use one of its extension, spectral 1183 bloom filter (Cohen and Matias, 2003), which can efficiently calculate the upper bound of the frequencies of elements. The base data structure of a spectral bloom filter is a vector of L counters, where all counters are initialized with 0. The spectral bloom filter uses m hash functions, h1, ... , hm, that map the elements to the range {1,... L}. When adding an element f to the bloom filter, we hash it using the m hash functions, and get the hash codes h1(f), ... , hm(f), then we check the counters at positions h1(f), ... , hm(f), and get the counts {c1, ... , cm}. Let c* be the minimal count among these counts: c* = min{c1, ... </context>
</contexts>
<marker>Cohen, Matias, 2003</marker>
<rawString>Saar Cohen and Yossi Matias. 2003. Spectral bloom filters. In Proc. of SIGMOD, SIGMOD ’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron.</title>
<date>2002</date>
<booktitle>In Proc. of ACL, ACL ’02.</booktitle>
<contexts>
<context position="18151" citStr="Collins and Duffy (2002)" startWordPosition="3169" endWordPosition="3172">re not adjacent siblings. 5 Efficient Candidate Generation 5.1 Polynomial Kernel As mentioned above, we generate the r + 1- order candidates by combining the candidates of order r. An efficient feature generation algorithm should be carefully designed to avoid duplicates, otherwise #f+ and #f− may be over counted. The candidate generation algorithm is kernel dependent. For polynomial kernel, we just combine any two r-order candidates and remove the combined feature if its order is not r + 1. This method requires square running time for each example. 5.2 Dependency Tree Kernel 5.2.1 Definition Collins and Duffy (2002) proposed tree kernels for constituent parsing which includes the all-subtree features. Similarly, we define dependency tree kernel for dependency parsing. For compatibility with the previously studied subtree features for dependency parsing, we propose a new dependency tree kernel that is different from Culotta and Sorensen’s (Culotta and Sorensen, 2004). Given a dependency parse tree T composed of L words, L − 1 arcs, each arc has several basic features, such as the concatenation of the head word and the modifier word, the concatenation of the word left to the head and the lower case of the </context>
<context position="27528" citStr="Collins and Duffy (2002)" startWordPosition="4828" endWordPosition="4831">o the left of modifier node, pm+1: POS to the right of head node, pm−1: POS to the left of modifier node, pb: POS of a word in between head and modifier nodes. direction of the arcs, and the POS tags of the words lying between the head and modifier, as shown in Table 1. The POS tags are automatically generated by 10 fold cross validation during training, and a POS tagger trained using the full training data during testing which has an accuracy of 96.9% on the development data and 97.3% on the test data. As kernel methods are not scalable for large datasets, we applied the strategy proposed by Collins and Duffy (2002), to break the training set into 10 chunks of roughly equal size, and trained 10 separate kernel perceptrons on these data sets. The outputs from the 10 runs on test examples were combined through the voting procedure. For feature selection, we set the maximum iteration number M = 100. We use the first order and second order features for pre-training. We choose the constant step size αt = 1 because we find this could quickly reduce the prediction error in very few iterations. We use the SHA-1 hash function to generate the hash codes for the spectral bloom filter. The SHA-1 hash function produc</context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>Michael Collins and Nigel Duffy. 2002. New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron. In Proc. of ACL, ACL ’02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Jeffrey Sorensen</author>
</authors>
<title>Dependency tree kernels for relation extraction.</title>
<date>2004</date>
<booktitle>In Proc. of ACL, ACL ’04.</booktitle>
<contexts>
<context position="18508" citStr="Culotta and Sorensen, 2004" startWordPosition="3218" endWordPosition="3221">kernel dependent. For polynomial kernel, we just combine any two r-order candidates and remove the combined feature if its order is not r + 1. This method requires square running time for each example. 5.2 Dependency Tree Kernel 5.2.1 Definition Collins and Duffy (2002) proposed tree kernels for constituent parsing which includes the all-subtree features. Similarly, we define dependency tree kernel for dependency parsing. For compatibility with the previously studied subtree features for dependency parsing, we propose a new dependency tree kernel that is different from Culotta and Sorensen’s (Culotta and Sorensen, 2004). Given a dependency parse tree T composed of L words, L − 1 arcs, each arc has several basic features, such as the concatenation of the head word and the modifier word, the concatenation of the word left to the head and the lower case of the word right to the modifier, the distance of the arc, the direction of the arc, the He game today the won won game today He game today won 1184 concatenation of the POS tags of the head and the modifier, etc. A feature tree of T is a tree that has the same structure as T, while each arc is replaced by any of its basic features. For a parse tree that has L </context>
</contexts>
<marker>Culotta, Sorensen, 2004</marker>
<rawString>Aron Culotta and Jeffrey Sorensen. 2004. Dependency tree kernels for relation extraction. In Proc. of ACL, ACL ’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katsuhiko Hayashi</author>
<author>Shuhei Kondo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Efficient stacked dependency parsing by forest reranking.</title>
<date>2013</date>
<journal>TACL,</journal>
<volume>1</volume>
<contexts>
<context position="29320" citStr="Hayashi et al., 2013" startWordPosition="5151" endWordPosition="5154"> L1 System UAS Training Time Third Order Parser 93.07 20 hrs Quadratic Kernel(QK) 93.41 6 hrs Biquadratic Kernel(BK) 93.45 6 hrs 8-th Degree Polynomial Kernel(8K) 93.27 6 hrs Dependency Tree Kernel (DTK) 93.65 10 days LM with Template Features 93.39 4 mins LM with Features in QK 93.39 9 mins LM with Features in BK 93.44 0.5 hrs LM with Features in 8K 93.30 6 hrs LM with Features in DTK 93.72 36 hrs (Zhang and McDonald, 2014) 93.57 N/A (Zhang et al., 2013) 93.50 N/A (Ma and Zhao, 2012) 93.40 N/A (Bohnet and Kuhn, 2012) 93.39 N/A (Rush and Petrov, 2012) 93.30 N/A (Qian and Liu, 2013) 93.17 N/A (Hayashi et al., 2013) 93.12 1 hr (Martins et al., 2013) 93.07 N/A (Zhang and McDonald, 2012) 93.06 N/A (Koo and Collins, 2010) 93.04 N/A (Zhang and Nivre, 2011) 92.90 N/A Table 2: Comparison between our system and the state-of-art systems on English dataset. LM is short for Linear Model, hrs, mins are short for hours and minutes respectively SVM for testing, instead, we trained an averaged perceptron with the selected features. Because we find that the averaged perceptron significantly outperforms L1 SVM. 6.1.2 Results Experimental results are listed in Table 2, all systems run on a 64 bit Fedora operation system </context>
</contexts>
<marker>Hayashi, Kondo, Matsumoto, 2013</marker>
<rawString>Katsuhiko Hayashi, Shuhei Kondo, and Yuji Matsumoto. 2013. Efficient stacked dependency parsing by forest reranking. TACL, 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylvain Kahane</author>
<author>Alexis Nasr</author>
<author>Owen Rambow</author>
</authors>
<title>Pseudo-projectivity, a polynomially parsable non-projective dependency grammar.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics,</booktitle>
<volume>1</volume>
<pages>646--652</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montreal, Quebec, Canada,</location>
<contexts>
<context position="32522" citStr="Kahane et al., 1998" startWordPosition="5664" endWordPosition="5667">the impact of our system on non-English treebanks. We evaluate our system on six other languages from the CoNLL 2009 sharedtask. We used the best setting in the previous experiment: reranking model is trained using the features selected in the dependency tree kernel space. For POS tag features we used the predicted tags. As the third order parser can not handle non-projective parse trees, we used the graph transformation techniques to produce nonprojective structures (Nivre and Nilsson, 2005). First, the training data for the parser is projectivized by applying a number of lifting operations (Kahane et al., 1998) and encoding information about these lifts in arc labels. We used the path encoding scheme where the label of each arc is concatenated with two binary tags, one indicates if the arc is lifted, the other indicates if the arc is along the lifting path from the syntactic to the linear head. Then we train a projective Language Ours Official Best Chinese 76.77 79.17 Japanese 92.68 92.57 German 87.40 87.48 Spanish 87.82 87.64 Czech 80.51 80.38 Catalan 86.98 87.86 Table 4: Experimental Results on CoNLL 2009 non-English datasets. parser on the transformed data without arc label information and a clas</context>
</contexts>
<marker>Kahane, Nasr, Rambow, 1998</marker>
<rawString>Sylvain Kahane, Alexis Nasr, and Owen Rambow. 1998. Pseudo-projectivity, a polynomially parsable non-projective dependency grammar. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1, pages 646–652, Montreal, Quebec, Canada, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Michael Collins</author>
</authors>
<title>Efficient thirdorder dependency parsers.</title>
<date>2010</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="25311" citStr="Koo and Collins, 2010" startWordPosition="4448" endWordPosition="4451">ental Results on English Dataset 6.1.1 Settings First we used the English Penn Tree Bank (PTB) with standard train/develop/test for evaluation. Sections 2-21 (around 40K sentences) were used as training data, section 22 was used as the development set and section 23 was used as the final test set. We extracted dependencies using Joakim Nivre’s Penn2Malt tool with Yamada and Matsumoto’s rules (Yamada and Matsumoto, 2003). Unlabeled attachment score (UAS) ignoring punctuation is used to evaluate parsing quality. We apply our technique to rerank the parse trees generated by a third order parser (Koo and Collins, 2010) trained using 10 best MIRA algorithm with 10 iterations. We generate the top 10 best candidate parse trees using 10 fold cross validation for each sentence in the training data. The gold parse tree is added if it is not in the candidate list. Then we learn a reranking model using these candidate trees. During testing, the score for a parse tree T is a linear combination of the two models: score(T) = QscoreO3(T) + scorererank(T) where the meta-parameter Q = 5 is tuned by grid search using the development dataset. scoreO3(T) and scorererank(T) are the outputs of the third order parser and the r</context>
<context position="29425" citStr="Koo and Collins, 2010" startWordPosition="5170" endWordPosition="5173">c Kernel(BK) 93.45 6 hrs 8-th Degree Polynomial Kernel(8K) 93.27 6 hrs Dependency Tree Kernel (DTK) 93.65 10 days LM with Template Features 93.39 4 mins LM with Features in QK 93.39 9 mins LM with Features in BK 93.44 0.5 hrs LM with Features in 8K 93.30 6 hrs LM with Features in DTK 93.72 36 hrs (Zhang and McDonald, 2014) 93.57 N/A (Zhang et al., 2013) 93.50 N/A (Ma and Zhao, 2012) 93.40 N/A (Bohnet and Kuhn, 2012) 93.39 N/A (Rush and Petrov, 2012) 93.30 N/A (Qian and Liu, 2013) 93.17 N/A (Hayashi et al., 2013) 93.12 1 hr (Martins et al., 2013) 93.07 N/A (Zhang and McDonald, 2012) 93.06 N/A (Koo and Collins, 2010) 93.04 N/A (Zhang and Nivre, 2011) 92.90 N/A Table 2: Comparison between our system and the state-of-art systems on English dataset. LM is short for Linear Model, hrs, mins are short for hours and minutes respectively SVM for testing, instead, we trained an averaged perceptron with the selected features. Because we find that the averaged perceptron significantly outperforms L1 SVM. 6.1.2 Results Experimental results are listed in Table 2, all systems run on a 64 bit Fedora operation system with a single Intel core i7 3.40GHz and 32G memory. We also include results of representative state-of-th</context>
</contexts>
<marker>Koo, Collins, 2010</marker>
<rawString>Terry Koo and Michael Collins. 2010. Efficient thirdorder dependency parsers. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Jun Suzuki</author>
<author>Hideki Isozaki</author>
</authors>
<title>Boosting-based parse reranking with subtree features.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>189--196</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="22367" citStr="Kudo et al., 2005" startWordPosition="3931" endWordPosition="3934">e trees requires |T1|2 ∗ |T2|2 ∗ min{|T1|, |T2|} running time in the worst case, as we need to enumerate p, q, l and n1, n2. One way to incorporate the dependency tree kernel for parsing is to rerank the K best candidate parse trees generated by a simple linear model. Suppose there are n training samples, the size of the kernel matrix is (K ∗ n)2, which is unacceptable for large datasets. 5.2.2 Candidate Generation For constituent parsing, Kudo et al. showed such an all-subtrees representation is extremely redundant and a comparable accuracy can be achieved using just a small set of subtrees (Kudo et al., 2005). Suzuki et al. even showed that the over-fitting problem often arises when convolution kernels are used in NLP tasks (Suzuki et al., 2004). Now we attempt to select representative sub b a 1185 feature trees in the kernel space using Algorithm 1. The r-order features in dependency tree kernel space are the sub feature trees with r arcs. The candidate feature generation in Line 17 has two steps: first we generate the subtrees with r arcs, then we generate the sub feature trees for each subtree. The simplest way for subtree generation is to enumerate the combinations of r + 2 words in the senten</context>
<context position="23818" citStr="Kudo et al., 2005" startWordPosition="4195" endWordPosition="4198">s form a subtree. This method has three issues: first, the time complexity is exponential in the length of the sentence, as there are 2L combinations of words, L is the sentence length; second, it may generate duplicated subtrees, and over counts the gradients. For example, there are two ways to generate the subtree He won the game in Figure 1: we can either add word He to the subtree won the game, or add word the to the subtree He won game; third, checking a fragment requires O(L) time. These issues can be solved using the well known rightmost-extension method (Zaki, 2002; Asai et al., 2002; Kudo et al., 2005) which enumerates all subtrees from a given tree efficiently. This method starts with a set of trees consisting of single nodes, and then expands each subtree attaching a new node. Specifically, it first indexes the words in the preorder of the parse tree. When generating 5r+1, only the words whose indices are larger than the greatest index of the words in 5r are considered. In this way, each subtree is generated only once. Thus, we only need to consider two types of words: (i) the children of the rightmost leaf of 5r, (ii) the adjacent right sibling of the any node in 5r, as shown in Figure 2</context>
</contexts>
<marker>Kudo, Suzuki, Isozaki, 2005</marker>
<rawString>Taku Kudo, Jun Suzuki, and Hideki Isozaki. 2005. Boosting-based parse reranking with subtree features. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 189–196, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiyun Lu</author>
<author>Avner May</author>
<author>Kuan Liu</author>
<author>Alireza Bagheri Garakani</author>
<author>Dong Guo</author>
<author>Aur´elien Bellet</author>
<author>Linxi Fan</author>
<author>Michael Collins</author>
<author>Brian Kingsbury</author>
<author>Michael Picheny</author>
<author>Fei Sha</author>
</authors>
<title>How to scale up kernel methods to be as good as deep neural nets.</title>
<date>2014</date>
<tech>CoRR, abs/1411.4000.</tech>
<contexts>
<context position="4140" citStr="Lu et al., 2014" startWordPosition="617" endWordPosition="620">iation for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1180–1190, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics 2 Related Works There are two solutions for learning in ultra high dimensional feature space: kernel method and feature selection. Fast kernel methods have been intensively studied in the past few years. Recently, randomized methods have attracted more attention due to its theoretical and empirical success, such as the Nystr¨om method (Williams and Seeger, 2001) and random projection (Lu et al., 2014). In NLP domain, previous studies mainly focused on polynomial kernels, such as the splitSVM and approximate polynomial kernel (Wu et al., 2007). In feature selection domain, there has been plenty of work focusing on fast computation, while feature selection in extremely high dimensional feature space is relatively less studied. Zhang et al. (2006) proposed a progressive feature selection framework that splits the feature space into tractable disjoint sub-spaces such that a feature selection algorithm can be performed on each one of them, and then merges the selected features from different su</context>
</contexts>
<marker>Lu, May, Liu, Garakani, Guo, Bellet, Fan, Collins, Kingsbury, Picheny, Sha, 2014</marker>
<rawString>Zhiyun Lu, Avner May, Kuan Liu, Alireza Bagheri Garakani, Dong Guo, Aur´elien Bellet, Linxi Fan, Michael Collins, Brian Kingsbury, Michael Picheny, and Fei Sha. 2014. How to scale up kernel methods to be as good as deep neural nets. CoRR, abs/1411.4000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuezhe Ma</author>
<author>Hai Zhao</author>
</authors>
<title>Fourthorder dependency parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012: Posters,</booktitle>
<pages>785--796</pages>
<location>Mumbai, India,</location>
<contexts>
<context position="29188" citStr="Ma and Zhao, 2012" startWordPosition="5127" endWordPosition="5130"> Step 15 of Algorithm 1 whenever possible, and load them into the memory when needed. After feature selection, we did not use the L1 System UAS Training Time Third Order Parser 93.07 20 hrs Quadratic Kernel(QK) 93.41 6 hrs Biquadratic Kernel(BK) 93.45 6 hrs 8-th Degree Polynomial Kernel(8K) 93.27 6 hrs Dependency Tree Kernel (DTK) 93.65 10 days LM with Template Features 93.39 4 mins LM with Features in QK 93.39 9 mins LM with Features in BK 93.44 0.5 hrs LM with Features in 8K 93.30 6 hrs LM with Features in DTK 93.72 36 hrs (Zhang and McDonald, 2014) 93.57 N/A (Zhang et al., 2013) 93.50 N/A (Ma and Zhao, 2012) 93.40 N/A (Bohnet and Kuhn, 2012) 93.39 N/A (Rush and Petrov, 2012) 93.30 N/A (Qian and Liu, 2013) 93.17 N/A (Hayashi et al., 2013) 93.12 1 hr (Martins et al., 2013) 93.07 N/A (Zhang and McDonald, 2012) 93.06 N/A (Koo and Collins, 2010) 93.04 N/A (Zhang and Nivre, 2011) 92.90 N/A Table 2: Comparison between our system and the state-of-art systems on English dataset. LM is short for Linear Model, hrs, mins are short for hours and minutes respectively SVM for testing, instead, we trained an averaged perceptron with the selected features. Because we find that the averaged perceptron significantl</context>
</contexts>
<marker>Ma, Zhao, 2012</marker>
<rawString>Xuezhe Ma and Hai Zhao. 2012. Fourthorder dependency parsing. In Proceedings of COLING 2012: Posters, pages 785–796, Mumbai, India, December. The COLING 2012 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andre Martins</author>
<author>Miguel Almeida</author>
<author>Noah A Smith</author>
</authors>
<title>Turning on the turbo: Fast third-order nonprojective turbo parsers.</title>
<date>2013</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="29354" citStr="Martins et al., 2013" startWordPosition="5158" endWordPosition="5161"> Order Parser 93.07 20 hrs Quadratic Kernel(QK) 93.41 6 hrs Biquadratic Kernel(BK) 93.45 6 hrs 8-th Degree Polynomial Kernel(8K) 93.27 6 hrs Dependency Tree Kernel (DTK) 93.65 10 days LM with Template Features 93.39 4 mins LM with Features in QK 93.39 9 mins LM with Features in BK 93.44 0.5 hrs LM with Features in 8K 93.30 6 hrs LM with Features in DTK 93.72 36 hrs (Zhang and McDonald, 2014) 93.57 N/A (Zhang et al., 2013) 93.50 N/A (Ma and Zhao, 2012) 93.40 N/A (Bohnet and Kuhn, 2012) 93.39 N/A (Rush and Petrov, 2012) 93.30 N/A (Qian and Liu, 2013) 93.17 N/A (Hayashi et al., 2013) 93.12 1 hr (Martins et al., 2013) 93.07 N/A (Zhang and McDonald, 2012) 93.06 N/A (Koo and Collins, 2010) 93.04 N/A (Zhang and Nivre, 2011) 92.90 N/A Table 2: Comparison between our system and the state-of-art systems on English dataset. LM is short for Linear Model, hrs, mins are short for hours and minutes respectively SVM for testing, instead, we trained an averaged perceptron with the selected features. Because we find that the averaged perceptron significantly outperforms L1 SVM. 6.1.2 Results Experimental results are listed in Table 2, all systems run on a 64 bit Fedora operation system with a single Intel core i7 3.40GH</context>
<context position="31200" citStr="Martins et al., 2013" startWordPosition="5450" endWordPosition="5453">ation of two separated arc features. When no additional resource is available, our parser achieved the best reported performance 93.72% UAS on English PTB dataset. It is 1187 C #Feat #Template Hours Mem(G) UAS 1 0.34G N/A stalled OOM N/A 2 0.34G N/A stalled OOM N/A 3 33.1M 11.4K 36 4.0 93.72 5 6.32M 2.1K 20 2.2 93.55 10 2.10M 1.6K 5 1.4 93.40 Table 3: Feature selection in dependency kernel space with different threshold C. worth pointing that our method is orthogonal to other reported systems that benefit from advanced inference algorthms, such as cube pruning (Zhang and McDonald, 2014), AD3 (Martins et al., 2013), etc. We believe that combining our techniques with others’ will achieve further improvement. Reranking the candidate parse trees of 2416 testing sentences takes 67 seconds, about 36 sentences per second. To further understand the complexity of our algorithm, we perform feature selection in dependency tree kernel space with different thresholds C and record the number of selected features and feature templates, the speed and memory cost. Table 3 shows the results. We can see that our algorithm works efficiently when C ≥ 3, but for C &lt; 3, the number of selected features grows drastically, and </context>
</contexts>
<marker>Martins, Almeida, Smith, 2013</marker>
<rawString>Andre Martins, Miguel Almeida, and Noah A. Smith. 2013. Turning on the turbo: Fast third-order nonprojective turbo parsers. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Jens Nilsson</author>
</authors>
<title>Pseudoprojective dependency parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>99--106</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="32399" citStr="Nivre and Nilsson, 2005" startWordPosition="5644" endWordPosition="5647">s grows drastically, and the program runs out of memory (OOM). 6.2 Experimental Results on CoNLL 2009 Dataset Now we looked at the impact of our system on non-English treebanks. We evaluate our system on six other languages from the CoNLL 2009 sharedtask. We used the best setting in the previous experiment: reranking model is trained using the features selected in the dependency tree kernel space. For POS tag features we used the predicted tags. As the third order parser can not handle non-projective parse trees, we used the graph transformation techniques to produce nonprojective structures (Nivre and Nilsson, 2005). First, the training data for the parser is projectivized by applying a number of lifting operations (Kahane et al., 1998) and encoding information about these lifts in arc labels. We used the path encoding scheme where the label of each arc is concatenated with two binary tags, one indicates if the arc is lifted, the other indicates if the arc is along the lifting path from the syntactic to the linear head. Then we train a projective Language Ours Official Best Chinese 76.77 79.17 Japanese 92.68 92.57 German 87.40 87.48 Spanish 87.82 87.64 Czech 80.51 80.38 Catalan 86.98 87.86 Table 4: Exper</context>
</contexts>
<marker>Nivre, Nilsson, 2005</marker>
<rawString>Joakim Nivre and Jens Nilsson. 2005. Pseudoprojective dependency parsing. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05, pages 99– 106, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Okanohara</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Learning combination features with l1 regularization.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers,</booktitle>
<pages>97--100</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="5731" citStr="Okanohara and Tsujii, 2009" startWordPosition="857" endWordPosition="860">o prune feature candidates. The key idea for efficient pruning is to estimate the upper bound of statistics of features without explicit calculation. The simplest example is frequent mining where for any n-gram feature, its frequency is bounded by any of its substrings. Suzuki et al. (Suzuki et al., 2004) proposed to select features in convolution kernel space based on their chi-squared values. They derived a concise form to estimate the upper bound of chisquare values, and used PrefixScan algorithm to enumerates all the significant sub-sequences of features efficiently. Okanohara and Tsujii (Okanohara and Tsujii, 2009) further combined the pruning technique with L1 regularization. They showed the connection between L1 regularization and frequent mining: the L1 regularizer provides a minimum support threshold to prune the gradients of parameters. They selected the combination features in a coarse-to-fine order, the gradient value for a combination feature can be bounded by each of its component feature, hence may be pruned without explicit calculation. They also sorted the features to tighten the bound. Our idea is similar with theirs, the difference is that our search space is much larger: we did not restri</context>
</contexts>
<marker>Okanohara, Tsujii, 2009</marker>
<rawString>Daisuke Okanohara and Jun’ichi Tsujii. 2009. Learning combination features with l1 regularization. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers, pages 97–100, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xian Qian</author>
<author>Yang Liu</author>
</authors>
<title>Branch and bound algorithm for dependency parsing with non-local features.</title>
<date>2013</date>
<journal>TACL,</journal>
<volume>1</volume>
<contexts>
<context position="29287" citStr="Qian and Liu, 2013" startWordPosition="5145" endWordPosition="5148">e selection, we did not use the L1 System UAS Training Time Third Order Parser 93.07 20 hrs Quadratic Kernel(QK) 93.41 6 hrs Biquadratic Kernel(BK) 93.45 6 hrs 8-th Degree Polynomial Kernel(8K) 93.27 6 hrs Dependency Tree Kernel (DTK) 93.65 10 days LM with Template Features 93.39 4 mins LM with Features in QK 93.39 9 mins LM with Features in BK 93.44 0.5 hrs LM with Features in 8K 93.30 6 hrs LM with Features in DTK 93.72 36 hrs (Zhang and McDonald, 2014) 93.57 N/A (Zhang et al., 2013) 93.50 N/A (Ma and Zhao, 2012) 93.40 N/A (Bohnet and Kuhn, 2012) 93.39 N/A (Rush and Petrov, 2012) 93.30 N/A (Qian and Liu, 2013) 93.17 N/A (Hayashi et al., 2013) 93.12 1 hr (Martins et al., 2013) 93.07 N/A (Zhang and McDonald, 2012) 93.06 N/A (Koo and Collins, 2010) 93.04 N/A (Zhang and Nivre, 2011) 92.90 N/A Table 2: Comparison between our system and the state-of-art systems on English dataset. LM is short for Linear Model, hrs, mins are short for hours and minutes respectively SVM for testing, instead, we trained an averaged perceptron with the selected features. Because we find that the averaged perceptron significantly outperforms L1 SVM. 6.1.2 Results Experimental results are listed in Table 2, all systems run on </context>
</contexts>
<marker>Qian, Liu, 2013</marker>
<rawString>Xian Qian and Yang Liu. 2013. Branch and bound algorithm for dependency parsing with non-local features. TACL, 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Rush</author>
<author>Slav Petrov</author>
</authors>
<title>Vine pruning for efficient multi-pass dependency parsing.</title>
<date>2012</date>
<booktitle>In Proc. of NAACL. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="29256" citStr="Rush and Petrov, 2012" startWordPosition="5139" endWordPosition="5142">e memory when needed. After feature selection, we did not use the L1 System UAS Training Time Third Order Parser 93.07 20 hrs Quadratic Kernel(QK) 93.41 6 hrs Biquadratic Kernel(BK) 93.45 6 hrs 8-th Degree Polynomial Kernel(8K) 93.27 6 hrs Dependency Tree Kernel (DTK) 93.65 10 days LM with Template Features 93.39 4 mins LM with Features in QK 93.39 9 mins LM with Features in BK 93.44 0.5 hrs LM with Features in 8K 93.30 6 hrs LM with Features in DTK 93.72 36 hrs (Zhang and McDonald, 2014) 93.57 N/A (Zhang et al., 2013) 93.50 N/A (Ma and Zhao, 2012) 93.40 N/A (Bohnet and Kuhn, 2012) 93.39 N/A (Rush and Petrov, 2012) 93.30 N/A (Qian and Liu, 2013) 93.17 N/A (Hayashi et al., 2013) 93.12 1 hr (Martins et al., 2013) 93.07 N/A (Zhang and McDonald, 2012) 93.06 N/A (Koo and Collins, 2010) 93.04 N/A (Zhang and Nivre, 2011) 92.90 N/A Table 2: Comparison between our system and the state-of-art systems on English dataset. LM is short for Linear Model, hrs, mins are short for hours and minutes respectively SVM for testing, instead, we trained an averaged perceptron with the selected features. Because we find that the averaged perceptron significantly outperforms L1 SVM. 6.1.2 Results Experimental results are listed </context>
</contexts>
<marker>Rush, Petrov, 2012</marker>
<rawString>Alexander Rush and Slav Petrov. 2012. Vine pruning for efficient multi-pass dependency parsing. In Proc. of NAACL. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Suzuki</author>
<author>Hideki Isozaki</author>
<author>Eisaku Maeda</author>
</authors>
<title>Convolution kernels with feature selection for natural language processing tasks.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume,</booktitle>
<pages>119--126</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="5410" citStr="Suzuki et al., 2004" startWordPosition="810" endWordPosition="813">more than 20 million features. Tan et al. (2012) proposed adaptive feature scaling (AFS) scheme for ultra-high dimensional feature selection. The dimensionality of the features in their experiments is up to 30 millions. Previous studies on feature selection in kernel space typically used mining based approaches to prune feature candidates. The key idea for efficient pruning is to estimate the upper bound of statistics of features without explicit calculation. The simplest example is frequent mining where for any n-gram feature, its frequency is bounded by any of its substrings. Suzuki et al. (Suzuki et al., 2004) proposed to select features in convolution kernel space based on their chi-squared values. They derived a concise form to estimate the upper bound of chisquare values, and used PrefixScan algorithm to enumerates all the significant sub-sequences of features efficiently. Okanohara and Tsujii (Okanohara and Tsujii, 2009) further combined the pruning technique with L1 regularization. They showed the connection between L1 regularization and frequent mining: the L1 regularizer provides a minimum support threshold to prune the gradients of parameters. They selected the combination features in a coa</context>
<context position="22506" citStr="Suzuki et al., 2004" startWordPosition="3954" endWordPosition="3957">ncorporate the dependency tree kernel for parsing is to rerank the K best candidate parse trees generated by a simple linear model. Suppose there are n training samples, the size of the kernel matrix is (K ∗ n)2, which is unacceptable for large datasets. 5.2.2 Candidate Generation For constituent parsing, Kudo et al. showed such an all-subtrees representation is extremely redundant and a comparable accuracy can be achieved using just a small set of subtrees (Kudo et al., 2005). Suzuki et al. even showed that the over-fitting problem often arises when convolution kernels are used in NLP tasks (Suzuki et al., 2004). Now we attempt to select representative sub b a 1185 feature trees in the kernel space using Algorithm 1. The r-order features in dependency tree kernel space are the sub feature trees with r arcs. The candidate feature generation in Line 17 has two steps: first we generate the subtrees with r arcs, then we generate the sub feature trees for each subtree. The simplest way for subtree generation is to enumerate the combinations of r + 2 words in the sentence, and check if these words form a subtree. We can speed up the generation by using the results of the subtrees with r + 1 words (r arcs).</context>
</contexts>
<marker>Suzuki, Isozaki, Maeda, 2004</marker>
<rawString>Jun Suzuki, Hideki Isozaki, and Eisaku Maeda. 2004. Convolution kernels with feature selection for natural language processing tasks. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume, pages 119–126, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mingkui Tan</author>
<author>Ivor W Tsang</author>
<author>Li Wang</author>
</authors>
<title>Towards large-scale and ultrahigh dimensional feature selection via feature generation.</title>
<date>2012</date>
<location>CoRR, abs/1209.5260.</location>
<contexts>
<context position="4838" citStr="Tan et al. (2012)" startWordPosition="723" endWordPosition="726">he splitSVM and approximate polynomial kernel (Wu et al., 2007). In feature selection domain, there has been plenty of work focusing on fast computation, while feature selection in extremely high dimensional feature space is relatively less studied. Zhang et al. (2006) proposed a progressive feature selection framework that splits the feature space into tractable disjoint sub-spaces such that a feature selection algorithm can be performed on each one of them, and then merges the selected features from different sub-spaces. The search space they studied contained more than 20 million features. Tan et al. (2012) proposed adaptive feature scaling (AFS) scheme for ultra-high dimensional feature selection. The dimensionality of the features in their experiments is up to 30 millions. Previous studies on feature selection in kernel space typically used mining based approaches to prune feature candidates. The key idea for efficient pruning is to estimate the upper bound of statistics of features without explicit calculation. The simplest example is frequent mining where for any n-gram feature, its frequency is bounded by any of its substrings. Suzuki et al. (Suzuki et al., 2004) proposed to select features</context>
</contexts>
<marker>Tan, Tsang, Wang, 2012</marker>
<rawString>Mingkui Tan, Ivor W. Tsang, and Li Wang. 2012. Towards large-scale and ultrahigh dimensional feature selection via feature generation. CoRR, abs/1209.5260.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
</authors>
<title>Learning Structured Prediction Models: A Large Margin Approach.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="6904" citStr="Taskar, 2004" startWordPosition="1052" endWordPosition="1053"> space is much larger: we did not restrict the number of component features. We recursively pruned the feature set and in each recursion we selected feature in a batch manner. We further adopted an efficient data structure, spectral bloom filter, to estimate the gradients for the candidate features without generating them. 3 The Proposed Method 3.1 Basic Idea Given n training samples x1 ... xn with labels y1 ... yn E Y, we extend the kernel over the input space to the joint input and output space by simply defining fT (xi, y)f(xi, y′) = K(xi, xj)I(y == y′), which is the same as Taskar’s (see (Taskar, 2004), Page 68), where f is the explicit feature map for the kernel, and I(·,·) is the indicator function. Our task is to select a subset of representative elements in the feature vector f. Unlike previously studied feature selection problems, the dimension of f could be extremely high. It is impossible to store the feature vector in the memory or even on the disk. For easy illustration, we describe our method for the polynomial kernel, and it can be easily extended to the tree kernel space. The R degree polynomial kernel space is established by a set of basic features B = {b0 = 1, b1, ... , b|B|} </context>
</contexts>
<marker>Taskar, 2004</marker>
<rawString>Ben Taskar. 2004. Learning Structured Prediction Models: A Large Margin Approach. Ph.D. thesis, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher K I Williams</author>
<author>Matthias Seeger</author>
</authors>
<title>Using the nystr¨om method to speed up kernel machines.</title>
<date>2001</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="4100" citStr="Williams and Seeger, 2001" startWordPosition="610" endWordPosition="613">roceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1180–1190, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics 2 Related Works There are two solutions for learning in ultra high dimensional feature space: kernel method and feature selection. Fast kernel methods have been intensively studied in the past few years. Recently, randomized methods have attracted more attention due to its theoretical and empirical success, such as the Nystr¨om method (Williams and Seeger, 2001) and random projection (Lu et al., 2014). In NLP domain, previous studies mainly focused on polynomial kernels, such as the splitSVM and approximate polynomial kernel (Wu et al., 2007). In feature selection domain, there has been plenty of work focusing on fast computation, while feature selection in extremely high dimensional feature space is relatively less studied. Zhang et al. (2006) proposed a progressive feature selection framework that splits the feature space into tractable disjoint sub-spaces such that a feature selection algorithm can be performed on each one of them, and then merges</context>
</contexts>
<marker>Williams, Seeger, 2001</marker>
<rawString>Christopher K. I. Williams and Matthias Seeger. 2001. Using the nystr¨om method to speed up kernel machines. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yu-Chieh Wu</author>
<author>Jie-Chi Yang</author>
<author>Yue-Shi Lee</author>
</authors>
<title>An approximate approach for training polynomial kernel svms in linear time.</title>
<date>2007</date>
<booktitle>In Proc. of ACL, ACL ’07.</booktitle>
<contexts>
<context position="4284" citStr="Wu et al., 2007" startWordPosition="639" endWordPosition="642"> July 26-31, 2015. c�2015 Association for Computational Linguistics 2 Related Works There are two solutions for learning in ultra high dimensional feature space: kernel method and feature selection. Fast kernel methods have been intensively studied in the past few years. Recently, randomized methods have attracted more attention due to its theoretical and empirical success, such as the Nystr¨om method (Williams and Seeger, 2001) and random projection (Lu et al., 2014). In NLP domain, previous studies mainly focused on polynomial kernels, such as the splitSVM and approximate polynomial kernel (Wu et al., 2007). In feature selection domain, there has been plenty of work focusing on fast computation, while feature selection in extremely high dimensional feature space is relatively less studied. Zhang et al. (2006) proposed a progressive feature selection framework that splits the feature space into tractable disjoint sub-spaces such that a feature selection algorithm can be performed on each one of them, and then merges the selected features from different sub-spaces. The search space they studied contained more than 20 million features. Tan et al. (2012) proposed adaptive feature scaling (AFS) schem</context>
</contexts>
<marker>Wu, Yang, Lee, 2007</marker>
<rawString>Yu-Chieh Wu, Jie-Chi Yang, and Yue-Shi Lee. 2007. An approximate approach for training polynomial kernel svms in linear time. In Proc. of ACL, ACL ’07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyasu Yamada</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proc. of IWPT.</booktitle>
<contexts>
<context position="25112" citStr="Yamada and Matsumoto, 2003" startWordPosition="4417" endWordPosition="4420">evel of a subtree is less than L, and for the children of each node, there are at most L2 subsequences of siblings. Therefore the time complexity for subtree extraction is O(L3). 6 Experiments 6.1 Experimental Results on English Dataset 6.1.1 Settings First we used the English Penn Tree Bank (PTB) with standard train/develop/test for evaluation. Sections 2-21 (around 40K sentences) were used as training data, section 22 was used as the development set and section 23 was used as the final test set. We extracted dependencies using Joakim Nivre’s Penn2Malt tool with Yamada and Matsumoto’s rules (Yamada and Matsumoto, 2003). Unlabeled attachment score (UAS) ignoring punctuation is used to evaluate parsing quality. We apply our technique to rerank the parse trees generated by a third order parser (Koo and Collins, 2010) trained using 10 best MIRA algorithm with 10 iterations. We generate the top 10 best candidate parse trees using 10 fold cross validation for each sentence in the training data. The gold parse tree is added if it is not in the candidate list. Then we learn a reranking model using these candidate trees. During testing, the score for a parse tree T is a linear combination of the two models: score(T)</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proc. of IWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammed J Zaki</author>
</authors>
<title>Efficiently mining frequent trees in a forest.</title>
<date>2002</date>
<booktitle>In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’02,</booktitle>
<pages>71--80</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="23779" citStr="Zaki, 2002" startWordPosition="4189" endWordPosition="4190">r and generate 5r+1 if the words form a subtree. This method has three issues: first, the time complexity is exponential in the length of the sentence, as there are 2L combinations of words, L is the sentence length; second, it may generate duplicated subtrees, and over counts the gradients. For example, there are two ways to generate the subtree He won the game in Figure 1: we can either add word He to the subtree won the game, or add word the to the subtree He won game; third, checking a fragment requires O(L) time. These issues can be solved using the well known rightmost-extension method (Zaki, 2002; Asai et al., 2002; Kudo et al., 2005) which enumerates all subtrees from a given tree efficiently. This method starts with a set of trees consisting of single nodes, and then expands each subtree attaching a new node. Specifically, it first indexes the words in the preorder of the parse tree. When generating 5r+1, only the words whose indices are larger than the greatest index of the words in 5r are considered. In this way, each subtree is generated only once. Thus, we only need to consider two types of words: (i) the children of the rightmost leaf of 5r, (ii) the adjacent right sibling of t</context>
</contexts>
<marker>Zaki, 2002</marker>
<rawString>Mohammed J. Zaki. 2002. Efficiently mining frequent trees in a forest. In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’02, pages 71– 80, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Ryan McDonald</author>
</authors>
<title>Generalized higher-order dependency parsing with cube pruning.</title>
<date>2012</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="29391" citStr="Zhang and McDonald, 2012" startWordPosition="5164" endWordPosition="5167">tic Kernel(QK) 93.41 6 hrs Biquadratic Kernel(BK) 93.45 6 hrs 8-th Degree Polynomial Kernel(8K) 93.27 6 hrs Dependency Tree Kernel (DTK) 93.65 10 days LM with Template Features 93.39 4 mins LM with Features in QK 93.39 9 mins LM with Features in BK 93.44 0.5 hrs LM with Features in 8K 93.30 6 hrs LM with Features in DTK 93.72 36 hrs (Zhang and McDonald, 2014) 93.57 N/A (Zhang et al., 2013) 93.50 N/A (Ma and Zhao, 2012) 93.40 N/A (Bohnet and Kuhn, 2012) 93.39 N/A (Rush and Petrov, 2012) 93.30 N/A (Qian and Liu, 2013) 93.17 N/A (Hayashi et al., 2013) 93.12 1 hr (Martins et al., 2013) 93.07 N/A (Zhang and McDonald, 2012) 93.06 N/A (Koo and Collins, 2010) 93.04 N/A (Zhang and Nivre, 2011) 92.90 N/A Table 2: Comparison between our system and the state-of-art systems on English dataset. LM is short for Linear Model, hrs, mins are short for hours and minutes respectively SVM for testing, instead, we trained an averaged perceptron with the selected features. Because we find that the averaged perceptron significantly outperforms L1 SVM. 6.1.2 Results Experimental results are listed in Table 2, all systems run on a 64 bit Fedora operation system with a single Intel core i7 3.40GHz and 32G memory. We also include res</context>
</contexts>
<marker>Zhang, McDonald, 2012</marker>
<rawString>Hao Zhang and Ryan McDonald. 2012. Generalized higher-order dependency parsing with cube pruning. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Ryan McDonald</author>
</authors>
<title>Enforcing structural diversity in cube-pruned dependency parsing.</title>
<date>2014</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="29127" citStr="Zhang and McDonald, 2014" startWordPosition="5115" endWordPosition="5118">t, we save the local data structure such as the selected features in Step 15 of Algorithm 1 whenever possible, and load them into the memory when needed. After feature selection, we did not use the L1 System UAS Training Time Third Order Parser 93.07 20 hrs Quadratic Kernel(QK) 93.41 6 hrs Biquadratic Kernel(BK) 93.45 6 hrs 8-th Degree Polynomial Kernel(8K) 93.27 6 hrs Dependency Tree Kernel (DTK) 93.65 10 days LM with Template Features 93.39 4 mins LM with Features in QK 93.39 9 mins LM with Features in BK 93.44 0.5 hrs LM with Features in 8K 93.30 6 hrs LM with Features in DTK 93.72 36 hrs (Zhang and McDonald, 2014) 93.57 N/A (Zhang et al., 2013) 93.50 N/A (Ma and Zhao, 2012) 93.40 N/A (Bohnet and Kuhn, 2012) 93.39 N/A (Rush and Petrov, 2012) 93.30 N/A (Qian and Liu, 2013) 93.17 N/A (Hayashi et al., 2013) 93.12 1 hr (Martins et al., 2013) 93.07 N/A (Zhang and McDonald, 2012) 93.06 N/A (Koo and Collins, 2010) 93.04 N/A (Zhang and Nivre, 2011) 92.90 N/A Table 2: Comparison between our system and the state-of-art systems on English dataset. LM is short for Linear Model, hrs, mins are short for hours and minutes respectively SVM for testing, instead, we trained an averaged perceptron with the selected featur</context>
<context position="31172" citStr="Zhang and McDonald, 2014" startWordPosition="5445" endWordPosition="5448"> sibling feature from the combination of two separated arc features. When no additional resource is available, our parser achieved the best reported performance 93.72% UAS on English PTB dataset. It is 1187 C #Feat #Template Hours Mem(G) UAS 1 0.34G N/A stalled OOM N/A 2 0.34G N/A stalled OOM N/A 3 33.1M 11.4K 36 4.0 93.72 5 6.32M 2.1K 20 2.2 93.55 10 2.10M 1.6K 5 1.4 93.40 Table 3: Feature selection in dependency kernel space with different threshold C. worth pointing that our method is orthogonal to other reported systems that benefit from advanced inference algorthms, such as cube pruning (Zhang and McDonald, 2014), AD3 (Martins et al., 2013), etc. We believe that combining our techniques with others’ will achieve further improvement. Reranking the candidate parse trees of 2416 testing sentences takes 67 seconds, about 36 sentences per second. To further understand the complexity of our algorithm, we perform feature selection in dependency tree kernel space with different thresholds C and record the number of selected features and feature templates, the speed and memory cost. Table 3 shows the results. We can see that our algorithm works efficiently when C ≥ 3, but for C &lt; 3, the number of selected feat</context>
</contexts>
<marker>Zhang, McDonald, 2014</marker>
<rawString>Hao Zhang and Ryan McDonald. 2014. Enforcing structural diversity in cube-pruned dependency parsing. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Joakim Nivre</author>
</authors>
<title>Transition-based dependency parsing with rich non-local features.</title>
<date>2011</date>
<booktitle>In Proc. of ACL-HLT.</booktitle>
<contexts>
<context position="29459" citStr="Zhang and Nivre, 2011" startWordPosition="5176" endWordPosition="5179">ee Polynomial Kernel(8K) 93.27 6 hrs Dependency Tree Kernel (DTK) 93.65 10 days LM with Template Features 93.39 4 mins LM with Features in QK 93.39 9 mins LM with Features in BK 93.44 0.5 hrs LM with Features in 8K 93.30 6 hrs LM with Features in DTK 93.72 36 hrs (Zhang and McDonald, 2014) 93.57 N/A (Zhang et al., 2013) 93.50 N/A (Ma and Zhao, 2012) 93.40 N/A (Bohnet and Kuhn, 2012) 93.39 N/A (Rush and Petrov, 2012) 93.30 N/A (Qian and Liu, 2013) 93.17 N/A (Hayashi et al., 2013) 93.12 1 hr (Martins et al., 2013) 93.07 N/A (Zhang and McDonald, 2012) 93.06 N/A (Koo and Collins, 2010) 93.04 N/A (Zhang and Nivre, 2011) 92.90 N/A Table 2: Comparison between our system and the state-of-art systems on English dataset. LM is short for Linear Model, hrs, mins are short for hours and minutes respectively SVM for testing, instead, we trained an averaged perceptron with the selected features. Because we find that the averaged perceptron significantly outperforms L1 SVM. 6.1.2 Results Experimental results are listed in Table 2, all systems run on a 64 bit Fedora operation system with a single Intel core i7 3.40GHz and 32G memory. We also include results of representative state-of-the art systems. It is clear that th</context>
</contexts>
<marker>Zhang, Nivre, 2011</marker>
<rawString>Yue Zhang and Joakim Nivre. 2011. Transition-based dependency parsing with rich non-local features. In Proc. of ACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qi Zhang</author>
<author>Fuliang Weng</author>
<author>Zhe Feng</author>
</authors>
<title>A progressive feature selection algorithm for ultra large feature spaces.</title>
<date>2006</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="4490" citStr="Zhang et al. (2006)" startWordPosition="670" endWordPosition="673"> kernel methods have been intensively studied in the past few years. Recently, randomized methods have attracted more attention due to its theoretical and empirical success, such as the Nystr¨om method (Williams and Seeger, 2001) and random projection (Lu et al., 2014). In NLP domain, previous studies mainly focused on polynomial kernels, such as the splitSVM and approximate polynomial kernel (Wu et al., 2007). In feature selection domain, there has been plenty of work focusing on fast computation, while feature selection in extremely high dimensional feature space is relatively less studied. Zhang et al. (2006) proposed a progressive feature selection framework that splits the feature space into tractable disjoint sub-spaces such that a feature selection algorithm can be performed on each one of them, and then merges the selected features from different sub-spaces. The search space they studied contained more than 20 million features. Tan et al. (2012) proposed adaptive feature scaling (AFS) scheme for ultra-high dimensional feature selection. The dimensionality of the features in their experiments is up to 30 millions. Previous studies on feature selection in kernel space typically used mining base</context>
</contexts>
<marker>Zhang, Weng, Feng, 2006</marker>
<rawString>Qi Zhang, Fuliang Weng, and Zhe Feng. 2006. A progressive feature selection algorithm for ultra large feature spaces. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Liang Huang</author>
<author>Kai Zhao</author>
<author>Ryan McDonald</author>
</authors>
<title>Online learning for inexact hypergraph search.</title>
<date>2013</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>908--913</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="29158" citStr="Zhang et al., 2013" startWordPosition="5121" endWordPosition="5124">uch as the selected features in Step 15 of Algorithm 1 whenever possible, and load them into the memory when needed. After feature selection, we did not use the L1 System UAS Training Time Third Order Parser 93.07 20 hrs Quadratic Kernel(QK) 93.41 6 hrs Biquadratic Kernel(BK) 93.45 6 hrs 8-th Degree Polynomial Kernel(8K) 93.27 6 hrs Dependency Tree Kernel (DTK) 93.65 10 days LM with Template Features 93.39 4 mins LM with Features in QK 93.39 9 mins LM with Features in BK 93.44 0.5 hrs LM with Features in 8K 93.30 6 hrs LM with Features in DTK 93.72 36 hrs (Zhang and McDonald, 2014) 93.57 N/A (Zhang et al., 2013) 93.50 N/A (Ma and Zhao, 2012) 93.40 N/A (Bohnet and Kuhn, 2012) 93.39 N/A (Rush and Petrov, 2012) 93.30 N/A (Qian and Liu, 2013) 93.17 N/A (Hayashi et al., 2013) 93.12 1 hr (Martins et al., 2013) 93.07 N/A (Zhang and McDonald, 2012) 93.06 N/A (Koo and Collins, 2010) 93.04 N/A (Zhang and Nivre, 2011) 92.90 N/A Table 2: Comparison between our system and the state-of-art systems on English dataset. LM is short for Linear Model, hrs, mins are short for hours and minutes respectively SVM for testing, instead, we trained an averaged perceptron with the selected features. Because we find that the av</context>
</contexts>
<marker>Zhang, Huang, Zhao, McDonald, 2013</marker>
<rawString>Hao Zhang, Liang Huang, Kai Zhao, and Ryan McDonald. 2013. Online learning for inexact hypergraph search. In Proc. of EMNLP, pages 908– 913. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>