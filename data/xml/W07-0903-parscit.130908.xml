<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000024">
<title confidence="0.9807535">
Retrieving lost information from textual databases:
rediscovering expeditions from an animal specimen database
</title>
<author confidence="0.863604">
Marieke van Erp
</author>
<affiliation confidence="0.8382225">
Dept. of Language and Information Sciences
Tilburg University, P.O. Box 90153
</affiliation>
<address confidence="0.694192">
NL-5000 LE Tilburg, The Netherlands
</address>
<email confidence="0.997893">
M.G.J.vanErp@uvt.nl
</email>
<sectionHeader confidence="0.995629" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999922076923077">
Importing large amounts of data into
databases does not always go without the
loss of important information. In this work,
methods are presented that aim to rediscover
this information by inferring it from the in-
formation that is available in the database.
From and animal specimen database, the
information to which expedition an ani-
mal that was found belongs is rediscovered.
While the work is in an early stage, the ob-
tained results are promising, and prove that
it is possible to rediscover expedition infor-
mation from the database.
</bodyText>
<sectionHeader confidence="0.998981" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999397733333333">
Databases made up of textual material tend to con-
tain a wealth of information that remains unexplored
with simple keyword-based search. Maintainers of
the databases are often not aware of the possibilities
offered by text mining methods to discover hidden
information to enrich the basic data. In this work
several machine learning methods are explored to
investigate whether ‘hidden information’ can be ex-
tracted from an animal specimen database belonging
to the Dutch National Museum for Natural History,
Naturalis1. The database is a combination of infor-
mation about objects in the museum collection from
handwritten data sources in the museum, such as
journal-like entries that are kept by biologists while
collecting animal or plant specimens on expedition
</bodyText>
<footnote confidence="0.894902">
1http://www.naturalis.nl
</footnote>
<bodyText confidence="0.995518029411765">
and tables that link the journal entries to the mu-
seum register. What is not preserved in the transition
from the written sources to the database is the name
of the expedition druing which an animal specimen
was found.
By expedition, the following event is implied: a
group of biologists went on expedition together in
a country during a certain time period. Entries in
the database that belong to this expedition can be
collected by one or a subset of the participating bi-
ologists. For researchers at the natural history mu-
seum it would be helpful to have access to expedi-
tion information in their database, as for biodiver-
sity research they sometimes need overviews of ex-
peditions. It may also help further enrichment of
the database and cleansing, because if the expedi-
tion information is available, missing information in
certain fields, such as the country where a specimen
was found, may be inferred from the information on
other specimens found during the same expedition.
Currently, if one wants to retrieve all objects from
the database that belong to an expedition, one would
have to create a database query that contains the ex-
act data boundaries of the expeditions and the names
of all collectors involved. Either one of these bits of
information is not enough, as the same group of bi-
ologists may have participated in an expedition more
than once, and the database may also contain expe-
ditions that overlap in time. In this paper a series
of experiments is described to find a way to infer
expedition information from the information avail-
able in the database. To this end, three approaches
are compared: supervised machine learning, unsu-
pervised machine learning, and rule-based methods.
</bodyText>
<page confidence="0.991894">
17
</page>
<note confidence="0.991867">
Proceedings of the Workshop on Language Technology for Cultural Heritage Data (LaTeCH 2007), pages 17–24,
Prague, 28 June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.997188333333333">
The obtained results vary, but prove that it is pos-
sible to extract the expedition information from the
data at hand.
</bodyText>
<sectionHeader confidence="0.999291" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999961">
The field of data mining, which is concerned with
the extraction of implicit, previously unknown and
potentially useful information from data (Frawley et
al., 1992), is a branch of research that has become
quite important recently as every day the world is
flooded with larger amounts of information that are
impossible to analyse manually. Data mining can,
for instance, help banks identify suspicious trans-
actions among the millions of transactions that are
executed daily (Fayyad and Uthurusamy, 1996), or
automatically classify protein sequences in genome
databases (Mewes et al., 1999), or aid a company
in creating better customer profiles to present cus-
tomers with personalised ads and notifications (Lin-
den et al., 2003). Knowledge discovery approaches
often rely on machine learning techniques as these
are particularly well suited to process large amounts
of data to find similarities or dissimilarities between
instances (Mitchell, 1997).
Traditionally, governments and companies have
been interested in gaining more insight into their
data by applying data mining techniques. Only re-
cently , digitisation of data in the cultural heritage
domain has taken off, which means that there has
not been much work done on knowledge discovery
in this domain. Databases in this domain are often
created and maintained manually and are thus of-
ten significantly smaller than automatically gener-
ated databases from, for example, customers’ pur-
chase information in a large company.
This means it is not clear whether data mining
techniques, aimed at analysing enormous amounts
of data, will work for the data at hand. This is in-
vestigated here. Manual data typically also contains
more spelling variations/errors and other inconsis-
tencies than automatically generated databases, due
to different persons entering data into the database.
Therefore, before one can start the actual process of
knowledge discovery, it is very important to care-
fully select, clean and model the data one wants
to use in order to avoid using data that is too
sparse (Chapman, 2003). This applies in particular
to databases that contain large amounts of textual in-
formation, which are quite prevalent in the cultural
heritage domain. Examples of textual databases can
be found freely on the internet, such as the databases
of the Global Biodiversity Information Facility2, the
University of St. Andrews Photographic Collec-
tion3, and the Internet Movie Database4.
</bodyText>
<sectionHeader confidence="0.99616" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.999985888888889">
The data that has been used in this experiment is an
animal specimen database from the Dutch National
Museum for Natural History. The database currently
contains 16,870 entries that each represent an object
stored in the museum’s reptiles and amphibians col-
lection. The entries provide a variety of information
about the objects in 37 columns, such as the scien-
tific name of the object, how the specimen is kept
(in alcohol, stuffed, pinned) and under which regis-
tration number, where it was found, by whom and
under which circumstances, the name of the person
who determined the species of the animal and the
name of the person who first described the species.
Most fields are rather compact; they only contain a
numeric value or a textual value consisting of one or
several words. The database also contains fields of
which the entries consist of longer stretches of text,
such as the ‘special remarks’ field, describing any-
thing about the object that did not fit in the other
database fields and ‘biotope’, describing the biotic
and abiotic components of the habitat from which
the object was collected. Dutch is the most frequent
language in the database, followed by English. Also
some Portuguese and German entries occur. Taxo-
nomic values, i.e., the scientific names of the animal
specimens, are in a restricted type of Latin. A snip-
pet of the database can be found in Figure 1.
</bodyText>
<subsectionHeader confidence="0.999213">
3.1 Data Construction
</subsectionHeader>
<bodyText confidence="0.9999775">
In order to be able the measure the performance of
the approaches used in the experiments, the database
was annotated manually with expedition informa-
tion. Adding this information was possible because
there was access to the original field books from
which the database is made up. Annotating 8166
</bodyText>
<footnote confidence="0.999877333333333">
2http://www.gbif.org/
3http://special.st-andrews.ac.uk/saspecial/
4http://www.imdb.com/
</footnote>
<page confidence="0.99667">
18
</page>
<table confidence="0.989894777777778">
Collector Coll. Date Coll. # Class Genus Species Country Expedition
Buttikofer, J. 30-07-1881 424 Reptilia Lamprolepis lineatus 132 buttikoferliberia1881
Buttikofer, J. &amp; Sala 09-10-1881 504 Amphibia Bufo regularis 132 buttikoferliberia1881
M. Dachsel 02-05-1971 971-MSH186 Reptilia Blanus mettetali 156 mshbrazil71
Hoogmoed, M.S. 04-05-1971 1971-MSH187 Reptilia Quendenfeldtia trachyblepharus 156 mshbrazil71
Hoogmoed, M.S. 09-05-1971 1971-MSH202 Reptilia Lacerta hispanica 156 mshbrazil71
C. Schuil 14-03-1972 1972-MSH35 Amphibia Ptychadaena sp. 92 mshghana72
P. Lavelle -03-1972 1972-MSH40 Reptilia Crotaphopeltis hotamboeia 92 mshghana72
Hoogmoed, M.S. 23-03-1972 1972-MSH55 Amphibia Phrynobatrachus plicatus 92 mshghana72
</table>
<figureCaption confidence="0.998489">
Figure 1: Snippet of the animal specimen database
</figureCaption>
<bodyText confidence="0.999896346153847">
entries with this information took one person about
2 days. There were 8704 entries to which no ex-
pedition is assigned, either because these specimens
were not collected during an expedition or because it
was not possible to determine the expedition. These
entries were excluded from the experiments . Ex-
peditions which contained 10 or fewer entries were
also excluded because these would make the data
set too sparse. A total of 7831 database entries
were used in this work, divided into 60 expeditions.
Although the ‘smallest’ expeditions were excluded
from the experiments, the sizes of the expeditions
still vary greatly: between 2170 and 11 items (Q =
310.04). This is mainly due to the fact that new
items are still added to the database continuously, in
a rather random order, hence some expeditions are
more completely represented than others.
The database contains several fields that contain
information that will probably not be that useful for
this work. Information that was excluded was the
specimen’s sex, the number of specimens (in cases
where one database entry refers to several speci-
mens, for instance kept together in a jar), how the
animal is preserved, and fields that contain informa-
tion not on the specimen itself or how it was found
but on the database (e.g., when the database entry
was added and by whom). Values from the ‘alti-
tude’ and ‘coordinates’ fields were also not included
in the experiments as this is information is too of-
ten missing in the database to be of any use (altitude
information is missing in 85% of the entries and co-
ordinates in 96%).
Some information in the database is repetitive;
there is for instance a field called ‘country’ contain-
ing the name of the country in which a specimen was
found, but there is also a field called ‘country-id’ in
which the same information is encoded as a numer-
ical value. The latter is more often filled than the
‘country’ field, which also contains values in differ-
ent languages, and thus it makes more sense to only
include values from the ‘country-id’ field. A small
conversion is applied to rule out that an algorithm
will interpret the intervals between the different val-
ues as a measure of geographical proximity between
the values, as the country values are chosen alpha-
betically and do not encode geographical location.
In some cases it seemed useful to have an al-
gorithm employ interval relations between num-
bers. The fields ‘registration number’ and ‘collec-
tion number’ were used as such. These fields some-
times contain some alphabetical values: certain col-
lectors, for instance, included their initials in their
series of collection registration numbers. These
were converted to a numeric code to obtain com-
pletely numeric values with preservation of the col-
lector information. This also goes for the fields in
the database that contain information on dates, i.e.,
the ‘date of determination’, the ‘date the specimen
came into the museum’ and the ‘collection date’
fields. The collection date is the most important
date here as this directly links to an expedition. The
other dates might provide indirect information, for
instance if the collection date is missing (which is
the case in 14%). To aid clustering, the dates were
normalised to a number, possibly the algorithm ben-
efits from the fact that a small numerical interval
means that the dates are close together.
Person names from the ‘author’, ‘collector’, ‘de-
terminer’, and ‘donator’ fields were normalised to
a ‘first name - last name’ format. From values
from the taxonomic fields (‘class’, ‘order’, ‘fam-
ily’, ‘genus’, ‘species’, and ‘sub species’), and
‘town/village’ and ‘province/state’ fields, as well as
from the person name fields, capitals, umlauts, ac-
cents and any other non-alphanumerical characters
were removed.
It proved that certain database fields were not suit-
able for inclusion in the experiments. This goes for
</bodyText>
<page confidence="0.995419">
19
</page>
<bodyText confidence="0.999977444444444">
the free text fields ‘biotope’, ‘location’ and ‘special
remarks’. Treating these values as they are will re-
sult in data that is too sparse, as their values are ex-
tremely varied. Preliminary experiments to see if
it was possible to select only certain parts of these
fields did not yield any satisfying results and was
therefore abandoned.
This resulted in feature vectors containing 18 fea-
tures, plus the manually assigned expedition class.
</bodyText>
<sectionHeader confidence="0.997925" genericHeader="method">
4 Methodology
</sectionHeader>
<bodyText confidence="0.999974375">
The majority of the experiments that were carried
out in an attempt to infer the expedition informa-
tion from the database involved machine learning.
Therefore in this section three algorithms for super-
vised learning are described, followed by a cluster-
ing algorithm for unsupervised learning. This sec-
tion is concluded with a description of the evaluation
metrics for clusters used by the different approaches.
</bodyText>
<sectionHeader confidence="0.389888" genericHeader="method">
Algorithms
</sectionHeader>
<bodyText confidence="0.998104">
The first algorithm that was used is the k-Nearest
Neighbour algorithm (k-NN) (Aha et al., 1991;
Cover and Hart, 1967; DeVijver and Kittler, 1982).
This algorithm is an example of a lazy learner: it
does not model the training data it is given, but sim-
ply stores each instance of the training data in mem-
ory. During classification it compares the item it
needs to classify to each item in its memory and
assigns the majority class of the closest k (in these
experiments k=1) instances to the new item. To
determine which instances are closest, a variety of
distance metrics can be applied. In this experi-
ment the standard settings in the TiMBL implemen-
tation (Daelemans et al., 2004), developed at the
ILK research group at Tilburg University, were used.
The standard distance metric in the TiMLB imple-
mentation of k-NN is the Overlap metric, given in
Equations1 and 2. A(X,Y) is the distance between
instances X and Y, represented by n features, where
6 is the distance between the features.
</bodyText>
<equation confidence="0.964695857142857">
n
A(X, Y ) = s(xi, yi) (1)
i=1
where:
s(xi, yi) = { abs if numeric, else (2)
0 ifxi = yi
1 ifxi =� yi
</equation>
<bodyText confidence="0.999911111111111">
The second algorithm that was used is the C4.5
decision tree algorithm (Quinlan, 1986). In the
learning phase, it creates a decision tree in a re-
cursive top-down process in which the database is
partitioned according to the feature that separates
the classes best; each node in the tree represents
one partition. Deeper nodes represent more class-
homogeneous partitions. During classification, C4.5
traverses the tree in a deterministic top-down pass
until it meets a class-homogeneous end node, or a
non-ending node when a feature-value test is not
represented in the tree.
Naive Bayes is the third algorithm that was used
in the experiments. It computes the probability
of a certain expedition, given the observed train-
ing data according to the formula given in Equa-
tion 3. In this formula vNB is the target expedition
value, chosen from the maximally probably hypoth-
</bodyText>
<equation confidence="0.8668475">
esis (argmax
υj�V P(vj), i.e., the expedition with the high-
est probability) given the product of the probabilities
�
of the features (i P(ai|vj)).
argmax �7
vjEV P(vj)11
i
</equation>
<bodyText confidence="0.999937666666667">
For both the C4.5 algorithm and Naive Bayes the
WEKA machine learning environment (Witten and
Frank, 2005), that was developed at the University
of Waikato, New Zealand, was used.
A quite different machine learning approach that
was applied to try to identify expeditions in the rep-
tiles and amphibians database is clustering. Clus-
tering methods are unsupervised, i.e., they do not
require annotated data, and in some cases not even
the number of expeditions that are in the data. Items
in the data set are grouped according to similarity.
A maximum dissimilarity between the group mem-
bers may be specified to steer the algorithm, but
other than that it runs on its own. For an exten-
sive overview of clustering methods see Jain et al.,
(1999). For this work, the options in choosing an
implementation of a clustering algorithm were lim-
ited because many data mining tools are designed
</bodyText>
<equation confidence="0.9988075">
vNB =
P(aiIvj) (3)
</equation>
<page confidence="0.944827">
20
</page>
<bodyText confidence="0.9999595">
only for numerical data, therefore the WEKA ma-
chine learning environment was also used for the
clustering experiments. As clustering is computa-
tionally expensive, it was only possible to run ex-
periments with WEKA’s implementation of the Ex-
pectation Maximisation (EM) algorithm (Dempster
et al., 1977). Preliminary experiments with other al-
gorithms indicated execution times in the order of
months. The EM algorithm iteratively tries to con-
verge to a maximum likelihood by first computing
an expectation of the likelihood of a certain cluster-
ing, then maximising this likelihood by computing
the maximum likelihood estimates of the features.
Termination of the algorithm occurs when the pre-
defined number of iterations has been carried out,
or when the overall likelihood (the measure of how
‘good’ a clustering is) does not increase significantly
with each iteration.
</bodyText>
<subsectionHeader confidence="0.683485">
Cluster Evaluation
</subsectionHeader>
<bodyText confidence="0.999974">
Since the data is annotated with expedition in-
formation it was possible to use external quality
measures (Steinbach et al., 2000). Three differ-
ent evaluation measures were used: accuracy, en-
tropy (Shannon, 1948), and the F-measure (van Ri-
jsbergen, 1979).
The evaluation of results for the supervised learn-
ing algorithms was calculated in a straightforward
way: because the classifier knows which expedi-
tions there are and which entries belong to which
expedition, it checks the expeditions it assigned to
the database entries to the manually assigned expe-
ditions and reports the overlap as accuracy.
It gets a little bit more complicated with entropy.
Entropy is a measure of informativity, i.e., the min-
imum number of bits of information needed to en-
code the classification of each instance. If the ex-
pedition clusters are uniform, i.e., all items in the
cluster are very similar, the entropy will be low. The
main problem with using entropy for evaluation of
clusters is that the best score (an entropy of 0) is
reached when every cluster contains exactly on in-
stance. Entropy is calculated as follows: first, the
main class distribution, i.e., per cluster the probabil-
ity that a member of that cluster belongs to a certain
cluster, is computed. Using that distribution the en-
tropy of each cluster is calculated via the formula in
Equation 4. For a set of clusters the total entropy
is then computed via the formula in Equation 5, in
which m is the total number of clusters, sy the size
of cluster y and n the total number of instances.
</bodyText>
<equation confidence="0.998852666666667">
�Ey = − Pxylo9(Pxy) (4)
x
Etotal =
</equation>
<bodyText confidence="0.999398111111111">
The F-measure is the harmonic mean of precision
and recall, and is commonly used in information re-
trieval. In information retrieval recall is the propor-
tion of relevant documents retrieved out of the total
set of relevant documents. When applied to clus-
tering a ‘relevant document’ is an instance that is
assigned correctly to a certain expedition, the set of
all relevant documents is the set of all instances be-
loning to that expedition. Precision is the number of
relevant documents retrieved from the total number
of documents. So when applied to cluster evalua-
tion this means the number of instances of an expe-
dition that were retrieved from the total number of
instances (Larsen and Aone, 1999). This boils down
to Equations 6 and 7 in which x stands for expedi-
tion, y for cluster, nxy for the number of instances
belonging to expedition x that were assigned to clus-
ter y, and nx is the number of items in expedition x.
</bodyText>
<equation confidence="0.930697833333333">
nxy
Recall(x,
y) = nx
nxy
Precision(x, y) =
ny
</equation>
<bodyText confidence="0.999799666666667">
The F-measure for a cluster y with respect to ex-
pedition x is then computed via Equation 8. The
F-measure of the entire set of clusters is computed
through the function in Equation 9, which takes the
weighted average of the maximum F-measure per
expedition.
</bodyText>
<equation confidence="0.980774125">
2 · Recall(x, y) · Precision(x, y)
F(x, y) = Precision(x, y) + Recall(x, y) (8)
max{F(x, y)} (9)
M sy · Ey
y−1 n
(5)
�F =
x
</equation>
<page confidence="0.727647">
nx
n
21
</page>
<sectionHeader confidence="0.99391" genericHeader="evaluation">
5 Experiments and Results
</sectionHeader>
<bodyText confidence="0.99995609375">
First, two baselines were set to illustrate the situation
if no machine learning or other techniques would be
applied to the database. if one were to randomly
assign one of the 60 expeditions t the entries this
would go well in 1.7% of the cases. If all entries
were labelled as belonging to the largest expedition
this would yield an accuracy of 28%. In all ma-
chine learning experiments 10-fold cross validation
was used for testing performance.
A series of supervised machine learning experi-
ments was carried out first to investigate whether it
is possible to extract the expeditions during which
the animal specimens were found at all. Three
learning algorithms were applied to the complete
data set, which yielded accuracies between 88%
and 98%. Feature selection experiments with the
C4.5 decision tree algorithm indicated that fea-
tures ‘town/village’, ‘collection number’, ‘registra-
tion number’, ‘collector’ and ‘collection date’ were
considered most informative for this task, hence the
experiments were repeated with a data set contain-
ing only those features. The results of both series
of experiments are to be found in Table 1. For
the C4.5 and Naive Bayes experiments the accu-
racy deteriorates significantly when using only the
selected features (α = 0.05, computing using McNe-
mar’s test (McNemar, 1962)), but it stays stable for
the k-NN classifier. This indicates that not all data
is needed to infer the expeditions, but that it mat-
ters greatly which approach is taken. However, as
neither of the algorithm benefits from it, feature se-
lection was not further explored.
</bodyText>
<table confidence="0.99962425">
Algorithm All feat. Sel. feat.
k-NN 95.9% 95.9%
C.4.5 98.3% 94.4%
NaiveBayes 88.1% 73.5%
</table>
<tableCaption confidence="0.999607">
Table 1: Accuracy of supervised machine learning
</tableCaption>
<bodyText confidence="0.9736476875">
experiments using all features and selected features
In these experiments all database entries were an-
notated with expedition information, which in a real
setting is of course not the case. Through running
a series of experiments with significantly smaller
amounts of training data it was found that by us-
ing only as little as 5% of the training data (amount-
ing to 392 instances) already an accuracy of 85% is
reached. Annotating this amount of data with expe-
Anntating this amount of data with expediio i
dition information would take on person less than an
formaion woud tak one person less than an hou
hour. By only using 45% of the training data an ac-
curacy of 97% is reached5. In Figure 2 the complete
curve of the k-NN classifier is shown.
learning curve of the k-NN classifier is shown.
</bodyText>
<figureCaption confidence="0.9979665">
Fg. 2: Accuracy of k-N per percentage of training
Figure 2: Accuracy of k-NN per percentage of train-
</figureCaption>
<bodyText confidence="0.923906206896551">
dt
ing data
Ideally, one does not want to annotate data at all,
refore the use of custering algorithms was explored
Fo hi the EM lgoih fom he WEKA Mhi
Learning envionmen was used The results as show
therefore the use of a clustering algorithm was ex-
plored. For this, he EM lgorithm from the WEKA
n Table 2, are not quite satisfying, but still well above
machine larning environmnt was usd. The re-
he set baselines As can be seen in Table the clus
sult,as shown in Table 2, is not quite satisfying, but
ering agorithms do no come up with anywhere near
my clut a dd nd nfttely WEKA
does notpresent te use with many options to remedy
still well above the set baselines. As can be seen in
Table 2, th clustering algorithm does ot com up
his. An ntermediate experiment between completely
supervised and unsupervised was attemped, i., pre
with anywhere near as many clusters as needed and
pcyg umber o ts eagm t
dfi bt thi
unfortunately WEKA does not present the user with
ttinll t p t
carryut
many options to remedy this. An intermediate ex-
periment between completely supervised and unsu-
pervised machine learning was attempted, i.e., pre-
Algorithm # Custers Accuracy
</bodyText>
<equation confidence="0.539086">
EM 0%
</equation>
<bodyText confidence="0.989197">
specifying a number of clusters for the algorithm to
define, ut this was cmptatioally too expensive
</bodyText>
<tableCaption confidence="0.989783">
Table 2: Resuls of clustering experiments
</tableCaption>
<table confidence="0.896471625">
to carry out.
is satisfyi enough to se in a rea
Algorithm # Clusters Accuracy
ised l d
otated
EM qu 46.0%
7 proach
ff
</table>
<bodyText confidence="0.714485375">
an
accuracy
lector informaion and cuntry information.
Since the clustering algorithm does not achieve
that is satisfying enough to use in a real
1 Sort date in ascending order start a new exsetting and suprvisd learing requires annoated
pedition when the distance between two dates isdata, also a trdiionl, and quie different approach
greater than the avrage distance of the collection
</bodyText>
<tableCaption confidence="0.885457">
ple rues the dataset was split into possi
Table 2: Result of clustering experiment
</tableCaption>
<footnote confidence="0.847098833333333">
slightly higher achieved accuracy in the learning curve
5The
experiments is due to the fact that the learnig curve was not
2 First sort collector information in ascending or-
computed via cross-validation
der, then sort collecti
</footnote>
<page confidence="0.996255">
22
</page>
<bodyText confidence="0.995269666666667">
was tried: namely finding expeditions via rules. Via
a couple of simple rules the data set was split into
possible expeditions using only information on col-
lection dates, collector information and country in-
formation.
1. Sort dates in ascending order, start a new expe-
dition when the distance between two sequen-
tial dates is greater than the average distance of
the collection dates
2. First, sort collector information in ascending
order, then sort collection dates in ascending
order, start a new expeditions when the distance
between two dates is greater than the average
distance between dates or when a new collector
is encountered
3. First, sort by country information, then by col-
lector, and finally by collection date, start a new
expedition when country or collectors change,
or when the distance between two dates is
greater than the average distance between dates
Surprisingly, only grouping collection dates al-
ready yields an F-measure of .83. This includes
1299 entries that contain no information on the col-
lection date, leaving those out would increase preci-
sion on the entries whose collection date is not miss-
ing to an F-measure of .94. In Table 3 results of
the rule-based experiments are shown. It is expected
that when the database is further populated the date-
rule will work less well as there will be more expe-
ditions that overlap. The date+collector-rule should
remedy this, although it does not work very well yet
as spelling variations in the collector names are not
taken into account at the moment.
</bodyText>
<table confidence="0.99670975">
Rules # Clusters F-measure Entropy
1 78 .83 .16
2 199 .75 .15
3 216 .73 .11
</table>
<tableCaption confidence="0.999827">
Table 3: Results of the rule-based experiments
</tableCaption>
<sectionHeader confidence="0.995884" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.99997056">
In this work various approaches were presented to
rediscover expedition information from an animal
specimen database. As expected, the supervised
learning algorithms performed best, but the disad-
vantage in using such an approach is the require-
ment to provide annotated data. However, a series
of experiments to gain more insight into the quanti-
ties of data necessary for a supervised approach to
perform well, indicate that only a small set of an-
notated data is required in this case to obtain very
reasonable results. If no training data is available,
a rule-based approach is a realistic alternative. Al-
though it must be kept in mind that rules need to be
created manually for every new data set. For this
data set relatively simple rules already proved to be
quite effective, but for other data sets deriving rules
can be much more complicated and thus more ex-
pensive. This particular set of rules is also expected
to behave differently when the database is extended
with more entries from overlapping expeditions.
For the experiments presented in this work, only
entries from the database of which the expedition
they belonged to was known were used, which
constitutes only half of the database entries. Re-
searchers at Naturalis estimate that about 30% of
the database entries do not belong to an expedi-
tion, while the other 20% not included here belong
to unknown expeditions. The decision to exclude
the expedition-less entries was made as these en-
tries would imbalance the data and impair evalua-
tion as it would not be possible to check predictions
against a ‘real value’. If all database entries would
belong to a known expedition the performance of
the approaches described in this paper that satisfac-
tory results could be achieved over the complete data
set. To prove this hypothesis one would need to
test the approaches on other data sets which can be
completely annotated. Performing such tests might
provide more insight into how well the approaches
would deal with a data set where all entries have
an associated expedition. The natural history mu-
seum has several other similar (but smaller) data
sets, which might be suitable for this task, and which
will be tested as part of future work for evaluating
the approaches described here. It may also be inter-
esting to investigate what can be inferred from the
other fields defined in other data sets.
A less satisfying aspect of the research described
in this paper is that many of the intended experi-
ments with unsupervised machine learning were too
</bodyText>
<page confidence="0.991405">
23
</page>
<bodyText confidence="0.999969090909091">
computationally expensive to be executed. Potential
workarounds to the limitation of certain implemen-
tations of clustering algorithms, in that they only
work on numeric data, are sought in converting the
textual data to numeric values and in the investi-
gations into implementations of algorithms that can
deal with textual data.
A particular peculiarity of textual data, from
which the rule-based approach suffers, is the fact
that the same name or meaning can be conveyed in
several ways. Spelling variations and errors were
for instance not normalised. Hence the approaches
treated ‘Hoogmoed’ and ‘M S Hoogmoed’ as two
different values whereas they may very well refer to
the same entity.
From this work it can be concluded that the ex-
pedition information can definitely be reconstructed
from the animal specimen database that was used
here, but for it to be used in a real world applica-
tion it still needs to be tested and fine-tuned on other
data sets and extended to be able to deal with entries
that are not associated with any expedition.
</bodyText>
<sectionHeader confidence="0.998201" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999495">
The research reported in this paper was funded by
NWO, the Netherlands Organisation of Scientific
Research as part of the CATCH programme. The
author would like to thank the anonymous reviewers,
and Antal van den Bosch and Caroline Sporleder for
their helpful suggestions and comments.
</bodyText>
<sectionHeader confidence="0.999281" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99987001724138">
David W. Aha, Dennis Kibler, and Mark K. Albert. 1991.
Instance-based learning algorithms. Machine Learn-
ing, 6:37–66.
Arthur D. Chapman. 2003. Notes on Environmen-
tal Data Quality-b. Data Cleaning Tools. Internal re-
port, Centro de Referˆencia em Informac¸˜ao Ambiental
(CRIA).
T. M. Cover and P. E. Hart. 1967. Nearest neighbor
pattern classification. Institute of Electrical and Elec-
tronics Engineers Transactions on Information The-
ory, 13:21–27.
Walter Daelemans, Jakub Zavrel, Ko Van der Sloot, and
Antal Van den Bosch. 2004. Timbl: Tilburg memory
based learner, version 5.1, reference guide. Technical
Report 04-02, ILK/Tilburg University.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data using the
em algorithm. Journal of the Royal Statistical Society.
Series B (Methodology), 39(1):1–38.
P. A. DeVijver and J. Kittler. 1982. Pattern recognition.
A statistical approach. Prentice-Hall, London.
U. Fayyad and R. Uthurusamy. 1996. Data mining and
knowledge discovery in databases. Communications
of the ACM, 39(11):24–26.
William J. Frawley, Gregory Piatetsky-Shapiro, and
Christopher J. Matheus. 1992. Knowledge discovery
in databases: An overview. AI Magazine, 13:57–70.
A. K. Jain, M. N. Murty, and P. J. Flynn. 1999.
Data clustering: A review. ACM Computing Surveys,
31(3):264–323, September.
Bjorner Larsen and Chinatsu Aone. 1999. Fast and effec-
tive text mining using linear-time document clustering.
In Proceedings of KDD-99, San Diego, CA.
G. Linden, B. Smith, and J. York. 2003. Amazon.com
recommendations: item-to-item collaborative filtering.
IEEE Internet Computing, 7(1):76–80, Jan/Feb.
Q. McNemar. 1962. Psychological Statistics. Wiley,
New York.
H. W. Mewes, K. Heumann, A. Kaps, K. Mayer, F. Pfeif-
fer, S. Stocker, and D. Frishman. 1999. Mips: a
database for genomes and protein sequences. Nucleic
Acids Research, 27(1):44–48.
Tom M. Mitchell. 1997. Machine Learning. McGraw-
Hill.
J. R. Quinlan. 1986. Induction of decision trees. Ma-
chine Learning, 1:81–106.
Claude E. Shannon. 1948. A mathematical theory
of communication. Bell System Technical Journal,
27:379–423, July.
Michael Steinbach, George Karypis, and Vipin Kumar.
2000. A comparison of document clustering tech-
niques. Technical report, Department of Computer
Science, University of Minnesota.
Cornelis Joost van Rijsbergen. 1979. Information Re-
trieval. Buttersworth.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical machine learning tools and techniques. Morgan
Kaufmann, San Francisco, 2nd edition.
</reference>
<page confidence="0.999173">
24
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.839206">
<title confidence="0.974431">Retrieving lost information from textual rediscovering expeditions from an animal specimen database</title>
<author confidence="0.999675">Marieke van</author>
<affiliation confidence="0.970797">Dept. of Language and Information Tilburg University, P.O. Box</affiliation>
<address confidence="0.99318">NL-5000 LE Tilburg, The</address>
<email confidence="0.994219">M.G.J.vanErp@uvt.nl</email>
<abstract confidence="0.995969214285714">Importing large amounts of data into databases does not always go without the loss of important information. In this work, methods are presented that aim to rediscover this information by inferring it from the information that is available in the database. From and animal specimen database, the information to which expedition an animal that was found belongs is rediscovered. While the work is in an early stage, the obtained results are promising, and prove that it is possible to rediscover expedition information from the database.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David W Aha</author>
<author>Dennis Kibler</author>
<author>Mark K Albert</author>
</authors>
<title>Instance-based learning algorithms.</title>
<date>1991</date>
<booktitle>Machine Learning,</booktitle>
<pages>6--37</pages>
<contexts>
<context position="13500" citStr="Aha et al., 1991" startWordPosition="2132" endWordPosition="2135">feature vectors containing 18 features, plus the manually assigned expedition class. 4 Methodology The majority of the experiments that were carried out in an attempt to infer the expedition information from the database involved machine learning. Therefore in this section three algorithms for supervised learning are described, followed by a clustering algorithm for unsupervised learning. This section is concluded with a description of the evaluation metrics for clusters used by the different approaches. Algorithms The first algorithm that was used is the k-Nearest Neighbour algorithm (k-NN) (Aha et al., 1991; Cover and Hart, 1967; DeVijver and Kittler, 1982). This algorithm is an example of a lazy learner: it does not model the training data it is given, but simply stores each instance of the training data in memory. During classification it compares the item it needs to classify to each item in its memory and assigns the majority class of the closest k (in these experiments k=1) instances to the new item. To determine which instances are closest, a variety of distance metrics can be applied. In this experiment the standard settings in the TiMBL implementation (Daelemans et al., 2004), developed </context>
</contexts>
<marker>Aha, Kibler, Albert, 1991</marker>
<rawString>David W. Aha, Dennis Kibler, and Mark K. Albert. 1991. Instance-based learning algorithms. Machine Learning, 6:37–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur D Chapman</author>
</authors>
<title>Notes on Environmental Data Quality-b. Data Cleaning Tools. Internal report, Centro de Referˆencia em Informac¸˜ao Ambiental (CRIA).</title>
<date>2003</date>
<contexts>
<context position="5681" citStr="Chapman, 2003" startWordPosition="896" endWordPosition="897">hase information in a large company. This means it is not clear whether data mining techniques, aimed at analysing enormous amounts of data, will work for the data at hand. This is investigated here. Manual data typically also contains more spelling variations/errors and other inconsistencies than automatically generated databases, due to different persons entering data into the database. Therefore, before one can start the actual process of knowledge discovery, it is very important to carefully select, clean and model the data one wants to use in order to avoid using data that is too sparse (Chapman, 2003). This applies in particular to databases that contain large amounts of textual information, which are quite prevalent in the cultural heritage domain. Examples of textual databases can be found freely on the internet, such as the databases of the Global Biodiversity Information Facility2, the University of St. Andrews Photographic Collection3, and the Internet Movie Database4. 3 Data The data that has been used in this experiment is an animal specimen database from the Dutch National Museum for Natural History. The database currently contains 16,870 entries that each represent an object store</context>
</contexts>
<marker>Chapman, 2003</marker>
<rawString>Arthur D. Chapman. 2003. Notes on Environmental Data Quality-b. Data Cleaning Tools. Internal report, Centro de Referˆencia em Informac¸˜ao Ambiental (CRIA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T M Cover</author>
<author>P E Hart</author>
</authors>
<title>Nearest neighbor pattern classification.</title>
<date>1967</date>
<booktitle>Institute of Electrical and Electronics Engineers Transactions on Information Theory,</booktitle>
<pages>13--21</pages>
<contexts>
<context position="13522" citStr="Cover and Hart, 1967" startWordPosition="2136" endWordPosition="2139">ntaining 18 features, plus the manually assigned expedition class. 4 Methodology The majority of the experiments that were carried out in an attempt to infer the expedition information from the database involved machine learning. Therefore in this section three algorithms for supervised learning are described, followed by a clustering algorithm for unsupervised learning. This section is concluded with a description of the evaluation metrics for clusters used by the different approaches. Algorithms The first algorithm that was used is the k-Nearest Neighbour algorithm (k-NN) (Aha et al., 1991; Cover and Hart, 1967; DeVijver and Kittler, 1982). This algorithm is an example of a lazy learner: it does not model the training data it is given, but simply stores each instance of the training data in memory. During classification it compares the item it needs to classify to each item in its memory and assigns the majority class of the closest k (in these experiments k=1) instances to the new item. To determine which instances are closest, a variety of distance metrics can be applied. In this experiment the standard settings in the TiMBL implementation (Daelemans et al., 2004), developed at the ILK research gr</context>
</contexts>
<marker>Cover, Hart, 1967</marker>
<rawString>T. M. Cover and P. E. Hart. 1967. Nearest neighbor pattern classification. Institute of Electrical and Electronics Engineers Transactions on Information Theory, 13:21–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Jakub Zavrel</author>
<author>Ko Van der Sloot</author>
<author>Antal Van den Bosch</author>
</authors>
<title>Timbl: Tilburg memory based learner, version 5.1, reference guide.</title>
<date>2004</date>
<tech>Technical Report 04-02,</tech>
<institution>ILK/Tilburg University.</institution>
<marker>Daelemans, Zavrel, Van der Sloot, Van den Bosch, 2004</marker>
<rawString>Walter Daelemans, Jakub Zavrel, Ko Van der Sloot, and Antal Van den Bosch. 2004. Timbl: Tilburg memory based learner, version 5.1, reference guide. Technical Report 04-02, ILK/Tilburg University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data using the em algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society. Series B (Methodology),</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="16746" citStr="Dempster et al., 1977" startWordPosition="2685" endWordPosition="2688">mbers may be specified to steer the algorithm, but other than that it runs on its own. For an extensive overview of clustering methods see Jain et al., (1999). For this work, the options in choosing an implementation of a clustering algorithm were limited because many data mining tools are designed vNB = P(aiIvj) (3) 20 only for numerical data, therefore the WEKA machine learning environment was also used for the clustering experiments. As clustering is computationally expensive, it was only possible to run experiments with WEKA’s implementation of the Expectation Maximisation (EM) algorithm (Dempster et al., 1977). Preliminary experiments with other algorithms indicated execution times in the order of months. The EM algorithm iteratively tries to converge to a maximum likelihood by first computing an expectation of the likelihood of a certain clustering, then maximising this likelihood by computing the maximum likelihood estimates of the features. Termination of the algorithm occurs when the predefined number of iterations has been carried out, or when the overall likelihood (the measure of how ‘good’ a clustering is) does not increase significantly with each iteration. Cluster Evaluation Since the dat</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum likelihood from incomplete data using the em algorithm. Journal of the Royal Statistical Society. Series B (Methodology), 39(1):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P A DeVijver</author>
<author>J Kittler</author>
</authors>
<title>Pattern recognition. A statistical approach.</title>
<date>1982</date>
<publisher>Prentice-Hall,</publisher>
<location>London.</location>
<contexts>
<context position="13551" citStr="DeVijver and Kittler, 1982" startWordPosition="2140" endWordPosition="2143">plus the manually assigned expedition class. 4 Methodology The majority of the experiments that were carried out in an attempt to infer the expedition information from the database involved machine learning. Therefore in this section three algorithms for supervised learning are described, followed by a clustering algorithm for unsupervised learning. This section is concluded with a description of the evaluation metrics for clusters used by the different approaches. Algorithms The first algorithm that was used is the k-Nearest Neighbour algorithm (k-NN) (Aha et al., 1991; Cover and Hart, 1967; DeVijver and Kittler, 1982). This algorithm is an example of a lazy learner: it does not model the training data it is given, but simply stores each instance of the training data in memory. During classification it compares the item it needs to classify to each item in its memory and assigns the majority class of the closest k (in these experiments k=1) instances to the new item. To determine which instances are closest, a variety of distance metrics can be applied. In this experiment the standard settings in the TiMBL implementation (Daelemans et al., 2004), developed at the ILK research group at Tilburg University, we</context>
</contexts>
<marker>DeVijver, Kittler, 1982</marker>
<rawString>P. A. DeVijver and J. Kittler. 1982. Pattern recognition. A statistical approach. Prentice-Hall, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Fayyad</author>
<author>R Uthurusamy</author>
</authors>
<title>Data mining and knowledge discovery in databases.</title>
<date>1996</date>
<journal>Communications of the ACM,</journal>
<volume>39</volume>
<issue>11</issue>
<contexts>
<context position="4128" citStr="Fayyad and Uthurusamy, 1996" startWordPosition="652" endWordPosition="655">ned results vary, but prove that it is possible to extract the expedition information from the data at hand. 2 Related Work The field of data mining, which is concerned with the extraction of implicit, previously unknown and potentially useful information from data (Frawley et al., 1992), is a branch of research that has become quite important recently as every day the world is flooded with larger amounts of information that are impossible to analyse manually. Data mining can, for instance, help banks identify suspicious transactions among the millions of transactions that are executed daily (Fayyad and Uthurusamy, 1996), or automatically classify protein sequences in genome databases (Mewes et al., 1999), or aid a company in creating better customer profiles to present customers with personalised ads and notifications (Linden et al., 2003). Knowledge discovery approaches often rely on machine learning techniques as these are particularly well suited to process large amounts of data to find similarities or dissimilarities between instances (Mitchell, 1997). Traditionally, governments and companies have been interested in gaining more insight into their data by applying data mining techniques. Only recently , </context>
</contexts>
<marker>Fayyad, Uthurusamy, 1996</marker>
<rawString>U. Fayyad and R. Uthurusamy. 1996. Data mining and knowledge discovery in databases. Communications of the ACM, 39(11):24–26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William J Frawley</author>
<author>Gregory Piatetsky-Shapiro</author>
<author>Christopher J Matheus</author>
</authors>
<title>Knowledge discovery in databases: An overview.</title>
<date>1992</date>
<journal>AI Magazine,</journal>
<pages>13--57</pages>
<contexts>
<context position="3788" citStr="Frawley et al., 1992" startWordPosition="599" endWordPosition="602">n the database. To this end, three approaches are compared: supervised machine learning, unsupervised machine learning, and rule-based methods. 17 Proceedings of the Workshop on Language Technology for Cultural Heritage Data (LaTeCH 2007), pages 17–24, Prague, 28 June 2007. c�2007 Association for Computational Linguistics The obtained results vary, but prove that it is possible to extract the expedition information from the data at hand. 2 Related Work The field of data mining, which is concerned with the extraction of implicit, previously unknown and potentially useful information from data (Frawley et al., 1992), is a branch of research that has become quite important recently as every day the world is flooded with larger amounts of information that are impossible to analyse manually. Data mining can, for instance, help banks identify suspicious transactions among the millions of transactions that are executed daily (Fayyad and Uthurusamy, 1996), or automatically classify protein sequences in genome databases (Mewes et al., 1999), or aid a company in creating better customer profiles to present customers with personalised ads and notifications (Linden et al., 2003). Knowledge discovery approaches oft</context>
</contexts>
<marker>Frawley, Piatetsky-Shapiro, Matheus, 1992</marker>
<rawString>William J. Frawley, Gregory Piatetsky-Shapiro, and Christopher J. Matheus. 1992. Knowledge discovery in databases: An overview. AI Magazine, 13:57–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Jain</author>
<author>M N Murty</author>
<author>P J Flynn</author>
</authors>
<title>Data clustering: A review.</title>
<date>1999</date>
<journal>ACM Computing Surveys,</journal>
<volume>31</volume>
<issue>3</issue>
<contexts>
<context position="16282" citStr="Jain et al., (1999)" startWordPosition="2611" endWordPosition="2614">ped at the University of Waikato, New Zealand, was used. A quite different machine learning approach that was applied to try to identify expeditions in the reptiles and amphibians database is clustering. Clustering methods are unsupervised, i.e., they do not require annotated data, and in some cases not even the number of expeditions that are in the data. Items in the data set are grouped according to similarity. A maximum dissimilarity between the group members may be specified to steer the algorithm, but other than that it runs on its own. For an extensive overview of clustering methods see Jain et al., (1999). For this work, the options in choosing an implementation of a clustering algorithm were limited because many data mining tools are designed vNB = P(aiIvj) (3) 20 only for numerical data, therefore the WEKA machine learning environment was also used for the clustering experiments. As clustering is computationally expensive, it was only possible to run experiments with WEKA’s implementation of the Expectation Maximisation (EM) algorithm (Dempster et al., 1977). Preliminary experiments with other algorithms indicated execution times in the order of months. The EM algorithm iteratively tries to </context>
</contexts>
<marker>Jain, Murty, Flynn, 1999</marker>
<rawString>A. K. Jain, M. N. Murty, and P. J. Flynn. 1999. Data clustering: A review. ACM Computing Surveys, 31(3):264–323, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bjorner Larsen</author>
<author>Chinatsu Aone</author>
</authors>
<title>Fast and effective text mining using linear-time document clustering.</title>
<date>1999</date>
<booktitle>In Proceedings of KDD-99,</booktitle>
<location>San Diego, CA.</location>
<contexts>
<context position="19607" citStr="Larsen and Aone, 1999" startWordPosition="3163" endWordPosition="3166">ed in information retrieval. In information retrieval recall is the proportion of relevant documents retrieved out of the total set of relevant documents. When applied to clustering a ‘relevant document’ is an instance that is assigned correctly to a certain expedition, the set of all relevant documents is the set of all instances beloning to that expedition. Precision is the number of relevant documents retrieved from the total number of documents. So when applied to cluster evaluation this means the number of instances of an expedition that were retrieved from the total number of instances (Larsen and Aone, 1999). This boils down to Equations 6 and 7 in which x stands for expedition, y for cluster, nxy for the number of instances belonging to expedition x that were assigned to cluster y, and nx is the number of items in expedition x. nxy Recall(x, y) = nx nxy Precision(x, y) = ny The F-measure for a cluster y with respect to expedition x is then computed via Equation 8. The F-measure of the entire set of clusters is computed through the function in Equation 9, which takes the weighted average of the maximum F-measure per expedition. 2 · Recall(x, y) · Precision(x, y) F(x, y) = Precision(x, y) + Recall</context>
</contexts>
<marker>Larsen, Aone, 1999</marker>
<rawString>Bjorner Larsen and Chinatsu Aone. 1999. Fast and effective text mining using linear-time document clustering. In Proceedings of KDD-99, San Diego, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Linden</author>
<author>B Smith</author>
<author>J York</author>
</authors>
<title>Amazon.com recommendations: item-to-item collaborative filtering.</title>
<date>2003</date>
<journal>IEEE Internet Computing,</journal>
<volume>7</volume>
<issue>1</issue>
<pages>Jan/Feb.</pages>
<contexts>
<context position="4352" citStr="Linden et al., 2003" startWordPosition="686" endWordPosition="690">ally useful information from data (Frawley et al., 1992), is a branch of research that has become quite important recently as every day the world is flooded with larger amounts of information that are impossible to analyse manually. Data mining can, for instance, help banks identify suspicious transactions among the millions of transactions that are executed daily (Fayyad and Uthurusamy, 1996), or automatically classify protein sequences in genome databases (Mewes et al., 1999), or aid a company in creating better customer profiles to present customers with personalised ads and notifications (Linden et al., 2003). Knowledge discovery approaches often rely on machine learning techniques as these are particularly well suited to process large amounts of data to find similarities or dissimilarities between instances (Mitchell, 1997). Traditionally, governments and companies have been interested in gaining more insight into their data by applying data mining techniques. Only recently , digitisation of data in the cultural heritage domain has taken off, which means that there has not been much work done on knowledge discovery in this domain. Databases in this domain are often created and maintained manually</context>
</contexts>
<marker>Linden, Smith, York, 2003</marker>
<rawString>G. Linden, B. Smith, and J. York. 2003. Amazon.com recommendations: item-to-item collaborative filtering. IEEE Internet Computing, 7(1):76–80, Jan/Feb.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q McNemar</author>
</authors>
<date>1962</date>
<publisher>Psychological Statistics. Wiley,</publisher>
<location>New York.</location>
<contexts>
<context position="21610" citStr="McNemar, 1962" startWordPosition="3512" endWordPosition="3513">et, which yielded accuracies between 88% and 98%. Feature selection experiments with the C4.5 decision tree algorithm indicated that features ‘town/village’, ‘collection number’, ‘registration number’, ‘collector’ and ‘collection date’ were considered most informative for this task, hence the experiments were repeated with a data set containing only those features. The results of both series of experiments are to be found in Table 1. For the C4.5 and Naive Bayes experiments the accuracy deteriorates significantly when using only the selected features (α = 0.05, computing using McNemar’s test (McNemar, 1962)), but it stays stable for the k-NN classifier. This indicates that not all data is needed to infer the expeditions, but that it matters greatly which approach is taken. However, as neither of the algorithm benefits from it, feature selection was not further explored. Algorithm All feat. Sel. feat. k-NN 95.9% 95.9% C.4.5 98.3% 94.4% NaiveBayes 88.1% 73.5% Table 1: Accuracy of supervised machine learning experiments using all features and selected features In these experiments all database entries were annotated with expedition information, which in a real setting is of course not the case. Thr</context>
</contexts>
<marker>McNemar, 1962</marker>
<rawString>Q. McNemar. 1962. Psychological Statistics. Wiley, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H W Mewes</author>
<author>K Heumann</author>
<author>A Kaps</author>
<author>K Mayer</author>
<author>F Pfeiffer</author>
<author>S Stocker</author>
<author>D Frishman</author>
</authors>
<title>Mips: a database for genomes and protein sequences.</title>
<date>1999</date>
<journal>Nucleic Acids Research,</journal>
<volume>27</volume>
<issue>1</issue>
<contexts>
<context position="4214" citStr="Mewes et al., 1999" startWordPosition="664" endWordPosition="667">data at hand. 2 Related Work The field of data mining, which is concerned with the extraction of implicit, previously unknown and potentially useful information from data (Frawley et al., 1992), is a branch of research that has become quite important recently as every day the world is flooded with larger amounts of information that are impossible to analyse manually. Data mining can, for instance, help banks identify suspicious transactions among the millions of transactions that are executed daily (Fayyad and Uthurusamy, 1996), or automatically classify protein sequences in genome databases (Mewes et al., 1999), or aid a company in creating better customer profiles to present customers with personalised ads and notifications (Linden et al., 2003). Knowledge discovery approaches often rely on machine learning techniques as these are particularly well suited to process large amounts of data to find similarities or dissimilarities between instances (Mitchell, 1997). Traditionally, governments and companies have been interested in gaining more insight into their data by applying data mining techniques. Only recently , digitisation of data in the cultural heritage domain has taken off, which means that t</context>
</contexts>
<marker>Mewes, Heumann, Kaps, Mayer, Pfeiffer, Stocker, Frishman, 1999</marker>
<rawString>H. W. Mewes, K. Heumann, A. Kaps, K. Mayer, F. Pfeiffer, S. Stocker, and D. Frishman. 1999. Mips: a database for genomes and protein sequences. Nucleic Acids Research, 27(1):44–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom M Mitchell</author>
</authors>
<date>1997</date>
<journal>Machine Learning. McGrawHill.</journal>
<contexts>
<context position="4572" citStr="Mitchell, 1997" startWordPosition="719" endWordPosition="720">nually. Data mining can, for instance, help banks identify suspicious transactions among the millions of transactions that are executed daily (Fayyad and Uthurusamy, 1996), or automatically classify protein sequences in genome databases (Mewes et al., 1999), or aid a company in creating better customer profiles to present customers with personalised ads and notifications (Linden et al., 2003). Knowledge discovery approaches often rely on machine learning techniques as these are particularly well suited to process large amounts of data to find similarities or dissimilarities between instances (Mitchell, 1997). Traditionally, governments and companies have been interested in gaining more insight into their data by applying data mining techniques. Only recently , digitisation of data in the cultural heritage domain has taken off, which means that there has not been much work done on knowledge discovery in this domain. Databases in this domain are often created and maintained manually and are thus often significantly smaller than automatically generated databases from, for example, customers’ purchase information in a large company. This means it is not clear whether data mining techniques, aimed at </context>
</contexts>
<marker>Mitchell, 1997</marker>
<rawString>Tom M. Mitchell. 1997. Machine Learning. McGrawHill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Quinlan</author>
</authors>
<title>Induction of decision trees.</title>
<date>1986</date>
<booktitle>Machine Learning,</booktitle>
<pages>1--81</pages>
<contexts>
<context position="14586" citStr="Quinlan, 1986" startWordPosition="2331" endWordPosition="2332">etrics can be applied. In this experiment the standard settings in the TiMBL implementation (Daelemans et al., 2004), developed at the ILK research group at Tilburg University, were used. The standard distance metric in the TiMLB implementation of k-NN is the Overlap metric, given in Equations1 and 2. A(X,Y) is the distance between instances X and Y, represented by n features, where 6 is the distance between the features. n A(X, Y ) = s(xi, yi) (1) i=1 where: s(xi, yi) = { abs if numeric, else (2) 0 ifxi = yi 1 ifxi =� yi The second algorithm that was used is the C4.5 decision tree algorithm (Quinlan, 1986). In the learning phase, it creates a decision tree in a recursive top-down process in which the database is partitioned according to the feature that separates the classes best; each node in the tree represents one partition. Deeper nodes represent more classhomogeneous partitions. During classification, C4.5 traverses the tree in a deterministic top-down pass until it meets a class-homogeneous end node, or a non-ending node when a feature-value test is not represented in the tree. Naive Bayes is the third algorithm that was used in the experiments. It computes the probability of a certain ex</context>
</contexts>
<marker>Quinlan, 1986</marker>
<rawString>J. R. Quinlan. 1986. Induction of decision trees. Machine Learning, 1:81–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claude E Shannon</author>
</authors>
<title>A mathematical theory of communication.</title>
<date>1948</date>
<journal>Bell System Technical Journal,</journal>
<pages>27--379</pages>
<contexts>
<context position="17544" citStr="Shannon, 1948" startWordPosition="2810" endWordPosition="2811">ing an expectation of the likelihood of a certain clustering, then maximising this likelihood by computing the maximum likelihood estimates of the features. Termination of the algorithm occurs when the predefined number of iterations has been carried out, or when the overall likelihood (the measure of how ‘good’ a clustering is) does not increase significantly with each iteration. Cluster Evaluation Since the data is annotated with expedition information it was possible to use external quality measures (Steinbach et al., 2000). Three different evaluation measures were used: accuracy, entropy (Shannon, 1948), and the F-measure (van Rijsbergen, 1979). The evaluation of results for the supervised learning algorithms was calculated in a straightforward way: because the classifier knows which expeditions there are and which entries belong to which expedition, it checks the expeditions it assigned to the database entries to the manually assigned expeditions and reports the overlap as accuracy. It gets a little bit more complicated with entropy. Entropy is a measure of informativity, i.e., the minimum number of bits of information needed to encode the classification of each instance. If the expedition </context>
</contexts>
<marker>Shannon, 1948</marker>
<rawString>Claude E. Shannon. 1948. A mathematical theory of communication. Bell System Technical Journal, 27:379–423, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Steinbach</author>
<author>George Karypis</author>
<author>Vipin Kumar</author>
</authors>
<title>A comparison of document clustering techniques.</title>
<date>2000</date>
<tech>Technical report,</tech>
<institution>Department of Computer Science, University of Minnesota.</institution>
<contexts>
<context position="17462" citStr="Steinbach et al., 2000" startWordPosition="2796" endWordPosition="2799">ths. The EM algorithm iteratively tries to converge to a maximum likelihood by first computing an expectation of the likelihood of a certain clustering, then maximising this likelihood by computing the maximum likelihood estimates of the features. Termination of the algorithm occurs when the predefined number of iterations has been carried out, or when the overall likelihood (the measure of how ‘good’ a clustering is) does not increase significantly with each iteration. Cluster Evaluation Since the data is annotated with expedition information it was possible to use external quality measures (Steinbach et al., 2000). Three different evaluation measures were used: accuracy, entropy (Shannon, 1948), and the F-measure (van Rijsbergen, 1979). The evaluation of results for the supervised learning algorithms was calculated in a straightforward way: because the classifier knows which expeditions there are and which entries belong to which expedition, it checks the expeditions it assigned to the database entries to the manually assigned expeditions and reports the overlap as accuracy. It gets a little bit more complicated with entropy. Entropy is a measure of informativity, i.e., the minimum number of bits of in</context>
</contexts>
<marker>Steinbach, Karypis, Kumar, 2000</marker>
<rawString>Michael Steinbach, George Karypis, and Vipin Kumar. 2000. A comparison of document clustering techniques. Technical report, Department of Computer Science, University of Minnesota.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cornelis Joost van Rijsbergen</author>
</authors>
<date>1979</date>
<journal>Information Retrieval. Buttersworth.</journal>
<marker>van Rijsbergen, 1979</marker>
<rawString>Cornelis Joost van Rijsbergen. 1979. Information Retrieval. Buttersworth.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Eibe Frank</author>
</authors>
<title>Data Mining: Practical machine learning tools and techniques.</title>
<date>2005</date>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco,</location>
<contexts>
<context position="15646" citStr="Witten and Frank, 2005" startWordPosition="2502" endWordPosition="2505">feature-value test is not represented in the tree. Naive Bayes is the third algorithm that was used in the experiments. It computes the probability of a certain expedition, given the observed training data according to the formula given in Equation 3. In this formula vNB is the target expedition value, chosen from the maximally probably hypothesis (argmax υj�V P(vj), i.e., the expedition with the highest probability) given the product of the probabilities � of the features (i P(ai|vj)). argmax �7 vjEV P(vj)11 i For both the C4.5 algorithm and Naive Bayes the WEKA machine learning environment (Witten and Frank, 2005), that was developed at the University of Waikato, New Zealand, was used. A quite different machine learning approach that was applied to try to identify expeditions in the reptiles and amphibians database is clustering. Clustering methods are unsupervised, i.e., they do not require annotated data, and in some cases not even the number of expeditions that are in the data. Items in the data set are grouped according to similarity. A maximum dissimilarity between the group members may be specified to steer the algorithm, but other than that it runs on its own. For an extensive overview of cluste</context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>Ian H. Witten and Eibe Frank. 2005. Data Mining: Practical machine learning tools and techniques. Morgan Kaufmann, San Francisco, 2nd edition.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>